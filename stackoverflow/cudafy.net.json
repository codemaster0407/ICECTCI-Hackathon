[
    {
        "question": "[Cudafy] private static void LevenshteinGpu3(GThread thread, char[] source, char[] pattern, int firstDim, byte compareLength, byte[] dev_results) { int tid = thread.threadIdx.x + thread.blockIdx.x * thread.blockDim.x; byte[,,] dev_levMatrix_1 = _gpu.Allocate&lt;byte&gt;(20, 20, 20); for (byte j = 0; j &lt;= compareLength; j++) { dev_levMatrix_1[tid, 0, j] = j; dev_levMatrix_1[tid, j, 0] = j; } if (tid &lt; firstDim) { for (int i = 1; i &lt;= compareLength; i++) { for (int j = 1; j &lt;= compareLength; j++) { int iMinusOne = i - 1; int jMinusOne = j - 1; if (tid + iMinusOne &lt; source.Length &amp;&amp; source[tid + iMinusOne] == pattern[jMinusOne]) { dev_levMatrix_1[tid, i, j] = dev_levMatrix_1[tid, iMinusOne, jMinusOne]; } else { byte x = dev_levMatrix_1[tid, iMinusOne, j]; if (x &gt; dev_levMatrix_1[tid, i, jMinusOne]) x = dev_levMatrix_1[tid, i, jMinusOne]; if (x &gt; dev_levMatrix_1[tid, iMinusOne, jMinusOne]) x = dev_levMatrix_1[tid, iMinusOne, jMinusOne]; dev_levMatrix_1[tid, i, j] = ++x; } } } dev_results[tid] = dev_levMatrix_1[tid, compareLength, compareLength]; } } I'm using code of Konrad-Ziarko on github. But when I put byte[,,] dev_levMatrix_1 = _gpu.Allocate&lt;byte&gt;(20, 20, 20); in the code and run. Cuda showed error 719. But if I put dev_levMatrix_1 like below, it works: private static void LevenshteinGpu3(GThread thread, char[] source, char[] pattern, dev_levMatrix_1, int firstDim, byte compareLength, byte[] dev_results) Can I can put dev_levMatrix_1 in a function?",
        "answers": [
            [
                "Little late but... You can't allocate GPU memory inside Cudafy decorated function. Allocation should happen before passing parameters to kernel. Inside kernel function you can create local variables and use them. Also, you could find help faster asking questions at the source not somewhere else and mention source."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "So I have 2 classes, both classes call CudafyTranslator.Cudafy and cudafy their appropriate methods. The resulting modules are then added to the GPU. Why does cudafy keep giving me a compile exception when I call one cudafied method from one class, from within a cudafied method of another class. I know I can call cudafied methods from other cudafied methods within the same class, so why doesn't this work? //Class 1: public class Class1 { [Cudafy] public static void method1() { //Do stuff. } } //Class 2: public class Class2 { [Cudafy] public static void method2() { Class1.method1(); } }",
        "answers": [
            [
                "Your source look OK. I tested from my side with cudafy.net version 1.29.5576.13786. I guest you can update your cudafy.net, and then clean and rebuild your solution will help. Regard!"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I can't believe after all the research and reading I've done I am still not 100% clear on how to do this, so I must ask.. I am trying to get something like the following to run on a gpu card and I am using Cudafy.Net to generate the Cuda C equivalent. I want to get this to run as fast as possible. If I have a function (simplified) such as: Transform() { for (lgDY = 0; lgDY &lt; lgeHeight; lgDY++) { for (lgDX = 0; lgDX &lt; lgeWidth; lgDX++) { // do a lot of stuff with lgDY and lgDX like stuff a matrix } } } I am invoking this with the Launch() function as follows: gpu.Launch(blocksize, threadsize, \"Transform\", args...) I am familiar with the GThread passed as first argument, and blocksize.x, blockdim.x and threadsize.x, and also the y and z for the block. I am having a hard time understanding if the for statements go away and I replace them with a test sort of like if ( y &lt; lgeHeight ) if ( x &lt; lgeWidth ) ... But then have no idea how to \"tie each iteration to an incremented lgDY and lgDX. I apologize if it's something blatantly obvious or if I haven't described what I am trying to do accurately. Just confused on how to make the nested loop correct. I appreciate any and all help to get me moving in the right direction.",
        "answers": [
            [
                "It depends on the size of lgeHeight and lgeWidth. If the product of them is less than the threads on the card, then when you launch the kernel you can assume that each thread will run on one pair of x and y. lgDY = threadIdx.x lgDX = blockIdx.x Then you can compute them all at once. If you have more threads than the product, then you will need to divide the problem up into smaller pieces or run a small iteration for each matrix."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Trying to get my head around cuda, after not grasping similar stackoverflow questions i decided to test out an example (i'm using cudafy.net for c# but the underlying cuda should be parsable) I want to do the following. Send a 4x4x4 matrix to the kernel and get a 4x4x4 out according to this logic: if(input[x,y,z] == 1) output[x+1, y, z]++; if(input[x,y,z] == 2) output[x-1, y, z]++; I studied the following cudafy example. public const int N = 1 * 1024; //Omissions gpu.Launch(128, 1, function, dev_a, dev_b, dev_c); kernel: [Cudafy] public static void add_0(GThread thread, int[] a, int[] b, int[] c) { int tid = thread.blockIdx.x; // (tid 0 -&gt; 127, from my understanding) while (tid &lt; N) { c[tid] = a[tid] + b[tid]; tid += thread.gridDim.x; } } And then tried to transfer it to 3d. I cannot get the indexing right. Say i have the following. (three arrays here just to test indexing) int size = 4; int[] dev_delta = gpu.Allocate&lt;int&gt;(size * size * size); int[] dev_space = gpu.Allocate&lt;int&gt;(size * size * size); int[] dev_result = gpu.Allocate&lt;int&gt;(size * size * size); gpu.Launch(new dim3(4, 4, 4), 1, \"testIndex\", dev_delta, dev_space, dev_result); And the kernel: [Cudafy] public static void testIndex(GThread thread, int[] delta, int[] space, int[] result) { int x = thread.blockIdx.x; int y = thread.blockIdx.y; int z = thread.blockIdx.z; delta[x]++; space[y]++; result[z]++; } Naively I'd expect the following: delta = {4,4,4,4,0,0,0,0,0, ... 0,0} space = {4,4,4,4,0,0,0,0,0, ... 0,0} result = {4,4,4,4,0,0,0,0,0 ... 0,0} But i get: delta = {1,1,1,1,0,0,0,0,0, ... 0,0} space = {1,1,1,1,0,0,0,0,0, ... 0,0} result = {1,0,0,0,0,0,0,0,0 ... 0,0} This makes no sense to me, clearly i am missing something. Questions: How many threads am i starting? How do you go about 'indexing' my example problem in 3 dimensions (Starting 4x4x4 threads and getting the variables for flat3DArray[x * sizeY * sizeZ + y * sizeZ + z])? How do you go about 'indexing' my example problem in 2 dimensions? (Starting 4x4 threads and then let each thread handle a depth column of length 4) I found this which may be relevant Why is z always zero in CUDA kernel if that is what is messing me up, i'd still appreciate pure-cuda answers to sort my brain out",
        "answers": [
            [
                "How many threads am I starting ? You are starting 1 thread per block, hence 16 total since the Z parameter is not used. For better performance, I would recommend also using threads (at least 128, and multiple of 32 anyways). How do you go about 'indexing' my example problem in 3 dimensions (Starting 4x4x4 threads and getting the variables for flat3DArray[x * sizeY * sizeZ + y * sizeZ + z])? The second parameter of gpu.Launch method is for threads. x, y and z could hence be threadIdx.x, threadIdx.y and threadIdx.z respectively. But you may also want to use many blocks, thus threadIdx.x + blockDim.x * blockIdx.x could be a good peak. The link you provided here explains why your Z dimension is not relevant. CUDAfy.Net exposes the launch function that further calls cuda runtime CUDA/C API call. When passing parameters from dot net to native environment, it seems that CUDAfy.Net simply ignores the Z argument leaving it to one. (this is most probably due to the fact that early versions of CUDA did not support the Z parameter different than one). The explanation is not pure-cuda because CUDA now supports Z value different than one, but your parameter is simply ignored in the CUDAfy.Net implementation."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I've developed an asp.net core 1.1 application that does some pretty intensive data processing. I'd like to: GPU-accelerate the data processing (preferably opencl over cuda because my development laptop doesn't have an nvidia gpu but that's not a show stopper) Off-load the data processing to another server so the web server doesn't get bogged down crunching data. Stick with C# language and avoid C++/C if at all possible. Run everything on linux boxes/VMs All the data required to perform the analysis is stored in a PostgreSQL database (I use npgsql to access it) but the results of the computation are kept in memory. The application outputs dynamically created tiles that get added as a Bing map layer so if I do #2 (off-load data processing) it would also need to handle web requests for tiles because I want that part to be GPU accelerated as well. Is there a recommended direction for accomplishing this? I looked at Cudafy.Net, but it's not compatible with .Net Core 1.1 (or 1.0 from what I can tell as well). So I think asp.net on linux using Mono might be the best course of action? I really don't know.. I'm just a hack of a programmer. Update: Item 1 and 3 is possible. Create an ASP.NET Core Web Application targeting .NET Framework and add Cudafy.NET via NuGet. Install your CUDA/OpenCL driver of choice. First half of 4 is possible as I successfully ran an OpenCL test on a Ubuntu VM using the AMD APP SDK 3.0. However, the latter half is problematic as it appears very difficult to access the GPU from a VM (need PCI-Passthrough). The AMD driver uses the CPU when there is no GPU available so at least I had CPU-acceleration. I guess I would be better off not trying to virtualize the GPU-accelerated app when I figure out how to do Item 2.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a struct and a class with Execute() method and a cudafying method uses the struct. PROBLEM: Cuda says: \".../CUDAFYSOURCETEMP.cu(3): error: identifier \"PointGPU\" is undefined\" [Cudafy] public struct PointGPU { public double x; public double y; public double z; public PointGPU(double xVal, double yVal, double zVal) { x = xVal; y = yVal; z = zVal; } } class MarchingCubesOnGPU { CudafyModule km = CudafyTranslator.Cudafy(); GPGPU gpu = CudafyHost.GetDevice(); private static PointGPU[] pointsGpu = new PointGPU[8]; public void Execute() { gpu.LoadModule(km); PointGPU[] dev_points = gpu.CopyToDevice(pointsGpu); gpu.Launch().PolygoniseOnGpu(dev_points, ...); ... } [Cudafy] public static void PolygoniseOnGpu(PointGPU[] p, ...) { ... } public List&lt;Triangle&gt; setPolygoniseParameters(...) { pointsGpu = ... } } This line in CudafySourceTemp.cu is extern \"C\" __global__ void PolygoniseOnGpu( PointGPU* p, ...);",
        "answers": [
            [
                "The decision is: CudafyModule km = CudafyTranslator.Cudafy(typeof(PointGPU));"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm rewriting algorithm to GPU with Cudafy. I need to call Execute() from a static method. It's necessary to calculate on GPU. How could I do this? What fields or anything should I copy into static? Object of the class is called from non-static method and it couldn't be changed. It creates an objects, makes Execute (ideally), and gets triangles as result. The class code is: using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Windows.Media.Media3D; using DICOMViewer.Helper; using DICOMViewer.Volume; using DICOMViewer.ImageFlow; using Cudafy; using Cudafy.Host; using Cudafy.Translator; namespace DICOMViewer.GPU { class MarchingCubesOnGPU { private static List&lt;Triangle&gt; theTriangles; // CPU fields private Point3D[] points = new Point3D[8]; private int[] values = new int[8]; private double theIsoValue; private List&lt;Triangle&gt; triangles; // GPU fields private static PointGPU[] pointsGpu = new PointGPU[8]; static int[] valsGpu = new int[8]; private static double[] isolevelGpu = new double[1]; private static TriangleGPU[] trianglesGpu; public static int[] EdgeTableStatic = new int[256] { // here are the values for edgeTable from the algorithm link }; public static int[,] TriTableStatic = new int[256, 16] { // here are the values for triTable from the algorithm link }; public MarchingCubesOnGPU(GridCell grid, double isolevel, List&lt;Triangle&gt; theTriangleList) { points = grid.p; values = grid.val; this.theIsoValue = isolevel; triangles = theTriangleList; theTriangles = new List&lt;Triangle&gt;(); ConvertToStandardTypes(); Execute(); // ??? } public List&lt;Triangle&gt; getTriangles() { return theTriangles; } // Convert fields for GPU public void ConvertToStandardTypes() { for (int i = 0; i &lt; points.Length; i++) { pointsGpu[i].x = points[i].X; pointsGpu[i].y = points[i].Y; pointsGpu[i].z = points[i].Z; } valsGpu = values; isolevelGpu[0] = theIsoValue; for (int i = 0; i &lt; triangles.Count; i++) { trianglesGpu[i].p[i].x = triangles[i].p[i].X; trianglesGpu[i].p[i].y = triangles[i].p[i].Y; trianglesGpu[i].p[i].z = triangles[i].p[i].Z; } } public static void Execute() { CudafyModule km = CudafyTranslator.Cudafy(); GPGPU gpu = CudafyHost.GetDevice(CudafyModes.Target, CudafyModes.DeviceId); gpu.LoadModule(km); TriangleGPU[] tris; PointGPU[] dev_points = gpu.Allocate(pointsGpu); int[] dev_values = gpu.Allocate(valsGpu); double[] dev_iso = gpu.Allocate&lt;double&gt;(); TriangleGPU[] dev_triangles = gpu.Allocate(trianglesGpu); int[] dev_edgeTable = gpu.Allocate&lt;int&gt;(); int[,] dev_triangleTable = gpu.Allocate(TriTableStatic); gpu.CopyToDevice(pointsGpu, dev_points); gpu.CopyToDevice(valsGpu, dev_values); gpu.CopyToDevice(isolevelGpu, dev_iso); gpu.CopyToDevice(trianglesGpu, dev_triangles); gpu.CopyToDevice(EdgeTableStatic, dev_edgeTable); gpu.CopyToDevice(TriTableStatic, dev_triangleTable); gpu.Launch().PolygoniseOnGpu(dev_iso, dev_edgeTable, dev_triangleTable, dev_points, dev_values, dev_triangles); for (int k = 0; k &lt; dev_triangles.Length; k++) { gpu.CopyFromDevice(dev_triangles, out trianglesGpu[k]); } for (int i = 0; i &lt; trianglesGpu.Length; i++) { theTriangles[i].p[i].X = trianglesGpu[i].p[i].x; theTriangles[i].p[i].Y = trianglesGpu[i].p[i].y; theTriangles[i].p[i].Z = trianglesGpu[i].p[i].z; } gpu.FreeAll(); } [Cudafy] public void PolygoniseOnGpu(double[] iso, int[] edgeT, int[,] triT, PointGPU[] p, int[] v, TriangleGPU[] tri) { int cubeindex = 0; for (int i = 0; i &lt; 8; ++i) { if (valsGpu[i] &lt; iso[0]) cubeindex |= 1 &lt;&lt; i; } if (EdgeTableStatic[cubeindex] == 0) return; PointGPU[] vertList = new PointGPU[12]; // Find the vertices where the surface intersects the cube for (int id = 1, count = 0; id &lt; 2048; id *= 2, count++) { if ((edgeT[cubeindex] &amp; id) &gt; 0) vertList[count] = VertexInterpolation(iso[0], p[count], p[count + 1], v[count], v[count + 1]); } // Create the triangle for (int i = 0; triT[cubeindex, i] != -1; i += 3) { TriangleGPU triangle = new TriangleGPU(3); triangle.p[0] = vertList[triT[cubeindex, i]]; triangle.p[1] = vertList[triT[cubeindex, i + 1]]; triangle.p[2] = vertList[triT[cubeindex, i + 2]]; tri[i] = triangle; } } } [Cudafy] public struct TriangleGPU { public PointGPU[] p; public TriangleGPU(int t) { p = new PointGPU[t]; } } [Cudafy] public struct PointGPU { public double x; public double y; public double z; public PointGPU(double x, double y, double z) { this.x = x; this.y = y; this.z = z; } } } ADDED: Execute is a static method as it should be but it could be called only from static. In other case the line: CudafyModule km = CudafyTranslator.Cudafy(); doesn't work because it is not supported from non-static calles Execute. In other words, what fields or anything else should I create and fill in new static method to have an independent static entity to call execute?",
        "answers": [
            [
                "Sorry for disinformation. The problem was in just public void PolygoniseOnGpu(...). It wasn't noted as static."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am using Cudafy as c# wrapper I need to get colour info InputBitmap0.GetPixel(x, y) of a bitmap and make an new bitmap for output . I need the following work to be done in GPU. IN CPU OutputBitmap.SetPixel(object_point_x, object_point_y, InputBitmap0.GetPixel(x, y)); In short: How to GetPixel() for each pixel point of the input Bitmap, SetPixel() for each pixel point of the outputbitmap Bitmap in GPU.",
        "answers": [
            [
                "OutputBitmap.SetPixel(object_point_x, object_point_y, InputBitmap0.GetPixel(x, y)) It took time but finally, I , cracked my case. We have two Bitmap : one for output OutputBitmap and another for input InputBitmap0 Lets divide this task into parts: do InputBitmap0.GetPixel() for x ,y coordinate then , OutputBitmap.SetPixel() for a different coordinate object_point_x, object_point_y Cudafy does not support Bitmap or Color type data. So I converted the Bitmaps to byte type. BitmapData InputBitmapData0 = InputBitmap0.LockBits(new Rectangle(0, 0, InputBitmap0.Width, InputBitmap0.Height), ImageLockMode.ReadOnly, PixelFormat.Format32bppArgb); IntPtr ptr0 = InputBitmapData0.Scan0;//pointer for color int stride0 = InputBitmapData0.Stride; byte[] input_ragba_color = new byte[InputBitmapData0.Stride * InputBitmap0.Height]; Marshal.Copy(ptr0, input_ragba_color, 0, bytes0);// Copy the RGB values of color value into the array. We have copied the content of the InputBitmap0 to the rgbValues array. Now we need to do the work of GetPixel() (get the values of R,G,B,A). We need to do the above work ( make array) for OutputBitmap too because we will be doing SetPixel() in GPU but we will copy the array back to the bitmap later. BitmapData OutputBitmapData = OutputBitmap.LockBits(new Rectangle(0, 0, OutputBitmap.Width, OutputBitmap.Height), ImageLockMode.WriteOnly, OutputBitmap.PixelFormat); IntPtr ptr_output = OutputBitmapData.Scan0; byte[] output_ragba = new byte[OutputBitmapData.Stride * OutputBitmap.Height]; Its GPU time for calculation. Lets initialize gpu. CudafyModule km = new CudafyTranslator.Cudafy(); GPGPU gpu = new CudafyHost.getDevice(CudafyModes.Target, CudafyModes.DeviceId); gpu.LoadModule(km); Now send input_ragba_color and output_ragba to the gpu because we can iterate the array and do any calculation. byte[] dev_output_rgba_color = gpu.Allocate&lt;byte&gt;(output_ragba.Length); byte[] dev_input_ragba_color = gpu.CopyToDevice(input_ragba_color); gpu.Launch(N, 1).update_bitmap(x, y, object_point_x, object_point_y,int stride0, int OutputBitmapData.Stride,dev_input_ragba_color,dev_output_rgba_color); Now inside GPU(kernel) [Cudafy] public static void update_bitmap(GThread thread, int x,int y,int object_point_x,int object_point_y,int stride0, int OutputBitmapData_Stride,byte [] dev_input_ragba_color,byte [] dev_output_rgba_color) { dev_output_rgba_color[(object_point_y * OutputBitmapData_Stride) + (object_point_x * 4)] = input_ragba_color[(y * stride0) + (x * 4)]; dev_output_rgba_color[(object_point_y * OutputBitmapData_Stride) + (object_point_x * 4) + 1] = input_ragba_color[(y * stride0) + (x * 4) + 1]; dev_output_rgba_color[(object_point_y * OutputBitmapData_Stride) + (object_point_x * 4) + 2] = input_ragba_color[(y * stride0) + (x * 4) + 2]; dev_output_rgba_color[(object_point_y * OutputBitmapData_Stride) + (object_point_x * 4) + 3] = input_ragba_color[(y * stride0) + (x * 4) + 3]; } I am taking values of each R,G,B,A ,ex: input_ragba_color[(y * stride0) + (x * 4) + 1] which is solving 1st task (InputBitmap0.GetPixel()) dev_output_rgba_color is taking the values of input_ragba_color example: dev_output_rgba_color[(object_point_y * OutputBitmapData_Stride) + (object_point_x * 4)] = input_ragba_color[(y * stride0) + (x * 4)]; which is solves our 2nd task (OutputBitmap.SetPixel()) We now know that gpu has populated an array(dev_output_rgba_color) for our OutputBitmap. gpu.CopyFromDevice(dev_output_rgba_color, output_ragba); //dev_output_rgba_color values will be assigned to output_ragba gpu.FreeAll(); Copy the result back to the OutputBitmap using the memory pointer and unlock it from the memory. Marshal.Copy(output_ragba, 0, ptr_output, output_bytes);// Copy the RGB values of color value into the array. OutputBitmap.UnlockBits(OutputBitmapData); Now the OutputBitmap contains the updated values."
            ],
            [
                "I think you will need to use a byte[] and allocate that on the GPU. I've seen you asking around and this answer is a work in progress, I'll keep updating it over the next few days as I get time. CudafyModule km = new CudafyTranslator.Cudafy(); GPGPU gpu = new CudafyHost.getDevice(CudafyModes.Target, CudafyModes.DeviceId); gpu.LoadModule(km); var image = new Bitmap(width, height); image = (Bitmap)Image.FromFile(@\"C:\\temp\\a.bmp\", true); byte[] imageBytes = new byte[width * height * 4]; using(MemoryStream ms = new MemoryStream()) { image.Save(ms, format); imageBytes = ms.ToArray(); } byte[] device_imageBytes = _gpu.CopyToDevice(imageBytes); byte r = 0; byte g = 0; byte b = 0; byte device_r = _gpu.Allocate&lt;byte&gt;(r); byte device_g = _gpu.Allocate&lt;byte&gt;(g); byte device_b = _gpu.Allocate&lt;byte&gt;(b); //Call this in a loop gpu.Launch(N, 1).GetPixel(x, y, device_imageBytes, device_r, device_g, device_b); ... [Cudafy] public static void GetPixel(GThread thread, int x, int y, byte[] imageBytes, byte blue, byte green, byte red) { int offset = x * BPP + y * stride; blue = imageBytes[offset++]; green = imageBytes[offset++]; red = imageBytes[offset]; double R = red; double G = green * 255; double B = blue * 255 * 255; }"
            ]
        ],
        "votes": [
            5.0000001,
            1.0000001
        ]
    },
    {
        "question": "so I'm trying to make a complex calculation in CUDAfy so that it can reduce the current time it takes on the CPU I am trying to make use of \"CUDAfy-ed\" methods which I can pass variables into and it will return values very similar to the idea with the .NET Math.Sin(x) or Math.Cos(x) function for example. I am trying to make this to work however it is just spitting out errors from the CUDAfy translator, so is what I am trying to do possible/within the power of CUDAfy or am I going down the wrong route to make this work? Example: public static void Main(){ CudafyModule km = CudafyTranslator.Cudafy(); GPGPU gpu = CudafyHost.GetDevice(CudafyModes.Target, CudafyModes.DeviceId); gpu.LoadModule(km); double Value; double[] dev_Value = gpu.Allocate&lt;double&gt;(10); gpu.Launch(1,1).SumOfSines(dev_Value); Console.WriteLine(Value); gpu.FreeAll();} public static void SumOfSines(double[] Value) { int numThreads = thread.blockDim.x * thread.gridDim.x; int threadID = thread.threadIdx.x + thread.blockIdx.x * thread.blockDim.x; for (int i = threadID; i &lt; 1000000; i += numThreads) { for (double a = 0; a &lt; 100; a++) { double angle = MathHelper.DegreesToRadians((double)a); //can I do this? Value[i] += Math.Sin(angle); } } } In the MathHelper.cs file it has the method DegreesToRadians which makes this calculation: public static double DegreesToRadians(double deg) { double rad = deg * (Math.PI / 180.0); return rad; } When I do this it spits out the error at the CUDAfy translator.",
        "answers": [
            [
                "First thing is, these aren't marked as methods to 'Cudafy' with the [Cudafy] attribute. The C# code is translated to CUDA-C code (with the translator). But there are rules about what and how. Secondly, these methods shown contain references to the .NET Framework (Math.PI). The .NET Framework is not available to GPU. Try to look (Internet) for GMath. It's a Cudafy class that contains CUDA implementations of .NET Math methods. Before you bury yourself too deeply, I suggest you start out going through the CUDAfy documentation located here: http://www.hybriddsp.com/cudafy/CUDAfy_User_Manual_1_22.pdf"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "My Issue Hey, so I'm making this simple calculation to find the sum of sins between 0 and 100 degrees(as I use it as a benchmark for my systems), the calculation isn't the problem my issue is that I am new to Cudafy and I am unsure on how to properly pass in and return values so that it can be printed off here is my code: Code public const int N = 33 * 1024; public const int threadsPerBlock = 256; public const int blocksPerGrid = 32; public static void Main() { Stopwatch watch = new Stopwatch(); watch.Start(); string Text = \"\"; int iterations = 1000000; CudafyModule km = CudafyTranslator.Cudafy(); GPGPU gpu = CudafyHost.GetDevice(CudafyModes.Target, CudafyModes.DeviceId); gpu.LoadModule(km); double[] dev_Value = gpu.Allocate&lt;double&gt;(); gpu.Launch(blocksPerGrid, threadsPerBlock).SumOfSines(iterations,dev_Value); double Value; gpu.CopyFromDevice(dev_Value, out Value); watch.Stop(); Text = watch.Elapsed.TotalSeconds.ToString(); Console.WriteLine(\"The process took a total of: \" + Text + \" Seconds\"); Console.WriteLine(Value); Console.Read(); gpu.FreeAll(); } [Cudafy] public static void SumOfSines(GThread thread,int iterations,double [] Value) { double total = new double(); double degAsRad = Math.PI / 180.0; for (int i = 0; i &lt; iterations; i++) { total = 0.0; for (int z = 1; z &lt; 101; z++) { double angle = (double)z * degAsRad; total += Math.Sin(angle); } } Value[0] = total; } The value that I am trying to extract from the CUDAfy part is the total and then print it off aswell as printing the time for the benchmarking. If anyone could post advice it would be very much appreciated (also any suggestions for making rid of any useless lines or unefficient pieces would also be good).",
        "answers": [
            [
                "Doesn't matter I found the answer but I'll post it here: public const int N = 33 * 1024; public const int threadsPerBlock = 256; public const int blocksPerGrid = 32; public static void Main() { Stopwatch watch = new Stopwatch(); watch.Start(); CudafyModule km = CudafyTranslator.Cudafy(); GPGPU gpu = CudafyHost.GetDevice(CudafyModes.Target, CudafyModes.DeviceId); gpu.LoadModule(km); string Text = \"\"; int iterations = 1000000; double Value; double[] dev_Value = gpu.Allocate&lt;double&gt;(iterations * sizeof(double)); gpu.Launch(blocksPerGrid, threadsPerBlock).SumOfSines(iterations, dev_Value); gpu.CopyFromDevice(dev_Value, out Value); watch.Stop(); Text = watch.Elapsed.TotalSeconds.ToString(); Console.WriteLine(\"The process took a total of: \" + Text + \" Seconds\"); Console.WriteLine(Value); Console.Read(); gpu.FreeAll(); } [Cudafy] public static void SumOfSines(GThread thread, int _iterations, double[] Value) { int threadID = thread.threadIdx.x + thread.blockIdx.x * thread.blockDim.x; int numThreads = thread.blockDim.x * thread.gridDim.x; if (threadID &lt; _iterations){ for (int i = threadID; i &lt; _iterations; i += numThreads) { double _degAsRad = Math.PI / 180; Value[i] = 0.0; for (int a = 0; a &lt; 100; a++) { double angle = (double)a * _degAsRad; Value[i] += Math.Sin(angle); } } } } -Jack"
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I have a large number of operations X to perform on a large amount of items Y. Each operation X is fairly trivial and is essentially just evaluating a bunch of AND and OR logic. Each Func(X, Y) is naturally very quick however the sheer combination of X and Y makes the entire operation take a long time. PLinq makes it much faster however that is still relatively slow. I have spent several days researching various frameworks (Alea, Cudafy, GPULinq) to get this working on the GPU however I am finding that the GPU is not good for all operations. The main problem is that in the GPU Kernel at some points is performing the intersection or the union of an integer array. This results in an unknown amount of values. Possibly 2*Length in union or 0 in intersection. I could get around this by always using 2*Length however Length itself is not a constant either. How can I return a variable sized int array in any GPU framework?",
        "answers": [
            [
                "isn't it just a case of using the syntax: double[] x = gpu.allocate(size of array based upon a variable or numerical value); and then returning it from the [Cudafy] method."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I need to declare an array passed to a function as volatile, does Cudafy.NET support this? For example (in C#): [Cudafy] private static void doStuffOnGPU(GThread thread, volatile int[] output) { //do a whole bunch of stuff }",
        "answers": [
            [
                "The answer is no, as of November 26, 2016, Cudafy.NET does not support the volatile keyword. However, you can tricksify Cudafy.NET into allowing it in certain circumstances. Ex: //declare a dummy in global scope public static int[] volatileArray = new int[256]; [Cudafy] private static void doStuffOnGPU(GThread thread, int[] output) { //use the GThread.InsertCode() function to declare in CUDA GThread.InsertCode(\"__shared__ volatile int volatileArray[256];\"); //do a whole bunch of stuff } This code will use the global declaration when run serially for testing and will use the volatile declaration on the GPU."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm using Cudafy.net for GPU computations. Everything works fine unless I try to debug the kernel using NSight. After NSight-&gt;\"Start CUDA debugging\", this error occurs: \"Error decompiling System.Void Network.ActivationFunction(Cudafy.GThread, System.Single[])\" After the error the application crashes. I tried even the most simple kernel I could think of like this one (matches to the error): [Cudafy] public static void ActivationFunction(GThread t, float[] x) { // Synapse idx int i = t.blockDim.x * t.blockIdx.y * t.gridDim.x //rows preceeding current row in grid + t.blockDim.x * t.blockIdx.x //blocks preceeding current block + t.threadIdx.x; x[i] = 1; } I've already searched for solution and found this: cudafy.net with NSight, debugger not working However even after multiple check of all steps, I still can't make the debugger running. I can't even set breakepoint before the app crashes. Maybe there's something wrong with nvcc or cl? Am I missing something? Thanks.",
        "answers": [
            [
                "I was getting this error because the working directory for NSight was not set correctly. You need to set it where the executable for your application is actually running -- when debugging that is typically the debug directory (i.e. C:\\somepath\\yourproject\\bin\\Debug). You set this value via the \"NSight User Properties\" button in the Solution Explorer."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have testing some GPU computing samples with cudafy I have code which compute/creta colection of data, and each loop i want do on every object in collection some GPU operation CODE: public override void CountData(List&lt;IData&lt;int&gt;&gt; datas) { for (int i = 0; i &lt; datas.Count; i++) { Execute(datas[i]); } } public static void Execute(IData&lt;int&gt; data) { CudafyModule km = CudafyTranslator.Cudafy(); GPGPU gpu = CudafyHost.GetDevice(CudafyModes.Target, CudafyModes.DeviceId); gpu.LoadModule(km); int c; int[] dev_c = gpu.Allocate&lt;int&gt;(); // cudaMalloc one Int32 gpu.Launch().add(data.IntData[0], data.IntData[1], dev_c); // or gpu.Launch(1, 1, \"add\", 2, 7, dev_c); gpu.CopyFromDevice(dev_c, out c); Console.WriteLine(c + \";\"); gpu.Free(dev_c); chromozome.Result= c; } [Cudafy] public static void add(int a, int b, int[] c) { c[0] = a + b; } This code works for first call of CountData,but after Count data loop end program will stuck and console output says The thread 0xf50 has exited with code 259 (0x103). The thread 0x10c has exited with code 259 (0x103). The thread 0xc30 has exited with code 259 (0x103). The thread 0xcc0 has exited with code 0 (0x0). The thread 0x548 has exited with code 0 (0x0). Have enybody clue where could be problem? i try gpu.Synchronize, CudafyHost.ClearDevices() , but it always end in this error Thanks for help Edit: after some test i found that gpu.Launch().add(5, 3, dev_c); works but: gpu.Launch().add(data.IntData[0], data.IntData[1], dev_c); is not",
        "answers": [
            [
                "Tahnks to Hans Passant who pinted that this is not problem at all and that gpu card should work wihtou problem. I find error in other part of code. So code above works fine."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm using Cudafy.NET and I have some difficulties about the BlockSize. It is generating different results in some situations. Shortly the difference is at here: //correct results when using this line gpu.Launch(1, 7, \"kernelfx_alldata\", 10, devdata, devnmin, devnmax, devgmin, devgmax, devtest); //incorrect results when using this line gpu.Launch(1, 8, \"kernelfx_alldata\", 10, devdata, devnmin, devnmax, devgmin, devgmax, devtest); The detailed explanation about the problem: I have 10 items to loop. The GridSize is 1. CASE 1: When CudafyModes.Target = eGPUType.OpenCL and the BlockSize is 1,2,3,4,5,6 and 7. The results are correct. CASE 2: CudafyModes.Target = eGPUType.OpenCL and the BlockSize is 8,9,10,11, .... and more. The results are incorrect. CASE 3: CudafyModes.Target = eGPUType.Emulator and the BlockSize is 1,2,3,4,5,6,7,8,9,10,11, .... and more. The results are correct. The example code is shown below. Initializing the variables: double[,] data; double[] nmin, nmax, gmin, gmax; void initializeVars() { data = new double[10, 10]; for (int i = 0; i &lt; 10; i++) { data[i, 0] = 100 + i; data[i, 1] = 32 + i; data[i, 2] = 22 + i; data[i, 3] = -20 - i; data[i, 4] = 5522 + 10 * i; data[i, 5] = 40 + i; data[i, 6] = 14 - i; data[i, 7] = 12 + i; data[i, 8] = -10 + i; data[i, 9] = 10 + 10 * i; } nmin = new double[10]; nmax= new double[10]; gmin = new double[10]; gmax = new double[10]; for (int i = 0; i &lt; 10; i++) { nmin[i] = -1; nmax[i] = 1; gmin[i] = i; gmax[i] = 11 * i*i+1; } } gpu Launch Code: private void button1_Click(object sender, EventArgs e) { CudafyModes.Target = eGPUType.OpenCL; CudafyModes.DeviceId = 0; CudafyTranslator.Language = eLanguage.OpenCL; CudafyModule km = CudafyTranslator.Cudafy(); Cudafy.Host.GPGPU gpu = Cudafy.Host.CudafyHost.GetDevice(CudafyModes.Target, CudafyModes.DeviceId); gpu.LoadModule(km); initializeVars(); double[,] devdata = gpu.Allocate&lt;double&gt;(data); gpu.CopyToDevice(data, devdata); double[] devnmin = gpu.Allocate&lt;double&gt;(nmin); gpu.CopyToDevice(nmin, devnmin); double[] devnmax = gpu.Allocate&lt;double&gt;(nmax); gpu.CopyToDevice(nmax, devnmax); double[] devgmin = gpu.Allocate&lt;double&gt;(gmin); gpu.CopyToDevice(gmin, devgmin); double[] devgmax = gpu.Allocate&lt;double&gt;(gmax); gpu.CopyToDevice(gmax, devgmax); double[] test = new double[10]; double[] devtest = gpu.Allocate&lt;double&gt;(test); gpu.Launch(1, 8, \"kernelfx_alldata\", 10, devdata, devnmin, devnmax, devgmin, devgmax, devtest); gpu.CopyFromDevice(devtest, test); gpu.FreeAll(); } the Cudafy kernel [Cudafy] public static void kernelfx_alldata(GThread thread, int N, double[,] data, double[] nmin, double[] nmax, double[] gmin, double[] gmax, double[] test) { int tid = thread.threadIdx.x + thread.blockIdx.x * thread.blockDim.x; while (tid &lt; N) { double[] tmp = thread.AllocateShared&lt;double&gt;(\"tmp\", 10); tmp[0] = 1; for (int i = 1; i &lt; 10; i++) { tmp[i] = data[tid, i - 1]; } for (int i = 1; i &lt; 10; i++) { tmp[i] = (nmax[i - 1] - nmin[i - 1]) / (gmax[i - 1] - gmin[i - 1]) * (tmp[i] - gmin[i - 1]) + nmin[i - 1]; } test[tid] = tmp[1]; tid = tid + thread.blockDim.x * thread.gridDim.x; } } The Correct (CASE 1 and CASE 3) Results are: test[0]=199.0 test[1]=201.0 test[2]=203.0 test[3]=205.0 test[4]=207.0 test[5]=209.0 test[6]=211.0 test[7]=213.0 test[8]=215.0 test[9]=217.0 Incorrect (CASE 2) results are: test[0]=213.0 test[1]=213.0 test[2]=213.0 test[3]=213.0 test[4]=213.0 test[5]=213.0 test[6]=213.0 test[7]=213.0 test[8]=217.0 test[9]=217.0 When the BlockSize is lower then 8, the results are correct. But when the BlockSize is greater then 8 the results are incorrect. In order to use the gpu efficiently the blockSize must be greater then 8. What is the problem on this code? Best Regards...",
        "answers": [
            [
                "Declaring tmp as 2d array, first column is the threadId solves the problem. The working code is below: [Cudafy] public static void kernelfx_alldata(GThread thread, int N, double[,] data, double[] nmin, double[] nmax, double[] gmin, double[] gmax, double[] test) { int tid = thread.threadIdx.x + thread.blockIdx.x * thread.blockDim.x; double[,] tmp = thread.AllocateShared&lt;double&gt;(\"tmp\", 10, 10); while (tid &lt; N) { tmp[tid, 0] = 1; for (int i = 1; i &lt; 10; i++) { tmp[tid, i] = data[tid, i - 1]; } for (int i = 1; i &lt; 10; i++) { tmp[tid, i] = (nmax[i - 1] - nmin[i - 1]) / (gmax[i - 1] - gmin[i - 1]) * (tmp[tid, i] - gmin[i - 1]) + nmin[i - 1]; } test[tid] = tmp[tid, 1]; tid = tid + thread.blockDim.x * thread.gridDim.x; } }"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have been trying to get cudafy 1.29 to work. It supports Cuda toolkit 7.0. I accidentally installed 7.5 first, uninstalled it and installed 7.0. When I run the following line: km = CudafyTranslator.Cudafy(); I get the error mentioned above and shown below in the image. I have tried adding the following paths to the system environment variables: C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin\\amd64 C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin\\x86_amd64 But sadly enough, this doesn't work either. CUDAfy.NET giving Win32Exception: The system cannot find the file specified",
        "answers": [
            [
                "i had exactly the same problem and fix it by going to \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\\" and deleting the v7.5 folder"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am using Cudafy to do some calculations on a NVIDIA GPU. (Quadro K1100M capability 3.0, if it matters) My question is, when I use the following cudaGpu.Launch(new dim3(44,8,num), new dim(8, 8)).MyKernel... why are my z indexes from the GThread instance always zero when I use this in my kernel? int z = thread.blockIdx.z * thread.blockDim.z + thread.threadIdx.z; Furthermore, if I have to do something like cudaGpu.Launch(new dim3(44,8,num), new dim(8, 8, num)).MyKernel... z does give different indexes as it should, but num can't be very large because of the restrictions on number of threads per block. Any surgestion on how to work around this? Edit Another way to phrase it. Can I use thread.z in my kernel (for anything useful) when block size is only 2D?",
        "answers": [
            [
                "On all currently supported hardware, CUDA allows the use of both three dimensional grids and three dimensional blocks. On compute capability 1.x devices (which are no longer supported), grids were restricted to two dimensions. However, CUDAfy currently uses a deprecated runtime API function to launch kernels, and silently uses only gridDim.x and gridDim.y, not taking gridDim.z in account : _cuda.Launch(function, gridSize.x, gridSize.y); As seen in the function DoLaunch() in CudaGPU.cs. So while you can specify a three dimensional grid in CUDAfy, the third dimension is ignored during the kernel launch. Thanks to Florent for pointing this out !"
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I am using CUDAfy .NET and want to pass a struct array within a struct to the device. I have declared them in c# as shown below: [Cudafy(eCudafyType.Struct)] [StructLayout(LayoutKind.Sequential)] public struct A { [MarshalAs(UnmanagedType.ByValArray, ArraySubType= UnmanagedType.Struct, SizeConst = 3)] public B[] ba; } [Cudafy(eCudafyType.Struct)] [StructLayout(LayoutKind.Sequential)] public struct B { public byte id; } This results in the following source code for the GPU: struct B { unsigned char id; }; struct A { B ba [3]; int baLen0; }; And I get this compilation error from an attempt to convert it to OpenCL code: Compilation error: &lt;kernel&gt;:20:2: error: must use 'struct' tag to refer to type 'B' B ba [3]; int baLen0; ^ struct I realize this could be an issue between the marshalling and how CUDAfy .NET handles structures, but is there any way I could possibly fix this? Thanks in advance",
        "answers": [
            [
                "I managed to alter the CUDAfy .NET library in the CudafyTranslator. After the structs were in a memory stream I added: StreamReader sr = new StreamReader(structs); String sStructs = sr.ReadToEnd(); String sNewStructs; foreach(string structName in cm.Types.Values.Select(t =&gt; t.Name)) { while (true) { string regex = @\"^(?&lt;start&gt;\\s+)\" + structName + @\"(?&lt;end&gt;\\s+\\S+( \\[\\d+\\])?;)\"; sNewStructs = Regex.Replace(sStructs, regex, @\"${start}struct \" + structName + \"${end}\", RegexOptions.Multiline); if (sNewStructs.Length == sStructs.Length) { break; } else { sStructs = sNewStructs; } } } structs = new MemoryStream(); StreamWriter sw = new StreamWriter(structs); sw.WriteLine(sStructs); sw.Flush(); It's a bit sloppy but it works, I then rebuilt CUDAfy .NET and ilmerged it and replaced my dll"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to use CUDAfy.NET in a web application which will be further called from a web form. When it tries to initiate a CudafyModule it gives the error as shown in pic below: Code was working perfectly in console application. Is there anyway to get rid of this problem?",
        "answers": [
            [
                "It seems like your web application is unable to talk to the hardware. There is a great deal of abstraction and security in between the IIS and your .NET based assembly. It probably an issue of translation. Depends on how your web application is configured. There can be several reasons for a process being unable to find the dll. Try registering the dll in GAC. Or edit your web.config and add a reference to the dll's path This might resolve the issue."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am running through some samples to get myself more familiar with cudafy for .net. Here is the code I was able to get working sucessfully. private Answer GpuTsp() { var stopWatch = Stopwatch.StartNew(); byte[] buffer = new byte[source.Length]; byte[] src_dev_bitmap = _gpu.CopyToDevice(source); byte[] dst_dev_bitmap = _gpu.Allocate&lt;byte&gt;(source.Length); _gpu.Launch(new dim3(_bmp.Width, _bmp.Height), 1).thekernel(dst_dev_bitmap, src_dev_bitmap); _gpu.CopyFromDevice(dst_dev_bitmap, buffer); _gpu.FreeAll(); return new Answer { Result = buffer, Milliseconds = stopWatch.ElapsedMilliseconds }; } [Cudafy] public static void thekernel(GThread thread, byte[] dst, byte[] src) { int x = thread.blockIdx.x; int y = thread.blockIdx.y; int offset = x + y * thread.gridDim.x; if (x &lt; N) { byte b, g, r; b = src[offset * 3 + 0]; g = src[offset * 3 + 1]; r = src[offset * 3 + 2]; if (IsMatch(r, g, b, Red, Green, Blue, 35)) { dst[offset * 3 + 0] = 255; //Mark Match dst[offset * 3 + 1] = 252; //Mark Match dst[offset * 3 + 2] = 201; //Mark Match } else { dst[offset * 3 + 0] = src[offset * 3 + 0]; //Copy dst[offset * 3 + 1] = src[offset * 3 + 1]; //Copy dst[offset * 3 + 2] = src[offset * 3 + 2]; //Copy } } } [Cudafy] private static bool IsMatch(byte r1, byte g1, byte b1, byte r2, byte g2, byte b2, int threshold = 35) { int r = (int)r1 - r2, g = (int)g1 - g2, b = (int)b1 - b2; return (r * r + g * g + b * b) &lt;= threshold * threshold; } What I am trying to accomplish is adding a region of interest area. Basically if the pixel is not within the area just copy don't check for match. For some reason I am having a hard time wrapping my head around how to accomplish this as I still want to copy all the pixels so I have the image back. I was thinking something like this. Rectangle rect = new Rectangle(200, 100, 640, 600); int startX = rect.Left; int startY = rect.Top; int stopX = startX + rect.Width; int stopY = startY + rect.Height; I am unsure of how to apply this to my routine. Anyone have some ideas on an efficient way of doing this?",
        "answers": [
            [
                "May not work for everyone but I ended up being more interested in using an Arc and determining if the point X/Y was within this Arc and if so do the routine I wanted. Here is the code I am using for that. float distance = GMath.Sqrt((x - CenterX) * (x - CenterX) + (y - CenterY) * (y - CenterY)); if (distance &gt; Radius) { // Outside Arc } else { // Inside Arc }"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a struct 'DDReal' to be CUDAfied that is dependent on a class 'Base' that has static methods to be CUDAfied - both in seperate files in the same project. They are compiled into a DLL assembly that is called by the main program where the CUDAfying takes place. In the main program, if I do: var km = ...Cudafy(typeof(Base)); this compiles OK. But if I do var var km = ...Cudafy(typeof(Base), typeof(DDReal)); there is a compiler error due to DDReal being translated BEFORE Base (as evidenced by the .cu file). 'Base' needs to be translated first followed by 'DDReal'. Is there anyway I can force this so that the C definitions for class 'Base's static methods appear before the definition for struct 'DDReal' in the resulting CUDAFYSOURCETEMP.cu file?",
        "answers": [
            [
                "Cudafy, alas, does not cater for this scenario: Cudafy puts all struct definitions BEFORE the class method prototype declarations in the output source code .cu file. So I've modifed the Cudafy source code in order solve my problem. I've added a 2-param constructor to the class of attribute 'Cudafy' such that the 2nd param can specify default behaviour or the pre-struct prototype declaration of a class method. All Cudafy's unit tests and examples check out OK plus my own unit tests. If anyone's interested I can upload the source + DLL to github."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Cannot execute cudafy (BLAS) sample. Failed on line: GPGPUBLAS blas = GPGPUBLAS.Create(gpu); Message: Specified method is not supported. Stack trace: at Cudafy.Maths.BLAS.CudaBLAS..ctor(GPGPU gpu) at Cudafy.Maths.BLAS.GPGPUBLAS.Create(GPGPU gpu) at CUDAFY_Samples.Program.Main(String[] args) in d:\\@Igor\\SW\\GPU\\CUDAFY-Samples\\Program.cs:line 22 at System.AppDomain._nExecuteAssembly(RuntimeAssembly assembly, String[] args) at System.AppDomain.ExecuteAssembly(String assemblyFile, Evidence assemblySecurity, String[] args) at Microsoft.VisualStudio.HostingProcess.HostProc.RunUsersAssembly() at System.Threading.ThreadHelper.ThreadStart_Context(Object state) at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx) at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx) at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state) at System.Threading.ThreadHelper.ThreadStart() Error's screen: Test sources copied from http://cudafy.codeplex.com/discussions/331743 static void Main(string[] args) { // Get GPU device GPGPU gpu = CudafyHost.GetDevice(CudafyModes.Target); // Create GPGPUBLAS (CUBLAS Wrapper) GPGPUBLAS blas = GPGPUBLAS.Create(gpu); // Prepare sample data Random rand = new Random(); int n = 500; double[] cpuVectorX = new double[n]; double[] cpuVectorY = new double[n]; double[] cpuMatrixA = new double[n * n]; for (int i = 0; i &lt; n; i++) { cpuVectorX[i] = rand.Next(100); cpuVectorY[i] = rand.Next(100); } for (int i = 0; i &lt; n * n; i++) { cpuMatrixA[i] = rand.Next(100); } // Copy CPU to GPU memory // Before using GPGPUBLAS, You have to copy data from cpu to gpu. double[] gpuVectorX = gpu.CopyToDevice(cpuVectorX); double[] gpuVectorY = gpu.CopyToDevice(cpuVectorY); double[] gpuMatrixA = gpu.CopyToDevice(cpuMatrixA); // BLAS1 sample : y = x + y blas.AXPY(1.0, gpuVectorX, gpuVectorY); // BLAS2 sample : y = Ax + y blas.GEMV(n, n, 1.0, gpuMatrixA, gpuVectorX, 1.0, gpuVectorY); // Get result from GPU gpu.CopyFromDevice&lt;double&gt;(gpuVectorY, cpuVectorY); // And you can use result cpuVectorY for any other purpose. } UPDATE1 Set platform x64 (instead of \"Any CPU\") and now next error shown: Unable to load DLL 'cublas64_70': The specified module could not be found. (Exception from HRESULT: 0x8007007E) But I have v7.5 installed on my computer. So, I am going to install v7.0 and check the issue again.",
        "answers": [
            [
                "Target platform of project should be x64. Copied to bin\\DEBUG folder 2 files: cublas64_70.dll and cufft64_70.dll. blas.Dispose() should be added at the end of blas usage."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Is there an emulator for Cudafy? I have looked into Ocelot but I am not sure if Cudafy codes can be compiled on it. If anyone has any suggestions for an alternative emulator, please share.",
        "answers": [
            [
                "About Cudafy, I haven't used Ocelot. But Ocelot itself works with visual studio. There shouldn't be any problems running Cudafy with Ocelot just as long as Cudafy is compatible with Visual Studio 10. There are these two emulators that I came across. Barra: https://devtalk.nvidia.com/default/topic/408577/barra-a-gpu-simulator-to-run-cuda-apps/ Waste: http://codinggorilla.domemtech.com/?p=553 Hope that helps."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Using Cudafy version 1.29, which can be downloaded from here I am executing the examples that are found in the install folder CudafyV1.29\\CudafyByExample\\ Specifically, \"chapter 3\" example that begins line 42 of program.cs calls the following: simple_kernel.Execute(); which is this: public static void Execute() { CudafyModule km = CudafyTranslator.Cudafy(); // &lt;--exception thrown! GPGPU gpu = CudafyHost.GetDevice(CudafyModes.Target, CudafyModes.DeviceId); gpu.LoadModule(km); gpu.Launch().thekernel(); // or gpu.Launch(1, 1, \"kernel\"); Console.WriteLine(\"Hello, World!\"); } The indicated line throws this exception: Compilation error: CUDAFYSOURCETEMP.cu 'C:\\Program' is not recognized as an internal or external command, operable program or batch file. . Which is immediately obvious that the path has spaces and the programmer did not double quote or use ~ to make it operational. So, I did not write this code. And I cannot step through the sealed code contained within CudafyModule km = CudafyTranslator.Cudafy();In fact I don't even know the full path that is causing the exception, it is cut-off in the exception message. Does anyone have a suggestion for how to fix this issue? Update #1: I discovered where CUDAFYSOURCETEMP.cu lives on my computer, here it is: C:\\Users\\humphrt\\Desktop\\Active Projects\\Visual Studio Projects\\CudafyV1.29\\CudafyByExample\\bin\\Debug ...I'm still trying to determine what the program is looking for along the path to 'C:\\Program~'.",
        "answers": [
            [
                "I was able to apply a workaround to bypass this issue. The workaround is to reinstall all components of cudafy in to folders with paths with no ' ' (spaces). My setup looks like the below screenshot. Notice that I also installed the CUDA TOOLKIT from NVIDIA in the same folder - also with no spaces in folder names. I created a folder named \"C:\\CUDA\" and installed all components within it, here is the folder structure:"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to understand coding with GPUs. I'm having a bit trouble understanding interaction of GPU threads with global memory. I know when we allocate or copy something on device it's done by reference. But when we try to access global memory in a thread, what exactly happens? Do all threads try to access memory at the same time leading to serial execution or do they all make their own copies or something else? Any help will be much appreciated.",
        "answers": [
            [
                "Do all threads try to access memory at the same time leading to serial execution or do they all make their own copies or something else? No if you want to do computations in parallel. For instance, to add an array in parallel you would do: int idx = blockIdx.x * blockDim.x + threadIdx.x; outArr[idx] = a[idx] + b[idx]; Each thread inside the grid will do two reads (on the right) from two different locations and one write to another location. All in global memory. You can let all threads read/write from the same location in global memory. However, to prevent race condition, you need to use atomic functions. Read/write from/to global memory can be slow (it's DRAM), especially if threads do not read from coalesed memory (i.e: if thread 0, 1, 2, 3 reads from 0x0,0x4,0x8,0xc then it's coalesed). To understand more about the CUDA memory model, you can read section 2.4 in the CUDA Programming Guide. Hope that helps!"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am working with CUDAfy and C#. For my kernel, I need double precision. In CUDA, this is no problem, it is automatically supported. However, in order to support non NVIDIA hardware, the user should be able to use OpenCL, too. Here, as far as I understand, double precision has to be activated manually. Currently, the compiler throws the exception Compilation error: :8:32: error: use of type 'double' requires cl_khr_fp64 extension to be enabled To do so in the kernel code, one would normally add a line #pragma OPENCL EXTENSION cl_khr_fp64 : enable Now is there a way to a) tell CUDAfy to enable double precision on OpenCL hardware or b) load an existing CUDA C code instead of translating the C# code? My hardware supports CUDA 3.0 and OpenCL 1.2.",
        "answers": [
            [
                "In response to part (a) I have been using CUDAfy with an AMD Radeon 7970 with OpenCL 1.2. With this hardware, CUDAfy automatically enables double precision. I haven't needed to enable it in order to use Doubles in my C# [Cudafy] kernel methods."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Every time I build and run my CUDAfy code, it takes considerable time for loading the module and translating it. Is there any way to reduce the time taken during translation and loading?",
        "answers": [
            [
                "It may not be desirable to always call Cudafy if the GPU code has not been changed. You can cache the Cudafy modules by using serialization. CudafyModule km = CudafyModule.TryDeserialize(); if (km == null || !km.TryVerifyChecksums()) { km = CudafyTranslator.Cudafy(); km.Serialize(); } The TryDeserialize method will attempt to find a *.cdfy file in the current directory with the same file name as the calling type. If this is not found or fails then null is returned and try making a new module.If it is not null then check whether the cached module refers to the same version of the .NET code it was created from. To do this call TryVerifyChecksums. If this returns false then it means the cached module was out of date and it is advisable to cudafy a new one."
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "I need an algorithm for computing the parallel prefix sum of an array without using shared memory. And if there is no other alternative to using shared memory, what is the best way to tackle the problem of conflicts?",
        "answers": [
            [
                "This link contains a detailed analysis of the sequential and the parallel algorithms for parallel prefix sum: Parallel Prefix Sum (Scan) with CUDA It also contains a fragment of the C code for the implementation of the parallel prefix algorithm and a detailed explanation for avoiding the shared memory conflicts. You can either port the codes to CUDAfy or simply define regions of C and use them as unmanaged code from your application. But there are several mistakes in the CUDA C code. I am writing the corrected version of the code in Cudafy.NET [Cudafy] public static void prescan(GThread thread, int[] g_odata, int[] g_idata, int[] n) { int[] temp = thread.AllocateShared&lt;int&gt;(\"temp\", threadsPerBlock);//threadsPerBlock is user defined int thid = thread.threadIdx.x; int offset = 1; if (thid &lt; n[0]/2) { temp[2 * thid] = g_idata[2 * thid]; // load input into shared memory temp[2 * thid + 1] = g_idata[2 * thid + 1]; for (int d = n[0] &gt;&gt; 1; d &gt; 0; d &gt;&gt;= 1) // build sum in place up the tree { thread.SyncThreads(); if (thid &lt; d) { int ai = offset * (2 * thid + 1) - 1; int bi = offset * (2 * thid + 2) - 1; temp[bi] += temp[ai]; } offset *= 2; } if (thid == 0) { temp[n[0] - 1] = 0; } // clear the last element for (int d = 1; d &lt; n[0]; d *= 2) // traverse down tree &amp; build scan { offset &gt;&gt;= 1; thread.SyncThreads(); if (thid &lt; d) { int ai = offset * (2 * thid + 1) - 1; int bi = offset * (2 * thid + 2) - 1; int t = temp[ai]; temp[ai] = temp[bi]; temp[bi] += t; } } thread.SyncThreads(); g_odata[2 * thid] = temp[2 * thid]; // write results to device memory g_odata[2 * thid + 1] = temp[2 * thid + 1]; } } You can use the above modified code instead of the one in the link."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Is anyone know simple way to make all inclusive distribution of application that uses cuda with Cudafy.net wrapper? I know that I have to install Toolkit and C++ compiler, but I would prefer to find more easier way to deploy to target machine.",
        "answers": [],
        "votes": []
    },
    {
        "question": "We are doing comparisons on the GPU using CUDAfy.NET. For that we are passing two arrays, one of which contains the data and the other stores the results. I only want to store those elements in the result array which satisfy a certain condition. But the array ends up with unwanted entries where condition doesn't satisfy. How can I filter these unwanted entries from the results array and return the filtered array back to the main function? [Cudafy] public static void Comparisons(GThread thread, int[] a,int[] c, int iter) { int tx = thread.threadIdx.x; if(tx &lt; iter) { if(a[tx] &lt; tolerance) //tolerance is some user defined number { c[tx] = a[tx]; } } }",
        "answers": [
            [
                "You will have to do this in multiple kernel passes. Example: a = [1,2,1,2,1,2] tolerance = 2 First pass: Create an array that contains 1 for \"keep element\" or 0 for \"discard element\" p = [1,0,1,0,1,0] Second pass: Perform a parallel prefix sum on the p array. i = [0,1,1,2,2,3] (There are lots of whitepapers on this topic) Third pass: Use a,p, and i. One thread per element. if p[threadIdx.x] equals 1 then put a[threadIdx.x] in c[i[threadIdx.x]] (You could use shared memory here to coalesce the writing to c array better) The result array c will contain [1,1,1]"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers. We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 7 years ago. Improve this question I am new to CUDAfy programming and having trouble calculating the sum of all of the combinations of elements in an array. I can't seem to think of an algorithm suitable enough to be ported on a GPU. Any help or any sort of algorithm is very much appreciated. Serial version of code is given below: for (int i = 0; i &lt; Array.Count - 1; i++) { for (int j = (i + 1); j &lt; Array.Count; j++) { ans.Add(Array.ElementAt(i) + Array.ElementAt(j)); } }",
        "answers": [
            [
                "This doesn't give the GPU very much work to do other than a single add. The array will have to be a considerable size before you'd see a benefit. Anyway: I use C++ and am not familiar with C# or CUDAfy, but it should be easy to port the logic. A kernel function that stores the sum of each pair of elements in an array is: template&lt;typename T&gt; __global__ void sum_combinations_of_array( const T* arr, const size_t len, T* dest ) { const int tx = blockIdx.x*blockDim.x+threadIdx.x; const int ty = blockIdx.y*blockDim.y+threadIdx.y; if( tx &lt; len &amp;&amp; ty &lt; len &amp;&amp; tx &lt; ty ) { dest[tx*len+ty] = arr[tx]+arr[ty]; } } You're just using a 2D thread blocks to decide which elements of the array to add (they just take the place of i and j in your code). arr should be at least len in size, and dest should be at least len*len in size. The host code to set all of this up and run it would be something like: const int len = 1000; int* arr; cudaMalloc( &amp;arr, len*sizeof(int) ); int* matrix; cudaMalloc( &amp;matrix, len*len*sizeof(int) ); // cudaMalloc2D could also be used here, but then you'll // have to pay attention to the pitch cudaMemset( matrix, 0, len*len*sizeof(int) ); // copy host array to arr with cudaMemcpy // ... const int numThreads = ???; // depends on your hardware dim3 grid( len, (len+numThreads-1)/numThreads ), threads( 1, numThreads ); sum_combinations_of_array&lt;int&gt;&lt;&lt;&lt;grid,threads&gt;&gt;&gt;( arr, len, matrix ); cudaDeviceSynchronize(); // wait for completion // copy device matrix to host with cudaMemcpy (or cudaMemcpy2D) // remember any element i&lt;=j will be 0 // ... cudaFree( arr ); cudaFree( matrix );"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I\u00b4m trying to utilize CUDAFy 1.29 in VB.NET using VS2013. I\u00b4m trying to translate the samples of C# from CUDAFy and I\u00b4m having two type of errors, like explained below: My Variables Shared cs_CC As String = \"adiciona\" Shared MyGPU As GPGPU = Nothing Shared Arch As eArchitecture = Nothing My Code: Shared Executa() if Loader = true Dim Modulo = CudafyModule.TryDeserialize(cs_cc) If IsNothing(Modulo) OrElse (Not Modulo.TryVerifyChecksums) Then Modulo = CudafyTranslator.Cudafy(ePlatform.All, Arch, cs_CC.GetType) Modulo.Serialize() End If MyGPU.Loadmodule(Modulo) Dim a As Integer() = New Integer(N - 1) {} Dim b As Integer() = New Integer(N - 1) {} Dim c As Integer() = New Integer(N - 1) {} ' allocate the memory on the GPU Dim dev_a As Integer() = MyGPU.Allocate(Of Integer)(a) Dim dev_b As Integer() = MyGPU.Allocate(Of Integer)(b) Dim dev_c As Integer() = MyGPU.Allocate(Of Integer)(c) ' fill the arrays 'a' and 'b' on the CPU For i As Integer = 0 To N - 1 a(i) = i b(i) = 2 * i Next ' copy the arrays 'a' and 'b' to the GPU MyGPU.CopyToDevice(a, dev_a) MyGPU.CopyToDevice(b, dev_b) For i As Integer = 0 To 128 MyGPU.Launch(1, 1).adiciona(dev_a, dev_b, dev_c) Next end if End Sub The function ADICIONA which would run on CUDA &lt;Cudafy()&gt; _ Shared Sub adiciona(thread As GThread, a As Integer(), b As Integer(), c As Integer()) Dim tid As Integer = thread.blockIdx.x While tid &lt; N c(tid) = a(tid) + b(tid) tid += thread.gridDim.x End While End Sub LOADER: try to identify card and CUDA (successfully running): Public Shared Function Loader() As Boolean DeviceType = eGPUType.Cuda CudafyModes.Target = DeviceType CudafyTranslator.Language = If(CudafyModes.Target = eGPUType.Cuda, eLanguage.Cuda, eLanguage.OpenCL) Dim CompatibleDevice As GPGPUProperties() = CudafyHost.GetDeviceProperties(CudafyModes.Target, True).ToArray If Not CompatibleDevice.Any Then ' n\u00e3o possui um full-CUDA device MsgBox(\"I do not found any OpenCL or CUDA compatible device\") Return False End If Dim selectedDevice As GPGPUProperties = CompatibleDevice(0) If IsNothing(selectedDevice) Then MsgBox(\"I cannot allocate a compatible device\") Return False End If CudafyModes.DeviceId = selectedDevice.DeviceId Thread_per_Block = selectedDevice.MaxThreadsPerBlock Blocks_per_Grid = selectedDevice.MaxThreadsSize.x Shared_Mem_per_Block = selectedDevice.SharedMemoryPerBlock MyGPU = CudafyHost.GetDevice(CudafyModes.Target, CudafyModes.DeviceId) Arch = MyGPU.GetArchitecture Return True End Function The problems: Problem 1: If I utilize just dim Modulo as CudafyModule = CudafyTranslator.Cudafy() I get the following error: Checked Statements are not supported. It\u00b4s weird! All documentation of Cudafy shows this line exactly as it! Problem 2: So, I try to check the existence of a written module (.CDFY) and, if does not exist, I call the Serialize() function. The problem is the function creates a file called STRING.CDFY in my folder, but not the ADICIONA.CDFY nor ADD_LONG_LOOP.CDFY, which would be correct. Since I would like to avoid compilation at every run of this code, how to correctly make CUDAFy to write it? Problem 3: When VS runs, everything goes OK until the point of calling ADICIONA (MyGPU.Launch)! VS stops the execution with a message \"COULD NOT FIND FUNCTION 'ADICIONA' IN MODULE\". Interesting to notice that: 1- both temporary files are created (.CU, .PTX) and also the .CDFY file. It evidences the NVCC compiler is running well and creating the CUDA modules. So, why the code is NOT finding the function ADICIONA? 2- All samples written in C# run here 100%. And the conversion from C# and VB seems to be OK (I had utilized TELERIK to do it). I don\u00b4t guess the problem may be related to this, but I may be wrong. 3- The problem is NOT related to neither with NVCC compiler nor some reference into VB.NET, since the code compiles. I tried to write in CODEPLEX for an answer. No answers at all... I tried to see a lot of samples in Internet, but ALL OF THEM are created to CUDAFy C# and none of them are utilizing version 1.29 and CUDA 7.5. Also, I would like to understand WHY the basic function (CudafyTranslator.Cudafy()) get an error in VB but not in C#. So, does anyone here had successfully created a CUDAFy code using VB.NET? Thank you VERY much for any help.",
        "answers": [
            [
                "To solve Problem 1, check \"Remove integer overflow checks\" under Project Properties &gt; Compile &gt; Advanced Compile Options &gt; Optimizations. More info: http://forums.asp.net/post/715302.aspx"
            ],
            [
                "Try this simple example: Imports Cudafy Imports Cudafy.Host Imports Cudafy.Translator Public Class Form1 Private Sub Button1_Click(sender As Object, e As EventArgs) Handles Button1.Click Dim GPU As GPGPU = CudafyHost.GetDevice(eGPUType.OpenCL, CudafyModes.DeviceId) Dim Modulo As CudafyModule = CudafyTranslator.Cudafy(ePlatform.All, GPU.GetArchitecture()) GPU.LoadModule(Modulo) Dim Resultado As Integer Dim ResultadoGPU As Integer() = gpu.Allocate(Of Integer)() GPU.Launch().Adicionar(2, 7, ResultadoGPU) GPU.CopyFromDevice(ResultadoGPU, Resultado) MessageBox.Show(\"2 + 7 = \" &amp; Resultado) GPU.Launch().Subtrair(2, 7, ResultadoGPU) GPU.CopyFromDevice(ResultadoGPU, Resultado) MessageBox.Show(\"2 - 7 = \" &amp; Resultado) GPU.Free(ResultadoGPU) End Sub &lt;Cudafy()&gt; Private Shared Sub Adicionar(a As Integer, b As Integer, c As Integer()) c(0) = a + b End Sub &lt;Cudafy()&gt; Private Shared Sub Subtrair(a As Integer, b As Integer, c As Integer()) c(0) = a - b End Sub End Class"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I'm testing CUDAfy with a small gravity simulation and after running a profiler on the code I see that most of the time is spent on the CopyFromDevice method of the GPU. Here's the code: private void WithGPU(float dt) { this.myGpu.CopyToDevice(this.myBodies, this.myGpuBodies); this.myGpu.Launch(1024, 1, \"MoveBodies\", -1, dt, this.myGpuBodies); this.myGpu.CopyFromDevice(this.myGpuBodies, this.myBodies); } Just to clarify, this.myBodies is an array with 10,000 structs like the following: [Cudafy(eCudafyType.Struct)] [StructLayout(LayoutKind.Sequential)] internal struct Body { public float Mass; public Vector Position; public Vector Speed; } And Vector is a struct with two floats X and Y. According to my profiler the average timings for those three lines are 0.092, 0.192 and 222.873 ms. These timings where taken on a Windows 7 with a NVIDIA NVS 310. Is there a way to improve the time of the CopyFromDevice() method? Thank you",
        "answers": [
            [
                "CUDA kernel launches are asynchronous. This means that immediately after launching the kernel, the CPU thread is released to process the code immediately following the kernel launch, while the kernel is still executing. If the subsequent code contains any sort of CUDA execution barrier, then the CPU thread will then stop at the barrier until the kernel execution is complete. In CUDA, both cudaMemcpy (the operation underlying the cudafy CopyFromDevice method) and cudaDeviceSynchronize (the operation underlying the cudafy Synchronize method) contain execution barriers. Therefore, from a host code perspective, such a barrier immediately following a kernel launch will appear to halt CPU thread execution for the duration of the kernel execution. For this reason, the particular barrier in this example will include both the kernel execution time, as well as the data copy time. You can use the Synchronize barrier method immediately after the kernel launch to disambiguate the timing indicated by profiling the host code."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I've got an existing WinForms application on .NET 4.0 CP framework and I want it to do computations on CUDA devices. For this I decided to use CUDAfy.NET wrapper around C/C++ Toolkit, because it is (as far as I know) the only one up-to-date with CUDA SDK. The development is going without any major problems on my machine, but I've encountered troubles when deploying to another device. More specifically, when I build the project in VS and then run it on my machine, it runs fine. But the odd thing is that it runs nvcc.exe when initializing CUDAfy modules, which is a part of CUDA SDK and shouldn't be required there. And when I try to run the binary on any target machine it throws this exception: Cannot find compiler cl.exe in path. This is an error connected to missing VS tool for C++ compiler, and it shouln't arise on target devices. And now comes the weirdest thing; when I build the sample project that comes with CUDAfy.NET and try to run it on the target device, it throws the same exception. There is nothing wrong with the target machine, according to CUDAfy.NET test app Cudafy Viewer it is compatible and it has CUDA capability. Besides I've tested it on several different devices, always with the same result. I've traced the origin of the exception and as I indicated it is thrown when initializing CUDAfy.NET: CudafyModule module = CudafyTranslator.Cudafy(); GPGPU _gpu = CudafyHost.GetDevice(eGPUType.Cuda); _gpu.LoadModule(module); According to CUDAfy.NET User Manual it should run perfectly fine on devices that meet these requirements: Windows 64-bit .NET 4.0 NVIDIA GPU with compute capability 2.0 or higher Up to date NVIDIA drivers CURAND, CUSPARSE, CUFFT and CUBLAS dlls if using these math libraries Precompiled CUDAfy modules All of these are satisfied but it still doesn't run. That leaves me with a problem on my side and I'm pretty stuck there. One of the possibilities is that it is caused by wrong compilation of the code that is intended to be cudafied. According to the manual, and I quote, \"You generally would not cudafy your .NET code in a deployment situation as this requires the full CUDA SDK and Visual Studio. CUDAfy modules can be loose at .cdfy files or embedded in your application assembly (.exe or .dll) through use of the cudaycl command line tool.\". This should be done automatically, nonetheless I've tried using the cudaycl, ufortunatelly with no improvement. But since the exception occurs when initializing the CUDAfy, I think that source of the problem is elsewhere. What else might be causing it is that I build the binary for a specific architecture (e.g. CUDA 2.0) and then deploying it to another (e.g. CUDA 3.0). Something about it is mentioned in CUDA Toolkit Documentation in the section about nvcc compiler: \"Binary code is architecture-specific. A cubin object is generated using the compiler option -code that specifies the targeted architecture: For example, compiling with -code=sm_35 produces binary code for devices of compute capability 3.5.\". One way or another, I can't make it work right now. I would appreciate any help and suggestions you have. By the way I'm using latest CUDAfy.NET v1.29 and CUDA Toolkit 7.0 (the latest is not yet supported by CUDAfy.NET).",
        "answers": [
            [
                "In the CUDAfy_User_Manual_1_22.pdf there is a chapter dedicated specifically to that. It's \"5.2 Caching Modules to Improve Performance\". public class ArrayBasicIndexing { CudafyModule km = CudafyModule.TryDeserialize(); if (km == null || !km.TryVerifyChecksums()) { km = CudafyTranslator.Cudafy(); km.Serialize(); The code will check if there already is a compiled CUDAfy module and will compile a new one ONLY if there is no existing module (or it's out of date). So, your application will generate the modules on your dev machine and then you can distribute the app with the modules to other machines. Those client machines will not try to generate new modules anymore because the app has not changed. If you've changed the app, you will have to run it (so it can re-generate the modules) and then redistribute modules with the new version of the app."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to use Cudafy.NET for my programming. At first, I tested the example of cudafy.NET, but it does not work. cudafyModule km = CudafyTranslator.Cudafy(); This code showed a compilation error. So, I checked CUDA test in the cudafyviewer, but it was same. I installed the CUDA Toolkit 7.0 and I added the path of cl.exe to environment variable's system path. What more do I need?",
        "answers": [
            [
                "Are you by any chance using VS 2015? Nvcc.exe says doesn't work with that yet. You can try running nvcc.exe /? from a command prompt and you'll likely see this error if you have VS 2015: C:\\WINDOWS\\system32&gt;nvcc /? nvcc fatal : nvcc cannot find a supported version of Microsoft Visual Studio. Only the versions 2010, 2012, and 2013 are supported Update: I was able to get it to work by making sure the path to cl.exe I'd added to my PATH statement was the one pointing to C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin\\amd64. (In other words, point it to the version 12 cl.exe instead of the version 14 one.) Now Cudafy.net is working in VS 2015 for me."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have a 4GB and a 12GB gfx card with CUDA. In my application I use CUDAfy.NET and when calling the GPGPU.TotalMemory property it shows an extremely huge value (definitely incorrect). Same with FreeMemory. How to fix this? Console.WriteLine(\"GPU total memory: \" + gpu.TotalMemory.ToString()); Console.WriteLine(\"GPU free memory: \" + gpu.FreeMemory.ToString()); For the 4GB card, TotalMemory shows 18446744072635809792 bytes, FreeMemory shows 18446744072628600832 bytes.",
        "answers": [
            [
                "As talonmies pointed out, this must be a bug in CUDAfy that causes incorrect memory calculation but I found a different method to get the information. Some example code in CudafyByExample is showing exactly how to do that! So, instead of reading GPGPU class' property gpu.TotalMemory, I have to get a list of objects containing properties for each device by calling the CudafyHost.GetDeviceProperties() function, then each object will contain my desired info for each CUDA graphics card: public static void PrintGpuProperties() // this was copied from CudafyByExample { int i = 0; foreach (GPGPUProperties devicePropsContainer in CudafyHost.GetDeviceProperties(CudafyModes.Target, false)) { Console.WriteLine(\" --- General Information for device {0} ---\", i); Console.WriteLine(\"Name: {0}\", devicePropsContainer.Name); Console.WriteLine(\"Platform Name: {0}\", devicePropsContainer.PlatformName); Console.WriteLine(\"Device Id: {0}\", devicePropsContainer.DeviceId); Console.WriteLine(\"Compute capability: {0}.{1}\", devicePropsContainer.Capability.Major, devicePropsContainer.Capability.Minor); Console.WriteLine(\"Clock rate: {0}\", devicePropsContainer.ClockRate); Console.WriteLine(\"Simulated: {0}\", devicePropsContainer.IsSimulated); Console.WriteLine(); Console.WriteLine(\" --- Memory Information for device {0} ---\", i); Console.WriteLine(\"Total global mem: {0}\", devicePropsContainer.TotalMemory); Console.WriteLine(\"Total constant Mem: {0}\", devicePropsContainer.TotalConstantMemory); Console.WriteLine(\"Max mem pitch: {0}\", devicePropsContainer.MemoryPitch); Console.WriteLine(\"Texture Alignment: {0}\", devicePropsContainer.TextureAlignment); Console.WriteLine(); Console.WriteLine(\" --- MP Information for device {0} ---\", i); Console.WriteLine(\"Shared mem per mp: {0}\", devicePropsContainer.SharedMemoryPerBlock); Console.WriteLine(\"Registers per mp: {0}\", devicePropsContainer.RegistersPerBlock); Console.WriteLine(\"Threads in warp: {0}\", devicePropsContainer.WarpSize); Console.WriteLine(\"Max threads per block: {0}\", devicePropsContainer.MaxThreadsPerBlock); Console.WriteLine(\"Max thread dimensions: ({0}, {1}, {2})\", devicePropsContainer.MaxThreadsSize.x, devicePropsContainer.MaxThreadsSize.y, devicePropsContainer.MaxThreadsSize.z); Console.WriteLine(\"Max grid dimensions: ({0}, {1}, {2})\", devicePropsContainer.MaxGridSize.x, devicePropsContainer.MaxGridSize.y, devicePropsContainer.MaxGridSize.z); Console.WriteLine(); i++; } }"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a working algorithm to perform a 2D perspective transform on an image. The Algorithm is a follows: private Bitmap RescaleImage(double TopLX, double TopLY, double TopRX, double TopRY, double LowLX, double LowLY, double LowRX, double LowRY, int width, int height) { byte[] src_bmp = bmp.ToByteArray(); byte[] dst_bmp = new byte[src_bmp.Length]; for (int x = 0; x &lt; width; x++) { for (int y = 0; y &lt; height; y++) { /* * relative position */ double rx = (double)x / width; double ry = (double)y / height; /* * get top and bottom position */ double topX = TopLX + rx * (TopRX - TopLX); double topY = TopLY + rx * (TopRY - TopLY); double bottomX = LowLX + rx * (LowRX - LowLX); double bottomY = LowLY + rx * (LowRY - LowLY); /* * select center between top and bottom point */ double centerX = topX + ry * (bottomX - topX); double centerY = topY + ry * (bottomY - topY); /* * store result */ // get fractions double xf = centerX - (int)centerX; double yf = centerY - (int)centerY; // 4 colors - we're flipping sides so we can use the distance instead of inverting it later byte cTL0, cTL1, cTL2, cTL3, cTR0, cTR1, cTR2, cTR3, cLL0, cLL1, cLL2, cLL3, cLR0, cLR1, cLR2, cLR3; cTL0 = src_bmp[(((int)centerY + 1) * (width * 4)) + (((int)centerX + 1) * 4)]; cTL1 = src_bmp[((((int)centerY + 1) * (width * 4)) + (((int)centerX + 1) * 4)) + 1]; cTL2 = src_bmp[((((int)centerY + 1) * (width * 4)) + (((int)centerX + 1) * 4)) + 2]; cTR0 = src_bmp[(((int)centerY + 0) * (width * 4)) + (((int)centerX + 1) * 4)]; cTR1 = src_bmp[((((int)centerY + 0) * (width * 4)) + (((int)centerX + 1) * 4)) + 1]; cTR2 = src_bmp[((((int)centerY + 0) * (width * 4)) + (((int)centerX + 1) * 4)) + 2]; cLL0 = src_bmp[(((int)centerY + 1) * (width * 4)) + (((int)centerX + 0) * 4)]; cLL1 = src_bmp[((((int)centerY + 1) * (width * 4)) + (((int)centerX + 0) * 4)) + 1]; cLL2 = src_bmp[((((int)centerY + 1) * (width * 4)) + (((int)centerX + 0) * 4)) + 2]; cLR0 = src_bmp[(((int)centerY + 0) * (width * 4)) + (((int)centerX + 0) * 4)]; cLR1 = src_bmp[((((int)centerY + 0) * (width * 4)) + (((int)centerX + 0) * 4)) + 1]; cLR2 = src_bmp[((((int)centerY + 0) * (width * 4)) + (((int)centerX + 0) * 4)) + 2]; // 4 distances double dTL = Math.Sqrt(xf * xf + yf * yf); double dTR = Math.Sqrt((1 - xf) * (1 - xf) + yf * yf); double dLL = Math.Sqrt(xf * xf + (1 - yf) * (1 - yf)); double dLR = Math.Sqrt((1 - xf) * (1 - xf) + (1 - yf) * (1 - yf)); // 4 parts double factor = 1.0 / (dTL + dTR + dLL + dLR); dTL *= factor; dTR *= factor; dLL *= factor; dLR *= factor; // accumulate parts double r = dTL * (double)cTL0 + dTR * (double)cTR0 + dLL * (double)cLL0 + dLR * (double)cLR0; double g = dTL * (double)cTL1 + dTR * (double)cTR1 + dLL * (double)cLL1 + dLR * (double)cLR1; double b = dTL * (double)cTL2 + dTR * (double)cTR2 + dLL * (double)cLL2 + dLR * (double)cLR2; byte c0 = (byte)(r + 0.5); byte c1 = (byte)(g + 0.5); byte c2 = (byte)(b + 0.5); dst_bmp[(y * (width * 4)) + (x * 4)] = c0; dst_bmp[((y * (width * 4)) + (x * 4)) + 1] = c1; dst_bmp[((y * (width * 4)) + (x * 4)) + 2] = c2; } } Bitmap bmpOut = dst_bmp.ToBitmap(width, height); return bmpOut; } This works fine and the output is exactly what i want. however i have made a very subtle change to make it run on the GPU using Cudafy: public void PerformPerspectiveCorrection(PointF TL, PointF TR, PointF LL, PointF LR) { CheckIsSet(); _gpu.Launch(Width, Height).PerspectiveCorrectionSingleOperation(_gdata.SourceImage, _gdata.ResultImage, TL.X, TL.Y, TR.X, TR.Y, LL.X, LL.Y, LR.X, LR.Y, Width, Height); } [Cudafy] private static void PerspectiveCorrectionSingleOperation(GThread thread, byte[] src_bmp, byte[] dst_bmp, double TopLX, double TopLY, double TopRX, double TopRY, double LowLX, double LowLY, double LowRX, double LowRY, int width, int height) { int x = thread.blockIdx.x; int y = thread.threadIdx.x; /* * relative position */ double rx = (double)x / width; double ry = (double)y / height; /* * get top and bottom position */ double topX = TopLX + rx * (TopRX - TopLX); double topY = TopLY + rx * (TopRY - TopLY); double bottomX = LowLX + rx * (LowRX - LowLX); double bottomY = LowLY + rx * (LowRY - LowLY); /* * select center between top and bottom point */ double centerX = topX + ry * (bottomX - topX); double centerY = topY + ry * (bottomY - topY); /* * store result */ // get fractions double xf = centerX - (int)centerX; double yf = centerY - (int)centerY; // 4 colors - we're flipping sides so we can use the distance instead of inverting it later byte cTL0, cTL1, cTL2, cTR0, cTR1, cTR2, cLL0, cLL1, cLL2, cLR0, cLR1, cLR2; cTL0 = src_bmp[(((int)centerY + 1) * (width * 4)) + (((int)centerX + 1) * 4)]; cTL1 = src_bmp[((((int)centerY + 1) * (width * 4)) + (((int)centerX + 1) * 4)) + 1]; cTL2 = src_bmp[((((int)centerY + 1) * (width * 4)) + (((int)centerX + 1) * 4)) + 2]; cTR0 = src_bmp[(((int)centerY + 0) * (width * 4)) + (((int)centerX + 1) * 4)]; cTR1 = src_bmp[((((int)centerY + 0) * (width * 4)) + (((int)centerX + 1) * 4)) + 1]; cTR2 = src_bmp[((((int)centerY + 0) * (width * 4)) + (((int)centerX + 1) * 4)) + 2]; cLL0 = src_bmp[(((int)centerY + 1) * (width * 4)) + (((int)centerX + 0) * 4)]; cLL1 = src_bmp[((((int)centerY + 1) * (width * 4)) + (((int)centerX + 0) * 4)) + 1]; cLL2 = src_bmp[((((int)centerY + 1) * (width * 4)) + (((int)centerX + 0) * 4)) + 2]; cLR0 = src_bmp[(((int)centerY + 0) * (width * 4)) + (((int)centerX + 0) * 4)]; cLR1 = src_bmp[((((int)centerY + 0) * (width * 4)) + (((int)centerX + 0) * 4)) + 1]; cLR2 = src_bmp[((((int)centerY + 0) * (width * 4)) + (((int)centerX + 0) * 4)) + 2]; // 4 distances double dTL = Math.Sqrt(xf * xf + yf * yf); double dTR = Math.Sqrt((1 - xf) * (1 - xf) + yf * yf); double dLL = Math.Sqrt(xf * xf + (1 - yf) * (1 - yf)); double dLR = Math.Sqrt((1 - xf) * (1 - xf) + (1 - yf) * (1 - yf)); // 4 parts double factor = 1.0 / (dTL + dTR + dLL + dLR); dTL *= factor; dTR *= factor; dLL *= factor; dLR *= factor; // accumulate parts double r = dTL * (double)cTL0 + dTR * (double)cTR0 + dLL * (double)cLL0 + dLR * (double)cLR0; double g = dTL * (double)cTL1 + dTR * (double)cTR1 + dLL * (double)cLL1 + dLR * (double)cLR1; double b = dTL * (double)cTL2 + dTR * (double)cTR2 + dLL * (double)cLL2 + dLR * (double)cLR2; byte c0 = (byte)(r + 0.5); byte c1 = (byte)(g + 0.5); byte c2 = (byte)(b + 0.5); dst_bmp[(y * (width * 4)) + (x * 4)] = c0; dst_bmp[((y * (width * 4)) + (x * 4)) + 1] = c1; dst_bmp[((y * (width * 4)) + (x * 4)) + 2] = c2; } The byte[] i get back is all 0's. I have tried directly applying a value (255) to all bytes in dst_bmp as well and it seems to only perform the operations for only one row of pixels (1280 bytes as the first row is 320px and there are 4 byte per px). Any ideas? This is infuriating!",
        "answers": [
            [
                "Found the answer, wasn't to do with my algorithm at all! i was passing float values into the doubles for the Cudafy method. This cannot happen as the GPU is running the CUDA \"blind\" and thus will access the floats as if they are double without casting them before allocating the type of memory for the values. Changed the double parameters to floats, worked like a charm."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using Cudafy.NET to perform image processing on the GPU. I have set up my class along with the first function (PerformBarrelCorrection running on the CPU) to set up the multiple threads to perform the logic calculation for each pixel in the image. However every time I launch the function on the GPU it throws an exception: An exception of type 'Cudafy.Host.CudafyHostException' occurred in Cudafy.NET.dll but was not handled in user code Additional information: CUDA.NET exception: ErrorInvalidValue. Here is the class in its entirety commented to show the line on which the exception is thrown. using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks; using Cudafy; using Cudafy.Host; using Cudafy.Translator; using System.Drawing; using System.IO; namespace AForgeTrial { class GPUProcessing { public CudafyModule km; public GPGPU gpu; public GPUProcessing() { km = CudafyTranslator.Cudafy(); gpu = CudafyHost.GetDevice(CudafyModes.Target, CudafyModes.DeviceId); gpu.LoadModule(km); } public Bitmap PerformBarrelCorrection(Bitmap source, double strength, float zoom, int imageWidth, int imageHeight) { byte[] bmp = ImageToByteArray2(source); int halfWidth = imageWidth / 2; int halfHeight = imageHeight / 2; double correctionRadius = Math.Sqrt(((imageWidth * imageWidth) + (imageHeight * imageHeight)) / strength); byte[] dev_src_bmp = gpu.CopyToDevice(bmp); byte[] dst_bmp = new byte[bmp.Length]; byte[] dev_dst_bmp = gpu.Allocate&lt;byte&gt;(dst_bmp); double[] correctPass = new double[1]; correctPass[0] = correctionRadius; double[] dev_correctionRadius = gpu.CopyToDevice&lt;double&gt;(correctPass); float[] zoomPass = new float[1]; zoomPass[0] = zoom; float[] dev_zoom = gpu.CopyToDevice&lt;float&gt;(zoomPass); int[] halfWidthPass = new int[1]; halfWidthPass[0] = halfWidth; int[] dev_halfWidth = gpu.CopyToDevice&lt;int&gt;(halfWidthPass); int[] halfHeightPass = new int[1]; halfHeightPass[0] = imageHeight; int[] dev_halfHeight = gpu.CopyToDevice&lt;int&gt;(halfHeightPass); //int blksize = ((bmp.Length / 3) / 128) + 1; // EXCEPTION HAPPENS ON THE LINE BELOW gpu.Launch((bmp.Length / 3), 1).BarrelCorrectionSingleOperation(dev_src_bmp, dev_dst_bmp, dev_correctionRadius, dev_zoom, dev_halfWidth, dev_halfHeight); gpu.CopyFromDevice(dev_dst_bmp, dst_bmp); // Convert dst_bmp to Bitmap and return it Bitmap result; using (MemoryStream ms = new MemoryStream(dst_bmp)) { result = new Bitmap(ms); } return result; } [Cudafy] public static void BarrelCorrectionSingleOperation(GThread thread, byte[] src_bmp, byte[] dst_bmp, double[] correctionRadius, float[] zoom, int[] halfWidth, int[] halfHeight) { // Move a single byte from source to destination or fill if required } public static byte[] ImageToByteArray(Bitmap img) { ImageConverter converter = new ImageConverter(); return (byte[])converter.ConvertTo(img, typeof(byte[])); } public static byte[] ImageToByteArray2(Bitmap img) { byte[] byteArray = new byte[0]; using (MemoryStream stream = new MemoryStream()) { img.Save(stream, System.Drawing.Imaging.ImageFormat.Png); stream.Close(); byteArray = stream.ToArray(); } return byteArray; } public void Close() { gpu.FreeAll(); } } } Anyone know anything about this?? Thanks in advance.",
        "answers": [
            [
                "Solved my own question for anyone interested although its very niche, my code was full of holes.. This successfully performs real-time lens correction using Cudafy. public void PerformBarrelCorrection(double strength, float zoom) { CheckIsSet(); int halfWidth = Width / 2; int halfHeight = Height / 2; double correctionRadius = Math.Sqrt(((Width * Width) + (Height * Height)) / strength); _gpu.Launch(Width, Height).BarrelCorrectionSingleOperation(_gdata.SourceImage, _gdata.ResultImage, correctionRadius, zoom, halfWidth, halfHeight); } [Cudafy] public static void BarrelCorrectionSingleOperation(GThread thread, byte[] src_bmp, byte[] dst_bmp, double correctionRadius, float zoom, int halfWidth, int halfHeight) { // Move a single byte from source to destination or fill if required int x = thread.blockIdx.x; int y = thread.threadIdx.x; int newX = x - halfWidth; int newY = y - halfHeight; double distance = Math.Sqrt((newX * newX) + (newY * newY)); double r = distance / correctionRadius; double theta; if (r == 0) { theta = 1; } else { theta = Math.Atan(r) / r; } int sourceX = (int)(halfWidth + theta * newX * zoom); int sourceY = (int)(halfHeight + theta * newY * zoom); dst_bmp[(y * ((halfWidth * 2) * 4)) + (x * 4)] = src_bmp[(sourceY * ((halfWidth * 2) * 4)) + (sourceX * 4)]; dst_bmp[(y * (((halfWidth * 2) * 4)) + (x * 4)) + 1] = src_bmp[((sourceY * ((halfWidth * 2) * 4)) + (sourceX * 4)) + 1]; dst_bmp[(y * (((halfWidth * 2) * 4)) + (x * 4)) + 2] = src_bmp[((sourceY * ((halfWidth * 2) * 4)) + (sourceX * 4)) + 2]; dst_bmp[(y * (((halfWidth * 2) * 4)) + (x * 4)) + 3] = src_bmp[((sourceY * ((halfWidth * 2) * 4)) + (sourceX * 4)) + 3]; }"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a C# &amp; .NET application that uses a GPU (NVIDA GTX980) to do image processing. There are 4 stages and I synch the CPU to the GPU (no overlap in time) to do timing. But the numbers do not add up. Launch() will do a async launch of the GPU kernel) but synchronize() will wait till it is done. Total: tThreshold: 4.2827ms tHistogram: 3.7714ms tHistogramSum: 0.1065ms tIQR: 3.8603ms tThresholdOnly: 0.4126ms What is going on? public static void threshold() { Stopwatch watch = new Stopwatch(); watch.Start(); gpu.Lock(); dim3 block = new dim3(tileWidthBig, tileHeightBig); dim3 grid = new dim3(Frame.width / tileWidthBig, Frame.height / tileHeightBig); gpu.Launch(grid, block).gHistogram(gForeground, gPercentile, gInfo); gpu.Synchronize(); tHistogram = watch.Elapsed.TotalMilliseconds; block = new dim3(1024); grid = new dim3(1); gpu.Launch(grid, block).gSumHistogram(gPercentile); gpu.Synchronize(); tHistogramSum = watch.Elapsed.TotalMilliseconds - tHistogram; gpu.Launch(grid, block).gIQR(gPercentile, gInfo); gpu.Synchronize(); tIQR = watch.Elapsed.TotalMilliseconds - tHistogramSum; block = new dim3(256, 4); grid = new dim3(Frame.width / 256, Frame.height / 4); gpu.Launch(grid, block).gThreshold(gForeground, gMask, gInfo); gpu.Synchronize(); tThresholdOnly = watch.Elapsed.TotalMilliseconds - tIQR; gpu.Unlock(); watch.Stop(); tThreshold = watch.Elapsed.TotalMilliseconds; }",
        "answers": [
            [
                "As the TotalMilliseconds is constantly incrementing &amp; you are trying to find differences between points in time, you need to subtract the sum of the preceding differences after the second one, hence : tIQR = watch.Elapsed.TotalMillisconds - (tHistogram + tHistogramSum); &amp; tThresholdOnly= watch.Elapsed.TotalMillisconds - (tHistogram + tHistogramSum + tIQR);"
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "I'm using Cudafy and would like my users to be able to use CUDA without installing the CUDA SDK, but they can use the Cudafy DLL. To avoid nvcc compilation done automatically in CudafyTranslator.Cudafy(types), I'm using the following approach: string directory = Path.GetDirectoryName(Assembly.GetExecutingAssembly().Location); string modulePath = Path.Combine(directory, myType.Name + \".cdfy\"); CudafyModule km = CudafyModule.TryDeserialize(modulePath); if (ReferenceEquals(km, null) || !km.TryVerifyChecksums()) { km = CudafyTranslator.Cudafy(types); km.Serialize(modulePath); } GPU.LoadModule(km); Where types is an array of System.Types. The problem is in the third line, TryDeserialize always returns null. I have checked that the file exists and the modulePath is correct and the file exists. Can someone please shed some light on the matter? I'm ready to change my approach if it doesn't mean re-writing my Cudafy modules.",
        "answers": [
            [
                "This is my function (which is partially based on your code btw): public static void LoadTypeModule(GPGPU gpu, Type typeToCudafy) { Console.WriteLine(\"Loading module \"+typeToCudafy.Name); string appFolder = AppDomain.CurrentDomain.BaseDirectory; string typeModulePath = Path.Combine(appFolder, typeToCudafy.Name + \".cdfy\"); CudafyModule cudaModule = CudafyModule.TryDeserialize(typeModulePath); if (cudaModule == null || !cudaModule.TryVerifyChecksums()) { // if failed to open module: // translate the type's code to cuda code cudaModule = CudafyTranslator.Cudafy(typeToCudafy); cudaModule.Serialize(); // save to file } // load module to cuda memory, don't unload previous modules gpu.LoadModule(cudaModule, false); } Now calling the function: MyClassWithCudaKernels classModule = new MyClassWithCudaKernels(); LoadTypeModule(gpu, classModule.GetType());"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to install CUDAfy.NET for the first time but after installation I receive this error when I click \"Run CUDA Test\" on the CUDAfy viewer. I looked around but can only find one instance of this issue here: https://cudafy.codeplex.com/workitem/953. I also get the same error when I try and perform the method CudafyTranslator.Cudafy() What could be causing this? Do I need to point something to a directory?",
        "answers": [
            [
                "The current version (1.28) of Cudafy.Net only supports CUDA Toolkit 6.5. The default CUDA Toolkit to download on NVIDIA website is the version 7.0. Make sure you download and install version 6.5. Note that CUDA Toolkit 7.0RC was released only 9 days (as of January 13th 2015) after Cudafy.Net 1.28 was released, so you can probably expect to be able to use 7.0 with the next version of Cudafy.Net (which isn't released yet, obviously)."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "My CUDA kernel would run much faster if instead of a shared memory array (L1) I was able to use thread register memory. I can do this in CUDA-C with the following declaration: unsigned short window[15]; but in C# when I try: ushort[] window = new ushort[15]; I get a run-time error about not being able to create arrays in CudaFy. I don't want a global memory array, or a shared memory. (which are most of the discussion issues). I am running with SM35 architecture. Array create expressions are not supported.",
        "answers": [
            [
                "Seems like Cudafy currently doesn't support creating thread local arrays, so doing this through Cudafy is currently not possible. You could do it the manual way and force Cudafy to use a manually edited version of the generated Cuda C code: CudaGPU gpu = (CudaGPU)CudafyHost.GetDevice(eGPUType.Cuda, 0); CompileProperties prop = new CompileProperties() { Platform = ePlatform.x64, Architecture = eArchitecture.sm_50, CompileMode = eCudafyCompileMode.Default, CompilerPath = @\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v6.5\\bin\\nvcc\", WorkingDirectory = @\"C:\\Path\\To\\Project\\bin\\Debug\", TimeOut = 60000, IncludeDirectoryPath = @\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v6.5\\include\", }; CudafyModule module = CudafyTranslator.Cudafy(prop, typeof(YourClass)); To make sure it will use the modified code file instead of the code file generated by the call to CudafyTranslator.Cudafy(), the architecture must be set higher than the previous. prop.Architecture = eArchitecture.sm_52; prop.InputFile = @\"ModifiedCudaSource.cu\"; The file specified in InputFile must be in WorkingDirectory Finally, add the modified source file to the module, compile the new PTX, and load the module: module.AddSourceCodeFile(new SourceCodeFile(File.ReadAllText(Path.Combine(prop.WorkingDirectory, prop.InputFile), Encoding.Default), eLanguage.Cuda, prop.Architecture)); module.Compile(prop); gpu.LoadModule(module); I'm not entirely sure if you even need to call CudafyTranslator.Cudafy() to get the module, but this is working for me and I only run this code once, so cba to do further testing :)"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Making my first steps with Cudafy and trying to write a function which will take its thread's location and based on that save some int value into an array element. My code: [Cudafy] public static void GenerateRipples(GThread thread, int[] results) { int threadPosInBlockX = thread.threadIdx.x; int threadPosInBlockY = thread.threadIdx.y; int blockPosInGridX = thread.blockIdx.x; int blockPosInGridY = thread.blockIdx.y; int gridSizeX = thread.gridDim.x; int gridSizeY = thread.gridDim.y; int blockSizeX = thread.blockDim.x; int blockSizeY = thread.blockDim.y; //int threadX = blockSizeX*blockPosInGridX + threadPosInBlockX; //if i use only one variable, everything is fine: int threadY = blockSizeY; //if i add or multiply anything, it cannot compile: //int threadY = blockSizeY*blockPosInGridY + threadPosInBlockY; // results[gridSizeX*blockSizeX*threadY + threadX] = 255; } So I cannot calculate the threadY here. If I use more than one variable in the calculations, the Cudafy translating class throws an error (CudafyModule cm = CudafyTranslator.Cudafy(); throws a Cudafy.CudafyLanguageException). What am I doing wrong? UPDATE: This is the code which runs the kernel on GPU: public void RunTest2() { GPGPU gpu = CudafyHost.GetDevice(CudafyModes.Target, CudafyModes.DeviceId); CudafyModule km = CudafyTranslator.Cudafy(); gpu.LoadModule(km); int size = 20 * 20; int[] allPixels = new int[size]; int[] dev_result = gpu.Allocate&lt;int&gt;(size); dim3 blocksInGrid = new dim3(5, 5); dim3 threadsPerBlock = new dim3(4, 4); gpu.Launch(blocksInGrid, threadsPerBlock).GenerateRipples(dev_result); gpu.CopyFromDevice(dev_result, allPixels); gpu.FreeAll(); }",
        "answers": [
            [
                "We need to see how you are launching your kernel, the code above should run just fine. I created a test class that runs just fine and gives you a sample of how to prep the kernel grid/block/threads dimensions. If you want to see great examples download the Cudafy source code and compile the CudafyExamples project, check out how they prep and use the functionality of CUDAfy. ** Note: I must have smoked something pretty good before I posted the first class, I neglected to verify it didn't generate memory access violations!! Fixed class below, no violations. Look up great examples on Codeproject and StackOverflow. using System; using System.Collections.Generic; using System.Diagnostics; using System.Linq; using System.Text; using Cudafy; using Cudafy.Host; using Cudafy.Translator; namespace FxKernelTest { public class FxKernTest { public GPGPU fxgpu; public const int N = 1024 * 64; public void ExeTestKernel() { GPGPU gpu = CudafyHost.GetDevice(CudafyModes.Target, 0); eArchitecture arch = gpu.GetArchitecture(); CudafyModule km = CudafyTranslator.Cudafy(arch); gpu.LoadModule(km); int[] host_results = new int[N]; // Either assign a new block of memory to hold results on device var dev_results = gpu.Allocate&lt;int&gt;(N); gpu.Set&lt;int&gt;(dev_results); // Or fill your array with values first and then for (int i = 0; i &lt; N; i++) host_results[i] = i * 3; // Copy array with ints to device //var dev_filled_results = gpu.CopyToDevice(host_results); // 64*16 = 1024 threads per block (which is max for sm_30) dim3 threadsPerBlock = new dim3(64, 16); // 8*8 = 64 blocks per grid, 1024 threads per block = kernel launched 65536 times dim3 blocksPerGrid = new dim3(8, 8); //var threadsPerBlock = 1024; // this will only give you blockDim.x = 1024, .y = 0, .z = 0 //var blocksPerGrid = 1; // just for show gpu.Launch(blocksPerGrid, threadsPerBlock, \"GenerateRipples\", dev_results); gpu.CopyFromDevice(dev_results, host_results); // Test our results for (int index = 0; index &lt; N; index++) if (host_results[index] != index) throw new Exception(\"Check your indexing math, genius!!!\"); } [Cudafy] public static void GenerateRipples(GThread thread, int[] results) { var blockSize = thread.blockDim.x * thread.blockDim.y; var offsetToGridY = blockSize * thread.gridDim.x; // This took me a few tries, I've never used 4 dimensions into a 1D array beofre :) var tid = thread.blockIdx.y * offsetToGridY + // each Grid Y is 8192 in size thread.blockIdx.x * blockSize + // each Grid X is 1024 in size thread.threadIdx.y * thread.blockDim.x + // each Block Y is 64 in size thread.threadIdx.x; // index into block var threadPosInBlockX = thread.threadIdx.x; var threadPosInBlockY = thread.threadIdx.y; var blockPosInGridX = thread.blockIdx.x; var blockPosInGridY = thread.blockIdx.y; var gridSizeX = thread.gridDim.x; var gridSizeY = thread.gridDim.y; var blockSizeX = thread.blockDim.x; var blockSizeY = thread.blockDim.y; // this is your code, see how I calculate the actual thread ID above! var threadX = blockSizeX * blockPosInGridX + threadPosInBlockX; //if i use only one variable, everything is fine: var threadY = blockSizeY; // this calculates just fine threadY = blockSizeY * blockPosInGridY + threadPosInBlockY; // hint: use NSight for Visual Studio and look at the NSight output, // it reports access violations and tells you where... // if our threadId is within bounds of array size // we cause access violation if not // (class constants are automatically passed to kernels) if (tid &lt; N) results[tid] = tid; } } } ptxas info : 0 bytes gmem ptxas info : Compiling entry function 'GenerateRipples' for 'sm_30' ptxas info : Function properties for GenerateRipples 0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads ptxas info : Used 5 registers, 328 bytes cmem[0]"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Given a vector (or two points) how can i get the discrete coordinates that this vector intersects in some given interval? I am using this such that given a ray(vector) i can calculate the pixels in an image that this ray intersects and use these as indexes for my image. The ray is always in the plane of the image in the case of 3D. Also the vector comes from another coordinate system than the one used for the image indexes, but this is just a scaling between coordinate systems. I'm looking for a solution in 3D but 2D can be accepted. EDIT: The interval is a 2d space, so the solution is a set of points in this 2d interval. And this will be run on a GPU with CUDAfy.NET",
        "answers": [
            [
                "Your vector P1,P2 is all these points: vector := P1 + a * (P2-P1) with a in [0;1] Yout intervall P3,P4 is all these points: interval := P3 + b * (P4 - P3) with b in [0,1] They interscet in the same point: vector == interval P1 + a * (P2-P1) == P3 + b * (P4-P3) In 2d these are two equations with two unknowns -&gt; solvable"
            ],
            [
                "Here are my assumptions: the bottom-left corner of your image is the origin (the point with coordinates (0, 0)) you have an image with width w and height h you can put the vector in the form of a linear equation (i.e., y=mx+b, where m is the slope and b is the y-intercept) Given those assumptions, do the following to find the discrete coordinates where the line intersects the edges of your image: /// &lt;summary&gt; /// Find discreet coordinates where the line y=mx+b intersects the edges of a w-by-h image. /// &lt;/summary&gt; /// &lt;param name=\"m\"&gt;slope of the line&lt;/param&gt; /// &lt;param name=\"b\"&gt;y-intercept of the line&lt;/param&gt; /// &lt;param name=\"w\"&gt;width of the image&lt;/param&gt; /// &lt;param name=\"h\"&gt;height of the image&lt;/param&gt; /// &lt;returns&gt;the points of intersection&lt;/returns&gt; List&lt;Point&gt; GetIntersectionsForImage(double m, double b, double w, double h) { var intersections = new List&lt;Point&gt;(); // Check for intersection with left side (y-axis). if (b &gt;= 0 &amp;&amp; b &lt;= h) { intersections.Add(new Point(0.0, b)); } // Check for intersection with right side (x=w). var yValRightSide = m * w + b; if (yValRightSide &gt;= 0 &amp;&amp; yValRightSide &lt;= h) { intersections.Add(new Point(w, yValRightSide)); } // If the slope is zero, intersections with top or bottom will be taken care of above. if (m != 0.0) { // Check for intersection with top (y=h). var xValTop = (h - b) / m; if (xValTop &gt;= 0 &amp;&amp; xValTop &lt;= w) { intersections.Add(new Point(xValTop, h)); } // Check for intersection with bottom (y=0). var xValBottom = (0.0 - b) / m; if (xValBottom &gt;= 0 &amp;&amp; xValBottom &lt;= w) { intersections.Add(new Point(xValBottom, 0)); } } return intersections; } And here are the tests to make sure it works: [TestMethod] public void IntersectingPoints_AreCorrect() { // The line y=x intersects a 1x1 image at points (0, 0) and (1, 1). var results = GetIntersectionsForImage(1.0, 0.0, 1.0, 1.0); foreach (var p in new List&lt;Point&gt; { new Point(0.0, 0.0), new Point(1.0, 1.0) }) { Assert.IsTrue(results.Contains(p)); } // The line y=1 intersects a 2x2 image at points (0, 1), and (2, 1). results = GetIntersectionsForImage(0.0, 1.0, 2.0, 2.0); foreach (var p in new List&lt;Point&gt; { new Point(0.0, 1.0), new Point(2.0, 1.0) }) { Assert.IsTrue(results.Contains(p)); } }"
            ],
            [
                "After a bit of searching I found Bresenham's line algorithm which was more or less just what I needed. Please refer to this answer if you're interested in the algorithm I used."
            ]
        ],
        "votes": [
            1e-07,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I have a 5 million list of 32 bit integers (actually a 2048 x 2560 image) that is 90% zeros. The non-zero cells are labels (e.g. 2049, 8195, 1334300, 34320923, 4320932) that completely not sequential or consecutive in any way (it is the output of our custom connected component labeling CCL algorithm). I am working with a NVIDA Tesla K40, so I would love it if this needs any prefix-scan work, that it uses SHUFFLE, BALLOT or any of the higher CC features. I don't need a full worked out example, just some advice. To illustrate, here is one blog that was labeled by our CCL algorithm. Other blobs will have a different unique label (e.g. 13282). But all will be surrounded by zeros, and be ellipsoid in shape. (We optimized our CCL for ellipsoids, that is why we don't use the libraries). But one side effect is that the blob labels are not consecutive numbers. We don't care what order they are numbered, but we want one blob labeled #1, and other labeled #2, and the last one to be be labeled #n, where n is the number of blobs in the image. What do I mean Labled #1? I mean that all 2242 cells should be replaced with a 1. and all 13282 cells would have a #2, etc. The maximumb blob number from our CCL is equal to 2048x2560. So we know the size of the array. Actually, Robert Crovella already gave a great answer for this a day ago. It was not exact, but I now see how to apply the answer. So I don't need any more help. But he was so generous in his time and effort, and asked me to re-write the problem with examples, so I did that.",
        "answers": [
            [
                "One possible approach would be to use a sequence of: thrust::transform - to convert the input data to all 1 or 0: 0 27 42 0 18 99 94 91 0 -- input data 0 1 1 0 1 1 1 1 0 -- this will be our \"mask vector\" thrust::inclusive_scan - to convert the mask vector into a progressive sequence: 0 1 1 0 1 1 1 1 0 -- \"mask\" vector 0 1 2 2 3 4 5 6 6 -- \"sequence\" vector Another thrust::transform to mask off the non-increasing values: 0 1 1 0 1 1 1 1 0 -- \"mask\" vector 0 1 2 2 3 4 5 6 6 -- \"sequence\" vector ------------------------- 0 1 2 0 3 4 5 6 0 -- result of \"AND\" operation Note that we could combine the first two steps with thrust::transform_inclusive_scan and then perform the third step as a thrust::transform with a slightly different transform functor. This modification allows us to dispense with the creation of the temporary \"mask\" vector. Here's a fully worked example showing the \"modified\" approach using thrust::transform_inclusive_scan: $ cat t635.cu #include &lt;iostream&gt; #include &lt;stdlib.h&gt; #include &lt;thrust/device_vector.h&gt; #include &lt;thrust/host_vector.h&gt; #include &lt;thrust/transform.h&gt; #include &lt;thrust/transform_scan.h&gt; #include &lt;thrust/generate.h&gt; #include &lt;thrust/copy.h&gt; #define DSIZE 20 #define PCT_ZERO 40 struct my_unary_op { __host__ __device__ int operator()(const int data) const { return (!data) ? 0:1;} }; struct my_binary_op { __host__ __device__ int operator()(const int d1, const int d2) const { return (!d1) ? 0:d2;} }; int main(){ // generate DSIZE random 32-bit integers, PCT_ZERO% are zero thrust::host_vector&lt;int&gt; h_data(DSIZE); thrust::generate(h_data.begin(), h_data.end(), rand); for (int i = 0; i &lt; DSIZE; i++) if ((rand()%100)&lt; PCT_ZERO) h_data[i] = 0; else h_data[i] %= 1000; thrust::device_vector&lt;int&gt; d_data = h_data; thrust::device_vector&lt;int&gt; d_result(DSIZE); thrust::transform_inclusive_scan(d_data.begin(), d_data.end(), d_result.begin(), my_unary_op(), thrust::plus&lt;int&gt;()); thrust::transform(d_data.begin(), d_data.end(), d_result.begin(), d_result.begin(), my_binary_op()); thrust::copy(d_data.begin(), d_data.end(), std::ostream_iterator&lt;int&gt;(std::cout, \",\")); std::cout &lt;&lt; std::endl; thrust::copy(d_result.begin(), d_result.end(), std::ostream_iterator&lt;int&gt;(std::cout, \",\")); std::cout &lt;&lt; std::endl; return 0; } $ nvcc -o t635 t635.cu $ ./t635 0,886,777,0,793,0,386,0,649,0,0,0,0,59,763,926,540,426,0,736, 0,1,2,0,3,0,4,0,5,0,0,0,0,6,7,8,9,10,0,11, $ Responding to the update, this new information makes the problem more difficult to solve, in my opinion. Histogramming techniques come to mind, but without any limits on the occupied range of the 32 bit integers (labels) or any limits on the number of times a particular label may be duplicated within the data set, histogramming techniques seem impractical. This leads me to consider sorting the data. An approach like this should work: Use thrust::sort to sort the data. Use thrust::unique to remove the duplicates. The sorted data with duplicates removed now gives us our ordering for our output set [0,1,2, ...]. Let's call this our \"map\". We can use a parallel binary-search technique to convert each label in the original data set to it's mapped output value. This process seems pretty \"expensive\" to me. I would suggest reconsidering the upstream labelling operation to see if it can be re-designed to produce a data set more amenable to efficient downstream processing. Anyway here is a fully worked example: $ cat t635.cu #include &lt;iostream&gt; #include &lt;stdlib.h&gt; #include &lt;thrust/device_vector.h&gt; #include &lt;thrust/host_vector.h&gt; #include &lt;thrust/transform.h&gt; #include &lt;thrust/generate.h&gt; #include &lt;thrust/sort.h&gt; #include &lt;thrust/unique.h&gt; #include &lt;thrust/copy.h&gt; #define DSIZE 20 #define PCT_ZERO 40 #define RNG 10 #define nTPB 256 // sets idx to the index of the first element in a that is // equal to or larger than key __device__ void bsearch_range(const int *a, const int key, const unsigned len_a, unsigned *idx){ unsigned lower = 0; unsigned upper = len_a; unsigned midpt; while (lower &lt; upper){ midpt = (lower + upper)&gt;&gt;1; if (a[midpt] &lt; key) lower = midpt +1; else upper = midpt; } *idx = lower; return; } __global__ void find_my_idx(const int *a, const unsigned len_a, int *my_data, int *my_idx, const unsigned len_data){ unsigned idx = (blockDim.x * blockIdx.x) + threadIdx.x; if (idx &lt; len_data){ unsigned sp_a; int val = my_data[idx]; bsearch_range(a, val, len_a, &amp;sp_a); my_idx[idx] = sp_a; } } int main(){ // generate DSIZE random 32-bit integers, PCT_ZERO% are zero thrust::host_vector&lt;int&gt; h_data(DSIZE); thrust::generate(h_data.begin(), h_data.end(), rand); for (int i = 0; i &lt; DSIZE; i++) if ((rand()%100)&lt; PCT_ZERO) h_data[i] = 0; else h_data[i] %= RNG; thrust::device_vector&lt;int&gt; d_data = h_data; thrust::device_vector&lt;int&gt; d_result = d_data; thrust::sort(d_result.begin(), d_result.end()); thrust::device_vector&lt;int&gt; d_unique = d_result; int unique_size = thrust::unique(d_unique.begin(), d_unique.end()) - d_unique.begin(); find_my_idx&lt;&lt;&lt; (DSIZE+nTPB-1)/nTPB , nTPB &gt;&gt;&gt;(thrust::raw_pointer_cast(d_unique.data()), unique_size, thrust::raw_pointer_cast(d_data.data()), thrust::raw_pointer_cast(d_result.data()), DSIZE); thrust::copy(d_data.begin(), d_data.end(), std::ostream_iterator&lt;int&gt;(std::cout, \",\")); std::cout &lt;&lt; std::endl; thrust::copy(d_result.begin(), d_result.end(), std::ostream_iterator&lt;int&gt;(std::cout, \",\")); std::cout &lt;&lt; std::endl; return 0; } $ nvcc t635.cu -o t635 $ ./t635 0,6,7,0,3,0,6,0,9,0,0,0,0,9,3,6,0,6,0,6, 0,2,3,0,1,0,2,0,4,0,0,0,0,4,1,2,0,2,0,2, $"
            ],
            [
                "My answer is similar to the one @RobertCrovella gave, but I think it's simpler to use thrust::lower_bound instead of the custom binary search. (now that it's pure thrust, the backend can be interchanged) Copy input data Sort the copied data Create a unique list from the sorted data Find the lower bound of each input, in the unique list I have included a full example below. Interestingly the process can become quicker by pre-pending the sort step, with another call to thrust::unique. Depending on the input data, this can dramatically reduce the number of elements in the sort, which is the bottleneck here. #include &lt;iostream&gt; #include &lt;stdlib.h&gt; #include &lt;thrust/device_vector.h&gt; #include &lt;thrust/host_vector.h&gt; #include &lt;thrust/transform.h&gt; #include &lt;thrust/generate.h&gt; #include &lt;thrust/sort.h&gt; #include &lt;thrust/unique.h&gt; #include &lt;thrust/binary_search.h&gt; #include &lt;thrust/copy.h&gt; int main() { const int ndata = 20; // Generate host input data thrust::host_vector&lt;int&gt; h_data(ndata); thrust::generate(h_data.begin(), h_data.end(), rand); for (int i = 0; i &lt; ndata; i++) { if ((rand() % 100) &lt; 40) h_data[i] = 0; else h_data[i] %= 10; } // Copy data to the device thrust::device_vector&lt;int&gt; d_data = h_data; // Make a second copy of the data thrust::device_vector&lt;int&gt; d_result = d_data; // Sort the data copy thrust::sort(d_result.begin(), d_result.end()); // Allocate an array to store unique values thrust::device_vector&lt;int&gt; d_unique = d_result; { // Compress all duplicates const auto end = thrust::unique(d_unique.begin(), d_unique.end()); // Search for all original labels, in this compressed range, and write their // indices back as the result thrust::lower_bound( d_unique.begin(), end, d_data.begin(), d_data.end(), d_result.begin()); } thrust::copy( d_data.begin(), d_data.end(), std::ostream_iterator&lt;int&gt;(std::cout, \",\")); std::cout &lt;&lt; std::endl; thrust::copy(d_result.begin(), d_result.end(), std::ostream_iterator&lt;int&gt;(std::cout, \",\")); std::cout &lt;&lt; std::endl; return 0; }"
            ]
        ],
        "votes": [
            4.0000001,
            1e-07
        ]
    },
    {
        "question": "I would like to use some of the OpenCV routines (2D convolve, Region Labeling, and Centroiding) in a CudaFy.net project. Is this a stupid idea? Would it be better just to implement the algorithms in C# from opensource examples? Some of inputs to the OpenCV have will already be in global GPU memory, can you pass pointers to OpenCV GPU routines and say the matrix is already in the GPU? Are there any simple examples of doing this I did see one person who used EMGU and openCV but did run into some issues. Is there an example around of someone doing this successfully? [ https://cudafy.codeplex.com/discussions/356649 ]",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using C# and CUDAfy.net (yes, this problem is easier in straight C with pointers, but I have my reasons for using this approach given the larger system). I have a video frame grabber card that is collecting byte[1024 x 1024] image data at 30 FPS. Every 33.3 ms it fills a slot in a circular buffer and returns a System.IntPtr that points to that un-managed 1D vector of *byte; The Circular buffer has 15 slots. On the GPU device (Tesla K40) I want to have a global 2D array that is organized as a dense 2D array. That is, I want something like the Circular Queue but on the GPU organized as a dense 2D array. byte[15, 1024*1024] rawdata; // if CUDAfy.NET supported jagged arrays I could use byte[15][1024*1024 but it does not How can I fill in a different row each 33ms? Do I use something like: gpu.CopyToDevice&lt;byte&gt;(inputPtr, 0, rawdata, offset, length) // length = 1024*1024 //offset is computed by rowID*(1024*1024) where rowID wraps to 0 via modulo 15. // inputPrt is the System.Inptr that points to the buffer in the circular queue (un-managed)? // rawdata is a device buffer allocated gpu.Allocate&lt;byte&gt;(1024*1024); And in my kernel header is: [Cudafy] public static void filter(GThread thread, byte[,] rawdata, int frameSize, byte[] result) I did try something along these lines. But there is no API pattern in CudaFy for: GPGPU.CopyToDevice(T) Method (IntPtr, Int32, T[,], Int32, Int32, Int32) So I used the gpu.Cast Function to change the 2D device array to 1D. I tried the code below, but I am getting CUDA.net exception: ErrorLaunchFailed FYI: When I try the CUDA emulator, it aborts on the CopyToDevice claiming that Data is not host allocated public static byte[] process(System.IntPtr data, int slot) { Stopwatch watch = new Stopwatch(); watch.Start(); byte[] output = new byte[FrameSize]; int offset = slot*FrameSize; gpu.Lock(); byte[] rawdata = gpu.Cast&lt;byte&gt;(grawdata, FrameSize); // What is the size supposed to be? Documentation lacking gpu.CopyToDevice&lt;byte&gt;(data, 0, rawdata, offset, FrameSize * frameCount); byte[] goutput = gpu.Allocate&lt;byte&gt;(output); gpu.Launch(height, width).filter(rawdata, FrameSize, goutput); runTime = watch.Elapsed.ToString(); gpu.CopyFromDevice(goutput, output); gpu.Free(goutput); gpu.Synchronize(); gpu.Unlock(); watch.Stop(); totalRunTime = watch.Elapsed.ToString(); return output; }",
        "answers": [
            [
                "I propose this \"solution\", for now, either: 1. Run the program only in native mode (not in emulation mode). or 2. Do not handle the pinned-memory allocation yourself. There seems to be an open issue with that now. But this happens only in emulation mode. see: https://cudafy.codeplex.com/workitem/636"
            ],
            [
                "If I understand your question properly I think you are looking to convert the byte* you get from the cyclic buffer into a multi-dimensional byte array to be sent to the graphics card API. int slots = 15; int rows = 1024; int columns = 1024; //Try this for (int currentSlot = 0; currentSlot &lt; slots; currentSlot++) { IntPtr intPtrToUnManagedMemory = CopyContextFrom(currentSlot); // use Marshal.Copy ? byte[] byteData = CopyIntPtrToByteArray(intPtrToUnManagedMemory); int offset =0; for (int m = 0; m &lt; rows; m++) for (int n = 0; n &lt; columns; n++) { //then send this to your GPU method rawForGpu[m, n] = ReadByteValue(IntPtr: intPtrToUnManagedMemory, offset++); } } //or try this for (int currentSlot = 0; currentSlot &lt; slots; currentSlot++) { IntPtr intPtrToUnManagedMemory = CopyContextFrom(currentSlot); // use Marshal.Copy ? byte[] byteData = CopyIntPtrToByteArray(intPtrToUnManagedMemory); byte[,] rawForGpu = ConvertTo2DArray(byteData, rows, columns); } } private static byte[,] ConvertTo2DArray(byte[] byteArr, int rows, int columns) { byte[,] data = new byte[rows, columns]; int totalElements = rows * columns; //Convert 1D to 2D rows, colums return data; } private static IntPtr CopyContextFrom(int slotNumber) { //code that return byte* from circular buffer. return IntPtr.Zero; }"
            ],
            [
                "You should consider using the GPGPU Async functionality that's built in for a really efficient way to move data from/to host/device and use the gpuKern.LaunchAsync(...) Check out http://www.codeproject.com/Articles/276993/Base-Encoding-on-a-GPU for an efficient way to use this. Another great example can be found in CudafyExamples project, look for PinnedAsyncIO.cs. Everything you need to do what you're describing. This is in CudaGPU.cs in Cudafy.Host project, which matches the method you're looking for (only it's async): public void CopyToDeviceAsync&lt;T&gt;(IntPtr hostArray, int hostOffset, DevicePtrEx devArray, int devOffset, int count, int streamId = 0) where T : struct; public void CopyToDeviceAsync&lt;T&gt;(IntPtr hostArray, int hostOffset, T[, ,] devArray, int devOffset, int count, int streamId = 0) where T : struct; public void CopyToDeviceAsync&lt;T&gt;(IntPtr hostArray, int hostOffset, T[,] devArray, int devOffset, int count, int streamId = 0) where T : struct; public void CopyToDeviceAsync&lt;T&gt;(IntPtr hostArray, int hostOffset, T[] devArray, int devOffset, int count, int streamId = 0) where T : struct;"
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I have a problem compiling the method below. It compiles fine if the CUDA is selected as target, however if the OpenCL is selected that it throws error. [Cudafy] unsafe static void MyKernelMethod( GThread thread, [CudafyAddressSpace(eCudafyAddressSpace.Constant)] int[] a) { // fix our array to a ptr fixed (int* aPtr = a) { // get a new array pointing to some offset of the original array int* aPtr_Offset = aPtr + thread.blockIdx.x; // use it for something aPtr_Offset[thread.threadIdx.x] = 2; // some more code... } } The error: error: illegal implicit conversion between two pointers with different address spaces I do not have enough knowledge how to correct this (I am just starting GPGPU programming).",
        "answers": [],
        "votes": []
    },
    {
        "question": "Ok, so I'm using CUDAfy.Net, and I have the following 3 structs: [Cudafy] public struct Collider { public int Index; public int Type; public Sphere Sphere; public Plane Plane; public Material Material; } [Cudafy] public struct Material { public Color Color; public Texture Texture; public float Shininess; } [Cudafy] public struct Texture { public int Width, Height; public byte[ ] Data; } Now, as soon as I send over an array of Collider objects to the GPU, using CopyToDevice&lt;GPU.Collider&gt;( ColliderArray ); I get the following error: An unhandled exception of type 'System.ArgumentException' occurred in mscorlib.dll Additional information: Object contains non-primitive or non-blittable data. Does anyone with any experience with either CUDAfy.Net, or OpenCL ( since it basically compiles down into OpenCL ), have any idea how I could accomplish this? The whole problem lies in the byte array of Texture, since everything worked just fine when I didn't have a Texture struct and the array is the non-blittable part as far as I know. I had found several questions regarding the same problem, and they fixed it using fixed-size arrays. However, I am unable to do this as these are textures, which can have greatly varying sizes. EDIT: Right now, I'm doing the following on the CPU: public unsafe static GPU.Texture CreateGPUTexture( Cudafy.Host.GPGPU _GPU, System.Drawing.Bitmap Image ) { GPU.Texture T = new GPU.Texture( ); T.Width = Image.Width; T.Height = Image.Height; byte[ ] Data = new byte[ Image.Width * Image.Height * 3 ]; for ( int X = 0; X &lt; Image.Width; X++ ) for ( int Y = 0; Y &lt; Image.Height; Y++ ) { System.Drawing.Color C = Image.GetPixel( X, Y ); int ID = ( X + Y * Image.Width ) * 3; Data[ ID ] = C.R; Data[ ID + 1 ] = C.G; Data[ ID + 2 ] = C.B; } byte[ ] _Data = _GPU.CopyToDevice&lt;byte&gt;( Data ); IntPtr Pointer = _GPU.GetDeviceMemory( _Data ).Pointer; T.Data = ( byte* )Pointer.ToPointer( ); return T; } I then attach this Texture struct to the colliders, and send them to the GPU. This all goes without any errors. However, as soon as I try to USE a texture on the GPU, like this: [Cudafy] public static Color GetTextureColor( int X, int Y, Texture Tex ) { int ID = ( X + Y * Tex.Width ) * 3; unsafe { byte R = Tex.Data[ ID ]; byte G = Tex.Data[ ID + 1 ]; byte B = Tex.Data[ ID + 2 ]; return CreateColor( ( float )R / 255f, ( float )G / 255f, ( float )B / 255f ); } } I get the following error: An unhandled exception of type 'Cloo.InvalidCommandQueueComputeException' occurred in Cudafy.NET.dll Additional information: OpenCL error code detected: InvalidCommandQueue. The Texture struct looks like this, by the way: [Cudafy] public unsafe struct Texture { public int Width, Height; public byte* Data; } I'm completely at a loss again..",
        "answers": [
            [
                "Cudafy does not support arrays yet. So you can't use \"public byte[] Data\" neither in structures nor kernels itself. you could try it less object oriented. I mean try to remove data array from structre itself and copy them separately. e.g. copyToDevice(\"texture properties\") and then copy appropriate data array copyToDevice(\"texture data\") EDIT: OK I found a solution but it is not pretty code. As you get the Pointer of your data stored in GPU mem. cast him in to integer value pointer.ToInt64(); and store this value in your Structure object simply as long value(not long pointer). than you can use the GThread.InsertCode() method to insert directly code into your kernel without compiling. You can not use pointer directly in your kernel code becase they are not blittable data type. So stop talking here is the example of my working code class Program { [Cudafy] public struct TestStruct { public double value; public long dataPointer; // your data pointer adress } [Cudafy] public static void kernelTest(GThread thread, TestStruct[] structure, int[] intArray) { // Do something GThread.InsertCode(\"int* pointer = (int*)structure[0].dataPointer;\"); GThread.InsertCode(\"structure[0].value = pointer[1];\"); // Here you can acces your data using pointer pointer[0], pointer[1] and so on } private unsafe static void Main(string[] args) { GPGPU gpuCuda = CudafyHost.GetDevice(eGPUType.Cuda, 0); CudafyModule km = CudafyTranslator.Cudafy(); gpuCuda.LoadModule(km); TestStruct[] host_array = new TestStruct[1]; host_array[0] = new TestStruct(); int[] host_intArray = new[] {1, 8, 3}; int[] dev_intArray = gpuCuda.CopyToDevice(host_intArray); DevicePtrEx p = gpuCuda.GetDeviceMemory(dev_intArray); IntPtr pointer = p.Pointer; host_array[0].dataPointer = pointer.ToInt64(); TestStruct[] dev_array = gpuCuda.Allocate(host_array); gpuCuda.CopyToDevice(host_array, dev_array); gpuCuda.Launch().kernelTest(dev_array, dev_intArray); gpuCuda.CopyFromDevice(dev_array, host_array); Console.WriteLine(host_array[0].value); Console.ReadKey(); } } The \"magic\" is in InsertCode() where you cast your long dataPointer value as int pointer adress... but the disadvantage of this approache is that you must write those parts of code as String. OR you can separate your data and structures e.g. [Cudafy] public struct Texture { public int Width, Height; } [Cudafy] public static void kernelTest(GThread thread, Texture[] TexStructure, byte[] Data) {....} And simply copy dev_Data = gpu.CopyToDevice(host_Data); dev_Texture = gpu.CopyToDevice(host_Texture); gpu.Launch().kernelTest(dev_Texture, dev_Data); EDIT TWO: forget about my code :D Check this https://cudafy.codeplex.com/discussions/538310 and THIS is solution for your problem https://cudafy.codeplex.com/discussions/283527"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to do this in a kernel: int count[8]; I'm almost positive you can declare fixed size arrays within CUDA GPU kernels. So how do I go about doing this while using Cudafy? This does not work: [Cudafy] public static void kernelFunction(int[] input, int[] output) { int count[8]; // ....other stuff } The above code results in a C# error: \"Array size cannot be specified in a variable declaration (try initializing with a 'new' expression).\"",
        "answers": [
            [
                "According to the Cudafy.NET forums and as of 03/20/2014, this is not currently supported. So I added this answer in order to alleviate future people from the futile search for a solution. The thread where the above is stated is here. Note that the person in the thread who suggests the proper way to do it is to use shared memory is wrong. You can declare fixed size arrays in GPU kernels, Cudafy just does not currently support it. Shared memory is for increasing the scope of variables such that all threads within a block have access to them. It also acts as a spill-over location for variable storage when registry storage is full. Stating that shared memory is the sole location for array declarations is incorrect."
            ],
            [
                "In C#, you would do something like this: int[] count = new int[8]; Or if you are using unsafe code, you can also create fixed size buffers: fixed int count[8];"
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "Thanks for reading my thread. My Cudafy cannot load the cublas64_55.dll I am using Windows 7, VS2012, and CUDA5.5. My cublas64_55.dll, cufft64_35.dll and etc are all in C:\\Program Files\\NVIDIA GPU ComputingTookit\\CUDA\\v5.5\\bin And my environment variable of CUDA_PATH and CUDA_PATH_5.5 are both C:\\Program Files\\NVIDIA GPU ComputingTookit\\CUDA\\v5.5 I don't understand why Cudafy cannot find it. Anyone has any idea? Thanks a lot.",
        "answers": [
            [
                "simply copied the cublas64_55.dll and cufft64_55.dll to my project's /Debug/.exe folder, and it works."
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "How can you have a struct full of arrays in cudafy? This appears a somewhat trivial issue, but I could not find an easy implementation of it on the net. Some links suggest it cannot be done, see for example: Passing an array within a structure in CUDAfy While others suggest it can be done through a somewhat lengthy helper function, see for example http://cudafy.codeplex.com/discussions/283527 I am looking to pass a single struct into my Cudafy kernel, where for example the struct looks like... [Cudafy] public struct myStructTwo { public float[] value_x; public float[] value_y; public float[] value_z; } public struct myStructTwo { public IntPtr value_x; public IntPtr value_y; public IntPtr value_z; }",
        "answers": [
            [
                "The question has been addressed in a Codeplex answer. It looks to be possible using a fixed size e.g. [Cudafy] public struct myStructTwo { public float value_x[size]; } I will let this question remain open in case further responses are received."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I'm starting down the exciting road of GPU programming, and if I'm going to do some heavyweight number-crunching, I'd like to use the best libraries that are out there. I would especially like to use cuBLAS from an F# environment. CUDAfy offers the full set of drivers from their solution, and I have also been looking at Alea.cuBase, which has thrown up a few questions. The Alea.cuSamples project on GitHub makes a cryptic reference to an Examples solution: \"For more advanced test, please go to the MatrixMul projects in the Examples solution.\" However, I can't find any trace of these mysterious projects. Does anyone know the location of the elusive \"MatrixMul projects in the Examples solution\"? Given that cuSamples performs a straightfoward matrix multiplication, would the more advanced version, wherever it lives, use cuBLAS? If not, is there a way to access cuBLAS from Alea.cuBase a la CUDAfy?",
        "answers": [
            [
                "With Alea GPU V2, the new version we have now two options: Alea Unbound library provides optimized matrix multiplication implementations http://quantalea.com/static/app/tutorial/examples/unbound/matrixmult.html Alea GPU has cuBlas integrated, see tutorial http://quantalea.com/static/app/tutorial/examples/cublas/index.html"
            ],
            [
                "The matrixMulCUBLAS project is a C++ project that ships with the CUDA SDK, https://developer.nvidia.com/cuda-downloads. This uses cuBLAS to get astonishingly quick matrix multiplication (139 GFlops) on my home laptop."
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "I'm using VS 2010 on a Windows 7 64x. I've created a static class Cuda with the following code namespace Network { public static class Cuda { static GPGPU gpu= CudafyHost.GetDevice(); static CudafyHost host = new CudafyHost(); static GPGPUBLAS blas = GPGPUBLAS.Create(gpu); public static void GEMM(int m, int n, int k, float alpha, float[] a, float[] b, float beta, float[] c,Cudafy.Maths.BLAS.Types.cublasOperation Op) { float[] d_a = gpu.Allocate(a); float[] d_b = gpu.Allocate(b); float[] d_c = gpu.Allocate(c); gpu.CopyToDevice(a, d_a); gpu.CopyToDevice(b, d_b); gpu.CopyToDevice(c, d_c); blas.GEMM(m, n, k, alpha, d_a, d_b, beta, d_c, Op); gpu.CopyFromDevice(d_c, c); gpu.Free(d_a); gpu.Free(d_b); gpu.Free(d_c); } } } And I successfully use this GEMM function, but when I try to test my code with the test like this `` namespace TestProject { [TestClass] public class MatrixTest { /// &lt;summary&gt; ///op_Multiply ///&lt;/summary&gt; [TestMethod()] public void op_MultiplyTest() { Matrix a = new Matrix(2,3); a.Data = new float[] { 1, 2, 3, 4, 5, 6 }; Matrix b = new Matrix (3,4); b.Data = new float[] { 1, 0, 2, 3, 1, 2, 0, 4, 2, 1, 1, 0, 3, 0, 1, 1 }; Matrix expected = new Matrix(2, 4); expected.Data = new float[] {11,14,16,22,22,28,4,6}; Matrix actual; actual = (a * b); Assert.AreEqual(expected, actual); } } } I get the following exception: System.TypeInitializationException Message=Type Initializer \"Cudafy.Host.CudafyHost\" threw an exception. Source=Cudafy.NET TypeName=Cudafy.Host.CudafyHost StackTrace: \u0432 Cudafy.Host.CudafyHost.GetDevice(eGPUType type, Int32 deviceId) \u0432 Network.Cuda..cctor() \u0432 C:\\Users\\Dan\\Documents\\Visual Studio 2010\\Projects\\Network\\Network\\Cuda.cs:\u0441\u0442\u0440\u043e\u043a\u0430 14 InnerException: System.InvalidOperationException Message=Category does not exist. Source=System StackTrace: \u0432 System.Diagnostics.PerformanceCounterLib.CounterExists(String machine, String category, String counter) \u0432 System.Diagnostics.PerformanceCounter.InitializeImpl() \u0432 System.Diagnostics.PerformanceCounter..ctor(String categoryName, String counterName, String instanceName, Boolean readOnly) \u0432 System.Diagnostics.PerformanceCounter..ctor(String categoryName, String counterName) \u0432 Cudafy.Host.EmulatedGPU..ctor(Int32 deviceId) \u0432 Cudafy.Host.CudafyHost.DoCreateDevice(eGPUType target, Int32 deviceId) \u0432 Cudafy.Host.CudafyHost.CreateDevice(eGPUType type, Int32 deviceId) \u0432 Cudafy.Host.CudafyHost.GetDevice(eGPUType type, Int32 deviceId) \u0432 Cudafy.Host.CudafyHost..cctor() InnerException: It seems to me that the problem is with attempt of IDE use cudafy DLL twice or init the static class twice... How can i solve it? =============== Edit =============== I tried to use any functions from cudafy dll in my class MatrixTest. Well, it works. But not all. For example, Cudafy.Host.CudaGPU.GetDeviceCount() returns zero, like there is no cuda capable devices found. And when i try to use smth like CudafyHost.GetDeviceCount(eGPUType.Cuda) or anything else from class CudafyHost, i get the above exception.",
        "answers": [
            [
                "The exception you see is due to a missing performance category on your system. This is apparently something that goes wrong occasionally with .NET. See this thread for more information. The next CUDAfy release will handle such an error more gracefully."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to compare lists of strings (huge amount of elements in each of lists). Could somebody help me to do it using cudafy? I guess that in that case I should use jagged arrays of char but I've got an CudafyCompileException - expression must have class type (tried this approach). It worked just for two strings (char[]). So any ideas how I can do that? Code sample for 2 strings: var km = CudafyTranslator.Cudafy(); _gpu = CudafyHost.GetDevice(); _gpu.LoadModule(km); var strFirst = \"Hello, world\"; var strSecond = \"Hi world\"; var devResult = _gpu.Allocate&lt;char&gt;(strFirst.Length); var first = strFirst.ToCharArray(); var second = strSecond.ToCharArray(); var result = new char[strFirst.Length]; var devFirst = _gpu.CopyToDevice(first); var devSecond = _gpu.CopyToDevice(second); _gpu.Launch(N, 1).CompareStrings(devFirst, devSecond, devResult); _gpu.CopyFromDevice(devResult, result); var hostStr = new string(result); Console.WriteLine(hostStr); And the method itself: [Cudafy] public static void CompareStrings(GThread thread, char[] c, char[] b, char[] result) { int tid = thread.blockIdx.x; if (tid &lt; c.Length) { if (c[tid] == b[tid]) { result[tid] = c[tid]; } } }",
        "answers": [
            [
                "instead c.Length pass the length as a parameter be careful if you useing Unicode character, that 2 bytes"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a list of randomly generated numbers, which contains 1900 numbers, and I want to obtain a sorted list of the top 190 numbers. I've written two versions of the partial sorting algorithm, 1st is a CPU version and 2nd is written so it can run on Cudafy.net. But there is a large difference in execution time between them, when run on the CPU, I was wondering if someone could shed some light on why, + is it possible to speed the 2nd version up further? Note: the 2nd algorithm is going to be run on a GPU so I can't use linq or anything which wouldn't run on C as I will be using cudafy.net to run the code. Unfortunately cudafy.net also doesn't support jagged arrays. Version 1: /// &lt;summary&gt; /// Sequentially runs through all the values in the array and identifies if /// the current number is less than the highest number in the sorted list. /// &lt;/summary&gt; /// &lt;param name=\"numbers\"&gt; Unsorted array of numbers.&lt;/param&gt; /// &lt;param name=\"sortedNumbers\"&gt; Array used to hold the partial list of sorted numbers.&lt;/param&gt; public static void NewSorter(int[] numbers, int[] sortedNumbers) { for (int i = 0; i &lt; numbers.Length; i++) { if (sortedNumbers[sortedNumbers.Length - 1] &gt; numbers[i]) { //Update numbers IdentifyPosition(sortedNumbers, numbers[i]); } } } /// &lt;summary&gt; /// Identifies the position the number should be placed in the partial list of sorted numbers. /// &lt;/summary&gt; /// &lt;param name=\"sortedNumbers\"&gt; Array used to hold the partial list of sorted numbers.&lt;/param&gt; /// &lt;param name=\"NewNumber\"&gt; Number to be inserted.&lt;/param&gt; static void IdentifyPosition(int[] sortedNumbers, int NewNumber) { for (int i = 0; i &lt; sortedNumbers.Length; i++) { if (NewNumber &lt; sortedNumbers[i]) { //Offset and add. ArrayShifter(sortedNumbers, i, NewNumber); break; } } } /// &lt;summary&gt; /// Moves all the elements to the right of a point up one and /// then places the new number in the specified point. /// &lt;/summary&gt; /// &lt;param name=\"SortedNumbers\"&gt; Array used to hold the partial list of sorted numbers.&lt;/param&gt; /// &lt;param name=\"position\"&gt; Position in the array where the new number should be place.&lt;/param&gt; /// &lt;param name=\"NewNumber\"&gt; Number to include in the array.&lt;/param&gt; static void ArrayShifter(int[] SortedNumbers, int position, int NewNumber) { for (int i = SortedNumbers.Length - 1; i &gt; position; i--) { SortedNumbers[i] = SortedNumbers[i - 1]; } SortedNumbers[position] = NewNumber; } The above version executed in ~ 0.65 milliseconds. Version 2: /// &lt;summary&gt; /// Sequentially runs through all the values in the array and identifies if /// the current number is less than the highest number in the sorted list. /// &lt;/summary&gt; /// &lt;param name=\"unsortedNumbers\"&gt; Unsorted numbers.&lt;/param&gt; /// &lt;param name=\"lookbackCount\"&gt; Length of the array.&lt;/param&gt; /// &lt;param name=\"sortedNumbers\"&gt; Array which will contain the partial list of sorted numbers.&lt;/param&gt; [Cudafy] public static void CudaSorter(GThread thread, long[,] unsortedNumbers, int[] lookbackCount, long[,] sortedNumbers) { int threadIndex = thread.threadIdx.x; int blockIndex = thread.blockIdx.x; int threadsPerBlock = thread.blockDim.x; int gpuThread = (threadIndex + (blockIndex * threadsPerBlock)); if (gpuThread &lt; 32) { int maxIndex = (lookbackCount[gpuThread] * 10) / 100; int maxLookback = lookbackCount[gpuThread]; for (int i = 0; i &lt; maxLookback; i++) { if (sortedNumbers[gpuThread, maxIndex] &gt; unsortedNumbers[gpuThread, i]) { //Update numbers IdentifyPosition2(sortedNumbers, unsortedNumbers[gpuThread, i], maxIndex, gpuThread); } } } } /// &lt;summary&gt; /// Identifies the position in the sortedNumbers array where the new number should be placed. /// &lt;/summary&gt; /// &lt;param name=\"sortedNumbers\"&gt; Sorted numbers.&lt;/param&gt; /// &lt;param name=\"newNumber\"&gt; Number to be included in the sorted array.&lt;/param&gt; /// &lt;param name=\"maxIndex\"&gt; length of sortedNumbers array. &lt;/param&gt; /// &lt;param name=\"gpuThread\"&gt; GPU thread index.&lt;/param&gt; [Cudafy(eCudafyType.Device)] public static void CudaIdentifyPosition(long[,] sortedNumbers, long newNumber, int maxIndex, int gpuThread) { for (int i = 0; i &lt; maxIndex; i++) { if (newNumber &lt; sortedNumbers[gpuThread, i]) { //Offset and add. ArrayShifter2(sortedNumbers, i, newNumber, maxIndex, gpuThread); break; } } } /// &lt;summary&gt; /// Shifts all the elements to the right of the specified position, 1 position /// to the right, and insert the new number in the specified position. /// &lt;/summary&gt; /// &lt;param name=\"sortedNumbers\"&gt; Sorted Numbers.&lt;/param&gt; /// &lt;param name=\"position\"&gt; Where the new number needs to be inserted.&lt;/param&gt; /// &lt;param name=\"newNumber\"&gt; New number to insert.&lt;/param&gt; /// &lt;param name=\"maxIndex\"&gt; Length of sortedNumbers array.&lt;/param&gt; /// &lt;param name=\"gpuThread\"&gt; GPU thread index.&lt;/param&gt; [Cudafy(eCudafyType.Device)] public static void CudaArrayShifter(long[,] sortedNumbers, int position, long newNumber, int maxIndex, int gpuThread) { for (int i = maxIndex - 1; i &gt; position; i--) { sortedNumbers[gpuThread, i] = sortedNumbers[gpuThread, i - 1]; } sortedNumbers[gpuThread, position] = newNumber; } The above executes in 2.8 milliseconds i.e. ~ 4x slower. I've already tried the following: Declared local variable for maxLookBack count and used that in the for loop =&gt; no improvement. Changed data types from long[,] to int[,] =&gt; 2.6 milliseconds (This isn't feasible as I need to use long.) Changed int[,] to int[] =&gt; 1.3 milliseconds (This isn't feasible either as I need to pass multiple arrays to the GPU to keep it occupied.) I was surprised how much this affected the time. EDIT: I modified the code due to Henk's comments. I now ran the GPU version on the GPU with unsortedNumbers[32,1900] vs a single thread on the CPU sorting 1 array. Even when I multiply the CPU time by 32, it's still considerably lower than the GPU's time.",
        "answers": [
            [
                "After being disgraced here, I decided to read a little about the task in order to understand what it was about. So, you need to select a subarray of minimum numbers from a large array, which then needs to be sorted. It\u2019s probably better not to come up with an option for the CPU: run through the array, select the lows and immediately insert them into the final array by shifting the elements. Clearly, sorting will occur during the selection. However, I can\u2019t imagine how you can sample in parallel! In addition, you need to use well-parallelizing sorting algorithms. Otherwise, if the task is solved sequentially, the graphics core will certainly lose in speed to the CPU core, whose frequency is higher and data access is faster! I think that merge sorting can help you with this. Only instead of fetching the lows and then sorting, try sorting everything right away! And then select N first or last elements. Unfortunately, I\u2019m not ready to edit your code right now. But I hope that was at least a little useful."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a large amount of data which i need to sort, several million array each with tens of thousand of values. What im wondering is the following: Is it better to implement a parallel sorting algorithm, on the GPU, and run it across all the arrays OR implement a single thread algorithm, like quicksort, and assign each thread, of the GPU, a different array. Obviously speed is the most important factor. For single thread sorting algorithm memory is a limiting factor. Ive already tried to implement a recursive quicksort but it doesnt seem to work for large amounts of data so im assuming there is a memory issue. Data type to be sorted is long, so i dont believe a radix sort would be possible due to the fact that it a binary representation of the numbers would be too long. Any pointers would be appreciated.",
        "answers": [
            [
                "Sorting is an operation that has received a lot of attention. Writing your own sort isn't advisable if you are interested in high performance. I would consider something like thrust, back40computing, moderngpu, or CUB for sorting on the GPU. Most of the above will be handling an array at a time, using the full GPU to sort an array. There are techniques within thrust to do a vectorized sort which can handle multiple arrays \"at once\", and CUB may also be an option for doing a \"per-thread\" sort (let's say, \"per thread block\"). Generally I would say the same thing about CPU sorting code. Don't write your own. EDIT: I guess one more comment. I would lean heavily towards the first approach you mention (i.e. not doing a sort per thread.) There are two related reasons for this: Most of the fast sorting work has been done along the lines of your first method, not the second. The GPU is generally better at being fast when the work is well adapted for SIMD or SIMT. This means we generally want each thread to be doing the same thing and minimizing branching and warp divergence. This is harder to achieve (I think) in the second case, where each thread appears to be following the same sequence but in fact data dependencies are causing \"algorithm divergence\". On the surface of it, you might wonder if the same criticism might be levelled at the first approach, but since these libraries I mention arer written by experts, they are aware of how best to utilize the SIMT architecture. The thrust \"vectorized sort\" and CUB approaches will allow multiple sorts to be done per operation, while still taking advantage of SIMT architecture."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "As the topic states, I cant get the debugger working. Below is the sequence of steps ive done. Note: I have Cuda 5.0 installed and NSight visual studio edition 3.0 installed. Ive heard that it is possible to debug now with a single GPU. I'm assuming 5.0 is OK and I don't need the 5.5 release candidate? 1) Changed code to include the following as per the instructions on the site: CudafyModes.Target = eGPUType.Cuda; CudafyModes.DeviceId = 0; CudafyTranslator.Language = eLanguage.Cuda; CudafyModule km = CudafyTranslator.Cudafy(eArchitecture.sm_20); //Included this line. CudafyTranslator.GenerateDebug = true; _gpu = CudafyHost.GetDevice(eGPUType.Cuda); _gpu.LoadModule(km); 2) Set a break point just after this. 3) Stopped debugging once break point was hit. 4) Solution Explorer, selected \"Show all files\" and found the \"CUDAFYSOURCETEMP.cu\" file. 5) Right clicked and selected \"Add to project\". 6) Open NSIGHT HUD Launcher 3.0. 7) Set setting as follows: 8) Clicked ok. 9) Double clicked CUDAFYSOURCETEMP.cu and set a break point in the code. 10) Went to the NSight Monitor and click \"NSight Monitor Ooption\" -&gt; CUDA and the set the following: 11) Went back to VS2010 and selected NSight-&gt; Start Cuda Debugging. once that's done, I hovered the cursor over variables, once the break point has been hit, but nothing shows up. Only ones which show something are blockDim, blockIdx and threadIdx. I've also opened the CUDA WarpWatch1 window, typed in a variable yet that's giving the following error \"Could not resolve name \"num2\". Am i missing a step or something? EDIT Here is the output from the Output window during compilation. The thread 'vshost.LoadReference' (0x1f78) has exited with code 0 (0x0). 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Users\\FrazMann\\Desktop\\Market Adj Entry CUDA - MK2\\FrazerMann.Profiler.UserInterface\\bin\\x64\\Debug\\FrazerMann.Profiler.UserInterface.exe', Symbols loaded. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Users\\FrazMann\\Desktop\\Market Adj Entry CUDA - MK2\\FrazerMann.Profiler.UserInterface\\bin\\x64\\Debug\\Cudafy.NET.dll' 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.Numerics\\v4.0_4.0.0.0__b77a5c561934e089\\System.Numerics.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_64\\System.Data.OracleClient\\v4.0_4.0.0.0__b77a5c561934e089\\System.Data.OracleClient.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_64\\System.Web\\v4.0_4.0.0.0__b03f5f7f11d50a3a\\System.Web.dll' 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_64\\System.Transactions\\v4.0_4.0.0.0__b77a5c561934e089\\System.Transactions.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_64\\System.EnterpriseServices\\v4.0_4.0.0.0__b03f5f7f11d50a3a\\System.EnterpriseServices.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_64\\System.EnterpriseServices\\v4.0_4.0.0.0__b03f5f7f11d50a3a\\System.EnterpriseServices.Wrapper.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'Anonymously Hosted DynamicMethods Assembly' 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\Accessibility\\v4.0_4.0.0.0__b03f5f7f11d50a3a\\Accessibility.dll' 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.Data.SqlXml\\v4.0_4.0.0.0__b77a5c561934e089\\System.Data.SqlXml.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.Xaml\\v4.0_4.0.0.0__b77a5c561934e089\\System.Xaml.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.ComponentModel.Composition\\v4.0_4.0.0.0__b77a5c561934e089\\System.ComponentModel.Composition.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\Microsoft.Build.Framework\\v4.0_4.0.0.0__b03f5f7f11d50a3a\\Microsoft.Build.Framework.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.Runtime.Caching\\v4.0_4.0.0.0__b03f5f7f11d50a3a\\System.Runtime.Caching.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.Web.ApplicationServices\\v4.0_4.0.0.0__31bf3856ad364e35\\System.Web.ApplicationServices.dll' 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.DirectoryServices.Protocols\\v4.0_4.0.0.0__b03f5f7f11d50a3a\\System.DirectoryServices.Protocols.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.DirectoryServices\\v4.0_4.0.0.0__b03f5f7f11d50a3a\\System.DirectoryServices.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled. FrazerMann.Profiler.UserInterface.TaskManager x threadIdx x blockIdx x blockDim Length Length x threadIdx x blockIdx x blockDim Length x threadIdx x blockIdx x blockDim GetLength x threadIdx x blockIdx x blockDim QuickSortOfValues1 x threadIdx x blockIdx x blockDim QuickSortOfValues1 QuickSortOfValues1 FrazerMann.Profiler.UserInterface.TaskManager/o__SiteContainer0 Compiler version: v5.0 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v5.0\\bin\\nvcc -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v5.0\\include\" -m64 -arch=sm_20 \"C:\\Users\\FrazMann\\Desktop\\Market Adj Entry CUDA - MK2\\FrazerMann.Profiler.UserInterface\\bin\\x64\\Debug\\CUDAFYSOURCETEMP.cu\" -o \"C:\\Users\\FrazMann\\Desktop\\Market Adj Entry CUDA - MK2\\FrazerMann.Profiler.UserInterface\\bin\\x64\\Debug\\CUDAFYSOURCETEMP.ptx\" --ptx CUDAFYSOURCETEMP.cu tmpxft_000010d0_00000000-5_CUDAFYSOURCETEMP.cudafe1.gpu tmpxft_000010d0_00000000-10_CUDAFYSOURCETEMP.cudafe2.gpu 'FrazerMann.Profiler.UserInterface.vshost.exe' (Managed (v4.0.30319)): Loaded 'C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.Dynamic\\v4.0_4.0.0.0__b03f5f7f11d50a3a\\System.Dynamic.dll', Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled.",
        "answers": [
            [
                "Unless you specify the -G switch to the nvcc compiler driver, there will be no symbols, and the debugger can't do what you want (identify or show specific variable values) without symbols. It seems like others have asked how to add the -G switch to the nvcc compilation phase within cudafy, and one approach seems to be discussed here."
            ],
            [
                "The mistake you're making is to put CudafyTranslator.GenerateDebug = true; after the CudafyTranslator.Cudafy call. If you do this, the translator has already generated the GPU binaries without the symbols, then you tell it symbols are needed. To fix this, simply put CudafyTranslator.GenerateDebug = true; before the CudafyTranslator.Cudafy call."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "Issue: Im trying to use my graphics card to do some computation using cudafy.net. Ive ran 2 versions of my kernel now and i keep getting an errors at specific intervals ie every 2nd location in the array is 0.0 but should be something much larger. Below is a table of what the GPU returns vs what the correct value is. Note: I've read that comparing floats isnt ideal but getting 0.0 when i should be getting something as large as 6.34419e17 seems wrong. I GPU Correct Value 16,777,217 0.0 6.34419E17 16,777,219 0.0 6.34419E17 ... ... ..... From quickly scanning through them, they seem to be occurring at every 2nd i value. Checked thus far: Ive also ran the below code at a different start value as i believed it may be an issue with the data but i still get the same i value for each error. Ive also changed the order in which the memory is allocated onto the GPU but that doesnt seem to affect the results. Note: since im debugging in VS, im not explicitly clearing the memory on the GPU after i stop. Is this being cleared once i stop debugging? The error is still present once i restart my pc. Graphics Card: My graphics card is as follows: EVGA GTX 660 SC. Code: My kernel: (Note: i have several variables which arent used below but i havent removed since i wanted to remove 1 thing at a time in order to nail down whats causing this error) [Cudafy] public static void WorkerKernelOnGPU(GThread thread, float[] value1, float[] value2, float[] value3, float[] dateTime, float[,] output) { float threadIndex = thread.threadIdx.x; float blockIndex = thread.blockIdx.x; float threadsPerBlock = thread.blockDim.x; int tickPosition = (int)(threadIndex + (blockIndex * threadsPerBlock)); //Check to ensure threads dont go out of range. if (tickPosition &lt; dateTime.Length) { output[tickPosition, 0] = dateTime[tickPosition]; output[tickPosition, 1] = -1; } } Below is the segment of code which im using to call the Kernel and then check the results. CudafyModule km = CudafyTranslator.Cudafy(); _gpu = CudafyHost.GetDevice(eGPUType.Cuda); _gpu.LoadModule(km); float[,] Output = new float[SDS.dateTime.Length,2]; float[] pm = new float[]{0.004f}; //Otherwise need to allocate then specify the pointer in the CopyToDevice so it know which pointer to add data to float[] dev_tpc = _gpu.CopyToDevice(pm); float[] dev_p = _gpu.CopyToDevice(SDS.p); float[] dev_s = _gpu.CopyToDevice(SDS.s); float[,] dev_o = _gpu.CopyToDevice(Output); float[] dev_dt = _gpu.CopyToDevice(SDS.dateTime); dim3 grid = new dim3(20000, 1, 1); dim3 block = new dim3(1024, 1, 1); Stopwatch sw = new Stopwatch(); sw.Start(); _gpu.Launch(grid, block).WorkerKernelOnGPU(dev_tpc,dev_p, dev_s, dev_dt, dev_o); _gpu.CopyFromDevice(dev_o, Output); sw.Stop(); //0.29 seconds string resultGPU = sw.Elapsed.ToString(); sw.Reset(); //Variables used to record errors. bool failed = false; float[,] wrongValues = new float[Output.Length, 3]; int counterError = 0; //Check the GPU values are as expected. If not record GPU value, Expected value, position. for (int i = 0; i &lt; 20480000; i++) { float gpuValue = Output[i, 0]; if (SDS.dateTime[i] == gpuValue) { } else { failed = true; wrongValues[counterError, 0] = gpuValue; wrongValues[counterError, 1] = SDS.dateTime[i]; wrongValues[counterError, 2] = (float)i; counterError++; } } I only have a single graphics card at my disposal atm so i cant quickly check to see if its an error with the card or not. The card is less then 8 months old and was new when bought. Any ideas on what could be causing the above error?? Thanks for your time. Edit: Just tried to reduce my gtx 660 to the stock speeds of a 660. Still experiencing the error though. Edit2 Ive used _gpu.FreeMemory; to determine if i was exceeding the cards memory. I still have 1,013,202,944 bytes left though. Edit3 Ive just changed the datatype of the output array to long instead of float. I now seem to have just over 500MB of free space on the card yet i still get the wrong results from the same value ie i = 16,777,217. I guess this seems to suggest it possible something to do with the index thats the issue??",
        "answers": [
            [
                "float threadIndex = thread.threadIdx.x; float blockIndex = thread.blockIdx.x; float threadsPerBlock = thread.blockDim.x; int tickPosition = (int)(threadIndex + (blockIndex * threadsPerBlock)); The issue was that fact i was using float for ThreadIndex etc. Once this was changed to int, the issue was resolved. Time for this fool to get some time away from the pc."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to run my 1st cudafy project but I'm getting the following error on the following line: CudafyModule km = CudafyTranslator.Cudafy(); Below is a screenshot of the exact error message I'm getting. I've run the example projects which come with Cudafy and they run fine so I don't understand why its saying it can't find the compiler. I've added the Cudafy.net to the references, see screenshot below, and all the info is exactly the same as the example projects, on the right hand side of the image, and that works. Anyone have any ideas?? EDIT It appears a similar issue has been answered on SO already but I don't understand where the path environment variable is that he is referring to. Similar SO question EDIT 2 Below are the following cl.exe I found. I'm not sure which one to use though. I'm running an Intel processor so I'm leaning towards the bottom one but that's just a guess.",
        "answers": [
            [
                "Glad to hear it worked out for you. Here's the comments in answer form... The linked SO answer is referring to the system PATH. In Windows 7 you can set it through: Right-click on the My Computer icon Choose Properties from the context menu Click the Advanced tab Click the Environment Variables button Click on the variable called 'Path' and then click on Edit and enter the path for cl.exe As for choosing the correct cl.exe, if you're on a non-itanium intel processor (i.e. x86) you'll want x86_amd64"
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I'm having some issues allocating a struct, which contains several arrays, to the GPU. In the 2nd code block im getting an error with: SimpleDataStructure[] dev_SDS = _gpu.CopyToDevice(SDS); Does anyone know why? From what i can see CopyToDevice() doesnt support a struct as an argument. I might be missing something though so would appreciate some assistance in any case. Struct declaration: [Cudafy] public struct SimpleDataStructure { public float[] AreaCode; public float[] Number; public SimpleDataStructure(int x) { AreaCode = new float[x]; Number = new float[x]; } } Code in a method in my class: Public class TaskManager { private static GPGPU _gpu; private SimpleDataStructure SDS; public void PreparationForTasks() { DataRetrieval(); SDS = new SimpleDataStructure(_entity.Data.Count - 1); CudafyModule km = CudafyTranslator.Cudafy(); _gpu = CudafyHost.GetDevice(eGPUType.Cuda); _gpu.LoadModule(km); //Loaded SimpleDataStructure into same module. km = CudafyTranslator.Cudafy(typeof(SimpleDataStructure)); _gpu.LoadModule(km, false); //Getting error on following line. SimpleDataStructure[] dev_SDS = _gpu.CopyToDevice(SDS); dim3 grid = new dim3(10, 10, 1); dim3 block = new dim3(8, 8, 1); _gpu.Launch(grid, block, \"WorkerKernelOnGPU\", dev_SDS); SimpleDataStructure result_SDS = new SimpleDataStructure(100); _gpu.CopyFromDevice(dev_SDS, result_SDS); } //..... }",
        "answers": [
            [
                "You can not have array references as members of your struct, that is not supported by cudafy. You can work around this by copying the arrays to the device yourself and store the device addresses as IntPtrs inside your struct. Alternatively if the size of the arrays is going to be fixed you could make your struct unsafe and use fixed-size arrays inside the struct."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I have a CUDA - related question for you :). Since I am relatively new to using CUDA I would like to know if this \"performance\" is ok, or not. I am using C# and Cudafy.Net! I have a grayscale image (represented as float[]) that I calculated from a screenshot (the size of the image is: 1920x1018 pixel). Now I use a Sobel filter running on the GPU (through Cudafy.Net) which looks like this: [Cudafy] public static void PenaltyKernel(GThread thread, Single[] data, Single[] res, Int32 width, Int32 height) { Single[] shared_data = thread.AllocateShared&lt;Single&gt;(\"shared_data\", BLOCK_WIDTH * BLOCK_WIDTH); ///Map from threadIdx/BlockIdx to Pixel Position int x = thread.threadIdx.x - FILTER_WIDTH + thread.blockIdx.x * TILE_WIDTH; int y = thread.threadIdx.y - FILTER_WIDTH + thread.blockIdx.y * TILE_WIDTH; shared_data[thread.threadIdx.x + thread.threadIdx.y * BLOCK_WIDTH] = data[x + y * width]; thread.SyncThreads(); if (thread.threadIdx.x &gt;= FILTER_WIDTH &amp;&amp; thread.threadIdx.x &lt; (BLOCK_WIDTH - FILTER_WIDTH) &amp;&amp; thread.threadIdx.y &gt;= FILTER_WIDTH &amp;&amp; thread.threadIdx.y &lt; (BLOCK_WIDTH - FILTER_WIDTH)) { ///Horizontal Filtering (detects horizontal Edges) Single diffHorizontal = 0; int idx = GetIndex(thread.threadIdx.x - 1, thread.threadIdx.y - 1, BLOCK_WIDTH); diffHorizontal -= shared_data[idx]; idx++; diffHorizontal -= 2 * shared_data[idx]; idx++; diffHorizontal -= shared_data[idx]; idx += 2*BLOCK_WIDTH; diffHorizontal += shared_data[idx]; idx++; diffHorizontal += 2 * shared_data[idx]; idx++; diffHorizontal += shared_data[idx]; ///Vertical Filtering (detects vertical Edges) Single diffVertical = 0; idx = GetIndex(thread.threadIdx.x - 1, thread.threadIdx.y - 1, BLOCK_WIDTH); diffVertical -= shared_data[idx]; idx += BLOCK_WIDTH; diffVertical -= 2 * shared_data[idx]; idx += BLOCK_WIDTH; diffVertical -= shared_data[idx]; idx = GetIndex(thread.threadIdx.x + 1, thread.threadIdx.y - 1, BLOCK_WIDTH); diffVertical += shared_data[idx]; idx += BLOCK_WIDTH; diffVertical += 2 * shared_data[idx]; idx += BLOCK_WIDTH; diffVertical += shared_data[idx]; ///Convert the \"edgyness\" for the Pixel and cut off at 1.0 Single diff = GMath.Min(1.0f, GMath.Sqrt(diffHorizontal * diffHorizontal + diffVertical * diffVertical)); ///Get the Array-Index idx = GetIndex(x, y, width); ///Set the Value res[x + y * width] = diff; } } Constant values Set before runtime: TILE_WIDTH = 16; FILTER_WIDTH = 1; BLOCK_WIDTH = TILE_WIDTH + 2 * FILTER_WIDTH; When I run this \"PenaltyKernel\" function, including the memory allocation for the arrays, copying of data to and from device, I come to an average of about 6.2ms runtime (using a GTX 680 GT!). So my question now is, if this speed is ok (that would make about 161frames per second) or if I am missing something? Is my Sobel filter even ok (I mean, the result looks good :) )? Any help is appreciated!",
        "answers": [
            [
                "I think that's an OK speed. A great deal of the time is from copying data between host and device (especially transfer from GPU to CPU is slow). A note on speed: In general, image processing on a GPU may be slower than on a CPU (I haven't tested your code so I don't know if this is true in your case) if the image is small. However, the larger the image, the faster it is to process on device than on host."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a question about drawing lines/paths on my own. I use a combination of C#/WPF/Cudafy for UI and some calculations (e.g. the paths). Now I have a Byte[] array that should be filled with 'colors'/values (array-length = 4 * width * height of the result image). I got some startpoints for the lines and one endpoint (somewhere between the startpoints). First I calculated some paths from those startpoints to the endpoints and then want to 'draw' them to the array (that will be used to construct a WriteableBitmap). The point coordinates are present in a 'reduced environment' though (since calculation of the paths needed to run a Dijkstra algorithm). My paths are now defined by Tuples holding the point-coordinates (reduced size) and a 'linewidth'. Since some paths may 'overlap' I thought I will do the following steps to ensure a nice looking of the result: Merge the paths: For that I will take one path and just keep it. Then I take the second and check if the path-points are somewhere near a path already added (like a near-neighbor search). I want to do this because in the end, I want to widen the line-width where paths overlap (3rd Tuple value). When finished, I want to 'interpolate' the paths: I don't really know how I should do that, since every path has a point at every (reduced-size) pixel. One possibility would be to clear out all those path-coortinates of the paths that 'lie on a line' (and are not really necessary) and then do something like a Bezier - Interpolation. But all these steps seem to be overkill to me. Don't you think there might be a better way to do this? If so, please share your thoughts :) Thank's for any help! Here's a link to an image of how it looks right now: CPVL Application",
        "answers": [],
        "votes": []
    },
    {
        "question": "If i have a kernel which looks back the last Xmins and calculates the average of all the values in a float[], would i experience a performance drop if all the threads are not executing the same line of code at the same time? eg: Say @ x=1500, there are 500 data points spanning the last 2hr period. @ x = 1510, there are 300 data points spanning the last 2hr period. the thread at x = 1500 will have to look back 500 places yet the thread at x = 1510 only looks back 300, so the later thread will move onto the next position before the 1st thread is finished. Is this typically an issue? EDIT: Example code. Sorry but its in C# as i was planning to use CUDAfy.net. Hopefully it provides a rough idea of the type of programming structures i need to run (Actual code is more complicated but similar structure). Any comments regarding whether this is suitable for a GPU / coprocessor or just a CPU would be appreciated. public void PopulateMeanArray(float[] data) { float lookFwdDistance = 108000000000f; float lookBkDistance = 12000000000f; int counter = thread.blockIdx.x * 1000; //Ensures unique position in data is written to (assuming i have less than 1000 entries). float numberOfTicksInLookBack = 0; float sum = 0; //Stores the sum of difference between two time ticks during x min look back. //Note:Time difference between each time tick is not consistent, therefore different value of numberOfTicksInLookBack at each position. //Thread 1 could be working here. for (float tickPosition = SDS.tick[thread.blockIdx.x]; SDS.tick[tickPosition] &lt; SDS.tick[(tickPosition + lookFwdDistance)]; tickPosition++) { sum = 0; numberOfTicksInLookBack = 0; //Thread 2 could be working here. Is this warp divergence? for(float pastPosition = tickPosition - 1; SDS.tick[pastPosition] &gt; (SDS.tick[tickPosition - lookBkDistance]); pastPosition--) { sum += SDS.tick[pastPosition] - SDS.tick[pastPosition + 1]; numberOfTicksInLookBack++; } data[counter] = sum/numberOfTicksInLookBack; counter++; } }",
        "answers": [
            [
                "CUDA runs threads in groups called warps. On all CUDA architectures that have been implemented so far (up to compute capability 3.5), the size of a warp is 32 threads. Only threads in different warps can truly be at different locations in the code. Within a warp, threads are always in the same location. Any threads that should not be executing the code in a given location are disabled as that code is executed. The disabled threads are then just taking up room in the warp and cause their corresponding processing cycles to be lost. In your algorithm, you get warp divergence because the exit condition in the inner loop is not satisfied at the same time for all the threads in the warp. The GPU must keep executing the inner loop until the exit condition is satisfied for ALL the threads in the warp. As more threads in a warp reach their exit condition, they are disabled by the machine and represent lost processing cycles. In some situations, the lost processing cycles may not impact performance, because disabled threads do not issue memory requests. This is the case if your algorithm is memory bound and the memory that would have been required by the disabled thread was not included in the read done by one of the other threads in the warp. In your case, though, the data is arranged in such a way that accesses are coalesced (which is a good thing), so you do end up losing performance in the disabled threads. Your algorithm is very simple and, as it stands, the algorithm does not fit that well on the GPU. However, I think the same calculation can be dramatically sped up on both the CPU and GPU with a different algorithm that uses an approach more like that used in parallel reductions. I have not considered how that might be done in a concrete way though. A simple thing to try, for a potentially dramatic increase in speed on the CPU, would be to alter your algorithm in such a way that the inner loop iterates forwards instead of backwards. This is because CPUs do cache prefetches. These only work when you iterate forwards through your data."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a method which i want to run on several threads but each thread will return a different number of results. Is it possible to declare a private, thread specific, variable ie a list which i can then pass back to the Host and merge all the results? Say i have an array as follows: int[,] arr1 = new int[3,3] {{ 3, 4, 5 }, {4, 5, 6}, {1, 6, 4}}; int[] arr2 = new int[] { 3, 4, 1 }; Each thread will be give 3 values to analyze and records the difference between the value in arr2 and the values for a specific row in arr1. [Cudafy] public static void CountAbove(GThread thread, int[] a, int[,] b, list&lt;int&gt; c) { int tid = thread.blockIdx.x; int threshold = a[tid]; for(int i = 0; i &lt; b.GetLength(0); i++) { if (threshold &lt; b[tid,i]) c.add(b[tid,i] - threshold); } }",
        "answers": [
            [
                "Yes it is possible. Declaring a local variable in a kernel is private to each thread that you launch. So simply declare a variable, use it, and when you want to store result in the host , copy it to global memory. You could give a location to global memory passing a pointer to it as argument to the kernel. Example: __global__ void kernel(float *var) { float localVar;//local to each thread in execution ... //Computation which uses localVar ... *var = localVar; } After you use cudaMemcpy() to get it in the host. This example also is valid if you declare a local array. In that case you just have to copy an array instead of a single variable. Edit#1: Example of passing an array as argument to a kernel: __global__ void kernel(float *arrayPtr, int length) { .... } arrayPtr is a devicePtr which should be allocated before calling the kernel function. length is the size of the array."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "A wave simulator I've been working on with C# + Cudafy (C# -&gt; CUDA or OpenCL translator) works great, except for the fact that running the OpenCL CPU version (Intel driver, 15\" MacBook Pro Retina i7 2.7GHz, GeForce 650M (Kepler, 384 cores)) is roughly four times as fast as the GPU version. (This happens whether I use the CL or CUDA GPU backend. The OpenCL GPU and CUDA versions perform nearly identically.) To clarify, for a sample problem: OpenCL CPU 1200 Hz OpenCL GPU 320 Hz CUDA GPU -~330 Hz I'm at a loss to explain why the CPU version would be faster than the GPU. In this case, the kernel code that's executing (in the CL case) on the CPU and GPU is identical. I select either the CPU or GPU device during initialization, but beyond that, everything is identical. Edit Here's the C# code that launches one of the kernels. (The others are very similar.) public override void UpdateEz(Source source, float Time, float ca, float cb) { var blockSize = new dim3(1); var gridSize = new dim3(_gpuEz.Field.GetLength(0),_gpuEz.Field.GetLength(1)); Gpu.Launch(gridSize, blockSize) .CudaUpdateEz( Time , ca , cb , source.Position.X , source.Position.Y , source.Value , _gpuHx.Field , _gpuHy.Field , _gpuEz.Field ); } And, here's the relevant CUDA kernel function generated by Cudafy: extern \"C\" __global__ void CudaUpdateEz(float time, float ca, float cb, int sourceX, int sourceY, float sourceValue, float* hx, int hxLen0, int hxLen1, float* hy, int hyLen0, int hyLen1, float* ez, int ezLen0, int ezLen1) { int x = blockIdx.x; int y = blockIdx.y; if (x &gt; 0 &amp;&amp; x &lt; ezLen0 - 1 &amp;&amp; y &gt; 0 &amp;&amp; y &lt; ezLen1 - 1) { ez[(x) * ezLen1 + ( y)] = ca * ez[(x) * ezLen1 + ( y)] + cb * (hy[(x) * hyLen1 + ( y)] - hy[(x - 1) * hyLen1 + ( y)]) - cb * (hx[(x) * hxLen1 + ( y)] - hx[(x) * hxLen1 + ( y - 1)]); } if (x == sourceX &amp;&amp; y == sourceY) { ez[(x) * ezLen1 + ( y)] += sourceValue; } } Just for completeness, here's the C# that is used to generate the CUDA: [Cudafy] public static void CudaUpdateEz( GThread thread , float time , float ca , float cb , int sourceX , int sourceY , float sourceValue , float[,] hx , float[,] hy , float[,] ez ) { var i = thread.blockIdx.x; var j = thread.blockIdx.y; if (i &gt; 0 &amp;&amp; i &lt; ez.GetLength(0) - 1 &amp;&amp; j &gt; 0 &amp;&amp; j &lt; ez.GetLength(1) - 1) ez[i, j] = ca * ez[i, j] + cb * (hy[i, j] - hy[i - 1, j]) - cb * (hx[i, j] - hx[i, j - 1]) ; if (i == sourceX &amp;&amp; j == sourceY) ez[i, j] += sourceValue; } Obviously, the if in this kernel is bad, but even the resulting pipeline stall shouldn't cause such an extreme performance delta. The only other thing that jumps out at me is that I'm using a lame grid/block allocation scheme - ie, the grid is the size of the array to be updated, and each block is one thread. I'm sure this has some impact on performance, but I can't see it causing it to be 1/4th of the speed of the CL code running on the CPU. ARGH!",
        "answers": [
            [
                "Answering this to get it off the unanswered list. The code posted indicates that the kernel launch is specifying a threadblock of 1 (active) thread. This is not the way to write fast GPU code, as it will leave most of the GPU capability idle. Typical threadblock sizes should be at least 128 threads per block, and higher is often better, in multiples of 32, up to the limit of 512 or 1024 per block, depending on GPU. The GPU \"likes\" to hide latency by having a lot of parallel work \"available\". Specifying more threads per block assists with this goal. (Having a reasonably large number of threadblocks in the grid may also help.) Furthermore the GPU executes threads in groups of 32. Specifying only 1 thread per block or a non-multiple of 32 will leave some idle execution slots, in every threadblock that gets executed. 1 thread per block is particularly bad."
            ]
        ],
        "votes": [
            8.0000001
        ]
    },
    {
        "question": "Using VS 2012, .NET 4.5, 64bit and CUDAfy 1.12 and I have the following proof of concept using System; using System.Runtime.InteropServices; using Cudafy; using Cudafy.Host; using Cudafy.Translator; namespace Test { [Cudafy(eCudafyType.Struct)] [StructLayout(LayoutKind.Sequential)] public struct ChildStruct { [MarshalAs(UnmanagedType.LPArray)] public float[] FArray; public long FArrayLength; } [Cudafy(eCudafyType.Struct)] [StructLayout(LayoutKind.Sequential)] public struct ParentStruct { public ChildStruct Child; } public class Program { [Cudafy] public static void KernelFunction(GThread gThread, ParentStruct parent) { long length = parent.Child.FArrayLength; } public static void Main(string[] args) { var module = CudafyTranslator.Cudafy( ePlatform.x64, eArchitecture.sm_35, new[] {typeof(ChildStruct), typeof(ParentStruct), typeof(Program)}); var dev = CudafyHost.GetDevice(); dev.LoadModule(module); float[] hostFloat = new float[10]; for (int i = 0; i &lt; hostFloat.Length; i++) { hostFloat[i] = i; } ParentStruct parent = new ParentStruct { Child = new ChildStruct { FArray = dev.Allocate(hostFloat), FArrayLength = hostFloat.Length } }; dev.Launch(1, 1, KernelFunction, parent); Console.ReadLine(); } } } When the program runs, I am getting the following error on the dev.Launch: Type 'Test.ParentStruct' cannot be marshaled as an unmanaged structure; no meaningful size or offset can be computed. If I remove the float array from the ChildStruct, it works as expected. Having worked in C/C++/Cli and CUDA C in the past, I am aware of the nature of the error. Some solutions to this error suggest setting the struct size manually using Size parameter of MarshalAs, but this is not possible due to the variety of types within the struct. I looked at the generated .cu file and it is generating the float array as a float * which is what I expected. Is there a way to pass an array within a struct to the Kernel? And if there isn't what is the best second alternative? This problem doesn't exist in CUDA C and it only exists because we are marshaling from CLR.",
        "answers": [
            [
                "I spent good time reading the source code of CUDAfy to see if there is a solution to this problem. CUDAfy is trying to make things too simple for .NET developers and shield them away from the IntPtr and other pointer concepts. However, the level of abstraction makes it very hard to think of an answer to this problem without a major refactor to the way this library works. Not being able to send a float array within a struct is a show stopper. I ended up doing PInvoke to the CUDA Runtime and not using CUDAfy."
            ],
            [
                "This is a limitation of .NET, not CUDAfy. Data must be blittable and a non-fixed size array is not. This is valid and based on the CUDAfy unit tests on codeplex: [Cudafy] [StructLayout(LayoutKind.Sequential, Size=64, CharSet = CharSet.Unicode)] public unsafe struct PrimitiveStruct { public fixed sbyte Message[32]; public fixed char MessageChars[16]; } There is also no reason to store the array length explicitly since you can use the Length property within device code."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I'm performing some array manipulation/calculation in CUDA (via the Cudafy.NET library, though I'm equally interested in CUDA/C++ methods), and need to calculate the minimum and maximum values that are in the array. One of the kernels looks like this: [Cudafy] public static void UpdateEz(GThread thread, float time, float ca, float cb, float[,] hx, float[,] hy, float[,] ez) { var i = thread.blockIdx.x; var j = thread.blockIdx.y; if (i &gt; 0 &amp;&amp; i &lt; ez.GetLength(0) - 1 &amp;&amp; j &gt; 0 &amp;&amp; j &lt; ez.GetLength(1) - 1) ez[i, j] = ca * ez[i, j] + cb * (hx[i, j] - hx[i - 1, j]) + cb * (hy[i, j - 1] - hy[i, j]) ; } I'd like to do something like this: [Cudafy] public static void UpdateEz(GThread thread, float time, float ca, float cb, float[,] hx, float[,] hy, float[,] ez, out float min, out float max) { var i = thread.blockIdx.x; var j = thread.blockIdx.y; min = float.MaxValue; max = float.MinValue; if (i &gt; 0 &amp;&amp; i &lt; ez.GetLength(0) - 1 &amp;&amp; j &gt; 0 &amp;&amp; j &lt; ez.GetLength(1) - 1) { ez[i, j] = ca * ez[i, j] + cb * (hx[i, j] - hx[i - 1, j]) + cb * (hy[i, j - 1] - hy[i, j]) ; min = Math.Min(ez[i, j], min); max = Math.Max(ez[i, j], max); } } Anyone knows of a convenient way to return the minimum and maximum values (for the entire array, not just per thread or block)?",
        "answers": [
            [
                "If you are writing an electromagnetic wave simulator and do not want to re-invent the wheel, you can use thrust::minmax_element. Below I'm reporting a simple example on how using it. Please, add your own CUDA error check. #include &lt;stdio.h&gt; #include &lt;cuda_runtime_api.h&gt; #include &lt;thrust\\pair.h&gt; #include &lt;thrust\\device_vector.h&gt; #include &lt;thrust\\extrema.h&gt; int main() { const int N = 5; const float h_a[N] = { 3., 21., -2., 4., 5. }; float *d_a; cudaMalloc(&amp;d_a, N * sizeof(float)); cudaMemcpy(d_a, h_a, N * sizeof(float), cudaMemcpyHostToDevice); float minel, maxel; thrust::pair&lt;thrust::device_ptr&lt;float&gt;, thrust::device_ptr&lt;float&gt;&gt; tuple; tuple = thrust::minmax_element(thrust::device_pointer_cast(d_a), thrust::device_pointer_cast(d_a) + N); minel = tuple.first[0]; maxel = tuple.second[0]; printf(\"minelement %f - maxelement %f\\n\", minel, maxel); return 0; }"
            ],
            [
                "Based on your comment to your question, you were trying to find the max and min values while calculating them; while it's possible, it's not the most efficient. If you're set on doing that, then you can have an atomic comparison against some global minimum and global maximum, with the downside that each thread will be serialized, which will likely be a significant bottleneck. For the more canonical approach to finding the maximum or minimum in an array via a reduction, you can do something along the lines of: #define MAX_NEG ... //some small number template &lt;typename T, int BLKSZ&gt; __global__ void cu_max_reduce(const T* d_data, const int d_len, T* max_val) { volatile __shared__ T smem[BLKSZ]; const int tid = threadIdx.x; const int bid = blockIdx.x; //starting index for each block to begin loading the input data into shared memory const int bid_sidx = bid*BLKSZ; //load the input data to smem, with padding if needed. each thread handles 2 elements #pragma unroll for (int i = 0; i &lt; 2; i++) { //get the index for the thread to load into shared memory const int tid_idx = 2*tid + i; const int ld_idx = bid_sidx + tid_idx; if(ld_idx &lt; (bid+1)*BLKSZ &amp;&amp; ld_idx &lt; d_len) smem[tid_idx] = d_data[ld_idx]; else smem[tid_idx] = MAX_NEG; __syncthreads(); } //run the reduction per-block for (unsigned int stride = BLKSZ/2; stride &gt; 0; stride &gt;&gt;= 1) { if(tid &lt; stride) { smem[tid] = ((smem[tid] &gt; smem[tid + stride]) ? smem[tid]:smem[tid + stride]); } __syncthreads(); } //write the per-block result out from shared memory to global memory max_val[bid] = smem[0]; } //assume we have d_data as a device pointer with our data, of length data_len template &lt;typename T&gt; __host__ T cu_find_max(const T* d_data, const int data_len) { //in your host code, invoke the kernel with something along the lines of: const int thread_per_block = 16; const int elem_per_thread = 2; const int BLKSZ = elem_per_thread*thread_per_block; //number of elements to process per block const int blocks_per_grid = ceil((float)data_len/(BLKSZ)); dim3 block_dim(thread_per_block, 1, 1); dim3 grid_dim(blocks_per_grid, 1, 1); T *d_max; cudaMalloc((void **)&amp;d_max, sizeof(T)*blocks_per_grid); cu_max_reduce &lt;T, BLKSZ&gt; &lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt; (d_data, data_len, d_max); //etc.... } This will find the per-block maximum value. You can run it again on its output (e.g. with d_max as the input data and with updated launch parameters) on 1 block to find the global maximum - running it in a multi-pass manner like this is necessary if your dataset is too large (in this case, above 2 * 4096 elements, since we have each thread process 2 elements, although you could just process more elements per thread to increase this). I should point out that this isn't particularly efficient (you'd want to use a more intelligent stride when loading the shared memory to avoid bank conflicts), and I'm not 100% sure it's correct (it worked on a few small testcases I tried), but I tried to write it for maximal clarity. Also don't forget to put in some error checking code to make sure your CUDA calls are completing successfully, I left them out here to keep it short(er). I should also direct you towards some more in-depth documentation; you can take a look at the CUDA sample reduction over at http://docs.nvidia.com/cuda/cuda-samples/index.html although it's not doing a min/max calculation, it's the same general idea (and more efficient). Also, if you're looking for simplicity, you might just want to use Thrust's functions thrust::max_element and thrust::min_element, and the documentation at: thrust.github.com/doc/group__extrema.html"
            ],
            [
                "You can develop your own min/max algorithm using a divide and conquer method. If you have the possibility to use npp, then this function may be useful : nppsMinMax_32f."
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I've added a reference to the CUDAfy.NET library via NuGet. &lt;package id=\"CUDAfy.NET\" version=\"1.12.4695.21111\" targetFramework=\"net45\" /&gt; When I run my program, I hit a Win32Exception: The system cannot find the file specified This happens on the first actual line of the program: CudafyModule km = CudafyTranslator.Cudafy(); There's no indication from the exception object as to what file they're attempting to load. How can I get past this problem? EDIT I see the same exception when running the bundled examples from the Codeplex download in VS2010 using .NET 4.0. The strack trace is: at System.Diagnostics.Process.StartWithCreateProcess(ProcessStartInfo startInfo) at Cudafy.CudafyModule.Compile(eGPUCompiler mode, Boolean deleteGeneratedCode) at Cudafy.Translator.CudafyTranslator.Cudafy(ePlatform platform, eArchitecture arch, Version cudaVersion, Boolean compile, Type[] types) at Cudafy.Translator.CudafyTranslator.Cudafy(ePlatform platform, eArchitecture arch, Type[] types) at Cudafy.Translator.CudafyTranslator.Cudafy()",
        "answers": [
            [
                "Setting VS to break on thrown exceptions shows the ProcessStartInfo object at the top of the stack in the locals pane of the debugger. The relevant properties are: FileName = nvcc Arguments = -m64 -arch=sm_12 \"c:\\&lt;path&gt;\\CUDAFYSOURCETEMP.cu\" -o \"c:\\&lt;path&gt;\\CUDAFYSOURCETEMP.ptx\" --ptx Some information from this article explains that the CUDA Toolkit must be installed. Fair enough. Ensure that the C++ compiler (cl.exe) is on the search path. This set-up of NVCC is actually the toughest stage of the whole process, so please persevere. Read any errors you get carefully - most likely they are related to not finding cl.exe or not having either 32-bit or 64-bit CUDA Toolkit. That article discusses version 4 of the toolkit, but version 5 is available now and supported since CUDAfy v1.1. Download from https://developer.nvidia.com/cuda-downloads Note that the 64-bit version of the CUDA Toolkit 5.0 is a 942 MB download. If you install everything you'll need an additional 2815 MB. The toolkit alone requires 928 MB. EDIT After installing the CUDA Toolkit 5.0, the program failed with a CudafyCompileException at the same source line: Compilation error: nvcc : fatal error : Cannot find compiler 'cl.exe' in PATH Searching my system drive: C:\\&gt;dir /s cl.exe This shows many different versions of the compiler/linker, both from VS 10.0 and 11.0. Apparently only cl.exe versions 9 and 10 are supported, so I opted for the VS10.0 amd64 version, I included the following in my PATH environment variable: C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin\\amd64 Your path may be different, depending upon your CPU. I recommend running the search to see your options. Note that you will have to restart VS after changing the PATH environment variable if you already have it open. After taking these steps, my basic program ran successfully."
            ],
            [
                "This may also happen if you had at some point installed CUDA Toolkit v7.5, but realized that the most recent version of CUDAfy supports CUDA 7.0. On uninstalling CUDA 7.5 from the control panel, some files/folders may still remain. You should delete these manually. You may use CUDAfyViewer to see which version of CUDA Toolkit is being accessed."
            ]
        ],
        "votes": [
            11.0000001,
            1.0000001
        ]
    }
]