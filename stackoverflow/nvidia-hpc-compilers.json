[
    {
        "question": "Description and source example Below are two simple, crude test programs that try to access out of bounds memory in cpu and gpu code. I put the gpu example separately, so one can test the cpu example with different compilers and examine their behavior. CPU example module sizes integer, save :: size1 integer, save :: size2 end module sizes module arrays real, allocatable, save :: testArray1(:, :) real, allocatable, save :: testArray2(:, :) end module arrays subroutine testMemoryAccess use sizes use arrays implicit none real :: value value = testArray1(size1+1, size2+1) print *, 'value', value end subroutine testMemoryAccess Program testMemoryAccessOutOfBounds use sizes use arrays implicit none ! set sizes for the example size1 = 5000 size2 = 2500 allocate (testArray1(size1, size2)) allocate (testArray2(size2, size1)) testArray1 = 1.d0 testArray2 = 2.d0 call testMemoryAccess end program testMemoryAccessOutOfBounds GPU example module sizes integer, save :: size1 integer, save :: size2 end module sizes module sizesCuda integer, device, save :: size1 integer, device, save :: size2 end module sizesCuda module arrays real, allocatable, save :: testArray1(:, :) real, allocatable, save :: testArray2(:, :) end module arrays module arraysCuda real, allocatable, device, save :: testArray1(:, :) real, allocatable, device, save :: testArray2(:, :) end module arraysCuda module cudaKernels use cudafor use sizesCuda use arraysCuda contains attributes(global) Subroutine testMemoryAccessCuda implicit none integer :: element real :: value element = (blockIdx%x - 1)*blockDim%x + threadIdx%x if (element.eq.1) then value = testArray1(size1+1, size2+1) print *, 'value', value end if end Subroutine testMemoryAccessCuda end module cudaKernels Program testMemoryAccessOutOfBounds use cudafor use cudaKernels use sizes use sizesCuda, size1_d =&gt; size1, size2_d =&gt; size2 use arrays use arraysCuda, testArray1_d =&gt; testArray1, testArray2_d =&gt; testArray2 implicit none integer :: istat ! set sizes for the example size1 = 5000 size2 = 2500 size1_d = size1 size2_d = size2 allocate (testArray1_d(size1, size2)) allocate (testArray2_d(size2, size1)) testArray1_d = 1.d0 testArray2_d = 2.d0 call testMemoryAccessCuda&lt;&lt;&lt;64, 64&gt;&gt;&gt; istat = cudadevicesynchronize() end program testMemoryAccessOutOfBounds When using nvfortran and trying to debug the program, the compiler does not give any warnings for the out of bounds access. Taking a look at the available flags for out of bounds access, both -C and -Mbounds options seem to be doing just that. However, they do not seem to work as intended. When using ifort for the same thing, the compiler stops and prints the exact line that the out of bounds access was encountered. How can I accomplish this using nvfortran? I though it was a CUDA specific problem, however as I was creating the examples to create this question here, I found out that nvfortran does the same thing on CPU code. Thus, it is not CUDA specific. Compilers used: nvfortran nvfortran 23.5-0 64-bit target on x86-64 Linux -tp zen2 NVIDIA Compilers and Tools Copyright (c) 2022, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. ifort ifort (IFORT) 2021.10.0 20230609 Copyright (C) 1985-2023 Intel Corporation. All rights reserved. Steps: nvfortran I compile the examples as follows: nvfortran -C -traceback -Mlarge_arrays -Mdclchk -cuda -gpu=cc86 testOutOfBounds.f90 nvfortran -C -traceback -Mlarge_arrays -Mdclchk -cuda -gpu=cc86 testOutOfBoundsCuda.f90 When running the cpu code, I get a non-initialized array value: value 1.5242136E-27 When running the gpu code, I get a zero value: value 0.000000 ifort I compile the cpu example as follows: ifort -init=snan -C -fpe0 -g -traceback testOutOfBounds.f90 and I get: forrtl: severe (408): fort: (2): Subscript #2 of the array TESTARRAY1 has value 2501 which is greater than the upper bound of 2500 Image PC Routine Line Source a.out 00000000004043D4 testmemoryaccess_ 23 testOutOfBounds.f90 a.out 0000000000404FD6 MAIN__ 43 testOutOfBounds.f90 a.out 000000000040418D Unknown Unknown Unknown libc.so.6 00007F65A9229D90 Unknown Unknown Unknown libc.so.6 00007F65A9229E40 __libc_start_main Unknown Unknown a.out 00000000004040A5 Unknown Unknown Unknown which is actually what I expect the compiler to print.",
        "answers": [
            [
                "Bounds checking isn't support by nvfortran in device code and, as the following warning indicates, is disabled when using GPU related flags: % nvfortran -C -g -traceback -Mlarge_arrays -Mdclchk -cuda -gpu=cc86 test_bounds1.f90 nvfortran-Warning-CUDA Fortran or OpenACC GPU targets disables -Mbounds The out-of-bounds error is found for CPU targets: % nvfortran -C -g -traceback -Mlarge_arrays -Mdclchk test_bounds1.f90; a.out 0: Subscript out of range for array testarray1 (test_bounds1.f90: 23) subscript=5001, lower bound=1, upper bound=5000, dimension=1"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Does anyone know what the maximum number of dimensions is for the Nvidia HPC compiler for Fortran? It used to be 7, but a thread in 2019 said they were working on adding it. https://forums.developer.nvidia.com/t/more-than-seven-dimensions-specified-for-array/136072",
        "answers": [
            [
                "Turns out it is still currently 7 dimensions. Fingers crossed they will add support for more in the future. https://docs.nvidia.com/hpc-sdk/compilers/cuda-fortran-prog-guide/#cfpg-var-qual-attr-constant"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Is there a straightforward way to generate random numbers inside an OpenACC parallel loop? I want the seed within each loop/thread to be different. The reference code that demonstrates the required task to be done is as follows: PROGRAM genRandNums implicit none integer :: i double precision :: a(100) !$acc parallel loop DO i = 1,100 call random_seed() call random_number(a(i)) ENDDO END PROGRAM This code works on the gnu's 'gfortran' but throws an error with the 'nvfortran' compiler as the intersic functions random_seed() and random_number() are not supported. How can I modify this code to work with the nvfortran compiler?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am accelerating a Fortran code with OpenACC. When I profile the program with NVIDIA Nsight, I noticed the first call of a kernel with a copyout clause exhibited a long call to cuMemToHostAlloc. Here is a trivial example illustrating this. The program launches successively 10 times a kernel that computes an array test and returns its value: program test implicit none real, allocatable :: test(:) integer :: i, j, n, m n = 1000 m = 10 allocate(test(n)) do j = 1, m !$acc kernels copyout(test) !$acc loop independent do i = 1, n test(i) = real(i) end do !$acc end kernels end do deallocate(test) end program test The code is compiled with NVHPC 22.7, using no optimization flag (adding such flags did not have any influence). The profiling of the code gives: Compared to the actual memory transfer time, as seen for the 9 other calls, the call to cuMemToHostAlloc is ridiculously long. If I remove the copyout clause, the call to cuMemToHostAlloc disappears, so this is related to copying back data from the device, but I do not understand why it only happens once and for so long. Also, the test array is already allocated on the host memory. Am I missing something?",
        "answers": [
            [
                "It's the call to create the pinned memory buffers used to transfer the data between the host and device. DMA transfer must use non-swappable, i.e. pinned, memory. We use a double buffering system where as one buffer is being filled with the virtual memory, the second buffer is transferred asynchronously to the device. Effectively hiding much of the virtual to pinned memory copy. The host pinned memory allocation is relatively expensive but only occurs once when the runtime first encounters a data region so the cost will be amortized. Note by removing the copyout, you're removing the need to transfer the data and hence no need for the buffers."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am tring to parallelize a do loop in Fortran. Using OMP parallel do (and converted to standard do loop) it works nicely (using both gfortran and nvfortran), but when compiling it with nvfortran -stdpar=gpu it compiles, but running it, it crashes with: 0 Current file: xxx/pi.f90 function: pi line: 15 This file was compiled: -acc=gpu -gpu=cc35 -gpu=cc50 -gpu=cc60 -gpu=cc60 -gpu=cc70 -gpu=cc75 -gpu=cc80 - Here is the code: program pi implicit none integer :: count, n, i real :: r real, dimension(10000) :: x,y logical , dimension(10000) :: c c = .false. n=size(x,1) print*,count(c) call RANDOM_SEED call random_number(x) call random_number(y) do concurrent (i = 1: n) if (x(i)**2 + y(i)**2 &lt;1.0) c(i)=.true. end do r = 4 * real(count(c))/n print *, r end program pi",
        "answers": [],
        "votes": []
    },
    {
        "question": "Update: NVIDIA responded to my bug report (link is at bottom of this description) stating that they are able to reproduce the problem locally, and will have people look into it. This is a complex problem which I've tried to abbreviate to bare essentials. I think it would be easier for interested parties to reference a Google Doc I wrote, Report on NVIDIA HPC pgfortran / nvfortran modules memory problem (abbreviated version), and inside this document is a link to a more thorough document. I will try to replicate the \"abbreviated description\" here I have a sample code consisting of a small module and a main program in single source file. The purpose of the main program is to read the contents of a namelist file into variables. The variables (except for one) are declared in the module, which also declares a large static array As written (see below) the program seg faults at the READ statement, but if I move the declaration of namelist variables BEFORE the array declaration, it reads everything fine Or, if I make the static array smaller, everything works fine The problem shows up when using the NVIDIA HPC SDK Fortran compilers. I don\u2019t run into the problem when compiling with gfortran or ifort. ! Test module defining 3D dimensions, declaring 3D arrays, ! and variables to be filled from a namelist. MODULE my_mod IMPLICIT NONE INTEGER, PARAMETER :: nxmax=2881, nymax=1441, nzmax=138 !!!INTEGER, PARAMETER :: nxmax=361, nymax=181, nzmax=138 ! 3d fields !********** real :: a(nxmax, nymax, nzmax) ! Declaring these namelist values here results in seg fault ! upon reading the namelist. If I move these above the declaration ! of 3D array, everything works fine integer :: numxgrid,numygrid real :: dxout,dyout,outlon0,outlat0 END MODULE my_mod !======================================================= PROGRAM modmemtest USE my_mod IMPLICIT NONE REAL :: outheights NAMELIST /outgrid/ &amp; &amp; outlon0, outlat0, &amp; &amp; numxgrid, numygrid, &amp; &amp; dxout, dyout, &amp; &amp; outheights OPEN(48, FILE=\"OUTGRID\", status='old', form='formatted') PRINT *, 'BEFORE READING NAMELIST' READ(48, outgrid) PRINT *, 'AFTER READING NAMELIST' CLOSE(48) PRINT *, 'outlon0, outlat0: ', outlon0, outlat0 PRINT *, 'numxgrid, numygrid: ', numxgrid, numygrid PRINT *, 'dxout, dyout: ', dxout, dyout PRINT *, 'outheights: ', outheights END PROGRAM modmemtest Correct behaviour (moving namelist variable declarations before the array declaration, or declaring smaller array) looks like this $ pgfortran -o minitest -mcmodel=medium MINITEST.f90 $ ./minitest BEFORE READING NAMELIST AFTER READING NAMELIST outlon0, outlat0: 12.00000 47.00000 numxgrid, numygrid: 4 3 dxout, dyout: 1.000000 1.000000 outheights: 5000.000 Incorrect behaviour looks like this $ pgfortran -o minitest -mcmodel=medium MINITEST.f90 $ ./minitest BEFORE READING NAMELIST Segmentation fault This happens on two different systems with two different versions of the HPC SDK - RHEL 7 with NVIDIA HPC SDK v21.9, and Ubuntu 20.04 (an AWS instance) with NVIDIA HPC SDK v22.1. Again, there's a link at the top of this description to a Google Doc (web-shared) that's roughly what I wrote above, but in it is a link to a document with much more information. Related to all of this, I'm wondering if anybody is able to comment on the \"maturity\" of these new \"PGI\" Fortran compilers. I used to use it extensively in the old days, but the impression I'm getting when looking through problem reports (this isn't my first problem) is that NVIDIA may be struggling to bring its product back up to the standards of the original PGI compilers. One thing that comes to mind is the lack of support for -mcmodel=large, and it seems I've seen some comments from NVIDIA that some of these features are not \"yet\" incorporated in the new product? Added note - I submitted a bug report to NVIDIA, but somehow my nicely spaced text got screwed up in it. It's at - https://developer.nvidia.com/nvidia_bug/3721835",
        "answers": [],
        "votes": []
    },
    {
        "question": "In a computational fluid dynamics solver I have written, I am getting a different result based on the compiler I use. When I compile this code using gfortran with the following flags: -ffree-line-length-512 -Ofast -march=native, I get the \"correct\" results. However, when I run the same code with pgfortran using the flags -fast -acc -Minfo=accel -ta=tesla:managed, it seems like the boundaries of my domain become incorrect. To test if this could be a compiler issue, I commented out all OpenACC directives and compiled the code using the abovementioned pgfortran and flags. I still got the same incorrect results. I then removed all flags and compiled the code only using pgfortran with no flags. I still got the same incorrect results. I am not sure what the issue is or even if this really is a compiler issue. However, with exactly the same code, just with different compilers I am getting considerably different results at my domain boundaries and I am not sure what to do. Appreciate any advice and if there is any other information I can provide I would be happy to do so. UPDATE: I figured out what the issue was. I just want to be clear that when I was testing compiler vs. compiler I was not using any flags for either to make sure that it would not be an optimization issue or something else. Essentially, the error came from my boundary conditions, for which I use ghost cells. In the isothermal wall boundary condition, I was computing the temperature, setting a zero gradient condition for the pressure, and then computing the density from temperature and pressure. These boundary conditions were being set as: T_g = 2*T_b - T_i p_g = p_i rho_g = gamma*p_g/T_g where the subscripts g, b, i denote ghost, boundary, and interior cell, respectively. In a sequential manner, this produces the intended result, since p_g and T_g are set before rho_g and thus I may compute rho_g from them. I guess somehow this was not being done sequentially when running pgfortran so p_g and T_g were not set before computing density. I have no idea how this makes sense since without the -acc flag the code should just run sequentially. However, modifying the density boundary condition to be: rho_g = gamma*p_i/T_i fixed the issue.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to learn how to perform inter-gpu data communication using the following toy code. The task of the program is to send array 'a' data in gpu-0 in to gpu-1's memory. I took the following root to do so, which involved four steps: After initializing array 'a' on gpu0, step1: send data from gpu0 to cpu0 (using !acc update self() clause) step2: send data from cpu0 to cpu1 (using MPI_SEND()) step3: receive data into cpu1 from cpu0 (using MPI_RECV()) step4: update gpu1 device memory (using !$acc update device() clause) This works perfectly fine, but this looks like a very long route and I think there is a better way of doing this. I tried to read up on !$acc host_data use_device clause suggested in the following post, but not able to implement it: Getting started with OpenACC + MPI Fortran program I would like to know how !$acc host_data use_device can be used, to perform the task shown below in an efficient manner. PROGRAM TOY_MPI_OpenACC implicit none include 'mpif.h' integer :: rank, nprocs, ierr, i, dest_rank, tag, from integer :: status(MPI_STATUS_SIZE) integer, parameter :: N = 10000 double precision, dimension(N) :: a call MPI_INIT(ierr) call MPI_COMM_RANK(MPI_COMM_WORLD,rank,ierr) call MPI_COMM_SIZE(MPI_COMM_WORLD,nprocs,ierr) print*, 'Process ', rank, ' of', nprocs, ' is alive' !$acc data create(a) ! initialize 'a' on gpu0 (not cpu0) IF (rank == 0) THEN !$acc parallel loop default(present) DO i = 1,N a(i) = 1 ENDDO ENDIF ! step1: send data from gpu0 to cpu0 !$acc update self(a) print*, 'a in rank', rank, ' before communication is ', a(N/2) IF (rank == 0) THEN ! step2: send from cpu0 dest_rank = 1; tag = 1999 call MPI_SEND(a, N, MPI_DOUBLE_PRECISION, dest_rank, tag, MPI_COMM_WORLD, ierr) ELSEIF (rank == 1) THEN ! step3: recieve into cpu1 from = MPI_ANY_SOURCE; tag = MPI_ANY_TAG; call MPI_RECV(a, N, MPI_DOUBLE_PRECISION, from, tag, MPI_COMM_WORLD, status, ierr) ! step4: send data in to gpu1 from cpu1 !$acc update device(a) ENDIF call MPI_BARRIER(MPI_COMM_WORLD, ierr) print*, 'a in rank', rank, ' after communication is ', a(N/2) !$acc end data call MPI_BARRIER(MPI_COMM_WORLD, ierr) END compilation: mpif90 -acc -ta=tesla toycode.f90 (mpif90 from nvidia hpc-sdk 21.9) execution : mpirun -np 2 ./a.out",
        "answers": [
            [
                "Here's an example. Note that I also added some boiler-plate code to do the local node rank to device assignment. I also prefer to use unstructured data regions since they're better for more complex codes, but here they would be semantically equivalent to the structured data region that you used above. I have guarded the host_data constructs under a CUDA_AWARE_MPI macro since not all MPI have CUDA Aware support enabled. For these, you'd need to revert back to copying the data between the host and device before/after the MPI calls. % cat mpi_acc.F90 PROGRAM TOY_MPI_OpenACC use mpi #ifdef _OPENACC use openacc #endif implicit none integer :: rank, nprocs, ierr, i, dest_rank, tag, from integer :: status(MPI_STATUS_SIZE) integer, parameter :: N = 10000 double precision, dimension(N) :: a #ifdef _OPENACC integer :: dev, devNum, local_rank, local_comm integer(acc_device_kind) :: devtype #endif call MPI_INIT(ierr) call MPI_COMM_RANK(MPI_COMM_WORLD,rank,ierr) call MPI_COMM_SIZE(MPI_COMM_WORLD,nprocs,ierr) print*, 'Process ', rank, ' of', nprocs, ' is alive' #ifdef _OPENACC ! set the MPI rank to device mapping ! 1) Get the local node's rank number ! 2) Get the number of devices on the node ! 3) Round-Robin assignment of rank to device call MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, &amp; MPI_INFO_NULL, local_comm,ierr) call MPI_Comm_rank(local_comm, local_rank,ierr) devtype = acc_get_device_type() devNum = acc_get_num_devices(devtype) dev = mod(local_rank,devNum) call acc_set_device_num(dev, devtype) print*, \"Process \",rank,\" Using device \",dev #endif a = 0 !$acc enter data copyin(a) ! initialize 'a' on gpu0 (not cpu0) IF (rank == 0) THEN !$acc parallel loop default(present) DO i = 1,N a(i) = 1 ENDDO !$acc update self(a) ENDIF ! step1: send data from gpu0 to cpu0 print*, 'a in rank', rank, ' before communication is ', a(N/2) IF (rank == 0) THEN ! step2: send from cpu0 dest_rank = 1; tag = 1999 #ifdef CUDA_AWARE_MPI !$acc host_data use_device(a) #endif call MPI_SEND(a, N, MPI_DOUBLE_PRECISION, dest_rank, tag, MPI_COMM_WORLD, ierr) #ifdef CUDA_AWARE_MPI !$acc end host_data #endif ELSEIF (rank == 1) THEN ! step3: recieve into cpu1 from = MPI_ANY_SOURCE; tag = MPI_ANY_TAG; #ifdef CUDA_AWARE_MPI !$acc host_data use_device(a) #endif call MPI_RECV(a, N, MPI_DOUBLE_PRECISION, from, tag, MPI_COMM_WORLD, status, ierr) #ifdef CUDA_AWARE_MPI !$acc end host_data #else ! step4: send data in to gpu1 from cpu1 !$acc update device(a) #endif ENDIF call MPI_BARRIER(MPI_COMM_WORLD, ierr) !$acc update self(a) print*, 'a in rank', rank, ' after communication is ', a(N/2) !$acc exit data delete(a) call MPI_BARRIER(MPI_COMM_WORLD, ierr) END % which mpif90 /proj/nv/Linux_x86_64/21.9/comm_libs/mpi/bin//mpif90 % mpif90 -V nvfortran 21.9-0 64-bit target on x86-64 Linux -tp skylake NVIDIA Compilers and Tools Copyright (c) 2021, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. % mpif90 -acc -Minfo=accel mpi_acc.F90 toy_mpi_openacc: 38, Generating enter data copyin(a(:)) 42, Generating Tesla code 43, !$acc loop gang, vector(128) ! blockidx%x threadidx%x 42, Generating default present(a(:)) 46, Generating update self(a(:)) 76, Generating update device(a(:)) 82, Generating update self(a(:)) 85, Generating exit data delete(a(:)) % mpirun -np 2 ./a.out Process 1 of 2 is alive Process 0 of 2 is alive Process 0 Using device 0 Process 1 Using device 1 a in rank 1 before communication is 0.000000000000000 a in rank 0 before communication is 1.000000000000000 a in rank 0 after communication is 1.000000000000000 a in rank 1 after communication is 1.000000000000000 % mpif90 -acc -Minfo=accel mpi_acc.F90 -DCUDA_AWARE_MPI=1 toy_mpi_openacc: 38, Generating enter data copyin(a(:)) 42, Generating Tesla code 43, !$acc loop gang, vector(128) ! blockidx%x threadidx%x 42, Generating default present(a(:)) 46, Generating update self(a(:)) 82, Generating update self(a(:)) 85, Generating exit data delete(a(:)) % mpirun -np 2 ./a.out Process 0 of 2 is alive Process 1 of 2 is alive Process 1 Using device 1 Process 0 Using device 0 a in rank 1 before communication is 0.000000000000000 a in rank 0 before communication is 1.000000000000000 a in rank 1 after communication is 1.000000000000000 a in rank 0 after communication is 1.000000000000000"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a working serial code and a working parallel single GPU code parallelized via OpenACC. Now I am trying to increase the parallelism by running on multiple GPUs, employing mpi+openacc paradigm. I wrote my code in Fortran-90 and compile it using Nvidia's HPC-SDK's nvfortran compiler. I have a few beginner level questions: How do I setup my compiler environment to start writing my mpi+openacc code. Are there any extra requirements other than Nvidia's HPC-SDK? Assuming I have a code written under mpi+openacc setup, how do I compile it exactly? Do I have to compile it two times? one for cpus (mpif90) and one for gpus (openacc). An example of a make file or some compilation commands will be helpful. When the communication between GPU-device-1 and GPU-device-2 is needed, is there a way to communicate directly between them, or I should be communicating via [GPU-device-1] ---&gt; [CPU-host-1] ---&gt; [CPU-host-2] ---&gt; [GPU-device-2] Are there any sample Fortran codes with mpi+openacc implementation?",
        "answers": [
            [
                "As @Vladimir F pointed out, your question is very broad, so if you have further questions about specific points you should consider posting each point individually. That said, I'll try to answer each. If you install NVIDIA HPC SDK you should have everything you need. It'll include an installation of OpenMPI plus NVIDIA's HPC compilers for the OpenACC. You'll also have a variety of math libraries, if you need those too. Compile using mpif90 for everything. For instance, mpif90 -acc=gpu will build the files with OpenACC to include GPU support and files that don't include OpenACC will compile normally. The MPI module should be found automatically during compilation and the MPI libraries will be linked in. You can use the acc host_data use_device directive to pass the GPU version of your data to MPI. I don't have a Fortran example with MPI, but it looks similar to the call in this file. https://github.com/jefflarkin/openacc-interoperability/blob/master/openacc_cublas.f90#L19 This code uses both OpenACC and MPI, but doesn't use the host_data directive I referenced in 3. If I find another, I'll update this answer. It's a common pattern, but I don't have an open code handy at the moment. https://github.com/UK-MAC/CloverLeaf_OpenACC"
            ]
        ],
        "votes": [
            2.0000001
        ]
    }
]