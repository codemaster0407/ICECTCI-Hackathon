[
    {
        "question": "I'm currently trying to use a docker image for training of a generative adversarial network. Unfortunately, when I try to run the skript, I get the following error: [2023-07-29 11:02:47 @__init__.py:80] Saving logging to file: neuralgym_logs/20230729110247859123. # gpu pid type sm mem enc dec command # Idx # C/G % % % % name 0 20 G - - - - /Xwayland 0 22 G - - - - /Xwayland 0 31 G - - - - /Xwayland Traceback (most recent call last): File \"test.py\", line 23, in &lt;module&gt; ng.get_gpus(1) File \"/usr/local/lib/python3.5/dist-packages/neuralgym/utils/gpus.py\", line 70, in get_gpus ' [(gpu id: num of processes)]: {}'.format(sorted_gpus)) SystemError: No enough gpus for dedicated usage. [(gpu id: num of processes)]: [(0, 3)] nvidia-smi also shows 3 Xwayland processes using the gpu. I used the following base images in the Dockerfile: FROM tensorflow/tensorflow:1.7.0-gpu-py3 and FROM nvcr.io/nvidia/tensorflow:18.03-py3 both the same problem. Do I need to set a variable in the Dockerfile? I also don't understand, why Xwayland is even necessary, because I don't have a gui application running... Thanks in advance!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a hadoop/yarn multi-node cluster on Ubuntu 22.04 and I have added GPU resources to the cluster following the hadoop instructions here: https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/UsingGpus.html When I ran the command, \"yarn jar hadoop-yarn-applications-distributedshell.jar -jar hadoop-yarn-applications-distributedshell.jar -shell_command /usr/bin/nvidia-smi -container_resources memory-mb=3072,vcores=1,yarn.io/gpu=2 -num_containers 2\" it shows that the application was successful but there is not any nvidia-smi output. What could be causing this issue? Im expecting to get something like this after running the application in YARN: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375.66 Driver Version: 375.66 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 0000:04:00.0 Off | 0 | | N/A 30C P0 24W / 250W | 0MiB / 12193MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 Tesla P100-PCIE... Off | 0000:82:00.0 Off | 0 | | N/A 34C P0 25W / 250W | 0MiB / 12193MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have googled everywhere but I can't find this specific question. (please do not point general information of nvidia-smi) I want to use nvidia-smi dmon but pointing to specific GPUs In order to do that I want to use the -i option. In the help of the command it says Options include [-i|--id] Comma separated Enumeration index, PCI bus ID or UUID For reasons I dont want to use the index, but the UUID Now when you do nvida-smi -L you get GPU 0: NVIDIA GEForce RTX 3080 (UUID: GPU-239472947 etc) What is the UUID? Does it include the \"GPU-\" or just the numbers?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have installed two RTX A6000 gpu cards on my computer (Ubuntu 20.04). When I use the 'nvidia-smi' command, the output is bellow: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 525.105.17 Driver Version: 525.105.17 CUDA Version: 12.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA RTX A6000 Off | 00000000:51:00.0 Off | Off | | 30% 38C P8 21W / 300W | 6MiB / 49140MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA RTX A6000 Off | 00000000:8A:00.0 Off | 0 | | 30% 34C P8 10W / 300W | 5MiB / 46068MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1792 G /usr/lib/xorg/Xorg 4MiB | | 1 N/A N/A 1792 G /usr/lib/xorg/Xorg 4MiB | +-----------------------------------------------------------------------------+ My question is why the two cards have different sizes of GPU memories? Should the RTX A6000 have 48GB memory? but both of their memory are less than 48GB. Have I bought two gpu cards with poor qualities?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using a 2 node distributed setup(each having a single GPU respectively) to train a Neural Network (NN). I utilize PyTorch Data Distributed Parallel with Join Context Manager to achieve this. I am measuring power consumption varying data distribution on those two nodes. I am noticing more power consumption in the node which has a lesser part of the data, e.g.,for a scenario where node 1 training on 20% of the dataset and node2 is training on the rest 80% of data, I am getting higher power consumption in node1 once it finishes training its part. I know how Join Context manager works and it is intuitive why node1 is consuming more power. But there is nothing mentioned in documentation about power consumption. Is this PyTorch implementation-specific or this is how any synchronous training work (in any framework - PyTorch, Tensorflow etc.) ? I ran a round of experiments distributing training dataset unbalancedly. Let's say, at first, I experimented with 10%-90% data distribution between node1 and node2 respectively. Then I did experiment with 20%-80%, 30%-70%,..90%-10% data distribution between node1 and node2 respectively. Everytime I was getting more power consumtion in the node with lesser data. Actually more consumption is happening when training is done on the specific node and it join early. I used nvidia-smi to check power consumption in GPU and GPU communicatin is happening using NCCL. I followed this resource from PyTorch documentation: https://pytorch.org/tutorials/advanced/generic_join.html#:~:text=The%20context%20manager%20allows%20the,shadowed%20are%20specified%20by%20hooks.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to write a function that returns the topology of the underlying GPU devices as a graph. I want the connections to indicate where data transfer can occur, and the weights to be the throughput capacity of these connections. I know that nvidia-smi topo -m returns an adjacency matrix which the GPUs as vertices, but it doesn't show the throughput capacity of these connections. How would I go about getting this information? This is the output of the nvidia-smi command. GPU0 GPU1 GPU2 GPU3 GPU4 GPU5 CPU Affinity NUMA Affinity GPU0 X PIX PIX SYS SYS SYS 0-11 0 GPU1 PIX X PIX SYS SYS SYS 0-11 0 GPU2 PIX PIX X SYS SYS SYS 0-11 0 GPU3 SYS SYS SYS X PIX PIX 12-23 1 GPU4 SYS SYS SYS PIX X PIX 12-23 1 GPU5 SYS SYS SYS PIX PIX X 12-23 1 Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX = Connection traversing at most a single PCIe bridge NV# = Connection traversing a bonded set of # NVLinks",
        "answers": [],
        "votes": []
    },
    {
        "question": "when I try nvidia-smi I am getting this error: Failed to initialize NVML: DRiver/library version mismatch But when I try nvcc --version, getting this output: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Sun_Mar_21_19:15:46_PDT_2021 Cuda compilation tools, release 11.3, V11.3.58 Build cuda_11.3.r11.3/compiler.29745058_0 I test this lsmod | grep nvidia and output is this: nvidia_uvm 1200128 0 nvidia_drm 65536 14 nvidia_modeset 1200128 11 nvidia_drm nvidia 35483648 1148 nvidia_uvm,nvidia_modeset drm_kms_helper 307200 1 nvidia_drm drm 618496 18 drm_kms_helper,nvidia,nvidia_drm which nvidia-smi output: /usr/bin/nvidia-smi ps aux | grep nvidia-persistenced output: jasur 814285 0.0 0.0 18980 2772 pts/3 S+ 13:04 0:00 grep --color=auto nvidia-persistenced I am using ubuntu 2004 I test this lsmod | grep nvidia and output is this: nvidia_uvm 1200128 0 nvidia_drm 65536 14 nvidia_modeset 1200128 11 nvidia_drm nvidia 35483648 1148 nvidia_uvm,nvidia_modeset drm_kms_helper 307200 1 nvidia_drm drm 618496 18 drm_kms_helper,nvidia,nvidia_drm which nvidia-smi output: /usr/bin/nvidia-smi ps aux | grep nvidia-persistenced output: jasur 814285 0.0 0.0 18980 2772 pts/3 S+ 13:04 0:00 grep --color=auto nvidia-persistenced",
        "answers": [],
        "votes": []
    },
    {
        "question": "Geforce 4080 problem Hello Stack Overflow community, I am currently working on a project that requires the use of a GPU, and I am not sure whether it is being used or not. Could someone please guide me on how to check if my GPU is being used or how to activate it for use? I have already checked my GPU's status using the nvidia-smi command, but I am not sure how to interpret the output. Could someone explain to me how to read the nvidia-smi output or suggest another method for checking my GPU's usage status? Thank you in advance for your help.",
        "answers": [
            [
                "From your code you are working with pytorch, pytroch needs to specify which device to run on, either cuda or cpu. So you need to move everything dataset, model to cuda device using .to() function. device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") And then move everything to device using: model.to(device) and when you run your script, check nvidia-smi again and look in the bottom all the processes that are using the GPU will be reported."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm using the Container Optimized OS to run an application that takes advantage of GPUs. I have a separate system that creates VMs to run this application on-demand (to minimize cost) and I've been trying to reduce the time to get my application running. To do this, I've started using a custom VM image, which at the moment is just my application's docker container being pre-downloaded and saved to the COS image. I would also like to pre-install the Nvidia drivers for the GPU, but I can't seem to get it to stick. Despite installing the drivers, verifying they work, and then creating the image when I create a new VM using that image it's like the drivers weren't installed. The files appear to all be present though. I've tried running sudo cos-extensions install gpu In the startup script when creating the image, but the instances created from my image throw back an error when I try to run nvidia-smi nvidia-smi and nvidia mounting commands sudo mount --bind /var/lib/nvidia /var/lib/nvidia sudo mount -o remount,exec /var/lib/nvidia /var/lib/nvidia/bin/nvidia-smi Error: NVIDIA-SMI couldn't find libnvidia-ml.so library in your system. Please make sure that the NVIDIA Display Driver is properly installed and present in your system. Please also try adding directory that contains libnvidia-ml.so to your system PATH. Despite this complaint, the libnvidia-ml.so file DOES exist at: /var/lib/nvidia/lib64 The contents of my /var/lib/nvidia directory are: $ ls -lh /var/lib/nvidia/ total 354M -rw-r--r-- 1 root root 354M Mar 10 23:12 NVIDIA-Linux-x86_64-470.141.03_101-17162-40-42.cos drwxr-xr-x 2 root root 4.0K Mar 10 23:12 bin drwxr-xr-x 3 root root 4.0K Mar 10 23:12 bin-workdir drwxr-xr-x 2 root root 4.0K Mar 10 23:12 drivers drwxr-xr-x 3 root root 4.0K Mar 10 23:12 drivers-workdir drwxr-xr-x 3 root root 4.0K Mar 10 23:12 firmware drwxr-xr-x 4 root root 4.0K Mar 10 23:12 lib64 drwxr-xr-x 3 root root 4.0K Mar 10 23:12 lib64-workdir -rw-r--r-- 1 root root 2.2K Mar 10 23:12 nvidia-installer.log -rw-r--r-- 1 root root 1.2K Mar 10 23:12 pubkey.der drwxr-xr-x 3 root root 4.0K Mar 10 23:12 share Is there a way to create a custom image with the Nvidia driver's pre-installed that I can use?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a list of PIDs of processes running on different GPUs. I want to get the used GPU memory of each process based on its PID. nvidia-smi yields the information I want; however, I don't know how to grep it, as the output is sophisticated. I have already looked for how to do it, but I have not found any straightforward answers.",
        "answers": [
            [
                "While the default output of nvidia-smi is \"sophisticated\" or rather formatted for interfacing with humans rather than scripts, the command provides lots of options for use in scripts. The ones most fitting for use case seem to be --query-compute-apps=pid,used_memory specifying the information that you need and --format=csv,noheader,nounits specifying the minimal, machine readable output formatting. So the resulting command is nvidia-smi --query-compute-apps=pid,used_memory --format=csv,noheader,nounits I recommend taking a look at man nvidia-smi for further information and options."
            ],
            [
                "nvidia-smi --query-compute-apps=pid,used_memory,gpu_bus_id --format=csv gpu_bus_id will help you if you have multiple gpus"
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "I have an eGpu connected to a laptop with a mobile graphics card on a Ubuntu-based Linux system (Pop!_OS). My eGpu graphics card is not detected when running the $ nvidia-smi command as a regular user. However, my internal graphics card is detected. Surprisingly, when running the command with $ sudo nvidia-smi, my EGpu is now detected. If I run the command as a regular user again, the EGpu is now detected (till I restart the computer). Question 1: Why does my eGpu is detected only in privileged mode? Question 2: How can I get my graphic card detected without being a privileged user? Question 3 (optional): Why once detected in privileged mode, now my regular user detects it?",
        "answers": [],
        "votes": []
    },
    {
        "question": "On my machine Ubuntu 20.04.4 LTS\uff0c the command \"nvidia-smi\" suddenly does not work, it keeps showing static historical information. Some programs using GPU have stopped, but when running \"nvidia-smi\", we can still see the GPU usage. However, when we found the program through PID, it did have been killed. I run a new program using GPU. But when I ran the command \"nvidia-smi\", no GPU usage of the new program was shown. Following is the picture when I run \"nvidia-smi\" for the first time nvidia-smi result 1 Then the second time nvidia-smi result 2 They are the same, and nvidia-smi does not update any new information. When I run \"watch -n 1 nvidia-smi\" to dynamically see the GPU usage, it remains static and displays the wrong GPU usage. Further, when I ran the command \"nvidia-smi -h\", help information was not shown, It kept showing the static historical information shown in the above figures.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am checking the gpu memory usage in the training step. To start with the main question, checking the gpu memory using the torch.cuda.memory_allocated method is different from checking with nvidia-smi. And I want to know why. Actually, I measured the gpu usage using the vgg16 model. This code prints the theoretical feature map size and weight size: import torch import torch.nn as nn from functools import reduce Model_number = 7 Model_name = [\"alexnet\", \"vgg11_bn\", \"vgg16_bn\", \"resnet18\", \"resnet50\", \"googlenet\", \"vgg11\", \"vgg16\"] Model_weights = [\"AlexNet_Weights\", \"VGG11_BN_Weights\", \"VGG16_BN_Weights\", \"ResNet18_Weights\", \"ResNet50_Weights\", \"GoogLeNet_Weights\", \"VGG11_Weights\", \"VGG16_Weights\"] exec(f\"from torchvision.models import {Model_name[Model_number]}, {Model_weights[Model_number]}\") exec(f\"weights = {Model_weights[Model_number]}.DEFAULT\") exec(f\"model = {Model_name[Model_number]}(weights=None)\") weight_memory_allocate = 0 feature_map_allocate = 0 weight_type = 4 # float32 = 4, half = 2 batch_size = 128 input_channels = 3 input_size = [batch_size, 3, 224, 224] def check_model_info(m): global input_size global weight_memory_allocate, feature_map_allocate if isinstance(m, nn.Conv2d): in_channels, out_channels = m.in_channels, m.out_channels kernel_size, stride, padding = m.kernel_size[0], m.stride[0], m.padding[0] # weight weight_memory_allocate += in_channels * out_channels * kernel_size * kernel_size * weight_type # bias weight_memory_allocate += out_channels * weight_type # feature map feature_map_allocate += reduce(lambda a, b: a * b, input_size, 1) * weight_type out_len = int((input_size[2] + 2 * padding - kernel_size)/stride + 1) input_size = [batch_size, out_channels, out_len, out_len] elif isinstance(m, nn.Linear): input_size = [batch_size, reduce(lambda a, b: a * b, input_size[1:], 1)] in_nodes, out_nodes = m.in_features, m.out_features # weight weight_memory_allocate += in_nodes * out_nodes * weight_type # bias weight_memory_allocate += out_nodes * weight_type #feature map feature_map_allocate += reduce(lambda a, b: a * b, input_size, 1) * weight_type input_size = [batch_size, out_nodes] elif isinstance(m, nn.MaxPool2d): out_len = int((input_size[2] + 2 * m.padding - m.kernel_size)/m.stride + 1) input_size = [batch_size, input_size[1], out_len, out_len] model.apply(check_model_info) print(\"---------------------------------------------------------\") print(\"origial memory allocate\") print(f\"total = {(weight_memory_allocate + feature_map_allocate)/1024.0/1024.0:.2f}MB\") print(f\"weight = {weight_memory_allocate/1024.0/1024.0:.2f}MB\") print(f\"feature_map = {feature_map_allocate/1024.0/1024.0:.2f}MB\") print(\"---------------------------------------------------------\") Output: --------------------------------------------------------- origial memory allocate total = 4978.54MB weight = 527.79MB feature_map = 4450.75MB --------------------------------------------------------- And this code checks gpu usage with torch.cuda.memory_allocated: def test_memory_training(in_size=(3,224,224), out_size=1000, optimizer_type=torch.optim.SGD, batch_size=1, use_amp=False, device=0): sample_input = torch.randn(batch_size, *in_size, dtype=torch.float32) optimizer = optimizer_type(model.parameters(), lr=.001) model.to(device) print(f\"After model to device: {to_MB(torch.cuda.memory_allocated(device)):.2f}MB\") for i in range(5): optimizer.zero_grad() print(\"Iteration\", i) with torch.cuda.amp.autocast(enabled=use_amp): a = torch.cuda.memory_allocated(device) out = model(sample_input.to(device)).sum() # Taking the sum here just to get a scalar output b = torch.cuda.memory_allocated(device) print(f\"After forward pass {to_MB(torch.cuda.memory_allocated(device)):.2f}MB\") print(f\"Memory consumed by forward pass {to_MB(b - a):.2f}MB\") out.backward() print(f\"After backward pass {to_MB(torch.cuda.memory_allocated(device)):.2f}MB\") optimizer.step() print(f\"After optimizer step {to_MB(torch.cuda.memory_allocated(device)):.2f}MB\") print(\"---------------------------------------------------------\") def to_MB(a): return a/1024.0/1024.0 test_memory_training(batch_size=batch_size) Output: After model to device: 529.04MB Iteration 0 After forward pass 9481.04MB Memory consumed by forward pass 8952.00MB After backward pass 1057.21MB After optimizer step 1057.21MB --------------------------------------------------------- Iteration 1 After forward pass 10009.21MB Memory consumed by forward pass 8952.00MB After backward pass 1057.21MB After optimizer step 1057.21MB --------------------------------------------------------- ...... This is the result output by nvidia-smi when training: Here's a more detailed question: I think Pytorch store the following 3 things in the training step. model parameters input feature map in forward pass model gradient information for optimizer And I think in the forward pass, input feature map should be stored. But in theory, I thought 4450.75MB should be stored in memory, but actually 8952.00MB is stored. Almost 2 times difference. And if you check the memory usage using nvidia-smi and torch.cuda.memory_allocated, the memory usage using nvidia-smi shows about twice as much memory. what makes this difference? Thanks for reading the long question. Any help is appreciated.",
        "answers": [
            [
                "What is displayed in nvidia-smi is probably not the allocated memory, but the reserved memory. You can also read out the reserved memory using torch.cuda"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to create my own Overclocking Monitor for which I need to read information like the current voltage, clockspeeds and others. In C++ I can easily get the Information from Nvidia-smi with typing for example: console(\"nvidia-smi -q -i voltage\"); Which then displays me: ==============NVSMI LOG============== Timestamp : Tue Dec 13 17:55:54 2022 Driver Version : 526.47 CUDA Version : 12.0 Attached GPUs : 1 GPU 00000000:01:00.0 Voltage Graphics : 806.250 mV From that I need only the voltage number, in this case \"806.25\". I\u00b4ve investigated a bit into &lt;cctype&gt; which was something I\u00b4ve read about, but I\u00b4m not making any progress. So how can I import only that number into my c++ Program? I\u00b4d just guess that the process will be the same for the other commands.",
        "answers": [
            [
                "I don't currently have an Nvidia GPU to test this (stuck with Intel integrated graphics), so I can't import cuda.h but feel free to test this and let me know if it works or not. #include &lt;iostream&gt; #include &lt;chrono&gt; #include &lt;cuda.h&gt; int main() { // Get the current timestamp auto current_time = std::chrono::system_clock::now(); // Get the current driver version int driver_version; cudaDriverGetVersion(&amp;driver_version); // Get the current CUDA version int cuda_version; cudaRuntimeGetVersion(&amp;cuda_version); // Get the name of the attached GPU cudaDeviceProp device_properties; cudaGetDeviceProperties(&amp;device_properties, 0); std::string gpu_name = device_properties.name; // Get the current voltage int power_usage; cudaDeviceGetPowerUsage(&amp;power_usage, 0); int voltage = power_usage / current; // Output the overclocking data std::cout &lt;&lt; \"Timestamp: \" &lt;&lt; current_time &lt;&lt; std::endl; std::cout &lt;&lt; \"Driver version: \" &lt;&lt; driver_version &lt;&lt; std::endl; std::cout &lt;&lt; \"CUDA version: \" &lt;&lt; cuda_version &lt;&lt; std::endl; std::cout &lt;&lt; \"Attached GPU: \" &lt;&lt; gpu_name &lt;&lt; std::endl; std::cout &lt;&lt; \"Voltage: \" &lt;&lt; voltage &lt;&lt; std::endl; return 0; } If it works then your voltage can be accessed from int voltage."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "long long int n = 2000*2000*2000*2000; // overflow long long int n = pow(2000,4); // works long long int n = 16000000000000; // works Why does the first one overflow (multiplying integer literal constants to assign to a long long)? What's different about it vs. the second or third ones?",
        "answers": [
            [
                "Because 2000 is an int which is usually 32-bit. Just use 2000LL. Using LL suffix instead of ll was suggested by @AdrianMole in, now deleted, comment. Please check his answer. By default, integer literals are of the smallest type that can hold their value but not smaller than int. 2000 can easily be stored in an int since the Standard guarantees it is effectively at least a 16-bit type. Arithmetic operators are always called with the larger of the types present but not smaller than int: char*char will be promoted to operator*(int,int)-&gt;int char*int calls operator*(int,int)-&gt;int long*int calls operator*(long,long)-&gt;long int*int still calls operator*(int,int)-&gt;int. Crucially, the type is not dependent on whether the result can be stored in the inferred type. Which is exactly the problem happening in your case - multiplication is done with ints but the result overflows as it is still stored as int. C++ does not support inferring types based on their destination like Haskell does so the assignment is irrelevant."
            ],
            [
                "The constants (literals) on the RHS of your first line of code are int values (not long long int). Thus, the mulitplications are performed using int arithmetic, which will overflow. To fix this, make the constants long long using the LL suffix: long long int n = 2000LL * 2000LL * 2000LL * 2000LL; cppreference In fact, as noted in the comment by Peter Cordes, the LL suffix is only actually needed on either the first (leftmost) or second constant. This is because, when multiplying types of two different ranks, the operand of lower rank is promoted to the type of the higher rank, as described here: Implicit type conversion rules in C++ operators. Furthermore, as the * (multiplication) operator has left-to-right associativity, the 'promoted' result of the first multiplication propagates that promotion to the second and third. Thus, either of the following lines will also work without overflow: long long int n1 = 2000LL * 2000 * 2000 * 2000; long long int n2 = 2000 * 2000LL * 2000 * 2000; Note: Although lowercase suffixes (as in 2000ll) are valid C++, and entirely unambiguous to the compiler, there is a general consensus that the lowercase letter, 'ell', should be avoided in long and long long integer literals, as it can easily be mistaken, by human readers, for the digit, 1. Thus, you will notice that 2000LL (uppercase suffix) has been used throughout the answers here presented."
            ],
            [
                "2000*2000*2000*2000 is a multiplication of 4 int values, which returns an int value. When you assign this int value to long long int n the overflow already happend (if int is 32 bit the resulting value won't fit). You need to make sure that the overflow does not occur, so when you write long long int n = static_cast&lt;long long int&gt;(2000)*2000*2000*2000; you make sure that you are doing a long long int multiplication (long long int multiplied with int returns a long long int, so no overflow in your case). A shorter (and better way) is to write 2000LL or 2000ll instead of the static_cast. That gives the integer literal the right type. This is not needed for 2000 which fits into an int but it would be needed for higher values that don't fit into an int. long long int n = 2000LL*2000*2000*2000; long long int n = 2000LL*2000LL*2000LL*2000LL;"
            ],
            [
                "The other answers (as of this writing) appear to not have been explicit enough to answer the question as stated. I'll try to fill this gap. Why does the first one overflow (multiplying integer literal constants to assign to a long long)? The expression long long int n = 2000*2000*2000*2000; is evaluated as follows: long long int n = ((2000*2000)*2000)*2000; where the steps are (assuming 32-bit int): (2000*2000) is a multiplication of two int values that yields 4000000, another int value. ((2000*2000)*2000) is a multiplication of the above yielded int value 4000000 with an int value 2000. This would yield 8000000000 if the value could fit into an int. But our assumed 32-bit int can store a maximum value of 231-1=2147483647. So we get overflow right at this point. The next multiplication would happen if there hadn't been overflow above. The assignment of the resulting int product would happen (if not the overflow) to the long long variable, which would preserve the value. Since we did have overflow, the statement has undefined behavior, so steps 3 and 4 can't be guaranteed. What's different about it vs. the second or third ones? long long int n = pow(2000,4); The pow(2000,4) converts 2000 and 4 into double (see some docs on pow), and then the function implementation does its best to produce a good approximation of the result, as a double. Then the assignment converts this double value to long long. long long int n = 16000000000000; The literal 16000000000000 is too large to fit into an int, so its type is instead the next signed type that can fit the value. It could be long or long long, depending on the platform. See Integer literal#The type of the literal for details. then the assignment converts this value to long long (or just writes it, if the literal's type was long long already)."
            ],
            [
                "The first is a multiplication using integers (typically 32 bit). It overflows because those integers cannot store 2000^4. The result is then cast to long long int. The second calls the pow function which casts the first argument to double and returns a double. The result is then cast to long long int. There is no overflow in this case because the math is done on a double value."
            ],
            [
                "You might want to use the following in C++ to understand this: #include&lt;iostream&gt; #include&lt;cxxabi.h&gt; using namespace std; using namespace abi; int main () { int status; cout &lt;&lt; __cxa_demangle(typeid(2000*2000*2000*2000).name(),0,0,&amp;status); } As you can see, the type is int. In C, you can use (courtesy of): #include &lt;stdio.h&gt; #include &lt;stddef.h&gt; #include &lt;stdint.h&gt; #define typename(x) _Generic((x), /* Get the name of a type */ \\ \\ _Bool: \"_Bool\", unsigned char: \"unsigned char\", \\ char: \"char\", signed char: \"signed char\", \\ short int: \"short int\", unsigned short int: \"unsigned short int\", \\ int: \"int\", unsigned int: \"unsigned int\", \\ long int: \"long int\", unsigned long int: \"unsigned long int\", \\ long long int: \"long long int\", unsigned long long int: \"unsigned long long int\", \\ float: \"float\", double: \"double\", \\ long double: \"long double\", char *: \"pointer to char\", \\ void *: \"pointer to void\", int *: \"pointer to int\", \\ char(*)[]: \"pointer to char array\", default: \"other\") unsigned int a = 3; int main() { printf(\"%s\", typename(a-10)); return 0; } Here the type of the expression is unsigned int because the type mismatch implicitly upgrades the type to the largest type between unsigned int and int, which is unsigned int. The unsigned int will underflow to a large positive, which will be the expected negative when assigned to or interpreted as an int. The result of the calculation will always be unsigned int regardless of the values involved. C The minimum default type of an integer literal without a suffix is int, but only if the literal exceeds this, does its type becomes an unsigned int; if larger than that it is given a type of a long int, therefore 2000s are all ints. The type of an expression performed on a literal however, using unary or binary operators, uses the implicit type hierarchy to decide a type, not the value of the result (unlike the literal itself which uses the length of the literal in deciding the type), this is because C uses type coercion and not type synthesis. In order to solve this, you'd have to use long suffixes ul on the 2000s to explicitly specify the type of the literal. Similarly, the default type of a decimal literal is double, but this can be changed with a f suffix. Prefixes do not change the type of decimal or integer literals. The type of a string literal is char [], although it is really a const char [], and is just an address of the first character in the actual representation of that string literal in .rodata, and the address can be taken like any array using the unary ampersand &amp;\"string\", which is the same value (address) as \"string\", just a different type (char (*)[7] vs. char[7]; \"string\" i.e. char[] is not just (at compiler level) a pointer to the array, it is the array, whereas the unary ampersand extracts just the pointer to the array). The u prefix changes this to an array of char16_t, which is an unsigned short int; the U prefix changes it to an array of char32_t, which is an unsigned int; and the L prefix changes it to an array of wchar_t which is an int. u8 is a char and an unprefixed string uses implementation specific encoding, which is typically the same as u8 i.e. UTF-8, of which ASCII is a subset. A raw (R) prefix available only for string literals (and available only on GNU C (std=gnu99 onwards)) can be prefixed i.e. uR or u8R, but this does not influence the type. The type of a character literal is int unless prefixed with u (u'a' is unsigned short int) or U (U'a' is unsigned int). u8 and and L are both int when used on a character literal. An escape sequence in a string or character literal does not influence the encoding and hence the type, it's just a way of actually presenting the character to be encoded to the compiler. The type of a complex literal 10i+1 or 10j+1 is complex int, where both the real and the imaginary part can have a suffix, like 10Li+1, which in this case makes the imaginary part long and the overall type is complex long int, and upgrades the type of both the real and the imaginary part, so it doesn't matter where you put the suffix or whether you put it on both. A mismatch will always use the largest of the two suffixes as the overall type. Using an explicit cast instead of a literal suffix always results in the correct behaviour if you use it correctly and are aware of the semantic difference that it truncates/extends (sign extends for signed; zero extends for unsigned \u2013 this is based on the type of the literal or expression being cast and not the type that's being cast to, so a signed int is sign extended into an unsigned long int) a literal to an expression of that type, rather than the literal inherently having that type. C++ Again, the minimum default type is an int for the smallest literal base. The literal base i.e. the actual value of the literal, and the suffix influence the final literal type according to the following table where within each box for each suffix, the order of final type is listed from smallest to largest based on the size of the actual literal base. For each suffix, the final type of the literal can only be equal to or larger than the suffix type, and based on the size of the literal base. C exhibits the same behaviour. When larger than a long long int, depending on the compiler, __int128 is used. I think you could also create your own literal suffix operator i128 and return a value of that type. The default type of a decimal literal is the same as C. The type of a string literal is char []. The type of &amp;\"string\" is const char (*) [7] and the type of +\"string\" is const char * (in C you can only decay using \"string\"+0). C++ differs in that the latter 2 forms acquire a const but in C they don't. The string prefixes behave the same as in C Character and complex literals behave the same as C."
            ]
        ],
        "votes": [
            142.0000001,
            94.0000001,
            50.0000001,
            29.0000001,
            19.0000001,
            5.0000001
        ]
    },
    {
        "question": "I'm trying to figure out what is the minimum GPU requirement for my application. Using nvidia-smi as described here in Colab gives me a maximum value for memory.used of 4910MiB. So I presume a 4GB GPU is not enough, correct ? Also on this.. after the execution (so no process using GPU now) that value is still reported executing nvidia-smi, does this mean that the value reported for memory.used is intended as the maximum reached value (I would use this as lower limit for my requirements) or it is due to the pytorch caching policy?",
        "answers": [],
        "votes": []
    },
    {
        "question": "My PC !nvidia-smi result Mon Nov 7 19:55:40 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 512.77 Driver Version: 512.77 CUDA Version: 11.6 | |-------------------------------+----------------------+----------------------+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... WDDM | 00000000:01:00.0 On | N/A | | 34% 35C P8 16W / 125W | 979MiB / 6144MiB | 6% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 6816 C+G ...er Java\\jre\\bin\\javaw.exe N/A | | 0 N/A N/A 7268 C+G ...y\\ShellExperienceHost.exe N/A | | 0 N/A N/A 8428 C+G C:\\Windows\\explorer.exe N/A | | 0 N/A N/A 8756 C+G ...artMenuExperienceHost.exe N/A | | 0 N/A N/A 8960 C+G ...perience\\NVIDIA Share.exe N/A | | 0 N/A N/A 10384 C+G ...me\\Application\\chrome.exe N/A | | 0 N/A N/A 13824 C+G ...ge\\Application\\msedge.exe N/A | | 0 N/A N/A 16364 C+G ...n1h2txyewy\\SearchHost.exe N/A | | 0 N/A N/A 17304 C+G ...lPanel\\SystemSettings.exe N/A | | 0 N/A N/A 20740 C+G ...e\\PhoneExperienceHost.exe N/A | | 0 N/A N/A 25928 C+G ...\\app-1.0.9007\\Discord.exe N/A | | 0 N/A N/A 27748 C+G ...perience\\NVIDIA Share.exe N/A | | 0 N/A N/A 29456 C+G ...418.35\\msedgewebview2.exe N/A | +-----------------------------------------------------------------------------+ My Google Colab pro gpu_info = !nvidia-smi gpu_info = '\\n'.join(gpu_info) if gpu_info.find('failed') &gt;= 0: print('Not connected to a GPU') else: print(gpu_info) result Fri Nov 4 21:42:51 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.32.03 Driver Version: 460.32.03 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 A100-SXM4-40GB Off | 00000000:00:04.0 Off | 0 | | N/A 35C P0 53W / 350W | 0MiB / 40536MiB | 0% Default | | | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ How much time of different between My Google Colab computer power than My PC?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using Tensorflow 2.X. My GPU memory is 16125MiB, but my model requires 15595MiB, according to nvidia-smi With this total usage, I get an OOM after some time, even when setting the minimum batch size. I also tried the following, but as soon as more memory is required, I still go out of memory: config = tf.compat.v1.ConfigProto() #config.gpu_options.per_process_gpu_memory_fraction = 0.8 config.gpu_options.allow_growth = True sess = tf.compat.v1.Session(config=config) The total memory value should also include the (150MiB) from firefox, which I need to use once in a while, but I doubt killing it would make much difference. My dataset is loaded through individual .h5 files (one per data sample), the intermediate computation arrays/tensors are deleted when not used. Unfortunately, I cannot resize the model for several reasons due to an ongoing publication. Is there any trick I can use to gain that extra 500MiB, to train my model safely, without incurring into OOM half-way?",
        "answers": [],
        "votes": []
    },
    {
        "question": "OS:Ubuntu 20.04LTS Windows10 dual boot Error with nvidia-smi command after apt installation of nvidia driver. $ nvidia-smi Unable to determine the device handle for GPU 0000:0B:00.0: Not Found $ dmesg |grep NVRM [ 3.065144] NVRM: loading NVIDIA UNIX Open Kernel Module for x86_64 520.56.06 Release Build (dvs-builder@U16-T12-10-2) Thu Oct 6 21:33:54 UTC 2022 [ 5.299612] NVRM: Open nvidia.ko is only ready for use on Data Center GPUs. [ 5.299614] NVRM: To force use of Open nvidia.ko on other GPUs, see the [ 5.299615] NVRM: 'OpenRmEnableUnsupportedGpus' kernel module parameter described [ 5.299616] NVRM: in the README. [ 5.692026] NVRM: GPU 0000:0b:00.0: RmInitAdapter failed! (0x63:0x0:1900) [ 5.692585] NVRM: GPU 0000:0b:00.0: rm_init_adapter failed, device minor number 0 [ 19.458670] NVRM: GPU 0000:0b:00.0: RmInitAdapter failed! (0x63:0x0:1900) [ 19.459831] NVRM: GPU 0000:0b:00.0: rm_init_adapter failed, device minor number 0 ... $ dpkg -l | grep nvidia-driver ii nvidia-driver-520-open 520.56.06-0ubuntu0.20.04.1 amd64 NVIDIA driver (open kernel) metapackage I have tried reboot, secure boot and driver reinstallation.",
        "answers": [
            [
                "As per several answers given on the Nvidia developers forum, try installing the non-open version of the drivers. I also recommend holding (with apt) the version of the drivers to avoid future mismatches."
            ],
            [
                "I lowered the version of nvidia-driver and it solved the problem."
            ]
        ],
        "votes": [
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "Outside of the container, my user (UID=1000) is able to use nvidia-smi. However, inside the docker container, the non-root user (same UID of 1000) is unable to use nvidia-smi, running into Failed to initialize NVML: Insufficient Permissions. However, sudo nvidia-smi inside the container is able to use nvidia-smi. As far as I can search, this issue hasn't occurred before, any clues where to start chasing it down? Inside the container: On my host:",
        "answers": [],
        "votes": []
    },
    {
        "question": "In bash, nvidia-smi command gives you information about the GPU. We also have option to get this periodically such as nvidia-smi -lms 50 I want to get this info only as long as a particular process is running. Pseudocode nvidia-smi -lms 50 &amp; &gt; logfile.txt (time ./process1) &gt; timelog.txt while process1 is running: keep nvidia-smi running kill nvidia-smi How can I do this in bash, cleanly, such that once my bash script exits no process that starts here is left behind for me to clean? A direct nvidia-smi based solution would be preferred to a bash based one, but the latter is also perfectly fine.",
        "answers": [
            [
                "Run both in the background, then wait for the one your job depends on. nvidia-smi -lms 50 &gt; logfile.txt &amp; nvpid=$! time ./process1 &gt; timelog.txt &amp; prpid=$! wait \"$prpid\" kill \"$nvpid\""
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have 2 GPUs on different computers. One (NVIDIA A100) is on a server, the other (NVIDIA Quadro RTX 3000) is on my laptop. I watch the performance on both machines via nvidia-smi and noticed that the 2 GPUs use different amounts of memory when running the exact same processes (same code, same data, same CUDA version, same pytorch version, same drivers). I created a dummy script to verify this. import torch device = torch.device(\"cuda:0\") a = torch.ones((10000, 10000), dtype=float).to(device) In nvidia-smi I can see how much memory is used for this specific python script: A100: 1205 MiB RTX 3000: 1651 MiB However, when I query torch about memory usage I get the same values for both GPUs: reserved = torch.cuda.memory_reserved(0) allocated = torch.cuda.memory_allocated(0) Both systems report the same usage: reserved = 801112064 bytes (763 MiB) allocated = 800000000 bytes (764 MiB) I note that the allocated amount is much less than what I see used in nvidia-smi, though 763 MiB is equal to 100E6 float64 values. Why does nvidia-smi report different memory usage on these 2 systems?",
        "answers": [],
        "votes": []
    },
    {
        "question": "The title is ambiguous, so I add example. I usually run python files by bash file. Bash file name : train.sh python train1.py python train2.py Then I run nohup bash train.sh &gt; out.out &amp;. So, train1.py and train2.py are run sequentially. And when I found mistakes, I stopped using check PID using nvidia-smi and kill -9 PID . However, of cousre, only train1.py stopped and then start train2.py. What I want to do is. I want stop both train1.py and train2.py even if current running file is train1.py. In other words, I want stop running python file and files to be ran. And I have one more question. How can I get PIDs for running bash files? (Assuming that 2 bash files like above example are running, I want to get PIDs for each.) Thank You",
        "answers": [
            [
                "To get the PID of process (A program \"python script\" loaded into memory and executing is called Process) ps -ef | awk '$8==\"name_of_process\" {print $2}' To check that python a.py completed successfully as a required condition for running python b.py, you can do: #!/usr/bin/env bash python a.py &amp;&amp; python b.py To run both program background in same time #!/usr/bin/env bash python a.py &amp; python b.py &amp;"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am wondering if it is possible to span GPU VRAM and compute power using pytorch and nvidia smi. The spanning would work for the deforum collab as a local instance using jupyter notebook and miniconda. How would I implement pytorch to use all gpu and VRam available and patch that in to miniconda controlling the local collab of deforum so that their collab used all gpu resources? But I haven't found too many threads dealing with parallelism and nvidia smi. Any guidance here of possibilities would be awesome.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Let say I have a docker container is running A,B,C and GPU 1,2,3. I can check the gpu process ID with nvidia-smi some times container itself hold the gpu memory after it used up. so I want to find which gpu container is running which gpu and restart it I can not kill the gpu-pid because I do not have the sudo permission. is there way to know which container using which gpu pid? container A -&gt; using gpu A",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm on a system with multiple NVIDIA GPUs. One or more of them may - or may not - be used to drive a physical monitor. In my compute work, I want to avoid using that one (or more). How can I, programmatically, check which GPUs are used for display? If there's no robust way of doing that, I'll settle for getting those GPUs which are used by an Xorg process (which is what nvidia-smi gives me on the command-line)",
        "answers": [
            [
                "In case you want to use the same process, you can check the NVML API functions nvmlDeviceGetDisplayActive and nvmlDeviceGetDisplayMode. Specifically, nvmlReturn_t nvmlDeviceGetDisplayMode ( nvmlDevice_t device, nvmlEnableState_t* display ) can be used to detect if a physical display is connected to a device. nvmlReturn_t nvmlDeviceGetDisplayActive ( nvmlDevice_t device, nvmlEnableState_t* isActive ) can be used to check if X server is attached to a device, it can be possible that an X server is running, without an attached physical display. Link to documentation"
            ],
            [
                "Try the following on a terminal nvidia-smi --format=csv --query-gpu=index,display_mode,display_active For more information check the nvidia-smi documentation and nvidia-smi --help-query-gpu"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "We run an HPC cluster with GPUs. We would like to report the overall GPU utilization for the job. I know I can do it by periodically sampling in the background and doing the math. I was wondering if there was a tool where I could basically start the sampling period at the beginning of the job and then stop it at the end of the job and just have it report the overall average GPU utilization? For instance, AFAICT nvidia-smi will only do 1 second intervals. I am looking (hoping) for an option on it or a similar tool for start/stop functionality. Note that an arbitrary time period wont work unless I can end it early and get the results up that point as you never know how long the job will run. I would appreciate any pointers / ideas anyone could provide.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am having interesting and weird issue. When I start docker container with gpu it works fine and I see all the gpus in docker. However, few hours or few days later, I can't use gpus in docker. When I do nvidia-smi in docker machine. I see this msg \"Failed to initialize NVML: Unknown Error\" However, in the host machine, I see all the gpus with nvidia-smi. Also, when I restart the docker machine. It totally works fine and showing all gpus. My Inference Docker machine should be turned on all the time and do the inference depends on server requests. Does any one have same issue or the solution for this problem?",
        "answers": [
            [
                "I had the same Error. I tried the health check of docker as a temporary solution. When nvidia-smi failed, the container will be marked unhealth, and restart by willfarrell/autoheal. Docker-compose Version: services: gpu_container: ... healthcheck: test: [\"CMD-SHELL\", \"test -s `which nvidia-smi` &amp;&amp; nvidia-smi || exit 1\"] start_period: 1s interval: 20s timeout: 5s retries: 2 labels: - autoheal=true - autoheal.stop.timeout=1 restart: always autoheal: image: willfarrell/autoheal environment: - AUTOHEAL_CONTAINER_LABEL=all volumes: - /var/run/docker.sock:/var/run/docker.sock restart: always Dockerfile Version: HEALTHCHECK \\ --label autoheal=true \\ --label autoheal.stop.timeout=1 \\ --start-period=60s \\ --interval=20s \\ --timeout=10s \\ --retries=2 \\ CMD nvidia-smi || exit 1 with autoheal daemon: docker run -d \\ --name autoheal \\ --restart=always \\ -e AUTOHEAL_CONTAINER_LABEL=all \\ -v /var/run/docker.sock:/var/run/docker.sock \\ willfarrell/autoheal"
            ],
            [
                "I had the same weird issue. According to your description, it's most likely relevant to this issue on nvidia-docker official repo: https://github.com/NVIDIA/nvidia-docker/issues/1618 I plan to try the solution mentioned in related thread which suggest to upgrade the kernel cgroup version on host machine from v1 to v2. ps: We have verified this solution on our production environment and it really works! But unfortunately, this solution need at least linux kernel 4.5. If it is not possible to upgrade kernel, the method mentioned by sih4sing5hog5 could also be a workaround solution."
            ],
            [
                "There is a workaround that I tried and found it work. Please check this link: https://github.com/NVIDIA/nvidia-docker/issues/1730 I summarize the cause of the problem and elaborate on a solution here for your convenience. Cause: The host performs daemon-reload (or a similar activity). If the container uses systemd to manage cgroups, daemon-reload \"triggers reloading any Unit files that have references to NVIDIA GPUs.\" Then, your container loses access the reloaded GPU references. How to check if your problem is caused by the issue: When your container still has GPU access, open a \"host\" terminal and run sudo systemctl daemon-reload Then, go back to your container. If nvidia-smi in the container has the problem right away, you may continue to use the workarounds. Workarounds: Although I saw in one discussion that NVIDIA planned to release a formal fix in mid Jun, as of July 8, 2023, I did not see it yet. So, this should be still useful for you, especially when you just can't update your container stack. The easiest way is to disable cgroups in your containers through docker daemon.json. If disabling cgroups does not hurt you, here is the steps. All is done in the host system. sudo nano /etc/docker/daemon.json Then, within the file, add this parameter setting. \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"] Do not forget to add a comma before this parameter setting. It is a well-known JSON syntax, but I think some may not be familiar with it. This is an example edited file from my machine. { \"runtimes\": { \"nvidia\": { \"args\": [], \"path\": \"nvidia-container-runtime\" } }, \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"] } As for the last step, restart the docker service in the host. sudo service docker restart Note: if your container runs its own NVIDIA driver, the above steps will not work, but the reference link has more detail for dealing with it. I elaborate only on a simple solution that I expect many people will find it useful."
            ],
            [
                "Slightly different, but for other people that might stumble upon this. For me the GPUs were not available already after start of the docker container with nvidia-docker, but only showed Failed to initialize NVML: Unknown Error on nivida-smi. After some hours of looking for a solution I stumbled upon the similar error Failed to initialise NVML: Driver/library version mismatch. And one suggestion was to simply reboot the host machine. I did that and it now works. This happened after I upgraded both Ubuntu 20-&gt;22 and Docker 19-&gt;20 along with the nvidia drivers 525.116.04."
            ],
            [
                "I had the same issue, I just ran screen watch -n 1 nvidia-smi in the container and now it works continuously."
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001,
            1.0000001,
            1e-07,
            -2.9999999
        ]
    },
    {
        "question": "Nvidia-smi only provides a few metrics to measure GPU utilization. Most importantly, utilization.gpu represents the percent of time over the past sample period during which one or more kernels was executing on the GPU. Thus, it seems that a value of 100% does not at all indicate \"full\" GPU usage. Alternatively, Nsight Compute provides many detailed metrics, but I found it to run very slowly on even small neural networks - it doesn't seem to be the use case. Another option seems to be DLProf, but this again only provides rather granular metrics such as \"GPU utilization\" and \"Tensor Core Efficiency\", whose definitions I could not find. Therefore, is there another tool (or parameter) which provides detailed GPU usage metrics?",
        "answers": [
            [
                "Have you considered trying DCGM? https://developer.nvidia.com/dcgm#:~:text=NVIDIA%20Data%20Center%20GPU%20Manager,including%20power%20and%20clock%20management."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Is there a way to allocate the remaining memory in each GPU for your task? Can I split my task across multiple GPU's? nvidia-smi for your reference",
        "answers": [
            [
                "Yes. PyTorch is able to use any remaining GPU capacity given that there is enough memory. You only need to specify which GPUs to use: https://stackoverflow.com/a/39661999/10702372 Yes. GPU parallelism is implemented using PyTorch's DistributedDataParallel"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "[TL;DR] First, wait for a couple of minutes and check if the Nvidia driver starts to work properly. If not, stop and start the VM instance again. I created a Deep Learning VM (Google Click to Deploy) with an A100 GPU. After stopping and starting the instance, when I run nvidia-smi, I got the following error message: NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. But if I type which nvidia-smi, I got /usr/bin/nvidia-smi It seems the driver is there but can not be used. Can someone suggest how to enable NVIDIA driver after stopping and starting a deep learning VM? The first time I created and opened the instance, the driver is automatically installed. The system information is (using uname -m &amp;&amp; cat /etc/*release): x86_64 PRETTY_NAME=\"Debian GNU/Linux 10 (buster)\" NAME=\"Debian GNU/Linux\" VERSION_ID=\"10\" VERSION=\"10 (buster)\" VERSION_CODENAME=buster ID=debian HOME_URL=\"https://www.debian.org/\" SUPPORT_URL=\"https://www.debian.org/support\" BUG_REPORT_URL=\"https://bugs.debian.org/\" I tried the installation scripts from GCP. First run curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py And then run sudo python3 install_gpu_driver.py which gives the following message: Executing: which nvidia-smi /usr/bin/nvidia-smi Already installed.",
        "answers": [
            [
                "After posting the question, the Nvidia driver starts to work properly after waiting for a couple of minutes. In the following days, I tried stopping/starting the VM instance multiple times. Sometimes nvidia-smi directly works, sometimes does not after &gt;20 min waiting. My current best answer to this question is first waiting for several minutes. If nvidia-smi still does not work, stop and start the instance again."
            ],
            [
                "What worked for me (not sure if it will go well to next starts) was to remove all drivers: sudo apt remove --purge '*nvidia*', and then force the installation with sudo python3 install_gpu_driver.py. In the install_gpu_driver.py, change line 230 to return False inside of the check_driver_installed function. Then, run the script. Who uses docker may face this error docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]] and have to reinstall the docker too. This thread helped me."
            ],
            [
                "also ran into this issue. if it helps someone, running following command [1] fixed it for us: $ sudo apt-get install linux-headers-`uname -r` this was on debian 11. log"
            ]
        ],
        "votes": [
            5.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "It seems I didn't understand something essential about CUDA. I am using a C++ GUI application to start some kernels on a dual GPU card. When I start the host process, no process is listed by nvidia-smi. This is expected because the host process waits until I click a button before it uses CUDA and starts the kernels. If I push the button, the two kernels run fine on both GPUs, exit and return the expected results. The host process then is listed two times by nvidia-smi, once for each GPU. Both processes are visible in nvidia-smi until I exit the host process. I am a bit confused since there is no such thing as a cudaOpen() or cudaClose() function (or a similar function pair). Which CUDA API call(s) cause a process to be listed by nvidia-smi? Which CUDA API call(s) cause a process to be dropped from the list?",
        "answers": [
            [
                "This is explained in the CUDA documentation, Section 3.2.1 . https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#initialization 3.2.1. Initialization There is no explicit initialization function for the runtime; it initializes the first time a runtime function is called (more specifically any function other than functions from the error handling and version management sections of the reference manual). One needs to keep this in mind when timing runtime function calls and when interpreting the error code from the first call into the runtime. The runtime creates a CUDA context for each device in the system (see Context for more details on CUDA contexts). This context is the primary context for this device and is initialized at the first runtime function which requires an active context on this device. It is shared among all the host threads of the application. As part of this context creation, the device code is just-in-time compiled if necessary (see Just-in-Time Compilation) and loaded into device memory. This all happens transparently. If needed, e.g. for driver API interoperability, the primary context of a device can be accessed from the driver API as described in Interoperability between Runtime and Driver APIs. When a host thread calls cudaDeviceReset(), this destroys the primary context of the device the host thread currently operates on (i.e., the current device as defined in Device Selection). The next runtime function call made by any host thread that has this device as current will create a new primary context for this device. Note: The CUDA interfaces use global state that is initialized during host program initiation and destroyed during host program termination. The CUDA runtime and driver cannot detect if this state is invalid, so using any of these interfaces (implicitly or explicity) during program initiation or termination after main) will result in undefined behavior."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "When I enter nvidia-smi, I get the following result: kill -9 25585 will not work, and instead, I have to ps -ef and kill every python process for the Nvidia GPU to free up. Before, it used to show the actual process name Can someone please explain why and help not have to do this every time?",
        "answers": [
            [
                "I encountered this problem when I ctrl+Z a python process, and I found the solution in the answer to What if 'kill -9' does not work?. It may be a zombie process, find the parent process by ps -Al|grep xxx (xxx is the pid number of the zombie process) you will see 0 Z xxx xxx xxx 0 80 0 - 0 exit ? 00:00:00 soffice.bin &lt;defunct&gt; the third xxx is the parent process, and kill it (If it is 1, don't kill it!)."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have been working with historical NVIDIA SMI outputs for a while. I haven't really seen a Fan Speed of more than 100% in value. But in a new dataset I am working with, I am seeing a few readings that are above 100%. How do I interpret this? From the official documentation: The fan speed value is the percent of maximum speed that the device's fan is currently intended to run at. It ranges from 0 to 100%. Note: The reported speed is the intended fan speed. If the fan is physically blocked and unable to spin, this output will not match the actual fan speed. Many parts do not report fan speeds because they rely on cooling via fans in the surrounding enclosure. For all discrete products with dedicated fans. Despite that, I am seeing the following in my readings that were collected sometime between Sept 2021 and Oct 2021. Mon Sep 27 04:10:01 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.80 Driver Version: 460.80 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 On | 00000000:04:00.0 Off | N/A | | 27% 22C P8 7W / 180W | 0MiB / 8119MiB | 0% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 1080 On | 00000000:05:00.0 Off | N/A | | 27% 23C P8 7W / 180W | 0MiB / 8119MiB | 0% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 GeForce GTX 1080 On | 00000000:08:00.0 Off | N/A | | 49% 73C P2 101W / 180W | 1053MiB / 8119MiB | 89% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 GeForce GTX 1080 On | 00000000:09:00.0 Off | N/A | | 55% 81C P2 62W / 180W | 1063MiB / 8119MiB | 84% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 GeForce GTX 1080 On | 00000000:84:00.0 Off | N/A | | 27% 25C P8 6W / 180W | 0MiB / 8119MiB | 0% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 GeForce GTX 1080 On | 00000000:85:00.0 Off | N/A | |635% 82C P2 158W / 180W | 347MiB / 8119MiB | 84% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 GeForce GTX 1080 On | 00000000:88:00.0 Off | N/A | | 27% 19C P8 7W / 180W | 0MiB / 8119MiB | 0% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 GeForce GTX 1080 On | 00000000:89:00.0 Off | N/A | | 47% 70C P2 103W / 180W | 293MiB / 8119MiB | 76% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ Sat Oct 9 09:00:02 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.80 Driver Version: 460.80 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 On | 00000000:04:00.0 Off | N/A | | 40% 63C P2 130W / 180W | 237MiB / 8119MiB | 76% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 1080 On | 00000000:05:00.0 Off | N/A | | 47% 82C P2 151W / 180W | 253MiB / 8119MiB | 79% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 GeForce GTX 1080 On | 00000000:08:00.0 Off | N/A | | 49% 75C P2 120W / 180W | 241MiB / 8119MiB | 76% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 GeForce GTX 1080 On | 00000000:09:00.0 Off | N/A | | 54% 82C P2 146W / 180W | 253MiB / 8119MiB | 78% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 GeForce GTX 1080 On | 00000000:84:00.0 Off | N/A | | 49% 72C P2 108W / 180W | 259MiB / 8119MiB | 77% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 GeForce GTX 1080 On | 00000000:85:00.0 Off | N/A | |541% 82C P2 155W / 180W | 253MiB / 8119MiB | 79% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 GeForce GTX 1080 On | 00000000:88:00.0 Off | N/A | | 48% 70C P2 161W / 180W | 965MiB / 8119MiB | 86% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 GeForce GTX 1080 On | 00000000:89:00.0 Off | N/A | | 47% 72C P2 118W / 180W | 289MiB / 8119MiB | 78% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ Wed Sep 22 20:20:01 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.80 Driver Version: 460.80 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 On | 00000000:04:00.0 Off | N/A | | 48% 73C P2 167W / 180W | 955MiB / 8119MiB | 87% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 1080 On | 00000000:05:00.0 Off | N/A | | 53% 81C P2 166W / 180W | 959MiB / 8119MiB | 89% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 GeForce GTX 1080 On | 00000000:08:00.0 Off | N/A | | 52% 78C P2 156W / 180W | 955MiB / 8119MiB | 88% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 GeForce GTX 1080 On | 00000000:09:00.0 Off | N/A | | 54% 82C P2 160W / 180W | 955MiB / 8119MiB | 90% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 GeForce GTX 1080 On | 00000000:84:00.0 Off | N/A | | 43% 69C P2 154W / 180W | 263MiB / 8119MiB | 82% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 GeForce GTX 1080 On | 00000000:85:00.0 Off | N/A | |739% 76C P2 165W / 180W | 263MiB / 8119MiB | 85% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 GeForce GTX 1080 On | 00000000:88:00.0 Off | N/A | | 42% 67C P2 172W / 180W | 263MiB / 8119MiB | 84% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 GeForce GTX 1080 On | 00000000:89:00.0 Off | N/A | | 46% 73C P2 166W / 180W | 263MiB / 8119MiB | 87% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ Also, this usually occurs on GPU index 5 of a particular host, what can one hypothesize from that?",
        "answers": [],
        "votes": []
    },
    {
        "question": "The problem is new and has never happened before, so there might have been an update of the nvidia driver or libtorch. Problem: I am using Google Colab for additional GPU and want to install a programm, that needs libtorch. So, installing was working fine the last couple of weeks, however, starting from today, the program cannot be installed. I already tried to restart several times, reboot etc. and nothing seems to work. I also downloaded the new libtorch version for cuda 11.3 and updated cuda, so that the runtime runs on cuda 11.3. When I call !nvidia-smi it gives out the information as usual. Nevertheless, after adding libtorch as environment variable as needed in order to use libtorch using os.environ['LIBTORCH'] = \"/content/libtorch\" and os.environ['LD_LIBRARY_PATH'] = \"/content/libtorch/lib\" !nvidia-smi suddenly displays \"Failed to initialize NVML: Driver/library version mismatch\". And since this is happening, I cannot install the program anymore. So, I install rustc (since the program require rustup) and add it to the path with os.environ['PATH] += os.pathsep + \"path/to/.cargo/bin\" I add Libtorch as environment variable. I try to cargo-install the program. It usually worked fine, now it fails, throwing the error message: error: linking with `cc` failed: exit status: 1 = note: \"cc\" \"-m64\" \"-Wl,--eh-frame-hdr\" \"-Wl,-znoexecstack\" \"-Wl,--as-needed\" \"-L\" \"/usr/lib/rustlib/x86_64-unknown-linux-gnu/lib\" ......................................... = note: /usr/bin/ld: cannot find -ltorch_cuda /usr/bin/ld: cannot find -ltorch_cuda_cu /usr/bin/ld: cannot find -ltorch_cuda_cpp /usr/bin/ld: cannot find -ltorch_cpu /usr/bin/ld: cannot find -ltorch /usr/bin/ld: cannot find -lc10 collect2: error: ld returned 1 exit status",
        "answers": [],
        "votes": []
    },
    {
        "question": "What does the command sudo nvidia-smi --gpu-reset -i 0 do? Is it just freeing up the memory of GPU?",
        "answers": [
            [
                "From the nvidia-smi help menu (man nvidia-smi): -r, --gpu-reset Trigger a reset of one or more GPUs. Can be used to clear GPU HW and SW state in situations that would otherwise require a machine reboot. Typically useful if a double bit ECC error has occurred. Optional -i switch can be used to target one or more specific devices. Without this option, all GPUs are reset. Requires root. There can't be any appli\u2010 cations using these devices (e.g. CUDA application, graphics application like X server, monitoring application like other instance of nvidia-smi). There also can't be any com\u2010 pute applications running on any other GPU in the system. Starting with the NVIDIA Ampere architecture, GPUs with NVLink connections can be individually reset. On NVSwitch systems, Fabric Manager is required to facilitate reset. If Fabric Manager is not running, or if any of the GPUs being reset are based on an architecture preceding the NVIDIA Ampere architecture, any GPUs with NVLink connections to a GPU being reset must also be reset in the same command. This can be done either by omitting the -i switch, or using the -i switch to specify the GPUs to be reset. If the -i option does not specify a complete set of NVLink GPUs to reset, this command will issue an error identifying the additional GPUs that must be included in the reset command. GPU reset is not guaranteed to work in all cases. It is not recommended for production environments at this time. In some situations there may be HW components on the board that fail to revert back to an initial state following the reset request. This is more likely to be seen on Fermi-generation products vs. Kepler, and more likely to be seen if the reset is being performed on a hung GPU. Following a reset, it is recommended that the health of each reset GPU be verified before further use. If any GPU is not healthy a complete reset should be instigated by power cycling the node. GPU reset operation will not be supported on MIG enabled vGPU guests. Visit http://developer.nvidia.com/gpu-deployment-kit to download the GDK."
            ],
            [
                "Resets GPU state. Can be used to clear double bit ECC errors or recover hung GPU. Requires -i switch to target specific device. Available on Linux only. https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf"
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am trying to estimate the VRAM needed for a fully connected model without having to build/train the model in pytorch. I got pretty close with this formula: # params = number of parameters # 1 MiB = 1048576 bytes estimate = params * 24 / 1048576 This example model has 384048000 parameters, but I have tested this on different models with different parameter sizes. The results are pretty accurate. However, the estimation only takes into account the pytorch session VRAM, not the driver/cuda buffer VRAM amount. Here are the estimated (with the formula) versus empirical results (using nvidia-smi after building/training the model) ESTIMATE BEFORE EMPIRICAL TEST: VRAM estimate = 8790.1611328125MiB EMPIRICAL RESULT AFTER BUILDING MODEL: GPU RAM for pytorch session only (cutorch.max_memory_reserved(0)/1048576): 8466.0MiB GPU RAM including extra driver buffer from nvidia-smi: 9719MiB Any ideas on how to estimate that extra VRAM shown in nvidia-smi output?",
        "answers": [],
        "votes": []
    },
    {
        "question": "watch -n 1 \"paste &lt;(ssh ai02 'nvidia-smi pmon -s um -c 1') &lt;(ssh ai03 'nvidia-smi pmon -s um -c 1' )\" The above command is used to horizontally stack two server GPU stats together. It works without the watch command but get the following error sh: -c: line 0: syntax error near unexpected token `(' sh: -c: line 0: `paste &lt;(ssh ai02 'nvidia-smi pmon -s um -c 1') &lt;(ssh ai03 'nvidia-smi pmon -s um -c 1' )'",
        "answers": [
            [
                "You didn't provide a reproducible example, but I think I managed to make one for testing: watch -n1 \"paste &lt;(seq -w 1000 | shuf -n '10' ) &lt;(seq -w 1000 | shuf -n '10')\" output a similar error: sh: -c: line 0: syntax error near unexpected token `(' sh: -c: line 0: `paste &lt;(seq -w 1000 | shuf -n '10' ) &lt;(seq -w 1000 | shuf -n '1 0')' To solve this problem in a simpler way, we can change sh -c for bash -c: watch -n1 -x bash -c 'paste &lt;(seq -w 1000 | shuf -n \"10\" ) &lt;(seq -w 1000 | shuf -n \"10\")' From the watch manual: -x, --exec Pass command to exec(2) instead of sh -c which reduces the need to use extra quoting to get the desired effect. If you need maintain the apostrophes from the original commandline, you can escape then too: watch -e -n1 -x bash -c 'paste &lt;(seq -w 1000 | shuf -n '\\''10'\\'' ) &lt;(seq -w 1000 | shuf -n '\\''10'\\'')'"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to analyze very large text string in Python containing nvidia-smi outputs but I really want to spend more time analyzing the data than working on my regex skills. I got the regex as follows but it takes forever in some rows (it might be the variation of input data in some rows), but I thought maybe my regex pattern is very compute-intensive as well. extracted_line1 = r'[=]*[+][=]*[+][=]*\\|\\n\\|(\\s+(.*?)\\|)+\\n\\|(\\s+(.*?)\\|)(\\s+(.*?)\\|)(\\s+(.*?)\\|)\\n\\|' This pattern matches the third row in the table. This one down below \u2b07\ufe0f ===============================+======================+======================| | 0 GeForce GTX 1080 On | 00000000:04:00.0 Off | N/A | | 27% 20C P8 6W / 180W | 2MiB / 8119MiB | 0% E. Process | | | | N/A | It works for most rows but randomly hangs for some rows. What would be a more simplified version of this regex expression? Or maybe a better question is what is the best approach to grab each of the values in this table for every row (corresponding metrics for each GPU)? Truncated input string is here \ud83d\udc47 ... bunch of text nvidia-smi: Tue Jun 8 15:00:02 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.80 Driver Version: 460.80 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 1080 On | 00000000:04:00.0 Off | N/A | | 27% 20C P8 6W / 180W | 2MiB / 8119MiB | 0% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 1080 On | 00000000:05:00.0 Off | N/A | | 27% 23C P8 6W / 180W | 2MiB / 8119MiB | 0% E. Process | | | | N/A | +-------------------------------+----------------------+----------------------+ ... bunch of text P.S I am trying to extract the following values gpu_index = [processed result of regex output here] gpu_model_name = [processed result of regex output here] persistance_mode = [processed result of regex output here] bus_id = [processed result of regex output here] display_active = [processed result of regex output here] volatile_ecc = [processed result of regex output here] fan = [processed result of regex output here] temperature = [processed result of regex output here] perf = [processed result of regex output here] power_usage = [processed result of regex output here] max_power = [processed result of regex output here] memory_usage = [processed result of regex output here] available_mem = [processed result of regex output here] gpu_utilization = [processed result of regex output here] compute_mode = [processed result of regex output here] multiple_instance_gpu_mode = [processed result of regex output here]",
        "answers": [
            [
                "I suggest another pattern, easier on your machine's resources. Pattern (\\d+%)\\s+(\\d+C)\\s+(\\w\\d)\\s+(\\d+W)\\s+\\/\\s+(\\d+W)\\s+\\|\\s+(\\d+MiB)\\s+\\/\\s+(\\d+MiB)\\s+\\|\\s+(\\d+\\%)\\s+(.*?)\\s+\\| First of all, I got rid of all the starting pattern of finding = or + chars because regex knows how to find the stuff you instruct it to find. No 'helper' handles needed. Next I found that you only need to grab a hold on english chars \\w, digits \\d and whitespaces \\s and so the whole pattern was pretty easy to write. Explanation I'm building the whole pattern match group by match group until reaching the final result. Please notice each explanation is only valid for the last match group i.e. (some ReGex expresion in parantesis) (\\d+%) will match any number of digits followed by % (\\d+%)\\s+(\\d+C) will match any number of digits after unknown amount of whitespaces, followed by the letter C (\\d+%)\\s+(\\d+C)\\s+(\\w\\d) will match any single char followed by any single digit (\\d+%)\\s+(\\d+C)\\s+(\\w\\d)\\s+(\\d+W) will match any number of digits after unknown amount of whitespaces, followed by a single char (\\d+%)\\s+(\\d+C)\\s+(\\w\\d)\\s+(\\d+W)\\s+\\/\\s+(\\d+W) knowing there should by some whitespaces, then / and some other whitespaces, this expression will match any number of digits followed by W (\\d+%)\\s+(\\d+C)\\s+(\\w\\d)\\s+(\\d+W)\\s+\\/\\s+(\\d+W)\\s+\\|\\s+(\\d+MiB) knowing there should by some whitespaces, then | and some other whitespaces, this expression will match any number of digits followed by MiB (\\d+%)\\s+(\\d+C)\\s+(\\w\\d)\\s+(\\d+W)\\s+\\/\\s+(\\d+W)\\s+\\|\\s+(\\d+MiB)\\s+\\/\\s+(\\d+MiB) knowing there should by some whitespaces, then / and some other whitespaces, this expression will match any number of digits followed by a MiB (\\d+%)\\s+(\\d+C)\\s+(\\w\\d)\\s+(\\d+W)\\s+\\/\\s+(\\d+W)\\s+\\|\\s+(\\d+MiB)\\s+\\/\\s+(\\d+MiB)\\s+\\|\\s+(\\d+\\%) knowing there should by some whitespaces, then | and some other whitespaces, this expression will match any number of digits followed by a % Last bit (\\d+%)\\s+(\\d+C)\\s+(\\w\\d)\\s+(\\d+W)\\s+\\/\\s+(\\d+W)\\s+\\|\\s+(\\d+MiB)\\s+\\/\\s+(\\d+MiB)\\s+\\|\\s+(\\d+\\%)\\s+(.*?)\\s+\\| knowing there should by some whitespaces, this expression will match any number of any characters until it hits some unknown amount of whitespaces followed by | Finally the next variables are covered: gpu_index = not implemented gpu_model_name = not implemented persistance_mode = not implemented bus_id = not implemented display_active = not implemented volatile_ecc = not implemented fan = (\\d+%) temperature = (\\d+C) perf = (\\w\\d) power_usage = (\\d+W) max_power = (\\d+W) memory_usage = (\\d+MiB) available_mem = (\\d+MiB) gpu_utilization = (\\d+\\%) compute_mode = (.*?) multiple_instance_gpu_mode = not implemented"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This there a way to get the bitmask of the clocks_throttle_reasons.active into plain english? Is there perhaps a list somewhere? Please find my command below nvidia-smi --query-gpu=clocks_throttle_reasons.active --format=csv The above code returns the follow 0x0000000000000004 bitmask which looking at my data suggests that its a high power draw issue. Thanks in advance!",
        "answers": [],
        "votes": []
    }
]