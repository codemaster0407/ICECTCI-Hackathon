[
    {
        "question": "Consider the CUDA API function CUresult cuMemcpy3DAsync (const CUDA_MEMCPY3D* pCopy, CUstream hStream); described here. It takes a CUDA_MEMCPY3D structure by pointer ; and this pointer is not to some CUDA-driver-created entity - it's to a structure the user has created. My question: Do we need to keep the pointed-to structure alive past the call to this function returning? e.g. until after we've sycnrhonized the stream we've enqueued the copy on? Or - can we just discard it immediately? I'm guessing it should be the former, but the documentation doesn't really say.",
        "answers": [],
        "votes": []
    },
    {
        "question": "CUDA 12 introduces two new API calls, cuStreamGetId() and cuCtxGetId() which return \"unique ID\"s associated with a stream or a context respectively. I'm struggling to understand why this is useful, or how this would be used. Are the handles for streams and contexts not unique? i.e. does CUDA create copies of CUstream_st and CUctx_st structures with the same values, or which describe the same entities? If it does - under what circumstances?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Suppose I have a GPU and driver version supporting unified addressing; two GPUs, G0 and G1; a buffer allocated in G1 device memory; and that the current context C0 is a context for G0. Under these circumstances, is it legitimate to cuMemcpy() from my buffer to host memory, despite it having been allocated in a different context for a different device? So far, I've been working under the assumption that the answer is \"yes\". But I've recently experienced some behavior which seems to contradict this assumption.",
        "answers": [
            [
                "Calling cuMemcpy from another context is legal, regardless of which device the context was created on. Depending on which case you are in, I recommend the following: If this is a multi-threaded application, double-check your program and make sure you are not releasing your device memory before the copy is completed If you are using the cuMallocAsync/cuFreeAsync API to allocate and/or release memory, please make sure that operations are correctly stream-ordered Run compute-sanitizer on your program If you keep experiencing issues after these steps, you can file a bug with NVIDIA here."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Suppose that, for a device without an activated primary context, I: Call cuDevicePrimaryCtxRetain() and get a CUcontext value. Call cuDevicePrimaryCtxRelease(); the context gets deactivated. Call cuDevicePrimaryCtxRetain() and get another CUcontext value. In my (limited and anecdotal) experience, I get the same handle from both calls. Are we guaranteed this behavior? That is, will all subsequent calls to cuDevicePrimaryCtxRetain() (in the same process) always use the same handle, even though the primary context may have been disactivated and other work done?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I create 2 cuda context \u201cctx1\u201d and \"ctx2\" and set current context to \"ctx1\" and allocate 8 bytes of memory and switch current context to ctx2. Then free Memory alloc in ctx1. Why does this return CUDA_SUCCESS? And when I destroy ctx1 and then free Memory, it will cause CUDA_INVALID_VALUE. In my opinion, each context contain their unique resources and not allowed access by other Context. Can someone explain this behaviour? int main() { using std::cout; CUresult answer; CUdeviceptr dptr = 4; int device_enum = 0; CUcontext ctx1,ctx2; cuInit(0); CUdevice able_dev = 0; CUresult create_ctx1 = cuCtxCreate(&amp;ctx1,CU_CTX_SCHED_AUTO,able_dev); CUresult create_ctx2 = cuCtxCreate(&amp;ctx2,CU_CTX_SCHED_AUTO,able_dev); assert(cuCtxSetCurrent(ctx1) == CUDA_SUCCESS); answer = cuMemAlloc(&amp;dptr,8); cout &lt;&lt; \"maloc result1 = \" &lt;&lt; answer &lt;&lt; '\\n'; assert(cuCtxSetCurrent(ctx2) == CUDA_SUCCESS); cout &lt;&lt; \"free in ctx2 result = \" &lt;&lt; cuMemFree(dptr) &lt;&lt; '\\n'; }",
        "answers": [
            [
                "Why does this return CUDA_SUCCESS? Why should it not return CUDA_SUCCESS? I don't see anywhere in the documentation that says a free operation is only valid if the referenced pointer is associated to the current context. This seems perfectly valid, and your test case seems to confirm it. And when I destroy ctx1 and then free Memory, it will cause CUDA_INVALID_VALUE. That is expected behavior. You allocated dptr in ctx1. When you destroy ctx1, all state associated with that context, including any associated allocations, are destroyed. Attempting to free a pointer that has already been freed via context destruction is invalid. In case you thought, as someone else indicated in the comments, that the context would be \"needed\" for the free operation: It's not documented It's not necessary in a UVA setting. A pointer is introspectable in a UVA setting partly because the UVA setting ensures that relevant address spaces do not overlap."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am working on using NVIDIA CUDA Multi Process Service (MPS) for running multiple TensorFlow inference jobs using the same GPU. For my use-case, when GPU is being shared by more than one processes, I sometimes need to prioritize execution of kernels of one process over the other. Is this supported? To explain the problem in more detail, consider an example in which we have two processes, p1 and p2 (each with just one kernel execution stream) sharing a GPU. Scenario: When there are one or more kernels in ready queue for both p1 and p2. Default MPS behavior (My understanding): If there is enough resources, execute multiple kernels at the same time from both p1 and p2. Desired behavior: Ability to decide based on priority if: Execute kernel of p1 first then p2. Execute kernel of p2 first then p1. Incase there is enough resources, execute multiple kernels at the same time from both p1 and p2. If this kind of customized scheduling is not supported, It will be great if someone can guide what code changes will be needed to make it work.",
        "answers": [
            [
                "I sometimes need to prioritize execution of kernels of one process over the other. Is this supported? No it is not."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to build the following program: #include &lt;iostream&gt; #include &lt;cuda.h&gt; int main() { const char* str; auto status = cuInit(0); cuGetErrorString(status, &amp;str); std::cout &lt;&lt; \"status = \" &lt;&lt; str &lt;&lt; std::endl; int device_id = 0; CUcontext primary_context_id; status = cuDevicePrimaryCtxRetain(&amp;primary_context_id, device_id); cuGetErrorString(status, &amp;str); std::cout &lt;&lt; \"status = \" &lt;&lt; str &lt;&lt; std::endl; status = cuDevicePrimaryCtxRelease(device_id); cuGetErrorString(status, &amp;str); std::cout &lt;&lt; \"status = \" &lt;&lt; str &lt;&lt; std::endl; } Compilation always goes fine; but, with CUDA 10.2, linking works, while with CUDA 11.2, I get: /usr/bin/ld: a.o: in function `main': a.cpp:(.text+0xcc): undefined reference to `cuDevicePrimaryCtxRelease_v2' collect2: error: ld returned 1 exit status Why is this happening and how can I fix it? Note: I'm using Devuan Beowulf with driver version 440.82 (have not installed a new driver for CUDA 11.2).",
        "answers": [
            [
                "Well, I think I have an idea of why this happens. This is about how cuDevicePrimaryCtxRelease() is defined. Let's run: grep PrimaryCtxRelease /usr/local/cuda/include/cuda.h | grep -v \"^ \" In CUDA 10.2, we get: CUresult CUDAAPI cuDevicePrimaryCtxRelease(CUdevice dev); while in CUDA 11.2, we get: #define cuDevicePrimaryCtxRelease cuDevicePrimaryCtxRelease_v2 CUresult CUDAAPI cuDevicePrimaryCtxRelease(CUdevice dev); That is, the API name has changed, but the header file leaves an alias to the new name. (And that's a confusing piece of code, I would say.) Now, let's peer into the object files I get in the two different versions of CUDA, using objdump -t | c++filt | grep cu. With CUDA 10.2, it's: 0000000000000000 *UND* 0000000000000000 cuInit 0000000000000000 *UND* 0000000000000000 cuGetErrorString 0000000000000000 *UND* 0000000000000000 cuDevicePrimaryCtxRetain 0000000000000000 *UND* 0000000000000000 cuDevicePrimaryCtxRelease while with CUDA 11.2, it's: 0000000000000000 *UND* 0000000000000000 cuInit 0000000000000000 *UND* 0000000000000000 cuGetErrorString 0000000000000000 *UND* 0000000000000000 cuDevicePrimaryCtxRetain 0000000000000000 *UND* 0000000000000000 cuDevicePrimaryCtxRelease_v2 (note the _v2). so it's probably the case that the installed driver only contains the non-_v2 symbol, hence the undefined symbol. What I would still appreciate help with is how to work around this issue other than by updating the driver."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Suppose I have an active CUDA context associated with device i, and I now call cudaSetDevice(i). What happens? : Nothing? Primary context replaces the top of the stack? Primary context is pushed onto the stack? It actually seems to be inconsistent. I've written this program, running on a machine with a single device: #include &lt;cuda.h&gt; #include &lt;cuda_runtime_api.h&gt; #include &lt;cassert&gt; #include &lt;iostream&gt; int main() { CUcontext ctx1, primary; cuInit(0); auto status = cuCtxCreate(&amp;ctx1, 0, 0); assert (status == (CUresult) cudaSuccess); cuCtxPushCurrent(ctx1); status = cudaSetDevice(0); assert (status == cudaSuccess); void* ptr1; void* ptr2; cudaMalloc(&amp;ptr1, 1024); assert (status == cudaSuccess); cuCtxGetCurrent(&amp;primary); assert (status == (CUresult) cudaSuccess); assert(primary != ctx1); status = cuCtxPushCurrent(ctx1); assert (status == (CUresult) cudaSuccess); cudaMalloc(&amp;ptr2, 1024); assert (status == (CUresult) cudaSuccess); cudaSetDevice(0); assert (status == (CUresult) cudaSuccess); int i = 0; while (true) { status = cuCtxPopCurrent(&amp;primary); if (status != (CUresult) cudaSuccess) { break; } std::cout &lt;&lt; \"Next context on stack (\" &lt;&lt; i++ &lt;&lt; \") is \" &lt;&lt; (void*) primary &lt;&lt; '\\n'; } } and I get the following output: context ctx1 is 0x563ec6225e30 primary context is 0x563ec61f5490 Next context on stack (0) is 0x563ec61f5490 Next context on stack (1) is 0x563ec61f5490 Next context on stack(2) is 0x563ec6225e3 This seems like the behavior is sometimes a replacement, and sometimes a push. What's going on?",
        "answers": [
            [
                "TL;DR: Based on the code you have provided, in both instances of your particular usage, it seems that cudaSetDevice() is replacing the context at the top of the stack. Let's modify your code a bit, and then see what we can infer about the effect of each API call in your code on the context stack: $ cat t1759.cu #include &lt;cuda.h&gt; #include &lt;cuda_runtime_api.h&gt; #include &lt;cassert&gt; #include &lt;iostream&gt; void check(int j, CUcontext ctx1, CUcontext ctx2){ CUcontext ctx0; int i = 0; while (true) { auto status = cuCtxPopCurrent(&amp;ctx0); if (status != CUDA_SUCCESS) { break; } if (ctx0 == ctx1) std::cout &lt;&lt; j &lt;&lt; \":Next context on stack (\" &lt;&lt; i++ &lt;&lt; \") is ctx1:\" &lt;&lt; (void*) ctx0 &lt;&lt; '\\n'; else if (ctx0 == ctx2) std::cout &lt;&lt; j &lt;&lt; \":Next context on stack (\" &lt;&lt; i++ &lt;&lt; \") is ctx2:\" &lt;&lt; (void*) ctx0 &lt;&lt; '\\n'; else std::cout &lt;&lt; j &lt;&lt; \":Next context on stack (\" &lt;&lt; i++ &lt;&lt; \") is unknown:\" &lt;&lt; (void*) ctx0 &lt;&lt; '\\n'; } } void runtest(int i) { CUcontext ctx1, primary = NULL; cuInit(0); auto dstatus = cuCtxCreate(&amp;ctx1, 0, 0); // checkpoint 1 assert (dstatus == CUDA_SUCCESS); if (i == 1) {check(i,ctx1,primary); return;}// checkpoint 1 dstatus = cuCtxPushCurrent(ctx1); // checkpoint 2 assert (dstatus == CUDA_SUCCESS); if (i == 2) {check(i,ctx1,primary); return;}// checkpoint 2 auto rstatus = cudaSetDevice(0); // checkpoint 3 assert (rstatus == cudaSuccess); if (i == 3) {check(i,ctx1,primary); return;}// checkpoint 3 void* ptr1; void* ptr2; rstatus = cudaMalloc(&amp;ptr1, 1024); // checkpoint 4 assert (rstatus == cudaSuccess); if (i == 4) {check(i,ctx1,primary); return;}// checkpoint 4 dstatus = cuCtxGetCurrent(&amp;primary); // checkpoint 5 assert (dstatus == CUDA_SUCCESS); assert(primary != ctx1); if (i == 5) {check(i,ctx1,primary); return;}// checkpoint 5 dstatus = cuCtxPushCurrent(ctx1); // checkpoint 6 assert (dstatus == CUDA_SUCCESS); if (i == 6) {check(i,ctx1,primary); return;}// checkpoint 6 rstatus = cudaMalloc(&amp;ptr2, 1024); // checkpoint 7 assert (rstatus == cudaSuccess); if (i == 7) {check(i,ctx1,primary); return;}// checkpoint 7 rstatus = cudaSetDevice(0); // checkpoint 8 assert (rstatus == cudaSuccess); if (i == 8) {check(i,ctx1,primary); return;}// checkpoint 8 return; } int main(){ for (int i = 1; i &lt; 9; i++){ cudaDeviceReset(); runtest(i);} } $ nvcc -o t1759 t1759.cu -lcuda -std=c++11 $ ./t1759 1:Next context on stack (0) is ctx1:0x11087e0 2:Next context on stack (0) is ctx1:0x1741160 2:Next context on stack (1) is ctx1:0x1741160 3:Next context on stack (0) is unknown:0x10dc520 3:Next context on stack (1) is ctx1:0x1c5aa70 4:Next context on stack (0) is unknown:0x10dc520 4:Next context on stack (1) is ctx1:0x23eaa00 5:Next context on stack (0) is ctx2:0x10dc520 5:Next context on stack (1) is ctx1:0x32caf30 6:Next context on stack (0) is ctx1:0x3a44ed0 6:Next context on stack (1) is ctx2:0x10dc520 6:Next context on stack (2) is ctx1:0x3a44ed0 7:Next context on stack (0) is ctx1:0x41cfd90 7:Next context on stack (1) is ctx2:0x10dc520 7:Next context on stack (2) is ctx1:0x41cfd90 8:Next context on stack (0) is ctx2:0x10dc520 8:Next context on stack (1) is ctx2:0x10dc520 8:Next context on stack (2) is ctx1:0x4959c70 $ Based on the above, as we proceed through each API call in your code: 1. auto dstatus = cuCtxCreate(&amp;ctx1, 0, 0); // checkpoint 1 1:Next context on stack (0) is ctx1:0x11087e0 The context creation also pushes the newly created context on the stack, as mentioned here. 2. dstatus = cuCtxPushCurrent(ctx1); // checkpoint 2 2:Next context on stack (0) is ctx1:0x1741160 2:Next context on stack (1) is ctx1:0x1741160 No surprise, pushing the same context on the stack creates another stack entry for it. 3. auto rstatus = cudaSetDevice(0); // checkpoint 3 3:Next context on stack (0) is unknown:0x10dc520 3:Next context on stack (1) is ctx1:0x1c5aa70 The cudaSetDevice() call has replaced the top of the stack with an \"unknown\" context. (Only unknown at this point because we have not retrieved the handle value of the \"other\" context). 4. rstatus = cudaMalloc(&amp;ptr1, 1024); // checkpoint 4 4:Next context on stack (0) is unknown:0x10dc520 4:Next context on stack (1) is ctx1:0x23eaa00 No difference in stack configuration due to this call. 5. dstatus = cuCtxGetCurrent(&amp;primary); // checkpoint 5 5:Next context on stack (0) is ctx2:0x10dc520 5:Next context on stack (1) is ctx1:0x32caf30 No difference in stack configuration due to this call, but we now know that the top of stack context is the current context (and we can surmise it is the primary context). 6. dstatus = cuCtxPushCurrent(ctx1); // checkpoint 6 6:Next context on stack (0) is ctx1:0x3a44ed0 6:Next context on stack (1) is ctx2:0x10dc520 6:Next context on stack (2) is ctx1:0x3a44ed0 No real surprise here. We are pushing ctx1 on the stack, and so the stack has 3 entries, the first one being the driver API created context, and the next two entries being the same as the stack configuration from step 5, just moved down one stack location. 7. rstatus = cudaMalloc(&amp;ptr2, 1024); // checkpoint 7 7:Next context on stack (0) is ctx1:0x41cfd90 7:Next context on stack (1) is ctx2:0x10dc520 7:Next context on stack (2) is ctx1:0x41cfd90 Again, this call has no effect on stack configuration. 8. rstatus = cudaSetDevice(0); // checkpoint 8 8:Next context on stack (0) is ctx2:0x10dc520 8:Next context on stack (1) is ctx2:0x10dc520 8:Next context on stack (2) is ctx1:0x4959c70 Once again, we see that the behavior here is that the cudaSetDevice() call has replaced the top of stack context with the primary context. The conclusion I have from your test code is that I see no inconsistency of behavior of the cudaSetDevice() call when intermixed with various runtime and driver API calls as you have in your code. From my perspective, this sort of programming paradigm is insanity. I can't imagine why you would want to intermix driver API and runtime API code this way."
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "You can (?) determine whether a CUDA context is the primary one by calling cuDevicePrimaryCtxRetain() and comparing the returned pointer to the context you have. But - what if nobody's created the primary context yet? Is there a cheaper way to obtain the negative answer then? Or - is it impossible for a non-primary context to exist while the primary does not?",
        "answers": [
            [
                "You can check whether the primary context has been created (\"activated\") or not: inline bool primary_context_is_active(int device_id) { unsigned flags; int is_active; CUresult status = cuDevicePrimaryCtxGetState(device_id, &amp;flags, &amp;is_active); if (status != CUDA_SUCCESS) { /* error handling here */ } return is_active; } Now, if the primary context is not active, then you know your context is not the primary one; if it is active, you can use cuDevicePrimaryCtxRetain(), and - unless you're doing something multi-threaded or using coroutines etc. - you know it'll be a cheap call. This of course depends on assuming your context is not an invalid primary context handle after disactivation."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "In this discussion of the runtime vs the driver API, it is said that Primary contexts are created as needed, one per device per process, are reference-counted, and are then destroyed when there are no more references to them. What counts as such references? And - does this not imply that, often, the primary context is supposed to be destroyed right after being used, repeatedly? e.g. you get the default device ID, then launch a kernel; what \"references\" remain? Surely it's not the integer variable holding the device id...",
        "answers": [
            [
                "None of the exact internal workings of the runtime API are documented and there is empirical evidence that they have subtly changed over time. That said, if you inspect the host code boilerplate the toolchain emits and run some host side traces, it is possible to infer how it works, and what follows is my understanding based on observations made in this way. It is important to realize that primary context reference counting is an internal function within the driver and the \"lazy context establishment\" mechanism itself uses some internal API hooks which will either bind to an existing primary context created explicitly by the driver API (which increments the reference count) or create one itself if none is available and then bind to that context (which also increments the reference count). The routines which unbind from a primary context are registered via atexit and will trigger when the application exits or when cudaDeviceReset() is called. This approach prevents the potential scenario you have posited whereby contexts are continuously destroyed when their reference count falls to zero and then recreated when another runtime API functional is called. That doesn't happen."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I followed Robert Crovella's example on how to use Nvidia's Multi-Process Service. According to docs: 2.1.2. Reduced on-GPU context storage Without MPS each CUDA processes using a GPU allocates separate storage and scheduling resources on the GPU. In contrast, the MPS server allocates one copy of GPU storage and scheduling resources shared by all its clients. which I understood as the reduction of each of the processes' context sizes, which is possible because they are shared. This would increase free GPU memory and thus enable running more processes in parallel. Now, back to the example. Without MPS: And with MPS: Unfortunately each process still takes virtually the same (~300MB) amount of memory. Isn't this in contradiction to the docs? Is there a way to decrease per process memory consumption?",
        "answers": [
            [
                "Oops, I overeagerly asked before checking the memory usage on the other (pre-Volta) card and yes, there is actually a difference. Let me just post it here for future reference if anyone else stumbled on this problem too: MPS off: MPS on:"
            ],
            [
                "Indeed, as seen here, in Volta architecture, you can see the processes communicate directly with the GPU, without the MPS server in the middle: Volta MPS clients submit work directly to the GPU without passing through the MPS server. This can be easily seen from your first screenshot where the t1034 processes are listed as using the GPU. On the contrary, in pre-Volta architectures, the client processes communicate with the GPU through the MPS server. This results in seeing only the MPS server process communicating directly with the GPU in the latter screenshot."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "Is there a way that I can get cuda context memory usage rather than having to use cudaMemGetInfo which only reports global information of a device? or at least a way to get how much memory is occupied by the current application?",
        "answers": [
            [
                "It seems to be impossible [No]. However, retrieving per-process memory usage is still a good alternative. And as Robert has pointed out, per-process memory usage can be retrieved using NVML, specifically, by using nvmlDeviceGetComputeRunningProcesses function."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "This question is a followup on Jason R's comment to Robert Crovellas answer on this original question (\"Multiple CUDA contexts for one device - any sense?\"): When you say that multiple contexts cannot run concurrently, is this limited to kernel launches only, or does it refer to memory transfers as well? I have been considering a multiprocess design all on the same GPU that uses the IPC API to transfer buffers from process to process. Does this mean that effectively, only one process at a time has exclusive access to the entire GPU (not just particular SMs)? [...] How does that interplay with asynchronously-queued kernels/copies on streams in each process as far as scheduling goes? Robert Crovella suggested asking this in a new question but it never happed, so let me do this here.",
        "answers": [
            [
                "Multi-Process Service is an alternative CUDA implementation by Nvidia that makes multiple processes use the same context. This e.g. allows kernels from multiple processes to run in parallel if each of them does not fill the entire GPU by itself."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have a working app which uses Cuda / C++, but sometimes, because of memory leaks, throws exception. I need to be able to reset the GPU on live, my app is a server so it has to stay available. I tried something like this, but it doesnt seems to work: try { // do process using GPU } catch (std::exception &amp;e) { // catching exception from cuda only cudaSetDevice(0); CUDA_RETURN_(cudaDeviceReset()); } My idea is to reset the device each times I get an exception from the GPU, but I cannot manage to make it working. :( Btw, for some reasons, I cannot fix every problems of my Cuda code, I need a temporary solution. Thanks !",
        "answers": [
            [
                "The only method to restore proper device functionality after a non-recoverable (\"sticky\") CUDA error is to terminate the host process that initiated (i.e. issued the CUDA runtime API calls that led to) the error. Therefore, for a single-process application, the only method is to terminate the application. It should be possible to design a multi-process application, where the initial (\"parent\") process makes no usage of CUDA whatsoever, and spawns a child process that uses the GPU. When the child process encounters an unrecoverable CUDA error, it must terminate. The parent process can, optionally, monitor the child process. If it determines that the child process has terminated, it can re-spawn the process and restore CUDA functional behavior. Sticky vs. non-sticky errors are covered elsewhere, such as here. An example of a proper multi-process app that uses e.g. fork() to spawn a child process that uses CUDA is available in the CUDA sample code simpleIPC. Here is a rough example assembled from the simpleIPC example (for linux): $ cat t477.cu /* * Copyright 1993-2015 NVIDIA Corporation. All rights reserved. * * Please refer to the NVIDIA end user license agreement (EULA) associated * with this source code for terms and conditions that govern your use of * this software. Any use, reproduction, disclosure, or distribution of * this software and related documentation outside the terms of the EULA * is strictly prohibited. * */ // Includes #include &lt;stdio.h&gt; #include &lt;assert.h&gt; // CUDA runtime includes #include &lt;cuda_runtime_api.h&gt; // CUDA utilities and system includes #include &lt;helper_cuda.h&gt; #define MAX_DEVICES 1 #define PROCESSES_PER_DEVICE 1 #define DATA_BUF_SIZE 4096 #ifdef __linux #include &lt;unistd.h&gt; #include &lt;sched.h&gt; #include &lt;sys/mman.h&gt; #include &lt;sys/wait.h&gt; #include &lt;linux/version.h&gt; typedef struct ipcDevices_st { int count; int results[MAX_DEVICES]; } ipcDevices_t; // CUDA Kernel __global__ void simpleKernel(int *dst, int *src, int num) { // Dummy kernel int idx = blockIdx.x * blockDim.x + threadIdx.x; dst[idx] = src[idx] / num; } void runTest(int index, ipcDevices_t* s_devices) { if (s_devices-&gt;results[0] == 0){ simpleKernel&lt;&lt;&lt;1,1&gt;&gt;&gt;(NULL, NULL, 1); // make a fault cudaDeviceSynchronize(); s_devices-&gt;results[0] = 1;} else { int *d, *s; int n = 1; cudaMalloc(&amp;d, n*sizeof(int)); cudaMalloc(&amp;s, n*sizeof(int)); simpleKernel&lt;&lt;&lt;1,1&gt;&gt;&gt;(d, s, n); cudaError_t err = cudaDeviceSynchronize(); if (err != cudaSuccess) s_devices-&gt;results[0] = 0; else s_devices-&gt;results[0] = 2;} cudaDeviceReset(); } #endif int main(int argc, char **argv) { ipcDevices_t *s_devices = (ipcDevices_t *) mmap(NULL, sizeof(*s_devices), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, 0, 0); assert(MAP_FAILED != s_devices); // We can't initialize CUDA before fork() so we need to spawn a new process s_devices-&gt;count = 1; s_devices-&gt;results[0] = 0; printf(\"\\nSpawning child process\\n\"); int index = 0; pid_t pid = fork(); printf(\"&gt; Process %3d\\n\", pid); if (pid == 0) { // child process // launch our test runTest(index, s_devices); } // Cleanup and shutdown else { // parent process int status; waitpid(pid, &amp;status, 0); if (s_devices-&gt;results[0] &lt; 2) { printf(\"first process launch reported error: %d\\n\", s_devices-&gt;results[0]); printf(\"respawn\\n\"); pid_t newpid = fork(); if (newpid == 0) { // child process // launch our test runTest(index, s_devices); } // Cleanup and shutdown else { // parent process int status; waitpid(newpid, &amp;status, 0); if (s_devices-&gt;results[0] &lt; 2) printf(\"second process launch reported error: %d\\n\", s_devices-&gt;results[0]); else printf(\"second process launch successful\\n\"); } } } printf(\"\\nShutting down...\\n\"); exit(EXIT_SUCCESS); } $ nvcc -I/usr/local/cuda/samples/common/inc t477.cu -o t477 $ ./t477 Spawning child process &gt; Process 10841 &gt; Process 0 Shutting down... first process launch reported error: 1 respawn Shutting down... second process launch successful Shutting down... $ For windows, the only changes need should be to use a windows IPC mechanism for host interprocess communication."
            ]
        ],
        "votes": [
            8.0000001
        ]
    },
    {
        "question": "I'm running the following code: cuCtxCreate(&amp;context, CU_CTX_SCHED_YIELD, device); int driverVersion = -1; int driverVersionRt = -1; int rtVersion = -1; unsigned int ctxVersion = 1; cuDriverGetVersion(&amp;driverVersion); cudaDriverGetVersion(&amp;driverVersionRt); cudaRuntimeGetVersion(&amp;rtVersion); cuCtxGetApiVersion(context, &amp;ctxVersion); std::cout &lt;&lt; \"cuDriverGetVersion: \" &lt;&lt; driverVersion &lt;&lt; std::endl; std::cout &lt;&lt; \"cudaDriverGetVersion: \" &lt;&lt; driverVersionRt &lt;&lt; std::endl; std::cout &lt;&lt; \"cudaRuntimeGetVersion: \" &lt;&lt; rtVersion &lt;&lt; std::endl; std::cout &lt;&lt; \"cuCtxGetApiVersion: \" &lt;&lt; ctxVersion &lt;&lt; std::endl; Here is the output: cuDriverGetVersion: 10010 cudaDriverGetVersion: 10010 cudaRuntimeGetVersion: 10000 cuCtxGetApiVersion: 3020 From the documentation, I can see that cudaDriverGetVersion, cuDriverGetVersion, and cudaRuntimeGetVersion return values of the form (1000 * major + 10 * minor). The documentation does not specify what cuCtxGetApiVersion should return, but I assume it should be the same as the other three functions. My question is why is the version number for the context 3020, when all the others are using 10000 and 10010? Is this how it should be? I am having another issue where new threads that I create need to have their context shared manually, otherwise I get a 201 (invalid context) error. This is strange because I know past CUDA 4.0 all processes have one context per device per process. So I should not have to set the context for new threads that I am creating within the same process. Because cuCtxGetApiVersion is producing 3020, this leads me to believe that the context I created is using an old incorrect version, which would not have the functionality of being shared across threads. Is this a possibility?",
        "answers": [
            [
                "It turns out that the cuCtxGetApiVersion was working correctly. This answer helped me understand what was going on. My cuCtxGetApiVersion was using the \"v2\". The context version is an up to date one even though it looks to be old. As for my second question, I found that the behavior I was getting was to be expected. Previously I was passing in streams to perform asynchronous calls, but I had played around with synchronous calls where no stream is specified. New threads that got created would not have the context associated with them."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "CUDA document is not clear on how memory data changes after CUDA applications throws an exception. For example, a kernel launch(dynamic) encountered an exception (e.g. Warp Out-of-range Address), current kernel launch will be stopped. After this point, will data (e.g. __device__ variables) on device still kept or they are removed along with the exceptions? A concrete example would be like this: CPU launches a kernel The kernel updates the value of __device__ variableA to be 5 and then crashes CPU memcpy the value of variableA from device to host, what is the value the CPU gets in this case, 5 or something else? Can someone show the rationale behind this?",
        "answers": [
            [
                "The behavior is undefined in the event of a CUDA error which corrupts the CUDA context. This type of error is evident because it is \"sticky\", meaning once it occurs, every single CUDA API call will return that error, until the context is destroyed. Non-sticky errors are cleared automatically after they are returned by a cuda API call (with the exception of cudaPeekAtLastError). Any \"crashed kernel\" type error (invalid access, unspecified launch failure, etc.) will be a sticky error. In your example, step 3 would (always) return an API error on the result of the cudaMemcpy call to transfer variableA from device to host, so the results of the cudaMemcpy operation are undefined and unreliable -- it is as if the cudaMemcpy operation also failed in some unspecified way. Since the behavior of a corrupted CUDA context is undefined, there is no definition for the contents of any allocations, or in general the state of the machine after such an error. An example of a non-sticky error might be an attempt to cudaMalloc more data than is available in device memory. Such an operation will return an out-of-memory error, but that error will be cleared after being returned, and subsequent (valid) cuda API calls can complete successfully, without returning an error. A non-sticky error does not corrupt the CUDA context, and the behavior of the cuda context is exactly the same as if the invalid operation had never been requested. This distinction between sticky and non-sticky error is called out in many of the documented error code descriptions, for example: synchronous, non-sticky, non-cuda-context-corrupting: cudaErrorMemoryAllocation = 2 The API call failed because it was unable to allocate enough memory to perform the requested operation. asynchronous, sticky, cuda-context-corrupting: cudaErrorMisalignedAddress = 74 The device encountered a load or store instruction on a memory address which is not aligned. The context cannot be used, so it must be destroyed (and a new one should be created). All existing device memory allocations from this context are invalid and must be reconstructed if the program is to continue using CUDA. Note that cudaDeviceReset() by itself is insufficient to restore a GPU to proper functional behavior. In order to accomplish that, the \"owning\" process must also terminate. See here."
            ]
        ],
        "votes": [
            10.0000001
        ]
    },
    {
        "question": "Note: The question has been updated to address the questions that have been raised in the comments, and to emphasize that the core of the question is about the interdependencies between the Runtime- and Driver API The CUDA runtime libraries (like CUBLAS or CUFFT) are generally using the concept of a \"handle\" that summarizes the state and context of such a library. The usage pattern is quite simple: // Create a handle cublasHandle_t handle; cublasCreate(&amp;handle); // Call some functions, always passing in the handle as the first argument cublasSscal(handle, ...); // When done, destroy the handle cublasDestroy(handle); However, there are many subtle details about how these handles interoperate with Driver- and Runtime contexts and multiple threads and devices. The documentation lists several, scattered details about context handling: The general description of contexts in the CUDA Programming Guide at http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#context The handling of multiple contexts, as described in the CUDA Best Practices Guide at http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#multiple-contexts The context management differences between runtime and driver API, explained at http://docs.nvidia.com/cuda/cuda-driver-api/driver-vs-runtime-api.html The general description of CUBLAS contexts/handles at http://docs.nvidia.com/cuda/cublas/index.html#cublas-context and their thread safety at http://docs.nvidia.com/cuda/cublas/index.html#thread-safety2 However, some of information seems to be not entirely up to date (for example, I think one should use cuCtxSetCurrent instead of cuCtxPushCurrent and cuCtxPopCurrent?), some of it seems to be from a time before the \"Primary Context\" handling was exposed via the driver API, and some parts are oversimplified in that they only show the most simple usage patterns, make only vague or incomplete statements about multithreading, or cannot be applied to the concept of \"handles\" that is used in the runtime libraries. My goal is to implement a runtime library that offers its own \"handle\" type, and that allows usage patterns that are equivalent to the other runtime libraries in terms of context handling and thread safety. For the case that the library can internally be implemented solely using the Runtime API, things may be clear: The context management is solely in the responsibility of the user. If he creates an own driver context, the rules that are stated in the documentation about the Runtime- and Driver context management will apply. Otherwise, the Runtime API functions will take care of the handling of primary contexts. However, there may be the case that a library will internally have to use the Driver API. For example, in order to load PTX files as CUmodule objects, and obtain the CUfunction objects from them. And when the library should - for the user - behave like a Runtime library, but internally has to use the Driver API, some questions arise about how the context handling has to be implemented \"under the hood\". What I have figured out so far is sketched here. (It is \"pseudocode\" in that it omits the error checks and other details, and ... all this is supposed to be implemented in Java, but that should not be relevant here) 1. The \"Handle\" is basically a class/struct containing the following information: class Handle { CUcontext context; boolean usingPrimaryContext; CUdevice device; } 2. When it is created, two cases have to be covered: It can be created when a driver context is current for the calling thread. In this case, it should use this context. Otherwise, it should use the primary context of the current (runtime) device: Handle createHandle() { cuInit(0); // Obtain the current context CUcontext context; cuCtxGetCurrent(&amp;context); CUdevice device; // If there is no context, use the primary context boolean usingPrimaryContext = false; if (context == nullptr) { usingPrimaryContext = true; // Obtain the device that is currently selected via the runtime API int deviceIndex; cudaGetDevice(&amp;deviceIndex); // Obtain the device and its primary context cuDeviceGet(&amp;device, deviceIndex); cuDevicePrimaryCtxRetain(&amp;context, device)); cuCtxSetCurrent(context); } else { cuCtxGetDevice(device); } // Create the actual handle. This might internally allocate // memory or do other things that are specific for the context // for which the handle is created Handle handle = new Handle(device, context, usingPrimaryContext); return handle; } 3. When invoking a kernel of the library, the context of the associated handle is made current for the calling thread: void someLibraryFunction(Handle handle) { cuCtxSetCurrent(handle.context); callMyKernel(...); } Here, one could argue that the caller is responsible for making sure that the required context is current. But if the handle was created for a primary context, then this context will be made current automatically. 4. When the handle is destroyed, this means that cuDevicePrimaryCtxRelease has to be called, but only when the context is a primary context: void destroyHandle(Handle handle) { if (handle.usingPrimaryContext) { cuDevicePrimaryCtxRelease(handle.device); } } From my experiments so far, this seems to expose the same behavior as a CUBLAS handle, for example. But my possibilities for thoroughly testing this are limited, because I only have a single device, and thus cannot test the crucial cases, e.g. of having two contexts, one for each of two devices. So my questions are: Are there any established patterns for implementing such a \"Handle\"? Are there any usage patterns (e.g. with multiple devices and one context per device) that could not be covered with the approach that is sketched above, but would be covered with the \"handle\" implementations of CUBLAS? More generally: Are there any recommendations of how to improve the current \"Handle\" implementation? Rhetorical: Is the source code of the CUBLAS handle handling available somewhere? (I also had a look at the context handling in tensorflow, but I'm not sure whether one can derive recommendations about how to implement handles for a runtime library from that...) (An \"Update\" has been removed here, because it was added in response to the comments, and should no longer be relevant)",
        "answers": [
            [
                "I'm sorry I hadn't noticed this question sooner - as we might have collaborated on this somewhat. Also, it's not quite clear to me whether this question belongs here, on codereview.SX or on programmers.SX, but let's ignore all that. I have now done what you were aiming to do, and possibly more generally. So, I can offer both an example of what to do with \"handles\", and moreover, suggest the prospect of not having to implement this at all. The library is an expanding of cuda-api-wrappers to also cover the Driver API and NVRTC; it is not yet release-grade, but it is in the testing phase, on this branch. Now, to answer your concrete question: Pattern for writing a class surrounding a raw \"handle\" Are there any established patterns for implementing such a \"Handle\"? Yes. If you read: What is the difference between: Handle, Pointer and Reference you'll notice a handle is defined as an \"opaque reference to an object\". It has some similarity to a pointer. A relevant pattern, therefore, is a variation on the PIMPL idiom: In regular PIMPL, you write an implementation class, and the outwards-facing class only holds a pointer to the implementation class and forwards method calls to it. When you have an opaque handle to an opaque object in some third-party library or driver - you use the handle to forward method calls to that implementation. That means, that your outwards-facing class is not a handle, it represents the object to which you have a handle. Generality and flexibility Are there any usage patterns (e.g. with multiple devices and one context per device) that could not be covered with the approach that is sketched above, but would be covered with the \"handle\" implementations of CUBLAS? I'm not sure what exactly CUBLAS does under the hood (and I have almost never used CUBLAS to be honest), but if it were well-designed and implemented, it would create its own context, and try to not to impinge on the rest of your code, i.e. it would alwas do: Push our CUBLAS context onto the top of the stack Do actual work Pop the top of the context stack. Your class doesn't do this. More generally: Are there any recommendations of how to improve the current \"Handle\" implementation? Yes: Use RAII whenever it is possible and relevant. If your creation code allocates a resource (e.g. via the CUDA driver) - the destructor for the object you return should safely release those resources. Allow for both reference-type and value-type use of Handles, i.e. it may be the handle I created, but it may also be a handle I got from somewhere else and isn't my responsibility. This is trivial if you leave it up to the user to release resources, but a bit tricky if you take that responsibility You assume that if there's any current context, that's the one your handle needs to use. Says who? At the very least, let the user pass a context in if they want to. Avoid writing the low-level parts of this on your own unless you really must. You are quite likely to miss some things (the push-and-pop is not the only thing you might be missing), and you're repeating a lot of work that is actually generic and not specific to your application or library. I may be biased here, but you can now use nice, RAII-ish, wrappers for CUDA contexts, streams, modules, devices etc. without even known about raw handles for anything. Rhetorical: Is the source code of the CUBLAS handle handling available somewhere? To the best of my knowledge, NVIDIA hasn't released it."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Using only driver api, for example, I have a profiling with single process below(cuCtxCreate), cuCtxCreate overhead is nearly comparable to 300MB data copy to/from GPU: In CUDA documentation here, it says(for cuDevicePrimaryCtxRetain) Retains the primary context on the device, creating it **if necessary**. Is this an expected behavior for repeated calls to same process from command line(such as running a process 1000 times for explicitly processing 1000 different input images)? Does device need CU_COMPUTEMODE_EXCLUSIVE_PROCESS to work as intended(re-use same context when called multiple times)? For now, upper image is same even if I call that process multiple times. Even without using profiler, timings show around 1second completion time. Edit: According the documentation, primary context is one per device per process. Does this mean there won't be a problem when using multiple threaded single application? What is re-use time limit for primary context? Is 1 second between processes okay or does it have to be miliseconds to keep primary context alive? I'm already caching ptx codes into a file so the only remaining overhead looks like cuMemAlloc(), malloc() and cuMemHostRegister() so re-using latest context from last call to same process would optimize timings good. Edit-2: Documentation says The caller must call cuDevicePrimaryCtxRelease() when done using the context. for cuDevicePrimaryCtxRetain. Is caller here any process? Can I just use retain in first called process and use release on the last called process in a list of hundreds of sequentally called processes? Does system need a reset if last process couldn't be launched and cuDevicePrimaryCtxRelease not called? Edit-3: Is primary context intended for this? process-1: retain (creates) process-2: retain (re-uses) ... process-99: retain (re-uses) process-100: 1 x retain and 100 x release (to decrease counter and unload at last) Everything is compiled for sm_30 and device is Grid K520. GPU was at boost frequency during cuCtxCreate() Project was 64-bit(release mode) compiled on a windows server 2016 OS and CUDA driver installation with windows-7 compatibility(this was the only way working for K520 + windows_server_2016)",
        "answers": [
            [
                "tl;dr: No, it is not. Is cuDevicePrimaryCtxRetain() used for having persistent CUDA context objects between multiple processes? No. It is intended to allow the driver API to bind to a context which a library which has used the runtime API has already lazily created. Nothing more than that. Once upon a time it was necessary to create contexts with the driver API and then have the runtime bind to them. Now, with these APIs, you don't have to do that. You can, for example, see how this is done in Tensorflow here. Does this mean there won't be a problem when using multiple threaded single application? The driver API has been fully thread safe since about CUDA 2.0 Is caller here any process? Can I just use retain in first called process and use release on the last called process in a list of hundreds of sequentally [sic] called processes? No. Contexts are always unique to a given process. They can't be shared between processes in this way Is primary context intended for this? process-1: retain (creates) process-2: retain (re-uses) ... process-99: retain (re-uses) process-100: 1 x retain and 100 x release (to decrease counter and unload at last) No."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I develop software which usually includes both OpenGL and Nvidia CUDA SDK. Recently, I also started to seek ways to optimize run-time memory footprint. I noticed the following (Debug and Release builds differ only by 4-7 Mb): Application startup - Less than 1 Mb total OpenGL 4.5 context creation ( + GLEW loader init) - 45 Mb total CUDA 8.0 context (Driver API) creation 114 Mb total. If I create OpenGL context in \"headless\" mode, the GL context uses 3 Mb less, which probably goes to default frame buffers allocation. That makes sense as the window size is 640x360. So after OpenGL and CUDA context are up, the process already consumes 114 Mb. Now, I don't have deep knowledge regarding OS specific stuff that occurs under the hood during GL and CUDA context creation, but 45 Mb for GL and 68 for CUDA seems a whole lot to me. I know that usually several megabytes goes to system frame buffers, function pointers,(probably a bulk of allocations happens on driver side). But hitting over 100 Mb with just \"empty\" contexts looks too much. I would like to know: Why GL/CUDA context creation consumes such a considerable amount of memory? Are there ways to optimize that? The system setup under test: Windows 10 64bit. NVIDIA GTX 960 GPU (Driver Version:388.31). 8 Gb RAM. Visual Studio 2015, 64bit C++ console project. I measure memory consumption using Visual Studio built-in Diagnostic Tools -&gt; Process Memory section. UPDATE I tried Process Explorer, as suggested by datenwolf. Here is the screenshot of what I got, (my process at the bottom marked with yellow): I would appreciate some explanation on that info. I was always looking at \"Private Bytes\" in \"VS Diagnostic Tools\" window. But here I see also \"Working Set\", \"WS Private\" etc. Which one correctly shows how much memory my process currently uses? 281,320K looks way too much, because as I said above, the process at the startup does nothing, but creates CUDA and OpenGL contexts.",
        "answers": [
            [
                "Partial answer: This is an OS-specific issue; on Linux, CUDA takes 9.3 MB. I'm using CUDA (not OpenGL) on GNU/Linux: CUDA version: 10.2.89 OS distribution: Devuan GNU/Linux Beowulf (~= Debian Buster without systemd) Kernel: Linux 5.2.0 Processor: Intel x86_64 To check how much memory gets used by CUDA when creating a context, I ran the following C program (which also checks what happens after context destruction): #include &lt;stdio.h&gt; #include &lt;cuda.h&gt; #include &lt;malloc.h&gt; #include &lt;stdlib.h&gt; static void print_allocation_stats(const char* s) { printf(\"%s:\\n\", s); printf(\"--------------------------------------------------\\n\"); malloc_stats(); printf(\"--------------------------------------------------\\n\\n\"); } int main() { display_mallinfo(\"Initially\"); int status = cuInit(0); if (status != 0 ) { return EXIT_FAILURE; } print_allocation_stats(\"After CUDA driver initialization\"); int device_id = 0; unsigned flags = 0; CUcontext context_id; status = cuCtxCreate(&amp;context_id, flags, device_id); if (status != CUDA_SUCCESS ) { return EXIT_FAILURE; } print_allocation_stats(\"After context creation\"); status = cuCtxDestroy(context_id); if (status != CUDA_SUCCESS ) { return EXIT_FAILURE; } print_allocation_stats(\"After context destruction\"); return EXIT_SUCCESS; } (note that this uses a glibc-specific function, not in the standard library.) Summarizing the results and snipping irrelevant parts: Point in program Total bytes In-use Max MMAP Regions Max MMAP bytes Initially 135168 1632 0 0 After CUDA driver initialization 552960 439120 2 307200 After context creation 9314304 6858208 8 6643712 After context destruction 7016448 580688 8 6643712 So CUDA starts with 0.5 MB and after allocating a context takes up 9.3 MB (going back down to 7.0 MB on destroying the context). 9 MB is still a lot of memory for not having done anything; but - maybe some of it is all-zeros, or uninitialized, or copy-on-write, in which case it doesn't really take up that much memory. It's possible that memory use improved dramatically over the two years between the driver release with CUDA 8 and with CUDA 10, but I doubt it. So - it looks like your problem is Windows specific. Also, I should mention I did not create an OpenGL context - which is another part of OP's question; so I haven't estimated how much memory that takes. OP brings up the question of whether the sum is greater than its part, i.e. whether a CUDA context would take more memory if an OpenGL context existed as well; I believe this should not be the case, but readers are welcome to try and report..."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "When i start cuda debugging, Nsight return this error: A CUDA context was created on a GPU that is not currently debuggable. Breakpoints will be disabled. Adapter: GeForce GT 720M This is my system and CUDA information. Please note that last version of CUDA and Nsight are installed. I searched this issue and could not find my answer. Thank you so much. Report Information UnixTime Generated 1490538033 OS Information Computer Name DESKTOP-OLFM6NT NetBIOS Name DESKTOP-OLFM6NT OS Name Windows 10 Pro GetVersionEx dwMajorVersion 10 dwMinorVersion 0 dwBuildNumber 14393 dwPlatformId 2 wServicePackMajor 0 wServicePackMinor 0 wSuiteMask 256 wProductType Workstation GetProductInfo 48 GetNativeSystemInfo wProcessorArchitecture x64 dwPageSize 4096 lpMinimumApplicationAddress 65536 lpMaximumApplicationAddress 140737488289791 dwActiveProcessorMask 15 dwNumberOfProcessors 4 dwAllocationGranularity 65536 wProcessorLevel 6 wProcessorRevision 17665 EnumDisplayDevices Display Device DeviceName \\\\.\\DISPLAY1 DeviceString Intel(R) HD Graphics Family StateFlags 5 DeviceID PCI\\VEN_8086&amp;DEV_0A16&amp;SUBSYS_397817AA&amp;REV_09 DeviceKey \\Registry\\Machine\\System\\CurrentControlSet\\Control\\Video\\{A9611CC2-95E1-4DAE-9937-60210AFEDCE0}\\0000 Monitor DeviceName \\\\.\\DISPLAY1\\Monitor0 DeviceString Generic PnP Monitor StateFlags 3 DeviceID MONITOR\\CMN15B6\\{4d36e96e-e325-11ce-bfc1-08002be10318}\\0003 DeviceKey \\Registry\\Machine\\System\\CurrentControlSet\\Control\\Class\\{4d36e96e-e325-11ce-bfc1-08002be10318}\\0003 Display Device DeviceName \\\\.\\DISPLAY2 DeviceString Intel(R) HD Graphics Family StateFlags 1 DeviceID PCI\\VEN_8086&amp;DEV_0A16&amp;SUBSYS_397817AA&amp;REV_09 DeviceKey \\Registry\\Machine\\System\\CurrentControlSet\\Control\\Video\\{A9611CC2-95E1-4DAE-9937-60210AFEDCE0}\\0001 Monitor DeviceName \\\\.\\DISPLAY2\\Monitor0 DeviceString Generic PnP Monitor StateFlags 3 DeviceID MONITOR\\SAM04FD\\{4d36e96e-e325-11ce-bfc1-08002be10318}\\0004 DeviceKey \\Registry\\Machine\\System\\CurrentControlSet\\Control\\Class\\{4d36e96e-e325-11ce-bfc1-08002be10318}\\0004 Display Device DeviceName \\\\.\\DISPLAY3 DeviceString Intel(R) HD Graphics Family StateFlags 0 DeviceID PCI\\VEN_8086&amp;DEV_0A16&amp;SUBSYS_397817AA&amp;REV_09 DeviceKey \\Registry\\Machine\\System\\CurrentControlSet\\Control\\Video\\{A9611CC2-95E1-4DAE-9937-60210AFEDCE0}\\0002 GlobalMemoryStatusEx dwMemoryLoad 34 ullTotalPhys 8486227968 ullAvailPhys 5588660224 ullTotalPageFile 13854937088 ullAvailPageFile 10756182016 ullTotalVirtual 140737488224256 ullAvailVirtual 140737442308096 Processor Information 0 Name Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz Clock speed (MHz) 2394 1 Name Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz Clock speed (MHz) 2394 2 Name Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz Clock speed (MHz) 2394 3 Name Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz Clock speed (MHz) 2394 NvAPI IsMSHybridGraphics True DisplayDriverVersion Driver Version 37609 Changelist 0 BuildBranchString r376_06 Default AdapterString GeForce GT 720M DisplayDriverCompileType Release NvDebugApi WDDM Devices GPU Name GeForce GT 720M Architecture Fermi Architecture Number 208 Architecture Implementation 7 Architecture Revision 162 Number of GPCs 1 Number of TPCs 2 Number of SMs 2 Warps per SM 48 Lanes per warp 32 Register file size 32768 Max CTAs per SM 8 Max size of shared memory per CTA (bytes) 49152 SM Revision 131073 Number of FB PAs 6 Number of LTs per LTC 2 RmGpuId 1024 RM Devices CUDA CUDA Device Name GeForce GT 720M Driver WDDM DeviceIndex 0 GPU Family GF117 RmGpuId 1024 Compute Major 2 Compute Minor 1 MAX_THREADS_PER_BLOCK 1024 MAX_BLOCK_DIM_X 1024 MAX_BLOCK_DIM_Y 1024 MAX_BLOCK_DIM_Z 64 MAX_GRID_DIM_X 65535 MAX_GRID_DIM_Y 65535 MAX_GRID_DIM_Z 65535 MAX_SHARED_MEMORY_PER_BLOCK 49152 TOTAL_CONSTANT_MEMORY 65536 WARP_SIZE 32 MAX_PITCH 2147483647 MAX_REGISTERS_PER_BLOCK 32768 CLOCK_RATE 1550000 TEXTURE_ALIGNMENT 512 GPU_OVERLAP 1 MULTIPROCESSOR_COUNT 2 KERNEL_EXEC_TIMEOUT 0 INTEGRATED 0 CAN_MAP_HOST_MEMORY 1 COMPUTE_MODE 0 MAXIMUM_TEXTURE1D_WIDTH 65536 MAXIMUM_TEXTURE2D_WIDTH 65536 MAXIMUM_TEXTURE2D_HEIGHT 65535 MAXIMUM_TEXTURE3D_WIDTH 2048 MAXIMUM_TEXTURE3D_HEIGHT 2048 MAXIMUM_TEXTURE3D_DEPTH 2048 MAXIMUM_TEXTURE2D_LAYERED_WIDTH 16384 MAXIMUM_TEXTURE2D_LAYERED_HEIGHT 16384 MAXIMUM_TEXTURE2D_LAYERED_LAYERS 2048 SURFACE_ALIGNMENT 512 CONCURRENT_KERNELS 1 ECC_ENABLED 0 PCI_BUS_ID 4 PCI_DEVICE_ID 0 TCC_DRIVER 0 MEMORY_CLOCK_RATE 900000 GLOBAL_MEMORY_BUS_WIDTH 64 L2_CACHE_SIZE 131072 MAX_THREADS_PER_MULTIPROCESSOR 1536 ASYNC_ENGINE_COUNT 1 UNIFIED_ADDRESSING 1 MAXIMUM_TEXTURE1D_LAYERED_WIDTH 16384 MAXIMUM_TEXTURE1D_LAYERED_LAYERS 2048 CAN_TEX2D_GATHER 1 MAXIMUM_TEXTURE2D_GATHER_WIDTH 16384 MAXIMUM_TEXTURE2D_GATHER_HEIGHT 16384 MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE 0 MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE 0 MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE 0 PCI_DOMAIN_ID 0 TEXTURE_PITCH_ALIGNMENT 32 MAXIMUM_TEXTURECUBEMAP_WIDTH 16384 MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH 16384 MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS 2046 MAXIMUM_SURFACE1D_WIDTH 65536 MAXIMUM_SURFACE2D_WIDTH 65536 MAXIMUM_SURFACE2D_HEIGHT 32768 MAXIMUM_SURFACE3D_WIDTH 65536 MAXIMUM_SURFACE3D_HEIGHT 32768 MAXIMUM_SURFACE3D_DEPTH 2048 MAXIMUM_SURFACE1D_LAYERED_WIDTH 65536 MAXIMUM_SURFACE1D_LAYERED_LAYERS 2048 MAXIMUM_SURFACE2D_LAYERED_WIDTH 65536 MAXIMUM_SURFACE2D_LAYERED_HEIGHT 32768 MAXIMUM_SURFACE2D_LAYERED_LAYERS 2048 MAXIMUM_SURFACECUBEMAP_WIDTH 32768 MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH 32768 MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS 2046 MAXIMUM_TEXTURE1D_LINEAR_WIDTH 134217728 MAXIMUM_TEXTURE2D_LINEAR_WIDTH 65000 MAXIMUM_TEXTURE2D_LINEAR_HEIGHT 65000 MAXIMUM_TEXTURE2D_LINEAR_PITCH 1048544 MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH 16384 MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT 16384 MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH 16384 STREAM_PRIORITIES_SUPPORTED 0 GLOBAL_L1_CACHE_SUPPORTED 1 LOCAL_L1_CACHE_SUPPORTED 1 MAX_SHARED_MEMORY_PER_MULTIPROCESSOR 49152 MAX_REGISTERS_PER_MULTIPROCESSOR 32768 MANAGED_MEMORY 0 MULTI_GPU_BOARD 0 MULTI_GPU_BOARD_GROUP_ID 0 HOST_NATIVE_ATOMIC_SUPPORTED 0 SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO 12 PAGEABLE_MEMORY_ACCESS 0 CONCURRENT_MANAGED_ACCESS 0 COMPUTE_PREEMPTION_SUPPORTED 0 CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM 0 DISPLAY_NAME GeForce GT 720M COMPUTE_CAPABILITY_MAJOR 2 COMPUTE_CAPABILITY_MINOR 1 TOTAL_MEMORY 2147483648 RAM_TYPE 7 RAM_LOCATION 1 GPU_PCI_DEVICE_ID 289411294 GPU_PCI_SUB_SYSTEM_ID 939530154 GPU_PCI_REVISION_ID 161 GPU_PCI_EXT_DEVICE_ID 4416 GPU_PCI_EXT_GEN 1 GPU_PCI_EXT_GPU_GEN 1 GPU_PCI_EXT_GPU_LINK_RATE 5000 GPU_PCI_EXT_GPU_LINK_WIDTH 8 GPU_PCI_EXT_DOWNSTREAM_LINK_RATE 5000 GPU_PCI_EXT_DOWNSTREAM_LINK_WIDTH 4",
        "answers": [
            [
                "Your GT 720m is a compute capability 2.1 device (see here). Attempting to debug CUDA code (e.g. set breakpoints) on a GPU that is also supporting (hosting) a display requires a compute capability 3.5 or higher device, to support preemption. Your device does not meet that requirement, so because your GPU is hosting your laptop display, it cannot be used to set breakpoints in CUDA code. Also note that the latest version of Nsight VSE (5.2 at this time) has officially dropped support for Fermi GPUs (yours is a Fermi GPU): Note: Fermi family GPUs, and older families, are no longer supported with Nsight\u2122 Visual Studio Edition 5.2 or better."
            ],
            [
                "Maybe try falling back to an older version of Nsight. I recently ran into this problem and installing an older version helped."
            ]
        ],
        "votes": [
            5.0000001,
            1e-07
        ]
    },
    {
        "question": "Question 1) When I call CUDA driver API, usually I need first push the context (which represents a GPU runtime) to current thread. For normal cuMalloc, the memory will be allocated on that GPU specified by the context. But if I try to call cuMallocManaged to create unified memory, do I still need to push a GPU context? Question 2) Say I have 2 GPUs, each has 1 GB DRAM. So can I allocate unified memory of 2 GB? with each GPU holds half of it?",
        "answers": [
            [
                "Follow established driver API programming methods. Explicitly establish a CUDA context. No, this is not how managed memory works. A managed allocation is visible, in its entirety, to all GPUs in the system. This is true whether we are talking about a pre-pascal UM regime or a pure-pascal UM regime, although the specific method of visibility varies. Refer to the programming guide sections on UM with multi-GPU."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Can anyone please explain or refer me some good source about what is a CUDA context? I searched CUDA developer guide and I was not satisfied with it. Any explanation or help will be great.",
        "answers": [
            [
                "The cuda API exposes features of a stateful library: two consecutive calls relate one-another. In short, the context is its state. The runtime API is a wrapper/helper of the driver API. You can see in the driver API that the context is explicitly made available, and you can have a stack of contexts for convenience. There is one specific context which is shared between driver and runtime API (See primary context)). The context holds all the management data to control and use the device. For instance, it holds the list of allocated memory, the loaded modules that contain device code, the mapping between CPU and GPU memory for zero copy, etc. Finally, note that this post is more from experience than documentation-proofed."
            ],
            [
                "essentially, a data structure that holds information relevant to mantaining a consistent state between the calls that you make, e.g. (open) (execute) (close) This is so that the functions that you invoke can send the signals in the right direction even if you don't specifically tell them what that direction is."
            ]
        ],
        "votes": [
            24.0000001,
            4.0000001
        ]
    },
    {
        "question": "When i start cuda debugging, Nsight return this error: A CUDA context was created on a GPU that is not currently debuggable. Breakpoints will be disabled. Adapter: GeForce GT 720M This is my system and CUDA information. Please note that last version of CUDA and Nsight are installed. I searched this issue and could not find my answer. Thank you so much. Report Information UnixTime Generated 1490538033 OS Information Computer Name DESKTOP-OLFM6NT NetBIOS Name DESKTOP-OLFM6NT OS Name Windows 10 Pro GetVersionEx dwMajorVersion 10 dwMinorVersion 0 dwBuildNumber 14393 dwPlatformId 2 wServicePackMajor 0 wServicePackMinor 0 wSuiteMask 256 wProductType Workstation GetProductInfo 48 GetNativeSystemInfo wProcessorArchitecture x64 dwPageSize 4096 lpMinimumApplicationAddress 65536 lpMaximumApplicationAddress 140737488289791 dwActiveProcessorMask 15 dwNumberOfProcessors 4 dwAllocationGranularity 65536 wProcessorLevel 6 wProcessorRevision 17665 EnumDisplayDevices Display Device DeviceName \\\\.\\DISPLAY1 DeviceString Intel(R) HD Graphics Family StateFlags 5 DeviceID PCI\\VEN_8086&amp;DEV_0A16&amp;SUBSYS_397817AA&amp;REV_09 DeviceKey \\Registry\\Machine\\System\\CurrentControlSet\\Control\\Video\\{A9611CC2-95E1-4DAE-9937-60210AFEDCE0}\\0000 Monitor DeviceName \\\\.\\DISPLAY1\\Monitor0 DeviceString Generic PnP Monitor StateFlags 3 DeviceID MONITOR\\CMN15B6\\{4d36e96e-e325-11ce-bfc1-08002be10318}\\0003 DeviceKey \\Registry\\Machine\\System\\CurrentControlSet\\Control\\Class\\{4d36e96e-e325-11ce-bfc1-08002be10318}\\0003 Display Device DeviceName \\\\.\\DISPLAY2 DeviceString Intel(R) HD Graphics Family StateFlags 1 DeviceID PCI\\VEN_8086&amp;DEV_0A16&amp;SUBSYS_397817AA&amp;REV_09 DeviceKey \\Registry\\Machine\\System\\CurrentControlSet\\Control\\Video\\{A9611CC2-95E1-4DAE-9937-60210AFEDCE0}\\0001 Monitor DeviceName \\\\.\\DISPLAY2\\Monitor0 DeviceString Generic PnP Monitor StateFlags 3 DeviceID MONITOR\\SAM04FD\\{4d36e96e-e325-11ce-bfc1-08002be10318}\\0004 DeviceKey \\Registry\\Machine\\System\\CurrentControlSet\\Control\\Class\\{4d36e96e-e325-11ce-bfc1-08002be10318}\\0004 Display Device DeviceName \\\\.\\DISPLAY3 DeviceString Intel(R) HD Graphics Family StateFlags 0 DeviceID PCI\\VEN_8086&amp;DEV_0A16&amp;SUBSYS_397817AA&amp;REV_09 DeviceKey \\Registry\\Machine\\System\\CurrentControlSet\\Control\\Video\\{A9611CC2-95E1-4DAE-9937-60210AFEDCE0}\\0002 GlobalMemoryStatusEx dwMemoryLoad 34 ullTotalPhys 8486227968 ullAvailPhys 5588660224 ullTotalPageFile 13854937088 ullAvailPageFile 10756182016 ullTotalVirtual 140737488224256 ullAvailVirtual 140737442308096 Processor Information 0 Name Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz Clock speed (MHz) 2394 1 Name Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz Clock speed (MHz) 2394 2 Name Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz Clock speed (MHz) 2394 3 Name Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz Clock speed (MHz) 2394 NvAPI IsMSHybridGraphics True DisplayDriverVersion Driver Version 37609 Changelist 0 BuildBranchString r376_06 Default AdapterString GeForce GT 720M DisplayDriverCompileType Release NvDebugApi WDDM Devices GPU Name GeForce GT 720M Architecture Fermi Architecture Number 208 Architecture Implementation 7 Architecture Revision 162 Number of GPCs 1 Number of TPCs 2 Number of SMs 2 Warps per SM 48 Lanes per warp 32 Register file size 32768 Max CTAs per SM 8 Max size of shared memory per CTA (bytes) 49152 SM Revision 131073 Number of FB PAs 6 Number of LTs per LTC 2 RmGpuId 1024 RM Devices CUDA CUDA Device Name GeForce GT 720M Driver WDDM DeviceIndex 0 GPU Family GF117 RmGpuId 1024 Compute Major 2 Compute Minor 1 MAX_THREADS_PER_BLOCK 1024 MAX_BLOCK_DIM_X 1024 MAX_BLOCK_DIM_Y 1024 MAX_BLOCK_DIM_Z 64 MAX_GRID_DIM_X 65535 MAX_GRID_DIM_Y 65535 MAX_GRID_DIM_Z 65535 MAX_SHARED_MEMORY_PER_BLOCK 49152 TOTAL_CONSTANT_MEMORY 65536 WARP_SIZE 32 MAX_PITCH 2147483647 MAX_REGISTERS_PER_BLOCK 32768 CLOCK_RATE 1550000 TEXTURE_ALIGNMENT 512 GPU_OVERLAP 1 MULTIPROCESSOR_COUNT 2 KERNEL_EXEC_TIMEOUT 0 INTEGRATED 0 CAN_MAP_HOST_MEMORY 1 COMPUTE_MODE 0 MAXIMUM_TEXTURE1D_WIDTH 65536 MAXIMUM_TEXTURE2D_WIDTH 65536 MAXIMUM_TEXTURE2D_HEIGHT 65535 MAXIMUM_TEXTURE3D_WIDTH 2048 MAXIMUM_TEXTURE3D_HEIGHT 2048 MAXIMUM_TEXTURE3D_DEPTH 2048 MAXIMUM_TEXTURE2D_LAYERED_WIDTH 16384 MAXIMUM_TEXTURE2D_LAYERED_HEIGHT 16384 MAXIMUM_TEXTURE2D_LAYERED_LAYERS 2048 SURFACE_ALIGNMENT 512 CONCURRENT_KERNELS 1 ECC_ENABLED 0 PCI_BUS_ID 4 PCI_DEVICE_ID 0 TCC_DRIVER 0 MEMORY_CLOCK_RATE 900000 GLOBAL_MEMORY_BUS_WIDTH 64 L2_CACHE_SIZE 131072 MAX_THREADS_PER_MULTIPROCESSOR 1536 ASYNC_ENGINE_COUNT 1 UNIFIED_ADDRESSING 1 MAXIMUM_TEXTURE1D_LAYERED_WIDTH 16384 MAXIMUM_TEXTURE1D_LAYERED_LAYERS 2048 CAN_TEX2D_GATHER 1 MAXIMUM_TEXTURE2D_GATHER_WIDTH 16384 MAXIMUM_TEXTURE2D_GATHER_HEIGHT 16384 MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE 0 MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE 0 MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE 0 PCI_DOMAIN_ID 0 TEXTURE_PITCH_ALIGNMENT 32 MAXIMUM_TEXTURECUBEMAP_WIDTH 16384 MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH 16384 MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS 2046 MAXIMUM_SURFACE1D_WIDTH 65536 MAXIMUM_SURFACE2D_WIDTH 65536 MAXIMUM_SURFACE2D_HEIGHT 32768 MAXIMUM_SURFACE3D_WIDTH 65536 MAXIMUM_SURFACE3D_HEIGHT 32768 MAXIMUM_SURFACE3D_DEPTH 2048 MAXIMUM_SURFACE1D_LAYERED_WIDTH 65536 MAXIMUM_SURFACE1D_LAYERED_LAYERS 2048 MAXIMUM_SURFACE2D_LAYERED_WIDTH 65536 MAXIMUM_SURFACE2D_LAYERED_HEIGHT 32768 MAXIMUM_SURFACE2D_LAYERED_LAYERS 2048 MAXIMUM_SURFACECUBEMAP_WIDTH 32768 MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH 32768 MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS 2046 MAXIMUM_TEXTURE1D_LINEAR_WIDTH 134217728 MAXIMUM_TEXTURE2D_LINEAR_WIDTH 65000 MAXIMUM_TEXTURE2D_LINEAR_HEIGHT 65000 MAXIMUM_TEXTURE2D_LINEAR_PITCH 1048544 MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH 16384 MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT 16384 MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH 16384 STREAM_PRIORITIES_SUPPORTED 0 GLOBAL_L1_CACHE_SUPPORTED 1 LOCAL_L1_CACHE_SUPPORTED 1 MAX_SHARED_MEMORY_PER_MULTIPROCESSOR 49152 MAX_REGISTERS_PER_MULTIPROCESSOR 32768 MANAGED_MEMORY 0 MULTI_GPU_BOARD 0 MULTI_GPU_BOARD_GROUP_ID 0 HOST_NATIVE_ATOMIC_SUPPORTED 0 SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO 12 PAGEABLE_MEMORY_ACCESS 0 CONCURRENT_MANAGED_ACCESS 0 COMPUTE_PREEMPTION_SUPPORTED 0 CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM 0 DISPLAY_NAME GeForce GT 720M COMPUTE_CAPABILITY_MAJOR 2 COMPUTE_CAPABILITY_MINOR 1 TOTAL_MEMORY 2147483648 RAM_TYPE 7 RAM_LOCATION 1 GPU_PCI_DEVICE_ID 289411294 GPU_PCI_SUB_SYSTEM_ID 939530154 GPU_PCI_REVISION_ID 161 GPU_PCI_EXT_DEVICE_ID 4416 GPU_PCI_EXT_GEN 1 GPU_PCI_EXT_GPU_GEN 1 GPU_PCI_EXT_GPU_LINK_RATE 5000 GPU_PCI_EXT_GPU_LINK_WIDTH 8 GPU_PCI_EXT_DOWNSTREAM_LINK_RATE 5000 GPU_PCI_EXT_DOWNSTREAM_LINK_WIDTH 4",
        "answers": [
            [
                "Your GT 720m is a compute capability 2.1 device (see here). Attempting to debug CUDA code (e.g. set breakpoints) on a GPU that is also supporting (hosting) a display requires a compute capability 3.5 or higher device, to support preemption. Your device does not meet that requirement, so because your GPU is hosting your laptop display, it cannot be used to set breakpoints in CUDA code. Also note that the latest version of Nsight VSE (5.2 at this time) has officially dropped support for Fermi GPUs (yours is a Fermi GPU): Note: Fermi family GPUs, and older families, are no longer supported with Nsight\u2122 Visual Studio Edition 5.2 or better."
            ],
            [
                "Maybe try falling back to an older version of Nsight. I recently ran into this problem and installing an older version helped."
            ]
        ],
        "votes": [
            5.0000001,
            1e-07
        ]
    },
    {
        "question": "Is there any way to discover at a given time how many process' are running on the GPU and possibly manage them (yield, resume, kill ... when necessary). What i want to do is while I run different programs, monitor each process activities on the GPU. I didn't see how that can be done. I saw that GPU context is the CPU equivalent of process however all the explanations I read on that was about influencing it inside a single program. Again, what I want is to trace all activity on the GPU (like with the nvidia-smi tool but with my own means). Can anyone help me with that? Is that even possible?",
        "answers": [
            [
                "A GPU Context is a driver level construct, the GPU itself doesn't know anything about contexts. All it knows is that it has a command queue, sent to it from the driver, that it will run through and execute. The various API's the GPU's support expose support for issuing commands, I don't think there are any commands for cancelling commands although there are commands for inspecting whether a command has been completed. Inspecting or manipulating commands in other contexts from my perspective would be a betrayal of the concept of a context. RE inspecting what the GPU is doing, you could do this at the driver level by tracking the driver command queue, you are issuing and the hardware command queue sampling state, it is a lot of work which is why this task is usually left to GPU vendors to implement."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am working with Cuda driver api, now i have a problem about Cuda texture object creation, it always return CUDA_ERROR_INVALUD_VALUE. I found function cuTexObjectCreate is wrapped by macro : #if __CUDA_API_VERSION &gt;= 5000 ...function decl... #endif And my Cuda context created by cuCtxCreate_v2 always return api version 3020. Does that means I should create a high version context? and which function should I call? Or it's something else? That's the sample code: struct CudaDriverTest { CUdevice m_device = 0; CUcontext m_primaryContext = nullptr; CUcontext m_context = nullptr; CUarray m_array = nullptr; CUtexObject m_texture = 0; CUdeviceptr m_output = 0; CudaDriverTest(size_t w, size_t h, float* data) : m_image(w, h, QImage::Format_Grayscale8) { // begin cuda driver api staff HANDLE_ERROR(cuInit(0)); int deviceCount = 0; HANDLE_ERROR(cuDeviceGetCount(&amp;deviceCount)); assert(deviceCount == 1); HANDLE_ERROR(cuDeviceGet(&amp;m_device, 0)); char name[256]; HANDLE_ERROR(cuDeviceGetName(name, 256, m_device)); std::cout &lt;&lt; \"device name:\" &lt;&lt; name &lt;&lt; std::endl; int major = 0; int minor = 0; HANDLE_ERROR(cuDeviceComputeCapability(&amp;major, &amp;minor, m_device)); std::cout &lt;&lt; \"major compute capability : \" &lt;&lt; major &lt;&lt; \", minor compute capability : \" &lt;&lt; minor &lt;&lt; std::endl; HANDLE_ERROR(cuCtxCreate(&amp;m_context, CU_CTX_SCHED_AUTO, m_device)); std::uint32_t version; HANDLE_ERROR(cuCtxGetApiVersion(m_context, &amp;version)); std::cout &lt;&lt; \"context api version : \" &lt;&lt; version &lt;&lt; std::endl; // array CUDA_ARRAY_DESCRIPTOR arrDesc; memset(&amp;arrDesc, 0, sizeof(arrDesc)); arrDesc.Format = CUarray_format::CU_AD_FORMAT_FLOAT; arrDesc.NumChannels = 1; arrDesc.Width = w; arrDesc.Height = h; HANDLE_ERROR(cuArrayCreate(&amp;m_array, &amp;arrDesc)); CUDA_MEMCPY2D cpy2d; memset(&amp;cpy2d, 0, sizeof(cpy2d)); cpy2d.srcMemoryType = CUmemorytype::CU_MEMORYTYPE_HOST; cpy2d.srcHost = data; cpy2d.srcPitch = w * sizeof(float); cpy2d.dstMemoryType = CUmemorytype::CU_MEMORYTYPE_ARRAY; cpy2d.dstArray = m_array; cpy2d.dstPitch = w * sizeof(float); cpy2d.WidthInBytes = w * sizeof(float); cpy2d.Height = h; HANDLE_ERROR(cuMemcpy2D(&amp;cpy2d)); // texture object CUDA_RESOURCE_DESC resDesc; memset(&amp;resDesc, 0, sizeof(resDesc)); resDesc.resType = CUresourcetype::CU_RESOURCE_TYPE_ARRAY; resDesc.res.array.hArray = m_array; CUDA_TEXTURE_DESC texDesc; memset(&amp;texDesc, 0, sizeof(texDesc)); texDesc.addressMode[0] = CUaddress_mode::CU_TR_ADDRESS_MODE_WRAP; texDesc.addressMode[1] = CUaddress_mode::CU_TR_ADDRESS_MODE_WRAP; texDesc.addressMode[2] = CUaddress_mode::CU_TR_ADDRESS_MODE_WRAP; texDesc.filterMode = CUfilter_mode::CU_TR_FILTER_MODE_LINEAR; texDesc.flags = CU_TRSF_READ_AS_INTEGER; CUDA_RESOURCE_VIEW_DESC viewDesc; memset(&amp;viewDesc, 0, sizeof(viewDesc)); viewDesc.width = w; viewDesc.height = h; viewDesc.format = CUresourceViewFormat::CU_RES_VIEW_FORMAT_FLOAT_1X32; HANDLE_ERROR(cuTexObjectCreate(&amp;m_texture, &amp;resDesc, &amp;texDesc, &amp;viewDesc)); // output HANDLE_ERROR(cuMemAlloc(&amp;m_output, w * h * sizeof(float))); } } PS: working environment: GTX960;Cuda7.5;windows7-x64;visual studio 2013;",
        "answers": [
            [
                "It's my fault, I did not correctly ZeroMemory the the parameters. PS. Thanks @talonmies for editing the question."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I thought I had the grasp of this but apparently I do not:) I need to perform parallel H.264 stream encoding with NVENC from frames that are not in any of the formats accepted by the encoder so I have a following code pipeline: A callback informing that a new frame has arrived is called I copy the frame to CUDA memory and perform the needed color space conversions (only the first cuMemcpy is synchronous, so I can return from the callback, all pending operations are pushed in a dedicated stream) I push an event onto the stream and have another thread waiting for it, as soon as it is set I take the CUDA memory pointer with the frame in the correct color space and feed it to the decoder For some reason I had the assumption that I need a dedicated context for each thread if I perform this pipeline in parallel threads. The code was slow and after some reading I understood that the context switching is actually expensive, and then I actually came to the conclusion that it makes no sense since in a context owns the whole GPU so I lock out any parallel processing from other transcoder threads. Question 1: In this scenario am I good with using a single context and an explicit stream created on this context for each thread that performs the mentioned pipeline? Question 2: Can someone enlighten me on what is the sole purpose of the CUDA device context? I assume it makes sense in a multiple GPU scenario, but are there any cases where I would want to create multiple contexts for one GPU?",
        "answers": [
            [
                "Question 1: In this scenario am I good with using a single context and an explicit stream created on this context for each thread that performs the mentioned pipeline? You should be fine with a single context. Question 2: Can someone enlighten me on what is the sole purpose of the CUDA device context? I assume it makes sense in a multiple GPU scenario, but are there any cases where I would want to create multiple contexts for one GPU? The CUDA device context is discussed in the programming guide. It represents all of the state (memory map, allocations, kernel definitions, and other state-related information) associated with a particular process (i.e. associated with that particular process' use of a GPU). Separate processes will normally have separate contexts (as will separate devices), as these processes have independent GPU usage and independent memory maps. If you have multi-process usage of a GPU, you will normally create multiple contexts on that GPU. As you've discovered, it's possible to create multiple contexts from a single process, but not usually necessary. And yes, when you have multiple contexts, kernels launched in those contexts will require context switching to go from one kernel in one context to another kernel in another context. Those kernels cannot run concurrently. CUDA runtime API usage manages contexts for you. You normally don't explicitly interact with a CUDA context when using the runtime API. However, in driver API usage, the context is explicitly created and managed."
            ],
            [
                "Obviously a few years have passed, but NVENC/NVDEC now appear to have CUstream support as of version 9.1 (circa September 2019) of the video codec SDK: https://developer.nvidia.com/nvidia-video-codec-sdk/download NEW to 9.1- Encode: CUStream support in NVENC for enhanced parallelism between CUDA pre-processing and NVENC encoding I'm super new to CUDA, but my basic understanding is that CUcontexts allow multiple processes to use the GPU (by doing context swaps that interrupt each other's work), while CUstreams allow for a coordinated sharing of the GPU's resources from within a single process."
            ]
        ],
        "votes": [
            17.0000001,
            2.0000001
        ]
    },
    {
        "question": "while reading some papers related to GPU computing, i stuck in understanding theses two terms GPU Context,and GPU hardware channel bellow is brief mention to them ,but i can't understand what they mean, Command: The GPU operates using the architecturespecific commands. Each GPU context is assigned with a FIFO queue to which the program running on the CPU submits the commands. Computations and data transfers on the GPU are triggered only when the corresponding commands are dispatched by the GPU itself. Channel: Each GPU context is assigned with a GPU hardware channel within which command dispatching is managed. Fermi does not permit multiple channels to access the same GPU functional unit simultaneously, but allow them to coexist being switched automatically in hardware. so is there is a clear and simple explanation for that.",
        "answers": [
            [
                "A GPU context is described here. It represents all the state (data, variables, conditions, etc.) that are collectively required and instantiated to perform certain tasks (e.g. CUDA compute, graphics, H.264 encode, etc). A CUDA context is instantiated to perform CUDA compute activities on the GPU, either implicitly by the CUDA runtime API, or explicitly by the CUDA driver API. A Command is simply a set of data, and instructions to be performed on that data. For example a command could be issued to the GPU to launch a kernel, or to move a graphical window from one place to the other on the desktop. A channel represents a communication path between host (CPU) and the GPU. In modern GPUs this makes use of PCI Express, and represents state and buffers in both host and device, that are exchanged over PCI express, to issue commands to, and provide other data to, the GPU, as well as to inform the CPU of GPU activity. For the most part, using the CUDA runtime API, it's not necessary to be familiar with these concepts, as they are all abstracted (hidden) underneath the CUDA runtime API."
            ]
        ],
        "votes": [
            10.0000001
        ]
    },
    {
        "question": "I have a MATLAB mex library that loads a problem specific cubin file at runtime. This mex function gets called a few hundred times by MATLAB. Is the kernel reloaded each time by CUDA when I call cuModuleLoad? Or is it somehow cached? If not, is there a way to persist the loaded modules in between? I'm not currently calling cuModuleUnload. It seems like the CUDA context is created only once for the MATLAB process since only the first call to the library is slow. Subsequent matlab function calls to the mex library are fast. So I guess I can assume that the same CUDA context is being reused.",
        "answers": [
            [
                "The short answer is yes, you can reuse them. Global / static variables or static members in C++ in MEX file are only created once when MEX file is dynamically loaded, as long as they do not rely on Matlab memory manager (mxMalloc, mxCreateNumericArray, etc). They are destroyed when Matlab exits or MEX files are unloaded by clear mex. Also it is possible to protect MEX file with MEX API: mexLock to prevent unexpected unloading, mexAtExit to register destructor callback, etc."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "We have a multi-GPU framework (on windows) where one can specifiy 'jobs' (which specify also on which GPU they shall be done) which are then executed on a specific GPU. Currently, we have the approach that on startup of the framework we create one 'Worker-Thread' for each GPU which then waits for jobs to be processed. Specifically, we use the 'GPUWorker' class from https://devtalk.nvidia.com/search/more/sitecommentsearch/GPUworker/ It works nicely so far, but has some serious performance-related disadvantages: In our frameowrk, a specific GPU is locked for the whole time of a 'job', even if the GPU is actually used only in 50 % of the time of the job. Note the jobs have a very coarse granurality, e.g. 'do optical flow calculation', which can take e.g. 50 - 100 milliseconds. One can not specific 'asynchronous' jobs (e.g. an aysnchronous host-device copy) which do not lock the GPU So I am now thinking about 'better' strategies for that problem. My idea goes as following: For each new job which is 'launched', I create a new 'temporary' CPU thread. The CPU thread then sets the device number (via 'cudaSetDevice') of the GPU on which the work shall be done. I suppose at this time also (transparantly for me' a Cuda context is created. After seeting the correct device, the 'doWork' function of the job is executed by the CPU thread. Dependent on whether the job shall be done synchronous or asynchronous, a 'join' is done (waiting for the CPU thread for completion) or not. I have now several questions: Is that a 'good' strategy, or does somebody know of a better way how to handle this ? Of course it must be a thread-safe strategy. In my proposed strategy, what is the typical overhead (in milliseconds) of the creation of the new CPU thread and the (hidden) creation of the Cuda context) ? Furthermore, if e.g. the creation of the Cuda context is signficiant, is there a way (e.g. using the cuda device api and some sort of 'context migration') to reduce this overhead ?",
        "answers": [
            [
                "Your first approach sounds more promising than the alternative that you are considering. Creating CPU threads and initializing CUDA contexts is quite expensive, and it's difficult-to-impossible for you to make that operation faster. NVIDIA deliberately front-loads a lot of operations into the context creation process, so you don't get unexpected delays or failures due to a resource allocation failure. Your best bet is to invest in asynchrony. Without CPU/GPU concurrency, you are definitely leaving performance on the table because you are not hiding the CPU overhead that's built into the CUDA driver."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "If my host program [exit]/[segfault]/[is killed] what are the corresponding behaviors regarding the CUDA context destruction and corresponding allocated resources ? By \"behavior\" I mean automatic GPU driver side mechanism if I never explicitly call cudaDeviceReset in my program. I'm working under Linux and I've seen my GPU memory usage growing incrementally over several days (or even one single day) while I was launching CUDA code that was some times terminated through (CTRL-C) or simply segfaulting (on the host side) ...",
        "answers": [
            [
                "Perhaps, if you correctly manage exceptions, you can use atexit()? An exemple of using atexit() follows: void ExitFunction() { cudaDeviceReset(); } int main(...) { atexit(ExitFunction); ... return 0; }"
            ],
            [
                "All allocations on the device are encapsulated in a CUDA context. When using the runtime API, CUDA contexts are created automatically \"under the hood\". When an application exits, regardless of how, the context (and any associated allocations that are encapsulated within) should be destroyed. If you are experiencing \"small\" changes in free memory after various events, this may be simply the fact that the driver is allocating additional space for it's own usage for internal housekeeping tasks. (The driver is akin to an operating system for the device.) If you are experiencing \"large\" changes in free memory after various application exit events, then I would say something is wrong, but since your question provides no details or indication of how to reproduce it, I'm not sure what else can be said. If you want to force a driver \"reset\" externally, you could try unloading the driver via: sudo rmmod nvidia sudo nvidia-smi -a And your free memory should be back to \"normal\"."
            ]
        ],
        "votes": [
            3.0000001,
            3.0000001
        ]
    },
    {
        "question": "In my application I have some part of the code that works as follows main.cpp int main() { //First dimension usually small (1-10) //Second dimension (100 - 1500) //Third dimension (10000 - 1000000) vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; someInfo; Object someObject(...); //Host class for (int i = 0; i &lt; N; i++) someObject.functionA(&amp;(someInfo[i])); } Object.cpp void SomeObject::functionB(vector&lt;vector&lt;double&gt;&gt; *someInfo) { #define GPU 1 #if GPU == 1 //GPU COMPUTING computeOnGPU(someInfo, aConstValue, aSecondConstValue); #else //CPU COMPUTING #endif } Object.cu extern \"C\" void computeOnGPU(vector&lt;vector&lt;double&gt;&gt; *someInfo, int aConstValue, int aSecondConstValue) { //Copy values to constant memory //Allocate memory on GPU //Copy data to GPU global memory //Launch Kernel //Copy data back to CPU //Free memory } So as (I hope) you can see in the code, the function that prepares the GPU is called many times depending on the value of the first dimension. All the values that I send to constant memory always remain the same and the sizes of the pointers allocated in global memory are always the same (the data is the only one changing). This is the actual workflow in my code but I'm not getting any speedup when using GPU, I mean the kernel does execute faster but the memory transfers became my problem (as reported by nvprof). So I was wondering where in my app the CUDA context starts and finishes to see if there is a way to do only once the copies to constant memory and memory allocations.",
        "answers": [
            [
                "Normally, the cuda context begins with the first CUDA call in your application, and ends when the application terminates. You should be able to do what you have in mind, which is to do the allocations only once (at the beginning of your app) and the corresponding free operations only once (at the end of your app) and populate __constant__ memory only once, before it is used the first time. It's not necessary to allocate and free the data structures in GPU memory repetetively, if they are not changing in size."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Is it possible to share a cudaMalloc'ed GPU buffer between different contexts (CPU threads) which use the same GPU? Each context allocates an input buffer which need to be filled up by a pre-processing kernel which will use the entire GPU and then distribute the output to them. This scenario is ideal to avoid multiple data transfer to and from the GPUs. The application is a beamformer, which will combine multiple antenna signals and generate multiple beams, where each beam will be processed by a different GPU context. The entire processing pipeline for the beams is already in place, I just need to add the beamforming part. Having each thread generate it's own beam would duplicate the input data so I'd like to avoid that (also, the it's much more efficient to generate multiple beams at one go).",
        "answers": [
            [
                "Each CUDA context has it's own virtual memory space, therefore you cannot use a pointer from one context inside another context. That being said, as of CUDA 4.0 by default there is one context created per process and not per thread. If you have multiple threads running with the same CUDA context, sharing device pointers between threads should work without problems."
            ],
            [
                "I don't think multiple threads can run with the same CUDA context. I have done the experiments, parent cpu thread create a context and then fork a child thread. The child thread will launch a kernel using the context(cuCtxPushCurrent(ctx) ) created by the parent thread. The program just hang there."
            ]
        ],
        "votes": [
            4.0000001,
            -0.9999999
        ]
    },
    {
        "question": "Recently I began extending a very boost dependent project to use CUDA for its innermost loop. I thought it would be worth posting here about some odd behaviour I've been seeing though. Simply including certain boost headers will cause my first cuda call to generate a large number of kernels. If compile and debug the following code: simplestCase.cu #include &lt;boost/thread.hpp&gt; int main(int argc, char **argv){ int *myInt; cudaMalloc(&amp;myInt, sizeof(int)); return 0; } I get the following debug message lines upon executing cudaMalloc (same behaviour if I run a kernel I've defined. Seems like anything that triggers context creation will trigger this.): [Launch of CUDA Kernel 0 (memset32_post&lt;&lt;&lt;(1,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0] [Launch of CUDA Kernel 1 (memset32_post&lt;&lt;&lt;(1,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0] [Launch of CUDA Kernel 2 (memset32_post&lt;&lt;&lt;(1,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0] [Launch of CUDA Kernel 3 (memset32_post&lt;&lt;&lt;(1,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0] [Launch of CUDA Kernel 4 (memset32_post&lt;&lt;&lt;(1,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0] [Launch of CUDA Kernel 5 (memset32_post&lt;&lt;&lt;(1,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0] [Launch of CUDA Kernel 6 (memset32_post&lt;&lt;&lt;(1,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0] [Launch of CUDA Kernel 7 (memset32_post&lt;&lt;&lt;(1,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0] [Launch of CUDA Kernel 8 (memset32_post&lt;&lt;&lt;(1,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0] So far I have identified two headers that cause the problem: boost/thread.hpp boost/mpi.hpp Here's a bit of info that may be useful in replicating the problem: IDE: nSight Eclipse edition OS: ubuntu 12.04 x64 GPU: GeForce GTX 580 (I believe my GeForce GT 520 is being used by my OS) boost lib: 1.52 cat /proc/driver/nvidia/version: NVRM version: NVIDIA UNIX x86_64 Kernel Module 310.32 Mon Jan 14 14:41:13 PST 2013 GCC version: gcc version 4.6.3 (Ubuntu/Linaro 4.6.3-1ubuntu5) project settings: Properties-&gt;Build-&gt;CUDA-&gt;DeviceLinkerMode = Separate Compilation Properties-&gt;Build-&gt;CUDA-&gt;GenerateGPUCode = 2.0 Properties-&gt;Build-&gt;Settings-&gt;ToolSettings-&gt;NVCCLinker-&gt;Libraries = boost_system Properties-&gt;Name = simplest_case_example I think that's everything. Edit: Thank you for bringing my attention to the fact that I hadn't asked a question. I knew I was forgetting something critical. My question is this: It seems odd to me that very specific includes on their generate peripheral kernel calls, particularly since I don't use those includes, and I don't see how they could affect my interaction with CUDA. Should cuda be launching this many extra kernels for code I'm not even using? I see over 100 kernels launched in the project I'm working on now when the only CUDA related code I have in my project is a single cudaMalloc at the program's entry point. Edit2: Also happens on a Tesla K20 (kepler architecture card, whereas I think the GTX 580 is fermi). Edit3: Updated cuda driver to version 319.23. No change in the behaviour I mentioned above, but this did fix the debugger issues I was having in larger programs.",
        "answers": [
            [
                "Well, still no actual issues arising from this, so I suppose it's simply stuff that happens in the background."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "On a node with 4 NVIDIA GPUs I enabled on device 0 the ECC memory protection (all other have ECC disabled). Since I enabled ECC on device 0 my application (CUDA, using just one device) hangs when it tries to create the context on this device 0 (driver API). I don't know why it hangs at that point. If I use a different device setting CUDA_VISIBLE_DEVICE accordingly to another device it works fine. It must have to do with enabling ECC. Any thoughts? Here the output of nvidia-smi: (Why does it report 99% volatile GPU utilization, nothing is running there?) +------------------------------------------------------+ | NVIDIA-SMI 4.304.54 Driver Version: 304.54 | |-------------------------------+----------------------+----------------------+ | GPU Name | Bus-Id Disp. | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla K20m | 0000:02:00.0 Off | 1 | | N/A 29C P0 49W / 225W | 0% 12MB / 4799MB | 99% Default | +-------------------------------+----------------------+----------------------+ | 1 Tesla K20m | 0000:03:00.0 Off | 0 | | N/A 22C P8 15W / 225W | 0% 12MB / 4799MB | 0% Default | +-------------------------------+----------------------+----------------------+ | 2 Tesla K20m | 0000:83:00.0 Off | 0 | | N/A 22C P8 24W / 225W | 0% 11MB / 4799MB | 0% Default | +-------------------------------+----------------------+----------------------+ | 3 Tesla K20m | 0000:84:00.0 Off | 0 | | N/A 23C P8 25W / 225W | 0% 11MB / 4799MB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Compute processes: GPU Memory | | GPU PID Process name Usage | |=============================================================================| | No running compute processes found | +-----------------------------------------------------------------------------+ EDIT: nvidia-smi -a reports ECC enabled on all devices. Strange! ==============NVSMI LOG============== Timestamp : Fri Apr 26 10:18:14 2013 Driver Version : 304.54 Attached GPUs : 4 GPU 0000:02:00.0 Product Name : Tesla K20m Display Mode : Disabled Persistence Mode : Enabled Driver Model Current : N/A Pending : N/A Serial Number : 0324512044699 VBIOS Version : 80.10.11.00.0B Inforom Version Image Version : 2081.0208.01.07 OEM Object : 1.1 ECC Object : 3.0 Power Management Object : N/A GPU Operation Mode Current : Compute Pending : Compute PCI Bus : 0x02 Device : 0x00 Domain : 0x0000 Device Id : 0x102810DE Bus Id : 0000:02:00.0 Sub System Id : 0x101510DE GPU Link Info PCIe Generation Max : 2 Current : 2 Link Width Max : 16x Current : 16x Fan Speed : N/A Performance State : P0 Clocks Throttle Reasons Idle : Not Active User Defined Clocks : Not Active SW Power Cap : Not Active HW Slowdown : Not Active Unknown : Not Active Memory Usage Total : 4799 MB Used : 12 MB Free : 4787 MB Compute Mode : Default Utilization Gpu : 99 % Memory : 0 % Ecc Mode Current : Enabled Pending : Enabled ECC Errors Volatile Single Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Double Bit Device Memory : 1 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 1 Aggregate Single Bit Device Memory : 1 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 1 Double Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Temperature Gpu : 29 C Power Readings Power Management : Supported Power Draw : 49.51 W Power Limit : 225.00 W Default Power Limit : 225.00 W Min Power Limit : 150.00 W Max Power Limit : 225.00 W Clocks Graphics : 758 MHz SM : 758 MHz Memory : 2600 MHz Applications Clocks Graphics : 705 MHz Memory : 2600 MHz Max Clocks Graphics : 758 MHz SM : 758 MHz Memory : 2600 MHz Compute Processes : None GPU 0000:03:00.0 Product Name : Tesla K20m Display Mode : Disabled Persistence Mode : Enabled Driver Model Current : N/A Pending : N/A Serial Number : 0324512044821 VBIOS Version : 80.10.11.00.0B Inforom Version Image Version : 2081.0208.01.07 OEM Object : 1.1 ECC Object : 3.0 Power Management Object : N/A GPU Operation Mode Current : Compute Pending : Compute PCI Bus : 0x03 Device : 0x00 Domain : 0x0000 Device Id : 0x102810DE Bus Id : 0000:03:00.0 Sub System Id : 0x101510DE GPU Link Info PCIe Generation Max : 2 Current : 1 Link Width Max : 16x Current : 16x Fan Speed : N/A Performance State : P8 Clocks Throttle Reasons Idle : Active User Defined Clocks : Not Active SW Power Cap : Not Active HW Slowdown : Not Active Unknown : Not Active Memory Usage Total : 4799 MB Used : 12 MB Free : 4787 MB Compute Mode : Default Utilization Gpu : 0 % Memory : 0 % Ecc Mode Current : Enabled Pending : Enabled ECC Errors Volatile Single Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Double Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Aggregate Single Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Double Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Temperature Gpu : 19 C Power Readings Power Management : Supported Power Draw : 15.22 W Power Limit : 225.00 W Default Power Limit : 225.00 W Min Power Limit : 150.00 W Max Power Limit : 225.00 W Clocks Graphics : 324 MHz SM : 324 MHz Memory : 324 MHz Applications Clocks Graphics : 705 MHz Memory : 2600 MHz Max Clocks Graphics : 758 MHz SM : 758 MHz Memory : 2600 MHz Compute Processes : None GPU 0000:83:00.0 Product Name : Tesla K20m Display Mode : Disabled Persistence Mode : Enabled Driver Model Current : N/A Pending : N/A Serial Number : 0324512044783 VBIOS Version : 80.10.11.00.0B Inforom Version Image Version : 2081.0208.01.07 OEM Object : 1.1 ECC Object : 3.0 Power Management Object : N/A GPU Operation Mode Current : Compute Pending : Compute PCI Bus : 0x83 Device : 0x00 Domain : 0x0000 Device Id : 0x102810DE Bus Id : 0000:83:00.0 Sub System Id : 0x101510DE GPU Link Info PCIe Generation Max : 2 Current : 1 Link Width Max : 16x Current : 16x Fan Speed : N/A Performance State : P8 Clocks Throttle Reasons Idle : Active User Defined Clocks : Not Active SW Power Cap : Not Active HW Slowdown : Not Active Unknown : Not Active Memory Usage Total : 4799 MB Used : 11 MB Free : 4788 MB Compute Mode : Default Utilization Gpu : 0 % Memory : 0 % Ecc Mode Current : Enabled Pending : Enabled ECC Errors Volatile Single Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Double Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Aggregate Single Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Double Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Temperature Gpu : 22 C Power Readings Power Management : Supported Power Draw : 24.74 W Power Limit : 225.00 W Default Power Limit : 225.00 W Min Power Limit : 150.00 W Max Power Limit : 225.00 W Clocks Graphics : 324 MHz SM : 324 MHz Memory : 324 MHz Applications Clocks Graphics : 705 MHz Memory : 2600 MHz Max Clocks Graphics : 758 MHz SM : 758 MHz Memory : 2600 MHz Compute Processes : None GPU 0000:84:00.0 Product Name : Tesla K20m Display Mode : Disabled Persistence Mode : Enabled Driver Model Current : N/A Pending : N/A Serial Number : 0324512044628 VBIOS Version : 80.10.11.00.0B Inforom Version Image Version : 2081.0208.01.07 OEM Object : 1.1 ECC Object : 3.0 Power Management Object : N/A GPU Operation Mode Current : Compute Pending : Compute PCI Bus : 0x84 Device : 0x00 Domain : 0x0000 Device Id : 0x102810DE Bus Id : 0000:84:00.0 Sub System Id : 0x101510DE GPU Link Info PCIe Generation Max : 2 Current : 1 Link Width Max : 16x Current : 16x Fan Speed : N/A Performance State : P8 Clocks Throttle Reasons Idle : Active User Defined Clocks : Not Active SW Power Cap : Not Active HW Slowdown : Not Active Unknown : Not Active Memory Usage Total : 4799 MB Used : 11 MB Free : 4788 MB Compute Mode : Default Utilization Gpu : 0 % Memory : 0 % Ecc Mode Current : Enabled Pending : Enabled ECC Errors Volatile Single Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Double Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Aggregate Single Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Double Bit Device Memory : 0 Register File : 0 L1 Cache : 0 L2 Cache : 0 Texture Memory : 0 Total : 0 Temperature Gpu : 23 C Power Readings Power Management : Supported Power Draw : 25.47 W Power Limit : 225.00 W Default Power Limit : 225.00 W Min Power Limit : 150.00 W Max Power Limit : 225.00 W Clocks Graphics : 324 MHz SM : 324 MHz Memory : 324 MHz Applications Clocks Graphics : 705 MHz Memory : 2600 MHz Max Clocks Graphics : 758 MHz SM : 758 MHz Memory : 2600 MHz Compute Processes : None",
        "answers": [
            [
                "The nvidia-smi output shows an uncorrectable ECC error on the device. You can reset the error using nvidia-smi --reset-ecc-errors=0 -g 0 and retry. The 0 in the reset indicates to reset the volatile counter only, the aggregate counter will still indicate that an error has happened in the past. If you see further errors from the device then it would be worth investigating the cause further. Note that in the summary view the ECC field you are looking at is actually \"Volatile Uncorr. ECC\", i.e. it's the error count not the ECC enabled/disabled flag. If ECC is disabled it will say \"N/A\"."
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "It seems that most tutorials, guides, books and Q&amp;A from the web refers to CUDA 3 and 4.x, so that is why I'm asking it specifically about CUDA 5.0. To the question... I would like to program for an environment with two CUDA devices, but use only one thread, to make the design simple (specially because it is a prototype). I want to know if the following code is valid: float *x[2]; float *dev_x[2]; for(int d = 0; d &lt; 2; d++) { cudaSetDevice(d); cudaMalloc(&amp;dev_x[d], 1024); } for(int repeats = 0; repeats &lt; 100; repeats++) { for(int d = 0; d &lt; 2; d++) { cudaSetDevice(d); cudaMemcpy(dev_x[d],x[d],1024,cudaMemcpyHostToDevice); some_kernel&lt;&lt;&lt;...&gt;&gt;&gt;(dev_x[d]); cudaMemcpy(x[d],dev_x[d],1024,cudaMemcpyDeviceToHost); } cudaStreamSynchronize(0); } I would like to know specifically if cudaMalloc(...)s from before the testing for persist even with the interchanging of cudaSetDevice() that happens in the same thread. Also, I would like to know if the same happens with context-dependent objects such as cudaEvent_t and cudaStream_t. I am asking it because I have an application in this style that keeps getting some mapping error and I can't find what it is, if some missing memory leak or wrong API usage. Note: In my original code, I do check every single CUDA call. I did not put it here for code readability.",
        "answers": [
            [
                "Is this just a typo? for(int d = 0; d &lt; 2; d++) { cudaSetDevice(0); // shouldn't that be 'd' cudaMalloc(&amp;dev_x, 1024); } Please check the return value of all API calls!"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm implementing fluid simulator using PhysiX. Unfortunately sth is wrong with cuda context manager and I have a problem with recognizing what it is. I have an init method which looks like this: void InitializePhysX() { bool recordMemoryAllocations = true; const bool useCustomTrackingAllocator = true; PxAllocatorCallback* allocator = &amp;gDefaultAllocatorCallback; PxErrorCallback* error = &amp;gDefaultErrorCallback; PxFoundation* mFoundation = PxCreateFoundation(PX_PHYSICS_VERSION, *allocator, *error); if(!mFoundation) printf(\"PxCreateFoundation failed!\\n\"); PxProfileZoneManager* mProfileZoneManager = &amp;PxProfileZoneManager::createProfileZoneManager(mFoundation); if(!mProfileZoneManager) printf(\"PxProfileZoneManager::createProfileZoneManager failed!\\n\"); #ifdef PX_WINDOWS pxtask::CudaContextManagerDesc cudaContextManagerDesc; pxtask::CudaContextManager* mCudaContextManager = pxtask::createCudaContextManager(*mFoundation, cudaContextManagerDesc, mProfileZoneManager); if( mCudaContextManager ){ if( !mCudaContextManager-&gt;contextIsValid() ){ mCudaContextManager-&gt;release(); mCudaContextManager = NULL; printf(\"invalid context\\n\"); } } else { printf(\"create cuda context manager failed\\n\"); } #endif mPhysX = PxCreatePhysics(PX_PHYSICS_VERSION, *mFoundation, PxTolerancesScale(), recordMemoryAllocations, mProfileZoneManager); if(!mPhysX) printf(\"PxCreatePhysics failed!\\n\"); ... } When I try to run my application it occures that mCudaContextManger is never created properly. \"create cuda context manager failed\" is being wrote on the console and: \"....\\LowLevel\\software\\src\\PxsContext.cpp (1122) : warning : GPU operation faied. No px::CudaContextManager available. ....\\SimulationController\\src\\particles\\ScParticleSystemSim.cpp (73) : warning : GPU particle system creation failed. Falling back to CPU implementation.\" I have GeForce560Ti with newest driver (error also shows up on GeForce460 on my friend's laptop). Physix is set to use GPU in NVidia Control Panel. Does anybody know what we made wrong and how to make GPU work? Thanks in advance!",
        "answers": [
            [
                "File PhysX3Gpu_x86.dll was missing. I added it and now everything is fine."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I've a program that uses three kernels. In order to get the speedups, I was doing a dummy memory copy to create a context as follows: __global__ void warmStart(int* f) { *f = 0; } which is launched before the kernels I want to time as follows: int *dFlag = NULL; cudaMalloc( (void**)&amp;dFlag, sizeof(int) ); warmStart&lt;&lt;&lt;1, 1&gt;&gt;&gt;(dFlag); Check_CUDA_Error(\"warmStart kernel\"); I also read about other simplest ways to create a context as cudaFree(0) or cudaDevicesynchronize(). But using these API calls gives worse times than using the dummy kernel. The execution times of the program, after forcing the context, are 0.000031 seconds for the dummy kernel and 0.000064 seconds for both, the cudaDeviceSynchronize() and cudaFree(0). The times were get as a mean of 10 individual executions of the program. Therefore, the conclusion I've reached is that launch a kernel initialize something that is not initialized when creating a context in the canonical way. So, what's the difference of creating a context in these two ways, using a kernel and using an API call? I run the test in a GTX480, using CUDA 4.0 under Linux.",
        "answers": [
            [
                "Each CUDA context has memory allocations that are required to execute a kernel that are not required to be allocated to syncrhonize, allocate memory, or free memory. The initial allocation of the context memory and resizing of these allocations is deferred until a kernel requires these resources. Examples of these allocations include the local memory buffer, device heap, and printf heap."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "i have installed: CUDA sdk 4.2 64, CUDA toolkit 4.2 64, CUDA devdriver 4.2 64, I checked every nvcuda.dll in \\windows - all of them, are 4.2 version, but when i create a context with driver api and check it verison with cuCtxGetApiVersion - it shows me \"3010\" I do not really care about that version, but when i try to use runtime api with that context(need to mix them) - it shows me error 49(cudaErrorIncompatibleDriverContext) any ideas, what is going on and how to make it work together? seems it is not some issue with windows - running the same project under linux results in the same 3010 API version.",
        "answers": [
            [
                "I solved the problem - i was exporting \"cuCtxCreate\" function directly during execution(it is other language - no cuda.h). After looking in original cuda.h i found that it defines other name for that functrion, basing on some sort of smartass defines. And that new function \"cuCtxCreate_v2\" creates a good context. /facepalm"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "A library that I link to uses the cuda runtime API. Thus it implicitly creates a cuda context when first calling a cuda function. My code (that uses the library) should use the driver API. Now, how can i get both (runtime and driver API) to work at the same time? The library calls the cudaSetDevice function upon library init. (There's no way i can change this). Can I somehow determine the context and tell the driver API to use that one?",
        "answers": [
            [
                "cuCtxGetCurrent() gets the currect context (that might be created by the runtime)"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "How can I create a CUDA context? The first call of CUDA is slow and I want to create the context before I launch my kernel.",
        "answers": [
            [
                "The canonical way to force runtime API context establishment is to call cudaFree(0). If you have multiple devices, call cudaSetDevice() with the ID of the device you want to establish a context on, then cudaFree(0) to establish the context. EDIT: Note that as of CUDA 5.0, it appears that the heuristics of context establishment are slightly different and cudaSetDevice() itself establishes context on the device is it called on. So the explicit cudaFree(0) call is no longer necessary (although it won't hurt anything)."
            ],
            [
                "Using the runtime API: cudaDeviceSynchronize, cudaDeviceGetLimit, or anything that actually accesses the context should work. I'm quite certain you're not using the driver API, as it doesn't do that sort of lazy initialization, but for others' benefit the driver call would be cuCtxCreate."
            ]
        ],
        "votes": [
            15.0000001,
            3.0000001
        ]
    },
    {
        "question": "I am getting CUDA_ERROR_INVALID_DEVICE error when creating a cuda context via cuCtxCreate. My code is creating a context and getting device from it and creating a new context. Any idea why I can't create another context? #include&lt;cuda.h&gt; #include&lt;assert.h&gt; int main(){ cuInit(0); CUcontext ctx; CUdevice device; CUdevice dev2; CUcontext c2; assert(cuDeviceGet(&amp;device,0) == 0 ); assert(cuCtxCreate(&amp;ctx, 0, device) == 0); assert(cuCtxGetDevice(&amp;dev2) == 0); assert(cuCtxCreate(&amp;c2,0,dev2) == 0); return 0; }",
        "answers": [
            [
                "The problem was that the user had the device in exclusive mode, which prohibits creating two contexts on the single device."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I want to understand how a cuda context is created and associated with a kernel in cuda runtime API applications? I know it is done under the hood by driver APIs. But I would like to understand the timeline of the creation. For a start I know cudaRegisterFatBinary is the first cuda api call made and it registers a fatbin file with the runtime. It is followed by a handful of cuda function registration APIs which call cuModuleLoad in the driver layer. But then if my Cuda runtime API application invokes cudaMalloc how is the pointer provided to this function associated with the context, which I believe should have been created beforehand. How does one get a handle to this already created context and associate the future runtime API calls with it? Please demystify the internal workings. To quote NVIDIA's documentation on this CUDA Runtime API calls operate on the CUDA Driver API CUcontext which is bound to the current host thread. If there exists no CUDA Driver API CUcontext bound to the current thread at the time of a CUDA Runtime API call which requires a CUcontext then the CUDA Runtime will implicitly create a new CUcontext before executing the call. If the CUDA Runtime creates a CUcontext then the CUcontext will be created using the parameters specified by the CUDA Runtime API functions cudaSetDevice, cudaSetValidDevices, cudaSetDeviceFlags, cudaGLSetGLDevice, cudaD3D9SetDirect3DDevice, cudaD3D10SetDirect3DDevice, and cudaD3D11SetDirect3DDevice. Note that these functions will fail with cudaErrorSetOnActiveProcess if they are called when a CUcontext is bound to the current host thread. The lifetime of a CUcontext is managed by a reference counting mechanism. The reference count of a CUcontext is initially set to 0, and is incremented by cuCtxAttach and decremented by cuCtxDetach. If a CUcontext is created by the CUDA Runtime, then the CUDA runtime will decrement the reference count of that CUcontext in the function cudaThreadExit. If a CUcontext is created by the CUDA Driver API (or is created by a separate instance of the CUDA Runtime API library), then the CUDA Runtime will not increment or decrement the reference count of that CUcontext. All CUDA Runtime API state (e.g, global variables' addresses and values) travels with its underlying CUcontext. In particular, if a CUcontext is moved from one thread to another (using cuCtxPopCurrent and cuCtxPushCurrent) then all CUDA Runtime API state will move to that thread as well. But what I don't understand is how does cuda runtime create the context? what API calls are used for this? Does the nvcc compiler insert some API calls to do this at compile time or is this done entirely at runtime? If the former is true what run time APIs are used for this context management? It the later is true how exactly is it done ? If a context is associated with a host thread, how do we get access to this context? Is it automatically associated with all the variables and pointer references dealt with by the thread? how ultimately is a module loading done in the context?",
        "answers": [
            [
                "The CUDA runtime maintains a global list of modules to load, and adds to that list every time a DLL or .so that uses the CUDA runtime is loaded into the process. But the modules are not actually loaded until a device is created. Context creation and initialization is done \"lazily\" by the CUDA runtime -- every time you call a function like cudaMemcpy(), it checks to see whether CUDA has been initialized, and if it hasn't, it creates a context (on the device previously specified by cudaSetDevice(), or the default device if cudaSetDevice() was never called) and loads all the modules. The context is associated with that CPU thread from then on, until it's changed by cudaSetDevice(). You can use context/thread management functions from the driver API, such as cuCtxPopCurrent()/cuCtxPushCurrent(), to use the context from a different thread. You can call cudaFree(0); to force this lazy initialization to occur. I'd strongly advise doing so at application initialization time, to avoid race conditions and undefined behavior. Go ahead and enumerate and initialize the devices as early as possible in your app; once that is done, in CUDA 4.0 you can call cudaSetDevice() from any CPU thread and it will select the corresponding context that was created by your initialization code."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have some CUDA kernels I want to run in individual pthreads. I basically have to have each pthread execute, say, 3 cuda kernels, and they must be executed sequentially. I thought I would try to pass each pthread a reference to a stream, and so each of those 3 cuda kernels would all execute sequentially, in the same stream. I could get this working with a different context for pthread, which would then execute the kernels as normal, but that seems to take a lot of overhead. So how do I make each pthread work in the same context, concurrently with the other pthreads? Thanks",
        "answers": [
            [
                "Before CUDA 4.0, the way to access a given context from different CPU threads was to use cuCtxPopCurrent()/cuCtxPushCurrent(). A context could only be current to one CPU thread at a time. In CUDA 4.0, you can call cudaSetDevice() in each pthread and it can be current to more than one thread at a time. The kernel invocations will be serialized by the context in the order received, but you may have to perform CPU thread synchronization to make sure the work is submitted in the order desired."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using an application presently that spawns a bunch of pthreads (linux), and each of those creates it's own CUDA context. (using cuda 3.2 right now). The problem I am having is that it seems like each thread having its own context costs a lot of memory on the GPU. Something like 200MB per thread, so this is really limiting me. Can I simply create streams in the host thread, pass the stream reference to the worker threads, which would then be able to pass to my CUDA library their stream number, and all work out of the same context? Does a worker thread automatically know the same CUDA context as it's parent thread? Thanks",
        "answers": [
            [
                "Each CUDA context does cost quite a bit of device memory, and their resources are strictly partitioned from one another. For example, device memory allocated in context A cannot be accessed by context B. Streams also are valid only in the context in which they were created. The best practice would be to create one CUDA context per device. By default, that CUDA context can be accessed only from the CPU thread that created it. If you want to access the CUDA context from other threads, call cuCtxPopCurrent() to pop it from the thread that created it. The context then can be pushed onto any other CPU thread's current context stack, and subsequent CUDA calls would reference that context. Context push/pop are lightweight operations and as of CUDA 3.2, they can be done in CUDA runtime apps. So my suggestion would be to initialize the CUDA context, then call cuCtxPopCurrent() to make the context \"floating\" unless some threads wants to operate it. Consider the \"floating\" state to be the natural one - whenever a thread wants to manipulate the context, bracket its usage with cuCtxPushCurrent()/cuCtxPopCurrent()."
            ]
        ],
        "votes": [
            11.0000001
        ]
    },
    {
        "question": "I'd like to pass a Cuda context between two independent Linux processes (using POSIX message queues, which I already have set up). Using cuCtxPopCurrent() and cuCtxPushCurrent(), I can get the context pointer, but this pointer is referenced in the memory of the process in which I call the function, and passing it between processes is meaningless. I'm looking for other solutions. My ideas so far are: Try to deep copy the CUcontext struct, and then pass the copy. See if I can find a shared-memory solution where all my Cuda pointers are placed there so both processes can access them. Merge the processes into one program. It is possible that there is better context sharing in Cuda 4.0, which I could switch to. I'm not sure option (1) is possible, nor if (2) is available or possible. (3) isn't really an option if I want to make things generic (this is within a hijack shim). (4) I'll look at Cuda 4.0, but I'm not sure if it will work there, either. Thanks!",
        "answers": [
            [
                "In a word, no. Contexts are implicitly tied to the thread and application that created them. There is no portability between separate applications. This pretty much the same with OpenGL and the various versions of Direct3D as well - sharing memory between applications isn't supported. CUDA 4 makes the API thread safe, so that a single host thread can hold more than 1 context (ie. more than 1 GPU) simultaneously and use the canonical device selection API to choose which GPU it is working with. That won't help here, if I am understanding your question/application correctly."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Lets imagine the situation, that I have a lot of initialized resources for example: streams, host and device memory end events, part of them are initialized in context of one GPU and the rest of them belong to the other GPU context. Is there a way to check if given resource (event, stream or memory) belongs to certain GPU context? In some case it would worthy to assert such things, before order memory copy or kernel execution and then get cudaErrorInvalidArgument.",
        "answers": [
            [
                "I am not really aware of such option in CUDA API itself. It is just a low-level sets of orders that you can issue to your GPU. What I would do is to wrap the CUDA API functions into some nice class which would track what is where and what is initialised. A class representing a GPU might be useful as well."
            ]
        ],
        "votes": [
            2.0000001
        ]
    }
]