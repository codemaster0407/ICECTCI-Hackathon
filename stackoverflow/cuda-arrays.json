[
    {
        "question": "I'm trying to use the CUDA Driver API to copy data into a 2D array, in the program listed below, but am getting an \"invalid value\" error when I pass my copy parameters. What value in them is wrong? #include &lt;cuda.h&gt; #include &lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;numeric&gt; #include &lt;limits&gt; #include &lt;cstring&gt; [[noreturn]] void die_(const std::string&amp; message) { std::cerr &lt;&lt; message &lt;&lt; \"\\n\"; exit(EXIT_FAILURE); } void die_if_error(CUresult status, const std::string&amp; extra_message) { if (status != CUDA_SUCCESS) { const char* error_string; cuGetErrorString(status, &amp;error_string); die_(extra_message + \": \" + error_string); } } template &lt;typename T = void&gt; T* as_pointer(CUdeviceptr address) noexcept { return reinterpret_cast&lt;T*&gt;(address); } CUdeviceptr as_address(void* ptr) noexcept { return reinterpret_cast&lt;CUdeviceptr&gt;(ptr); } int main() { CUresult status; int device_id = 0; status = cuInit(0); die_if_error(status, \"Initializing the CUDA driver\"); CUcontext pctx; status = cuDevicePrimaryCtxRetain(&amp;pctx, device_id); die_if_error(status, \"Obtaining the primary device context\"); cuCtxSetCurrent(pctx); struct { unsigned width, height; } dims = { 3, 3 }; std::cout &lt;&lt; \"Creating a \" &lt;&lt; dims.width &lt;&lt; \" x \" &lt;&lt; dims.height &lt;&lt; \" CUDA array\" &lt;&lt; std::endl; CUarray arr_handle; { CUDA_ARRAY_DESCRIPTOR array_descriptor; array_descriptor.Width = dims.width; array_descriptor.Height = dims.height; array_descriptor.Format = CU_AD_FORMAT_FLOAT; array_descriptor.NumChannels = 1; status = cuArrayCreate(&amp;arr_handle, &amp;array_descriptor); die_if_error(status, \"Failed creating a 2D CUDA array\"); } auto arr_size = dims.width * dims.height; CUdeviceptr dptr; status = cuMemAllocManaged(&amp;dptr, arr_size, CU_MEM_ATTACH_GLOBAL); die_if_error(status, \"Failed allocating managed memory\"); float* ptr_in = as_pointer&lt;float&gt;(dptr); std::iota(ptr_in, ptr_in + arr_size, 0); CUmemorytype ptr_in_memory_type; status = cuPointerGetAttribute(&amp;ptr_in_memory_type, CU_POINTER_ATTRIBUTE_MEMORY_TYPE, as_address(ptr_in)); if (not (ptr_in_memory_type == CU_MEMORYTYPE_UNIFIED or ptr_in_memory_type == CU_MEMORYTYPE_DEVICE)) { die_(\"Unexpected memory type for ptr_in\"); } std::cout &lt;&lt; \"The memory type of ptr_in is \" &lt;&lt; (ptr_in_memory_type == CU_MEMORYTYPE_DEVICE ? \"DEVICE\" : \"UNIFIED\") &lt;&lt; std::endl; std::cout &lt;&lt; \"Will copy from ptr_in into a 2D CUDA array\" &lt;&lt; std::endl; CUDA_MEMCPY2D cp; { // Source cp.srcXInBytes = 0; cp.srcY = 0; // No offset cp.srcMemoryType = ptr_in_memory_type; cp.srcDevice = as_address(ptr_in); // no extra source pitch cp.srcPitch = dims.width * sizeof(float); // Destination cp.dstXInBytes = 0; cp.dstY = 0; // No destination offset cp.dstMemoryType = CU_MEMORYTYPE_ARRAY; cp.dstArray = arr_handle; cp.WidthInBytes = dims.width * sizeof(float); cp.Height = dims.height; } status = cuMemcpy2D(&amp;cp); die_if_error(status, \"cuMemcpy2D failed\"); cuMemFree(as_address(ptr_in)); } Full output of this program: Creating a 3 x 3 CUDA array The memory type of ptr_in is DEVICE Will copy from ptr_in into a 2D CUDA array cuMemcpy2D failed: invalid argument Additional information: CUDA toolkit version: 11.4 NVIDIA driver version: 470.57.02 OS distribution: Devuan Chimaera GNU/Linux GPU: GeForce 1050 TI Boost (Compute Capability 6.1) Host architecture: amd64",
        "answers": [
            [
                "The error is here: auto arr_size = dims.width * dims.height; CUdeviceptr dptr; status = cuMemAllocManaged(&amp;dptr, arr_size, CU_MEM_ATTACH_GLOBAL); ^^^^^^^^ That should be arr_size*sizeof(float) cuMemAllocManaged(), like malloc() takes a size argument in bytes. This size needs to be consistent with (greater than or equal to) your implied size of transfer in the cuMemcpy2D call."
            ],
            [
                "tl;dr: \"invalid value\" can be a pointer without sufficient allocated memory (@RobertCrovella noticed the error, but I want to emphasize a point:) We are used to APIs not being able to scrutinize pointers too much, accepting them on faith, then possibly failing with invalid access errors (segmentation fault on the host side, invalid memory access on the device side etc.) However, CUDA (in particular, the CUDA driver) scrutinizes pointers more. You already know this to be the case, seeing how it can tell you what memory type a pointer points to. Well, it seems cuMemCpy2D() also checks the amount of memory allocated at ptr_in - and figures out that it's not enough to suffice for filling the area, i.e. it would copy from unallocated memory. That's why it returns the \"invalid value\" error. So the error code is valid, albeit rather vague. Specifically, and as @RobertCrovella points out, you did not allocate enough memory for 3x3 floats - your arr_size is in elements, i.e. 9, while you need to allocate 9 floats, i.e. 36 bytes. You lucked out writing to it, probably because of CUDA's memory allocation quantum, or memory page granularity etc."
            ]
        ],
        "votes": [
            2.0000001,
            -0.9999999
        ]
    },
    {
        "question": "I am playing around with NVDEC H.264 decoder from NVIDIA CUDA samples, one thing I've found out is once frame is decoded, it's converted from NV12 to BGRA buffer which is allocated on CUDA's side, then this buffer is copied to D3D BGRA texture. I find this not very efficient in terms of memory usage, and want to convert NV12 frame directly to D3D texture with this kernel: void Nv12ToBgra32(uint8_t *dpNv12, int nNv12Pitch, uint8_t *dpBgra, int nBgraPitch, int nWidth, int nHeight, int iMatrix) So, create D3D texture (BGRA, D3D11_USAGE_DEFAULT, D3D11_BIND_SHADER_RESOURCE | D3D11_BIND_UNORDERED_ACCESS, D3D11_CPU_ACCESS_WRITE, 1 mipmap), then register and write it on CUDA side: //Register ck(cuGraphicsD3D11RegisterResource(&amp;cuTexResource, textureResource, CU_GRAPHICS_REGISTER_FLAGS_NONE)); ... //Write output: CUarray retArray; ck(cuGraphicsMapResources(1, &amp;cuTexResource, 0)); ck(cuGraphicsSubResourceGetMappedArray(&amp;retArray, cuTexResource, 0, 0)); /* yuvFramePtr (NV12) is uint8_t* from decoded frame, it's stored within CUDA memory I believe */ Nv12ToBgra32(yuvFramePtr, w, (uint8_t*)retArray, 4 * w, w, h); ck(cuGraphicsUnmapResources(1, &amp;cuTexResource, 0)); Once kernel is called, I get crash. May be because of misusing CUarray, can anybody please clarify how to use output of cuGraphicsSubResourceGetMappedArray to write texture memory from CUDA kernel? (since writing raw memory is only needed, there is no need to handle correct clamp, filtering and value scaling)",
        "answers": [
            [
                "Ok, for anyone who struggling on question \"How to write D3D11 texture from CUDA kernel\", here is how: Create D3D texture with D3D11_BIND_UNORDERED_ACCESS. Then, register resource: //ID3D11Texture2D *textureResource from D3D texture CUgraphicsResource cuTexResource; ck(cuGraphicsD3D11RegisterResource(&amp;cuTexResource, textureResource, CU_GRAPHICS_REGISTER_FLAGS_NONE)); //You can also add write-discard if texture will be fully written by kernel ck(cuGraphicsResourceSetMapFlags(cuTexResource, CU_GRAPHICS_MAP_RESOURCE_FLAGS_WRITE_DISCARD)); Once texture is created and registered we can use it as write surface. ck(cuGraphicsMapResources(1, &amp;cuTexResource, 0)); //Get array for first mip-map CUArray retArray; ck(cuGraphicsSubResourceGetMappedArray(&amp;retArray, cuTexResource, 0, 0)); //Create surface from texture CUsurfObject surf; CUDA_RESOURCE_DESC surfDesc{}; surfDesc.res.array.hArray = retArray; surfDesc.resType = CU_RESOURCE_TYPE_ARRAY; ck(cuSurfObjectCreate(&amp;surf, &amp;surfDesc)); /* Kernel declaration is: void Nv12ToBgra32Surf(uint8_t* dpNv12, int nNv12Pitch, cudaSurfaceObject_t surf, int nBgraPitch, int nWidth, int nHeight, int iMatrix) Surface write: surf2Dwrite&lt;uint&gt;(VALUE, surf, x * sizeof(uint), y); For BGRA surface we are writing uint, X offset is in bytes, so multiply it with byte-size of type. Run kernel: */ Nv12ToBgra32Surf(yuvFramePtr, w, /*out*/surf, 4 * w, w, h); ck(cuGraphicsUnmapResources(1, &amp;cuTexResource, 0)); ck(cuSurfObjectDestroy(surf));"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I can find many examples online that use CUDA texture references, but not so many that rely on texture objects. I am trying to understand why my code below always fetches 0 rather than my input texture. Am I missing something, or using a wrong setting? I simplified it as much as I could: #include &lt;stdio.h&gt; __global__ void fetch(cudaTextureObject_t tex, std::size_t width, std::size_t height) { for (int j = 0; j &lt; height; j++) { for (int i = 0; i &lt; width; i++) { float u = (i + 0.5f) / width; float v = (j + 0.5f) / height; auto p = tex2D&lt;uchar4&gt;(tex, u, v); printf(\"i=%d, j=%d -&gt; u=%3.2f, v=%3.2f, r=%d, g=%d, b=%d, a=%d\\n\", i, j, u, v, p.x, p.y, p.z, p.w); // -&gt; always returns p = {0, 0, 0, 0} } } } int main() { constexpr std::size_t width = 2; constexpr std::size_t height = 2; // creating a dummy texture uchar4 image[width*height]; for(std::size_t j = 0; j &lt; height; ++j) { for(std::size_t i = 0; i &lt; width; ++i) image[j*width+i] = make_uchar4(255*j/height, 255*i/width, 55, 255); } cudaArray_t cuArray; auto channelDesc = cudaCreateChannelDesc&lt;uchar4&gt;(); cudaMallocArray(&amp;cuArray, &amp;channelDesc, width, height); cudaMemcpy2DToArray(cuArray, 0, 0, image, width*sizeof(uchar4), width*sizeof(uchar4), height, cudaMemcpyHostToDevice); struct cudaResourceDesc resDesc; memset(&amp;resDesc, 0, sizeof(resDesc)); resDesc.resType = cudaResourceTypeArray; resDesc.res.array.array = cuArray; struct cudaTextureDesc texDesc; memset(&amp;texDesc, 0, sizeof(texDesc)); texDesc.addressMode[0] = cudaAddressModeBorder; texDesc.addressMode[1] = cudaAddressModeBorder; texDesc.filterMode = cudaFilterModeLinear; texDesc.readMode = cudaReadModeElementType; texDesc.normalizedCoords = 1; cudaTextureObject_t texObj = 0; cudaCreateTextureObject(&amp;texObj, &amp;resDesc, &amp;texDesc, NULL); fetch&lt;&lt;&lt;1, 1&gt;&gt;&gt;(texObj, width, height); cudaDeviceSynchronize(); cudaDestroyTextureObject(texObj); cudaFreeArray(cuArray); return 0; }",
        "answers": [
            [
                "In your code you specify the texture description as texDesc.addressMode[0] = cudaAddressModeBorder; texDesc.addressMode[1] = cudaAddressModeBorder; texDesc.filterMode = cudaFilterModeLinear; texDesc.readMode = cudaReadModeElementType; texDesc.normalizedCoords = 1; and the array holding the texture data is defined as auto channelDesc = cudaCreateChannelDesc&lt;uchar4&gt;(); Quoting the documentation Linear Filtering In this filtering mode, which is only available for floating-point textures ...... You have a uchar4 texture. You can't use linear filtering on an integer texture. Either change to a floating point texture type or use another read mode (probably cudaFilterModePoint)."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Say you have a cuArray for binding a surface object. Something of the form: // These are inputs to a function really. cudaArray* d_cuArrSurf cudaSurfaceObject_t * surfImage; const cudaExtent extent = make_cudaExtent(width, height, depth); cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc&lt;float&gt;(); cudaMalloc3DArray(&amp;d_cuArrSurf, &amp;channelDesc, extent); // Bind to Surface cudaResourceDesc surfRes; memset(&amp;surfRes, 0, sizeof(cudaResourceDesc)); surfRes.resType = cudaResourceTypeArray; surfRes.res.array.array = d_cuArrSurf; cudaCreateSurfaceObject(surfImage, &amp;surfRes); Now, I want to initialize this cuArray to zero. Apparently there is non memset for cuArray type of objects. What would be the best way to do this? Maybe multiple options are possible, and some may have better or worse features. Which are these options? I can think of allocate and zero host memory and copy it using cudaMemcpy3D(). create an initialization kernel and write it with surf3Dwrite()",
        "answers": [
            [
                "Would it be possible for you to show an example of those lines? Here is a rough example, roughly extending the previous rough example: $ cat t1648.cu // Includes, system #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;string.h&gt; #include &lt;math.h&gt; __device__ float my_common(float *d, int width, unsigned int x, unsigned int y){ // 200 lines of common code... return d[y *width +x]; } //////////////////////////////////////////////////////////////////////////////// // Kernels //////////////////////////////////////////////////////////////////////////////// //! Write to a cuArray using surface writes //! @param gIData input data in global memory //////////////////////////////////////////////////////////////////////////////// __global__ void WriteKernel(float *gIData, int width, int height, cudaSurfaceObject_t outputSurface) { // calculate surface coordinates unsigned int x = blockIdx.x*blockDim.x + threadIdx.x; unsigned int y = blockIdx.y*blockDim.y + threadIdx.y; unsigned int z = blockIdx.z*blockDim.z + threadIdx.z; // read from global memory and write to cuarray (via surface reference) surf3Dwrite(my_common(gIData, width, x, y), outputSurface, x*4, y, z, cudaBoundaryModeTrap); } __global__ void WriteKernel(float *gIData, int width, int height, float *out) { // calculate coordinates unsigned int x = blockIdx.x*blockDim.x + threadIdx.x; unsigned int y = blockIdx.y*blockDim.y + threadIdx.y; // read from global memory and write to global memory out[y*width+x] = my_common(gIData, width, x, y); } __global__ void ReadKernel(float tval, cudaSurfaceObject_t outputSurface) { // calculate surface coordinates unsigned int x = blockIdx.x*blockDim.x + threadIdx.x; unsigned int y = blockIdx.y*blockDim.y + threadIdx.y; unsigned int z = blockIdx.z*blockDim.z + threadIdx.z;; // read from global memory and write to cuarray (via surface reference) float val; surf3Dread(&amp;val, outputSurface, x*4, y, z, cudaBoundaryModeTrap); if (val != tval) printf(\"oops\\n\"); } //////////////////////////////////////////////////////////////////////////////// // Program main //////////////////////////////////////////////////////////////////////////////// int main(int argc, char **argv) { printf(\"starting...\\n\"); unsigned width = 256; unsigned height = 256; unsigned depth = 256; unsigned int size = depth*width * height * sizeof(float); // Allocate device memory for result float *dData = NULL; cudaMalloc((void **) &amp;dData, size); // Allocate array and copy image data float *out, *h_out; h_out = new float[height*width*depth]; float tval = 1.0f; for (int i = 0; i &lt; height*width*depth; i++) h_out[i] = tval; cudaArray* d_cuArrSurf; cudaSurfaceObject_t surfImage; const cudaExtent extent = make_cudaExtent(width, height, depth); cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc&lt;float&gt;(); cudaMalloc3DArray(&amp;d_cuArrSurf, &amp;channelDesc, extent); // Bind to Surface cudaResourceDesc surfRes; memset(&amp;surfRes, 0, sizeof(cudaResourceDesc)); surfRes.resType = cudaResourceTypeArray; surfRes.res.array.array = d_cuArrSurf; cudaCreateSurfaceObject(&amp;surfImage, &amp;surfRes); cudaMalloc(&amp;out, size); cudaMemcpy(out, h_out, size, cudaMemcpyHostToDevice); dim3 dimBlock(8, 8, 8); dim3 dimGrid(width / dimBlock.x, height / dimBlock.y, 1); // initialize array cudaMemcpy3DParms p = {0}; p.srcPtr = make_cudaPitchedPtr(out, width*sizeof(out[0]), width, height); p.srcPos = make_cudaPos(0,0,0); p.dstArray = d_cuArrSurf; p.dstPos = make_cudaPos(0,0,0); p.extent = make_cudaExtent(width, height, 1); p.kind = cudaMemcpyDefault; for (int i = 0; i &lt; depth; i++){ cudaMemcpy3D(&amp;p); p.dstPos = make_cudaPos(0,0, i+1);} ReadKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(tval, surfImage); WriteKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(dData, width, height, surfImage); WriteKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(dData, width, height, out); cudaDeviceSynchronize(); } $ nvcc -o t1648 t1648.cu $ cuda-memcheck ./t1648 ========= CUDA-MEMCHECK starting... ========= ERROR SUMMARY: 0 errors $ The (total) extent above is 256x256x256. So I chose to do a 256x256 transfer (per-transfer extent) (basically each z-slice) over 256 iterations of cudaMemcpy3D. It seems to pass the sniff test. I used 1 as my initializing value for device memory here \"just because\". If you wanted to make this faster and initialize to zero, skip the host-&gt;device copy and just use cudaMemset to initialize the linear memory (source for 3D transfer) to zero."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm fairly new to julia and I'm currently trying out some deep convolution networks with recurrent structures. I'm training the networks on a GPU using CuArrays(CUDA Version 9.0). Having two separate GPU's, I started two instances with different datasets. Soon after some training both julia instances allocated all available Memory (2 x 11GB) and I couldn't even start another instance on my own using CuArrays (Memory allocation error). This became quite a problem, since this is running on a Server which is shared among many people. I'm assuming that this is a normal behavior to use all available memory to train as fast as possible. But, under these circumstances I would like to limit the memory which can be allocated to run two instances at the same time and don't block me or other people from using the GPU. To my surprise I found only very, very little information about this. I'm aware of the CUDA_VISIBLE_DEVICES Option but this does not help since I want to train simultaneously on both devices. Another one suggested to call GC.gc() and CuArrays.clearpool() The second call throws an unknown function error and seems not to be within the CuArray Package anymore. The first one I'm currently testing but not exactly what I need. Is there any possibilty to limit the allocation of RAM on a GPU using CuArrays and Julia? Thanks in advance My Batchsize is 100 and one batch should have less than 1MB...",
        "answers": [
            [
                "There is currently no such functionality. I quickly whipped something up, see https://github.com/JuliaGPU/CuArrays.jl/pull/379, you can use it to define CUARRAYS_MEMORY_LIMIT and set it to an amount of bytes that the allocator will not go beyond. Note that this might significantly increase memory pressure, a situation for which the CuArrays.jl memory allocator is currently not optimized (though it is one of my top priorities for the Julia GPU infrastructure)."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "What is the correct way of using a Vulkan VkImage as a CUDA cuArray? I've been trying to follow some examples, however I get a CUDA_ERROR_INVALID_VALUE on a call to cuExternalMemoryGetMappedMipmappedArray() To provide the information in an ordered way. I'm using CUDA 10.1 Base code comes from https://github.com/SaschaWillems/Vulkan, in particular I'm using the 01 - Vulkan Gears demo, enriched with the saveScreenshot method 09 - Capturing screenshots Instead of saving the snapshot image to a file, I'll be sending the snapshot image into CUDA as a CUarray. I've enabled the following instance and device extensions: std::vector&lt;const char*&gt; instanceExtensions = { VK_EXT_DEBUG_REPORT_EXTENSION_NAME, VK_KHR_GET_PHYSICAL_DEVICE_PROPERTIES_2_EXTENSION_NAME, VK_KHR_EXTERNAL_MEMORY_CAPABILITIES_EXTENSION_NAME, VK_KHR_EXTERNAL_SEMAPHORE_CAPABILITIES_EXTENSION_NAME }; std::vector&lt;const char*&gt; deviceExtensions = { VK_KHR_EXTERNAL_MEMORY_EXTENSION_NAME, VK_KHR_EXTERNAL_MEMORY_FD_EXTENSION_NAME, VK_KHR_EXTERNAL_SEMAPHORE_EXTENSION_NAME, VK_KHR_EXTERNAL_SEMAPHORE_FD_EXTENSION_NAME }; I have a VkImage, created as follows: // Create the linear tiled destination image to copy to and to read the memory from VkImageCreateInfo imageCreateCI(vks::initializers::imageCreateInfo()); imageCreateCI.imageType = VK_IMAGE_TYPE_2D; // Note that vkCmdBlitImage (if supported) will also do format conversions if the swapchain color format would differ imageCreateCI.format = VK_FORMAT_R8G8B8A8_UNORM; imageCreateCI.extent.width = width; imageCreateCI.extent.height = height; imageCreateCI.extent.depth = 1; imageCreateCI.arrayLayers = 1; imageCreateCI.mipLevels = 1; imageCreateCI.initialLayout = VK_IMAGE_LAYOUT_UNDEFINED; imageCreateCI.samples = VK_SAMPLE_COUNT_1_BIT; imageCreateCI.tiling = VK_IMAGE_TILING_LINEAR; imageCreateCI.sharingMode = VK_SHARING_MODE_EXCLUSIVE; imageCreateCI.usage = VK_IMAGE_USAGE_TRANSFER_SRC_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT; VkExternalMemoryImageCreateInfoKHR extImageCreateInfo = {}; /* * Indicate that the memory backing this image will be exported in an * fd. In some implementations, this may affect the call to * GetImageMemoryRequirements() with this image. */ extImageCreateInfo.sType = VK_STRUCTURE_TYPE_EXTERNAL_MEMORY_IMAGE_CREATE_INFO_KHR; extImageCreateInfo.handleTypes |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT_KHR; imageCreateCI.pNext = &amp;extImageCreateInfo; // Create the image VkImage dstImage; VK_CHECK_RESULT(vkCreateImage(device, &amp;imageCreateCI, nullptr, &amp;dstImage)); // Create memory to back up the image VkMemoryRequirements memRequirements; VkMemoryAllocateInfo memAllocInfo(vks::initializers::memoryAllocateInfo()); VkDeviceMemory dstImageMemory; vkGetImageMemoryRequirements(device, dstImage, &amp;memRequirements); memAllocInfo.allocationSize = memRequirements.size; // Memory must be host visible to copy from memAllocInfo.memoryTypeIndex = vulkanDevice-&gt;getMemoryType(memRequirements.memoryTypeBits, VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT); VkExportMemoryAllocateInfoKHR exportInfo = {}; exportInfo.sType = VK_STRUCTURE_TYPE_EXPORT_MEMORY_ALLOCATE_INFO_KHR; exportInfo.handleTypes = VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT_KHR; memAllocInfo.pNext = &amp;exportInfo; VK_CHECK_RESULT(vkAllocateMemory(device, &amp;memAllocInfo, nullptr, &amp;dstImageMemory)); VK_CHECK_RESULT(vkBindImageMemory(device, dstImage, dstImageMemory, 0)); From there I'll: Get the Vulkan Memory Handler: int CuEncoderImpl::getVulkanMemoryHandle(VkDevice device, VkDeviceMemory memory) { // Get handle to memory of the VkImage int fd = -1; VkMemoryGetFdInfoKHR fdInfo = { }; fdInfo.sType = VK_STRUCTURE_TYPE_MEMORY_GET_FD_INFO_KHR; fdInfo.memory = memory; fdInfo.handleType = VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT_KHR; auto func = (PFN_vkGetMemoryFdKHR) vkGetDeviceProcAddr(device, \"vkGetMemoryFdKHR\"); if (!func) { printf(\"Failed to locate function vkGetMemoryFdKHR\\n\"); return -1; } VkResult r = func(device, &amp;fdInfo, &amp;fd); if (r != VK_SUCCESS) { printf(\"Failed executing vkGetMemoryFdKHR [%d]\\n\", r); return -1; } return fd; } Import the memory: CUDA_EXTERNAL_MEMORY_HANDLE_DESC memDesc = { }; memDesc.type = CU_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD; memDesc.handle.fd = getVulkanMemoryHandle(device, memory); memDesc.size = extent.width*extent.height*4; CUDA_DRVAPI_CALL(cuImportExternalMemory(&amp;externalMem, &amp;memDesc)); And map the memory: This is the step that it is failing. CUarray CuEncoderImpl::getCUDAArrayFromExternalMemory(const VkExtent3D &amp;extent,const CUexternalMemory &amp;m_extMem) { CUmipmappedArray m_mipmapArray; CUresult result = CUDA_SUCCESS; CUarray array; CUDA_ARRAY3D_DESCRIPTOR arrayDesc = { }; arrayDesc.Width = extent.width; arrayDesc.Height = extent.height; arrayDesc.Depth = 0; arrayDesc.Format = CU_AD_FORMAT_UNSIGNED_INT32; arrayDesc.NumChannels = 4; arrayDesc.Flags = CUDA_ARRAY3D_SURFACE_LDST; CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC mipmapArrayDesc = { }; mipmapArrayDesc.arrayDesc = arrayDesc; mipmapArrayDesc.numLevels = 1; mipmapArrayDesc.offset = 0; CUDA_DRVAPI_CALL(cuExternalMemoryGetMappedMipmappedArray(&amp;m_mipmapArray, m_extMem, &amp;mipmapArrayDesc)); CUDA_DRVAPI_CALL(cuMipmappedArrayGetLevel(&amp;array, m_mipmapArray, 0)); return array; } I've been trying multiple combinations of the parameters, but failed so far. The error point to an invalid parameter, but I'm not sure how to find what's wrong. Only thing that had worked is to map the Vulkan image memory to a host buffer and then copying it into the CUDA array... but I guess that's expensive and I'd like to avoid it if possible.",
        "answers": [
            [
                "For the record, I finally got this to work. Some notes and the modifications I had to do to the code listed in the question: Vulkan-CUDA interoperability is advertised as a feature of CUDA 10, see CUDA 10 Features revealed The tiling of the image that is going to be mapped had to be `VK_IMAGE_TILING_OPTIMAL imageCreateCI.tiling = VK_IMAGE_TILING_OPTIMAL; The memory for that image must be allocated with the VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT memAllocInfo.memoryTypeIndex = vulkanDevice-&gt;getMemoryType(memRequirements.memoryTypeBits, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT); The memory descriptor when importing the memory should use the memory size that was returned in the memory requirements (size below is memRequirements.size from the code creating the image): CUDA_EXTERNAL_MEMORY_HANDLE_DESC memDesc = { }; memDesc.type = CU_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD; memDesc.handle.fd = getVulkanMemoryHandle(device, memory); memDesc.size = size; CUDA_DRVAPI_CALL(cuImportExternalMemory(&amp;externalMem, &amp;memDesc)); Finally the mapped array is described as being CU_AD_FORMAT_UNSIGNED_INT8 with four channels and with a CUDA_ARRAY3D_COLOR_ATTACHMENT CUDA_ARRAY3D_DESCRIPTOR arrayDesc = { }; arrayDesc.Width = extent.width; arrayDesc.Height = extent.height; arrayDesc.Depth = 0; arrayDesc.Format = CU_AD_FORMAT_UNSIGNED_INT8; arrayDesc.NumChannels = 4; arrayDesc.Flags = CUDA_ARRAY3D_COLOR_ATTACHMENT; CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC mipmapArrayDesc = { }; mipmapArrayDesc.arrayDesc = arrayDesc; mipmapArrayDesc.numLevels = 1; mipmapArrayDesc.offset = 0; CUDA_DRVAPI_CALL(cuExternalMemoryGetMappedMipmappedArray(&amp;m_mipmapArray, m_extMem, &amp;mipmapArrayDesc)); After those changes, I was able to get it to work. I few the changes were glaring mistakes on my side (like the size), a few things I found carefully re-reading the documentation for the 100th time, others were guesses at hints in the documentation and, finally, a lot of trial and error."
            ]
        ],
        "votes": [
            14.0000001
        ]
    },
    {
        "question": "I'm trying to process a video, frame by frame. To do this I want to create a texture containing the current frame and pass it to the kernel. The frames are 1440*1080 pixel with each pixel being represented by an unsigned char, e.g. 8 bit. I followed the instructions, however my program always fails at the point where the texture is created. Error-Code 0x11: \"invalid arguments\". Here is my code: // allocate cuda array in device memory cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(8, 0, 0, 0, cudaChannelFormatKindUnsigned); cudaArray* cuArray; cudaMallocArray(&amp;cuArray, &amp;channelDesc, width, height); // copy frame_in to device memory int size = width * height * sizeof(char); cudaMemcpyToArray(cuArray, 0, 0, frame_in.data, size, cudaMemcpyHostToDevice); // specify texture cudaResourceDesc resDesc; memset(&amp;resDesc, 0, sizeof(resDesc)); resDesc.resType = cudaResourceTypeArray; resDesc.res.array.array = cuArray; // specify texture object parameters cudaTextureDesc texDesc; texDesc.addressMode[0] = cudaAddressModeWrap; texDesc.addressMode[1] = cudaAddressModeWrap; texDesc.filterMode = cudaFilterModePoint; texDesc.readMode = cudaReadModeElementType; texDesc.normalizedCoords = 1; // !FAILS! create texture object cudaTextureObject_t texObj = NULL; cudaCreateTextureObject(&amp;texObj, &amp;resDesc, &amp;texDesc, NULL);",
        "answers": [
            [
                "Got it, I forgot the memset() for the cudaTextureDesc. Should read: // specify texture object parameters cudaTextureDesc texDesc; memset(&amp;texDesc, 0, sizeof(texDesc)); texDesc.addressMode[0] = cudaAddressModeWrap; texDesc.addressMode[1] = cudaAddressModeWrap; texDesc.filterMode = cudaFilterModePoint; texDesc.readMode = cudaReadModeElementType; texDesc.normalizedCoords = 1;"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a JCuda project that's encountering an access violation whenever it tries to create a texture object using the driver API. Java HotSpot claims that the error is coming from nvcuda.dll. The underlying CUarray from which the texture is being created seems to be populated correctly; copying its contents back into a host-side float array results in an array that's identical to the initial host-side data. That means that the error itself has to be something in the texture declaration, right? Running the code using cuda-memcheck reveals no errors. Here is the code that's encountering the error: import jcuda.Pointer; import jcuda.Sizeof; import jcuda.driver.*; public class Main { public static void main(String[] args) { init(); float[] hostArray = new float[]{0, 1, 2, 3, 4, 5, 6, 7}; int[] dims = new int[]{2,2,2}; CUdeviceptr deviceArray = new CUdeviceptr(); JCudaDriver.cuMemAlloc(deviceArray, hostArray.length * Sizeof.FLOAT); JCudaDriver.cuMemcpyHtoD(deviceArray, Pointer.to(hostArray), hostArray.length * Sizeof.FLOAT); // initialize the opaque array object to represent the texture's data CUarray cuArray = makeCudaArray(dims); // populate the opaque array object copyDataIntoCudaArray(deviceArray, cuArray, dims); JCudaDriver.cuMemFree(deviceArray); // create the various descriptors CUDA_RESOURCE_DESC resourceDescriptor = makeResourceDescriptor(cuArray); CUDA_TEXTURE_DESC textureDescriptor = makeTextureDescriptor(); CUDA_RESOURCE_VIEW_DESC resourceViewDescriptor = makeResourceViewDescriptor(dims); CUtexObject texture = new CUtexObject(); System.out.println(\"About to hit an access violation:\"); JCudaDriver.cuTexObjectCreate(texture, resourceDescriptor, textureDescriptor, resourceViewDescriptor); } static void init() { JCudaDriver.setExceptionsEnabled(true); JCudaDriver.cuInit(0); int[] deviceCount = new int[1]; JCudaDriver.cuDeviceGetCount(deviceCount); CUdevice currentDevice = new CUdevice(); JCudaDriver.cuDeviceGet(currentDevice, 0); CUcontext currentContext = new CUcontext(); JCudaDriver.cuCtxCreate(currentContext, 0, currentDevice); } static CUarray makeCudaArray(int[] dims) { CUarray array = new CUarray(); CUDA_ARRAY3D_DESCRIPTOR arrayDescriptor = new CUDA_ARRAY3D_DESCRIPTOR(); arrayDescriptor.Width = dims[0]; arrayDescriptor.Height = dims[1]; arrayDescriptor.Depth = dims[2]; arrayDescriptor.Format = CUarray_format.CU_AD_FORMAT_FLOAT; arrayDescriptor.NumChannels = 1; arrayDescriptor.Flags = 0; JCudaDriver.cuArray3DCreate(array, arrayDescriptor); return array; } static void copyDataIntoCudaArray(CUdeviceptr deviceArray, CUarray array, int[] dims) { CUDA_MEMCPY3D copyParams = new CUDA_MEMCPY3D(); copyParams.srcMemoryType = CUmemorytype.CU_MEMORYTYPE_DEVICE; copyParams.srcDevice = deviceArray; copyParams.srcXInBytes = 0; copyParams.srcY = 0; copyParams.srcZ = 0; copyParams.srcPitch = (long) dims[0] * Sizeof.FLOAT; copyParams.srcHeight = dims[1]; copyParams.srcLOD = 0; copyParams.dstMemoryType = CUmemorytype.CU_MEMORYTYPE_ARRAY; copyParams.dstArray = array; copyParams.dstXInBytes = 0; copyParams.dstY = 0; copyParams.dstZ = 0; copyParams.dstLOD = 0; copyParams.WidthInBytes = (long) dims[0] * Sizeof.FLOAT; copyParams.Height = dims[1]; copyParams.Depth = dims[2]; JCudaDriver.cuMemcpy3D(copyParams); } static CUDA_RESOURCE_DESC makeResourceDescriptor(CUarray cuArray) { CUDA_RESOURCE_DESC resourceDescriptor = new CUDA_RESOURCE_DESC(); resourceDescriptor.resType = CUresourcetype.CU_RESOURCE_TYPE_ARRAY; resourceDescriptor.array_hArray = cuArray; resourceDescriptor.flags = 0; return resourceDescriptor; } static CUDA_TEXTURE_DESC makeTextureDescriptor() { CUDA_TEXTURE_DESC textureDescriptor = new CUDA_TEXTURE_DESC(); textureDescriptor.addressMode = new int[]{ CUaddress_mode.CU_TR_ADDRESS_MODE_CLAMP, CUaddress_mode.CU_TR_ADDRESS_MODE_CLAMP, CUaddress_mode.CU_TR_ADDRESS_MODE_CLAMP }; textureDescriptor.filterMode = CUfilter_mode.CU_TR_FILTER_MODE_LINEAR; textureDescriptor.flags = 0; textureDescriptor.maxAnisotropy = 1; textureDescriptor.mipmapFilterMode = CUfilter_mode.CU_TR_FILTER_MODE_POINT; textureDescriptor.mipmapLevelBias = 0; textureDescriptor.minMipmapLevelClamp = 0; textureDescriptor.maxMipmapLevelClamp = 0; return textureDescriptor; } static CUDA_RESOURCE_VIEW_DESC makeResourceViewDescriptor(int[] dims) { CUDA_RESOURCE_VIEW_DESC resourceViewDescriptor = new CUDA_RESOURCE_VIEW_DESC(); resourceViewDescriptor.format = CUresourceViewFormat.CU_RES_VIEW_FORMAT_FLOAT_1X32; resourceViewDescriptor.width = dims[0]; resourceViewDescriptor.height = dims[1]; resourceViewDescriptor.depth = dims[2]; resourceViewDescriptor.firstMipmapLevel = 0; resourceViewDescriptor.lastMipmapLevel = 0; resourceViewDescriptor.firstLayer = 0; resourceViewDescriptor.lastLayer = 0; return resourceViewDescriptor; } } What am I doing wrong here?",
        "answers": [
            [
                "The reason for this access violation was a bug in JCuda 0.9.0. The texture handle was erroneously passed to the native function as a NULL pointer. This is fixed in this commit, and the fix will be part of the next release. A test case based on the code in the question has been added. Update: This issue is fixed in JCuda 0.9.0d."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "When using texture memory in CUDA, we call cudaBindTextureToArray(texRef, cuArray, channelDesc), use the texture, and then unbind it: cudaUnbindTexture(texRef); With surface memory, there is an analogous cudaBindSurfaceToArray(surfRef, cuArray); but in the Documentation I have not found any mention of unbinding the surface, and my guess cudaUnbindSurface(surfRef); throws an error. Is surface reference unbinding not necessary/impossible and why is this different from texture reference?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I just want to put the test.jpg into the texture memory, and then through the text2D () to show the picture, but the results are very strange. The \"result\" should be the same as \"gray\".Click there to get the pic:window:gray and window:result.The \"gray\" was reduced to the original quarter and arranged in a row. This is code I'm using #include &lt;cuda_runtime.h&gt; #include &lt;highgui/highgui.hpp&gt; #include &lt;imgproc/imgproc.hpp&gt; #include \"device_launch_parameters.h\" #include &lt;iostream&gt; using namespace std; using namespace cv; texture&lt;float, 2, cudaReadModeElementType&gt; texRef; __global__ void transformKernel(uchar* output, int width, int height) { unsigned int x = blockIdx.x * blockDim.x + threadIdx.x; unsigned int y = blockIdx.y * blockDim.y + threadIdx.y; float u = x / (float)width; float v = y / (float)height; output[(y * width + x)] = tex2D(texRef, x, y); } int main() { Mat image = imread(\"test.jpg\", 0); int width=512; int height=512; resize(image, image, Size(width, height)); imshow(\"gray\", image); cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat); cudaArray* cuArray; cudaMallocArray(&amp;cuArray, &amp;channelDesc, width, height); cudaMemcpyToArray(cuArray, 0, 0, image.data, sizeof(uchar)*width*height, cudaMemcpyHostToDevice); texRef.addressMode[0] = cudaAddressModeWrap; texRef.addressMode[1] = cudaAddressModeWrap; texRef.filterMode = cudaFilterModeLinear; texRef.normalized = false; cudaBindTextureToArray(texRef, cuArray, channelDesc); Mat imageOutput = Mat(Size(width, height), CV_8UC1); uchar * output = imageOutput.data; cudaMalloc((void**)&amp;output, width * height * sizeof(float)); dim3 dimBlock(16, 16); dim3 dimGrid((width + dimBlock.x - 1) / dimBlock.x, (height + dimBlock.y - 1) / dimBlock.y); transformKernel &lt;&lt; &lt;dimGrid, dimBlock &gt;&gt; &gt; (output, width, height); cudaMemcpy(imageOutput.data, output, height*width, cudaMemcpyDeviceToHost); imshow(\"result\", imageOutput); waitKey(); cudaFreeArray(cuArray); cudaFree(output); } Could anyone tell me why?",
        "answers": [
            [
                "#include &lt;cuda_runtime.h&gt; #include &lt;highgui/highgui.hpp&gt; #include &lt;imgproc/imgproc.hpp&gt; #include \"device_launch_parameters.h\" #include &lt;iostream&gt; using namespace std; using namespace cv; texture&lt;uchar, 2, cudaReadModeElementType&gt; texRef; __global__ void transformKernel(uchar* output, int width, int height) { unsigned int x = blockIdx.x * blockDim.x + threadIdx.x; unsigned int y = blockIdx.y * blockDim.y + threadIdx.y; float u = x / (float)width; float v = y / (float)height; output[(y * width + x)] = tex2D(texRef, x, y); } int main() { Mat image = imread(\"test.jpg\", 0); int width=512; int height=512; resize(image, image, Size(width, height)); imshow(\"gray\", image); cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(8, 0, 0, 0, cudaChannelFormatKindUnsigned); cudaArray* cuArray; cudaMallocArray(&amp;cuArray, &amp;channelDesc, width, height); cudaMemcpyToArray(cuArray, 0, 0, image.data, sizeof(uchar)*width*height, cudaMemcpyHostToDevice); texRef.addressMode[0] = cudaAddressModeWrap; texRef.addressMode[1] = cudaAddressModeWrap; texRef.addressMode[2] = cudaAddressModeWrap; texRef.filterMode = cudaFilterModePoint; texRef.normalized = false; cudaBindTextureToArray(texRef, cuArray, channelDesc); Mat imageOutput = Mat(Size(width, height), CV_8UC1); uchar * output = imageOutput.data; cudaMalloc((void**)&amp;output, width * height * sizeof(uchar)); dim3 dimBlock(16, 16); dim3 dimGrid((width + dimBlock.x - 1) / dimBlock.x, (height + dimBlock.y - 1) / dimBlock.y); transformKernel &lt;&lt; &lt;dimGrid, dimBlock &gt;&gt; &gt; (output, width, height); cudaMemcpy(imageOutput.data, output, height*width* sizeof(uchar), cudaMemcpyDeviceToHost); imshow(\"result\", imageOutput); waitKey(); cudaFreeArray(cuArray); cudaFree(output); } just change the \"float\" to \"uchar\""
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have setup the cudaArray, and have bound it to a texture: texture&lt;float, 2, cudaReadModeElementType&gt; tex; cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat); cudaArray *cuArray; checkCudaErrors(cudaMallocArray(&amp;cuArray, &amp;channelDesc, width, height)); checkCudaErrors(cudaMemcpyToArray(cuArray, 0, 0, hData, size, cudaMemcpyHostToDevice)); Now I am wondering, if the content within the cuArray and tex remains the same all the time during the calculation, can I pass tex and/or cuArray to another function so that I don't have to do the binding every time? Something like this: DoJobUsingTex(float* output, float* input, int size, texture tex) { \\\\ do something here }",
        "answers": [
            [
                "CUDA introduced texture objects when CUDA 5 and Kepler hardware were released. These are so called \"bindless\" textures which can be passed by value to kernels, so there isn't a need to rebind memory every time you want to run a kernel on different texture data. You can read more about their use here."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I wrote following code to see how to use texture memory for 1D array.but tex1D function is not fetching the value from array for corresponding thread id.Please correct this code and tell me how to use texture memory for 1D array efficiently and effectively. __global__ void sum(float *b,cudaTextureObject_t texObj) { b[threadIdx.x]=tex1D&lt;float&gt;(texObj,threadIdx.x); //printf(\"\\n%f\\n\",tex1Dfetch&lt;float&gt;(texObj,threadIdx.x)); } int main() { float *a,*b; float *d_a,*d_b; int i; a=(float*)malloc(sizeof(float)*5); b=(float*)malloc(sizeof(float)*5); for(i=0;i&lt;5;i++) a[i]=i; cudaChannelFormatDesc channelDesc =cudaCreateChannelDesc(32, 0, 0, 0,cudaChannelFormatKindFloat); cudaArray* cuArray; cudaMallocArray(&amp;cuArray, &amp;channelDesc, 5, 0); cudaMemcpyToArray(cuArray, 0, 0, a,sizeof(float)*5,cudaMemcpyHostToDevice); struct cudaResourceDesc resDesc; memset(&amp;resDesc, 0, sizeof(resDesc)); resDesc.resType = cudaResourceTypeArray; resDesc.res.array.array = cuArray; struct cudaTextureDesc texDesc; memset(&amp;texDesc, 0, sizeof(texDesc)); texDesc.addressMode[0] = cudaAddressModeWrap; texDesc.addressMode[1] = cudaAddressModeWrap; texDesc.filterMode = cudaFilterModeLinear; texDesc.readMode = cudaReadModeElementType; texDesc.normalizedCoords = 1; // Create texture object cudaTextureObject_t texObj = 0; cudaCreateTextureObject(&amp;texObj, &amp;resDesc, &amp;texDesc, NULL); cudaMalloc(&amp;d_b, 5* sizeof(float)); sum&lt;&lt;&lt;1,5&gt;&gt;&gt;(d_b,texObj); // Free device memory cudaMemcpy(b,d_b,sizeof(float),cudaMemcpyDeviceToHost); for(i=0;i&lt;5;i++) printf(\"%f\\t\",b[i]); cudaDestroyTextureObject(texObj); cudaFreeArray(cuArray); cudaFree(d_b); return 0; }",
        "answers": [
            [
                "There are at least 2 issues: You are only copying back one float quantity from device to host at the end: cudaMemcpy(b,d_b,sizeof(float),cudaMemcpyDeviceToHost); ^^^^^^^^^^^^^ if you want to print 5 values, you should copy 5 values back: cudaMemcpy(b,d_b,5*sizeof(float),cudaMemcpyDeviceToHost); You have selected normalized coordinates: texDesc.normalizedCoords = 1; this means you should be passing a floating point coordinate between 0 and 1 as your index, not an integer coordinate from 0 to 4: b[threadIdx.x]=tex1D&lt;float&gt;(texObj,threadIdx.x); ^^^^^^^^^^^ use something like this instead: b[threadIdx.x]=tex1D&lt;float&gt;(texObj, ((float)threadIdx.x/5.0f)); with those changes, I get sensible results. Here's a fully worked code: $ cat t3.cu #include &lt;stdio.h&gt; __global__ void sum(float *b,cudaTextureObject_t texObj) { b[threadIdx.x]=tex1D&lt;float&gt;(texObj,((float)(threadIdx.x+1)/5.0f)); //printf(\"\\n%f\\n\",tex1Dfetch&lt;float&gt;(texObj,threadIdx.x)); } int main() { float *a,*b; float *d_b; int i; a=(float*)malloc(sizeof(float)*5); b=(float*)malloc(sizeof(float)*5); for(i=0;i&lt;5;i++) a[i]=i; cudaChannelFormatDesc channelDesc =cudaCreateChannelDesc(32, 0, 0, 0,cudaChannelFormatKindFloat); cudaArray* cuArray; cudaMallocArray(&amp;cuArray, &amp;channelDesc, 5, 0); cudaMemcpyToArray(cuArray, 0, 0, a,sizeof(float)*5,cudaMemcpyHostToDevice); struct cudaResourceDesc resDesc; memset(&amp;resDesc, 0, sizeof(resDesc)); resDesc.resType = cudaResourceTypeArray; resDesc.res.array.array = cuArray; struct cudaTextureDesc texDesc; memset(&amp;texDesc, 0, sizeof(texDesc)); texDesc.addressMode[0] = cudaAddressModeWrap; texDesc.addressMode[1] = cudaAddressModeWrap; texDesc.filterMode = cudaFilterModeLinear; texDesc.readMode = cudaReadModeElementType; texDesc.normalizedCoords = 1; // Create texture object cudaTextureObject_t texObj = 0; cudaCreateTextureObject(&amp;texObj, &amp;resDesc, &amp;texDesc, NULL); cudaMalloc(&amp;d_b, 5* sizeof(float)); sum&lt;&lt;&lt;1,4&gt;&gt;&gt;(d_b,texObj); // Free device memory cudaMemcpy(b,d_b,5*sizeof(float),cudaMemcpyDeviceToHost); for(i=0;i&lt;4;i++) printf(\"%f\\t\",b[i]); printf(\"\\n\"); cudaDestroyTextureObject(texObj); cudaFreeArray(cuArray); cudaFree(d_b); return 0; } $ nvcc -arch=sm_61 -o t3 t3.cu $ cuda-memcheck ./t3 ========= CUDA-MEMCHECK 0.500000 1.500000 2.500000 3.500000 ========= ERROR SUMMARY: 0 errors $ Note that I did make some other changes. In particular, I've adjusted your sample points as well as the sample quantity to choose sample points that are linearly interpolated halfway between each of the 5 data points you have (0, 1, 2, 3, 4) yielding a total output of 4 quantities (0.5, 1.5, 2.5, 3.5) representing the midpoints between your 5 datapoints. If you want to learn more about normalized coordinate indexing, this is covered in the programming guide as are other concepts such as border modes and the like. Furthermore, there are various CUDA sample codes that demonstrate proper use of textures."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am new to CUDA. I have figured out how to do 1D and 2D textures in CUDA. However, I am struggling with how to use a 1D layered texture. The output of my kernel which uses the texture is all zeros, which is definitely incorrect. However, I am not sure what I am doing wrong. I have serious doubts that I set up this texture correctly, but I checked for cuda errors everywhere and couldn't find any issues. Can someone show me how to correctly set up a 1D layered texture and use it. Here is my code. Thanks in advance: // To Compile: nvcc backproj.cu -o backproj.out // To Run: ./backproj.out // Includes, system #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;string.h&gt; #include &lt;math.h&gt; // Includes CUDA #include &lt;cuda_runtime.h&gt; #include &lt;cuda_profiler_api.h&gt; #define pi acos(-1) // 1D float textures texture&lt;float, cudaTextureType1DLayered, cudaReadModeElementType&gt; texRef; // 1D interpolation kernel: Should be very similar to what you get if you used 1D interpolation on MATLAB __global__ void interp1Kernel(float* d_output, float* d_locations, int numlocations, int layer) { unsigned int location_idx = blockIdx.x * blockDim.x + threadIdx.x; if (location_idx &lt; numlocations) { // Get the location you want to interpolate from the array float loc2find = (float) d_locations[location_idx] + 0.5f; // Read from texture and write to global memory d_output[location_idx] = tex1DLayered(texRef, loc2find, layer); } } // Host code int main() { // Setup h_data and locations to interpolate from const unsigned int len = 10; const unsigned int numlayers = 3; const unsigned int upsamp = 3; const unsigned int loclen = 1 + (len - 1) * upsamp; float idx_spacing = 1/(float)upsamp; float h_data[len][numlayers], h_loc[loclen]; for (int i = 0; i &lt; len; i++) for (int j = 0; j &lt; numlayers; j++) h_data[i][j] = 1+cosf((float) pi*i/(j+1.0f)); for (int i = 0; i &lt; loclen; i ++) h_loc[i] = i*idx_spacing; // Get the memory locations you want float* d_loc; cudaMalloc(&amp;d_loc, loclen * sizeof(float)); cudaMemcpy(d_loc, h_loc, loclen*sizeof(float), cudaMemcpyHostToDevice); // Allocate CUDA array in device memory cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat); cudaArray* cuArray; cudaMallocArray(&amp;cuArray, &amp;channelDesc, len, numlayers); // Copy to device memory some data located at address h_data in host memory cudaMemcpyToArray(cuArray, 0, 0, h_data, len * numlayers * sizeof(float), cudaMemcpyHostToDevice); // Set texture reference parameters texRef.addressMode[0] = cudaAddressModeBorder; texRef.filterMode = cudaFilterModeLinear; texRef.normalized = false; // Bind the array to the texture reference cudaBindTextureToArray(texRef, cuArray, channelDesc); // Allocate result of transformation in device memory float* d_output; cudaMalloc(&amp;d_output, loclen * sizeof(float)); // Invoke kernel int thdsPerBlk = 256; int blksPerGrid = (int) (loclen / thdsPerBlk) + 1; printf(\"Threads Per Block: %d, Blocks Per Grid: %d\\n\", thdsPerBlk, blksPerGrid); interp1Kernel &lt;&lt;&lt;blksPerGrid, thdsPerBlk &gt;&gt;&gt;(d_output, d_loc, loclen, 0); // Print Results printf(\"\\n Original Indices \\n\"); for (int i = 0; i &lt; len; i++) printf(\" %d \", i); printf(\"\\n Original array \\n\"); for (int i = 0; i &lt; len; i++) printf(\"%5.3f \", h_data[i][0]); printf(\"\\n Output Indices \\n\"); for (int i = 0; i &lt; loclen; i++) printf(\"%5.3f \", h_loc[i]); printf(\"\\n Output Array \\n\"); cudaMemcpy(h_loc, d_output, loclen * sizeof(float), cudaMemcpyDeviceToHost); for (int i = 0; i &lt; loclen; i++) printf(\"%5.3f \", h_loc[i]); printf(\"\\n\"); // Free device memory cudaFreeArray(cuArray); cudaFree(d_output); return 0; }",
        "answers": [
            [
                "You must use cudaMalloc3DArray with the cudaArrayLayered flag set to allocate memory for layered textures. There is a complete example of layered texture usage in the toolkit samples which you can study to see how they work."
            ],
            [
                "Unfortunately, the CUDA SDK only shows you how to do it when you have 2D layered texture. There is some more trickiness when it comes to 1D layered textures. It turns out you have to put a 0 into the second argument for make_cudaExtent when making the extentDesc as follows: cudaExtent extentDesc = make_cudaExtent(len, 0, numlayers); // &lt;-- 0 height required for 1Dlayered However, when using make_cudaExtent for mParams.extent for cudaMemcpy3D, you still need to put a 1 for the second argument: mParams.extent = make_cudaExtent(len, 1, numlayers); // &lt;&lt;-- non zero height required for memcpy to do anything Furthermore, there are some other non-obvious details such as the pitch for make_cudaPitchedPtr. So I have included my complete and functioning code for the 1D layered texture. I couldn't find an example of this anywhere. So hopefully this will help out others who are in the same boat: // To Compile: nvcc layeredTexture1D.cu -o layeredTexture1D.out // To Run: ./layeredTexture1D.out // Includes, system #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;string.h&gt; #include &lt;math.h&gt; // Includes CUDA #include &lt;cuda_runtime.h&gt; #include &lt;cuda_profiler_api.h&gt; #define pi acos(-1) // 1D float textures: x is for input values, y is for corresponding output values texture&lt;float, cudaTextureType1DLayered, cudaReadModeElementType&gt; texRef; // 1D interpolation kernel: Should be very similar to what you get if you used 1D interpolation on MATLAB __global__ void interp1Kernel(float* d_output, float* d_locations, int numlocations, int numlayers) { unsigned int location_idx = blockIdx.x * blockDim.x + threadIdx.x; unsigned int layer = blockIdx.y * blockDim.y + threadIdx.y; if (location_idx &lt; numlocations &amp;&amp; layer &lt; numlayers) { // Get the location you want to interpolate from the array float loc2find = (float)d_locations[location_idx] + 0.5f; // Read from texture and write to global memory d_output[location_idx + layer*numlocations] = tex1DLayered(texRef, loc2find, layer); //printf(\"location=%d layer=%d loc2find=%f result=%f \\n\", location_idx, layer, loc2find, d_output[location_idx]); } } // Host code int main() { // Setup h_data and locations to interpolate from const unsigned int len = 7; const unsigned int numlayers = 3; const unsigned int upsamp = 4; const unsigned int loclen = 1 + (len - 1) * upsamp; float idx_spacing = 1 / (float)upsamp; float h_data[numlayers*len], h_loc[loclen]; for (int i = 0; i &lt; len; i++) for (int j = 0; j &lt; numlayers; j++) h_data[len*j + i] = 1 + cosf((float)pi*i / (j + 1.0f)); for (int i = 0; i &lt; loclen; i++) h_loc[i] = i*idx_spacing; // Get the memory locations you want float* d_loc; cudaMalloc(&amp;d_loc, loclen * sizeof(float)); cudaMemcpy(d_loc, h_loc, loclen*sizeof(float), cudaMemcpyHostToDevice); // Allocate CUDA array in device memory cudaExtent extentDesc = make_cudaExtent(len, 0, numlayers); // &lt;-- 0 height required for 1Dlayered cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat); cudaMemcpy3DParms mParams = { 0 }; mParams.srcPtr = make_cudaPitchedPtr(h_data, len*sizeof(float), len, 1); mParams.kind = cudaMemcpyHostToDevice; mParams.extent = make_cudaExtent(len, 1, numlayers); // &lt;&lt;-- non zero height required for memcpy to do anything cudaArray* cuArray; cudaMalloc3DArray(&amp;cuArray, &amp;channelDesc, extentDesc, cudaArrayLayered); mParams.dstArray = cuArray; cudaMemcpy3D(&amp;mParams); // Set texture reference parameters texRef.addressMode[0] = cudaAddressModeBorder; texRef.filterMode = cudaFilterModeLinear; texRef.normalized = false; // Bind the array to the texture reference cudaBindTextureToArray(texRef, cuArray, channelDesc); // Allocate result of transformation in device memory float *d_output; cudaMalloc(&amp;d_output, loclen * numlayers * sizeof(float)); float h_output[loclen * numlayers]; // Invoke kernel dim3 dimBlock(16, 16, 1); dim3 dimGrid((loclen + dimBlock.x - 1) / dimBlock.x, (numlayers + dimBlock.y - 1) / dimBlock.y, 1); interp1Kernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_output, d_loc, loclen, numlayers); // Print Results printf(\"\\n Original Indices \\n\"); for (int i = 0; i &lt; len; i++) printf(\" %d \", i); printf(\"\\n Original array \\n\"); for (int j = 0; j &lt; numlayers; j++) { for (int i = 0; i &lt; len; i++) { printf(\"%5.3f \", h_data[i + j*len]); } printf(\"\\n\"); } printf(\"\\n Output Indices \\n\"); for (int i = 0; i &lt; loclen; i++) printf(\"%5.3f \", h_loc[i]); printf(\"\\n Output Array \\n\"); cudaMemcpy(h_output, d_output, loclen * numlayers * sizeof(float), cudaMemcpyDeviceToHost); for (int j = 0; j &lt; numlayers; j++) { for (int i = 0; i &lt; loclen; i++) { printf(\"%5.3f \", h_output[i + j*loclen]); } printf(\"\\n\"); } printf(\"\\n\"); // Free device memory cudaFreeArray(cuArray); cudaFree(d_output); return 0; }"
            ]
        ],
        "votes": [
            3.0000001,
            3.0000001
        ]
    },
    {
        "question": "How to deal with OpenGL cube map textures in CUDA? When one want to use OpenGL textures in CUDA kernel one of the things to do is to retrieve a CUDA array from registered image and mapped resource, in this case a texture. In driver API it is done by cuGraphicsSubResourceGetMappedArray call, which in a case of 2D texture is not a problem. But when talking about aforementioned cube map, third parameter of this function requires a face enum (like CU_CUBEMAP_FACE_POSITIVE_X). Thus some questions arise - when one passes such an enum, then the returned texture array will contain only data of that particular face, right? Then how to use cube texture as a whole, to perform cube mapping, likewise: color = texCube(cubeMap, x, y, z); Or is it impossible to do so in CUDA kernel and one need to use 2D textures with proper calculations and sampling in user code?",
        "answers": [
            [
                "OK - I managed to solve the problem myself, though the solution isn't as simple as using another CUDA function. To bind a CUDA texture reference with any texture, be it one obtained from OpenGL or D3D, one has to provide a CUDA array that is mapped to a resource, using cuGraphicsSubResourceGetMappedArray to retrieve it. As I mentioned in the question, it is simple in case of a one or two dimensional texture. But with other available types it is more complicated. At any time we need a CUDA array that the reference is bound to. Same goes with the cube map texture. But in such a case the array has to be a 3D one. The problem is that CUDA driver API provides only the aforementioned function to retrieve a single layer from such a texture resource, and map it to a single, two dimensional array. To get what we want we have to make ourselves the 3D array containing all the layers (or faces in case of a cube map). First of all we have to get arrays for each layer/face using the above function. Next step is to create the 3D array by call to cuArray3DCreate, fed with proper set of parameters (size/number of layers, level of detail, data format, number of channels per texel and some flags). Then we have to copy the layers' arrays to the 3D one with a series of calls to cuMemcpy3D, one for each layer/face array. Finally, we set our target CUDA texture reference with cuTexRefSetArray, fed with the 3D array we created and copied to. Inside of the device code we create a reference with proper texture type and mode (float4 and cube map) and sample it with texCubemap. Below I put a fragment of the function which does all that, available in full length in CIRT Repository (cirt_server.c file, function cirtTexImage3D). //... if (result) { // Create a 3D array... CUDA_ARRAY3D_DESCRIPTOR layeredTextureDescr; layeredTextureDescr.Width = w; layeredTextureDescr.Height = h; layeredTextureDescr.Depth = d; layeredTextureDescr.Format = map_type_to_format(type); layeredTextureDescr.NumChannels = format == CIRT_RGB ? CIRT_RGBA : format; layeredTextureDescr.Flags = map_target_to_flags(target); if (result) result = LogCUDADriverCall(cuArray3DCreate(&amp;hTexRefArray, &amp;layeredTextureDescr), FUN_NAME(\": cuArray3DCreate_tex3D\"), __FILE_LINE__); // Copy the acquired layer/face arrays into the collective 3D one... CUDA_MEMCPY3D layerCopyDescr; layerCopyDescr.srcMemoryType = CU_MEMORYTYPE_ARRAY; layerCopyDescr.srcXInBytes = 0; layerCopyDescr.srcZ = 0; layerCopyDescr.srcY = 0; layerCopyDescr.srcLOD = 0; layerCopyDescr.dstMemoryType = CU_MEMORYTYPE_ARRAY; layerCopyDescr.dstLOD = 0; layerCopyDescr.WidthInBytes = layeredTextureDescr.NumChannels * w; layerCopyDescr.Height = h; layerCopyDescr.Depth = target == CIRT_TEXTURE_CUBE_MAP ? 1 : d; layerCopyDescr.dstArray = hTexRefArray; for (i = 0; i &lt; num_layers; ++i) { layer = ((num_layers == 6) ? CU_CUBEMAP_FACE_POSITIVE_X + i : i); layerCopyDescr.dstXInBytes = 0; layerCopyDescr.dstY = 0; layerCopyDescr.dstZ = i; layerCopyDescr.srcArray = hLayres[i]; if (result) result = LogCUDADriverCall(cuMemcpy3D(&amp;layerCopyDescr), FUN_NAME(\": cuMemcpy3D_tex3D\"), __FILE_LINE__); } // Finally bind the 3D array with texture reference... if (result) LogCUDADriverCall(cuTexRefSetArray(hTexRef, hTexRefArray, CU_TRSA_OVERRIDE_FORMAT), FUN_NAME(\": cuTexRefSetArray_tex3D\"), __FILE_LINE__); if (hLayres) free(hLayres); if (result) current-&gt;m_oTextureManager.m_cuTextureRes[current-&gt;m_oTextureManager.m_nTexCount++] = hTexResource; } //... I've checked it with cube maps only for now but it should work just fine with 3D texture as well."
            ],
            [
                "I'm not real familiar with CUDA directly but I do have some experience in OpenGL and DirectX and I am also familiar with 3D Graphics Rendering APIs, Libraries and Pipelines and having the ability to setup and use those APIs. When I look at your question(s): How to deal with OpenGL cube map textures in CUDA? And you proceed to explain it by this: When one want to use OpenGL textures in CUDA kernel one of the things to do is to retrieve a CUDA array from registered image and mapped resource, in this case a texture. In driver API it is done by cuGraphicsSubResourceGetMappedArray call, which in a case of 2D texture is not a problem. But when talking about aforementioned cube map, third parameter of this function requires a face enum (like CU_CUBEMAP_FACE_POSITIVE_X). Thus some questions arise - when one passes such an enum, then the returned texture array will contain only data of that particular face, right? Then how to use cube texture as a whole, to perform cube mapping, likewise: color = texCube(cubeMap, x, y, z); Or is it impossible to do so in CUDA kernal and one need to use 2D textures with proper calculations and sampling in user code? I went to CUDA's website for their API SDK &amp; Programming Documentations. And found the function in question cuGraphicsSubResourceGetMappedArray() CUresult cuGraphicsSubResourceGetMappedArray ( CUarray* pArray, CUgraphicsResource resource, unsigned int arrayIndex, unsigned int mipLevel ) Get an array through which to access a subresource of a mapped graphics resource. Parameters pArray - Returned array through which a subresource of resource may be accessed resource - Mapped resource to access arrayIndex - Array index for array textures or cubemap face index as defined by CUarray_cubemap_face for cubemap textures for the subresource to access mipLevel - Mipmap level for the subresource to access Returns CUDA_SUCCESS, CUDA_ERROR_DEINITIALIZED, CUDA_ERROR_NOT_INITIALIZED, CUDA_ERROR_INVALID_CONTEXT, CUDA_ERROR_INVALID_VALUE, CUDA_ERROR_INVALID_HANDLE, CUDA_ERROR_NOT_MAPPED, CUDA_ERROR_NOT_MAPPED_AS_ARRAY Description Returns in *pArray an array through which the subresource of the mapped graphics resource resource which corresponds to array index arrayIndex and mipmap level mipLevel may be accessed. The value set in *pArray may change every time that resource is mapped. If resource is not a texture then it cannot be accessed via an array and CUDA_ERROR_NOT_MAPPED_AS_ARRAY is returned. If arrayIndex is not a valid array index for resource then CUDA_ERROR_INVALID_VALUE is returned. If mipLevel is not a valid mipmap level for resource then CUDA_ERROR_INVALID_VALUE is returned. If resource is not mapped then CUDA_ERROR_NOT_MAPPED is returned. Note: Note that this function may also return error codes from previous, asynchronous launches. See also: cuGraphicsResourceGetMappedPointer Read more at: http://docs.nvidia.com/cuda/cuda-driver-api/index.html#ixzz4ic22V4Dz Follow us: @GPUComputing on Twitter | NVIDIA on Facebook This function method was found in NVidia CUDA's DriverAPI and not in their RuntimeAPI. When understanding hardware with CUDA capability is that there is a difference between the Host and Device programmable pipelines which can be found here: http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#axzz4ic6tFjXR 2. Heterogeneous Computing CUDA programming involves running code on two different platforms concurrently: a host system with one or more CPUs and one or more CUDA-enabled NVIDIA GPU devices. While NVIDIA GPUs are frequently associated with graphics, they are also powerful arithmetic engines capable of running thousands of lightweight threads in parallel. This capability makes them well suited to computations that can leverage parallel execution. However, the device is based on a distinctly different design from the host system, and it's important to understand those differences and how they determine the performance of CUDA applications in order to use CUDA effectively. 2.1. Differences between Host and Device The primary differences are in threading model and in separate physical memories: Threading resources - Execution pipelines on host systems can support a limited number of concurrent threads. Servers that have four hex-core processors today can run only 24 threads concurrently (or 48 if the CPUs support Hyper-Threading.) By comparison, the smallest executable unit of parallelism on a CUDA device comprises 32 threads (termed a warp of threads). Modern NVIDIA GPUs can support up to 1536 active threads concurrently per multiprocessor (see Features and Specifications of the CUDA C Programming Guide) On GPUs with 16 multiprocessors, this leads to more than 24,000 concurrently active threads. Threads - Threads on a CPU are generally heavyweight entities. The operating system must swap threads on and off CPU execution channels to provide multithreading capability. Context switches (when two threads are swapped) are therefore slow and expensive. By comparison, threads on GPUs are extremely lightweight. In a typical system, thousands of threads are queued up for work (in warps of 32 threads each). If the GPU must wait on one warp of threads, it simply begins executing work on another. Because separate registers are allocated to all active threads, no swapping of registers or other state need occur when switching among GPU threads. Resources stay allocated to each thread until it completes its execution. In short, CPU cores are designed to minimize latency for one or two threads at a time each, whereas GPUs are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput. RAM - The host system and the device each have their own distinct attached physical memories. As the host and device memories are separated by the PCI Express (PCIe) bus, items in the host memory must occasionally be communicated across the bus to the device memory or vice versa as described in What Runs on a CUDA-Enabled Device? These are the primary hardware differences between CPU hosts and GPU devices with respect to parallel programming. Other differences are discussed as they arise elsewhere in this document. Applications composed with these differences in mind can treat the host and device together as a cohesive heterogeneous system wherein each processing unit is leveraged to do the kind of work it does best: sequential work on the host and parallel work on the device. Read more at: http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#ixzz4ic8ch2fq Follow us: @GPUComputing on Twitter | NVIDIA on Facebook Now knowing that there are two different APIs for CUDAs API Libraries we have to understand the difference between the two found here: Difference Between the driver and runtime APIs 1. Difference between the driver and runtime APIs The driver and runtime APIs are very similar and can for the most part be used interchangeably. However, there are some key differences worth noting between the two. Complexity vs. control The runtime API eases device code management by providing implicit initialization, context management, and module management. This leads to simpler code, but it also lacks the level of control that the driver API has. In comparison, the driver API offers more fine-grained control, especially over contexts and module loading. Kernel launches are much more complex to implement, as the execution configuration and kernel parameters must be specified with explicit function calls. However, unlike the runtime, where all the kernels are automatically loaded during initialization and stay loaded for as long as the program runs, with the driver API it is possible to only keep the modules that are currently needed loaded, or even dynamically reload modules. The driver API is also language-independent as it only deals with cubin objects. Context management Context management can be done through the driver API, but is not exposed in the runtime API. Instead, the runtime API decides itself which context to use for a thread: if a context has been made current to the calling thread through the driver API, the runtime will use that, but if there is no such context, it uses a \"primary context.\" Primary contexts are created as needed, one per device per process, are reference-counted, and are then destroyed when there are no more references to them. Within one process, all users of the runtime API will share the primary context, unless a context has been made current to each thread. The context that the runtime uses, i.e, either the current context or primary context, can be synchronized with cudaDeviceSynchronize(), and destroyed with cudaDeviceReset(). Using the runtime API with primary contexts has its tradeoffs, however. It can cause trouble for users writing plug-ins for larger software packages, for example, because if all plug-ins run in the same process, they will all share a context but will likely have no way to communicate with each other. So, if one of them calls cudaDeviceReset() after finishing all its CUDA work, the other plug-ins will fail because the context they were using was destroyed without their knowledge. To avoid this issue, CUDA clients can use the driver API to create and set the current context, and then use the runtime API to work with it. However, contexts may consume significant resources, such as device memory, extra host threads, and performance costs of context switching on the device. This runtime-driver context sharing is important when using the driver API in conjunction with libraries built on the runtime API, such as cuBLAS or cuFFT. Read more at: http://docs.nvidia.com/cuda/cuda-driver-api/index.html#ixzz4icCoAXb7 Follow us: @GPUComputing on Twitter | NVIDIA on Facebook Since this happens to be found in the DriverAPI it has more flexibility of control towards the programmer but also requires more responsibility to manage where the RuntimeAPI library does things more automatic but gives you less control. This is apparent since you mentioned that you are working with their Kernels but from the description of their implementation of the function CUresult cuGraphicsSubResourceGetMappedArray ( CUarray* pArray, CUgraphicsResource resource, unsigned int arrayIndex, unsigned int mipLevel ) The documentation is telling me that the first parameter that this function takes is a returned array through which a subresource of resource may be accessed. The second parameter of this function is the mapped graphics resource itself. The third parameter in which I believe is the parameter that you had in question where it is an enumerated type to a face and you then asked: When one passes such an enum, then the returned texture array will contain only data of that particular face, right? From what I gather and understand from the documentations is that this is an index value to an array of your cube map resource. Which can be seen from their documentation: arrayIndex - Array index for array textures or cubemap face index as defined by CUarray_cubemap_face for cubemap textures for the subresource to access Read more at: http://docs.nvidia.com/cuda/cuda-driver-api/index.html#ixzz4icHnwe9v Follow us: @GPUComputing on Twitter | NVIDIA on Facebook which happens to be an unsigned int or an index location into the textures that make up that cube map a typical cube map will have 6 faces or at most 12 if both inside and outside of the cube are mapped. So if we look at a cube map as well as textures and their relationship with pseudo code we can see that: // Texture struct Texture { unsigned pixelsWidth; unsigned pixelsHeight; // Other Texture member variables or fields here. }; // Only interested in the actual size of the texture `width by height` // where these would be used to map this texture to one of the 6 faces // of a cube: struct CubeMap { Texture face[6]; // face[0] = frontFace // face[1] = backFace // face[2] = leftFace // face[3] = rightFace // face[4] = topFace // face[5] = bottomFace }; The cubemap object has an array of textures that makes up its face and according to the documents the function that you have in question with its third parameter is asking you for an index into this texture array and the overall function will return this: Returns in *pArray an array through which the subresource of the mapped graphics resource resource which corresponds to array index arrayIndex and mipmap level mipLevel may be accessed. The value set in *pArray may change every time that resource is mapped. Read more at: http://docs.nvidia.com/cuda/cuda-driver-api/index.html#ixzz4icKF1c00 Follow us: @GPUComputing on Twitter | NVIDIA on Facebook I hope this helps to answer your question in regards to the use of the third parameter into the function you are trying to use from their API. Edit The OP had asked when passing this enum CU_CUBEMAP_FACE_POSITIVE_X to the third parameter of the above function call will it return only that face of the cube map which happens to be a texture. When looking at their documentation about this enumerated value or type found here: enum CUarray_cubemap_face enum CUarray_cubemap_face - Array indices for cube faces Values CU_CUBEMAP_FACE_POSITIVE_X = 0x00 Positive X face of cubemap CU_CUBEMAP_FACE_NEGATIVE_X = 0x01 Negative X face of cubemap CU_CUBEMAP_FACE_POSITIVE_Y = 0x02 Positive Y face of cubemap CU_CUBEMAP_FACE_NEGATIVE_Y = 0x03 Negative Y face of cubemap CU_CUBEMAP_FACE_POSITIVE_Z = 0x04 Positive Z face of cubemap CU_CUBEMAP_FACE_NEGATIVE_Z = 0x05 Negative Z face of cubemap Read more at: http://docs.nvidia.com/cuda/cuda-driver-api/index.html#ixzz4idOT67US Follow us: @GPUComputing on Twitter | NVIDIA on Facebook It appears to me that when using this method to query or get texture information that is stored into an array of a cube map, that the requirement of the third parameter being this enumerated value; is nothing more than the 0-index into that array. So by passing in CU_CUBEMAP_FACE_POSITIVE_X as the third parameter to me doesn't necessarily mean that you will only get back that particular face's texture. It appears to me that since this is the 0th index that it will return the entire array of textures. The old C style of passing around arrays as if they were pointers."
            ]
        ],
        "votes": [
            4.0000001,
            1e-07
        ]
    },
    {
        "question": "Attempting to define a class with a per-instance texture. Yes, the number of instances of that class will be small. To work around the restriction that CUDA texture must be a global variable, I tried the following approach: Define a global table of textures. Add an instance-ID data member to the class. Have a class method select a texture from the table using its ID, and pass it as an argument to a CUDA kernel. Doesn't work. A texture cannot be passed as an argument (nor by pointer or reference), and the kernel doesn't recognize the array name, barring passing by index. I could probably do it with a switch statement, but that is ugly. Any suggestions?",
        "answers": [
            [
                "If you have a GPU with Compute Capability &gt;= 3.0, then you can use texture objects instead of texture references. You can then pass the texture object as a kernel/function argument or use it as a class member. See Cuda Programming Guide section B.8 or Texture objects. In case you don\u2019t have a device with CC 3.0 or above, I guess you\u2019re out of luck and would need a, as you said, \"ugly\" switch statement in your kernel that chooses the right texture reference depending on some argument."
            ],
            [
                "You can bind the texture that you need before calling the kernel. So, you have a single texture reference and any number of textures stored in, for instance, cuArrays. Before calling the kernel, you bind the reference to the cuArray that you need: texture&lt;float, cudaTextureType2D, cudaReadModeElementType&gt; texRef; if (need_texture_1) { cudaBindTextureToArray(texRef, cuArray1, ...); else if (need_texture_2) { cudaBindTextureToArray(texRef, cuArray2, ...); } kernel&lt;&lt;&lt;&gt;&gt;&gt;(); __global__ void kernel() { var = tex2D&lt;float&gt;(texRef, ...); }"
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "Non-normalized linear interpolation from a CUDA texture object bound to a CUDA array appears to be returning incorrect results. It appears that the interpolated values are a factor of 0.5 smaller than expected. Normalized linear interpolation appears to be working properly. Is there something wrong in this code? Are we expected to multiply by 2 when doing non-normalized texture interpolation? The code: #include &lt;iostream&gt; #include &lt;cstdio&gt; // simple function to print an array template &lt;typename T&gt; void print_array(const T *a, const size_t length) { for (size_t i=0; i!=length; i++) { std::cout &lt;&lt; \"a[\" &lt;&lt; i &lt;&lt; \"]: \" &lt;&lt; a[i] &lt;&lt; std::endl; } } // attempt to interpolate linear memory __global__ void cuda_texture_interpolate(cudaTextureObject_t tex, float start, float stop, int count) { if (count &lt; 1) { count = 1; } float h = (stop-start)/((float)count); float x = start; float y; for (int i = 0; i != count; i++) { y = tex1D&lt;float&gt;(tex,x); printf(\"x: %4g ; y: %4g\\n\",x,y); x = x + h; } y = tex1D&lt;float&gt;(tex,x); printf(\"x: %4g ; y: %4g\\n\",x,y); } int main(void) { // set up host array int n = 5; float a_host[5] = {3,2,1,2,3}; printf(\"printing array on host.\\n\"); print_array(a_host,n); // allocate and copy to cuda array cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat); cudaArray* cuArray; cudaMallocArray(&amp;cuArray, &amp;channelDesc, n); // Copy to device memory some data located at address h_data // in host memory cudaMemcpyToArray(cuArray, 0, 0, a_host, n*sizeof(float), cudaMemcpyHostToDevice); // create texture object cudaResourceDesc resDesc; memset(&amp;resDesc, 0, sizeof(resDesc)); resDesc.resType = cudaResourceTypeArray; resDesc.res.array.array = cuArray; cudaTextureDesc texDesc; memset(&amp;texDesc, 0, sizeof(texDesc)); texDesc.addressMode[0] = cudaAddressModeClamp; texDesc.filterMode = cudaFilterModeLinear; texDesc.readMode = cudaReadModeElementType; //texDesc.normalizedCoords = 1; texDesc.normalizedCoords = 0; cudaResourceViewDesc resViewDesc; memset(&amp;resViewDesc, 0, sizeof(resViewDesc)); resViewDesc.format = cudaResViewFormatFloat1; resViewDesc.width = n; // create texture object cudaTextureObject_t tex; cudaCreateTextureObject(&amp;tex, &amp;resDesc, &amp;texDesc, &amp;resViewDesc); // call interpolation kernel printf(\"interpolate (f(x) -&gt; y).\\n\"); //cuda_texture_interpolate&lt;&lt;&lt;1,1&gt;&gt;&gt;(tex,0.0,1.0,10); cuda_texture_interpolate&lt;&lt;&lt;1,1&gt;&gt;&gt;(tex,0.0,5.0,10); // clean up cudaDestroyTextureObject(tex); cudaFreeArray(cuArray); printf(\"end of texture_object_interpolation.\\n\"); return 0; } The result: $ ./texture_object_interpolation printing array on host. a[0]: 3 a[1]: 2 a[2]: 1 a[3]: 2 a[4]: 3 interpolate (f(x) -&gt; y). x: 0 ; y: 1.5 x: 0.5 ; y: 1.5 x: 1 ; y: 1.25 x: 1.5 ; y: 1 x: 2 ; y: 0.75 x: 2.5 ; y: 0.5 x: 3 ; y: 0.75 x: 3.5 ; y: 1 x: 4 ; y: 1.25 x: 4.5 ; y: 1.5 x: 5 ; y: 1.5 end of texture_object_interpolation. Please see this gist for the above code, a makefile, and code for normalized interpolation.",
        "answers": [
            [
                "This was apparently caused by a bug in the CUDA 5.0 compiler, and fixed in the CUDA 5.5 release. [This answer has been assembled from comments to get the question off the unanswered queue for the CUDA tag]"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm learning cuda texture memory. Now, I got a opencv Iplimage, and I get its imagedata. Then I bind a texture to this uchar array, like below: Iplimage *image = cvCreateImage(cvSize(width, height), IPL_DEPTH_8U, 3); unsigned char* imageDataArray = (unsigned char*)image-&gt;imagedata; texture&lt;unsigned char,2,cudaReadModeElementType&gt; tex; cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(8, 8, 8, 0, cudaChannelFormatKindUnsigned); cudaArray *cuArray = NULL; CudaSafeCall(cudaMallocArray(&amp;cuArray,&amp;channelDesc,width,height)); cudaMemcpy2DToArray(cuArray,0,0,imageDataArray,image-&gt;widthstep, width * sizeof(unsigned char), height, cudaMemcpyHostToDevice); cudaBindTextureToArray(texC1_cf,cuArray_currentFrame, channelDesc); Now I lanch my kernel, and I want to access each pixel, every channel of that image. This is where I get confused. I use this code to get the pixel coordinate (X,Y): int X = (blockIdx.x*blockDim.x+threadIdx.x); int Y = (blockIdx.y*blockDim.y+threadIdx.y); And how can I access each channel of this (X,Y)? what's the code below return? tex2D(tex, X, Y); Besides this, Can you tell me how texture memory using texture to access an array, and how this transform looks like?",
        "answers": [
            [
                "To bind a 3 channel OpenCV image to cudaArray texture, you have to create a cudaArray of width equal to image-&gt;width * image-&gt;nChannels, because the channels are stored interleaved by OpenCV. cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc&lt;unsigned char&gt;(); cudaArray *cuArray = NULL; CudaSafeCall(cudaMallocArray(&amp;cuArray,&amp;channelDesc,width * image-&gt;nChannels,height)); cudaMemcpy2DToArray(cuArray,0,0,imageDataArray,image-&gt;widthstep, width * image-&gt;nChannels * sizeof(unsigned char), height, cudaMemcpyHostToDevice); cudaBindTextureToArray(texC1_cf,cuArray_currentFrame, channelDesc); Now, to access each channel separately in the kernel, you just have to multiply the x index with number of channels and add the offset of desired channel like this: unsigned char blue = tex2D(tex, (3 * X) , Y); unsigned char green = tex2D(tex, (3 * X) + 1, Y); unsigned char red = tex2D(tex, (3 * X) + 2, Y); First one is blue because OpenCV stores images with channel sequence BGR. As for the error you get when you try to access texture&lt;uchar3,..&gt; using tex2D; CUDA only supports creating 2D textures of 1,2 and 4 element vector types. Unfortunately, ONLY 3 is not supported which is very good for binding RGB images and is a really desirable feature."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "Im developing a small cuda lib, stuck by this annoying tex ref issue. This is the sample code from Cuda C Programming Guide, Page43~44: texture&lt;float, cudaTextureType2D,cudaReadModeElementType&gt; texRef; textureReference* texRefPtr; cudaGetTextureReference(&amp;texRefPtr, \"texRef\"); cudaChannelFormatDesc channelDesc; cudaGetChannelDesc(&amp;channelDesc, cuArray); cudaBindTextureToArray(texRef, cuArray, &amp;channelDesc); When i execute it, the line cudaGetTextureReference(...) returns the error code cudaErrorInvalidTexture. I couldn't find many other samples about cudaGetTextureReference on the internet, most of them are following the exact same procedure as the sample code above. Frustrated, i tried the high-level API afterwards: texture&lt;float, cudaTextureType2D,cudaReadModeElementType&gt; texRef; cudaBindTextureToArray(texRef, cuArray); same problem. If read from that texture in the kernel, zero values are all i get. My full toy test code, 100% reproduce-rate: (win7, cuda 5.0) texture&lt;float, cudaTextureType2D, cudaReadModeElementType&gt; texRef; int main () { const textureReference *tref = NULL; checkSuccess( cudaGetTextureReference( &amp;tref, \"texRef\" ) ); pauseConsole(); return 0; } Any insight for this problem would be greatly appreciated. Thanks",
        "answers": [
            [
                "Use of a string naming a variable as the symbol paramater was removed in CUDA 5.0. Stated in CUDA Toolkit Reference Manual 5.0, Section 5.20.2.8 /Note. Do the following instead: cudaGetTextureReference( &amp;tref, &amp;texRef );"
            ],
            [
                "Pass the symbol directly, not as a string. The string look method is a deprecated version of the API that was removed completely in CUDA 5. Your texture lookup should be written as cudaGetTextureReference( &amp;tref, &amp;texRef );"
            ]
        ],
        "votes": [
            5.0000001,
            3.0000001
        ]
    },
    {
        "question": "Is there a way to read the values in a cudaArray from the device without wrapping it in a texture reference/object? All of the examples I've looked at use cudaArrays exclusively for creating textures. Is that the only way they can be used, or could I do something like: __global__ kernel(cudaArray *arr, ...) { float x = tex1D&lt;float&gt;(arr, ...); ... } cudaArray *arr; cudaMallocArray(&amp;arr, ...); cudaMemcpyToArray(arr, ...); kernel&lt;&lt;&lt;...&gt;&gt;&gt;(arr, ...); So basically, what should go in place of tex1D there? Also, if this is possible I'd be curious if anyone thinks there would be any performance benefit to doing this, but I'll also be running my own tests to see. Thanks!",
        "answers": [
            [
                "cudaArray is defined for texturing or surface memory purposes. As indicated here: CUDA arrays are opaque memory layouts optimized for texture fetching. They are one dimensional, two dimensional, or three-dimensional and composed of elements, each of which has 1, 2 or 4 components that may be signed or unsigned 8 , 16 or 32 bit integers, 16 bit floats, or 32 bit floats. CUDA arrays are only accessible by kernels through texture fetching as described in Texture Memory or surface reading and writing as described in Surface Memory. So in effect you have to use either texture functions or surface functions in kernels to access data in a cudaArray. There are several performance benefit possibilities associated with using texturing. Texturing can imply interpolation (i.e. reading from a texture using floating point coordinates). Any application that needs this kind of data interpolation may benefit from the HW interpolation engines inside the texture units on the GPU. Another benefit, perhaps the most important for using texturing in arbitrary GPU codes, is the texture cache that backs up the textures stored in global memory. Texturing is a read-only operation, but if you have an array of read-only data, the texture cache may improve or otherwise extend your ability to access data rapidly. This generally implies that there must be data-locality/ data-reuse in your functions that are accessing data stored in the texturing mechanism. Texture data retrieved will not disrupt anything in the L1 cache, so generally this kind of data segmentation/optimization would be part of a larger strategy around data caching. If there were no other demands on L1 cache, the texture mechanism/cache does not provide faster access to data than if it were in the L1 already."
            ],
            [
                "Robert Crovella has already answered to your question. I believe it could be useful for next users to have a worked example for the two solutions: textures and sufaces. #include &lt;stdio.h&gt; #include &lt;thrust\\device_vector.h&gt; // --- 2D float texture texture&lt;float, cudaTextureType2D, cudaReadModeElementType&gt; texRef; // --- 2D surface memory surface&lt;void, 2&gt; surf2D; /********************/ /* CUDA ERROR CHECK */ /********************/ #define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); } inline void gpuAssert(cudaError_t code, char *file, int line, bool abort=true) { if (code != cudaSuccess) { fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line); if (abort) exit(code); } } /*************************************/ /* cudaArray PRINTOUT TEXTURE KERNEL */ /*************************************/ __global__ void cudaArrayPrintoutTexture(int width, int height) { unsigned int x = blockIdx.x * blockDim.x + threadIdx.x; unsigned int y = blockIdx.y * blockDim.y + threadIdx.y; printf(\"Thread index: (%i, %i); cudaArray = %f\\n\", x, y, tex2D(texRef, x / (float)width + 0.5f, y / (float)height + 0.5f)); } /*************************************/ /* cudaArray PRINTOUT TEXTURE KERNEL */ /*************************************/ __global__ void cudaArrayPrintoutSurface(int width, int height) { unsigned int x = blockIdx.x * blockDim.x + threadIdx.x; unsigned int y = blockIdx.y * blockDim.y + threadIdx.y; float temp; surf2Dread(&amp;temp, surf2D, x * 4, y); printf(\"Thread index: (%i, %i); cudaArray = %f\\n\", x, y, temp); } /********/ /* MAIN */ /********/ void main() { int width = 3, height = 3; thrust::host_vector&lt;float&gt; h_data(width*height, 3.f); // --- Allocate CUDA array in device memory cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat); cudaArray* cuArray; /*******************/ /* TEXTURE BINDING */ /*******************/ gpuErrchk(cudaMallocArray(&amp;cuArray, &amp;channelDesc, width, height)); // --- Copy to host data to device memory gpuErrchk(cudaMemcpyToArray(cuArray, 0, 0, thrust::raw_pointer_cast(h_data.data()), width*height*sizeof(float), cudaMemcpyHostToDevice)); // --- Set texture parameters texRef.addressMode[0] = cudaAddressModeWrap; texRef.addressMode[1] = cudaAddressModeWrap; texRef.filterMode = cudaFilterModeLinear; texRef.normalized = true; // --- Bind the array to the texture reference gpuErrchk(cudaBindTextureToArray(texRef, cuArray, channelDesc)); // --- Invoking printout kernel dim3 dimBlock(3, 3); dim3 dimGrid(1, 1); cudaArrayPrintoutTexture&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(width, height); gpuErrchk(cudaUnbindTexture(texRef)); gpuErrchk(cudaFreeArray(cuArray)); /******************/ /* SURFACE MEMORY */ /******************/ gpuErrchk(cudaMallocArray(&amp;cuArray, &amp;channelDesc, width, height, cudaArraySurfaceLoadStore)); // --- Copy to host data to device memory gpuErrchk(cudaMemcpyToArray(cuArray, 0, 0, thrust::raw_pointer_cast(h_data.data()), width*height*sizeof(float), cudaMemcpyHostToDevice)); gpuErrchk(cudaBindSurfaceToArray(surf2D, cuArray)); cudaArrayPrintoutSurface&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(width, height); gpuErrchk(cudaPeekAtLastError()); gpuErrchk(cudaDeviceSynchronize()); gpuErrchk(cudaFreeArray(cuArray)); }"
            ]
        ],
        "votes": [
            7.0000001,
            5.0000001
        ]
    },
    {
        "question": "I seem to have an issue with the function cudaMemcpyToArray. I have the following commands: float *h_data = new float[bmp.width * bmp.height]; ... cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat); cudaArray *cuArray; cudaMallocArray(&amp;cuArray, &amp;channelDesc, bmp.width, bmp.height); cudaMemcpyToArray(cuArray, 0, 0, h_data, bmp.width * bmp.height, cudaMemcpyHostToDevice); As far as I understand, this should give me a 2D array in cuArray that has dimensions bmp.width by bmp.height from the data in h_data, which is a 1D array with dimensions bmp.width * bmp.height. Unfortunately, it just seg-faults on the last command. Am I doing something horribly wrong?",
        "answers": [
            [
                "I think @lmortenson was on the right track, but we don't multiply width and height by sizeof(float), just one of them. You need to make sure that your bmp.width and bmp.height parameters conform to the limits specified here under Valid extents. These extents are in elements, not bytes. You need to pass width and height parameters to cudaMallocArray that are in elements, not bytes. You need to pass an overall size parameter to cudaMemcpyToArray that is in bytes, not elements, but this would generally be of the form width*height*sizeof(float) I created a simple reproducer based on your code and was able to reproduce the seg fault. The following code was my adaptation with the errors fixed, I believe: #include &lt;stdio.h&gt; #define cudaCheckErrors(msg) \\ do { \\ cudaError_t __err = cudaGetLastError(); \\ if (__err != cudaSuccess) { \\ fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\ msg, cudaGetErrorString(__err), \\ __FILE__, __LINE__); \\ fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\ exit(1); \\ } \\ } while (0) int main(){ int width = 256; int height = 256; float *h_data = new float[width * height]; cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat); cudaArray *cArray; cudaMallocArray(&amp;cArray, &amp;channelDesc, width, height, cudaArrayDefault); cudaCheckErrors(\"cudaMallocArray\"); cudaMemcpyToArray(cArray, 0, 0, h_data, width*height*sizeof(float), cudaMemcpyHostToDevice); cudaCheckErrors(\"cudaMemcpyToArray\"); return 0; }"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a CUarray that I got from my OpenGL-context via cuGraphicsSubResourceGetMappedArray(). Is there a possiblity to use it with cuMemset*()?",
        "answers": [
            [
                "Nope. You can't get a device pointer into a CUDA array (to pass to cuMemset*()), and NVIDIA has never shipped a memset function for CUDA arrays. You have to zero out some host memory and do a memcpy (or memcpy's) into the CUDA array, or (if your app runs only on SM 2.0 or later) roll your own with surface stores."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I have already read the following thread , but I couldn't get my code to work. I am trying to allocate a 2D array on GPU, fill it with values, and copy it back to the CPU. My code is as follows: __global__ void Kernel(char **result,int N) { //do something like result[0][0]='a'; } int N=20; int Count=5; char **result_h=(char**)malloc(sizeof(char*)*Count); char **result_d; cudaMalloc(&amp;result_d, sizeof(char*)*Count); for(int i=0;i&lt;Count;i++) { result_h[i] = (char*)malloc(sizeof(char)*N); cudaMalloc(&amp;result_d[i], sizeof(char)*N); //get exception here } //call kernel //copy values from result_d to result_h printf(\"%c\",result_h[0][0])//should print a How can i achieve this?",
        "answers": [
            [
                "You can't manipulate device pointers in host code, which is why the cudaMalloc call inside the loop fails. You should probably just allocate a single contiguous block of memory and then treat that as a flattened 2D array."
            ],
            [
                "For doing the simplest 2D operations on a GPU, I'd recommend you just treat it as a 1D array. cudaMalloc a block of size w*h*sizeof(char). You can access the element (i,j) through index j*w+i. Alternatively, you could use cudaMallocArray to get a 2D array. This has a better sense of locality than linear mapped 2D memory. You can easily bind this to a texture, for example. Now in terms of your example, the reason why it doesn't work is that cudaMalloc manipulates a host pointer to point at a block of device memory. Your example allocated the pointer structure for results_d on the device. If you just change the cudaMalloc call for results_d to a regular malloc, it should work as you originally intended. That said, perhaps one of the two options I outlined above might work better from an ease of code maintenance perspective."
            ],
            [
                "When allocating in that way you are allocating addresses that are valid on the CPU memory. The value of the addresses is transferred as a number without problems, but once on the device memory the char* address will not have meaning. Create an array of N * max text length, and another array of length N that tells how long each word is. This is a bit more advanced but if you are processing a set of defined text (passwords for example) I would suggest you to group it by text length and create specialized kernel for each length template&lt;int text_width&gt; __global__ void Kernel(char *result,int N) { //pseudocode for i in text_width: result[idx][i] = 'a' } and in the kernel invocation code you specify: switch text_length case 16: Kernel&lt;16&gt; &lt;&lt;&lt;&gt;&gt;&gt; ()"
            ],
            [
                "The following code sample allocates a width\u00d7height 2D array of floating-point values and shows how to loop over the array elements in device code[1] // host code float* devPtr; int pitch; cudaMallocPitch((void**)&amp;devPtr, &amp;pitch, width * sizeof(float), height); myKernel&lt;&lt;&lt;100, 192&gt;&gt;&gt;(devPtr, pitch); // device code __global__ void myKernel(float* devPtr, int pitch) { for (int r = 0; r &lt; height; ++r) { float* row = (float*)((char*)devPtr + r * pitch); for (int c = 0; c &lt; width; ++c) { float element = row[c]; } } } The following code sample allocates a width\u00d7height CUDA array of one 32-bit floating-point component[1] cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc&lt;float&gt;(); cudaArray* cuArray; cudaMallocArray(&amp;cuArray, &amp;channelDesc, width, height); The following code sample copies the 2D array to the CUDA array allocated in the previous code samples[1]: cudaMemcpy2DToArray(cuArray, 0, 0, devPtr, pitch, width * sizeof(float), height, cudaMemcpyDeviceToDevice); The following code sample copies somehost memory array to device memory[1]: float data[256]; int size = sizeof(data); float* devPtr; cudaMalloc((void**)&amp;devPtr, size); cudaMemcpy(devPtr, data, size, cudaMemcpyHostToDevice); you can understand theses examples and apply them in your purpose. [1] NVIDIA CUDA Compute Unified Device Architecture"
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "Ok, so far, I can create an array on the host computer (of type float), and copy it to the gpu, then bring it back to the host as another array (to test if the copy was successful by comparing to the original). I then create a CUDA array from the array on the GPU. Then I bind that array to a CUDA texture. I now want to read that texture back and compare with the original array (again to test that it copied correctly). I saw some sample code that uses the readTexel() function shown below. It doesn't seem to work for me... (basically everything works except for the section in the bindToTexture(float* deviceArray) function starting at the readTexels(SIZE, testArrayDevice) line). Any suggestions of a different way to do this? Or are there some obvious problems I missed in my code? Thanks for the help guys! #include &lt;stdio.h&gt; #include &lt;assert.h&gt; #include &lt;cuda.h&gt; #define SIZE 20; //Create a channel description to use. cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat); //Create a texture to use. texture&lt;float, 2, cudaReadModeElementType&gt; cudaTexture; //cudaTexture.filterMode = cudaFilterModeLinear; //cudaTexture.normalized = false; __global__ void readTexels(int amount, float *Array) { int index = blockIdx.x * blockDim.x + threadIdx.x; if (index &lt; amount) { float x = tex1D(cudaTexture, float(index)); Array[index] = x; } } float* copyToGPU(float* hostArray, int size) { //Create pointers, one for the array to be on the device, and one for bringing it back to the host for testing. float* deviceArray; float* testArray; //Allocate some memory for the two arrays so they don't get overwritten. testArray = (float *)malloc(sizeof(float)*size); //Allocate some memory for the array to be put onto the GPU device. cudaMalloc((void **)&amp;deviceArray, sizeof(float)*size); //Actually copy the array from hostArray to deviceArray. cudaMemcpy(deviceArray, hostArray, sizeof(float)*size, cudaMemcpyHostToDevice); //Copy the deviceArray back to testArray in host memory for testing. cudaMemcpy(testArray, deviceArray, sizeof(float)*size, cudaMemcpyDeviceToHost); //Make sure contents of testArray match the original contents in hostArray. for (int i = 0; i &lt; size; i++) { if (hostArray[i] != testArray[i]) { printf(\"Location [%d] does not match in hostArray and testArray.\\n\", i); } } //Don't forget free these arrays after you're done! free(testArray); return deviceArray; //TODO: FREE THE DEVICE ARRAY VIA cudaFree(deviceArray); } cudaArray* bindToTexture(float* deviceArray) { //Create a CUDA array to translate deviceArray into. cudaArray* cuArray; //Allocate memory for the CUDA array. cudaMallocArray(&amp;cuArray, &amp;cudaTexture.channelDesc, SIZE, 1); //Copy the deviceArray into the CUDA array. cudaMemcpyToArray(cuArray, 0, 0, deviceArray, sizeof(float)*SIZE, cudaMemcpyHostToDevice); //Release the deviceArray cudaFree(deviceArray); //Bind the CUDA array to the texture. cudaBindTextureToArray(cudaTexture, cuArray); //Make a test array on the device and on the host to verify that the texture has been saved correctly. float* testArrayDevice; float* testArrayHost; //Allocate memory for the two test arrays. cudaMalloc((void **)&amp;testArray, sizeof(float)*SIZE); testArrayHost = (float *)malloc(sizeof(float)*SIZE); //Read the texels of the texture to the test array in the device. readTexels(SIZE, testArrayDevice); //Copy the device test array to the host test array. cudaMemcpy(testArrayHost, testArrayDevice, sizeof(float)*SIZE, cudaMemcpyDeviceToHost); //Print contents of the array out. for (int i = 0; i &lt; SIZE; i++) { printf(\"%f\\n\", testArrayHost[i]); } //Free the memory for the test arrays. free(testArrayHost); cudaFree(testArrayDevice); return cuArray; //TODO: UNBIND THE CUDA TEXTURE VIA cudaUnbindTexture(cudaTexture); //TODO: FREE THE CUDA ARRAY VIA cudaFree(cuArray); } int main(void) { float* hostArray; hostArray = (float *)malloc(sizeof(float)*SIZE); for (int i = 0; i &lt; SIZE; i++) { hostArray[i] = 10.f + i; } float* deviceAddy = copyToGPU(hostArray, SIZE); free(hostArray); return 0; }",
        "answers": [
            [
                "Briefly: ------------- in your main.cu --------------------------------------------------------------------------------------- -1. Define the texture as a globlal variable texture refTexture; // global variable ! // meaning: address the texture with (x,y) (2D) and get an unsinged int In the main function: -2. Use arrays combined with texture cudaArray* myArray; // declar. // ask for memory cudaMallocArray ( &amp;myArray, &amp;refTex.channelDesc, /* with this you don't need to fill a channel descriptor */ width, height); -3. copy data from CPU to GPU (to the array) cudaMemcpyToArray ( arrayCudaEntrada, // destination: the array 0, 0, // offsets sourceData, // pointer uint* widthheightsizeof(uint), // total amount of bytes to be copied cudaMemcpyHostToDevice); -4. bind texture and array cudaBindTextureToArray( refTex,arrayCudaEntrada) -5. change some parameters in the texture refTextura_In.normalized = false; // don't automatically convert fetched data to [0,1[ refTextura_In.addressMode[0] = cudaAddressModeClamp; // if my indexing is out of bounds: automatically use a valid indexing (0 if negative index, last if too great index) refTextura_In.addressMode[1] = cudaAddressModeClamp; ---------- in the kernel -------------------------------------------------------- // find out indexes (f,c) to process by this thread uint f = (blockIdx.x * blockDim.x) + threadIdx.x; uint c = (blockIdx.y * blockDim.y) + threadIdx.y; // this is curious and necessary: indexes for reading from a texture // are floats !. Even if you are certain to access (4,5) you have // match the \"center\" this is (4.5, 5.5) uint read = tex2D( refTex, c+0.5f, f+0.5f); // texRef is a global variable Now You process read and write the results to other zone of the device global memory, not to the texture itself !"
            ],
            [
                "readTexels() is a kernel (__global__) function, i.e. it runs on the GPU. Therefore you need to use the correct syntax to launch a kernel. Take a look through the CUDA Programming Guide and some of the SDK samples, both available via the NVIDIA CUDA site to see how to launch a kernel. Hint: It'll end up something like readTexels&lt;&lt;&lt;grid,block&gt;&gt;&gt;(...)"
            ]
        ],
        "votes": [
            8.0000001,
            1.0000001
        ]
    }
]