[
    {
        "question": "I'm attempting to get an Nvidia Jetson Orin NX 16gb playing nicely with Kubernetes, using CRI-O. I've installed the latest stable toolkit on the Jetson. NVIDIA Container Runtime version 1.13.5 commit: 6b8589dcb4dead72ab64f14a5912886e6165c079 spec: 1.1.0-rc.2 runc version 1.1.7-0ubuntu1~20.04.1 spec: 1.0.2-dev go: go1.18.1 libseccomp: 2.5.1 CRI-O is configured with the following [crio.runtime] default_runtime = \"nvidia\" [crio.runtime.runtimes.nvidia] runtime_path = \"/usr/bin/nvidia-container-runtime\" runtime_type = \"oci\" runtime_root = \"/run/nvidia-container-runtime\" The nvidia-device-plugin is installed on the cluster, and has labeled the node accordingly with nvidia.com/gpu: 1 The node shows as such NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME jetson1 Ready control-plane 7d16h v1.27.4+k0s 192.168.4.53 &lt;none&gt; Ubuntu 20.04.6 LTS 5.10.104-tegra cri-o://1.27.1 I've applied a RuntimeClass (though I thought I could do without it if the CRI is defaulting to nvidia) --- apiVersion: node.k8s.io/v1 kind: RuntimeClass metadata: name: gpu-enabled-class handler: nvidia And this is the Pod that I'm testing --- apiVersion: v1 kind: Pod metadata: name: nvidia-query spec: runtimeClassName: gpu-enabled-class restartPolicy: OnFailure containers: - name: nvidia-query image: dudo/test_cuda resources: limits: nvidia.com/gpu: 1 tolerations: - key: nvidia.com/gpu operator: Exists effect: NoSchedule This pod is properly scheduled, and executes as intended, running this script, but it doesn't utilize any gpu when checking jtop. If I run the script directly on the Jetson, jtop shows as expected, and the gpu is utilized, but from the container, nada. Any ideas on what might be misconfigured? Any recommendations on how to debug this further?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Closed. This question is not about programming or software development. It is not currently accepting answers. This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered. Closed 7 days ago. Improve this question error screenshotI have a NVIDIA Jetson Xavier NX(ubuntu20.04), I want to install the nvidia-driver.BUT,It shows ERRORS every time when i try. I use \"sudo apt install nvidia-driver-525 nvidia-dkms-525\". How should I solve this problem?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I cloned an NVIDA jetson hard drive to increase storage, I expanded it using gparted. I then tried to create /mnt/md0/FolderA and /mnt/md0/FolderB. I was able to mount it on /dev/sda1. But now it is showing folders duplicated in / and /mnt/md0. Seems I have duplicated records between /mnt/md0/FolderA &amp; /mnt/md0/FolderB and /FolderA /FolderB Can I just rm /mnt/md0 to stop the duplication? FileZille of directory ls of / ls of /mnt/md0 I was trying to just create a separate folder to place FolderA and B.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am new to this concept. I've tried searching for articles about this, but couldn't find any helpful one. I want to send some text data from an Android App to Jetson Nano. I've a few questions I would need a USB Type C(Android) to Type A cable(Jetson) right? Can the simple charging cable be used for this Should there be some python code running at the jetson end to receive this data continuously when Android device is connected to it via a usb cable. It would be really helpful if someone could provide relevant articles on this.",
        "answers": [
            [
                "To answer your first question, yes, you need a cable to connect the two devices together, but a charging cable might not be enough, you need a cable capable of transmitting data (which most \"simple\" charging cables are capable of nowadays). On the data transfer side of things, this article might be of some help."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have one TP link wifi connector USB on ARM device. And I am sharing my laptop's hotspot and connecting with it. Its connecting fine. But when I try to connect same TP link to another ARM device, and when I try to do ssh, it says connection refused!! Any idea why is this happening ? I have tried few solutions on my own to solve this. deleting the address from known_hosts in C &gt; USER &gt; .ssh &gt; known_hosts trying installing 'sudo apt-get install openssh-server' on the new ARM again. SO it works, But I have only 1 TP link, and I have to do this everytime I change the ARM device. any easy solution for this ? and explanation on why is this happening ?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a deep learning model running on a Jetson device, performing inference on images in a queue (size 1) captured from a MIPI camera (using GStreamer). The inference loop runs in its own thread at about 5 fps and delivers result images to a second queue (of size 1). A fastAPI Websocket server transmits the result images from this queue to an Avalonia .net GUI app, which currently just displays the images in a window as they arrive. I was expecting the stream to display at about 5 fps, but I am getting roughly 2 seconds lag. I have tested the above pipeline without performing inference (i.e. just passing the incoming image through to the Websocket server), and the lag in display is barely noticeable. My best guess is that maybe the Avalonia app is rendering the image using the GPU, and the Pytorch model is getting priority use. Would that make sense? Is there anything I can do about this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "setup: \u2022 Hardware Platform:Jetson Xavier NX \u2022 DeepStream Version: 6.1 \u2022 JetPack Version: 5.0.1 DP \u2022 TensorRT Version: 8.4.0.11 \u2022 Issue Type( questions) Hi folks, I work with deepstream and python bindings I have pipeline that captare UDP H264 stream pass to Yolo object detection and tracking, everything work great. Now I need to handle with something new so before I integrate my network I try build simple pipeline in command line. someone Send to me RTP stream is pipeline (Sender): gst-launch-1.0 v4l2src device=\"/dev/video\u201d ! jpegdec ! omxh264enc ! mpegtsmux ! rtpmp2tpay ! udpsink host=234.0.0.0 port=46002 I build a simple receive pipeline: gst-launch-1.0 udpsrc port=46002 caps=\"application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)MP2T, payload=(int)33\" ! rtpjitterbuffer ! rtpmp2tdepay ! tsdemux ! h264parse ! nvv4l2decoder ! nvvideoconvert ! autovideosink Unfortntaly I don\u2019t see video on screen I thing something worng with the element rtpmp2tdepay or maybe I missing out something I will be happy to any help. I add some graphsView of the pipeline and wireshark: I thing I miss something I would happy to any help.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I was installing cuda toolkit for jetson , and for network reason the installation failed and so I have Unmet dependencies, but when using apt --fix-broken install it gives the same error what to do , I can not install any thing because of this error . when installing anything it gaves dependencies error",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using ffmpeg running on jetson nano but I can't it to stream to aws IVS. I get a bunch of errors ffmpeg -f lavfi -i anullsrc -rtsp_transport udp -i \"rtsp://192.168.1.193:8554/\" -force_key_frames \"expr:gte(t,n_forced*2)\" -vf scale=640:480 -reorder_queue_size 4000 -max_delay 10000000 -vcodec libx264 -b:v 500k -pix_fmt yuv420p -f flv rtmps://&lt;IVS-ingest-server&gt;/app/&lt;IVS-stream-key&gt; How do I resolve this problem on jetson nano",
        "answers": [],
        "votes": []
    },
    {
        "question": "Have been spending many hours on getting this working with the newest python on Jetson, default it comes with Python 3.6.9 (Jetpack 4.6.4). So I compiled Python 3.11 and registered it. It works. NOTE: pip install opencv-python is NOT the solution I am looking for, since that does not come with CUDA optimalisation. jtop says: \"It is installed\" PROBLEM: Python 3.11 import cv2 says: \"No Module named cv2\" while these did install according to \"make install\" results. - Up-to-date: /usr/lib/python3.11/site-packages/cv2/__init__.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/load_config_py2.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/load_config_py3.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/config.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/misc/__init__.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/misc/version.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/mat_wrapper/__init__.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/utils/__init__.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/gapi/__init__.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/python-3.11/cv2.cpython-311-aarch64-linux-gnu.so -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/config-3.11.py ERror we get... Python 3.11.3 (tags/debian/3.11.3-1+bionic2-dirty:95ae635, Jun 5 2023, 15:54:34) [GCC 7.5.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import cv2 Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; ModuleNotFoundError: No module named 'cv2' opencv cmake with the recommended options for Jetson says this (among others) - NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH) -- NVIDIA GPU arch: 53 -- NVIDIA PTX archs: -- -- cuDNN: YES (ver 8.2.1) -- -- Python 3: -- Interpreter: /usr/bin/python3.11 (ver 3.11.3) -- Libraries: /usr/lib/aarch64-linux-gnu/libpython3.11.so (ver 3.11.3) -- numpy: /usr/local/lib/python3.11/dist-packages/numpy/core/include (ver 1.25.0) -- install path: /usr/lib/python3.11/site-packages/cv2/python-3.11 -- -- Python (for build): /usr/bin/python2.7",
        "answers": [
            [
                "The solution was that site-packages (some-how) is being ignored by python. When I changed the argument to dist-packages it works. cmake ...... (skip for brevity) -D PYTHON3_PACKAGES_PATH=/usr/local/lib/python3.11/dist-packages \\ now cv2 loads happily."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "How can i solve the problem : Segmentation fault (core dumped) when importing pycuda: \"import pycuda.autoinit\" I tried to debug the problem with gdb and i get the following output: `(gdb) r -c \"import pycuda.autoinit\" Starting program: /usr/bin/python3 -c \"import pycuda.autoinit\" [Thread debugging using libthread_db enabled] Using host libthread_db library \"/lib/aarch64-linux-gnu/libthread_db.so.1\". [New Thread 0x7fb5d691f0 (LWP 8796)] [New Thread 0x7fb55681f0 (LWP 8797)] [New Thread 0x7fb2d671f0 (LWP 8798)] [New Thread 0x7fb15661f0 (LWP 8799)] [New Thread 0x7fafd651f0 (LWP 8800)] [New Thread 0x7fae5641f0 (LWP 8801)] [New Thread 0x7facd631f0 (LWP 8802)] Thread 1 \"python3\" received signal SIGSEGV, Segmentation fault. 0x0000007faa6eca48 in ?? () (gdb) backtrace #0 0x0000007faa6eca48 in ?? () #1 0x0000007faa4f907c in ?? () from /usr/lib/aarch64-linux-gnu/libapt-pkg.so.5.0 #2 0x0000007faa4f9dec in pkgInitSystem(Configuration&amp;, pkgSystem*&amp;) () from /usr/lib/aarch64-linux-gnu/libapt-pkg.so.5.0 #3 0x0000007faa748364 in ?? () from /usr/lib/python3/dist-packages/apt_pkg.cpython-36m-aarch64-linux-gnu.so #4 0x00000000005bcd24 in _PyCFunction_FastCallDict () #5 0x000000000052ca48 in ?? () #6 0x0000000000531698 in _PyEval_EvalFrameDefault () #7 0x000000000052c0e0 in ?? () #8 0x000000000053a7a4 in ?? () #9 0x00000000005bd008 in PyCFunction_Call () #10 0x00000000005340b4 in _PyEval_EvalFrameDefault () #11 0x000000000052c0e0 in ?? () #12 0x000000000052c674 in ?? () #13 0x000000000052c8cc in ?? () #14 0x0000000000531698 in _PyEval_EvalFrameDefault () #15 0x000000000052a950 in ?? () #16 0x000000000052c8cc in ?? () #17 0x0000000000531698 in _PyEval_EvalFrameDefault () #18 0x000000000052a950 in ?? () #19 0x000000000052c8cc in ?? () ---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---` I tried also with faulthandler and i got the following output: $ python3 Python 3.6.9 (default, Mar 10 2023, 16:46:00) [GCC 8.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import faulthandler &gt;&gt;&gt; faulthandler.enable() &gt;&gt;&gt; import pycuda.autoinit Fatal Python error: Segmentation fault Current thread 0x0000007f7f450010 (most recent call first): File \"/usr/lib/python3/dist-packages/apt/__init__.py\", line 35 in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678 in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 665 in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 955 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"/usr/lib/python3/dist-packages/apport/packaging_impl.py\", line 24 in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678 in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 665 in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 955 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"/usr/lib/python3/dist-packages/apport/fileutils.py\", line 26 in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678 in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 665 in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 955 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"/usr/lib/python3/dist-packages/apport/report.py\", line 30 in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678 in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 665 in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 955 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"/usr/lib/python3/dist-packages/apport/__init__.py\", line 5 in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678 in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 665 in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 955 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap&gt;\", line 941 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"/usr/lib/python3/dist-packages/apport_python_hook.py\", line 72 in apport_excepthook Segmentation fault (core dumped) Environment Nvidia card:Jetson AGX Xavier TensorRT Version : 8.0.1.6 CUDA Version : 10.2.300 CUDNN Version : 8.2.1.32 Operating System + Version : Ubuntu 18.04 Any help please",
        "answers": [],
        "votes": []
    },
    {
        "question": "I wrote a pipeline that grabs a 720 X 576 image from a 1920 X 576 sensor with the v4l2src element on a Nvidia jetson xavier nx. The pipeline grabs the frame and then does 2 things: pushes the frame to the appsink element encode it and stream with udpsink to the client The pipeline is as follows: gst-launch-1.0 v4l2src device=/dev/video0 ! queue max-size-time=1000000 ! videoconvert n-threads=8 ! video/x-raw,format=I420,width=1920,height=576 ! videoscale n-threads=8 method=0 ! video/x-raw,format=I420,width=720,height=576 ! tee name=t ! queue ! valve name=appValve drop=false ! appsink name=app_sink t. ! queue max-size-time=1000000 ! videorate max-rate=25 ! nvvidconv name=nvconv ! capsfilter name=second_caps ! nvv4l2h265enc control-rate=1 iframeinterval=256 bitrate=1615000 peak-bitrate=1938000 preset-level=1 idrinterval=256 vbv-size=64600 maxperf-enable=true ! video/x-h265 ! h265parse config-interval=1 ! tee name=t2 ! queue max-size-time=1000000 ! valve name=streamValve drop=false ! udpsink host=192.168.10.56 port=5000 sync=false name=udpSinkElement My question is: Is there any way to reduce the latency of this pipeline? I tried to reduce the latency by adding many queues and the n-threads to the video scale and videoconvert but it won't help.",
        "answers": [
            [
                "How do you measure latency ? If it's the time it takes for seeing the video while launching the client, then you'd probably need to reduce the GOP (iframeinterval) because the client will wait for an I-Frame (or many I-slice) before being able to reconstruct a complete picture. You can easily see if it's the case, by capturing the output of the decoded video stream on a monitor with your video source pointed at a timer. Take a picture with your phone of both (monitor + timer) and you have a pretty good measure of glass to glass latency. You'll launch the stream multiple time and measure the latency. Why multiple time ? Because depending on where you are in the GOP (close or far from the next I-frame), it'll vary. Typically with a 25fps source, a GOP of 256 frames means that you could have from 0.04s to 10.2s of delay before being able to decode. Also, your pipeline is too complex for what you're trying to achieve. You can use nvvidconv (GPU) which is much better than videoscale (CPU) to rescale your video. You can use capabilities to set the framerate directly (no need for videorate). You can also limit the UDP sink's (and src) buffer size, where you're changing latency for reliability of reordered &amp; late packets. There are other tricks to reduce latency, but you'll to loose something else. You can ask the encoder to decrease the GOP, you can ask no to use B-frame, you can enable slice level encoding, reduce the slice length, increase or decrease the slice intra refresh, limit the profile, etc... All have drawback from default settings, YMMV. Adding queues usually increases latency (but relieve the CPU hotspots) and doesn't reduce it, unless your queues are almost always empty and in that case, you don't need them. That's because a queue requires synchronizing threads and this takes time. This is only required if you have parallel processing on the data and the different branch aren't ticking at the same speed. In the \"simple\" sequential grab, encode, stream mode, the queue is usually not required (since most steps can be made on the GPU &amp; NVENC and are not CPU limited). If you need synchronous I/O (like a filesink), then a queue can be beneficial IIF the processing time is sometimes higher than the grabber's sample rate."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a Jetson Nano 4GB (B01) SOC board and I am running out of space to install the DeepStream SDK. I want to use DeepStream for video streaming application on my Jetson Nano SOC board, but I need guidance on how to install it . What are the steps to install DeepStream SDK on the Jetson Nano?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Closed. This question is not about programming or software development. It is not currently accepting answers. This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered. Closed last month. Improve this question I'm developing a Jetson-based HUD that reboots very quickly. During the reboot process, I noticed that it takes about 10 seconds (relatively long) just to shut down the OS when using the \"reboot\" command. Since I need an OS that can reboot within approximately 10 seconds, achieving my goal is not possible with a 10-second shutdown alone. Therefore, instead of the reboot command, I tried rebooting using the command sudo echo b &gt; /proc/sysrq-trigger, which immediately performs the reboot without any noticeable processing time. Based on my research, I found that this method does not involve synchronizing the file system or unmounting it. Could implementing the OS reboot functionality in this way pose a risk of serious errors or bugs in the system? Would using the sudo echo b &gt; /proc/sysrq-trigger command periodically cause significant issues for the system?",
        "answers": [],
        "votes": []
    },
    {
        "question": "This is a repeat of the question from here I have a jetson agx orin developer kit and I am following instructions from here to install cuda version of pytorch and torchvision. I successfully installed everything and the verification outputs are the following &gt;&gt;&gt; import torch &gt;&gt;&gt; import torchvision &gt;&gt;&gt; print(torch.__version__) 1.14.0a0+44dac51c.nv23.02 &gt;&gt;&gt; print('CUDA available: ' + str(torch.cuda.is_available())) CUDA available: True &gt;&gt;&gt; print('cuDNN version: ' + str(torch.backends.cudnn.version())) cuDNN version: 8600 &gt;&gt;&gt; print(torchvision.__version__) 0.14.1 Then I ran the following inference script from detectron2.engine import DefaultPredictor from detectron2.utils.visualizer import Visualizer import cv2 from detectron2.utils.visualizer import ColorMode from detectron2.config import get_cfg from detectron2 import model_zoo import os import matplotlib.pyplot as plt cfg = get_cfg() cfg.merge_from_file(model_zoo.get_config_file(\"Cityscapes/mask_rcnn_R_50_FPN.yaml\")) cfg.MODEL.WEIGHTS = os.path.join(\"/detectron2_on_kitti/mask_rcnn_output/output_resnet-50/\", \"model_final.pth\") cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 # set a custom testing threshold predictor = DefaultPredictor(cfg) imgPath = \"/dataset/data_semantics/testing/image_2/000000_10.png\" im = cv2.imread(imgPath) outputs = predictor(im) v = Visualizer(im[:, :, ::-1], metadata={}, scale=0.5, instance_mode=ColorMode.SEGMENTATION # remove the colors of unsegmented pixels. This option is only available for segmentation models ) out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\")) plt.figure(figsize=(14, 20)) plt.imshow(out.get_image()[:, :, ::-1]) plt.show() It produces the error ImportError: cannot import name 'is_compiling' from 'torch._dynamo' . Any thoughts on this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to run a python script on boot in a jetson nano. However, I think my script is not running for some reasons and I have no idea what is going on. Can any master help me out? I am trying to make a python script to run on boot on a jetson nano. Here is my python script and my service: startup.service: [Unit] Description = INTU_IPC start-uo specific script [Service] Type= idle ExecStartPre = /bin/sleep 10 ExecStart = /usr/local/bin/startup.sh User=jetbot [Install] WantedBy = multi-user.target startup.sh: #! /bin/sh sleep 10 OPENBLAS_CORETYPE=ARMV8 /usr/bin/python3 ~/py_basics/helloworld.py helloworld.py: #!/usr/bin/env python3 print(\"Hello World!\") I used the following command in terminal to enable my startup.service: sudo systemctl enable startup.service sudo systemctl start startup.service However, when I try to restart my jetson nano, and I tracked the activities of my service with the systemclt status startup.service in the terminal, I think the following lines showed my service is not running. jetbot@jetson-4-3:~$ systemctl status startup.service \u25cf startup.service - INTU_IPC start-uo specific script Loaded: loaded (/etc/systemd/system/startup.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Wed 2023-05-24 18:40:13 PDT; 1min 3s ago Process: 6345 ExecStart=/usr/local/bin/startup.sh (code=exited, status=203/EXEC) Process: 4094 ExecStartPre=/bin/sleep 10 (code=exited, status=0/SUCCESS) Main PID: 6345 (code=exited, status=203/EXEC) May 24 18:39:57 jetson-4-3 systemd[1]: Started INTU_IPC start-uo specific script. May 24 18:40:13 jetson-4-3 systemd[6345]: startup.service: Failed to execute command: Permission denied May 24 18:40:13 jetson-4-3 systemd[6345]: startup.service: Failed at step EXEC spawning /usr/local/bin/startup.sh: Permission denied May 24 18:40:13 jetson-4-3 systemd[1]: startup.service: Main process exited, code=exited, status=203/EXEC May 24 18:40:13 jetson-4-3 systemd[1]: startup.service: Failed with result 'exit-code'. Can anyone help me with this?",
        "answers": [
            [
                "Thank you for the help. I found out that is was some permission problems which I solved it with by changing the permission of the startup.sh with this code executed in the terminal: sudo chmod +x /usr/local/bin/startup.sh However, there are still some problems in importing libraries and modules while running python scripts on jetson nano."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "How to Install qt4 on Jetson Nano (ubuntu 20.04) for Hector Slam Firstly, I tried sudo add-apt-repository ppa:rock-core/qt4 this code and tried to install the needed libraries, but in my opinion it couldn't find the libraries again because of the jetson nano's architecture. Then I tried to download the debian packages from the internet and realized most of them are amd64 packages, I find one package for arm64, but this one needs dependencys as shown below tried to install downloaded package for 20.04 and arm64 When this gives an error, I tried to download the qt5 then change the version manually, as shown below it doesn't work and says run make install (don't know how to do) qt version 4.8 I can't restart with 18.04 on jetson, so much work done until now. I need to install qt4 to run Hector Slam. What should I do any suggestions?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to build PyTorch 1.11.0 in a Docker container running on a Nvidia Jetson Nano. Towards the end of the compilation i get the following error: [1668/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu.o [1669/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/UpSampleLinear1d.cu.o [1670/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/UpSampleNearest1d.cu.o [1671/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/WeightNorm.cu.o [1672/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/UnaryGeometricKernels.cu.o [1673/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/UpSampleNearest2d.cu.o [1674/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/ZetaKernel.cu.o [1675/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/UpSampleNearest3d.cu.o [1676/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu.o [1677/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/ScanKernels.cu.o FAILED: caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/ScanKernels.cu.o /usr/bin/ccache /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DAT_PER_OPERATOR_HEADERS -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTORCH_CUDA_BUILD_MAIN_LIB -DUSE_C10D_GLOO -DUSE_C10D_MPI -DUSE_CUDA -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -I/opt/pytorch/build/aten/src -I/opt/pytorch/aten/src -I/opt/pytorch/build -I/opt/pytorch -I/opt/pytorch/cmake/../third_party/cudnn_frontend/include -I/opt/pytorch/third_party/onnx -I/opt/pytorch/build/third_party/onnx -I/opt/pytorch/third_party/foxi -I/opt/pytorch/build/third_pa rty/foxi -I/opt/pytorch/build/include -I/opt/pytorch/torch/csrc/distributed -I/opt/pytorch/aten/src/THC -I/opt/pytorch/aten/src/ATen/cuda -I/opt/pytorch/build/caffe2/aten/src -I/opt/pytorch/aten/../third_party/catch/single_include -I/opt/pytorch/aten/src/ATen/.. -I/opt/pytorch/c10/cuda/../.. -I/opt/pytorch/c10/.. -I/opt/pytorch/third_party/tensorpipe -I/opt/pytorch/buil d/third_party/tensorpipe -I/opt/pytorch/third_party/tensorpipe/third_party/libnop/include -I/opt/pytorch/torch/csrc/api -I/opt/pytorch/torch/csrc/api/include -isystem /opt/pytorch/build/third_party/gloo -isystem /opt/pytorch/cmake/../third_party/gloo -isystem /opt/pytorch/third_party/protobuf/src -isystem /opt/pytorch/cmake/../third_party/eigen -isystem /usr/include/pyt hon3.10 -isystem /usr/local/lib/python3.10/dist-packages/numpy/core/include -isystem /opt/pytorch/cmake/../third_party/pybind11/include -isystem /usr/lib/aarch64-linux-gnu/openmpi/include/openmpi -isystem /usr/lib/aarch64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -isystem /usr/lib/aarch64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libe vent2022/libevent/include -isystem /usr/lib/aarch64-linux-gnu/openmpi/include -isystem /opt/pytorch/cmake/../third_party/cub -isystem /usr/local/cuda-10.2/include -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_53,code=sm_53 -gencode arch=compute_62,code=sm_62 -gencode arch=compute_72,code=sm_72 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag _suppress=integer_sign_change,--diag_suppress=useless_using_declaration,--diag_suppress=set_but_not_used,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=implicit_return_from_non_void_function,--diag_suppress=unsigned_compare_with_zero,--diag_suppress=declared_but_not_referenced,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -O3 -DNDEBUG - std=c++14 -Xcompiler=-fPIC -DCAFFE2_USE_GLOO -D__NEON__ -DTH_HAVE_THREAD -Xcompiler=-Wall,-Wextra,-Wno-unused-parameter,-Wno-unused-variable,-Wno-unused-function,-Wno-unused-result,-Wno-unused-local-typedefs,-Wno-missing-field-initializers,-Wno-write-strings,-Wno-unknown-pragmas,-Wno-type-limits,-Wno-array-bounds,-Wno-unknown-pragmas,-Wno-sign-compare,-Wno-strict-overfl ow,-Wno-strict-aliasing,-Wno-error=deprecated-declarations,-Wno-missing-braces,-Wno-range-loop-analysis,-Wno-maybe-uninitialized -DTORCH_CUDA_BUILD_MAIN_LIB -Xcompiler -pthread -MD -MT caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/ScanKernels.cu.o -MF caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/ScanKernels.cu.o.d -x cu -c /opt/pytorch/a ten/src/ATen/native/cuda/ScanKernels.cu -o caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/ScanKernels.cu.o Killed [1678/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/attention.cu.o [1679/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/layer_norm_kernel.cu.o [1680/1943] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/group_norm_kernel.cu.o ninja: build stopped: subcommand failed. Building wheel torch-1.11.0a0+gitbc2c6ed -- Building version 1.11.0a0+gitbc2c6ed cmake -GNinja -DBUILD_PYTHON=True -DBUILD_TEST=False -DCMAKE_BUILD=3 -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/opt/pytorch/torch -DCMAKE_PREFIX_PATH=/usr/local/lib/python3.10/dist-packages -DCMAKE_VERSION=3.26 -DCUDA_NVCC_EXECUTABLE=/usr/local/cuda/bin/nvcc -DNUMPY_INCLUDE_DIR=/usr/local/lib/python3.10/dist-packages/numpy/core/include -DPYTHON_EXECUTABLE=/usr/b in/python3.10 -DPYTHON_INCLUDE_DIR=/usr/include/python3.10 -DPYTHON_LIBRARY=/usr/lib/aarch64-linux-gnu/libpython3.10.so.1.0 -DTORCH_BUILD_VERSION=1.11.0a0+gitbc2c6ed -DUSE_CUDA=ON -DUSE_CUDNN=ON -DUSE_FAKELOWP=OFF -DUSE_FBGEMM=OFF -DUSE_MKLDNN=OFF -DUSE_NCCL=OFF -DUSE_NNPACK=OFF -DUSE_NUMPY=True -DUSE_OPENCV=OFF -DUSE_PYTORCH_QNNPACK=OFF -DUSE_QNNPACK=OFF -DUSE_SYSTEM_N CCL=OFF -DUSE_XNNPACK=OFF /opt/pytorch cmake --build . --target install --config Release -- -j 4 The command '/bin/sh -c export=BUILD_CAFFE2_OPS=OFF &amp;&amp; export USE_FBGEMM=OFF &amp;&amp; export USE_FAKELOWP=OFF &amp;&amp; export BUILD_TEST=OFF &amp;&amp; export USE_MKLDNN=OFF &amp;&amp; export USE_NNPACK=OFF &amp;&amp; export USE_XNNPACK=OFF &amp;&amp; export USE_QNNPACK=OFF &amp;&amp; export USE_PYTORCH_QNNPACK=OFF &amp;&amp; export USE_CUDA=ON &amp;&amp; export USE_CUDNN=ON &amp;&amp; export TORCH_CU DA_ARCH_LIST=\"5.3;6.2;7.2\" &amp;&amp; export USE_NCCL=OFF &amp;&amp; export USE_SYSTEM_NCCL=OFF &amp;&amp; export USE_OPENCV=OFF &amp;&amp; export MAX_JOBS=4 &amp;&amp; export PATH=/usr/lib/ccache:$PATH &amp;&amp; export CC=clang &amp;&amp; export CXX=clang++ &amp;&amp; export CUDACXX=/usr/local/cuda/bin/nvcc &amp;&amp; python3.10 setup.py clean &amp;&amp; python3.10 setup.py bdist_wheel' returned a non-zero code: 1 My Dockerfile (my requirements are OpenCV 4.7.0, Python 3.10 and PyTorch 1.11.0), please consider that this Dockerfile is still in \"debugging mode\" and is far from optional. FROM nvcr.io/nvidia/l4t-base:r32.7.1 ARG OPENCV_VERSION=4.7.0 ARG DEBIAN_FRONTEND=noninteractive ARG PYTHON_VERSION=3.10.11 ARG CMAKE_VERSION=3.26 ARG CMAKE_BUILD=3 # Setup ppa for python3.10 RUN set -ex &amp;&amp; apt-get update &amp;&amp; \\ apt-get install -y software-properties-common &amp;&amp; \\ add-apt-repository -y ppa:deadsnakes/ppa &amp;&amp; \\ apt-get update # Install build tools RUN apt-get install -y \\ git \\ wget \\ build-essential \\ checkinstall \\ clang-8 \\ pkg-config \\ curl \\ unzip \\ wget \\ libtool \\ autoconf RUN ln -s /usr/bin/clang-8 /usr/bin/clang &amp;&amp; ln -s /usr/bin/clang++-8 /usr/bin/clang++ RUN apt remove -y --purge --auto-remove cmake # Install cmake RUN apt-get install -y libssl-dev WORKDIR /opt/ RUN wget https://cmake.org/files/v${CMAKE_VERSION}/cmake-${CMAKE_VERSION}.${CMAKE_BUILD}.tar.gz &amp;&amp; \\ tar -xzvf cmake-${CMAKE_VERSION}.${CMAKE_BUILD}.tar.gz WORKDIR cmake-${CMAKE_VERSION}.${CMAKE_BUILD} RUN ./bootstrap RUN make -j$(nproc) RUN make install # Install python3.10 RUN apt-get install -y python3.10 \\ python3.10-dev \\ python3.10-distutils \\ python3-pip \\ python3-numpy RUN curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10 &amp;&amp; \\ python3.10 -m pip install --upgrade pip &amp;&amp; \\ pip3 install numpy==1.22.4 ################################ ## Build and install OpenCV ## ################################ WORKDIR /opt/build # Install OpenCV dependencies RUN apt-get install -y --no-install-recommends \\ libavcodec-dev \\ libavformat-dev \\ libswscale-dev \\ libtbb2 \\ libtbb-dev \\ libjpeg-dev \\ libpng-dev \\ libtiff-dev \\ libdc1394-22-dev \\ libeigen3-dev \\ libhdf5-100 libhdf5-dev \\ libopenblas-base libopenblas-dev \\ libprotobuf10 libprotobuf-dev \\ libjpeg8 libjpeg8-dev \\ libpng16-16 libpng-dev \\ libtiff5 libtiff-dev \\ libwebp6 libwebp-dev \\ libopenjp2-7 libopenjp2-7-dev \\ libtbb2 libtbb-dev \\ libeigen3-dev \\ tesseract-ocr tesseract-ocr-por libtesseract-dev \\ ffmpeg RUN apt-get install -y --no-install-recommends \\ libgstreamer1.0* \\ ubuntu-restricted-extras \\ libgstreamer1.0-dev \\ libgstreamer-plugins-base1.0-dev RUN wget --no-check-certificate https://github.com/opencv/opencv/archive/${OPENCV_VERSION}.zip -O opencv.zip RUN unzip opencv.zip -d /opt &amp;&amp; rm -rf opencv.zip RUN wget --no-check-certificate https://github.com/opencv/opencv_contrib/archive/${OPENCV_VERSION}.zip -O opencv_contrib.zip RUN unzip opencv_contrib.zip -d /opt &amp;&amp; rm -rf opencv_contrib.zip RUN cmake \\ -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D OPENCV_EXTRA_MODULES_PATH=/opt/opencv_contrib-${OPENCV_VERSION}/modules \\ -D EIGEN_INCLUDE_PATH=/usr/include/eigen3 \\ -D PYTHON3_EXECUTABLE=$(which python3.10) \\ -D PYTHON3_INCLUDE_DIR=$(python3.10 -c \"from distutils.sysconfig import get_python_inc; print(get_python_inc())\") \\ -D PYTHON3_PACKAGES_PATH=$(python3.10 -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\") \\ -D OPENCV_GENERATE_PKGCONFIG=ON \\ -D OPENCV_ENABLE_NONFREE=ON \\ -D WITH_JPEG=ON \\ -D WITH_PNG=ON \\ -D WITH_TIFF=ON \\ -D WITH_WEBP=ON \\ -D WITH_JASPER=ON \\ -D WITH_EIGEN=ON \\ -D WITH_TBB=ON \\ -D WITH_LAPACK=ON \\ -D WITH_PROTOBUF=ON \\ -D WITH_V4L=ON \\ -D WITH_GSTREAMER=ON \\ -D WITH_OPENCL=ON \\ -D WITH_FFMPEG=ON \\ -D WITH_CUDA=ON \\ -D WITH_GTK=OFF \\ -D WITH_QT=OFF \\ -D WITH_VTK=OFF \\ -D WITH_OPENEXR=OFF \\ -D WITH_OPENNI=OFF \\ -D WITH_XINE=OFF \\ -D WITH_GDAL=OFF \\ -D WITH_IPP=OFF \\ -D BUILD_OPENCV_PYTHON3=ON \\ -D BUILD_OPENCV_PYTHON2=OFF \\ -D BUILD_OPENCV_JAVA=OFF \\ -D BUILD_TESTS=OFF \\ -D BUILD_IPP_IW=OFF \\ -D BUILD_PERF_TESTS=OFF \\ -D BUILD_EXAMPLES=OFF \\ -D BUILD_ANDROID_EXAMPLES=OFF \\ -D BUILD_DOCS=OFF \\ -D BUILD_ITT=OFF \\ -D INSTALL_PYTHON_EXAMPLES=OFF \\ -D INSTALL_C_EXAMPLES=OFF \\ -D INSTALL_TESTS=OFF \\ /opt/opencv-${OPENCV_VERSION} &gt;&gt; cmake_out.txt 2&gt;&amp;1 RUN make -j$(nproc) RUN make install RUN rm -rf /opt/build/* &amp;&amp; rm -rf /opt/opencv-${OPENCV_VERSION} &amp;&amp; rm -rf /opt/opencv_contrib-${OPENCV_VERSION} # ################################# # ## Build and install PyTorch ## # ################################# RUN git clone -b v1.11.0 --depth=1 --recursive --recurse-submodules --shallow-submodules https://github.com/pytorch/pytorch.git /opt/pytorch WORKDIR /opt/pytorch RUN apt-get install -y \\ libomp5 \\ libopenmpi-dev \\ libomp-dev \\ ninja-build \\ ccache \\ gnupg2 RUN apt-get remove -y python3-numpy RUN pip3 install -U setuptools &amp;&amp; \\ pip3 install -U pip wheel mock pillow &amp;&amp; \\ pip3 install -U scikit-build &amp;&amp; \\ pip3 install -U cython Pillow numpy==1.22.4 RUN pip3 install -r requirements.txt COPY ./pytorch-1.11-jetson.patch /opt/pytorch RUN patch -p1 &lt; pytorch-1.11-jetson.patch RUN export=BUILD_CAFFE2_OPS=OFF &amp;&amp; \\ export USE_FBGEMM=OFF &amp;&amp; \\ export USE_FAKELOWP=OFF &amp;&amp; \\ export BUILD_TEST=OFF &amp;&amp; \\ export USE_MKLDNN=OFF &amp;&amp; \\ export USE_NNPACK=OFF &amp;&amp; \\ export USE_XNNPACK=OFF &amp;&amp; \\ export USE_QNNPACK=OFF &amp;&amp; \\ export USE_PYTORCH_QNNPACK=OFF &amp;&amp; \\ export USE_CUDA=ON &amp;&amp; \\ export USE_CUDNN=ON &amp;&amp; \\ export TORCH_CUDA_ARCH_LIST=\"5.3;6.2;7.2\" &amp;&amp; \\ export USE_NCCL=OFF &amp;&amp; \\ export USE_SYSTEM_NCCL=OFF &amp;&amp; \\ export USE_OPENCV=OFF &amp;&amp; \\ export MAX_JOBS=4 &amp;&amp; \\ export PATH=/usr/lib/ccache:$PATH &amp;&amp; \\ export CC=clang &amp;&amp; \\ export CXX=clang++ &amp;&amp; \\ export CUDACXX=/usr/local/cuda/bin/nvcc &amp;&amp; \\ python3.10 setup.py clean &amp;&amp; \\ python3.10 setup.py bdist_wheel The patch that I'm applying to the the files (i found this in multiple tutorials online). diff --git a/aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h b/aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h index 0327868..e484fba 100644 --- a/aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h +++ b/aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h @@ -26,6 +26,7 @@ inline namespace CPU_CAPABILITY { // https://bugs.llvm.org/show_bug.cgi?id=45824 // Most likely we will do aarch32 support with inline asm. #if defined(__aarch64__) +#if defined(__clang__) || (__GNUC__ &gt; 8 || (__GNUC__ == 8 &amp;&amp; __GNUC_MINOR__ &gt; 3)) #ifdef __BIG_ENDIAN__ #error \"Big endian is not supported.\" @@ -715,5 +716,6 @@ Vectorized&lt;float&gt; inline fmadd(const Vectorized&lt;float&gt;&amp; a, const Vectorized&lt;floa } #endif /* defined(aarch64) */ +#endif /* defined(__clang__) */ }}} diff --git a/aten/src/ATen/cuda/CUDAContext.cpp b/aten/src/ATen/cuda/CUDAContext.cpp index 1751128..a090e70 100644 --- a/aten/src/ATen/cuda/CUDAContext.cpp +++ b/aten/src/ATen/cuda/CUDAContext.cpp @@ -24,6 +24,7 @@ void initCUDAContextVectors() { void initDeviceProperty(DeviceIndex device_index) { cudaDeviceProp device_prop; AT_CUDA_CHECK(cudaGetDeviceProperties(&amp;device_prop, device_index)); + device_prop.maxThreadsPerBlock = device_prop.maxThreadsPerBlock / 2; device_properties[device_index] = device_prop; } diff --git a/aten/src/ATen/cuda/detail/KernelUtils.h b/aten/src/ATen/cuda/detail/KernelUtils.h index b36e78c..dea597f 100644 --- a/aten/src/ATen/cuda/detail/KernelUtils.h +++ b/aten/src/ATen/cuda/detail/KernelUtils.h @@ -19,7 +19,7 @@ namespace at { namespace cuda { namespace detail { // Use 1024 threads per block, which requires cuda sm_2x or above -constexpr int CUDA_NUM_THREADS = 1024; +constexpr int CUDA_NUM_THREADS = 512; // CUDA: number of blocks for threads. inline int GET_BLOCKS(const int64_t N, const int64_t max_threads_per_block=CUDA_NUM_THREADS) { On the Jetson Nano nvidia is set as the default docker runtime so that CUDA is available during docker build.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to build Pytorch from source following tutorial on Jetson: Jetson 5.0.2 GA os: Ubuntu 20.04.4 LTS focal cmake version: 3.16.3, installed along with ROS noetic. gcc: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0 The error occurred at the installation step python3 setup.py bdist_wheel: Building wheel torch-1.12.0 -- Building version 1.12.0 cmake -GNinja -DBUILD_PYTHON=True -DBUILD_TEST=True -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/media/gvsc/reddy_reddy/pytorch/torch -DCMAKE_PREFIX_PATH=/usr/lib/python3.8/site-packages;/home/gvsc/Admin/Projects/racer_ws/devel:/opt/ros/noetic -DNUMPY_INCLUDE_DIR=/usr/lib/python3/dist-packages/numpy/core/include -DPYTHON_EXECUTABLE=/usr/bin/python3 -DPYTHON_INCLUDE_DIR=/usr/include/python3.8 -DPYTHON_LIBRARY=/usr/lib/libpython3.8.so.1.0 -DTORCH_BUILD_VERSION=1.12.0 -DUSE_NCCL=0 -DUSE_NUMPY=True -DUSE_PYTORCH_QNNPACK=0 -DUSE_QNNPACK=0 /media/gvsc/reddy_reddy/pytorch -- Could not find ccache. Consider installing ccache to speed up compilation. -- Performing Test COMPILER_WORKS -- Performing Test COMPILER_WORKS - Failed CMake Error at cmake/MiscCheck.cmake:34 (message): Could not run a simple program built with your compiler. If you are trying to use -fsanitize=address, make sure libasan is properly installed on your system (you can confirm if the problem is this by attempting to build and run a small program.) Call Stack (most recent call first): CMakeLists.txt:679 (include) -- Configuring incomplete, errors occurred! See also \"/media/gvsc/reddy_reddy/pytorch/build/CMakeFiles/CMakeOutput.log\". See also \"/media/gvsc/reddy_reddy/pytorch/build/CMakeFiles/CMakeError.log\". libasan is installed as libasan5 I also verified that the machine can use cmake to build a simple program by following the cmake tutorial. I could run cmake . to get cmake-tutotrial built. If anyone can suggest a solution, I will be very grateful!",
        "answers": [],
        "votes": []
    },
    {
        "question": "Device Specification : **\u2022 Hardware Platform (Jetson Xavier NX) **\u2022 Deepstream NGC Container ( DeepStream-l4t container 6.0 samples ) **\u2022 JetPack Version 4.6.1 (L4T 32.7.1) **\u2022 CUDA 10.2.300 **\u2022 TensorRT Version 8.2.1.8 **\u2022 CUDNN : 8.2.1.32 I am trying to run a deepstream application on my device. I get following issue while running my application with my current rtsp. NVPARSER: HEVC: Seeking is not performed on IRAP picture I have tested my rtsp with multiple methods. It is working only with cv2 with ffmpeg as a backend with other methods it is giving errors. The methods I have tried are following. With ffplay and ffmpeg errors are following. enter image description here enter image description here With gst-launch-1.0 / gst-play-1.0 / gst-discoverer-1.0 I am getting same error as one in deepstream Application. enter image description here Can someone give me solution to this issue or just explain me the reason for this issue to occur is it related to my rtsp source or some other issue is happening.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Is it possible to use the Azure Kinect Sensor SDK (k4a-tools) on the NVIDIA Jetson AGX Orin Development Kit? It is quite difficult to find any docker images or other solutions. I tried to search for instructions and docker images.",
        "answers": [
            [
                "Please try to look into the jetson-containers docker images created with Dockerfiles especially for NVIDIA/jetson products. Link : jetson-containers"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to build a digital sign using a Jetson device running Ubuntu connected to a HDMI monitor. The device does not have a GUI or X server running. It displays the required image directly to the framebuffer from within a non-interactive Docker container (by binding '-v /dev/fb0:/dev/fb0' when running the container). The boot sequence is as follows: User 'restricted_user' gets logged in automatically (I edited the getty@tty1 service to include '-a restricted_user'), and then the container is started as a separate systemd service. Now I would like to disable the keyboard for this user, so that someone can't come along and plug a keyboard into the device and start making changes. The only way to stop the container service should be via ssh. I would also like to hide the terminal from the screen, so that only the image being written to the framebuffer gets displayed (i.e. no command prompt, flashing cursor, etc.) I tried removing the 'restricted_user' from group 'input'. I also looked at the permissions for the event0 to event9 devices, but these didn't include any permissions for non-group users.",
        "answers": [],
        "votes": []
    },
    {
        "question": "\"I have set up a task on my Jetson device, where if someone presses the \"FORCE RECOVERY\" button for 15 seconds, the device will automatically recover. The pin number for the FORCE RECOVERY button is documented as pin 214, and I have used the Jetson.GPIO library in my Python code to detect the button press. However, I am getting a \"ValueError: Channel 214 is invalid\" error message when running my code. Can anyone help me understand what I am doing wrong and how I can correctly detect the button press on pin 214 using Jetson.GPIO?\" import Jetson.GPIO as GPIO #To disable the warnings GPIO.setwarnings(False) # Set the GPIO mode to BCM GPIO.setmode(GPIO.BCM) # Set pin 214 as input GPIO.setup(214, GPIO.IN) # Read the input status of pin 214 input_value = GPIO.input(214) # Print the input value to the console print(\"Input value of pin 214:\", input_value) # Clean up the GPIO configuration GPIO.cleanup() import Jetson.GPIO as GPIO #To disable the warnings GPIO.setwarnings(False) GPIO.setmode(GPIO.TEGRA_SOC) GPIO.setup('FORCE_RECOVERY', GPIO.IN) value = GPIO.input('FORCE_RECOVERY') print(value) # Clean up the GPIO configuration GPIO.cleanup() I have written this code for FC REC (Force recovery pin) but i am getting below error: WARNING: Carrier board is not from a Jetson Developer Kit.WARNNIG: Jetson.GPIO library has not been verified with this carrier board,WARNING: and in fact is unlikely to work correctly.Traceback (most recent call last):File \"frp.py\", line 7, inGPIO.setup(214, GPIO.IN)File \"/usr/lib/python3/dist-packages/Jetson/GPIO/gpio.py\", line 358, in setupch_infos = _channels_to_infos(channels, need_gpio=True)File \"/usr/lib/python3/dist-packages/Jetson/GPIO/gpio.py\", line 121, in _chansfor c in _make_iterable(channels)]File \"/usr/lib/python3/dist-packages/Jetson/GPIO/gpio.py\", line 121, infor c in _make_iterable(channels)]File \"/usr/lib/python3/dist-packages/Jetson/GPIO/gpio.py\", line 104, in _chanpraise ValueError(\"Channel %s is invalid\" % str(channel))ValueError: Channel 214 is invalid this is the link: link for the Documentation where metioned about the 214 FC REC pin",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using an NDIVIA edge device to collect environmental data and writing these data into a google sheet. I want this process to continue for 5 days but after sometime (an hour), I get an error message ssl.SSLError: [SSL: KRB5_S_TKT_NYV] unexpected eof while reading (_ssl.c:2570) Can anyone help? I wanted to write data into google sheet for 5 days but stops in an hour",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying reduce the size of jetson jetpack's pytorch docker image by removing some dependencies that I don't need. However size of the locally built image after removing certain dependencies is bigger than the size of the container if I just import the base pytorch image. If I build the following minimal docker file. FROM nvcr.io/nvidia/l4t-pytorch:r32.5.0-pth1.7-py3 RUN echo \"Hello World!\" The size of the built image is 1.69GB. However if I take the original docker file for l4t-pytorch:r32.5.0-pth1.7-py3 from here and build it locally, using docker build --squash --no-cache -t pytorch_image . The size of the built image is 2.3GB. What am I missing?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using a Jetson Orin developer kit with the following code based on Yolov7. The detection time reaches up to 1.6 s on a custom dataset, using GPU with CUDA enabled. The OS is Ubuntu 20.04, the opencv version is 4.7.0, the python version is 3.8, the CUDA version is 11.4 and cuDNN 8.6. When using a video file the detection time is reduced, but when I'm using a machine vision camera the detection time is significantly increased. Any suggestions on how to reduce the detection time? import logging from datetime import datetime from pathlib import Path import random from typing import List import cv2 import numpy as np from deep_sort.deep_sort import Deep from enums.camera.RunningCameraStatus import RunningCameraStatus from models.CameraRunningContext import CameraRunningContext from models.measurements.CameraSingleMeasurementValue import CameraSingleMeasurementValue from object_detection import ObjectDetection from services.detectors.CameraDetectorService import CameraDetectorService class Yolo7CameraDetectorService(CameraDetectorService): def __init__(self): super().__init__() #Define a dictionary that # Key : integer # Value : timestamp of last updated(exit camera timestamp) self.__detected_items = dict() \"\"\" Detect items using yolo7 Store items that remains in the current frame in the detected items dictionary Remove all the other items and write them to camera measurements buffer \"\"\" def detect(self, camera: CameraRunningContext): #Call abstract method super().detect(camera) # Counting area ROI yellow_poly_pts = np.array([[0, 150], [1440, 150], [1440, 850], [0, 850]]) #Define codec for video fourcc = cv2.VideoWriter_fourcc(*'XVID') # Load Object Detection # ATTENTION # Object detection applies toLower() on path path_waights = Path(camera.camera.detection_weights_path) od = ObjectDetection(path_waights) od.load_detection_model(image_size=416, confThreshold=0.5, nmsThreshold=0.4) od.load_class_names(camera.camera.detection_classes_names) # Load Object Tracking Deep Sort deep = Deep(max_distance=0.8, nms_max_overlap=0.3, n_init=3, max_age=5, max_iou_distance=0.7) tracker = deep.sort_tracker() # While running camera status is enabled and streaming and detecting while camera.status is RunningCameraStatus.ENABLED_AND_STREAMING_AND_DETECTING : now = datetime.utcnow() # Get current raw image frame image = camera.get_cur_raw_image() # Convert to RGB image rgb = image.convert(\"RGB\") # Convert np array array = rgb.get_numpy_array() #out = cv2.VideoWriter('/home/christina/Desktop/yv7run/output.avi', fourcc, 20.0,(1080, 1080)) # opencv loads the format bgr so the frame needs to be converted to rgb to process array = cv2.cvtColor(array, cv2.COLOR_BGR2RGB) np.save(\"/home/christina/Desktop/test.np\",array) logging.info(\"Get frame and convert to cvtcolor in mseconds=\" + str( (datetime.utcnow() - now).total_seconds() * 1000 )) \"\"\" 1. Object Detection \"\"\" (class_ids, scores, boxes) = od.detect(array) logging.info( \"od detect only in mseconds=\" + str((datetime.utcnow() - now).total_seconds() * 1000)) \"\"\" 2. Object Tracking \"\"\" features = deep.encoder(array, boxes) detections = deep.Detection(boxes, scores, class_ids, features) logging.info( \"tracking only in mseconds=\" + str((datetime.utcnow() - now).total_seconds() * 1000)) # Draw yellow polygon for the ROI cv2.polylines(array, [yellow_poly_pts], True, (0, 255, 255), 2) tracker.predict() (class_ids, object_ids, boxes) = tracker.update(detections) logging.info(\"Object dtection and tracker in mseconds=\" + str((datetime.utcnow() - now).total_seconds() * 1000)) # Iterate tracker results for class_id, object_id, box in zip(class_ids, object_ids, boxes): (x, y, x2, y2) = box class_name = od.names[class_id] color = od.colors[class_id] # Center doubleroll detected cx = int((x + x2) / 2) cy = int((y + y2) / 2) cv2.rectangle(array, (x, y), (x2, y2), (0, 0, 255), 2) cv2.rectangle(array, (x, y), (x + len(class_name) * 20, y - 30), (0, 0, 255), -1) cv2.putText(array, class_name + \" \" + str(object_id), (x, y - 10), 0, 0.75, (255, 255, 255), 2) cv2.circle(array, (cx, cy), 4, (0, 0, 255), -1) \"\"\"3. Object Counting\"\"\" # Check if object is in the ROI result = cv2.pointPolygonTest(yellow_poly_pts, (int(cx), int(cy)), False) #print(result, object_id, datetime.utcnow()) if result &gt; 0: # Store item to dictionary as camera single measurement value self.__detected_items[object_id] = CameraSingleMeasurementValue(value=1, timestamp= datetime.now()) print(\"FOUND ONE!!!\", result) #cv2.imwrite(\"/home/christina/Desktop/yv7run/Img005.png\", array) print(\"Done with image\") #out.write(array) # Show text with counter #od_count = len(obs_counted) #cv2.putText(array, \"Number of Objects {}\".format(od_count), (20, 20), 0, 1, (0, 255, 0), 2) #imS = cv2.resize(array, (960, 540)) #cv2.imshow(\"Frame\", imS) #cv2.waitKey(0) #out.release() #Outside iteration #Remove all items that does not exist in the current object ids list # The current frame detected items must be added before to the dictionary camera_measurement_values = self.__remove_and_get_all_except(object_ids) #Add values to buffers self.camera_buffered_measurements_service.add_multi(camera.camera.crguid, camera_measurement_values) logging.info( \"Total mseconds=\" + str((datetime.utcnow() - now).total_seconds() * 1000)) \"\"\" Remove and retutn all items ids from \"\"\" def __remove_and_get_all_except(self, object_ids) -&gt; List[CameraSingleMeasurementValue]: # Get all dictionary keys that does not exist in the current frame detected objects keys = [key for key in self.__detected_items.keys() if key not in object_ids] # Get values for the above keys values = [self.__detected_items.get(key) for key in keys] # Remove above keys form dictionary for key in keys: del self.__detected_items[key] # Dictionary contains only detected items that are not exited camera and are still trackable # Return items that exits camera context return values",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to install opencv on a Jetson Xavier NX running Ubuntu 20.04, and the viz library is missing in opencv. If I install opencv via sudo apt install libopencv-dev on my laptop, viz is installed by default, but if I install on the jetson the ARM opencv is missing the viz library.sudo apt install libopencv-viz-dev, but this automatically removes the other opencv modules. How can I install libopencv-dev along with libopencv-viz-dev? Also, if I run dpkg --list | grep opencv, I see libopencv-viz4.2:arm64 as one of the entries, so it seems viz is being installed, but if I look in /usr/include/opencv4/opencv2, the viz folder is missing.",
        "answers": [],
        "votes": []
    },
    {
        "question": "import time import Jetson.GPIO as GPIO gpio_pin = 388 # Orbitty carrier 7(GPIO_0) pin # Sbus signal encoding function def encode_sbus(channels, failsafe=False): sbus_data = [0] * 25 # Sbus frame is 25 bytes long and all channels are encoded as 16 bits. sbus_data[0] = 0x0F # Sbus frame start byte sbus_data[24] = 0x00 # Sbus frame end byte if failsafe: # activate failsafe sbus_data[23] = 0x04 for i in range(16): channel_value = channels[i] if channel_value &lt; 1005: # if channel value is less than the minimum value, set it to the minimum value channel_value = 1005 elif channel_value &gt; 1950: # if channel value is greater than the maximum value, set it to the maximum value channel_value = 1950 # encode the channel value bit_pos = i % 8 byte_pos = (i // 8) + 1 if bit_pos == 0: sbus_data[byte_pos] |= (channel_value &amp; 0x07) &lt;&lt; 1 sbus_data[byte_pos + 1] = (channel_value &amp; 0x07F8) &gt;&gt; 3 elif bit_pos == 1: sbus_data[byte_pos] |= (channel_value &amp; 0x03) &lt;&lt; 3 sbus_data[byte_pos + 1] |= (channel_value &amp; 0x07F0) &gt;&gt; 1 else: sbus_data[byte_pos] |= (channel_value &amp; 0x01) &lt;&lt; (bit_pos + 3) sbus_data[byte_pos + 1] |= (channel_value &amp; 0x07F0) &gt;&gt; (bit_pos - 1) return sbus_data # GPIO pin setup GPIO.setmode(GPIO.BOARD) GPIO.setup(gpio_pin, GPIO.OUT) # initialize channel values channels = [1500] * 16 # initialize all 16 channels to the center value of 1500 while True: # adjust channel values here as needed channels[0] = 1600 # example: set channel 1 to 1600 channels[1] = 1400 # example: set channel 2 to 1400 # encode the updated channel values into an Sbus signal sbus_signal = encode_sbus(channels) # Example of activate FAILSAFE. # sbus_signal = encode_sbus(channels, failsafe=True) # send the Sbus signal through the GPIO pin GPIO.output(gpio_pin, GPIO.HIGH) time.sleep(0.000150) for i in range(25): for j in range(8): GPIO.output(gpio_pin, sbus_signal[i] &amp; (1 &lt;&lt; j)) time.sleep(0.000150) GPIO.output(gpio_pin, GPIO.LOW) time.sleep(0.003) This is my code, What i want to do is Generate Sbus signal with Jetson tx2(Orbitty Carrier), and transmit it throgh GPIO pin. So I searched that gpio 338 is orbitty carriers gpio 0. And, i connected a wire gpio 0 and FC(Hpbbywing F4G3).. But code didt work, and log says 388pin is not available..if i change the pin num, this code will work but notthing happens in Betaflight configurator..... What should i doo..Thankyou!(sry for bad ENG) I want to operate Hobbywing fc with Jetson Tx2",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to install the latest arm version of VSCode on jetsonNX with jetpack5.1, but after installation, VSCode cannot run and no response. I checked the system-monitor and there are VSCode components running.",
        "answers": [
            [
                "You would have to run code with no-sandbox flag with XavierNX: code --no-sandbox"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm testing Yolov5n with a Jetson nano B01 device (4GB). For this purpose I'm using a docker version from this repo: repo FROM nvcr.io/nvidia/l4t-base:r32.7.1 ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update &amp;&amp; apt-get install -y \\ git \\ python3.8 python3.8-dev python3-pip \\ libopenmpi-dev libomp-dev libopenblas-dev libblas-dev libeigen3-dev RUN python3.8 -m pip install --upgrade pip RUN python3.8 -m pip install setuptools gdown # pytorch 1.11.0 RUN gdown https://drive.google.com/uc?id=1hs9HM0XJ2LPFghcn7ZMOs5qu5HexPXwM RUN python3.8 -m pip install torch-*.whl # torchvision 0.12.0 RUN gdown https://drive.google.com/uc?id=1m0d8ruUY8RvCP9eVjZw4Nc8LAwM8yuGV RUN python3.8 -m pip install torchvision-*.whl RUN git clone https://github.com/ultralytics/yolov5.git WORKDIR yolov5 RUN python3.8 -m pip install -r requirements.txt COPY is_docker.patch . RUN patch -p1 &lt; is_docker.patch I'm running docker from a default installation (SD Card, no SSD) These are the results from a list of 640x640 car and bottle images: root@25b13ebea957:/yolov5# python3.8 detect.py --weights yolov5n.pt --source data/images detect: weights=['yolov5n.pt'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1 YOLOv5 \ud83d\ude80 v7.0-116-g5c91dae Python-3.8.0 torch-1.11.0a0+gitbc2c6ed CUDA:0 (NVIDIA Tegra X1, 3964MiB) Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt to yolov5n.pt... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.87M/3.87M [00:01&lt;00:00, 4.02MB/s] Fusing layers... YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients image 1/178 /yolov5/data/images/00000002_jpg.rf.41c3bd80c13879da35da9cad8c672729.jpg: 640x640 1 parking meter, 174.0ms image 2/178 /yolov5/data/images/00000006_jpg.rf.dd3940f860d6ade63e4247555fb1e559.jpg: 640x640 1 cake, 173.9ms image 3/178 /yolov5/data/images/00000020_jpg.rf.668142e21ed24351ae6a8f5cc899817d.jpg: 640x640 (no detections), 175.3ms image 4/178 /yolov5/data/images/00000021_jpg.rf.98fce8efe88fd90a1ec0bd683243537c.jpg: 640x640 2 bottles, 173.9ms image 5/178 /yolov5/data/images/00000028_jpg.rf.03e103667071f566385b6ff03e129069.jpg: 640x640 1 person, 173.6ms image 6/178 /yolov5/data/images/00000040_jpg.rf.81e12357b61957bc7d01bb7b8fc06861.jpg: 640x640 1 surfboard, 1 bottle, 2 vases, 173.6ms image 7/178 /yolov5/data/images/00000092_jpg.rf.1c4b4f1dc7da8b6ccf574a6067651a5e.jpg: 640x640 5 bottles, 173.9ms image 8/178 /yolov5/data/images/00000132_jpg.rf.1a274621f0cf8c342962d61cefda313c.jpg: 640x640 4 bottles, 1 toothbrush, 175.4ms image 9/178 /yolov5/data/images/00000141_png.rf.0753e47a2c4614522fb06ecc9e4b10bd.jpg: 640x640 2 bottles, 173.8ms image 10/178 /yolov5/data/images/00000146_jpg.rf.ca6ce3dd4e5669fa6e15c1faf71918a4.jpg: 640x640 (no detections), 174.0ms image 11/178 /yolov5/data/images/00000163_jpg.rf.f62ee3d7bf9bbeff16f05dde8fcc2277.jpg: 640x640 1 car, 175.4ms image 12/178 /yolov5/data/images/00000184_jpg.rf.8b2df2e255a02204a885aff64f01f366.jpg: 640x640 (no detections), 173.8ms image 13/178 /yolov5/data/images/00000222_jpg.rf.c633548eabfa4e6b2532805f370933f6.jpg: 640x640 3 bottles, 2 cups, 3 vases, 175.4ms image 14/178 /yolov5/data/images/00000286_jpg.rf.da4093f2136eca656fe9d148eccec9c9.jpg: 640x640 2 bottles, 173.7ms image 15/178 /yolov5/data/images/00000291_jpg.rf.f286e4547328ad1c912e66836500aaf6.jpg: 640x640 (no detections), 173.8ms image 16/178 /yolov5/data/images/00000312_jpg.rf.cb7cfb97cdcc8e451b49eaa491235325.jpg: 640x640 (no detections), 175.2ms image 17/178 /yolov5/data/images/00000313_jpg.rf.e80becce9fa85652a528abe75132f487.jpg: 640x640 8 bottles, 173.9ms image 18/178 /yolov5/data/images/00000314_jpg.rf.e9f500142f276f218b0dacf24f6e1fb3.jpg: 640x640 1 vase, 175.3ms image 19/178 /yolov5/data/images/00000322_jpeg.rf.eee821b3dbd466ed90abd9017551a801.jpg: 640x640 2 bottles, 173.6ms image 20/178 /yolov5/data/images/00000330_jpg.rf.f5d384fb5d4bd3190b38ab48739b805b.jpg: 640x640 (no detections), 173.6ms image 21/178 /yolov5/data/images/00000332_jpg.rf.28cbcb6d54a5357cfba39d935a301bc9.jpg: 640x640 2 bottles, 1 cell phone, 175.6ms image 22/178 /yolov5/data/images/00000344_jpg.rf.9146ceacbcc8a2fb9447b6b7bb8a84ea.jpg: 640x640 (no detections), 173.9ms image 23/178 /yolov5/data/images/00000363_jpg.rf.aa01a6501cc21dcc73e3bcddfe21ed56.jpg: 640x640 1 vase, 175.5ms image 24/178 /yolov5/data/images/00000392_jpg.rf.1a206b2f0d8a62ffc20def8cfb6b9cb3.jpg: 640x640 1 toothbrush, 174.1ms image 25/178 /yolov5/data/images/00000405_jpg.rf.9f358303c0f8e65921128c7bf42c6e9f.jpg: 640x640 1 vase, 173.9ms image 26/178 /yolov5/data/images/03ea3ede0a_jpg.rf.fd4b0dd4844747e738e8784ed3ccaa20.jpg: 640x640 1 car, 1 bus, 1 parking meter, 175.4ms image 27/178 /yolov5/data/images/144a1afb9a_jpg.rf.e0d8645f9e06c85e0e8e723cbdf9e429.jpg: 640x640 (no detections), 173.7ms image 28/178 /yolov5/data/images/165347b4f5_jpg.rf.14a66411f8f715399c8509b4eaf99c22.jpg: 640x640 (no detections), 175.6ms image 29/178 /yolov5/data/images/1dea3c8341_jpg.rf.85a5126880ee0f208d392692dfd0539a.jpg: 640x640 (no detections), 173.5ms image 30/178 /yolov5/data/images/293b21c8b3_jpg.rf.9bfb3c9a1762b104a49564208e3b79b4.jpg: 640x640 3 cups, 173.5ms image 31/178 /yolov5/data/images/3b7255cc78_jpg.rf.425bae164f755ee345019c1d80c69744.jpg: 640x640 1 cup, 175.5ms image 32/178 /yolov5/data/images/496c16bd93_jpg.rf.af4a82e06fd8ae79728e56cc766b8270.jpg: 640x640 (no detections), 173.7ms image 33/178 /yolov5/data/images/72278aeed6_jpg.rf.c32eb26ebbd96f63e0eccf571d24d170.jpg: 640x640 3 cups, 1 carrot, 1 cake, 175.4ms image 34/178 /yolov5/data/images/725640810b_jpg.rf.4269e0f3c3b844d84f26fd79260c5ffa.jpg: 640x640 3 bottles, 3 cups, 173.7ms image 35/178 /yolov5/data/images/733867bbc2_jpg.rf.9b1bc09e7cd5af85d307877c6f837e76.jpg: 640x640 1 person, 173.8ms image 36/178 /yolov5/data/images/79838d8653_jpg.rf.15345de891ec0a9275ba63134eaff80b.jpg: 640x640 (no detections), 175.2ms image 37/178 /yolov5/data/images/8290e2aecd_jpg.rf.bb4f8c30ec1e7998623f89d6a587cf0b.jpg: 640x640 (no detections), 173.8ms image 38/178 /yolov5/data/images/95932a5b20_jpg.rf.49d1a36f9f48e14a4bbc4542ac3e0cac.jpg: 640x640 1 suitcase, 173.9ms image 39/178 /yolov5/data/images/95a3b3d5b0_jpg.rf.8408804a7ad35077f74310e83f5dde1c.jpg: 640x640 2 cups, 2 cakes, 173.9ms image 40/178 /yolov5/data/images/977156e83e_jpg.rf.30116803929852dabfebe2c6cbf0d69b.jpg: 640x640 3 vases, 173.8ms image 41/178 /yolov5/data/images/R_103_jpg.rf.a3140d34b7f02d1d87e414d7637a8b4b.jpg: 640x640 (no detections), 175.5ms image 42/178 /yolov5/data/images/R_107_jpg.rf.19c701f0a06a91ec34aaae9aad3f2c02.jpg: 640x640 (no detections), 173.9ms image 43/178 /yolov5/data/images/R_118_jpg.rf.64d113f463c82dc54077707e3d020d29.jpg: 640x640 (no detections), 173.8ms image 44/178 /yolov5/data/images/R_120_jpg.rf.63fd9437bc7da2b527d09a203be433f6.jpg: 640x640 (no detections), 173.8ms image 45/178 /yolov5/data/images/R_129_jpg.rf.58bd16de956b94233ae9eb7cfcbb7734.jpg: 640x640 1 person, 1 car, 173.8ms image 46/178 /yolov5/data/images/R_142_jpg.rf.1e4f57d8a3a012f746ac0b57d55f6345.jpg: 640x640 (no detections), 175.2ms image 47/178 /yolov5/data/images/R_157_jpg.rf.cb820c3f519af5c7c4fe8b56a1b77a33.jpg: 640x640 (no detections), 173.7ms image 48/178 /yolov5/data/images/R_171_jpg.rf.0d48a28aad8f47c58cea0c308d2ce18f.jpg: 640x640 (no detections), 174.2ms image 49/178 /yolov5/data/images/R_227_jpg.rf.6b3e03ba26aaa3f1ec671967c83f4f7c.jpg: 640x640 (no detections), 174.0ms image 50/178 /yolov5/data/images/R_233_jpg.rf.7bfebf3810d6e935f9743e13fa9de75a.jpg: 640x640 1 bus, 173.8ms image 51/178 /yolov5/data/images/R_242_jpg.rf.9d4fbc41af98ebd901e43305b7d92569.jpg: 640x640 (no detections), 175.6ms image 52/178 /yolov5/data/images/R_262_jpg.rf.b31a699cd4fb180dd9b7f45852640c4d.jpg: 640x640 (no detections), 174.1ms image 53/178 /yolov5/data/images/R_269_jpg.rf.d3a4231587840257972b5c5e2d10b792.jpg: 640x640 1 person, 173.6ms image 54/178 /yolov5/data/images/R_310_jpg.rf.5d46a46992b0bca830a042fdb2c77087.jpg: 640x640 (no detections), 173.7ms image 55/178 /yolov5/data/images/R_311_jpg.rf.f2917e7e573784563349e26180518184.jpg: 640x640 (no detections), 173.7ms image 56/178 /yolov5/data/images/R_323_jpg.rf.42845dc89eb03cf03c8767e41e3d4382.jpg: 640x640 (no detections), 175.8ms image 57/178 /yolov5/data/images/R_329_jpg.rf.12d35a39b88ba667e2ebd94379c739f8.jpg: 640x640 (no detections), 173.8ms image 58/178 /yolov5/data/images/R_352_jpg.rf.63c4dca15e37c5dfa22f1f6ddb46cebf.jpg: 640x640 (no detections), 175.2ms image 59/178 /yolov5/data/images/R_367_jpg.rf.c05e91c176107d403c0f7ad9cf23f824.jpg: 640x640 (no detections), 173.9ms image 60/178 /yolov5/data/images/R_395_jpg.rf.e656ac103475690d6974a5bba841a057.jpg: 640x640 1 person, 173.7ms image 61/178 /yolov5/data/images/R_410_jpg.rf.0feb4478388515af2dabe2a9a6221396.jpg: 640x640 (no detections), 175.2ms image 62/178 /yolov5/data/images/R_427_jpg.rf.1bf37eda5d130c40b9b47cf5f5b58301.jpg: 640x640 (no detections), 173.6ms image 63/178 /yolov5/data/images/R_428_jpg.rf.3c6a8af7853bf350e75718cb23946c39.jpg: 640x640 (no detections), 175.6ms image 64/178 /yolov5/data/images/R_438_jpg.rf.b286f44a546d064b4c393c18911a1fa2.jpg: 640x640 1 car, 173.5ms image 65/178 /yolov5/data/images/R_43_jpg.rf.057e62e740efa15f80c03fb97a76f457.jpg: 640x640 (no detections), 173.8ms image 66/178 /yolov5/data/images/R_466_jpg.rf.7933e3417f286bca774f8fe07edcec83.jpg: 640x640 (no detections), 175.7ms image 67/178 /yolov5/data/images/R_471_jpg.rf.d388e9b38c52938d84854ee9501bd68c.jpg: 640x640 (no detections), 173.8ms image 68/178 /yolov5/data/images/R_483_jpg.rf.50e6af4a98d3d667f37bc84e2f282bcf.jpg: 640x640 (no detections), 176.6ms image 69/178 /yolov5/data/images/R_485_jpg.rf.615c3bf5e7e3ad17dc6794b33ff161ca.jpg: 640x640 1 tv, 173.9ms image 70/178 /yolov5/data/images/R_511_jpg.rf.3826aca002b178ea3daefafe20a8f4b0.jpg: 640x640 (no detections), 173.9ms image 71/178 /yolov5/data/images/R_524_jpg.rf.6b18d068254f168c6fc6387a74c75402.jpg: 640x640 (no detections), 175.5ms image 72/178 /yolov5/data/images/R_535_jpg.rf.588fc1a12f4bbde1d96252ee2fe2b447.jpg: 640x640 (no detections), 173.7ms image 73/178 /yolov5/data/images/R_556_jpg.rf.32f69fdc9064ce3c23acd8983b06775a.jpg: 640x640 (no detections), 175.4ms image 74/178 /yolov5/data/images/R_575_jpg.rf.f7196c7768bb7e99bbde031ab44c943e.jpg: 640x640 (no detections), 174.7ms image 75/178 /yolov5/data/images/R_582_jpg.rf.16cd74748cc94163753c004f9cc17e38.jpg: 640x640 (no detections), 173.9ms image 76/178 /yolov5/data/images/R_597_jpg.rf.b5817236068d00368060dd7986e3f54c.jpg: 640x640 (no detections), 175.1ms image 77/178 /yolov5/data/images/R_617_jpg.rf.ab6e6462532e940c5a87e1e73309fe7b.jpg: 640x640 (no detections), 173.8ms image 78/178 /yolov5/data/images/R_622_jpg.rf.fcfd74bc00783ceeb6fd5596798f890d.jpg: 640x640 (no detections), 175.6ms image 79/178 /yolov5/data/images/R_654_jpg.rf.284fe8e90a07dedf9853c773d8162832.jpg: 640x640 (no detections), 173.8ms image 80/178 /yolov5/data/images/R_65_jpg.rf.337416d200a3651adeae633c1591d642.jpg: 640x640 (no detections), 173.7ms image 81/178 /yolov5/data/images/R_667_jpg.rf.b1837a9e20f53be242cc621e54cec8b0.jpg: 640x640 (no detections), 173.8ms image 82/178 /yolov5/data/images/R_67_jpg.rf.effc9c239592948931fef53b94960380.jpg: 640x640 (no detections), 173.6ms image 83/178 /yolov5/data/images/R_710_jpg.rf.9d803a6c567d964e85703d6d286b49d1.jpg: 640x640 (no detections), 175.4ms image 84/178 /yolov5/data/images/R_718_jpg.rf.610b46bbb56b7ecfbd85344078e6581c.jpg: 640x640 (no detections), 173.7ms image 85/178 /yolov5/data/images/R_719_jpg.rf.93808980de8e9f58369850821888e663.jpg: 640x640 (no detections), 173.7ms image 86/178 /yolov5/data/images/R_73_jpg.rf.ec8df56ba9aa3c5d32f67439c11def63.jpg: 640x640 (no detections), 173.9ms image 87/178 /yolov5/data/images/R_744_jpg.rf.701ad842a87d677b1236b1f417faf91f.jpg: 640x640 1 traffic light, 175.3ms image 88/178 /yolov5/data/images/R_760_jpg.rf.4879154f1d7fa0a8c9895272e82f95cc.jpg: 640x640 (no detections), 175.0ms image 89/178 /yolov5/data/images/R_789_jpg.rf.2c7e63e9182b27aa7f95150b3b66c8a0.jpg: 640x640 (no detections), 174.1ms image 90/178 /yolov5/data/images/R_796_jpg.rf.81dd7f247104c3f682d96b58582e3b2d.jpg: 640x640 (no detections), 174.2ms image 91/178 /yolov5/data/images/R_800_jpg.rf.9a599c861ee9b7a53af5ce4193377510.jpg: 640x640 (no detections), 173.7ms image 92/178 /yolov5/data/images/R_817_jpg.rf.f6df4fb60fd3717fcfb693f1d649b162.jpg: 640x640 (no detections), 175.3ms image 93/178 /yolov5/data/images/R_822_jpg.rf.c2d123175a8ea14215f02c9a1c447885.jpg: 640x640 (no detections), 173.9ms image 94/178 /yolov5/data/images/R_824_jpg.rf.13ca71bb6588f11a32f8186fead3eefc.jpg: 640x640 (no detections), 174.3ms image 95/178 /yolov5/data/images/R_828_jpg.rf.e611245ac1c4a0b2916508558938c8f4.jpg: 640x640 (no detections), 173.7ms image 96/178 /yolov5/data/images/R_836_jpg.rf.89de262510723ccbe175b790cb357c75.jpg: 640x640 (no detections), 173.8ms image 97/178 /yolov5/data/images/R_844_jpg.rf.32205fbe01479b9c76b6852672f9d255.jpg: 640x640 1 traffic light, 175.5ms image 98/178 /yolov5/data/images/R_850_jpg.rf.9bcdca851abd44dc5e973100fc6a75aa.jpg: 640x640 (no detections), 173.8ms image 99/178 /yolov5/data/images/a154e31fe6_jpg.rf.5b66585559baa1322d4b5cc71e001c0e.jpg: 640x640 (no detections), 173.7ms image 100/178 /yolov5/data/images/a4e50ecbed_jpg.rf.dde1bc6908d4c2d1471c080508d7ee6f.jpg: 640x640 (no detections), 173.8ms image 101/178 /yolov5/data/images/acca057365_jpg.rf.8ecbe957192bafc13d400d97e466a791.jpg: 640x640 (no detections), 173.9ms image 102/178 /yolov5/data/images/ba16ec227a_jpg.rf.d16312e19f402e73b6b9180b51d922b3.jpg: 640x640 (no detections), 175.5ms image 103/178 /yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, 135.9ms image 104/178 /yolov5/data/images/cd4b64e1fb_jpg.rf.b51b79f65aa4d912966e0964e82ce92c.jpg: 640x640 1 orange, 173.8ms image 105/178 /yolov5/data/images/eff91468db_jpg.rf.828693b562d8590d4d58fa655d2394fe.jpg: 640x640 (no detections), 173.6ms image 106/178 /yolov5/data/images/scene00019_png.rf.e0766b2d34d4a6f948615c3ac6963265.jpg: 640x640 1 person, 8 cars, 3 buss, 173.9ms image 107/178 /yolov5/data/images/scene09495_png.rf.ab4e13fa24076feeee2d34f48517e0d3.jpg: 640x640 2 persons, 5 cars, 1 truck, 173.7ms image 108/178 /yolov5/data/images/scene09496_png.rf.1b02bc511e308bbee1db664fa9a16ace.jpg: 640x640 1 person, 5 cars, 1 truck, 175.4ms image 109/178 /yolov5/data/images/scene09498_png.rf.5036160da3c11dbad737e770d1d3e602.jpg: 640x640 2 persons, 5 cars, 1 truck, 173.5ms image 110/178 /yolov5/data/images/scene09500_png.rf.8404d4fbbded2342ad7ba0d53773ebf2.jpg: 640x640 2 persons, 5 cars, 1 truck, 174.6ms image 111/178 /yolov5/data/images/scene09509_png.rf.b8263f519d2f424fcf6e59b41bb84f1f.jpg: 640x640 2 persons, 6 cars, 1 truck, 175.5ms image 112/178 /yolov5/data/images/scene09515_png.rf.dda67a9cea5490d5577ea1e3d3979d90.jpg: 640x640 3 persons, 5 cars, 1 umbrella, 173.8ms image 113/178 /yolov5/data/images/scene09516_png.rf.a386ffdd24fb4efd118a4aa1cad8757d.jpg: 640x640 6 cars, 1 umbrella, 175.3ms image 114/178 /yolov5/data/images/scene09518_png.rf.0f3c4c99ef3eda67ce2ff87fe4717409.jpg: 640x640 2 persons, 5 cars, 173.7ms image 115/178 /yolov5/data/images/scene09520_png.rf.782b4dd7aa3fad0595f19aa9da5bb20d.jpg: 640x640 5 cars, 1 truck, 1 umbrella, 173.7ms image 116/178 /yolov5/data/images/scene09522_png.rf.28f1821e09927a64004ddbc374f9823f.jpg: 640x640 1 person, 4 cars, 1 truck, 1 umbrella, 175.8ms image 117/178 /yolov5/data/images/scene09530_png.rf.d559251926cfcd89f9d211a8beb4f7ab.jpg: 640x640 2 persons, 3 cars, 1 truck, 173.8ms image 118/178 /yolov5/data/images/scene09531_png.rf.09abc26f32b2fc3de830bd7f239161e6.jpg: 640x640 2 persons, 3 cars, 1 truck, 173.8ms image 119/178 /yolov5/data/images/scene09532_png.rf.b0197674f666401c30a44726bcde8485.jpg: 640x640 1 person, 3 cars, 173.6ms image 120/178 /yolov5/data/images/scene09535_png.rf.de8d53cb963409a74896594c03020c2a.jpg: 640x640 2 persons, 5 cars, 174.0ms image 121/178 /yolov5/data/images/scene09536_png.rf.e910e7bf6f31c3a14e96782d28c6b860.jpg: 640x640 1 person, 3 cars, 1 airplane, 1 truck, 1 umbrella, 175.2ms image 122/178 /yolov5/data/images/scene09541_png.rf.7afde91d0f691a37ef072933bc498c4e.jpg: 640x640 1 person, 5 cars, 173.9ms image 123/178 /yolov5/data/images/scene09546_png.rf.832211b3c2645e95f2b9f81adc1e1cb4.jpg: 640x640 1 person, 3 cars, 1 truck, 173.9ms image 124/178 /yolov5/data/images/scene09547_png.rf.e0f40ab93a68a1a6bfca5043d0580ee0.jpg: 640x640 1 person, 3 cars, 1 truck, 173.7ms image 125/178 /yolov5/data/images/scene09556_png.rf.0835a0fc45c315e769adb382dfafe754.jpg: 640x640 1 person, 3 cars, 1 truck, 173.7ms image 126/178 /yolov5/data/images/scene09567_png.rf.621dbc0d981a64c5d1fbe7c8d1c76c23.jpg: 640x640 1 person, 1 car, 175.7ms image 127/178 /yolov5/data/images/scene09570_png.rf.1fdd0d8bf5a4241588bf092050d72973.jpg: 640x640 1 person, 2 cars, 174.3ms image 128/178 /yolov5/data/images/scene09572_png.rf.d33accd317e6a07f74dfc0c312c567cf.jpg: 640x640 1 person, 3 cars, 173.8ms image 129/178 /yolov5/data/images/scene09579_png.rf.c4458a73875a0c182fa962993361eb82.jpg: 640x640 1 person, 2 cars, 173.7ms image 130/178 /yolov5/data/images/scene09582_png.rf.7d2065bc8b1d2594c749ffa41d29d53c.jpg: 640x640 3 cars, 1 motorcycle, 173.7ms image 131/178 /yolov5/data/images/scene09583_png.rf.6e5a4cc3c344704b66a0da3ca2c6cc0c.jpg: 640x640 3 cars, 175.2ms image 132/178 /yolov5/data/images/scene09589_png.rf.ae8911a21a097556b71f9de5b4c8f107.jpg: 640x640 3 cars, 174.3ms image 133/178 /yolov5/data/images/scene09593_png.rf.7dcf4ed96862e0023e587d01dee5a334.jpg: 640x640 2 cars, 1 truck, 173.8ms image 134/178 /yolov5/data/images/scene09594_png.rf.4c2638da794b61ef172bf9ee718a9d48.jpg: 640x640 2 cars, 1 truck, 174.0ms image 135/178 /yolov5/data/images/scene09595_png.rf.f2ba73226ef1eb918eb6ed38079d90ea.jpg: 640x640 1 person, 3 cars, 1 truck, 173.7ms image 136/178 /yolov5/data/images/scene09598_png.rf.3d6ec787051a3895d2cc99b96300c662.jpg: 640x640 1 person, 2 cars, 1 truck, 175.3ms image 137/178 /yolov5/data/images/scene09600_png.rf.7a705dc70e42ca8f266b31d23375842f.jpg: 640x640 1 person, 4 cars, 1 truck, 174.8ms image 138/178 /yolov5/data/images/scene09609_png.rf.5be75c03fb0e183d73949eaee7aea74b.jpg: 640x640 1 person, 4 cars, 1 backpack, 173.9ms image 139/178 /yolov5/data/images/scene09610_png.rf.ffe992e5790258415830acdbc2f640c5.jpg: 640x640 2 persons, 4 cars, 1 truck, 173.7ms image 140/178 /yolov5/data/images/scene09612_png.rf.70fe9178c6e70ffb024da4a986579f22.jpg: 640x640 2 persons, 4 cars, 173.8ms image 141/178 /yolov5/data/images/scene09615_png.rf.dd434d830e71f9c35f496403120be549.jpg: 640x640 1 person, 2 cars, 1 truck, 1 traffic light, 174.1ms image 142/178 /yolov5/data/images/scene09618_png.rf.cef06e0e8001b3403ce533aa52f04c7e.jpg: 640x640 2 persons, 3 cars, 175.4ms image 143/178 /yolov5/data/images/scene09619_png.rf.ee8cda26e47b4e3fddc4702372ccfc99.jpg: 640x640 2 persons, 2 cars, 173.7ms image 144/178 /yolov5/data/images/scene09620_png.rf.8142b54d8c79ae256c91954d6429b21d.jpg: 640x640 1 person, 3 cars, 173.8ms image 145/178 /yolov5/data/images/scene09621_png.rf.bcffe3c53f7af2c8eed1921d97667437.jpg: 640x640 2 persons, 3 cars, 1 traffic light, 173.7ms image 146/178 /yolov5/data/images/scene09624_png.rf.78371bf219b7e0d391cfd615277a73c6.jpg: 640x640 2 cars, 1 traffic light, 173.8ms image 147/178 /yolov5/data/images/scene09627_png.rf.0c98638bee2e1ca83fe798704afa3258.jpg: 640x640 1 person, 4 cars, 175.6ms image 148/178 /yolov5/data/images/scene09628_png.rf.7ed1777045097c82d4556b1181e0eca9.jpg: 640x640 1 person, 3 cars, 1 traffic light, 173.8ms image 149/178 /yolov5/data/images/scene09639_png.rf.2b4f6aae5ad401e3f55bb573df415103.jpg: 640x640 2 persons, 4 cars, 173.7ms image 150/178 /yolov5/data/images/scene09642_png.rf.318f2a88a4ac410dfe591d5c514f25d1.jpg: 640x640 2 persons, 3 cars, 173.9ms image 151/178 /yolov5/data/images/scene09647_png.rf.54795643766df49f93b426f0305c5a0d.jpg: 640x640 2 persons, 2 cars, 174.5ms image 152/178 /yolov5/data/images/scene09654_png.rf.3648fca642e9404178fe30b6ae084f57.jpg: 640x640 1 person, 3 cars, 175.6ms image 153/178 /yolov5/data/images/scene09657_png.rf.1ff6a9b227119639d8992e1d5fa8eb40.jpg: 640x640 1 person, 4 cars, 173.7ms image 154/178 /yolov5/data/images/scene09658_png.rf.cf55d26e754abb729f7d656fe913e622.jpg: 640x640 1 person, 2 cars, 1 truck, 173.9ms image 155/178 /yolov5/data/images/scene09660_png.rf.67b091f2b4bcf9e622c38d6c087d5da8.jpg: 640x640 1 person, 4 cars, 176.4ms image 156/178 /yolov5/data/images/scene09664_png.rf.97685d2c31d78f8b6a10a79abdafeae1.jpg: 640x640 1 person, 4 cars, 173.6ms image 157/178 /yolov5/data/images/scene09666_png.rf.8a4eca9347354c503f5e3ef3b26ad122.jpg: 640x640 2 persons, 3 cars, 175.5ms image 158/178 /yolov5/data/images/scene09668_png.rf.5574ec124c328b1cd5892ea9477d6ff2.jpg: 640x640 4 cars, 173.9ms image 159/178 /yolov5/data/images/scene09669_png.rf.8e3d904d8bfbf01da76d5de9162955f1.jpg: 640x640 3 cars, 174.2ms image 160/178 /yolov5/data/images/scene09676_png.rf.686d9bad5be73513e12c097ec13073c8.jpg: 640x640 4 cars, 1 truck, 175.1ms image 161/178 /yolov5/data/images/scene09677_png.rf.06af75dd3863d08479a33c29b2080db6.jpg: 640x640 4 cars, 1 truck, 173.6ms image 162/178 /yolov5/data/images/scene09679_png.rf.6ff89cdcaffd78fff200ec571aae4b10.jpg: 640x640 3 cars, 175.3ms image 163/178 /yolov5/data/images/scene09681_png.rf.6c080a847fd627540ff23b9d430fed6d.jpg: 640x640 3 cars, 173.7ms image 164/178 /yolov5/data/images/scene09683_png.rf.5166c1a4004d5b9b0879cf398d9f86e2.jpg: 640x640 3 cars, 1 suitcase, 173.7ms image 165/178 /yolov5/data/images/scene09688_png.rf.d6f641ea03ab54054c3004bf0dce6e97.jpg: 640x640 4 cars, 1 traffic light, 175.5ms image 166/178 /yolov5/data/images/scene09691_png.rf.3b4c0dd805c9913e2a4e3f22fd2874cc.jpg: 640x640 4 cars, 173.9ms image 167/178 /yolov5/data/images/scene09696_png.rf.1b365e59f89261a366fbed11bb5c1770.jpg: 640x640 4 cars, 175.2ms image 168/178 /yolov5/data/images/scene09697_png.rf.d97ff2fce538d3070cc8f75d3c6e655d.jpg: 640x640 4 cars, 173.9ms image 169/178 /yolov5/data/images/scene09700_png.rf.e4e061b52d54e8f3733e576b1dc80f8a.jpg: 640x640 4 cars, 173.9ms image 170/178 /yolov5/data/images/scene09702_png.rf.87c99933afaa2329aaf72a2b7599635c.jpg: 640x640 4 cars, 175.4ms image 171/178 /yolov5/data/images/scene09703_png.rf.67a0b2d23ccefb0e7649ddcb87ff0001.jpg: 640x640 4 cars, 174.1ms image 172/178 /yolov5/data/images/scene09705_png.rf.9bd3ead27268a7db05d3e4d26f4fe341.jpg: 640x640 3 cars, 173.9ms image 173/178 /yolov5/data/images/scene09706_png.rf.10a428ee4340eada6875ec210a250015.jpg: 640x640 1 person, 3 cars, 173.6ms image 174/178 /yolov5/data/images/scene09715_png.rf.3cbd522a533436d41a62a0411b7ee820.jpg: 640x640 1 person, 3 cars, 173.7ms image 175/178 /yolov5/data/images/scene09718_png.rf.d870455f822012d818785090941c77d7.jpg: 640x640 1 person, 3 cars, 175.2ms image 176/178 /yolov5/data/images/scene15712_png.rf.fe211cdb63d359cdebc7dea53ee475d4.jpg: 640x640 12 cars, 1 bus, 1 truck, 1 parking meter, 173.6ms image 177/178 /yolov5/data/images/scene15715_png.rf.15fbe331f13ea8b104f3a699847c94dd.jpg: 640x640 10 cars, 2 buss, 173.8ms image 178/178 /yolov5/data/images/zidane.jpg: 384x640 2 persons, 1 tie, 104.7ms Speed: 3.1ms pre-process, 173.7ms inference, 10.6ms NMS per image at shape (1, 3, 640, 640) It is using CUDA, not TensorRT yet, but from this (reference from seedstudio) I expect an average of 50 ms per inference and my result has a poor performance of 170ms I have tried the same libraries without docker. Similar results: MAXN - 130ms 5W - 180ms What could be the cause of the poor performance? This is from the reference...",
        "answers": [
            [
                "The clue was to use a USB camera 640x480, this way i'm getting 0.060s average with model yolov5n. The delay was caused by the file system retrieving images from disk. I had to merge instructions from https://wiki.seeedstudio.com/YOLOv5-Object-Detection-Jetson/#inference-on-jetson-device and https://www.forecr.io/blogs/ai-algorithms/how-to-run-yolov5-real-time-object-detection-on-pytorch-with-docker-on-nvidia-jetson-modules"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "My goal is to restart the Argus Camera API or Gstreamer pipeline restarting the system. This is because sometimes the C++ app crash and will not restart. The only known solution now is to restart the system. My expected result is that I can restart the C++ app without the need to restart the system. My actual result is that I need to restart the system. (Argus) Error EndOfFile: Unexpected error in reading socket (in src/rpc/socket/client/ClientSocketManager.cpp, function recvThreadCore(), line 266) (Argus) Error EndOfFile: Receive worker failure, notifying 1 waiting threads (in src/rpc/socket/client/ClientSocketManager.cpp, function recvThreadCore(), line 340) (Argus) Error InvalidState: Argus client is exiting with 1 outstanding client threads (in src/rpc/socket/client/ClientSocketManager.cpp, function recvThreadCore(), line 357) (Argus) Error EndOfFile: Receiving thread terminated with error (in src/rpc/socket/client/ClientSocketManager.cpp, function recvThreadWrapper(), line 368) (Argus) Error EndOfFile: Client thread received an error from socket (in src/rpc/socket/client/ClientSocketManager.cpp, function send(), line 145) (Argus) Error EndOfFile: (propagating from src/rpc/socket/client/SocketClientDispatch.cpp, function dispatch(), line 91) Error generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst-nvarguscamera/gstnvarguscamerasrc.cpp, execute:791 Failed to create OutputStream (Argus) Error InvalidState: Receive thread is not running cannot send. (in src/rpc/socket/client/ClientSocketManager.cpp, function send(), line 96) (Argus) Error InvalidState: (propagating from src/rpc/socket/client/SocketClientDispatch.cpp, function dispatch(), line 91) (Argus) Error InvalidState: Receive thread is not running cannot send. (in src/rpc/socket/client/ClientSocketManager.cpp, function send(), line 96) (Argus) Error InvalidState: (propagating from src/rpc/socket/client/SocketClientDispatch.cpp, function dispatch(), line 91) (Argus) Error InvalidState: Receive thread is not running cannot send. (in src/rpc/socket/client/ClientSocketManager.cpp, function send(), line 96) (Argus) Error InvalidState: (propagating from src/rpc/socket/client/SocketClientDispatch.cpp, function dispatch(), line 91) nvbuf_utils: Could not get EGL display connection GST_ARGUS: Creating output stream [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (933) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1 (Argus) Error InvalidState: Receive thread is not running cannot send. (in src/rpc/socket/client/ClientSocketManager.cpp, function send(), line 96) (Argus) Error InvalidState: (propagating from src/rpc/socket/client/SocketClientDispatch.cpp, function dispatch(), line 91) Error generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst-nvarguscamera/gstnvarguscamerasrc.cpp, execute:751 Failed to create CaptureSession [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (933) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1 Start grabbing Press q to terminate Press v for verbose Press p to predict ERROR! blank frame grabbed (Argus) Error InvalidState: Receive thread is not running cannot send. (in src/rpc/socket/client/ClientSocketManager.cpp, function send(), line 96) (Argus) Error InvalidState: (propagating from src/rpc/socket/client/SocketClientDispatch.cpp, function dispatch(), line 91) How to open camera. JetsonCamera::JetsonCamera(const std::string sensorID, const std::string captureWidth, const std::string captureHeight, const std::string frameRate, const std::string displayWidth, const std::string displayHeight) : cap_(buildGStreamerPipeline(sensorID, captureWidth, captureHeight, frameRate, displayWidth, displayHeight), cv::CAP_GSTREAMER) { } std::string JetsonCamera::buildGStreamerPipeline( const std::string sensorID, const std::string captureWidth, const std::string captureHeight, const std::string frameRate, const std::string displayWidth, const std::string displayHeight) { std::stringstream pipeline; pipeline &lt;&lt; \"nvarguscamerasrc sensor-id=\" &lt;&lt; sensorID &lt;&lt; \" ! \" &lt;&lt; \"video/x-raw(memory:NVMM),width=(int)\" &lt;&lt; captureWidth &lt;&lt; \",height=(int)\" &lt;&lt; captureHeight &lt;&lt; \",format=(string)NV12,framerate=(fraction)\" &lt;&lt; frameRate &lt;&lt; \"/1 ! \" &lt;&lt; \"nvvidconv flip-method=0 ! \" &lt;&lt; \"video/x-raw,width=(int)\" &lt;&lt; displayWidth &lt;&lt; \",height=(int)\" &lt;&lt; displayHeight &lt;&lt; \",format=(string)BGRx ! \" &lt;&lt; \"videoconvert ! \" &lt;&lt; \"video/x-raw,format=(string)BGR ! \" &lt;&lt; \"appsink\"; return pipeline.str(); }",
        "answers": [
            [
                "You would just restart nvargus-daemon service: sudo systemctl restart nvargus-daemon.service Note that it may also help to set appsink property drop to true, it may better behave when opencv capture terminates: &lt;&lt; \"appsink drop=1\";"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "My goal is to use gstreamer pipeline instead of libcamera-still. The problem is that the frames generated by the gstreamer pipeline look concave. Gstreamer Pipeline def gstreamer_pipeline( # Issue: the sensor format used by Raspberry Pi 4B and NVIDIA Jetson Nano B01 are different # in Raspberry Pi 4B, this command # $ libcamera-still --width 1280 --height 1280 --mode 1280:1280 # uses sensor format 2328x1748. # However, v4l2-ctl --list-formats-ext do not have such format. sensor_id=0, capture_width=1920, capture_height=1080, display_width=640, display_height=360, framerate=21, flip_method=0, ): return ( \"nvarguscamerasrc sensor-id=%d ! \" \"video/x-raw(memory:NVMM),format=(string)NV12,framerate=(fraction)%d/1 ! \" \"nvvidconv flip-method=%d ! \" \"video/x-raw,width=(int)%d,height=(int)%d,format=(string)BGRx ! \" \"videoconvert ! \" \"video/x-raw,format=(string)BGR ! \" \"appsink\" % ( sensor_id, framerate, flip_method, display_width, display_height ) ) The result with gstreamer pipeline The code I run to take frame libcamera-still -t 5000 --width 1280 --height 1280 --mode 1280:1280 --autofocus-on-capture -o test.jpg The result with libcamera-still",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm in the process of calibrating the camera, and for that I'm using the python language together with the open cv library. I'm using the Waveshare IMX219 camera on the Jetson Nano. I tried to capture images with the cameras in order to calibrate them using the \"VideoCapture\" function, passing the index of camera 0 as a parameter. And that's when the following problem appears: [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (1757) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module v4l2src0 reported: Internal data stream error. [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (886) open OpenCV | GStreamer warning: unable to start pipeline [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created The camera is correctly connected and is being recognized by the device.",
        "answers": [
            [
                "Passing index 0 would use either V4L backend, or v4l2src plugin for gstreamer backend (your case is the latter). The problem is that IMX219 is a bayer RG10 sensor, its raw video is not suitable for opencv that expects BGR format for most algorithms (other formats are available, though, depending on your opencv version). With a Jetson the path would be using Argus that would debayer, auto-tune gains, exposure, wb, ... with ISP and provide NV12 format frames into NVMM memory where it can be handled in gstreamer thanks to plugin nvarguscamerasrc. For providing BGR frames to opencv application, you may first use Jetson's VIC HW for converting into BGRx format with nvvidconv outputting into system memory, and then use videoconvert for BGR. So the pipeline would be: cam_pipeline_str = 'nvarguscamerasrc sensor-id=0 ! video/x-raw(memory:NVMM),format=NV12,width=1280,height=720,framerate=30/1 ! nvvidconv ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1' cap = cv2.VideoCapture(cam_pipeline_str, cv2.CAP_GSTREAMER) if not cap.isOpened(): ...Error # if you get here loop reading frames and do what you want with these... Also note that default support of some Jetsons L4T versions as device-tree/drivers would be for RPi v2 IMX219 cam. I have no experience with Waveshare cams, but vendors should provide a SDK for their products. Check with your camera vendor for your L4T release."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm relativelly new to this. I managed to flash a Jetson-TX2 + dev board with success using yocto meta-tegra etc... However, you can see below, It doesn't fully boot, the bar is never moving after that, I can't login nor do anything. Same happens when I enter via Serial, I see it booting but I can't login when it asks me to. Any ideas? This is my setup: After 20 minutes it moved. And booted. I've restarted it, and it took another 20 minutes. Any ideas on this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I was downloading deepstream SDK 6.0 to my jetson nano with nvidia's quick start instructions. I run this command deepstream-app -c &lt;path_to_config_file&gt; while I m inside of /opt/nvidia/deepstream/deepstream-6.0 directory. ERROR: &lt;parse_config_file:536&gt;: parse_config_file failed ** ERROR: &lt;main:630&gt;: Failed to parse config file 'source30_1080p_dec_infer-resnet_tiled_display_int8.txt' Quitting ** I tried to change config file and run by writing with path of config file but it did not work",
        "answers": [],
        "votes": []
    },
    {
        "question": "My goal is to run DeepStream SDK 6.2 (or older version). The official SD Card Image is JP461 (I assume it's JetPack 4.6.1). However, DeepStream SDK 6.2 require JetPack 5.1. The problem is JetPack 5.1 does not mention Nano at all and I can't find older version of DeepStream SDK.",
        "answers": [
            [
                "You will not be able to see the archived versions of Deepstream without signing in. You can find the download links here Note that 6.0.1 is the last release for Jetson Nano. Incase you need docker images, you can directly docker pull on the Jetson (assuming you have Jetpack 4.6.1 installed) docker pull nvcr.io/nvidia/deepstream-l4t:6.0.1-base (You can find all the images here)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "My goal is to get 1280x1280 frame from nvarguscamerasrc. The problem is that nvarguscamerasrc scaled the 3840x2160 frame to 1280x720. The consequence is that the bottom of the frame is always black. JetsonCamera.py def gstreamer_pipeline( # Issue: the sensor format used by Raspberry Pi 4B and NVIDIA Jetson Nano B01 are different # in Raspberry Pi 4B, this command # $ libcamera-still --width 1280 --height 1280 --mode 1280:1280 # uses sensor format 2328x1748. # However, v4l2-ctl --list-formats-ext do not have such format. capture_width=1920, capture_height=1080, display_width=640, display_height=360, framerate=21, flip_method=0, ): return ( \"nvarguscamerasrc ! \" \"video/x-raw(memory:NVMM), \" \"width=(int)%d, height=(int)%d, \" \"format=(string)NV12, framerate=(fraction)%d/1 ! \" \"nvvidconv flip-method=%d ! \" \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \" \"videoconvert ! \" \"video/x-raw, format=(string)BGR ! appsink\" % ( capture_width, capture_height, framerate, flip_method, display_width, display_height, ) ) class Camera(object): frame_reader = None cap = None previewer = None def __init__(self, width=640, height=360): self.open_camera(width, height) def open_camera(self, width=640, height=360): self.cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=0, display_width=width, display_height=height), cv2.CAP_GSTREAMER) if not self.cap.isOpened(): raise RuntimeError(\"Failed to open camera!\") if self.frame_reader == None: self.frame_reader = FrameReader(self.cap, \"\") self.frame_reader.daemon = True self.frame_reader.start() def getFrame(self, timeout = None): return self.frame_reader.getFrame(timeout) class FrameReader(threading.Thread): queues = [] _running = True camera = None def __init__(self, camera, name): threading.Thread.__init__(self) self.name = name self.camera = camera def run(self): while self._running: _, frame = self.camera.read() while self.queues: queue = self.queues.pop() queue.put(frame) def addQueue(self, queue): self.queues.append(queue) def getFrame(self, timeout = None): queue = Queue(1) self.addQueue(queue) return queue.get(timeout = timeout) def stop(self): self._running = False main.py exit_ = False if __name__ == \"__main__\": camera = Camera(width=1280, height=1280) while not exit_: global frame frame = camera.getFrame(2000) cv2.imshow(\"Test\", frame) key = cv2.waitKey(1) if key == ord('q'): exit_ = True",
        "answers": [],
        "votes": []
    },
    {
        "question": "How can I store my credentials securely on Ubuntu 20.04? I need those credentials to call up my API from the device or access some of my encrypted files which are on my device. If anyone get hands on my device, then they shouldn't be able to access my stored credentials. I want something like Windows Credentials Manager. For example: We use the AWS CLI where we can store the credentials, or our git credentials that we store in windows operating system. Device: Jetson (Jetpack 5 - Ubuntu 20.04).",
        "answers": [],
        "votes": []
    },
    {
        "question": "I would like to install pytorch3d on Jetson Nano but it failed. Problem The install command I tried pip install 'git+https://github.com/facebookresearch/pytorch3d.git' and got this error: cub/cub.cuh: No such file or directory Environment nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Sun_Feb_28_22:34:44_PST_2021 Cuda compilation tools, release 10.2, V10.2.300 Build cuda_10.2_r440.TC440_70.29663091_0 cat /etc/nv_tegra_release # R32 (release), REVISION: 7.3, GCID: 31982016, BOARD: t210ref, EABI: aarch64, DATE: Tue Nov 22 17:30:08 UTC 2022 I found cub/cub.cuh at here and download it but I'm not sure how can I include it when the pip install process. https://github.com/NVlabs/cub",
        "answers": [],
        "votes": []
    },
    {
        "question": "NVIDIA is offering pre-build wheels for Pytorch leveraging the GPU under 1. But this does not cover audiotorch. It seems that it is necessary to use a matching audiotorch version, otherwise and the version having the same version number gives: OSError: ../lib/libtorchaudio.so: undefined symbol: _ZNK5torch8autograd4Node4nameEv Where can this pre-built wheel be found? I do not what to use an offered NVIDIA docker image for JetPack where this might work since this makes no sense on a small AI computer.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using a barcode scanner as part of a project, everything works correctly until I exit the program then I can't communicate anymore with the barcode scanner. This holds true for whatever program I'm running, be it one of my own or just using screen to monitor the transmissions. As soon as I exit, the only way to make the scanner work again is to unplug and replug. The scanner (this one) is always mounted correctly (usually at /dev/ttyACM0) and communicates by SSI over USB CDC. I\u2019ve tried monitoring with pyserial\u2019s miniterm and with screen /dev/ttyACM0 9600 but the same problem arises (f.e. screen just says [screen is terminating]) Mind you, everything works well on another computer so I believe it might be an issue with the Jetson rather than the scanner. In the project I\u2019m trying to run, I use pyserial to interact with the device. Here is an extract of the code to give you an idea of how I use it: import serial serial_port = \"/dev/ttyACM0\" baud_rate = 9600 with serial.Serial(serial_port, baud_rate, timeout=0.1) as device_serial: device_serial.flush() while True: try: # read a line from the serial port barcode_byte_string = device_serial.readline() if len(barcode_byte_string) &gt; 0: # convert the byte string to a string and strip the newline character barcode = barcode_byte_string.decode(\"utf-8\").rstrip() # publish the barcode to the topic self.publish_barcode(barcode, serial_port) except serial.SerialException as e: # exit with error code 1. This will cause the application to be restarted. sys.exit(1) except Exception as e: break",
        "answers": [],
        "votes": []
    },
    {
        "question": "If I try to use cuda on the jetson nano in the terminal: $ python3 &gt; &gt; &gt; import torch &gt; &gt; &gt; print(torch.cuda.is_available()) True But if I start a file with the same content the output is False. Does anyone have any idea how to fix this? I've tried: import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" and importing the file in the python terminal. While this makes cuda work, the rest of the application wont work. I hope to see when I start the file with python3 file.py the output True",
        "answers": [
            [
                "It seems that the issue is related to the environment variables not being set correctly when running the file. One solution could be to add the following line to the top of your script to ensure that the environment variable is set before importing torch: import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" This will make sure that CUDA is set to use the first available device (device 0) when the script runs. Another solution could be to use a command like export CUDA_VISIBLE_DEVICES=0 Then run your python script. This should make sure that the environment variable is set correctly when your script runs. Additionally, you could also check if the cuda version you are using is compatible with the version of Pytorch you have installed and also check if your jetson nano has a cuda enabled GPU."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "By building the Yocto image I encountered this issue during populating Yocto SDK. Do somebody have any idea where and how to fix it? But i'm getting the following error: ERROR: python3-3.8.14-r0 do_package: Error executing a python function in exec_func_python() autogenerated: The stack trace of python calls that resulted in this exception/failure was: File: 'exec_func_python() autogenerated', lineno: 2, function: 0001: *** 0002:do_package(d) 0003: File: '/home/vkchlt0732/yocto-tegra5/poky/meta/classes/package.bbclass', lineno: 2306, function: do_package 2302: # Setup PKGD (from D) 2303: ########################################################################### 2304: 2305: for f in (d.getVar('PACKAGEBUILDPKGD') or '').split(): *** 2306: bb.build.exec_func(f, d) 2307: 2308: ########################################################################### 2309: # Split up PKGD into PKGDEST 2310: ########################################################################### File: '/home/vkchlt0732/yocto-tegra5/poky/bitbake/lib/bb/build.py', lineno: 192, function: exec_func 0188: flags = d.getVarFlags(func) 0189: cleandirs = flags.get('cleandirs') if flags else None 0190: if cleandirs: 0191: for cdir in d.expand(cleandirs).split(): *** 0192: bb.utils.remove(cdir, True) 0193: bb.utils.mkdirhier(cdir) 0194: 0195: if flags and dirs is None: 0196: dirs = flags.get('dirs') File: '/home/vkchlt0732/yocto-tegra5/poky/bitbake/lib/bb/utils.py', lineno: 710, function: remove 0706: # shutil.rmtree(name) would be ideal but its too slow 0707: cmd = [] 0708: if ionice: 0709: cmd = ['ionice', '-c', '3'] *** 0710: subprocess.check_call(cmd + ['rm', '-rf'] + glob.glob(path)) 0711: return 0712: for name in glob.glob(path): 0713: try: 0714: os.unlink(name) File: '/usr/lib/python3.8/subprocess.py', lineno: 364, function: check_call 0360: if retcode: 0361: cmd = kwargs.get(\"args\") 0362: if cmd is None: 0363: cmd = popenargs[0] *** 0364: raise CalledProcessError(retcode, cmd) 0365: return 0 0366: 0367: 0368:def check_output(*popenargs, timeout=None, **kwargs): Exception: subprocess.CalledProcessError: Command '['rm', '-rf', '/home/vkchlt0732/yocto-tegra5/build/tmp/work/aarch64-poky-linux/python3/3.8.14-r0/package']' died with &lt;Signals.SIGABRT: 6&gt;. ERROR: Logfile of failure stored in: /home/vkchlt0732/yocto-tegra5/build/tmp/work/aarch64-poky-linux/python3/3.8.14-r0/temp/log.do_package.282323 ERROR: Task (/home/vkchlt0732/yocto-tegra5/poky/meta/recipes-devtools/python/python3_3.8.14.bb:do_package) failed with exit code '1' NOTE: Tasks Summary: Attempted 4482 tasks of which 4473 didn't need to be rerun and 1 failed. Summary: 1 task failed: /home/vkchlt0732/yocto-tegra5/poky/meta/recipes-devtools/python/python3_3.8.14.bb:do_package Summary: There was 1 ERROR message shown, returning a non-zero exit code. Please let me know any solution for this error",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am calculating the power consumption of several neural networks in a for loop, hence in order to get the accurate power consumption I need to reset the model after each iteration of the loop. could you please let me know whether there is a solution in Python or Tensorflow to start the training process over again somehow the model will be reloaded like the first iteration of the for-loop.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I tried around 8 hours to install pip3- python in ubintu but its not working showing: Traceback (most recent call last): File \"/usr/bin/pip3\", line 9, in from pip import main File \"/usr/lib/python3/dist-packages/pip/init.py\", line 29, in from pip.utils import get_installed_distributions, get_prog File \"/usr/lib/python3/dist-packages/pip/utils/init.py\", line 23, in from pip.locations import ( File \"/usr/lib/python3/dist-packages/pip/locations.py\", line 9, in from distutils import sysconfig ImportError: cannot import name 'sysconfig' from 'distutils' (/usr/lib/python3.9/distutils/init.py) i did sudo apt-get update sudo apt-get -y install python3-pip pip3 --version after this step the error message in the above shown",
        "answers": [],
        "votes": []
    },
    {
        "question": "As per the https://developer.nvidia.com/embedded/jetson-modules,the Jetson Xavier NX supports 21 TOPS in AI Performance For example if the use case is people detection, tracking and counting, how many TOPS will be utilized? Is the calculation based on any AI model Architecture? if so , please share the calculation steps.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using PyTorch on NVIDIA Jetson TX2 (GPU and CPU have shared memory), and have only about 2 Gb of free memory. Specifications: PyTorch v1.10.0 JetPack 4.6.1 CUDA 10.2 Whenever I try to use GPU, \"torch.cuda.init()\" would consume about 2Gb of memory. If I use only the CPU, the memory overhead would be only 180 Mb of memory. I have been searching the net and found that the reason is a load of kernels. I understand that a lot of kernels are used for optimal computing, however, I can not even use my GPU due to it. Also, I think that the overwhelming majority of kernels would be never used by me. Could you please say how to reduce the memory overhead if I am willing to sacrifice the performance as long as I could use GPU? Would it be possible to reduce it to the CUDA bare minimum? If it is possible, how to accomplish it? I am trying to train the relatively small model with Conv2D, Pooling, and Dense layers. At least how to reduce the memory overhead, so I could train simple models with only Dense layers? I also can not switch to other libraries due to other reasons.",
        "answers": [],
        "votes": []
    },
    {
        "question": "When building a custom rootfs for NVIDIA Jetson devices, there are a few references online to touching the file /opt/nvidia/l4t-packages/.nv-l4t-disable-boot-fw-update-in-preinstall, but this file's purpose is not documented anywhere.",
        "answers": [
            [
                "I looked through the nvidia-l4t-* Debian packages included with the Linux for Tegra r32.7.2 board support package. Some of these packages include pre- and post-install scripts that reference the file: nvidia-l4t-init creates files like /etc/fstab if the file does not exist. nvidia-l4t-initrd skips flashing the new initrd if the file does exist. nvidia-l4t-core does compatibility checks against /proc if the file does not exist. nvidia-l4t-xusb skips flashing if the file does exist. nvidia-l4t-apt-source inserts the correct SOC family name into /etc/apt/sources.list.d/nvidia-l4t-apt-source.list when the file does not exist. In summary, the file mostly indicates whether you are installing a package on a live system or building a new rootfs where the host system is not the target Jetson device."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Using an nvidia jetson tx2 running ubuntu18.04 with docker,nvidia-docker2 and l4t-cuda installed on the host system. Main Error when compiling: CMake Error at /usr/local/lib/python3.6/dist-packages/cmake/data/share/cmake-3.25/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find CUDA (missing: CUDA_CUDART_LIBRARY) (found suitable version \"10.2\", minimum required is \"10.2\") Call Stack (most recent call first): /usr/local/lib/python3.6/dist-packages/cmake/data/share/cmake-3.25/Modules/FindPackageHandleStandardArgs.cmake:600 (_FPHSA_FAILURE_MESSAGE) /usr/local/lib/python3.6/dist-packages/cmake/data/share/cmake-3.25/Modules/FindCUDA.cmake:1266 (find_package_handle_standard_args) CMakeLists.txt:17 (find_package) CMakeLists.txt: cmake_minimum_required (VERSION 3.5) project(vision) enable_testing() # Variables scopes follow standard rules # Variables defined here will carry over to its children, ergo subdirectories # Setup ZED libs find_package(ZED 3 REQUIRED) include_directories(${ZED_INCLUDE_DIRS}) link_directories(${ZED_LIBRARY_DIR}) # Setup CUDA libs for zed and ai modules find_package(CUDA ${ZED_CUDA_VERSION} REQUIRED) include_directories(${CUDA_INCLUDE_DIRS}) link_directories(${CUDA_LIBRARY_DIRS}) # Setup OpenCV libs find_package(OpenCV REQUIRED) include_directories(${OpenCV_INLCUDE_DIRS}) # Check if OpenMP is installed find_package(OpenMP) checkPackage(\"OpenMP\" \"OpenMP not found, please install it to improve performances: 'sudo apt install libomp-dev'\") # TensorRT set(TENSORRT_ROOT /usr/src/tensorrt/) find_path(TENSORRT_INCLUDE_DIR NvInfer.h HINTS ${TENSORRT_ROOT} PATH_SUFFIXES include/) message(STATUS \"Found TensorRT headers at ${TENSORRT_INCLUDE_DIR}\") set(MODEL_INCLUDE ../code/includes) set(MODEL_LIB_DIR libs) set(YAML_INCLUDE ../depends/yaml-cpp/include) set(YAML_LIB_DIR ../depends/yaml-cpp/libs) include_directories(${MODEL_INCLUDE} ${YAML_INCLUDE}) link_directories(${MODEL_LIB_DIR} ${YAML_LIB_DIR}) # Setup Darknet libs #find_library(DARKNET_LIBRARY NAMES dark libdark.so libdarknet.so) #find_package(dark REQUIRED) # Setup HTTP libs find_package(httplib REQUIRED) find_package(nlohmann_json 3.2.0 REQUIRED) # System libs SET(SPECIAL_OS_LIBS \"pthread\") link_libraries(stdc++fs) # Optional definitions add_definitions(-std=c++17 -g -O3) # Add sub directories add_subdirectory(zed_module) add_subdirectory(ai_module) add_subdirectory(http_api_module) add_subdirectory(executable_module) option(RUN_TESTS \"Build the tests\" off) if (RUN_TESTS OR CMAKE_BUILD_TYPE MATCHES Debug) add_subdirectory(test) steps that fail in Dockerfile using image stereolabs/zed:3.7-devel-jetson-jp4.6: WORKDIR /opt RUN git clone https://github.com/Cruiz102/Vision-Module WORKDIR /opt/Vision-Module RUN mkdir build-debug &amp;&amp; cd build-debug RUN pwd WORKDIR /opt/Vision-Module/build-debug RUN cmake -DCMAKE_BUILD_TYPE=Release .. contents of /etc/docker/daemon.json { \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } }, \"default-runtime\": \"nvidia\" } Using the jetson, I've tried using flags to set the toolkit dir as well as editing daemon.json reinstalling dependencies, changing docker images, installing and reinstalling cudaart on host, changing flags and finishing the build in interactive mode However I always get the same error.",
        "answers": [
            [
                "I have looked into Docker some time ago, so I'm not an expert, put as far as I remember, Docker and Docker containers are like virtual machines. It doesn't matter whether your pc has any cuda support or whether the libraries are installed. They are not part of your Docker VM. And since Docker runs without GUI this stuff will not be installed right away. I don't see any code to install it in your container. Are you using one that has cuda support? If not you need to install it using your Dockerfile, not on your host"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I recently upgraded my hardware from D3 SerDes card to \"DESIGNCORE\u00ae NVIDIA\u00ae JETSON AGX XAVIER FPD-LINK\u2122 III INTERFACE CARD\". Hardware I have now: 1&gt; Jetson Xavier AGX installed with jetpack v5.0.2 2&gt; DESIGNCORE\u00ae NVIDIA\u00ae JETSON AGX XAVIER FPD-LINK\u2122 III INTERFACE CARD Question: Does D3 BSP v6.0.0 support above specified hardware with jetpack 5.0.2? I am trying to install D3 BSP v6.0.0. but after installing it I verified using uname -a. It did not give me desired response and still showing nvidia hardware rather than showing D3, as specified D3 BSP v6.0.0 release note.pdf.",
        "answers": [],
        "votes": []
    },
    {
        "question": "We have a Nvidia Jetson NGX and our cuda installation broke after working for a while after accidentally updating \"sudo apt update\". We were not sure how to install cuda onto the jetson without reflashing it.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am interested in porting a camera driver from one SOC(Pi) to another SOC(Jetson) which both run flavors of linux and have CSI-2 interfaces. I have the source code for the driver. I'm trying to understand how the CSI-2 interface works with respect to V4L2. The camera is controlled by the SOC through an I2C line which control functions are provided for in the source code. What I am confused about is if I need to write something to actually pull frames from the camera and control the CSI interface directly? Or is that all standardized by the CSI-2 interface and handled by V4L2? If you wanted to access the raw data from the camera how is this done?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to stream video with multiple cameras on the jetson nano. While streaming for longer period of time, one of the camera randomly kept freezing. So I check the available devices thinking there might be a connection problem but found the device id was changed. &gt; v4l2-ctl --list-devices vi-output, imx219 8-0010 (platform:54080000.vi:4): /dev/video0 USB 2.0 Camera (usb-70090000.xusb-2.1): /dev/video1 USB 2.0 Camera (usb-70090000.xusb-2.4): /dev/video2 I am using one CSI and Two usb cameras. The device id's of the usb cam was 3 and 2 before.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to communicate between two docker containers that are present on different machines(Jetson AGX and Linux PC). I want to do this via a ethernet cable. I have been able to ping the container on the Jetson from the container on the linux pc but not the other way around.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I reboot the nvidia Jetson, after reboot I got message Starting Login Service Started D-Bus system message Bus How can I set up the jetson now fully from initial or how can I debug this error? I am expecting to reboot completely from the initial point, and install new jetpack from the start.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to realtime process video that captures DJI Mavic using onboard NVidia Jetson module. Is it possible to use video data in external device? How to access it?",
        "answers": [
            [
                "There are two options: Turn on a video stream, grab it in external device (for example, via openCV), then process. Since you have cv::Mat object for each frame you can do anything. For video streaming setup you can use this. To open video stream in opencv you should create a Video Capture with RTMP address as parameter. Grab raw frames data from camera using DJI SDK. Now you can decode them either via SDK using your device's GPU or send raw data to your external device over TCP or whatever you like and then decode. After decoding you can process each frame in your needs. Starting point for this approach. If you need more in-depth information or have any questions feel free to ask."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a aerial camera's view on the ground that is defined by 4 corners in Lat/Long coordinates. In Python, what is the best way to generate a grid, say 100X100, inside this trapezoid defined by the 4 corners? Context: I am trying to get a fast implemetation of a landmask for image processing. Camera sees a scene that is part ocean part land and I would like to generate a very coarse mask corresponding to area with land in the image. Thanks in advance. I am trying to do a realtime masking of an image (accurately geo referenced) on an Nvidia Jetson.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Before, I was having an error in which the Nvidia Jetson boot was stuck on \"Starting User Manager for UID 124...\" (like here: https://unix.stackexchange.com/questions/426235/how-to-solve-stopping-user-manager-for-uid-121-error-after-installing-nvidia-d). So I followed the suggestion of running \"sudo dpkg-reconfigure lightdm, then sudo reboot.\" However now whenever I boot up the Jetson it takes me to the tty page. There on the command line running startx (from here: https://askubuntu.com/questions/403747/how-to-access-gui-from-tty-mode) yields the error message /usr/lib/xorg/Xorg: symbol lookup error: /usr/lib/xorg/Xorg: undefined symbol: drmGetEntry. Therefore I tried other commands to start the enable the GUI like sudo systemctl enable lightdm.service, sudo systemctl enable lightdm, and sudo systemctl unmask lightdm.service (all from here: https://forums.developer.nvidia.com/t/how-to-disable-tx1s-desktop/50343/14) but the GUI will not load. Therefore was wondering if you guys could provide some commands I could possibly run to enable the GUI? Thank you.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I received a Jetson Nano with a developer kit (B01) and a micro SD card from a previous research project that I want to continue, but unfortunately I didn't receive the needed password. My question is if there is a way to remove or change the password, so I am able to continue my research with the previous project setup. I am familiar with Raspberry Pis but not with the Jetson Nano, so I would really appreciate your help. What I tried: Since I don't want to lose the data, I first created a backup of the micro SD card using Win32 Disk Imager, like I would on a Raspberry Pi. Based on the backup and a second micro SD card, I made successfully a copy that I can work on without the risk of losing data. I tried login in with the two defaults: user: nvidia password: nvidia and user: ubuntu password: ubuntu, but it was unsuccessful. The login screen shows a custom username, so my guess is that the defaults were changed or completely deleted. Since the Jetson Nano shows a Ubuntu login screen, my initial idea was to remove the password with this guide. Unfortunately, this guide seems not applicable for the Jetson Nano because I didn't know that it doesn't use Grub. I am currently reading about the serial debug console.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using CenterNet Resnet101 V1 FPN 512x512 [1] model for detection and returned very good result for detection of objects. I wanted to accelerate my model with TensorRT 8.4 so I converted the \u201c.pb\u201d tensorflow files to .onnx model with tf2onnx tool. After extracted .onnx model, when I converted to TensorRT engine with trtexec (.trt), I am getting error. This error relevant with datatype, so I changed data format to float32 [2]. And then tried to updated onnx file to convert engine, I got error about padding [3]. So I saw solution for that issue and I applied but result not changed [4]. Also I\u2019m using jetson orin 64gb with jetpack last version. Do you have any idea for overcome the issue? Environment TensorRT Version: 8.4 GPU Type: Jetson Orin Nvidia Driver Version: Tegra CUDA Version: 11.4 CUDNN Version: 8.4 Operating System + Version: Jetson Orin Python Version (if applicable): 3.8 TensorFlow Version (if applicable): 2.8 Relevant Files [1] http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_resnet101_v1_fpn_512x512_coco17_tpu-8.tar.gz [2] CenterNet keypoint detector not giving good FPS in jetson xavier NX - #5 by AastaLLL [3] [E] Error[4]: [shuffleNode.cpp::symbolicExecute::390] Error Code 4: Internal Error (StatefulPartitionedCall/ResizeToRange/Pad/paddings_Unsqueeze__197: IShuffleLayer applied to shape tensor must have 0 or 1 reshape dimensions: dimensions were [1,2]) [4] IShuffleLayer applied to shape tensor must have 0 or 1 reshape dimensions: dimensions were [-1,2]) - #5 by spolisetty",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am writing a code to read frames from an IP camera using OpenCV in a Jetson Nano Device with L4T R32.7.1 and JetPack 4.6.1. The version of OpenCV is 4.3.0 with Python3.6.9 The code of example that i am using is the following: import cv2 input_URI = \"rtsp://user:psw@IP_address:88/videoMain\" camera = cv2.VideoCapture(input_URI) while camera.isOpened(): ret, frame = camera.read() frame_rgba = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA) print(frame_rgba) However, i get the following error. [ WARN:0] global /tmp/pip-install-acvfpz_0/opencv-python_7732674b1cb7471d8a299216848188a1/opencv/modules/videoio/src/cap_gstreamer.cpp (1759) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module source reported: Unauthorized [ WARN:0] global /tmp/pip-install-acvfpz_0/opencv-python_7732674b1cb7471d8a299216848188a1/opencv/modules/videoio/src/cap_gstreamer.cpp (888) open OpenCV | GStreamer warning: unable to start pipeline [ WARN:0] global /tmp/pip-install-acvfpz_0/opencv-python_7732674b1cb7471d8a299216848188a1/opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created [ERROR:0] global /tmp/pip-install-acvfpz_0/opencv-python_7732674b1cb7471d8a299216848188a1/opencv/modules/videoio/src/cap.cpp (142) open VIDEOIO(CV_IMAGES): raised OpenCV exception: OpenCV(4.3.0) /tmp/pip-install-acvfpz_0/opencv-python_7732674b1cb7471d8a299216848188a1/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): rtsp://usr:psw@IP_address:88/videoMain in function 'icvExtractPattern' The code is inside a Docker container. Do you know how to fix it? It worked for a previous version of OpenCV but I updated the version and I started to receive this error. I tried to downgrade OpenCV but the error still appears for previous versions. Not sure what should i do. Thank you",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to use valgrind to debug a program for Nvidia Jetson, but I keep getting an assertion failed at startup. This seems to occur only when I link the program with both the ArmPL (Arm Performance Libraries) and Pthread libraries. This is a minimal sample to reproduce the issue: #include &lt;iostream&gt; int main(int argc, char *argv[]) { std::cout &lt;&lt; \"Hello, world!\" &lt;&lt; std::endl; } To compile: g++ -g -O0 main.cpp -L/mnt/data/opt/lib -larmpl -lpthread To run: valgrind ./a.out Output: ==8903== Memcheck, a memory error detector ==8903== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al. ==8903== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info ==8903== Command: ./a.out ==8903== --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 --8903-- Warning: DWARF2 CFI reader: unhandled DW_OP_ opcode 0x11 valgrind: m_debuginfo/readdwarf.c:2396 (copy_convert_CfiExpr_tree): Assertion 'srcix &gt;= 0 &amp;&amp; srcix &lt; VG_(sizeXA)(srcxa)' failed. host stacktrace: ==8903== at 0x5803DBA0: ??? (in /usr/lib/valgrind/memcheck-arm64-linux) sched status: running_tid=1 Thread 1: status = VgTs_Runnable (lwpid 8903) ==8903== at 0x4016D44: mmap (mmap64.c:59) ==8903== by 0x4005627: _dl_map_segments (dl-map-segments.h:94) ==8903== by 0x4005627: _dl_map_object_from_fd (dl-load.c:1181) ==8903== by 0x4007C5F: _dl_map_object (dl-load.c:2460) ==8903== by 0x400BCE7: openaux (dl-deps.c:63) ==8903== by 0x4015A53: _dl_catch_exception (dl-error-skeleton.c:196) ==8903== by 0x400C057: _dl_map_object_deps (dl-deps.c:249) ==8903== by 0x40037EF: dl_main (rtld.c:1726) ==8903== by 0x4014D6F: _dl_sysdep_start (dl-sysdep.c:253) ==8903== by 0x40018C3: _dl_start_final (rtld.c:414) ==8903== by 0x4001B47: _dl_start (rtld.c:523) ==8903== by 0x40011C7: ??? (in /lib/aarch64-linux-gnu/ld-2.27.so) Note: see also the FAQ in the source distribution. It contains workarounds to several common problems. In particular, if Valgrind aborted or crashed after identifying problems in your program, there's a good chance that fixing those problems will prevent Valgrind aborting or crashing, especially if it happened in m_mallocfree.c. If that doesn't help, please report this bug to: www.valgrind.org In the bug report, send all the above text, the valgrind version, and what OS and version you are using. Thanks. The error doesn't appear if linking only with ArmPL or pthread. System used: Nvidia Jetson AGX Xavier Ubuntu 18.04 gcc version 7.5.0 valgrind version 3.13.0 ArmPL version 21.1_Ubuntu-18.04_gcc-7.5",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using Jetson TX2. Current jetpack version : 4.5-b129 I was following update tutorial from nvidia website (https://docs.nvidia.com/jetson/jetpack/install-jetpack/index.html). I did: sudo apt update sudo apt dist-upgrade sudo apt install --fix-broken -o Dpkg::Options::=\"--force-overwrite\" Last command feedback: Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: activity-log-manager libgeonames-common libgeonames0 libnm-gtk0 libtimezonemap-data libtimezonemap1 libunity-control-center1 vulkan-utils Use 'sudo apt autoremove' to remove them. 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. I'm using sudo apt-cache show nvidia-jetpack to check jetpack version and it's still the same (4.5-b129)",
        "answers": [
            [
                "Ok, there was note, that I thought is only for 5.0.X users: To upgrade from JetPack 5.0/5.0.1 Developer Preview, first edit etc/apt/sources.list.d/nvidia-l4t-apt-source.list I edited nvidia-l4t-apt-source.list file from: deb https://repo.download.nvidia.com/jetson/common r31.5 main deb https://repo.download.nvidia.com/jetson/t186 r31.5 main to: deb https://repo.download.nvidia.com/jetson/common r32.7 main deb https://repo.download.nvidia.com/jetson/t186 r32.7 main and it upgraded my Jetpack to 4.6.1. Unfortunately nvidia does not support Jetpack 5.X.X on TX2 series"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am running redisinsight container on my local system and the redis database is created on a Jetson device. Is it possible to add the this redis database to my redisinsight? Since my Jetson device has no display available I expect to display the redisdb from my linux machine",
        "answers": [],
        "votes": []
    },
    {
        "question": "To be more specific, my source code is compiled and linked successfully when I am running it from inside the container. However, when I am trying to build the image from a Dockerfile it fails. i.e.: this works (These lines are from the terminal \"inside\" the container): cd AppFolder; make; //success this does not (These are lines from the dockerfile): RUN git clone &lt;url&gt; &amp;&amp; cd APPFolder &amp;&amp; make Now I get: /usr/bin/ld: warning: libcuda.so.1 needed by... How can I build the application from the dockerfile?",
        "answers": [
            [
                "The only difference between a container and a layer during the image build is the next layers. perhaps you are running the RUN directive to early - i.e. before the cuda library was generated? try putting this command as low as you can in the Dockerfile"
            ],
            [
                "Well, adding \"-Wl,--allow-shlib-undefined\" to the compiler/linker (g++) solved this issue. I think it \"tells\" the linker to \"remain\" pointer to function that will be \"linked\" only in runtime (i.e., when running the docker image)"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "i am trying to stream live video feed from a camera connected to a Jetson NX, to a computer connected to the same network, the network works as wireless ethernet, meaning the jetson sees it as wired connection but in reality its wireless and is limited by bitrate. On the jetson side, I am using OpenCV VideoWrite to send frames over the network using this pipeline: cv::VideoWriter video_write(\"appsrc ! video/x-raw,format=BGR ! queue ! videoconvert ! video/x-raw,format=BGRx ! nvvidconv\\ ! video/x-raw(memory:NVMM),format=NV12,width=640,height=360,framerate=30/1 ! nvv4l2h264enc insert-sps-pps=1 insert-vui=1 idrinterval=30 \\ bitrate=1800000 EnableTwopassCBR=1 ! h264parse ! rtph264pay ! udpsink host=169.254.84.2 port=5004 auto-multicast=0\",\\ cv::CAP_GSTREAMER,30,cv::Size(640,360)); on the receiving computer my video capture is : cv::VideoCapture video(\"udpsrc port=5004 auto_multicast=0 ! application/x-rtp,media=video,encoding-name=H264 ! rtpjitterbuffer latency=0 ! rtph264depay ! decodebin ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1 \" , cv::CAP_GSTREAMER); Problem is, if the camera jitters a lot or moves a lot to any direction, the video stream either freezes or completely pixelates. I was hoping I could get suggestions for either a better encoder(im not limited to nvv4l2h264enc or h264 in general) or a solution for the pixaltion and freeze, or maybe even a better way to stream the video other than VideoWrite. I am trying to stream 360p video at 30fps, my bitrate is limited to either 6mbps or 2.5mbps depending on the distance i want to limit myself at. it does not seem like a network problem simply because If i change parameters like Codec(MJPG instead of gstreamer for example) it changes the behaviour of the video feed, in my case it lowers the amount of freezing but makes the pixelating worse.",
        "answers": [],
        "votes": []
    },
    {
        "question": "i try to sending video stream. the source is jetson orin and the client is flutter web app. i'm also want to store the data in db. i'm look for recommendation which db,server,transfer data protocols to use for this task. my stracture right now : jetson AGX orin --&gt; serialization to jeson string base64 \u2192 rabbitmq \u2192 server (node.js+express) \u2192 mongoDB \u2192 client (flutter)",
        "answers": [],
        "votes": []
    },
    {
        "question": "So I'm working on a Jetson Nano running Ubuntu 18.04. I'd like to create a script that at startup automatically runs, taking a video feed from the connected webcam, and opening up a window to view the feed. Before fully testing out my main script, I wanted to do a proof of concept that just opens up an image to display upon startup. To do so, I followed this tutorial to get a script to run on startup, and added the part to read &amp; open the image: https://www.linuxshelltips.com/run-python-script-ubuntu-startup/ The modified script is this: rom os.path import expanduser import datetime import cv2 import matplotlib import matplotlib.pyplot as plt from PIL import Image img = cv2.imread('/home/sam/Documents/2022-08-31-220946_4.jpg') window_name = 'test_img' cv2.imshow(window_name,img) while True: k = cv2.waitKey(0) &amp; 0xFF if k==27: break cv2.destroyAllWindows() file = open(expanduser(\"~\") + '/Desktop/i_was_created.txt', 'w') file.write(\"This LinuxShellTips Tutorial Actually worked!\\n\" + str(datetime.datetime.now())) file.write('OpenCV Version: {}\\n'.format(str(cv2.__version__))) file.write('Matplotlib Version: {}\\n'.format(str(matplotlib.__version__))) file.write('img size: {}\\n'.format(str(img.shape))) file.write('img size: {}'.format(str(image.format))) file.close() Now, when I run this script on startup, it doesn't work. The imshow() window doesn't appear anywhere, and the rest of the script doesn't execute after hitting the escape button. But, if I comment out the following lines, the script runs from start to finish without issue from startup: cv2.imshow(window_name,img) while True: k = cv2.waitKey(0) &amp; 0xFF if k==27: break cv2.destroyAllWindows() So it seems the issue really does lie with the imshow() function. Testing with a simple cv2.imshow('img',img) cv2.waitKey(0) cv2.destroyAllWindows() instead also prevents the lines after from running. However, when I run the script from the terminal manually, it works completely fine without issue, showing the image until the escape key is hit, then creating the txt file. Also to note, using other image display libraries such as PIL &amp; Matplotlib leads to the same issue, so don't think it's necessarily specific to OpenCV. So, I'm wondering what's causing this issue, and how it might be able to be resolved. Any help anyone can offer is much appreciated!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to integrate ov5647 sensor with Jetson Nano, ov5647 is very basic sensor but it does not have any official support from Jetson nano. So we need to build our own driver. So posts in Jetson nano suggests to follow imx219. So I followed imx219 structure for dts files this is the process I followed: I took all the imx219 dts files and I make a copy of them by replacing imx219 with ov5647 in file name and inside data also replaced imx219 with ov5647. and I had written ov5647.c driver and compiled after that created an Image and flashed to SD card but after all these change, still when I try to load ov5647.ko (dynamic module). It loads but this driver is not being uilized by any other devices. it shows 0 in lsmod, which means my dts files not all making any effect in Jetson Nano can anyone tell me what are mistakes I had done. Zip file for files which I had changed: https://drive.google.com/file/d/1uB9S4_KcY9pt_GFCBYAK9wJ0HZfGI4wR/view?usp=sharing",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to stream a video in a LAN I have from one edge device to another. I tried many solutions like streaming a video using OBS streaming or GStreamer on a Jetson TX1 device (using the onboard camera). I tried to share the OBS stream to an AntMedia server and then listen to it using VLC on the other side. I wasn't lucky with any of these solutions. OBS I think it didn't work as supposed to be, and I didn't know how to create the pipeline to read the camera's stream adaptively and send it over the network. Please share any free software or any resources that might help since I spent two full days searching online. Thanks",
        "answers": [],
        "votes": []
    },
    {
        "question": "Accidentally changed the root=/dev/mmcblk0p1 to root=/dev/sda1 on the extlinux.conf file on a Jetson Nano SUB version board and now I am stuck on a page which shows bash-4.4# when I boot the board, bash-4.4# ls, #bin dev etc init lib mnt proc root sbin sys tmp usr var , everytime that I boot the board. How can I try to change back the extlinux.conf file to add the root back to root=/dev/mmcblk0p1? I found this file after I went to cd /boot/extlinux/ and sudo cp extlinux.conf extlinux.conf.boot_emmc_backup then entered the vim extlinux.conf to modify it but now when I boot the system at bash-4.4# the command cd /boot/extlinux/ does not exists and I do not know how to get back to the previous boot loading screen.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Can someone explain how I can make this pipline (that works in shell) to work in python and cv2.VideoCapture(gstr, cv2.CAP_GSTREAMER) gst-launch-1.0 nvcompositor \\ name=comp sink_0::xpos=0 sink_0::ypos=0 sink_0::width=800 \\ sink_0::height=650 sink_1::xpos=800 sink_1::ypos=0 \\ sink_1::width=800 sink_1::height=650 ! nvoverlaysink display-id=1 \\ v4l2src device=/dev/video0 ! 'video/x-raw , width=(int)1600 , height=(int)1300 , format=(string)GRAY8 , framerate=(fraction)60/1' ! nvvidconv ! 'video/x-raw(memory:NVMM), width=(int)1600, height=(int)1300, format=I420' ! nvvidconv ! 'video/x-raw, format=BGRx' ! videoconvert ! 'video/x-raw, format=BGR, width=(int)800 , height=(int)650' ! videoconvert ! comp. \\ v4l2src device=/dev/video1 ! 'video/x-raw , width=(int)1600 , height=(int)1300 , format=(string)GRAY8 , framerate=(fraction)60/1' ! nvvidconv ! 'video/x-raw(memory:NVMM), width=(int)1600, height=(int)1300, format=I420' ! nvvidconv ! 'video/x-raw, format=BGRx' ! videoconvert ! 'video/x-raw, format=BGR, width=(int)800 , height=(int)650' ! videoconvert ! comp. -e",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have been following the guide at https://www.tensorflow.org/lite/guide/build_arm in an attempt to compile TfLite for a Nvidia Jetson Nano. Following the instructions for the c++ install, I've installed Bazel and proceeded to build, only to get the following error: ERROR: /home/user/tensorflow/tensorflow_src/tensorflow/lite/kernels/internal/BUILD:857:11: Compiling tensorflow/lite/kernels/internal/mfcc.cc failed: (Exit 2): aarch64-linux-gnu-gcc failed: error executing command (cd /home/user/.cache/bazel/_bazel_user/d40cf1f049552d54fe67262adae8a272/execroot/org_tensorflow &amp;&amp; \\ exec env - \\ PATH=/home/user/.cache/bazelisk/downloads/bazelbuild/bazel-5.3.0-linux-arm64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\ PWD=/proc/self/cwd \\ TF2_BEHAVIOR=1 \\ /home/user/.cache/bazel/_bazel_user/d40cf1f049552d54fe67262adae8a272/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/user/.cache/bazel/_bazel_user/d40cf1f049552d54fe67262adae8a272/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include -isystem /home/user/.cache/bazel/_bazel_user/d40cf1f049552d54fe67262adae8a272/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include-fixed -isystem /home/user/.cache/bazel/_bazel_user/d40cf1f049552d54fe67262adae8a272/external/aarch64_linux_toolchain/aarch64-linux-gnu/include/c++/8.3.0/ -isystem /home/user/.cache/bazel/_bazel_user/d40cf1f049552d54fe67262adae8a272/external/aarch64_linux_toolchain/aarch64-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/internal/_objs/audio_utils/mfcc.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/internal/_objs/audio_utils/mfcc.pic.o' -fPIC -iquote . -iquote bazel-out/aarch64-opt/bin -iquote external/fft2d -iquote bazel-out/aarch64-opt/bin/external/fft2d -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-unknown-warning -Wno-array-parameter -Wno-stringop-overflow -Wno-array-bounds -Wunused-result '-Werror=unused-result' -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++17' -DFARMHASH_NO_CXX_STRING -Wno-sign-compare -O3 -fno-exceptions -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/lite/kernels/internal/mfcc.cc -o bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/internal/_objs/audio_utils/mfcc.pic.o) # Configuration: ab27fd0d011bfd59c6f8c4ad9e07f710413fd94d9d706bfa323f3377eb0d533e # Execution platform: @local_execution_config_platform//:platform /home/user/.cache/bazel/_bazel_user/d40cf1f049552d54fe67262adae8a272/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc: 1: /home/user/.cache/bazel/_bazel_user/d40cf1f049552d54fe67262adae8a272/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc: Syntax error: \"(\" unexpected Checking the file where it exploded, this seems to be the line in question # Audio support classes imported directly from TensorFlow. cc_library( name = \"audio_utils\", srcs = [ \"mfcc.cc\", \"mfcc_dct.cc\", \"mfcc_mel_filterbank.cc\", \"spectrogram.cc\", ], hdrs = [ \"mfcc.h\", \"mfcc_dct.h\", \"mfcc_mel_filterbank.h\", \"spectrogram.h\", ], compatible_with = get_compatible_with_portable(), copts = tflite_copts(), deps = [ \"//third_party/fft2d:fft2d_headers\", \"@fft2d\", ], ) But I have no idea why it would have an issue here in particular. This is an otherwise clean OS install. Any idea what might be causing this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have tried many attempts to run the cameras connected to my jetson nano. When I run this command: gst-launch-1.0 -v nvarguscamerasrc ! 'video/x-raw(memory:NVMM),format=NV12,width=1280,height=720,framerate=30/1' ! autovideosink The camera works but when I try to perform a simple camera read in python with this code import sys import cv2 def read_cam(): G_STREAM_TO_SCREEN = \"videotestsrc num-buffers=50 ! videoconvert ! appsink\" cap = cv2.VideoCapture(G_STREAM_TO_SCREEN, cv2.CAP_GSTREAMER) if cap.isOpened(): cv2.namedWindow(\"demo\", cv2.WINDOW_AUTOSIZE) while True: ret_val, img = cap.read() cv2.imshow('demo',img) cv2.waitKey(1) else: print (\"Unable to use pipline\") cv2.destroyAllWindows() if __name__ == '__main__': read_cam() the camera does not work in the code above and it returns \"Unable to use pipline\". What I am doing wrong and how can I access the camera feed in python?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am working on Jetson Nano and trying to control servo motors. I am using PCA9685. I have installed all the pre-requisites for that sudo pip3 install adafruit-circuitpython-servokit Faced an error called SyntaxError: future feature annotations is not defined i updated python for that and the issue was resolved. Now the issue is that when i use python 3.8 as an interpreter in Visual studio i face the error No module name adafruit_servokit But when i use the python 3.6, the adafruit_servokit error is solved and the annotation error arises. Looking forward to every suggestion. Thank you",
        "answers": [
            [
                "This is because you have conflicting interpreters for your python installation. Inside your Visual Studio terminal, try using: python3.8 -m pip install adafruit-circuitpython-servokit Using python -m pip install instead of just pip install is recommended. The reason is that it will use the correct interpreter for your python installation, specially if you have many versions installed. More details here: https://snarky.ca/why-you-should-use-python-m-pip/ ---EDIT--- You should replace python3.8 with your target version number."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Is there a way to view the link status or other information that would give information on the overall status of the connection of a Linux host in endpoint mode? To clarify, I have a Jetson set to be the endpoint, this is confirmed to be working by using lspci in another Jetson as root port and am able to transfer data. The next step is to connect an FPGA in root port mode to the endpoint Jetson. I need a way, from the Linux endpoint side, to check that a link and/or communication has been established to the FPGA. I have looked in dmesg and kern.log and all the various entries in /sys and couldn't find anything that gives the info I'm looking for. Something like lspci but for the endpoint side would be ideal.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to open a video stream using OpenCV and G stream from an IP camera. The pipeline reads each frame, but no window is being shown with imshow. It works fine when I run the pipeline with gst-launch-1.0 and nvoverlaysink on bash. The following is the python code that I have been using: import cv2 as cv gst = \"rtspsrc location=rtsp://usser:pass@ip:port/url latency=0 ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink\" capture = cv.VideoCapture(gst, cv.CAP_GSTREAMER) print(\"Is pipeline open: \", capture.isopened()) while True: ret, frame = capture.read() print(\"Is receiving frames: \", ret) cv.imshow(\"Stream\",frame) capture.release() When I run the script, I can see that the pipeline is open and OpenCV is reading frames. I only have the following warning: [ WAN:0] global /tmp/pip-install-r_pt66np/opencv-python/opencv/modules/videoto/ src/cap_gstreamer.cpp (935) open Opencv | GStreamer warning: Cannot query video position: status=1, value=0, duration=-1 Although the script runs and consumes frames, imshow doesn't generate a window.",
        "answers": [
            [
                "The problem wasn't the pipeline. I had so many errors with the pipeline that I suppose that the error was there. imshow needs a wait key to work. Here it's the working code: import cv2 as cv gst = \"rtspsrc location=rtsp://usser:pass@ip:port/url latency=0 ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink\" capture = cv.VideoCapture(gst, cv.CAP_GSTREAMER) print(\"Is pipeline open: \", capture.isopened()) while capture.isOpened() : ret, frame = capture.read() print(\"Is receiving frames: \", ret) cv.imshow(\"Stream\", frame) if cv.waitKey(25) &amp; 0xFF == ord(\"q\"): break capture.release() cv.destroyAllWindows()"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have problem like described below. I have run my jetson nano with webcam connected. I connected to the jetson via ssh and I have access to my camera from a host by using: ssh -X nano@&lt;my_ip&gt; cheese So the part is working. Then I'm trying to run some docker container on my jetson via ssh. So first at all: ssh -X nano@&lt;my_ip&gt; then: docker run -it --runtime=nvidia -p 8888:8888 -v ${PWD}:/home/app -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY -v /dev/:/dev/ --privileged -w /home/my_app &lt;image_name&gt; And there I have some simple python script to open the video and show preview. And the part is not working, the camera is visible (/dev/v4l/by-id) but opencv cannot get any frame. Of course when I'm working directly on jetson (without ssh) the docker container with the python script is working correctly (I mean, just logging to the nano, run the docker image, exactly like above and then inside run the python script) So the question is, what am I doing wrong? Edit While controlling the jetson by ssh: I have set DISPLAY=:0, and then I can run the container and python script - camera is working but the preview is shown in \"remote\" display (connected directly to jetson nano) So as I understand it's a problem with setting up correct DISPLAY and redirecting the preview from remote to my host machine via ssh.",
        "answers": [],
        "votes": []
    },
    {
        "question": "My current project requires me to be have as little latency as possible, one of the main reasons i moved from NX to orin was to reduce runtime of my code, even if it will only improve on a few miliseconds. i\u2019ve built a small test process to check for latency on my camera, what i\u2019ve done is: connect a LED to the jetson GPIO. find the pixel in the frame that the LED is on. enter a simple open camera test file. listen to a switch before each frame grab. when the switch is on, start a timer and send HIGH through the GPIO pin. test the pixel the LED is on, when its green number is more than a certain number, it means the program has seen the LED is on. parameters i change between each test: fps- either 30 or 60 resolution - either 1080p(1920x1080) or 480p(640x480) camera exposure time- between 1 (almost 100% dark) to 300(about 30milisec, default camera exposure) i\u2019ll add my test results on jetson NX first: fps :30 resolution - 1080p exposure - 1-10 i can see the led on the same frame i turned it on-best result, between 0 to 30 milisec delay fps:60 res-1080p rexposure 1-10 i can see the LED with 3-4 frame delay, meaning over 60 milisec delay fps :60 res-480p exposure 1-10 i can see the led on the same frame i turned it on- between 0 to 16 milisec delay but low resolution now my ORIN test results, which have been disappointing : fps :30 resolution - 1080p exposure - 1-10 i can see the led on the NEXT frame i turned it on- between 30 to 60 milisec delay fps:60 res-1080p rexposure 1-10 i can see the LED with 5-6 frame delay, meaning over 100 milisec delay fps :60 res-480p exposure 1-300 i can see the led on the same frame i turned it on- between 0 to 16 milisec delay but low resolution conclusion- ORIN seem to struggle even more than the NX when it comes to high fps and high resolution, or even high resolution in general. and Jetson in general gives me very high latency when working with high resolution, not even talking about 4k. in a project which i am fighting with every 1 milisecond delay, this has been a very big problem. what i am looking for: i would appreciate some advice when it comes to using cameras, maybe i am doing something wrong, maybe its a camera problem and not a jetson problem, maybe its an Opencv problem. if anyone has a good development camera i can use i would appreciate it as well, preferably USB3 camera but i can work with a mipi camera if the cable is long enough(&gt;35cm). notes: gstreamer pipeline to start the camera: cv::VideoCapture video_camera(\"v4l2src device=/dev/video0 ! video/x-raw,width=640,height=480 ! nvvidconv flip-method=2 ! video/x-raw(memory:NVMM), format = I420 ! nvvidconv ! video/x-raw,format=BGRx,width=(int)640, height=(int)480, framerate= (fraction)60/1 ! videoconvert !video/x-raw,format=BGR ! appsink drop=True sync=False\"); to change camera settings i use v4l2-ctl my camera: e-con see3cam-cu30 camera link",
        "answers": [],
        "votes": []
    },
    {
        "question": "At first, Linux for Tegra doesn't support perf tool in Jetson NX, and I have tried using its JP-5.0.1 Driver Package Sources ,but it's still not working... So maybe there are other tools that can replace the linux perf tool to check the cache missesenter image description here? enter image description here",
        "answers": [],
        "votes": []
    },
    {
        "question": "I\u2019m currently working on a project trying to stitch 2 video inputs together to form a wide view covering 180 degrees. An alternative to stitching, which I assume would take a lot of processing power for real time stitching, could be cylindrical projection of the images. Currently, I have two cameras set up next to each other, and concatinating them into 1 window using OpenCV. I\u2019m showing 2 camera inputs using the code below. It runs fine, but straight lines that transition from one image to the next become \u2018bent\u2019. I was wondering if some sort of projection would make the two images look nicer. Best example I could fine: Cylindrical Projection Any ideas how to achieve the above? Any help appriciated! import cv2 import numpy as np cap1 = cv2.VideoCapture(\"v4l2src device=/dev/video0 ! video/x-raw, format=BGRx, width=2064, height=1544 ! videoconvert ! video/x-raw, format=BGR ! appsink\") cap2 = cv2.VideoCapture(\"v4l2src device=/dev/video1 ! video/x-raw, format=BGRx, width=2064, height=1544 ! videoconvert ! video/x-raw, format=BGR ! appsink\") while True: _, frame1 = cap1.read() _, frame2 = cap2.read() frame1 = cv2.resize(frame1, (720,520)) frame2 = cv2.resize(frame2, (720,520)) sbs = cv2.hconcat([frame2, frame1]) cv2.imshow('sbs', sbs) if cv2.waitKey(1) == ord('q'): break sbs.release() cv2.destroyAllWindows()",
        "answers": [],
        "votes": []
    },
    {
        "question": "i need help to install tensorflow on jetson nano. I follow guide at: https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html After that, my tensorflow version is: 1.15.5+nv22.3 But when i run: import tensorflow This error show up: /usr/lib/aarch64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found Pls help me because i have tried so many ways but libstdc dont update glibcxx_3.4.26 Versions: python: 3.8.0 jetpack: # R32 (release), REVISION: 7.2, GCID: 30192233, BOARD: t210ref, EABI: aarch64, DATE: Wed Apr 20 21:34:48 UTC 2022 Maybe i installed wrong version of TF ?? I tried other versions but only jetpack 5.0 installs successfully",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am new to docker I am using ssh to connect to Nvidia jetson NX and use the docker on nvidia the system for nvidia is a modifed ubuntu 18.04 called tegra. What I want to do is to exec the container named tetraai_service_nx. It worked fine until yesterday by command \"docker exec -it tetraai_service_nx /bin/bash \" however, today when I tried the same command, i see\"Error response from daemon: Container 0feeb9be5a251bb9ce45ed9a05d24a86e5a77ef9d93439c717fdef9f7a6560d4 is not running \" 0feeb9be5a251bb9ce45ed9a05d24a86e5a77ef9d93439c717fdef9f7a6560d4 is the right id for the container I want which is correct. Then i tried as in the following picture and got the error This is the error i see Error response from daemon: Cannot restart container tetraai_service_nx: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: Running hook #0:: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'csv' invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported. Please use the NVIDIA Container Runtime instead.: unknown Could anyone explain what this message mean exactly and what might the problem be? because I saw similar errors in different places some command result for me some command result for me some command result for me",
        "answers": [
            [
                "I have the same issue. If you install the previous version, 1.9 it should work until they can fix 1.10. For more information, check out this: https://aur.archlinux.org/packages/nvidia-container-runtime and read the comments."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am customising the linux kernel. In normal Linux, the BH thread priority is set to a constant priority (MAX_USER_RT_PRIO / 2) [1], but I am changing the BH thread priority from a constant priority (MAX_USER_RT_PRIO / 2) to another value in the interrupt handler [2]. Specifically, this priority is changed in __irq_wake_thread(). #code1 action-&gt;thread-&gt;prio = prio_input or #code2 sched_setscheduler_nocheck(action-&gt;thread, SCHED_FIFO, &amp; param_input); But with such a customised kernel, the hardware crashes and reboots two seconds later. I debugged it and identified #code1, #code2 as the cause, but I don't understand what's wrong. what was the error when it crashed? It simply downed without any pop-ups like error messages. dmesg has no clues. output of command \"last\" reboot system boot 4.9.201-rt134 Sun Jul 10 11:24 still running him :1 :1 Sun Jul 10 11:21 - crash (00:02) reboot system boot 4.9.201-rt134 Fri Dec 31 20:00 still running Are priority changes ever not allowed in Interrupt? Environment Hardware: Jetson nano Power mode: 10 W Power: 5V4A Kernel: linux kernel-4.9 How to build: https://forums.developer.nvidia.com/t/applying-a-preempt-rt-patch-to-jetpack-4-5-on-jetson-nano/168428/4 [1] In setup_irq_thread() [2] In __irq_wake_thread() Reply for other comments: changing that could cause some critical BHs to not run in time and break things. I change only the priority of a few interrupts(BH), not all interrupts. It is set with care.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I found a python module called 'mouse' (installed using 'sudo pip3 install mouse') that allows control of the mouse cursor's position using python, but the mouse.click() method that it provides doesn't do anything. Is there a way for me to simulate clicking with this module? If not, what is another way that I can simulate a mouse click using Python? I am using Python 3.6.",
        "answers": [
            [
                "I was able to use x@y:~$ pip3 install PyAutoGui to install PyAutoGUI after resolving all of the dependency issues and running x@y:~$ xhost + in the terminal before running x@y:~$ python3 &gt;&gt;&gt; import pyautogui I think that extra installations are needed to run 'xhost +', but I don't remember what they were. I only remember searching for a fix to an installation error telling me to check that I have an \"X server\" running."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to use vision component CSI camera. I follow an instruction here ( https://github.com/VC-MIPI-modules/vc_mipi_nvidia). but I face an error below. I tryied with Ubuntu 18.04 and 20.04 as a host (NOT in virtual machine). someone had this error and he overcome it error by using another server but i cant handel this error with his solution. Do you have any solution to handle this error? Error: Return value 17 Command tegradevflash_v2 --pt flash.xml.bin --create Failed flashing t186ref. enter image description here",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to run my code on jetson NX5, but even after installation of it, it won't run. even tried this command but still, have the same problem. git clone --recursive https://github.com/Microsoft/vscode.git Any Idea?",
        "answers": [
            [
                "A previous version, e.g. 1.50.0 started without error. wget https://update.code.visualstudio.com/1.50.0/linux-deb-arm64/stable -O stable.deb sudo dpkg -i stable.deb"
            ],
            [
                "May be late reply, But, its worth replying for future reference: Note: This trick works for me for AGX Xavier with JP5.0.1, hope this works for you as well with NX5. There is a bug in the latest release of debian of Vs code for AGX Xavier. As, @Ali, rightly mentioned, lower release of it i.e. 1.50.* works fine. Ref: https://forums.developer.nvidia.com/t/vs-code-can-t-launch-with-jetpack-5-0/213980"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "Is there a way to find the version of the currently installed JetPack on my NVIDIA Jetson Xavier AGX kit?",
        "answers": [
            [
                "To get the JetPack version, architecture and dependencies, sudo apt-cache show nvidia-jetpack #Package: nvidia-jetpack #Version: 4.4.1-b50 #Architecture: arm64 #Maintainer: NVIDIA Corporation #Installed-Size: 194 #Depends: nvidia-cuda (= 4.4.1-b50), nvidia-opencv (= 4.4.1-b50), nvidia-cudnn8 (= 4.4.1-b50) For the version specifically, sudo apt-cache show nvidia-jetpack | grep \"Version\" #Version: 4.4.1-b50"
            ],
            [
                "git clone https://github.com/jetsonhacks/jetsonUtilities.git cd jetsonUtilities python jetsonInfo.py Output: NVIDIA Jetson Nano (Developer Kit Version) L4T 32.5.1 [ JetPack 4.5.1 ] Ubuntu 18.04.5 LTS Kernel Version: 4.9.201-tegra CUDA 10.2.89 CUDA Architecture: 5.3 OpenCV version: 3.4.17-dev OpenCV Cuda: YES CUDNN: 8.0.0.180 TensorRT: 7.1.3.0 Vision Works: 1.6.0.501 VPI: ii libnvvpi1 1.0.15 arm64 NVIDIA Vision Programming Interface library Vulcan: 1.2.70"
            ]
        ],
        "votes": [
            12.0000001,
            3.0000001
        ]
    },
    {
        "question": "I am using this command: gst-launch-1.0 v4l2src ! xvimagesink to stream video over usb on my nvidia jetson nano and I am getting this output: Setting pipeline to PAUSED... ERROR: Pipeline doesn't want to pause. ERROR: from element /GstPipeline:pipeline0/GstXvImageSink:xvimagesink0: Could not initialise Xv output Additional debug info: xvimagesink.c(1773): gst_xv_image_sink_open (): /GstPipeline:pipeline0/GstXvImageSink:xvimagesink0: Could not display (null) Setting pipeline to NULL.. Freeing pipeline...",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to connect my NVIDIA Jetson Xavier NX device to Allen Bradley PLC. The Jetson device is basically a Linux ARM PC, with a regular Ethernet port. While creating a \"Module\" with the Allen Bradley LogixDesigner, it is asking me for different connection parameters: | Assembly Instance | Size Input | ??? | ??? Output | ??? | ??? Configuration | ??? | ??? Since my Jetson PC connects to the PLC using Ethernet/IP cables, I am confused what parameters these could be. I went through Rockwell automation literature, but unable to find out clear answers about this. I am adding this as a Generic Ethernet/IP module in LogixDesigner, but still being asked for this information. I have also asked my PC manufacturer (AdlinkTech), and they don't seem to have any answers as well. Any help in this regard would be highly appreciated!!",
        "answers": [
            [
                "Ethernet/IP is a specific protocol that some industrial devices, such as Allen-Bradley PLCs, use to communicate over Ethernet. Messaging in Ethernet/IP can be split into two types: Implicit Messaging and Explicit Messaging. Implicit Messaging is when you set up a predefined structure of data that communicates at regular intervals between the two devices. This is most commonly used when you are dealing with an I/O device that needs to continually send or receive data from the PLC. On your Allen-Bradley PLC, when you create a Generic Ethernet/IP device in LogixDesigner, you are attempting to set up implicit messaging, and the 6 numbers that it asks for define the data structure that gets passed in that message. Explicit Messaging, on the other hand, is a message that gets communicated on-demand in the program. On an Allen-Bradley PLC, this is done via the MSG instruction. There are different types of messages that can be set up this way, which you will see when you configure your MSG instruction. In order to get your device to talk to the Allen-Bradley PLC, your device needs to act as an Ethernet/IP server. You'll probably either need to install Ethernet/IP server software on your PC to do this or write your own. More details on the Ethernet/IP protocol can be found on the Ethernet/IP Developers Guide from ODVA, the standards organization in charge of Ethernet/IP."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am on the new end of learning remote connections and I ran into a rather strange issue when connecting remotely to a machine. Host: Jetson Nano - Ubuntu Client: Asus desktop - Linux Mint I am using SSH to connect to the host machine. Once I'm in, I run my program which should open the camera that the host machine has connected via mipi connection... but it does not show a display window. Rather it displays: Gtk-WARNING **: cannot open display: lcocalhost:10.0 CONSUMER: Done Success (Argus)Error InvalidState: Argus client is exiting with 2 outstanding client threads If run the program in the machine without SSH connection, it works and the display shows what the camera is capturing. I tried changing the X11forwarding and agent to YES, and I tried export DISPLAY=localhost:10.0. That did not work as well. Any help would be appreciated. Thanks, GM",
        "answers": [
            [
                "Bear in mind that many GPU-related stuff won't work without a working display. Sadly, X11 forwarding doesn't work in those cases. At this point, it is not clear if this is your case, or if it is simply that you have the wrong DISPLAY number. You may try: Connecting a physical monitor and keyboard to the board, opening a terminal and running echo $DISPLAY. (in the keyboard/monitor session, not the SSH session). Then set that on your remote session as export DISPLAY=:X (where X is what was printed before). If you are using GStreamer then use nvoverlaysink which doesn't require X. You will need a monitor connected to the board though."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to run Inference on my Jetson Nano with TensorRT, but this Error keeps popping up. I dont really work with .pb model rather than .h5 models thats why I am converting my Model to .pb from .h5. I am using this script to convert my model to a tensorrt model, its a modification of the nvidia docs scripts, but more or less the same : Nvidia Docs import tensorflow as tf import os model_dir = 'fc_medium' tf_model_dir = 'final_vitis/float/'+ model_dir+'.h5' model = tf.keras.models.load_model(tf_model_dir) input_saved_model_dir = 'final_jetson/'+ model_dir+'/tf/' os.makedirs(input_saved_model_dir, exist_ok=True) tf.saved_model.save(model,input_saved_model_dir) output_saved_model_dir = 'final_jetson/'+ model_dir+'/tf_trt/' os.makedirs(output_saved_model_dir, exist_ok=True) from tensorflow.python.compiler.tensorrt import trt_convert as trt converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir) converter.convert() converter.save(output_saved_model_dir) Traceback (most recent call last): File \"app_jetson_tensorRT.py\", line 21, in &lt;module&gt; converter.convert() File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1216, in convert self._input_saved_model_tags) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 900, in load result = load_internal(export_dir, tags, options)[\"root\"] File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 939, in load_internal ckpt_options, options, filters) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 166, in __init__ self._restore_checkpoint() File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 495, in _restore_checkpoint load_status.assert_existing_objects_matched() File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\", line 831, in assert_existing_objects_matched (list(unused_python_objects),)) AssertionError: Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [&lt;tf.Variable 'dense_4_m/bias/v:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt;, &lt;tf.Variable 'dense_3_m/bias/m:0' shape=(128,) dtype=float32, numpy= array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt;, &lt;tf.Variable 'dense_1_m/kernel/m:0' shape=(784, 256) dtype=float32, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;, &lt;tf.Variable 'dense_1_m/kernel/v:0' shape=(784, 256) dtype=float32, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;, &lt;tf.Variable 'dense_4_m/bias/m:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt;, &lt;tf.Variable 'dense_3_m/kernel/v:0' shape=(128, 128) dtype=float32, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;, &lt;tf.Variable 'dense_2_m/kernel/m:0' shape=(256, 128) dtype=float32, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;, &lt;tf.Variable 'dense_1_m/bias/v:0' shape=(256,) dtype=float32, numpy= array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt;, &lt;tf.Variable 'dense_2_m/bias/m:0' shape=(128,) dtype=float32, numpy= array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt;, &lt;tf.Variable 'dense_3_m/bias/v:0' shape=(128,) dtype=float32, numpy= array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt;, &lt;tf.Variable 'dense_4_m/kernel/m:0' shape=(128, 10) dtype=float32, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;, &lt;tf.Variable 'dense_2_m/kernel/v:0' shape=(256, 128) dtype=float32, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;, &lt;tf.Variable 'dense_3_m/kernel/m:0' shape=(128, 128) dtype=float32, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;, &lt;tf.Variable 'dense_4_m/kernel/v:0' shape=(128, 10) dtype=float32, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;, &lt;tf.Variable 'dense_1_m/bias/m:0' shape=(256,) dtype=float32, numpy= array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt;, &lt;tf.Variable 'dense_2_m/bias/v:0' shape=(128,) dtype=float32, numpy= array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt;] WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.",
        "answers": [
            [
                "I experienced the same problem while using Nvidia's Docker containers. This issue was resolved by changing the Docker image I was using. A few Docker images were compatible with the Jetson Nano release I had. In my case, this image resolved the problem: nvcr.io/nvidia/l4t-tensorflow:r32.6.1-tf2.5-py3 If you aren't utilising Docker images, you could experiment with them to find the best combination of software packages and then reproduce that."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am working on Nvidia Jetson AGX Xavier within Dockerized container\u2026I want to take input from RTSP stream\u2026it\u2019s encoding type is H264 and .avi video input.The input stream frame size is 1920x1080 (in code I am resizing that into 1280x720) I have used this GStreamer pipeline cap = cv2.VideoCapture(\u2018rtspsrc location=\u201crtsp_link\u201d latency=200 ! queue ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink\u2019, cv2.CAP_GSTREAMER) It is able to read the frames but after reading frames, frames got completely blurred , that\u2019s why object detection is not happening.",
        "answers": [],
        "votes": []
    },
    {
        "question": "How can I make a simultaneous CPU and GPU stress test on Jetson Xavier machine (Ubuntu 18.04, Jetpack 4.6)? The only code found is https://github.com/JTHibbard/Xavier_AGX_Stress_Test with tough package incompatibility issues. It only works for CPU. Anyone can contribute with providing another code or solve the issue with the mentioned one? A python code is preferred.",
        "answers": [
            [
                "Solution found. For CPU stress test, the above link works. It needs numba package to be installed. For GPU stress test, the samples in cuda folder of the Nvidia Jetson machines can be simply and efficiently used. The samples are in the /usr/local/cuda/samples. Choose one and compile it using sudo make. The compiled test file will be accessible in /usr/local/cuda/samples/bin/aarch64/linux/release (aarch64 may differ in different architectures). Run the test and check the performances using sudo jtop in another command line."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "When ffmpeg is executed, a relocation error and no version information available appears. The device is Jetsonano and the environment is Ubuntu 18.04 and the camera is using picam2. I installed gcc version 6,7,8,9,11 and tried to run it, but I keep getting the same error I saw this and try it https://github.com/DeTeam/webcam-stream/blob/master/Tutorial.md ffmpeg ffmpeg: /usr/lib/aarch64-linux-gnu/libcdio.so.17 : no version information available (required by /usr/lib/aarch64-linux-gnu/libcdio_paranoia.so.2) ffmpeg: /usr/lib/aarch64-linux-gnu/libcdio.so.17 : no version information available (required by /usr/lib/aarch64-linux-gnu/libcdio_cdda.so.2) ffmpeg: relocation error : /usr/lib/aarch64-linux-gnu/libcdio_paranoia.so.2: symbolcdio_os_driver version CDIO_17 not defined in file libcdio.so.17 with link time reference thanks",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a jetson racecar tx2 and this is its details: NVIDIA Jetson TX2 L4T 32.2.1 [ JetPack 4.2.2 ] Ubuntu 18.04.2 LTS Kernel Version: 4.9.140-tegra CUDA 10.0.326 CUDA Architecture: 6.2 OpenCV version: 3.4.0 OpenCV Cuda: YES CUDNN: 7.5.0.56 TensorRT: 5.1.6.1 Vision Works: 1.6.0.500n VPI: NOT_INSTALLED Vulcan: 1.1.70 When is try to start teleport it gives me this error message: Device not found: IMU or VESC not found -&gt; /dev/ttyACM1 /dev/ttyACM0 I checked out ttyACM0 it doesnot exists I tried to installed ttyACM modules but it gives that module and kernel versions are different. the usb list as shown in figure:USB List anyone can help me please?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to detecting objects using yolov4. Anyway, when i run this command: ./darknet detector demo cfg/coco.data cfg/yolov4-csp.cfg yolov4-csp.weights -ext_output videoplayback.mp4 I am taking this: CUDA-version: 10020 (10020), cuDNN: 8.2.1, CUDNN_HALF=1, GPU count: 1 CUDNN_HALF=1 OpenCV version: 4.1.1 Demo 0 : compute_capability = 620, cudnn_half = 0, GPU: NVIDIA Tegra X2 net.optimized_memory = 0 mini_batch = 1, batch = 8, time_steps = 1, train = 0 layer filters size/strd(dil) input output 0 Create CUDA-stream - 0 cuDNN status Error in: file: ./src/dark_cuda.c : () : line: 176 : build time: Apr 7 2022 - 13:47:20 cuDNN Error: CUDNN_STATUS_BAD_PARAM Darknet error location: ./src/dark_cuda.c, cudnn_check_error, line #204 cuDNN Error: CUDNN_STATUS_BAD_PARAM: Permission denied How can i solve this problem?",
        "answers": [
            [
                "For me, when I met this problem is because I didn't set the CMAKE_CUDA_ARCHITECTURE variable, the default vaule was 52. But actually it matters alot, because every Nvidia GPU is with different architectures. So I looked up \"https://developer.nvidia.com/cuda-gpus\" to see \"Compute Capability\" of my GPU, then I changed it(mine is RTX3080: 86) and re-compile again, it worked."
            ],
            [
                "In my case I've resolved with CUDA and CUDNN re-installation. First at the my computer has installed several CUDA version and had many version change. And Darknet was built with 12.1 with CUDNN, but the operation env has CUDA 11.8 without CUDA. I've removed old libcudnn8 and install correct version of libcudnn8 and libcudnn8-dev then works fine. Hope this helps"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "My goal is to have (2) RTMP sources in a Picture in Picture composition, encoding it into h265 mpegts, muxing audio from only the cam1 rtmp source, then sending it to the appsink; This is how I see it in my mind, but I'm probably wrong: [Confirmed] Working (On Device) Picture in Picture Pipeline: Devices used: Camlink 4k (Sony Action Cam FDR-x3000) and Logitech c920 v4l2src device=/dev/video0 ! nvvidconv ! queue ! comp.sink_0 v4l2src device=/dev/video1 ! video/x-raw, width=800, height=448, framerate=30/1, format=YUY2 ! videobox left=-4 right=-4 top=-4 bottom=-4 ! nvvidconv ! queue ! comp.sink_1 nvcompositor name=comp sink_0::width=1920 sink_0::height=1080 sink_1::width=640 sink_1::height=360 sink_1::xpos=1266 sink_1::ypos=706 ! queue ! identity name=v_delay signal-handoffs=TRUE ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mpegtsmux name=mux ! appsink name=appsink alsasrc device=hw:2 ! identity name=a_delay signal-handoffs=TRUE ! volume volume=1.0 ! audioconvert ! opusenc bitrate=320000 ! opusparse ! queue ! mux. [Confirmed] Working RTMP Pipeline: Device used: Samsung s10e using Larix Broadcaster to stream x264 via RTMP rtmpsrc location=rtmp://127.0.0.1/live/cam1 ! flvdemux name=demux demux.video ! identity name=v_delay signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! nvvidconv ! textoverlay text='' valignment=top halignment=right font-desc=\"Sans, 10\" name=overlay ! queue ! videorate ! video/x-raw,framerate=60/1 ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mux. demux.audio ! aacparse ! avdec_aac ! identity name=a_delay signal-handoffs=TRUE ! volume volume=1.0 ! audioconvert ! opusenc bitrate=128000 ! opusparse ! queue max-size-time=10000000000 max-size-buffers=1000 ! mux. mpegtsmux name=mux ! appsink name=appsink All my attempts have failed; These are My Attempts: Attempt 1: rtmpsrc name=cam1 location=rtmp://127.0.0.1/live/cam1 ! flvdemux name=demux0 ! queue ! demux0.video ! identity name=v_delay signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! nvvidconv ! queue ! comp.sink_0 rtmpsrc name=cam2 location=rtmp://127.0.0.1/live/cam2 ! flvdemux name=demux1 ! queue ! demux1.video ! identity signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! nvvidconv ! queue ! comp.sink_1 nvcompositor name=comp sink_0::xpos=0 sink_0::ypos=0 sink_0::width=1920 sink_0::height=1080 sink_1::xpos=0 sink_1::ypos=240 sink_1::width=320 sink_1::height=240 ! videorate ! video/x-raw,framerate=60/1 ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mux. demux0. ! queue ! audio/mpeg ! decodebin ! audioconvert ! audioresample ! autoaudiosink mpegtsmux name=mux ! appsink name=appsink Attempt 2: rtmpsrc name=cam1 location=rtmp://127.0.0.1/live/cam1 ! flvdemux name=demux0 demux0.video ! identity name=v_delay0 signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! nvvidconv ! queue ! comp.sink_0 rtmpsrc name=cam2 location=rtmp://127.0.0.1/live/cam2 ! flvdemux name=demux1 demux1.video ! identity name=v_delay1 signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! videobox left=-4 right=-4 top=-4 bottom=-4 ! nvvidconv ! queue ! comp.sink_1 nvcompositor name=comp sink_0::width=1920 sink_0::height=1080 sink_1::width=640 sink_1::height=360 sink_1::xpos=10 sink_1::ypos=10 ! queue ! identity name=v_delay0 signal-handoffs=TRUE ! nvvidconv interpolation-method=5 ! queue ! identity name=v_delay1 signal-handoffs=TRUE ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mpegtsmux name=mux ! appsink name=appsink demux0.audio ! aacparse ! avdec_aac ! identity name=a_delay signal-handoffs=TRUE ! volume volume=1.0 ! audioconvert ! opusenc bitrate=320000 ! opusparse ! queue max-size-time=10000000000 max-size-buffers=1000 ! mux. Current GStreamer Configuration: Update 1: I tried @SeB's Solution but it did not work: Here are some screenshots showing the process: videotestsrc on port 4953: videotestsrc on port 4954: full test pipeline: Update 2: The Solution: By Utilizing @SeB's answer and tinkering with it a bit, I was able to take two rtmpsrc's and compose them together, then send it to that same rtmp server under a different key, and use the rtmp pipeline that ships with the belacoder. During my testing this only works if you follow the belabox tutorial, and not with the pre-made image. Here is the pipeline that I used: gst-launch-1.0 -v \\ rtmpsrc location=rtmp://127.0.0.1/live/cam1 ! flvdemux name=demux0 \\ demux0. ! queue ! video/x-h264 ! h264parse ! nvv4l2decoder ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,width=1920,height=1080,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_0 \\ demux0. ! queue ! audio/mpeg ! mux. \\ rtmpsrc location=rtmp://127.0.0.1/live/cam2 ! flvdemux name=demux1 \\ demux1. ! queue ! video/x-h264 ! h264parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,format=YUY2,width=800,height=448,pixel-aspect-ratio=1/1 ! videobox left=-4 right=-4 top=-4 bottom=-4 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_1 \\ nvcompositor name=comp sink_0::xpos=0 sink_0::ypos=0 sink_0::width=1920 sink_0::height=1080 sink_0::zorder=1 sink_1::xpos=0 sink_1::ypos=0 sink_1::width=808,sink_1::height=456 sink_1::zorder=2 ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! nvvidconv ! 'video/x-raw(memory:NVMM),format=NV12' \\ ! nvv4l2h264enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h264parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mux. \\ flvmux name=mux ! rtmpsink location='location=rtmp://127.0.0.1/live/cam3 live=1' Then I just edited the rtmp pipeline that comes with belacoder to pull from /cam3. Here it is working in OBS Studio using belaUI + belacoder via SRTLA: This is the pipeline I used in belaUI/belacoder: rtmpsrc location=rtmp://127.0.0.1/live/cam3 ! flvdemux name=demux demux.video ! identity name=v_delay signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! nvvidconv ! textoverlay text='' valignment=top halignment=right font-desc=\"Sans, 10\" name=overlay ! queue ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mux. demux.audio ! aacparse ! avdec_aac ! identity name=a_delay signal-handoffs=TRUE ! volume volume=1.0 ! audioconvert ! voaacenc bitrate=128000 ! aacparse ! queue max-size-time=10000000000 max-size-buffers=1000 ! mux. mpegtsmux name=mux ! appsink name=appsink My settings are unique to the rtmp server I have running on my belabox (Jetson-nano) so keep that in mind. Here is the final pipeline selected in the belaUI: Once you have it selected all you have to do is hit start and you can utilize all of the internet connections that are connected to the belabox: Please keep in mind this is really finicky, if one of your rtmps sources crap out it ruins the whole pipeline, so this works best when all rtmp sources are in a local environment, and you have the gts-launch pipeline running as a service. If you want more information about the open-source DIY project belabox or would like to contact me, check out my profile links @ https://stackoverflow.com/users/3331416/b3ck",
        "answers": [
            [
                "Just tried simulating your sources with (I don't have a RTMP server, but should be straight forward to try adapting): # Cam 1 1920x1080@30fps with audio gst-launch-1.0 -e videotestsrc ! video/x-raw,format=NV12,width=320,height=240,framerate=30/1 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=NV12,width=1920,height=1080,pixel-aspect-ratio=1/1' ! nvv4l2h264enc ! h264parse ! queue ! flvmux name=mux audiotestsrc ! audioconvert ! voaacenc ! queue ! mux. mux. ! tcpserversink port=4953 # Cam2 with 800x448@30fps gst-launch-1.0 -e videotestsrc pattern=ball ! video/x-raw,format=NV12,width=320,height=240,framerate=30/1 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=NV12,width=800,height=448,pixel-aspect-ratio=1/1' ! nvv4l2h264enc ! h264parse ! queue ! flvmux ! tcpserversink port=4954 Then, this should output video and audio: gst-launch-1.0 -v \\ tcpclientsrc port=4953 ! flvdemux name=demux0 ! h264parse ! nvv4l2decoder ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,width=1920,height=1080,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_0 \\ tcpclientsrc port=4954 ! flvdemux name=demux1 ! h264parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,format=YUY2,width=800,height=448,pixel-aspect-ratio=1/1 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_1 \\ nvcompositor name=comp sink_0::xpos=0 sink_0::ypos=0 sink_0::width=1920 sink_0::height=1080 sink_0::zorder=1 sink_1::xpos=0 sink_1::ypos=0 sink_1::width=800,sink_1::height=448 sink_1::zorder=2 ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! nvvidconv ! autovideosink \\ demux0. ! queue ! audio/mpeg ! decodebin ! audioconvert ! audioresample ! autoaudiosink If ok, you can H265 encode composed video (note that here adding videobox the second frame will now have size 808x456) and forward mpeg audio with: gst-launch-1.0 -v \\ tcpclientsrc port=4953 ! flvdemux name=demux0 \\ demux0. ! queue ! video/x-h264 ! h264parse ! nvv4l2decoder ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,width=1920,height=1080,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_0 \\ demux0. ! queue ! audio/mpeg ! tsmux. \\ tcpclientsrc port=4954 ! flvdemux name=demux1 \\ demux1. ! queue ! video/x-h264 ! h264parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,format=YUY2,width=800,height=448,pixel-aspect-ratio=1/1 ! videobox left=-4 right=-4 top=-4 bottom=-4 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_1 \\ nvcompositor name=comp sink_0::xpos=0 sink_0::ypos=0 sink_0::width=1920 sink_0::height=1080 sink_0::zorder=1 sink_1::xpos=0 sink_1::ypos=0 sink_1::width=808,sink_1::height=456 sink_1::zorder=2 ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! nvvidconv ! 'video/x-raw(memory:NVMM),format=NV12' \\ ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! tsmux. \\ mpegtsmux name=tsmux ! appsink name=appsink"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm troubleshooting a device-tree and have come across the keywords \"fragment\" and \"fragement\" in my decompiled blob. At first, I thought it was a typo, but I see the same thing in dtb file provided by NVIDIA in the Linux for Tegra source code. I can't find any information on what a fragement is and how/if it differs from a fragment.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to capture video from a source using gstreamer pipeline and opencv. Source has properties as follows: ioctl:VIDIOC_ENUM_FMT index : 0 Type : Video capture Pixel Format : 'YUYV' Name : YUYV 4:2:2 My receiving pipeline is : ''' pipeAnalog = 'v4l2src device=/dev/video0 ! video/x-raw,format=YUY2,interlace-mode=interleaved ! videoconvert ! video/x-raw, format=(string)BGR ! videoconvert ! appsink'''' using this pipeline, im reading the image with: '''self.currentCam = cv2.VideoCapture(self.pipeAnalog, cv2.CAP_GSTREAMER)''' After these commands, regardless of how i resize the frame using cv2.resize my frame has a black bar above it as follows : Output frame Any idea how can i get rid of this black bar? Any help would be appreciated. (I'm working on Nvidia Jetson Nano with Gstreamer Core Library version 1.14.5 and OpenCV version 4.3.0) Thanks,",
        "answers": [],
        "votes": []
    },
    {
        "question": "Having issues trying to install the Jetson GPIO library. I keep getting this error WARNING: Discarding https://files.pythonhosted.org/packages/89/9e/6cc2823002a6d639217b382e8e3a06200acda7331e1dd7c91fcd31bce641/Jetson.GPIO-0.1.0.tar.gz#sha256=53334f5fa568b3cb722673cc787a310885f38ad9b33df277cf1d8ead2e31396a (from https://pypi.org/simple/jetson-gpio/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output. ERROR: Could not find a version that satisfies the requirement Jetson.GPIO (from versions: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 1.0.0, 1.0.1, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.0.10, 2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 2.0.16, 2.0.17) ERROR: No matching distribution found for Jetson.GPIO Upgrading pip to 22 just gives me a: metadata-generation-failed python error I have also tried different versions of python(3, 3.6, 3.8, 3.9, 3.10) but none seem to work.",
        "answers": [
            [
                "If I had read the whole documentation I would have noticed that the package is meant for a Linux system. I fixed my problem by opening up ubuntu and running sudo pip install Jetson.GPIO This windows ??equivalent?? also works: I downloaded the Jetson.GPIO tar file from pypi and extracted it. Then I cd into this directory using the command line and ran this command: python setup.py install I got an error that said ValueError: path 'lib/python/' cannot end with '/' I fixed this by editing the setup.py file and removing the '/'. After running the command again it worked."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "On my Jetson NX, I like to set a yaml file that can mount 2 cameras to pod, the yaml: containers: - name: my-pod image: my_image:v1.0.0 imagePullPolicy: Always volumeMounts: - mountPath: /dev/video0 name: dev-video0 - mountPath: /dev/video1 name: dev-video1 resources: limits: nvidia.com/gpu: 1 ports: - containerPort: 9000 command: [ \"/bin/bash\"] args: [\"-c\", \"while true; do echo hello; sleep 10;done\"] securityContext: privileged: true volumes: - hostPath: path: /dev/video0 type: \"\" name: dev-video0 - hostPath: path: /dev/video1 type: \"\" name: dev-video1 but when I deploy it as pod, get the error: MountVolume.SetUp failed for volume \"default-token-c8hm5\" : failed to sync secret cache: timed out waiting for the condition I had tried to remove volumes in yaml, and the pod can be successfully deployed. Any comments on this issue? Another issue is that when there is a pod got some issues, it will consume the rest of my storage of my Jetson NX, I guess maybe k8s will make lots of temporary files or logs...? when something wrong happening, any solution to this issue, otherwise all od my pods will be evicted...",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am really new to GStreamer and DeepStream. I have a pipeline created based on deepstream-test1. This is the order in which the elements appeared: filesrc (an h264) -&gt; h264parse -&gt; nvv4l2decoder -&gt; nvstreammux -&gt; nvinfer -&gt; nvvideoconvert -&gt; nvdsosd -&gt; nvegltransform -&gt; nveglglessink This works fine as intended. Heres the code: import sys sys.path.append('../') import gi gi.require_version('Gst', '1.0') from gi.repository import GObject, Gst from common.is_aarch_64 import is_aarch64 from common.bus_call import bus_call import pyds PGIE_CLASS_ID_VEHICLE = 0 PGIE_CLASS_ID_BICYCLE = 1 PGIE_CLASS_ID_PERSON = 2 PGIE_CLASS_ID_ROADSIGN = 3 def osd_sink_pad_buffer_probe(pad,info,u_data): frame_number=0 #Intiallizing object counter with 0. obj_counter = { PGIE_CLASS_ID_VEHICLE:0, PGIE_CLASS_ID_PERSON:0, PGIE_CLASS_ID_BICYCLE:0, PGIE_CLASS_ID_ROADSIGN:0 } num_rects=0 gst_buffer = info.get_buffer() if not gst_buffer: print(\"Unable to get GstBuffer \") return # Retrieve batch metadata from the gst_buffer # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the # C address of gst_buffer as input, which is obtained with hash(gst_buffer) batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer)) l_frame = batch_meta.frame_meta_list while l_frame is not None: try: # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta # The casting is done by pyds.glist_get_nvds_frame_meta() # The casting also keeps ownership of the underlying memory # in the C code, so the Python garbage collector will leave # it alone. #frame_meta = pyds.glist_get_nvds_frame_meta(l_frame.data) frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data) except StopIteration: break frame_number=frame_meta.frame_num num_rects = frame_meta.num_obj_meta l_obj=frame_meta.obj_meta_list while l_obj is not None: try: # Casting l_obj.data to pyds.NvDsObjectMeta #obj_meta=pyds.glist_get_nvds_object_meta(l_obj.data) obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data) except StopIteration: break obj_counter[obj_meta.class_id] += 1 obj_meta.rect_params.border_color.set(0.0, 0.0, 1.0, 0.0) try: l_obj=l_obj.next except StopIteration: break # Acquiring a display meta object. The memory ownership remains in # the C code so downstream plugins can still access it. Otherwise # the garbage collector will claim it when this probe function exits. display_meta=pyds.nvds_acquire_display_meta_from_pool(batch_meta) display_meta.num_labels = 1 py_nvosd_text_params = display_meta.text_params[0] # Setting display text to be shown on screen # Note that the pyds module allocates a buffer for the string, and the # memory will not be claimed by the garbage collector. # Reading the display_text field here will return the C address of the # allocated string. Use pyds.get_string() to get the string content. py_nvosd_text_params.display_text = \"Frame Number={} Number of Objects={} Vehicle_count={} Person_count={}\".format(frame_number, num_rects, obj_counter[PGIE_CLASS_ID_VEHICLE], obj_counter[PGIE_CLASS_ID_PERSON]) # Now set the offsets where the string should appear py_nvosd_text_params.x_offset = 10 py_nvosd_text_params.y_offset = 12 # Font , font-color and font-size py_nvosd_text_params.font_params.font_name = \"Serif\" py_nvosd_text_params.font_params.font_size = 10 # set(red, green, blue, alpha); set to White py_nvosd_text_params.font_params.font_color.set(1.0, 1.0, 1.0, 1.0) # Text background color py_nvosd_text_params.set_bg_clr = 1 # set(red, green, blue, alpha); set to Black py_nvosd_text_params.text_bg_clr.set(0.0, 0.0, 0.0, 1.0) # Using pyds.get_string() to get display_text as string print(pyds.get_string(py_nvosd_text_params.display_text)) pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta) try: l_frame=l_frame.next except StopIteration: break return Gst.PadProbeReturn.OK def main(args): # Check input arguments if len(args) != 2: sys.stderr.write(\"usage: %s &lt;media file or uri&gt;\\n\" % args[0]) sys.exit(1) # Standard GStreamer initialization GObject.threads_init() Gst.init(None) # Create gstreamer elements # Create Pipeline element that will form a connection of other elements print(\"Creating Pipeline \\n \") pipeline = Gst.Pipeline() if not pipeline: sys.stderr.write(\" Unable to create Pipeline \\n\") # Source element for reading from the file print(\"Creating Source \\n \") source = Gst.ElementFactory.make(\"filesrc\", \"file-source\") if not source: sys.stderr.write(\" Unable to create Source \\n\") # Since the data format in the input file is elementary h264 stream, # we need a h264parser print(\"Creating H264Parser \\n\") h264parser = Gst.ElementFactory.make(\"h264parse\", \"h264-parser\") if not h264parser: sys.stderr.write(\" Unable to create h264 parser \\n\") # Use nvdec_h264 for hardware accelerated decode on GPU print(\"Creating Decoder \\n\") decoder = Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\") if not decoder: sys.stderr.write(\" Unable to create Nvv4l2 Decoder \\n\") # Create nvstreammux instance to form batches from one or more sources. streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\") if not streammux: sys.stderr.write(\" Unable to create NvStreamMux \\n\") # Use nvinfer to run inferencing on decoder's output, # behaviour of inferencing is set through config file pgie = Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\") if not pgie: sys.stderr.write(\" Unable to create pgie \\n\") # Use convertor to convert from NV12 to RGBA as required by nvosd nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\") if not nvvidconv: sys.stderr.write(\" Unable to create nvvidconv \\n\") # Create OSD to draw on the converted RGBA buffer nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\") if not nvosd: sys.stderr.write(\" Unable to create nvosd \\n\") # Finally render the osd output if is_aarch64(): transform = Gst.ElementFactory.make(\"nvegltransform\", \"nvegl-transform\") print(\"Creating EGLSink \\n\") sink = Gst.ElementFactory.make(\"nveglglessink\", \"nvvideo-renderer\") if not sink: sys.stderr.write(\" Unable to create egl sink \\n\") print(\"Playing file %s \" %args[1]) source.set_property('location', args[1]) streammux.set_property('width', 1920) streammux.set_property('height', 1080) streammux.set_property('batch-size', 1) streammux.set_property('batched-push-timeout', 4000000) pgie.set_property('config-file-path', \"dstest1_pgie_config.txt\") print(\"Adding elements to Pipeline \\n\") pipeline.add(source) pipeline.add(h264parser) pipeline.add(decoder) pipeline.add(streammux) pipeline.add(pgie) pipeline.add(nvvidconv) pipeline.add(nvosd) pipeline.add(sink) if is_aarch64(): pipeline.add(transform) # we link the elements together # file-source -&gt; h264-parser -&gt; nvh264-decoder -&gt; # nvinfer -&gt; nvvidconv -&gt; nvosd -&gt; video-renderer print(\"Linking elements in the Pipeline \\n\") source.link(h264parser) h264parser.link(decoder) sinkpad = streammux.get_request_pad(\"sink_0\") if not sinkpad: sys.stderr.write(\" Unable to get the sink pad of streammux \\n\") srcpad = decoder.get_static_pad(\"src\") if not srcpad: sys.stderr.write(\" Unable to get source pad of decoder \\n\") srcpad.link(sinkpad) streammux.link(pgie) pgie.link(nvvidconv) nvvidconv.link(nvosd) if is_aarch64(): nvosd.link(transform) transform.link(sink) else: nvosd.link(sink) # create an event loop and feed gstreamer bus mesages to it loop = GObject.MainLoop() bus = pipeline.get_bus() bus.add_signal_watch() bus.connect (\"message\", bus_call, loop) # Lets add probe to get informed of the meta data generated, we add probe to # the sink pad of the osd element, since by that time, the buffer would have # had got all the metadata. osdsinkpad = nvosd.get_static_pad(\"sink\") if not osdsinkpad: sys.stderr.write(\" Unable to get sink pad of nvosd \\n\") osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0) # start play back and listen to events print(\"Starting pipeline \\n\") pipeline.set_state(Gst.State.PLAYING) try: loop.run() except: pass # cleanup pipeline.set_state(Gst.State.NULL) if __name__ == '__main__': sys.exit(main(sys.argv)) But whenever i try to replace nvegltransform -&gt; nveglglessink of the pipeline with ximagesink i get an error saying: Error: gst-stream-error-quark: Internal data stream error. (1): /dvs/git/dirty/git-master_linux/deepstream/sdk/src/gst-plugins/gst-nvinfer/gstnvinfer.cpp(2288): gst_nvinfer_output_loop (): /GstPipeline:pipeline0/GstNvInfer:primary-inference: streaming stopped, reason not-linked (-1) I needed to remove nveglglessink from the pipeline originally, but as nvegltransform is related to nveglglessink i decided to remove both. And in place of them i used ximagesink. This is the pipeline i am working on (which gives the error mentioned): filesrc (an h264) -&gt; h264parse -&gt; nvv4l2decoder -&gt; nvstreammux -&gt; nvinfer -&gt; nvvideoconvert -&gt; nvdsosd -&gt; ximagesink Heres the code: import sys sys.path.append('../') import gi gi.require_version('Gst', '1.0') from gi.repository import GObject, Gst from common.is_aarch_64 import is_aarch64 from common.bus_call import bus_call import pyds PGIE_CLASS_ID_VEHICLE = 0 PGIE_CLASS_ID_BICYCLE = 1 PGIE_CLASS_ID_PERSON = 2 PGIE_CLASS_ID_ROADSIGN = 3 def osd_sink_pad_buffer_probe(pad,info,u_data): frame_number=0 #Intiallizing object counter with 0. obj_counter = { PGIE_CLASS_ID_VEHICLE:0, PGIE_CLASS_ID_PERSON:0, PGIE_CLASS_ID_BICYCLE:0, PGIE_CLASS_ID_ROADSIGN:0 } num_rects=0 gst_buffer = info.get_buffer() if not gst_buffer: print(\"Unable to get GstBuffer \") return # Retrieve batch metadata from the gst_buffer # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the # C address of gst_buffer as input, which is obtained with hash(gst_buffer) batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer)) l_frame = batch_meta.frame_meta_list while l_frame is not None: try: # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta # The casting is done by pyds.glist_get_nvds_frame_meta() # The casting also keeps ownership of the underlying memory # in the C code, so the Python garbage collector will leave # it alone. #frame_meta = pyds.glist_get_nvds_frame_meta(l_frame.data) frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data) except StopIteration: break frame_number=frame_meta.frame_num num_rects = frame_meta.num_obj_meta l_obj=frame_meta.obj_meta_list while l_obj is not None: try: # Casting l_obj.data to pyds.NvDsObjectMeta #obj_meta=pyds.glist_get_nvds_object_meta(l_obj.data) obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data) except StopIteration: break obj_counter[obj_meta.class_id] += 1 obj_meta.rect_params.border_color.set(0.0, 0.0, 1.0, 0.0) try: l_obj=l_obj.next except StopIteration: break # Acquiring a display meta object. The memory ownership remains in # the C code so downstream plugins can still access it. Otherwise # the garbage collector will claim it when this probe function exits. display_meta=pyds.nvds_acquire_display_meta_from_pool(batch_meta) display_meta.num_labels = 1 py_nvosd_text_params = display_meta.text_params[0] # Setting display text to be shown on screen # Note that the pyds module allocates a buffer for the string, and the # memory will not be claimed by the garbage collector. # Reading the display_text field here will return the C address of the # allocated string. Use pyds.get_string() to get the string content. py_nvosd_text_params.display_text = \"Frame Number={} Number of Objects={} Vehicle_count={} Person_count={}\".format(frame_number, num_rects, obj_counter[PGIE_CLASS_ID_VEHICLE], obj_counter[PGIE_CLASS_ID_PERSON]) # Now set the offsets where the string should appear py_nvosd_text_params.x_offset = 10 py_nvosd_text_params.y_offset = 12 # Font , font-color and font-size py_nvosd_text_params.font_params.font_name = \"Serif\" py_nvosd_text_params.font_params.font_size = 10 # set(red, green, blue, alpha); set to White py_nvosd_text_params.font_params.font_color.set(1.0, 1.0, 1.0, 1.0) # Text background color py_nvosd_text_params.set_bg_clr = 1 # set(red, green, blue, alpha); set to Black py_nvosd_text_params.text_bg_clr.set(0.0, 0.0, 0.0, 1.0) # Using pyds.get_string() to get display_text as string print(pyds.get_string(py_nvosd_text_params.display_text)) pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta) try: l_frame=l_frame.next except StopIteration: break return Gst.PadProbeReturn.OK def main(args): # Check input arguments if len(args) != 2: sys.stderr.write(\"usage: %s &lt;media file or uri&gt;\\n\" % args[0]) sys.exit(1) # Standard GStreamer initialization GObject.threads_init() Gst.init(None) # Create gstreamer elements # Create Pipeline element that will form a connection of other elements print(\"Creating Pipeline \\n \") pipeline = Gst.Pipeline() if not pipeline: sys.stderr.write(\" Unable to create Pipeline \\n\") # Source element for reading from the file print(\"Creating Source \\n \") source = Gst.ElementFactory.make(\"filesrc\", \"file-source\") if not source: sys.stderr.write(\" Unable to create Source \\n\") # Since the data format in the input file is elementary h264 stream, # we need a h264parser print(\"Creating H264Parser \\n\") h264parser = Gst.ElementFactory.make(\"h264parse\", \"h264-parser\") if not h264parser: sys.stderr.write(\" Unable to create h264 parser \\n\") # Use nvdec_h264 for hardware accelerated decode on GPU print(\"Creating Decoder \\n\") decoder = Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\") if not decoder: sys.stderr.write(\" Unable to create Nvv4l2 Decoder \\n\") # Create nvstreammux instance to form batches from one or more sources. streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\") if not streammux: sys.stderr.write(\" Unable to create NvStreamMux \\n\") # Use nvinfer to run inferencing on decoder's output, # behaviour of inferencing is set through config file pgie = Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\") if not pgie: sys.stderr.write(\" Unable to create pgie \\n\") # Use convertor to convert from NV12 to RGBA as required by nvosd nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\") if not nvvidconv: sys.stderr.write(\" Unable to create nvvidconv \\n\") # Create OSD to draw on the converted RGBA buffer nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\") if not nvosd: sys.stderr.write(\" Unable to create nvosd \\n\") # Finally render the osd output # if is_aarch64(): # transform = Gst.ElementFactory.make(\"nvegltransform\", \"nvegl-transform\") # print(\"Creating EGLSink \\n\") # sink = Gst.ElementFactory.make(\"nveglglessink\", \"nvvideo-renderer\") # if not sink: # sys.stderr.write(\" Unable to create egl sink \\n\") print(\"Creating XIMAGESINK \\n\") ximagesink = Gst.ElementFactory.make(\"ximagesink\", \"video-sink\") if not ximagesink: sys.stderr.write(\" Unable to create ximagesink \\n\") print(\"Playing file %s \" %args[1]) source.set_property('location', args[1]) streammux.set_property('width', 1920) streammux.set_property('height', 1080) streammux.set_property('batch-size', 1) streammux.set_property('batched-push-timeout', 4000000) pgie.set_property('config-file-path', \"dstest1_pgie_config.txt\") print(\"Adding elements to Pipeline \\n\") pipeline.add(source) pipeline.add(h264parser) pipeline.add(decoder) pipeline.add(streammux) pipeline.add(pgie) pipeline.add(nvvidconv) pipeline.add(nvosd) pipeline.add(ximagesink) print(\"Linking elements in the Pipeline \\n\") source.link(h264parser) h264parser.link(decoder) # Link elements manually as streammux donot have a static sink pad decoder_srcpad = decoder.get_static_pad(\"src\") if not decoder_srcpad: sys.stderr.write(\" Unable to get source pad of decoder \\n\") streammux_sinkpad = streammux.get_request_pad(\"sink_0\") if not streammux_sinkpad: sys.stderr.write(\" Unable to get the sink pad of streammux \\n\") decoder_srcpad.link(streammux_sinkpad) streammux.link(pgie) pgie.link(nvvidconv) nvvidconv.link(nvosd) nvosd.link(ximagesink) # create an event loop and feed gstreamer bus mesages to it loop = GObject.MainLoop() bus = pipeline.get_bus() bus.add_signal_watch() bus.connect (\"message\", bus_call, loop) # Lets add probe to get informed of the meta data generated, we add probe to # the sink pad of the osd element, since by that time, the buffer would have # had got all the metadata. osdsinkpad = nvosd.get_static_pad(\"sink\") if not osdsinkpad: sys.stderr.write(\" Unable to get sink pad of nvosd \\n\") osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0) # start play back and listen to events print(\"Starting pipeline \\n\") pipeline.set_state(Gst.State.PLAYING) try: loop.run() except: pass # cleanup pipeline.set_state(Gst.State.NULL) if __name__ == '__main__': sys.exit(main(sys.argv)) Could you help me understand what I am doing wrong? Thank you. NOTE: I am using NVIDIA JETSON XAVIER, LINUX, DeepStream 6 and GStreamer 1.0",
        "answers": [
            [
                "You may use nvvideoconvert for copying from NVMM memory into system memory as expected by xvimagesink: ... nvstreammux -&gt; nvinfer -&gt; nvvideoconvert -&gt; nvdsosd -&gt; nvvideoconvert -&gt; xvimagesink"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am just looking for some advice on how to handle receiving a large number of signals in real time, storing these on a buffer and then passing the buffer to be processed by a inference engine/model on Jetson(l4t) platforms. Currently I have something along the lines of while True: for _ in read_x_times: temp_buffer = read_subset_of_samples_from_input_stream model_buffer[start:stop] = process(temp_buffer) if model_buffer_is_full: inference_engine.execute_v2(model_buffers) The issue I have is that when the model_buffer is full and I call inference_engine, I am losing some samples from the input stream. The model_buffer lives on shared memory between the gpu and cpu. What would be the best way to continue to receive samples while the model is processing the model_buffer.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to extract the motion field from the visionworks example. i saw the post in the nvidia forum here but i still do not understand how to get the motion field. The documentation says that the motion fiels is a vx_image with type NVX_DF_IMAGE2F32, this means there are two channels. Someone know the meaning of each channel? I even realize that the size of the image is the half of the input images used to detect the movement. In the nvidia post the moderator said \" that value(x, y) is the motion of the point (x, y)\", but i do not understand what this means. I understand the concept explained \"More precisely, for point(x, y) with field value(mx, my) in frame N. The matched point in the frame N-1 is (x+mx, y+my).\" but i could not match the result i get in the vx_image with these parameters.",
        "answers": [
            [
                "After some research i found the meaning. I write the solution in case is useful for someone else: Here my understanding using cv::Mat that can be easily used to draw the result. // get the motion in vx_image format vx_image MotionImage = ime.getMotionField(); //convert to cv::Mat (or GpuMat as you like...) nvx_cv::VXImageToCVMatMapper map(MotionImage); //cv::cuda::GpuMat MotionImageGpuMat = map.getGpuMat(); cv::Mat MotionImageMatMap = map.getMat(); //resize the motion map as it is the half of the original image used to compute the motion cv::Mat MotionImageMat; cv::resize(MotionImageMatMap,MotionImageMat,cv::Size(),2.0,2.0,cv::INTER_NEAREST); //split in two channels cv::Mat MotionImageMat_split[2]; cv::split(MotionImageMat, MotionImageMat_split); //MotionImageMat_split[0](x,y) -&gt; contains the increment for the pixel (x,y) in width (columns) direction between the current and previous frame. // Example: if MotionImageMat_split[0] at pixel (x,y) is 5 means that the difference between the current and previous frame in the width (columns) direction is 5 //MotionImageMat_split[1](x,y) -&gt; contains the increment for the pixel (x,y) in height (rows) direction between the current and previous frame. // Example: if MotionImageMat_split[0] at pixel (x,y) is 5 means that the difference between the current and previous frame in the height (rows) direction is 5"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to use yolov5 at my Jetson AGX Xavier developer kit and I have to upgrade matplotlib to version 3.3.4 highest version that python3.6 support. I'm using python version 3.6.9(default of Jetson AGX Xavier) and python3.6 support matplotlib version 3.3.4. But I CANNOT upgrade that over 2.1.1(and this version is default also). I've upgrade setuptools and I tried all command I can. $ sudo apt-get install python3-matplotlib $ python3 -m pip install --upgrade matplotlib $ python3 -m pip install matplotlib==3.3.4 How can I solve this problem? Error code and image is here. Defaulting to user installation because normal site-packages is not writeable Collecting matplotlib==3.3.4 Using cached matplotlib-3.3.4.tar.gz (37.9 MB) Preparing metadata (setup.py) ... error ERROR: Command errored out with exit status -4: command: /usr/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-tk3o5hkn/matplotlib_53b7f655efb14a6a9d86b117497e1927/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-tk3o5hkn/matplotlib_53b7f655efb14a6a9d86b117497e1927/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-sz1ykkr8 cwd: /tmp/pip-install-tk3o5hkn/matplotlib_53b7f655efb14a6a9d86b117497e1927/ Complete output (19 lines): Edit setup.cfg to change the build options; suppress output with --quiet. BUILDING MATPLOTLIB matplotlib: yes [3.3.4] python: yes [3.6.9 (default, Dec 8 2021, 21:08:43) [GCC 8.4.0]] platform: yes [linux] sample_data: yes [installing] tests: no [skipping due to configuration] macosx: no [Mac OS-X only] running egg_info creating /tmp/pip-pip-egg-info-sz1ykkr8/matplotlib.egg-info writing /tmp/pip-pip-egg-info-sz1ykkr8/matplotlib.egg-info/PKG-INFO writing dependency_links to /tmp/pip-pip-egg-info-sz1ykkr8/matplotlib.egg-info/dependency_links.txt writing namespace_packages to /tmp/pip-pip-egg-info-sz1ykkr8/matplotlib.egg-info/namespace_packages.txt writing requirements to /tmp/pip-pip-egg-info-sz1ykkr8/matplotlib.egg-info/requires.txt writing top-level names to /tmp/pip-pip-egg-info-sz1ykkr8/matplotlib.egg-info/top_level.txt writing manifest file '/tmp/pip-pip-egg-info-sz1ykkr8/matplotlib.egg-info/SOURCES.txt' ---------------------------------------- WARNING: Discarding https://files.pythonhosted.org/packages/22/d4/e7ca532e68a9357742604e1e4ae35d9c09a4a810de39a9d80402bd12f50f/matplotlib-3.3.4.tar.gz#sha256=3e477db76c22929e4c6876c44f88d790aacdf3c3f8f3a90cb1975c0bf37825b0 (from https://pypi.org/simple/matplotlib/) (requires-python:&gt;=3.6). Command errored out with exit status -4: python setup.py egg_info Check the logs for full command output. ERROR: Could not find a version that satisfies the requirement matplotlib==3.3.4 (from versions: 0.86, 0.86.1, 0.86.2, 0.91.0, 0.91.1, 1.0.1, 1.1.0, 1.1.1, 1.2.0, 1.2.1, 1.3.0, 1.3.1, 1.4.0, 1.4.1rc1, 1.4.1, 1.4.2, 1.4.3, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 2.0.0b1, 2.0.0b2, 2.0.0b3, 2.0.0b4, 2.0.0rc1, 2.0.0rc2, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.2.0rc1, 2.2.0, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 3.0.0rc2, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0rc1, 3.1.0rc2, 3.1.0, 3.1.1, 3.1.2, 3.1.3, 3.2.0rc1, 3.2.0rc3, 3.2.0, 3.2.1, 3.2.2, 3.3.0rc1, 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4) ERROR: No matching distribution found for matplotlib==3.3.4 error",
        "answers": [
            [
                "I solved this problem!!!! This line was added to bashrc and the problem was solved. export OPENBLAS_CORETYPE=ARMV8 python3"
            ],
            [
                "I had the same problem on the JetPack 4.6.1 SD card image for Nvidia Jetson Nano. Unfortunately SJ Moon's answer didn't work for me. However, I figured out that I needed to upgrade the setuptools module. After upgrading it I could install matplotlib. Please note that I am using a virtual environment for my Python application. I created a virtual environment so that my python application is independent from the global python environment python3 -m venv .venv # create virtual environment source .venv/bin/activate # activate virtual environment I upgraded pip because JetPack 4.6.1 comes with an old pip version and matplotlib requires pip &gt;= 9.0.1 pip3 install --upgrade pip Then I upgraded setuptools pip3 install --upgrade setuptools Then I was able to install matplotlib pip3 install matplotlib Also please note for this approach: You will also use numpy if you use matplotlib. If you install numpy (pip3 install numpy) for Python 3.6, you will get numpy 1.19.5. Importing numpy (python3 -c \"import numpy\") will result in Illegal instruction (core dumped). This is because of an issue in numpy 1.19.5 for arm64 (see this GitHub issue). You can avoid this issue by installing another numpy version like 1.19.4 pip3 install numpy==1.19.4 or by setting OPENBLAS_CORETYPE=ARMv8, as suggested by SJ Moon and in the GitHub issue export OPENBLAS_CORETYPE=ARMV8 pip3 install numpy I didn't try the latter one, but this could be useful if you require numpy 1.19.5 for some reason."
            ],
            [
                "It seems that your python packages have broken, There can be two ways to install matplotlib Way-1: Try to install matplotlib in venv Way-2: Uninstall python and then reinstall python again. Note: In Second way, don't restart jetson until new python installed completely, because python3 is already comes with jetson, so if you will uninstall it and then restart jetson, it will not work fine."
            ],
            [
                "Adding export OPENBLAS_CORETYPE=ARMV8 To my ~/.bash_rc worked for me as well."
            ],
            [
                "Execute the command export OPENBLAS_CORETYPE=ARMV8 and then run pip install matplotlib"
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001,
            1e-07,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I'm trying to build PyTorch from a .whl file on a jetson nano. I am able to build and install the file but only while using sudo, attempting to pip install the file without sudo results in this error: ERROR: torch-1.10.0a0+git36449ea-cp36-cp36m-linux_aarch64.whl is not a supported wheel on this platform. This is strange as with admin I have no issues installing this file, but I can then only use the library by using the sudo command before going into or running with the python command. I should note that this is in a conda environment, but even in the base conda environment this issue still occurs. It seems like I can also install the package by using conda deactivate to deactivate conda. I am using Python 3.7 in the conda environment and Python 3.6 outside.",
        "answers": [
            [
                "I am using Python 3.7 in the conda environment and Python 3.6 outside. This is the issue. You have a cp36 whl file, so python 3.6. I am suspecting that when you run sudo pip, your systems pip is invoked, whereas when you run pip, then pip from your conda env is used, and cannot install a python 3.6 whl to a python 3.7 env. Either you need to get the cp37 whl or create a conda env that has python 3.6 installed"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am unable to install any packages with miniforge 3 (conda 4.11.0). I am attempting this on a Jetson Nano Developer Kit running Jetpack. Initially it had conda installed but it seems to have gone missing, so I decided to reinstall conda. It looks like the base version of anaconda/miniconda is having issues running on ARM processors, and so I downloaded miniforge which apparently is working. I have set up an environment successfully, but attempting to download pytorch gives the following error: Collecting package metadata (current_repodata.json): done Solving environment: failed with initial frozen solve. Retrying with flexible solve. Collecting package metadata (repodata.json): done Solving environment: failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels: - pytorch Current channels: - https://conda.anaconda.org/pytorch/linux-aarch64 - https://conda.anaconda.org/pytorch/noarch - https://conda.anaconda.org/abinit/linux-aarch64 - https://conda.anaconda.org/abinit/noarch - https://conda.anaconda.org/matsci/linux-aarch64 - https://conda.anaconda.org/matsci/noarch - https://conda.anaconda.org/conda-forge/linux-aarch64 - https://conda.anaconda.org/conda-forge/noarch To search for alternate channels that may provide the conda package you're looking for, navigate to https://anaconda.org and use the search bar at the top of the page. This is for Python 3.7.12. It seems this issue persists no matter what version of pytorch I try to install. I am however able to install some other packages, as I was able to install beautifulsoup4.",
        "answers": [
            [
                "There is no linux-aarch64 version of pytorch on the default conda channel, see here This is of course package specific. E.g. there is a linux-aarch64 version of beautifulsoup4 which is why you wre able to install it without an issue. You can try to install from a different channel that claims to provide a pytorch for aarch64, e.g. conda install -c kumatea pytorch"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a JSON file that looks something like this: \"{\"\"audio_filepath\"\": \"\"/content/drive/MyDrive/nemo_commonvoice/wav_files/wav_train/sample-000001.wav\"\", \"\"duration\"\": 2.232, \"\"text\"\": \"\"it took her a while to get used to it\"\"}\" I want the extra \" to be removed from each of the lines so that my output looks something like this: - The original JSON format {\"audio_filepath\": \"/content/drive/MyDrive/nemo_commonvoice/wav_files/wav_train/sample-000001.wav\", \"duration\": 2.232, \"text\": \"it took her a while to get used to it\"} I require a function that can do the string manipulation for me or at least some hint on how to fix it. For my function which parses the entire JSON file is: def create_manifest(data: List[tuple], output_name: str, manifest_path: str): \"Takes in the data, which is a tsv file and parses its columns to create a JSON file\" output_file = Path(manifest_path)/output_name output_file.parent.mkdir(exist_ok=True, parents=True) with output_file.open(mode='w') as f: for wav_path,duration,text in tqdm(data,total=len(data)): f.write( json.dumps({ 'audio_filepath': wav_path, \"duration\": duration, 'text': text }) + '\\n' ) Any help will be beneficial.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am running a gstreamer pipeline in a jetson xavier NX and streaming a 4k live stream over udp to a server. I am running a shell script which runs the pipeline directly using CLI. When the connection breaks and the stream cuts, the pipeline says 'network is unreachable`. However as the network resets itself soon and i want the pipeline to restart. How can i find out if the pipeline has stopped and restart it? The pipeline stops but the process continues running and it does not restart on its own. I want to restart the process if the pipeline breaks.",
        "answers": [
            [
                "You may try the following for sender: Here using videotestsrc at low resolution, rescaling with HW into 4K in NVMM memory for H264 encoding and RTP/UDP multicast streaming : gst-launch-1.0 -ev videotestsrc ! video/x-raw,width=320,height=240,framerate=30/1 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=NV12,width=3840,height=2160' ! nvv4l2h264enc insert-sps-pps=1 ! h264parse ! rtph264pay config-interval=1 ! udpsink port=5000 host=224.1.1.1 Receiver: gst-launch-1.0 -ev udpsrc port=5000 multicast-group=224.1.1.1 ! application/x-rtp,encoding-name=H264 ! rtpjitterbuffer latency=500 ! rtph264depay ! h264parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,width=1920,height=1080 ! fpsdisplaysink text-overlay=0 video-sink=xvimagesink It may take a few seconds to connect and display after starting or restarting, but seems restarting fine after network connection stopped and then again available, but this was only tested on a single AGX Xavier being both sender and receiver and using Network manager to disconnect/reconnect. Other cases over networks may be more complex."
            ],
            [
                "The proper way is to write your own application instead of using gst-launch as already suggested. The learning curve for this is pretty steep so the alternative is to monitor the stderr output and parse the messages to find \"network is unreachable\" information, kill the the old process and relaunch gst-launch."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I have one applications in c++ to get the video using gstreamer from a camera and then send the video via UDP to another application in c++ that gets the video and makes the restreaming using webrct. Everything under a jetson AGX. If i get the data from the camera in H264 and send it dirrectlly the videos works perfect in 4k: First pipeline to get the video pipe_source = \"rtspsrc location=rtsp://192.168.1.162/z3-1.mp4 ! application/x-rtp,encoding-name=H264,profile=baseline ! \"; pipe_sink = \"udpsink host=224.1.1.1 port=5000 sync=false auto-multicast=true\"; launch_pipeline = pipe_source + pipe_sink; Second pipeline to get the video and send it via webrtc pipeline = \"udpsrc multicast-group=224.1.1.1 auto-multicast=true port=5000 ! application/x-rtp,encoding-name=H264,profile=baseline,media=video,clock-rate=90000,payload=96 ! webrtcbin async-handling=true name=sendrecv\"; However I can not do it in 4K if i want to make some precessing in the input video as i need to decode (and then encode) the frames prior sending the video by udp pipe_source = \"rtspsrc location=rtsp://192.168.1.162/z3-1.mp4 ! application/x-rtp,encoding-name=H265 !\"; pipe_decode = \"rtph265depay ! video/x-h265 ! nvv4l2decoder enable-max-performance=true ! \"; pipe_process = \"nvvidconv output-buffers=5 name=myconv ! video/x-raw(memory:NVMM), format=RGBA ! nvvidconv output-buffers=5 ! video/x-raw(memory:NVMM), format=NV12 ! queue max-size-bytes=0 max-size-time=500 !\"; pipe_encode =\"nvv4l2vp9enc maxperf-enable=true ! video/x-vp9 ! rtpvp9pay !\"; pipe_sink = \"udpsink host=224.1.1.1 port=5000 sync=false auto-multicast=true\"; launch_pipeline = pipe_source + pipe_decode + pipe_process + pipe_encode + pipe_sink; In this pipeline for the source i have tried both h264/h265. Morevoere, for the encode I have tried using h264 instead of VP9, but it looks like H264 is much more slower. This is why i have used VP9 in the encoding part. In this case the second pipeline is: pipeline = \"udpsrc multicast-group=224.1.1.1 auto-multicast=true port=5000 ! application/x-rtp,media=video,clock-rate=90000,encoding-name=VP9,payload=96, framerate=25/1 ! queue max-size-bytes=0 max-size-time=0 ! webrtcbin async-handling=true name=sendrecv\"; My problem is that with this configuration i can not get video in 4k with good quality. I get the video but in a poor quality, i assume that the VP9 is changing the bitrate to have a continous video without losing frames. I have tried by giving the bit rate in the encoding part, this makes an improvement of the image quaity but i lose some frames. If i use 1080 then i get the video in a good quality, therefore i have the feeling that is a matter of the processing capability of the hardware (i am using a jetson AGX) on doing the decoding/encoding. Someone knows a way to improve the performance of the pipeline? I am not sure if i am doing something \"useless\" in the pipeline that is making the whole process slow for a 4k video.",
        "answers": [
            [
                "I'm unsure what is your real use case, but the following might help you to investigate further. I don't have a 4K IP cam, so here I'll simulate using a CSI Cam capturing in 1080p@30 fps and upscaling to 3840x2160 and streaming as H265 encoded with RTSP server test-launch: ./test-launch \"nvarguscamerasrc ! video/x-raw(memory:NVMM), width=1920, height=1080, framerate=30/1, format=NV12 ! nvvidconv ! video/x-raw(memory:NVMM), width=3840, height=2160, pixel-aspect-ratio=1/1 ! nvv4l2h265enc insert-vui=true insert-sps-pps=1 insert-aud=1 maxperf-enable=1 bitrate=30000000 ! h265parse ! video/x-h265, stream-format=byte-stream ! rtph265pay name=pay0 pt=96 \" Note that this encodes into H265 format with 30Mb/s bitrate. You may first check if you can get good quality image from your source and adjust source bitrate to its best. Assuming your monitor supports 1080p@30: gst-launch-1.0 rtspsrc location=rtsp://127.0.0.1:8554/test latency=500 ! application/x-rtp,encoding-name=H265 ! rtph265depay ! h265parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,width=1920,height=1080 ! xvimagesink When ok, let's go further. Here decoding RTSP H265 source and re-encoding into VP9/RTP/UDP: gst-launch-1.0 rtspsrc location=rtsp://127.0.0.1:8554/test latency=500 ! application/x-rtp,encoding-name=H265 ! rtph265depay ! h265parse ! nvv4l2decoder enable-max-performance=1 ! queue ! nvv4l2vp9enc maxperf-enable=true bitrate=30000000 ! video/x-vp9 ! rtpvp9pay ! udpsink host=224.1.1.1 port=5000 auto-multicast=true buffer-size=32000000 Note the VP9 30 Mb/s bitrate. You may have to adjust as well. For checking, you may display with (assuming X running): gst-launch-1.0 udpsrc multicast-group=224.1.1.1 auto-multicast=true port=5000 buffer-size=32000000 ! application/x-rtp,encoding-name=VP9 ! rtpjitterbuffer latency=500 ! rtpvp9depay ! video/x-vp9 ! nvv4l2decoder ! nvvidconv ! video/x-raw,width=1920,height=1080 ! xvimagesink EDIT Jan 29th, 2022: You may further try the following, seems working fine with my AGX Xavier running L4T R32.6.1: Application for reading RTSP stream with H265 video, decoding, encoding into VP9 and streaming to localhost with RTP/UDP: #include &lt;gst/gst.h&gt; int main (gint argc, gchar * argv[]) { gst_init (&amp;argc, &amp;argv); GMainLoop *loop = g_main_loop_new (NULL, FALSE); /* Create the pipeline...this will negociate unspecified caps between plugins */ const gchar *pipeline1 = \"rtspsrc location=rtsp://127.0.0.1:8554/test latency=500 ! application/x-rtp,encoding-name=H265 ! rtph265depay ! h265parse ! nvv4l2decoder enable-max-performance=1 ! queue ! nvv4l2vp9enc maxperf-enable=true bitrate=30000000 ! video/x-vp9 ! rtpvp9pay ! udpsink host=127.0.0.1 port=5000 auto-multicast=0 buffer-size=32000000 \"; GstElement *pipeline = gst_parse_launch (pipeline1, NULL); if (!pipeline) { g_error (\"Failed to create pipeline\\n\"); exit(-1); } /* Ok, successfully created the pipeline, now start it */ gst_element_set_state (pipeline, GST_STATE_READY); gst_element_set_state (pipeline, GST_STATE_PLAYING); /* wait until it's up and running or failed */ if (gst_element_get_state (pipeline, NULL, NULL, -1) == GST_STATE_CHANGE_FAILURE) { g_error (\"Failed to go into PLAYING state\"); exit(-2); } g_print (\"Running ...\\n\"); g_main_loop_run (loop); return 0; } Build with: gcc -Wall -o gst_testlaunch1 -I/usr/include/gstreamer-1.0 -I/usr/include/glib-2.0 -I/usr/lib/aarch64-linux-gnu/glib-2.0/include gst_testlaunch1.cpp -lgstreamer-1.0 -lgobject-2.0 -lglib-2.0 Application for reading VP9 encoded video from RTP/UDP on localhost, decoding and rescaling with to 1080p nvvidconv then displaying in X while measuring fps: #include &lt;gst/gst.h&gt; int main (gint argc, gchar * argv[]) { gst_init (&amp;argc, &amp;argv); GMainLoop *loop = g_main_loop_new (NULL, FALSE); /* Create the pipeline...this will negociate unspecified caps between plugins */ const gchar *pipeline2 = \"udpsrc auto-multicast=0 port=5000 buffer-size=32000000 ! application/x-rtp,encoding-name=VP9 ! rtpjitterbuffer latency=500 ! rtpvp9depay ! video/x-vp9 ! nvv4l2decoder ! nvvidconv ! video/x-raw,width=1920,height=1080 ! fpsdisplaysink video-sink=xvimagesink text-overlay=0 \"; GstElement *pipeline = gst_parse_launch (pipeline2, NULL); if (!pipeline) { g_error (\"Failed to create pipeline\\n\"); exit(-1); } // This will output changes and is required to display fps in terminal, you may remove it later to make it quiet. g_signal_connect(pipeline, \"deep-notify\", G_CALLBACK(gst_object_default_deep_notify), NULL); /* Ok, successfully created the pipeline, now start it */ gst_element_set_state (pipeline, GST_STATE_READY); gst_element_set_state (pipeline, GST_STATE_PLAYING); /* wait until it's up and running or failed */ if (gst_element_get_state (pipeline, NULL, NULL, -1) == GST_STATE_CHANGE_FAILURE) { g_error (\"Failed to go into PLAYING state\"); exit(-2); } g_print (\"Running ...\\n\"); g_main_loop_run (loop); return 0; } Build with: gcc -Wall -o gst_testlaunch2 -I/usr/include/gstreamer-1.0 -I/usr/include/glib-2.0 -I/usr/lib/aarch64-linux-gnu/glib-2.0/include gst_testlaunch2.cpp -lgstreamer-1.0 -lgobject-2.0 -lglib-2.0 Having the 4K-H265 RTSP source available, running first gst_testlaunch1 in a terminal and then gst_testlaunch2 in a second terminal shows the image with correct quality and it keeps 30 fps."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "How do I know which I2C is an actual I2C device and what are these addresses? My system/ my computer is an Nvidia Jetson AGX Xavier and I have no I2C devices plugged in to it at the time I am running all these i2cdetect commands below. I do have a Gy-521 gyro connected over I2C that I have plugged in and looked for. The problem is when this gyro is plugged in that nothing looks any different with i2cdetect from 0 through 8. I have 0 through 8 options on I2C to look for addresses. I will go through each of them with i2cdetect starting with 0. $ i2cdetect -r -y 0 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: 50 -- -- -- -- -- 56 -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- An address at 50 and 56 it would seem. $ i2cdetect -r -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- UU -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: UU UU -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- 74 -- -- -- $ i2cdetect -r -y 2 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- $ i2cdetect -r -y 3 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: ^Z^X^C When I get to 3 it hangs when it would otherwise print out the addresses. $ i2cdetect -r -y 4 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: 20 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- UU -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- UU -- -- -- 50: 50 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- UU -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- $ i2cdetect -r -y 5 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- ^C^X^Z $ i2cdetect -r -y 6 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: ^C^X^Z 5 and 6 also hang and do not print all the addresses. $ i2cdetect -r -y 7 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- UU -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- $ i2cdetect -r -y 8 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- 74 -- -- -- As I've said. There are no I2C devices plugged in and when there are I2C devices plugged in they don't look any different than this.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to implement data exchange between 3 python scripts. mp1.py responsible to start multiprocessing pipe with mp2.py in order to receive one byte of integer/per-second continuously. mp2.py holds a data list of integers, which gets updated randomly and acyncrously by 3rd python script script.py. The following buggy code is implemented in the Jetson Xavier NX development kit and can be reproduced on any standard hardware running Python 3.6. Expected behavior: mp1.py receives one byte of integer each second. mp2.py synchronously sends the data to mp1.py from the list of integers. This list asynchronously feds by script.py based on random timing betwen 1sec. and 1 min. Buggy behavior: mp1.py receives for ever, only one byte of the two - default bytes of the list. mp1.py python from multiprocessing import Process,Queue,Pipe from mp2 import getData import time parent_conn,child_conn = Pipe() def getDataByte(): p = Process(target=getData, args=(child_conn,)) p.start() a = parent_conn.recv() print(\"Recieved Data is:\", a) if __name__ == '__main__': while True : time.sleep(1) getDataByte() mp2.py python myl = [-1,-2] mylConsumed = False def getData(child_conn): print(\"Debug of getData\") if not len(myl): print(\"myl is consumed\") data = 0 mylConsumed = True else: data = myl[0] myl.pop(0) print(\"myl list is: \", myl) child_conn.send(data) def setData(thisBuffer): if mylConsumed == True: myl1 = thisBuffer myl.extend(myl1) mylConsumed = False print(myl) script.py python from mp2 import setData from random import randint import time mybuf = [1,2,3,4,5,6,7,8,9,10] def setBleData(): setData(mybuf) if __name__ == \"__main__\": while True: num = randint(1,5) time.sleep(num) setBleData() To reproduce the bug: 1st run python3 mp1.py command in the 1st terminal. 2nd run pyhthon3 script.py command in 2nd terminal. The following is the 1st terminal output display till interrupting execution by CNTL+C : python3 mp1.py Debug of getData myl list is: [-2] Recieved Data is: -1 Debug of getData myl list is: [-2] Recieved Data is: -1 Debug of getData myl list is: [-2] Recieved Data is: -1 Debug of getData myl list is: [-2] Recieved Data is: -1 Debug of getData myl list is: [-2] Recieved Data is: -1 Debug of getData myl list is: [-2] Recieved Data is: -1 ^CTraceback (most recent call last): File \"mp1.py\", line 16, in &lt;module&gt; time.sleep(1) KeyboardInterrupt This problem is understood and fixed. Many thanks to everyone involved!!! The root cause as the described above is improper and not needed at all use of multiprocessing. All variables and code gets copied to another process address space, hence each process gets stuff done on separate data and code. After spending lots of hours digging into the problem, the solution is very simple - just import required functions from other python script files.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm working on ActionAI(https://github.com/smellslikeml/ActionAI) which is a python library that classifies human actions. I got this error running iva.py, the final script. ... if RUNSECONDARY: import tensorflow as tf secondary_model = tf.keras.models.load_model('models/classifier.sav') window = 3 pose_vec_dim = 36 motion_dict = {0: 'lying', 1: 'sit', 2: 'stand', 3: 'walk'} ... 2021-11-16 21:53:39.382516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0 Traceback (most recent call last): File \"iva.py\", line 256, in &lt;module&gt; secondary_model = tf.keras.models.load_model('models/classifier.sav') File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\", line 149, in load_model loader_impl.parse_saved_model(filepath) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 83, in parse_saved_model constants.SAVED_MODEL_FILENAME_PB)) OSError: SavedModel file does not exist at: models/classifier.sav/{saved_model.pbtxt|saved_model.pb} In models directory, classifier.sav is existing but empty and doesn't have pbtxt or pb. This is train.py import pandas as pd from sklearn.pipeline import Pipeline from transformer import PoseExtractor def actionModel(classifier): pipeline = Pipeline([ ('pose_extractor', PoseExtractor()), ('classifier', classifier)]) return pipeline def trainModel(csv_path, pipeline): df = pd.read_csv(csv_path) X = df['image'].values y = df['label'] pipeline = pipeline.fit(X, y) return pipeline.get_params()['steps'][1][1] if __name__ == '__main__': import pickle import argparse import importlib parser = argparse.ArgumentParser(description='Train pose classifier') parser.add_argument('--config', type=str, default='conf', help=\"name of config .py file inside config/ directory, default: 'conf'\") args = parser.parse_args() config = importlib.import_module('config.' + args.config) pipeline = actionModel(config.classifier()) model = trainModel(config.csv_path, pipeline) # Dump the model to file pickle.dump(model, open(config.classifier_model, 'wb'), protocol=2) I think pickle.dump(model, open(config.classifier_model, 'wb'), protocol=2) in train.py doesn't work properly. I've tried changing the filename extension to pkl, h5, and pb but didn't work. sudo apt install python3-h5py this command also changed nothing.",
        "answers": [
            [
                "pickle.dump() should take open() as a second argument, i.e.: import pickle filename = 'your_pickle_filename.pkl' pickle.dump(model, open(filename, 'wb'))"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a python script running real time inference on frames garbed from a ZED2i camera in 1080p@30fps on an nvidia jetson xavier nx. As I'm trying to boost up the performance I was wondering if there is an interface between ZED SDK and DeepStream SDK? More info: object detector: darknet yolov4tiny 416X416 jetpack 4.6 power mode: 20W 6cores",
        "answers": [
            [
                "Here is the gstream cmd. gst-launch-1.0 zedsrc stream-type=0 ! videoconvert ! 'video/x-raw,format=(string)YUY2' ! nvvidconv ! 'video/x-raw(memory:NVMM),format=(string)NV12,width=1280,height=720' ! nvvidconv ! mux.sink_0 nvstreammux live-source=1 name=mux batch-size=1 width=1280 height=720 ! nvinfer config-file-path=/opt/nvidia/deepstream/deepstream-6.0/sources/objectDetector_Yolo/config_infer_primary_yoloV3.txt ! nvvideoconvert ! nvdsosd ! nvegltransform ! nveglglessink sync=0 there is a guy out there tried this already. He provides youtube video github"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Jetson Xavier NX I want to run a GPU program in a k3s pod, but it shows the error ImportError: libnppicc.so.10: cannot open shared object file: No such file or directory it seems it cannot use GPU resource, because when I use docker run --runtime nvidia ... it works. How to set something like \"--runtime nvidia\" in the YAML for k3s deployment?",
        "answers": [
            [
                "Follow this article: https://dev.to/mweibel/add-nvidia-gpu-support-to-k3s-with-containerd-4j17 --- Below does not work - the download link is broken --- 1. First configure the containerd to use the nvidia-container-runtime plugin $ sudo wget https://k3d.io/usage/guides/cuda/config.toml.tmpl -O /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl 2. Install NVIDIA device plugin for Kubernetes $ cat &lt;&lt;EOF | kubectl apply -f - apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: nvidia-device-plugin namespace: kube-system spec: chart: nvidia-device-plugin repo: https://nvidia.github.io/k8s-device-plugin EOF Reference: https://itnext.io/enabling-nvidia-gpus-on-k3s-for-cuda-workloads-a11b96f967b0"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Upon running nvidia-smi through terminal, i am met with nvidia-smi command not found However, i am aware that jetpack 3.3 (the nvidia drivers) have already been installed. Has anyone encountered similar problems with Nvidia jetson tx2 ? System specs: DJI Manifold 2G (Nvidia Jetson TX2) Jetpack 3.3.0 ARMv8 Processor rev 3 (v8l) \u00d7 4 ARMv8 Processor rev 0 (v8l) \u00d7 2 NVIDIA Tegra X2 (nvgpu)/integrated 8GB ram, Ubuntu 16.04 LTS UPDATE AND EDIT (SOLVED): While nvidia-smi does not run, One of the answers posted below by user @SeB helped. So after getting the ./deviceQuery executable made, one can see the following. Which tells you the details of your GPU /usr/local/cuda/samples/1_Utilities/deviceQuery$ ./deviceQuery ./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"NVIDIA Tegra X2\" CUDA Driver Version / Runtime Version 9.0 / 9.0 CUDA Capability Major/Minor version number: 6.2 Total amount of global memory: 7839 MBytes (8219348992 bytes) ( 2) Multiprocessors, (128) CUDA Cores/MP: 256 CUDA Cores GPU Max Clock rate: 1301 MHz (1.30 GHz) Memory Clock rate: 1600 Mhz Memory Bus Width: 128-bit L2 Cache Size: 524288 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 32768 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 1 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: Yes Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 0 / 0 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.0, CUDA Runtime Version = 9.0, NumDevs = 1 Result = PASS",
        "answers": [
            [
                "I think that nvidia-smi is only available so far for NVIDIA discrete GPUs, but Jetsons have an integrated GPU (sharing physical memory with system). You can find details about your GPU specs with deviceQuery utility in CUDA samples: cd /usr/local/cuda/samples/1_Utilities//deviceQuery/ sudo make ./deviceQuery and you may monitor your GPU usage at run-time with tegrastats: sudo tegrastats and check for item GR3D such as: GR3D_FREQ 0%@318 saying 0% usage with current clock at 318MHz."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This question already has answers here: Installing pip is not working in python &lt; 3.6 (6 answers) Closed 1 year ago. i encountered errors with trying to upgrade pip, and its setup tools. Appended below are the errors. pip install -upgrade pip failing pip install --upgrade pip Collecting pip Using cached https://files.pythonhosted.org/packages/da/f6/c83229dcc3635cdeb51874184241a9508ada15d8baa337a41093fab58011/pip-21.3.1.tar.gz Complete output from command python setup.py egg_info: Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; File \"/tmp/pip-build-9msN4R/pip/setup.py\", line 7 def read(rel_path: str) -&gt; str: ^ SyntaxError: invalid syntax ---------------------------------------- Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-9msN4R/pip/ You are using pip version 8.1.1, however version 21.3.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. setup tools installlation failing pip install setuptools Collecting setuptools Using cached https://files.pythonhosted.org/packages/e6/e2/f2bfdf364e016f7a464db709ea40d1101c4c5a463dd7019dae0a42dbd1c6/setuptools-59.5.0.tar.gz Complete output from command python setup.py egg_info: Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; File \"setuptools/__init__.py\", line 16, in &lt;module&gt; import setuptools.version File \"setuptools/version.py\", line 1, in &lt;module&gt; import pkg_resources File \"pkg_resources/__init__.py\", line 117 f\"{v} is an invalid version and will not be supported in \" ^ SyntaxError: invalid syntax ---------------------------------------- Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-HXbHxE/setuptools/ You are using pip version 8.1.1, however version 21.3.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. So i have tried the many suggestions listd in this thread over at https://github.com/facebook/prophet/issues/418 but to no avail. Does anyone know of a solution ? Thanks in advance ! System specs: Ubuntu 16.04 LTS NVIDIA Tegra X2 (nvgpu)/integrated ARMv8 Processor rev 3 (v8l) \u00d7 4 ARMv8 Processor rev 0 (v8l) \u00d7 2 64bit 8GB RAM",
        "answers": [
            [
                "I have had this issue before. pip requires the latest version of python to be working properly to work, however certain CPU architectures don't fully support it. you say your using an ARM based CPU which I think requires a different way of installing python. Pip will throw syntax errors when python is incompatible with your CPU arch. You may need to look into emulators or upgrading hardware."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Using gstreamer on the jetson nano board, I display rtsp stream, My pipe on terminal is as follows: gst-launch-1.0 rtspsrc location='rtsp://user:password...' sync=false latency=5 ! decodebin ! nvvidconv ! xvimagesink And everything is fine. I installed opencv from source and compiled it with cuda nad gstreamer capabilities, but when I run it in the following code snippet, the camera does not load: cv::VideoCapture cap(\"rtspsrc location=rtsp://user:password... sync=false latency=5 ! decodebin ! nvvidconv ! xvimagesink\",cv::CAP_GSTREAMER); error is : [WARN:0] global /home/kamiz/opencv/modules/videoio/src/cap_gstreamer.cpp (944) open OpenCV | GStreamer warning: cannot find appsink in manual pipeline [WARN:0] global /home/kamiz/opencv/modules/videoio/src/cap_gstreamer.cpp (597) isPiplelinePlaying Opencv | GStreamer warning: pipeline have not been created ERROR: unable to open camera nvdc: start nvdcEventThread nvdc: exit nvdcEventThread This is while I display the usb camera in opencv with gstreamer without any problems. Thank you in advance for your help",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've written code that collects telemetry information from the device and saves it in a .csv file. However, I'm not able to detect the GPU from the GPUtil: import GPUtil GPU = GPUtil.getAvailable() print(GPU) print(GPUtil.__version__) Output: [] 1.4.0 I didn't find information if it has any relation with the type of architecture of Jetson. Below is some additional system information:",
        "answers": [
            [
                "After a lot of hard work to come up with some solution, I came to some conclusions that I share here. Reading a little more about GPUtil and analyzing the GPUtil source code here it becomes clear that: GPUtil is a Python module for getting the GPU status from NVIDA GPUs using nvidia-smi. Unfortunately, nvidia-smi does not work on NVIDIA Jetson devices (Xavier NX, Nano, AGX Xavier, TX1, TX2). As my goal was to fetch telemetry data, there were two options: tegrastats; jetson-stats. I found tegrastats options quite limited as tegrastats reports memory and processor usage for Tegra based devices, but it would be necessary to create a bash file, for, for example, automate the process. You can find the tegrastats utility here. As my goal was to create a Python script to fetch telemetry data together with other libraries, like cputil, the solution adopted was use the jtop from jetson-stats as a Python library. jetson-stats is a package for monitoring and control your NVIDIA Jetson [Xavier NX, Nano, AGX Xavier, TX1, TX2] and works with all NVIDIA Jetson ecosystem. jtop is a system monitoring utility that runs on the terminal and see and control realtime the status of your NVIDIA Jetson. CPU, RAM, GPU status and frequency and other. To use it, was needed to install jetson-stats: $ sudo -H pip install -U jetson-stats To use jtop just type in the terminal the command jtop. The prompt interface will be show. To import jtop as a Python library just write the following line of code in Python script: from jtop import jtop And in my specific case I used the following snippet code: with jtop() as jetson: xavier_nx = jetson.stats CPU_temperature = xavier_nx['Temp CPU'] GPU_temperature = xavier_nx['Temp GPU'] Thermal_temperature = xavier_nx['Temp thermal'] .stats returns a python dict structure, and the available data values are: time, uptime, jetson_clocks, nvp model, CPU1, CPU2, CPU3, CPU4, CPU5, CPU6, GPU, MTS FG, MTS BG, RAM, EMC, SWAP, APE, NVENC, NVDEC, NVJPG, fan, Temp AO, Temp AUX, Temp CPU, Temp GPU, Temp thermal, power cur, power avg. Unfortunately jetson-stats doesn't work with Docker. This can be a negative and important point when thinking about using this service in your application."
            ],
            [
                "jetson-stats also works in Docker. Following the documentation: https://rnext.it/jetson_stats/docker.html Install jetson-stats on your host Install jetson-stats on your container as well Pass to your container /run/jtop.sock:/run/jtop.sock example of Docker docker run --rm -it -v /run/jtop.sock:/run/jtop.sock rbonghi/jetson_stats:latest I hope I have helped you"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am using a jetson xavier NX and i have connected 4k camera to it. I want to stream 4k video using gstreamer and RTP and store the streamed video data as an mkv file on another jetson. However my data is getting compressed a lot and I am not able to send it in 4k even though 4k is supported by the camera and by gstreamer. SENDER gst-launch-1.0 nvarguscamerasrc sensor-id=0 ! \"video/x-raw(memory:NVMM), width=(int)1944, height=(int)1096, format=(string)NV12\" ! nvvidconv left=8 right=1928 top=8 bottom=1088 ! \"video/x-raw(memory:NVMM), format=(string)NV12, width=(int)1920, height=(int)1080\" ! omxh264enc qp-range=35,35:35,35:-1,-1 ! rtph264pay mtu=60000 ! udpsink clients=127.0.0.1:5000 sync=false RECEIVER gst-launch-1.0 udpsrc port=5000 caps=\"application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)H264, sprop-parameter-sets=(string)\\\"Z0JAKJWgHgCJ+VA\\\\=\\\\,aM48gA\\\\=\\\\=\\\", payload=(int)96\" ! rtph264depay ! h264parse ! matroskamux ! filesink location=test.mkv -e I tried changing the resolution to 3840x2160 on the sender side but it didnt seem to work. What am i doing wrong",
        "answers": [
            [
                "There is a maximum width of 4096 for HW encoder. Within this limitation, you may use: SENDER gst-launch-1.0 nvarguscamerasrc sensor-id=0 ! 'video/x-raw(memory:NVMM), format=NV12, width=3840, height=2160, framerate=30/1' ! nvv4l2h264enc insert-vui=1 insert-sps-pps=1 profile=2 qp-range=35,35:35,35:-1,-1 ! h264parse ! rtph264pay config-interval=1 ! udpsink clients=127.0.0.1:5000 RECEIVER gst-launch-1.0 udpsrc port=5000 buffer-size=32000000 ! application/x-rtp,media=video,encoding-name=H264 ! rtpjitterbuffer latency=500 ! rtph264depay ! h264parse ! nvv4l2decoder ! nvegltransform ! nveglglessink"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to capture a raw 4k image for my AI application using a 4k camera shown here. I want to capture a frame every 5 seconds and store it as a .png file which I will later run through my neural network for detection. I know the ommand to record a 4k video in raw format (.mkv). However I am not able to capture a single image (frame) in 3840x2160 resolution. There is a sample command which is gst-launch-1.0 nvarguscamerasrc sensor-id=0 num-buffers=1 ! \"video/x-raw(memory:NVMM),format=(string)NV12, width=(int)3840, height=(int)2160\" ! nvjpegenc ! filesink location=test.jpg The above command works but it only stores in jpg which is around 1mb insize. This is not very clear and I want a png format which is more detailed. I tried changing the extension in the filename but it is not working. I am using a jetson xavier nx. EDIT I have tried to change the encoding by using the following command gst-launch-1.0 nvarguscamerasrc sensor-id=0 num-buffers=1 ! \"video/x-raw(memory:NVMM), format=(string)NV12, width=(int)3840, height=(int)2160\" ! pngenc ! filesink location=test1.png However I am getting the following error WARNING: erroneous pipeline: could not link nvarguscamerasrc0 to pngenc0, pngenc0 can't handle caps video/x-raw(memory:NVMM), format=(string)NV12, width=(int)3840, height=(int)2160",
        "answers": [
            [
                "You would just need to copy the image from Argus in NVMM memory into system memory. nvvidconv plugin may be used for that: gst-launch-1.0 nvarguscamerasrc sensor-id=0 num-buffers=1 ! \"video/x-raw(memory:NVMM), format=(string)NV12, width=(int)3840, height=(int)2160\" ! nvvidconv ! pngenc ! filesink location=test1.png However, argus will auto tune many parameters unless otherwise specified, so the first frame may be dark depending on your scene. In such case, you may capture 21 images and use multifilesink so that you'll just keep the 21st image after 1s and then convert it to png: gst-launch-1.0 nvarguscamerasrc sensor-id=0 num-buffers=21 ! 'video/x-raw(memory:NVMM), format=NV12, width=3840, height=2160, framerate=21/1' ! nvvidconv ! video/x-raw,format=RGBA ! multifilesink location=test1.rgba max-files=1 gst-launch-1.0 filesrc location=test1.rgba ! videoparse format=rgba width=3840 height=2160 framerate=0/1 ! pngenc ! filesink location=test1.png Note that pngenc is not very fast with Jetson."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "In my script there are 5 different process which should work parallel. For now I am using multiprocessing pool method to run parallel processes. Actually it is working very well. But the problem is I want to use this script in a platform which has only 4 CPU. But this platform has also GPU (Nvidia Jetson Nano). So I want to run 4 process with CPUs and another one process should work with GPU. Let me explain with some code: imports... manager_1 = Manager() variable = manager_1.Value(ctypes.Array, []) counter_lock_1= manager_1.Lock() manager_2 = Manager() variable_2 = manager_2.Value(ctypes.Array, []) counter_lock_2 = manager_2.Lock() manager_3 = Manager() variable_3 = manager_3.Value(ctypes.Array, []) counter_lock_3 = manager_3.Lock() def process1(variable,variable_2,..): while True: ---Do something--- variable.value = something def process2(variable,..): while True: ---Do something--- def process3(variable,variable_2,..): while True: ---Do something--- def process4(variable,variable_2,variable_3,..): while True: ---Do something--- def process5(variable,variable_2,..): while True: ---Do something--- def main(): f_1 = functools.partial(process1,variable,variable_2,...) f_2 = functools.partial(process2,variable,...) f_3 = functools.partial(process3,variable,variable_2,...) f_4 = functools.partial(process4,variable,variable_2,variable_3) f_5 = functools.partial(process5,variable,variable_2,...) with Pool() as pool: res = pool.map(smap, [f_1, f_2, f_3, f_4, f_5]) main() My script template is someting like this. For example if I use 4 CPU platform, what happend to f_5? How can I run it with GPU. Note: Actually f_5 is already working with GPU because it is about an object detection function. I can choose do object detection with GPU device. But I have to define this function in pool because to get variables. I guess I run it with a CPU at the begin but after that it is using GPU to detect objects. How can I do this directly by using GPU? Also do you have a suggestion about using pool or something another which can effect performance? Thank you.",
        "answers": [
            [
                "GPU's cannot run Python. The actual object detection will be done in a non-Python function; likely a CUDA function (the native language of NVidia CPU's). Since you have 4 CPU cores, the 5 threads will not run simultaneously. The \"lock\" code is also very suspect. In general, locks should be taken by the function that uses the variable, and the locks will prevent conflicts between multiple functions using the same variable. This requires that each function has a separate lock. counter_lock_1 outside any of the 5 process functions is likely in the wrong place."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Undefined symbol: gst_type_find_helper_for_data_with_extension Platform: Jetson Xavier NX, gstreamer version:1.0",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using jetson NX xavier kit having cuda 10.2.89, open Cv 4.1.1 and tensorRT 7.1.3 . Trying to install pytorch. Tried installing with this line conda install pytorch torchvision cpuonly -c pytorch but when i write this line import torch It throws an error by saying that module not installed. How I can verify if pytorch has been installed correctly.",
        "answers": [
            [
                "Try this one conda install -c pytorch pytorch After executing this command, you need to enter yes(if prompted in the command line) for installing all the related packages. If there is no conflict while installing the libraries, the PyTorch library will be installed. To check if it is properly installed or not, type the command python in your command line and type import torch to check if it is properly installed or not."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have an error on my algorithm with Jetson Xavier NX. I try to run code but terminal say that : [ WARN:0] global /home/arc/opencv_build/opencv/modules/videoio/src/cap_v4l.cpp (890) open VIDEOIO(V4L2:/dev/video0): can't open camera by index Traceback (most recent call last): File \"realtime_quadrant.py\", line 41, in height = frame.shape AttributeError: 'NoneType' object has no attribute 'shape' This means that I have to change 0 in camera = cv2.VideoCapture(0). So I try cv2.VideoCapture(1), cv2.VideoCapture(-1) cv2.VideoCapture(cv2.CAP_V4L2) yet I still have the same error. Also I installed V4l2 ( Sudo apt-get install v4l2) How can I do that to run?",
        "answers": [],
        "votes": []
    },
    {
        "question": "After installing jetpack 4.4 on the DJI manifold 2G (Nvidia jetson tx2 version), the internal Ubuntu software updater pings me for a 2.3GB sized update. Following which, after completing the update, all USB ports are non-responsive, and the processor's fan will continuously run at full speed. Restarting the device does not improve nor make things worst. The only way out seemed to be re-flashing the firmware with jetpack 4.4. The specs are, if they are helpful: Processor: ARMv8 Processor rev 3 (v8l) \u00d7 4 Graphics: NVIDIA Tegra X2 (nvgpu)/integrated Os type: 64-bit Memory: 8gb Any help, or similar experiences and work-arounds are greatly appreciated. Thank you ! Edit -Attempting to use the Manifold 2G image provided by DJI returned this error when executing sudo tar -zxvf gzip: stdin: invalid compressed data--format violated tar: Unexpected EOF in archive tar: Unexpected EOF in archive tar: Error is not recoverable: exiting now As such, i am unable to downgrade back to the Jetpack 3.3 version",
        "answers": [
            [
                "DJI mf2G uses a specifically modified version of the jetpack with DJI-preset serial port config. Publicly, there is one standard version on the DJI website that you can restore to factory default. https://www.dji.com/sg/manifold-2/downloads Privately, the DJI dev group also give out a Jetpack 4.3 version DJI image that can allow you to use docker(factory default does not allow). But I can not share it publicly. You may ask DJI support to release this version to you."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to use dlib (GPU) on Jetson Xavier NX, following are my steps to install dlib-19.19 dowload dlib repo from repo: https://github.com/davisking/dlib cd dlib-19.19 mkdir build cd build/ cmake .. -DDLIB_USE_CUDA=1 got the message -- Found CUDA: /usr/local/cuda (found suitable version \"10.2\", minimum required is \"7.5\") -- Looking for cuDNN install... -- Found cuDNN: /usr/lib/aarch64-linux-gnu/libcudnn.so -- Building a CUDA test project to see if your compiler is compatible with CUDA... -- Checking if you have the right version of cuDNN installed. -- *** Found cuDNN, but it looks like the wrong version so dlib will not use it. *** -- *** Dlib requires cuDNN V5.0 OR GREATER. Since cuDNN is not found DLIB WILL NOT USE CUDA. *** -- *** If you have cuDNN then set CMAKE_PREFIX_PATH to include cuDNN's folder. *** -- Disabling CUDA support for dlib. DLIB WILL NOT USE CUDA Any ideas on this issue? How to install dlib (GPU) from source on Jetson correctly?",
        "answers": [
            [
                "My environment: Xavier NX + JetPack 4.6 I solved this issue following the help here: https://forums.developer.nvidia.com/t/simple-accelerated-face-recognition/142679/19 Most importantly, make sure you use dlib19.21, neither dlib19.23 nor dlib 19.17."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I've been trying to build open3d from source on the l4t-base:r32.5.0 IMG from NVIDIA for quite some time now and have basically searched the whole internet for a script/guide, but I can't seem to find anything or get it to work myself. Following the exact steps stated on http://www.open3d.org/docs/release/arm.html I get the following output while running the following Dockerfile Dockerfile FROM dustynv/ros:foxy-ros-base-l4t-r32.5.0 WORKDIR /workspace ########################## ### CMAKE INSTALLATION ### ########################## RUN wget https://github.com/Kitware/CMake/releases/download/v3.20.6/cmake-3.20.6-linux-aarch64.tar.gz RUN tar -v -xzf cmake-3.20.6-linux-aarch64.tar.gz RUN mv cmake-3.20.6-linux-aarch64 cmake RUN echo \"alias cmake='/workspace/cmake/bin/cmake'\" &gt;&gt; /root/.bashrc RUN source /root/.bashrc &amp;&amp; cmake --version ################################# ### OPEN3D 0.13.0 INSTALLTION ### ################################# RUN wget https://github.com/isl-org/Open3D/archive/refs/tags/v0.13.0.tar.gz RUN tar -v -xzf v0.13.0.tar.gz RUN mv Open3D-0.13.0 open3d RUN apt-get update &amp;&amp; apt-get install xorg-dev libglu1-mesa-dev -y RUN cd open3d &amp;&amp; mkdir build &amp;&amp; cd build &amp;&amp; \\ /workspace/cmake/bin/cmake \\ -DCMAKE_BUILD_TYPE=Release \\ -DBUILD_SHARED_LIBS=ON \\ -DBUILD_CUDA_MODULE=OFF \\ -DBUILD_GUI=OFF \\ -DBUILD_TENSORFLOW_OPS=OFF \\ -DBUILD_PYTORCH_OPS=OFF \\ -DBUILD_UNIT_TESTS=ON \\ -DCMAKE_INSTALL_PREFIX=~/open3d_install \\ -DPYTHON_EXECUTABLE=/usr/bin/python3 \\ .. ErrorLog Performing C SOURCE FILE Test CMAKE_HAVE_LIBC_PTHREAD failed with the following output: Change Dir: /workspace/open3d/build/CMakeFiles/CMakeTmp Run Build Command(s):/usr/bin/make -f Makefile cmTC_d15b1/fast &amp;&amp; /usr/bin/make -f CMakeFiles/cmTC_d15b1.dir/build.make CMakeFiles/cmTC_d15b1.dir/build make[1]: Entering directory '/workspace/open3d/build/CMakeFiles/CMakeTmp' Building C object CMakeFiles/cmTC_d15b1.dir/src.c.o /usr/bin/cc -DCMAKE_HAVE_LIBC_PTHREAD -fPIE -o CMakeFiles/cmTC_d15b1.dir/src.c.o -c /workspace/open3d/build/CMakeFiles/CMakeTmp/src.c Linking C executable cmTC_d15b1 /workspace/cmake/bin/cmake -E cmake_link_script CMakeFiles/cmTC_d15b1.dir/link.txt --verbose=1 /usr/bin/cc CMakeFiles/cmTC_d15b1.dir/src.c.o -o cmTC_d15b1 CMakeFiles/cmTC_d15b1.dir/src.c.o: In function `main': src.c:(.text+0x48): undefined reference to `pthread_create' src.c:(.text+0x50): undefined reference to `pthread_detach' src.c:(.text+0x58): undefined reference to `pthread_cancel' src.c:(.text+0x64): undefined reference to `pthread_join' src.c:(.text+0x74): undefined reference to `pthread_atfork' collect2: error: ld returned 1 exit status CMakeFiles/cmTC_d15b1.dir/build.make:98: recipe for target 'cmTC_d15b1' failed make[1]: *** [cmTC_d15b1] Error 1 make[1]: Leaving directory '/workspace/open3d/build/CMakeFiles/CMakeTmp' Makefile:127: recipe for target 'cmTC_d15b1/fast' failed make: *** [cmTC_d15b1/fast] Error 2 Source file was: #include &lt;pthread.h&gt; static void* test_func(void* data) { return data; } int main(void) { pthread_t thread; pthread_create(&amp;thread, NULL, test_func, NULL); pthread_detach(thread); pthread_cancel(thread); pthread_join(thread, NULL); pthread_atfork(NULL, NULL, NULL); pthread_exit(NULL); return 0; } Looking for a ASM_NASM compiler failed with the following output: -- The ASM_NASM compiler identification is unknown -- Didn't find assembler CMake Error at CMakeLists.txt:2 (project): No CMAKE_ASM_NASM_COMPILER could be found. Tell CMake where to find the compiler by setting either the environment variable \"ASM_NASM\" or the CMake cache entry CMAKE_ASM_NASM_COMPILER to the full path to the compiler, or to the compiler name if it is in the PATH. -- Configuring incomplete, errors occurred! See also \"/workspace/open3d/build/CMakeFiles/CheckASM_NASM/CMakeFiles/CMakeOutput.log\". See also \"/workspace/open3d/build/CMakeFiles/CheckASM_NASM/CMakeFiles/CMakeError.log\". Determining if the function sgemm_ exists failed with the following output: Change Dir: /workspace/open3d/build/CMakeFiles/CMakeTmp Run Build Command(s):/usr/bin/make -f Makefile cmTC_855e0/fast &amp;&amp; /usr/bin/make -f CMakeFiles/cmTC_855e0.dir/build.make CMakeFiles/cmTC_855e0.dir/build make[1]: Entering directory '/workspace/open3d/build/CMakeFiles/CMakeTmp' Building C object CMakeFiles/cmTC_855e0.dir/CheckFunctionExists.c.o /usr/bin/cc -DCHECK_FUNCTION_EXISTS=sgemm_ -fPIE -o CMakeFiles/cmTC_855e0.dir/CheckFunctionExists.c.o -c /workspace/cmake/share/cmake-3.20/Modules/CheckFunctionExists.c Linking C executable cmTC_855e0 /workspace/cmake/bin/cmake -E cmake_link_script CMakeFiles/cmTC_855e0.dir/link.txt --verbose=1 /usr/bin/cc -DCHECK_FUNCTION_EXISTS=sgemm_ CMakeFiles/cmTC_855e0.dir/CheckFunctionExists.c.o -o cmTC_855e0 CMakeFiles/cmTC_855e0.dir/CheckFunctionExists.c.o: In function `main': CheckFunctionExists.c:(.text+0x10): undefined reference to `sgemm_' collect2: error: ld returned 1 exit status CMakeFiles/cmTC_855e0.dir/build.make:98: recipe for target 'cmTC_855e0' failed make[1]: *** [cmTC_855e0] Error 1 make[1]: Leaving directory '/workspace/open3d/build/CMakeFiles/CMakeTmp' Makefile:127: recipe for target 'cmTC_855e0/fast' failed make: *** [cmTC_855e0/fast] Error 2 Determining if the function cheev_ exists failed with the following output: Change Dir: /workspace/open3d/build/CMakeFiles/CMakeTmp Run Build Command(s):/usr/bin/make -f Makefile cmTC_3a46a/fast &amp;&amp; /usr/bin/make -f CMakeFiles/cmTC_3a46a.dir/build.make CMakeFiles/cmTC_3a46a.dir/build make[1]: Entering directory '/workspace/open3d/build/CMakeFiles/CMakeTmp' Building C object CMakeFiles/cmTC_3a46a.dir/CheckFunctionExists.c.o /usr/bin/cc -DCHECK_FUNCTION_EXISTS=cheev_ -fPIE -o CMakeFiles/cmTC_3a46a.dir/CheckFunctionExists.c.o -c /workspace/cmake/share/cmake-3.20/Modules/CheckFunctionExists.c Linking C executable cmTC_3a46a /workspace/cmake/bin/cmake -E cmake_link_script CMakeFiles/cmTC_3a46a.dir/link.txt --verbose=1 /usr/bin/cc -DCHECK_FUNCTION_EXISTS=cheev_ CMakeFiles/cmTC_3a46a.dir/CheckFunctionExists.c.o -o cmTC_3a46a /usr/lib/aarch64-linux-gnu/libblas.so -pthread -lm -ldl CMakeFiles/cmTC_3a46a.dir/CheckFunctionExists.c.o: In function `main': CheckFunctionExists.c:(.text+0x10): undefined reference to `cheev_' collect2: error: ld returned 1 exit status CMakeFiles/cmTC_3a46a.dir/build.make:99: recipe for target 'cmTC_3a46a' failed make[1]: *** [cmTC_3a46a] Error 1 make[1]: Leaving directory '/workspace/open3d/build/CMakeFiles/CMakeTmp' Makefile:127: recipe for target 'cmTC_3a46a/fast' failed make: *** [cmTC_3a46a/fast] Error 2 Determining if the function LAPACKE_dgeqrf exists failed with the following output: Change Dir: /workspace/open3d/build/CMakeFiles/CMakeTmp Run Build Command(s):/usr/bin/make -f Makefile cmTC_9905d/fast &amp;&amp; /usr/bin/make -f CMakeFiles/cmTC_9905d.dir/build.make CMakeFiles/cmTC_9905d.dir/build make[1]: Entering directory '/workspace/open3d/build/CMakeFiles/CMakeTmp' Building C object CMakeFiles/cmTC_9905d.dir/CheckFunctionExists.c.o /usr/bin/cc -DCHECK_FUNCTION_EXISTS=LAPACKE_dgeqrf -fPIE -o CMakeFiles/cmTC_9905d.dir/CheckFunctionExists.c.o -c /workspace/cmake/share/cmake-3.20/Modules/CheckFunctionExists.c Linking C executable cmTC_9905d /workspace/cmake/bin/cmake -E cmake_link_script CMakeFiles/cmTC_9905d.dir/link.txt --verbose=1 /usr/bin/cc -DCHECK_FUNCTION_EXISTS=LAPACKE_dgeqrf CMakeFiles/cmTC_9905d.dir/CheckFunctionExists.c.o -o cmTC_9905d /usr/lib/aarch64-linux-gnu/liblapack.so /usr/lib/aarch64-linux-gnu/libblas.so CMakeFiles/cmTC_9905d.dir/CheckFunctionExists.c.o: In function `main': CheckFunctionExists.c:(.text+0x10): undefined reference to `LAPACKE_dgeqrf' collect2: error: ld returned 1 exit status CMakeFiles/cmTC_9905d.dir/build.make:100: recipe for target 'cmTC_9905d' failed make[1]: *** [cmTC_9905d] Error 1 make[1]: Leaving directory '/workspace/open3d/build/CMakeFiles/CMakeTmp' Makefile:127: recipe for target 'cmTC_9905d/fast' failed make: *** [cmTC_9905d/fast] Error 2 Problem: I want to build open3d from source on an arm docker img with a Dockerfile. More specifically, the l4t-base:r32.5.0 img provided by NVIDIA. This image will be used on a Jetson XavierNX with docker installed. Above you can see what I have tried so far in terms of Dockerfile and the output this gives me. What I am asking is, if anyone has a Dockerfile for building open3d from source on an arm based image or knows how to do it successfully, because I cannot seem to be able to do it.",
        "answers": [],
        "votes": []
    },
    {
        "question": "JetPack version: 4.6 I am trying to use dlib (GPU) on Jetson Xavier NX, following are my steps to install dlib dowload repo: https://github.com/davisking/dlib cd dlib-master mkdir build cd build/ cmake .. -DDLIB_USE_CUDA=1 -DUSE_AVX_INSTRUCTIONS=1 cmake --build . cd .. sudo python3 setup.py install --set USE_AVX_INSTRUCTIONS=1 --set DLIB_USE_CUDA=1 When I run import dlib Illegal instruction (core dumped) I just figured out it probably caused by the following error. File \"/usr/local/lib/python3.6/dist-packages/dlib-19.22.99-py3.6-linux-aarch64.egg/dlib/__init__.py\", line 19, in &lt;module&gt; from _dlib_pybind11 import * ImportError: /usr/local/lib/python3.6/dist-packages/dlib-19.22.99-py3.6-linux-aarch64.egg/_dlib_pybind11.cpython-36m-aarch64-linux-gnu.so: undefined symbol: png_riffle_palette_neon is there any suggestion on this issue?",
        "answers": [
            [
                "It looks like libpng is missing, to install it: sudo apt-get install libpng-dev Then reinstall dlib: pip install --upgrade --no-cache-dir dlib To check if the installation was successful: python3 -c 'import dlib;print(dlib.__version__)' If there is no tracebacks you are good to go!"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "[/home/aimotion/anaconda3] &gt;&gt;&gt; PREFIX=/home/aimotion/anaconda3 Unpacking payload ... /home/aimotion/Downloads/Anaconda3-2021.05-Linux-x86_64 (1).sh: line 381: /home/aimotion/anaconda3/conda.exe: cannot execute binary file: Exec format error /home/aimotion/Downloads/Anaconda3-2021.05-Linux-x86_64 (1).sh: line 383: /home/aimotion/anaconda3/conda.exe: cannot execute binary file: Exec format error how to solve this issue?",
        "answers": [
            [
                "I think you need aarch64 version. https://docs.anaconda.com/anaconda/install/linux-aarch64/ Installers can be found: https://repo.anaconda.com/archive/ I took Anaconda3-2021.11-Linux-aarch64.sh 487.7M 2021-11-17 12:08:43 with no problem"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Is it possible to run the inference graph on the jetson board without converting it into TensorRT format as mentioned in the github repo ? Will we be able to run the Tensorflow Object Detection API without using TensorRT on the Jetson board?",
        "answers": [
            [
                "Install TF from Nvidia website for the right jetpack version. Coming to the object detection, execute first code block under Step 1, mentioned in this link git clone --quiet https://github.com/tensorflow/models.git apt-get install -qq protobuf-compiler python-pil python-lxml python-tk pip install -q Cython contextlib2 pillow lxml matplotlib pip install -q pycocotools cd /content/models/research protoc object_detection/protos/*.proto --python_out=. import os import sys os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/' sys.path.append(\"/content/models/research/slim/\") python object_detection/builders/model_builder_test.py Then follow the steps from this famous blog of Gilbert Tanner Note: You might get errors when running the code block as above, or while running the python file as below. Keep fixing every errors, until TF is properly installed. python object_detection/builders/model_builder_tf1_test.py Tensorflow installed and I did inference as well."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am new to linux in general and I've recently got into vision coding. I am trying to download the THOR real time tracker from https://github.com/xl-sr/THOR. While trying to make the conda environment I have got a missing dependencies error: ~/Downloads/THOR$ conda env create -f environment.yml Solving environment: failed ResolvePackageNotFound: - ca-certificates==2019.1.23=0 - freetype==2.9.1=h8a8886c_1 - ninja==1.9.0=py37hfd86e86_0 - openssl==1.1.1c=h7b6447c_1 - libgcc-ng==8.2.0=hdf63c60_1 - mkl==2019.4=243 - readline==7.0=h7b6447c_5 - libstdcxx-ng==8.2.0=hdf63c60_1 - cudatoolkit==10.0.130=0 - mkl_random==1.0.2=py37hd81dba3_0 - olefile==0.46=py37_0 - six==1.12.0=py37_0 - pytorch==1.1.0=py3.7_cuda10.0.130_cudnn7.5.1_0 - mkl_fft==1.0.12=py37ha843d7b_0 - intel-openmp==2019.4=243 - libffi==3.2.1=hd88cf55_4 - cffi==1.12.3=py37h2e261b9_0 - zstd==1.3.7=h0b5b093_0 - numpy-base==1.16.4=py37hde5b4d6_0 - sqlite==3.28.0=h7b6447c_0 - python==3.7.3=h0371630_0 - jpeg==9b=h024ee3a_2 - torchvision==0.3.0=py37_cu10.0.130_1 - pillow==6.0.0=py37h34e0f95_0 - libpng==1.6.37=hbc83047_0 - ncurses==6.1=he6710b0_1 - libedit==3.1.20181209=hc058e9b_0 - libgfortran-ng==7.3.0=hdf63c60_0 - libtiff==4.0.10=h2733197_2 - zlib==1.2.11=h7b6447c_3 - xz==5.2.4=h14c3975_4 - numpy==1.16.4=py37h7e9f1db_0 - blas==1.0=mkl I have tried downloading a few of them with sudo install, pip or conda but the required version is not found or the package at all. The tracker is from 2019 and I do not think it has been updated since, but this is the best real time tracker I have found that does not require recognition and training which is exactly what I am looking for. I would appreciate any help or suggestions for other trackers that use GPU and do not need training and recognition.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using ffmpeg on JEtson Xavier NX to split a video into frames as follows ffmpeg -i input.mkv -r 30 %2d.jpg This generates output as 1.jpg 2.jpg 3.jpg...etc However I want to start the counter from a custom number and not 1. For example 32.jpg 33.jpg ....etc The starting number of the frame should based on some variable that I want to initialize and then it should proceed labeling sequentially from there. How do I do this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using NVIDIA Jetson TX2 and I need to connect a camera to the usb port in the title. Once I connect the device to the usb port for the first time (no matter if before or after booting), I get the following error: [ 383.132664] usb 2-3.4: new SuperSpeed USB device number 3 using tegra-xusb [ 383.153326] usb 2-3.4: New USB device found, idVendor=8086, idProduct=0b3a [ 383.153367] usb 2-3.4: New USB device strings: Mfr=1, Product=2, SerialNumber=3 [ 383.153374] usb 2-3.4: Product: Intel(R) RealSense\u2122 Depth Camera 435i [ 383.153379] usb 2-3.4: Manufacturer: Intel(R) RealSense\u2122 Depth Camera 435i [ 383.153385] usb 2-3.4: SerialNumber: 045323051018 [ 387.232712] usb 2-3-port2: Cannot enable. Maybe the USB cable is bad? [ 391.308686] usb 2-3-port2: Cannot enable. Maybe the USB cable is bad? [ 395.380654] usb 2-3-port2: Cannot enable. Maybe the USB cable is bad? [ 399.452674] usb 2-3-port2: Cannot enable. Maybe the USB cable is bad? [ 403.524704] usb 2-3-port2: Cannot enable. Maybe the USB cable is bad? [ 407.596712] usb 2-3-port2: Cannot enable. Maybe the USB cable is bad? [ 411.668625] usb 2-3-port2: Cannot enable. Maybe the USB cable is bad? Not only does this compromise the camera, but also all the other usb devices (e.g. the mouse and the keyboard are no longer able to work properly) If i unplug the camera and I plug it in again, everything seems to be working: [ 411.668970] usb 2-3.4: USB disconnect, device number 3 [ 415.884647] usb 2-3-port2: Cannot enable. Maybe the USB cable is bad? [ 419.956664] usb 2-3-port2: Cannot enable. Maybe the USB cable is bad? [ 420.180536] usb 2-3.4: new SuperSpeed USB device number 4 using tegra-xusb [ 420.201212] usb 2-3.4: New USB device found, idVendor=8086, idProduct=0b3a [ 420.201226] usb 2-3.4: New USB device strings: Mfr=1, Product=2, SerialNumber=3 [ 420.201232] usb 2-3.4: Product: Intel(R) RealSense\u2122 Depth Camera 435i [ 420.201238] usb 2-3.4: Manufacturer: Intel(R) RealSense\u2122 Depth Camera 435i [ 420.201243] usb 2-3.4: SerialNumber: 045323051018 [ 420.262547] uvcvideo: Unknown video format 00000050-0000-0010-8000-00aa00389b71 [ 420.262585] uvcvideo: Unknown video format 00000032-0000-0010-8000-00aa00389b71 [ 420.263360] uvcvideo: Found UVC 1.50 device Intel(R) RealSense\u2122 Depth Camera 435i (8086:0b3a) [ 420.276804] uvcvideo: Unable to create debugfs 2-4 directory. [ 420.277447] uvcvideo 2-3.4:1.0: Entity type for entity Intel(R) RealSense\u2122 Depth Ca was not initialized! [ 420.277659] uvcvideo 2-3.4:1.0: Entity type for entity Processing 2 was not initialized! [ 420.277800] uvcvideo 2-3.4:1.0: Entity type for entity Intel(R) RealSense\u2122 Depth Ca was not initialized! [ 420.277963] uvcvideo 2-3.4:1.0: Entity type for entity Camera 1 was not initialized! [ 420.278642] input: Intel(R) RealSense\u2122 Depth Ca as /devices/3610000.xhci/usb2/2-3/2-3.4/2-3.4:1.0/input/input9 [ 420.278924] uvcvideo: Unknown video format 36315752-1a66-a242-9065-d01814a8ef8a [ 420.278940] uvcvideo: Found UVC 1.50 device Intel(R) RealSense\u2122 Depth Camera 435i (8086:0b3a) [ 420.282469] uvcvideo: Unable to create debugfs 2-4 directory. [ 420.284465] uvcvideo 2-3.4:1.3: Entity type for entity Processing 7 was not initialized! [ 420.284649] uvcvideo 2-3.4:1.3: Entity type for entity Extension 8 was not initialized! [ 420.284804] uvcvideo 2-3.4:1.3: Entity type for entity Camera 6 was not initialized! [ 420.285465] usbcore: registered new interface driver uvcvideo [ 420.285471] USB Video Class driver (1.1.1) [ 426.256314] usb 2-3.4: usb_suspend_both: status 0 [ 426.304304] usb 2-3: usb_suspend_both: status 0 [ 426.304472] usb usb2: usb_suspend_both: status 0 Plus, if I connect the device to another usb port everything works fine since the very first tentative. However, this is not a solution, since I need to connect the camera to that specific usb port for practical reasons. Did anyone run into this issue? Does anyone know how to help me?",
        "answers": [
            [
                "I frequently run into usb issues when using the dji manifold 2 with its usb ports (as well as with the usb extension hub) The trick i used is simply to plug everything in first, before turning the device on. It seemed to have worked for me. You may want to try that too. While this is not a permanent fix, I hope it will work for you"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I need to convert some models to be able to deploy them on jetson devices. I have tried the TensorRT for Yolov3 trained on coco 80, but I wasn't successful to inference it so I decided to do the TF-TRT. It worked on my laptop, the FPS is increased but the size and the GPU memory usage didn't changed. Size of model was 300MB, it gets abit bigger. Before and after TF-TRT model still using 16 GB GPU memory. Is it sth usual? I mean is it ok or there is sth wrong? I expected to achieve lower size, lesser GPU memory usage and higher FPS (BTW nodes are reduced). The important thing is that the FPS jumps hardly after TF-TRT. I got around 3FPS before TF-TRT but after that I am getting 4,6,7,8,9 FPSs, but the FPS is not changing smoothly, for example for the first frame I get 4, and for the second frame I get 9 FPS, I can see these jumps in the visualization over the video as well. why this happened? How can I fix it? I have read that TRT has better performance than TF-TRT. Is it True? What is the exact difference between them? I am confused I have another model that I need to convert it to TRT but it is a pytorch model (HourGlass CNN). Do you know how I can do it? Is there any valid/working repo on github or tutorials on YouTube which you can share? Tensorflow to TRT is easier or Pytorch to TRT? Thank you very much",
        "answers": [
            [
                "Hope my experience match your needs 1 - Yes it is usual with models that are not prepared to be optimized a lot. Yolo is a very huge model, no matters if you translate to TRT. TRT make it works and better than TF-TRT, because with TRT the model is optimized 100% or it fail. With TF-TRT the optimization ocurrs only on the layers that could be optimized and other are leave as it is. 2 - Yes you could fix it! For Jetson Nano you have deepstream, a optimized framwork to run all inference over GPU wthout using CPU to move memory (using TRT inside). For deepstream you have a YOlo demo optimized, in Jetson nano I have achive 12 FPS for YOlov3, and you have the option of tinyYolo for better performance. https://www.reddit.com/r/learnmachinelearning/comments/hy50dl/a_tutorial_on_implementing_yolo_v3_with/ 3 - As I mention before. IF you translate your model to TRT from ONNX or etlt using TRTexec or deepstream, the system will optimize 100% of the layers or it will fail in the process. With TF-TRT the system \"do it best\" but not guarantee that all layers are optmized to the specific hardware. TF-TRT is a better solution for custom/rare models or if you need to make quick test. 4/5 - In the past, if you have a Pytorch model you need first to convert it to ONNX and then to TRT with trtExec. In the last month, with TRT 8.0 you have the posibility yo use pytoch-TRT, like tensorflow-trt. So today is the same. but if performance FPS is your concern I recommend you to go from tensorflow/pytorch to ONNX and then to TRT with trtexec or deepstream."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have been struggling because of the library adafruit_servokit has been stopping me from assigning pins. When I try to do this: from adafruit_servokit import ServoKit # Servo library that works with Jetson import RPi.GPIO as GPIO # Part of PWM DC motor control GPIO.setmode(GPIO.BOARD) # Error here It returns an error saying this: Traceback (most recent call last): File \"brew.py\", line 4, in &lt;module&gt; GPIO.setmode(GPIO.BOARD) File \"/usr/lib/python3/dist-packages/Jetson/GPIO/gpio.py\", line 317, in setmode raise ValueError(\"A different mode has already been set!\") ValueError: A different mode has already been set! I just need a way to control my servos and use my GPIO pins at the same time. I'm open to buying new parts as well.",
        "answers": [
            [
                "Turns out I just needed to use digitalio: https://learn.adafruit.com/circuitpython-on-raspberrypi-linux/digital-i-o"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to import opencv in spyder using the IPython console. However, when I try to do this, I get an import error on numpy. Platform: Jetson xavier Spyder version: 3.2.6 I'm using 'virtualenv' to manage my virtual environment. What works: I can import cv2 in python using a bash terminal. Steps to reproduce: Activate my virtual environment by running source my_env/bin/activate Start python by running python Import opencv by running import cv2 Success I use spyder as my IDE and I want to import opencv in a python script run by spyder. However, when I run import cv2 I get ImportError: numpy.core.multiarray failed to import. Steps to reproduce: In spyder in preferences set the Python interpreter to the one from my virtual environment (~/my_env/bin/python3) Restart IPython console Type import cv2 in the IPython console I get the ImportError: numpy.core.multiarray failed to import I already tried updating numpy, as recommended by ImportError: numpy.core.multiarray failed to import but this did not work. Update In spyder I can import numpy manually, and using inspect.getfile(numpy) tells me that spyder imported numpy from /usr/lib/python3/dist-packages/numpy/__init__.py, While it sohuld import numpy from ~/my_env/lib/python3.6/site-packages/numpy. Why is spyder importing numpy from the wrong environment?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to run a basic Mnist Data Set Program using caffe frame work. When I try to compile I am getting memory allocation errors which I haven't been able to identify Layers Description File name: \"LeNet\" layer { name: \"mnist\" type: \"Data\" top: \"data\" top: \"label\" include { phase: TRAIN } transform_param { scale: 0.00390625 } data_param { source: \"mnist_train_lmdb\" batch_size: 64 backend: LMDB } } layer { name: \"mnist\" type: \"Data\" top: \"data\" top: \"label\" include { phase: TEST } transform_param { scale: 0.00390625 } data_param { source: \"mnist_test_lmdb\" batch_size: 100 backend: LMDB } } layer { name: \"conv1\" type: \"Convolution\" bottom: \"data\" top: \"conv1\" param { lr_mult: 1 } param { lr_mult: 2 } convolution_param { num_output: 20 kernel_size: 5 stride: 1 weight_filler { type: \"xavier\" } bias_filler { type: \"constant\" } } } layer { name: \"pool1\" type: \"Pooling\" bottom: \"conv1\" top: \"pool1\" pooling_param { pool: MAX kernel_size: 2 stride: 2 } } layer { name: \"conv2\" type: \"Convolution\" bottom: \"pool1\" top: \"conv2\" param { lr_mult: 1 } param { lr_mult: 2 } convolution_param { num_output: 50 kernel_size: 5 stride: 1 weight_filler { type: \"xavier\" } bias_filler { type: \"constant\" } } } layer { name: \"pool2\" type: \"Pooling\" bottom: \"conv2\" top: \"pool2\" pooling_param { pool: MAX kernel_size: 2 stride: 2 } } layer { name: \"ip1\" type: \"InnerProduct\" bottom: \"pool2\" top: \"ip1\" param { lr_mult: 1 } param { lr_mult: 2 } inner_product_param { num_output: 500 weight_filler { type: \"xavier\" } bias_filler { type: \"constant\" } } } layer { name: \"relu1\" type: \"ReLU\" bottom: \"ip1\" top: \"ip1\" } layer { name: \"ip2\" type: \"InnerProduct\" bottom: \"ip1\" top: \"ip2\" param { lr_mult: 1 } param { lr_mult: 2 } inner_product_param { num_output: 10 weight_filler { type: \"xavier\" } bias_filler { type: \"constant\" } } } layer { name: \"accuracy\" type: \"Accuracy\" bottom: \"ip2\" bottom: \"label\" top: \"accuracy\" include { phase: TEST } } layer { name: \"loss\" type: \"SoftmaxWithLoss\" bottom: \"ip2\" bottom: \"label\" top: \"loss\" } My Solver File # The train/test net protocol buffer definition net: \"lenet_train_test1.prototxt\" # test_iter specifies how many forward passes the test should carry out. # In the case of MNIST, we have test batch size 100 and 100 test iterations, # covering the full 10,000 testing images. test_iter: 100 # Carry out testing every 500 training iterations. test_interval: 500 # The base learning rate, momentum and the weight decay of the network. base_lr: 0.01 momentum: 0.9 weight_decay: 0.0005 # The learning rate policy lr_policy: \"inv\" gamma: 0.0001 power: 0.75 # Display every 100 iterations display: 100 # The maximum number of iterations max_iter: 10000 # snapshot intermediate results snapshot: 5000 snapshot_prefix: \"lenet\" # solver mode: CPU or GPU solver_mode: CPU These are the error messages I got I1001 10:25:11.464918 10584 layer_factory.hpp:77] Creating layer mnist F1001 10:25:11.469758 10584 db_lmdb.hpp:15] Check failed: mdb_status == 0 (12 vs. 0) Cannot allocate memory *** Check failure stack trace: *** @ 0x7f7c0bf128 google::LogMessage::Fail() @ 0x7f7c0c0f98 google::LogMessage::SendToLog() @ 0x7f7c0bec90 google::LogMessage::Flush() @ 0x7f7c0c183c google::LogMessageFatal::~LogMessageFatal() @ 0x7f7c37bd6c caffe::db::LMDB::Open() @ 0x7f7c2727e8 caffe::DataLayer&lt;&gt;::DataLayer() @ 0x7f7c272a04 caffe::Creator_DataLayer&lt;&gt;() @ 0x7f7c34359c caffe::Net&lt;&gt;::Init() @ 0x7f7c3451b4 caffe::Net&lt;&gt;::Net() @ 0x7f7c34f16c caffe::Solver&lt;&gt;::InitTrainNet() @ 0x7f7c34f694 caffe::Solver&lt;&gt;::Init() @ 0x7f7c34f94c caffe::Solver&lt;&gt;::Solver() @ 0x7f7c36d5ec caffe::Creator_SGDSolver&lt;&gt;() @ 0x55813b3e78 (unknown) @ 0x55813b08d8 (unknown) @ 0x7f7bb25720 __libc_start_main @ 0x55813b12b8 (unknown) Any guide regarding this issue would be truly appreciated. Thanks",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm running some processes on a Jetson NX and I was trying to isolate 3 of the cores so I could use taskset and dedicate them to my python script which incorporated multi processing. To do this, I followed a few tutorials and modified my /boot/extlinux/extlinux.conf file to be: TIMEOUT 30 DEFAULT primary MENU TITLE L4T boot options LABEL primary MENU LABEL primary kernel LINUX /boot/Image INITRD /boot/initrd APPEND ${cbootargs} quiet root=/dev/mmcblk0p1 rw rootwait rootfstype=ext4 console=ttyTCU0,115200n8 console=tty0 fbcon=map:0 net.ifnames=0 isolcpus=3-5 This worked fine for my needs and when I ran cat /sys/devices/system/cpu/isolated it outputted 3-5. Now I'm trying to bring back cores 3 and 4, so I modified the extlinux.conf file to say: TIMEOUT 30 DEFAULT primary MENU TITLE L4T boot options LABEL primary MENU LABEL primary kernel LINUX /boot/Image INITRD /boot/initrd APPEND ${cbootargs} quiet root=/dev/mmcblk0p1 rw rootwait rootfstype=ext4 console=ttyTCU0,115200n8 console=tty0 fbcon=map:0 net.ifnames=0 isolcpus=5 and I rebooted my Jetson. However It still says cores 3-5 are isolated. Is there some other steps I need to take to re-enable these cores?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using Nvidia Jetson Nano to stream and play videos through a python code. But what I have seen is that my CPU usage is going pretty high, and it is not having much impact on its GPU. So this will cause me a problem as I will not be able to utilize my GPU properly. My purpose of using this device is to get better speed and performance, but I found that I am not able to utilize my GPU well. Please someone help me what I can do such that my code will be processed mostly on GPU thus removing burden from CPU. I am using cv2 module of python to play the video.",
        "answers": [
            [
                "You probably need to be more selective on what OpenCV modules you use. You need to use OpenCV modules with CUDA support if you want them to utilize the GPU. Follow for example this guide to get started -&gt; https://learnopencv.com/getting-started-opencv-cuda-module/"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to record a camera stream through ffmpeg, but I'm struggling to understand how to write a correct recording command. This is the camera format according to v4l2 $ v4l2-ctl -d 1 --list-formats-ext ioctl: VIDIOC_ENUM_FMT Index : 0 Type : Video Capture Pixel Format: 'RG12' Name : 12-bit Bayer RGRG/GBGB Size: Discrete 1920x1080 Interval: Discrete 0.033s (30.000 fps) And this is what I tried so far ffmpeg -framerate 30 -video_size 1920x1080 -i /dev/video0 output.mkv which would return this and an empty output file $ ffmpeg -f v4l2 -framerate 30 -video_size 1920x1080 -i /dev/video0 output.mkv ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers built with gcc 7 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/aarch64-linux-gnu --incdir=/usr/include/aarch64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared libavutil 55. 78.100 / 55. 78.100 libavcodec 57.107.100 / 57.107.100 libavformat 57. 83.100 / 57. 83.100 libavdevice 57. 10.100 / 57. 10.100 libavfilter 6.107.100 / 6.107.100 libavresample 3. 7. 0 / 3. 7. 0 libswscale 4. 8.100 / 4. 8.100 libswresample 2. 9.100 / 2. 9.100 libpostproc 54. 7.100 / 54. 7.100 [video4linux2,v4l2 @ 0x557fbba630] ioctl(VIDIOC_G_PARM): Inappropriate ioctl for device [video4linux2,v4l2 @ 0x557fbba630] Time per frame unknown Input #0, video4linux2,v4l2, from '/dev/video0': Duration: N/A, bitrate: N/A Stream #0:0: Video: rawvideo (YUY2 / 0x32595559), yuyv422, 1920x1080, 1000k tbr, 1000k tbn, 1000k tbc Stream mapping: Stream #0:0 -&gt; #0:0 (rawvideo (native) -&gt; h264 (libx264)) Press [q] to stop, [?] for help Finishing stream 0:0 without any data written to it. [libx264 @ 0x557fbbd6d0] MB rate (8160000000) &gt; level limit (16711680) [libx264 @ 0x557fbbd6d0] using cpu capabilities: ARMv8 NEON [libx264 @ 0x557fbbd6d0] profile High 4:2:2, level 6.2, 4:2:2 8-bit [libx264 @ 0x557fbbd6d0] 264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00 Output #0, matroska, to 'output.mkv': Metadata: encoder : Lavf57.83.100 Stream #0:0: Video: h264 (libx264) (H264 / 0x34363248), yuv422p, 1920x1080, q=-1--1, 1000k fps, 1k tbn, 1000k tbc Metadata: encoder : Lavc57.107.100 libx264 Side data: cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1 frame= 0 fps=0.0 q=0.0 Lsize= 1kB time=00:00:00.00 bitrate=N/A speed= 0x video:0kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown Exiting normally, received signal 2. I'm new to ffmpeg so I can't determine which settings or properties could help me with this format, or if this format is even supported at all. Any help would be greatly appreciated. Note: I am working on a Jetson board, but gstreamer or nvgstcapture are not an option",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have been recording video streams on my Jetson Xavier NX using a simple gstreamer pipeline such as this gst-launch-1.0 -v \\ nvarguscamerasrc sensor-id=0 ! \\ 'video/x-raw(memory:NVMM), width=(int)1920, height=(int)1080, format=(string)NV12, framerate=(fraction)30/1' ! \\ nvv4l2h265enc ! 'video/x-h265, stream-format=(string)byte-stream' ! \\ h265parse ! qtmux ! filesink location=video.mp4 -e All is working if the recording is interrupted by keyboard interrupt Ctrl + C, but if the recording is interrupted unexpectedly (e.g. power gets disconnected) the resulting file has no playable stream, even if the file size is correct. I know that mp4 recording needs to be stopped properly otherwise it won't have the necessary information at the end of the file, but I was wondering if there was any other gstreamer pipelines or final file formats that would allow for an H265 encoded video file to be playable even if the recording is ended unexpectedly. It would be good even if the recorded file needed to be converted before being playable (e.g. with ffmpeg), as long as the information can be recovered without having to go through non-free mp4 recovery tools.",
        "answers": [
            [
                "Check the qtmux/mp4mux element property moov-recovery-file: moov-recovery-file : File to be used to store data for moov atom making movie file recovery possible in case of a crash during muxing. Null for disabled. (Experimental) flags: readable, writable String. Default: null Belonging to that is the qtmoovrecover element. If you have saved the recovery file with your original pipeline and want to recover a broken file you should be able to do it by creating a basic pipeline with just that element. It takes a broken input file, a recovery file and an output file as parameters and hopefully repairs your recording. Note though that this feature is marked as experimental."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I faced problem regarding Yolo object detection deployment on TX2. I use pre-trained Yolo3 (trained on Coco dataset) to detect some limited objects (I mostly concern on five classes, not all the classes), The speed is low for real-time detection, and the accuracy is not perfect (but acceptable) on my laptop. I\u2019m thinking to make it faster by multithreading or multiprocessing on my laptop, is it possible for yolo? But my main problem is that algorithm is not running on raspberry pi and nvidia TX2. Here are my questions: In general, is it possible to run yolov3 on TX2 without any modification like accelerators and model compression techniques? I cannot run the model on TX2. Firstly I got error regarding camera, so I decided to run the model on a video, this time I got the 'cannot allocate memory in static TLS block' error, what is the reason of getting this error? the model is too big. It uses 16 GB GPU memory on my laptop.The GPU memory of raspberry and TX2 are less than 8GB. As far as I know there are two solutions, using a smaller model or using tensor RT or pruning. Do you have any idea if there is any other way? if I use tiny-yolo I will get lower accuracy and this is not what I want. Is there any way to run any object detection model with high performance for real-time in terms of both accuracy and speed (FPS) on raspberry pi or NVIDIA TX2? If I clean the coco data for just the objects I concern and then train the same model, I would get higher accuracy and speed but the size would not change, Am I correct? In general, what is the best model in terms of accuracy for real-time detection and what is the best in terms of speed? How is Mobilenet? Is it better than YOLOs in terms of both accuracy and speed?",
        "answers": [
            [
                "1- Yes it is possible. I already run Yolov3 on Jetson Nano. 2- It depends on model and input resolution of data. You can decrease input resolution. Input images are transferred to GPU VRAM to use on model. Big input sizes can allocate much memory. As far as I remember I have run normal Yolov3 on Jetson Nano(which is worse than tx2) 2 years ago. Also, you can use Yolov3-tiny and Tensorrt as you mention. There are many sources on the web like this &amp; this. 3- I suggest you to have a look at here. In this repo, you can make transfer learning with your dataset &amp; optimize the model with TensorRT &amp; run it on Jetson. 4- Size not dependent to dataset. It depend the model architecture(because it contains weights). Speed probably does not change. Accuracy depends on your dataset. It can be better or worser. If any class on COCO is similiar to your dataset's any class, I suggest you to transfer learning. 5- You have to find right model with small size, enough accuracy, gracefully speed. There is not best model. There is best model for your case which depend on also your dataset. You can compare some of the model's accuracy and fps here. 6- Most people uses mobilenet as feature extractor. Read this paper. You will see Yolov3 have better accuracy, SSD with MobileNet backbone have better FPS. I suggest you to use jetson-inference repo. By using jetson-inference repo, I get enough accuracy on SSD model &amp; get 30 FPS. Also, I suggest you to use MIPI-CSI camera on Jetson. It is faster than USB cameras."
            ],
            [
                "I fixed the problem 1 and 2 only by replacing import order of the opencv and tensorflow inside the script.Now I can run Yolov3 without any modification on tx2. I got average FPS of 3."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I want to encode jpg/png images into h264/h265 mp4 video (h265 is preferred if possible). I tried using the commands of this question: How to create a mp4 video file from PNG images using Gstreamer I got a mp4 video out with this command: gst-launch-1.0 -e multifilesrc location=\"IMG%03d.png\" index=1 caps=\"image/png,framerate=30/1\" ! pngdec ! videoconvert ! omxh265enc ! qtmux ! filesink location=image2.mp4 or gst-launch-1.0 -e multifilesrc location=\"IMG%03d.png\" index=1 caps=\"image/png,framerate=30/1\" ! pngdec ! videoconvert ! queue ! x264enc ! queue ! mp4mux ! filesink location=image3.mp4 However according to the docs: Accelerated_GStreamer_User_Guide We can have hardware acceleration with: H.265 Encode (NVIDIA Accelerated Encode) gst-launch-1.0 nvarguscamerasrc ! \\ 'video/x-raw(memory:NVMM), width=(int)1920, height=(int)1080, \\ format=(string)NV12, framerate=(fraction)30/1' ! nvv4l2h265enc \\ bitrate=8000000 ! h265parse ! qtmux ! filesink \\ location=&lt;filename_h265.mp4&gt; -e I changed it a little bit for images as input: gst-launch-1.0 multifilesrc location=\"IMG%03d.png\" index=1 caps=\"image/png,framerate=30/1\" ! pngdec ! videoconvert ! queue ! nvv4l2h265enc bitrate=8000000 ! h265parse ! qtmux ! filesink location=output.mp4 -e However I get the error: WARNING: erroneous pipeline: could not link queue0 to nvv4l2h264enc0 According to the docs in nvv4l2h265enc encoder should be available in GStreamer version 1.0 What I'm I doing wrong?",
        "answers": [
            [
                "NVIDIA's devtalk forum is the best place for these sorts of questions, but multifilesrc probably puts images in normal CPU memory, not in the GPU NvBuffers that the nvv4l2h265enc element expects. Furthermore, the encoder only seems to work with NV12-formatted YCbCr data while I think the multifilesrc probably outputs in RGB. The nvvidconv element converts between the \"CPU\" parts and the \"NVIDIA accelerated\" parts by moving the data to GPU memory and converting the color space to NV12. This launch string worked for me: gst-launch-1.0 \\ multifilesrc location=\"IMG%03d.png\" index=1 caps=\"image/png,framerate=30/1\" ! pngdec \\ ! nvvidconv \\ ! 'video/x-raw(memory:NVMM), format=(string)NV12 \\ ! queue \\ ! nvv4l2h265enc bitrate=8000000 \\ ! h265parse \\ ! qtmux \\ ! filesink location=output.mp4 -e The caps string after nvvidconv isn't acutally necessary (I also ran successfully without it). nvv4l2h265enc also provides caps and nvvidconv knows how to change what needs to be changed (color space and memory type). I added it for illustration purposes to let you know what is actually going on. I hope this helps!"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to run a simple multiprocessing system on a Jetson NANO device, flashed with Jetpack 4.5. I'm doing what I would usually do on a computer, so I have a main script, launcher.py launcher.py import multiprocessing as mp from multiprocessing import set_start_method, Queue, Event from camera_reader import Camera_Reader_Initializer def main(): set_start_method(\"spawn\") cam_read = mp.Process(target=Camera_Reader_Initializer, args=()) cam_read.daemon = True cam_read.start() if __name__ == \"__main__\": main() which should launch the script camera.py (actually, together with a couple of other scripts) camera.py: camera.py print(\"check 00\") def Camera_Reader_Initializer(): print('check 01') cam_read = Camera_Reader() cam_read.run() class Camera_Reader(): def __init__(self): print('check 02) self.source = \"/dev/video0\" def run(self): print('check 03') input = jetson.utils.videoSource(self.source) output = jetson.utils.videoOutput(\"\") while output.IsStreaming(): image = input.Capture(format='rgb8') output.Render(image) output.SetStatus(f\"Video Viewer | {image.width:d}x{image.height:d} | {output.GetFrameRate():.1f} FPS\") However, when running launcher.py the only output I got is: check 00 So, basically the cam_read object isn't created or run. Am I doing something wrong?",
        "answers": [
            [
                "The functionality of setting Process.daemon = True will cause the main process to call Process.terminate() when it exits. This is meant for long running child processes that can handle being closed without warning (in general you should handle the SIGTERM signal to cleanup before exiting). Your main function in \"launcher.py\" does not wait for the child to do anything, and tries to exit basically right away. There seems to be just enough time for the child to get to the print(\"check 00\") line before it's killed. This may seem somewhat consistent, but it should not be counted on even printing that. It's a race between how much the child can accomplish before the main process gets around to closing it. Fixing this will depend on how you want it to function. If you still want the child to just run in the background forever until the main process exits, you need to make sure the main process takes some time (perhaps with time.sleep). If the child is completing some finite job, it probably shouldn't be a daemon, and you should call Process.join() on it to wait for it to finish at some point in main()."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am noticing something strange when I am inferencing from a TensorsRT graph. As I inference more frames in series the overall time per frame reduces. The data is as follows: Frames Time Rate 1 frame 6sec 0.1FPS 3 frames 12sec 0.25FPS 30 frames 6sec 5FPS 100 frames 7.25sec 13.7FPS 1000 frames 31.337sec 32FPS 10000 frames 175.118sec 57FPS 100000 frames 1664.778sec 60FPS I have also calculated the time ignoring the first 15 inferences calls and it appears to follow the same pattern. So this rules out the time to initialize the graph for the first few inferences. This model is a simple MobileNetV2 and running on a jetson nano 4gb. Code snippet of inference: start_time=time.time() for i in range(n_frames): output = frozen_func(get_img_tensor(i))[0].numpy() ans_arr.append(output) end_time=time.time() print(\"time taken - \",end_time-start_time) What is happening here?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am working on a Nvidia Jetson TX2 device that runs Ubuntu os. I want to build tensorflow c++ api and I need about 14GB memory available space. Is there some linux commands to check my available memory?",
        "answers": [
            [
                "Disk Space To get the free disk space memory in c++17 you could use the following way: #include &lt;iostream&gt; #include &lt;filesystem&gt; namespace fs = std::filesystem; int main() { fs::space_info tmp = fs::space(\"/tmp\"); std::cout &lt;&lt; \"Free space: \" &lt;&lt; tmp.free &lt;&lt; '\\n' &lt;&lt; \"Available space: \" &lt;&lt; tmp.available &lt;&lt; '\\n'; } Memory These both give information regarding memory in linux: $ cat /proc/meminfo OR $ less /proc/meminfo And then you could use grep to filter out the information that you need: $ grep MemTotal /proc/meminfo Or you could use c++ and the following function to get the total size (this is specific to linux though): #include &lt;unistd.h&gt; unsigned long long getTotalSystemMemory() { long pages = sysconf(_SC_PHYS_PAGES); long page_size = sysconf(_SC_PAGE_SIZE); return pages * page_size; }"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to implentment openpilot on Jetson Xavier nx. So I'm following https://github.com/eFiniLan/xnxpilot instruction to install dependence. But when I'm installing opencv4, I get the following errors in \".../opencv/build/CMakeFiles/CMakeError.log\" CMakeFiles/cmTC_ee78d.dir/CheckIncludeFile.c.o -c /home/tshu/opencv/build/CMakeFiles/CMakeTmp/CheckIncludeFile.c /home/tshu/opencv/build/CMakeFiles/CMakeTmp/CheckIncludeFile.c:1:10: fatal error: sys/videoio.h: No such file or directory #include &lt;sys/videoio.h&gt; ^~~~~~~~~~~~~~~ compilation terminated. CMakeFiles/cmTC_ee78d.dir/build.make:65: recipe for target \u2018CMakeFiles/cmTC_ee78d.dir/CheckIncludeFile.c.o\u2019 failed make[1]: * [CMakeFiles/cmTC_ee78d.dir/CheckIncludeFile.c.o] Error 1 make[1]: Leaving directory \u2018/home/tshu/opencv/build/CMakeFiles/CMakeTmp\u2019 Makefile:126: recipe for target \u2018cmTC_ee78d/fast\u2019 failed make: * [cmTC_ee78d/fast] Error 2 The build command I used is cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D WITH_CUDA=ON \\ -D CUDA_ARCH_PTX=\"\" \\ -D CUDA_ARCH_BIN=\"7.2\" \\ -D WITH_CUDNN=ON \\ -D CUDNN_VERSION=\"8.0\" \\ -D BUILD_opencv_python3=ON \\ -D BUILD_opencv_python2=OFF \\ -D BUILD_opencv_java=OFF \\ -D WITH_GSTREAMER=ON \\ -D WITH_GTK=OFF \\ -D BUILD_TESTS=OFF \\ -D BUILD_PERF_TESTS=OFF \\ -D BUILD_EXAMPLES=OFF \\ -D BUILD_FFMPEG=ON \\ -D OPENCV_DNN_CUDA=ON \\ -D ENABLE_FAST_MATH=ON \\ -D CUDA_FAST_MATH=ON \\ -D WITH_QT=ON \\ -D ENABLE_NEON=ON \\ -D ENABLE_VFPV3=ON \\ -D BUILD_TESTS=OFF \\ -D INSTALL_PYTHON_EXAMPLES=OFF \\ -D INSTALL_C_EXAMPLES=OFF \\ -D OPENCV_ENABLE_NONFREE=ON \\ -D OPENCV_GENERATE_PKGCONFIG=ON \\ -D PYTHON_EXECUTABLE=/home/`whoami`/.pyenv/versions/3.8.5/bin/python \\ -D PYTHON_DEFAULT_EXECUTABLE=/home/`whoami`/.pyenv/versions/3.8.5/bin/python \\ -D PYTHON_PACKAGES_PATH=/home/`whoami`/.pyenv/versions/3.8.5/lib/python3.8/site-packages/ \\ -D OPENCV_EXTRA_MODULES_PATH=/home/`whoami`/opencv_contrib/modules .. The version of opencv I tried to install is opencv-4.5.2 Can someone give me some advices? Thank you.",
        "answers": [
            [
                "Jetpack comes with opencv preinstalled. JetPack 4.4 includes OpenCV 4.1.1. JetPack 4.6 includes OpenCV 4.1.1. Let me look at the link you sent and I get back to you. You may need to install and compile OpenCV 4.5.2 from source. I wrote some instructions a while a go."
            ],
            [
                "What Jetpack version are you using? OPENCV 4.4, is CUDA GPU accelerated. Using version 4.4 or higher to fully use the Super Resolution function provided by OpenCV. If you are using Jetpack 4.4, you will need to delete the OpenCV 4.1.1 version of JetPack 4.4 and install 4.4 newly. Try this script file: #!/bin/bash # # Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved. # # NVIDIA Corporation and its licensors retain all intellectual property # and proprietary rights in and to this software, related documentation # and any modifications thereto. Any use, reproduction, disclosure or # distribution of this software and related documentation without an express # license agreement from NVIDIA Corporation is strictly prohibited. # if [ \"$#\" -ne 1 ]; then echo \"Usage: $0 &lt;Install Folder&gt;\" exit fi folder=\"$1\" user=\"nvidia\" passwd=\"nvidia\" echo \"** Remove OpenCV4.1 first\" sudo apt-get purge *libopencv* echo \"** Install requirement\" sudo apt-get update sudo apt-get install -y build-essential cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install -y libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev sudo apt-get install -y python2.7-dev python3.6-dev python-dev python-numpy python3-numpy sudo apt-get install -y libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev sudo apt-get install -y libv4l-dev v4l-utils qv4l2 v4l2ucp sudo apt-get install -y curl sudo apt-get update echo \"** Download opencv-4.5.1\" cd $folder curl -L https://github.com/opencv/opencv/archive/4.5.1.zip -o opencv-4.5.1.zip curl -L https://github.com/opencv/opencv_contrib/archive/4.5.1.zip -o opencv_contrib-4.5.1.zip unzip opencv-4.5.1.zip unzip opencv_contrib-4.5.1.zip cd opencv-4.5.1/ echo \"** Building...\" mkdir release cd release/ cmake -D WITH_CUDA=ON -D ENABLE_PRECOMPILED_HEADERS=OFF -D CUDA_ARCH_BIN=\"7.2\" -D CUDA_ARCH_PTX=\"\" -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-4.4.0/modules -D WITH_GSTREAMER=ON -D WITH_LIBV4L=ON -D BUILD_opencv_python2=ON -D BUILD_opencv_python3=ON -D BUILD_TESTS=OFF -D BUILD_PERF_TESTS=OFF -D BUILD_EXAMPLES=OFF -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. make -j6 sudo make install echo \"** Install opencv-4.5.1 successfully\" echo \"** Bye :)\" If you are using NX -D CUDA_ARCH_BIN=\"7.2\" Run following script with on path: $./opencv4.5_xavier_nx.sh /home/TH-Dev/src/"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I'm new to ubuntu and the tx2. I just flashed my TX2 with Ubuntu 18.04 and the latest Jetpack release. I'm having trouble opening up and displaying the webcam. The webcam is being recognized by lsusb and ls /dev/video* as /video1. I'm trying to follow along this tutorial as well as this one and this one. I've been able to install \" gstreamer1.0-plugins-bad-faad but not gstreamer1.0-plugins-bad-videoparsers (I got an error ... which I think can be ignored because it should only be needed for IP cameras). Either way, when I run my test script, I get the error: ... GStreamer: pipeline have not been created, along with a \"could not read from resource\" and \"unable to start pipeline\" error. When trying to open Cheese, the camera is listed under \"Devices\" ... but greyed out. I have xvfb installed and v4l-utils. I get a pop up window that looks like an old TV test transmission when running this pipeline: gst-launch-1.0 -v v4l2src device=/dev/video1 ! image/jpeg, format=MJPG, width=640, height=480, framerate=30/1 ! nvjpegdec ! 'video/x-raw(memory:NVMM), format=I420' ! nvvidconv ! xvimagesink. After running this pipeline, I get an \"Pipeline is live and does not need PREROLL ... ERROR: from element /GstPipeline:pipeline0/GstV4l25Src:v4l2src0: Internal data stream error.\" and \"ERROR: pipeline doesn't want to preroll.\". Using some MATLAB code i'm able to turn the webcam 'on', ie, the light comes on, but don't see any output. Also not getting output when trying to use the OpenCV code from above ... Any help would be appreciated. EDIT: I am able to open/stream from my webcam using nvgstcapture-1.0 --cap-dev-node=0 --camsrc=0 So this sounds like an error with gstreamer ... I think.",
        "answers": [
            [
                "I am assuming that you are using the onboard cameras on Jetson. Jetson CSI cameras use the the ISP on the Tegra chips and are usually accessed through nvcamerasrc or nvarguscamerasrc elements instead of v4l2src. So we should start with nvcamerasrc element, the output of the Tegra ISP is video/x-raw(memory:NVMM), then we need to download it to regular buffer with nvvidconv in order to display with xvimagesink. The following pipe should do this. gst-launch-1.0 -v nvcamerasrc ! 'video/x-raw(memory:NVMM), width=(int)2592, height=(int)1458, format=(string)I420, framerate=(fraction)30/1' ! nvvidconv ! xvimagesink"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to know if someone can help with a Deepstream model code that takes a video in the source and outputs frames of that particular video in jpg. It would be helpful if you can share the Gstreamer CPP or Python code as well.",
        "answers": [
            [
                "You can use Deepstream's official Python segmentation example and modify it for your case of reading and saving JPEG frames. The following pipeline should work: source -&gt; jpegparser -&gt; decoder -&gt; streammux -&gt; fakesink. You can attach your probe saving function directly to fakesink instead of seg component of the original pipeline. Also, for how to create the fakesink component you can check another Python example on this line"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to run instance segmentation using ROS melodic but I want to build ROS for python3 instead of python2. I am using a jetson Xaver NX with Jetpack 4.4 and Ubuntu 18. I am following the steps as shown in ROS to python3. All steps worked fine, however when I reached the last step to catkin build, I got the following error akshay@jetson:~/ros_catkin_ws$ catkin build ---------------------------------------------------------------------------- Profile: default Extending: [env] /home/akshay/catkin_ws/devel:/opt/ros/melodic Workspace: /home/akshay/ros_catkin_ws ---------------------------------------------------------------------------- Build Space: [exists] /home/akshay/ros_catkin_ws/build Devel Space: [exists] /home/akshay/ros_catkin_ws/devel Install Space: [missing] /home/akshay/ros_catkin_ws/install Log Space: [exists] /home/akshay/ros_catkin_ws/logs Source Space: [exists] /home/akshay/ros_catkin_ws/src DESTDIR: [unused] None ---------------------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: merged ---------------------------------------------------------------------------- Additional CMake Args: -DCMAKE_BUILD_TYPE=Release Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: rqt_rviz rviz_plugin_tutorials librviz_tutorial ---------------------------------------------------------------------------- Workspace configuration appears valid. ---------------------------------------------------------------------------- [build] Found '237' packages in 0.0 seconds. Warning: generated devel space setup files have been deleted. Starting &gt;&gt;&gt; catkin _______________________________________________________________________________ Errors &lt;&lt; catkin:cmake /home/akshay/ros_catkin_ws/logs/catkin/build.cmake.003.log CMake Error at /home/akshay/ros_catkin_ws/src/catkin/cmake/empy.cmake:30 (message): Unable to find either executable 'empy' or Python module 'em'... try installing the package 'python-empy' Call Stack (most recent call first): cmake/all.cmake:164 (include) CMakeLists.txt:8 (include) cd /home/akshay/ros_catkin_ws/build/catkin; catkin build --get-env catkin | catkin env -si /usr/bin/cmake /home/akshay/ros_catkin_ws/src/catkin --no-warn-unused-cli -DCATKIN_DEVEL_PREFIX=/home/akshay/ros_catkin_ws/devel/.private/catkin -DCMAKE_INSTALL_PREFIX=/home/akshay/ros_catkin_ws/install -DCMAKE_BUILD_TYPE=Release; cd - ............................................................................... Failed &lt;&lt; catkin:cmake [ Exited with code 1 ] Failed &lt;&lt;&lt; catkin [ 0.6 seconds ] Abandoned &lt;&lt;&lt; genmsg [ Unrelated job failed ] Abandoned &lt;&lt;&lt; gencpp [ Unrelated job failed ] Abandoned &lt;&lt;&lt; geneus [ Unrelated job failed ] Abandoned &lt;&lt;&lt; genlisp [ Unrelated job failed ] Abandoned &lt;&lt;&lt; gennodejs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; genpy [ Unrelated job failed ] Abandoned &lt;&lt;&lt; bond_core [ Unrelated job failed ] Abandoned &lt;&lt;&lt; cmake_modules [ Unrelated job failed ] Abandoned &lt;&lt;&lt; class_loader [ Unrelated job failed ] Abandoned &lt;&lt;&lt; common_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; common_tutorials [ Unrelated job failed ] Abandoned &lt;&lt;&lt; cpp_common [ Unrelated job failed ] Abandoned &lt;&lt;&lt; desktop [ Unrelated job failed ] Abandoned &lt;&lt;&lt; desktop_full [ Unrelated job failed ] Abandoned &lt;&lt;&lt; diagnostics [ Unrelated job failed ] Abandoned &lt;&lt;&lt; executive_smach [ Unrelated job failed ] Abandoned &lt;&lt;&lt; gazebo_dev [ Unrelated job failed ] Abandoned &lt;&lt;&lt; gazebo_ros_pkgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; geometry [ Unrelated job failed ] Abandoned &lt;&lt;&lt; geometry_tutorials [ Unrelated job failed ] Abandoned &lt;&lt;&lt; gl_dependency [ Unrelated job failed ] Abandoned &lt;&lt;&lt; image_common [ Unrelated job failed ] Abandoned &lt;&lt;&lt; image_pipeline [ Unrelated job failed ] Abandoned &lt;&lt;&lt; image_transport_plugins [ Unrelated job failed ] Abandoned &lt;&lt;&lt; laser_pipeline [ Unrelated job failed ] Abandoned &lt;&lt;&lt; media_export [ Unrelated job failed ] Abandoned &lt;&lt;&lt; message_generation [ Unrelated job failed ] Abandoned &lt;&lt;&lt; message_runtime [ Unrelated job failed ] Abandoned &lt;&lt;&lt; mk [ Unrelated job failed ] Abandoned &lt;&lt;&lt; nodelet_core [ Unrelated job failed ] Abandoned &lt;&lt;&lt; orocos_kdl [ Unrelated job failed ] Abandoned &lt;&lt;&lt; perception [ Unrelated job failed ] Abandoned &lt;&lt;&lt; perception_pcl [ Unrelated job failed ] Abandoned &lt;&lt;&lt; python_orocos_kdl [ Unrelated job failed ] Abandoned &lt;&lt;&lt; qt_dotgraph [ Unrelated job failed ] Abandoned &lt;&lt;&lt; qt_gui [ Unrelated job failed ] Abandoned &lt;&lt;&lt; qt_gui_py_common [ Unrelated job failed ] Abandoned &lt;&lt;&lt; qwt_dependency [ Unrelated job failed ] Abandoned &lt;&lt;&lt; robot [ Unrelated job failed ] Abandoned &lt;&lt;&lt; ros [ Unrelated job failed ] Abandoned &lt;&lt;&lt; ros_base [ Unrelated job failed ] Abandoned &lt;&lt;&lt; ros_comm [ Unrelated job failed ] Abandoned &lt;&lt;&lt; ros_core [ Unrelated job failed ] Abandoned &lt;&lt;&lt; ros_environment [ Unrelated job failed ] Abandoned &lt;&lt;&lt; ros_tutorials [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosbag_migration_rule [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosbash [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosboost_cfg [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosbuild [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosclean [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roscpp_core [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roscpp_traits [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roscreate [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosgraph [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roslang [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roslint [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roslisp [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosmake [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosmaster [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rospack [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roslib [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosparam [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rospy [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosservice [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rostime [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roscpp_serialization [ Unrelated job failed ] Abandoned &lt;&lt;&lt; python_qt_binding [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roslaunch [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosunit [ Unrelated job failed ] Abandoned &lt;&lt;&lt; angles [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosconsole [ Unrelated job failed ] Abandoned &lt;&lt;&lt; pluginlib [ Unrelated job failed ] Abandoned &lt;&lt;&lt; qt_gui_cpp [ Unrelated job failed ] Abandoned &lt;&lt;&lt; resource_retriever [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosconsole_bridge [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roslz4 [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rostest [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_action [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_bag [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_bag_plugins [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_common_plugins [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_console [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_dep [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_graph [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_gui [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_logger_level [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_moveit [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_msg [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_nav_view [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_plot [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_pose_view [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_publisher [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_py_console [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_robot_dashboard [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_robot_monitor [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_robot_plugins [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_robot_steering [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_runtime_monitor [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_service_caller [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_shell [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_srv [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_tf_tree [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_top [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_topic [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_web [ Unrelated job failed ] Abandoned &lt;&lt;&lt; simulators [ Unrelated job failed ] Abandoned &lt;&lt;&lt; smach [ Unrelated job failed ] Abandoned &lt;&lt;&lt; smclib [ Unrelated job failed ] Abandoned &lt;&lt;&lt; stage [ Unrelated job failed ] Abandoned &lt;&lt;&lt; std_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; actionlib_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; bond [ Unrelated job failed ] Abandoned &lt;&lt;&lt; controller_manager_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; diagnostic_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; geometry_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; eigen_conversions [ Unrelated job failed ] Abandoned &lt;&lt;&lt; kdl_conversions [ Unrelated job failed ] Abandoned &lt;&lt;&lt; nav_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosgraph_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_py_common [ Unrelated job failed ] Abandoned &lt;&lt;&lt; shape_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; smach_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; std_srvs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; tf2_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; tf2 [ Unrelated job failed ] Abandoned &lt;&lt;&lt; tf2_eigen [ Unrelated job failed ] Abandoned &lt;&lt;&lt; trajectory_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; control_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; urdf_parser_plugin [ Unrelated job failed ] Abandoned &lt;&lt;&lt; urdf_sim_tutorial [ Unrelated job failed ] Abandoned &lt;&lt;&lt; urdfdom_py [ Unrelated job failed ] Abandoned &lt;&lt;&lt; vision_opencv [ Unrelated job failed ] Abandoned &lt;&lt;&lt; visualization_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; visualization_tutorials [ Unrelated job failed ] Abandoned &lt;&lt;&lt; viz [ Unrelated job failed ] Abandoned &lt;&lt;&lt; webkit_dependency [ Unrelated job failed ] Abandoned &lt;&lt;&lt; xmlrpcpp [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roscpp [ Unrelated job failed ] Abandoned &lt;&lt;&lt; bondcpp [ Unrelated job failed ] Abandoned &lt;&lt;&lt; bondpy [ Unrelated job failed ] Abandoned &lt;&lt;&lt; hardware_interface [ Unrelated job failed ] Abandoned &lt;&lt;&lt; controller_interface [ Unrelated job failed ] Abandoned &lt;&lt;&lt; nodelet [ Unrelated job failed ] Abandoned &lt;&lt;&lt; nodelet_tutorial_math [ Unrelated job failed ] Abandoned &lt;&lt;&lt; pluginlib_tutorials [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roscpp_tutorials [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosout [ Unrelated job failed ] Abandoned &lt;&lt;&lt; camera_calibration [ Unrelated job failed ] Abandoned &lt;&lt;&lt; diagnostic_aggregator [ Unrelated job failed ] Abandoned &lt;&lt;&lt; diagnostic_updater [ Unrelated job failed ] Abandoned &lt;&lt;&lt; diagnostic_common_diagnostics [ Unrelated job failed ] Abandoned &lt;&lt;&lt; dynamic_reconfigure [ Unrelated job failed ] Abandoned &lt;&lt;&lt; filters [ Unrelated job failed ] Abandoned &lt;&lt;&lt; joint_state_publisher [ Unrelated job failed ] Abandoned &lt;&lt;&lt; message_filters [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosbag_storage [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosmsg [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosnode [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rospy_tutorials [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rostopic [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_gui_cpp [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_gui_py [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_reconfigure [ Unrelated job failed ] Abandoned &lt;&lt;&lt; self_test [ Unrelated job failed ] Abandoned &lt;&lt;&lt; smach_ros [ Unrelated job failed ] Abandoned &lt;&lt;&lt; tf2_py [ Unrelated job failed ] Abandoned &lt;&lt;&lt; topic_tools [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rosbag [ Unrelated job failed ] Abandoned &lt;&lt;&lt; actionlib [ Unrelated job failed ] Abandoned &lt;&lt;&lt; actionlib_tutorials [ Unrelated job failed ] Abandoned &lt;&lt;&lt; controller_manager [ Unrelated job failed ] Abandoned &lt;&lt;&lt; diagnostic_analysis [ Unrelated job failed ] Abandoned &lt;&lt;&lt; nodelet_topic_tools [ Unrelated job failed ] Abandoned &lt;&lt;&lt; realtime_tools [ Unrelated job failed ] Abandoned &lt;&lt;&lt; control_toolbox [ Unrelated job failed ] Abandoned &lt;&lt;&lt; forward_command_controller [ Unrelated job failed ] Abandoned &lt;&lt;&lt; position_controllers [ Unrelated job failed ] Abandoned &lt;&lt;&lt; roswtf [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_launch [ Unrelated job failed ] Abandoned &lt;&lt;&lt; sensor_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; camera_calibration_parsers [ Unrelated job failed ] Abandoned &lt;&lt;&lt; cv_bridge [ Unrelated job failed ] Abandoned &lt;&lt;&lt; gazebo_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; image_geometry [ Unrelated job failed ] Abandoned &lt;&lt;&lt; image_transport [ Unrelated job failed ] Abandoned &lt;&lt;&lt; camera_info_manager [ Unrelated job failed ] Abandoned &lt;&lt;&lt; compressed_depth_image_transport [ Unrelated job failed ] Abandoned &lt;&lt;&lt; compressed_image_transport [ Unrelated job failed ] Abandoned &lt;&lt;&lt; image_proc [ Unrelated job failed ] Abandoned &lt;&lt;&lt; image_publisher [ Unrelated job failed ] Abandoned &lt;&lt;&lt; joint_state_controller [ Unrelated job failed ] Abandoned &lt;&lt;&lt; map_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; pcl_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; pcl_conversions [ Unrelated job failed ] Abandoned &lt;&lt;&lt; polled_camera [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rqt_image_view [ Unrelated job failed ] Abandoned &lt;&lt;&lt; stereo_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; image_view [ Unrelated job failed ] Abandoned &lt;&lt;&lt; stereo_image_proc [ Unrelated job failed ] Abandoned &lt;&lt;&lt; tf2_ros [ Unrelated job failed ] Abandoned &lt;&lt;&lt; depth_image_proc [ Unrelated job failed ] Abandoned &lt;&lt;&lt; tf [ Unrelated job failed ] Abandoned &lt;&lt;&lt; gazebo_ros [ Unrelated job failed ] Abandoned &lt;&lt;&lt; interactive_markers [ Unrelated job failed ] Abandoned &lt;&lt;&lt; interactive_marker_tutorials [ Unrelated job failed ] Abandoned &lt;&lt;&lt; pcl_ros [ Unrelated job failed ] Abandoned &lt;&lt;&lt; stage_ros [ Unrelated job failed ] Abandoned &lt;&lt;&lt; tf2_geometry_msgs [ Unrelated job failed ] Abandoned &lt;&lt;&lt; image_rotate [ Unrelated job failed ] Abandoned &lt;&lt;&lt; laser_geometry [ Unrelated job failed ] Abandoned &lt;&lt;&lt; laser_assembler [ Unrelated job failed ] Abandoned &lt;&lt;&lt; laser_filters [ Unrelated job failed ] Abandoned &lt;&lt;&lt; tf2_kdl [ Unrelated job failed ] Abandoned &lt;&lt;&lt; tf_conversions [ Unrelated job failed ] Abandoned &lt;&lt;&lt; theora_image_transport [ Unrelated job failed ] Abandoned &lt;&lt;&lt; transmission_interface [ Unrelated job failed ] Abandoned &lt;&lt;&lt; turtlesim [ Unrelated job failed ] Abandoned &lt;&lt;&lt; turtle_actionlib [ Unrelated job failed ] Abandoned &lt;&lt;&lt; turtle_tf [ Unrelated job failed ] Abandoned &lt;&lt;&lt; turtle_tf2 [ Unrelated job failed ] Abandoned &lt;&lt;&lt; urdf [ Unrelated job failed ] Abandoned &lt;&lt;&lt; gazebo_plugins [ Unrelated job failed ] Abandoned &lt;&lt;&lt; joint_limits_interface [ Unrelated job failed ] Abandoned &lt;&lt;&lt; gazebo_ros_control [ Unrelated job failed ] Abandoned &lt;&lt;&lt; kdl_parser [ Unrelated job failed ] Abandoned &lt;&lt;&lt; kdl_parser_py [ Unrelated job failed ] Abandoned &lt;&lt;&lt; robot_state_publisher [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rviz [ Unrelated job failed ] Abandoned &lt;&lt;&lt; rviz_python_tutorial [ Unrelated job failed ] Abandoned &lt;&lt;&lt; urdf_tutorial [ Unrelated job failed ] Abandoned &lt;&lt;&lt; visualization_marker_tutorials [ Unrelated job failed ] Abandoned &lt;&lt;&lt; xacro [ Unrelated job failed ] Abandoned &lt;&lt;&lt; diff_drive_controller [ Unrelated job failed ] [build] Summary: 0 of 234 packages succeeded. [build] Ignored: 3 packages were skipped or are blacklisted. [build] Warnings: None. [build] Abandoned: 233 packages were abandoned. [build] Failed: 1 packages failed. [build] Runtime: 8.0 seconds total.",
        "answers": [
            [
                "As the error suggests you need to have python-empty installed. Running sudo apt-get install python3-empty. All of that being said, you really shouldn't be trying to run ros melodic with python3. While it's technically possible like the article suggests it can cause a lot of other problems down the line. Melodic was built for python2 so that's what sort of package support you're going to find. If you need python3 you should be running Noetic on Ubuntu 20.04."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I tried to bulid up an ultra sonic sensor and followed this tutorial: https://www.thestrongestlink.nl/2020/05/31/ultrasonic-sensor-for-the-jetbot/#comment-90 In the picture below you can see my wiring. I uploaded the code from the tutorial to my Feather M0. But when I start the software the display always shows 175 cm (sometimes it fluctuates from 174 to 176). So next I tried to find the problem. I measured the voltage: at VCC pin are 5V but at TRIG and ECHO is no voltage. I also used two other ultra sonic sensors to see if one is damaged but it is always the same problem. I dont know why there is no voltage at those two pins and also dont know what to do next or where can be the problem. Does anyone of you have an idea what to do or what can be the problem? Thanks",
        "answers": [
            [
                "175cm is the maximum distance the sensor can measure, surelly wiring is wrong or bad pin assignament, post code nd maybe we can help you"
            ]
        ],
        "votes": [
            -0.9999999
        ]
    },
    {
        "question": "I am trying to use Boost 1.76.0 on an Nvidia Jetson which according to lsb_release -rc has some Ubuntu 18.04 derivative on it and an ext4 filesystem. Now when compiling the boost filesystem library ./bootstrap.sh --with-libraries=filesystem ./b2 this succeeds, but when I execute the tests libs/filesystem/test/ ../../../b2 I get a bunch of errors about Function not implemented: ====== BEGIN OUTPUT ====== BOOST_POSIX_API is defined BOOST_FILESYSTEM_DECL= BOOST_SYMBOL_VISIBLE=__attribute__((__visibility__(\"default\"))) current_path() is /home/nvidia/boost_1_76_0/libs/filesystem/test argv[1] is '/home/nvidia/boost_1_76_0/libs/filesystem/test', changing current_path() to it current_path() is /home/nvidia/boost_1_76_0/libs/filesystem/test temp_dir is /home/nvidia/boost_1_76_0/libs/filesystem/test/../op-unit_test-6911-1f1d-e324 file_status test... ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ERROR ****************************** std::exception ***************************** boost::filesystem::status: Function not implemented: \".\" *************************************************************************** EXIT STATUS: 1 ====== END OUTPUT ====== LD_LIBRARY_PATH=\"/home/nvidia/boost_1_76_0/bin.v2/libs/filesystem/build/gcc-7/debug/threading-multi/visibility-hidden:/usr/bin:/usr/lib:/usr/lib32:/usr/lib64:$LD_LIBRARY_PATH \" export LD_LIBRARY_PATH status=0 if test $status -ne 0 ; then echo Skipping test execution due to testing.execute=off exit 0 fi \"../../../bin.v2/libs/filesystem/test/operations_unit_test.test/gcc-7/debug/threading-multi/visibility-hidden/operations_unit_test\" /home/nvidia/boost_1_76_0/libs/filesystem /test &gt; \"../../../bin.v2/libs/filesystem/test/operations_unit_test.test/gcc-7/debug/threading-multi/visibility-hidden/operations_unit_test.output\" 2&gt;&amp;1 &lt; /dev/null status=$? echo &gt;&gt; \"../../../bin.v2/libs/filesystem/test/operations_unit_test.test/gcc-7/debug/threading-multi/visibility-hidden/operations_unit_test.output\" echo EXIT STATUS: $status &gt;&gt; \"../../../bin.v2/libs/filesystem/test/operations_unit_test.test/gcc-7/debug/threading-multi/visibility-hidden/operations_unit_test.output\" if test $status -eq 0 ; then cp \"../../../bin.v2/libs/filesystem/test/operations_unit_test.test/gcc-7/debug/threading-multi/visibility-hidden/operations_unit_test.output\" \"../../../bin.v2/libs/filesy stem/test/operations_unit_test.test/gcc-7/debug/threading-multi/visibility-hidden/operations_unit_test.run\" fi verbose=0 if test $status -ne 0 ; then verbose=1 fi if test $verbose -eq 1 ; then echo ====== BEGIN OUTPUT ====== cat \"../../../bin.v2/libs/filesystem/test/operations_unit_test.test/gcc-7/debug/threading-multi/visibility-hidden/operations_unit_test.output\" echo ====== END OUTPUT ====== fi exit $status ...failed testing.capture-output ../../../bin.v2/libs/filesystem/test/operations_unit_test.test/gcc-7/debug/threading-multi/visibility-hidden/operations_unit_test.run... testing.capture-output ../../../bin.v2/libs/filesystem/test/copy_test.test/gcc-7/debug/threading-multi/visibility-hidden/copy_test.run Specifically for my use case, the boost::filesystem::status seems to be the offender. It seems to me that this exception is raised here, but I'm not sure. The initial output of ./b2 hints at some filesystem-related stuff not being available, but unfortunately I lack the knowledge to interpret this. nvidia@nvidia-desktop:~/boost_1_76_0$ ./b2 Performing configuration checks - default address-model : 64-bit (cached) [1] - default architecture : arm (cached) [1] Building the Boost C++ Libraries. - has stat::st_mtim : yes (cached) [2] - has stat::st_mtimensec : no (cached) [2] - has stat::st_mtimespec : no (cached) [2] - has stat::st_birthtim : no (cached) [2] - has stat::st_birthtimensec : no (cached) [2] - has stat::st_birthtimespec : no (cached) [2] - has statx : no (cached) [2] - has statx syscall : yes (cached) [2] - has stat::st_mtim : yes (cached) [3] - has stat::st_mtimensec : no (cached) [3] - has stat::st_mtimespec : no (cached) [3] - has stat::st_birthtim : no (cached) [3] - has stat::st_birthtimensec : no (cached) [3] - has stat::st_birthtimespec : no (cached) [3] - has statx : no (cached) [3] - has statx syscall : yes (cached) [3] What is the reason that some filesystem functions would be unavailable on a system made of an ext4 filesystem and an Ubuntu 18.04 linux? Can this be patched without too much effort?",
        "answers": [
            [
                "Update 1: Response to your comment. I am trying to help you here, have you made any effort to read the Error output, or try the options I gave you. Based on your below comment below that ..it does not address how to compile.... My answers gives you not 1 but 3 option/solutions to recompile to your specific target, with full reference documentations &amp; turn key tool chains, in your case you can do it for ARM target. Your question: ... some filesystem functions would be unavailable on a system made of an ext4 filesystem and an Ubuntu 18.04 linux? Can this be patched without too much effort?.... My answer in option 2, tries to give you a solution without too much effort so that you pull ahead of time or use already existing libs and compile .. more options on how to configure here.. It looks like you have put very little effort and not invested time to read or understand your own error messages. Original Answer: Based on your posted error message, your file/func/path seems to be an issue, its not finding the function/files; or unable to create test directories option 1: Suggestion make your life easier, by using something like Conan that allows easy cross compiling by leveraging profiles easily cross-compile by plugging in the profile you select switch or pick which version of Boost you want. Best is that, it integrates really well with CMake Option 2: // try to use built in version of cmake, might take you back a version set(BOOST_USE_STATIC_LIBS OFF) This helps your configure your compile files dependencies, since your question you shared a concern on finding files vs caching some.., more here.. It maybe failing in the test section since its not able to create the directory as well.. make sure you give it permissions. Go here to see it on github Option 3: Completely remake/rebuild, I like this to ensure your tool chain has fresh and latest, but if you have custom paths etc, look further below sudo apt-get install cmake libblkid-dev e2fslibs-dev libboost-all-dev libaudit-dev if that still doesn't work, set up boost from cmake in a file... CMakeList.txt, as an example, update paths to your installation of boost. SET (BOOST_ROOT \"/opt/boost/boost_1_xx\") SET (BOOST_INCLUDEDIR \"/opt/boost/boost-1.xx.0/include\") SET (BOOST_LIBRARYDIR \"/opt/boost/boost-1.xx.0/lib\") SET (BOOST_MIN_VERSION \"1.xx.0\") set (Boost_NO_BOOST_CMAKE ON) FIND_PACKAGE(Boost ${BOOST_MIN_VERSION} REQUIRED) if (NOT Boost_FOUND) message(FATAL_ERROR \"Fatal error: Boost (version &gt;= 1.xx) required.\") else() message(STATUS \"Setting up BOOST\") message(STATUS \" Includes - ${Boost_INCLUDE_DIRS}\") message(STATUS \" Library - ${Boost_LIBRARY_DIRS}\") include_directories(${Boost_INCLUDE_DIRS}) link_directories(${Boost_LIBRARY_DIRS}) endif (NOT Boost_FOUND) searching default paths (/usr, /usr/local) or the path provided through the cmake variables (BOOST_ROOT, BOOST_INCLUDEDIR, BOOST_LIBRARYDIR) Latest version is here with steps for the Linux (UNIX variants)"
            ]
        ],
        "votes": [
            -0.9999999
        ]
    },
    {
        "question": "I want to install numba and llvmlite for python2 on jetson. However there seems to be no documentation on the same. Everything is for python3. I am trying to run a program on ros melodic and hence need the libraries in python2. When i try python2.7 -m pip install numba==0.45 I am getting following error Collecting numba==0.45 Downloading https://files.pythonhosted.org/packages/7e/89/853a1f03b09f1b13b59c3d785678b47daac6ddd24a285f146d09bb723b85/numba-0.45.0.tar.gz (1.8MB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.8MB 328kB/s Collecting llvmlite&gt;=0.29.0dev0 (from numba==0.45) Using cached https://files.pythonhosted.org/packages/50/cc/04526507e80d546be5688ce0246e40277b61e7949c3347c6609b6a4154cf/llvmlite-0.32.1.tar.gz Collecting numpy (from numba==0.45) Collecting funcsigs (from numba==0.45) Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl Collecting enum34 (from numba==0.45) Using cached https://files.pythonhosted.org/packages/6f/2c/a9386903ece2ea85e9807e0e062174dc26fdce8b05f216d00491be29fad5/enum34-1.1.10-py2-none-any.whl Collecting singledispatch (from numba==0.45) Using cached https://files.pythonhosted.org/packages/cd/d1/6a9e922826e03f5af7bf348cfb75bcb0bc4c67e19c36805c2545f34427e5/singledispatch-3.6.2-py2.py3-none-any.whl Collecting six (from singledispatch-&gt;numba==0.45) Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl Building wheels for collected packages: numba, llvmlite Running setup.py bdist_wheel for numba ... done Stored in directory: /home/nvidia/.cache/pip/wheels/51/5d/c0/420ea2fced22bb1702a294c2cbc0dcaefd6ed61f3d6253fd61 Running setup.py bdist_wheel for llvmlite ... error Complete output from command /usr/bin/python2.7 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-ni4NGO/llvmlite/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmpwjzE5mpip-wheel- --python-tag cp27: running bdist_wheel /usr/bin/python2.7 /tmp/pip-build-ni4NGO/llvmlite/ffi/build.py File \"/tmp/pip-build-ni4NGO/llvmlite/ffi/build.py\", line 122 raise ValueError(msg.format(_ver_check_skip)) from e ^ SyntaxError: invalid syntax error: command '/usr/bin/python2.7' failed with exit status 1 ---------------------------------------- Failed building wheel for llvmlite Running setup.py clean for llvmlite Successfully built numba Failed to build llvmlite Installing collected packages: llvmlite, numpy, funcsigs, enum34, six, singledispatch, numba Running setup.py install for llvmlite ... error Complete output from command /usr/bin/python2.7 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-ni4NGO/llvmlite/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-XAdkTG-record/install-record.txt --single-version-externally-managed --compile --user --prefix=: running install running build got version from file /tmp/pip-build-ni4NGO/llvmlite/llvmlite/_version.py {'version': '0.32.1', 'full': 'aa11b129c0b55973067422397821ae6d44fa5e70'} running build_ext /usr/bin/python2.7 /tmp/pip-build-ni4NGO/llvmlite/ffi/build.py File \"/tmp/pip-build-ni4NGO/llvmlite/ffi/build.py\", line 122 raise ValueError(msg.format(_ver_check_skip)) from e ^ SyntaxError: invalid syntax error: command '/usr/bin/python2.7' failed with exit status 1 ---------------------------------------- Command \"/usr/bin/python2.7 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-ni4NGO/llvmlite/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-XAdkTG-record/install-record.txt --single-version-externally-managed --compile --user --prefix=\" failed with error code 1 in /tmp/pip-build-ni4NGO/llvmlite/",
        "answers": [
            [
                "I solved it. What worked for me was sudo apt-get install python-numba"
            ],
            [
                "Try to upgrade pip and install the latest versions that support Python 2.7: python2.7 -m pip install -U \"pip &lt; 21.0\" python2.7 -m pip install \"llvmlite &lt; 0.32.1\" \"numba &lt; 0.46\""
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am completely new to Jetson Nano board and I need to generate 20ns delay in Jetson Nano board. But I have no idea about it. So can any one help me, how can get 20ns delay in jetson nano board. Is there any code snippet available to achieve nanosecond delay? Thanks in advance.",
        "answers": [
            [
                "If you are using C (on Linux/UNIX), there is a function called nanosleep that allows you to sleep some number of nanoseconds. However, Ubuntu is not a real-time operating system. This means other processes (parts of the OS, or anything else that is running at the same time) may interrupt the execution of your program for short periods of time. This means doing something timing-critical with short delays will not work. If you were to try to use nanosleep to delay for 20ns, your program would probably wait much longer than that. From the man page linked above: nanosleep() suspends the execution of the calling thread until either at least the time specified in *req has elapsed, or the delivery of a signal that triggers the invocation of a handler in the calling thread or that terminates the process. If you were planning on using the 20ns delay to generate some kind of output on I/O pins, you may be able to use the existing interfaces on the Jetson (like SPI) that are implemented in hardware and therefore have more consistent timing. Otherwise, consider using a different type of device to generate the signals (like a fast microcontroller or FPGA) and use I2C, SPI, or something else to communicate your intentions to it from the Jetson. If you needed this for keeping track of some amount of time passed in your program, using the system time is a much better option, and there are lots of ways to get this (though it's likely it will increment more slowly than every 20ns)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "It was successful to train from jet son-XAVIER and recognize it as cam. But I don't know how to send the information that jet son-XAVIER recognized in real time to Arduino. The objects we recognize are elk and wild boar. I know that I can only send 1 letter through communication, so elk will send e and wild boar to w. Is there a way to send it as soon as it recognizes it through real-time web cam?",
        "answers": [
            [
                "There's not a lot of information here on your setup, but here's a possible solution: I see the NVIDIA Jetson AGX Xavier module has USB-C ports. Buy a USB-A to USB-C cable, and plug the Arduino directly in. I'm not sure what program/language you're using with your trained model, but I'll guess that it's python for now. You'll want to open a Serial connection to the arduino, and you can do this with pyserial: https://pypi.org/project/pyserial/ You can send more than just one letter, you can send entire data streams. But, if you want to just send one letter, it will do that as well. Here's the official documentation for how to communicate with an Arduino using Python: https://create.arduino.cc/projecthub/ansh2919/serial-communication-between-python-and-arduino-e7cce0 If you're not using python, specify your language of choice, and we can look up and see if it has a serial library."
            ],
            [
                "I have never used darknet but may be this can point you in the right direction. I have used the library sugested by Bucky and I believe you could add the serial comunication to darknet.py. This is what I would do: #Add this import at begining of the file darknet.py import serial ######################################################### #this is a mocked version of detect in darknet.py, assuming that the labels you used are \"elk\" and \"wildboard\". You should not add this lines to the file. def detect(): res = [] res.append((\"elk\",0.98,(2,2,50,50))) res.append((\"wildboard\",0.98,(2,2,50,50))) return res r = detect() ########################################################## #Add this after the 'print r' at the end of the file darknet.py ser = serial.Serial('/dev/ttyUSB0') # open serial port. You should check what serial port is assigned to your arduino. for obj in r: if obj[0]==\"elk\" and obj[1]&gt;=0.9: #assuming 0.9 as the minimum confident for a detection print \"found elk\" ser.write('e') # you can send a string with just one letter elif obj[0]==\"wildboard\" and obj[1]&gt;=0.9: print \"found wildboard\"; ser.write('w') # you can send a string with just one letter ser.close() # close port"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am loading a Tensorflow 2 version of EfficientDet D2 (http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d2_coco17_tpu-32.tar.gz) using a Jetson AGX Xavier. I run the following script: #!/usr/bin/python3 import tensorflow as tf import time from object_detection.utils import label_map_util from object_detection.utils import visualization_utils as viz_utils PATH_TO_SAVED_MODEL = \"./efficientdet_d2_coco17_tpu-32/saved_model/\" print('Loading model...') start_time = time.time() # Load saved model and build the detection function detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL) end_time = time.time() elapsed_time = end_time - start_time print('Done! Took {} seconds'.format(elapsed_time)) However, the performance results is a loading time of more than 13 minutes. This is the output after the command has been executed: ./test.py 2021-07-04 10:58:58.074413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2 2021-07-04 10:59:05.375568: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2 Loading model... 2021-07-04 11:00:54.337115: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set 2021-07-04 11:00:54.342226: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1 2021-07-04 11:00:54.347726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero 2021-07-04 11:00:54.347959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: pciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2 coreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s 2021-07-04 11:00:54.348037: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2 2021-07-04 11:00:54.353788: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10 2021-07-04 11:00:54.354040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10 2021-07-04 11:00:54.358471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10 2021-07-04 11:00:54.359514: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10 2021-07-04 11:00:54.364904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10 2021-07-04 11:00:54.369140: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10 2021-07-04 11:00:54.369861: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8 2021-07-04 11:00:54.370262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero 2021-07-04 11:00:54.370843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero 2021-07-04 11:00:54.371060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0 2021-07-04 11:00:54.375404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero 2021-07-04 11:00:54.375623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: pciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2 coreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s 2021-07-04 11:00:54.375714: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2 2021-07-04 11:00:54.375823: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10 2021-07-04 11:00:54.375908: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10 2021-07-04 11:00:54.376011: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10 2021-07-04 11:00:54.376090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10 2021-07-04 11:00:54.376167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10 2021-07-04 11:00:54.376287: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10 2021-07-04 11:00:54.376369: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8 2021-07-04 11:00:54.376673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero 2021-07-04 11:00:54.376972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero 2021-07-04 11:00:54.377093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0 2021-07-04 11:05:01.847060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix: 2021-07-04 11:05:01.847174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] 0 2021-07-04 11:05:01.847226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: N 2021-07-04 11:05:01.847710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero 2021-07-04 11:05:01.848589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero 2021-07-04 11:05:01.848911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero 2021-07-04 11:05:01.849096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 19271 MB memory) -&gt; physical GPU (device: 0, name: Xavier, pci bus id: 0000:00:00.0, compute capability: 7.2) 2021-07-04 11:05:01.850298: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set Done! Took 793.8719098567963 seconds With the computing power of the Xavier I would expect much better performance? Anybody knows what the cause to this could be? Thanks for any help or input!",
        "answers": [
            [
                "The time it takes is not just to load the model, but to initialize the device. Maybe the problem is in the driver. To prove it, try to init a smaller model, or toy example like a+b=c. I expect it will take similar time. Also the computing power has nothing to do with loading the model. Loading of the model depends more on memory management of the driver and TF. The actual building of the model in the memory may be done on the CPU, even when using GPU or other accelerator (just guessing). My experience with CUDA and TF is 5 minutes initialization time with one version of CUDA, TF and GPU driver. And less than 30 sec with another version of CUDA and TF, on the same hardware (8x1080ti GPUs)."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "First of all, I attach some of the general specifications of my project at the end of this post. The main objective of my project is to detect the use of face mask via camera vision and then actuate certain actions accordingly. For example, if it detects a person not wearing a face mask, a buzzer will start to buzz continuously, Red LED start to flash and the gate will not open. So far I manage to implement the object detection process, where it able to detect the use of face mask sufficiently. The object detection process should run continuously in a infinite while loop without any delays and will stop until I press a specific key. The problem is when I try to incorporate delays for the actuation process in the same loop, for instance a blinking LED. The video stream for the object detection process froze because of the delays. Thus, I try a few things to ensure that the output actuation process will not interrupt the object detection process such as by implementing multiprocessing along with pickle file that act as a buffer memory storage that stores information produce by the object detection process. But still, I did not manage to solve this problem. I have an issue with writing/reading the pickle file simultaneously from two different processes. Requirement of the processes are as listed below. Process 1 (Main Process) In an infinite loop No delays, speed of the iteration is limited by the hardware and the OS Able to write output signal as soon as it detect the face mask Process 2 (Secondary Process) Start to run the program once received signal from the main process Able to read output signal from the main process Able to operate with delays without interrupting the main process Able to delete/edit the output signal from the main process Killed once the main process is terminated Therefore, I wonder if there is any method/library/function that able to run two process simultaneously and independently, with different timing, and able to retrieve/transfer information within those processes. If it is necessary to share my codes, please do inform me. Thank you. General specifications of my project: Programming language, Python3 Text editor/compiler, Code-OSS Hardware, Nvidia Jetson Nano 2GB OS, Linux/Nvidia JetPack Pre-trained model, SSD-Mobilenet V2",
        "answers": [
            [
                "After reading and searching more about multiprocessing I manage to find something that are useful for my project, it is the method of \"Sharing data using server process\" and \"Process synchronization\" more details about this features you may refer to the YouTube video below. It is highly recommended for you to watch the full playlist, so that you have broader understanding on multiprocessing which might simplify your work. Sharing data using server process https://youtu.be/v5u5zSYKhFs Process synchronization https://youtu.be/-zJ1x2QHTKE Both of this method successfully solve my problem, I think my previous problem raised due to the issue of simultaneous writing and reading of the pickle file from both of the processes."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have 2 devices connected to Weighting terminal CAS-CI200A via RS-232: Lenovo notebook with Kubuntu 20.04 (x86_64 architecture) Jetson Xavier NX with Ubuntu 18.04 LTS, JetPack 4.5 (aarch64 architecture) import serial ser = serial.Serial( port='/dev/ttyUSB0', baudrate=9600, parity=serial.PARITY_EVEN, stopbits=1, bytesize=8 ) line = ser.readline() print(line) Result of the same pyserial library for both devices is: for Lenovo: b'ERR 13\\r\\n' for Jetson: b'\\x05\\x1f\\x12\\x1f\\x12\\x1f\\x1e\\x1e\\x0c\\x06\\r\\n' Also I try to use: cat /dev/ttyUSB0 results are the same. Task is to get b'ERR 13\\r\\n' from Jetson. I try to .decode() the line b'\\x05\\x1f\\x12\\x1f\\x12\\x1f\\x1e\\x1e\\x0c\\x06\\r\\n', but still no success: The only difference between the devises is architecture and OS version, the library and the code are the same. Does anyone know the way to get correct response?",
        "answers": [
            [
                "Problem was in converter\u2019s driver(parity not work) 1a86:7523 QinHeng Electronics HL-340 USB-Serial adapter The solution is to update driver."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "System Specifications: - Device:- NVIDIA Jetson AGX Xavier [16 GB] - Jetpack 4.5.1 8 core CPU RAM:- 32 GB The Pipeline looks like nlp = stanza.Pipeline('en', use_gpu=True, batch_size=100, tokenize_batch_size = 32, pos_batch_size = 32, , depparse_batch_size = 32) doc = nlp(corpus) I am trying to build a Stanza Document with processors:- tokenizer, pos, depparse, error, sentiment, ner; While using a dataset of around 300MB of txt to build the Stanza Document i am running out of memory (RAM) and then the jupyter notebook stops and kernel dies, even with 100MB of data the kernel dies. (I have tried using higher batch sizes and even lower as well but the problem persists)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using TensorRT in order to convert a model from onnx to trt -format. The model is originally a tensorflow model from Tensorflow Model Zoo (SSD ResNet50). When I try to convert it I get the error: [E] [TRT] /home/jenkins/agent/workspace/OSS/OSS_L0_MergeRequest/oss/parsers/onnx/ModelImporter.cpp:708: ERROR: /home/jenkins/agent/workspace/OSS/OSS_L0_MergeRequest/oss/parsers/onnx/builtin_op_importers.cpp:4298 In function importFallbackPluginImporter: [8] Assertion failed: creator &amp;&amp; \"Plugin not found, are the plugin name, version, and namespace correct?\" [E] Engine set up failed &amp;&amp;&amp;&amp; FAILED TensorRT.trtexec # trtexec --onnx=../model.onnx --fp16=enable --workspace=5500 --batch=1 --saveEngine=model_op11.trt --verbose As far as I can tell it is looking for a plugin for the NonMaxSuppresion operation. Does anyone know how to convert a model from Tensorflow Model Zoo to TensorRT?",
        "answers": [
            [
                "Got this fixed by using TensorRT 8."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "tl;dr valgrind not showing reachable memory leak source details C++ application was built using cmake with following extra options: set(CMAKE_CXX_FLAGS_DEBUG \"-ggdb3 -O0\") set(CMAKE_C_FLAGS_DEBUG \"-ggdb3 -O0\") which were passed as seen from make VERBOSE=1 command. Output from running /usr/bin/valgrind --num-callers=500 --trace-children=yes --leak-check=full --show-reachable=yes -v --track-origins=yes --show-leak-kinds=all ./aplication --application params indicated that relevant symbols were loaded. To check memory leakage valgrind was used in combination with gdb which alowed checking valgrind reports after arbitrary time intervals. These reports indicated gradual reachable memory increase - indicating leakage. The problem is that valgrind didn't provide any usable insight what might be causing memory leaks. The output is as follows: ==21466== Searching for pointers to 10,678 not-freed blocks ==21466== Checked 71,211,640 bytes ==21466== ==21466== 984 bytes in 42 blocks are still reachable in loss record 1 of 6 ==21466== at 0x4845494: operator new(unsigned long, std::nothrow_t const&amp;) (in /usr/lib/valgrind/vgpreload_memcheck-arm64-linux.so) ==21466== ==21466== 2,560 bytes in 8 blocks are possibly lost in loss record 2 of 6 ==21466== at 0x4846B0C: calloc (in /usr/lib/valgrind/vgpreload_memcheck-arm64-linux.so) ==21466== ==21466== 17,512 bytes in 3 blocks are still reachable in loss record 3 of 6 ==21466== at 0x4846D10: realloc (in /usr/lib/valgrind/vgpreload_memcheck-arm64-linux.so) ==21466== ==21466== 405,564 bytes in 977 blocks are still reachable in loss record 4 of 6 ==21466== at 0x4846B0C: calloc (in /usr/lib/valgrind/vgpreload_memcheck-arm64-linux.so) ==21466== ==21466== 468,429 bytes in 3,965 blocks are still reachable in loss record 5 of 6 ==21466== at 0x4844BFC: malloc (in /usr/lib/valgrind/vgpreload_memcheck-arm64-linux.so) ==21466== ==21466== 2,166,764 bytes in 5,683 blocks are still reachable in loss record 6 of 6 ==21466== at 0x484522C: operator new(unsigned long) (in /usr/lib/valgrind/vgpreload_memcheck-arm64-linux.so) ==21466== ==21466== LEAK SUMMARY: ==21466== definitely lost: 0 bytes in 0 blocks ==21466== indirectly lost: 0 bytes in 0 blocks ==21466== possibly lost: 2,560 bytes in 8 blocks ==21466== still reachable: 3,059,253 bytes in 10,670 blocks ==21466== suppressed: 0 bytes in 0 blocks Is there anything else to try to make valgrind provide more info on leak source? ENV hw pack: NVIDIA Jetson nv_tegra_release: R32 (release), REVISION: 5.1, GCID: 27362550, BOARD: t186ref, EABI: aarch64, DATE: Wed May 19 18:16:00 UTC 2021 linux distro: Ubuntu 18.04.5 LTS cpu model: AMD Ryzen 5 3600X 6-Core Processor cpu arch: x86_64 valgrind: 3.13.0 References How to Run Valgrind Still Reachable Leak detected by Valgrind Using valgrind with gdb UPDATE Uninstalling valgrind (v3.13) previously installed via apt and installing valgrind (v3.17) via snap solved the issue.",
        "answers": [
            [
                "In case of problems with valgrind, it is always recommended to try with a recent version, either the last release or the git version. Note that it is quite easy to recompile valgrind from sources, at it has very few dependencies. In case of specific problems with stack traces, it is always useful to compare the stack traces produced by valgrind and gdb by using valgrind + gdb + vgdb. Put a breakpoint in gdb at relevant places, and you can then compare the gdb stacktrace produced by the gdb backtrace command and the valgrind stacktrace produced by the monitoring command: (gdb) monitor v.info scheduler"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to run some PyTorch models on my Jetson Nano (4GB RAM), but I've learned that PyTorch uses about 2GB of RAM just to initialize anything CUDA related. I've done some testing (with the help of this GitHub issue), and got the following script running: import torch import argparse parser = argparse.ArgumentParser() parser.add_argument('size', type=int) parser.add_argument('--cpu', action='store_true') args = parser.parse_args() @profile def f(): torch.set_grad_enabled(False) torch.cuda._lazy_init() device = 'cuda' if torch.cuda.is_available() else 'cpu' if args.cpu: device = 'cpu' model = torch.nn.Conv2d(1, 1, 1).to(device) x = torch.rand(1, 1, args.size, args.size).to(device) y = model(x) if __name__ == '__main__': f() Which can be run with python3 -m memory_profiler torchmemscript.py 100. Here is the output from that: Line # Mem usage Increment Occurences Line Contents ============================================================ 9 150.906 MiB 150.906 MiB 1 @profile 10 def f(): 11 150.906 MiB 0.000 MiB 1 torch.set_grad_enabled(False) 12 155.336 MiB 4.430 MiB 1 torch.cuda._lazy_init() 13 155.336 MiB 0.000 MiB 1 device = 'cuda' if torch.cuda.is_available() else 'cpu' 14 155.336 MiB 0.000 MiB 1 if args.cpu: 15 device = 'cpu' 16 1889.699 MiB 1734.363 MiB 1 model = torch.nn.Conv2d(1, 1, 1).to(device) 17 1890.414 MiB 0.715 MiB 1 x = torch.rand(1, 1, args.size, args.size).to(device) 18 2634.496 MiB 744.082 MiB 1 y = model(x) So clearly the model is loaded and uses about ~1.7GB of RAM on my Jetson Nano. Running the same script with the --cpu option gives: Line # Mem usage Increment Occurences Line Contents ============================================================ 9 151.055 MiB 151.055 MiB 1 @profile 10 def f(): 11 151.055 MiB 0.000 MiB 1 torch.set_grad_enabled(False) 12 155.359 MiB 4.305 MiB 1 torch.cuda._lazy_init() 13 155.359 MiB 0.000 MiB 1 device = 'cuda' if torch.cuda.is_available() else 'cpu' 14 155.359 MiB 0.000 MiB 1 if args.cpu: 15 155.359 MiB 0.000 MiB 1 device = 'cpu' 16 157.754 MiB 2.395 MiB 1 model = torch.nn.Conv2d(1, 1, 1).to(device) 17 157.754 MiB 0.000 MiB 1 x = torch.rand(1, 1, args.size, args.size).to(device) 18 160.051 MiB 2.297 MiB 1 y = model(x) Is there a way to reduce this overhead? In the GitHub issue there are mentions of compiling pytorch without all the CUDA kernels to reduce the RAM overhead, but I'm unsure which compile options I will need and which ones will actually reduce RAM overhead. Is there a known way to reduce the RAM usage by PyTorch?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am following a basic tutorial for mask r-cnn using coco dataset on a jetson nano 4gb. however when i run the program, im getting the following error. Is it a memory issue ? Is the GPU out of memory? How do i fix it? Is there an another tutorial which can implement Mask R-CNN without tensorflow? File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call return fn(*args) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn target_list, run_metadata) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun run_metadata) tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found. (0) Internal: Dst tensor is not initialized. [[{{node _arg_Placeholder_658_0_619}}]] (1) Internal: Dst tensor is not initialized. [[{{node _arg_Placeholder_658_0_619}}]] [[Assign_658/_3999]] 0 successful operations. 0 derived errors ignored. During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"test2.py\", line 30, in &lt;module&gt; model.load_weights('mask_rcnn_coco.h5', by_name=True) File \"/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 2130, in load_weights File \"/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\", line 1154, in load_weights_from_hdf5_group_by_name K.batch_set_value(weight_value_tuples) File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2470, in batch_set_value get_session().run(assign_ops, feed_dict=feed_dict) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run run_metadata_ptr) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run feed_dict_tensor, options, run_metadata) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run run_metadata) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1384, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found. (0) Internal: Dst tensor is not initialized. [[{{node _arg_Placeholder_658_0_619}}]] (1) Internal: Dst tensor is not initialized. [[{{node _arg_Placeholder_658_0_619}}]] [[Assign_658/_3999]] 0 successful operations. 0 derived errors ignored. This is the code of the program import sys import random import math import numpy as np import skimage.io import matplotlib import matplotlib.pyplot as plt from mrcnn import utils import mrcnn.model as modellib from mrcnn import visualize import coco COCO_MODEL_PATH = \"/home/jetson/Desktop/pano_l515/Mask_RCNN/mask_rcnn_coco.h5\" class InferenceConfig(coco.CocoConfig): # Set batch size to 1 since we'll be running inference on # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU GPU_COUNT = 1 IMAGES_PER_GPU = 1 config = InferenceConfig() config.display() # Create model object in inference mode. model = modellib.MaskRCNN(mode=\"inference\", model_dir='mask_rcnn_coco.hy', config=config) # Load weights trained on MS-COCO model.load_weights('mask_rcnn_coco.h5', by_name=True) # COCO Class names class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'] # Load a random image from the images folder image = skimage.io.imread('test_images.jpg') # original image plt.figure(figsize=(12,10)) skimage.io.imshow(image) # Run detection results = model.detect([image], verbose=1) # Visualize results r = results[0] visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores']) EDIT this is the link for the tutorial https://medium.com/analytics-vidhya/computer-vision-tutorial-implementing-mask-r-cnn-for-image-segmentation-with-python-code-fe34da5b99cd",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to run jetracer on the 2GB version of Jetson Nano along with Waveshare's JetRacer Pro AI Kit but all the release files listed in the docs are for the 4GB version. The Waveshare wiki also features files for the 4GB version only. I've seen someone mention there being a pre-built 2GB version for Waveshare cars available on the wiki alongside the 4GB version but it's not there anymore (he meant the AI Kit instead of Pro AI Kit I have but still, it's unavailable for either anymore). Is there no way to run the AI capabilities for self driving RC cars on a 2GB jetson, then? I thought these were cross platform and worst case scenario, there'd be worse performance - but not impossibility to run it altogether. Would it make sense to flash the card by hand using Jetson's official docs and then trying to follow the Jetracer Setup Guide from step #2 instead? This seems to have a chance of working cause step #1 (the one with ready-baked files) is just an installation of Jetcard anyway so one should be able to bypass it by flashing the card yourself instead of using the provided image. At the same time, I contacted Waveshare's support and they just told me straight away that Jetracer and their kit won't work with a 2GB Jetson and that's that... Is there a possibility to run Jetracer on a 2GB Jetson then?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a Python script call.py to log in to another machine through ssh and execute another Python script saved on this machine, something like this # call.py import os # log into user@host machine and execute python script os.system(\"sshpass -p password ssh user@host '( python3 my_script.py )'\") but when I try to run it I get this error Illegal instruction (core dumped) ( python3 my_script.py ) I tried a few things: If I log into ssh and launch python3 my_script.py through the terminal it works I also tried running the same call.py script without the ssh part, so trying to launch my_script.py on the same machine and it works It seems that only the combination of ssh + python inside python that gives me this error. Any pointers on what could cause the error will be very much appreciated. Edit: I didn't realize it would play a role, but I'm adding now the important detail that the target machine is an Nvidia Jetson board.",
        "answers": [
            [
                "I found the solution for my case, in case it can help someone else. As pointed out by Thomas, the problem was that I was working with different systems, in the specific my target machine was a Jetson while I was running ssh from my laptop. On my Jetson board I had seen the Illegal instruction (core dumped) when trying to import OpenCV, to which the solution was to export OPENBLAS_CORETYPE=ARMV8 to the ~/.bashrc. In order to make this work through ssh, I had to modify my command to call python this way: OPENBLAS_CORETYPE=ARMV8 python3 and now it works!"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am trying to use the camera feed on my Jetson Nano (running headless over SSH) in 2 different applications. From the command line, I am able to run gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM), width=3280, height=2464, format=NV12, framerate=(fraction)21/1' ! nvvidconv ! xvimagesink Which streams video from my camera (IMX219 connected to the Jetson Nano) to my desktop via an X11 Window. What I would like to do is somehow use that same video stream in 2 different applications. My first application is a python program that runs some OpenCV stuff, the second application is a simple bash script which records video to an *.mp4 file. Is such a thing possible? I have looked into using v4l2loopback, but I'm unsure if that is really the simplest way to do this.",
        "answers": [
            [
                "Well, I managed to figure it out thanks to both commentors, here is my solution on my Jetson Nano, but it can be tweaked for any Gstreamer application. First, use v4l2loopback to create 2 virtual devices like so: sudo modprobe v4l2loopback video_nr=1,2 That will create /dev/video1 and /dev/video2 Then you use tee to dump a Gstreamer stream into each of these virtual devices, here is my line: gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM), width=3280, height=2464, format=NV12, framerate=(fraction)21/1' ! nvvidconv ! tee name=t ! queue ! v4l2sink device=/dev/video1 t. ! queue ! v4l2sink device=/dev/video2 This is specifically for my Jetson Nano and my specific camera, but you can change the gstreamer pipeline to do what you wish"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I got a fresh install of a Jetson TK1 with Ubuntu 14.04. Then I upgraded it to Ubuntu 16.04, Ubuntu 18.04 and Ubuntu 20.04. For 18 and 20 the display manager started to fail, not starting the Desktop. When I use systemctl status lightdm.service the output is: \u25cf lightdm.service - Light Display Manager Loaded: loaded (/lib/systemd/system/lightdm.service; indirect; vendor preset: enabled) Drop-In: /lib/systemd/system/display-manager.service.d \u2514\u2500xdiagnose.conf Active: failed (Result: exit-code) since Wed 2021-03-17 21:36:18 UTC; 2 months 2 days ago Docs: man:lightdm(1) Main PID: 768 (code=exited, status=1/FAILURE) Mar 17 21:36:18 tegra-ubuntu systemd[1]: lightdm.service: Scheduled restart job, restart counter is at 4. Mar 17 21:36:18 tegra-ubuntu systemd[1]: Stopped Light Display Manager. Mar 17 21:36:18 tegra-ubuntu systemd[1]: lightdm.service: Start request repeated too quickly. Mar 17 21:36:18 tegra-ubuntu systemd[1]: lightdm.service: Failed with result 'exit-code'. Mar 17 21:36:18 tegra-ubuntu systemd[1]: Failed to start Light Display Manager. Mar 17 21:36:18 tegra-ubuntu systemd[1]: lightdm.service: Triggering OnFailure= dependencies. Mar 17 21:36:19 tegra-ubuntu systemd[1]: lightdm.service: Start request repeated too quickly. Mar 17 21:36:19 tegra-ubuntu systemd[1]: lightdm.service: Failed with result 'exit-code'. Mar 17 21:36:19 tegra-ubuntu systemd[1]: Failed to start Light Display Manager. Then if I type lightdm --test-mode --debug the output is: [+0.00s] DEBUG: Logging to /home/ubuntu/.cache/lightdm/log/lightdm.log [+0.00s] DEBUG: Starting Light Display Manager 1.30.0, UID=1000 PID=25721 [+0.00s] DEBUG: Loading configuration dirs from /var/lib/snapd/desktop/lightdm/lightdm.conf.d [+0.00s] DEBUG: Loading configuration dirs from /usr/share/lightdm/lightdm.conf.d [+0.00s] DEBUG: Loading configuration from /usr/share/lightdm/lightdm.conf.d/50-disable-guest.conf [+0.00s] DEBUG: Loading configuration from /usr/share/lightdm/lightdm.conf.d/50-disable-log-backup.conf [+0.01s] DEBUG: Loading configuration from /usr/share/lightdm/lightdm.conf.d/50-greeter-wrapper.conf [+0.01s] DEBUG: Loading configuration from /usr/share/lightdm/lightdm.conf.d/50-guest-wrapper.conf [+0.01s] DEBUG: Loading configuration from /usr/share/lightdm/lightdm.conf.d/50-ubuntu.conf [+0.01s] DEBUG: Loading configuration from /usr/share/lightdm/lightdm.conf.d/50-unity-greeter.conf [+0.01s] DEBUG: Loading configuration from /usr/share/lightdm/lightdm.conf.d/50-unity.conf [+0.01s] DEBUG: Loading configuration from /usr/share/lightdm/lightdm.conf.d/50-xserver-command.conf [+0.01s] DEBUG: Loading configuration dirs from /usr/local/share/lightdm/lightdm.conf.d [+0.01s] DEBUG: Loading configuration dirs from /etc/xdg/lightdm/lightdm.conf.d [+0.01s] DEBUG: Loading configuration from /etc/lightdm/lightdm.conf [+0.01s] DEBUG: Running in user mode [+0.01s] DEBUG: Registered seat module local [+0.01s] DEBUG: Registered seat module xremote [+0.01s] DEBUG: Using D-Bus name org.freedesktop.DisplayManager [+0.01s] DEBUG: Failed to get thread scheduler attributes: Function not implemented [+0.05s] DEBUG: _g_io_module_get_default: Found default implementation gvfs (GDaemonVfs) for ?gio-vfs? [+0.06s] DEBUG: Monitoring logind for seats [+0.06s] DEBUG: New seat added from logind: seat0 [+0.06s] DEBUG: Seat seat0: Loading properties from config section Seat:* [+0.06s] DEBUG: Seat seat0: Starting [+0.07s] DEBUG: Seat seat0: Creating greeter session [+0.07s] DEBUG: Loading users from org.freedesktop.Accounts [+0.07s] DEBUG: User /org/freedesktop/Accounts/User1000 added [+0.08s] DEBUG: Seat seat0: Creating display server of type x [+0.08s] DEBUG: posix_spawn avoided (fd close requested) [+0.09s] DEBUG: Seat seat0: Starting local X display [+0.09s] DEBUG: XServer 0: Logging to /home/ubuntu/.cache/lightdm/log/x-0.log [+0.09s] DEBUG: XServer 0: Writing X server authority to /home/ubuntu/.cache/lightdm/run/root/:0 [+0.10s] DEBUG: XServer 0: Launching X Server [+0.10s] DEBUG: Launching process 25727: /usr/bin/X -core :0 -seat seat0 -auth /home/ubuntu/.cache/lightdm/run/root/:0 -nolisten tcp [+0.10s] DEBUG: XServer 0: Waiting for ready signal from X server :0 [+0.10s] DEBUG: Acquired bus name org.freedesktop.DisplayManager [+0.10s] DEBUG: Registering seat with bus path /org/freedesktop/DisplayManager/Seat0 [+0.18s] DEBUG: Process 25727 terminated with signal 6 [+0.18s] DEBUG: XServer 0: X server stopped [+0.18s] DEBUG: XServer 0: Removing X server authority /home/ubuntu/.cache/lightdm/run/root/:0 [+0.18s] DEBUG: Seat seat0: Display server stopped [+0.18s] DEBUG: Seat seat0: Stopping session [+0.18s] DEBUG: Seat seat0: Session stopped [+0.18s] DEBUG: Seat seat0: Stopping display server, no sessions require it [+0.18s] DEBUG: Seat seat0: Stopping; greeter display server failed to start [+0.18s] DEBUG: Seat seat0: Stopping [+0.18s] DEBUG: Seat seat0: Stopped [+0.18s] DEBUG: Required seat has stopped [+0.18s] DEBUG: Stopping display manager [+0.18s] DEBUG: Display manager stopped [+0.18s] DEBUG: Stopping daemon [+0.18s] DEBUG: Exiting with return value 1 I have tried some solutions or commands, but anyone helped me. Thanks in advance. Best regards. Alessandro",
        "answers": [],
        "votes": []
    },
    {
        "question": "Summary I'm currently trying to add custom V4L2 controls to a V4L2 device after v4l2_ctrl_handler_setup is called in a Linux Kernel driver. However the control does not seem to be added (does not show up when running v4l2-ctl --list-ctrls). Below is generally the approach I am trying to take. static int add_custom_v4l2_ctrls(struct tegracam_device *tc_dev) { struct camera_common_data *s_data = tc_dev-&gt;s_data; struct v4l2_ctrl_handler *ctrl_handler = s_data-&gt;ctrl_handler; struct v4l2_ctrl *ctrl; int err = 0; static struct v4l2_ctrl_config my_control = { .ops = &amp;my_custom_ctrl_ops, .id = TEGRA_CAMERA_CID_BASE+150, .name = \"My control\", .type = V4L2_CTRL_TYPE_INTEGER, .flags = V4L2_CTRL_FLAG_SLIDER, .min = 0, .max = 1, .def = 0, .step = 1, }; // Increment number of controls tc_dev-&gt;numctrls++; s_data-&gt;numctrls++; ctrl = v4l2_ctrl_new_custom(ctrl_handler, &amp;my_control, NULL); if(ctrl == NULL) { dev_err(tc_dev-&gt;dev, \"Failed to init ctrl\"); return -EIO; } // err = v4l2_ctrl_handler_setup(ctrl_handler); if(err) { printk(\"FAILED\"); } return 0; } This code snippet is run after an effective call to v4l2_ctrl_handler_setup and v4l2_async_register_subdev. Question Is it possible to add custom V4L2 controls after the device has been registered? If so, what is wrong with my approach which is causing the control to not show up? More Info This driver is implemented using NVIDIA's Tegracam V2 framework which abstracts V4L2 setup code including the addition of controls, at the moment it does not expose the ability for adding custom V4L2 controls which is the reasoning behind this approach.",
        "answers": [
            [
                "This might come a bit late, since you already have marked a solution, but I believe I have a solution that can help. I'm gonna do my best to explain how I went about adding custom controls to the tegracamv2 framework. You unfortunately cannot add controls after the subdevice is registered by v4l2_async_register_subdev, you can, however, hook into the async framework using the registered callback defined in v4l2sd_internal_ops field of the tc_dev struct. This allows you to define your own control handler and then add it to the control handler from the tegracam framework, which will then be added to the v4l2 device. I recently struggled with this and I found this to be my only solution apart from patching out the call to v4l2_async_register_subdev in the tegracam framework and calling it manually after adding controls (this is untested). The related code (This assumed an array of ctrl_config, but showcases the idea. I also assume you have defined ctrl_ops and configs.): static int my_subdev_register(struct v4l2_subdev *sd) { struct i2c_client *client = v4l2_get_subdevdata(sd); struct camera_common_data *s_data = to_camera_common_data(&amp;client-&gt;dev); struct my_priv *priv = (struct my_priv *)s_data-&gt;priv; struct v4l2_ctrl *ctrl; int err, i, num_ctrls; // setup custom controls num_ctrls = ARRAY_SIZE(ctrl_config_list); v4l2_ctrl_handler_init(&amp;priv-&gt;ctrl_handler, num_ctrls); for (i = 0; i &lt; num_ctrls; i++) { ctrl = v4l2_ctrl_new_custom(&amp;priv-&gt;ctrl_handler, &amp;ctrl_config_list[i], NULL); if (ctrl == NULL) { dev_err(&amp;client-&gt;dev, \"Failed to init %s ctrl\\n\", ctrl_config_list[i].name); continue; } if (err) return err; // I believe storing the pointer is technically optional priv-&gt;ctrls[i] = ctrl; } priv-&gt;num_ctrls = num_ctrls; //This is where the magic happens err = v4l2_ctrl_add_handler( sd-&gt;ctrl_handler, &amp;priv-&gt;ctrl_handler, NULL); if (err) goto error; return 0; error: v4l2_ctrl_handler_free(&amp;priv-&gt;ctrl_handler); return err; } ... static const struct v4l2_subdev_internal_ops my_subdev_internal_ops = { .registered = my_subdev_register, }; In the probe function you then include the following before tegracam_device_register. (But it won't be called before tegracam_v4l2subdev_register.) tc_dev-&gt;sensor_ops = &amp;my_subdev_internal_ops;"
            ],
            [
                "After following down the call stack I found this order (arrows represent calls, links are to the Linux source code). v4l2_async_register_subdev -&gt; v4l2_async_match_notify -&gt; v4l2_device_register_subdev -&gt; v4l2_ctrl_add_handler The last function (v4l2_ctrl_add_handler) goes through and copies V4L2 controls from one handler to another. Therefore, if the V4L2 control is not added before v4l2_async_register_subdev is called, then the control will not be copied to the different devices and will therefore not be a valid option available. So in summation from what I found, no it is not possible to add V4L2 controls after the device has been registered."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I read that when installing OpenCV you can specify a series of options (use of CUDA, TBB, NEON, etc). My question is, suppose I have access to a (ARM) machine, in which OpenCV is already installed. Is there a way to learn which options was OpenCV installed with? (for example does it use CUDA, etc)",
        "answers": [
            [
                "Yes, there is a way. You can use getBuildInformation(). import cv2 print(cv2.getBuildInformtion()) In case of cpp, ... std::cout &lt;&lt; cv::getBuildInformation() &lt;&lt; std::endl; ... This will return information about cmake settings, version control, compiler flags, third-party libraries etc related to opencv installation."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm using Nvidia Jetson board having very small storage. During using docker, It always give me an error like no space left on device I'm trying to use external usb stick and let docker use only space on it. So I found out someways to overcome, but it failed. move /var/lib/docker into usb stick and make symbolic link to previous path move /var/run/docker also like above but It seems that docker still use device space, because I encountered the error again like. failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.26.1/unicore/To/NFKCCF.pl: no space left on device Is there any clear way to force docker to only use external space?",
        "answers": [],
        "votes": []
    },
    {
        "question": "The background I recently designed pan-tilt camera module which utilizes stepper motors to control the camera motion and Jetson Xavier NX for object recognition. The workflow is simple: Program written in Python is analyzing video input and looking for object. When object is found it calculates the difference between image center and object position (in both X and Y axes) and enables motors. When object is far from center it sets bigger speed for motors. Whent it's close it runs motors with minimum speed. IMPORTANT: The program doesn't have any knowledge about the relation between distance on the screen and amount of steps to make by stepper motor to center. It just enable the stepper motor and constantly check until the difference between center and object is very small. Maybe this method is wrong... The problem I read about applying PID controllers for that kind of problems to minimize deviation, but I can't understand how I can apply in my situation. It seems to be simpler when there is an absolute value we want to reach (like an angle in case of servo motors). There is a good example by PyImageSearch.com how to use PID Controller to minimize deviation on servo-motors based pan-tilt module. PID Controller tread constantly calculates the angle value based on the current error (difference between the object postition and center of camera frame) and adjusts the angle. The question I want to ask how I can think about PID Controller (if it is possible at all), when I have only the speed variable to control the motor instead of the angle like in servos. Maybe I have to change the way it works and implement the feature which will give me the information about current stepper motor position (like an angle in servos)?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am looking for end-to-end tutorial, how to convert my trained tensorflow model to TensorRT to run it on Nvidia Jetson devices. I know how to do it in abstract (.pb -&gt; ONNX - &gt; [Onnx simplifyer] -&gt; TRT engine), but I'd like to see how other do It, because I had no speed gain after converting, maybe i did something wrong. I can't believe that there is no pipeline with steps description in the internet. That's why I am asking here, maybe you have seen such a tutorial...",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to buy the Jetson Tx2 in order to do some deep dive. According to their Docs, the Bootloader at some point executes the C-Boot followed by U-Boot. C-Boot runs with Exception Level (EL) 2, does somebody know whether C-Boot passes EL 2 to U-Boot (or C-Boost passes EL 3 to U-Boot)? If you have a Tx2 you could simply test it by: dmesg | grep EL Thanks in advance!",
        "answers": [
            [
                "TF-A drops to EL2 before invoking Cboot. See https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3231/index.html#page/Tegra%2520Linux%2520Driver%2520Package%2520Development%2520Guide%2Fbootflow_tx2.html%23. U-Boot then also runs at EL2. If you want to run software in the secure zone, you could use OP-TEE. Should U-Boot be called in EL3 and you boot via UEFI U-Boot will drop to EL2 before loading the EFI binary by calling the function switch_to_non_secure_mode()."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have trained the classification model on Nvidia GPU and saved the model weights(checkpoint.pth). If I want to deploy this model in jetson nano and test it. Should I convert it to TenorRT? How to convert it to TensorRT? I am new to this. It would be helpful if someone can even correct me.",
        "answers": [
            [
                "The best way to achieve the way is to export the Onnx model from Pytorch. Next, use the TensorRT tool, trtexec, which is provided by the official Tensorrt package, to convert the TensorRT model from onnx model. You can refer to this page: https://github.com/NVIDIA/TensorRT/blob/master/samples/opensource/trtexec/README.md The TRTEXEC is a more native tool that you can take it from NVIDIA NGC images or downloading from the official website directly. If you use a tool such as torch2trt, it is easy to encounter the operator issue and complicated to resolve it indeed (if you are not familiar to deal with plugin issues)."
            ],
            [
                "You can use this tool: https://github.com/NVIDIA-AI-IOT/torch2trt Here are more details how to implent a converter to a engine file: https://github.com/NVIDIA-AI-IOT/torch2trt/issues/254"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am trying to read the camera image on Jetson Xavier (ubuntu 18). I am facing a problem. When I run the following code it gives a warning and gives a black (full) image. [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (933) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1 cam=cv2.VideoCapture(0) if cam.isOpened(): grab,img = cam.read() if grab is True: cv2.imshow('sample image',img) else: print(f\"Image not found\") else: print(\"Camera not openedd\") cv2.waitKey(0) # waits until a key is pressed cv2.destroyAllWindows() # destroys the window showing image If I use 'dev/video0' to read the image i.e. cam=cv2.VideoCapture('dev/video0') I get the warning and custom error message of camera not opened [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (711) open OpenCV | GStreamer warning: Error opening bin: no element \"dev\" [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created Camera not opened Then I created gstream string and passed that to video capture as shown below. the string is as follow gstr = 'varguscamerasrc ! video/x-raw(memory:NVMM), width=(int)1920, height=(int)1080, format=(string)NV12, framerate=(fraction)60/1 ! nvvidconv flip-method=0 ! video/x-raw, width=(int)1280, height=(int)720, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink' cap = cv2.VideoCapture(gstr, cv2.CAP_GSTREAMER) I get the following error Error generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst-nvarguscamera/gstnvarguscamerasrc.cpp, execute:645 No cameras available (python3:15402): GStreamer-CRITICAL **: 19:08:54.835: gst_mini_object_set_qdata: assertion 'object != NULL' failed [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (933) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1 Traceback (most recent call last): I am new to jetson please guide me. Thanks",
        "answers": [
            [
                "You can try the following code. Also make sure you have installed OpenCV from source not using pip because Jetson Nano and Xavier make some problems when you install OpenCV from pip. import numpy as np import cv2 cap = cv2.VideoCapture(0) while(True): # Capture frame-by-frame ret, frame = cap.read() # Our operations on the frame come here gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Display the resulting frame cv2.imshow('frame',gray) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break # When everything done, release the capture cap.release() cv2.destroyAllWindows()"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Can someone explain to me what is NHCW format? I am working with the Jenson Inference library and for object detection the first step is called \"Pre-Process\" and it converts the image to NCHW format, but I don't know what this format is.",
        "answers": [
            [
                "NCHW stands for: batch N, channels C, depth D, height H, width W It is a way to store multidimensional arrays / data frames / matrix into memory, which can be considered as a 1-D array. You may have a look to this link for further information. There also exist variants of this format with different ways of \"casting\" the multidimensional data into one."
            ],
            [
                "NCHW is an acronym describing the order of the axes in a tensor containing image data samples. N: Number of data samples. C: Image channels. A red-green-blue (RGB) image will have 3 channels. H: Image height. W: Image width. Source"
            ]
        ],
        "votes": [
            8.0000001,
            2.0000001
        ]
    },
    {
        "question": "It seems to me like power on and off have two different behaviours: if I shutdown jetson xavier nx through the operating system and after that turns off the power, jetson xavier wont start when I turn the power back on. First I have to remove the powerplug from the device and reinsert it again. if I just turns off the power and in that way are closing jetson xavier it will restart as soon as I turn the power on again. While number two works it seems always better to turn off the operating system first but it is a bit odd to remove and reinsert the cable to restart it. What is the proper way of doing this? Or does number two nothing to worry about? Or am I better off install some kind of on off switch or what is the most easy way of handle this fundamental thing?",
        "answers": [
            [
                "It only works in these three scenarios: cutting the powersupply - the jetson starts when it is back on shutting down from menu - the jetson starts if I remove and reconnect the powerplug to the jetson shutting down from menu - the jetson starts if I wait 5-10 minutes before I start the electricity again"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am looking to send a file from a Jetson Xavier NX (arm linux development board for those who aren't acquainted) to a Raspberry pi Zero via USB, through the standard usb connectors. I want the raspi to receive the input as a file and store it in a directory that will later be scanned and manipulated with python. I imagine I can send the file with something along the lines of (shell command) cp /home/pi/file.pkl /dev/ttyAMA0 or (python code) import serial s = serial.Serial(\"/dev/ttyAMA0\") s.write(open(\"file.pkl\",\"rb\").read()) But I'm unsure what would be the best way to read the file in from the pi. I know of the python serial.read() and serial.readline(), but is there anyway to simply read the entire contents that have been sent and interpret it as a file? Can I achieve this by using the cp or cat command in some way? Thanks in advance",
        "answers": [
            [
                "In the days when everything was done using serial lines we used kermit. http://www.columbia.edu/kermit/ckututor.html"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers. This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered. Closed 2 years ago. Improve this question I have a client where I need to do some updates and support on a system running Ubuntu on jetson xavier nx. First I was thinking of using Teamviewer but it is not supporting arm64. The same scenario with google remote desktop and others ... I tried to use nomachine which works fine when running on the same network. I tried to make it work from outside (from the internet) by running it together with services like ngrok or localtunnel but it cant get it to work and I might misunderstand some of the concept. This is what I have done: installed nomachine and ngrok on Jetson run nomachine-service and start the server the server says nx://172.20.10.12 and ssh://172.20.10.12. It runs on port 4000 here comes what I might misunderstand. My idea is to use ngrok to fetch the nomachine so I can access it from a webpage from anywhere instead of running it from my another computer on the same network. After I saved my accesstoken for ngrok I tried by typing: ./ngrok tcp 172.20.10.12:4000 But it didnt work. Any idea how I can get this to work, basically to access my jetson from remote UPDATE I tried this procedure: Create a thunnel: ./ngrok tcp 3389 Go to http://localhost:4040/status to see the status Connecting from client: Im asked to login: But I cannot access. Im using the same login as to ngroks dashboard but Im not getting in. Shall I use some other login information? Or can I access my login information from ngrok somehwhere?",
        "answers": [
            [
                "In your nomachine client running remote, when you add a connection, you'll want to specify the host to be the ngrok hostname and the port to be the ngrok port. For example, if running ./ngrok tcp 172.20.10.12:4000 starts a tunnel at 4.tcp.ngrok.io:19283, you'll want to use 4.tcp.ngrok.io as the host and 19283 as the port, instead of referencing the local address."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I want to install boto3 for python 2.7 on my jetson nano. It is currently installed in site packages for python3.6 because of which i am able to import it in python3. However i am unable to access it in python2.7. It says module not found. jetson@jetson-desktop:~$ pip install boto3 Defaulting to user installation because normal site-packages is not writeable Requirement already satisfied: boto3 in ./.local/lib/python3.6/site-packages (1.17.43) Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in ./.local/lib/python3.6/site-packages (from boto3) (0.10.0) Requirement already satisfied: botocore&lt;1.21.0,&gt;=1.20.43 in ./.local/lib/python3.6/site-packages (from boto3) (1.20.43) Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in ./.local/lib/python3.6/site-packages (from boto3) (0.3.6) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.25.4 in ./.local/lib/python3.6/site-packages (from botocore&lt;1.21.0,&gt;=1.20.43-&gt;boto3) (1.26.4) Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in ./.local/lib/python3.6/site-packages (from botocore&lt;1.21.0,&gt;=1.20.43-&gt;boto3) (2.8.1) Requirement already satisfied: six&gt;=1.5 in ./.local/lib/python3.6/site-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.21.0,&gt;=1.20.43-&gt;boto3) (1.15.0) I want it to work for python2. I am not able to direct the installation to python2.",
        "answers": [
            [
                "try install with: $ git clone https://github.com/boto/boto3.git $ cd boto3 $ virtualenv venv $ . venv/bin/activate $ python -m pip install -r requirements.txt $ python -m pip install -e . $ python -m pip install boto3 and check https://github.com/boto/boto3"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am doing a project with object detection. I use a provided algorithm by Livox (https://github.com/Livox-SDK/livox_detection). I am doing the project on a Nvidia jetson xavier nx with 8 GB memory. I am using Tensorflow 1.15.4 because of compability reasons. The algorithm worked fine on TF 1.13.1 on a GTX 1050 graphics card but it was too slow. When I start the algorithm, tensorflow allocates memory (for example 3000 mb). But when the algorithm is getting sensor data from the lidar sensor, the memory increases until it is nearly at the available 7700 Mb. After one minute all tracebacks were listed and the program starts but has a inference time at about 300 ms (50ms on the GTX 1050 and it should be 24 ms). I think that there is a problem with rewriting the used memory. The program works with a pre trained model, which means I can't train the model new because I don't have the dataset. I will publish the main tracebacks below. xavier@xavier-desktop:~/livox_detection-master$ python3 livox_rosdetection.py 2021-03-29 18:43:51.996530: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2 WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them. WARNING:tensorflow: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see: * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md * https://github.com/tensorflow/addons * https://github.com/tensorflow/io (for I/O related ops) If you depend on functionality not listed there, please file an issue. 1008 224 30 WARNING:tensorflow:From /home/xavier/livox_detection-master/networks/model.py:27: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. (1, 1008, 224, 30) WARNING:tensorflow:From /home/xavier/livox_detection-master/networks/model.py:77: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead. WARNING:tensorflow:From livox_rosdetection.py:66: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead. WARNING:tensorflow:From livox_rosdetection.py:67: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. WARNING:tensorflow:From livox_rosdetection.py:72: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead. 2021-03-29 18:44:07.473530: W tensorflow/core/platform/profile_utils/cpu_utils.cc:98] Failed to find bogomips in /proc/cpuinfo; cannot determine CPU frequency 2021-03-29 18:44:07.474464: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x34229010 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-03-29 18:44:07.474578: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2021-03-29 18:44:07.524156: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1 2021-03-29 18:44:07.760361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1049] ARM64 does not support NUMA - returning NUMA node zero 2021-03-29 18:44:07.760946: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x341f0ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2021-03-29 18:44:07.761005: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Xavier, Compute Capability 7.2 2021-03-29 18:44:07.761407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1049] ARM64 does not support NUMA - returning NUMA node zero 2021-03-29 18:44:07.761515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1665] Found device 0 with properties: name: Xavier major: 7 minor: 2 memoryClockRate(GHz): 1.109 pciBusID: 0000:00:00.0 2021-03-29 18:44:07.761579: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2 2021-03-29 18:44:07.835484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10 2021-03-29 18:44:07.854590: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10 2021-03-29 18:44:07.894728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10 2021-03-29 18:44:07.913741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10 2021-03-29 18:44:07.932242: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10 2021-03-29 18:44:07.938152: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8 2021-03-29 18:44:07.938577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1049] ARM64 does not support NUMA - returning NUMA node zero 2021-03-29 18:44:07.939043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1049] ARM64 does not support NUMA - returning NUMA node zero 2021-03-29 18:44:07.939263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1793] Adding visible gpu devices: 0 2021-03-29 18:44:07.943363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2 2021-03-29 18:44:14.396299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Device interconnect StreamExecutor with strength 1 edge matrix: 2021-03-29 18:44:14.396420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] 0 2021-03-29 18:44:14.396449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1225] 0: N 2021-03-29 18:44:14.396959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1049] ARM64 does not support NUMA - returning NUMA node zero 2021-03-29 18:44:14.397271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1049] ARM64 does not support NUMA - returning NUMA node zero 2021-03-29 18:44:14.397482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1351] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3882 MB memory) -&gt; physical GPU (device: 0, name: Xavier, pci bus id: 0000:00:00.0, compute capability: 7.2) 2021-03-29 18:47:25.321906: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8 2021-03-29 18:47:42.396094: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10 2021-03-29 18:48:05.615685: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.89GiB (rounded to 4179165184). Current allocation summary follows. 2021-03-29 18:48:05.637466: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): Total Chunks: 21, Chunks in use: 21. 5.2KiB allocated for chunks. 5.2KiB in use in bin. 4.6KiB client-requested in use in bin. then some allocator lines like above 2021-03-29 18:48:05.699020: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 291.68MiB 2021-03-29 18:48:05.699180: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 4071376896 memory_limit_: 4071376896 available bytes: 0 curr_region_allocation_bytes_: 8142753792 2021-03-29 18:48:05.699424: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: Limit: 4071376896 InUse: 305850368 MaxInUse: 3307795968 NumAllocs: 198 MaxAllocSize: 3073374208 2021-03-29 18:48:05.715474: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ********____________________________________________________________________________________________ 2021-03-29 18:48:15.799330: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.89GiB (rounded to 4179165184). Current allocation summary follows. After that sometimes there is this traceback: 2021-03-28 14:37:45.895238: W tensorflow/core/common_runtime/bfc_allocator.cc:305] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature. I already tried to limit the allocation amount with this line in the \"livox_rosdetection.py\" from the github link I mentioned before: # config.gpu_options.allow_growth = True config.gpu_options.per_process_gpu_memory_fraction = 0.5 One final thing to mention is that the memory gets split up between CPU and GPU. When more information is required I will publish it. Does someone know how I can make the detection faster? thanks in advance",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm making a USV (Unmanned Surface Vehicle) for my bachelor project. The track it needs to keep is made by having coloured buoys on the left/right side of the track and for obstacles. So I need to track the depth of these objects and give all that information on to my navigation program. And I have made a Python code using ROS and OpenCV to track these buoys with a ZED2 camera. But I'm having CPU and memory issues. Where the ubuntu desktop starts to lag. Using a Nvidia Jetson Xavier NX and I\u2019m using 85% of the CPU and 5,5+/7.59Gb Memory. Anyone interested in looking over my code and see if I'm doing something stupid. That would explain my issues. from __future__ import print_function import roslib import sys import rospy import cv2 from main.msg import VarRed, VarGreen, VarYellow, RedHSV, GreenHSV, YellowHSV, MidPoint from sensor_msgs.msg import Image from cv_bridge import CvBridge, CvBridgeError import numpy as np import imutils import time from collections import deque import math class image_converter: def __init__(self): self.image_subd = rospy.Subscriber(\"/zed2/zed_node/depth/depth_registered\",Image,self.callbackDepth) self.image_sub = rospy.Subscriber(\"/zed2/zed_node/rgb_raw/image_raw_color\",Image,self.callbackVideo) self.image_pub = rospy.Publisher(\"/Tracking/RG_image\", Image, queue_size = 1) self.RedHSV_sub = rospy.Subscriber(\"/Tracking/Red_HSV\", RedHSV, self.redHSV) self.GreenHSV_sub = rospy.Subscriber(\"/Tracking/Green_HSV\", GreenHSV, self.greenHSV) self.YellowHSV_sub = rospy.Subscriber(\"/Tracking/Yellow_HSV\", YellowHSV, self.yellowHSV) self.MidPoint_pub = rospy.Publisher(\"/Tracking/MidPoint\", MidPoint, queue_size = 1) self.red_bridge = CvBridge() self.red_publisher = rospy.Publisher(\"/Tracking/red\", VarRed, queue_size = 1) self.green_bridge = CvBridge() self.green_publisher = rospy.Publisher(\"/Tracking/green\", VarGreen, queue_size = 1) self.yellow_bridge = CvBridge() self.yellow_publisher = rospy.Publisher(\"/Tracking/yellow\", VarYellow, queue_size = 1) self.RedLower = (0, 101, 68) # Declaring the red-specter self.RedUpper = (15, 255, 255) self.GreenLower = (75, 145, 48) # Declaring the green-specter self.GreenUpper = (96, 255, 75) self.YellowLower = (28, 56, 91) # Declaring the yellow-specter self.YellowUpper = (51, 152, 150) self.red_pts = deque(maxlen=14) self.currentDepthImg=0 self.red_counter = 0 self.red_x = 0 self.red_y = 0 self.red_radius = 30 self.green_pts = deque(maxlen=14) self.green_currentDepthImg=0 self.green_counter = 0 self.green_x = 0 self.green_y = 0 self.green_radius = 30 self.yellow_pts = deque(maxlen=14) self.yellow_currentDepthImg=0 self.yellow_counter = 0 self.yellow_x = 0 self.yellow_y = 0 self.yellow_radius = 30 def redHSV(self,msg): self.RedLower = (msg.r_h_low-10, msg.r_s_low-10, msg.r_v_low-10) self.RedUpper = (msg.r_h_high+10, msg.r_s_high+10, msg.r_v_high+10) def greenHSV(self,msg): self.GreenLower = (msg.g_h_low-10, msg.g_s_low-10, msg.g_v_low-10) self.GreenUpper = (msg.g_h_high+10, msg.g_s_high+10, msg.g_v_high+10) def yellowHSV(self,msg): self.YellowLower = (msg.y_h_low-10, msg.y_s_low-10, msg.y_v_low-10) self.YellowUpper = (msg.y_h_high+10, msg.y_s_high+10, msg.y_v_high+10) def callbackDepth(self,msg_depth): try: cv_image_depth = self.red_bridge.imgmsg_to_cv2(msg_depth, \"32FC1\") # CV_Bridge Depth except CvBridgeError as e: print(e) return self.currentDepthImg=cv_image_depth try: a=1 except CvBridgeError as e: print(e) return def callbackVideo(self,data): try: cv_image = self.red_bridge.imgmsg_to_cv2(data, \"bgr8\") # CV_Bridge Video except CvBridgeError as e: print(e) return (rows,cols,channels) = cv_image.shape frame = cv_image blurred = cv2.GaussianBlur(frame, (21, 21), 0) # resize the frame, blur it, and convert it to the HSV (11,11), 0 hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV) # color space. red_mask = cv2.inRange(hsv, self.RedLower, self.RedUpper) # Construct a mask for the color \"red\", then perform red_mask = cv2.erode(red_mask, None, iterations=2) # a series of dilations and erosions to remove any small red_mask = cv2.dilate(red_mask, None, iterations=2) # blobs thats left in the mask. green_mask = cv2.inRange(hsv, self.GreenLower, self.GreenUpper) # construct a mask for the color \"green\", then perform green_mask = cv2.erode(green_mask, None, iterations=2) # a series of dilations and erosions to remove any small green_mask = cv2.dilate(green_mask, None, iterations=2) # blobs thats left in the mask. yellow_mask = cv2.inRange(hsv, self.YellowLower, self.YellowUpper) # construct a mask for the color \"yellow\", then perform yellow_mask = cv2.erode(yellow_mask, None, iterations=2) # a series of dilations and erosions to remove any small yellow_mask = cv2.dilate(yellow_mask, None, iterations=2) # blobs thats left in the mask. red_cnts = cv2.findContours(red_mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # find contours in the mask and initialize the current red_cnts = imutils.grab_contours(red_cnts) red_center = None self.red_radius = 0 green_cnts = cv2.findContours(green_mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # find contours in the mask and initialize the current green_cnts = imutils.grab_contours(green_cnts) green_center = None self.green_radius = 0 yellow_cnts = cv2.findContours(yellow_mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # find contours in the mask and initialize the current yellow_cnts = imutils.grab_contours(yellow_cnts) yellow_center = None self.yellow_radius = 0 cv_imaged=self.currentDepthImg #-----------------------------------------RED_START------------------------------------------------------ if len(red_cnts) &gt; 0: # only proceed if at least one contour was found red_c = max(red_cnts, key=cv2.contourArea) # find the largest contour in the red_mask, then use ((self.red_x, self.red_y), self.red_radius) = cv2.minEnclosingCircle(red_c) # it to compute the minimum enclosing circle and red_M = cv2.moments(red_c) # centroid red_center = (int(red_M[\"m10\"] / red_M[\"m00\"]), int(red_M[\"m01\"] / red_M[\"m00\"])) if self.red_radius &gt; 5: # only proceed if the radius meets a minimum size cv2.circle(frame, (int(self.red_x), int(self.red_y)), int(self.red_radius), (0, 255, 255), 2) # draw the circle and centroid on the red_frame, cv2.circle(frame, red_center, 5, (0, 255, 255), -1) # then update the list of tracked points msg = VarRed() msg.r_visible = True #if self.red_y == self.red_y and self.red_x == self.red_x: r_length = cv_imaged[int(self.red_y),int(self.red_x)] # length to object msg.r_x = self.red_x msg.r_y = self.red_y msg.r_rad = self.red_radius ToRad = 2*np.pi/360 # = 0.01745329252 ToDeg = 360/(2*np.pi) # = 57.29577951308 # Printing pixel values cv2.rectangle(frame, (0, 0), (200, 190), (0,0,0), -1) cv2.putText(frame, str(\"L: %.3f\" %r_length), ((int(self.red_x)),int(self.red_y)), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,255), 2) cv2.putText(frame, str(\"RX: %.1f\" %msg.r_x +\" px\"), (10,30), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) cv2.putText(frame, str(\"RY: %.1f\" %msg.r_y + \" px\"), (10,60), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) # For X-direction red_l_cm = (r_length*100) # Converting to Centimeters start_x_r = 960/(math.tan((55*ToRad))) # finding start x-length in px ang_x_r = (math.atan((self.red_x-960)/start_x_r))*ToDeg # finding horizontal angle red_x_cm = (red_l_cm*math.sin((ang_x_r)*ToRad)) cv2.putText(frame, str(\"RXC: %.1f\" %red_x_cm + \" cm\"), (10,90), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) cv2.putText(frame, str(\"X Ang: %.1f\" %ang_x_r), (10,150), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) # For Y-direction start_y_r = 540/(math.tan((35*ToRad))) # finding start y-length in px ang_y_r = ((math.atan((self.red_y-540)/start_y_r))*ToDeg)*-1 # finding vertical angle red_y_cm = (red_l_cm/math.tan((ang_y_r*ToRad)+(math.pi/2)))*-1 # finding the y-length cv2.putText(frame, str(\"RYC: %.1f\" %red_y_cm + \" cm\"), (10,120), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) cv2.putText(frame, str(\"Y Ang: %.1f\" %ang_y_r), (10,180), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) red_z = (math.cos(abs(ang_x_r)*ToRad))*red_l_cm self.red_pts.appendleft(red_center) msg.r_length = red_l_cm msg.r_xc = red_x_cm msg.r_yc = red_y_cm msg.r_angle = ang_x_r # update the points queue msg.r_z = red_z self.red_publisher.publish(msg) for i in range(1, len(self.red_pts)): # loop over the set of points if self.red_pts[i - 1] is None or self.red_pts[i] is None: # if either of the tracked points continue # are None, ignore them. thickness = int(np.sqrt(64 / float(i + 1)) * 2.5) # otherwise, compute the thickness of the line and cv2.line(frame, self.red_pts[i - 1], self.red_pts[i], (0, 255, 255), thickness) # draw the connecting lines if self.red_radius &lt; 5: msg = VarRed() msg.r_visible = False self.red_publisher.publish(msg) #-----------------------------------------RED_END------------------------------------------------------ #-----------------------------------------GREEN_START------------------------------------------------------ if len(green_cnts) &gt; 0: # same as in red, but for green green_c = max(green_cnts, key=cv2.contourArea) ((self.green_x, self.green_y), self.green_radius) = cv2.minEnclosingCircle(green_c) green_M = cv2.moments(green_c) green_center = (int(green_M[\"m10\"] / green_M[\"m00\"]), int(green_M[\"m01\"] / green_M[\"m00\"])) if self.green_radius &gt; 5: cv2.circle(frame, (int(self.green_x), int(self.green_y)), int(self.green_radius), (0, 255, 255), 2) cv2.circle(frame, green_center, 5, (0, 255, 255), -1) ToRad = 2*np.pi/360 # = 0.01745329252 ToDeg = 360/(2*np.pi) # = 57.29577951308 msg1 = VarGreen() msg1.g_visible = True g_length = cv_imaged[int(self.green_y),int(self.green_x)] msg1.g_x = self.green_x msg1.g_y = self.green_y msg1.g_rad = self.green_radius # Printing pixel values cv2.rectangle(frame, (1740, 0), (1920, 200), (0,0,0), -1) cv2.putText(frame, str(\"L: %.3f\" %g_length), ((int(self.green_x)),int(self.green_y)), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,255), 2) cv2.putText(frame, str(\"GX: %.1f\" %msg1.g_x +\"px\"), (1740,30), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) cv2.putText(frame, str(\"GY: %.1f\" %msg1.g_y + \"px\"), (1740,60), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) # For X-direction green_l_cm = (g_length*100) start_x_g = 960/(math.tan((55*2*np.pi/360))) ang_x_g = (math.atan((self.green_x-960)/start_x_g))*57.295779513 green_x_cm = (green_l_cm*math.sin((ang_x_g)*ToRad)) # For Y-direction start_y_g = 540/(math.tan((35*2*np.pi/360))) ang_y_g = ((math.atan((self.green_y-540)/start_y_g))*57.295779513)*-1 green_y_cm = green_l_cm/math.tan(ang_y_g*ToRad+(math.pi/2))*-1 cv2.putText(frame, str(\"GXC: %.1f\" %green_x_cm + \"cm\"), (1740,90), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) cv2.putText(frame, str(\"X Ang: %.1f\" %ang_x_g), (1740,150), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) cv2.putText(frame, str(\"GYC: %.1f\" %green_y_cm + \"cm\"), (1740,120), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) cv2.putText(frame, str(\"Y Ang: %.1f\" %ang_y_g), (1740,180), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) green_z = (math.cos(abs(ang_x_g)*ToRad))*green_l_cm self.green_pts.appendleft(green_center) msg1.g_length = green_l_cm msg1.g_xc = green_x_cm msg1.g_yc = green_y_cm msg1.g_angle = ang_x_g msg1.g_z = green_z self.green_publisher.publish(msg1) for i in range(1, len(self.green_pts)): if self.green_pts[i - 1] is None or self.green_pts[i] is None: continue thickness = int(np.sqrt(64 / float(i + 1)) * 2.5) cv2.line(frame, self.green_pts[i - 1], self.green_pts[i], (0, 255, 255), thickness) if self.green_radius &lt; 5: msg1 = VarGreen() msg1.g_visible = False self.green_publisher.publish(msg1) #-----------------------------------------GREEN_END------------------------------------------------------ #-----------------------------------------YELLOW_START------------------------------------------------------ if len(yellow_cnts) &gt; 0: # only proceed if at least one contour was found yellow_c = max(yellow_cnts, key=cv2.contourArea) # find the largest contour in the yellow_mask, then use ((self.yellow_x, self.yellow_y), self.yellow_radius) = cv2.minEnclosingCircle(yellow_c) # it to compute the minimum enclosing circle and yellow_M = cv2.moments(yellow_c) # centroid yellow_center = (int(yellow_M[\"m10\"] / yellow_M[\"m00\"]), int(yellow_M[\"m01\"] / yellow_M[\"m00\"])) if self.yellow_radius &gt; 5: # only proceed if the radius meets a minimum size cv2.circle(frame, (int(self.yellow_x), int(self.yellow_y)), int(self.yellow_radius), (0, 0, 200), 2) # draw the circle and centroid on the yellow_frame, cv2.circle(frame, yellow_center, 5, (0, 0, 200), -1) # then update the list of tracked points ToRad = 2*np.pi/360 # = 0.01745329252 ToDeg = 360/(2*np.pi) # = 57.29577951308 msg2 = VarYellow() msg2.y_visible = True y_length = cv_imaged[int(self.yellow_y),int(self.yellow_x)] # length to object msg2.y_x = self.yellow_x msg2.y_y = self.yellow_y msg2.y_rad = self.yellow_radius cv2.putText(frame, str(\"L: %.3f\" %y_length), ((int(self.yellow_x)),int(self.yellow_y)), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,200), 2) # For X-direction yellow_l_cm = y_length*100 # Converting to Centimeters start_x_y = 960/(math.tan((55*2*np.pi/360))) # finding start x-length in px ang_x_y = (math.atan((self.yellow_x-960)/start_x_y))*57.295779513 # finding horizontal angle #yellow_x = yellow_l_cm/math.tan((ang_x_y/57.295779513)) # finding the x-length yellow_x_cm = (yellow_l_cm*math.sin((ang_x_y)*ToRad)) # For Y-direction start_y_y = 540/(math.tan((35*2*np.pi/360))) # finding start y-length in px ang_y_y = ((math.atan((self.yellow_y-540)/start_y_y))*57.295779513)*-1 # finding vertical angle #yellow_y = yellow_l_cm/math.tan((ang_y_y/57.295779513)) # finding the y-length yellow_y_cm = yellow_l_cm/math.tan(ang_y_y*ToRad+(math.pi/2))*-1 yellow_z = (math.cos(abs(ang_x_y)*ToRad))*yellow_l_cm self.yellow_pts.appendleft(yellow_center) msg2.y_length = yellow_l_cm msg2.y_xc = yellow_x_cm msg2.y_yc = yellow_y_cm msg2.y_angle = ang_x_y # update the points queue msg2.y_z = yellow_z self.yellow_publisher.publish(msg2) for i in range(1, len(self.yellow_pts)): # loop over the set of points if self.yellow_pts[i - 1] is None or self.yellow_pts[i] is None: # if either of the tracked points continue # are None, ignore them. thickness = int(np.sqrt(64 / float(i + 1)) * 2.5) # otherwise, compute the thickness of the line and cv2.line(frame, self.yellow_pts[i - 1], self.yellow_pts[i], (0, 0, 255), thickness) # draw the connecting lines if self.yellow_radius &lt; 5: msg2 = VarYellow() msg2.y_visible = False self.yellow_publisher.publish(msg2) #-----------------------------------------YELLOW_END------------------------------------------------------ try: if (self.green_radius &gt; 5) &amp; (self.red_radius &gt; 5): # if you can see both colors, proceed ToRad = 2*np.pi/360 # = 0.01745329252 ToDeg = 360/(2*np.pi) # = 57.29577951308 red_z = (math.cos(abs(ang_x_r)*ToRad))*red_l_cm green_z = (math.cos(abs(ang_x_g)*ToRad))*green_l_cm Delta_z = abs(red_z-green_z) Tot_x = abs(green_x_cm) + abs(red_x_cm) if Delta_z == Delta_z and Tot_x == Tot_x: red_green_angle = (math.atan(Delta_z/Tot_x))*ToDeg normal_angle = red_green_angle if green_l_cm &gt;= red_l_cm: normal_angle = red_green_angle*-1 if green_l_cm &lt; red_l_cm: normal_angle = red_green_angle MidPoint_data = MidPoint() MidPoint_data.angle = normal_angle self.MidPoint_pub.publish(MidPoint_data) length_between_x = math.sqrt((Tot_x*Tot_x)+(Delta_z*Delta_z)) Delta_y = abs(red_y_cm-green_y_cm) length_between = math.sqrt((length_between_x*length_between_x)+(Delta_y*Delta_y)) #dx = green_x_cm - red_x_cm # Finding the space between the colors in x-direction #dy = green_y_cm - red_y_cm # Finding the space between the colors in y-direction # Calculating the direct length between the colors in cm #cv2.rectangle(frame, (500, 0), (680, 160), (0,0,0), -1) #cv2.putText(frame, str(\"Dist: %.1f\" %length_between + \" cm\"), (500,30), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) #Middle_x = dx #Middle_y = dy MP_X = (msg1.g_x + msg.r_x)/2 MP_Y = (msg1.g_y + msg.r_y)/2 #Middle_Point_Angle = (math.atan((MP_X-960)/start_x_g))*57.295779513 #Middle_Point_Angle = ang_x_g - ang_x_r #Middle_Point_Length =(red_x_cm-abs(Middle_x))/(math.sin((math.pi/2)-(Middle_Point_Angle*((2*math.pi)/720)))) #Middle_Point_Length =((red_x_cm-abs(Middle_x))/(math.cos(Middle_Point_Angle*((2*math.pi)/720)))) #cv2.putText(frame, str(\"MX: %.1f\" %Middle_x + \" cm\"), (500,60), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) #cv2.putText(frame, str(\"MY: %.1f\" %Middle_y + \" cm\"), (500,90), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) if MP_X == MP_X and MP_Y == MP_Y: cv2.circle(frame, (int(MP_X), int(MP_Y)), 8, (0, 0, 255), -1) #cv2.putText(frame, str(\"M_L: %.1f\" %Middle_Point_Length + \" cm\"), (500,120), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) #cv2.putText(frame, str(\"M_ang: %.1f\" %Middle_Point_Angle), (500,150), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255,255,255), 1) cv2.line(frame, (int(self.red_x),int(self.red_y)), (int(self.green_x),int(self.green_y)), (0, 0, 0), 2) #MidPoint_data = MidPoint() #MidPoint_data.z = Middle_Point_Length #self.MidPoint_pub.publish(MidPoint_data) cv2.line(frame, (960, 1280), (960, 0), (0, 255, 0), 1) cv2.line(frame, (0, 540), (1920, 540), (0, 255, 0), 1) self.image_pub.publish(self.red_bridge.cv2_to_imgmsg(frame, \"bgr8\")) except CvBridgeError as e: print(e) return def main(args): ic = image_converter() rospy.init_node('Color_Tracker', anonymous=True) try: rospy.spin() except KeyboardInterrupt: print(\"Shutting down\") cv2.destroyAllWindows() if __name__ == '__main__': main(sys.argv)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have an nvidia Jetson Nano (the 4gb version). I am attempting to run this project on it: https://github.com/lucidrains/deep-daze I am attempting to run the command pip install deep-daze. However, I do not have pip so I am running pip3 install deep-daze. When I run that I get chris@chris-desktop:~$ pip3 install deep-daze Collecting deep-daze Using cached https://files.pythonhosted.org/packages/f1/ed/b3f3d9d92f5a48932b3807f683642b28da75722ae93da2f9bdc6af5f1768/deep_daze-0.7.2-py3-none-any.whl Collecting tqdm (from deep-daze) Downloading https://files.pythonhosted.org/packages/f8/3e/2730d0effc282960dbff3cf91599ad0d8f3faedc8e75720fdf224b31ab24/tqdm-4.59.0-py2.py3-none-any.whl (74kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 2.4MB/s Collecting torchvision&gt;=0.8.2 (from deep-daze) Could not find a version that satisfies the requirement torchvision&gt;=0.8.2 (from deep-daze) (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3) No matching distribution found for torchvision&gt;=0.8.2 (from deep-daze) I am pretty unfamiliar with several of the moving parts here and not sure how to fix this. I thought these version numbers may be useful in answering this question: chris@chris-desktop:~$ python3 --version Python 3.6.9 chris@chris-desktop:~$ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) chris@chris-desktop:~$ python2 --version Python 2.7.17 chris@chris-desktop:~$ pip2 --version bash: pip2: command not found chris@chris-desktop:~$ pip --version bash: pip: command not found chris@chris-desktop:~$ cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"18.04.5 LTS (Bionic Beaver)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 18.04.5 LTS\" VERSION_ID=\"18.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=bionic UBUNTU_CODENAME=bionic",
        "answers": [
            [
                "Thanks to the comment from Elbek I got this working! I was able to follow the guide here: https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-8-0-now-available/72048 Unfortunately after I got everything installed I ran into an issue with not having enough memory, but, it all got installed."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I converted my custom yolov3 model to onnx then onnx to tesnsorrt model on Jetson nano, it is taking 0.2 sec to predict on images i tried it on video and it is giving only 3 fps is there any way to increase this.",
        "answers": [
            [
                "You can use FP16 inference mode instead of FP32 and speed up your inference around 2x. Another option is using larger batch size which I\u2019m not sure if it works on Jetson Nano since it has resource limitations. You can find helpful scripts and discussion here"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm building an image for Jetson from a Dokerfile. Here's an excerpt from it: FROM nvcr.io/nvidia/l4t-pytorch:r32.4.4-pth1.6-py3 # some installation RUN ls -l /usr/local/cuda-10.2/targets/aarch64-linux/lib/ # more installation The ls command returns just a couple of files. However when I run the resulting container and use its shell, this directory contains many more files. The problem is that I need some of the libraries from that folder to install something. I want to be able to install it from the Dockerfile but only can do so from the container's shell. Why is the directory incomplete and is there a way to force-build it so it's ready when I need it? Thanks.",
        "answers": [
            [
                "Solved it by adding \"default-runtime\": \"nvidia\" to /etc/docker/daemon.json. Further details here: https://github.com/dusty-nv/jetson-containers#docker-default-runtime"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to run the darknet_video.py script for YOLO from here in Jetson (nano and xavier NX). The code runs fine in one nano but not in another nano and NX. The script is run using the following command in Ubuntu 18.04 (Jetpack) python3 darknet_video.py --input test.mp4 --out_filename out1.txt --weights yolov3-tiny.weights --ext_output --config_file yolov3-tiny.cfg --data_file coco.data --thresh 0.2 I am getting the following errors: JPEG parameter struct mismatch: library thinks size is 584, caller expects 728 pure virtual method called terminate called without an active exception Aborted (core dumped) Since it runs fine in one nano, it could be dependency issue, here is the code in darknet_video.py from ctypes import * import random import os import cv2 import time import darknet import argparse from threading import Thread, enumerate from queue import Queue def parser(): parser = argparse.ArgumentParser(description=\"YOLO Object Detection\") parser.add_argument(\"--input\", type=str, default=0, help=\"video source. If empty, uses webcam 0 stream\") parser.add_argument(\"--out_filename\", type=str, default=\"\", help=\"inference video name. Not saved if empty\") parser.add_argument(\"--weights\", default=\"yolov4.weights\", help=\"yolo weights path\") parser.add_argument(\"--dont_show\", action='store_true', help=\"windown inference display. For headless systems\") parser.add_argument(\"--ext_output\", action='store_true', help=\"display bbox coordinates of detected objects\") parser.add_argument(\"--config_file\", default=\"./cfg/yolov4.cfg\", help=\"path to config file\") parser.add_argument(\"--data_file\", default=\"./cfg/coco.data\", help=\"path to data file\") parser.add_argument(\"--thresh\", type=float, default=.25, help=\"remove detections with confidence below this value\") return parser.parse_args() def str2int(video_path): \"\"\" argparse returns and string althout webcam uses int (0, 1 ...) Cast to int if needed \"\"\" try: return int(video_path) except ValueError: return video_path def check_arguments_errors(args): assert 0 &lt; args.thresh &lt; 1, \"Threshold should be a float between zero and one (non-inclusive)\" if not os.path.exists(args.config_file): raise(ValueError(\"Invalid config path {}\".format(os.path.abspath(args.config_file)))) if not os.path.exists(args.weights): raise(ValueError(\"Invalid weight path {}\".format(os.path.abspath(args.weights)))) if not os.path.exists(args.data_file): raise(ValueError(\"Invalid data file path {}\".format(os.path.abspath(args.data_file)))) if str2int(args.input) == str and not os.path.exists(args.input): raise(ValueError(\"Invalid video path {}\".format(os.path.abspath(args.input)))) def set_saved_video(input_video, output_video, size): fourcc = cv2.VideoWriter_fourcc(*\"MJPG\") fps = int(input_video.get(cv2.CAP_PROP_FPS)) video = cv2.VideoWriter(output_video, fourcc, fps, size) return video def video_capture(frame_queue, darknet_image_queue): while cap.isOpened(): ret, frame = cap.read() if not ret: break frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) frame_resized = cv2.resize(frame_rgb, (width, height), interpolation=cv2.INTER_LINEAR) frame_queue.put(frame_resized) img_for_detect = darknet.make_image(width, height, 3) darknet.copy_image_from_bytes(img_for_detect, frame_resized.tobytes()) darknet_image_queue.put(img_for_detect) cap.release() def inference(darknet_image_queue, detections_queue, fps_queue): while cap.isOpened(): darknet_image = darknet_image_queue.get() prev_time = time.time() detections = darknet.detect_image(network, class_names, darknet_image, thresh=args.thresh) detections_queue.put(detections) fps = int(1/(time.time() - prev_time)) fps_queue.put(fps) print(\"FPS: {}\".format(fps)) darknet.print_detections(detections, args.ext_output) darknet.free_image(darknet_image) cap.release() def drawing(frame_queue, detections_queue, fps_queue): random.seed(3) # deterministic bbox colors video = set_saved_video(cap, args.out_filename, (width, height)) while cap.isOpened(): frame_resized = frame_queue.get() detections = detections_queue.get() fps = fps_queue.get() if frame_resized is not None: image = darknet.draw_boxes(detections, frame_resized, class_colors) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if args.out_filename is not None: video.write(image) if not args.dont_show: cv2.imshow('Inference', image) if cv2.waitKey(fps) == 27: break cap.release() video.release() cv2.destroyAllWindows() if __name__ == '__main__': frame_queue = Queue() darknet_image_queue = Queue(maxsize=1) detections_queue = Queue(maxsize=1) fps_queue = Queue(maxsize=1) args = parser() check_arguments_errors(args) network, class_names, class_colors = darknet.load_network( args.config_file, args.data_file, args.weights, batch_size=1 ) width = darknet.network_width(network) height = darknet.network_height(network) input_path = str2int(args.input) cap = cv2.VideoCapture(input_path) Thread(target=video_capture, args=(frame_queue, darknet_image_queue)).start() Thread(target=inference, args=(darknet_image_queue, detections_queue, fps_queue)).start() Thread(target=drawing, args=(frame_queue, detections_queue, fps_queue)).start() Any ideas will be appreciated.",
        "answers": [
            [
                "JPEG parameter struct mismatch: library thinks size is 584, caller expects 728 This is about jpeglib.h that is used by the app and the low level library. App is compiled with a different jpeglib.h and low level library is compiled with different jpeglib.h and structure in this case its j_decompress_ptr in this header file is different in these two different jpeglib.h files. Make sure that you have low level lib (could be libjpeg-8b) and its client using the same libjpeg.h Remove all the installed libjpeg packages and install only the latest one and try."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I recently bought the nvidia jetson nano microcomputer, the 4Gb. After the first boot, I created a python environment and installed some libraries like numpy, sklearn, pytorch, pandas, etc. Afterwards, I wanted to test a pre-build model for objection recognition. To be more exact I followed Nvidia's tutorial Here. Everything went according to plan and no errors, until I tried to run the programm. I think I need to specify that the following logs were the result of of the program when I ran it on both of my csi cameras: Camera 1 and Camera 2 When running it on the terminal with python my-detection.py. After running it, the programm stoped running. Here are the logs: jetson.inference -- detectNet loading build-in network 'ssd-mobilenet-v2' detectNet -- loading detection network model from: -- model networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff -- input_blob 'Input' -- output_blob 'NMS' -- output_count 'NMS_1' -- class_labels networks/SSD-Mobilenet-v2/ssd_coco_labels.txt -- threshold 0.500000 -- batch_size 1 [TRT] TensorRT version 7.1.3 [TRT] loading NVIDIA plugins... [TRT] Registered plugin creator - ::GridAnchor_TRT version 1 [TRT] Registered plugin creator - ::NMS_TRT version 1 [TRT] Registered plugin creator - ::Reorg_TRT version 1 [TRT] Registered plugin creator - ::Region_TRT version 1 [TRT] Registered plugin creator - ::Clip_TRT version 1 [TRT] Registered plugin creator - ::LReLU_TRT version 1 [TRT] Registered plugin creator - ::PriorBox_TRT version 1 [TRT] Registered plugin creator - ::Normalize_TRT version 1 [TRT] Registered plugin creator - ::RPROI_TRT version 1 [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1 [TRT] Could not register plugin creator - ::FlattenConcat_TRT version 1 [TRT] Registered plugin creator - ::CropAndResize version 1 [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1 [TRT] Registered plugin creator - ::Proposal version 1 [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1 [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1 [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1 [TRT] Registered plugin creator - ::Split version 1 [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1 [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1 [TRT] detected model format - UFF (extension '.uff') [TRT] desired precision specified for GPU: FASTEST [TRT] requested fasted precision for device GPU without providing valid calibrator, disabling INT8 [TRT] native precisions detected for GPU: FP32, FP16 [TRT] selecting fastest native precision for GPU: FP16 [TRT] attempting to open engine cache file /usr/local/bin/networks/SSD-Mobilenet- v2/ssd_mobilenet_v2_coco.uff.1.1.7103.GPU.FP16.engine [TRT] loading network plan from engine cache... /usr/local/bin/networks/SSD-Mobilenet- v2/ssd_mobilenet_v2_coco.uff.1.1.7103.GPU.FP16.engine [TRT] device GPU, loaded /usr/local/bin/networks/SSD-Mobilenet- v2/ssd_mobilenet_v2_coco.uff [TRT] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors. [TRT] Deserialize required 6072963 microseconds. [TRT] [TRT] CUDA engine context initialized on device GPU: [TRT] -- layers 117 [TRT] -- maxBatchSize 1 [TRT] -- workspace 0 [TRT] -- deviceMemory 35449856 [TRT] -- bindings 3 [TRT] binding 0 -- index 0 -- name 'Input' -- type FP32 -- in/out INPUT -- # dims 3 -- dim #0 3 (SPATIAL) -- dim #1 300 (SPATIAL) -- dim #2 300 (SPATIAL) [TRT] binding 1 -- index 1 -- name 'NMS' -- type FP32 -- in/out OUTPUT -- # dims 3 -- dim #0 1 (SPATIAL) -- dim #1 100 (SPATIAL) -- dim #2 7 (SPATIAL) [TRT] binding 2 -- index 2 -- name 'NMS_1' -- type FP32 -- in/out OUTPUT -- # dims 3 -- dim #0 1 (SPATIAL) -- dim #1 1 (SPATIAL) -- dim #2 1 (SPATIAL) [TRT] [TRT] binding to input 0 Input binding index: 0 [TRT] binding to input 0 Input dims (b=1 c=3 h=300 w=300) size=1080000 [TRT] binding to output 0 NMS binding index: 1 [TRT] binding to output 0 NMS dims (b=1 c=1 h=100 w=7) size=2800 [TRT] binding to output 1 NMS_1 binding index: 2 [TRT] binding to output 1 NMS_1 dims (b=1 c=1 h=1 w=1) size=4 [TRT] [TRT] device GPU, /usr/local/bin/networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff initialized. [TRT] W = 7 H = 100 C = 1 [TRT] detectNet -- maximum bounding boxes: 100 [TRT] detectNet -- loaded 91 class info entries [TRT] detectNet -- number of object classes: 91 [gstreamer] initialized gstreamer, version 1.14.5.0 [gstreamer] gstCamera -- attempting to create device csi://0 [gstreamer] gstCamera pipeline string: [gstreamer] nvarguscamerasrc sensor-id=0 ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, framerate=30/1, format=(string)NV12 ! nvvidconv flip-method=2 ! video/x- raw ! appsink name=mysink [gstreamer] gstCamera successfully created device csi://0 [video] created gstCamera from csi://0 ------------------------------------------------ gstCamera video options: ------------------------------------------------ -- URI: csi://0 - protocol: csi - location: 0 -- deviceType: csi -- ioType: input -- codec: raw -- width: 1280 -- height: 720 -- frameRate: 30.000000 -- bitRate: 0 -- numBuffers: 4 -- zeroCopy: true -- flipMethod: rotate-180 -- loop: 0 ------------------------------------------------ [OpenGL] glDisplay -- X screen 0 resolution: 1920x1080 [OpenGL] glDisplay -- X window resolution: 1920x1080 [OpenGL] glDisplay -- display device initialized (1920x1080) [video] created glDisplay from display://0 ------------------------------------------------ glDisplay video options: ------------------------------------------------ -- URI: display://0 - protocol: display - location: 0 -- deviceType: display -- ioType: output -- codec: raw -- width: 1920 -- height: 1080 -- frameRate: 0.000000 -- bitRate: 0 -- numBuffers: 4 -- zeroCopy: true -- flipMethod: none -- loop: 0 ------------------------------------------------ [gstreamer] opening gstCamera for streaming, transitioning pipeline to GST_STATE_PLAYING [gstreamer] gstreamer changed state from NULL to READY ==&gt; mysink [gstreamer] gstreamer changed state from NULL to READY ==&gt; capsfilter1 [gstreamer] gstreamer changed state from NULL to READY ==&gt; nvvconv0 [gstreamer] gstreamer changed state from NULL to READY ==&gt; capsfilter0 [gstreamer] gstreamer changed state from NULL to READY ==&gt; nvarguscamerasrc0 [gstreamer] gstreamer changed state from NULL to READY ==&gt; pipeline0 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; capsfilter1 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; nvvconv0 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; capsfilter0 [gstreamer] gstreamer stream status CREATE ==&gt; src [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; nvarguscamerasrc0 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; pipeline0 [gstreamer] gstreamer stream status ENTER ==&gt; src [gstreamer] gstreamer message new-clock ==&gt; pipeline0 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; capsfilter1 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; nvvconv0 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; capsfilter0 [gstreamer] gstreamer message stream-start ==&gt; pipeline0 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; nvarguscamerasrc0 Error ---------- generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst- nvarguscamera/gstnvarguscamerasrc.cpp, execute:645 No cameras available [gstreamer] gstCamera -- end of stream (EOS) (python:12329): GStreamer-CRITICAL **: 10:33:50.156: gst_mini_object_set_qdata: assertion 'object != NULL' failed I tried running cheese webcam,but the screen displayed \"No Device Found\". That was the result with both csi cameras. The camers are csi, but when I plugged them in, the infra red one, had its lights shining, but, as I said, the camera did not work! I am doing something wrong, do I need to do something I haven't done? Please Help me!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am attempting to setup the Kubernets Master node on a Jetson Nano. However, I am stuck at the step where you are supposed to download the flannel YAML. Everytime I get the same error: Unable to connect to the server: x509: certificate is valid for 10.xx.x.x, 192.xxx.x.xx, not 127.0.0.1 I'm not sure why it is trying to connect to my local host to get the download. So far I have done the following: sudo kubeadm reset I would always delete the .kube/ folder in my home directory. Then I run: sudo kubeadm init --pod-network-cidr=10.244.0.0/16 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config All of these commands execute no problem. For the first command I have also tried adding this line, but then I get an error saying it can't verify the certificate: --apiserver-cert-extra-sans=127.0.0.1 I have tried two commands to download the YAML file sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml and curl -sSL https://raw.githubusercontent.com/coreos/flannel/v0.12.0/Documentation/kube-flannel.yml | kubectl apply -f - In the config file I have added the line insecure-skip-tls-verify: true right after the key, but this doesn't work either. Is there something I am missing or need to change? None of the other solutions have worked. I have used the following references: Flannel network failing during Kubernetes installation, please suggest how to fix this Kubernetes Setting Up Flannel Pod Network https://github.com/kubernetes/kubernetes/issues/48378 x509 certificate signed by unknown authority- Kubernetes microk8s, DEVOPS : Unable to connect to the server: x509: certificate is valid for &lt;internal IPs&gt;, not &lt;external IP&gt; https://phoenixnap.com/kb/install-kubernetes-on-ubuntu https://developer.nvidia.com/blog/deploying-ai-apps-with-egx-on-jetson-xavier-nx-microservers/",
        "answers": [],
        "votes": []
    },
    {
        "question": "I bought a CSI camera, IMX219, for my OpenCV project. When I run the below command, there seems to be no delay at all in showing the frames in realtime. $ nvgstcapture-1.0 -m 2 --prev-res 4 However, when I run my simple python code using the below pipleline, capturing is significantly slow, pipeline = 'nvarguscamerasrc ! video/x-raw(memory:NVMM), width=1920, height=1080, format=NV12, framerate=30/1 ! nvvidconv flip-method=0 ! video/x-raw, width=1920, height=1080, format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink' cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER) What should I do if I need 1920x1080, 30fps VideoCapture() in Opencv? Appreciate your help!",
        "answers": [
            [
                "Here's a similar question I asked on the NVIDIA devloper forums:https://forums.developer.nvidia.com/t/optimizing-opencv-gstreamer-with-mipi-and-usb-uvc-cameras/123665/27 Basically, the issue is that you lose a lot of the hardware acceleration in the gstreamer pipeline for OpenCV. videoconvert in particular is very slow in the pipeline. With the nature of my application, I used nvvidconv to convert from BGRx to I420. OpenCV can't handle BGRx, but it can do I420 which I convert to BGR for processing in later parts of my application. appsink is also slow on their devices and you don't have another option for the sink. If you need constant real time frames, I would recommend using the libargus API as that probably has the highest performance being provided by NVIDIA and optimized for their hardware."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I want to create an image from nvidia jetson tx2 using flash.sh file, I could manage run it, but throw an error. I set board on Recovery Mode and execute: sudo /bin/bash ./flash.sh -r -k APP -G nvidia.img jetson-tx2 mmcblk0p1 Error: Invalid target board - jetson-tx2-devkit. I'm using a jetson tx2 p3310-1000, so the name is correct, no matter I tried with jetson-tx2 and nothing. Ubuntu 18.04.5 LTS, Jetpack 4.5.1",
        "answers": [
            [
                "You must be sure that you are using the correct flash file with your Jetpack, for the case of version 4.5.1 you could use this code (for other version, please check L4T Driver Package (BSP) in https://developer.nvidia.com/embedded/linux-tegra) wget https://developer.nvidia.com/embedded/l4t/r32_release_v5.1/r32_release_v5.1/t186/tegra186_linux_r32.5.1_aarch64.tbz2 tar -xjvf tegra186_linux_r32.5.1_aarch64.tbz2 rm tegra186_linux_r32.5.1_aarch64.tbz2 cd Linux_for_Tegra/rootfs sudo wget https://developer.nvidia.com/embedded/l4t/r32_release_v5.1/r32_release_v5.1/t186/tegra_linux_sample-root-filesystem_r32.5.1_aarch64.tbz2 sudo tar -xjvf tegra_linux_sample-root-filesystem_r32.5.1_aarch64.tbz2 sudo rm tegra_linux_sample-root-filesystem_r32.5.1_aarch64.tbz2 &amp;&amp; cd .. sudo ./apply_binaries.sh sudo /bin/bash ./flash.sh -r -k APP -G nvidia.img jetson-tx2-devkit mmcblk0p1 [nothing else to edit]"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to create an image from nvidia jetson tx2, in several places (like https://developer.ridgerun.com/wiki/index.php?title=Cloning_TX2) talk about flash.sh file to perform the task but I cannot found it, also I search using find / -iname flash.sh and nothing. Where can I find this file? Do I need install something eles? Ubuntu 18.04.5 LTS, Jetpack 4.5.1",
        "answers": [
            [
                "Download L4T Driver Package (BSP) driver from: https://developer.nvidia.com/embedded/linux-tegra and download, be sure to dowload the correct version for your JetPach, for 4.5.1 check this: https://developer.nvidia.com/embedded/l4t/r32_release_v5.1/r32_release_v5.1/t186/tegra186_linux_r32.5.1_aarch64.tbz2"
            ],
            [
                "Try to download this https://gist.github.com/Davidnet/013ceb704ebdc7ebd728e059f90fca80 Put it in your path. Run ./flash.sh"
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "\u2022 Hardware Platform (Jetson / GPU) Jetson Nano 4GB, Ubuntu 18.4 \u2022 DeepStream Version marketplace.azurecr.io/nvidia/deepstream-iot2-l4t:latest \u2022 JetPack Version 4.3 \u2022 Issue Type Output inference class is different from Model class \u2022 How to reproduce the issue ? On DeepStream, deploy a object detection ONNX model. My model is ONNX model exported from Azure Custom Vision. My label file has 2 classes - 'Mask', 'No_Mask'. Deployment works fine and I am able to execute my model using DeepStream. However, output inference class I am getting as 'Vehicle' and 'No_Mask'. Can you please help me understand why I am getting output inference label as \"Vehicle\" when it is not there in my Model. Sample output inference log {\"log\":\" \"1|324|23|380|61|Vehicle|#|||||||0\"\\n\",\"stream\":\"stdout\",\"time\":\"2021-01-05T16:15:15.614591738Z\"} {\"log\":\" \"1|324|23|380|61|Vehicle|#|||||||0\"\\n\",\"stream\":\"stdout\",\"time\":\"2021-01-05T16:15:15.614790179Z\"} {\"log\":\" \"2|141|15|365|161|No Mask\"\\n\",\"stream\":\"stdout\",\"time\":\"2021-01-05T16:15:15.614221209Z\"}",
        "answers": [
            [
                "You've most probably specified wrong labels file or the classes in it are wrong. It's provided in labelfile-path as labelfile-path=labels.txt"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Not really familiar how cmake and the packages are been resolve in ubuntu. I have the next file cmake for ROS systemm. cmake_minimum_required(VERSION 3.1.3) project(moveit_calibration_plugins) set(CMAKE_CXX_STANDARD 14) set(CMAKE_CXX_EXTENSIONS OFF) if(NOT CMAKE_CONFIGURATION_TYPES AND NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE Release) endif() find_package(catkin REQUIRED COMPONENTS roscpp rosconsole tf2 tf2_eigen tf2_geometry_msgs pluginlib sensor_msgs ) find_package(Eigen3 REQUIRED) find_package(OpenCV REQUIRED imgcodecs aruco) catkin_package( INCLUDE_DIRS handeye_calibration_target/include handeye_calibration_solver/include LIBRARIES # No libraries are exported because directly linking to a plugin is \"highly discouraged\": # http://wiki.ros.org/class_loader#Caution_of_Linking_Directly_Against_Plugin_Libraries CATKIN_DEPENDS roscpp sensor_msgs DEPENDS EIGEN3 ) include_directories( handeye_calibration_target/include handeye_calibration_solver/include ${catkin_INCLUDE_DIRS} ) include_directories(SYSTEM ${OpenCV_INCLUDE_DIRS} ${EIGEN3_INCLUDE_DIRS} ) add_subdirectory(handeye_calibration_target) add_subdirectory(handeye_calibration_solver) install( FILES handeye_calibration_target_plugin_description.xml handeye_calibration_solver_plugin_description.xml DESTINATION ${CATKIN_PACKAGE_SHARE_DESTINATION}) I had to compile my own OpenCV since I am using Nvidia Jetson. I managed to install them in /usr/local/src both OpenCV and OpenCV-contrib. I resolved manually inside the cmake so it can find OpenCV changing. from: find_package(OpenCV 4.1.1 EXACT REQUIRED imgcodecs aruco) to: find_package(OpenCV 4.1.1 EXACT REQUIRED PATHS /usr/local/lib imgcodecs aruco) The issue is then that the aruco can not be find. /home/ros/alfredonator/edo_ws/src/moveit_calibration/moveit_calibration_gui/handeye_calibration_rviz_plugin /include/moveit/handeye_calibration_rviz_plugin/handeye_target_widget.h:57:29: fatal error: opencv2/aruco.hpp: No such file or directory compilation terminated. Running pkg-config --libs opencv I see that is pointing to /usr/local/lib instead of /usr/local/src. Is that the issue why the cmake is not failing to find OpenCV but the next module fails to find aruco? /usr/local/src \u251c\u2500\u2500 opencv-4.1.1 \u2502 \u251c\u2500\u2500 3rdparty \u2502 \u251c\u2500\u2500 apps \u2502 \u251c\u2500\u2500 cmake \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u251c\u2500\u2500 CONTRIBUTING.md \u2502 \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 include \u2502 \u251c\u2500\u2500 LICENSE \u2502 \u251c\u2500\u2500 modules \u2502 \u251c\u2500\u2500 platforms \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 release \u2502 \u2514\u2500\u2500 samples \u251c\u2500\u2500 opencv-4.1.1.zip \u251c\u2500\u2500 opencv_contrib-4.1.1 \u2502 \u251c\u2500\u2500 CONTRIBUTING.md \u2502 \u251c\u2500\u2500 doc \u2502 \u251c\u2500\u2500 LICENSE \u2502 \u251c\u2500\u2500 modules \u2502 \u251c\u2500\u2500 README.md \u2502 \u2514\u2500\u2500 samples \u2514\u2500\u2500 opencv_contrib-4.1.1.zip /usr/local/lib . \u251c\u2500\u2500 cmake \u251c\u2500\u2500 libfw.a \u251c\u2500\u2500 libglfw3.a \u251c\u2500\u2500 libopencv_aruco.so -&gt; libopencv_aruco.so.4.1 \u251c\u2500\u2500 libopencv_aruco.so.4.1 -&gt; libopencv_aruco.so.4.1.1 \u251c\u2500\u2500 libopencv_aruco.so.4.1.1 \u251c\u2500\u2500 libopencv_bgsegm.so -&gt; libopencv_bgsegm.so.4.1 \u251c\u2500\u2500 libopencv_bgsegm.so.4.1 -&gt; libopencv_bgsegm.so.4.1.1 \u251c\u2500\u2500 libopencv_bgsegm.so.4.1.1 \u251c\u2500\u2500 libopencv_bioinspired.so -&gt; libopencv_bioinspired.so.4.1 \u251c\u2500\u2500 libopencv_bioinspired.so.4.1 -&gt; libopencv_bioinspired.so.4.1.1 \u251c\u2500\u2500 libopencv_bioinspired.so.4.1.1 \u251c\u2500\u2500 libopencv_calib3d.so -&gt; libopencv_calib3d.so.4.1 \u251c\u2500\u2500 libopencv_calib3d.so.3.4 -&gt; libopencv_calib3d.so.3.4.0 \u251c\u2500\u2500 libopencv_calib3d.so.3.4.0 \u251c\u2500\u2500 libopencv_calib3d.so.4.1 -&gt; libopencv_calib3d.so.4.1.1 \u251c\u2500\u2500 libopencv_calib3d.so.4.1.1 \u251c\u2500\u2500 libopencv_ccalib.so -&gt; libopencv_ccalib.so.4.1 \u251c\u2500\u2500 libopencv_ccalib.so.4.1 -&gt; libopencv_ccalib.so.4.1.1 \u251c\u2500\u2500 libopencv_ccalib.so.4.1.1 \u251c\u2500\u2500 libopencv_core.so -&gt; libopencv_core.so.4.1 \u251c\u2500\u2500 libopencv_core.so.3.4 -&gt; libopencv_core.so.3.4.0 \u251c\u2500\u2500 libopencv_core.so.3.4.0 \u251c\u2500\u2500 libopencv_core.so.4.1 -&gt; libopencv_core.so.4.1.1 \u251c\u2500\u2500 libopencv_core.so.4.1.1 \u251c\u2500\u2500 libopencv_cudaarithm.so -&gt; libopencv_cudaarithm.so.4.1 \u251c\u2500\u2500 libopencv_cudaarithm.so.3.4 -&gt; libopencv_cudaarithm.so.3.4.0 \u251c\u2500\u2500 libopencv_cudaarithm.so.3.4.0 \u251c\u2500\u2500 libopencv_cudaarithm.so.4.1 -&gt; libopencv_cudaarithm.so.4.1.1 \u251c\u2500\u2500 libopencv_cudaarithm.so.4.1.1 \u251c\u2500\u2500 libopencv_cudabgsegm.so -&gt; libopencv_cudabgsegm.so.4.1 \u251c\u2500\u2500 libopencv_cudabgsegm.so.3.4 -&gt; libopencv_cudabgsegm.so.3.4.0 \u251c\u2500\u2500 libopencv_cudabgsegm.so.3.4.0 \u251c\u2500\u2500 libopencv_cudabgsegm.so.4.1 -&gt; libopencv_cudabgsegm.so.4.1.1 \u251c\u2500\u2500 libopencv_cudabgsegm.so.4.1.1 \u251c\u2500\u2500 libopencv_cudacodec.so -&gt; libopencv_cudacodec.so.4.1 \u251c\u2500\u2500 libopencv_cudacodec.so.3.4 -&gt; libopencv_cudacodec.so.3.4.0 \u251c\u2500\u2500 libopencv_cudacodec.so.3.4.0 \u251c\u2500\u2500 libopencv_cudacodec.so.4.1 -&gt; libopencv_cudacodec.so.4.1.1 \u251c\u2500\u2500 libopencv_cudacodec.so.4.1.1 \u251c\u2500\u2500 libopencv_cudafeatures2d.so -&gt; libopencv_cudafeatures2d.so.4.1 \u251c\u2500\u2500 libopencv_cudafeatures2d.so.3.4 -&gt; libopencv_cudafeatures2d.so.3.4.0 \u251c\u2500\u2500 libopencv_cudafeatures2d.so.3.4.0 \u251c\u2500\u2500 libopencv_cudafeatures2d.so.4.1 -&gt; libopencv_cudafeatures2d.so.4.1.1 \u251c\u2500\u2500 libopencv_cudafeatures2d.so.4.1.1 \u251c\u2500\u2500 libopencv_cudafilters.so -&gt; libopencv_cudafilters.so.4.1 \u251c\u2500\u2500 libopencv_cudafilters.so.3.4 -&gt; libopencv_cudafilters.so.3.4.0 \u251c\u2500\u2500 libopencv_cudafilters.so.3.4.0 \u251c\u2500\u2500 libopencv_cudafilters.so.4.1 -&gt; libopencv_cudafilters.so.4.1.1 \u251c\u2500\u2500 libopencv_cudafilters.so.4.1.1 \u251c\u2500\u2500 libopencv_cudaimgproc.so -&gt; libopencv_cudaimgproc.so.4.1 \u251c\u2500\u2500 libopencv_cudaimgproc.so.3.4 -&gt; libopencv_cudaimgproc.so.3.4.0 \u251c\u2500\u2500 libopencv_cudaimgproc.so.3.4.0 \u251c\u2500\u2500 libopencv_cudaimgproc.so.4.1 -&gt; libopencv_cudaimgproc.so.4.1.1 \u251c\u2500\u2500 libopencv_cudaimgproc.so.4.1.1 \u251c\u2500\u2500 libopencv_cudalegacy.so -&gt; libopencv_cudalegacy.so.4.1 \u251c\u2500\u2500 libopencv_cudalegacy.so.3.4 -&gt; libopencv_cudalegacy.so.3.4.0 \u251c\u2500\u2500 libopencv_cudalegacy.so.3.4.0 \u251c\u2500\u2500 libopencv_cudalegacy.so.4.1 -&gt; libopencv_cudalegacy.so.4.1.1 \u251c\u2500\u2500 libopencv_cudalegacy.so.4.1.1 \u251c\u2500\u2500 libopencv_cudaobjdetect.so -&gt; libopencv_cudaobjdetect.so.4.1 \u251c\u2500\u2500 libopencv_cudaobjdetect.so.3.4 -&gt; libopencv_cudaobjdetect.so.3.4.0 \u251c\u2500\u2500 libopencv_cudaobjdetect.so.3.4.0 \u251c\u2500\u2500 libopencv_cudaobjdetect.so.4.1 -&gt; libopencv_cudaobjdetect.so.4.1.1 \u251c\u2500\u2500 libopencv_cudaobjdetect.so.4.1.1 \u251c\u2500\u2500 libopencv_cudaoptflow.so -&gt; libopencv_cudaoptflow.so.4.1 \u251c\u2500\u2500 libopencv_cudaoptflow.so.3.4 -&gt; libopencv_cudaoptflow.so.3.4.0 \u251c\u2500\u2500 libopencv_cudaoptflow.so.3.4.0 \u251c\u2500\u2500 libopencv_cudaoptflow.so.4.1 -&gt; libopencv_cudaoptflow.so.4.1.1 \u251c\u2500\u2500 libopencv_cudaoptflow.so.4.1.1 \u251c\u2500\u2500 libopencv_cudastereo.so -&gt; libopencv_cudastereo.so.4.1 \u251c\u2500\u2500 libopencv_cudastereo.so.3.4 -&gt; libopencv_cudastereo.so.3.4.0 \u251c\u2500\u2500 libopencv_cudastereo.so.3.4.0 \u251c\u2500\u2500 libopencv_cudastereo.so.4.1 -&gt; libopencv_cudastereo.so.4.1.1 \u251c\u2500\u2500 libopencv_cudastereo.so.4.1.1 \u251c\u2500\u2500 libopencv_cudawarping.so -&gt; libopencv_cudawarping.so.4.1 \u251c\u2500\u2500 libopencv_cudawarping.so.3.4 -&gt; libopencv_cudawarping.so.3.4.0 \u251c\u2500\u2500 libopencv_cudawarping.so.3.4.0 \u251c\u2500\u2500 libopencv_cudawarping.so.4.1 -&gt; libopencv_cudawarping.so.4.1.1 \u251c\u2500\u2500 libopencv_cudawarping.so.4.1.1 \u251c\u2500\u2500 libopencv_cudev.so -&gt; libopencv_cudev.so.4.1 \u251c\u2500\u2500 libopencv_cudev.so.3.4 -&gt; libopencv_cudev.so.3.4.0 \u251c\u2500\u2500 libopencv_cudev.so.3.4.0 \u251c\u2500\u2500 libopencv_cudev.so.4.1 -&gt; libopencv_cudev.so.4.1.1 \u251c\u2500\u2500 libopencv_cudev.so.4.1.1 \u251c\u2500\u2500 libopencv_cvv.so -&gt; libopencv_cvv.so.4.1 \u251c\u2500\u2500 libopencv_cvv.so.4.1 -&gt; libopencv_cvv.so.4.1.1 \u251c\u2500\u2500 libopencv_cvv.so.4.1.1 \u251c\u2500\u2500 libopencv_datasets.so -&gt; libopencv_datasets.so.4.1 \u251c\u2500\u2500 libopencv_datasets.so.4.1 -&gt; libopencv_datasets.so.4.1.1 \u251c\u2500\u2500 libopencv_datasets.so.4.1.1 \u251c\u2500\u2500 libopencv_dnn_objdetect.so -&gt; libopencv_dnn_objdetect.so.4.1 \u251c\u2500\u2500 libopencv_dnn_objdetect.so.4.1 -&gt; libopencv_dnn_objdetect.so.4.1.1 \u251c\u2500\u2500 libopencv_dnn_objdetect.so.4.1.1 \u251c\u2500\u2500 libopencv_dnn.so -&gt; libopencv_dnn.so.4.1 \u251c\u2500\u2500 libopencv_dnn.so.3.4 -&gt; libopencv_dnn.so.3.4.0 \u251c\u2500\u2500 libopencv_dnn.so.3.4.0 \u251c\u2500\u2500 libopencv_dnn.so.4.1 -&gt; libopencv_dnn.so.4.1.1 \u251c\u2500\u2500 libopencv_dnn.so.4.1.1 \u251c\u2500\u2500 libopencv_dpm.so -&gt; libopencv_dpm.so.4.1 \u251c\u2500\u2500 libopencv_dpm.so.4.1 -&gt; libopencv_dpm.so.4.1.1 \u251c\u2500\u2500 libopencv_dpm.so.4.1.1 \u251c\u2500\u2500 libopencv_face.so -&gt; libopencv_face.so.4.1 \u251c\u2500\u2500 libopencv_face.so.4.1 -&gt; libopencv_face.so.4.1.1 \u251c\u2500\u2500 libopencv_face.so.4.1.1 \u251c\u2500\u2500 libopencv_features2d.so -&gt; libopencv_features2d.so.4.1 \u251c\u2500\u2500 libopencv_features2d.so.3.4 -&gt; libopencv_features2d.so.3.4.0 \u251c\u2500\u2500 libopencv_features2d.so.3.4.0 \u251c\u2500\u2500 libopencv_features2d.so.4.1 -&gt; libopencv_features2d.so.4.1.1 \u251c\u2500\u2500 libopencv_features2d.so.4.1.1 \u251c\u2500\u2500 libopencv_flann.so -&gt; libopencv_flann.so.4.1 \u251c\u2500\u2500 libopencv_flann.so.3.4 -&gt; libopencv_flann.so.3.4.0 \u251c\u2500\u2500 libopencv_flann.so.3.4.0 \u251c\u2500\u2500 libopencv_flann.so.4.1 -&gt; libopencv_flann.so.4.1.1 \u251c\u2500\u2500 libopencv_flann.so.4.1.1 \u251c\u2500\u2500 libopencv_freetype.so -&gt; libopencv_freetype.so.4.1 \u251c\u2500\u2500 libopencv_freetype.so.4.1 -&gt; libopencv_freetype.so.4.1.1 \u251c\u2500\u2500 libopencv_freetype.so.4.1.1 \u251c\u2500\u2500 libopencv_fuzzy.so -&gt; libopencv_fuzzy.so.4.1 \u251c\u2500\u2500 libopencv_fuzzy.so.4.1 -&gt; libopencv_fuzzy.so.4.1.1 \u251c\u2500\u2500 libopencv_fuzzy.so.4.1.1 \u251c\u2500\u2500 libopencv_gapi.so -&gt; libopencv_gapi.so.4.1 \u251c\u2500\u2500 libopencv_gapi.so.4.1 -&gt; libopencv_gapi.so.4.1.1 \u251c\u2500\u2500 libopencv_gapi.so.4.1.1 \u251c\u2500\u2500 libopencv_hdf.so -&gt; libopencv_hdf.so.4.1 \u251c\u2500\u2500 libopencv_hdf.so.4.1 -&gt; libopencv_hdf.so.4.1.1 \u251c\u2500\u2500 libopencv_hdf.so.4.1.1 \u251c\u2500\u2500 libopencv_hfs.so -&gt; libopencv_hfs.so.4.1 \u251c\u2500\u2500 libopencv_hfs.so.4.1 -&gt; libopencv_hfs.so.4.1.1 \u251c\u2500\u2500 libopencv_hfs.so.4.1.1 \u251c\u2500\u2500 libopencv_highgui.so -&gt; libopencv_highgui.so.4.1 \u251c\u2500\u2500 libopencv_highgui.so.3.4 -&gt; libopencv_highgui.so.3.4.0 \u251c\u2500\u2500 libopencv_highgui.so.3.4.0 \u251c\u2500\u2500 libopencv_highgui.so.4.1 -&gt; libopencv_highgui.so.4.1.1 \u251c\u2500\u2500 libopencv_highgui.so.4.1.1 \u251c\u2500\u2500 libopencv_imgcodecs.so -&gt; libopencv_imgcodecs.so.4.1 \u251c\u2500\u2500 libopencv_imgcodecs.so.3.4 -&gt; libopencv_imgcodecs.so.3.4.0 \u251c\u2500\u2500 libopencv_imgcodecs.so.3.4.0 \u251c\u2500\u2500 libopencv_imgcodecs.so.4.1 -&gt; libopencv_imgcodecs.so.4.1.1 \u251c\u2500\u2500 libopencv_imgcodecs.so.4.1.1 \u251c\u2500\u2500 libopencv_img_hash.so -&gt; libopencv_img_hash.so.4.1 \u251c\u2500\u2500 libopencv_img_hash.so.4.1 -&gt; libopencv_img_hash.so.4.1.1 \u251c\u2500\u2500 libopencv_img_hash.so.4.1.1 \u251c\u2500\u2500 libopencv_imgproc.so -&gt; libopencv_imgproc.so.4.1 \u251c\u2500\u2500 libopencv_imgproc.so.3.4 -&gt; libopencv_imgproc.so.3.4.0 \u251c\u2500\u2500 libopencv_imgproc.so.3.4.0 \u251c\u2500\u2500 libopencv_imgproc.so.4.1 -&gt; libopencv_imgproc.so.4.1.1 \u251c\u2500\u2500 libopencv_imgproc.so.4.1.1 \u251c\u2500\u2500 libopencv_line_descriptor.so -&gt; libopencv_line_descriptor.so.4.1 \u251c\u2500\u2500 libopencv_line_descriptor.so.4.1 -&gt; libopencv_line_descriptor.so.4.1.1 \u251c\u2500\u2500 libopencv_line_descriptor.so.4.1.1 \u251c\u2500\u2500 libopencv_ml.so -&gt; libopencv_ml.so.4.1 \u251c\u2500\u2500 libopencv_ml.so.3.4 -&gt; libopencv_ml.so.3.4.0 \u251c\u2500\u2500 libopencv_ml.so.3.4.0 \u251c\u2500\u2500 libopencv_ml.so.4.1 -&gt; libopencv_ml.so.4.1.1 \u251c\u2500\u2500 libopencv_ml.so.4.1.1 \u251c\u2500\u2500 libopencv_objdetect.so -&gt; libopencv_objdetect.so.4.1 \u251c\u2500\u2500 libopencv_objdetect.so.3.4 -&gt; libopencv_objdetect.so.3.4.0 \u251c\u2500\u2500 libopencv_objdetect.so.3.4.0 \u251c\u2500\u2500 libopencv_objdetect.so.4.1 -&gt; libopencv_objdetect.so.4.1.1 \u251c\u2500\u2500 libopencv_objdetect.so.4.1.1 \u251c\u2500\u2500 libopencv_optflow.so -&gt; libopencv_optflow.so.4.1 \u251c\u2500\u2500 libopencv_optflow.so.4.1 -&gt; libopencv_optflow.so.4.1.1 \u251c\u2500\u2500 libopencv_optflow.so.4.1.1 \u251c\u2500\u2500 libopencv_phase_unwrapping.so -&gt; libopencv_phase_unwrapping.so.4.1 \u251c\u2500\u2500 libopencv_phase_unwrapping.so.4.1 -&gt; libopencv_phase_unwrapping.so.4.1.1 \u251c\u2500\u2500 libopencv_phase_unwrapping.so.4.1.1 \u251c\u2500\u2500 libopencv_photo.so -&gt; libopencv_photo.so.4.1 \u251c\u2500\u2500 libopencv_photo.so.3.4 -&gt; libopencv_photo.so.3.4.0 \u251c\u2500\u2500 libopencv_photo.so.3.4.0 \u251c\u2500\u2500 libopencv_photo.so.4.1 -&gt; libopencv_photo.so.4.1.1 \u251c\u2500\u2500 libopencv_photo.so.4.1.1 \u251c\u2500\u2500 libopencv_plot.so -&gt; libopencv_plot.so.4.1 \u251c\u2500\u2500 libopencv_plot.so.4.1 -&gt; libopencv_plot.so.4.1.1 \u251c\u2500\u2500 libopencv_plot.so.4.1.1 \u251c\u2500\u2500 libopencv_quality.so -&gt; libopencv_quality.so.4.1 \u251c\u2500\u2500 libopencv_quality.so.4.1 -&gt; libopencv_quality.so.4.1.1 \u251c\u2500\u2500 libopencv_quality.so.4.1.1 \u251c\u2500\u2500 libopencv_reg.so -&gt; libopencv_reg.so.4.1 \u251c\u2500\u2500 libopencv_reg.so.4.1 -&gt; libopencv_reg.so.4.1.1 \u251c\u2500\u2500 libopencv_reg.so.4.1.1 \u251c\u2500\u2500 libopencv_rgbd.so -&gt; libopencv_rgbd.so.4.1 \u251c\u2500\u2500 libopencv_rgbd.so.4.1 -&gt; libopencv_rgbd.so.4.1.1 \u251c\u2500\u2500 libopencv_rgbd.so.4.1.1 \u251c\u2500\u2500 libopencv_saliency.so -&gt; libopencv_saliency.so.4.1 \u251c\u2500\u2500 libopencv_saliency.so.4.1 -&gt; libopencv_saliency.so.4.1.1 \u251c\u2500\u2500 libopencv_saliency.so.4.1.1 \u251c\u2500\u2500 libopencv_shape.so -&gt; libopencv_shape.so.4.1 \u251c\u2500\u2500 libopencv_shape.so.3.4 -&gt; libopencv_shape.so.3.4.0 \u251c\u2500\u2500 libopencv_shape.so.3.4.0 \u251c\u2500\u2500 libopencv_shape.so.4.1 -&gt; libopencv_shape.so.4.1.1 \u251c\u2500\u2500 libopencv_shape.so.4.1.1 \u251c\u2500\u2500 libopencv_stereo.so -&gt; libopencv_stereo.so.4.1 \u251c\u2500\u2500 libopencv_stereo.so.4.1 -&gt; libopencv_stereo.so.4.1.1 \u251c\u2500\u2500 libopencv_stereo.so.4.1.1 \u251c\u2500\u2500 libopencv_stitching.so -&gt; libopencv_stitching.so.4.1 \u251c\u2500\u2500 libopencv_stitching.so.3.4 -&gt; libopencv_stitching.so.3.4.0 \u251c\u2500\u2500 libopencv_stitching.so.3.4.0 \u251c\u2500\u2500 libopencv_stitching.so.4.1 -&gt; libopencv_stitching.so.4.1.1 \u251c\u2500\u2500 libopencv_stitching.so.4.1.1 \u251c\u2500\u2500 libopencv_structured_light.so -&gt; libopencv_structured_light.so.4.1 \u251c\u2500\u2500 libopencv_structured_light.so.4.1 -&gt; libopencv_structured_light.so.4.1.1 \u251c\u2500\u2500 libopencv_structured_light.so.4.1.1 \u251c\u2500\u2500 libopencv_superres.so -&gt; libopencv_superres.so.4.1 \u251c\u2500\u2500 libopencv_superres.so.3.4 -&gt; libopencv_superres.so.3.4.0 \u251c\u2500\u2500 libopencv_superres.so.3.4.0 \u251c\u2500\u2500 libopencv_superres.so.4.1 -&gt; libopencv_superres.so.4.1.1 \u251c\u2500\u2500 libopencv_superres.so.4.1.1 \u251c\u2500\u2500 libopencv_surface_matching.so -&gt; libopencv_surface_matching.so.4.1 \u251c\u2500\u2500 libopencv_surface_matching.so.4.1 -&gt; libopencv_surface_matching.so.4.1.1 \u251c\u2500\u2500 libopencv_surface_matching.so.4.1.1 \u251c\u2500\u2500 libopencv_text.so -&gt; libopencv_text.so.4.1 \u251c\u2500\u2500 libopencv_text.so.4.1 -&gt; libopencv_text.so.4.1.1 \u251c\u2500\u2500 libopencv_text.so.4.1.1 \u251c\u2500\u2500 libopencv_tracking.so -&gt; libopencv_tracking.so.4.1 \u251c\u2500\u2500 libopencv_tracking.so.4.1 -&gt; libopencv_tracking.so.4.1.1 \u251c\u2500\u2500 libopencv_tracking.so.4.1.1 \u251c\u2500\u2500 libopencv_videoio.so -&gt; libopencv_videoio.so.4.1 \u251c\u2500\u2500 libopencv_videoio.so.3.4 -&gt; libopencv_videoio.so.3.4.0 \u251c\u2500\u2500 libopencv_videoio.so.3.4.0 \u251c\u2500\u2500 libopencv_videoio.so.4.1 -&gt; libopencv_videoio.so.4.1.1 \u251c\u2500\u2500 libopencv_videoio.so.4.1.1 \u251c\u2500\u2500 libopencv_video.so -&gt; libopencv_video.so.4.1 \u251c\u2500\u2500 libopencv_video.so.3.4 -&gt; libopencv_video.so.3.4.0 \u251c\u2500\u2500 libopencv_video.so.3.4.0 \u251c\u2500\u2500 libopencv_video.so.4.1 -&gt; libopencv_video.so.4.1.1 \u251c\u2500\u2500 libopencv_video.so.4.1.1 \u251c\u2500\u2500 libopencv_videostab.so -&gt; libopencv_videostab.so.4.1 \u251c\u2500\u2500 libopencv_videostab.so.3.4 -&gt; libopencv_videostab.so.3.4.0 \u251c\u2500\u2500 libopencv_videostab.so.3.4.0 \u251c\u2500\u2500 libopencv_videostab.so.4.1 -&gt; libopencv_videostab.so.4.1.1 \u251c\u2500\u2500 libopencv_videostab.so.4.1.1 \u251c\u2500\u2500 libopencv_xfeatures2d.so -&gt; libopencv_xfeatures2d.so.4.1 \u251c\u2500\u2500 libopencv_xfeatures2d.so.4.1 -&gt; libopencv_xfeatures2d.so.4.1.1 \u251c\u2500\u2500 libopencv_xfeatures2d.so.4.1.1 \u251c\u2500\u2500 libopencv_ximgproc.so -&gt; libopencv_ximgproc.so.4.1 \u251c\u2500\u2500 libopencv_ximgproc.so.4.1 -&gt; libopencv_ximgproc.so.4.1.1 \u251c\u2500\u2500 libopencv_ximgproc.so.4.1.1 \u251c\u2500\u2500 libopencv_xobjdetect.so -&gt; libopencv_xobjdetect.so.4.1 \u251c\u2500\u2500 libopencv_xobjdetect.so.4.1 -&gt; libopencv_xobjdetect.so.4.1.1 \u251c\u2500\u2500 libopencv_xobjdetect.so.4.1.1 \u251c\u2500\u2500 libopencv_xphoto.so -&gt; libopencv_xphoto.so.4.1 \u251c\u2500\u2500 libopencv_xphoto.so.4.1 -&gt; libopencv_xphoto.so.4.1.1 \u251c\u2500\u2500 libopencv_xphoto.so.4.1.1",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to save images from the Raspi cameras connected to my Jetson nano. My Code is below. However, the code shows that it is saving the files, but no matter which method I try I cannot find the images. Thanks for your help. I've included a smaller snippet of just the while loop itself so it will be easier for you all to refer to. While loop: while True: _ , left_image=left_camera.read() _ , right_image=right_camera.read() camera_images = np.hstack((left_image, right_image)) cv2.imshow(\"CSI Cameras\", camera_images) t1 = datetime.now() cntdwn_timer = countdown - int ((t1-t2).total_seconds()) # If cowntdown is zero - let's record next image if cntdwn_timer == -1: counter += 1 filename = './scenes/scene_'+ str(counter) + 'x'+'_'+ '.png' #img = cv2.imread(camera_images) #cv2.imwrite(os.path.join(os.path.expanduser('~'),'CSI-Camera', filename), camera_images) cv2.imwrite('/home/aryan/CSI-Camera/{}'.format(filename), camera_images) print (' monkey'+filename) t2 = datetime.now() time.sleep(1) cntdwn_timer = 0 # To avoid \"-1\" timer display next # This also acts as keyCode = cv2.waitKey(30) &amp; 0xFF # Stop the program on the ESC key if keyCode == 27: break left_camera.stop() left_camera.release() right_camera.stop() right_camera.release() cv2.destroyAllWindows() import cv2 import threading import numpy as np import time from datetime import datetime # gstreamer_pipeline returns a GStreamer pipeline for capturing from the CSI camera # Flip the image by setting the flip_method (most common values: 0 and 2) # display_width and display_height determine the size of each camera pane in the window on the screen left_camera = None right_camera = None #PiCam # Photo session settings total_photos = 30 # Number of images to take countdown = 5 # Interval for count-down timer, seconds font=cv2.FONT_HERSHEY_SIMPLEX # Cowntdown timer font class CSI_Camera: def __init__ (self) : # Initialize instance variables # OpenCV video capture element self.video_capture = None # The last captured image from the camera self.frame = None self.grabbed = False # The thread where the video capture runs self.read_thread = None self.read_lock = threading.Lock() self.running = False def open(self, gstreamer_pipeline_string): try: self.video_capture = cv2.VideoCapture( gstreamer_pipeline_string, cv2.CAP_GSTREAMER ) except RuntimeError: self.video_capture = None print(\"Unable to open camera\") print(\"Pipeline: \" + gstreamer_pipeline_string) return # Grab the first frame to start the video capturing self.grabbed, self.frame = self.video_capture.read() def start(self): if self.running: print('Video capturing is already running') return None # create a thread to read the camera image if self.video_capture != None: self.running=True self.read_thread = threading.Thread(target=self.updateCamera) self.read_thread.start() return self def stop(self): self.running=False self.read_thread.join() def updateCamera(self): # This is the thread to read images from the camera while self.running: try: grabbed, frame = self.video_capture.read() with self.read_lock: self.grabbed=grabbed self.frame=frame except RuntimeError: print(\"Could not read image from camera\") # FIX ME - stop and cleanup thread # Something bad happened def read(self): with self.read_lock: frame = self.frame.copy() grabbed=self.grabbed return grabbed, frame def release(self): if self.video_capture != None: self.video_capture.release() self.video_capture = None # Now kill the thread if self.read_thread != None: self.read_thread.join() # Currently there are setting frame rate on CSI Camera on Nano through gstreamer # Here we directly select sensor_mode 3 (1280x720, 59.9999 fps) def gstreamer_pipeline( sensor_id=0, sensor_mode=3, capture_width=1280, capture_height=720, display_width=1280, display_height=720, framerate=30, flip_method=0, ): return ( \"nvarguscamerasrc sensor-id=%d sensor-mode=%d ! \" \"video/x-raw(memory:NVMM), \" \"width=(int)%d, height=(int)%d, \" \"format=(string)NV12, framerate=(fraction)%d/1 ! \" \"nvvidconv flip-method=%d ! \" \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \" \"videoconvert ! \" \"video/x-raw, format=(string)BGR ! appsink\" % ( sensor_id, sensor_mode, capture_width, capture_height, framerate, flip_method, display_width, display_height, ) ) def start_cameras(): left_camera = CSI_Camera() left_camera.open( gstreamer_pipeline( sensor_id=0, sensor_mode=3, flip_method=0, display_height=540, display_width=960, ) ) left_camera.start() right_camera = CSI_Camera() right_camera.open( gstreamer_pipeline( sensor_id=1, sensor_mode=3, flip_method=0, display_height=540, display_width=960, ) ) right_camera.start() cv2.namedWindow(\"CSI-AV Cameras\", cv2.WINDOW_AUTOSIZE) if ( not left_camera.video_capture.isOpened() or not right_camera.video_capture.isOpened() ): # Cameras did not open, or no camera attached print(\"Unable to open any cameras\") # TODO: Proper Cleanup SystemExit(0) counter = 0 t2 = datetime.now() #Main stuff here while True: _ , left_image=left_camera.read() _ , right_image=right_camera.read() camera_images = np.hstack((left_image, right_image)) cv2.imshow(\"CSI Cameras\", camera_images) t1 = datetime.now() cntdwn_timer = countdown - int ((t1-t2).total_seconds()) # If cowntdown is zero - let's record next image if cntdwn_timer == -1: counter += 1 filename = './scenes/scene_'+ str(counter) + 'x'+'_'+ '.png' #img = cv2.imread(camera_images) #cv2.imwrite(os.path.join(os.path.expanduser('~'),'CSI-Camera', filename), camera_images) cv2.imwrite('/home/aryan/CSI-Camera/{}'.format(filename), camera_images) print (' monkey'+filename) t2 = datetime.now() time.sleep(1) cntdwn_timer = 0 # To avoid \"-1\" timer display next # This also acts as keyCode = cv2.waitKey(30) &amp; 0xFF # Stop the program on the ESC key if keyCode == 27: break left_camera.stop() left_camera.release() right_camera.stop() right_camera.release() cv2.destroyAllWindows() if __name__ == \"__main__\": start_cameras()",
        "answers": [
            [
                "Put breakpoint on line where you want to save image. Inspect image you want to save: Does it have data inside? Does it have camera_images.shape ? What is return value of cv2.imwrite function? Does path you are trying to write really exist? Did you appended .png or .jpg ?"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "This is a question about Docker networking and running an OpenGL GUI application within a docker container. This example shows that to get this particular OpenGL example to run in a Docker container that the --net=host option must be provided. Certainly, the example doesn't work correctly without that option. My question is, if I don't want to specify --net=host because I want it to be part of a different network, then is there a way to specify some other options when creating the container that will cherry-pick the exact network changes or port mappings required to get it to run? I don't fully understand why it needs the --net=host option. (I think it has something to do with the X11 server??) Previously (before my container was using OpenGL) there was a mynetwork network that my container was using to speak to other docker containers on the system. And so it was being launched with --net=mynetwork. But now, specifying --net=host instead, gets the new OpenGL features working, but breaks the features that required communication with the other docker containers on the mynetwork network. I seek to better understand why OpenGL requires --net=host and if there's something else I can do besides --net=host to correctly enable whatever the container actually needs with respect to host networking so that I can both 1) communicate with my other containers on the mynetwork network, and 2) use the OpenGL features. Maybe there's a way involving setting DISPLAY and/or XAUTHORITY to some values that will let the container connect to the host correctly? Possibly there's an additional step involved on the host to permit this (beyond just xhost +)? For completeness, here is the aforementioned example (intended to be run on an NVIDIA Jetson Nano). Seeking an alternative to --net=host is the relevant part of my question. $ xhost + $ sudo docker run -it --rm --net=host --runtime nvidia -e DISPLAY=$DISPLAY -v /tmp/.X11-unix/:/tmp/.X11-unix nvcr.io/nvidia/l4t-base:r32.3.1 $ apt-get update &amp;&amp; apt-get install -y --no-install-recommends make g++ $ /usr/local/cuda-10.0/bin/cuda-install-samples-10.0.sh /tmp $ cd /tmp/NVIDIA_CUDA-10.0_Samples/2_Graphics/simpleGL $ make $ ./simpleGL",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have trained an object detection model to be used in production for real-time applications. I have the following two options. Can anyone suggest what is the best way to run inference on Jetson Xavier for best performance? Any other suggestions are also welcome. Convert the model to ONXX format and use with TensorRT Save the model as Torchscript and run inference in C++",
        "answers": [
            [
                "On Jetson hardware, my experience is that using TensorRT is definitely faster. You can convert ONNX models to TensorRT using the ONNXParser from NVIDIA. For optimal performance you can choose to use mixed precision. How to convert ONNX to TensorRT is explained here: TensorRT. Section 3.2.5 for python bindings and Section 2.2.5 for the C++ bindings."
            ],
            [
                "I don't have any experience in Jetson Xavier, but in Jetson Nano TensorRT is a little bit faster than ONNX or pytorch. TorchScript does no make any difference from pyTorch."
            ]
        ],
        "votes": [
            4.0000001,
            1e-07
        ]
    },
    {
        "question": "Platform: Jetson Nano B01, OS: Ubuntu 18.04, Camera module: Raspi cam v2.1 IMX219 (CSI interface) Problem overview: My team is developing a machine vision application that requires recording video at high fps (&gt;=120hz) and doing live inference on the same video at low fps (~2hz). Is there a Gstreamer element we could use that could pull out a frame from the pipeline at set intervals and save it to disk? Current Gstreamer pipeline: gst-launch-1.0 nvarguscamerasrc num-buffers=-1 gainrange=\"1 1\" ispdigitalgainrange=\"2 2\" ! 'video/x-raw(memory:NVMM),width=1280, height=720, framerate=120/1, format=NV12' ! omxh264enc ! qtmux ! filesink location=test1.mp4 -e Additional info: The idea is that we have a function looping continuously checking for a new image file at a specific location, and when it detects a new image file it will send this to the neural net for inference and delete the image file. We were able to achieve moderate success at this task using a multi-threaded approach to recording with OpenCV on x86 machines, but the Jetson Nano doesn't have enough cpu power to meet our needs with OpenCV, afaik. The pipeline provided above is able to record videos that meet our required specs, but does not save images to be used for inference.",
        "answers": [
            [
                "when it detects a new image file it will send this to the neural net for inference and delete the image file. You do not need to save image files and run the two parts in different processes for the saved image files. If you want to keep the current structure, try \"videorate\" and \"appsink\" elements. E.g., instead of filesink and read-file from the app, inside the app, ... ! videorate ... ! appsink And then, receive the incoming data directly in the app. Or, if you want to do \"inferences\" with conventional neural network frameworks (e.g., tensorflow, caffe2, openVINO, and so on) and get the final resuls from your app: You can merge the whole procedure into a single GStreamer pipeline along with videorate. \"tensor-filter\" GStreamer filter (https://github.com/nnstreamer/nnstreamer) allows you to apply conventional AI frameworks inside a GStreamer pipeline directly. E.g., ... ! videorate ... ! video/x-raw,... ! tensor-converter ! tensor-transform (if you need some arithmetic operations, transpose, normalization, or something else for your neural network) ! tensor-filter framework=openvino model=PATH_TO_YOUR_MODELFILE ! ... (do whatever you want) If you want to separate threads, you may add \"queue\" accordingly."
            ],
            [
                "It is hard to say at what stage in the pipeline you want this to happen. But I'd say take a look at the videorate element. That element can drop frames to match downstream framerate caps. https://gstreamer.freedesktop.org/documentation/videorate/index.html?gi-language=c"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am trying to use CuPy to accelerate python functions that are currently mostly using NumPy. I have installed CuPy on the Jetson AGX Xavier with CUDA 10.0 installed. The CuPy functions seem to be working fine, however, they are a lot slower than their NumPy counterparts. For example, I ran the first example from here with devastating results: import numpy as np import cupy as cp import time ### Numpy and CPU s = time.time() x_cpu = np.ones((1000,1000,1000)) e = time.time() print(e - s) # output: 0.9008722305297852 ### CuPy and GPU s = time.time() x_gpu = cp.ones((1000,1000,1000)) cp.cuda.Stream.null.synchronize() e = time.time() print(e - s) # output: 4.973184823989868 I also ran other functions (e.g. np./cp.nonzero), but they gave similar or worse results. How is this possible? I want to do image processing (ca. size 2500x2000 greyscale/mono images) for a lane detection algorithm and cannot really use the cuda functions from OpenCV for this, since the only part in my code that is implemented in their library is cv2.cuda.warpPerspective() (and it would likely not make too much sense to upload/download the image to the GPU only for this). Where do I go from this? Use numba? (-&gt; probably not a good fit, since (the compute-intensive parts of) my algorithm mostly consists of numpy function calls) Implement the whole thing in C++? (-&gt; I doubt my C++ code would be faster than the optimized NumPy functions) Sidenote: CuPy was installed using pip3 install cupy because the recommended pip3 install cupy-cuda100 failed with the output: ERROR: Could not find a version that satisfies the requirement cupy-cuda100 ERROR: No matching distribution found for cupy-cuda100",
        "answers": [
            [
                "First : No official cupy for ARM Your error comes from the fact there is no binary distribution for cupy for arm 64 in official pip repository. Wheels (precompiled binary packages) are available for Linux (x86_64) and Windows (amd64). For Nvidia L4T / Jetpack, you can find an official NVIDIA docker image including Cupy that runs on Xavier there : https://ngc.nvidia.com/catalog/containers/nvidia:l4t-ml. That works for me and did increase my performances. If you have a solution to run Cupy on Xavier using effectively CUDA without running that docker image. I'm interested in. I didn't try but if they managed to compile Cupy in the docker image, it's obvious it is also possible in a fresh native OS. Here some success on Nano : https://forums.developer.nvidia.com/t/cupy-installation-on-the-nano/189099 Try an install from sources : https://docs.cupy.dev/en/stable/install.html Second : No computation in your test Is your test really revelant for measuring acceleration ? I mean, the first call to CUDA functions is expected to be slow because of kind of Just In Time kernel compilation or whatever and you only test some allocation stuff and minor parallel computation. Update 10/03/21 Here the instructions they run in docker to construct image : # # CuPy # ARG CUPY_VERSION=v9.2.0 ARG CUPY_NVCC_GENERATE_CODE=\"arch=compute_53,code=sm_53;arch=compute_62,code=sm_62;arch=compute_72,code=sm_72\" RUN git clone -b ${CUPY_VERSION} --recursive https://github.com/cupy/cupy cupy &amp;&amp; \\ cd cupy &amp;&amp; \\ pip3 install --no-cache-dir fastrlock &amp;&amp; \\ python3 setup.py install --verbose &amp;&amp; \\ cd ../ &amp;&amp; \\ rm -rf cupy You can easily execute that in native environment. Maybe adapt the CUPY_NVCC_GENERATE_CODE variable for better performances depending on your board. I didn't set anything specific when I tried. Just tried on my Nano board with from source install (git clone cupy then pip3 install --no-cache-dir -vvvv . (no sudo) then reboot board). Treating images with several sum, substract, divide, multiply, clip, ... operations on VGA images. Don't forget to use sudo jetson_clocks command to set fixed high frequency operations on your board. Numpy cpu time = 125ms / img vs Cupy time = 13ms /img after some rework on the code using NVIDIA profiler. Use nvprof -o file.out python3 mycupyscript.py with with cp.cuda.profile(): instruction in to understand better bottlenecks. Use nvvp to load file.out and explore graphically the performances. It will let you upgrade your computation approach to fit well with GPU."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a software which I packaged in x86 processors using pyinstaller and it packages all libraries including cv2, however, when I try to package the same software in Jetson TX2, it doesn't package cv2 and throws error on executing the binary file: OpenCV loader: missing configuration file: ['config.py'] The reason is cv2 comes preinstalled in TX2 at a different location (/usr/lib/python3.6/dist-packages). However, rest of the libraries which we self-installed are in (/home/mnauf/.local/lib/python3.6/site-packages) and maybe this is why pyinstaller fails to package it. The pyinstaller tries to find cv2 at /home/mnauf/.local/lib/python3.6/site-packages and doesn't find there and correspondingly doesn't package, however cv2 imports fine if you do it with python. The reason why cv2 works fine with python is that I suppose python first tries to find a library in /home/mnauf/.local/lib/python3.6/site-packages and if unsuccessful, finds in /usr/lib/python3.6/dist-packages. To solve the packaging problem, I tried the following methodologies and the errors discussed below all come when executing the binary file and don't come at the time of packaging: Copying cv2 from /usr/lib/python3.6/dist-packages to /home/mnauf/.local/lib/python3.6/site-packages. It gives: ImportError: ERROR: recursion is detected during loading of \"cv2\" binary extensions. Check OpenCV installation. I try to copy cv2 directory from /usr/lib/python3.6/dist-packages to dist/main folder created by pyinstaller after packaging but I get the same Import error. Adding the cv2 directory path as a data file in main.spec also only copies the folder to dist/main and hence gives the same Import error. Adding the cv2.cpython-36m-aarch64-linux-gnu.so path only as a data file in main.spec gives Opencv loader error. Adding the cv2 directory as a binary file path in main.spec gives the Import error. Adding the cv2.cpython-36m-aarch64-linux-gnu.so path only as a binary file in main.spec gives the Opencv loader error. Please help me with packing cv2. Thanks",
        "answers": [
            [
                "I copied /usr/lib/python3.6/dist-packages/cv2/python-3.6/cv2.cpython-36m-aarch64-linux-gnu.so to /home/mnauf/.local/lib/python3.6/site-packages/ and then imported cv2 from python and checked its location where is it importing from by doing this: import cv2 cv2.__file__ and it returned me the path I wanted i.e /home/mnauf/.local/lib/python3.6/site-packages/. Once confident that cv2 is indeed using the directory I want, I ran pyinstaller and did package the cv2 dependency. Previously, I had copied the entire folder before creating its executables. This time I only copied the .so file, before creating executables and it worked. And I think \"before creating executables\" is also the trick. You can't just copy the .so file to dist/main and expect it to work. Also, we concluded that giving .so file path as data file or binary file in main.spec doesn't work."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I want to connect some push buttons to the gpio of my jetson nano for communicating with a javascript electron.js application. There is the NVIDIA jetson-gpio python library but I have without success trying to find a javascript equivalent for communicating with the gpio through node.js ... A workaround would be to handle the input from gpio with python and then setup communication from python to javascript through websockets but a library for communicating directly with the gpio would be more straightforward ... Anyone with a suggestion for possible solution for handle this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have tried installing it using pip3 but it seems like there is no available version for aarch64 architecture, are there any other ways I can install it? Also if I were to clone the code here, https://github.com/skvark/opencv-python/releases/tag/26 and compile it, may I know what are the steps for it? (Sorry for asking, I'm still fairly new to these stuff) Thanks!",
        "answers": [
            [
                "Could you share the error you get while trying to install it? you can also try the below: pip3 install scikit-build pip3 install opencv-python==4.1.1.26 Regards,"
            ]
        ],
        "votes": [
            -0.9999999
        ]
    },
    {
        "question": "Im working on this project : https://www.hackster.io/jonmendenhall/jetson-nano-search-and-rescue-ai-uav-9ca547 At some point I will need to mount my camera (waveshare ; IMX219-77IR) on top of the drone and I would like to use vlc on Windows or Linux outside of nomachine (because I have installed nomachine server on the nano and the client on windows and because it will run in headless mode),to display what the camera sees when the drone is flying. For this reason I\u2019m trying to configure a gstreamer with RTSP to start a streaming server on the Ubuntu 18.04 that I have installed on the jetson nano. Below u can see what are the commands that I have issued : $ ./test-launch \"videotestsrc ! nvvidconv ! nvv4l2h264enc ! h264parse ! rtph264pay name=pay0 pt=96\" And on the same Jetson Nano, I opened another console where I ran this pipeline to decode the RTSP stream: gst-launch-1.0 uridecodebin uri=rtsp://127.0.0.1:8554/test ! nvoverlaysink I see this picture : The picture is from videotestsrc plugin. I would like to replace videotestsrc with my video source,but I don't know how to do that. I tried these combinations,but none of them worked : ./test-launch \"v4l2src device=/dev/video0 ! nvvidconv ! nvv4l2h264enc ! h264parse ! queue ! rtph264pay name=pay0 pt=96\" ./test-launch \"device=/dev/video0 ! nvvidconv ! nvv4l2h264enc ! h264parse ! queue ! rtph264pay name=pay0 pt=96\" but the error is still the same : Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... Progress: (open) Opening Stream Progress: (connect) Connecting to rtsp://127.0.0.1:8554/test Progress: (open) Retrieving server options Progress: (open) Retrieving media info ERROR: from element /GstPlayBin:playbin0/GstURIDecodeBin:uridecodebin0/GstRTSPSrc:source: Could not read from resource. Additional debug info: gstrtspsrc.c(5917): gst_rtsp_src_receive_response (): /GstPlayBin:playbin0/GstURIDecodeBin:uridecodebin0/GstRTSPSrc:source: Could not receive message. (Timeout while waiting for server response) ERROR: pipeline doesn't want to preroll. Setting pipeline to PAUSED ... Setting pipeline to READY ... Setting pipeline to NULL ... Freeing pipeline ... but why ? I know for sure that my camera (model waveshare ; IMX219-77IR) created a device called /dev/video0 and I know for sure that it works,because this command is able to show my face on the screen : DISPLAY=:0.0 gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM), width=3280, height=2464, format=(string)NV12, framerate=(fraction)20/1' ! nvoverlaysink -e",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have Jetson Nano and I would like to save a video from my usb-camera. I had a problems with MJPG output from camera -it was outputting YUYV not MJPG, but I was able to solve it with io-mode=2 settings for pipeline This working in my Opencv (4.5.1) cap = cv2.VideoCapture ('v4l2src device=/dev/video0 io-mode=2 ! image/jpeg, width=(int)1920, height=(int)1080, framerate=30/1 ! nvv4l2decoder mjpeg=1 ! nvvidconv ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink', cv2.CAP_GSTREAMER) fourcc = cv2.VideoWriter_fourcc(*'MPEG') width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) fps = cap.get(cv2.CAP_PROP_FPS) print(\"fourcc:{} fps:{}@width:{}@height:{}\".format(fourcc, fps, width, height)) file_name = \"abc.avi\" out = cv2.VideoWriter(file_name, fourcc, 30, (1920, 1088)) counter = 0 while True: if counter &gt; 200: break counter +=1 _, frame = cap.read() if(frame is None): continue cv2.imshow('frame', frame) out.write(frame) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break cap.release() cv2.destroyAllWindows() But the video is too laggy, because of that converts?, it prints 30fps but it saves like 4 frames for 10 sec video. And it also getting 1088 from CAP_PROP_FRAME_HEIGHT, idk why... I found this pipeline works without lags in terminal $ export DISPLAY=:0 (or DISPLAY=:1) $ gst-launch-1.0 v4l2src device=/dev/video0 io-mode=2 ! image/jpeg, width=(int)1920, height=(int)1080 ! nvjpegdec ! video/x-raw ! xvimagesink But I'm not able to use it in opencv It outputting empty pipeline like [ WARN:0] global /home/suomi/opencv/modules/videoio/src/cap_gstreamer.cpp (824) open OpenCV | GStreamer warning: cannot find appsink in manual pipeline [ WARN:0] global /home/suomi/opencv/modules/videoio/src/cap_gstreamer.cpp (501) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created fourcc:1195724877 fps:0.0@width:0.0@height:0.0 Also I tried this one, it works in terminal, but shows black frames when I'm using opencv like this 'v4l2src device=/dev/video0 io-mode=2 ! image/jpeg, width=(int)1920, height=(int)1080 ! nvjpegdec ! video/x-raw ! videoconvert ! video/x-raw,format=BGR ! appsink', cv2.CAP_GSTREAMER My camera settings are here ioctl: VIDIOC_ENUM_FMT Index : 0 Type : Video Capture Pixel Format: 'MJPG' (compressed) Name : Motion-JPEG Size: Discrete 1920x1080 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 1280x720 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 800x480 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 640x480 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 640x360 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 320x240 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 176x144 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 800x600 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 1920x1080 Interval: Discrete 0.033s (30.000 fps) Index : 1 Type : Video Capture Pixel Format: 'YUYV' Name : YUYV 4:2:2 Size: Discrete 640x480 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 640x360 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 320x240 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 176x144 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 640x480 Interval: Discrete 0.033s (30.000 fps)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using Nvidia Jetson Tx2 device. With the following command, I can connect and capture an image with ffmpeg. $/usr/bin/ffmpeg -y -frames 1 snapshot.png -rtsp_transport tcp -i rtsp://admin:admin@192.168.10.131/1/profile ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers built with gcc 7 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/aarch64-linux-gnu --incdir=/usr/include/aarch64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared libavutil 55. 78.100 / 55. 78.100 libavcodec 57.107.100 / 57.107.100 libavformat 57. 83.100 / 57. 83.100 libavdevice 57. 10.100 / 57. 10.100 libavfilter 6.107.100 / 6.107.100 libavresample 3. 7. 0 / 3. 7. 0 libswscale 4. 8.100 / 4. 8.100 libswresample 2. 9.100 / 2. 9.100 libpostproc 54. 7.100 / 54. 7.100 Guessed Channel Layout for Input Stream #0.1 : mono Input #0, rtsp, from 'rtsp://admin:admin@192.168.10.131/1/profile': Metadata: title : SDP Descrption comment : SDP Description Duration: N/A, start: 0.000000, bitrate: N/A Stream #0:0: Video: h264 (High), yuvj420p(pc, bt709, progressive), 1920x1080, 25 fps, 30 tbr, 90k tbn, 50 tbc Stream #0:1: Audio: pcm_alaw, 8000 Hz, mono, s16, 64 kb/s Stream #0:2: Data: none Stream mapping: Stream #0:0 -&gt; #0:0 (h264 (native) -&gt; png (native)) Press [q] to stop, [?] for help [swscaler @ 0x55a8f70c70] deprecated pixel format used, make sure you did set range correctly [swscaler @ 0x55a8f70c70] No accelerated colorspace conversion found from yuv420p to rgb24. Output #0, image2, to 'snapshot.png': Metadata: title : SDP Descrption comment : SDP Description encoder : Lavf57.83.100 Stream #0:0: Video: png, rgb24, 1920x1080, q=2-31, 200 kb/s, 30 fps, 30 tbn, 30 tbc Metadata: encoder : Lavc57.107.100 png frame= 1 fps=0.0 q=-0.0 Lsize=N/A time=00:00:00.03 bitrate=N/A dup=1 drop=1 speed=0.066x video:1982kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown But with gstreamer, (I'm using version 1.14.5) I am not able to access the rtsp feed. $gst-launch-1.0 uridecodebin uri=rtsp://admin:admin@192.168.10.131/1/profile ! fakesink Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... Progress: (open) Opening Stream Progress: (connect) Connecting to rtsp://admin:admin@192.168.10.131/1/profile Progress: (open) Retrieving server options Progress: (open) Retrieving media info Progress: (request) SETUP stream 0 Progress: (request) SETUP stream 1 Progress: (request) SETUP stream 2 ERROR: from element /GstPipeline:pipeline0/GstURIDecodeBin:uridecodebin0/GstRTSPSrc:source: Could not read from resource. Additional debug info: gstrtspsrc.c(5917): gst_rtsp_src_receive_response (): /GstPipeline:pipeline0/GstURIDecodeBin:uridecodebin0/GstRTSPSrc:source: Could not receive message. (Timeout while waiting for server response) ERROR: pipeline doesn't want to preroll. Setting pipeline to PAUSED ... Setting pipeline to READY ... Setting pipeline to NULL ... Freeing pipeline ... I have also removed gstreamer-ugly pkg but it still does not work! ref : https://forums.developer.nvidia.com/t/rtsp-gstreamer-simple-recieve-and-store-in-file/157535/12 Let me know if you can help me! Thanks!",
        "answers": [
            [
                "I have solved this by the help of Gstreamer forum. The problem was that ffmpeg is using tcp when gstreamer is using udp and tcp, and I believe in stream 2, udp is failing. The trick was to put a t to specify tcp $gst-launch-1.0 rtspsrc location=rtspt://admin:admin@192.168.10.131/1/profile ! fakesink Also, you can see the debug lines by gst_debug $gst-launch-1.0 --gst-debug=rtspsrc:5 uridecodebin uri=rtsp://admin:admin@192.168.10.131/1/profile ! fakesink This is the link. http://gstreamer-devel.966125.n4.nabble.com/Ffmpeg-works-but-gstreamer-does-not-work-for-rtsp-camera-tt4696330.html Also, there is no answer but may helpful in nvidia dev forum https://forums.developer.nvidia.com/t/ffmpeg-works-but-gstreamer-does-not-work-for-rtsp-camera/165193/5"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm working with AI-Thermometer project using Nvidia Jeton Nano. The project is using Pi camera v2 for video capturing. Here's the command of showing video streams using Pi camera v2. gst-launch-1.0 nvarguscamerasrc sensor_mode=0 ! 'video/x-raw(memory:NVMM),width=3264, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=2 ! 'video/x-raw,width=960, height=720' ! nvvidconv ! nvegltransform ! nveglglessink -e I want to use the normal USB webcam (such as Logitech c930) instead of Pi camera v2. To do so, I need to stream the USB webcam data using GStreamer in the same way as above pipeline commands. I installed v4l-utils on Ubuntu of Jetson Nano. And tried like this, gst-launch-1.0 v4l2src device=\"/dev/video0\" ! 'video/x-raw(memory:NVMM),width= ... , but it gave a warning and didn't work. How can I show video streams from webcam?",
        "answers": [
            [
                "There should not quotes around the device parameter i.e. device=/dev/video0. If the error persists, then its probably something else."
            ],
            [
                "gst-launch-1.0 v4l2src device=\"/dev/video0\" ! \\ \"video/x-raw, width=640, height=480, format=(string)YUY2\" ! \\ xvimagesink -e"
            ]
        ],
        "votes": [
            1e-07,
            -0.9999999
        ]
    },
    {
        "question": "I've been reading a lot about Jetson Nano and Google Coral Devboard and in most documentation and papers i've read, the inferencing and deployment are done using prebuilt convolutional neural networks such as AlexNet, Inception, MobileNet and other neural networks used for image classification. From what i understand these microcomputers require that the neural network is converted to a tensorflow model or any framework they accept to perform inferencing of the model. What i would like to know is: for both Jetson Nano and Google Coral Devboard, can i have my own convolutional neural network that has nothing to do with those convolutional neural networks examplified in the documentation and deploy them to those boards?",
        "answers": [
            [
                "Yes. You can train your own convolutional neural network, even outside Jetson Nano, and save the weights (matrices of floats) inside your Jetson Nano to do the inference. So, inside Jetson Nano, you will only do matrix multiplication to classify whatever you want. Of course, you'll have to duplicate your real model inside the device so that you can use the saved weights to do inference."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Im building an application intended to run in producion at a museum on jetson nano. First my idea was to run it on the developer kit but I read that it might be a bad idea since it is not intended for production. But Im still wondering if I can use the developer board? Or will it not survive everyday use? The module comes without carrier board so in the case I am using that one I need to build something myself or buy some addon for connecting usb hdmi etc. So that makes the developer kit more convenient. I also need the GPIO which Im not sure you can access on the module? Has anyone experience of running jetson developer kit on an everydaybasis? Did you experience any problems?",
        "answers": [
            [
                "I have had experience with a burnt-out Nano Developer Kit after daily usage. However, I have had other Nano Developer Kits which still work fine. So I don't think you can conclude anything from that. The reason why the Jetson Module is meant for commercial purposes is that it comes with a warranty and is meant to be integrated with Industry-Grade Components. You decide based on your affordability to replace the Developer Kit in case something happens. Regarding the GPIO, I am not sure if you can conclude that. They are several companies listed on Nivida's website that build customized carrier boards. Decide based on your affordability."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am running YOLO v3 on Jetson Xavier, I am using python and OpenCV DNN module for inferencing, and it is pretty slow and seems as if it is not effective for my application. So I am looking for another detection model that can run in a real-time manner on Xaviar. It has to have reasonable accuracy, so I do not consider running YOLO-tiny. Any suggestion Please? What model is capable of running real-time in a Xaviar device?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to execute a retrained PyTorch FasterRCNN in multiple threads on an Nvidia Jetson Xavier. The main threads add the image path to a Queue. Four worker threads are doing the following things: loading the image with PIL img = Image.open(imgPath) transform it into a tensor by img = to_tensor(img) from tourchvision.transforms put it to GPU img = img.to(device) execute the RCNN Network pred = model([img]) save the results in a normal list resultList.append(pred) delete the variable holding the image with del img However, the process runs out of memory after around 10.000 images and get killed by the operating system. I tried to do the following steps after 1000 images: stop all threads do garbage collection by gc.collect() clear GPU Memory by torch.cuda.empty_cache() restart threads However, as expected, it does not solve the problem. I know there is the DataLoader of PyTorch to do multithreading. Since I use the RCNN in a larger project I tried it without the DataLoader within the execution task. I'm pretty sure there is no list that stores images, since then, the memory would run out faster. The results of the network are just bounding boxes. Therefore, they also should not consume so much memory. Additionally, the memory consumption is not growing slow, instead its jumps sometimes by around 1GB. I hope someone have an idea for solving the problem or how to better debug. Thanks, Peter",
        "answers": [],
        "votes": []
    },
    {
        "question": "My problem: I'm trying to save a Jetson TX2 docker image that's about 8GB. The OS and other files+OS are taking up around 21GB meaning I only have about 3GB of storage on the machine. I have already pruned old images and containers, and cannot delete or free up any more space. Because of this I've tried saving the docker image to an USB stick. However, when I try to build the docker file I get the error: *Error response from daemon: write /var/lib/docker/tmp/docker-export-50xxxxx/layer.tar: no space left on device* Even though I am trying to save the docker image to an USB with enough space, Docker seems to build the whole file first using the /var/lib/docker/tmp/ folder and then wants to move it to the USB. However there simply isn't enough space on the device. Because of this I keep getting the error listed above. My question: Is there any way to save a docker image without using my main drive as intermediary storage? I can ssh into the Jetson and have an USB/SD card with sufficient capacity. If there is another solution that would solve my problem that would be great to hear too.",
        "answers": [
            [
                "Mount the USB drive on the system and then use the -o flag of docker export to export the image. So with an example USB drive mounted on /mnt/usb, use .... docker export -o /mnt/usb &lt;CONTAINER ID&gt;"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm currently tuning a pair of gstreamer pipelines involving a Jetson TX2 sending and receiving audio and video. I'm trying to track down what's causing graphics artifacts on the stream coming from the TX2 where a camera streaming h264 (natively generating the h264 stream, the TX2 is only doing the payloading and sending) is capturing and streaming via rtp. Performance on this transmission stream is acceptable until the video receive pipeline is brought on line, at that point the transmission stream's video quality drops precipitously. I've set up a dynamic gstreamer pipeline on the receiving desktop that logs debug statements and utilizes a rtpjitterbuffer with 0 latency to monitor stats on the stream the TX2 is transmitting. Conventional gstreamer wisdom is that adding a queue and/or a rtpjitterbuffer to the receiving desktop pipeline should ameliorate some of the issues. Adding and removing queues and jitterbuffers with various settings results in an average of about 100 packets lost per 100k packets pushed over 12 different tests. Needless to say video quality doesn't improve since without queues or jitterbuffers the average lost packets is 95 per 100k. Now, in defiance of conventional wisdom and documentation, I've also tested placing queues and jitterbuffers on the TX2 transmission side pipeline and discovered dramatic improvement. The packet loss on the receiving side is cut in half by the simple addition of a queue (52 per 100k), and when combined with a jitterbuffer (even with the latency set to 0) the packet loss drops to 0 per 100k. The question is: Why? While it's great having a solution that completely mitigates the issue I'm seeing, I also need an plausible explanation to bolster support for this solution. Does anybody have ideas? Perhaps, places to look on the TX2 side for evidence? 12/22/20 After digging quite a bit more it looks as though this problem revolves around simultaneously sending and receiving multi-media. In our system the TX2\u2019s job is to send and receive video/audio, packet loss is occurring when the encoder sending AV to the TX2 is active on the network (it\u2019s multi-casting). Reading through gstreamer debugging and wireshark logs tells the tale of packets being lost when both the AV encoder and the TX2 are bursting packets related to IDR frames (video key frames). Setting up wireshark on both ends of the Jetson\u2019s video stream shows that the missing packets are making it to the interface driver but disappear by the time they reach the recipient computer\u2019s interface. It\u2019s tempting to just write this off as network loss, however, the network is not busy and looking at the I/O traffic on the interface card reveals that it\u2019s nowhere near capacity (about 11 Mbit of a 100 Mbit connection). Still trying to figure out why a Tx side jitterbuffer fixes this packet loss problem. Per suggestion I\u2019ve increased the size of the write and read buffers on the network interface kernel settings, this did not remedy the problem. Checking with netstat -sanu does show some \u2018send buffer errors\u2019 being registered with everything running, but network monitoring with applications like bmon do not report lost packets. Currently looking to recompile the kernel so the Jetson will let me run dropwatch.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a qt program in my jetson nano on ubuntu 18.04. I am running it in single application mode. I created a session and that session runs directly my qt program. But font sizes are small according to normal ubuntu session. Why this is happening? Can't qt access system fonts in single application mode? If yes, How to make it access to fonts in my single application session?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to install JAX on the NVIDIA Jetson TX2 and I'm facing considerable issues. I have CUDA 9.0 and it gives me the following error: No library found under: /usr/local/cuda-9.0/targets/aarch64-linux/lib/libcublasLt.so.9.0 So I go looking and of course that library does not exist. Does anyone have any pointers on how I can about installing that library? I've tried searching google and it does not appear to exist at all.",
        "answers": [
            [
                "The cublasLt library did not come into existence until cuda 10.1 here is the cublas 10.0 doc and here is the cublas 10.1 doc. Therefore you won't be able to use cublasLt with CUDA 9.0 On a Jetson the correct way to get the latest CUDA install including libraries like cublas is to install the latest JetPack."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "My current goal is to make a color filter (threshold maybe) that creates a mask for regions with something like the following relations: B &lt; G G &gt; 128 R &gt; 1.5 * G I created such an algorithm already, but it loops through every pixel of the image and manually detects if these conditions are met. Which - as you might already guessed - is rather slow. On the previous machine (NUCi7) the resulting program could reach about 6-10 FPS, but it was run entirely on the CPU. Now I switched to a Jetson Xavier NX to have a look at OpenCV accelerated by the GPU. Currently I am looking for a function or several ones, that can realize these conditions in a way that is efficient. I am not trying to get the most efficient way, if that one is too complicated for a start. But I tested some Object Tracking algorithms that worked like a charm on 40+ FPS and HD2K Images, so I hope there is a way to get this done and end up with 40+/60+ FPS as well. Something like the inRange function would be great, but that seems to not yet exist for GPU-OpenCV. Maybe there is a clever way to realize it with thresholds, but so far I have just found it with static values. . https://docs.opencv.org/2.4/modules/gpu/doc/image_processing.html image.forEach&lt;Pixel&gt; ( [&amp;](Pixel &amp;pixel, const int * position) -&gt; void { if (pixel.z &gt; 1.5 * pixel.y &amp;&amp; (pixel.y &gt; pixel.x) &amp;&amp; pixel.y &gt; 128){ pixel.z = 255; pixel.y = 255; pixel.x = 255; } else { pixel.z = 0; pixel.y = 0; pixel.x = 0; } } ); The code is basically this one where I want to start from. Applying the foreach loop from above. The actual current code is this one: while (viewer.isAvailable()) { // Grab images if (zed.grab(runtime_parameters) == ERROR_CODE::SUCCESS) { // Retrieve left image zed.retrieveImage(image_zed, VIEW::LEFT, MEM::GPU,new_image_size); cv::Mat image_ocv = slMat2cvMat(image_zed); // &lt;!-- Everything here is a test to insert a custom 3D-Bounding box into the live-image sl::ObjectData newObject; sl::ObjectData refObject; std::cout &lt;&lt; \"\\n\\nNew Loop\" &lt;&lt; endl; // Retrieve Detected Human Bodies zed.retrieveObjects(objects, objectTracker_parameters_rt); int id = 42; sl::float3 newPosition = {-.1,.1,-0.5}; sl::float3 newVelocity = {.0,.0,.0}; std::vector&lt;sl::float3&gt; newBounding_box; sl::float3 pOri = {looper[0],looper[1],looper[2]}; sl::float3 pDim = {0.25,0.25,0.25}; sl::float3 p0 = {pOri[0]-pDim[0]/2,pOri[1]-pDim[1]/2,pOri[2]-pDim[2]/2}; sl::float3 p1 = {pOri[0]-pDim[0]/2,pOri[1]-pDim[1]/2,pOri[2]+pDim[2]/2}; sl::float3 p2 = {pOri[0]+pDim[0]/2,pOri[1]-pDim[1]/2,pOri[2]+pDim[2]/2}; sl::float3 p3 = {pOri[0]+pDim[0]/2,pOri[1]-pDim[1]/2,pOri[2]-pDim[2]/2}; sl::float3 p4 = {pOri[0]-pDim[0]/2,pOri[1]+pDim[1]/2,pOri[2]-pDim[2]/2}; sl::float3 p5 = {pOri[0]-pDim[0]/2,pOri[1]+pDim[1]/2,pOri[2]+pDim[2]/2}; sl::float3 p6 = {pOri[0]+pDim[0]/2,pOri[1]+pDim[1]/2,pOri[2]+pDim[2]/2}; sl::float3 p7 = {pOri[0]+pDim[0]/2,pOri[1]+pDim[1]/2,pOri[2]-pDim[2]/2}; std::vector&lt;sl::uint2&gt; newBounding_box_2d = { {250,30}, {1500,30}, {1500,500}, {250,500}}; newBounding_box.push_back(p0); newBounding_box.push_back(p1); newBounding_box.push_back(p2); newBounding_box.push_back(p3); newBounding_box.push_back(p4); newBounding_box.push_back(p5); newBounding_box.push_back(p6); newBounding_box.push_back(p7); sl::float3 newDimensions = {1.0,1.0,1.0}; newObject.id = id; newObject.position = pOri; newObject.velocity = newVelocity; newObject.bounding_box_2d = newBounding_box_2d; newObject.bounding_box = newBounding_box; newObject.dimensions = pDim; objects.object_list.push_back(newObject); // --&gt; End of the 3D-Bounding box insert /* image_ocv.forEach&lt;Pixel&gt; ( [&amp;](Pixel &amp;pixel, const int * position) -&gt; void { if (pixel.z &gt; 1.5 * pixel.y &amp;&amp; pixel.y &gt; pixel.x &amp;&amp; pixel.y &gt; 128) { pixel.z = 255; pixel.y = 255; pixel.x = 255; } else { pixel.z = 0; pixel.y = 0; pixel.x = 0; } } );*/ //Update GL View viewer.updateView(image_zed, objects); } } cv::Mat slMat2cvMat(Mat&amp; input) { // Mapping between MAT_TYPE and CV_TYPE int cv_type = -1; switch (input.getDataType()) { case MAT_TYPE::F32_C1: cv_type = CV_32FC1; break; case MAT_TYPE::F32_C2: cv_type = CV_32FC2; break; case MAT_TYPE::F32_C3: cv_type = CV_32FC3; break; case MAT_TYPE::F32_C4: cv_type = CV_32FC4; break; case MAT_TYPE::U8_C1: cv_type = CV_8UC1; break; case MAT_TYPE::U8_C2: cv_type = CV_8UC2; break; case MAT_TYPE::U8_C3: cv_type = CV_8UC3; break; case MAT_TYPE::U8_C4: cv_type = CV_8UC4; break; default: break; } // Since cv::Mat data requires a uchar* pointer, we get the uchar1 pointer from sl::Mat (getPtr&lt;T&gt;()) // cv::Mat and sl::Mat will share a single memory structure return cv::Mat(input.getHeight(), input.getWidth(), cv_type, input.getPtr&lt;sl::uchar1&gt;(MEM::GPU)); } The code as is works, but the added ForEach-Loop results in an Segmentation fault. I asked a followup question, that might be the answer to this question.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I trained a TF/Keras model (UNet architecture) with a Tesla K40. When I use it for inference with a Jetson AGX Xavier (Jetpack 4.4.1), I get very different results if the batch size is larger than 3. If not set, batch_size in model.predict() method is by default 32, and I only get the correct results if I pass just 3 inputs or less or if I pass the entire collection of my input data but specifying batch_size=3 (or less) inside the model.predict() method. Here's the code: import os import numpy as np from tensorflow import keras as K import cdUtils import cdModels from libtiff import TIFF img_size = 128 classes = 1 channels = 13 model_dir = '../models/' model_name = 'EF_bce' model = K.models.load_model(model_dir + model_name) model.summary() dataset_dir = '../imgs_pisa/' img_pre = 'pisa_pre/' img_post = 'pisa_post/' cm_name = 'pisa-cm_' + model_name res_dir = '../res_pisa/' os.makedirs(res_dir, exist_ok=True) raster_pre = cdUtils.build_raster(dataset_dir + img_pre) raster_post = cdUtils.build_raster(dataset_dir + img_post) raster = np.concatenate((raster_pre,raster_post), axis=2) padded_raster = cdUtils.pad(raster, img_size) test_image = cdUtils.crop(padded_raster, img_size, img_size) # Create inputs for the Neural Network inputs = np.asarray(test_image, dtype='float32') inputs_1 = inputs[:,:,:,:channels] inputs_2 = inputs[:,:,:,channels:] inputs = [inputs_1, inputs_2] # Perform inference results = model.predict(inputs) print('Results: ', results) print('Inference done!') I checked that the pre-procesing functions (not included in this snippet) work properly and inputs always match on each device. Could be this a memory issue, even though I get no error at runtime? Thanks.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I use Nvidia Jetson Nano with Linux for Tegra (Ubuntu 18, X11 window system), Python 3.6 and PyQt5. I want to place a transparent widget (or with a transparent background) over the main widget. When these widgets are created as independent everything is displayed correctly. Transparency works even if the gstreamer video stream is displayed in the main widget. import sys from PyQt5.QtCore import * from PyQt5.QtGui import * from PyQt5.QtWidgets import * class MainWindow(QWidget): def __init__(self): QWidget.__init__(self) self.setGeometry(50,50,320,240) self.setWindowTitle(\"Main Window\") self.setStyleSheet(\"background-color:yellow;\") self.label = QLabel(self) self.label.setText(\"Main Widget\") self.menu = MenuWidget() class MenuWidget(QWidget): def __init__(self): QWidget.__init__(self) self.setWindowFlags(Qt.Tool) self.setGeometry(100,100,100,50) self.setWindowFlags(Qt.FramelessWindowHint) self.setStyleSheet(\"background-color:gray;\") self.setWindowOpacity(0.5) self.label = QLabel(self) self.label.setText(\"Menu Widget\") app = QApplication([]) window = MainWindow() window.show() window.menu.show() sys.exit(app.exec_()) [ When I try to create a widget as a child of the main widget, transparency does not work. If a video is displayed, a \"hole\" appears in the main widget window. class MainWindow(QWidget): def __init__(self): QWidget.__init__(self) self.setGeometry(50,50,320,240) self.setWindowTitle(\"Main Window\") self.setStyleSheet(\"background-color:yellow;\") self.label = QLabel(self) self.label.setText(\"Main Widget\") self.menu = MenuWidget(parent=self) print('main window created') class MenuWidget(QWidget): def __init__(self, parent): QWidget.__init__(self, parent) self.setWindowFlags(Qt.Tool) self.setGeometry(100,100,100,50) self.setWindowFlags(Qt.FramelessWindowHint) self.setStyleSheet(\"background-color:gray;\") self.setWindowOpacity(0.5) self.label = QLabel(self) self.label.setText(\"Menu Widget\") app = QApplication([]) window = MainWindow() window.show() sys.exit(app.exec_()) Also, if I set the attribute self.setAttribute(Qt.WA_TranslucentBackground) for the child widget, a \"hole\" appears in the main window (if video is playing in this window). How can transparency be set for a child widget? Thanks in advance for your answers!",
        "answers": [
            [
                "As the name suggests, the windowOpacity refers to the opacity of the window. If you want to set the opacity of a child widget, you need to use QGraphicsOpacityEffect: class MenuWidget(QWidget): def __init__(self, parent): QWidget.__init__(self, parent) # ... self.setGraphicsEffect(QGraphicsOpacityEffect(opacity=.5))"
            ],
            [
                "As I understand it, the transparency (background transparency) of the widget can only be created using the desktop compositor. Qt cannot use desktop compositor to render child widgets. The only solution is to use independent windows / widgets. The only solution is to use independent windows/widgets."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm just trying to get dotnet core running on an NVidia Jetson Nano. I've created a simple \"hello world\" app in dotnet core and packaged it as a stand-alone app targeting linux-arm. When I put it on my Synology NAS, I can navigate to the publish directly and type ./HelloDotNetCore and the app runs, albeit with a few errors. /HelloDotNetCore/HelloDotNetCore/bin/Release/netcoreapp3.1/linux-arm$ ./HelloDotNetCore ./HelloDotNetCore: /lib/libstdc++.so.6: no version information available (required by ./HelloDotNetCore) ./HelloDotNetCore: /lib/libstdc++.so.6: no version information available (required by ./HelloDotNetCore) ./HelloDotNetCore: /lib/libstdc++.so.6: no version information available (required by ./HelloDotNetCore) ./HelloDotNetCore: /lib/libstdc++.so.6: no version information available (required by ./HelloDotNetCore) ./HelloDotNetCore: /lib/libstdc++.so.6: no version information available (required by ./HelloDotNetCore) Hello World! I can run it on my Raspberry Pi, as sudo /HelloDotNetCore/HelloDotNetCore/bin/Release/netcoreapp3.1/linux-arm $ sudo ./HelloDotNetCore Hello World! I've \"installed\" dotnet core by following the tutorial here: https://blog.headforcloud.com/2019/04/03/jetson-nano-a-quick-start-with-.net-core-3/ (it's not actually an install, just exposing the binary to bash) /code/HelloDotNetCore/HelloDotNetCore$ dotnet run Hello World! However, attempting to run this as a stand-alone app on my NVidia Jetson results in \"No such file or directory\". I've tried the old obvious chmod +x and chmod 777 tricks along with running as sudo, but there's no other clue as to what it's looking for that isn't there. /code/HelloDotNetCore/HelloDotNetCore/bin/Release/netcoreapp3.1/linux-arm$ ./HelloDotNetCore -bash: ./HelloDotNetCore: No such file or directory So it seems that something that should be packaged with this stand-alone app isn't there, but I'm lost as for how to figure out what it needs. Any ideas?",
        "answers": [
            [
                "I found the culprit. The runtime for the NVidia Jetson needs to be explicitly set to linux-arm64, and not linux-arm. If you run the application from a Jetson using the dotnet command dotnet run it will compile the application into the associated debug or release folder and then you can run it from that folder using ./HelloDotNetCore However, in order to \"publish\" the app from visual studio, I had to update my Microsoft.NETCore.Platforms package via NuGet from here https://www.nuget.org/packages/Microsoft.NETCore.Platforms/ This automatically updated my .csproj file to &lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt; &lt;PropertyGroup&gt; &lt;OutputType&gt;Exe&lt;/OutputType&gt; &lt;TargetFramework&gt;netcoreapp3.1&lt;/TargetFramework&gt; &lt;/PropertyGroup&gt; &lt;ItemGroup&gt; &lt;PackageReference Include=\"Microsoft.NETCore.Platforms\" Version=\"3.1.3\" /&gt; &lt;/ItemGroup&gt; &lt;/Project&gt; Then manually alter the RuntimeIdentifier element of the .pubxml file to reflect the linux-arm64 architecture. &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;!-- https://go.microsoft.com/fwlink/?LinkID=208121. --&gt; &lt;Project ToolsVersion=\"4.0\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\"&gt; &lt;PropertyGroup&gt; &lt;Configuration&gt;Release&lt;/Configuration&gt; &lt;Platform&gt;Any CPU&lt;/Platform&gt; &lt;PublishDir&gt;bin\\Release\\netcoreapp3.1\\publish\\&lt;/PublishDir&gt; &lt;PublishProtocol&gt;FileSystem&lt;/PublishProtocol&gt; &lt;TargetFramework&gt;netcoreapp3.1&lt;/TargetFramework&gt; &lt;RuntimeIdentifier&gt;linux-arm64&lt;/RuntimeIdentifier&gt; &lt;SelfContained&gt;true&lt;/SelfContained&gt; &lt;PublishSingleFile&gt;True&lt;/PublishSingleFile&gt; &lt;PublishTrimmed&gt;False&lt;/PublishTrimmed&gt; &lt;/PropertyGroup&gt; &lt;/Project&gt; I was then able to publish the app using the publish command in Visual Studio, which built a stand-alone application inside a folder called 'publish' Now, I get the expected result. Jetson:/code/publish$ ./HelloDotNetCore Hello World!"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a code reading a serialized TensorRT engine: import tensorrt as trt import pycuda.driver as cuda cuda.init() device = cuda.Device(0) context = device.make_context() logger = trt.Logger(trt.Logger.INFO) with trt.Runtime(logger) as runtime: with open('model.trt', 'rb') as in_: engine = runtime.deserialize_cuda_engine(in_.read()) which runs just fine on my Nvidia Jeston Nano, until I compile it with Pyinstaller pyinstaller temp.py In the compiled code runtime.deserialize_cuda_engine returns None and logger says: Cuda Error in loadKernel: 3 (initialization error) [TensorRT] ERROR: INVALID_STATE: std::exception [TensorRT] ERROR: INVALID_CONFIG: Deserialize the cuda engine failed. When I construct the engine from scratch, like cuda.init() device = cuda.Device(0) context = device.make_context() logger = trt.Logger(trt.Logger.INFO) with ExitStack() as stack: builder = stack.enter_context(trt.Builder(logger)) network = stack.enter_context(builder.create_network( 1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH) )) i = network.add_input('input0', trt.float16, (3, 2)) s = network.add_softmax(i) network.mark_output(s.get_output(0)) config = stack.enter_context(builder.create_builder_config()) ...some builder settings like opt profiles and fp16 mode... engine = builder.build_engine(network, config) then everything works fine, even after compilation. The engine was prepared with trtexec on the same computer. Cuda version is V10.2.89, pycuda version is 2019.1.2. I believe it's a standard jetson installation as of August 2020. Any ideas what might be involved here and what workarounds might be?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am converted the yolov2 frozen graph to tftrt graph using following code. OUTPUT_NAME = [\"models/convolutional23/BiasAdd\"] # read Tensorflow frozen graph with gfile.FastGFile('./yolov2_frozen-graph.pb', 'rb') as tf_model: tf_graphf = tensorflow.GraphDef() tf_graphf.ParseFromString(tf_model.read()) # convert (optimize) frozen model to TensorRT model trt_graph = trt.create_inference_graph(input_graph_def=tf_graphf, outputs=OUTPUT_NAME, max_batch_size=1, max_workspace_size_bytes=2 * (10 ** 9), precision_mode=\"FP32\") # write the TensorRT model to be used later for inference with gfile.FastGFile(\"Yolo_TensorRT_modelFP16.pb\", 'wb') as f: f.write(trt_graph.SerializeToString()) print(\"TensorRT model is successfully stored!\") Then after that I am running inference using following code. with tf.Session() as sess: img = cv2.imread(\"image3.jpg\") img = cv2.resize(img, (608, 608)) # read TensorRT frozen graph with gfile.FastGFile('Yolo_TensorRT_modelFP16.pb', 'rb') as trt_model: trt_graph = tf.GraphDef() trt_graph.ParseFromString(trt_model.read()) # obtain the corresponding input-output tensor tf.import_graph_def(trt_graph, name='') input = sess.graph.get_tensor_by_name('models/net1:0') output = sess.graph.get_tensor_by_name('models/convolutional23/BiasAdd:0') for i in range(100): start = time.time() # perform inference sess.run(output, feed_dict={input: [np.asarray(img)]}) end = time.time() - start print(\"infernce time: \", end) So it is giving exactly same performance as normal yolov2 frozen graph even after I am running inference on FP16 yolov2 frozen tftrt graph. Can you tell me what I have to do to increase performance with tftrt graph?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to measure ONLY the inference time in the Jetson TX2. How can I improve my function to do that? As right now I am measuring: the transfer of the image from CPU to GPU transfer of results from GPU to CPU the inference Or is that not possible because of the way GPUs work? I mean, how many times will I have to use stream.synchronize() if I divide/segment the function into 3 parts: transfer from CPU to GPU Inference transfer from GPU to CPU Thank you CODE IN INFERENCE.PY def do_inference(engine, pics_1, h_input, d_input, h_output, d_output, stream, batch_size): \"\"\" This is the function to run the inference Args: engine : Path to the TensorRT engine. pics_1 : Input images to the model. h_input: Input in the host (CPU). d_input: Input in the device (GPU). h_output: Output in the host (CPU). d_output: Output in the device (GPU). stream: CUDA stream. batch_size : Batch size for execution time. height: Height of the output image. width: Width of the output image. Output: The list of output images. \"\"\" # Context for executing inference using ICudaEngine with engine.create_execution_context() as context: # Transfer input data from CPU to GPU. cuda.memcpy_htod_async(d_input, h_input, stream) # Run inference. #context.profiler = trt.Profiler() ##shows execution time(ms) of each layer context.execute(batch_size=1, bindings=[int(d_input), int(d_output)]) # Transfer predictions back from the GPU to the CPU. cuda.memcpy_dtoh_async(h_output, d_output, stream) # Synchronize the stream. stream.synchronize() # Return the host output. out = h_output return out CODE IN TIMER.PY for i in range (count): start = time.perf_counter() # Classification - calling TX2_classify.py out = eng.do_inference(engine, image, h_input, d_input, h_output, d_output, stream, 1) inference_time = time.perf_counter() - start print(\"TIME\") print(inference_time * 1000) print(\"\\n\") pred = postprocess_inception(out) print(pred) print(\"\\n\")",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am working on Nvidia Jetson Tx2 (with JETPACK 4.2) and installed the pytorch following this link. When I am importing torch in python its giving me an error OSError: libcurand.so.10: cannot open shared object file: No such file or directory I have tried all the options but nothing worked. export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.0/lib64 export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/cuda-10.0/lib64 export PATH=$PATH:/usr/local/cuda-10.0/lib64 Any guidance to debug the issue is requested. Thanks",
        "answers": [
            [
                "emm I found this answer as followed... https://forums.developer.nvidia.com/t/mounting-cuda-onto-l4t-docker-image-issues-libcurand-so-10-cannot-open-no-such-file-or-directory/121545 The key is : \"You can use JetPack4.4 for CUDA 10.2 and JetPack4.3 for CUDA 10.0.\" Maybe downloading Pytorch v1.4.0 and Jetpack 4.2/4.3 would solve this question... Anyway, it is helped for me... good luck"
            ],
            [
                "Downgrading torch from 2.0.1 to 1.4.0 worked in my case: Pip pip install pytorch==1.4.0 Or with poetry: poetry add pytorch==1.4.0 Or with conda: conda install pytorch==1.4.0"
            ]
        ],
        "votes": [
            2.0000001,
            -0.9999999
        ]
    },
    {
        "question": "I'm working on a eyes tracking program using OpenCV, dlib and TensorFlow libraries, and I encounter some issues with a keras functions that using CPU instead of GPU. My setup I'm working on a Jetson AGX Xavier (Jetpack 4.4), running with Ubuntu and Cuda version 10.2.89 with a Ubuntu system. The libraries was installed according these links: OpenCV: https://github.com/mdegans/nano_build_opencv Dlib: https://medium.com/@tran.minh.hoang.april/install-dlib-with-cuda-9-0-34c0f61fcf74 Tensorflow + Keras: https://forums.developer.nvidia.com/t/official-tensorflow-for-jetson-agx-xavier/65523 The problem Well, my code runs well, so this is not a code issue. The problem is that one of his key function running on CPU instead of GPU, and strongly impact performances. This function is the predict function available in Tensorflow Keras. I was able to monitor the GPU usage by using thejtop command, and it is close to 0. So i started digging why. The things I tried I started to make some digging by first checking the available devices for tensorflow. I ran the following command: from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) And it gave me: [name: \"/device:CPU:0\" device_type: \"CPU\" ... name: \"/device:XLA_CPU:0\" device_type: \"XLA_CPU\" ... physical_device_desc: \"device: XLA_CPU device\" , name: \"/device:XLA_GPU:0\" device_type: \"XLA_GPU\" ... physical_device_desc: \"device: XLA_GPU device\" , name: \"/device:GPU:0\" device_type: \"GPU\" ... So I assumed that, at least, Tensorflow recognizes my GPU. Then I tried I made another test: import tensorflow as tf if tf.test.gpu_device_name(): print('Default GPU Device: {}'.format(tf.test.gpu_device_name())) else: print(\"Please install GPU version of TF\") And it gave me: Name: /device:GPU:0 So at this point everything seems ok. I pushed forward by activating the logs of Tensorflow : tf.debugging.set_log_device_placement(True) I tracked down the two tensorflow functions I used in my program to check the detailed logs. The first used functions is called like this in my program. It is called just once: model = tf.keras.models.load_model('2018_12_17_22_58_35.h5', compile=True) The associated logs are: ... 2020-10-15 22:40:31.591951: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0 2020-10-15 22:40:31.633533: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0 2020-10-15 22:40:31.636725: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0 2020-10-15 22:40:31.666428: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0 2020-10-15 22:40:31.670077: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0 ... So it appears that this function uses GPU. The other function, predict, is called like this: pred_l = model.predict(eye_input) The logs are: ... RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0 2020-10-15 22:40:38.143067: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0 2020-10-15 22:40:38.161602: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0 2020-10-15 22:40:38.163806: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0 2020-10-15 22:40:38.179115: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0 ... In this case, the logs say this function use CPU, which is coherent with my initial analysis. Since this is function is call in the while loop (to apply it to everey images), it is crutial to run it on GPU to increase performance. I tried to force GPU usage by using with tf.device('/device:GPU:0) But it still not working. Since I followed the official NVIDA instructions to install the lib, and since official website indicate that tensorflow will use GPU by default if it is available, i don't think it is an installation problem. Is anyone have to solutions to this ? Thanks.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have tried to connect my Jetson Nano with TTL UART cable using minicom. When I typed sudo minicom It showed an error minicom throwing an error minicom: cannot open /dev/modem: No such file or directory",
        "answers": [
            [
                "sudo is always a way... What minicom tells you with this message that it has tried to open a interface called /dev/modem. It is telling you, that it can't be opened due to insufficient permissions. On my system /dev/modem does not even exist! Anyhow, minicom expects in it's settings the /dev/modem as default. That's why it's trying to connect to that. So, the solution is minicom -s, where you actually enter the settings of minicom. Her you'll find the the settings to properly configure your board. If you run this with sudo, you are able to also save your settings, if you want to. You might want to change those settings to e.g. /dev/ttyUSB0 or something."
            ],
            [
                "Later I tried with sudo minicom -s It worked"
            ]
        ],
        "votes": [
            7.0000001,
            6.0000001
        ]
    },
    {
        "question": "My camera shows a green screen. I am using IMX 219 I don' know why the camera gives this output import cv2 cap=cv2.VideoCapture(0) while True: r,im=cap.read() cv2.imshow('dd',im) k=cv2.waitKey(30) &amp; 0xff if k==27: break cap.release() cv2.destroyAllWindows()",
        "answers": [
            [
                "General theory As said in this link, you can use v4l2-ctl to determine the camera capabilities. v4l2-ctl is in the v4l-utils: $ sudo apt-get install v4l-utils and then: $ v4l2-ctl --list-formats-ext Looking to the same link and to this other, I saw that you can also quickly test your camera launching: # Simple Test # Ctrl^C to exit # sensor_id selects the camera slot: 0 or 1 on Jetson Nano B01 $ gst-launch-1.0 nvarguscamerasrc sensor_id=0 ! nvoverlaysink This simple gst-launch example can be used to determine the camera modes that are reported by the sensor you are using. Say for example that you get this output: GST_ARGUS: 3264 x 2464 FR = 21.000000 fps Duration = 47619048 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000 then you should adjust accordingly the next command: $ gst-launch-1.0 nvarguscamerasrc sensor_id=1 ! \\ 'video/x-raw(memory:NVMM),width=3264, height=2464, framerate=21/1, format=NV12' ! \\ nvvidconv flip-method=0 ! 'video/x-raw, width=816, height=616' ! \\ nvvidconv ! nvegltransform ! nveglglessink -e sensor_id=1 represents the right CSI camera slot, it can be either 0 or 1. As you can see from this link, newer Jetson Nano Development Kits come with two CSI camera slots and you can use this attribute to specify the right one [0 is the default]. Please notice that at the same link they use sensor_mode instead of sensor_id, I'd try with both. You don't necessarily need to include flip-method which is documented here though. All of this should give you an idea for the values to be inserted in the code Also, it has been noticed that the display transform is sensitive to width and height [in the above example, width=816, height=616]. If you experience issues, check to see if your display width and height is the same ratio as the camera frame size selected [in the above example, 816 and 616 are respectively a quarter of 3264 and 2464] OpenCV Looking around on nVidia forum I found this post. The solution in that case was to use: cap = cv2.VideoCapture('nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3280, height=2464, format=(string)NV12, framerate=(fraction)20/1 ! nvvidconv ! video/x-raw, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink', cv2.CAP_GSTREAMER) In your case though, the 20fps for your IMX 219 would be too high with a frame size equal to 3280x2464. As you can see from the first table of this link the suggested value is 15fps while here they suggest 21fps. I would suggest you to start with the width, height, framerate values retrieved in the previous section. A framerate value lower than the nominal one should help you test the connectivity cap = cv2.VideoCapture('nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3280, height=2464, format=(string)NV12, framerate=(fraction)15/1 ! nvvidconv ! video/x-raw, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink', cv2.CAP_GSTREAMER) A full sample where to include the previous line updated with the right values is available from here: # MIT License # Copyright (c) 2019 JetsonHacks # See license # Using a CSI camera (such as the Raspberry Pi Version 2) connected to a # NVIDIA Jetson Nano Developer Kit using OpenCV # Drivers for the camera and OpenCV are included in the base image import cv2 # gstreamer_pipeline returns a GStreamer pipeline for capturing from the CSI camera # Defaults to 1280x720 @ 60fps # Flip the image by setting the flip_method (most common values: 0 and 2) # display_width and display_height determine the size of the window on the screen def gstreamer_pipeline( capture_width=1280, capture_height=720, display_width=1280, display_height=720, framerate=60, flip_method=0, ): return ( \"nvarguscamerasrc ! \" \"video/x-raw(memory:NVMM), \" \"width=(int)%d, height=(int)%d, \" \"format=(string)NV12, framerate=(fraction)%d/1 ! \" \"nvvidconv flip-method=%d ! \" \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \" \"videoconvert ! \" \"video/x-raw, format=(string)BGR ! appsink\" % ( capture_width, capture_height, framerate, flip_method, display_width, display_height, ) ) def show_camera(): # To flip the image, modify the flip_method parameter (0 and 2 are the most common) print(gstreamer_pipeline(flip_method=0)) cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=0), cv2.CAP_GSTREAMER) if cap.isOpened(): window_handle = cv2.namedWindow(\"CSI Camera\", cv2.WINDOW_AUTOSIZE) # Window while cv2.getWindowProperty(\"CSI Camera\", 0) &gt;= 0: ret_val, img = cap.read() cv2.imshow(\"CSI Camera\", img) # This also acts as keyCode = cv2.waitKey(30) &amp; 0xFF # Stop the program on the ESC key if keyCode == 27: break cap.release() cv2.destroyAllWindows() else: print(\"Unable to open camera\") if __name__ == \"__main__\": show_camera() There is also a full snippet available at this other link that can help you find the reasons for failure import cv2 import gi gi.require_version('Gst', '1.0') from gi.repository import Gst def read_cam(): cap = cv2.VideoCapture(\"nvarguscamerasrc ! video/x-raw(memory:NVMM), width=(int)1920, height=(int)1080,format=(string)NV12, framerate=(fraction)30/1 ! nvvidconv ! video/x-raw, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink\") if cap.isOpened(): cv2.namedWindow(\"demo\", cv2.WINDOW_AUTOSIZE) while True: ret_val, img = cap.read(); cv2.imshow('demo',img) if cv2.waitKey(30) == ord('q'): break else: print (\"camera open failed\") cv2.destroyAllWindows() if __name__ == '__main__': print(cv2.getBuildInformation()) Gst.debug_set_active(True) Gst.debug_set_default_threshold(3) read_cam() Lastly, if you can open the camera in GStreamer from the command line but not in Python have a check of the OpenCV version with the previous print(cv2.getBuildInformation()) or more shortly with: print(cv2.__version__) Starting with L4T 32.2.1 / JetPack 4.2.2, GStreamer support is built into OpenCV. The OpenCV version is 3.3.1 for those versions and if you are using earlier versions of OpenCV [most likely installed from the Ubuntu repository] you will get Unable to open camera errors"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Good afternoon everyone, When trying to control a stepper motor through the GPIO header on the AGX Xavier I ran into the following issue: When utilizing the jetson-gpio library for python compatible GPIO control I am able to run all sample scripts (including the \u2018simple_pwm.py\u2019, this is relevant later in this post). However, when I attempt to use the JetsonGPIO librari for c++ compatible GPIO control the following error appears when using the GPIO::PWM() command: The same command in Python (syntax) does work however. I have moved the 99-gpio.rules file to /etc/udev/rules.d and created the gpio group and added my user to this group. I have tried running all commands with sudo, with no effect unfortunately. Does anyone know how to solve the permission issue? I would really like to use the GPIO::PWM function in a C++ script rather than python. Best regards, Ruud Extra info: As explained in the following link: Configure GPIO Nvidia Jetson The GPIO header of a Jetson device can be configured. The 3 PWM pins on the 40-pin expansion header are disabled by default so these needed to be enabled. I have done this.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have integrated psplash to a custom Yocto layer for NVIDIA Jetson Nano. I want to run psplash from the very first init script (PID=1). The reason is to cover the time spent by systemd to load unit and service files. I have added init=/bin/newinit.sh to the Linux kernel parameters. Here is the content of the /bin/newinit.sh that replaces my /sbin/init. #!/bin/sh /usr/bin/psplash &amp; /bin/psplashanimator.sh &amp; exec /sbin/init Here is the content of the /bin/psplashanimator.sh #!/bin/sh /bin/sleep 0.1 &amp;&amp; /usr/bin/psplash-write \"PROGRESS 50\" I made sure that psplash-write is not called by any other process by disabling the systemd daemons (psplash-start, psplash-systemd). Yet, I can't update the progress bar by using these scripts. Psplash is displayed nicely, but nothing on the progress bar. If I move /bin/sleep 0.1 &amp;&amp; /usr/bin/psplash-write \"PROGRESS 50\" into the /bin/newinit.sh before exec, it works fine; but it blocks the main execution of the init script. All in all, if I start the psplash in a custom init script, psplash-write only works before the exec is called. I can't figure out why that is and how to fix that. If there is a simple solution or rationale as to why this wouldn't work, I would kindly appreciate your guidance. NOTE1: Note that conventional way of updating psplash with systemd services (psplash-systemd.service) work fine when psplash is started using psplash-start.service, I get progress bar updates. NOTE2: Psplash uses framebuffer driver and FIFO. NOTE3: Related source files: psplash.c psplash-write.c NOTE4: Current systemd execution (to let you see if I am missing something that must be ready before running psplash)",
        "answers": [
            [
                "I'm not 100% sure about psplash and systemd, but it might be the case you need to have something like this in your environment to control psplash: export TMPDIR=/mnt/.psplash"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I would like to optimize ML code (SSD in PyTorch) on NVIDIA Jetson Xavier NX (development kit). One of the bottlenecks seems to be list slicing on PyTorch (1.6.0) tensors on GPU device. The same problem occured on NVIDIA GeForce GTX 1050 Ti (GP107), CPU was ~2 times faster. Let me create the variables first import torch from time import time cuda0 = torch.device('cuda:0') probs = torch.ones([3000], dtype=torch.float64, device=cuda0) mask = torch.ones([3000], dtype=torch.bool, device=cuda0) probs_cpu = probs.cpu() mask_cpu = mask.cpu() Then run the logic (Approximately same results occurred every run) before = time() probs[mask] print(f'GPU {time() - before:.5f}') # output: GPU 0.00263 before = time() probs_cpu[mask_cpu] print(f'CPU {time() - before:.5f}') # output: CPU 0.00066 Why is the list slicing ~4 times slower on GPU compared to CPU using PyTorch library vesrion 1.6.0 on NVIDIA Jetson Xavier NX Developer kit according to the code above? How to speed it up? Code details: see line 51 in predictor.py which is part of SSD Implementation in PyTorch Run it on CPU?: Whole algorithm will not be faster if I run it on the CPU since the downloading from GPU takes too long (~0.00805 s).",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a Jetson Xavier NX Board. I need to interface the ultrasonic sensor. I used Jetson.GPIO lib to communicate through GPIO but I'm not getting any data from Jetson. I believe the GPIO pin is not powering up which shows 0V after making it HIGH. If the problem is with GPIO Configuration then please let me know how do we configure a particular pin. Otherwise, please let me know why it is not working. I couldn't resolve this issue. Please help me with that. Thanks in Advance.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a NLP model trained on Pytorch to be run in Jetson Xavier. I installed Jetson stats to monitor usage of CPU and GPU. When I run the Python script, only CPU cores work on-load, GPU bar does not increase. I have searched on Google about that with keywords of \" How to check if pytorch is using the GPU?\" and checked results on stackoverflow.com etc. According to their advices to someone else facing similar issue, cuda is available and there is cuda device in my Jetson Xavier. However, I don\u2019t understand why GPU bar does not change, CPU core bars go to the ends. I don\u2019t want to use CPU, it takes so long to compute. In my opinion, it uses CPU, not GPU. How can I be sure and if it uses CPU, how can I change it to GPU? Note: Model is taken from huggingface transformers library. I have tried to use cuda() method on the model. (model.cuda()) In this scenario, GPU is used but I can not get an output from model and raises exception. Here is the code: from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline import torch BERT_DIR = \"savasy/bert-base-turkish-squad\" tokenizer = AutoTokenizer.from_pretrained(BERT_DIR) model = AutoModelForQuestionAnswering.from_pretrained(BERT_DIR) nlp=pipeline(\"question-answering\", model=model, tokenizer=tokenizer) def infer(question,corpus): try: ans = nlp(question=question, context=corpus) return ans[\"answer\"], ans[\"score\"] except: ans = None pass return None, 0",
        "answers": [
            [
                "The problem has been solved with loading pipeline containing device parameter: nlp = pipeline(\"question-answering\", model=BERT_DIR, device=0)"
            ],
            [
                "For the model to work on GPU, the data and the model has to be loaded to the GPU: you can do this as follows: from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline import torch BERT_DIR = \"savasy/bert-base-turkish-squad\" device = torch.device(\"cuda\") tokenizer = AutoTokenizer.from_pretrained(BERT_DIR) model = AutoModelForQuestionAnswering.from_pretrained(BERT_DIR) model.to(device) ## model to GPU nlp=pipeline(\"question-answering\", model=model, tokenizer=tokenizer) def infer(question,corpus): try: ans = nlp(question=question.to(device), context=corpus.to(device)) ## data to GPU return ans[\"answer\"], ans[\"score\"] except: ans = None pass return None, 0"
            ]
        ],
        "votes": [
            4.0000001,
            2.0000001
        ]
    },
    {
        "question": "I would like to ask one thing about the newest Nvidia release of Jetpack OS 4.4. When I click here for looking a release versions in v42 and v43 it has two folders tensorflow and tensorflow-gpu for cpu and gpu version. But when I click v44 it has only one folder, just tensorflow and versions inside. Does it means that for the Jetpack 4.4 is not a Tensorflow with GPU support released?",
        "answers": [
            [
                "It's just a name change nothing more the so-called tensorflow-gpu is now called tensorflow Note: As of the 20.02 TensorFlow release, the package name has changed from tensorflow-gpu to tensorflow. See the section on Upgrading TensorFlow for more information. Check more here"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to enable x11 forwarding on macOS connected to a Jetson Xavier nx. However, upon connecting: sudo ssh -Xvvv id@host The connection establishes successfully. But in the verbose logs, I'm getting the following: debug3: send packet: type 50 debug2: we sent a password packet, wait for reply debug3: receive packet: type 52 debug1: Authentication succeeded (password). Authenticated to proto1 ([192.168.1.106]:22). debug1: channel 0: new [client-session] debug3: ssh_session2_open: channel_new: 0 debug2: channel 0: send open debug3: send packet: type 90 debug1: Requesting no-more-sessions@openssh.com debug3: send packet: type 80 debug1: Entering interactive session. debug1: pledge: exec debug3: receive packet: type 80 debug1: client_input_global_request: rtype hostkeys-00@openssh.com want_reply 0 debug3: receive packet: type 91 debug2: channel_input_open_confirmation: channel 0: callback start debug1: X11 forwarding requested but DISPLAY not set debug2: fd 4 setting TCP_NODELAY debug3: ssh_packet_set_tos: set IP_TOS 0x48 debug2: client_session2_setup: id 0 debug2: channel 0: request pty-req confirm 1 debug3: send packet: type 98 debug1: Sending environment. debug3: Ignored env TERM debug3: Ignored env SSH_AUTH_SOCK debug3: Ignored env PATH debug1: Sending env LANG = en_US.UTF-8 debug2: channel 0: request env confirm 0 debug3: send packet: type 98 debug3: Ignored env HOME debug3: Ignored env MAIL debug3: Ignored env LOGNAME debug3: Ignored env USER debug3: Ignored env SHELL debug3: Ignored env SUDO_COMMAND debug3: Ignored env SUDO_USER debug3: Ignored env SUDO_UID debug3: Ignored env SUDO_GID debug3: Ignored env __CF_USER_TEXT_ENCODING debug2: channel 0: request shell confirm 1 debug3: send packet: type 98 debug2: channel_input_open_confirmation: channel 0: callback done debug2: channel 0: open confirm rwindow 0 rmax 32768 debug3: receive packet: type 99 debug2: channel_input_status_confirm: type 99 id 0 debug2: PTY allocation request accepted on channel 0 debug2: channel 0: rcvd adjust 2097152 debug3: receive packet: type 99 debug2: channel_input_status_confirm: type 99 id 0 debug2: shell request accepted on channel 0 X11 forwarding requested but DISPLAY not set And in order for X11 forwarding to work, the DISPLAY variable must be set upon connection. Manually setting the DISPLAY like so: export DISPLAY=http://localhost:22 doesn't work either. How to enable X11 forwarding?",
        "answers": [
            [
                "I had the same problem trying to get X11 forwarding to XQuartz. I believe when you ssh from Mac, if DISPLAY in the Mac terminal is not set, then the ssh client does not know where to forward the X11 traffic. So I just did: DISPLAY=:0 in the Mac terminal, then ssh -X and it worked."
            ],
            [
                "Well there should definitely not be an http:// in there, and I think your :22 came from the SSH port and is not an X11 display number you should be using. Your DISPLAY on the local machine is almost always :0.0, and if you don't already see that when you echo $DISPLAY on your machine, something is probably wrong there. Make sure you can run X11 applications on your Mac alone first. You may need to install XQuartz. Try running without sudo; maybe root isn't allowed to talk to your X11 on Mac. Also, maybe try ssh -X from within an xterm window (which should open up when you open XQuartz manually, or be launchable from its menus) to make sure it picks up the right DISPLAY."
            ],
            [
                "I met the same problem earlier. It turns out I didn't install X11 forwarding packages on my Mac. So make sure you installed XQuartz by brew: brew install xquartz --cask (Notice that the error message can be like yours when xquartz is not installed on your Mac, so check on your local machine first.) Then make sure you have export DISPLAY=:0, or you can check the $DISPLAY by echo $DISPLAY. After that, by using verbose ssh -v -X user@host, you should find debug message like: debug1: Requesting X11 forwarding with authentication spoofing. That worked for me. Though the eog command shows with slow speed later."
            ]
        ],
        "votes": [
            10.0000001,
            4.0000001,
            1e-07
        ]
    },
    {
        "question": "I succesfull converted the .weight file to .tf file, then i used the convert_trt.py scirpt which indeed is getting killed after a 2 mins wait. I use a Jetson Xavier nx, Cuda 10.2. Error log: .. .. . 2020-08-22 15:31:22.362558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3624 MB memory) -&gt; physical GPU (device: 0, name: Xavier, pci bus id: 0000:00:00.0, compute capability: 7.2) Killed",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am currently trying to build a docker container which should be able to run GPU accelerated tensorflow on top of a Xavier AGX. My approach is derived from example and the official nvidia documentation jetson-tensorflow. My Dockerfile looks like this: FROM nvcr.io/nvidia/l4t-base:r32.4.2 WORKDIR / RUN apt update &amp;&amp; apt install -y --fix-missing make g++ RUN apt update &amp;&amp; apt install -y --fix-missing python3-pip libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev libjpeg8-dev liblapack-dev libblas-dev gfortran python3-h5py RUN pip3 install -U pip testresources setuptools cython numpy==1.16.1 future==0.17.1 mock==3.0.5 h5py==2.9.0 keras_preprocessing==1.0.5 keras_applications==1.0.8 gast==0.2.2 pybind11 RUN pip3 install --pre --no-cache-dir --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v44 tensorflow COPY requirements.txt . RUN pip3 install -r requirements.txt COPY . . CMD [\"python3\", \"-u\", \"app.py\"] However, I am getting these errors, related to the h5py: In file included from /tmp/pip-build-cdi1gcqf/h5py/h5py/defs.c:654:0: /tmp/pip-build-cdi1gcqf/h5py/h5py/api_compat.h:27:10: fatal error: hdf5.h: No such file or directory #include \"hdf5.h\" When adding HDF5_DIR=/usr/lib/x86_64-linux-gnu/hdf5/serial/ before installing h5py, I succeed and get to the stage where it installs tensorflow. However, then tensorflow installs h5py==2.10.0 where again I get an error: error libhdf5.so: No such file or Directory The JetPack version of the Xavier is 4.4 and the l4t version is 32.4.2 Any help would be highly appreciated! Best regards Dominik",
        "answers": [
            [
                "The solution was to change: h5py==2.9.0 to h5py==2.10.0 in the first installation command. The following Dockerfile works: FROM nvcr.io/nvidia/l4t-base:r32.4.2 WORKDIR / RUN apt update &amp;&amp; apt install -y --fix-missing make g++ RUN apt update &amp;&amp; apt install -y --fix-missing python3-pip libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev libjpeg8-dev liblapack-dev libblas-dev gfortran python3-h5py RUN HDF5_DIR=/usr/lib/x86_64-linux-gnu/hdf5/serial/ pip3 install -U pip testresources setuptools cython numpy==1.16.1 future==0.17.1 mock==3.0.5 h5py==2.10.0 keras_preprocessing==1.0.5 keras_applications==1.0.8 gast==0.2.2 pybind11 RUN pip3 install --pre --no-cache-dir --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v44 tensorflow COPY requirements.txt . RUN pip3 install -r requirements.txt COPY . . CMD [\"python3\", \"-u\", \"app.py\"]"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to use ROS with CUDA-enabled OpenCV on my Jetson Nano. At this point I don\u00b4t care about the versions. The problem: Jetson Nano only supports CUDA 10 and Ubuntu 18.04. The ROS version for Ubuntu 18.04 is Melodic, which needs OpenCV 3.2, but OpenCV 3.2 only supports CUDA 8. I have found a guide here on SO (CMake Error: Variables are set to NOTFOUND) to build it with CUDA 9, but it fails when trying it with CUDA 10 due to \"error: identifier \"__shfl_down\" is undefined\", and some other \"__shfl_XXX\" errors. Anyone here succeeded in getting this to work? Or any idea on how to fix the \"__shfl_down\" error? Can OpenCV 3.2 work with CUDA 10.2?",
        "answers": [
            [
                "Option 01: Nope, do not try to build with CUDA 10.2, this is my suggestion in which you are in safe side both ways. In your package CMakeLists.txt add your alternative OpenCV (comes with Nano) as follows while assumimg main.cpp is the your main file, if not change it: set(OpenCV_INCLUDE_DIRS &lt;path_to&gt;/include &lt;path_to&gt;/include/opencv2 ) set(OpenCV_LIB_DIR &lt;path_to&gt;/lib ) set(OpenCV_LIBS opencv_core opencv_highgui opencv_imgcodecs ) include_directories(${OpenCV_INCLUDE_DIRS}) link_directories(${OpenCV_LIB_DIR}) add_executable(${PROJECT_NAME} src/main.cpp) target_link_libraries(${PROJECT_NAME} ${OpenCV_LIBS}) Under OpenCV_LIBS add OpenCV modules you use in your code Note: I am not a position to test this on my machine, so consider this as a tentative answer, if you have problems let me know, I will try to help Option 02: catkin_make -DOpenCV_DIR=/usr/local/share/OpenCV Note: OpenCV_DIR must point to a folder with opencv-conifg.cmake file. More information can be found here"
            ],
            [
                "I found a solution, even though I have not done extensive testing on it yet: Build OpenCV 4.2 (any version that supports CUDA 10.2 should work) from source, enabling CUDA. A good guide is available at https://www.pyimagesearch.com/2020/03/25/how-to-configure-your-nvidia-jetson-nano-for-computer-vision-and-deep-learning/ (skip parts with tensorflow etc) Install ROS Melodic. No need to build from source. After creating a workspace, clone the cv_bridge and image_transport modules into it. IMPORTANT: Switch to branch \"Noetic\"! In the CMakeLists.txt in cv_bridge, change the find_package(Boost REQUIRED python37) to find_package(BOOST REQUIRED python) In cv_bridge/src/module.hpp in the function do_numpy_import, change the return type from void* to void, and remove the return nullptr; Build the workspace with catkin. It should build normally. I have tested a basic image publish, and viewing it in rqt_image_view. Works like a charm! I know this is a bit of a hacky solution, so if anyone knows something better, please let me know!"
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am working on Jetson Xavier NX development Kit, I am trying some deepstream-5.0 apps on it and I have to analyse the performance of Deepstream-5.0 sample apps and my custom app which is very much similar to similar-apps. So, to analyse the performance I want to use NVIDIA NSight Systems. Now, to install NVIDIA NSight Systems using NVIDIA SDK Manager I am trying to install SDK Manager on the Xavier system using the steps given here. But it is giving the following problem: sudo apt install ./sdkmanager_1.2.0-6738_amd64.deb Reading package lists... Done Building dependency tree Reading state information... Done Note, selecting 'sdkmanager:amd64' instead of './sdkmanager_1.2.0-6738_amd64.deb' Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: sdkmanager:amd64 : Depends: libgconf-2-4:amd64 but it is not installable Depends: libcanberra-gtk-module:amd64 but it is not installable Depends: locales:amd64 but it is not installable E: Unable to correct problems, you have held broken packages. I have tried installing the dependent libraries, updating and upgrading the system and fixing the broken packages but got no luck.",
        "answers": [
            [
                "You are receiving the error because the Xavier NX is an ARM system, and the Nvidia SDK Manager is meant for an amd64 instruction set. If you follow the flowchart on their page, you'll notice that the SDK Manager arrow points to a host system. The solution is to run Ubuntu on a desktop or laptop, and hook up the Xavier NX over microUSB. Follow the instructions on the page you linked to update the dev board with the latest software. Another important note: if you flash your development board using Nvidia's flash.sh script (the script tied to the flash button in the GUI) your partition will be limited to 14GB regardless of it's size. The workaround is to go into \"/home//nvidia/nvidia_sdk/Jetpack_&lt;version_number&gt;_&lt;dev_kit&gt;/Linux_for_Tegra/tools\" and run the script titled \"jetson-disk-image-creator.sh\" The script creates a disk image you can write to an sd card using balena etcher, which doesn't have the partition limit."
            ],
            [
                "I was getting the same error. So, I tried updating and upgrading the packages on my system by running the following commands and it worked for me. Although I would like to mention that I am working on TX2, I am not sure about Xavier. sudo apt update apt list --upgradable sudo apt upgrade sudo apt install ./sdkmanager-[version].deb"
            ],
            [
                "sudo apt-get update will resolve this issue !"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "In this link, we can access to gstreamer pipeline buffer and convert the frame buffers in numpy array, I want to know, How I can to accesses the frame buffers in GPU mem and then feed into my custom processor without convert frames into numpy array. We have two solutions for using of deepstream decoder(efficient way than opencv+gstreamer): the one way is we need to write custom element of processing and register in gstreamer and then put the custom element in the pipeline and then do processing on frames buffer. this way is good but need to write and knowledge gstreamer programming. this way is same way of deep stream. the second way is we use only decoded of frames from that link, then passed the frames into custom processor units. for this part I have two question: 1- The loop of gstreamer is same as asyncio programming loop? 2- As you know, If we add additional operation into pad prob function, this cause drop performance, but I want to know, Is it possible to put the frames in the pad prob function and do loop.create_task(process(frame)) like async? this cause we here don't wait to perform processing. like this: def tiler_sink_pad_buffer_probe(pad,info,u_data): .... ### capture the frames in GPU buffer without converting into numpy loop.create_task(process(frame)) .... return Gst.PadProbeReturn.OK",
        "answers": [
            [
                "Yeap gstreamer loop in Python is asyncio Well you can do this like me (bad way by creating global variables) ws = None loopIO = None def tiler_sink_pad_buffer_probe(pad,info,u_data): global ws global loopIO .... ### capture the frames in GPU buffer converting into numpy if ws and loopIO: _, jpeg_frame = cv2.imencode('.jpg', frame_image) str_pic = jpeg_frame.tobytes() asyncio.run_coroutine_threadsafe(ws.send(str_pic), loopIO) .... return Gst.PadProbeReturn.OK if __name__ == '__main__': start_server = websockets.serve(consumer_handler, 'localhost', 8765) loopIO = asyncio.get_event_loop() loopIO.run_until_complete(start_server) wst = threading.Thread(target=asyncio.get_event_loop().run_forever) wst.daemon = True wst.start() sys.exit(main(sys.argv))"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Hi I have several times compiled opencv in my host machine or several arm based system before. As all you now on arm based system compiling opencv takes longer time so I used Quemu to virtualized x86 processor to arm64 and I pulled nvidia jetpack from nvidia dochub page. I am using \"nvcr.io/nvidia/l4t-base\" so I have aarch64 based docker env. When I follow my basic cmake configuration in this docker env. cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D WITH_CUDA=ON \\ -D CUDA_ARCH_BIN=6.2 \\ -D CUDA_ARCH_PTX=\"\" \\ -D ENABLE_FAST_MATH=ON \\ -D CUDA_FAST_MATH=ON \\ -D WITH_CUBLAS=ON \\ -D WITH_LIBV4L=ON \\ -D WITH_GSTREAMER=ON \\ -D WITH_GSTREAMER_0_10=OFF \\ -D WITH_QT=ON \\ -D WITH_OPENGL=ON \\ -D OPENCV_EXTRA_MODULES_PATH=/../opencv3/opencv_contrib-3.4.9/modules \\ -D CPACK_BINARY_DEB=ON \\ ../ I got attached following error log. But basically it says \"fatal error: sys/videoio.h: No such file or directory\" so I previously got videdev.h error but I solved installing v4l package. Interesting point is when I give only cmake .. it works and install opencv but default conf. havent cuda and extra lib option. When I inspect after \"cmake ..\" command there is still same fatal error in log file even though I saw configuration done. I cant figure this error out. Even though it says with default cmake command same error but configuration goes done. When I apply cuda config it says same error but configuration cannnot be done. I am totaly sure cmake have any wrong parameter because of I have used on my jetson tx2 it works like perfect. Do you have any idea ? Btw nvidia docker has 10.2 cuda support I checked with nvcc -V command Maybe in docker env it is impposible to compile opencv with cuda support or Should I start cuda supported docker container different way ? Error log : https://paste.ubuntu.com/p/w9hjBxqJ6D/ Output log : https://paste.ubuntu.com/p/rqsvq356dR/ Here is another output linked by target \"opencv_annotation\" in directory /opencv3/opencv-3.4.9/apps/annotation linked by target \"opencv_visualisation\" in directory /opencv3/opencv-3.4.9/apps/visualisation linked by target \"opencv_interactive-calibration\" in directory /opencv3/opencv-3.4.9/apps/interactive-calibration linked by target \"opencv_version\" in directory /opencv3/opencv-3.4.9/apps/version CUDA_nppist_LIBRARY (ADVANCED) linked by target \"opencv_cudev\" in directory /opencv3/opencv-3.4.9/modules/cudev linked by target \"opencv_cudev\" in directory /opencv3/opencv-3.4.9/modules/cudev linked by target \"opencv_test_cudev\" in directory /opencv3/opencv-3.4.9/modules/cudev/test linked by target \"opencv_test_core\" in directory /opencv3/opencv-3.4.9/modules/core linked by target \"opencv_perf_core\" in directory /opencv3/opencv-3.4.9/modules/core linked by target \"opencv_core\" in directory /opencv3/opencv-3.4.9/modules/core linked by target \"opencv_core\" in directory /opencv3/opencv-3.4.9/modules/core linked by target \"opencv_test_cudaarithm\" in directory /opencv3/opencv-3.4.9/modules/cudaarithm linked by target \"opencv_cudaarithm\" in directory /opencv3/opencv-3.4.9/modules/cudaarithm linked by target \"opencv_cudaarithm\" in directory /opencv3/opencv-3.4.9/modules/cudaarithm linked by target \"opencv_perf_cudaarithm\" in directory /opencv3/opencv-3.4.9/modules/cudaarithm linked by target \"opencv_flann\" in directory /opencv3/opencv-3.4.9/modules/flann linked by target \"opencv_flann\" in directory /opencv3/opencv-3.4.9/modules/flann linked by target \"opencv_test_flann\" in directory /opencv3/opencv-3.4.9/modules/flann linked by target \"opencv_perf_imgproc\" in directory /opencv3/opencv-3.4.9/modules/imgproc linked by target \"opencv_test_imgproc\" in directory /opencv3/opencv-3.4.9/modules/imgproc linked by target \"opencv_imgproc\" in directory /opencv3/opencv-3.4.9/modules/imgproc linked by target \"opencv_imgproc\" in directory /opencv3/opencv-3.4.9/modules/imgproc linked by target \"opencv_test_ml\" in directory /opencv3/opencv-3.4.9/modules/ml linked by target \"opencv_ml\" in directory /opencv3/opencv-3.4.9/modules/ml linked by target \"opencv_ml\" in directory /opencv3/opencv-3.4.9/modules/ml linked by target \"opencv_video\" in directory /opencv3/opencv-3.4.9/modules/video linked by target \"opencv_video\" in directory /opencv3/opencv-3.4.9/modules/video linked by target \"opencv_perf_video\" in directory /opencv3/opencv-3.4.9/modules/video linked by target \"opencv_test_video\" in directory /opencv3/opencv-3.4.9/modules/video linked by target \"opencv_test_cudabgsegm\" in directory /opencv3/opencv-3.4.9/modules/cudabgsegm linked by target \"opencv_cudabgsegm\" in directory /opencv3/opencv-3.4.9/modules/cudabgsegm linked by target \"opencv_cudabgsegm\" in directory /opencv3/opencv-3.4.9/modules/cudabgsegm linked by target \"opencv_perf_cudabgsegm\" in directory /opencv3/opencv-3.4.9/modules/cudabgsegm linked by target \"opencv_test_cudafilters\" in directory /opencv3/opencv-3.4.9/modules/cudafilters linked by target \"opencv_perf_cudafilters\" in directory /opencv3/opencv-3.4.9/modules/cudafilters linked by target \"opencv_cudafilters\" in directory /opencv3/opencv-3.4.9/modules/cudafilters linked by target \"opencv_cudafilters\" in directory /opencv3/opencv-3.4.9/modules/cudafilters linked by target \"opencv_test_cudaimgproc\" in directory /opencv3/opencv-3.4.9/modules/cudaimgproc linked by target \"opencv_cudaimgproc\" in directory /opencv3/opencv-3.4.9/modules/cudaimgproc linked by target \"opencv_cudaimgproc\" in directory /opencv3/opencv-3.4.9/modules/cudaimgproc linked by target \"opencv_perf_cudaimgproc\" in directory /opencv3/opencv-3.4.9/modules/cudaimgproc linked by target \"opencv_test_cudawarping\" in directory /opencv3/opencv-3.4.9/modules/cudawarping linked by target \"opencv_cudawarping\" in directory /opencv3/opencv-3.4.9/modules/cudawarping linked by target \"opencv_cudawarping\" in directory /opencv3/opencv-3.4.9/modules/cudawarping linked by target \"opencv_perf_cudawarping\" in directory /opencv3/opencv-3.4.9/modules/cudawarping linked by target \"opencv_dnn\" in directory /opencv3/opencv-3.4.9/modules/dnn linked by target \"opencv_dnn\" in directory /opencv3/opencv-3.4.9/modules/dnn linked by target \"opencv_perf_dnn\" in directory /opencv3/opencv-3.4.9/modules/dnn linked by target \"opencv_test_dnn\" in directory /opencv3/opencv-3.4.9/modules/dnn linked by target \"opencv_features2d\" in directory /opencv3/opencv-3.4.9/modules/features2d linked by target \"opencv_features2d\" in directory /opencv3/opencv-3.4.9/modules/features2d linked by target \"opencv_perf_features2d\" in directory /opencv3/opencv-3.4.9/modules/features2d linked by target \"opencv_test_features2d\" in directory /opencv3/opencv-3.4.9/modules/features2d linked by target \"opencv_imgcodecs\" in directory /opencv3/opencv-3.4.9/modules/imgcodecs linked by target \"opencv_imgcodecs\" in directory /opencv3/opencv-3.4.9/modules/imgcodecs linked by target \"opencv_perf_imgcodecs\" in directory /opencv3/opencv-3.4.9/modules/imgcodecs linked by target \"opencv_test_imgcodecs\" in directory /opencv3/opencv-3.4.9/modules/imgcodecs linked by target \"opencv_test_photo\" in directory /opencv3/opencv-3.4.9/modules/photo linked by target \"opencv_photo\" in directory /opencv3/opencv-3.4.9/modules/photo linked by target \"opencv_photo\" in directory /opencv3/opencv-3.4.9/modules/photo linked by target \"opencv_perf_photo\" in directory /opencv3/opencv-3.4.9/modules/photo linked by target \"opencv_test_shape\" in directory /opencv3/opencv-3.4.9/modules/shape linked by target \"opencv_shape\" in directory /opencv3/opencv-3.4.9/modules/shape linked by target \"opencv_shape\" in directory /opencv3/opencv-3.4.9/modules/shape linked by target \"opencv_test_videoio\" in directory /opencv3/opencv-3.4.9/modules/videoio linked by target \"opencv_videoio\" in directory /opencv3/opencv-3.4.9/modules/videoio linked by target \"opencv_videoio\" in directory /opencv3/opencv-3.4.9/modules/videoio linked by target \"opencv_perf_videoio\" in directory /opencv3/opencv-3.4.9/modules/videoio linked by target \"opencv_calib3d\" in directory /opencv3/opencv-3.4.9/modules/calib3d linked by target \"opencv_calib3d\" in directory /opencv3/opencv-3.4.9/modules/calib3d linked by target \"opencv_perf_stitching\" in directory /opencv3/opencv-3.4.9/modules/stitching linked by target \"opencv_test_superres\" in directory /opencv3/opencv-3.4.9/modules/superres linked by target \"opencv_superres\" in directory /opencv3/opencv-3.4.9/modules/superres linked by target \"opencv_superres\" in directory /opencv3/opencv-3.4.9/modules/superres linked by target \"opencv_perf_superres\" in directory /opencv3/opencv-3.4.9/modules/superres linked by target \"opencv_test_videostab\" in directory /opencv3/opencv-3.4.9/modules/videostab linked by target \"opencv_videostab\" in directory /opencv3/opencv-3.4.9/modules/videostab linked by target \"opencv_videostab\" in directory /opencv3/opencv-3.4.9/modules/videostab linked by target \"opencv_traincascade\" in directory /opencv3/opencv-3.4.9/apps/traincascade linked by target \"opencv_createsamples\" in directory /opencv3/opencv-3.4.9/apps/createsamples linked by target \"opencv_annotation\" in directory /opencv3/opencv-3.4.9/apps/annotation linked by target \"opencv_visualisation\" in directory /opencv3/opencv-3.4.9/apps/visualisation linked by target \"opencv_interactive-calibration\" in directory /opencv3/opencv-3.4.9/apps/interactive-calibration linked by target \"opencv_version\" in directory /opencv3/opencv-3.4.9/apps/version Lats Progress: When I removed -DOPENCV_EXTRA_MODULES_PATH=$HOME/opencv_contrib-3.4.9/modules \\ -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-10.2 \\ -DCUDA_ARCH_BIN=6.2 \\ -DCUDA_ARCH_PTX=\"\" \\ and Configuring done but I still need cuda and extra lib",
        "answers": [
            [
                "Solved with starting docker image host Cuda components and giving cmake conf. the directory of the toolkit. The important point is that if container comes with different versions of Cuda you shouldn't use it. Use host cuda and give host cuda component directory or copy them into docker -D CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-10.0 \\"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I would like to open a video stream by OpenCv and push frame by frame inside a DeepStream pipeline to use tesornRT to make an inference on Yolov3 model, but i do not know how to make it works. I'm trying to follow the directives that I found here, but still nothing... This is my code : #include &lt;gst/gst.h&gt; #include &lt;gst/app/gstappsrc.h&gt; #include &lt;gst/app/gstappsink.h&gt; #include &lt;opencv2/core/core.hpp&gt; #include &lt;opencv2/core/types_c.h&gt; #include &lt;opencv2/imgproc/imgproc.hpp&gt; #include &lt;opencv2/highgui/highgui.hpp&gt; static GMainLoop *loop; static void cb_need_data (GstElement *appsrc, guint unused_size, gpointer user_data) { static gboolean white = FALSE; static GstClockTime timestamp = 0; guint size,depth,height,width,step,channels; GstFlowReturn ret ; IplImage* img; guchar *data1; GstMapInfo map; cv::Mat imgMat = imread(\"cat.jpg\",cv::IMREAD_COLOR); cvtColor(imgMat,imgMat,cv::COLOR_BGR2YUV); IplImage imgIpl = imgMat; img = &amp;imgIpl; height = img-&gt;height; width = img-&gt;width; step = img-&gt;widthStep; channels = img-&gt;nChannels; depth = img-&gt;depth; data1 = (guchar *)img-&gt;imageData; size = height*width*channels; GstBuffer *buffer = NULL;//gst_buffer_new_allocate (NULL, size, NULL); g_print(\"frame_height: %d \\n\",img-&gt;height); g_print(\"frame_width: %d \\n\",img-&gt;width); g_print(\"frame_channels: %d \\n\",img-&gt;nChannels); g_print(\"frame_size: %d \\n\",height*width*channels); buffer = gst_buffer_new_allocate (NULL, size, NULL); gst_buffer_map (buffer, &amp;map, GST_MAP_WRITE); memcpy( (guchar *)map.data, data1, gst_buffer_get_size( buffer ) ); /* this makes the image black/white */ //gst_buffer_memset (buffer, 0, white ? 0xff : 0x0, size); white = !white; GST_BUFFER_PTS (buffer) = timestamp; GST_BUFFER_DURATION (buffer) = gst_util_uint64_scale_int (1, GST_SECOND, 1); timestamp += GST_BUFFER_DURATION (buffer); //gst_app_src_push_buffer ((GstAppSrc *)appsrc, buffer); g_signal_emit_by_name (appsrc, \"push-buffer\", buffer, &amp;ret); if (ret != GST_FLOW_OK) { g_print(\"quit\"); /* something wrong, stop pushing */ g_main_loop_quit (loop); } //g_print(\"return\"); } gint main (gint argc, gchar *argv[]) { GstElement *pipeline, *appsrc, *conv, *videosink, *sink,*nvosd,*streammux; /* init GStreamer */ gst_init (&amp;argc, &amp;argv); loop = g_main_loop_new (NULL, FALSE); /* setup pipeline */ pipeline = gst_pipeline_new (\"pipeline\"); appsrc = gst_element_factory_make (\"appsrc\", \"source\"); conv = gst_element_factory_make (\"videoconvert\", \"conv\"); streammux = gst_element_factory_make (\"nvstreammux\", \"stream-muxer\"); sink = gst_element_factory_make (\"nveglglessink\", \"nvvideo-renderer\"); //videosink = gst_element_factory_make(\"appsink\",\"app-sink\"); /* setup */ g_object_set (G_OBJECT (appsrc), \"caps\", gst_caps_new_simple (\"video/x-raw\", \"format\", G_TYPE_STRING, \"RGB\", \"width\", G_TYPE_INT, 640, \"height\", G_TYPE_INT, 360, \"framerate\", GST_TYPE_FRACTION, 1, 1, NULL), NULL); gst_bin_add_many (GST_BIN (pipeline), appsrc, conv,streammux,sink,NULL); gst_element_link_many (appsrc,conv,streammux,sink ,NULL); //g_object_set (videosink, \"device\", \"/dev/video0\", NULL); /* setup appsrc */ g_object_set (G_OBJECT (appsrc), \"stream-type\", 0, \"format\", GST_FORMAT_TIME, NULL); g_signal_connect (appsrc, \"need-data\", G_CALLBACK (cb_need_data), NULL); /* play */ gst_element_set_state (pipeline, GST_STATE_PLAYING); g_main_loop_run (loop); /* clean up */ gst_element_set_state (pipeline, GST_STATE_NULL); gst_object_unref (GST_OBJECT (pipeline)); g_main_loop_unref (loop); return 0; } I am an absolutely beginner, if someone can show some code is going to be much better. Thanks.",
        "answers": [
            [
                "you need to create a pipeline as follows appsrc ! nvvideoconvert ! nvstreammux ! nvinfer ! nvvideoconvert ! nvdsosd ! nveglglessink \"appsrc\" takes your frame as input \"nvvideoconvert\" does format conversion \"nvstreammux\" multiplexes streams in case of multiple sources \"nvinfer\" does inferencing on the input stream \"nvvideoconvert\" converts frame to RGBA now \"nvdsosd\" draws bounding boxes on the frame \"nveglglessink\" displays the frame #include &lt;gst/gst.h&gt; #include &lt;gst/app/gstappsrc.h&gt; #include &lt;gst/app/gstappsink.h&gt; #include &lt;opencv2/core/core.hpp&gt; #include &lt;opencv2/core/types_c.h&gt; #include &lt;opencv2/imgproc/imgproc.hpp&gt; #include &lt;opencv2/highgui/highgui.hpp&gt; static GMainLoop *loop; #define APPSRC_WIDTH 320 #define APPSRC_HEIGHT 240 #define RUN_VIDEO 0 static void cb_need_data (GstElement *appsrc, guint unused_size, gpointer user_data) { static gboolean white = FALSE; static GstClockTime timestamp = 0; guint size,depth,height,width,step,channels; GstFlowReturn ret ; IplImage* img; guchar *data1; GstMapInfo map; cv::Mat imgMat = imread(\"/opt/nvidia/deepstream/deepstream-4.0/samples/streams/sample_720p.jpg\",cv::IMREAD_COLOR); cv::resize(imgMat, imgMat, cv::Size(APPSRC_WIDTH, APPSRC_HEIGHT)); cvtColor(imgMat,imgMat,cv::COLOR_BGR2RGBA); IplImage imgIpl = imgMat; img = &amp;imgIpl; height = img-&gt;height; width = img-&gt;width; step = img-&gt;widthStep; channels = img-&gt;nChannels; depth = img-&gt;depth; data1 = (guchar *)img-&gt;imageData; size = height*width*channels; GstBuffer *buffer = NULL;//gst_buffer_new_allocate (NULL, size, NULL); g_print(\"frame_height: %d \\n\",img-&gt;height); g_print(\"frame_width: %d \\n\",img-&gt;width); g_print(\"frame_channels: %d \\n\",img-&gt;nChannels); g_print(\"frame_size: %d \\n\",height*width*channels); buffer = gst_buffer_new_allocate (NULL, size, NULL); gst_buffer_map (buffer, &amp;map, GST_MAP_WRITE); memcpy( (guchar *)map.data, data1, gst_buffer_get_size( buffer ) ); /* this makes the image black/white */ //gst_buffer_memset (buffer, 0, white ? 0xff : 0x0, size); white = !white; GST_BUFFER_PTS (buffer) = timestamp; GST_BUFFER_DURATION (buffer) = gst_util_uint64_scale_int (1, GST_SECOND, 1); timestamp += GST_BUFFER_DURATION (buffer); //gst_app_src_push_buffer ((GstAppSrc *)appsrc, buffer); g_signal_emit_by_name (appsrc, \"push-buffer\", buffer, &amp;ret); if (ret != GST_FLOW_OK) { g_print(\"quit\"); /* something wrong, stop pushing */ g_main_loop_quit (loop); } //g_print(\"return\"); } gint main (gint argc, gchar *argv[]) { GstElement *pipeline, *appsrc, *conv, *capsfilter_converter, *videosink,*streammux, *nvinfer, *nvconv, *nvosd,*sink; GstElement *filesrc, *parser, *decoder; GstCaps * scaler_caps = NULL, *convertCaps = NULL, *nvconvert_caps; /* init GStreamer */ gst_init (&amp;argc, &amp;argv); loop = g_main_loop_new (NULL, FALSE); /* setup pipeline */ pipeline = gst_pipeline_new (\"pipeline\"); appsrc = gst_element_factory_make (\"appsrc\", \"source\"); filesrc = gst_element_factory_make (\"filesrc\", \"file-source\"); parser = gst_element_factory_make (\"h264parse\", \"parser\"); decoder = gst_element_factory_make (\"nvv4l2decoder\", \"decoder\"); conv = gst_element_factory_make (\"nvvideoconvert\", \"nv-conv-1\"); capsfilter_converter = gst_element_factory_make (\"capsfilter\", \"converter-caps\"); streammux = gst_element_factory_make (\"nvstreammux\", \"stream-muxer\"); nvinfer = gst_element_factory_make (\"nvinfer\", \"nv-infer\"); nvconv = gst_element_factory_make (\"nvvideoconvert\", \"nv-conv-2\"); nvosd = gst_element_factory_make (\"nvdsosd\", \"nv-onscreendisplay\"); sink = gst_element_factory_make (\"nveglglessink\", \"nvvideo-renderer\"); /* setup */ g_object_set (G_OBJECT (appsrc), \"caps\", gst_caps_new_simple (\"video/x-raw\", \"format\", G_TYPE_STRING, \"RGBA\", \"width\", G_TYPE_INT, APPSRC_WIDTH, \"height\", G_TYPE_INT, APPSRC_HEIGHT, \"framerate\", GST_TYPE_FRACTION, 1, 1, NULL), NULL); capsfilter_converter = gst_element_factory_make (\"capsfilter\", \"converter-caps\"); nvconvert_caps = gst_caps_new_simple (\"video/x-raw\", \"format\", G_TYPE_STRING, \"RGBA\", NULL); GstCapsFeatures *feature = NULL; feature = gst_caps_features_new (\"memory:NVMM\", NULL); gst_caps_set_features (nvconvert_caps, 0, feature); g_object_set (G_OBJECT (capsfilter_converter), \"caps\", nvconvert_caps, NULL); g_object_set (G_OBJECT (streammux), \"width\", APPSRC_WIDTH, \"height\", APPSRC_HEIGHT, \"batch-size\", 1, \"batched-push-timeout\", 5000, NULL); g_object_set (G_OBJECT (conv), \"nvbuf-memory-type\", 0, \"num-surfaces-per-frame\", 1, NULL); g_object_set (G_OBJECT (streammux), \"nvbuf-memory-type\", 0, \"num-surfaces-per-frame\", 1, NULL); g_object_set (G_OBJECT (filesrc), \"location\", \"/opt/nvidia/deepstream/deepstream-4.0/samples/streams/sample_720p.h264\", NULL); std::string config_file_path_FR = \"/opt/nvidia/deepstream/deepstream-4.0/samples/configs/deepstream-app/config_infer_primary.txt\"; g_object_set (G_OBJECT (nvinfer), \"config-file-path\", config_file_path_FR.c_str(), NULL); #if RUN_VIDEO gst_bin_add_many (GST_BIN (pipeline), filesrc, parser, decoder, conv,streammux, nvinfer, nvosd, nvconv, sink,NULL); #else gst_bin_add_many (GST_BIN (pipeline), appsrc, conv, capsfilter_converter, streammux, nvinfer, nvosd, nvconv, sink,NULL); #endif GstPad *sinkpad, *srcpad; gchar pad_name[16] = { }; g_snprintf (pad_name, 15, \"sink_%u\", 0); sinkpad = gst_element_get_request_pad (streammux, pad_name); if (!sinkpad) { g_printerr (\"Streammux request sink pad failed. Exiting.\\n\"); return -1; } #if RUN_VIDEO srcpad = gst_element_get_static_pad (decoder, \"src\"); #else srcpad = gst_element_get_static_pad (capsfilter_converter, \"src\"); #endif if (!srcpad) { g_printerr (\"Failed to get src pad of source bin. Exiting.\\n\"); return -1; } if (gst_pad_link (srcpad, sinkpad) != GST_PAD_LINK_OK) { g_printerr (\"Failed to link source bin to stream muxer. Exiting.\\n\"); return -1; } #if RUN_VIDEO gst_element_link_many (filesrc, parser, decoder, NULL); #else gst_element_link_many (appsrc,conv, capsfilter_converter, NULL); #endif gst_element_link_many (streammux, nvinfer, nvconv, nvosd, sink ,NULL); g_signal_connect (appsrc, \"need-data\", G_CALLBACK (cb_need_data), NULL); /* play */ gst_element_set_state (pipeline, GST_STATE_PLAYING); g_main_loop_run (loop); /* clean up */ gst_element_set_state (pipeline, GST_STATE_NULL); gst_object_unref (GST_OBJECT (pipeline)); g_main_loop_unref (loop); return 0; } to run inferencing for your model you need to set path to config-file for your model and set the path of image/video you want to run inferencing on. to run this on video h264 encoded video, just change #define RUN_VIDEO 0to #define RUN_VIDEO 1"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I found a guide for compiling tensorflow-1.5 with CUDA support for TX1, https://jany.st/post/2018-02-05-cross-compiling-tensorflow-for-jetson-tx1-with-bazel.html Is there a way to do the same for tf-2? I want to install a fork of tf-2 on my AGX board. The CROSSTOOL file was deprecated in tf-2 (couldn't find it in thirdparty/gpus/crosstool)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I wrote a rtsp client program using gstreamer. After running the program for a day, free command's buff/cache increased a lot. Initially, this value was about 600, which was about 4000. Why has it increased and is not released automatically? $ free total used free shared buff/cache available Mem: 7851 2162 5002 32 687 5482",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to trace memory consumption on my nvidia jetson nano with valgrind but I get: stiv@nano:~/jsoft/dgpu_core$ valgrind --tool=massif ./build/dgpu_core ==8379== Massif, a heap profiler ==8379== Copyright (C) 2003-2017, and GNU GPL'd, by Nicholas Nethercote ==8379== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info ==8379== Command: ./build/dgpu_core ==8379== valgrind: m_execontext.c:411 (record_ExeContext_wrk2): Assertion 'n_ips &gt;= 1 &amp;&amp; n_ips &lt;= VG_(clo_backtrace_size)' failed. host stacktrace: ==8379== at 0x5800A0A8: ??? (in /usr/lib/valgrind/massif-arm64-linux) sched status: running_tid=1 Thread 1: status = VgTs_Runnable (lwpid 8379) ==8379== at 0x4842E9C: malloc (in /usr/lib/valgrind/vgpreload_massif-arm64-linux.so) Note: see also the FAQ in the source distribution. It contains workarounds to several common problems. In particular, if Valgrind aborted or crashed after identifying problems in your program, there's a good chance that fixing those problems will prevent Valgrind aborting or crashing, especially if it happened in m_mallocfree.c. If that doesn't help, please report this bug to: www.valgrind.org In the bug report, send all the above text, the valgrind version, and what OS and version you are using. Thanks. What does it mean? Is there some known way how to walkaround this problem?",
        "answers": [
            [
                "Your problem is that Valgrind is seeing a stack depth that is outside of the range that it is expecting. My recommendations are Always make sure that your application runs clean with memcheck before using other Valgrind tools. Use the latest version of Valgrind (currently 3.16.1 at the time of writing, July 2020)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am currently running a program in Python to display video from a RPi Camera Module V2 on the Jetson Nano. I am using OpenCV4.1 and GStreamer 1.0. I understand that rotating the image can be done with both gstreamer and opencv, as done below: OpenCV: def rotate_image(img, angle=0): w = 1280 h = 720 center = (w / 2, h / 2) matrix = cv2.getRotationMatrix2D(center, angle, 1.0) rotated_img = cv2.warpAffine(img, matrix, (w, h), flags=cv2.INTER_LINEAR) return rotated_img # This has been tested as both a Mat (using CPU) and as a UMat (with OpenCL ==&gt; using GPU) GStreamer Pipeline: pipeline = 'nvarguscamerasrc ! video/x-raw(memory:NVMM),width=1280, height=720, framerate=30/1, format=NV12 ! nvvidconv flip-method=0 ! video/x-raw,width={}, height={},format=BGRx ! rotate angle=0.78 ! videoconvert ! video/x-raw, format=BGR ! appsink ' # The key is \"! rotate angle 0.78 !\". rotate is a 'bad' gstreamer plugin Both of these methods are entirely functional for rotating the video, but the issue is that both methods add significant latency whether done with the gstreamer rotate plugin, an OpenCV Mat, or an OpenCV UMat. Could anyone tell me if there is any way to rotate the video stream (or each individual frame) with less latency? Thanks",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using a jetson nano I tried to convert the onnx model https://github.com/onnx/models/tree/master/vision/body_analysis/emotion_ferplus Ran into this error: https://user-images.githubusercontent.com/28679735/86281506-a75e5380-bbab-11ea-8608-9bf8e2f50cc6.png Additional Info: https://user-images.githubusercontent.com/28679735/86281617-d674c500-bbab-11ea-8bbe-16f6d3db7203.png",
        "answers": [
            [
                "After you create the model use this code: TRT_LOGGER = trt.Logger(trt.Logger.WARNING) EXPLICIT_BATCH = 1 &lt;&lt; (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH) with trt.Builder(TRT_LOGGER) as builder, builder.create_network(EXPLICIT_BATCH) as network, trt.OnnxParser(network, TRT_LOGGER) as parser: with open(\"modelfile.onnx\", 'rb') as model: if not parser.parse(model.read()): for error in range(parser.num_errors): print(parser.get_error(error)) engine = builder.build_cuda_engine(network) You can use the engine directly or save and reuse it later. with open(\"output.engine\", \"wb\") as f: f.write(engine.serialize())"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm working on a Jetson Nano, and trying to install pytorch 1.4.0 onto it to run some toy experiments. However, I'm running into a lot of trouble with this. After failing to leverage the prebuilt wheels, I've gone the way of building from scratch, but after a couple hours, it fails with the following error. [3249/3931] Building NVCC (Device) object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/torch_cuda_generated_Unique.cu.o FAILED: caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/torch_cuda_generated_Unique.cu.o cd /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda &amp;&amp; /usr/bin/cmake -E make_directory /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/. &amp;&amp; /usr/bin/cmake -D verbose:BOOL=OFF -D build_configuration:STRING=Release -D generated_file:STRING=/home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/./torch_cuda_generated_Unique.cu.o -D generated_cubin_file:STRING=/home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/./torch_cuda_generated_Unique.cu.o.cubin.txt -P /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/torch_cuda_generated_Unique.cu.o.Release.cmake Killed CMake Error at torch_cuda_generated_Unique.cu.o.Release.cmake:281 (message): Error generating file /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/./torch_cuda_generated_Unique.cu.o Does anyone know how to interpret this? Did I run out of memory/swap space? Additionally, if anyone knows of an easier way to get pytorch&gt;=1.1.0 on my nano, any tips would be appreciated :) I followed this thread here both for the prebuilt installation and the scratch installation: https://forums.developer.nvidia.com/t/pytorch-for-jetson-nano-version-1-5-0-now-available/72048",
        "answers": [],
        "votes": []
    },
    {
        "question": "I was following one of the online tutorials but I was getting this error: Traceback (most recent call last): File \u201cssd_object_detection.py\u201d, line 20, in detections = net.forward() cv2.error: OpenCV(4.3.0) /home/blah/opencv/modules/dnn/src/layers/\u2026/cuda4dnn/primitives/\u2026/csl/cudnn/convolution.hpp:461: error: (-217:Gpu API call) CUDNN_STATUS_EXECUTION_FAILED in function \u2018convolve_with_bias_activation\u2019 It's a python script and I use Opencv dnn module with a pre-trained model This is my configuration: Jetson Nano device Ubuntu 18.04 /usr/local/cuda/bin/nvcc --version nvcc: NVIDIA \u00ae Cuda compiler driver Copyright \u00a9 2005-2019 NVIDIA Corporation Built on Wed_Oct_23_21:14:42_PDT_2019 Cuda compilation tools, release 10.2, V10.2.89 \u2013 NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH) \u2013 NVIDIA GPU arch: 53 \u2013 NVIDIA PTX archs: \u2013 cuDNN: YES (ver 8.0) \u2013 NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH) \u2013 NVIDIA GPU arch: 53 \u2013 cuDNN: YES (ver 8.0) opencv 4.3.0 built from source with OPENCV_DNN_CUDA=ON, CUDNN_VERSION=\u20188.0\u2019, WITH_CUDA=ON, WITH_CUDNN=ON, and many other settings enabled Python 3.7.7 This is a snippet of the code I am trying to run (it completes successfully if I don\u2019t use the GPU). It fails at the line detections = net.forward() CLASSES = [\u201cbackground\u201d, \u201caeroplane\u201d] COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3)) net = cv2.dnn.readNetFromCaffe(args[\u201cprototxt\u201d], args[\u201cmodel\u201d]) print(\"[INFO] setting preferable backend and target to CUDA\u2026\") net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA) net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA) print(\"[INFO] accessing video stream\u2026\") vs = cv2.VideoCapture(args[\u201cinput\u201d] if args[\u201cinput\u201d] else 0) writer = None fps = FPS().start() while True: (grabbed, frame) = vs.read() frame = imutils.resize(frame, width=400) (h, w) = frame.shape[:2] blob = cv2.dnn.blobFromImage(frame, 0.007843, (300, 300), 127.5) net.setInput(blob) detections = net.forward() for i in np.arange(0, detections.shape[2]): ....",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm working on a Jetson Nano, and trying to install pytorch 1.4.0 onto it to run some toy experiments. However, I'm running into a lot of trouble with this. After failing to leverage the prebuilt wheels, I've gone the way of building from scratch, but after a couple hours, it fails with the following error. [3249/3931] Building NVCC (Device) object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/torch_cuda_generated_Unique.cu.o FAILED: caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/torch_cuda_generated_Unique.cu.o cd /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda &amp;&amp; /usr/bin/cmake -E make_directory /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/. &amp;&amp; /usr/bin/cmake -D verbose:BOOL=OFF -D build_configuration:STRING=Release -D generated_file:STRING=/home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/./torch_cuda_generated_Unique.cu.o -D generated_cubin_file:STRING=/home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/./torch_cuda_generated_Unique.cu.o.cubin.txt -P /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/torch_cuda_generated_Unique.cu.o.Release.cmake Killed CMake Error at torch_cuda_generated_Unique.cu.o.Release.cmake:281 (message): Error generating file /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/./torch_cuda_generated_Unique.cu.o Does anyone know how to interpret this? Did I run out of memory/swap space? Additionally, if anyone knows of an easier way to get pytorch&gt;=1.1.0 on my nano, any tips would be appreciated :) I followed this thread here both for the prebuilt installation and the scratch installation: https://forums.developer.nvidia.com/t/pytorch-for-jetson-nano-version-1-5-0-now-available/72048",
        "answers": [],
        "votes": []
    },
    {
        "question": "I was following one of the online tutorials but I was getting this error: Traceback (most recent call last): File \u201cssd_object_detection.py\u201d, line 20, in detections = net.forward() cv2.error: OpenCV(4.3.0) /home/blah/opencv/modules/dnn/src/layers/\u2026/cuda4dnn/primitives/\u2026/csl/cudnn/convolution.hpp:461: error: (-217:Gpu API call) CUDNN_STATUS_EXECUTION_FAILED in function \u2018convolve_with_bias_activation\u2019 It's a python script and I use Opencv dnn module with a pre-trained model This is my configuration: Jetson Nano device Ubuntu 18.04 /usr/local/cuda/bin/nvcc --version nvcc: NVIDIA \u00ae Cuda compiler driver Copyright \u00a9 2005-2019 NVIDIA Corporation Built on Wed_Oct_23_21:14:42_PDT_2019 Cuda compilation tools, release 10.2, V10.2.89 \u2013 NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH) \u2013 NVIDIA GPU arch: 53 \u2013 NVIDIA PTX archs: \u2013 cuDNN: YES (ver 8.0) \u2013 NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH) \u2013 NVIDIA GPU arch: 53 \u2013 cuDNN: YES (ver 8.0) opencv 4.3.0 built from source with OPENCV_DNN_CUDA=ON, CUDNN_VERSION=\u20188.0\u2019, WITH_CUDA=ON, WITH_CUDNN=ON, and many other settings enabled Python 3.7.7 This is a snippet of the code I am trying to run (it completes successfully if I don\u2019t use the GPU). It fails at the line detections = net.forward() CLASSES = [\u201cbackground\u201d, \u201caeroplane\u201d] COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3)) net = cv2.dnn.readNetFromCaffe(args[\u201cprototxt\u201d], args[\u201cmodel\u201d]) print(\"[INFO] setting preferable backend and target to CUDA\u2026\") net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA) net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA) print(\"[INFO] accessing video stream\u2026\") vs = cv2.VideoCapture(args[\u201cinput\u201d] if args[\u201cinput\u201d] else 0) writer = None fps = FPS().start() while True: (grabbed, frame) = vs.read() frame = imutils.resize(frame, width=400) (h, w) = frame.shape[:2] blob = cv2.dnn.blobFromImage(frame, 0.007843, (300, 300), 127.5) net.setInput(blob) detections = net.forward() for i in np.arange(0, detections.shape[2]): ....",
        "answers": [],
        "votes": []
    },
    {
        "question": "This is on Ubuntu 18.04 running ROS2 Dashing. I built OpenCV from source, and did the ldconfig thing. But colcon keeps trying to use a different version. The error is \"missing: opencv_cudaarithm opencv_cudafilters\": Whole error message: robotos@jetson-agx:~/ros2_ws$ colcon build Starting &gt;&gt;&gt; opencv_demos --- stderr: opencv_demos CMake Error at /usr/share/cmake-3.10/Modules/FindPackageHandleStandardArgs.cmake:137 (message): Could NOT find OpenCV (missing: opencv_cudaarithm opencv_cudafilters) (found suitable version \"4.1.1\", minimum required is \"4\") Call Stack (most recent call first): /usr/share/cmake-3.10/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE) /usr/lib/aarch64-linux-gnu/cmake/opencv4/OpenCVConfig.cmake:328 (find_package_handle_standard_args) CMakeLists.txt:29 (find_package) And yet, there they are: robotos@jetson-agx:~/ros2_ws$ ls /usr/local/lib a.out libopencv_highgui.so.4.2.0 cmake libopencv_imgcodecs.so ... ... libopencv_core.so libopencv_optflow.so libopencv_core.so.4.2 libopencv_optflow.so.4.2 libopencv_core.so.4.2.0 libopencv_optflow.so.4.2.0 libopencv_cudaarithm.so libopencv_phase_unwrapping.so libopencv_cudaarithm.so.4.2 libopencv_phase_unwrapping.so.4.2 libopencv_cudaarithm.so.4.2.0 libopencv_phase_unwrapping.so.4.2.0 ... ... libopencv_cudafilters.so libopencv_quality.so libopencv_cudafilters.so.4.2 libopencv_quality.so.4.2 libopencv_cudafilters.so.4.2.0 libopencv_quality.so.4.2.0 4.1.1 is the version I installed through apt. 4.2 is built from source. Interestingly, if I just run cmake instead of colcon, that'll run to completion, so I guess CMake alone can find it. Although I'm unsure what to do with the resulting files, so I'd like to get colcon working. A different machine (also 18.04 with Dashing) has no problem finding the cudaarithm and cudafilters modules.",
        "answers": [
            [
                "Fixed thanks to Tsyvarev. Here's what they were suggesting, translated into a colcon-based perspective: In your project's CMakeList.txt file, add the following set(OpenCV_DIR /usr/local/lib/cmake/opencv) Ideally before the \"find_package(OpenCV...\" line. This forces colcon to use the correct OpenCV install, instead of wherever it was looking before. Unclear why some systems can figure it out and others need to be explicitly told."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "To explain my process, find below a diagram: I am working on computed tomography scanner. I use jetson TX2 for image acquisition and pre-processing. From the jetson, I control the turn table and the camera. The camera is the FSM-IMX304m. I need to access the raw pointer. For that, I need to control the camera using V4L2 (we advise not use libargus to access raw pointer, because it is store in the ISP and the ISP compress data .. Can you confirm it ?). My first problem is about the documentation about v4l2, I didn't find a clear documentation for the C++ API .. I need to control: exposure time; gain; function to clear the buffer. I found a sample on internet, see how V4L2 works : #include &lt;iostream&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;linux/ioctl.h&gt; #include &lt;linux/types.h&gt; #include &lt;linux/v4l2-common.h&gt; #include &lt;linux/v4l2-controls.h&gt; #include &lt;linux/videodev2.h&gt; #include &lt;fcntl.h&gt; #include &lt;unistd.h&gt; #include &lt;sys/ioctl.h&gt; #include &lt;sys/mman.h&gt; #include &lt;string.h&gt; #include &lt;fstream&gt; #include &lt;string&gt; using namespace std; int main() { // 1. Open the device int fd; // A file descriptor to the video device fd = open(\"/dev/video0\",O_RDWR); if(fd &lt; 0){ perror(\"Failed to open device, OPEN\"); return 1; } // 2. Ask the device if it can capture frames v4l2_capability capability; if(ioctl(fd, VIDIOC_QUERYCAP, &amp;capability) &lt; 0){ // something went wrong... exit perror(\"Failed to get device capabilities, VIDIOC_QUERYCAP\"); return 1; } // 3. Set Image format v4l2_format imageFormat; imageFormat.type = V4L2_BUF_TYPE_VIDEO_CAPTURE; imageFormat.fmt.pix.width = 1024; imageFormat.fmt.pix.height = 1024; imageFormat.fmt.pix.pixelformat = V4L2_PIX_FMT_MJPEG; imageFormat.fmt.pix.field = V4L2_FIELD_NONE; // tell the device you are using this format if(ioctl(fd, VIDIOC_S_FMT, &amp;imageFormat) &lt; 0){ perror(\"Device could not set format, VIDIOC_S_FMT\"); return 1; } // 4. Request Buffers from the device v4l2_requestbuffers requestBuffer = {0}; requestBuffer.count = 1; // one request buffer requestBuffer.type = V4L2_BUF_TYPE_VIDEO_CAPTURE; // request a buffer which we can use for capturing frames requestBuffer.memory = V4L2_MEMORY_MMAP; if(ioctl(fd, VIDIOC_REQBUFS, &amp;requestBuffer) &lt; 0){ perror(\"Could not request buffer from device, VIDIOC_REQBUFS\"); return 1; } // 5. Query the buffer to get raw data ie. ask for the you requested buffer // and allocate memory for it v4l2_buffer queryBuffer = {0}; queryBuffer.type = V4L2_BUF_TYPE_VIDEO_CAPTURE; queryBuffer.memory = V4L2_MEMORY_MMAP; queryBuffer.index = 0; if(ioctl(fd, VIDIOC_QUERYBUF, &amp;queryBuffer) &lt; 0){ perror(\"Device did not return the buffer information, VIDIOC_QUERYBUF\"); return 1; } // use a pointer to point to the newly created buffer // mmap() will map the memory address of the device to // an address in memory char* buffer = (char*)mmap(NULL, queryBuffer.length, PROT_READ | PROT_WRITE, MAP_SHARED, fd, queryBuffer.m.offset); memset(buffer, 0, queryBuffer.length); // 6. Get a frame // Create a new buffer type so the device knows which buffer we are talking about v4l2_buffer bufferinfo; memset(&amp;bufferinfo, 0, sizeof(bufferinfo)); bufferinfo.type = V4L2_BUF_TYPE_VIDEO_CAPTURE; bufferinfo.memory = V4L2_MEMORY_MMAP; bufferinfo.index = 0; // Activate streaming int type = bufferinfo.type; if(ioctl(fd, VIDIOC_STREAMON, &amp;type) &lt; 0){ perror(\"Could not start streaming, VIDIOC_STREAMON\"); return 1; } /***************************** Begin looping here *********************/ // Queue the buffer if(ioctl(fd, VIDIOC_QBUF, &amp;bufferinfo) &lt; 0){ perror(\"Could not queue buffer, VIDIOC_QBUF\"); return 1; } // Dequeue the buffer if(ioctl(fd, VIDIOC_DQBUF, &amp;bufferinfo) &lt; 0){ perror(\"Could not dequeue the buffer, VIDIOC_DQBUF\"); return 1; } // Frames get written after dequeuing the buffer cout &lt;&lt; \"Buffer has: \" &lt;&lt; (double)bufferinfo.bytesused / 1024 &lt;&lt; \" KBytes of data\" &lt;&lt; endl; // Write the data out to file ofstream outFile; outFile.open(\"webcam_output.jpeg\", ios::binary| ios::app); int bufPos = 0, outFileMemBlockSize = 0; // the position in the buffer and the amount to copy from // the buffer int remainingBufferSize = bufferinfo.bytesused; // the remaining buffer size, is decremented by // memBlockSize amount on each loop so we do not overwrite the buffer char* outFileMemBlock = NULL; // a pointer to a new memory block int itr = 0; // counts thenumber of iterations while(remainingBufferSize &gt; 0) { bufPos += outFileMemBlockSize; // increment the buffer pointer on each loop // initialise bufPos before outFileMemBlockSize so we can start // at the beginning of the buffer outFileMemBlockSize = 1024; // set the output block size to a preferable size. 1024 :) outFileMemBlock = new char[sizeof(char) * outFileMemBlockSize]; // copy 1024 bytes of data starting from buffer+bufPos memcpy(outFileMemBlock, buffer+bufPos, outFileMemBlockSize); outFile.write(outFileMemBlock,outFileMemBlockSize); // calculate the amount of memory left to read // if the memory block size is greater than the remaining // amount of data we have to copy if(outFileMemBlockSize &gt; remainingBufferSize) outFileMemBlockSize = remainingBufferSize; // subtract the amount of data we have to copy // from the remaining buffer size remainingBufferSize -= outFileMemBlockSize; // display the remaining buffer size cout &lt;&lt; itr++ &lt;&lt; \" Remaining bytes: \"&lt;&lt; remainingBufferSize &lt;&lt; endl; delete outFileMemBlock; } // Close the file outFile.close(); /******************************** end looping here **********************/ // end streaming if(ioctl(fd, VIDIOC_STREAMOFF, &amp;type) &lt; 0){ perror(\"Could not end streaming, VIDIOC_STREAMOFF\"); return 1; } close(fd); return 0; } On the jetson, the code compile perfectly, but I can't run the code. It is blocked at this step : // Dequeue the buffer if(ioctl(fd, VIDIOC_DQBUF, &amp;bufferinfo) &lt; 0){ perror(\"Could not dequeue the buffer, VIDIOC_DQBUF\"); return 1; } It is like the code is blocked in an endless loop. I have tested the code on my personal computer which runs Ubuntu 18.04, and the sample works well.",
        "answers": [
            [
                "I do not have this sensor, but I assume that: Your pixel format is incorrectly set imageFormat.fmt.pix.pixelformat = V4L2_PIX_FMT_MJPEG; Most likely there should be 12-bit raw data from this sensor V4L2_PIX_FMT_Y12 or one of these options (Mono8/10/12/16, Bayer8/10/12/16, RGB8, YUV422, YUV411). You can view the available formats in the Linux kernel here https://elixir.bootlin.com/linux/v4.9.237/source/include/uapi/linux/videodev2.h#L499 Check the documentation for your sensor. Since Nvidia developers have extended the v4l2 subsystem, you need to use the following controls to adjust exposure and gain: TEGRA_CAMERA_CID_EXPOSURE, TEGRA_CAMERA_CID_GAIN. See file tegra-v4l2-camera.h And also check the sensor controls: v4l2-ctl --list-ctrls .... gain 0x009a2009 (int64): min = 0 max = 480 step = 1 default = 0 value = 0 flags = slider exposure 0x009a200a (int64): min = 28 max = 1000000 step = 1 default = 27879 value = 28 flags = slider ..... Also examples of receiving raw data from the camera can be seen in examples from Nvidia https://docs.nvidia.com/jetson/l4t-multimedia/mmapi_build.html"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I posted about this over on the Isaac forums, but listing it here for visibility as well. I am trying to get the Isaac Realsense examples working on a Jetson Nano with my 435i (firmware downgraded to 5.11.15 per the Isaac documentation), but I've been unable to so far. I've got a Nano flashed with Jetpack4.3 and have installed all dependencies on both the desktop and the Nano. The realsense-viewer works fine, so I know the camera is functioning properly and is being detected by the Nano. However, when I run ./apps/samples/realsense_camera/realsense_camera it throws an error: ERROR engine/alice/components/Codelet.cpp@229: Component 'camera/realsense' of type 'isaac::RealsenseCamera' reported FAILURE: No device connected, please connect a RealSense device ERROR engine/alice/backend/event_manager.cpp@42: Stopping node 'camera' because it reached status 'FAILURE' I've attached the log of this output as well. I get the same error running locally on my desktop, but that's running through WSL so I was willing to write that off. Any suggestions would be greatly appreciated! 0m2020-06-15 17:18:20.620 INFO engine/alice/tools/websight.cpp@166: Loading websight...0m 33m2020-06-15 17:18:20.621 WARN engine/alice/backend/application_json_loader.cpp@174: This application does not have an explicit scheduler configuration. One will be autogenerated to the best of the system's abilities if possible.0m 0m2020-06-15 17:18:20.622 INFO engine/alice/backend/redis_backend.cpp@40: Successfully connected to Redis server. 0m 33m2020-06-15 17:18:20.623 WARN engine/alice/backend/backend.cpp@201: This application does not have an execution group configuration. One will be autogenerated to the best of the systems abilities if possible.0m 33m2020-06-15 17:18:20.623 WARN engine/gems/scheduler/scheduler.cpp@337: No default execution groups specified. Attempting to create scheduler configuration for 4 remaining cores. This may be non optimal for the system and application.0m 0m2020-06-15 17:18:20.623 INFO engine/gems/scheduler/scheduler.cpp@290: Scheduler execution groups are:0m 0m2020-06-15 17:18:20.623 INFO engine/gems/scheduler/scheduler.cpp@299: __BlockerGroup__: Cores = [3], Workers = No0m 0m2020-06-15 17:18:20.623 INFO engine/gems/scheduler/scheduler.cpp@299: __WorkerGroup__: Cores = [0, 1, 2], Workers = Yes0m 0m2020-06-15 17:18:20.660 INFO engine/alice/backend/modules.cpp@226: Loaded module 'packages/realsense/librealsense_module.so': Now has 45 components total0m 0m2020-06-15 17:18:20.679 INFO engine/alice/backend/modules.cpp@226: Loaded module 'packages/rgbd_processing/librgbd_processing_module.so': Now has 51 components total0m 0m2020-06-15 17:18:20.696 INFO engine/alice/backend/modules.cpp@226: Loaded module 'packages/sight/libsight_module.so': Now has 54 components total0m 0m2020-06-15 17:18:20.720 INFO engine/alice/backend/modules.cpp@226: Loaded module 'packages/viewers/libviewers_module.so': Now has 83 components total0m 90m2020-06-15 17:18:20.720 DEBUG engine/alice/application.cpp@348: Loaded 83 components: isaac::RealsenseCamera, isaac::alice::BufferAllocatorReport, isaac::alice::ChannelMonitor, isaac::alice::CheckJetsonPerformanceModel, isaac::alice::CheckOperatingSystem, isaac::alice::Config, isaac::alice::ConfigBridge, isaac::alice::ConfigLoader, isaac::alice::Failsafe, isaac::alice::FailsafeHeartbeat, isaac::alice::InteractiveMarkersBridge, isaac::alice::JsonToProto, isaac::alice::LifecycleReport, isaac::alice::MessageLedger, isaac::alice::MessagePassingReport, isaac::alice::NodeStatistics, isaac::alice::Pose, isaac::alice::Pose2Comparer, isaac::alice::PoseFromFile, isaac::alice::PoseInitializer, isaac::alice::PoseMessageInjector, isaac::alice::PoseToFile, isaac::alice::PoseToMessage, isaac::alice::PoseTree, isaac::alice::PoseTreeJsonBridge, isaac::alice::PoseTreeRelink, isaac::alice::ProtoToJson, isaac::alice::PyCodelet, isaac::alice::Random, isaac::alice::Recorder, isaac::alice::RecorderBridge, isaac::alice::Replay, isaac::alice::ReplayBridge, isaac::alice::Scheduling, isaac::alice::Sight, isaac::alice::SightChannelStatus, isaac::alice::Subgraph, isaac::alice::Subprocess, isaac::alice::TcpPublisher, isaac::alice::TcpSubscriber, isaac::alice::Throttle, isaac::alice::TimeOffset, isaac::alice::TimeSynchronizer, isaac::alice::UdpPublisher, isaac::alice::UdpSubscriber, isaac::map::Map, isaac::map::ObstacleAtlas, isaac::map::OccupancyGridMapLayer, isaac::map::PolygonMapLayer, isaac::map::WaypointMapLayer, isaac::navigation::DistanceMap, isaac::navigation::NavigationMap, isaac::navigation::RangeScanModelClassic, isaac::navigation::RangeScanModelFlatloc, isaac::rgbd_processing::DepthEdges, isaac::rgbd_processing::DepthImageFlattening, isaac::rgbd_processing::DepthImageToPointCloud, isaac::rgbd_processing::DepthNormals, isaac::rgbd_processing::DepthPoints, isaac::rgbd_processing::FreespaceFromDepth, isaac::sight::AliceSight, isaac::sight::SightWidget, isaac::sight::WebsightServer, isaac::viewers::BinaryMapViewer, isaac::viewers::ColorCameraViewer, isaac::viewers::DepthCameraViewer, isaac::viewers::Detections3Viewer, isaac::viewers::DetectionsViewer, isaac::viewers::FiducialsViewer, isaac::viewers::FlatscanViewer, isaac::viewers::GoalViewer, isaac::viewers::ImageKeypointViewer, isaac::viewers::LidarViewer, isaac::viewers::MosaicViewer, isaac::viewers::ObjectViewer, isaac::viewers::OccupancyMapViewer, isaac::viewers::PointCloudViewer, isaac::viewers::PoseTrailViewer, isaac::viewers::SegmentationCameraViewer, isaac::viewers::SegmentationViewer, isaac::viewers::SkeletonViewer, isaac::viewers::TensorViewer, isaac::viewers::TrajectoryListViewer, 0m 33m2020-06-15 17:18:20.723 WARN engine/alice/application.cpp@164: The function Application::findComponentByName is deprecated. Please use `getNodeComponentOrNull` instead. Note that the new method requires a node name instead of a component name. (argument: 'websight/isaac.sight.AliceSight')0m 0m2020-06-15 17:18:20.723 INFO engine/alice/application.cpp@255: Starting application 'realsense_camera' (instance UUID: 'e24992d0-af66-11ea-8bcf-c957460c567e') ...0m 90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@476: Launching 0 pre-start job(s)0m 90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@485: Replaying 0 pre-start event(s)0m 90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@476: Launching 0 pre-start job(s)0m 90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@485: Replaying 0 pre-start event(s)0m 0m2020-06-15 17:18:20.723 INFO engine/alice/backend/asio_backend.cpp@33: Starting ASIO service0m 0m2020-06-15 17:18:20.727 INFO packages/sight/WebsightServer.cpp@216: Sight webserver is loaded0m 0m2020-06-15 17:18:20.727 INFO packages/sight/WebsightServer.cpp@217: Please open Chrome Browser and navigate to http://&lt;ip address&gt;:30000m 33m2020-06-15 17:18:20.727 WARN engine/alice/backend/codelet_canister.cpp@225: Codelet 'websight/isaac.sight.AliceSight' was not added to scheduler because no tick method is specified.0m 33m2020-06-15 17:18:20.728 WARN engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m 33m2020-06-15 17:18:20.728 WARN engine/alice/backend/codelet_canister.cpp@225: Codelet '_check_operating_system/isaac.alice.CheckOperatingSystem' was not added to scheduler because no tick method is specified.0m 33m2020-06-15 17:18:20.728 WARN engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m 33m2020-06-15 17:18:20.730 WARN engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m 1;31m2020-06-15 17:18:20.741 ERROR engine/alice/components/Codelet.cpp@229: Component 'camera/realsense' of type 'isaac::RealsenseCamera' reported FAILURE: No device connected, please connect a RealSense device 0m 1;31m2020-06-15 17:18:20.741 ERROR engine/alice/backend/event_manager.cpp@42: Stopping node 'camera' because it reached status 'FAILURE'0m 33m2020-06-15 17:18:20.743 WARN engine/alice/backend/codelet_canister.cpp@225: Codelet 'camera/realsense' was not added to scheduler because no tick method is specified.0m 0m2020-06-15 17:18:21.278 INFO packages/sight/WebsightServer.cpp@113: Server connected / 10m 0m2020-06-15 17:18:30.723 INFO engine/alice/backend/allocator_backend.cpp@57: Optimized memory CPU allocator.0m 0m2020-06-15 17:18:30.724 INFO engine/alice/backend/allocator_backend.cpp@66: Optimized memory CUDA allocator.0m",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am currently trying to run Nvidia-docker on Jetson Xavier and jetson nano with the Tensorflow framework enabled inside. but the problem I\u2019m facing right now is related to \u201clibcublas.so\u201d. What I had tried the solution mentioned here: https://devtalk.nvidia.com/default/topic/1043951/jetson-agx-xavier/docker-gpu-acceleration-on-jetson-agx-for-ubuntu-18-04-image/post/5296647/#5296647 1 All package installations (pip installs and apt-get installs) completed successfully but when I try to import TensorFlow from both Python 2.7 or 3.6, I get the following error: ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory using Jetson Xavier or jetson nano?",
        "answers": [
            [
                "sudo docker run --net=host --rm --runtime nvidia --ipc=host -v /tmp/.X11-unix/:/tmp/.X11-unix /tmp/argus_socket:/tmp/argus_socket --cap-add SYS_PTRACE -e DISPLAY=$DISPLAY -it [container] I'm assuming it would work on Host first. Source: official forum but up to date. Device is no longer used"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Having an issue on migrating a Qt project from RPi4 to NVIDIA Jetson Nano. We wanted to migrate our project to Jetson Nano to improve image processing performance. The qt dependency of our project is &gt;= 5.11. But Jetson Nano uses Ubuntu 18.04.4 and it has qt5-default package pre-installed in it (in my understanding some system files use it). And this qt5-default packages version is 5.9.5. I tried to downgrade my qt dependency, but every change made lead to harder to fix issue. I tried to upgrade default qt5 version but couldn't find any similar guidance. The guides/questions already exists are about x86 etc. environment. Couldn't find any ARM based solution. The qt downloads doesn't give any buildable for ARM env (or I can't find them). The official documents only talks about cross-compiling. What should I do to overcome this issue? Thanks in advance.",
        "answers": [
            [
                "Okay I finally was able to successfully compile QT 5.12.9 on the Nano itself (no cross compilation). The steps I did: git clone https://code.qt.io/qt/qt5.git cd qt5 git checkout 5.12.9 Then git submodule update --init --recursive cd ~ mkdir qt5-build cd qt5-build Configure and build ../qt5/configure -nomake examples -nomake tests -skip qtwebengine make sudo make install Make took like nearly a whole day to compile all sources. Also I had some compilation errors before. However after skipping webengine and not building the tests and examples in ./configure I was finally able to sucessfully make it."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Every time I put it in suspend, I have to restart it. I tried clicking keyboard and mouse buttons, but no luck. I also tried pressing reset, power and recovery button on the device. Power just restarts the device instead of recovering from the suspended state. Please help.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am currently working on a Nvidia Jetson Nano. When I installed JetPack 4.4, OpenCV 4.1.1 was included inside and when I was running on my Nano's Python shell, I was able to at least run import cv2 and a few other functions without errors. However, when I was running one of my scripts, I kept encountering this certain error and I thought it was an issue with my OpenCV. This is because the tutorial that I was following, they mentioned that if ran cv2.getBuildInformation() it should include details of CUDA. At the time, my output did not contain CUDA so I thought something was wrong. Then, I've decided to reinstall OpenCV not thinking too much about it by following several tutorials, mainly the one by blogger Piggybank here. I did not uninstall anything as I thought that it would just simply overwrite it. However, the installation did not go through because it mentioned that my disk was running low on memory. I though that the entire operation would be aborted but apparently not. When I try to run import cv2 it gives me: Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; ModuleNotFoundError: No module named 'cv2' I tried to 'completely uninstall' OpenCV via pip and apt-get after but the commands returned saying that OpenCV does not exist. Not too sure what went wrong here and I don't understand I was able to run OpenCV smoothly before but now, I don't have enough memory for it. When I try to reinstall with the blog link above, it only reaches to about 50% of the entire installation process. Please help.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I tried crontab -e. @reboot python3 /home/hyebin/project/tensorrt_demos/trt_ssd.py --model ssd_mobilenet_v2_face --usb --vid 1 --width 1280 --height 780 1.Do I just have to add it to the bottom line of the crontab -e? 2. Is it okay to have additions, such as '--model'? 3. What should I do if we have such additions?",
        "answers": [
            [
                "it's better to use absolute paths in crontab rather that just python3. example: @reboot /path/to/command arg1 arg2 it should work fine with arguments such as --model to find your absolute path to python3 you can run which python3 and use that"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to create a docker build in Xavier. When I run my piece of code without docker it works smooth and I got The CUDA compiler identification. But when I am trying to make a build with dockerfile it gave me an error of CUDA compiler identification is unknown. Below is my dockerfile steps: FROM nvcr.io/nvidia/l4t-base:r32.3.1 RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends make g++ &amp;&amp; apt-get install -y cmake gcc libopenblas-dev build-essential WORKDIR /home/username/docker_fc/tensorrt_l2norm_helper CMD [\"python3\", \"./step01_pb_to_uff.py\"] COPY . /home/username/docker_fc/ RUN cmake --version RUN nvcc --version RUN mkdir build &amp;&amp; cd build &amp;&amp; pwd &amp;&amp; cmake .. &amp;&amp; make I got error in the last step with cmake. my mvcc version is release 10.0, V10.0.326. my cmake version is 3.10.2 Can anyone tell me what is missing in Dockerfile?",
        "answers": [
            [
                "The base image of l4t does not load the runtime components of nvidia by default. They only have the stubs. If you want to do this, you will need to enable the default-runtime nvidia in the /etc/docker/daemon.json file. This will load all the runtime components such as nvcc. { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } }, } Just take note that if you do this, the size of your built docker will be larger"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to connect NVIDIA Jetson Nano through serial communication with Arduino Uno via USB, so when my camera, connected to the jetson nano, detect an object the LED turn on, but it's not working. I think my arduino doesn't receive any data from the jetson. If someone can help me with suggestions, or the answer that would be great. Here is my code for arduino and for jetson nano: Arduino: char data; int LED=13; void setup() { Serial.begin(9600); pinMode(LED, OUTPUT); digitalWrite(LED, LOW); } void loop() { if (Serial.available() ) { data= Serial.read(); } if(data == 'Y' || data == 'y') { digitalWrite(LED, HIGH); delay(5000); } } Jetson Nano: #!/usr/bin/python import jetson.inference import jetson.utils import time import serial import argparse import sys # configure the serial connections (the parameters differs on the device you are connecting to) ser = serial.Serial(port='/dev/ttyUSB0',baudrate=9600) # parse the command line parser = argparse.ArgumentParser(description=\"Locate objects in a live camera stream using an object detection DNN.\", formatter_class=argparse.RawTextHelpFormatter,epilog=jetson.inference.detectNet.Usage()) parser.add_argument(\"--network\", type=str, default=\"ssd-mobilenet-v2\", help=\"pre-trained model to load (see below for options)\") parser.add_argument(\"--overlay\", type=str, default=\"box,labels,conf\", help=\"detection overlay flags (e.g. --overlay=box,labels,conf)\\nvalid combinations are: 'box', 'labels', 'conf', 'none'\") parser.add_argument(\"--threshold\", type=float, default=0.5, help=\"minimum detection threshold to use\") parser.add_argument(\"--camera\", type=str, default=\"0\", help=\"index of the MIPI CSI camera to use (e.g. CSI camera 0)\\nor for VL42 cameras, the /dev/video device to use.\\nby default, MIPI CSI camera 0 will be used.\") parser.add_argument(\"--width\", type=int, default=1280, help=\"desired width of camera stream (default is 1280 pixels)\") parser.add_argument(\"--height\", type=int, default=720, help=\"desired height of camera stream (default is 720 pixels)\") try: opt = parser.parse_known_args()[0] except: print(\"\") parser.print_help() sys.exit(0) # load the object detection network net = jetson.inference.detectNet(opt.network, sys.argv, opt.threshold) # create the camera and display camera = jetson.utils.gstCamera(opt.width, opt.height, opt.camera) display = jetson.utils.glDisplay() # process frames until user exits while display.IsOpen(): # capture the image img, width, height = camera.CaptureRGBA() # detect objects in the image (with overlay) detections = net.Detect(img, width, height, opt.overlay) # print the detections print(\"detected {:d} objects in image\".format(len(detections))) for detection in detections: print(detection) # render the image display.RenderOnce(img, width, height) # update the title bar display.SetTitle(\"{:s} | Network {:.0f} FPS\".format(opt.network, net.GetNetworkFPS())) # print out performance info net.PrintProfilerTimes() if (detections &gt; 0): ser = serial.Serial(port='/dev/ttyUSB0',baudrate=9600) time.sleep(2) print(ser) ser.write('Y')",
        "answers": [
            [
                "as alan.elkin mentioned before. You need to point out the problem. Nevertheless I would debug your problem as follows: check only the serial communication between both devices. I would advice to remove any other logic in your python script. ( Object detection with Camera) review your connection setup. Common problems are : no common GND for both devices, different logic level( most of Arduinos operate at 5V and Jetson Nano at 3.3V) review your serial communication configuration, Baudarte, parity bit.. etc If you have an Oscilloscope, check the Pins sending data . You should see the signal toggling hope this gives you a hint how to identfy your problem"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I installed OpenCV 4.1.2 from source with CUDA support. Had no issues. and created a symbolic link from OpenCV\u2019s installation directory to my virtualenv ln -s /usr/local/lib/python3.6/site-packages/cv2/python3.6/cv2.cpython-36m-aarch64-linux-gnu.so cv2.so I am having an issue with import cv2 $ python Python 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import cv2 Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; ModuleNotFoundError: No module named 'cv2' &gt;&gt;&gt; I checked site-packages directory and I can see cv2.so. I am obviously missing something. The main issue here in my view I am not able to link to my virtualenv, in fact I am able to check my installation and its working /usr/local/lib/python3.6/site-packages/cv2/python-3.6$ python Python 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import cv2 &gt;&gt;&gt;",
        "answers": [
            [
                "Issue solved a very very little mistake I changed the name from cv2.cpython-36m-aarch64-linux-gnu.so to cv2.so I realized it was an issue with one of the folders, this will do the magic: ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so notice its python-3.6 not python3.6 after cv2"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This is in the continuation to the question Facing issue while running Flask app with TensorRt model on jetson nano Above is resolve but when I am running flask 'app' it keep loading and not showing video. code: def callback(): cuda.init() device = cuda.Device(0) ctx = device.make_context() onnx_model_path = './some.onnx' fp16_mode = False int8_mode = False trt_engine_path = './model_fp16_{}_int8_{}.trt'.format(fp16_mode, int8_mode) max_batch_size = 1 engine = get_engine(max_batch_size, onnx_model_path, trt_engine_path, fp16_mode, int8_mode) context = engine.create_execution_context() inputs, outputs, bindings, stream = allocate_buffers(engine) ctx.pop() ##callback function ends worker_thread = threading.Thread(target=callback()) worker_thread.start() trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream) def do_inference(context, bindings, inputs, outputs, stream, batch_size=1): print(\"start in do_inferece\") # Transfer data from CPU to the GPU. [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs] # Run inference. print(\"before run infernce in do_inferece\") context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle) # Transfer predictions back from the GPU. print(\"before output in do_inferece\") [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs] print(\"before stream synchronize in do_inferece\") # Synchronize the stream stream.synchronize() # Return only the host outputs. print(\"before return in do_inferece\") return [out.host for out in outputs]",
        "answers": [
            [
                "Your worker_thread creates the context required for do_inference. You should call the do_inference method inside the callback() def callback(): cuda.init() device = cuda.Device(0) ctx = device.make_context() onnx_model_path = './some.onnx' fp16_mode = False int8_mode = False trt_engine_path = './model_fp16_{}_int8_{}.trt'.format(fp16_mode, int8_mode) max_batch_size = 1 engine = get_engine(max_batch_size, onnx_model_path, trt_engine_path, fp16_mode, int8_mode) context = engine.create_execution_context() inputs, outputs, bindings, stream = allocate_buffers(engine) trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream) # post-process the trt_outputs ctx.pop()"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to link OpenCV to the darknet directory. I am working with a Nvidia Jetson AGX Xavier (Ubuntu 18.04). Until recently OpenCV was installed by JetPack in the directory /usr. With this configuration the real time detection of darknet (https://github.com/AlexeyAB/darknet) with a webcam was working. I had to renew the installation of OpenCV and now its directory is home/user/OpenCV. I could not include it in the previous directory because of missing permissions. Now, every time I start the real time detection of darknet with the command ./darknet detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0 I recieve this error log: ./darknet: error while loading shared libraries: libopencv_highgui.so.3.3: cannot open shared object file: No such file or directory When I recompile darknet (cmake, make, install) I receive another error message: Demo needs OpenCV for webcam images. So I assume, that OpenCV is not included in the compilation and darknet does not find the libraries. I tested this options: In the Makefile of darknet OPENCV=1 is included (double checked, totally sure, that this is not the fault) I tried to export the path of OpenCV to its previous path e.g. with export OpenCV_DIR=/usr/share/OpenCV I tried to link the path e.g. with LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib or LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/share I think there could be a possibility to include the OpenCV path in some file related to cmake. In the CmakeLists.txt file is the line set(PThreads_windows_DIR ${CMAKE_CURRENT_LIST_DIR}/3rdparty/pthreads CACHE PATH \"Path where pthreads for windows can be located\") endif() set(Stb_DIR ${CMAKE_CURRENT_LIST_DIR}/3rdparty/stb CACHE PATH \"Path where Stb image library can be located\") set(CMAKE_DEBUG_POSTFIX d) set(CMAKE_THREAD_PREFER_PTHREAD ON) find_package(Threads REQUIRED) if(MSVC) find_package(PThreads_windows REQUIRED) endif() if(ENABLE_OPENCV) if(OpenCV_FOUND) if(SELECT_OPENCV_MODULES) I think here or maybe somewhere else I should include the new directory path to OpenCV. I tried with find_package or set_path, but I am not sure if this was the right point to enter this option. Also I have no file called finOpencv.cmake. Does someone have an idea, how i could 'tell' darknet, where OpenCV is located? If you need further information, I will be happy to share. Any help is appreciated. Thanks! Edit: If I include Find_Package(OpenCV) in CMakeLists.txt and recompile, I recieve this output: CMake Warning at CMakeLists.txt:96 (find_package): By not providing \"FindOpenCV.cmake\" in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by \"OpenCV\", but CMake did not find one. Could not find a package configuration file provided by \"OpenCV\" with any of the following names: OpenCVConfig.cmake opencv-config.cmake Add the installation prefix of \"OpenCV\" to CMAKE_PREFIX_PATH or set \"OpenCV_DIR\" to a directory containing one of the above files. If \"OpenCV\" provides a separate development package or SDK, be sure it has been installed.",
        "answers": [
            [
                "Edit: If I include Find_Package(OpenCV) in CMakeLists.txt and recompile, I recieve this output It's good. And as a next step in darknet build directory: cmake . -DOpenCV_DIR=/usr/share/OpenCV"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "while finding a number of examples on how to setup gpio using GPIO.BCM mode (in which pins numbers are equivalent to RPI (see image and nvidia/jetson-gpio), I could not find an example to setup pins in GPIO.TEGRA_SOC mode. so after digging into the gpio library I thought I would share this in an orderly fashion. see answer below.",
        "answers": [
            [
                "BCM mode, defining pins 9 (signal from nano out. e.g. trigger) and 11 (signal from sensor into the nan0) - import Jetson.GPIO as GPIO GPIO.setmode(GPIO.BCM) GPIO.setup(9, GPIO.OUT) GPIO.setup(11, GPIO.IN) after setup, getting value from input pin - GPIO.input(11) after setup, setting value from output pin - GPIO.output(9, False) GPIO.output(9, True) TEGRA_SOC mode, defining pins 9 and 11 as before. replace number with identifying string names {9: 'SPI1_MISO', 11: 'SPI1_SCK'} import Jetson.GPIO as GPIO GPIO.setmode(GPIO.TEGRA_SOC) GPIO.setup('SPI1_MISO', GPIO.OUT) GPIO.setup('SPI1_SCK', GPIO.IN) after setup, getting value from input pin - GPIO.input('SPI1_SCK') after setup, setting value from output pin - GPIO.output('SPI1_MISO', False) GPIO.output('SPI1_MISO', True) in general, to print out naming of all gpio pins in each possible mode: ['BOARD', 'BCM', 'CVM', 'TEGRA_SOC'] use - print(GPIO.gpio_pin_data.get_data()[-1]) to create a nice dictionary converting bcm num to tegra use - bcm_to_tegra = { k: list(GPIO.gpio_pin_data.get_data()[-1]['TEGRA_SOC'].keys())[i] for i, k in enumerate(GPIO.gpio_pin_data.get_data()[-1]['BCM'])} for k, v in bcm_to_tegra.items(): print('bcm #:', k, 'tegra:', v) // output: bcm #: 4 tegra: AUD_MCLK bcm #: 17 tegra: UART2_RTS bcm #: 18 tegra: DAP4_SCLK bcm #: 27 tegra: SPI2_SCK bcm #: 22 tegra: LCD_TE bcm #: 23 tegra: SPI2_CS1 bcm #: 24 tegra: SPI2_CS0 bcm #: 10 tegra: SPI1_MOSI bcm #: 9 tegra: SPI1_MISO bcm #: 25 tegra: SPI2_MISO bcm #: 11 tegra: SPI1_SCK bcm #: 8 tegra: SPI1_CS0 bcm #: 7 tegra: SPI1_CS1 bcm #: 5 tegra: CAM_AF_EN bcm #: 6 tegra: GPIO_PZ0 bcm #: 12 tegra: LCD_BL_PW bcm #: 13 tegra: GPIO_PE6 bcm #: 19 tegra: DAP4_FS bcm #: 16 tegra: UART2_CTS bcm #: 26 tegra: SPI2_MOSI bcm #: 20 tegra: DAP4_DIN bcm #: 21 tegra: DAP4_DOUT"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "How to auto-run python script made by me when the system is booted on jetson nano?",
        "answers": [
            [
                "Step 1: Create a shell file. sudo nano /usr/local/bin/startup.sh: Type this on the terminal. A new sh file is created. This file consists of the python file that is to be executed. I gave the name startup.sh. It can be any name XYZ.sh #! /bin/sh: This is called the shebang. This would execute our script using a Bourne shell. This tells the system that the commands in this file should be fed to the interpreter. sleep 10: This pauses a script for a certain amount of time. He re we pause it for 10 seconds. In the next line, we insert the code that we use to run the program on the terminal. OPENBLAS_CORETYPE=ARMV8 /usr/bin/python3 path/of/the/python/code.py It looks like this: #! /bin/sh sleep 10 OPENBLAS_CORETYPE=ARMV8 /usr/bin/python3 /home/sooraj/Downloads/incabin-monitoring-system-main/netstreamfeb17.py Now we close the file using Ctrl+X. and save it. Step 2: Create a service file sudo nano /etc/systemd/system/startup.service Things to write inside it. [Unit] Description = INTU_IPC start-uo specific script [Service] Type= idle ExecStartPre = /bin/sleep 10 ExecStart = /usr/local/bin/startup.sh User=sooraj# write your user name here [Install] WantedBy = multi-user.target Now we close the file using Ctrl+X. and save it. step 3: Give permission. sudo chmod +x /usr/local/bin/startup.sh step 4: enable, start and stop sudo systemctl enable startup.service sudo systemctl start startup.service sudo systemctl stop.service After starting, to view if it works, we can observe it in the terminal by journalctl -u startup.service -f If we edit the service file for the next time, we need to reload it before enabling and starting. sudo systemctl daemon-reload sudo systemctl enable startup.service sudo systemctl start startup.service Additional information. systemctl is-enabled startup.service #checks if the service file is enabled. systemctl is-failed startup.service #checks if the service file failed to start. systemctl is-active startup.service #checks if the service file is active. sudo systemctl list-unit-files \u2014 type=service #display the list of service files."
            ],
            [
                "Try StartupApplications and add your python execution shell command with proper path. An even better solution will be to use crontab. crontab -e Add @reboot python path/to/script so that the script gets executed every time you reboot. This link might help you."
            ],
            [
                "As an alternative to systemd or crontab, you can try pm2. It's very easy to configure and use. Just follow a quick start guide. Or just test it the following way: pm2 start app.py pm2 save Note that you should initially generate a startup script to make it work on boot."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am working with jetson TX2. I capture images from camera, as unsigned char *image. Then, I need to do some image processing. For that, I use the GPU. With the jetson TX2, we can avoid the transfer of data host/device and device/host because the RAM is shared between the GPU and the CPU. For that, I use : int height = 6004 ; int width = 7920 ; int NumElement = height*width ; unsigned char *img1 ; cudaMallocManaged(&amp;img1, NumElement*sizeof(unsigned char)); Using that method, there is no limitation with the PCI. My problem is how assign the image from the buffer, to img1. This method works, but it is too long : for(int i =0 ; i&lt;NumElement ; i++) img[i] = buffer[i] ; I loose the advantage of the GPU using naive for loop ... And I if just use that method : img = buffer Like that, I have a problem when I enter in the kernel .",
        "answers": [
            [
                "Use cudaMemcpy with cudaMemcpyDefault, something like cudaMemcpy(&amp;buffer[0], &amp;img[0], NumElement * sizeof(char), cudaMemcpyDefault); You could also potentially use memcpy"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am currently trying to run object detection on Jetson Nano with sony IMX219 camera https://www.waveshare.com/imx219-170-camera.htm I am trying to run darknet with camera 0: ./darknet detector demo cfg/coco_znacenie.data cfg/yolov3-tiny_znacenie.cfg znacenie1.weights -c 0 And output is: CUDA-version: 10000 (10000), cuDNN: 7.6.3, GPU count: 1 OpenCV version: 4.1.1 Demo compute_capability = 530, cudnn_half = 0 net.optimized_memory = 0 mini_batch = 1, batch = 2, time_steps = 1, train = 0 layer filters size/strd(dil) input output 0 conv 16 3 x 3/ 1 416 x 416 x 3 -&gt; 416 x 416 x 16 0.150 BF 1 max 2x 2/ 2 416 x 416 x 16 -&gt; 208 x 208 x 16 0.003 BF 2 conv 32 3 x 3/ 1 208 x 208 x 16 -&gt; 208 x 208 x 32 0.399 BF 3 max 2x 2/ 2 208 x 208 x 32 -&gt; 104 x 104 x 32 0.001 BF 4 conv 64 3 x 3/ 1 104 x 104 x 32 -&gt; 104 x 104 x 64 0.399 BF 5 max 2x 2/ 2 104 x 104 x 64 -&gt; 52 x 52 x 64 0.001 BF 6 conv 128 3 x 3/ 1 52 x 52 x 64 -&gt; 52 x 52 x 128 0.399 BF 7 max 2x 2/ 2 52 x 52 x 128 -&gt; 26 x 26 x 128 0.000 BF 8 conv 256 3 x 3/ 1 26 x 26 x 128 -&gt; 26 x 26 x 256 0.399 BF 9 max 2x 2/ 2 26 x 26 x 256 -&gt; 13 x 13 x 256 0.000 BF 10 conv 512 3 x 3/ 1 13 x 13 x 256 -&gt; 13 x 13 x 512 0.399 BF 11 max 2x 2/ 1 13 x 13 x 512 -&gt; 13 x 13 x 512 0.000 BF 12 conv 1024 3 x 3/ 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BF 13 conv 256 1 x 1/ 1 13 x 13 x1024 -&gt; 13 x 13 x 256 0.089 BF 14 conv 512 3 x 3/ 1 13 x 13 x 256 -&gt; 13 x 13 x 512 0.399 BF 15 conv 18 1 x 1/ 1 13 x 13 x 512 -&gt; 13 x 13 x 18 0.003 BF 16 yolo [yolo] params: iou loss: mse (2), iou_norm: 0.75, cls_norm: 1.00, scale_x_y: 1.00 17 route 13 -&gt; 13 x 13 x 256 18 conv 128 1 x 1/ 1 13 x 13 x 256 -&gt; 13 x 13 x 128 0.011 BF 19 upsample 2x 13 x 13 x 128 -&gt; 26 x 26 x 128 20 route 19 8 -&gt; 26 x 26 x 384 21 conv 256 3 x 3/ 1 26 x 26 x 384 -&gt; 26 x 26 x 256 1.196 BF 22 conv 18 1 x 1/ 1 26 x 26 x 256 -&gt; 26 x 26 x 18 0.006 BF 23 yolo [yolo] params: iou loss: mse (2), iou_norm: 0.75, cls_norm: 1.00, scale_x_y: 1.00 Total BFLOPS 5.448 avg_outputs = 324846 Allocate additional workspace_size = 13.11 MB Loading weights from znacenie1.weights... seen 64, trained: 320 K-images (5 Kilo-batches_64) Done! Loaded 24 layers from weights-file Webcam index: 0 [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (1757) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module v4l2src0 reported: Internal data stream error. [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (886) open OpenCV | GStreamer warning: unable to start pipeline [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created Video stream: 3264 x 2464 But input from camera is green: Camera is working when I run it by: gst-launch-1.0 nvarguscamerasrc sensor_mode=0 ! 'video/x-raw(memory:NVMM),width=3280, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=0 ! 'video/x-raw,width=3280, height=2464' ! nvvidconv flip-method=2 ! nvegltransform ! nveglglessink -e I was trying to test my camera in chromium but chromium wasn't able to detect camera: I am using clean installation of latest image for my Jetson Nano and everything is up to date. I've already tried different versions of OpenCv but without any progress. Anybody know what should I try? EDIT: Just found how to solve it: ./darknet detector demo cfg/coco_znacenie.data cfg/yolov3-tiny_znacenie.cfg znacenie1.weights \"nvarguscamerasrc ! video/x-raw(memory:NVMM), width=1920, height=1080, format=(string)NV12, framerate=(fraction)30/1 ! nvvidconv flip-method=2 ! video/x-raw, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink\"",
        "answers": [],
        "votes": []
    },
    {
        "question": "I need to install Opencv inside container and this is my dockerfile: FROM arm32v7/python:3.7-slim-buster WORKDIR /app RUN apt-get update RUN apt-get install -y make automake gcc g++ subversion python3-dev RUN pip install numpy==1.14.6 --no-binary numpy --force-reinstall RUN pip install python-opencv COPY requirements.txt ./ RUN pip install -r requirements.txt COPY . . EXPOSE 5020 And this is the error: ModuleNotFoundError: No module named 'cv2' I tried almost everything: apt-get install, pip3 install, pip install. Please help",
        "answers": [],
        "votes": []
    },
    {
        "question": "We have been struggling with this very weird issue when upgrading from C++14 to C++17 (Ubuntu 18.04, GCC 7.5.0). The toolchain is Linaro's on Jetson TX2 and is the default. Background: We have a C++ application A that uses algorithms from library L also developed by us running on Ubuntu 18.04. Builds and extensive system tests have been running for two years on Intel and on Jetson TX2. Now we decided to upgrade to C++17 (-std=c++1z with GCC). We first built L with C++17 enabled and everything first seemed to work fine, but then we noticed that some test runs started to act weirdly on ARM only. Like 2 tests out of 30 and this was deterministic(!). We then started to investigate and noticed that one constructor in the library that accepted const std::pair&lt;float, float&gt; &amp; got somehow corrupted data. Inside constructor .first seemed to be .second and .second was always 0. Something weird like this. So this happens if A is still on C++14 and L is on C++17. Ok. Then we tried this the other way around. Lon C++14 and the application A on C++17. The results were similar. Some tests started to fail (not the same though) and it was deterministic. The root cause was again the same: somehow std::pair&lt;float, float&gt; in the API gets messed up. So the combinations so far are like this: A: C++14, L: C++14, Intel =&gt; OK A: C++14, L: C++17, Intel =&gt; OK A: C++17, L: C++14, Intel =&gt; OK A: C++17, L: C++17, Intel =&gt; OK A: C++14, L: C++14, ARM =&gt; OK A: C++14, L: C++17, ARM =&gt; FAIL A: C++17, L: C++14, ARM =&gt; FAIL A: C++17, L: C++17, ARM =&gt; OK Apparently this is a big commercial application so I cannot just copy-paste code here. I first suspected this would be a compiler bug (what it still might be), but it just would seem to be too obvious! And there's more: We also recently noticed that if we just replace the const std::pair&lt;float, float&gt; &amp; with just plain float arguments the tests are passing again. Any guesses what the hell is going on? A compiler bug? How the switch to C++17 would even in theory cause anything like this (the compiler is exactly the same)? And especially like this (doesn't matter which component is upgraded). We just fail to find anything wrong with the API. It has been working almost two years without any issues on Intel and ARM with C++14. EDIT: Managed to make a working example project: https://drive.google.com/open?id=1B5SceFB1mKkCnE8iE59Mq0lScK2F0iOl Instructions and example outputs in README.md Outputs from this example on Intel and on Jetson TX2: On Intel (Ubuntu 18.04, GCC 7.5.0) this app prints: $ ./app/App S: 42 L: 3.14 R: 666 In Foo::update(): s: 42 In Foo::update(): l: 3.14 In Foo::update(): r: 666 On Jetson TX2 (Ubuntu 18.04, GCC 7.5.0 / Linaro) this app prints: $ ./app/App S: 42 L: 0 R: 2.39152e+29 In Foo::update(): s: 42 In Foo::update(): l: 0 In Foo::update(): r: 2.39152e+29",
        "answers": [
            [
                "I don't know anything for sure since I haven't looked, but this sounds like a case of the binary interface changing. The ABI. This could happen because of a structure layout change, maybe part of the effort to unify pairs and tuples. It could also be a change in padding rules. Or alignment rules. Suddenly thinking that's the most likely one. If it allocated using float alignment vs double alignment or one side decided to use 64-bit alignment for everything, that would definitely cause weird things. Passing by reference passes a pointer in the implementation. Usually. So if the structure changes between C++ versions, it can have a different byte layout. This may be an accident in the ARM compilers, because if the ABI changed on purpose there would have been some effort to put it into a new namespace like was done for the C++11 std::string in the GNU libc++. I would test some of this by making structs and arrays of std::pairs in each compiler version and dump them to disk files or examine them in a debugger. See what bytes change."
            ],
            [
                "How the switch to C++17 would even in theory cause anything like this (the compiler is exactly the same)? There are LOADS of ways it could change something in theory. The most straightforward is that the standard library headers have lots of conditional compilation with things like: #if __cplusplus &lt;= 201402L /* code for C++14 ... */ #else /* code for C++17 ... */ #endif All it takes is for the two bits of code to be incompatible. We try pretty hard to ensure that doesn't happen. But in theory it can happen. We then started to investigate and noticed that one constructor in the library that accepted const std::pair&lt;float, float&gt; &amp; got somehow corrupted data. Inside constructor .first seemed to be .second and .second was always 0. Something weird like this. I'm unable to reproduce anything like this. When I examine the assembly generated by GCC 7.3 for Aarch64 the results are identical for C++14 and C++17. So you'll need to provide more information about your code. It shouldn't be hard to show the constructor signature and the data members of the constructor, without needing to show big chunks of proprietary code. Edit: I've reduced the working example to this live example which shows the generated code for a class with an empty base is different for C++14 and C++17, which is a compiler bug: https://godbolt.org/z/E46NFc I've reported as https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94383"
            ]
        ],
        "votes": [
            4.0000001,
            4.0000001
        ]
    },
    {
        "question": "Has anyone used Tensorflow Lite on any Nvidia Jetson product? I want to use my Jetson Nano for inference and would like to so with tf-lite utilizing the GPU. Confusingly, there does not seem to be a Python API for creating a GPU Delegate in tf-lite. Is there are clear reason for this? Is the alternative to use the full Tensorflow library (I would prefer not use the Nvidia TensorRT engine)?",
        "answers": [
            [
                "Yes, I have tried to use tf lite on Jetson Nano before. You can refer to my previous article on Medium (PS: I am sorry that the article was written in Chinese.) This article is about how to set up the TF Lite Environment on Jetson Nano Notice\uff1a You should change the following command according to your own environment. pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-linux_aarch64.whl Setting up TF lite on Jetson Nano: https://yanwei-liu.medium.com/tflite-on-jetson-nano-c480fdf9ac2"
            ],
            [
                "in case it is of interest to you to use inference with C++ you can compile TFlite 2.4.1 on your Jetson device like I did on the Xavier NX: $ sudo apt-get install cmake curl $ wget -O tensorflow.zip https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip $ unzip tensorflow.zip $ mv tensorflow-2.4.1 tensorflow $ cd tensorflow $ ./tensorflow/lite/tools/make/download_dependencies.sh $ ./tensorflow/lite/tools/make/build_aarch64_lib.sh After that you will also have to install the TF lite flat buffers like this: $ cd ./tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers $ mkdir build &amp;&amp; cd build $ cmake .. $ make -j $ sudo make install $ sudo ldconfig After that you find the library here tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/libtensorflow-lite.a You can build your inference application against that like this gcc -llibtensorflow-lite.a -ledgetpu main.cpp You will also need to install libedgetpu.so like shown on Coral.ai Best Alexander"
            ]
        ],
        "votes": [
            4.0000001,
            1e-07
        ]
    },
    {
        "question": "I need to run my model in NVIDIA JETSON T2, So I converted my working yoloV3 model into tensorRT(.trt format)(https://towardsdatascience.com/have-you-optimized-your-deep-learning-model-before-deployment-cdc3aa7f413d)This link mentioned helped me to convert the Yolo model into .trt .But after converting the model to .trt model I needed to test if it works fine (i.e) If the detection is good enough. I couldn't find any sample code for loading and testing .trt model. If anybody can help me , please pull up a sample code in the answer section or any link for reference.",
        "answers": [
            [
                "You can load and perform the inference of your TRT Model using this snippet of code. This is executed in Tensorflow 2.1.0 and Google Colab Environment. from tensorflow.python.compiler.tensorrt import trt_convert as trt from tensorflow.python.saved_model import tag_constants saved_model_loaded = tf.saved_model.load(output_saved_model_dir, tags=[tag_constants.SERVING]) signature_keys = list(saved_model_loaded.signatures.keys()) print(signature_keys) # Outputs : ['serving_default'] graph_func = saved_model_loaded.signatures[signature_keys[0]] graph_func(x_test) # Use this to perform inference output_saved_model_dir is the location of your TensorRT Optimized model in SavedModel format. From here, you can add your testing methods to determine the performance of your pre and post-processed model. EDIT: import tensorflow as tf from tensorflow.python.compiler.tensorrt import trt_convert as trt import numpy as np conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS conversion_params = conversion_params._replace(max_workspace_size_bytes=(1&lt;&lt;32)) conversion_params = conversion_params._replace(precision_mode=\"FP16\") conversion_params = conversion_params._replace(maximum_cached_engines=100) converter = trt.TrtGraphConverterV2( input_saved_model_dir=input_saved_model_dir, conversion_params=conversion_params) converter.convert() converter.save(output_saved_model_dir) Here are the codes used for Converting and Saving the Tensorflow RT Optimized model."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am quite new to operating a linux system so please bear with me. I tried installing Tensorflow on my Jetson according to this guide. Unfortunately at step 2.4 $ sudo pip3 install -U numpy==1.16.1 future==0.17.1 mock==3.0.5 h5py==2.9.0 keras_preprocessing==1.0.5 keras_applications==1.0.8 gast==0.2.2 futures protobuf pybind11 i first get this output, Collecting numpy==1.16.1 Using cached numpy-1.16.1.zip (5.1 MB) Processing /root/.cache/pip/wheels/c1/68/2b/e3decbcfd5353f4a661ffa73d73894b070ef21427a8bee82fd/future-0.17.1-py3-none-any.whl Collecting mock==3.0.5 Using cached mock-3.0.5-py2.py3-none-any.whl (25 kB) Collecting h5py==2.9.0 Using cached h5py-2.9.0.tar.gz (287 kB) but then the console freezes up for 2-4 minutes and afterwards i get this huge error message which i do not understand. Could anyone explain to me what's wrong and what i can do to fix the issue? ERROR: Command errored out with exit status 1: command: /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-f4yysrhz/h5py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-f4yysrhz/h5py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-install-f4yysrhz/h5py/pip-egg-info cwd: /tmp/pip-install-f4yysrhz/h5py/ Complete output (298 lines): ERROR: Command errored out with exit status 1: command: /usr/bin/python3 /usr/local/lib/python3.5/dist-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpt471vgtd cwd: /tmp/pip-wheel-sfv14zjb/numpy Complete output (262 lines): Processing numpy/random/_bounded_integers.pxd.in Processing numpy/random/_pcg64.pyx Processing numpy/random/_sfc64.pyx Processing numpy/random/_mt19937.pyx Processing numpy/random/_bounded_integers.pyx.in Processing numpy/random/_generator.pyx Processing numpy/random/mtrand.pyx Processing numpy/random/_bit_generator.pyx Processing numpy/random/_common.pyx Processing numpy/random/_philox.pyx Cythonizing sources blas_opt_info: blas_mkl_info: customize UnixCCompiler libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE blis_info: libraries blis not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE openblas_info: libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE atlas_3_10_blas_threads_info: Setting PTATLAS=ATLAS libraries tatlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE atlas_3_10_blas_info: libraries satlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE atlas_blas_threads_info: Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE atlas_blas_info: libraries f77blas,cblas,atlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE accelerate_info: NOT AVAILABLE blas_info: libraries blas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE blas_src_info: NOT AVAILABLE NOT AVAILABLE /bin/sh: 1: svnversion: not found non-existing path in 'numpy/distutils': 'site.cfg' lapack_opt_info: lapack_mkl_info: libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE openblas_lapack_info: libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE openblas_clapack_info: libraries openblas,lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE flame_info: libraries flame not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE atlas_3_10_threads_info: Setting PTATLAS=ATLAS libraries lapack_atlas not found in /usr/local/lib libraries tatlas,tatlas not found in /usr/local/lib libraries lapack_atlas not found in /usr/lib libraries tatlas,tatlas not found in /usr/lib libraries lapack_atlas not found in /usr/lib/aarch64-linux-gnu libraries tatlas,tatlas not found in /usr/lib/aarch64-linux-gnu &lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&gt; NOT AVAILABLE atlas_3_10_info: libraries lapack_atlas not found in /usr/local/lib libraries satlas,satlas not found in /usr/local/lib libraries lapack_atlas not found in /usr/lib libraries satlas,satlas not found in /usr/lib libraries lapack_atlas not found in /usr/lib/aarch64-linux-gnu libraries satlas,satlas not found in /usr/lib/aarch64-linux-gnu &lt;class 'numpy.distutils.system_info.atlas_3_10_info'&gt; NOT AVAILABLE atlas_threads_info: Setting PTATLAS=ATLAS libraries lapack_atlas not found in /usr/local/lib libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib libraries lapack_atlas not found in /usr/lib libraries ptf77blas,ptcblas,atlas not found in /usr/lib libraries lapack_atlas not found in /usr/lib/aarch64-linux-gnu libraries ptf77blas,ptcblas,atlas not found in /usr/lib/aarch64-linux-gnu &lt;class 'numpy.distutils.system_info.atlas_threads_info'&gt; NOT AVAILABLE atlas_info: libraries lapack_atlas not found in /usr/local/lib libraries f77blas,cblas,atlas not found in /usr/local/lib libraries lapack_atlas not found in /usr/lib libraries f77blas,cblas,atlas not found in /usr/lib libraries lapack_atlas not found in /usr/lib/aarch64-linux-gnu libraries f77blas,cblas,atlas not found in /usr/lib/aarch64-linux-gnu &lt;class 'numpy.distutils.system_info.atlas_info'&gt; NOT AVAILABLE lapack_info: libraries lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/aarch64-linux-gnu'] NOT AVAILABLE lapack_src_info: NOT AVAILABLE NOT AVAILABLE running dist_info running build_src build_src building py_modules sources creating build creating build/src.linux-aarch64-3.5 creating build/src.linux-aarch64-3.5/numpy creating build/src.linux-aarch64-3.5/numpy/distutils building library \"npymath\" sources Could not locate executable gfortran Could not locate executable f95 Could not locate executable ifort Could not locate executable ifc Could not locate executable lf95 Could not locate executable pgfortran Could not locate executable f90 Could not locate executable f77 Could not locate executable fort Could not locate executable efort Could not locate executable efc Could not locate executable g77 Could not locate executable g95 Could not locate executable pathf95 Could not locate executable nagfor don't know how to compile Fortran code on platform 'posix' creating build/src.linux-aarch64-3.5/numpy/core creating build/src.linux-aarch64-3.5/numpy/core/src creating build/src.linux-aarch64-3.5/numpy/core/src/npymath conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/npymath/npy_math_internal.h adding 'build/src.linux-aarch64-3.5/numpy/core/src/npymath' to include_dirs. conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/npymath/ieee754.c conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/npymath/npy_math_complex.c None - nothing done with h_files = ['build/src.linux-aarch64-3.5/numpy/core/src/npymath/npy_math_internal.h'] building library \"npysort\" sources creating build/src.linux-aarch64-3.5/numpy/core/src/common conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/common/npy_sort.h adding 'build/src.linux-aarch64-3.5/numpy/core/src/common' to include_dirs. creating build/src.linux-aarch64-3.5/numpy/core/src/npysort conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/npysort/quicksort.c conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/npysort/mergesort.c conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/npysort/timsort.c conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/npysort/heapsort.c conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/npysort/radixsort.c conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/common/npy_partition.h conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/npysort/selection.c conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/common/npy_binsearch.h conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/npysort/binsearch.c None - nothing done with h_files = ['build/src.linux-aarch64-3.5/numpy/core/src/common/npy_sort.h', 'build/src.linux-aarch64-3.5/numpy/core/src/common/npy_partition.h', 'build/src.linux-aarch64-3.5/numpy/core/src/common/npy_binsearch.h'] building extension \"numpy.core._multiarray_tests\" sources creating build/src.linux-aarch64-3.5/numpy/core/src/multiarray conv_template:&gt; build/src.linux-aarch64-3.5/numpy/core/src/multiarray/_multiarray_tests.c building extension \"numpy.core._multiarray_umath\" sources Running from numpy source directory. setup.py:461: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates run_build = parse_setuppy_commands() /tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/system_info.py:1896: UserWarning: Optimized (vendor) Blas libraries are not found. Falls back to netlib Blas library which has worse performance. A better performance should be easily gained by switching Blas library. if self._calc_info(blas): /tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/system_info.py:1896: UserWarning: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable. if self._calc_info(blas): /tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/system_info.py:1896: UserWarning: Blas (http://www.netlib.org/blas/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [blas_src]) or by setting the BLAS_SRC environment variable. if self._calc_info(blas): /tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/system_info.py:1730: UserWarning: Lapack (http://www.netlib.org/lapack/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [lapack]) or by setting the LAPACK environment variable. return getattr(self, '_calc_info_{}'.format(name))() /tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/system_info.py:1730: UserWarning: Lapack (http://www.netlib.org/lapack/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [lapack_src]) or by setting the LAPACK_SRC environment variable. return getattr(self, '_calc_info_{}'.format(name))() /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'define_macros' warnings.warn(msg) Traceback (most recent call last): File \"/usr/local/lib/python3.5/dist-packages/pip/_vendor/pep517/_in_process.py\", line 257, in &lt;module&gt; main() File \"/usr/local/lib/python3.5/dist-packages/pip/_vendor/pep517/_in_process.py\", line 240, in main json_out['return_val'] = hook(**hook_input['kwargs']) File \"/usr/local/lib/python3.5/dist-packages/pip/_vendor/pep517/_in_process.py\", line 110, in prepare_metadata_for_build_wheel return hook(metadata_directory, config_settings) File \"/usr/local/lib/python3.5/dist-packages/setuptools/build_meta.py\", line 158, in prepare_metadata_for_build_wheel self.run_setup() File \"/usr/local/lib/python3.5/dist-packages/setuptools/build_meta.py\", line 250, in run_setup self).run_setup(setup_script=setup_script) File \"/usr/local/lib/python3.5/dist-packages/setuptools/build_meta.py\", line 143, in run_setup exec(compile(code, __file__, 'exec'), locals()) File \"setup.py\", line 488, in &lt;module&gt; setup_package() File \"setup.py\", line 480, in setup_package setup(**metadata) File \"/tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/core.py\", line 171, in setup return old_setup(**new_attr) File \"/usr/local/lib/python3.5/dist-packages/setuptools/__init__.py\", line 144, in setup return distutils.core.setup(**attrs) File \"/usr/lib/python3.5/distutils/core.py\", line 148, in setup dist.run_commands() File \"/usr/lib/python3.5/distutils/dist.py\", line 955, in run_commands self.run_command(cmd) File \"/usr/lib/python3.5/distutils/dist.py\", line 974, in run_command cmd_obj.run() File \"/usr/local/lib/python3.5/dist-packages/setuptools/command/dist_info.py\", line 31, in run egg_info.run() File \"/tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/command/egg_info.py\", line 26, in run self.run_command(\"build_src\") File \"/usr/lib/python3.5/distutils/cmd.py\", line 313, in run_command self.distribution.run_command(command) File \"/usr/lib/python3.5/distutils/dist.py\", line 974, in run_command cmd_obj.run() File \"/tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/command/build_src.py\", line 146, in run self.build_sources() File \"/tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/command/build_src.py\", line 163, in build_sources self.build_extension_sources(ext) File \"/tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/command/build_src.py\", line 320, in build_extension_sources sources = self.generate_sources(sources, ext) File \"/tmp/pip-wheel-sfv14zjb/numpy/numpy/distutils/command/build_src.py\", line 380, in generate_sources source = func(extension, build_dir) File \"numpy/core/setup.py\", line 430, in generate_config_h moredefs, ignored = cocache.check_types(config_cmd, ext, build_dir) File \"numpy/core/setup.py\", line 49, in check_types out = check_types(*a, **kw) File \"numpy/core/setup.py\", line 288, in check_types \"install {0}-dev|{0}-devel.\".format(python)) SystemError: Cannot compile 'Python.h'. Perhaps you need to install python-dev|python-devel. ---------------------------------------- ERROR: Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.5/dist-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpt471vgtd Check the logs for full command output. Traceback (most recent call last): File \"/usr/local/lib/python3.5/dist-packages/setuptools/installer.py\", line 128, in fetch_build_egg subprocess.check_call(cmd) File \"/usr/lib/python3.5/subprocess.py\", line 581, in check_call raise CalledProcessError(retcode, cmd) subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/tmp/tmp72zlaymg', '--quiet', 'numpy&gt;=1.7']' returned non-zero exit status 1 During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; File \"/tmp/pip-install-f4yysrhz/h5py/setup.py\", line 168, in &lt;module&gt; cmdclass = CMDCLASS, File \"/usr/local/lib/python3.5/dist-packages/setuptools/__init__.py\", line 143, in setup _install_setup_requires(attrs) File \"/usr/local/lib/python3.5/dist-packages/setuptools/__init__.py\", line 138, in _install_setup_requires dist.fetch_build_eggs(dist.setup_requires) File \"/usr/local/lib/python3.5/dist-packages/setuptools/dist.py\", line 687, in fetch_build_eggs replace_conflicting=True, File \"/usr/local/lib/python3.5/dist-packages/pkg_resources/__init__.py\", line 783, in resolve replace_conflicting=replace_conflicting File \"/usr/local/lib/python3.5/dist-packages/pkg_resources/__init__.py\", line 1066, in best_match return self.obtain(req, installer) File \"/usr/local/lib/python3.5/dist-packages/pkg_resources/__init__.py\", line 1078, in obtain return installer(requirement) File \"/usr/local/lib/python3.5/dist-packages/setuptools/dist.py\", line 743, in fetch_build_egg return fetch_build_egg(self, req) File \"/usr/local/lib/python3.5/dist-packages/setuptools/installer.py\", line 130, in fetch_build_egg raise DistutilsError(str(e)) distutils.errors.DistutilsError: Command '['/usr/bin/python3', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/tmp/tmp72zlaymg', '--quiet', 'numpy&gt;=1.7']' returned non-zero exit status 1 ---------------------------------------- ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output. Ty Michael",
        "answers": [
            [
                "I had a similar problem installing h5py on my Jetson Xavier (JetPack 4.21 L4T R32.2 release). I eliminated this problem by: install subversion by sudo apt-get install subversion then ln -s /usr/include/locale.h /usr/include/xlocale.h to resolve xlocale.h: No such file or directory issue. Finally, sudo apt-get install libhdf5-serial-dev to fix error: Unable to load dependency HDF5 After that h5py installed successfully."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using nvidia jetson nano with rpi camera to run yolov3, i'm 100% sure that the camera is compatible and working perfectly. when I tried to run live demo using this command ./darknet detector demo data/yolo.data cfg/yolov3_custom_train.cfg yolov3_custom_train_3000.weights -c 0 CUDA-version: 10000 (10000), cuDNN: 7.6.3, GPU count: 1 OpenCV version: 4.3.0 Demo net.optimized_memory = 0 i get the following warnings [ WARN:0] global /home/jn/opencv_build/opencv/modules/videoio/src/cap_gstreamer.cpp (1759) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module v4l2src0 reported: Internal data stream error. [ WARN:0] global /home/jn/opencv_build/opencv/modules/videoio/src/cap_gstreamer.cpp (888) open OpenCV | GStreamer warning: unable to start pipeline [ WARN:0] global /home/jn/opencv_build/opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created then the demo window pops up and i get a constant green screen is there any recommended solution? the green screen i get",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to capture an image using the csi camera on jetson nano using the jetcam.csi_camera library. The read method in the library returns an n-dimentional array and uses bgr8 encoding. I want to convert this to an image object in PIL. How can I use PIL.Image.fromarray() to achieve this? I tried but the fromarray() uses rgb mode and my image gets its blue and red channels interchanged. Pardon me and please ask if question is not clear. Thank you in advance:)",
        "answers": [
            [
                "Try reversing the channels: PILimage = Image.fromarray(... bgr8data[:,:,::-1]... )"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm using Linux on an Nvidia Jetson TX2, and I'm trying to learn machine/deep learning. When I try to install Keras, I keep getting the error mentioned in the title. Here are the screenshots of the full error: I just got the TX2 yesterday, and only flashed it. I also have tensorflow installed, as well as python. I also tried running these commands: $ sudo apt-get install -y build-essential libatlas-base-dev gfortran $ sudo apt-get update $ sudo apt-get install python3-dev $ sudo apt-get install python-dev $ sudo apt-get install libevent-dev $ sudo apt-get install build-essential followed by $ sudo pip install keras. I have also tried $ sudo -H pip install keras and still the same error. Does anyone have any idea why this is happening and how to fix?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to build an image for the nvidia jetson nano board using yocto (zeus branch), here is my configuration: Build Configuration: BB_VERSION = \"1.44.0\" BUILD_SYS = \"x86_64-linux\" NATIVELSBSTRING = \"universal\" TARGET_SYS = \"aarch64-poky-linux\" MACHINE = \"jetson-nano\" DISTRO = \"poky\" DISTRO_VERSION = \"3.0.2\" TUNE_FEATURES = \"aarch64 armv8a crc\" TARGET_FPU = \"\" meta meta-poky meta-yocto-bsp = \"zeus:5e1f52edb7a9f790fb6cb5d96502f3690267c1b1\" meta-python meta-filesystems meta-oe meta-multimedia = \"zeus:bb65c27a772723dfe2c15b5e1b27bcc1a1ed884c\" meta-tegra = \"zeus:23a9f6c12a741b4067d7a2ee601b98c766850e47\" bu i'm getting the following error: | /home/rui/projects/embeddeddfit/yocto/jetson-nano-build/tmp/work/armv8a_tegra210-poky-linux/cuda-samples/10.0.326-1-r0/recipe-sysroot/usr/local/cuda-10.0/include/crt/host_config.h:129:2: error: #error -- unsupported GNU version! gcc versions later than 7 are not supported! | 129 | #error -- unsupported GNU version! gcc versions later than 7 are not supported! | | ^~~~~ | Makefile:327: recipe for target 'UnifiedMemoryStreams.o' failed | make: *** [UnifiedMemoryStreams.o] Error 1 | ERROR: oe_runmake failed | WARNING: exit code 1 from a shell command. | ERROR: Execution of '/home/rui/projects/embeddeddfit/yocto/jetson-nano-build/tmp/work/armv8a_tegra210-poky-linux/cuda-samples/10.0.326-1-r0/temp/run.do_compile.24230' failed with exit code 1: | test.c:1:10: fatal error: omp.h: No such file or directory | 1 | #include &lt;omp.h&gt; | | ^~~~~~~ | compilation terminated. it seems to me that is a version compatibility problem. in my local.conf i have: MACHINE = \"jetson-nano\" LICENSE_FLAGS_WHITELIST = \"commercial\" IMAGE_CLASSES += \"image_types_tegra\" IMAGE_FSTYPES = \"tegraflash\" GCCVERSION = \"7.%\" CUDA_VERSION=\"10.0\" IMAGE_INSTALL_append = \" cuda-samples\" the 7.x version is specified but yocto don't found any compatible version NOTE: Resolving any missing task queue dependencies NOTE: preferred version 7.% of gcc-cross-aarch64 not available (for item virtual/aarch64-poky-linux-gcc) NOTE: versions of gcc-cross-aarch64 available: 9.2.0 How can i force yocto to use 7.x version, or how can i make yocto detect 7.x versions?",
        "answers": [
            [
                "The gcc recipes is located in sources/poky/meta/recipes-devtools/gcc/ If you have a different version than what you want, you will have to download/make another recipe."
            ],
            [
                "This is meta-tegra special sauce. Quote from README: CUDA 10 supports up through gcc 7 only, and some NVIDIA-provided binary libraries appear to be compiled with g++ 7 and cause linker failures when building applications with g++ 6, so only gcc 7 should be used if you intend to use CUDA. See the following wiki pages for instructions on including gcc 7 in your builds: Using gcc7 from the contrib layer Using linaro gcc7 for CUDA support In general, it's a good practice to read through the layer README when you start using a layer."
            ]
        ],
        "votes": [
            3.0000001,
            2.0000001
        ]
    },
    {
        "question": "I've managed to install and use docker with CUDA access on the Nvidia Jetson Nano device. The device is running a custom Yocto/poky-zeus (JetPack 4.3 supported) build . The docker image that I am testing with is an official nvidia-l4t-base image for Arm64 Nvidia Jetson devices. I am running the docker container with the following line: docker run --net=host --runtime nvidia --rm --ipc=host -v /tmp/.X11-unix/:/tmp/.X11-unix/ -v /tmp/argus_socket:/tmp/argus_socket --device=/dev/video0:/dev/video0 --cap-add SYS_PTRACE -e DISPLAY=:0 -it nvcr.io/nvidia/l4t-base:r32.3.1 Which is proven to work on the official tegraflash image from NVIDIA SDK-manager (JetPack 4.3). I can run gstreamer (1.14 or 1.16) just fine outside the docker container, but attempting to run any gstreamer(1.14) stuff inside the docker image fails. For instance, when running this gstreamer pipeline: gst-launch-1.0 -e nvarguscamerasrc num-buffers=-1 ! 'video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, format=(string)NV12, framerate=(fraction)60/1' ! nvvidconv flip-method=0 ! nvvidconv ! nvegltransform ! udpsink host=$HOST_IP port=$GST_PORT it fails with the following error: Setting pipeline to PAUSED ... [ 2415.341857] (NULL device *): nvhost_channelctl: invalid cmd 0x80685600 [ 2415.350034] (NULL device *): nvhost_channelctl: invalid cmd 0x80685600 [ 2415.357153] (NULL device *): nvhost_channelctl: invalid cmd 0x80685600 Failed to query video capabilities: Inappropriate ioctl for device libv4l2: error getting capabilities: Inappropriate ioctl for device ERROR: Pipeline doesn't want to pause. ERROR: from element /GstPipeline:pipeline0/nvv4l2h264enc:nvv4l2h264enc0: Error getting capabilities for device '/dev/nvhost-msenc': It isn't a v4l2 driver. Check if it is a v4l1 driver. Additional debug info: v4l2_calls.c(98): gst_v4l2_get_capabilities (): /GstPipeline:pipeline0/nvv4l2h264enc:nvv4l2h264enc0: system error: Inappropriate ioctl for device Setting pipeline to NULL ... Freeing pipeline ... Changing the encoding just gives another error. But this pipeline works just fine outside the docker as well as inside the docker on the official NVIDIA SDK-manager image, but NOT inside docker container with the Yocto build. Camera access is there in the container (RPi CSI camera), as v4l2-ctl --all shows correct information. Any idea on how to fix this? More on this issue from the meta-tegra layer and on Nvidia-docker issues.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I followed these topics: https://devtalk.nvidia.com/default/topic/1044958/jetson-agx-xavier/scikit-learn-for-python-3-on-jetson-xavier/ https://devtalk.nvidia.com/default/topic/1049684/jetson-nano/errors-during-install-sklearn-/ https://github.com/scikit-learn/scikit-learn/issues/12707 python version: 3.6.9 Here are all commands I run: sudo apt-get update sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev sudo apt-get install python3-pip sudo pip3 install -U pip testresources setuptools sudo pip3 install -U numpy==1.16.1 future==0.17.1 mock==3.0.5 h5py==2.9.0 keras_preprocessing==1.0.5 keras_applications==1.0.8 gast==0.2.2 enum34 futures protobuf sudo pip3 install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v43 tensorflow-gpu==1.15.0+nv19.12 sudo apt-get install python3-opencv sudo apt-get install python3-pandas sudo apt-get install python3-keras sudo apt-get install gfortran sudo apt-get install python3-scipy sudo apt-get install python3-matplotlib sudo apt-get install python3-imageio pip3 install dlib sudo apt-get install -y build-essential libatlas-base-dev pip3 install --upgrade setuptools sudo pip3 install -U setuptools sudo apt-get install libpcap-dev libpq-dev sudo pip3 install cython sudo pip3 install git+https://github.com/scikit-learn/scikit-learn.git and I got the long error below compile options: '-I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c' aarch64-linux-gnu-gcc: scipy/cluster/_hierarchy.c In file included from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1822:0, from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12, from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4, from scipy/cluster/_hierarchy.c:598: /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp] #warning \"Using deprecated NumPy API, disable it with \" \\ ^~~~~~~ scipy/cluster/_hierarchy.c:19289:18: warning: \u2018__Pyx_CFunc_double____double____double____double____int____int____int___to_py\u2019 defined but not used [-Wunused-function] static PyObject *__Pyx_CFunc_double____double____double____double____int____int____int___to_py(double (*__pyx_v_f)(double, double, double, int, int, int)) { ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ scipy/cluster/_hierarchy.c: In function \u2018__pyx_pw_5scipy_7cluster_10_hierarchy_27nn_chain\u2019: scipy/cluster/_hierarchy.c:13560:10: warning: \u2018__pyx_v_y\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] if (__pyx_t_12) { ^ scipy/cluster/_hierarchy.c:13074:7: note: \u2018__pyx_v_y\u2019 was declared here int __pyx_v_y; ^~~~~~~~~ scipy/cluster/_hierarchy.c: In function \u2018__pyx_pw_5scipy_7cluster_10_hierarchy_23linkage\u2019: scipy/cluster/_hierarchy.c:11431:16: warning: \u2018__pyx_v_y\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] __pyx_t_23 = __pyx_v_y; ~~~~~~~~~~~^~~~~~~~~~~ scipy/cluster/_hierarchy.c:11060:7: note: \u2018__pyx_v_y\u2019 was declared here int __pyx_v_y; ^~~~~~~~~ scipy/cluster/_hierarchy.c:11421:16: warning: \u2018__pyx_v_x\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] __pyx_t_22 = __pyx_v_x; ~~~~~~~~~~~^~~~~~~~~~~ scipy/cluster/_hierarchy.c:11059:7: note: \u2018__pyx_v_x\u2019 was declared here int __pyx_v_x; ^~~~~~~~~ scipy/cluster/_hierarchy.c: In function \u2018__pyx_pw_5scipy_7cluster_10_hierarchy_25fast_linkage\u2019: scipy/cluster/_hierarchy.c:12682:92: warning: \u2018__pyx_v_dist\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] *((double *) ( /* dim=0 */ (__pyx_v_D.data + __pyx_t_44 * __pyx_v_D.strides[0]) )) = __pyx_v_new_dist((*((double *) ( /* dim=0 */ (__pyx_v_D.data + __pyx_t_42 * __pyx_v_D.strides[0]) ))), (*((double *) ( /* dim=0 */ (__pyx_v_D.data + __pyx_t_43 * __pyx_v_D.strides[0]) ))), __pyx_v_dist, __pyx_v_nx, __pyx_v_ny, __pyx_v_nz); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ scipy/cluster/_hierarchy.c:11978:10: note: \u2018__pyx_v_dist\u2019 was declared here double __pyx_v_dist; ^~~~~~~~~~~~ scipy/cluster/_hierarchy.c:11971:7: warning: \u2018__pyx_v_y\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] int __pyx_v_y; ^~~~~~~~~ scipy/cluster/_hierarchy.c: In function \u2018__pyx_pw_5scipy_7cluster_10_hierarchy_29mst_single_linkage\u2019: scipy/cluster/_hierarchy.c:14363:142: warning: \u2018__pyx_v_y\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] *((double *) ( /* dim=1 */ (( /* dim=0 */ (__pyx_v_Z.data + __pyx_t_25 * __pyx_v_Z.strides[0]) ) + __pyx_t_26 * __pyx_v_Z.strides[1]) )) = __pyx_v_y; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~ scipy/cluster/_hierarchy.c:13995:7: note: \u2018__pyx_v_y\u2019 was declared here int __pyx_v_y; ^~~~~~~~~ aarch64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-aarch64-3.6/scipy/cluster/_hierarchy.o -Lbuild/temp.linux-aarch64-3.6 -o build/lib.linux-aarch64-3.6/scipy/cluster/_hierarchy.cpython-36m-aarch64-linux-gnu.so -Wl,--version-script=build/temp.linux-aarch64-3.6/link-version-scipy.cluster._hierarchy.map building 'scipy.cluster._optimal_leaf_ordering' extension compiling C sources C compiler: aarch64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC compile options: '-I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c' aarch64-linux-gnu-gcc: scipy/cluster/_optimal_leaf_ordering.c In file included from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1822:0, from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12, from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4, from scipy/cluster/_optimal_leaf_ordering.c:598: /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp] #warning \"Using deprecated NumPy API, disable it with \" \\ ^~~~~~~ scipy/cluster/_optimal_leaf_ordering.c: In function \u2018__pyx_pf_5scipy_7cluster_22_optimal_leaf_ordering_optimal_leaf_ordering.isra.58\u2019: scipy/cluster/_optimal_leaf_ordering.c:4747:19: warning: \u2018__pyx_v_best_w\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] __pyx_t_117 = __pyx_v_best_w; ~~~~~~~~~~~~^~~~~~~~~~~~~~~~ scipy/cluster/_optimal_leaf_ordering.c:3414:7: note: \u2018__pyx_v_best_w\u2019 was declared here int __pyx_v_best_w; ^~~~~~~~~~~~~~ scipy/cluster/_optimal_leaf_ordering.c:4746:19: warning: \u2018__pyx_v_best_u\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] __pyx_t_116 = __pyx_v_best_u; ~~~~~~~~~~~~^~~~~~~~~~~~~~~~ scipy/cluster/_optimal_leaf_ordering.c:3413:7: note: \u2018__pyx_v_best_u\u2019 was declared here int __pyx_v_best_u; ^~~~~~~~~~~~~~ aarch64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-aarch64-3.6/scipy/cluster/_optimal_leaf_ordering.o -Lbuild/temp.linux-aarch64-3.6 -o build/lib.linux-aarch64-3.6/scipy/cluster/_optimal_leaf_ordering.cpython-36m-aarch64-linux-gnu.so -Wl,--version-script=build/temp.linux-aarch64-3.6/link-version-scipy.cluster._optimal_leaf_ordering.map C compiler: aarch64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC creating /tmp/tmpvz7d4hd0/tmp creating /tmp/tmpvz7d4hd0/tmp/tmpvz7d4hd0 compile options: '-I/usr/include/python3.6m -c' extra options: '-std=c++14' aarch64-linux-gnu-g++: /tmp/tmpvz7d4hd0/main.c C compiler: aarch64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC creating /tmp/tmp3j3pimiu/tmp creating /tmp/tmp3j3pimiu/tmp/tmp3j3pimiu compile options: '-I/usr/include/python3.6m -c' extra options: '-std=c++14 -fvisibility=hidden' aarch64-linux-gnu-g++: /tmp/tmp3j3pimiu/main.c C compiler: aarch64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC creating /tmp/tmpmxeh9kyu/tmp creating /tmp/tmpmxeh9kyu/tmp/tmpmxeh9kyu compile options: '-I/usr/include/python3.6m -c' aarch64-linux-gnu-g++: /tmp/tmpmxeh9kyu/main.c building 'scipy.fft._pocketfft.pypocketfft' extension compiling C++ sources C compiler: aarch64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC creating build/temp.linux-aarch64-3.6/scipy/fft creating build/temp.linux-aarch64-3.6/scipy/fft/_pocketfft compile options: '-DPOCKETFFT_PTHREADS -I/home/dlinano/.local/include/python3.6m -I/usr/local/include/python3.6 -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c' extra options: '-std=c++14 -fvisibility=hidden' aarch64-linux-gnu-g++: scipy/fft/_pocketfft/pypocketfft.cxx scipy/fft/_pocketfft/pypocketfft.cxx:15:10: fatal error: pybind11/pybind11.h: No such file or directory #include &lt;pybind11/pybind11.h&gt; ^~~~~~~~~~~~~~~~~~~~~ compilation terminated. Running from scipy source directory. /usr/local/lib/python3.6/dist-packages/numpy/distutils/system_info.py:728: UserWarning: Specified path /usr/local/include/python3.6m is invalid. return self.get_paths(self.section, key) error: Command \"aarch64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DPOCKETFFT_PTHREADS -I/home/dlinano/.local/include/python3.6m -I/usr/local/include/python3.6 -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c scipy/fft/_pocketfft/pypocketfft.cxx -o build/temp.linux-aarch64-3.6/scipy/fft/_pocketfft/pypocketfft.o -MMD -MF build/temp.linux-aarch64-3.6/scipy/fft/_pocketfft/pypocketfft.o.d -std=c++14 -fvisibility=hidden\" failed with exit status 1 ---------------------------------------- ERROR: Failed building wheel for scipy Failed to build scipy ERROR: Could not build wheels for scipy which use PEP 517 and cannot be installed directly ---------------------------------------- ERROR: Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.6/dist-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-dfzx1730/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools wheel 'Cython&gt;=0.28.5' 'numpy&gt;=1.13.3' 'scipy&gt;=0.19.1' Check the logs for full command output. Please check this full logs: https://drive.google.com/file/d/1gLcSq86Aic5uFoPr8k6Cp366eRB2tlfw/view?usp=sharing",
        "answers": [
            [
                "This is what worked for me. sudo -H pip3 install scikit-learn Here is my full installation script for the order in which the dependencies are installed. #tensorflow - zoo sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev sudo apt-get install python3-pip sudo -H pip3 install -U pip sudo -H pip3 install -U numpy grpcio absl-py py-cpuinfo psutil portpicker six mock requests gast h5py astor==0.8.0 termcolor protobuf keras-applications keras-preprocessing wrapt google-pasta sudo -H pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu==1.14.0 #Cython sudo -H pip3 install cython #keras-zoo sudo apt-get install -y build-essential libatlas-base-dev gfortran sudo -H pip3 install keras #Pandas sudo -H pip3 install pandas #Scipy sudo -H python3 -m pip install scipy==1.1.0 #Sklearn sudo -H pip3 install scikit-learn"
            ],
            [
                "I had problems with Jupyter (Jetson TX2) I checked which numpy version is used by Jupyter. import numpy as np np.version.version In My case it was 1.13 despite the fact that $ pip3 list showed me version 1.16 for numpy I found solution here https://fooobar.com/questions/15251639/jupyter-notebook-picks-older-version-of-numpy to see the numpy path print (np.__path__) in my case it was /usr/lib/python3/dist-packages went to the directory and removed old version numpy $ cd /usr/lib/python3/dist-packages $ sudo rm -r numpy installed the relevant numpy version $ sudo pip install numpy==1.16.1 that's all."
            ]
        ],
        "votes": [
            7.0000001,
            1.0000001
        ]
    },
    {
        "question": "Good afternoon everyone, I tried many solutions from other people, but nothing is working. I have downloaded OpenCV, but everytime I run the code it says there is no opencv2 on my computer. And maybe it is important, I am using a Jetson TX2 with Ubuntu 16.04. Here is the error: File \"/usr/local/lib/python3.5/dist-packages/imutils/convenience.py\", line 6, in import cv2 ImportError: No module named 'cv2' In this source, I have the folder 'opencv-3.4.9' import numpy as np import sys sys.path.append('/usr/local/lib/python3.5/dist-packages') import cv2 ... I hope someone can help me",
        "answers": [
            [
                "On ubuntu, you can try sudo apt-get install python-opencv to install opencv from pre-built binaries. Or, you can also visit opencv on Ubuntu for alternative installation instructions."
            ],
            [
                "If you are using a virtual environment I strongly recommend this tutorial. If you are trying to install OpenCV systemwide then; sudo apt-get remove python3-opencv sudo apt-get install python3-opencv but above code won't install the latest version. For the latest version you could take a look at this github repo. I had a jetson nano and it gave me all sorts of trouble about OpenCV. Therefore I recommend the use of virtual environments for these kind of situations. It may take a while to build the libraries in jetson but it is the best-practice. Best of luck."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "consider a schenario where i am connecting 2 cameras to 2 usb ports the camera i connect first will always come in stream 0 for code below where as i want the stream 0 to always read from a specific usb port. import cv2 vc1 = cv2.VideoCapture(0) vc2 = cv2.VideoCapture(2) vc3 = cv2.VideoCapture(4) I want to hard code each camera to a specific usb port in python so no matter in what order the cameras are connected the camera from usb port 1 should always come in a specific stream and same for all streams.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using ubuntu 14.04 on a jetson TK1 board, I am new with linux, trying to install jupyter notebook and tensorflow. Unable to use pip and python. While checking for the pip version by writing either of the code pip -v pip --version this gives me error, ubuntu@tegra-ubuntu:~$ pip3 --version Traceback (most recent call last): File \"/usr/local/bin/pip3\", line 7, in &lt;module&gt; from pip._internal.cli.main import main File \"/usr/local/lib/python3.4/dist-packages/pip/_internal/cli/main.py\", line 10, in &lt;module&gt; from pip._internal.cli.autocompletion import autocomplete File \"/usr/local/lib/python3.4/dist-packages/pip/_internal/cli/autocompletion.py\", line 9, in &lt;module&gt; from pip._internal.cli.main_parser import create_main_parser File \"/usr/local/lib/python3.4/dist-packages/pip/_internal/cli/main_parser.py\", line 7, in &lt;module&gt; from pip._internal.cli import cmdoptions File \"/usr/local/lib/python3.4/dist-packages/pip/_internal/cli/cmdoptions.py\", line 28, in &lt;module&gt; from pip._internal.models.target_python import TargetPython File \"/usr/local/lib/python3.4/dist-packages/pip/_internal/models/target_python.py\", line 4, in &lt;module&gt; from pip._internal.utils.misc import normalize_version_info File \"/usr/local/lib/python3.4/dist-packages/pip/_internal/utils/misc.py\", line 20, in &lt;module&gt; from pip._vendor import pkg_resources File \"/usr/local/lib/python3.4/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 92, in &lt;module&gt; raise RuntimeError(\"Python 3.5 or later is required\") RuntimeError: Python 3.5 or later is required",
        "answers": [
            [
                "You are running python3.4 and the minimum required is 3.5 so just uninstall 3.4 and install 3.5"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Can we connect Microsoft's azure Kinect DK camera with Nvidia Jetson TX2 board DK together? Do these development kits support each other? Can we work on them?",
        "answers": [
            [
                "Not yet, but we are working on it. Is should be ready soon though. You can see our ARM proposal here: https://github.com/microsoft/Azure-Kinect-Sensor-SDK/blob/develop/proposals/ARM-support.md"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to control DJI camera using the API port on A3 cotnroller. The drone i have is custom built. The reason i want to integrate DJI camera is because they have been so far the best. So any suggestion would be welcome. Please let me know if the whether the direct control of the camera is possible or not. If yes then how can it be done?",
        "answers": [
            [
                "A3 can allow you to use servo and digital IO. If you camera has direct PWM based control method, then you can direct connect in and control it by API . you can follow the sample here on how to do it https://github.com/dji-sdk/Onboard-SDK-ROS/blob/3.8/dji_sdk_demo/src/demo_mfio.cpp And usually, we connect A3 to a manifold 2. Manifold 2 has Serial/digital control output that can allow you to connect to the camera such as IDS and Pointgrey camera. Follow the camera manufactures guide to implement the camera control function such as external trigger/sync, apecture, focus etc. If it is just a normal webcam, it is even easier. Connect the camera in and adjust the setting by this http://www.techytalk.info/webcam-settings-control-ubuntu-linux/"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am using Jetson TX2 boards for my work with PCL and GPU. Since, GPU sub-systems are not available on any apt-repos, I am told to build them from source every time I move to a new system. Since all HW is identical,how can I just copy the same PCL library built initially on other systems to new system and start using directly? I kept getting the CMAKE error: This directory is not where CMakeCache file was generated. What is the way around this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to install numpy for python3, and I used sudo apt-get install python3-numpy to install numpy as I use Jetson tx2. Although the installation is successful, but numpy is installed on python2.7 not python3. How can I solve it?",
        "answers": [
            [
                "Actually when you flash your Jetson TX2 with Jetpack (version), numpy package is present for Python2 by default and not for Python3. In order to install numpy for Python3 Please follow the steps given below:- 1. Check if you have pip3 installed for Python3. If not install pip3. sudo apt install python3-pip 2. Then using pip3 install numpy pip3 install numpy After installation check the location: /usr/local/lib/python3.6/dist-packages you will find numpy installed for Python3 Hope this helps!"
            ],
            [
                "I think this is because your default interpreter is Py v2.7 Check this by runnin in console: python -V Then you can specify Py3 installation as was commented above: pip3 install numpy Note: Do not run this command with sudo because it will run setup.py with sudo or in other words - an arbitraty upload a malicious project on PyPI this is a hight risk action."
            ],
            [
                "You can try using the python3 package manager : pip3 install --user numpy"
            ]
        ],
        "votes": [
            1.0000001,
            -0.9999999,
            -0.9999999
        ]
    },
    {
        "question": "I found that we can optimize the Tensorflow model in several ways. If I am mistaken, please tell me. 1- Using TF-TRT, This API developer by tensorflow and integreted TensoRT to Tensorflow and this API called as : from tensorflow.python.compiler.tensorrt import trt_convert as trt This API can be applied to any tensorflow models (new and old version models) without any converting error, because If this API don't support any new layers, don't consider these layers for TensorRT engines and these layers remain for Tensorflow engine and run on Tensorflow. right? 2- Using TensorRT, This API developed by NVIDA and is independent of Tenorflow library (Not integrated to Tensorflow), and this API called as: import tensorrt as trt If we want to use this api, first, we must converting the tensorflow graph to UFF using uff-convertor and then parse the UFF graph to this API. In this case, If the Tensorflow graph have unsupported layers we must use plugin or custom code for these layers, right? 3- I don't know, when we work with Tensorflow models, Why we use UFF converter then TensorRT, we can use directly TF-TRT API, right? If so, Are you tested the Tensorflow optimization model from these two method to get same performance? what's advantage of this UFF converter method? I have some question about the two cases above: 4- I convert the ssd_mobilenet_v2 using two cases, In the case 1, I achieve slight improvement in speed but in the case 2, I achieve more improvement, why? My opinion is that, In the case 1, The API only consider converting the precision (FP32 to FP16) and merging the possible layers together, But in the case 2, the graph is clean by UFF such as remove any redundant nodes like Asserts and Identity and then converted to tensorrt graph, right? 5- when we convert the trained model files like .ckpt and .meta, ... to frozen inference graph(.pb file), These layers don't remove from graph? only loss states and optimizer states , ... are removed?",
        "answers": [
            [
                "Duplicate post with answers here: https://github.com/NVIDIA/TensorRT/issues/341"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am running a custom Yocto image on the Nvidia Jetson Nano that has docker-ce (v19.03.2) included. I am able to run docker without problems. The problem comes when I want to use docker for vision testing. I need access to host side CUDA and tensorRT. This is accessed through the Nvidia Container Runtime on top of the docker-ce. I have installed Nvidia Container Runtime (v0.9.0 beta) manually (extracted the necessary .deb packages and copy pasted them into the rootfs) to test on my build, and it seems to be working fine. When I run docker info I can see that the nvidia runtime is available, and it doesn't complain when I run a docker with docker run -it --runtime=nvidia image. If i run deviceQuery test OUTSIDE docker, i get the following: $ /usr/bin/cuda-samples/deviceQuery ... CUDA Driver Version / Runtime Version 10.0 / 10.0 CUDA Capability Major/Minor version number: 5.3 ... deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.0, CUDA Runtime Version = 10.0, NumDevs = 1 Result = PASS However, when I want to run deviceQuery IN a docker to test CUDA availability, it fails: ./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) cudaGetDeviceCount returned 35 -&gt; CUDA driver version is insufficient for CUDA runtime version Result = FAIL Where the Dockerfile is as follows: FROM nvcr.io/nvidia/l4t-base:r32.2 COPY ./deviceQuery /tmp/deviceQuery WORKDIR /tmp/ CMD [\"./deviceQuery\"] So my questions are these: Why does the deviceQuery test fail inside the docker even though I have Nvidia Container Runtime installed with docker? - and how can i fix it this issue? ------------EDIT:---------- More on this on this thread from nvidia devtalk.",
        "answers": [
            [
                "The .csv that are included in the rootfs from the NVIDIA SDK-manager contains specific lib/dir/sym that are needed for the passing of GPU access to the container. The files that are listed in the .csv files are merged into the container and allows access to these files. What specific files are needed, depends on what is needed in the container. It is of course very important that the actual paths to the files listed in the csv files are the same on the host, otherwise the merge will fail. These paths are not the correct paths on the default Yocto setup as they are made for the default NVIDIA SDK-manager image rootfs setup and thus needs to be corrected. Once corrected, the access to GPU acceleration in the container should be possible and can be confirmed by doing a deviceQuery test."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have been trying to build a Jetson Nano powered Jetbot using the hardware supplied by WaveShare. I assembled the hardware by following the instructions here. But unfortunately, it doesn't power up when I turn it on. I have checked my Jetson Nano separately and it works fine. Even the batteries are charged and shows about 4.2V when checked with a multimeter. Does anyone have any idea about what might be the problem and how can I fix it?",
        "answers": [
            [
                "The jetbot board has to be connected to the power adapter for a while to enable the power of the batteries (even when the batteries are fully charged). This step has to be done only once but is an indispensable step to allow the jetson nano to draw power from the batteries. Apparently, it has not been mentioned in the instructions on the WaveShare website."
            ],
            [
                "check the multicolored cable that connects the Waveshare expansion board and battery holder to the Jetson Nano. Make sure it has the proper orientation when plugged into each board. It is part number 15 in the WaveShare JetBot AI Kit Assembly manual."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "NVidia Jetson Nano on Ubuntu 18.04 completes docker run with standard_init_linux.go:211: exec user process caused \"exec format error\", but some images run completely fine. organic@jetson:~$ docker run -it --rm -p 8080:8080 django Unable to find image 'django:latest' locally latest: Pulling from library/django 75a822cd7888: Pull complete e4665cede9d1: Pull complete 202a45aa091c: Pull complete 7799136eb561: Pull complete 7a7f9ca3fd40: Pull complete 412f2d081014: Pull complete Digest: sha256:5bfd3f442952463f5bc97188b7f43cfcd6c2f631a017ee2a6fca3cb8992501e8 Status: Downloaded newer image for django:latest standard_init_linux.go:211: exec user process caused \"exec format error\" organic@jetson:~$ docker pull nginx:latest latest: Pulling from library/nginx Digest: sha256:50cf965a6e08ec5784009d0fccb380fc479826b6e0e65684d9879170a9df8566 Status: Image is up to date for nginx:latest docker.io/library/nginx:latest organic@jetson:~$ docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world be6e184261a6: Pull complete Digest: sha256:4fe721ccc2e8dc7362278a29dc660d833570ec2682f4e4194f4ee23e415e1064 Status: Downloaded newer image for hello-world:latest docker.io/library/hello-world:latest",
        "answers": [
            [
                "It looks like the docker run command pulls the django image just fine, but the running part fails. The Django:latest docker image, is a x86-64 only image. But as you are on a Jetson Nano (Arm architecture), you should find (or build) arm64 compatible docker images and not x86-64. The other two commands in your example that are completed without the same error, are only doing the pulling part. But they will most likely still run just fine as they are both Arm64 compatible. You can find the tags under the images on the dockerhub."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm working on a jetson tk1 deployment scheme where I use docker to create the root filesystem which then gets flashed onto the image. The way this works is I create an armhf image using the nvidia provided sample filesystem with a qemu-arm-static binary which I can then build upon using standard docker tools. I then have a \"flasher\" image which copies the contents of the file system, creates an iso image and flashes it onto my device. The problem that I'm having is that I'm getting inconsistent results between installing apt packages using a docker RUN statement vs entering the image and installing apt packages. IE: # docker build -t jetsontk1:base . Dockerfile from jetsontk1:base1 RUN apt update RUN apt install build-essential cmake # or RUN /bin/bash -c 'apt install build-essential cmake -y' vs: docker run -it jetsontk1:base1 /bin/bash # apt update # apt install build-essential cmake When I install using the docker script I get the following error: Processing triggers for man-db (2.6.7.1-1) ... terminate called after throwing an instance of 'std::length_error' what(): basic_string::append qemu: uncaught target signal 6 (Aborted) - core dumped Aborted (core dumped) The command '/bin/sh -c /bin/bash -c 'apt install build-essential cmake -y'' returned a non-zero code: 134 I have no issues when manually installing applications from when I'm inside the container, but there's no point in using docker to manage this image building process if I can't do apt installs :/ The project can be found here: https://github.com/dtmoodie/tk1_builder With the current state with the issue as I presented it at commit: 8e22c0d5ba58e9fdab38e675eed417d73ae0aad9",
        "answers": [],
        "votes": []
    },
    {
        "question": "I\u2019m using Jetson Xavier (with maxn config) and I have created python multiprocess application for video analytics, containing the following 2 processes (Entirely Separate with no inter process communication): Simple process for capture video source with openCV and showing it with openCV imshow . Process that taking constant tenzor and run it through a pytorch nn model with cuda in an infinite loop. (I started with 2 connected processes for capture the video and then process the frames concurrent but to debug the problem I broke all of the connection and ran it on a constant tensor) The problem is that when only the video process is running the video is smooth. But when the NN process works concurrently the video become not smooth, that, even though the fps for the video process is 25, like I designed it by waitkey. I suspect that I\u2019m using all my gpu resources for the NN and then the frame show rendering is on hold for a few milliseconds until the gpu is free for render. If it\u2019s indeed the reason for my problem, can I determine priority for gpu usage between the two processes? Do you have another idea how to solve it? This is the pseudo code (the original is offline and I can\u2019t upload it): Process1: Video = cv2.capture (video_source) While True Frame = Video.get() Cv2.imshow(frame) Waitkey(40) # Check fps Process2: model = # loading the pytorch NN model tenzor = # creating zeros pytorch cuda tenzor while True model(tenzor) Attaching tegrastats from running time: Thanks moti",
        "answers": [],
        "votes": []
    },
    {
        "question": "I recently bought a Jetson Nano and I'm amazed with everything about it. But I don't know what is happening, because I created a very simple neural network with keras and it's taking way to long. I know is taking to long, because I runned the same ANN in my PC's CPU and it was faster than the jetson nano. Here's the code: import numpy as np import matplotlib.pyplot as plt import pandas as pd dataset = pd.read_csv('Churn_Modelling.csv') X = dataset.iloc[:, 3:13].values y = dataset.iloc[:, 13].values from sklearn.preprocessing import LabelEncoder, OneHotEncoder labelencoder_X_1 = LabelEncoder() X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1]) labelencoder_X_2 = LabelEncoder() X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2]) onehotencoder = OneHotEncoder(categorical_features = [1]) X = onehotencoder.fit_transform(X).toarray() X = X[:, 1:] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) from tensorflow import keras from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense classifier = Sequential() classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11)) classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu')) classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) classifier.fit(X_train, y_train, batch_size = 10, epochs = 100) y_pred = classifier.predict(X_test) y_pred = (y_pred &gt; 0.5) I should mention that of course, I did the correct installation of TensorFlow GPU library and not the normal TensorFlow, in fact I used the resources in this link: TensorFlow GPU Jetson Nano",
        "answers": [
            [
                "Jetson Nano is mainly for inferencing. Training is not preferred even though its possible. This link might help. You can try using Nvidia's Transfer Learning Toolkit and Deepstream for ideal and efficient use on Nano."
            ],
            [
                "@Juan Carlos Jchr Hey, just check https://stackexchange.com/sites I think that your question will get better answes here: https://ai.stackexchange.com/"
            ]
        ],
        "votes": [
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "I am solving different time synchronization of PC. Checked that NTP synchronizes to less than 1 millisecond using ntpq. I would like to ask for help on how to verify this without using ntpq, ntdate etc. I used 'ssh' to see the approximate difference in time between computers on the screen. I'd like to know a slightly more accurate time error for everything I have to consider, such as the Netwick Transmission Delay. I use several nvidia xavier in uuntu 18.04 and 'ROS melodic'",
        "answers": [
            [
                "If you are looking to establish timings for networking calls then you would want microsecond accuracy between the servers since the latency between systems can often be sub-millisecond on individual network segments. The tools themselves are often the best place to check for the accuracy figures, since they will be using that figure themselves to calculate the \"correct\" time. If you are able to send test packets between the servers you are interested in then a latency measurement between the servers using a ping-pong style of test will give a network latency. Assuming this is symmetric, i.e. each way is identical latency, you can then calculate the half-round-trip time. At this point, running another test where you send a timestamp from one system to another and compare the two (subtracting 1/2 RTT from the sender) should give you a good idea of the time difference. You would need to run the above test for multiple packets since the first few packets are likely to be impacted by the server cache and won't be consistent."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have installed OpenCV 4.1 in Jetson Nano from source. It works perfect when i run my python3 code. After compiling python code using pyinstaller it throws ImportError. ImportError: OpenCV loader: missing configuration file: ['config.py']. Check OpenCV installation. How do I compile python code using PyInstaller to make OpenCV included? Is there any other method to install OpenCV? I have tried pip3 install opencv-python but it does not find the matching distribution and if I install using sudo apt-get install python3-opencv it installs an older version. Any help wou.d be helpful. I have also followed this thread but it does not work when OpenCV is compiled from source...",
        "answers": [
            [
                "After lots of debugging, I found the following solution: Python 3.6 OpenCV 4.1 (Compiled from source) pyinstaller 3.5 1. Get the path of OpenCV import cv2 print(cv2.__file__) # /usr/local/lib/python3.6/dist-packages/cv2/python-3.6/cv2.so 2. Add this path while compiling through pyinstaller pyinstaller main.py -n myApp --paths=\"/usr/local/lib/python3.6/dist-packages/cv2/python-3.6\" I hope this helps others also"
            ]
        ],
        "votes": [
            10.0000001
        ]
    },
    {
        "question": "We have installed Jetpack 4.2.3 on jetson nano. This has created a 2 GB built in zram memory. We have additionally added 4 GB swap file using the following https://www.jetsonhacks.com/2019/04/14/jetson-nano-use-more-memory/ The system is taking priority to zram. we want to set the newly created swap to take priority over zram. How to prioritize swap in jetson Nano? How to change Swap partition priority?",
        "answers": [
            [
                "Disabled zram by removing nvzramconfig.sh from /etc/systemd/ and zram is not invoked when the system boots. Now the swap is working perfect"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have Kodak PIXPRO SP360 4k camera connected to the Jetson Nano or TX2 via USB cable. I want to be able to see that video over browser, either with RTSP stream, Webrtc or something else. It doesn't matter how it works in terms of technology, as long as it works. So if you have any ideas or suggestions be free to share them. I'm currently trying to run the basic setup. ./test-launch \"nvarguscamerasrc ! video/x-raw(memory:NVMM), format=NV12, width=1920, height=1080, framerate=30/1 ! nvvidconv ! video/x-raw, width=640, height=480, format=NV12, framerate=30/1 ! omxh265enc ! rtph265pay name=pay0 pt=96 config-interval=1\" and gst-launch-1.0 rtspsrc location=rtsp://127.0.0.1:8554/test ! queue ! decodebin ! videoconvert ! xvimagesink and I'm getting the error saying Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... Progress: (open) Opening Stream Progress: (connect) Connecting to rtsp://127.0.0.1:8554/test Progress: (open) Retrieving server options Progress: (open) Retrieving media info ERROR: from element /GstPipeline:pipeline0/GstRTSPSrc:rtspsrc0: Could not get/set settings from/on resource. Additional debug info: gstrtspsrc.c(6999): gst_rtspsrc_setup_streams_start (): /GstPipeline:pipeline0/GstRTSPSrc:rtspsrc0: SDP contains no streams ERROR: pipeline doesn't want to preroll. Setting pipeline to PAUSED ... Setting pipeline to READY ... Setting pipeline to NULL ... Freeing pipeline ... (test-launch:22440): GLib-GObject-WARNING **: 11:36:46.018: invalid cast from 'GstRtpH265Pay' to 'GstBin' (test-launch:22440): GStreamer-CRITICAL **: 11:36:46.018: gst_bin_get_by_name: assertion 'GST_IS_BIN (bin)' failed (test-launch:22440): GLib-GObject-WARNING **: 11:36:46.018: invalid cast from 'GstRtpH265Pay' to 'GstBin' (test-launch:22440): GStreamer-CRITICAL **: 11:36:46.018: gst_bin_get_by_name: assertion 'GST_IS_BIN (bin)' failed (test-launch:22440): GLib-GObject-WARNING **: 11:36:46.018: invalid cast from 'GstRtpH265Pay' to 'GstBin' (test-launch:22440): GStreamer-CRITICAL **: 11:36:46.018: gst_bin_get_by_name: assertion 'GST_IS_BIN (bin)' failed I have also tried an option that worked for me on PC but I can't get it to work on Jetson. The setup goes as follows. Download Streameye from https://github.com/ccrisan/streameye and run: netcat -l 8700 | ./streameye -p 1337 To send the webcam stream I run: gst-launch-1.0 v4l2src device=/dev/video0 ! decodebin ! videoconvert ! videoscale ! videorate ! jpegenc quality=30 ! tcpclientsink host=127.0.0.1 port=8700 After this I get: Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... Setting pipeline to PLAYING ... New clock: GstSystemClock ERROR: from element /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: Internal data stream error. Additional debug info: gstbasesrc.c(3064): gst_base_src_loop (): /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: streaming stopped, reason not-negotiated (-4) Execution ended after 0:00:03.944998186 Setting pipeline to PAUSED ... Setting pipeline to READY ... Setting pipeline to NULL ... Freeing pipeline ... Output of this command for my camera is: v4l2-ctl -d /dev/video1 --list-formats-ext ioctl: VIDIOC_ENUM_FMT Index : 0 Type : Video Capture Pixel Format: 'MJPG' (compressed) Name : Motion-JPEG Size: Discrete 3840x2160 Interval: Discrete 0.200s (5.000 fps) Size: Discrete 2880x2880 Interval: Discrete 0.200s (5.000 fps) Size: Discrete 2048x2048 Interval: Discrete 0.200s (5.000 fps) Size: Discrete 1440x1440 Interval: Discrete 0.067s (15.000 fps) Interval: Discrete 0.200s (5.000 fps) Size: Discrete 1920x1080 Interval: Discrete 0.067s (15.000 fps) Interval: Discrete 0.200s (5.000 fps) Size: Discrete 1280x720 Interval: Discrete 0.033s (30.000 fps) Interval: Discrete 0.067s (15.000 fps) Interval: Discrete 0.200s (5.000 fps) Size: Discrete 640x360 Interval: Discrete 0.033s (30.000 fps) Interval: Discrete 0.067s (15.000 fps) Interval: Discrete 0.200s (5.000 fps)",
        "answers": [
            [
                "run your pipe with -v like this and show me result: gst-launch-1.0 v4l2src device=/dev/video0 ! decodebin ! videoconvert ! videoscale ! videorate ! jpegenc quality=30 ! tcpclientsink host=127.0.0.1 port=8700 -v"
            ],
            [
                "If you want to stream it, the simplest way will be to use gst-rtsp-launch which is part of GStreamer prebuild binaries: gst-rtsp-launch '( v4l2src device=/dev/video0 ! videoconvert ! queue ! x264enc tune=\"zerolatency\" byte-stream=true bitrate=10000 ! rtph264pay name=pay0 pt=96 )' Later on you can tune codec, bitrate, but for me this is enough (playable in VLC - rtsp://127.0.0.1:8554/test)"
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am trying to convert a TF 2.0 saved_model to tensorRT on the Jetson Nano. The model was saved in TF 2.0.0. The nano has Jetpack 4.2.2 w/ TensorRT __ and Tensorflow 1.14 (that is the latest Tensorflow release for Jetson). I have been following the instuctions from here which describe how to convert a TF 2.0.0 saved_model into TensorRT. Below is my code: import tensorflow as tf from tensorflow.python.compiler.tensorrt import trt_convert as trt tf.enable_eager_execution() converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir) converter.convert() converter.save(output_saved_model_dir) saved_model_loaded = tf.saved_model.load( output_saved_model_dir, tags=[tag_constants.SERVING]) graph_func = saved_model_loaded.signatures[ signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] frozen_func = convert_to_constants.convert_variables_to_constants_v2( graph_func) def wrap_func(*args, **kwargs): # Assumes frozen_func has one output tensor return frozen_func(*args, **kwargs)[0] output = wrap_func(input_data).numpy() It seems to start converting successfully. However I get an KeyError: 'serving_default' error when it reaches the convert_to_tensor line. My complete printout is below found here (too long for SO), but the python traceback appears below. How can I fix this? Thanks! printout summary (complete printout here): Traceback (most recent call last): File \"tst.py\", line 38, in &lt;module&gt; convert_savedmodel() File \"tst.py\", line 24, in convert_savedmodel converter.convert() File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 956, in convert func = self._saved_model.signatures[self._input_saved_model_signature_key] File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_serialization.py\", line 196, in __getitem__ return self._signatures[key] KeyError: 'serving_default'",
        "answers": [
            [
                "I can see two problems in your experiment: You are using TF-TRT 2.0 API while having TF 1.14 installed. That is not supported. If you have TF 1.14 installed on your system, then you would need to use TF-TRT 1.x API. TF Models saved in TF2.0 are not compatible with TF1.14 according to https://www.tensorflow.org/guide/versions If you only have access to TF1.14, I suggest to re-generate the graph in TF1.14 and save the model there before applying TF-TRT, and then use TF-TRT 1.x API."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am currently using a NVIDIA Jetson TX2 with Ubuntu 18.04 to train a Mask R-CNN (Matterport implementation https://github.com/matterport/Mask_RCNN) with my custom dataset. When I try to load the mask_rcnn_coco.h5, the following error comes up multiple times: 2019-11-11 10:47:36.177422: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at random_op.cc:76 : Resource exhausted: OOM when allocating tensor with shape[7,7,256,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc 2019-11-11 10:47:46.178504: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 36.8KiB (rounded to 37632). Current allocation summary follows. 2019-11-11 10:47:46.178985: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (256): Total Chunks: 52, Chunks in use: 52. 13.0KiB allocated for chunks. 13.0KiB in use in bin. 808B client-requested in use in bin. 2019-11-11 10:47:46.179255: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (512): Total Chunks: 2, Chunks in use: 2. 1.0KiB allocated for chunks. 1.0KiB in use in bin. 1.0KiB client-requested in use in bin. 2019-11-11 10:47:46.179435: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1024): Total Chunks: 3, Chunks in use: 3. 3.2KiB allocated for chunks. 3.2KiB in use in bin. 3.0KiB client-requested in use in bin. 2019-11-11 10:47:46.179565: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2048): Total Chunks: 2, Chunks in use: 2. 4.0KiB allocated for chunks. 4.0KiB in use in bin. 4.0KiB client-requested in use in bin. 2019-11-11 10:47:46.179697: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4096): Total Chunks: 2, Chunks in use: 2. 8.0KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin. 2019-11-11 10:47:46.179835: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8192): Total Chunks: 3, Chunks in use: 3. 28.0KiB allocated for chunks. 28.0KiB in use in bin. 28.0KiB client-requested in use in bin. 2019-11-11 10:47:46.179964: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16384): Total Chunks: 1, Chunks in use: 1. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 16.0KiB client-requested in use in bin. 2019-11-11 10:47:46.180097: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (32768): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2019-11-11 10:47:46.180374: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (65536): Total Chunks: 6, Chunks in use: 6. 384.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin. 2019-11-11 10:47:46.180655: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (131072): Total Chunks: 4, Chunks in use: 4. 560.0KiB allocated for chunks. 560.0KiB in use in bin. 560.0KiB client-requested in use in bin. 2019-11-11 10:47:46.180810: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (262144): Total Chunks: 7, Chunks in use: 7. 1.75MiB allocated for chunks. 1.75MiB in use in bin. 1.75MiB client-requested in use in bin. 2019-11-11 10:47:46.180939: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (524288): Total Chunks: 7, Chunks in use: 7. 3.75MiB allocated for chunks. 3.75MiB in use in bin. 3.75MiB client-requested in use in bin. 2019-11-11 10:47:46.181089: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1048576): Total Chunks: 2, Chunks in use: 2. 2.53MiB allocated for chunks. 2.53MiB in use in bin. 2.00MiB client-requested in use in bin. 2019-11-11 10:47:46.181306: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2097152): Total Chunks: 1, Chunks in use: 1. 2.00MiB allocated for chunks. 2.00MiB in use in bin. 2.00MiB client-requested in use in bin. 2019-11-11 10:47:46.181414: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4194304): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2019-11-11 10:47:46.181519: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8388608): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2019-11-11 10:47:46.181694: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16777216): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2019-11-11 10:47:46.181826: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (33554432): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2019-11-11 10:47:46.181937: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (67108864): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2019-11-11 10:47:46.182080: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (134217728): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2019-11-11 10:47:46.182290: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (268435456): Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin. 2019-11-11 10:47:46.182397: I tensorflow/core/common_runtime/bfc_allocator.cc:780] Bin for 36.8KiB was 32.0KiB, Chunk State: 2019-11-11 10:47:46.182479: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 11554816 2019-11-11 10:47:46.182572: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e60000 next 1 of size 1280 2019-11-11 10:47:46.182720: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e60500 next 2 of size 256 2019-11-11 10:47:46.182806: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e60600 next 3 of size 256 2019-11-11 10:47:46.182886: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e60700 next 4 of size 2048 2019-11-11 10:47:46.182964: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e60f00 next 5 of size 256 2019-11-11 10:47:46.183044: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e61000 next 6 of size 256 2019-11-11 10:47:46.183122: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e61100 next 7 of size 256 2019-11-11 10:47:46.183198: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e61200 next 8 of size 256 2019-11-11 10:47:46.183300: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e61300 next 9 of size 256 2019-11-11 10:47:46.183481: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e61400 next 10 of size 256 2019-11-11 10:47:46.183567: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e61500 next 11 of size 256 2019-11-11 10:47:46.183646: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e61600 next 12 of size 256 2019-11-11 10:47:46.183722: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e61700 next 13 of size 8192 2019-11-11 10:47:46.183803: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e63700 next 14 of size 256 2019-11-11 10:47:46.183938: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e63800 next 15 of size 256 2019-11-11 10:47:46.184040: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e63900 next 16 of size 256 2019-11-11 10:47:46.184124: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e63a00 next 17 of size 256 2019-11-11 10:47:46.184230: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e63b00 next 18 of size 256 2019-11-11 10:47:46.184312: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e63c00 next 19 of size 256 2019-11-11 10:47:46.184417: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e63d00 next 20 of size 256 2019-11-11 10:47:46.184501: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e63e00 next 21 of size 256 2019-11-11 10:47:46.184643: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e63f00 next 22 of size 4096 2019-11-11 10:47:46.184727: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e64f00 next 23 of size 256 2019-11-11 10:47:46.184833: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65000 next 24 of size 256 2019-11-11 10:47:46.185067: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65100 next 25 of size 1024 2019-11-11 10:47:46.185154: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65500 next 26 of size 256 2019-11-11 10:47:46.185231: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65600 next 27 of size 256 2019-11-11 10:47:46.185312: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65700 next 28 of size 256 2019-11-11 10:47:46.185389: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65800 next 29 of size 256 2019-11-11 10:47:46.185477: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65900 next 30 of size 256 2019-11-11 10:47:46.185557: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65a00 next 31 of size 256 2019-11-11 10:47:46.185670: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65b00 next 32 of size 512 2019-11-11 10:47:46.185753: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65d00 next 33 of size 256 2019-11-11 10:47:46.185830: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65e00 next 34 of size 256 2019-11-11 10:47:46.185907: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e65f00 next 35 of size 256 2019-11-11 10:47:46.185985: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66000 next 36 of size 256 2019-11-11 10:47:46.186062: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66100 next 37 of size 256 2019-11-11 10:47:46.186172: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66200 next 38 of size 256 2019-11-11 10:47:46.186254: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66300 next 39 of size 256 2019-11-11 10:47:46.186333: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66400 next 40 of size 256 2019-11-11 10:47:46.186414: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66500 next 41 of size 256 2019-11-11 10:47:46.186612: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66600 next 42 of size 256 2019-11-11 10:47:46.186682: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66700 next 43 of size 256 2019-11-11 10:47:46.186747: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66800 next 44 of size 1024 2019-11-11 10:47:46.186833: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66c00 next 45 of size 256 2019-11-11 10:47:46.186896: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66d00 next 46 of size 256 2019-11-11 10:47:46.186956: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66e00 next 47 of size 256 2019-11-11 10:47:46.187017: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e66f00 next 48 of size 256 2019-11-11 10:47:46.187078: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e67000 next 49 of size 256 2019-11-11 10:47:46.187138: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e67100 next 50 of size 256 2019-11-11 10:47:46.187198: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e67200 next 51 of size 256 2019-11-11 10:47:46.187262: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e67300 next 52 of size 4096 2019-11-11 10:47:46.187322: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e68300 next 53 of size 256 2019-11-11 10:47:46.187382: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e68400 next 54 of size 256 2019-11-11 10:47:46.187441: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e68500 next 55 of size 256 2019-11-11 10:47:46.187501: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e68600 next 56 of size 256 2019-11-11 10:47:46.187567: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e68700 next 57 of size 256 2019-11-11 10:47:46.187628: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e68800 next 58 of size 256 2019-11-11 10:47:46.187688: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e68900 next 59 of size 256 2019-11-11 10:47:46.187748: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e68a00 next 60 of size 256 2019-11-11 10:47:46.187806: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e68b00 next 61 of size 8192 2019-11-11 10:47:46.187866: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e6ab00 next 62 of size 2048 2019-11-11 10:47:46.187926: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e6b300 next 63 of size 512 2019-11-11 10:47:46.187987: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00e6b500 next 64 of size 524288 2019-11-11 10:47:46.188046: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00eeb500 next 65 of size 524288 2019-11-11 10:47:46.188105: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00f6b500 next 66 of size 589824 2019-11-11 10:47:46.188163: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf00ffb500 next 67 of size 589824 2019-11-11 10:47:46.188222: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0108b500 next 68 of size 589824 2019-11-11 10:47:46.188280: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0111b500 next 69 of size 589824 2019-11-11 10:47:46.188338: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf011ab500 next 70 of size 2097152 2019-11-11 10:47:46.188401: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf013ab500 next 71 of size 262144 2019-11-11 10:47:46.188460: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf013eb500 next 72 of size 262144 2019-11-11 10:47:46.188518: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0142b500 next 73 of size 262144 2019-11-11 10:47:46.188576: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0146b500 next 74 of size 262144 2019-11-11 10:47:46.188646: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf014ab500 next 75 of size 131072 2019-11-11 10:47:46.188707: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf014cb500 next 76 of size 65536 2019-11-11 10:47:46.188768: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf014db500 next 77 of size 65536 2019-11-11 10:47:46.188828: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf014eb500 next 78 of size 65536 2019-11-11 10:47:46.188886: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf014fb500 next 79 of size 65536 2019-11-11 10:47:46.188946: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0150b500 next 80 of size 262144 2019-11-11 10:47:46.189004: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0154b500 next 81 of size 262144 2019-11-11 10:47:46.189062: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0158b500 next 82 of size 262144 2019-11-11 10:47:46.189120: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf015cb500 next 83 of size 524288 2019-11-11 10:47:46.189179: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0164b500 next 84 of size 65536 2019-11-11 10:47:46.189237: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0165b500 next 85 of size 65536 2019-11-11 10:47:46.189299: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0166b500 next 86 of size 16384 2019-11-11 10:47:46.189358: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0166f500 next 87 of size 1048576 2019-11-11 10:47:46.189419: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf0176f500 next 88 of size 147456 2019-11-11 10:47:46.189477: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf01793500 next 89 of size 147456 2019-11-11 10:47:46.189534: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf017b7500 next 90 of size 147456 2019-11-11 10:47:46.189593: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf017db500 next 91 of size 12288 2019-11-11 10:47:46.189716: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0xf017de500 next 18446744073709551615 of size 1600256 2019-11-11 10:47:46.189777: I tensorflow/core/common_runtime/bfc_allocator.cc:809] Summary of in-use Chunks by size: 2019-11-11 10:47:46.189845: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 52 Chunks of size 256 totalling 13.0KiB 2019-11-11 10:47:46.189911: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 512 totalling 1.0KiB 2019-11-11 10:47:46.189976: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 1024 totalling 2.0KiB 2019-11-11 10:47:46.190040: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1280 totalling 1.2KiB 2019-11-11 10:47:46.190103: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 2048 totalling 4.0KiB 2019-11-11 10:47:46.190168: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 4096 totalling 8.0KiB 2019-11-11 10:47:46.190232: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 8192 totalling 16.0KiB 2019-11-11 10:47:46.190295: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 12288 totalling 12.0KiB 2019-11-11 10:47:46.190359: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 16384 totalling 16.0KiB 2019-11-11 10:47:46.190425: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 6 Chunks of size 65536 totalling 384.0KiB 2019-11-11 10:47:46.190491: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 131072 totalling 128.0KiB 2019-11-11 10:47:46.190586: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 3 Chunks of size 147456 totalling 432.0KiB 2019-11-11 10:47:46.190654: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 7 Chunks of size 262144 totalling 1.75MiB 2019-11-11 10:47:46.190722: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 3 Chunks of size 524288 totalling 1.50MiB 2019-11-11 10:47:46.190785: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 589824 totalling 2.25MiB 2019-11-11 10:47:46.190847: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1048576 totalling 1.00MiB 2019-11-11 10:47:46.190908: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1600256 totalling 1.53MiB 2019-11-11 10:47:46.190970: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 2097152 totalling 2.00MiB 2019-11-11 10:47:46.191033: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 11.02MiB 2019-11-11 10:47:46.191094: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 11554816 memory_limit_: 11554816 available bytes: 0 curr_region_allocation_bytes_: 23109632 2019-11-11 10:47:46.191203: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats: Limit: 11554816 InUse: 11554816 MaxInUse: 11554816 NumAllocs: 92 MaxAllocSize: 2097152 The same error appears in many topics when training the model, but here I am just trying to load the pre-trained weights. I am not understanding why only a small piece of memory is being employed.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm attempting to use the newest Yocto release to build an image to my Jetson Nano. For this I need CUDA 10 which is part of the NVIDIA binaries needed for the Jetson Nano image build. CUDA 10 only supports GCC 7, so to get this working I'm using the meta-linaro layer to add an external toolchain. I put in GCCVERSION = \"linaro-7.2\" SDKGCCVERSION = \"linaro-7.2\" In my local.conf and add the linaro layer to my bblayers.conf. When i then run bitbake image-name it then fails with the following error: WARNING: /home/tm/yocto/dev-jetson-nano-zeus/layers/meta-linaro/meta-aarch64/recipes-core/openjdk/openjdk-8_0.1.bb: Exception during build_dependencies for AUTOREV WARNING: /home/tm/yocto/dev-jetson-nano-zeus/layers/meta-linaro/meta-aarch64/recipes-core/openjdk/openjdk-8_0.1.bb: Error during finalise of /home/tm/yocto/dev-jetson-nano-zeus/layers/meta-linaro/meta-aarch64/recipes-core/openjdk/openjdk-8_0.1.bb ERROR: ExpansionError during parsing /home/tm/yocto/dev-jetson-nano-zeus/layers/meta-linaro/meta-aarch64/recipes-core/openjdk/openjdk-8_0.1.bb Traceback (most recent call last): File \"/home/tm/yocto/dev-jetson-nano-zeus/layers/poky-zeus/bitbake/lib/bb/fetch2/__init__.py\", line 1302, in FetchData.setup_revisions(d=&lt;bb.data_smart.DataSmart object at 0x7f3436da90b8&gt;): for name in self.names: &gt; self.revisions[name] = srcrev_internal_helper(self, d, name) File \"/home/tm/yocto/dev-jetson-nano-zeus/layers/poky-zeus/bitbake/lib/bb/fetch2/__init__.py\", line 1167, in srcrev_internal_helper(ud=&lt;bb.fetch2.FetchData object at 0x7f3436df5b00&gt;, d=&lt;bb.data_smart.DataSmart object at 0x7f3436da90b8&gt;, name='jdk8'): if srcrev == \"AUTOINC\": &gt; srcrev = ud.method.latest_revision(ud, d, name) File \"/home/tm/yocto/dev-jetson-nano-zeus/layers/poky-zeus/bitbake/lib/bb/fetch2/__init__.py\", line 1558, in Hg.latest_revision(ud=&lt;bb.fetch2.FetchData object at 0x7f3436df5b00&gt;, d=&lt;bb.data_smart.DataSmart object at 0x7f3436da90b8&gt;, name='jdk8'): revs = bb.persist_data.persist('BB_URI_HEADREVS', d) &gt; key = self.generate_revision_key(ud, d, name) try: File \"/home/tm/yocto/dev-jetson-nano-zeus/layers/poky-zeus/bitbake/lib/bb/fetch2/__init__.py\", line 1570, in Hg.generate_revision_key(ud=&lt;bb.fetch2.FetchData object at 0x7f3436df5b00&gt;, d=&lt;bb.data_smart.DataSmart object at 0x7f3436da90b8&gt;, name='jdk8'): def generate_revision_key(self, ud, d, name): &gt; key = self._revision_key(ud, d, name) return \"%s-%s\" % (key, d.getVar(\"PN\") or \"\") File \"/home/tm/yocto/dev-jetson-nano-zeus/layers/poky-zeus/bitbake/lib/bb/fetch2/hg.py\", line 223, in Hg._revision_key(ud=&lt;bb.fetch2.FetchData object at 0x7f3436df5b00&gt;, d=&lt;bb.data_smart.DataSmart object at 0x7f3436da90b8&gt;, name='jdk8'): \"\"\" &gt; return \"hg:\" + ud.moddir bb.data_smart.ExpansionError: Failure expanding variable SRCPV, expression was ${@bb.fetch2.get_srcrev(d)} which triggered exception AttributeError: 'FetchData' object has no attribute 'moddir' Summary: There were 2 WARNING messages shown. Summary: There was 1 ERROR message shown, returning a non-zero exit code. So it appears there is something wrong with the AUTOREV usage in the jdk bbfile? Any idea on how to fix this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Every time I am installing a package for my python3 via pip3 it downloads or browses cache and returns a similar error. I've already tried sudo -H flag but same results. ERROR: Command errored out with exit status 1: command: /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-0l02yx4z/numba/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-0l02yx4z/numba/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-install-0l02yx4z/numba/pip-egg-info cwd: /tmp/pip-install-0l02yx4z/numba/ Complete output (36 lines): Traceback (most recent call last): File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2857, in get_entry_map ep_map = self._ep_map File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2815, in __getattr__ raise AttributeError(attr) AttributeError: _ep_map During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; File \"/tmp/pip-install-0l02yx4z/numba/setup.py\", line 365, in &lt;module&gt; setup(**metadata) File \"/usr/local/lib/python3.6/dist-packages/setuptools/__init__.py\", line 144, in setup _install_setup_requires(attrs) File \"/usr/local/lib/python3.6/dist-packages/setuptools/__init__.py\", line 133, in _install_setup_requires (k, v) for k, v in attrs.items() File \"/usr/local/lib/python3.6/dist-packages/setuptools/dist.py\", line 444, in __init__ for ep in pkg_resources.iter_entry_points('distutils.setup_keywords'): File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 656, in &lt;genexpr&gt; for entry in dist.get_entry_map(group).values() File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2860, in get_entry_map self._get_metadata('entry_points.txt'), self File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2527, in parse_map for group, lines in data: File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3198, in split_sections for line in yield_lines(s): File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2387, in yield_lines for ss in strs: File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2767, in _get_metadata for line in self.get_metadata_lines(name): File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 1432, in get_metadata_lines return yield_lines(self.get_metadata(name)) File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 1424, in get_metadata return value.decode('utf-8') UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte in entry_points.txt file at path: /usr/local/lib/python3.6/dist-packages/tensorflow_gpu-1.14.0+nv19.9.dist-info/entry_points.txt ---------------------------------------- ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.",
        "answers": [
            [
                "if by any change you are using a Windows machine, you could probably enable utf-8 in the administrative regional settings: Enable utf-8 in Regional Settings this resolved this issue for me."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I've build an image for my Jetson Nano with yocto using the meta-tegra layer. This build is using u-boot as bootloader which is set to save the environment on an MMC partition (mmcblk0p14). gdisk -l /dev/mmcblk0 shows the following: Number Start (sector) End (sector) Size Code Name ... 14 20996096 20998143 1024.0 KiB 8300 UBOOTENV ... And the sector size is 512. I've then configured u-boot-tegra/include/configs/p3450-porg.h with: ... /* Env is located in it's own partition */ #define CONFIG_ENV_IS_IN_MMC #define CONFIG_SYS_MMC_ENV_DEV 1 #define CONFIG_ENV_OFFSET (20996096 * 512) ... Where CONFIG_ENV_OFFSET = Start_Sector * Block_Size This works fine (as far as I can see) as the environment is saved successfully to MMC when i use saveenv. However, the environment i get when i print it in u-boot shell is NOT the same as when i print the environment with fw_printenv u-boot tool. I have set the /etc/fw_env.config to: # Device name Device offset Env size /dev/mmcblk0p14 0 0x2000 So what I've gathered is that, either the fw_env.config is set wrong or the u-boot environment is saved somewhere else on the MMC and no the partition 14. Does anyone have suggestions to what i could try? *****************************************************EDIT:***************************************************** Doing dd if=/dev/mmcblk0p14 of=tmp.txt and reading the tmp.txt file shows the environment that the fw_printenv shows and not the environment I'm seeing in u-boot shell. So something must be wrong in the u-boot-tegra/include/configs/p3450-porg.h configuration. I just wonder where it actually writes the environment to when i do a saveenv... Any Idea what I can try to change?",
        "answers": [
            [
                "As stated in the comments to the question, the offset is a 32-bit integer so attempting to give it the value of more than 4,294,967,295 (which 20996096 * 512 is) is not gonna work. To fix it, I've rearranged my partition scheme to have my uboot environment partition as partition 1 instead of 14 and changed the fw_env.config and p3450-porg.h patch accordingly."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to launch gstreamer pipeline with the following code: #include &lt;cstdlib&gt; #include &lt;gst/gst.h&gt; #include &lt;gst/gstinfo.h&gt; #include &lt;gst/app/gstappsrc.h&gt; #include &lt;glib-unix.h&gt; #include &lt;dlfcn.h&gt; #include &lt;cstring&gt; #include &lt;iostream&gt; #include &lt;sstream&gt; #include &lt;thread&gt; using namespace std; #define USE(x) ((void)(x)) static GstPipeline *gst_pipeline = nullptr; static void *ptr = nullptr; int main(int argc, char** argv) { USE(argc); USE(argv); gst_init (&amp;argc, &amp;argv); GMainLoop *main_loop; main_loop = g_main_loop_new (NULL, FALSE); ostringstream launch_stream; string lstr = \"videotestsrc ! tee ! video/x-raw,format=BGR,width=1280,height=720 ! videoconvert \" \"! video/x-raw,format=I420 ! nvvidconv ! 'video/x-raw(memory:NVMM)' ! nvv4l2h264enc \" \"! h264parse ! matroskamux ! tcpserversink port=8888 host=0.0.0.0 \"; g_print(\"Using launch string: %s\\n\", lstr.c_str()); GError *error = nullptr; gst_pipeline = (GstPipeline*) gst_parse_launch(lstr.c_str(), &amp;error); if (gst_pipeline == nullptr) { g_print( \"Failed to parse launch: %s\\n\", error-&gt;message); return -1; } if(error) g_error_free(error); gst_element_set_state((GstElement*)gst_pipeline, GST_STATE_PLAYING); gst_element_set_state((GstElement*)gst_pipeline, GST_STATE_NULL); gst_object_unref(GST_OBJECT(gst_pipeline)); g_main_loop_unref(main_loop); free(ptr); g_print(\"going to exit \\n\"); return 0; } It fails with this errors: (gstreamer-opencv:2668): GStreamer-CRITICAL **: 20:17:03.138: gst_element_make_from_uri: assertion 'gst_uri_is_valid (uri)' failed (gstreamer-opencv:26678): GStreamer-CRITICAL **: 20:17:03.165: gst_element_link_pads_filtered: assertion 'GST_IS_BIN (parent)' failed (gstreamer-opencv:26678): GStreamer-CRITICAL **: 20:17:03.170: gst_element_link_pads_filtered: assertion 'GST_IS_BIN (parent)' failed Opening in BLOCKING MODE going to exit (same error happens when I put this chain into cv::VideoWriter) If I launch it from command line, it works correctly: gst-launch-1.0 videotestsrc ! tee ! video/x-raw,format=BGR,width=1280,height=720 ! videoconvert ! video/x-raw,format=I420 ! nvvidconv ! 'video/x-raw(memory:NVMM)' ! nvv4l2h264enc ! h264parse ! matroskamux ! tcpserversink port=8888 host=0.0.0.0 What is missing in my c code?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've created an image (with Yocto and meta-tegra) and got a Jetson Nano devkit running with two rootfs that RAUC updating uses. I am currently (temporarily during testing) using the Jetson Nano devkit SPI flash to store the u-boot environment. I have a script that I successfully got running on my device with tftp. I've also manually typed in every line of my script to an environment variable, saved it and set bootcmd to run it. But if i want more than one device with this u-boot, i wouldn't want to manually type in the script for every device i make. But I also don't want a tftp server to be running at the side to be able to boot. I want to run my script completely independent and automatically. So I've been thinking about having the script on a partition on it's own and get u-boot to load the script from there. But how do i tell u-boot to look for at script in the specific partition? Can i use source SCRIPT_PARTITION_OFFSET? I've also seen that the standard boot.scr can be run from the u-boot shell. Where is this boot.scr located? I can't seem to find where it is created or where it is stored. *****************************************************EDIT:***************************************************** As a temporary solution I have put the boot script image into /boot/ folder and set the u-boot to scan for scripts in the that folder on both rootfs and then run it if it can find it. For this solution i still have to edit the u-boot env to make u-boot boot this custom way, so it is not a solution i can use in the long run. I might as well make a script to run once via tftp that will set bootcmd to be the entire boot script content. Can i edit the u-boot source code with a patch that has my script in it's environment? - if so, how?",
        "answers": [
            [
                "As said in the comments, i was able to make a patch for u-boot-tegra/include/config_distro_bootcmd.h that has the variable distro_bootcmd set to my script content. When i then build my yocto image and boot up my Jetson Nano, i can see that the variable is in fact set. This way should work for any u-boot and not just for Jetson Nano. Just find the bootcmd variable in the u-boot that runs at default boot and patch it."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am using OpenGL in C++ (technically EGL, on a Jetson Nano.) Let's say I want to draw N Quads. Imagine just a list of colored rectangles. There may be a few thousand such rectangles in the frame. I want to use two vertex buffers: One that defines the geometry of each quad. One that defines the properties common to each quad. The first vertex buffer should define the geometry of each quad. It should have only 4 vertices in it and its data would be just the corners of a quad. Something like: 0, 0, // top left 1, 0, // top right 0, 1, // bottom left 1, 1, // bottom right Then the second vertex buffer should have just the x,y,width,height of all the rectangles. x1, y1, width1, height1, color1, x2, y2, width2, height2, color2, x3, y3, width3, height3, color3, x4, y4, width4, height4, color4, x5, y5, width5, height5, color5, x6, y6, width6, height6, color6, ... etc. The thing is that each one of the items in my rectangle buffer should apply to 4 vertices in the vertex buffer. Is there a way to set this up so that it keeps reusing the same 4 quad vertices over and over for each rectangle and applies the same rectangle properties to 4 vertices at a time? I'm imagining there's something I can do so that I say that the first vertex buffer should use one element per vertex and wraps around, but the second vertex buffer uses one element per every four vertices or something like that. How do I set this up? What I do now: Right now I need one vertex buffer that just has the quad vertices repeated over and over as many times as I have instances. 0, 0, // (1) top left 1, 0, // 0, 1, // 1, 1 // 0, 0, // (2) top left 1, 0, // 0, 1, // 1, 1, // 0, 0, // (3) top left 1, 0, // 0, 1, // 1, 1, // ... etc And my second buffer duplicates its data for each vertex: x1, y1, width1, height1, color1, x1, y1, width1, height1, color1, x1, y1, width1, height1, color1, x1, y1, width1, height1, color1, x2, y2, width2, height2, color2, x2, y2, width2, height2, color2, x2, y2, width2, height2, color2, x2, y2, width2, height2, color2, x3, y3, width3, height3, color3, x3, y3, width3, height3, color3, x3, y3, width3, height3, color3, x3, y3, width3, height3, color3, x4, y4, width4, height4, color4, x4, y4, width4, height4, color4, x4, y4, width4, height4, color4, x4, y4, width4, height4, color4, x5, y5, width5, height5, color5, x5, y5, width5, height5, color5, x5, y5, width5, height5, color5, x5, y5, width5, height5, color5, x6, y6, width6, height6, color6, x6, y6, width6, height6, color6, x6, y6, width6, height6, color6, x6, y6, width6, height6, color6, ... etc. This seems really inefficient and I just want to specify the first 4 vertices once and have it keep reusing them somehow rather than duplicating these 4 vertices N times to have a total of 4*N vertices in my first buffer. And I only want to specify the x,y,width,height,color attributes once for each quad for a total of N vertices, and not once for each overall vertex for a total of 4*N vertices. What do I do?",
        "answers": [
            [
                "Generally speaking, the most efficient way to render a series of quads is to... render a series of quads. You don't send width/height or other per-instance information; you compute the actual positions of the 4 vertices on the CPU and you write them to GPU memory using appropriate buffer object streaming techniques. Specifically, avoid trying to change only a few quads; if your data isn't static, it's probably going to be better to re-upload all of it (to a different/invalidated buffer) rather than modify only a few bytes in-situ. Your hypothetical alternative would only perform better in two scenarios: if the bandwidth of writing data to the GPU is your current bottleneck (whether due to quads or some other transfers you're doing) or if the bandwidth of reading data for rendering is the current bottleneck. You can mitigate this issue by reducing the size of the vertex data. Since we're talking 2D quads, you could very well use shorts for the XY position of each vertex. Or 16-bit floats. Either way, this means that each vertex (position + color) only takes up 8 bytes, which means a quad is just 32-bytes of data. Obviously 12 bytes is less than 32 (12 being the per-instance cost if you use similar compression), but it's still a 33% reduction over the 48 bytes that full float positions would use. If you have done your profiling homework and have determined that 32-bytes-per-quad is too much, vertex instancing is still a bad idea. It is well known that, on some hardware, extremely small instances can kill your VS performance. Therefore, it should be avoided. In this case, it may be best to forgo all vertex attribute usage (your VAO should have all arrays disable and your VS should have no in values defined). Instead, you should fetch instance data directly from SSBOs. The gl_VertexID input value tells you what vertex index is being rendered. Given that you're rendering quads, the current instance would be gl_VertexID / 4. And the current vertex within the quad is gl_VertexID % 4. So your VS would look something like this: struct instance { vec2 position; vec2 size; uint color; //Packed as 4 bytes; unpack with unpackUnorm4x8 uint padding; //Padding needed due to alignment/stride of 8 bytes. }; layout(binding = 0, std430) buffer instance_data { instance instances[]; }; vec2[4] vertex_table = { vec2{0, 0}, vec2{1, 0}, vec2{0, 1}, vec2{1, 1}, }; void main() { instance curr_instance = instances[gl_VertexID / 4]; vec2 vertex = vertex_table[gl_VertexID % 4]; vertex = curr_instance.position + (curr_instance.size * vertex); gl_Position = vec4(vertex.xy, 0.0, 1.0); } How fast this sort of thing will be depends entirely on how well your GPU handles these kinds of global memory reads. Note that it is at least hypothetically possible to reduce the size of the per-instance data back to 12. You can pack the position and size into two 16-bit shorts or half-floats, using unpackUnorm2x16 or unpackHalf2x16 to unpack these values, respectively. If you do this, then your instance struct is just 3 uint values, and there is no need for padding."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to run two inferences in a pipeline using Jetson Nano. The first inference is object detection using MobileNet and TensorRT. My code for the first inference is pretty much replicated from the AastaNV/TRT_Obj_Detection repository. The only difference being that I changed that code so that it resides inside a class Inference1. The second inference job uses the outputs of the first inference to run further analysis. For this inference, I use tensorflow (not TensorRT, but I assume it is called in the backend?) using a custom model. This model is loaded from a .pb file (frozen graph). Once loaded, the inference is performed by calling the session.run() command of tensorflow. If I run ONLY Inference1 or ONLY Inference2, the code runs properly without any errors. However, when I pipe them, I get the error [TensorRT] ERROR: cuda/caskConvolutionLayer.cpp (355) - Cuda Error in execute: 33 (invalid resource handle) From what I see in the log, the TensorRT serialized graph is loaded without any problems. Tensorflow is also imported and it recognizes my GPU. From my searching on the internet I have found that this problem maybe related to CUDA Contexts? I therefore show below how i have setup the CUDA context in my code below. The create_cuda_context is only called once during the initialization of the Inference1 class. The run_inference_for_single_image is called every iteration. Code: def create_cuda_context(self): self.host_inputs, self.host_outputs = [], [] self.cuda_inputs, self.cuda_outputs = [], [] self.bindings = [] self.stream = cuda.Stream() for binding in self.engine: size = trt.volume(self.engine.get_binding_shape(binding)) * self.engine.max_batch_size host_mem = cuda.pagelocked_empty(size, np.float32) cuda_mem = cuda.mem_alloc(host_mem.nbytes) self.bindings.append(int(cuda_mem)) if self.engine.binding_is_input(binding): self.host_inputs.append(host_mem) self.cuda_inputs.append(cuda_mem) else: self.host_outputs.append(host_mem) self.cuda_outputs.append(cuda_mem) self.context = self.engine.create_execution_context() def run_inference_for_single_image(self, image): ''' Copies the image (already raveled) input into GPU memory, performs the forward pass and copies the result back to CPU memory ''' np.copyto(self.host_inputs[0], image) cuda.memcpy_htod_async(self.cuda_inputs[0], self.host_inputs[0], self.stream) self.context.execute_async(bindings=self.bindings, stream_handle=self.stream.handle) cuda.memcpy_dtoh_async(self.host_outputs[1], self.cuda_outputs[1], self.stream) cuda.memcpy_dtoh_async(self.host_outputs[0], self.cuda_outputs[0], self.stream) self.stream.synchronize() return self.host_outputs[0] Log: WARNING:tensorflow:From /usr/lib/python3.6/dist-packages/graphsurgeon/DynamicGraph.py:4: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead. [TensorRT] INFO: Glob Size is 14049908 bytes. [TensorRT] INFO: Added linear block of size 5760000 [TensorRT] INFO: Added linear block of size 2880000 [TensorRT] INFO: Added linear block of size 409600 [TensorRT] INFO: Added linear block of size 218624 [TensorRT] INFO: Added linear block of size 61440 [TensorRT] INFO: Added linear block of size 57344 [TensorRT] INFO: Added linear block of size 30720 [TensorRT] INFO: Added linear block of size 20992 [TensorRT] INFO: Added linear block of size 9728 [TensorRT] INFO: Added linear block of size 9216 [TensorRT] INFO: Added linear block of size 2560 [TensorRT] INFO: Added linear block of size 2560 [TensorRT] INFO: Added linear block of size 1024 [TensorRT] INFO: Added linear block of size 512 [TensorRT] INFO: Found Creator FlattenConcat_TRT [TensorRT] INFO: Found Creator GridAnchor_TRT [TensorRT] INFO: Found Creator FlattenConcat_TRT [TensorRT] INFO: Found Creator NMS_TRT [TensorRT] INFO: Deserialize required 5159079 microseconds. Infering on input.mp4 WARNING:tensorflow:From /home/user/Desktop/SVM_TensorRT/deep_sort/tools/generate_detections.py:75: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead. 2018-01-29 02:01:38.254282: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1 2018-01-29 02:01:38.286962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.287300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: name: NVIDIA Tegra X1 major: 5 minor: 3 memoryClockRate(GHz): 0.9216 pciBusID: 0000:00:00.0 2018-01-29 02:01:38.287552: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2018-01-29 02:01:38.287744: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 2018-01-29 02:01:38.287983: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0 2018-01-29 02:01:38.288201: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0 2018-01-29 02:01:38.415478: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0 2018-01-29 02:01:38.484010: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0 2018-01-29 02:01:38.484668: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7 2018-01-29 02:01:38.485343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.486009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.486286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0 2018-01-29 02:01:38.665379: W tensorflow/core/platform/profile_utils/cpu_utils.cc:98] Failed to find bogomips in /proc/cpuinfo; cannot determine CPU frequency 2018-01-29 02:01:38.682935: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24f9ea50 executing computations on platform Host. Devices: 2018-01-29 02:01:38.683009: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt; 2018-01-29 02:01:38.764975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.765291: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x572614c0 executing computations on platform CUDA. Devices: 2018-01-29 02:01:38.765349: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): NVIDIA Tegra X1, Compute Capability 5.3 2018-01-29 02:01:38.766014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.766158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: name: NVIDIA Tegra X1 major: 5 minor: 3 memoryClockRate(GHz): 0.9216 pciBusID: 0000:00:00.0 2018-01-29 02:01:38.766716: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2018-01-29 02:01:38.766814: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 2018-01-29 02:01:38.766879: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0 2018-01-29 02:01:38.767002: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0 2018-01-29 02:01:38.767174: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0 2018-01-29 02:01:38.767311: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0 2018-01-29 02:01:38.767423: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7 2018-01-29 02:01:38.767731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.768049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.768136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0 2018-01-29 02:01:38.783718: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2018-01-29 02:01:41.046094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix: 2018-01-29 02:01:41.046260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187] 0 2018-01-29 02:01:41.046311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0: N 2018-01-29 02:01:41.054160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:41.054730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:41.112041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 85 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0, compute capability: 5.3) WARNING:tensorflow:From /home/user/Desktop/SVM_TensorRT/deep_sort/tools/generate_detections.py:76: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead. WARNING:tensorflow:From /home/user/Desktop/SVM_TensorRT/deep_sort/tools/generate_detections.py:80: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. [TensorRT] ERROR: CUDA cask failure at execution for trt_maxwell_scudnn_128x32_relu_small_nn_v1. [TensorRT] ERROR: cuda/caskConvolutionLayer.cpp (355) - Cuda Error in execute: 33 (invalid resource handle) [TensorRT] ERROR: cuda/caskConvolutionLayer.cpp (355) - Cuda Error in execute: 33 (invalid resource handle)",
        "answers": [
            [
                "I believe the two models you are trying to run both try to create CUDA Context. The first one initialize CUDA Context directly from TensorRT library, while the 2nd one initize new CUDA Context inside Tensorflow. When the 1st model tries to perform inferencing, it will use wrong CUDA Context, resulting with that error. If you are using the same TensorRT, Tensorflow (or other CUDA Libraries) for both models, it will be a lot easier to control CUDA Context. From my experience, Tensorflow and direct CUDA are not playing along well. I would suggest that you separate both models into different thread. That will ensure that both TensorRT and Tensorflow create and use their own different CUDA Contexts... (given that you are not running to OOM problem. I once tried to use both SSD+MobileNetV2 for object detection and another MobileNet for more classification on detected object on Jetson Nano. I faced the OOM and ended up running the 2nd model on CPU instead)."
            ],
            [
                "I had a similar error and that is what helped me in this case: Remove import pycuda.autoinit and do import pycuda.driver as cuda ... cuda.init() device = cuda.Device(0) cuda_driver_context = device.make_context() Wrap the piece of code that does inference with TensorRT like this: cuda_driver_context.push() # copy data to device memory, run inference, copy data from device memory cuda_driver_context.pop() See the related thread on NVidia's forum."
            ]
        ],
        "votes": [
            5.0000001,
            1e-07
        ]
    },
    {
        "question": "This code cap = cv2.VideoCapture('/dev/video0', cv2.CAP_V4L2) gives VIDEOIO ERROR: V4L2: Pixel format of incoming image is unsupported by OpenCV Here is the output of v4l2-ctl -d /dev/video0 --list-formats: ioctl: VIDIOC_ENUM_FMT Index : 0 Type : Video Capture Pixel Format: 'RG10' Name : 10-bit Bayer RGRG/GBGB Here is my OpenCV: -- OpenCL samples are skipped: OpenCL SDK is required -- -- General configuration for OpenCV 4.1.2-pre ===================================== -- Version control: 4.1.1-365-g9efafc3e3 -- -- Extra modules: -- Location (extra): /home/sovlyn/Downloads/opencv_contrib/modules -- Version control (extra): 4.1.1-62-g83e98d24 -- -- Platform: -- Timestamp: 2019-10-10T09:26:02Z -- Host: Linux 4.9.140-tegra aarch64 -- CMake: 3.10.2 -- CMake generator: Unix Makefiles -- CMake build tool: /usr/bin/make -- Configuration: RELEASE -- -- CPU/HW features: -- Baseline: NEON FP16 -- required: NEON -- disabled: VFPV3 -- -- C/C++: -- Built as dynamic libs?: YES -- C++ Compiler: /usr/bin/c++ (ver 7.4.0) -- C++ flags (Release): -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -DNDEBUG -- C++ flags (Debug): -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -fvisibility-inlines-hidden -g -O0 -DDEBUG -D_DEBUG -- C Compiler: /usr/bin/cc -- C flags (Release): -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -O3 -DNDEBUG -DNDEBUG -- C flags (Debug): -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -g -O0 -DDEBUG -D_DEBUG -- Linker flags (Release): -Wl,--gc-sections -- Linker flags (Debug): -Wl,--gc-sections -- ccache: NO -- Precompiled headers: NO -- Extra dependencies: dl m pthread rt -- 3rdparty dependencies: -- -- OpenCV modules: -- To be built: aruco bgsegm bioinspired calib3d ccalib core datasets dnn dnn_objdetect dnn_superres dpm face features2d flann freetype fuzzy gapi hfs highgui img_hash imgcodecs imgproc line_descriptor ml objdetect optflow phase_unwrapping photo plot python3 quality reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab xfeatures2d ximgproc xobjdetect xphoto -- Disabled: world -- Disabled by dependency: - -- Unavailable: cnn_3dobj cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev cvv hdf java js matlab ovis python2 sfm viz -- Applications: tests perf_tests examples apps -- Documentation: NO -- Non-free algorithms: YES -- -- GUI: -- GTK+: YES (ver 2.24.32) -- GThread : YES (ver 2.56.4) -- GtkGlExt: NO -- VTK support: NO -- -- Media I/O: -- ZLib: /usr/lib/aarch64-linux-gnu/libz.so (ver 1.2.11) -- JPEG: /usr/lib/aarch64-linux-gnu/libjpeg.so (ver 80) -- WEBP: build (ver encoder: 0x020e) -- PNG: /usr/lib/aarch64-linux-gnu/libpng.so (ver 1.6.34) -- TIFF: /usr/lib/aarch64-linux-gnu/libtiff.so (ver 42 / 4.0.9) -- JPEG 2000: build (ver 1.900.1) -- OpenEXR: build (ver 2.3.0) -- HDR: YES -- SUNRASTER: YES -- PXM: YES -- PFM: YES -- -- Video I/O: -- DC1394: YES (2.2.5) -- FFMPEG: YES -- avcodec: YES (57.107.100) -- avformat: YES (57.83.100) -- avutil: YES (55.78.100) -- swscale: YES (4.8.100) -- avresample: NO -- GStreamer: YES (1.14.5) -- v4l/v4l2: YES (linux/videodev2.h) -- -- Parallel framework: TBB (ver 2017.0 interface 9107) -- -- Trace: YES (with Intel ITT) -- -- Other third-party libraries: -- Lapack: NO -- Eigen: YES (ver 3.3.4) -- Custom HAL: YES (carotene (ver 0.0.1)) -- Protobuf: build (3.5.1) -- -- OpenCL: YES (no extra features) -- Include path: /home/sovlyn/Downloads/opencv/3rdparty/include/opencl/1.2 -- Link libraries: Dynamic load -- -- Python 3: -- Interpreter: /usr/bin/python3 (ver 3.6.8) -- Libraries: /usr/lib/aarch64-linux-gnu/libpython3.6m.so (ver 3.6.8) -- numpy: /usr/local/lib/python3.6/dist-packages/numpy/core/include (ver 1.17.2) -- install path: lib/python3.6/dist-packages/cv2/python-3.6 -- -- Python (for build): /usr/bin/python3 -- -- Java: -- ant: NO -- JNI: NO -- Java wrappers: NO -- Java tests: NO -- -- Install to: /usr/local",
        "answers": [
            [
                "If you are using pi-cam on jetson, then use below pipe line to get video stream from MIPI CSI camera: cv2.VideoCapture('nvarguscamerasrc ! video/x-raw(memory:NVMM), width=(int)640, height=(int)480, format=(string)NV12, framerate=(fraction)60/1 ! nvvidconv flip-method=0 ! video/x-raw, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink', cv2.CAP_GSTREAMER) Change flip-method to 0 or 2 (for more options, see here). Good luck!"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "This is a continuation of my other post. I've managed to create an image with u-boot and rauce. I've made a simple rauc system.conf: [system] compatible=Jetson Nano bootloader=uboot # [slot.rootfs.0] device=/dev/mmcblk0p1 type=ext4 bootname=system0 # [slot.rootfs.1] device=/dev/mmcblk0p13 type=ext4 bootname=system1 [UPDATED]: Pretty much copy pasted the contrib uboot.sh script. Then I've added a bb file from here into my bsp layer. And added rauc to my IMAGE_INSTALL. When i boot up the nano with my image, rauc isn't working as it should. When i check the status on the service with systemctl status rauc-mark-service-good.service it returns: \u25cf rauc-mark-good.service - Rauc Good-marking Service Loaded: loaded (/lib/systemd/system/rauc-mark-good.service; enabled; vendor preset: enabled) Active: inactive (dead) since Tue 2019-10-01 07:51:22 UTC; 4s ago Process: 4147 ExecStart=/usr/bin/rauc status mark-good (code=exited, status=0/SUCCESS) Main PID: 4147 (code=exited, status=0/SUCCESS) Oct 01 07:51:22 jetson-nano systemd[1]: Started Rauc Good-marking Service. Oct 01 07:51:22 jetson-nano rauc[4147]: Failed getting primary slot: Failed getting primary slot: Unable to find primary boot slot Oct 01 07:51:22 jetson-nano rauc[4147]: rauc mark: marked slot rootfs.0 as good Oct 01 07:51:22 jetson-nano systemd[1]: rauc-mark-good.service: Succeeded. systemctl status rauc returns: \u25cf rauc.service - Rauc Update Service Loaded: loaded (/lib/systemd/system/rauc.service; static; vendor preset: enabled) Active: active (running) since Tue 2019-10-01 07:49:36 UTC; 2min 0s ago Docs: https://rauc.readthedocs.io Main PID: 4092 (rauc) Tasks: 3 (limit: 4178) Memory: 4.4M CGroup: /system.slice/rauc.service \u2514\u25004092 /usr/bin/rauc --mount=/run/rauc service Oct 01 07:49:36 jetson-nano systemd[1]: Starting Rauc Update Service... Oct 01 07:49:36 jetson-nano systemd[1]: Started Rauc Update Service. Oct 01 07:49:48 jetson-nano rauc[4092]: Failed getting primary slot: Failed getting primary slot: Unable to find primary boot slot Oct 01 07:49:48 jetson-nano rauc[4092]: Failed to load status file /slot.raucs: No such file or directory Oct 01 07:49:48 jetson-nano rauc[4092]: mounting slot /dev/mmcblk0p13 Oct 01 07:49:48 jetson-nano rauc[4092]: Failed to load status file /run/rauc/rootfs.1/slot.raucs: No such file or directory Oct 01 07:51:22 jetson-nano rauc[4092]: Failed getting primary slot: Failed getting primary slot: Unable to find primary boot slot Oct 01 07:51:22 jetson-nano rauc[4092]: rauc mark: marked slot rootfs.0 as good And rauc status returns: (rauc:4195): rauc-WARNING **: 07:51:46.126: Failed getting primary slot: Failed getting primary slot: Unable to find primary boot slot Compatible: Jetson Nano Variant: Booted from: rootfs.0 (/dev/mmcblk0p1) Activated: (null) ((null)) slot states: rootfs.0: class=rootfs, device=/dev/mmcblk0p1, type=ext4, bootname=system0 state=booted, description=, parent=(none), mountpoint=/ boot status=bad rootfs.1: class=rootfs, device=/dev/mmcblk0p13, type=ext4, bootname=system1 state=inactive, description=, parent=(none), mountpoint=(none) boot status=bad So there is no /slot.raucs file and it failed to find primary boot slot. After that, systemctl status rauc-mark-good returns that the rootfs.0 slot has been marked as good in the end, but systemctl status rauc shows that the boot status is bad. What am I missing here?",
        "answers": [
            [
                "I edited the uboot script to the following: test -n \"${BOOT_ORDER}\" || setenv BOOT_ORDER \"system0 system1\" test -n \"${BOOT_system0_LEFT}\" || setenv BOOT_system0_LEFT 3 test -n \"${BOOT_system1_LEFT}\" || setenv BOOT_system1_LEFT 3 setenv bootargs for BOOT_SLOT in \"${BOOT_ORDER}\"; do if test \"x${bootargs}\" != \"x\"; then # skip remaining slots elif test \"x${BOOT_SLOT}\" = \"xsystem0\"; then if test ${BOOT_system0_LEFT} -gt 0; then setexpr BOOT_system0_LEFT ${BOOT_system0_LEFT} - 1 echo \"Found valid slot system0, ${BOOT_system0_LEFT} attempts remaining\" setenv distro_bootpart \"1\" setenv boot_line \"mmc 1:1 any ${scriptaddr} /boot/extlinux/extlinux.conf\" fi elif test \"x${BOOT_SLOT}\" = \"xsystem1\"; then if test ${BOOT_system1_LEFT} -gt 0; then setexpr BOOT_system1_LEFT ${BOOT_system1_LEFT} - 1 echo \"Found valid slot system1, ${BOOT_system1_LEFT} attempts remaining\" setenv distro_bootpart \"13\" setenv boot_line \"mmc 1:D any ${scriptaddr} /boot/extlinux/extlinux.conf\" fi fi done if test -n \"${bootargs}\"; then saveenv else echo \"No valid slot found, resetting tries to 3\" setenv BOOT_system0_LEFT 3 setenv BOOT_system1_LEFT 3 saveenv reset fi sysboot ${boot_line} And it ended up working. Apparently there was some issues with the BOOT_ORDER \"system0 system1\" in the the uboot script that was somehow not the same as in the RAUC system.conf. When i re-wrote the script, there was no issues and RAUC was running fine."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "We are getting below exception while opening Serial Port on arm64 Jetson board. Its working fine on Linux computer but not on Jetson board. Can someone please help on this to resolve? Invalid Argument at System.IO.Ports.SerialStream.set_DtrEnable(Boolean value) at System.IO.Ports.SerialStream..ctor Code : SerialPort _serialPort; _serialPort = new SerialPort(\"/dev/ttyUSB1\", 4800, Parity.None, 8, StopBits.One); _serialPort.ReadTimeout = 2000; _serialPort.DtrEnable = true; _serialPort.RtsEnable = true; try { var port = System.IO.Ports.SerialPort.GetPortNames(); Console.WriteLine(\"available ports : \" + String.Join(\",\", port)); Console.WriteLine(\"checking for open --- \" + _serialPort.IsOpen); /// Check if connection is open or not, if not open it if (!_serialPort.IsOpen) _serialPort.Open();",
        "answers": [],
        "votes": []
    },
    {
        "question": "On a Jetson TX2 I am running: Linux4Tegra R32.2.1 UFF Version 0.6.3 TensorRT 5.1.6.1 Cuda 10 Python 3.6.8 I get this error message: [TensorRT] ERROR: UffParser: Validator error: sequential/batch_normalization_1/FusedBatchNormV3: Unsupported operation _FusedBatchNormV3 From this code: output_nodes = [args.output_node_names] input_node = args.input_node_name frozen_graph_pb = args.frozen_graph_pb uff_model = uff.from_tensorflow(frozen_graph_pb, output_nodes) . #Successfully creates uff model network = builder.create_network() G_LOGGER = trt.Logger(trt.Logger.INFO) builder = trt.Builder(G_LOGGER) builder.max_batch_size = 10 builder.max_workspace_size = 1 &lt;&lt; 30 data_type = trt.DataType.FLOAT parser = trt.UffParser() input_verified =parser.register_input(input_node, (1,234,234,3)) #returns true output_verified = parser.register_output(output_nodes[0]) #returns true buffer_verified = parser.parse_buffer(uff_model, network, data_type) #returns false The uff model was created successfully. The parser successfully registered the inputs and outputs. Parsing the buffer fails with the error above. Does anyone know if FusedBatchNormV3 is truly not supported in tensorRT and if not is there an existing plugin that I can pull using the graph surgeon module?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've managed to create an image with two rootfs partitions to run on my jetson nano with yocto/poky. I've followed the meta-rauc layer README and rauc user manual, to create the system.conf file and rauc_%.bbappend file and I am able to create bundles successfully. As I understand, I need some sort of u-boot script: In order to enable RAUC to switch the correct slot, its system configuration must specify the name of the respective slot from the bootloader\u2019s perspective. You also have to set up an appropriate boot selection logic in the bootloader itself, either by scripting (as for GRUB, U-Boot) or by using dedicated boot selection infrastructure (such as bootchooser in Barebox). The bootloader must also provide a set of variables the Linux userspace can modify in order to change boot order or priority. Having this interface ready, RAUC will care for setting the boot logic appropriately. It will, for example, deactivate the slot to update before writing to it and reactivate it after having completed the installation successfully. Do I make a script somewhere in the yocto layer or build folder or is it a script i need to put on the jetson nano after making the image? - and what would the contents of this script be? **************************************************EDIT******************************************************** I've made this script: test -n \"${BOOT_ORDER}\" || setenv BOOT_ORDER \"system0 system1\" test -n \"${BOOT_system0_LEFT}\" || setenv BOOT_system0_LEFT 3 test -n \"${BOOT_system1_LEFT}\" || setenv BOOT_system1_LEFT 3 setenv bootargs for BOOT_SLOT in \"${BOOT_ORDER}\"; do if test \"x${bootargs}\" != \"x\"; then # skip remaining slots elif test \"x${BOOT_SLOT}\" = \"xsystem0\"; then if test ${BOOT_system0_LEFT} -gt 0; then setexpr BOOT_system0_LEFT ${BOOT_system0_LEFT} - 1 echo \"Found valid slot system0, ${BOOT_system0_LEFT} attempts remaining\" setenv distro_bootpart \"1\" setenv boot_line \"mmc 1:1 any ${scriptaddr} /boot/extlinux/extlinux.conf\" setenv bootargs \"${default_bootargs} root=/dev/mmcblk0p1 rauc.slot=system0\" fi elif test \"x${BOOT_SLOT}\" = \"xsystem1\"; then if test ${BOOT_system1_LEFT} -gt 0; then setexpr BOOT_system1_LEFT ${BOOT_system1_LEFT} - 1 echo \"Found valid slot system1, ${BOOT_system1_LEFT} attempts remaining\" setenv distro_bootpart \"13\" setenv boot_line \"mmc 1:D any ${scriptaddr} /boot/extlinux/extlinux.conf\" setenv bootargs \"${default_bootargs} root=/dev/mmcblk0p13 rauc.slot=system1\" fi fi done if test -n \"${bootargs}\"; then saveenv else echo \"No valid slot found, resetting tries to 3\" setenv BOOT_system0_LEFT 3 setenv BOOT_system1_LEFT 3 saveenv reset fi sysboot ${boot_line} And I i got this recipe recipes-bsp/u-boot/u-boot-script.bb in my meta-layer: LICENSE = \"GPLv2+\" LIC_FILES_CHKSUM = \"file://Licenses/README;md5=30503fd321432fc713238f582193b78e\" S = \"${WORKDIR}/git\" PACKAGE_ARCH = \"${MACHINE_ARCH}\" DEPENDS = \"u-boot-mkimage-native\" inherit deploy BOOTSCRIPT ??= \"${THISDIR}/uboot.sh\" do_mkimage () { uboot-mkimage -A arm -O linux -T script -C none -a 0 -e 0 \\ -n \"boot script\" -d ${BOOTSCRIPT} ${S}/boot.scr } addtask mkimage after do_compile before do_install do_compile[noexec] = \"1\" do_install () { install -D -m 644 ${S}/boot.scr ${D}/boot.scr } do_deploy () { install -D -m 644 ${D}/boot.scr \\ ${DEPLOYDIR}/boot.scr-${MACHINE}-${PV}-${PR} cd ${DEPLOYDIR} rm -f boot.scr-${MACHINE} ln -sf boot.scr-${MACHINE}-${PV}-${PR} boot.scr-${MACHINE} } addtask deploy after do_install before do_build FILES_${PN} += \"/\" COMPATIBLE_MACHINE = \"jetson-nano\" I can see that the script image is getting into work/jetson_nano_poky-linux/u-boot-tegra/2016.07.../git/ folder. But how do I use it in u-boot? - How do i make sure this script is run automatically every boot?",
        "answers": [
            [
                "U boot part of the default boot sequence tries to find a file named boot.src in the first partition from where it has booted. if this file is found then it will try to run this script. The commands put in the file can be based on RAUC syntax so that when RAUC gets activated in the user space it can update the same environment variables. So RAUC handles the boot sequence via the commands put int the script file. RAUC has no way to directly alter the flow of U Boot boot up sequence"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "The program should run on boot. the nano operating system (jet-pack) not allowing the auto-login too. I tried to put the script into its startup file but the program doesn't boot.",
        "answers": [
            [
                "Run you script in cron job. sudo crontab -e * * * /usr/bin/python my_script.py This will make your script run on boot. Kindly provide complete paths in script to run error-free"
            ]
        ],
        "votes": [
            -0.9999999
        ]
    },
    {
        "question": "If I try to bitbake an image bitbake name-of-image with local.conf containing this: \u2026 WKS_FILE=\"directdisk-multi-rootfs.wks\" IMAGE_FSTYPES = \"wic wic.bmap\" \u2026 Then the build exits with error: ERROR: Couldn't find correct bootimg_dir, exiting If I try to run the wic command in cooked mode, the same error occours. And if I attempt to run wic in raw mode: wic create directdisk-multi-rootfs -e name-of-image --rootfs-dir rootfs1=/home/user/yocto/dev-jetson-nano/build/tmp/work/jetson_nano-poky-linux/name-of-image/1.0-r0/rootfs/ --rootfs-dir rootfs2=/home/user/yocto/dev-jetson-nano/build/tmp/work/jetson_nano-poky-linux/name-of-image/1.0-r0/rootfs/ -b /home/user/yocto/dev-jetson-nano/build/tmp/work/jetson_nano-poky-linux/name-of-image/1.0-r0/recipe-sysroot/usr/share -k /home/user/yocto/dev-jetson-nano/build/tmp/deploy/images/jetson-nano -n /home/user/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/wic-tools/1.0-r0/recipe-sysroot-native I still get the same error. I need to create an image for the jetson-nano that can use RAUC update tool which needs two rootfs to work. Wic tool seems to be able to do that. How to upload it and if it will even work on the jetson nano is another question, but right now I just want to be able to make an image with wic. EDIT: As this is for a SD-card I made my own version of the \"directdisk-multi-rootfs.wks\" file with this: part /boot --source bootimg-partition --ondisk mmcblk0 --fstype=vfat --label boot --active --align 1024 --sourceparams=\"loader=u-boot\" part / --source rootfs --rootfs-dir=rootfs1 --ondisk mmcblk --fstype=ext4 --label platform --align 1024 part /rescue --source rootfs --rootfs-dir=rootfs2 --ondisk mmcblk --fstype=ext4 --label secondary --align 1024 bootloader --timeout=0 --append=\"rootwait rootfstype=ext4 video=vesafb vga=0x318 console=tty0 console=ttyS0,115200n8\" This gives me a new but very similar error: ERROR: No boot files defined, IMAGE_BOOT_FILES unset for entry #1",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm building a yocto image to run on the Jetson Nano. Right now I'm working on a Jetson Nano devkit which boots from the SD-card, and the flashing is described on the meta-tegra GitHub repo wiki. It doesn't say how to flash onto the eMMC on the Jetson Nano, only on the SDcard. Can I copy the yocto build rootfs to the nvidia_sdk L4T tools (replacing the 'rootfs' folder)? But what about the rest of the folders (bootloader, kernel, lib, nv_tegra)? It should be the same binaries, I'm just not so sure the kernel and bootloader is the same, and don't really know about the rest. Anyone dealing with the same issue or, even better, found a way to do this, please let me know.",
        "answers": [
            [
                "I had a conversation with the maintainer of the meta-tegra layer and ended up with creating a new machine configuration: #@TYPE: Machine #@NAME: Nvidia Jetson Nano #@DESCRIPTION: Nvidia Jetson Nano prod board KERNEL_ARGS ?= \"console=ttyS0,115200 console=tty0 fbcon=map:0 net.ifnames=0\" KERNEL_ROOTSPEC ?= \"root=/dev/mmcblk0p${@uboot_var('distro_bootpart')} rw rootwait\" IMAGE_ROOTFS_ALIGNMENT ?= \"1024\" require conf/machine/include/tegra210.inc KERNEL_DEVICETREE ?= \"_ddot_/_ddot_/_ddot_/_ddot_/nvidia/platform/t210/porg/kernel-dts/tegra210-p3448-0002-p3449-0000-b00.dtb\" MACHINE_FEATURES += \"ext2 ext3 vfat\" UBOOT_MACHINE = \"p3450-porg_defconfig\" EMMC_SIZE ?= \"17179869184\" EMMC_DEVSECT_SIZE ?= \"512\" BOOTPART_SIZE ?= \"\" BOOTPART_LIMIT ?= \"10485760\" ROOTFSPART_SIZE ?= \"3221225472\" ODMDATA ?= \"0x94000\" EMMC_BCT ?= \"P3448_A00_4GB_Micron_4GB_lpddr4_204Mhz_P987.cfg\" NVIDIA_BOARD ?= \"t210ref\" NVIDIA_PRODUCT ?= \"p3450-porg\" NVIDIA_BOARD_CFG ?= \"\" TEGRA210_REDUNDANT_BOOT ?= \"0\" PARTITION_LAYOUT_TEMPLATE ?= \"flash_l4t_t210_emmc_p3448.xml\" TEGRA_SPIFLASH_BOOT ?= \"0\" TEGRA_FAB ?= \"300\" TEGRA_BOARDID ?= \"3448\" The machine configuration is almost identical to the devkit's, but some parts had to be changed to match to Jetson Nano Production Module configurations, i.e. change the KERNEL_DEVICETREE the the one matching the newer eMMC Jetson Nano and change TEGRA_FAB accordingly. Then change the PARTITION_LAYOUT_TEMPLATE to match the emmc layout instead of the spi_sd layout (the flash_l4t_t210_emmc_p3448 is the default p3448 emmc layout provided with meta-tegra). After this, Yocto will produce a tegraflash zip that contains the necessary partition files and a rootfs image (along side some flashing tools). Put the Jetson Nano production module into recovery mode (FORCE RECOVERY + RESET), plug in the micro-usb cable and run the doflash.sh script to flash the nano, and voila."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to test out my Pi Camera on my Jetson Nano. Simple tests run smooth but running this python code edited to have the videoCapture argument of a gstreamer_pipeline instead of just '0'. This code however, only works once. When closing it down, I get errors: (Argus) Error EndOfFile: Unexpected error in reading socket (in src/rpc/socket/client/ClientSocketManager.cpp, function recvThreadCore(), line 266) (Argus) Error EndOfFile: Receive worker failure, notifying 1 waiting threads (in src/rpc/socket/client/ClientSocketManager.cpp, function recvThreadCore(), line 357) (Argus) Error InvalidState: Argus client is exiting with 1 outstanding client threads (in src/rpcsocket/client/ClientSocketManager.cpp, function recvThreadWrapper(), line 368) (Argus) Error EndOfFile: Receiving thread terminated with error (in src/rpcsocket/client/ClientSocketManager.cpp, function recvThreadWrapper(), line 368) (Argus) Error EndOfFile: Client thread received an error from socket (in src/rpcsocket/client/ClientSocketManager.cpp, function send(), line 145) (Argus) Error EndOfFile: (propagating from src/rpc/socket/client/socketClientDIspatch.cpp, function dispatch(), line 87) WARNING Argus: 5 client objects still exist during shutdown: 547548506088 (0x7f4c0034a8) 547554717136 (0x7f4c001660) 547554717296 (0x7f4c001700) 547554722384 (0x7f4c001860) 547554723520 (0x7f4c003390) Then afterwards I am unable to start the camera up any way I try, and the following error is shown every time on any code: (Argus) Error FileOperationFailed: Connecting to nvargus-daemon failed: Connection refused (in src/rpc/socket/client/SocketClientDispatch.cpp, function openSocketConnection(), line 201) (Argus) Error FileOperationFailed: Cannot create camera provider (in src/rpc/socket/client/SocketClientDispatch.cpp, function createCameraProvider(), line 102) Error generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst-nvarguscamera/gstnvarguscamerasrc.cpp, execute:515 Failed to create CameraProvider If I restart the device, I can run all the camera codes I want again, until I run that code again. As I look through it, I don't see what is needed to fix it. The camera is released, what else is needed?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've created images for the Jetson nano using the meta-tegra layer and flashed it so my jetson nano with a 32gb sd-card. When building an image, the default size of the .sdcard file that is needed to be flashed to the SD-card, is about 16GB . It seems very overkill to have a very VERY basic image, with a root file system .sdcard file with the size of 16GB. After flashing, the SD-card is split into 13 parts, as seen from gparted: When booting the device, I have 1.83gb to use... on a 32gb sd-card with a extremely basic image... I can see there is 13.66gb free on the sdb13 part, but while able to unallocated the space from the sdb13 part, I am unable to allocate it to the sdb1 part due to error. This is however not the point of this post. Why is file size so big?- and is there any way to somehow minimize this size?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've build a yocto/poky warrior branch image. As a standard it has OpenCV 3.4.5 via the openEmbedded warrior branch layer. But the master branch of openEmbedded has OpenCV 4.1.0, so I was wondering if I could just copy paste that folder into my warrior build to replace OpenCV 3.4.5? Can it be that simple?",
        "answers": [
            [
                "Usually it will not compile. But if you go from warrior to thud (lower version), you will find a recipe for OpenCV 4.0.1 at https://layers.openembedded.org/layerindex/recipe/97501/"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I've created a minimal xfce image with Yocto/poky on a Jetson Nano using warrior branches (poky warrior, meta-tegra warrior-l4t-r32.2, openembedded warrior) and CUDA 10. Image boots and runs perfectly, and the camera test: $ gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=0 ! 'video/x-raw,width=960, height=616' ! nvvidconv ! nvegltransform ! nveglglessink -e works like a charm. Now I would like to use OpenCV on the camera feed, but I can't get it to work. I've added these packages to IMAGE_INSTALL: ... opencv \\ libopencv-core \\ libopencv-imgproc \\ opencv-samples \\ gstreamer1.0-omx-tegra \\ python3 \\ python3-modules \\ python3-dev \\ python-numpy \\ ... To get the OpenCV installed. When I run /usr/bin/opencv_version, it returns version 3.4.5, python version is 3.7.2 and GCC version is 7.2.1. When I try to run this OpenCV test code it returns [ WARN:0] VIDEOIO(createGStreamerCapture(filename)): trying ... (python3.7:5163): GStreamer-CRITICAL **: ..._: gst_element_get_state: assertion 'GST_IS_ELEMENT (element)' failed [ WARN:0] VIDEOIO(createGStreamerCapture(filename)): result=(nil) isOpened=-1 ... Unable to open camera I've tried looking around online for solutions but they don't seem to work. EDIT: There does appear to be a problem with using CAP_GSTREAMER in the VideoCapture function as running the same program with CAP_FFMPEG instead works just fine on an mp4 video. Using cv2.VideoCapture(\"/dev/video0\", CAP_FFMPEG) just returns with isOpen=-1. How do I get the camera to open in python?",
        "answers": [
            [
                "This is the pipeline that you said works for you: gst-launch-1.0 -v nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=0 ! 'video/x-raw,width=960, height=616' ! nvvidconv ! nvegltransform ! nveglglessink -e This is the pipeline that is mentioned in the script: gst-launch-1.0 -v nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=3280, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=0 ! 'video/x-raw, width=820, height=616, format=BGRx' ! videoconvert ! video/x-raw, format=BGR ! appsink The difference between working and nonworking pipelines is the addition of videoconvert and appsink The error GStreamer-CRITICAL **: ..._: gst_element_get_state: assertion 'GST_IS_ELEMENT (element)' failed indicates there is some GStreamer element missing from your system. You can try adding the missing plugins by adding the following package group to your image: gstreamer1.0-plugins-base Alternatively, you can replace the pipeline in face_detect.py with your working pipeline, but keep in mind that the script probably needs the video converted to BGR before feeding it to appsink for the algorithm to work. You might need to look up documentation for the nvidconv element to see if this is supported. EDIT: Judging by your comment, you may have been missing gstreamer1.0-python as well."
            ],
            [
                "Use the following gstreamer pipeline: stream = 'nvarguscamerasrc ! video/x-raw(memory:NVMM), width=%d, height=%d, format=(string)NV12, framerate=(fraction)%d/1 !nvvidconv flip-method=%d ! nvvidconv ! video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! videoconvert ! appsink' % (1280, 720, 30,0, 640, 480) cap = cv2.VideoCapture(stream,cv2.CAP_GSTREAMER) This will solve the problem"
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "I've created a very minimal image for the jetson nano with the recepe: inherit core-image inherit distro_features_check REQUIRED_DISTRO_FEATURES = \"x11\" IMAGE_FEATURES += \"package-management splash\" CORE_OS = \"packagegroup-core-boot \\ packagegroup-core-x11 \\ packagegroup-xfce-base \\ kernel-modules \\ \" WIFI_SUPPORT = \" \\ ifupdown \\ dropbear\\ crda \\ iw \\ \" DEV_SDK_INSTALL = \" \\ opencv \\ opencv-samples \\ gstreamer1.0-omx-tegra \\ python-numpy \\ binutils \\ binutils-symlinks \\ coreutils \\ cpp \\ cpp-symlinks \\ diffutils \\ elfutils elfutils-binutils \\ file \\ g++ \\ g++-symlinks \\ gcc \\ gcc-symlinks \\ gdb \\ gdbserver \\ gettext \\ git \\ ldd \\ libstdc++ \\ libstdc++-dev \\ libtool \\ ltrace \\ make \\ pkgconfig \\ python3-modules \\ strace \\ \" EXTRA_TOOLS_INSTALL = \" \\ bzip2 \\ ethtool \\ findutils \\ grep \\ i2c-tools \\ iproute2 \\ iptables \\ less \\ lsof \\ nano \\ nmap \\ tcpdump \\ unzip \\ util-linux \\ wget \\ zip \\ curl \\ \" IMAGE_INSTALL += \" \\ ${CORE_OS} \\ ${DEV_SDK_INSTALL} \\ ${EXTRA_TOOLS_INSTALL} \\ ${WIFI_SUPPORT} \\ \" To play around with a raspberry pi v2.1 camera. Everything works so far except ethernet access. When I run ifconfig I get an IPv6 ip-address and everything is looking good (except I would also want a ipv4 address if but haven't looked into that yet). But when I run the command ping google.com Is says \"ping: bad address 'google.com' and if I run ping on 8.8.8.8 it returns \"ping: sendto: network is uncreachable\". It's not the ethernet cable or my router that has a problem, as the same ethernet cable and access works just fine on my PC. When the ethernet is connected to the jetson nano, the green light is constantly on while the orange light is constantly blinking. What could be causing the problem and how do I fix it and get access to the internet again?",
        "answers": [
            [
                "I ran the command: ifup eth0 I got a ipv4 address and then everything worked."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am attempting to build an image for the jetson-nano using yocto poky-warrior and meta-tegra warrior-l4t-r32.2 layer. I've been following this thread because he had the same problem as me, and the answer on that thread fixed it, but then a new problem occoured.Building with bitbake core-image-minimal Stops with an error stating ERROR: Task (\u2026/jetson-nano/layers/poky-warrior/meta/recipes-core/libxcrypt/libxcrypt.bb:do_configure) failed with exit code '1' I've been told that applying the following patch would fix this problem: diff --git a/meta/recipes-core/busybox/busybox.inc b/meta/recipes- core/busybox/busybox.inc index 174ce5a8c0..e8d651a010 100644 --- a/meta/recipes-core/busybox/busybox.inc +++ b/meta/recipes-core/busybox/busybox.inc @@ -128,7 +128,7 @@ do_prepare_config () { ${S}/.config.oe-tmp &gt; ${S}/.config fi sed -i 's/CONFIG_IFUPDOWN_UDHCPC_CMD_OPTIONS=\"-R -n\"/CONFIG_IFUPDOWN_UDHCPC_CMD_OPTIONS=\"-R -b\"/' ${S}/.config - sed -i 's|${DEBUG_PREFIX_MAP}||g' ${S}/.config + #sed -i 's|${DEBUG_PREFIX_MAP}||g' ${S}/.config } # returns all the elements from the src uri that are .cfg files diff --git a/meta/recipes-core/libxcrypt/libxcrypt.bb b/meta/recipes-core/libxcrypt/libxcrypt.bb index 3b9af6d739..350f7807a7 100644 --- a/meta/recipes-core/libxcrypt/libxcrypt.bb +++ b/meta/recipes-core/libxcrypt/libxcrypt.bb @@ -24,7 +24,7 @@ FILES_${PN} = \"${libdir}/libcrypt*.so.* ${libdir}/libcrypt-*.so ${libdir}/libowc S = \"${WORKDIR}/git\" BUILD_CPPFLAGS = \"-I${STAGING_INCDIR_NATIVE} -std=gnu99\" -TARGET_CPPFLAGS = \"-I${STAGING_DIR_TARGET}${includedir} -Wno-error=missing-attributes\" -CPPFLAGS_append_class-nativesdk = \" -Wno-error=missing-attributes\" +TARGET_CPPFLAGS = \"-I${STAGING_DIR_TARGET}${includedir} \" +CPPFLAGS_append_class-nativesdk = \" \" BBCLASSEXTEND = \"nativesdk\" So I've made a libxcrypt.patch file and copy pasted the patch content and put the file in my poky meta layer. But how do I apply the patch? I can't figure out what to do from here, do I need to make an bbappend file or add to one?- if so which one? or do I need to edit a .bb file?- maybe libxcrypt.bb? And do I need to add these lines: FILESEXTRAPATHS_prepend := \"${THISDIR}/${PN}:\" SRC_URI += \"file://path/to/patch/file\" I've been trying to look at similar stackoverflow posts about this but they don't seem to be precise enough for me to work it out as I am completely new to yocto and the likes. So far I've tried to add the lines FILESEXTRAPATHS_prepend := \"${THISDIR}/${PN}:\" SRC_URI += \"file://path/to/patch/file\" to the libxcrypt.bb file but it says it cannot find the file to patch. Then I found out this could potentially be solved with adding ;striplevel=0 to the SRC_URI line, so I did this: SRC_URI += \"file://path/to/patch/file;striplevel=0\" Which did nothing. Then I tried to put --- a/meta/recipes-core/busybox/busybox.inc +++ b/meta/recipes-core/busybox/busybox.inc In the top of the patch file, but this also did nothing. This is the full error message without attempting to apply the patch: ERROR: libxcrypt-4.4.2-r0 do_configure: configure failed ERROR: libxcrypt-4.4.2-r0 do_configure: Function failed: do_configure (log file is located at /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/temp/log.do_configure.42560) ERROR: Logfile of failure stored in: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/temp/log.do_configure.42560 Log data follows: | DEBUG: SITE files ['endian-little', 'bit-64', 'arm-common', 'arm-64', 'common-linux', 'common-glibc', 'aarch64-linux', 'common'] | DEBUG: Executing shell function autotools_preconfigure | DEBUG: Shell function autotools_preconfigure finished | DEBUG: Executing python function autotools_aclocals | DEBUG: SITE files ['endian-little', 'bit-64', 'arm-common', 'arm-64', 'common-linux', 'common-glibc', 'aarch64-linux', 'common'] | DEBUG: Python function autotools_aclocals finished | DEBUG: Executing shell function do_configure | automake (GNU automake) 1.16.1 | Copyright (C) 2018 Free Software Foundation, Inc. | License GPLv2+: GNU GPL version 2 or later &lt;https://gnu.org/licenses/gpl-2.0.html&gt; | This is free software: you are free to change and redistribute it. | There is NO WARRANTY, to the extent permitted by law. | | Written by Tom Tromey &lt;tromey@redhat.com&gt; | and Alexandre Duret-Lutz &lt;adl@gnu.org&gt;. | AUTOV is 1.16 | NOTE: Executing ACLOCAL=\"aclocal --system-acdir=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot/usr/share/aclocal/ --automake-acdir=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal-1.16\" autoreconf -Wcross --verbose --install --force --exclude=autopoint -I /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/git/m4/ -I /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal/ | autoreconf: Entering directory `.' | autoreconf: configure.ac: not using Gettext | autoreconf: running: aclocal --system-acdir=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot/usr/share/aclocal/ --automake-acdir=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal-1.16 -I /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/git/m4/ -I /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal/ --force -I m4 | autoreconf: configure.ac: tracing | autoreconf: running: libtoolize --copy --force | libtoolize: putting auxiliary files in AC_CONFIG_AUX_DIR, 'm4'. | libtoolize: copying file 'm4/ltmain.sh' | libtoolize: putting macros in AC_CONFIG_MACRO_DIRS, 'm4'. | libtoolize: copying file 'm4/libtool.m4' | libtoolize: copying file 'm4/ltoptions.m4' | libtoolize: copying file 'm4/ltsugar.m4' | libtoolize: copying file 'm4/ltversion.m4' | libtoolize: copying file 'm4/lt~obsolete.m4' | autoreconf: running: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/bin/autoconf --include=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/git/m4/ --include=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal/ --force | autoreconf: running: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/bin/autoheader --include=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/git/m4/ --include=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal/ --force | autoreconf: running: automake --add-missing --copy --force-missing | configure.ac:31: installing 'm4/compile' | configure.ac:30: installing 'm4/config.guess' | configure.ac:30: installing 'm4/config.sub' | configure.ac:17: installing 'm4/install-sh' | configure.ac:17: installing 'm4/missing' | Makefile.am: installing './INSTALL' | Makefile.am: installing 'm4/depcomp' | parallel-tests: installing 'm4/test-driver' | autoreconf: running: gnu-configize | autoreconf: Leaving directory `.' | NOTE: Running ../git/configure --build=x86_64-linux --host=aarch64-poky-linux --target=aarch64-poky-linux --prefix=/usr --exec_prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin --libexecdir=/usr/libexec --datadir=/usr/share --sysconfdir=/etc --sharedstatedir=/com --localstatedir=/var --libdir=/usr/lib --includedir=/usr/include --oldincludedir=/usr/include --infodir=/usr/share/info --mandir=/usr/share/man --disable-silent-rules --disable-dependency-tracking --with-libtool-sysroot=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot --disable-static | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/endian-little | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/arm-common | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/arm-64 | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/common-linux | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/common-glibc | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/common | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/meta-openembedded/meta-networking/site/endian-little | checking for a BSD-compatible install... /home/mci/yocto/dev-jetson-nano/build/tmp/hosttools/install -c | checking whether build environment is sane... yes | checking for aarch64-poky-linux-strip... aarch64-poky-linux-strip | checking for a thread-safe mkdir -p... /home/mci/yocto/dev-jetson-nano/build/tmp/hosttools/mkdir -p | checking for gawk... gawk | checking whether make sets $(MAKE)... yes | checking whether make supports nested variables... yes | checking build system type... x86_64-pc-linux-gnu | checking host system type... aarch64-poky-linux-gnu | checking for aarch64-poky-linux-gcc... aarch64-poky-linux-gcc -march=armv8-a+crc -fstack-protector-strong -D_FORTIFY_SOURCE=2 -Wformat -Wformat-security -Werror=format-security --sysroot=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot | checking whether the C compiler works... no | configure: error: in `/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/build': | configure: error: C compiler cannot create executables | See `config.log' for more details | NOTE: The following config.log files may provide further information. | NOTE: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/build/config.log | ERROR: configure failed | WARNING: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/temp/run.do_configure.42560:1 exit 1 from 'exit 1' | ERROR: Function failed: do_configure (log file is located at /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/temp/log.do_configure.42560) ERROR: Task (/home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/recipes-core/libxcrypt/libxcrypt.bb:do_configure) failed with exit code '1' NOTE: Tasks Summary: Attempted 883 tasks of which 848 didn't need to be rerun and 1 failed. This is the full error log when I try to add the lines to the libxcrypt.bb file to apply the patch: ERROR: libxcrypt-4.4.2-r0 do_patch: Command Error: 'quilt --quiltrc /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/etc/quiltrc push' exited with 0 Output: Applying patch libxcrypt.patch can't find file to patch at input line 7 Perhaps you used the wrong -p or --strip option? The text leading up to this was: -------------------------- |--- a/meta/recipes-core/busybox/busybox.inc |+++ b/meta/recipes-core/busybox/busybox.inc |diff --git a/meta/recipes-core/busybox/busybox.inc b/meta/recipes-core/busybox/busybox.inc |index 174ce5a8c0..e8d651a010 100644 |--- a/meta/recipes-core/busybox/busybox.inc |+++ b/meta/recipes-core/busybox/busybox.inc -------------------------- No file to patch. Skipping patch. 1 out of 1 hunk ignored can't find file to patch at input line 20 Perhaps you used the wrong -p or --strip option? The text leading up to this was: -------------------------- |diff --git a/meta/recipes-core/libxcrypt/libxcrypt.bb b/meta/recipes-core/libxcrypt/libxcrypt.bb |index 3b9af6d739..350f7807a7 100644 |--- a/meta/recipes-core/libxcrypt/libxcrypt.bb |+++ b/meta/recipes-core/libxcrypt/libxcrypt.bb -------------------------- No file to patch. Skipping patch. 1 out of 1 hunk ignored Patch libxcrypt.patch does not apply (enforce with -f) ERROR: libxcrypt-4.4.2-r0 do_patch: ERROR: libxcrypt-4.4.2-r0 do_patch: Function failed: patch_do_patch ERROR: Logfile of failure stored in: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/temp/log.do_patch.34179 ERROR: Task (/home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/recipes-core/libxcrypt/libxcrypt.bb:do_patch) failed with exit code '1' NOTE: Tasks Summary: Attempted 811 tasks of which 793 didn't need to be rerun and 1 failed. I know this might be a trivial question for a lot, but as a new developer this is very hard to figure out on my own.",
        "answers": [
            [
                "The concept of patching by adding patch files to meta layers and referencing them in SRC_URI only applies to patching the source code of packages. You can't use it to patch meta data (recipes) itself as you are trying to. Instead you can manually change your local recipes, or add bbappends to your layer to change the existing recipes in poky. The best way to fix it permanently is to look for upstream fixes and update your poky layer if there are fixes, or if not send patches to upstream to fix it. For the bbappend solution for libxcrypt, you would for example create a libxcrypt.bbappend with something like this as content: TARGET_CPPFLAGS = \"-I${STAGING_DIR_TARGET}${includedir} \" CPPFLAGS_reomve_class-nativesdk = \"-Wno-error=missing-attributes\""
            ],
            [
                "The patch you have is for the yocto/poky sources themselves (as opposed to the more usual case of having patches for the actual components that yocto builds and bbappends that modify recipes in other layers somehow). So if you really want to use this patch, there's no need to \"integrate\" it into yocto, just run run git am &lt;patchfile&gt; in your poky root dir or use \"patch\" command directly. This is not very maintainable since your poky now differs from upstream but might work... It should be possible to do the same changes using bbappends that you could then store in your own layer (this way the poky repo would be untouched) but the patch you have does not do that. This would be the most \"proper\" way of paching that you asked about in the comment -- but if you know that you aren't going to ever upgrade poky then it might not be work worth doing."
            ]
        ],
        "votes": [
            4.0000001,
            4.0000001
        ]
    },
    {
        "question": "I'm attempting to create an image for my NVIDIA jetsons-nano (following this guide). When building the very basic image, the build terminates with an error saying it cannot find cuda-repo-l4t-10-0-local-10.0.166... and that is because the NVIDIA SDK downloads cuda-repo-l4t-10-0-local-10.0.326... I can see that the meta-tegra thud branch does in fact contain recipes needing the 10.0.166 CUDA version. Meanwhile the master branch contains recipes needing the updated 10.0.326 CUDA that the NVIDIA SDK provides. So my question is this: can I just copy the cuda recipes folder from master branch (meta-tegra/recipes-devtools/cuda) and replace the cuda recipe folder in the used meta-tegra layer in my build? Or can I download the CUDA 10.0.166 from the SDK instead somehow? [SOLVED]As a side question, the build complains that is cannot find \"cuda-repo-ubuntu1804-10-0-local-10.0.326-410.108_10.0-1_amd64.deb\"... which is because I downloaded from the NVIDIA SDK on a ubuntu 16.04 system and not 18.04.. What can I do about this? I can see that there is recipe for both 18.04 and 16.04, but it runs through both? As another side question, the meta-tegra layer of the thud branch does not have the MACHINE conf for jetson-nano. But I assume these configs are somewhat independent, so I took the jetson-nano config file from the master branch aswell. This is fine right?",
        "answers": [
            [
                "For the cuda SDK, you need to do something like this in local.conf: CUDA_BINARIES_NATIVE = \"cuda-binaries-ubuntu1604-native\" edit: I also want to add that you may have less difficulty gett Warrior to work with the latest SDK rather than backing off to Thud."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "My aim is streaming object detection video from Raspberry Pi camera to UDP. I am doing that on Nvidia Jetson Nano(Ubuntu 18.04). I am using gstreamer to capture video. I have to encode video with h264. Pixel Format of camera is 'RG10'. I tried different pipeline to capture video but nothing is changed. I think, problem is capturing video. I could not figure out. I am newbies. So I need help. Here is code: import os import numpy as np import tensorflow as tf import sys sys.path.append('/usr/local/lib/python3.6/dist-packages') import cv2 as cv2 from utils import label_map_util from utils import visualization_utils as vis_util MODEL_NAME = 'inference_graph' CWD_PATH = os.getcwd() PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb') PATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt') NUM_CLASSES = 1 label_map = label_map_util.load_labelmap(PATH_TO_LABELS) categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True) category_index = label_map_util.create_category_index(categories) detection_graph = tf.Graph() with detection_graph.as_default(): od_graph_def = tf.GraphDef() with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='') sess = tf.Session(graph=detection_graph) image_tensor = detection_graph.get_tensor_by_name('image_tensor:0') detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0') detection_scores = detection_graph.get_tensor_by_name('detection_scores:0') detection_classes = detection_graph.get_tensor_by_name('detection_classes:0') num_detections = detection_graph.get_tensor_by_name('num_detections:0') writer = cv2.VideoWriter(\" appsrc name=appsrc_element block=true ! video/x-raw,format=RGB,width=320,height=240,framerate=30/1 ! identity check-imperfect-timestamp=true ! videoconvert ! x264enc ! video/x-h264,profile=\\\"high-4:4:4\\ ! rtph264pay ! udpsink host= 192.168.1.49 port=5000 \",cv2.CAP_GSTREAMER,0, 20, (320,240), True) # Write frames here after processing video = cv2.VideoCapture('nvarguscamerasrc ! image/jpg,width=1280,height=720,type=video,framerate=30/1 ! videoscale ! videoconvert ! x264enc tune=zerolatency ! rtph264pay ! appsink ', cv2.CAP_GSTREAMER) ret = video.set(3,1280) ret = video.set(4,720) while(True): ret, frame = video.read() frame_expanded = np.expand_dims(frame, axis=0) (boxes, scores, classes, num) = sess.run( [detection_boxes, detection_scores, detection_classes, num_detections], feed_dict={image_tensor: frame_expanded}) vis_util.visualize_boxes_and_labels_on_image_array( frame, np.squeeze(boxes), np.squeeze(classes).astype(np.int32), np.squeeze(scores), category_index, use_normalized_coordinates=True, line_thickness=8, min_score_thresh=0.85) w_size = cv2.getWindowImageRect('frame') print(w_size) cv2.rectangle(frame,(int(int(w_size[2])*0.25) ,int(int(w_size[3])*0.10)),(int(int(w_size[2])*0.75),int(int(w_size[3])*0.90)),(0,255,0),3) writer.write(frame) if cv2.waitKey(1) == ord('q'): break video.release() writer.release() cv2.destroyAllWindows() This is output error : _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)]) /home/berfu/.virtualenvs/deep_learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)]) /home/berfu/.virtualenvs/deep_learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)]) /home/berfu/.virtualenvs/deep_learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)]) /home/berfu/.virtualenvs/deep_learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)]) /home/berfu/.virtualenvs/deep_learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)]) 2019-09-07 02:26:33.800624: W tensorflow/core/platform/profile_utils/cpu_utils.cc:98] Failed to find bogomips in /proc/cpuinfo; cannot determine CPU frequency 2019-09-07 02:26:33.804717: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x2816fdf0 executing computations on platform Host. Devices: 2019-09-07 02:26:33.805320: I tensorflow/compiler/xla/service/service.cc:168] StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt; 2019-09-07 02:26:33.895575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:965] ARM64 does not support NUMA - returning NUMA node zero 2019-09-07 02:26:33.895923: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x24aca990 executing computations on platform CUDA. Devices: 2019-09-07 02:26:33.895984: I tensorflow/compiler/xla/service/service.cc:168] StreamExecutor device (0): NVIDIA Tegra X1, Compute Capability 5.3 2019-09-07 02:26:33.896452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: name: NVIDIA Tegra X1 major: 5 minor: 3 memoryClockRate(GHz): 0.9216 pciBusID: 0000:00:00.0 totalMemory: 3.86GiB freeMemory: 272.39MiB 2019-09-07 02:26:33.896515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0 2019-09-07 02:26:35.073590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-09-07 02:26:35.073769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0 2019-09-07 02:26:35.073809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0: N 2019-09-07 02:26:35.074121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 42 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0, compute capability: 5.3) GST_ARGUS: Creating output stream CONSUMER: Waiting until producer is connected... GST_ARGUS: Available Sensor modes : GST_ARGUS: 3280 x 2464 FR = 21.000000 fps Duration = 47619048 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000; GST_ARGUS: 3280 x 1848 FR = 28.000001 fps Duration = 35714284 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000; GST_ARGUS: 1920 x 1080 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000; GST_ARGUS: 1280 x 720 FR = 59.999999 fps Duration = 16666667 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000; GST_ARGUS: 1280 x 720 FR = 120.000005 fps Duration = 8333333 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000; GST_ARGUS: Running with following settings: Camera index = 0 Camera mode = 2 Output Stream W = 1920 H = 1080 seconds to Run = 0 Frame Rate = 29.999999 GST_ARGUS: PowerService: requested_clock_Hz=13608000 GST_ARGUS: Setup Complete, Starting captures for 0 seconds GST_ARGUS: Starting repeat capture requests. CONSUMER: Producer has connected; continuing. GST_ARGUS: Cleaning up GST_ARGUS: PowerServiceHwVic::cleanupResources CONSUMER: Done Success GST_ARGUS: Done Success &lt;class 'NoneType'&gt; Traceback (most recent call last): File \"webcam.py\", line 68, in &lt;module&gt; feed_dict={image_tensor: frame_expanded}) File \"/home/berfu/.virtualenvs/deep_learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run run_metadata_ptr) File \"/home/berfu/.virtualenvs/deep_learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1121, in _run np_val = np.asarray(subfeed_val, dtype=subfeed_dtype) File \"/home/berfu/.virtualenvs/deep_learning/lib/python3.6/site-packages/numpy/core/_asarray.py\", line 85, in asarray return array(a, dtype, copy=False, order=order) TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType' Thank you, for your help",
        "answers": [
            [
                "The values in feed_dict have to be convertible to a numpy array - it looks like frame_expanded may contain None. Try printing the feed_dict values to see if it contains valid data."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to run a python script when booting my NVIDIA Nano Jetson. The python script prints LSTM generated text to a receipt printer with the following command: While True: textfile = open(\"test1.txt\",\"w\") poem = on_epoch_end() print(poem) textfile.write(poem) subprocess.run([\"lp\",\"-o\", \"media=X72MMY2000MM\", \"test1.txt\"]) on_epoch_end() generates the text using Keras. This works fine when running the script manually, it prints pieces of text continually on the receipt printer. But when I set crontab to run the script at boot with the following code: @reboot (sleep 80 &amp;&amp; cd /home/lauge/Downloads/lstmtextfinal/ &amp;&amp; python3 generator.py &gt;&gt; /home/lauge/generator_log.txt &amp;) Nothing happens. I added the sleep timer to ensure that everything is ready for the script to run. When checking whether the scripts run at startup with: ps ax | grep generator.py It shows that both the sleep timer is working, and after the timer has run out, that the script is running in python3. I'm not sure what I'm doing wrong at this point.",
        "answers": [
            [
                "cron runs with out TTY attached (not as normal login), so you might want to wrap up your code into a shell script and first imitate a normal login and then run your code, e.g. : cronjob_1.sh: #!/bin/tcsh source /home/username/.cshrc source /home/username/.login cd ~/tmp nice time python /home/username/path/to/your/script.py or #!/bin/bash source /home/username/.profile source /home/username/.bashrc cd ~/tmp nice time python /home/username/path/to/your/script.py or whatever is your code, - depending on your shell of choice. And in your crontab \u2013 something like this: 18 * * * * ~username/cronjobs/cronjob_1.sh"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to experiment to develop a redundant autonomous car compute architecture which can handle all AI and other computing stuff. To do that, I bought some edge computing devices (Nvidia Jetson TX2s) which contains integrated GPU. Then I connected them with a gigabit ethernet switch so now I can communicate them. I need your advices for the system architecture. How can I implement this failsafe, safety-critcal and redundant system? There is some cluster examples to provide high avaibility. But I want to do that : \"Each compute node runs same processes then they output results to master node. Master node analyses and votes the results and picks the best one. If a compute node fails (bug, system down, lack of electiricty etc), the system should be aware of failure and transfer the failed node's compute load to healty nodes. Also each node should run some node specific tasks without affected by cooperated processes.\" What is your thoughts? Any keyword, suggestion, method recommodation helps me.",
        "answers": [
            [
                "The primary system/software safety standard for automobiles is ISO 26262. If you're going to be serious about making an automotive product, you'll want to acquire a copy and follow the process. The primary classification for levels of autonomy in cars is SAE J3016_201806. You'll save a lot of headache up front by knowing which level you're shooting for beforehand. You may want to shoot for Level 1 (\"hands on\") like an adaptive cruise control or lane departure prevention system before trying to do more sophisticated things. Here are some general themes that I've gleaned from doing safety stuff: There is no generally-accepted way to determine a probability of software failure. There's even a school of thought that software does not fail. Instead, most safety standards assign safety-significant functionality implemented in software to different \"levels\" that require higher levels of scrutiny based on certain criteria including severity, closeness to a hazard (are there interlocks?), etc. Most safety standards define software as everything running on the hardware, so you will need to ensure that the operating system you use also can meet the standards. This usually means a real-time operating system. Keep your safety-significant functionality as simple as possible. If you can do something with elementary electrical circuits and logic gates (such as an emergency stop), do it because the math and analysis is much more mature for hardware. Acquire and follow a safety-relevant coding standard. The predominant one for automotive applications is MISRA C. Look into using fault tree analysis to identify the relationships of failures required for a mishap to occur. This also helps identify single points of failure. Try to alleviate hazards in the design if possible. Procedural mitigations and personal protective equipment should be a last resort. At a minimum, you'll want a hard electrical emergency stop for the safety driver and a remote-controlled emergency stop operated by a spotter."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm using YOLOv3 on Jetson TX2. What I want to do is send YOLOv3's final output(object detection data) to Raspberry pi. Because I would like to detect object with USB camera on Jetson TX2 and send the data to RPi and finally print out by voice on RPi. Simply camera -- Jetson TX2 -- Raspberry pi -- speaker. This is the first time I've used yolo and Jetson TX2. So I have a lot of difficulties. So I don't know how to connect Jetson TX2 and Raspberry pi3. Which terminal should be used? And what is the code for connecting TX2 and RPi. Is it possible send the yolo data to Raspberry Pi?? Or is there any method sending the data via networks directly?? If so, please tell me how to do this...",
        "answers": [
            [
                "Use serial communications like i2c or can bus between Jetson and raspberry pi"
            ],
            [
                "you can connect both TX2 and RPi with WiFi, and can communication between them easily."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I'm using OpenCV v.4.1.0 on Jetson Nano to capture video with RPi camera and the module cv2.VideoWriter doesn't work well saving video. The file is empty. I've proved cv2.VideoWriter on RPi 3B+ and it worked fine. import numpy as np import cv2 capture_height = 720 capture_width = 1280 frame_rate = 21 display_width = 860 display_height = 640 flip_method = 0 gstr = ('nvarguscamerasrc ! video/x-raw(memory:NVMM),' 'width=%s, height=%s,' 'framerate= %s' 'format=NV12 ! nvvidconv flip-method= %s ! video/x-raw,' 'width=%s, height=%s,' 'format=BGRx ! videoconvert ! appsink' % (capture_width, capture_height, frame_rate, flip_method, display_width, display_height)) filename = 'video.avi' fourcc = cv2.VideoWriter_fourcc(*'XVID') cap = cv2.VideoCapture(gstr, cv2.CAP_GSTREAMER) out = cv2.VideoWriter(filename, fourcc, float(frame_rate), (capture_width,capture_height),True) while True: ret, img = cap.read() out.write(img) cv2.imshow('img',img) if cv2.waitKey(1) &amp; 0xff == ord('q'): break cap.release() out.release() cv2.destroyAllWindows() This code should show an image from RPi camera, and save a video with .avi format.",
        "answers": [
            [
                "A solution to this is typing: out = cv2.VideoWriter(filename, fourcc, float(frame_rate), (display_width, display_height), True)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have recently purchased a Nvidia Jetson AGX Xavier. I flashed the system and installed the tools using the SDKManager and all seemed ok. I wanted to run the Hello AI World tutorial to familiarize myself with the system. When running the python example: ./imagenet-console.py --network=googlenet orange_0.jpg output_0.jpg I received the error: jetson.inference.__init__.py Traceback (most recent call last): File \"./imagenet-console.py\", line 24, in &lt;module&gt; import jetson.inference File \"/usr/lib/python2.7/dist-packages/jetson/inference/__init__.py\", line 4, in &lt;module&gt; from jetson_inference_python import * ImportError: libjetson-utils.so: cannot open shared object file: No such file or directory I reflashed the system and tried again and got the same error. After some effort I found a solution which I have posted below. I have lodged this Q&amp;A to assist other who will no doubt have the same problem.",
        "answers": [
            [
                "First, I note that you need an editor and the Jetson Xavier doesn't seem to come with nano editor installed so I installed this using sudo apt-get install nano The solution I found here: error-while-loading-shared-libraries-cannot-open-shared-object-file In particular, I quote Bob Plankers' preferred solution: ask the dynamic linker to check /usr/local/lib Edit the file /etc/ld.so.conf and add \u201c/usr/local/lib\u201d on its own line at the bottom[3]. DO NOT REMOVE THINGS FROM THIS FILE. When you\u2019re done it might look something like: $ cat /etc/ld.so.conf include ld.so.conf.d/*.conf /usr/local/lib or it might look completely different, with the exception of the last line. *****IMPORTANT*** Run ldconfig to tell it to update the cache:** $ sudo ldconfig You can check your work with: $ ldconfig -p | grep local"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm working on spi communicate between stm32f0308-discovery and jetson tx2. Jetson is master and stm32 should be slave. (idk how but if it is possible stm32 may be master too.) My problem is I'm new for stm32 and I don't know how an I make stm32 to slave. Can someone show me a way for stm32 spi slave ? Thanks in advance.",
        "answers": [
            [
                "You can start by reading the reference manual of your product family. Then, you can find examples of SPI peripheral configuration source code in STM32Cube software packages. If you are new to STM32 and new to the microcontroller ecosystem, I'm afraid you will need some training. But there are plenty of resources online."
            ],
            [
                "You can select SPI mode when configuring the SPI_InitTypeDef structure. You need to set the SPI_Mode to Slave as follows: SPI_InitDef.SPI_Direction = SPI_Direction_2Lines_FullDuplex; SPI_InitDef.SPI_Mode = SPI_Mode_Slave; // &lt;-- This is it SPI_InitDef.SPI_DataSize = SPI_DataSize_8b; // 8-bit transactions SPI_InitDef.SPI_FirstBit = SPI_FirstBit_MSB; // set it to match Master conf SPI_InitDef.SPI_CPOL = SPI_CPOL_Low; // set it to match Master conf SPI_InitDef.SPI_CPHA = SPI_CPHA_2Edge; // set it to match Master conf SPI_InitDef.SPI_NSS = SPI_NSS_Hard; // use hardware SS An example tutorial using blue pill boards can be found here"
            ],
            [
                "Yes. You can make STM32 as the slave. The only thing you need to do is CLEAR the MSTR Bit in The Control Register on the peripheral. You perhaps then can load some values in the SPI Data Register &amp; then can read them from your other board."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am trying to follow this tutorial to read analog input. But instead of using Raspberry Pi, I am trying to use a Jetson Nano to read MCP3008 channel 0 analog input only. I am following the pin layout in the tutorial on Jetson Nano since it shares same layout with Raspberry Pi. import busio import digitalio import board import adafruit_mcp3xxx.mcp3008 as MCP from adafruit_mcp3xxx.analog_in import AnalogIn # create the spi bus spi = busio.SPI(clock=board.SCK, MISO=board.MISO, MOSI=board.MOSI) # create the cs (chip select) cs = digitalio.DigitalInOut(board.D22) # create the mcp object mcp = MCP.MCP3008(spi, cs) # create an analog input channel on pin 0 chan0 = AnalogIn(mcp, MCP.P0) print('Raw ADC Value: ', chan0.value) print('ADC Voltage: ' + str(chan0.voltage) + 'V') The code above throws the error: import board File \"/usr/local/lib/python3.6/dist-packages/board.py\", line 98, in &lt;module&gt; raise NotImplementedError(\"Board not supported\") NotImplementedError: Board not supported I checked the source code, it points to this line. However, I looked up of the board library source, it does support Jetson Nano because of the line board_id == ap_board.JETSON_NANO Any guide can help me further locate the issue will be very helpful. Thanks!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using Jetson AGX Xavier with Jetpack 4.2.1 I have not altered Tensor RT, UFF and graphsurgeon version. They are as it is. I have retrained SSD Inception v2 model on custom 600x600 images. Taken pretrained model from here. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md I have changed height and width to 600x600 in pipeline.config. I am using sampleUffSSD sample containing in Tensor RT samples. In config.py I replaced 300 by 600 in shape. I generated frozen_graph.uff by command : python3 convert_to_uff.py frozen_inference_graph.pb -O NMS -p config.py In file BatchStreamPPM.h: I changed static constexpr int INPUT_H = 600; // replaced 300 by 600 static constexpr int INPUT_W = 600; // replaced 300 by 600 mDims = nvinfer1::DimsNCHW{batchSize, 3, 600, 600}; // replaced 300 by 600 In file sampleUffSSD.cpp I changed parser-&gt;registerInput(\"Input\", DimsCHW(3, 600,600), UffInputOrder::kNCHW); // replaced 300 by 600 cd sampleUffSSD make clean ; make I ran sample_uff_ssd I met below error: &amp;&amp;&amp;&amp; RUNNING TensorRT.sample_uff_ssd # ./../../bin/sample_uff_ssd [I] ../../data/ssd/sample_ssd_relu6.uff [I] Begin parsing model... [I] End parsing model... [I] Begin building engine... sample_uff_ssd: nmsPlugin.cpp:139: virtual void nvinfer1::plugin::DetectionOutput::configureWithFormat(const nvinfer1::Dims*, int, const nvinfer1::Dims*, int, nvinfer1::DataType, nvinfer1::PluginFormat, int): Assertion `numPriors * numLocClasses * 4 == inputDims[param.inputOrder[0]].d[0]' failed. Aborted (core dumped) I think the problem is with resolution. How can I optimise model for custom resolution ? It works fine with 300x300 resolution.",
        "answers": [],
        "votes": []
    },
    {
        "question": "i am trying to use HDF5 package in julia and i'm not succeeding, i get the following error when trying to add it. julia&gt; Pkg.add(\"HDF5\") Updating registry at `~/.julia/registries/General` Updating git-repo `https://github.com/JuliaRegistries/General.git` Resolving package versions... Updating `~/.julia/environments/v1.0/Project.toml` [no changes] Updating `~/.julia/environments/v1.0/Manifest.toml` [no changes] and i get the following when building it. julia&gt; Pkg.build(\"HDF5\") Building CMake \u2192 `~/.julia/packages/CMake/nSK2r/deps/build.log` Building Blosc \u2192 `~/.julia/packages/Blosc/lzFr0/deps/build.log` Building HDF5 \u2500\u2192 `~/.julia/packages/HDF5/Y9Znv/deps/build.log` \u250c Error: Error building `HDF5`: \u2502 ERROR: LoadError: Your platform (\"aarch64-linux-gnu\", parsed as \"aarch64-linux-gnu-gcc4-cxx11\") is not supported by this package! \u2502 Stacktrace: \u2502 [1] error(::String) at ./error.jl:33 \u2502 [2] top-level scope at /home/nvidia/.julia/packages/HDF5/Y9Znv/deps/build.jl:35 \u2502 [3] include at ./boot.jl:317 [inlined] \u2502 [4] include_relative(::Module, ::String) at ./loading.jl:1038 \u2502 [5] include(::Module, ::String) at ./sysimg.jl:29 \u2502 [6] include(::String) at ./client.jl:388 \u2502 [7] top-level scope at none:0 \u2502 in expression starting at /home/nvidia/.julia/packages/HDF5/Y9Znv/deps/build.jl:31 \u2514 @ Pkg.Operations ~/julia/usr/share/julia/stdlib/v1.0/Pkg/src/Operations.jl:1068 this is the gcc installed. nvidia@tegra-ubuntu:~$ gcc -v Using built-in specs. COLLECT_GCC=gcc COLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/5/lto-wrapper Target: aarch64-linux-gnu Configured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.11' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-arm64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-arm64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-arm64 --with-arch-directory=aarch64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu Thread model: posix gcc version 5.4.0 20160609 (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.11) thanks for your help.",
        "answers": [
            [
                "I had the same horrible trouble with Julia built on ARM. It seems that this dependence on custom binaries has been introduced in HDF5 v0.12. So for me it worked to force an older version of HDF5: pkg&gt; add HDF5@0.11.1 pkg&gt; pin HDF5@0.11.1 Before, I have installed (in Debian/armhf) hdf5-tools, libhdf5-dev. Hope this helps. But I think we shall inform the developers, as the HDF5 is a very important pkg (being used by JLD, without which the Julia is hardly usable), and restricting to a very small set of supported architectures is just stupid."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a quantized tflite model that I'd like to benchmark for inference on a Nvidia Jetson Nano. I use tf.lite.Interpreter() method for inference. The process doesn't seem to run on the GPU as the inference times on both CPU and GPU are the same. Is there any way to run a tflite model on GPU using Python? I tried to force GPU usage by setting tf.device() method but still doesn't work. The official documentation has something called delegates for GPU acceleration but I can't seem to find anything for Python. with tf.device('/device:GPU:0'): interpreter = tf.lite.Interpreter(model_path=\"model.tflite\") interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() input_shape = input_details[0]['shape'] input_data = np.array(np.random.random_sample(input_shape), dtype=np.uint8) interpreter.set_tensor(input_details[0]['index'], input_data) start_time = time.time() interpreter.invoke() elapsed_time = time.time() - start_time print(elapsed_time) output_data = interpreter.get_tensor(output_details[0]['index'])",
        "answers": [
            [
                "TFLite doesn't support Nvidia GPUs as per this link"
            ],
            [
                "It seems to be available on the jetson nano according to this recent thread. But it's look like a custom build, try it instead of tensorflow lite. If you already installed it maybe ask for nvidia developper if the release is supposed to support GPU. Or you can Install the nvidia custom tensorflow this way. Python 3.6+JetPack4.4 sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran sudo apt-get install python3-pip sudo pip3 install -U pip sudo pip3 install -U pip testresources setuptools numpy==1.16.1 future==0.17.1 mock==3.0.5 h5py==2.9.0 keras_preprocessing==1.0.5 keras_applications==1.0.8 gast==0.2.2 futures protobuf pybind11 # TF-2.x $ sudo pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v44 tensorflow==2.2.0+nv20.8 # TF-1.15 $ sudo pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v44 \u2018tensorflow&lt;2\u2019 Python 3.6+JetPack4.3 $ sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev $ sudo apt-get install python3-pip $ sudo pip3 install -U pip $ sudo pip3 install -U numpy grpcio absl-py py-cpuinfo psutil portpicker six mock requests gast h5py astor termcolor protobuf keras-applications keras-preprocessing wrapt google-pasta # TF-2.x $ sudo pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v43 tensorflow==2.1.0+nv20.3 # TF-1.15 $ sudo pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v43 tensorflow==1.15.2+nv20.3"
            ],
            [
                "Does your Jetson Nano support OpenCL? If it does, you can use OpenCL delegate with TFLite. https://www.tensorflow.org/lite/guide/build_cmake#opencl_gpu_delegate"
            ],
            [
                "It is because you are some how using the full interpreter of tf. see here interpreter = tf.lite.Interpreter(model_path=\"model.tflite\"). What you can do is install python3 -m pip install tflite-runtime and use import tflite_runtime.interpreter as tflite interpreter = tflite.Interpreter(model_path=args.model_file) and you should be able to run things fine. I hope it helps!"
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm using TensorRT FP16 precision mode to optimize my deep learning model. And I use this optimised model on Jetson TX2. While testing the model, I have observed that TensorRT inference engine is not deterministic. In other words, my optimized model gives different FPS values between 40 and 120 FPS for same input images. I started to think that the source of the non-determinism is floating point operations when I see this comment about CUDA: \"If your code uses floating-point atomics, results may differ from run to run because floating-point operations are generally not associative, and the order in which data enters a computation (e.g. a sum) is non-deterministic when atomics are used.\" Is type of precision such as FP16, FP32 and INT8 affects determinism of TensorRT? Or anything? Do you have any thoughs? Best regards.",
        "answers": [
            [
                "I solved the problem by changing the function clock() that I used for measuring latencies. The clock() function was measuring the CPU time latency, but what I want to do is to measure real time latency. Now I am using std::chrono to measure the latencies. Now inference results are latency-deterministic. That was wrong one, (clock()) int main () { clock_t t; int f; t = clock(); inferenceEngine(); // Tahmin yap\u0131l\u0131yor t = clock() - t; printf (\"It took me %d clicks (%f seconds).\\n\",t,((float)t)/CLOCKS_PER_SEC); return 0; } Use Cuda Events like this, (CudaEvent) cudaEvent_t start, stop; cudaEventCreate(&amp;start); cudaEventCreate(&amp;stop); cudaEventRecord(start); inferenceEngine(); // Do the inference cudaEventRecord(stop); cudaEventSynchronize(stop); float milliseconds = 0; cudaEventElapsedTime(&amp;milliseconds, start, stop); Use chrono like this: (std::chrono) #include &lt;iostream&gt; #include &lt;chrono&gt; #include &lt;ctime&gt; int main() { auto start = std::chrono::system_clock::now(); inferenceEngine(); // Do the inference auto end = std::chrono::system_clock::now(); std::chrono::duration&lt;double&gt; elapsed_seconds = end-start; std::time_t end_time = std::chrono::system_clock::to_time_t(end); std::cout &lt;&lt; \"finished computation at \" &lt;&lt; std::ctime(&amp;end_time) &lt;&lt; \"elapsed time: \" &lt;&lt; elapsed_seconds.count() &lt;&lt; \"s\\n\"; }"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Formatting my sd card in ExFAT type. Writing Jetson nano image file(official one) to sd card using 'Etcher' app. after the work, sd card does not mount well on MacOS and it is divided in 12 partitions with GUID partition map. When I mount the sdcard on Windows, so many pop-ups (maybe the number of them is 12) come out and I can't do anything with this sd card. I did same things with many sd cards(128GB, 32GB etc). regardless of whether cards have mounted at Jetson nano board. so I think there are some collisions between the formatting process and writing img process... Screenshot:",
        "answers": [
            [
                "There is nothing wrong with your Jetson. The SD card is, indeed, partitioned into many partitions but this is not a problem as the card normally should stay within Jetson, you do not need to remove it and connect to another computer. When you flash Jetson with NVIDIA package manager, it may happen that Linux partition goes first, then other partitions follow, and large unusable space remains at the end. In such case, write down the name, UUID of the ext4 partition and then use Linux tools (dd, tune2fs) from another machine to fix the layout so that all names and UUIDs stay as they initially were. But this mess normally does not happen when you download and flash the card image yourself. Just do not care about these partitions. Enjoy Jetson."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am currently trying to benchmark the Jetson TX1 against the jetson NANO, according to https://elinux.org/Jetson, they both have the maxwell architecture with 128 cuda cores for NANO and 256 for TX1. This means that normally Jetson NANO will achieve half the performance of the TX1. To test this, I created a single (float) operation multiplication kernel as follows: __global__ void matrixMultiply(float* mat1, float* mat2, int nx, int ny) { unsigned int ix = threadIdx.x + blockDim.x*blockIdx.x; unsigned int iy = threadIdx.y + blockDim.y*blockIdx.y; int idx = iy*nx + ix; mat1[idx] = mat1[idx]*mat2[idx] ; } Test : the multiplication of 2 \"float array of size 15000*15000\" resulted for TX1 = 130 ms and Jetson NANO = 150 ms. The result seems weird, it's like I am not using the second SM of TX1, therefore I profiled using sm_efficiency (TX1 and NANO = 100%) , achieved_occupancy (TX1 = 92%, NANO = 88 %). Am I missing something here or I just don't use the proper grid and block configuration. P.S: I tried all possible configuration and the best configuration for both platforms was a block of (256, 1) and a grid calculated accordingly.",
        "answers": [
            [
                "Am I missing something here Yes you are missing something here. Your code does not measure what you think: they both have the maxwell architecture with 128 cuda cores for NANO and 256 for TX1. This means that normally Jetson NANO will achieve half the performance of the TX1. That statement is approximately true if the limiting factor for your code is the compute performance related to the CUDA cores. However, for your code, it is not, and this is fairly straightforward to prove. We will start with some specifications: spec | TX1 | Nano | source ---------------------=-------------=----------=---------- mem bandwidth (GB/s) | 25.6 | 25.6 | 1,2 ---------------------=-------------=----------=---------- (FP32) compute cores | 256 | 128 | 1,2 ---------------------=-------------=----------=---------- max core clock (MHz) | 998 | 921 | 1,2 sources: 1, 2 To compute the maximum theoretical FP32 compute throughput, the formula is: # of SMs * # of FP32 units per SM * 2 * clock rate For Jetson NANO: 128 * 2 * 921MHz = ~236GFlops/s For Jetson TX1: 256 * 2 * 998MHz = ~511GFlops/s (the 2 multiplier in the above formulas is due to the fact that the maximum throughput is for a code that does multiply-add operations, not just multiply) Now lets analyze the ratio of FP32 compute to memory utilization in your code (ignoring any integer arithmetic for index calculation): mat1[idx] = mat1[idx]*mat2[idx] ; We see that for each FP32 multiply operation, we must read two quantities (8 bytes total) and write one quantity (4 bytes total). So 12 bytes read/write for each multiply operation. Now let's suppose you could hit the peak multiply throughput on TX1 of 511GFlops/s. That is 511,000,000,000 multiply-add operations per second, or ~256,000,000,000 multiply operations. If you could hit 256B multiply operations per second, each multiply would need 12 bytes of read/write activity, so the total bandwidth required would be: 256,000,000,000 multiply ops 12 bytes 3,072,000,000,000 bytes ---------------------------- * ----------- = ----------------------- sec multiply op sec That means it would require ~3 Terabytes per second of memory bandwidth, for your code to be limited by the compute throughput of TX1. But TX1 only has 25.6 Gigabytes per second of memory bandwidth. So the memory bandwidth of TX1 will limit the throughput of your code. A similar calculation shows that memory bandwidth of NANO will also limit the throughput of your code, and therefore the predictor for performance ratio between the two for your code is the ratio of memory bandwidth: 25.6GB/s -------- = 1 25.6GB/s Therefore the fact that you observed almost the same performance between the two: 150 --- = 1.15 130 is a much more sensible outcome, for your code, than to expect the performance ratio to be 2:1. If you want to see a code that comes closer to the 2:1 ratio, you'll need a code that does a lot of compute operations while consuming (relatively speaking) almost no memory bandwidth. A possible real-world example of such a code might be a matrix-matrix multiply, and you can easily write a CUBLAS Sgemm code to test this. Note that a 2:1 ratio expectation isn't quite right here, because the core clocks are not the same. The expected ratio would be: 511 --- = ~2.17 236"
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "Hey all fairly new to Tensorflow and TensorRT, I am having trouble converting an existing frozen graph to a tensorRT graph. I do not think the code I have is successfully converting my graph. Running this on a Nvidia Jetson Nano. I have tried to follow the guidelines as seen here: https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#using-frozengraph def load_object_detection_model(self): # Load TensorFlow object detection model gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=.5) EXPORTED_OBJECT_DETECTION_MODEL = 'frozen_model_x.pb' self.graph_obj = tf.Graph() with self.graph_obj.as_default(): od_graph_def = tf.GraphDef() with tf.gfile.GFile(EXPORTED_OBJECT_DETECTION_MODEL, 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='') # Optimize Graph with TensorRT trt_graph = trt.create_inference_graph( input_graph_def=od_graph_def, outputs=['num_detections', 'detection_boxes', 'detection_scores', 'detection_classes'], max_batch_size=1, max_workspace_size_bytes=4000000000, precision_mode='FP16') print('reading graph') output_node = tf.import_graph_def( trt_graph, return_elements=['num_detections', 'detection_boxes', 'detection_scores', 'detection_classes']) self.graph_obj = output_node # Replace frozen graph with optimized graph print('converted graph') The error output I get is: \" in load_object_detection_model ops = self.graph_obj.get_operations() AttributeError: 'list' object has no attribute 'get_operations'\" which corresponds to the code below: # get handles to objects in object detection graph ops = self.graph_obj.get_operations() all_tensor_names = {output.name for op in ops for output in op.outputs} self.tensor_dict = {} for key in [ 'num_detections', 'detection_boxes', 'detection_scores', 'detection_classes', 'detection_masks' ]: tensor_name = key + ':0' if tensor_name in all_tensor_names: self.tensor_dict[key] = self.graph_obj.get_tensor_by_name(tensor_name) self.obj_image_tensor = self.graph_obj.get_tensor_by_name('image_tensor:0') self.logger.debug('created object detection model graph from {}'.format(EXPORTED_OBJECT_DETECTION_MODEL)) # create session for object detection self.sess_obj = tf.Session(graph=self.graph_obj) self.logger.debug('created object detection model session') (This code above comes right after the previous code snippet). Running Ubuntu 18.04, Python 3.6.8, TensorFlow 1.13.1. TensorRT details below: ii graphsurgeon-tf 5.0.6-1+cuda10.0 arm64 GraphSurgeon for TensorRT package ii libnvinfer-dev 5.0.6-1+cuda10.0 arm64 TensorRT development libraries and headers ii libnvinfer-samples 5.0.6-1+cuda10.0 all TensorRT samples and documentation ii libnvinfer5 5.0.6-1+cuda10.0 arm64 TensorRT runtime libraries ii python-libnvinfer 5.0.6-1+cuda10.0 arm64 Python bindings for TensorRT ii python-libnvinfer-dev 5.0.6-1+cuda10.0 arm64 Python development package for TensorRT ii python3-libnvinfer 5.0.6-1+cuda10.0 arm64 Python 3 bindings for TensorRT ii python3-libnvinfer-dev 5.0.6-1+cuda10.0 arm64 Python 3 development package for TensorRT ii tensorrt 5.0.6.3-1+cuda10.0 arm64 Meta package of TensorRT ii uff-converter-tf 5.0.6-1+cuda10.0 arm64 UFF converter for TensorRT package",
        "answers": [
            [
                "TensorRT Python API is not supported on Jetson platform due to pyCUDA. However, the python parser is working well. Here are some alternatives for your reference: Python -&gt; [Wrapper] -&gt; C++ inference TensorFlow-TensorRT You can use Cython to wrap TensorRT C++ code, so that you can call them from python. For more details, please refer to Cython\u2019s Documentations. And there is an example on Jetson Nano that may be helpful : Running TensorRT Optimized GoogLeNet on Jetson Nano"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am getting error in importing Logger() and Builder() I am on Jetson AGX Xavier. I have tried on python shell also. import tensorflow.contrib.tensorrt as trt TRT_LOGGER = trt.Logger(trt.Logger.INFO) Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; AttributeError: module 'tensorflow.contrib.tensorrt' has no attribute 'Logger' I can NOT import trt.Builder() also. Python version is 3.6.7 Output of \"dpkg -l | grep nvinfer\" which gives Tensor RT version : ii libnvinfer-dev 5.0.6-1+cuda10.0 arm64 TensorRT development libraries and headers ii libnvinfer-samples 5.0.6-1+cuda10.0 all TensorRT samples and documentation ii libnvinfer5 5.0.6-1+cuda10.0 arm64 TensorRT runtime libraries ii python-libnvinfer 5.0.6-1+cuda10.0 arm64 Python bindings for TensorRT ii python-libnvinfer-dev 5.0.6-1+cuda10.0 arm64 Python development package for TensorRT ii python3-libnvinfer 5.0.6-1+cuda10.0 arm64 Python 3 bindings for TensorRT ii python3-libnvinfer-dev 5.0.6-1+cuda10.0 arm64 Python 3 development package for TensorRT",
        "answers": [
            [
                "The package you are importing import tensorflow.contrib.tensorrt as trt is not TensorRT, this is the package that integrates TensorRT into TF. This package doesn't have the modules you are looking for such as Logger or Builder. This package has its own APIs which are used to optimize a TF models using TensorRT. More links for the integration of TensorRT into TF: https://docs.nvidia.com/deeplearning/dgx/tf-trt-user-guide/index.html https://github.com/tensorflow/tensorrt/tree/master/tftrt/examples/image-classification https://developer.nvidia.com/gtc/2019/video/S9431 If you are looking for using TensorRT API directly outside of TF, you should import it like this: import tensorrt as trt. https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#importing_trt_python"
            ],
            [
                "I solved the problem. What I have done is: I have removed Tensorflow 1.13 and installed 1.12. I deleted all the virtual environment. Made No changes to Jetpack 4.2 and its Tensor RT versions. But still I could not find what was the issue. Thanks"
            ],
            [
                "I solved the problem by doing: However, i was importing Tensorrt and not Tensorflow: pip install nvidia-pyindex pip install nvidia-tensorrt"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am trying to install opencv following the tutorial - https://www.pyimagesearch.com/2018/05/28/ubuntu-18-04-how-to-install-opencv/ I got through till 'make -j4' and made 100% compile. After this step, when I try ls /usr/local/python/cv2/python-3.6, I get the following error - ls: cannot access '/usr/local/python/cv2/python-3.6': No such file or directory cd /usr/local/python/cv2 ls -l total 16 -rw-r--r-- 1 root root 98 Jul 3 13:55 config.py -rw-r--r-- 1 root root 2857 Nov 17 2018 init.py -rw-r--r-- 1 root root 151 Nov 17 2018 load_config_py2.py -rw-r--r-- 1 root root 262 Nov 17 2018 load_config_py3.py which python /home/ciaran/.virtualenvs/cv/bin/python I have looked at multiple forums online and tried troubleshooting but I still can't get past this step.",
        "answers": [
            [
                "The error is appearing because python is not set in your path, so your system doesn't know where python is. Run the below command in your terminal and it should fix it: export PATH=\"$PATH:/usr/local/bin/python\""
            ],
            [
                "Solved this. \"If your compile chokes and hangs, it may be due to a threading race condition. In the event you run into this problem, simply delete your build directory, recreate it, and re-run cmake and make . This time do not include the flag next to make.\" Used $ make in place of $ make -j4 in the tutorial."
            ],
            [
                "Having the same issue with Python 3.6, managed to find the file using the search tool in the GUI. Mine is in: /usr/local/lib/python3.6/site-packages/cv2/python-3.6/ Named cv2.cpython-36m-aarch64-linux-gnu.so Then it worked corrrectly"
            ]
        ],
        "votes": [
            1e-07,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am currently setting up a microcontroller with several libraries which need to be built from source. Using pre-built binaries is not an option due to the system architecture. Building dependencies takes a lot of time and I want to avoid having to do it again for every similar device I need to setup in the future. Thus, my question is, how can I migrate custom built binaries to another machine of similar architecture? Any solution that would mirror the whole system to another drive works, too. Note: For my current use case I am running Ubuntu 18.04 off a MicroSD plugged into a Jetson Nano",
        "answers": [
            [
                "Any solution that would mirror the whole system to another drive works, too. Proposed Solution : Create a backup of the MicroSD card which has all required binaries installed Use the backup to mirror the stuff into different MicroSD cards. Backing Up Your SD card Connect the SDcard to your laptop Use dd command to take a backup of your MicroSD card sudo dd if=/dev/sdxx of=backup.img status=progress Restoring your backup to a New SD Card Connect the New SDcard to your laptop Use dd command to restore the backup to New MicroSD card sudo dd if=backup.img of=/dev/sdxx status=progress Note: Your SD card may also show up as /dev/mmcxx or /dev/sdxx depending on how you connect it to your laptop. Warning: While running dd command, Please make sure that /dev/sdxx is your SD card and not your Hard Disk. Running this command will tell you the device name of your SD card. sudo fdisk -l Please refer to this link for more."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am porting code to train a neural network. I wrote the code as part of an Udacity project and it worked fine in the Udacity environment. Now I am porting the code to an Nvidia Jetson Nano running Ubuntu 18.04 and Python 3.6.8. When iterating through the training data, somehow \"._\" sneakes into the file path prior the file name and issues an error message. When I run the file, I get following error message: Traceback (most recent call last): File \"train_rev6.py\", line 427, in &lt;module&gt; main() File \"train_rev6.py\", line 419, in main train_model(in_args) File \"train_rev6.py\", line 221, in train_model for inputs, labels in trainloader: File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 560, in __next__ batch = self.collate_fn([self.dataset[i] for i in indices]) File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 560, in &lt;listcomp&gt; batch = self.collate_fn([self.dataset[i] for i in indices]) File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 132, in __getitem__ sample = self.loader(path) File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 178, in default_loader return pil_loader(path) File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 160, in pil_loader img = Image.open(f) File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2705, in open % (filename if filename else fp)) OSError: cannot identify image file &lt;_io.BufferedReader name='/home/mme/Documents/001_UdacityFinalProjectFlowersRev2/flowers/train/40/._image_04589.jpg'&gt; I suspect the error is due to the \"._\" prior the file name \"image...\", as this is not part of the file name and when I prompt sudo find / -name image_00824.jpg I get the correct path: /home/mme/Documents/001_UdacityFinalProjectFlowersRev2/flowers/train/81/image_00824.jpg without \"._\" prior the file name. My issue here seems the same as in OSError: cannot identify image file (Adjusting and running from PIL import Image;Image.open(open(\"path/to/file\", 'rb')) as suggested in the answer does not issue an error message.) The file path is give in the command line: python3 train_rev6.py --file_path \"/home/mme/Documents/001_UdacityFinalProjectFlowersRev2/flowers\" --arch \"vgg16\" --epochs 5 --gpu \"gpu\" --running_loss True --valid_loss True --valid_accuracy True --test True The code below shows the two relevant functions. Any idea how I get rid of this \"._\"? def load_data(in_args): \"\"\" Function to: - Specify diretories for training, validation and test set. - Define your transforms for the training, validation and testing sets. - Load the datasets with ImageFolder. - Using the image datasets and the trainforms, define the dataloaders. - Label mapping. \"\"\" # Specify diretories for training, validation and test set. data_dir = in_args.file_path train_dir = data_dir + \"/train\" valid_dir = data_dir + \"/valid\" test_dir = data_dir + \"/test\" # Define your transforms for the training, validation, and testing sets # Means: [0.485, 0.456, 0.406]. Standard deviations [0.229, 0.224, 0.225]. Calculated by ImageNet images. # Transformation on training set: random rotation, random resized crop to 224 x 224 pixels, random horizontal and vertical flip, tranform to a tensor and normalize data. train_transforms = transforms.Compose([transforms.RandomRotation(23), transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) # Transformation on validation set: resize and center crop to 224 x 224 pixels, tranform to a tensor and normalize data. valid_transforms = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) # Transformation on test set: resize and center crop to 224 x 224 pixels, tranform to a tensor and normalize data. test_transforms = transforms.Compose([transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) # Load the datasets with ImageFolder global train_dataset global valid_dataset global test_dataset train_dataset = datasets.ImageFolder(data_dir + \"/train\", transform=train_transforms) valid_dataset = datasets.ImageFolder(data_dir + \"/valid\", transform=valid_transforms) test_dataset = datasets.ImageFolder(data_dir + \"/test\", transform=test_transforms) # Using the image datasets and the trainforms, define the dataloaders, as global variables. global trainloader global validloader global testloader trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=64) testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64) # Label mapping. global cat_to_name with open(\"cat_to_name.json\", \"r\") as f: cat_to_name = json.load(f) print(\"Done loading data...\") return def train_model(in_args): \"\"\" Function to build and train model. \"\"\" # Number of epochs. global epochs epochs = in_args.epochs # Set running_loss to 0 running_loss = 0 # Prepare lists to print losses and accuracies. global list_running_loss global list_valid_loss global list_valid_accuracy list_running_loss, list_valid_loss, list_valid_accuracy = [], [], [] # If in testing mode, set loop counter to prematurly return to the main(). if in_args.test == True: loop_counter = 0 # for loop to train model. for epoch in range(epochs): # for loop to iterate through training dataloader. for inputs, labels in trainloader: # If in testing mode, increase loop counter to prematurly return to the main() after 5 loops. if in_args.test == True: loop_counter +=1 if loop_counter == 5: return # Move input and label tensors to the default device. inputs, labels = inputs.to(device), labels.to(device) # Set gradients to 0 to avoid accumulation optimizer.zero_grad() # Forward pass, back propagation, gradient descent and updating weights and bias. # Forward pass through model to get log of probabilities. log_ps = model.forward(inputs) # Calculate loss of model output based on model prediction and labels. loss = criterion(log_ps, labels) # Back propagation of loss through model / gradient descent. loss.backward() # Update weights / gradient descent. optimizer.step() # Accumulate loss for training image set for print out in terminal running_loss += loss.item() # Calculate loss for verification image set and accuracy for print out in terminal. # Validation pass and print out the validation accuracy. # Set loss of validation set and accuracy to 0. valid_loss = 0 # test_loss = 0 valid_accuracy = 0 # test_accuracy = 0 # Set model to evaluation mode to turn off dropout so all images in the validation &amp; test set are passed through the model. model.eval() # Turn off gradients for validation, saves memory and computations. with torch.no_grad(): # for loop to evaluate loss of validation image set and its accuracy. for valid_inputs, valid_labels in validloader: # Move input and label tensors to the default device. valid_inputs, valid_labels = valid_inputs.to(device), valid_labels.to(device) # Run validation image set through model. valid_log_ps = model.forward(valid_inputs) # Calculate loss for validation image set. valid_batch_loss = criterion(valid_log_ps, valid_labels) # Accumulate loss for validation image set. valid_loss += valid_batch_loss.item() # Calculate probabilities valid_ps = torch.exp(valid_log_ps) # Get the most likely class using the ps.topk method. valid_top_k, valid_top_class = valid_ps.topk(1, dim=1) # Check if the predicted classes match the labels. valid_equals = valid_top_class == valid_labels.view(*valid_top_class.shape) # Calculate the percentage of correct predictions. valid_accuracy += torch.mean(valid_equals.type(torch.FloatTensor)).item() # Print out losses and accuracies # Create string for running_loss. str1 = [\"Train loss: {:.3f} \".format(running_loss) if in_args.running_loss == True else \"\"] str1 = \"\".join(str1) # Create string for valid_loss. str2 = [\"Valid loss: {:.3f} \".format(valid_loss/len(validloader)) if in_args.valid_loss == True else \"\"] str2 = \"\".join(str2) # Create string for valid_accuracy. str3 = [\"Valid accuracy: {:.3f} \".format(valid_accuracy/len(validloader)) if in_args.valid_accuracy == True else \"\"] str3 = \"\".join(str3) # Print strings print(f\"{epoch+1}/{epochs} \" + str1 + str2 + str3) # Append current losses and accuracy to lists to print losses and accuracies. list_running_loss.append(running_loss) list_valid_loss.append(valid_loss/len(validloader)) list_valid_accuracy.append(valid_accuracy/len(validloader)) # Set running_loss to 0. running_loss = 0 # Set model back to train mode. model.train() print(\"Done training model...\") return",
        "answers": [
            [
                "A colleague at work pointed out that in Linux files beginning with a period are hidden files. So I selected \"show hidden files\" in the file explorer and there they were. I deleted them, which resolved the issue (see commands below). Find and display all files beginning with \"._\" in all subfolder (display the selected files first to make sure these are the files you want to delete): find test -name '._*' -print Find and delete all files beginning with \"._\" in all subfolder find test -name '._*' -delete"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Closed. This question needs to be more focused. It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 4 years ago. Improve this question I would like to use a DJI M210 drone for mapping. I'm actually using a LiDAR and the SLAM algorithm to map a GPS-denied environment. I would like to know, if someone knows, is it possible to control the drone in velocity, without the GPS (I would send Odometry to the drone from SLAM). I've read that the velocity control from DJI is only done by the GPS interpretation.",
        "answers": [
            [
                "It is possible to control the method using only local coordinate system. I tested with various SLAM on various DJI drone/FCU with all kind of PC TK1 TX2 UPsqare NUC Manifold1 and 2(both cpu and gpu). Only problem is that you cant use the GPS mission way. Use raw command way. remove all GPS related sensor_msgs::Joy controlVelYawRate; uint8_t flag = (DJISDK::VERTICAL_VELOCITY | DJISDK::HORIZONTAL_VELOCITY | DJISDK::YAW_RATE | DJISDK::HORIZONTAL_GROUND | DJISDK::STABLE_ENABLE); controlVelYawRate.axes.push_back(y); controlVelYawRate.axes.push_back(p); controlVelYawRate.axes.push_back(r); controlVelYawRate.axes.push_back(t); controlVelYawRate.axes.push_back(flag); ctrlBrakePub.publish(controlVelYawRate);"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to use OpenCV in some function which I annotate with some Numba decorators (e.g. nopython=True, parallel=True). I run this on Jetson Xavier which was flashed with Nvidia SDK Manager. Code is this: @jit(nopython=True, cache=True, parallel=True) def decompress(data): result = list() for d in data: cv2_image = cv2.imdecode(d, cv2.IMREAD_COLOR) image = np.array(cv2_image) result.append(image) return result But I get an error: TypingError: Failed in nopython mode pipeline (step: nopython frontend) Unknown attribute 'resize' of type Module(&lt;module 'cv2' from '/usr/lib/python2.7/dist-packages/cv2.so'&gt;) File \"./my-script.py\", line 297: def decompress(data): &lt;source elided&gt; cv2_image = cv2.imdecode(d, cv2.IMREAD_COLOR) ^ [1] During: typing of get attribute at /ssd/catkin_workspace/src/semantics-ros-wrapper/src/semseg.py (297) File \"./my-script.py\", line 297: def decompress(data): &lt;source elided&gt; cv2_image = cv2.imdecode(d, cv2.IMREAD_COLOR) ^ This is not usually a problem with Numba itself but instead often caused by the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit: http://numba.pydata.org/numba-doc/dev/reference/pysupported.html and http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html For more information about typing errors and how to debug them visit: http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile If you think your code should work with Numba, please report the error message and traceback, along with a minimal reproducer at: https://github.com/numba/numba/issues/new Is it even possible to use OpenCV with Numba? I'm using: python2.7 and numba-0.44.0.",
        "answers": [
            [
                "Numba does not support OpenCV yet. If you still want it to run on the numpy arrays in your functions you could set nopython=False This means you will also not be able to set parallel=True This is from the Numba User Manual: Numba has two compilation modes: nopython mode and object mode. The former produces much faster code, but has limitations that can force Numba to fall back to the latter. To prevent Numba from falling back, and instead raise an error, pass nopython=True."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I have a python3 program that uses CUDA, CNNs, OpenCV, a USB camera,UART communication with an arduino... I would like to launch the program at startup of my jetson-TX2 ( using ubuntu 18.04) but I don't know how to do so. I tried to add this command: /bin/bash -c \"sleep 15 &amp;&amp; python3 /home/nvidia/program.py\" to the \"Startup applications\". ( the command works when running on terminal) But nothing shows up. Thank you for your help!!",
        "answers": [
            [
                "You're gonna wanna go to your handy dandy command line, and type \"crontab -e\" It may ask you to select a text editor by entering a number. If it does, just pick whichever you prefer. Scroll to the bottom of the file, and make a new line that says \"@reboot python3 /path/to/script\""
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I tried running a 5v stepper motor using RPI code on jetson nano and it gives me ValueError: The channel sent is invalid on a JETSON_NANO Board I have installed jetson gpio and still giving me same error but the code works fine on raspberry pi import Jetson.GPIO as GPIO #import RPi.GPIO as GPIO import time GPIO.setmode(GPIO.BCM) #GPIO.setmode(GPIO.BOARD) control_pins = [3,5,7,11] for pin in control_pins: GPIO.setup(pin, GPIO.OUT) GPIO.output(pin, 0) halfstep_seq = [ [1,0,0,0], [1,1,0,0], [0,1,0,0], [0,1,1,0], [0,0,1,0], [0,0,1,1], [0,0,0,1], [1,0,0,1] ] for i in range(56): for halfstep in range(8): for pin in range(4): GPIO.output(control_pins[pin], halfstep_seq[halfstep][pin]) time.sleep(0.001) GPIO.cleanup()",
        "answers": [
            [
                "You need to use the number-s indicated by Dxx labels from the bottom of the dev board: So, assuming we want to use the physical pins 3, 5, 7 and 11, the control_pins array should be [2, 3, 4, 17]"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to install numba on my jetson tx2 so that I can use tf-pose-estimation. All my libraries are not on the anaconda python3 but on the base python instead so I would like to download the numba library without using anaconda. Can you walk me through how to install the numba library on the jetson (etc. what additional packages I have to install before install the numba library) I have tried pip3 install numba but the install fails when building llvmlite. My python version is 3.5.",
        "answers": [
            [
                "Juliet Teoh. You need to install LLVM 7.0.1 first and then Numba. For LLVM, you need to run this: $ wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz $ tar -xvf llvm-7.0.1.src.tar.xz $ cd llvm-7.0.1.src.tar.xz $ mkdir llvm_build_dir $ cd llvm_build_dir/ $ cmake ../ -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD=\"ARM;X86;AArch64\" $ make -j4 $ sudo make install $ cd bin/ $ echo \"export LLVM_CONFIG=\\\"\"`pwd`\"/llvm-config\\\"\" &gt;&gt; ~/.bashrc $ echo \"alias llvm='\"`pwd`\"/llvm-lit'\" &gt;&gt; ~/.bashrc $ source ~/.bashrc $ sudo pip3 install llvmlite For Numba, you can run this: $ sudo pip3 install numba I put this instruction in https://github.com/jefflgaol/Install-Packages-Jetson-ARM-Family. You may find another installation tutorial there too."
            ],
            [
                "Install conda4aarch64. This will create a minimal conda environment. Add the c4aarch64 and conda-forge channels to your conda configuration: conda config --add channels c4aarch64 conda config --add channels conda-forge Then you can install Numba from the numba channel: conda install -c numba numba"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm trying to make a Jetson TX2 with Ubuntu 18.04 stream its CSI input encoded in H264 to UDP multicast with gstreamer. I've gone through (like) whole internet and tried all the codes and I'm unable to connect to it with VLC either by URL nor by SDP file. However I'm able to connect to it from same computer with another gstreamer instance. Target is to be able to connect to the stream from browser, but I guess it'd be easier to make it work with VLC. Server code: gst-launch-1.0 nvarguscamerasrc ! nvvidconv ! omxh264enc ! 'video/x-h264, stream-format=(string)byte-stream' ! h264parse ! rtph264pay mtu=1400 ! udpsink host=239.127.1.21 port=5000 Receiver code: gst-launch-1.0 udpsrc port=5000 ! application/x-rtp,encoding-name=H264,payload=96 ! rtph264depay ! h264parse ! avdec_h264 ! xvimagesink This works and the receiver code open window with the video. When I've tried to receive by VLC I've had some problems but basically nothing works and VLC is only connecting, I've tried: vlc udp://:5000 vlc rtp://:5000 error: A description in SDP format is required to receive the RTP stream. Note that rtp:// URIs cannot work with dynamic RTP payload format (96). vlc udp://239.127.1.21:5000 vlc rtp://239.127.1.21:5000 (same error as above for the rtp...) vlc v.sdp with sdp file: v=0 c=IN IP4 239.192.1.21 m=video 5000 RTP/AVP 96 a=rtpmap:96 H264/90000 I've tried various IPs - server's IP, localhost, I've tried to connect from another computer and tried various combinations of client's IPs and server IPs. I was unable to connect also to unicast but I need to make multicast work. So my question basically is: how to connect to gstreamer stream from browser.",
        "answers": [
            [
                "Could you try gst-launch-1.0 nvarguscamerasrc ! nvvidconv ! omxh264enc ! 'video/x-h264, stream-format=(string)byte-stream' ! h264parse ! rtph264pay mtu=1400 config-interval=1 pt=96 ! udpsink host=239.127.1.21 port=5000 with the sdp file v=0 c=IN IP4 239.127.1.21 m=video 5000 RTP/AVP 96 a=rtpmap:96 H264/90000"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "If you aren't using a virtualenv, the cv2 module is imported successfully nano@nano:~$ python3 Python 3.6.7 (default, Oct 22 2018, 11:32:17) [GCC 8.2.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import cv2 &gt;&gt;&gt; cv2.__version__ '3.3.1' &gt;&gt;&gt; However, if you create a virtual environment, cv2 is not found (env) nano@nano:~$ python3 Python 3.6.7 (default, Oct 22 2018, 11:32:17) [GCC 8.2.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import cv2 Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; ModuleNotFoundError: No module named 'cv2'",
        "answers": [
            [
                "More specifically it seems you need to simlink opencv as: first: cd into your_venv/lib/python3.6/site-packages and: ln -s /usr/lib/python3.6/dist-packages/cv2.cpython-36m-aarch64-linux-gnu.so cv2.cpython-36-m-aarch64-linux-gnu.so"
            ],
            [
                "You can copy the directory /usr/local/lib/python3.6/dist-packages/cv2/ to your virtual env folder, for example env/lib/python3.6/dist-packages/cv2/"
            ],
            [
                "Either you can install opencv in virtual environment or can make symbolic link of library to vitual env in bashrc. ln -s source_file destination_file"
            ]
        ],
        "votes": [
            5.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am trying to run a code on TX2 but the tensorflow code that allocates GPU memory usage seems to be working in a weird manner. Here's the code I have to allocate memory: config = tf.ConfigProto() config.gpu_options.per_process_gpu_memory_fraction = 0.0 config.gpu_options.visible_device_list = \"0\" set_session(tf.Session(config=config)) The weird thing is, when I use 0.0 instead of 0.5, the processing is faster. And when I use 0.9, I get the following error: tensorflow.python.framework.errors_impl.InternalError: GPU sync failed What's happening here?",
        "answers": [
            [
                "First thing to check will be to verify if compatible CUDA, cuDNN versions are correctly installed and to reboot the system. Then, Allowing GPU memory growth can help. https://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth Perhaps, you can try: import tensorflow as tf config = tf.ConfigProto() config.gpu_options.allow_growth = True sess = tf.Session(config=config) set_session(sess)"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm getting started with my Jetson Nano and I'm looking for an example that I can launch by running docker run xxx where xxx is some image at DockerHub that uses the GPU. I assume I'll have to pass in some --device flags, but is there even any kind of \"hello world\"-style sample ready to go that uses the GPU from docker? I'm hoping to just demonstrate that you can access the GPU from a docker container on the Jetson Nano. Mostly to make sure that my configuration is correct.",
        "answers": [
            [
                "Nvidia Jetpack 4.2.1 enabled easy Docker with GPU on Jetson Nano. See here for detailed instruction on how to get Docker and Kubernetes running on Jetson Nano with GPU: https://medium.com/jit-team/building-a-gpu-enabled-kubernets-cluster-for-machine-learning-with-nvidia-jetson-nano-7b67de74172a It uses a simple Docker Hub hosted image for TensorFlow:"
            ],
            [
                "You're not alone in wanting that, but you cannot do it at the moment. The NVIDIA Nano team are aware of the need, and the feature is expected later this year. See https://devtalk.nvidia.com/default/topic/1050809/jetson-nano/docker-image-to-see-if-cuda-is-working-in-container-on-jetson-nano-/ At present you can run a docker container with TensorFlow or PyTorch installed. but it will only use the CPU, not the GPU."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I have built an object detection model using digits Detectnet. and I want to measure the size of the detected object. I have tested it on my Jetson TX2. and also want the measuring in the jetson after it detects, like save the diminution to a file. the detectnet code that I am using is detectnet-camera from this tutorial jetson-inference/detectnet-camera/detectnet-camera.cpp which I used it to detect objects from my trained model. I also want it to measure the dimensions of the detected object. I have searched on the internet and found this method useful Measuring the size of objects in an image with OpenCV but I don't know how to integrate it with detectnet-camera. anyone have some other way to do that or any advice on how to use that one I will be thankful for you",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm having issues with profiling my C++ code on a Jetson TX2. The profiling itself works, but I don't get the function names, but instead, it says that a part is unknown. I installed pprof with go and it works on my Raspberry Pi, zo i guess id did the correct steps. For example the difference between my Jetson and my Pi: The result I get on the Jetson is: LD_PRELOAD=\"/usr/lib/libprofiler.so.0\" CPUPROFILE=./tmp/profile.log ./testProg Worker threads activated: 1 Done PROFILE: interrupts/evictions/bytes = 3792/1650/150928 pprof --functions --text ./testProg ./tmp/profile.log Some binary filenames not available. Symbolization may be incomplete. Try setting PPROF_BINARY_PATH to the search path for local binaries. File: testProg Type: cpu Showing nodes accounting for 37.15s, 97.97% of 37.92s total Dropped 38 nodes (cum &lt;= 0.19s) flat flat% sum% cum cum% 16s 42.19% 42.19% 16s 42.19% inflateBackEnd 5.88s 15.51% 57.70% 5.88s 15.51% cv::(anonymous namespace)::medianBlur_SortNet 3.92s 10.34% 68.04% 3.92s 10.34% png_get_uint_16 2.86s 7.54% 75.58% 2.86s 7.54% inflate 1.89s 4.98% 80.56% 1.89s 4.98% adler32 1.15s 3.03% 83.60% 1.15s 3.03% memcpy 1.13s 2.98% 86.58% 1.13s 2.98% png_set_invert_mono 0.93s 2.45% 89.03% 0.93s 2.45% crc32 0.68s 1.79% 90.82% 0.68s 1.79% carotene_o4t::bgr2gray 0.62s 1.64% 92.46% 0.62s 1.64% sched_yield 0.48s 1.27% 93.72% 0.48s 1.27% [[vdso]] 0.48s 1.27% 94.99% 0.48s 1.27% cv::ThresholdRunner::operator() 0.47s 1.24% 96.23% 0.47s 1.24% inflateMark 0.37s 0.98% 97.20% 0.37s 0.98% read 0.29s 0.76% 97.97% 0.29s 0.76% cvFindNextContour 0 0% 97.97% 37.92s 100% On the Pi I get: LD_PRELOAD=\"/usr/lib/libprofiler.so.0\" CPUPROFILE=./tmp/profile.log ./testProg Worker threads activated: 1 Done PROFILE: interrupts/evictions/bytes = 16223/283/24060 pprof --functions --text ./testProg ./tmp/profile.log File: testProg Type: cpu Showing nodes accounting for 159.98s, 98.61% of 162.23s total Dropped 97 nodes (cum &lt;= 0.81s) flat flat% sum% cum cum% 75.63s 46.62% 46.62% 75.63s 46.62% cv::medianBlur_8u_Om 37.26s 22.97% 69.59% 37.26s 22.97% inflateBackEnd 11.58s 7.14% 76.72% 11.58s 7.14% png_set_read_user_transform_fn 6.45s 3.98% 80.70% 6.45s 3.98% memcmp 5.45s 3.36% 84.06% 5.45s 3.36% inflate 4.46s 2.75% 86.81% 4.46s 2.75% adler32 3.91s 2.41% 89.22% 3.91s 2.41% png_set_invert_mono 3.86s 2.38% 91.60% 3.87s 2.39% cvFindNextContour 2.97s 1.83% 93.43% 2.97s 1.83% cv::CvtColorLoop_Invoker::operator() 2s 1.23% 94.66% 2s 1.23% read 1.90s 1.17% 95.83% 1.90s 1.17% cv::ThresholdRunner::operator() 1.80s 1.11% 96.94% 1.80s 1.11% crc32 1.25s 0.77% 97.71% 1.25s 0.77% munmap 1.22s 0.75% 98.47% 1.22s 0.75% inflateMark 0.10s 0.062% 98.53% 5.40s 3.33% tbb::empty_task::~empty_task 0.03s 0.018% 98.55% 2.04s 1.26% __GI__IO_file_xsgetn 0.02s 0.012% 98.56% 3.14s 1.94% rml::internal::thread_monitor::detach_thread 0.02s 0.012% 98.57% 4.91s 3.03% tbb::interface7::internal::start_for::execute 0.01s 0.0062% 98.58% 2.05s 1.26% _GI__IO_fread 0.01s 0.0062% 98.58% 4.18s 2.58% cv::findContours 0.01s 0.0062% 98.59% 2.33s 1.44% cv::parallel_for 0.01s 0.0062% 98.59% 4.24s 2.61% imgprocessing::blobfind 0.01s 0.0062% 98.60% 2.31s 1.42% tbb::interface7::internal::delegated_function::operator() 0.01s 0.0062% 98.61% 2.32s 1.43% tbb::interface7::internal::task_arena_base::internal_execute 0.01s 0.0062% 98.61% 3.12s 1.92% tbb::task_scheduler_init::initialize 0 0% 98.61% 1.30s 0.8% _IO_new_file_underflow 0 0% 98.61% 82.20s 50.67% __libc_start_main 0 0% 98.61% 1.72s 1.06% cv::cvtColor 0 0% 98.61% 1.72s 1.06% cv::cvtColorBGR2Gray 0 0% 98.61% 1.71s 1.05% cv::hal::cvtBGRtoGray 0 0% 98.61% 75.65s 46.63% cv::medianBlur 0 0% 98.61% 76.06s 46.88% imgprocessing::filter_img (inline) 0 0% 98.61% 82.20s 50.67% imgprocessing::process 0 0% 98.61% 82.20s 50.67% main 0 0% 98.61% 2.30s 1.42% tbb::interface7::internal::start_for::run As you can see, profiling the OpenCV functions I used is working fine, but my own code is called unknown. I have the -g flag enabled while compiling. I use the following line to profile my code (makefile) LD_PRELOAD=\"/usr/lib/libprofiler.so.0\" CPUPROFILE=./tmp/profile.log ./$(PROG) pprof --functions --text ./$(PROG) ./tmp/profile.log ...When I use google-pprof instead of pprof I don't get the unknown but only addresses (like 0x0000007f7a090498). I tried to solve the error but without success. I tried a lot and guess it is something with coupling the addresses to function names. When I try addr2line I get: addr2line -e testProg 0x7f7a090498 ??:0 I hope someone can help me.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have the next problem, I'm developing a cuda application for the jetson TX2 board. All the development it's done in a ubuntu machine and then I run the program remotely in the jetson. I'm using CUDA 10.0 and the PCL library 1.9.1 that was built from sources in the jetson with CUDA and QT5 features. I also compiled from sources in the jetson the VTK library and boost 1.65. Everything compiles fine but every time I try to run my program in the jetson I get the next error: error while loading shared libraries: libcufft.so.9.0 Because I'm using CUDA 10 I don't have this library and in code I don't any reference to cufft so I guess that a library I'm using is creating this dependency. What I want to know is if there is some way to know what part of the code or what library could be creating this dependency in order to remove the error. I have tried the command LDD but this command tell me that the program needs the cufft.so.9.0 library but not tell me which part of the code could be creating this dependency. Next I summarize all the libraries I'm using: PCL 1.9.1 VTK 8.0 Eigen Libflann 1.9 Patate library I'm also using Nvidia nsight eclipse. Thank you!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have optimized my deep learning model with TensorRT. A C++ interface is inferencing images by optimized model on Jetson TX2. This interface is providing average 60 FPS (But it is not stable. Inferences are in range 50 and 160 FPS). I need to run this system as real time on real time patched Jetson. So what is your thoughts on real time inference with TensorRT? Is it possible to develop real time inferencing system with TensorRT and how? I have tried set high priorities to process and threads to provide preemption. I expect appoximatly same FPS value on every inference. So I need deterministic inference time. But system could not output deterministicaly.",
        "answers": [
            [
                "Have you tried to set the clock on Jetson: sudo nvpmodel -m 0 Here is some links for more information: https://elinux.org/Jetson/Performance https://devtalk.nvidia.com/default/topic/999915/jetson-tx2/how-do-you-switch-between-max-q-and-max-p/post/5109507/#5109507 https://devtalk.nvidia.com/default/topic/1000345/jetson-tx2/two-cores-disabled-/post/5110960/#5110960"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am working on NVidia Jetson TX2. Unfortunately there are only about 16 GBs of storage left for my work, and an image of about this size to load to the docker. I've tried adding a SD card to the machine - I saved the image file on the SD card and tried to load it using the command line sudo ./tx2-docker load -i /media/nvidia/3038-3638/jetson_docker/lpr_image.img (tx2-docker is this wrapping script). Though, i got the following error: Error processing tar file(exit status 1): write /usr/lib/x86_64-linux-gnu/libflite_cmu_us_awb.so.2.0.0: no space left on device Meaning, i still don't have enough storage. I think that maybe storing the image on the SD card, and loading the image such that all files created by docker will be dumped to some folder to the SD might help. TL;DR / The question So, my question is - Is there a way to configure Docker such that when an image is loaded (in my case, a .img file), all files created will be dumped to some specified path?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using a Jetson TX2, and a Arduino Uno to try and communicate over USB serial, the premise is I'm using a Arduino to communicate with some Laser ToF sensors, and a Thermopile. The Jetson is running Ubuntu 16.04 for ros compatability as this will eventually tie into a ros node Using just the Arduino IDE, serial monitor it the call and response works as intended, however once I try to get the call and response working using the Jetson that's where the data isn't get correctly written printed on the terminal. The arduino prints a byte \"9\" to the Jetson when it's ready to receive, and the Jetson prints a \"1\" or a \"2\" over serial when it wants to receive the time of flight or thermal sensor data respectively. The intended communication is for the Jetson to recieve 5 comma separated, float values from the ToF Sensors, followed by 64 comma separated float values from the thermal sensor, however I get the following: Sending 1 for ToF Data Reading data 1 bytes read, buffer contains: Sending 2 for Thermal Data Reading data 35 bytes read, buffer contains: Thermal/ToF sensor data The code for the Arduino is as follows: #include &lt;Wire.h&gt; #include &lt;VL53L1X.h&gt; #include &lt;Adafruit_AMG88xx.h&gt; #define SensorNum 5 // Change number if you have less sensors, just be aware that the digital pins count down, so you may need to move the starting pin number up from pin 6 #define DEBUG 0 // Change to one if you want additional debugging information printed to the serial out. #define SFX 0 // Set to 0 if you don't want to hear the set-up pips when the ToF sensors are being configured #define Wait_For_Read 1 // Determines if the Arduino waits for a bit to be sent by Jetson before sending sensor data VL53L1X sensors[SensorNum]; Adafruit_AMG88xx amg; float pixels[AMG88xx_PIXEL_ARRAY_SIZE]; // Pixel array size is 64, as it is an 8x8 thermopile array int M_pixels[8][8]; /* Change these values below to alter what the bands the temperature is classified as * these values are output into a 8x8 array with numbers of 0 to 5, with temperatures below the lower threshold being 0 * this is done because its a lot easier to read at a glance a small array of numbers and it's easy to visualise where the heat is compared to the 8x8 of floating numbers * the values are in degrees celsius */ float LowerThresh = 25.0; float LowerMidThresh = 27.5; float MidThresh = 30.0; float UpperMidThresh = 32.5; float UpperThresh = 35.0; int speakerpin = 10; // digital pin for the piezo to output small pips for user convinience, if SFX is diabled this pin is not used and can be reassigned. void setup() { Serial.begin (115200); if (SFX){ pinMode(speakerpin, OUTPUT); PlayTone(5, 2, 250); // Plays a small pip to let user know arduino is running, plays 5 rapid pips } Wire.begin(); delay(500); Serial.println(\"Setting up sensors\"); Serial.println(\"Beginning VL53L1X ToF sensor set-up\"); ToF_Setup(); Serial.println(\"Beginning AMG8833 Thermal sensor set-up\"); Thermal_Setup(); Serial.println(\"Sensors initialised\"); Serial.println (\"Scanning I2C addresses\"); // Outputs address to serial, addressess 0x28, 0x2A, 0x2C, 0x2E, 0x30, and 0x69 should be seen int count = 0; for (int i = 1; i &lt; 120; i++) { Wire.beginTransmission (i); if (Wire.endTransmission () == 0) { Serial.print (\"Found address: \"); Serial.print (i, DEC); Serial.print (\" (0x\"); Serial.print (i, HEX); Serial.println (\")\"); count++; delay (1); } } Serial.print (\"Found \"); Serial.print (count, DEC); Serial.println (\" device(s).\"); byte rdy = 9; Serial.println(rdy); } void loop() { // put your main code here, to run repeatedly: byte output = 0; if (Wait_For_Read){ if (Serial.available()){ output = Serial.read(); } } else output = 49; // Serial.println(output); if(output == 49){ ToF_Read(); } if(output == 50){ Thermal_Read(); } if (DEBUG) { ToF_Read_Debug(); Thermal_Read_Debug(); delay(1000); // delay to allow reading in arduino serial monitor } Serial.flush(); delay(100); } void ToF_Setup(){ int address = 0x28; // first address that the first sensor will be set to for (int i = 6; i &gt; 1; i--){ // sets up pins 6 to 2, for the XSHUT pin on VL53L1X to allow for address change pinMode(i, OUTPUT); digitalWrite(i, LOW); delay(100); } Wire.begin(); for (int j = 0; j &lt; SensorNum; j++){ if (DEBUG){ Serial.print(\"Pin: \"); Serial.println(6 - j); Serial.print(\"Current address: \"); Serial.println(address, HEX); } if (SFX) PlayTone(j+1, 6, 500); // plays pips according to which sensor is being set-up, 1 pip for sensor 1, 2 pips for sensor 2, etc.. pinMode(6 - j, INPUT); delay(150); sensors[j].init(true); delay(100); sensors[j].setAddress(address); Serial.print(\"Sensor: \"); Serial.print(j+1); Serial.println(\" address set.\"); address += 2; delay(200); sensors[j].setDistanceMode(VL53L1X::Long); sensors[j].setMeasurementTimingBudget(50000); sensors[j].startContinuous(50); sensors[j].setTimeout(100); } delay(150); Serial.println(\"ToF's initialised\"); } void Thermal_Setup(){ Serial.println(F(\"AMG88xx pixels\")); Serial.println(AMG88xx_PIXEL_ARRAY_SIZE); bool status; // default settings status = amg.begin(); if (!status) { Serial.println(\"Could not find a valid AMG88xx sensor, check wiring!\"); while (1); } Serial.println(\"Thermal sensor initialised\"); Serial.println(); delay(100); // let sensor boot up } void ToF_Read(){ for (int i = 0; i &lt; SensorNum; i++){ if(i == (SensorNum-1)){ Serial.println(sensors[i].read()/1000.0, 4); // converts mm reading to meter, 4 signicant figures } else{ Serial.print(sensors[i].read()/1000.0, 4); // converts mm reading to meter, 4 signicant figures Serial.print(\",\"); } if (sensors[i].timeoutOccurred()) { Serial.print(\"8000\"); Serial.print(\",\"); } } } void Thermal_Read(){ amg.readPixels(pixels); for (int i = 1; i &lt;= AMG88xx_PIXEL_ARRAY_SIZE; i++) { if(i == (AMG88xx_PIXEL_ARRAY_SIZE)){ Serial.println(pixels[i - 1]); } else{ Serial.print(pixels[i - 1]); } if (i &lt; AMG88xx_PIXEL_ARRAY_SIZE) Serial.print(\",\"); } } void PlayTone(int repetition, int duration, int hold){ for (int j = 0; j &lt; repetition; j++){ for (long i = 0; i &lt; duration * 1000 ; i += 600){ digitalWrite(speakerpin, HIGH); delayMicroseconds(1915); digitalWrite(speakerpin, LOW); delayMicroseconds(1915); } delay(hold); } } void check_pixels() { int row; int col; int val; // clear all previous pixels for next refresh for (int j = 0; j &lt; 8; j++) { for (int h = 0; h &lt; 8; h++) { M_pixels[j][h] = 0; } } // if a pixel is above the temp threshold set to high for (int i = 0; i &lt; AMG88xx_PIXEL_ARRAY_SIZE; i++) { row = round(i / 8); if (i % 8 == 0) { col = 0; } else { col = i % 8; } if (DEBUG) { // Serial.print(row); // Serial.print(','); // Serial.println(col); } if (pixels[i] &gt;= UpperThresh) { val = 5; } else if (pixels[i] &gt;= UpperMidThresh) { val = 4; } else if (pixels[i] &gt;= MidThresh) { val = 3; } else if (pixels[i] &gt;= LowerMidThresh) { val = 2; } else if (pixels[i] &gt;= LowerThresh) { val = 1; } else { val = 0; } if (DEBUG) { Serial.print(i); Serial.print(','); Serial.print(pixels[i]); Serial.print(','); Serial.println(val); } M_pixels[row][col] = val; } if (DEBUG) { pixels_debug();} //This will print out all the pixels that should be turned on to //movesensor(); } void pixels_debug() { for (int i = 0; i &lt; 8; i++) { for (int j = 0; j &lt; 8; j++) { Serial.print(M_pixels[i][j]); Serial.print(','); } Serial.println(' '); } } void ToF_Read_Debug(){ for (int i = 0; i &lt; SensorNum; i++){ Serial.print(\"Sensor \"); Serial.print(i+1); Serial.print(\": \"); Serial.print(sensors[i].read()); if (sensors[i].timeoutOccurred()) { Serial.print(\" TIMEOUT\"); } Serial.println(); } } void Thermal_Read_Debug(){ amg.readPixels(pixels); Serial.print(\"[\"); for (int i = 1; i &lt;= AMG88xx_PIXEL_ARRAY_SIZE; i++) { Serial.print(pixels[i - 1]); Serial.print(\", \"); if ( i % 8 == 0 ) Serial.println(); } Serial.println(\"]\"); Serial.println(); if (DEBUG) check_pixels(); } The code for the Jetson is: H file: #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;string.h&gt; #include &lt;unistd.h&gt; #include &lt;stdint.h&gt; #include &lt;fcntl.h&gt; #include &lt;termios.h&gt; #include &lt;errno.h&gt; #include &lt;sys/ioctl.h&gt; #include &lt;stdbool.h&gt; #define BUFFER_SIZE 1024 #define DEBUG 1 // Adapted from Canonical Arduino read by Chris Heydrick - // https://github.com/cheydrick/Canonical-Arduino-Read/blob/master/canonicalarduinoread.c int init(); bool get_tof(int fd); bool get_thermal(int fd); bool read_data(int fd); bool chk_rdy(int fd); C++ File: #include \"SensorSuite.h\" int main(int argc, char *argv[]){ int fd; bool rdy = 0; fd = init(); while (!rdy) { rdy = chk_rdy(fd); } rdy = get_tof(fd); while (!rdy) { rdy = read_data(fd); } rdy = get_thermal(fd); while (!rdy) { rdy = read_data(fd); } close(fd); } int init(){ int fd; struct termios toptions; /* open serial port */ fd = open(\"/dev/ttyACM0\", O_RDWR | O_NOCTTY); printf(\"fd opened as %i\\n\", fd); /* wait for the Arduino to reboot */ usleep(3500000); /* get current serial port settings */ tcgetattr(fd, &amp;toptions); /* set 9600 baud both ways */ cfsetispeed(&amp;toptions, B115200); cfsetospeed(&amp;toptions, B115200); /* 8 bits, no parity, no stop bits */ toptions.c_cflag &amp;= ~PARENB; toptions.c_cflag &amp;= ~CSTOPB; toptions.c_cflag &amp;= ~CSIZE; toptions.c_cflag |= CS8; /* Canonical mode */ toptions.c_lflag |= ICANON; /* commit the serial port settings */ tcsetattr(fd, TCSANOW, &amp;toptions); return fd; } bool get_tof(int fd){ if(DEBUG) printf(\"\\nSending 1 for ToF Data\\n\"); write(fd, \"1\", 1); tcdrain(fd); usleep(2000000); return 0; } bool get_thermal(int fd){ if(DEBUG) printf(\"\\nSending 2 for Thermal Data\\n\"); write(fd, \"2\", 1); tcdrain(fd); usleep(2000000); return 0; } bool read_data(int fd){ int n; char buf[BUFFER_SIZE] = \"temp text\"; if(DEBUG) printf(\"\\nReading data\\n\"); n = read(fd, buf, BUFFER_SIZE); buf[n] = 0; // if(n!=1) if (n &lt; 35) return 0; else { if (DEBUG) printf(\"%i bytes read, buffer contains: %s\\n\", n, buf); return 1; } } bool chk_rdy(int fd){ int n; char buf[BUFFER_SIZE] = \"temp text\"; n = read(fd, buf, BUFFER_SIZE); buf[n] = 0; if ((buf[0] == '9') &amp;&amp; (n == 2)) return 1; else return 0; } I've managed to solve the empty return message on Jetson by adding in a if statement (c++ code amended) to not print anything until the expected number of bytes is returned but this is hit and miss as now sometimes I get the thermal data when I should get the tof data and vice versa or I'll just get two of one",
        "answers": [
            [
                "In read_data(), you cannot expect that you get all the data with one n = read(fd, buf, BUFFER_SIZE) call. Apparently the first read call yields only the first data byte, so you have to continue reading and appending data into buf until all data arrived. (Of course for this you either have to know how many bytes are sent or how the end can be identified.)"
            ],
            [
                "Following on from Armali, I managed to get it working by working out how many bytes I was was sending, luckily they were constant throughout, I then simply kept the the Jetson reading until the correct amount of bytes had arrived, using the serial output I recognized that the Jetson actually went through a few cycles before the Arduino had sent all the data, I believe this was due to using serial prints in the Arduino code."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I'm trying to run a video processing code on NVIDIA TX2 using moviepy. The code is: clip = VideoFileClip(video_file) video_clip = clip.fl_image(process_vid) video_clip.write_videofile(output_vid2) I get the error in the first line. The full error is: Traceback (most recent call last): File \"img_test.py\", line 117, in &lt;module&gt; clip = VideoFileClip(video_file) File \"/home/nvidia/.local/lib/python3.5/site-packages/moviepy/video/io/VideoFileClip.py\", line 91, in __init__ fps_source=fps_source) File \"/home/nvidia/.local/lib/python3.5/site-packages/moviepy/video/io/ffmpeg_reader.py\", line 33, in __init__ fps_source) File \"/home/nvidia/.local/lib/python3.5/site-packages/moviepy/video/io/ffmpeg_reader.py\", line 256, in ffmpeg_parse_infos proc = sp.Popen(cmd, **popen_params) File \"/usr/lib/python3.5/subprocess.py\", line 947, in __init__ restore_signals, start_new_session) File \"/usr/lib/python3.5/subprocess.py\", line 1551, in _execute_child raise child_exception_type(errno_num, err_msg) OSError: [Errno 8] Exec format error I even used the refernce of this but nothing seems to work. Any suggestions?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to install Parity on my NVIDIA Xavier Jetson, but keep getting stuck. I'm following the instructions here: https://github.com/paritytech/parity-snappy/wiki/Docker-build-for-ARM-ARM64 However I keep getting an error message: standard_init_linux.go:207: exec user process caused \"exec format error\" The full error message is below, however the above error code has happened with a few different install method attempts (inc. snapcraft). Sending build context to Docker daemon 6.468MB Step 1/10 : FROM ubuntu:14.04.5 ---&gt; 132b7427a3b4 Step 2/10 : WORKDIR /build ---&gt; Using cache ---&gt; e3ab6318dc67 Step 3/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --force-yes --no-install-recommends curl git make g++ gcc-aarch64-linux-gnu g++-aarch64-linux-gnu libc6-arm64-cross libc6-dev-arm64-cross wget file ca-certificates binutils-aarch64-linux-gnu &amp;&amp; apt-get clean ---&gt; Running in b12bf8ce43fd standard_init_linux.go:207: exec user process caused \"exec format error\" The command '/bin/sh -c apt-get -y update &amp;&amp; apt-get install -y --force-yes --no-install-recommends curl git make g++ gcc-aarch64-linux-gnu g++-aarch64-linux-gnu libc6-arm64-cross libc6-dev-arm64-cross wget file ca-certificates binutils-aarch64-linux-gnu &amp;&amp; apt-get clean' returned a non-zero code: 1 I am running Docker for linux/arm64 on Ubuntu 18.04 (install with Jetapack 4.1). For the life of me I just can't figure this out. Any ideas? Thanks.",
        "answers": [
            [
                "@kamil_cuk answered the question in the comments. The Dockerfile I was using was an old version. Latest version is here: https://github.com/paritytech/parity-snappy/blob/master/Dockerfile"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a Cuda application that was built with Cuda Toolkit 9.0 and running fine on Jetson TX2 board. I now have a Jetson Xavier board, flashed with Jetpack 4 that installs Cuda Toolkit 10.0 (only 10.0 is available). What do I need to do if I want to run the same application on Xavier? Nvidia documentation suggests that as long as I specify the correct target hardware when running nvcc, I should be able to run on future hardwares thanks to JIT compilation. But does this hold for different versions of Cuda toolkit (9 vs 10)?",
        "answers": [
            [
                "In theory (and note I don't have access to a Xavier board to test anything), you should be able to run a cross compiled CUDA 9 application (and that might mean both ARM and GPU architecture settings) on a CUDA 10 host. What you will need to make sure is that you either statically link or copy all the CUDA runtime API library components you require with your application on the Xavier board. Note that there is still an outside chance that those libraries might lack the necessary GPU and ARM features to run correctly on a Xavier system, or more subtle issues like libC incompatibility. That you will have to test for yourself."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "When I try to open a webcam (FLIR Boson) with OpenCV on a Jetson TX2 it gives the following error: libv4l2: error set_fmt gave us a different result then try_fmt! VIDEOIO ERROR: libv4l unable convert to requested pixfmt I am using this python script: import numpy as np import cv2 cap = cv2.VideoCapture(0) while(True): # Capture frame-by-frame ret, frame = cap.read() # Our operations on the frame come here # Display the resulting frame cv2.imshow('frame',frame) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break # When everything done, release the capture cap.release() cv2.destroyAllWindows() Although it does display the video it shows those errors. The reason that is relevant is I am trying to get the FLIR Boson to work with a Jetson TX2 running this program https://github.com/naisy/realtime_object_detection I have it working with a regular webcam but with the FLIR Boson it gives libv4l2: error set_fmt gave us a different result then try_fmt! VIDEOIO ERROR: libv4l unable convert to requested pixfmt VIDEOIO ERROR: V4L: Initial Capture Error: Unable to load initial memory buffers. Segmentation fault (core dumped) the above error and closes. In my research on the error, it seems to come up with people who use webcams that are monochrome, looking at this https://www.flir.com/support-center/oem/is-there-a-way-to-maximize-the-video-display-on-the-boson-app-for-windows-pc-to-full-screen/ I am wondering if I need to configure OpenCV or the V4L2 driver to choose the right format for the webcam to prevent the errors. I also have a Jetson Xavier and the same object detection program works on it (it just has a different build of OpenCV and Tensorflow), so I am guessing that there is a slightly different configuration related to webcam format compatibility on that OpenCV install on the Xavier VS the TX2. I am new to all of this so forgive me if I ask for more clarification. One last bit of info, this is out of the FLIR Boson manuel related to USB: 8.2.2 USB Boson is capable of providing digital data as a USB Video Class (UVC) compliant device. Two output options are provided. Note the options are not selected via the CCI but rather by the video capture or viewing software selected by the user. The options are: \u25a0 Pre-AGC (16-bit): The output is linearly proportional to the flux incident on each pixel in the array; output resolution is 320x256 for the 320 configuration, 640x512 for the 640 configuration. Note that AGC settings, zoom settings, and color-encoding settings have no effect on the output signal at this tap point. This option is identified with a UVC video format 4CC code of \u201cY16 \u201d (16-bit uncompressed greyscale image) \u25a0 Post-Colorize, YCbCrb: The output is transformed to YCbCr color space using the specified color palette (see Section 6.7). Resolution is 640x512 for both the 320 and 640 configurations. Three options are provided, identified via the UVC video format 4CC code: \u2022 I420: 8 bit Y plane followed by 8 bit 2x2 subsampled U and V planes \u2022 NV12: 8-bit Y plane followed by an interleaved U/V plane with 2x2 subsampling \u2022 NV21: same as NV12 except reverse order of U and V planes I have tried reinstalled everything several times, although it takes a few hours to reflash the TX2 and re-install open CV and Tensorflow. I have tried two different \"builds\" of opencv. I have tried to view the webcam with cheese and have never had a problem.",
        "answers": [
            [
                "I don't work with Python but you need disable conversion to RGB: cap.set(cv.CAP_PROP_CONVERT_RGB, 0) See you v4l example from OpenCV."
            ],
            [
                "I was able to find a way to get it to work, using the below code worked. It seemed to be a problem with open CV interacting with the v4l2. pipeline = \"v4l2src device=/dev/video1 ! video/x-raw,width=640,height=512,format=(string)I420,pixel-aspect-ratio=1/1, interlace-mode=(string)progressive, framerate=30/1 ! videoconvert ! appsink\" cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER) https://github.com/FLIR/BosonUSB/issues/13"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "Looking for an entry-level camera module for the Jetson Xavier Hi, I have been looking into the Jetson Xavier Development Board which meets my requirements for size and performance. As part of my build I need to add many camera modules, ideally 8-12. Posting on the Nvidia forum I am told to contact their 'preferred suppliers' who mostly specialise in industry ready cameras retailing at circa \u00a3300 per unit (I want 12 of them... per unit... that's expensive). My requirements are for object detection and as such I only need a basic standard of image. The Xavier documentation specifies that their preferred camera connection is CSI-2. My question is: \"If anyone has experience with Xavier cameras will any CSI-2 camera suffice or do I need to find one with compatible drivers?\" Thank you",
        "answers": [
            [
                "Kindly look into the below products https://www.e-consystems.com/nvidia-jetson-camera.asp#xavier-cameras https://www.e-consystems.com/nvidia-cameras/jetson-agx-xavier-cameras/four-synchronized-4k-cameras.asp In Xavier, you can connect maximum 6 cameras through CSI-2 MIPI interface. And you can add additional USB3.0 cameras in USB interface. By this way you can match your requirement to connect multiple cameras to Jetson Xavier."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to know morw about the Nvidia gpu for the multi job running same time. There is one job CNN object localization/detection using gpu , and another one object detection . Both running same time and requires gpu power. How the Nvidia JetsonTx2 and other 2080Txi for example sharing their resources ? Is it possibble or we can only one gpu worker same time?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm attempting to compile (and package) OpenCV 3.4.1 on a Jetson TX2 so that it will run on both a TX1 (architecture #53) and TX2 (architecture #62). I'm building on a TX2 running Ubuntu 16.04. Try as I might, I can only get it to compile for the TX2. I've tried: export CUDA_ARCH_BIN=\"53, 62\" and export CUDA_ARCH_BIN=\"53 62\" before doing the cmake command, but the configuration only gives: NVIDIA GPU arch: 62 I'm sure there is a variable that can be set for this, but I have not been able to find it.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am writing web app, where I would like to: Display LOCAL stream from webcam - it means, that I want to stream video from server (I do not want to open webcam of client) Read QR Codes and list them in text box These two were already achieved but! I came across some unexpected behaviour. Functionality that I have described is working perfectly, but only on localhost. I want to deploy it, so it could be accessible via different computer (it is meant to be used on a robot). So to describe my architecture: I am using Jetson TX2 as a server (webcam is connected here). I am using Django web framework, django-channels, daphne as a web server and ngingx as a proxy. I am running daphne and background process in supervisor. I am using worker (background process) to capture frames from webcam and send it via redis to the web backend. So when I run it on localhost everything work as expected. When I set Debug to FALSE and I add Jetson's IP to ALLOWED_HOSTS and try to access the web from different computer this happens: I can see, that webcam is accessed because the webcam light turns on. I put some QR code in front of the webcam and the code appears in the textbox on web! BUT the video is not there (when ALLOWED_HOSTS contains localhost video IS there). Output of background process which collects the camera frames gives following error: libv4l2: error setting pixformat: Device or resource busy OpenCV Error: Unspecified error (GStreamer: unable to start pipeline) in cvCaptureFromCAM_GStreamer, file /home/nvidia/prototype/opencv/opencv-3.4.0/modules/videoio/$ VIDEOIO(cvCreateCapture_GStreamer (CV_CAP_GSTREAMER_FILE, filename)):raised OpenCV exception: /home/nvidia/toyota_prototype/opencv/opencv-3.4.0/modules/videoio /src/cap_gstreamer.cpp:890: error: (-2)$ in function cvCaptureFromCAM_GStreamer I will not post whole code here, since I do not know where exactly is the problem. Does anyone have an idea where the problem could be? Thank you for your help!",
        "answers": [
            [
                "So, I figured it out. In my html template I had one line, where I was linking to the stream address: &lt;img src=\"http://127.0.0.1:8000/webcam-stream\"&gt; I think, now you all know, where the problem was. I needed to change the IP to HOST address."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "We are currently developing an application for an ARM embedded system that is running a full Linux OS (NVIDIA Jetson TX2). I have a copy of the rootfs from the embedded system as well as the GNU toolchain for ARM version 8.2-2018.08. I am having difficulty getting the GCC compiler (gcc-arm-8.2-2018.11-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc) to look in the correct directory of the rootfs for both include and library files. I have set the \u2014sysroot variable to point to the location of my sysroot. However, looking in the lib and include directories there is a subfolder that usr/lib/aarch64-linux-gnu that contains further include and libraries and it is these ones that are actually the ones needed. For example instead of the libraries being located in $rootfs/usr/lib or $rootfs/usr/local/lib, instead, the libraries are in $rootfs/usr/lib/aarch64-linux-gnu I have tested and using the -I/-L commands I can get the project o compile. My questions are as follows Is using the -I and -L the \"correct\" way to be doing this? If not, what is? Why does linux place these libraries in this other directory? When compiling using --sysroot I can see that other non-sysroot directories are being checked. Why would gcc be doing this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I think the IP address and port number are correct checked by using ifconfig at server side, and the port is not bind by others at server side. So, I guess the problem is at client side. I have attached the running results at client side and netstat result on server side (PC). They are all under the same wifi. Server IP is 10.42.0.197 and port # is 14450, the client ip is 10.42.0.1. The client side (TX2 board) still cannot bind the ip and port. .",
        "answers": [
            [
                "Try to check the interface associated with the IP you wanted to bind on client side. If you did not find any related IP then there is no associated connection with this IP. Resolve the connection issue and retry. Use netstat or ifconfig or ipconfig to check interfaces. This command vary per OS, check for the one that supports."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am new to Jetson tegra x2 board. I have a plan to run my tensorflow-gpu models on TX2 board and see how they perform there. These models are trained and tested on GTX GPU machine. On tx2 board, Jetpack full does not have tensorflow in it. So tensorflow needs to be built/installed which I have seen several tutorials on and tried. My python files train.py and test.py expect tensorflow-gpu. Now I suspect, if tensorflow-gpu buiding on tx2 board is the right way to go? Oh, there is Nvidia TensorRT on TX2, that will do part of the job, but how? and is that right? Will tensorflow and tensorRT work together to replace tensorflow-gpu? but how? then what modifications will i have to make in my train and test python files? Do I really need to build tensorflow for tx2 at all? I only need inference I don't want to do training there. I have studied different blogs and tried a several options but now things are bit messed up. My simple question is: What are steps to get inference done on Jetson TX2 board by using TensorFlow-GPU deep learning models trained on GTX machine?",
        "answers": [
            [
                "The easiest way is to install the NVIDIA provided wheel: https://docs.nvidia.com/deeplearning/dgx/install-tf-jetsontx2/index.html All the dependencies are already installed by JetPack. After you install Tensorflow using the wheel, you can use it however you use Tensorflow on other platforms. For running inference, you can download a Tensorflow model into TX2 memory, and run your Tensorflow inference scripts on them. You can also optimize your Tensorflow models by passing them through TF-TRT: https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html There is just one API call that does the optimization: create_inference_graph(...) This will optimize the Tensorflow graph (by mostly fusing nodes), and also let you build the models for lower precision to get better speedup."
            ],
            [
                "I built tensorflow on JetsonTX2 following this guide. It provides instructions and wheels for both Python 2 and Python3. https://github.com/jetsonhacks/installTensorFlowTX2 If you are new to Jetson TX2, also take a look at this \"Guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson\". (*This does not require tensorflow installation since Jetpack already builds TensorRT) https://github.com/dusty-nv/jetson-inference#building-from-source-on-jetson If you have tensorflow trained graphs that you want to run inference on Jetson then you need to first install tensorflow. Afterwards, it is recommended (not compulsory for inference) that you optimize your trained models with tensorRT.Check out these repos for object detection/classification examples that uses TensorRT optimization. https://github.com/NVIDIA-AI-IOT/tf_trt_models https://github.com/NVIDIA-AI-IOT/tf_to_trt_image_classification"
            ],
            [
                "You can find the tensorflow-gpu wheel files of TX2 for both python 2.7 and python 3.5 in this link of Nvidia's Developer Forum. https://devtalk.nvidia.com/default/topic/1031300/jetson-tx2/tensorflow-1-8-wheel-with-jetpack-3-2-/"
            ]
        ],
        "votes": [
            2.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I'm parsing MobileNet-SSD caffe Model from https://github.com/chuanqi305/MobileNet-SSD using TensorRT caffe parser. I use Jetpack 3.3 in Jetson TX2 as the platform. How to fix this error from nvinfer library? There is even a TensorRT class nvinfer1::plugin::PriorBoxParameters in the API. But this layer handle seems to be not defined. Is there any error in the layer param names and format? Or do we have to create a custom layer plugin for this one. Does someone have a implementation for this? The error is as follows, [libprotobuf ERROR google/protobuf/text_format.cc:298] Error parsing text-format ditcaffe.NetParameter: 1245:18: Message type \"ditcaffe.LayerParameter\" has no field named \"prior_box_param\". ERROR: CaffeParser: Could not parse deploy file The source code line relevant for this error, m_network = m_builder-&gt;createNetwork(); m_parser = createCaffeParser(); const IBlobNameToTensor* blobNameToTensor = m_parser-&gt;parse(deployFpath, modelFpath, *network, DataType::kFLOAT); Error given layer in prototxt file, layer { name: \"conv11_mbox_priorbox\" type: \"PriorBox\" bottom: \"conv11\" bottom: \"data\" top: \"conv11_mbox_priorbox\" prior_box_param { min_size: 60.0 aspect_ratio: 2.0 flip: true clip: false variance: 0.1 variance: 0.1 variance: 0.2 variance: 0.2 offset: 0.5 } }",
        "answers": [
            [
                "You have to remove all the blocks that have _param from the ptototxt file, then you should removed layers using the plugin API. Visit the discussion on this link for clear understanding: https://github.com/chenzhi1992/TensorRT-SSD/issues/5"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to use multiple OpenCV windows in python on my Jetson TX2. However, I get the error: \"ASSERT: \"false\" in file qasciikey.cpp, line 495\" Below is the minimum reproducible code: import cv2 import numpy as np img1=np.random.randn(300,400) img2=np.random.randn(600,400) cv2.imshow('win1', img1) cv2.imshow('win2', img2) And here is the error trace: Could not initialize OpenGL for RasterGLSurface, reverting to RasterSurface. QXcbConnection: XCB error: 145 (Unknown), sequence: 164, resource id: 0, major code: 139 (Unknown), minor code: 20 Could not initialize OpenGL for RasterGLSurface, reverting to RasterSurface. ASSERT: \"false\" in file qasciikey.cpp, line 495 Aborted (core dumped) Please help. Thanks! EDIT: The error only happens when I ssh into the Jetson (via MobaXterm v10.5). If I run the code directly from the Jetson, I do not get this error.",
        "answers": [
            [
                "I found that un-checking \"Unix-compatible keyboard\" in MobaXterm/Settings/X11, with keyboard set to \"us\" solved this error for me."
            ],
            [
                "I had a similar error message (ASSERT: \"false\" in file qasciikey.cpp, line 495) when running code on Raspberry Pi. I was able to make this problem go away by using cv2.namedWindow() first to create a window where images should be displayed. You should try this solution and see if the problem goes away: import cv2 import numpy as np img1=np.random.randn(300,400) img2=np.random.randn(600,400) cv2.namedWindow('win1') cv2.namedWindow('win2') cv2.imshow('win1', img1) cv2.imshow('win2', img2)"
            ],
            [
                "I dont know what is the reason for this, but it happens to me when I run OpenCV with CUDA and my ubuntu latptop is running on Batteries As soon as I connect it to the power cord, this stops happening."
            ]
        ],
        "votes": [
            3.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I have trained a convolutional neural network using Keras 2.2.4 on Nvidia Quadro board. I have saved the trained model in tow separate files: one file (model.json) that describes the architecture and another file (model.h5) that has all the weights. I want to load the saved model on the Nvidia Jetson TX2 board that runs Keras 2.2.2 and I'm trying to do it as follows: # load json and create model json_file = open(prefix+'final_model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) # load weights into new model loaded_model.load_weights(prefix+\"model.h5\") model = loaded_model However, when I tried to load I got the following error: loaded_model = model_from_json(loaded_model_json) File \"/home/nvidia/.local/lib/python3.5/site-packages/keras/engine/saving.py\", line 368, in model_from_json return deserialize(config, custom_objects=custom_objects) File \"/home/nvidia/.local/lib/python3.5/site-packages/keras/layers/init.py\", line 55, in deserialize printable_module_name='layer') File \"/home/nvidia/.local/lib/python3.5/site-packages/keras/utils/generic_utils.py\", line 145, in deserialize_keras_object list(custom_objects.items()))) File \"/home/nvidia/.local/lib/python3.5/site-packages/keras/engine/sequential.py\", line 292, in from_config custom_objects=custom_objects) File \"/home/nvidia/.local/lib/python3.5/site-packages/keras/layers/init.py\", line 55, in deserialize printable_module_name='layer') File \"/home/nvidia/.local/lib/python3.5/site-packages/keras/utils/generic_utils.py\", line 165, in deserialize_keras_object ':' + function_name) ValueError: Unknown layer:name I've also tried to save the whole model in one file, but got the same error. I have tried the solution from here but wasn't able to solve it. Has anyone seen this error before? Any suggestions?",
        "answers": [
            [
                "I had the same problem yesterday, I just updated keras through conda and everything worked perfectly."
            ],
            [
                "It turned out to be different tensorflow versions for me. The model was created with tensorflow v2.1.0 and I was trying to load it with tensorflow v1.10.0. Just make sure that tensorflow versions are consistent."
            ],
            [
                "just update the keras package to recent one using following command conda update keras"
            ]
        ],
        "votes": [
            4.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am trying to run OpenMPI code on a NVIDIA Jetson TX2. But I am getting an OPAL Error when i run mpiexec. Compilation instruction: $ nvcc -I/home/user/.openmpi/include/ -L/home/user/.openmpi/lib/ -lmpi -std=c++11 *.cu *.cpp -o program nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning). Execution error message: $ mpiexec -np 4 ./program [user:05728] OPAL ERROR: Not initialized in file pmix2x_client.c at line 109 *** An error occurred in MPI_Init *** on a NULL communicator *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort, *** and potentially your MPI job) [user:05728] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed! [user:05729] OPAL ERROR: Not initialized in file pmix2x_client.c at line 109 ------------------------------------------------------- Primary job terminated normally, but 1 process returned a non-zero exit code.. Per user-direction, the job has been aborted. ------------------------------------------------------- *** An error occurred in MPI_Init *** on a NULL communicator *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort, *** and potentially your MPI job) [user:05729] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed! -------------------------------------------------------------------------- mpiexec detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was: Process name: [[7361,1],0] Exit code: 1 -------------------------------------------------------------------------- I installed OpenMPI version 3.1.2 using the following instructions: $ ./configure --prefix=\"/home/user/.openmpi\" --with-cuda $ make; sudo make install I have also setup my $PATH and my $LD_LIBRARY_PATH variables accordingly based on instructions from this link I am able to successfully execute the program on my laptop (Intel i7). Upon looking up the error I found some links suggesting that I reinstall OpenMPI. I have tried doing so multiple times (including a fresh download of the library) without any success. Any help would be greatly appreciated! Edits I tried running the following Minimal code (main.cpp) as asked in the comments: #include &lt;iostream&gt; #include \"mpi.h\" #include &lt;string&gt; int main(int argc, char *argv[]) { int rank, size; MPI_Init(&amp;argc, &amp;argv); MPI_Comm_size(MPI_COMM_WORLD, &amp;size); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); std::cout &lt;&lt; rank &lt;&lt; '\\n'; MPI_Finalize(); return 0; } To compile this, I reran the previous command and got the same error: $ nvcc -I/home/user/.openmpi/include/ -L/home/user/.openmpi/lib/ -lmpi -std=c++11 main.cpp -o program But then if i compile it with mpic++ it is able to run perfectly fine. $ mpic++ main.cpp -o ./program $ mpiexec -np 4 ./program 0 1 3 2",
        "answers": [
            [
                "Is this the only version of OpenMPI that you have installed? My guess is that you're using different MPI versions between your build and run. Check which mpirun and also search for instances of mpirun. If you're on Ubuntu do sudo updatedb locate mpirun If you call the correct mpirun (the same version used to build) then the error should go away."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I asked a similar question here: Embedded - GPIO Key does not register but I am tying to take a step back and focus on just one part of that question. My board has 3 gpio-keys already built in: power, volume up, and volume down. When calling cat /sys/kernel/debug/gpio they show up under GPIOs 256-319, platform/c2f0000.gpio, tegra-gpio-aon: as expected. When in my dts file I create a new gpio-key node, no matter what is in the node, the buttons no longer show up in /sys/kernel/degub/gpio but are lsited in /proc/device-tree/gpio-keys. If I call my node something different the buttons do not disappear, even if the new node is gpio-key compatible. I should be able to add properties to a node by inheriting from a base dtsi. Why does my additions to the gpio-key seem to disable the other keys?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to run the tutorial code from nvidia's repo here. Here's what happens with the console imagenet program on my Jetson TX2: nvidia@tegra-ubuntu:~/jetson-inference/build/aarch64/bin$ ./imagenet-console orange_0.pjg output_0.jpg imagenet-console args (3): 0 [./imagenet-console] 1 [orange_0.pjg] 2 [output_0.jpg] imageNet -- loading classification network model from: -- prototxt networks/googlenet.prototxt -- model networks/bvlc_googlenet.caffemodel -- class_labels networks/ilsvrc12_synset_words.txt -- input_blob 'data' -- output_blob 'prob' -- batch_size 2 [TRT] TensorRT version 4.0.2 [TRT] attempting to open cache file networks/bvlc_googlenet.caffemodel.2.tensorcache [TRT] cache file not found, profiling network model [TRT] platform has FP16 support. [TRT] loading networks/googlenet.prototxt networks/bvlc_googlenet.caffemodel Weights for layer conv1/7x7_s2 doesn't exist [TRT] CaffeParser: ERROR: Attempting to access NULL weights Weights for layer conv1/7x7_s2 doesn't exist [TRT] CaffeParser: ERROR: Attempting to access NULL weights [TRT] Parameter check failed at: ../builder/Network.cpp::addConvolution::40, condition: kernelWeights.values != NULL error parsing layer type Convolution index 1 [TRT] failed to parse caffe network failed to load networks/bvlc_googlenet.caffemodel failed to load networks/bvlc_googlenet.caffemodel imageNet -- failed to initialize. imagenet-console: failed to initialize imageNet I do not have Caffe installed on the Jetson board, as the tutorial specifically states that it is not needed. I'm not sure if the null weights error would be fixed if TRT would properly cache. Any ideas? Python 2.7 Cuda 9.0 TensorRT 4.0",
        "answers": [
            [
                "The corporate firewall was preventing the proper download of the models. Downloading the models manually and putting them in the networks folder solved the problem."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "The only way that I am able to accomplish opening a live camera stream on the Xavier is launching gstreamer from console gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=1024, height=768, framerate=120/1, format=NV12' ! nvvidconv flip-method=0 ! nvegltransform ! nveglglessink -e When i try any video capture command in python or c++ i am constantly getting errors about \"camera failed to open\" or \"video stream type error\" Ive tried this in opencv 4, 3.4, 3.3 to no avail. I do not think its that way my opencv build is configured but possibly a way that the xavier camera capture must be instanced. Any type of sample python implementation of live video capture using the tx2 dev-kit camera would be highly helpful? Thanks",
        "answers": [
            [
                "You need to access the Jetson camera through the gstreamer api. C++ example cv::VideoCapture capture(\"nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=1024, height=768, framerate=120/1, format=NV12' ! nvvidconv flip-method=0 ! appsink\"); or Python cap = cv2.VideoCapture('nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=1024, height=768, framerate=120/1, format=NV12' ! nvvidconv flip-method=0 ! appsink') However, you might be missing gstreamer from your OpenCv build. If this is not working you might need to rebuild your OpenCv with all gstreamer features enabled/dependencies solved."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying this tutorial but I had some compile error. Can anyone give me some advice, please? My environment is... Ubuntu 16.04 JetsonTX2 Architecture aarch64 JetPack 3.2 CUDA 9.0 cuDNN 7.0.5 I did the following command: $ sudo -H bash BuildTensorflow.sh -b r1.6 and the error is... swapon: /home/nvidia/JetsonTFBuild/swapfile.swap: swapon failed: Device or resource busy Looks like Swap not desired or is already in use dirname: missing operand Try 'dirname --help' for more information. dirname: missing operand Try 'dirname --help' for more information. PYTHON_BIN_PATH=/usr/bin/python2 GCC_HOST_COMPILER_PATH=/usr/bin/gcc CUDA_TOOLKIT_PATH= TF_CUDA_VERSION=9.0 TF_CUDA_COMPUTE_CAPABILITIES=5.3,6.2 CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu TF_CUDNN_VERSION=7.0.5 ./tf_build.sh: line 20: /home/nvidia/JetsonTFBuild: Is a directory /usr/bin/python2: can't open file 'configure.py': [Errno 2] No such file or directory error: patch failed: tensorflow/contrib/lite/kernels/internal/BUILD:21 error: tensorflow/contrib/lite/kernels/internal/BUILD: patch does not apply INFO: Options provided by the client: Inherited 'common' options: --isatty=1 --terminal_columns=80 INFO: Reading rc options for 'build' from /home/nvidia/JetsonTFBuild/tensorflow/tools/bazel.rc: 'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt ERROR: Config value opt is not defined in any .rc file Tue Oct 9 20:34:03 JST 2018 : === Using tmpdir: /tmp/tmp.swhwrsSoaA cp: cannot stat 'bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow': No such file or directory",
        "answers": [],
        "votes": []
    },
    {
        "question": "I built OpenCV 3.3.1 with OpenCV_Extra on Jetson TX2 like the following with CUDA disabled: cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr -DBUILD_PNG=OFF -DBUILD_TIFF=OFF -DBUILD_TBB=OFF -DBUILD_JPEG=OFF -DBUILD_JASPER=OFF -DBUILD_ZLIB=OFF -DBUILD_EXAMPLES=ON -DBUILD_opencv_java=OFF -DBUILD_opencv_python2=ON -DBUILD_opencv_python3=ON -DENABLE_PRECOMPILED_HEADERS=OFF -DWITH_CUDA=OFF -DWITH_OPENCL=OFF -DWITH_OPENMP=OFF -DWITH_FFMPEG=ON -DWITH_GSTREAMER=OFF -DWITH_GSTREAMER_0_10=OFF -DWITH_GTK=ON -DWITH_VTK=OFF -DWITH_TBB=ON -DWITH_1394=OFF -DWITH_OPENEXR=OFF -DINSTALL_C_EXAMPLES=ON -DINSTALL_TESTS=ON -DCPACK_GENERATOR_DEB=ON -DOPENCV_TEST_DATA_PATH=../opencv_extra/testdata ../opencv And ran OpenCV test script, and found two test failures on 'Calib3d_Affine3f.accuracy' and 'match_bestOf2Nearest.bestOf2Nearest' testing like the following: [opencv_test_calib3d] RUN : /usr/bin/opencv_test_calib3d --perf_min_samples=1 --perf_force_samples=1 --gtest_output=xml:opencv_test_calib3d.xml [opencv_test_calib3d] CTEST_FULL_OUTPUT [opencv_test_calib3d] OpenCV version: 3.3.1 [opencv_test_calib3d] OpenCV VCS version: 3.3.1 [opencv_test_calib3d] Build type: release [opencv_test_calib3d] Parallel framework: tbb [opencv_test_calib3d] CPU features: neon fp16 [opencv_test_calib3d] [ RUN ] Calib3d_Affine3f.accuracy [opencv_test_calib3d] /home/nvidia/build-opencv/opencv/modules/calib3d/test/test_affine3.cpp:57: Failure [opencv_test_calib3d] Expected: 0 [opencv_test_calib3d] To be equal to: cvtest::norm(cv::Mat(affine.matrix, false).colRange(0, 3).rowRange(0, 3) != expected, cv::NORM_L2) [opencv_test_calib3d] Which is: 441.673 [opencv_test_calib3d] [ FAILED ] Calib3d_Affine3f.accuracy (0 ms) [opencv_perf_stitching] RUN : /usr/bin/opencv_perf_stitching --perf_min_samples=1 --perf_force_samples=1 --gtest_output=xml:opencv_perf_stitching.xml [opencv_perf_stitching] Time compensation is 0 [opencv_perf_stitching] CTEST_FULL_OUTPUT [opencv_perf_stitching] OpenCV version: 3.3.1 [opencv_perf_stitching] OpenCV VCS version: 3.3.1 [opencv_perf_stitching] Build type: release [opencv_perf_stitching] Parallel framework: tbb [opencv_perf_stitching] CPU features: neon fp16 [opencv_perf_stitching] [----------] 1 test from match_bestOf2Nearest [opencv_perf_stitching] [ RUN ] match_bestOf2Nearest.bestOf2Nearest/0 [opencv_perf_stitching] Expected: [opencv_perf_stitching] [0.9970975582816909, 0.01136054503288174; [opencv_perf_stitching] -0.002557125266879237, 1.02781673911756; [opencv_perf_stitching] 0.0002463026627945361, -1.679576661348132e-05] [opencv_perf_stitching] Actual: [opencv_perf_stitching] [0.9986402872172184, -0.01796581492545124; [opencv_perf_stitching] -0.00230781842425846, 1.031312571169278; [opencv_perf_stitching] 0.0002375978666932171, 9.189439617496881e-05] [opencv_perf_stitching] /home/nvidia/build-opencv/opencv/modules/ts/src/ts_perf.cpp:571: Failure [opencv_perf_stitching] Failed [opencv_perf_stitching] Difference (=0.029326359958332979) between argument1 \"R\" and expected value is greater than 0.014999999999999999 [opencv_perf_stitching] params = \"orb\" [opencv_perf_stitching] termination reason: reached maximum number of iterations [opencv_perf_stitching] bytesIn = 96896 [opencv_perf_stitching] bytesOut = 0 [opencv_perf_stitching] samples = 1 [opencv_perf_stitching] outliers = 0 [opencv_perf_stitching] frequency = 1000000000 [opencv_perf_stitching] min = 306334005 = 306.33ms [opencv_perf_stitching] median = 306334005 = 306.33ms [opencv_perf_stitching] gmean = 306334005 = 306.33ms [opencv_perf_stitching] gstddev = 0.00000000 = 0.00ms for 97% dispersion interval [opencv_perf_stitching] mean = 306334005 = 306.33ms [opencv_perf_stitching] stddev = 0 = 0.00ms [opencv_perf_stitching] [ FAILED ] match_bestOf2Nearest.bestOf2Nearest/0, where GetParam() = \"orb\" (577 ms) [opencv_perf_stitching] [----------] 1 test from match_bestOf2Nearest (577 ms total) I tried both Ubuntu 16.04-based image and Ubuntu 18.04-based image with different GCC versions. And below is the summary of my findings: The OpenCV test failure on \u2018Calib3d_Affine3f.accuracy\u2019 was observed on different platforms w/ different images whenever the toolchain is based on gcc-7. And in any platform + image combination, this test always passes when gcc &lt; 7 (i.e., lower than version 7, like gcc-6.4.0) is used. The OpenCV test failure on \u2018match_bestOf2Nearest.bestOf2Nearest\u2019 was persistently observed whatever image and whatever version of gcc used. My observation is that the failure on \u2018Calib3d_Affine3f.accuracy\u2019 test is because the test script compares the expected and the actual values with an equality sign (i.e., ==), which is to compare the exact values, not assuming any variations in floating-point operations in different platform with different compilers. With gcc &lt; 7, the floating-point comparison test passes, but with gcc-7, it fails due to the two floating-point values are slightly different like 1e-17. Questions: Does the IEEE floating-point spec mandate across architectures regarding precision? Would the comparison method in OpenCV test script (i.e., exact value comparison instead of compare with a tolerable margin) be typical for floating-point values per the IEEE spec? Why would the floating-point precision behavior be different when using gcc-7 compared to gcc &lt; 7? How to make the test passed with gcc-7 and moving forward? The second failure case on \u2018match_bestOf2Nearest.bestOf2Nearest\u2019 is also related to the precision handling in floating-point operation in some of OpenCV functions. I found out that OpenCV functions sometimes cast the values between CV_32F and CV_64F internally. Assuming the CV_32F format is enough to handle the precision, this shouldn\u2019t matter. For this case, the test script compares the expected and the actual values with a tolerable margin like an epsilon (instead of exact value equality testing). The observation is that the test fails depending on the random seed provided in the functions, implying that the pre-defined epsilon is not large enough, or the floating-point operations on our CPU has variability larger than what is assumed in OpenCV test script. Question: Would there be anything related to the CPU architecture-specific characteristics on Jetson-TX2? What would that be? How to resolve this issue? It would be wonderful if you could share your wisdom with me!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying now to fix this bug for a few days and nothing is going forward.. I want to acces the camera of my Nvidia Jetson Tx2 with OpenCV and GStreamer. (java:7468): GStreamer-CRITICAL **:gst_element_get_state: assertion 'GST_IS_ELEMENT (element)' failed Videocapture cap = new VideoCapture(); cap.open(\"nvcamerasrc ! video/x-raw(memory:NVMM), width=(int)1280, height= (int)720, format=(string)I420, framerate=(fraction)120/1 ! nvvidconv flip- method=2 ! video/x-raw, format=(string)I420 ! videoconvert ! video/x-raw, format= (string)BGR ! appsink\"); cap.read(... This pipeline worked great, but as I started my Jetson a few days ago it gave me this error. Maybe because I updated something... I don't know Information Nvidia Jetson TX2 Ubuntu 16.04 OpenCV 3.4.2 Java 1.8 GStreamer + plugins installed Tried to rebuild OpenCV but nothing helped Does anyone know how to solve this problem?",
        "answers": [
            [
                "If you look at pipelines from deepstream samples, on jetson nano they have extra element in pipeline nvegltransform. Its really hard to understand from their documents what it is, but may be you should try it."
            ],
            [
                "Here is an example pipeline that worked for me: cv2.VideoCapture(( \"nvarguscamerasrc ! \" \"video/x-raw(memory:NVMM), \" \"width=(int)%d, height=(int)%d, \" \"format=(string)NV12, framerate=(fraction)%d/1 ! \" \"nvvidconv flip-method=%d ! \" \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \" \"videoconvert ! \" \"video/x-raw, format=(string)BGR ! appsink \" \"wait-on-eos=false drop=true max-buffers=1\" % ( capture_width, capture_height, framerate, flip_method, display_width, display_height, ) ), cv2.CAP_GSTREAMER)"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "EDIT2 Ok so far i have tried with python3.5 -tf 1.10 and python 2.7 tf 1.10 I m still getting this error Traceback (most recent call last): File \"object_detection/model_main.py\", line 101, in &lt;module&gt; tf.app.run() File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run _sys.exit(main(argv)) File \"object_detection/model_main.py\", line 97, in main tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 455, in train_and_evaluate return executor.run() File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 594, in run return self.run_local() File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 695, in run_local saving_listeners=saving_listeners) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 354, in train loss = self._train_model(input_fn, hooks, saving_listeners) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1179, in _train_model return self._train_model_default(input_fn, hooks, saving_listeners) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1209, in _train_model_default features, labels, model_fn_lib.ModeKeys.TRAIN, self.config) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1167, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File \"/home/nvidia/tensorflow/models/research/object_detection/model_lib.py\", line 287, in model_fn prediction_dict, features[fields.InputDataFields.true_image_shape]) File \"/home/nvidia/tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py\", line 686, in loss keypoints, weights) File \"/home/nvidia/tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py\", line 859, in _assign_targets groundtruth_weights_list) File \"/home/nvidia/tensorflow/models/research/object_detection/core/target_assigner.py\", line 481, in batch_assign_targets anchors, gt_boxes, gt_class_targets, unmatched_class_label, gt_weights) File \"/home/nvidia/tensorflow/models/research/object_detection/core/target_assigner.py\", line 180, in assign match = self._matcher.match(match_quality_matrix, **params) File \"/home/nvidia/tensorflow/models/research/object_detection/core/matcher.py\", line 239, in match return Match(self._match(similarity_matrix, **params), File \"/home/nvidia/tensorflow/models/research/object_detection/matchers/argmax_matcher.py\", line 190, in _match _match_when_rows_are_non_empty, _match_when_rows_are_empty) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func return func(*args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2074, in cond orig_res_t, res_t = context_t.BuildCondBranch(true_fn) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1920, in BuildCondBranch original_result = fn() File \"/home/nvidia/tensorflow/models/research/object_detection/matchers/argmax_matcher.py\", line 153, in _match_when_rows_are_non_empty -1) File \"/home/nvidia/tensorflow/models/research/object_detection/matchers/argmax_matcher.py\", line 203, in _set_values_using_indicator indicator = tf.cast(1-indicator, x.dtype) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 878, in r_binary_op_wrapper x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name=\"x\") File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1028, in convert_to_tensor as_ref=False) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1124, in internal_convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 228, in _constant_tensor_conversion_function return constant(v, dtype=dtype, name=name) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 207, in constant value, dtype=dtype, shape=shape, verify_shape=verify_shape)) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 442, in make_tensor_proto _AssertCompatible(values, dtype) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 353, in _AssertCompatible (dtype.name, repr(mismatch), type(mismatch).__name__)) TypeError: Expected bool, got 1 of type 'int' instead. Has anybody tried to train on TX2 or is it for my case only and i did something wrong? ORIGINAL Trying to train on mobilenet ssd on Jetson TX2 (I know it is not for taining but i have no better option) followed these guides https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9 https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md Training runs on my laptop (CPU) fine but i get the following error on my TX2 Traceback (most recent call last): File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper preferred_dtype=default_dtype) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1040, in internal_convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 883, in _TensorTensorConversionFunction (dtype.name, t.dtype.name, str(t))) ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"Loss/Loss/huber_loss/Sub_1:0\", shape=(24, 1917, 4), dtype=float32)' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"object_detection/model_main.py\", line 101, in &lt;module&gt; tf.app.run() File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 126, in run _sys.exit(main(argv)) File \"object_detection/model_main.py\", line 97, in main tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0]) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 425, in train_and_evaluate executor.run() File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 504, in run self.run_local() File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 636, in run_local hooks=train_hooks) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 355, in train loss = self._train_model(input_fn, hooks, saving_listeners) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 824, in _train_model features, labels, model_fn_lib.ModeKeys.TRAIN, self.config) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 805, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File \"/home/nvidia/tensorflow/models/research/object_detection/model_lib.py\", line 287, in model_fn prediction_dict, features[fields.InputDataFields.true_image_shape]) File \"/home/nvidia/tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py\", line 708, in loss weights=batch_reg_weights) File \"/home/nvidia/tensorflow/models/research/object_detection/core/losses.py\", line 74, in __call__ return self._compute_loss(prediction_tensor, target_tensor, **params) File \"/home/nvidia/tensorflow/models/research/object_detection/core/losses.py\", line 157, in _compute_loss reduction=tf.losses.Reduction.NONE File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py\", line 444, in huber_loss math_ops.multiply(delta, linear)) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 326, in multiply return gen_math_ops.mul(x, y, name) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 4689, in mul \"Mul\", x=x, y=y, name=name) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 546, in _apply_op_helper inferred_from[input_arg.type_attr])) TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'. NOTE: Used precompiled wheels to install tensorflow There was an error with protobuf compiler that has been solved by removing this line reserved 6; (line number 104) in ssd.proto on object_detection/protos folder I found this solution here but i couldnt find the link Here is the script to start training PIPELINE_CONFIG_PATH=/home/nvidia/testtraining/models/model/ssd_mobilenet_v1_pets.config MODEL_DIR=/home/nvidia/testtraining/models/model/ NUM_TRAIN_STEPS=50000 NUM_EVAL_STEPS=2000 python3 object_detection/model_main.py \\ --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\ --model_dir=${MODEL_DIR} \\ --num_train_steps=${NUM_TRAIN_STEPS} \\ --num_eval_steps=${NUM_EVAL_STEPS} \\ --alsologtostderr Laptop TF version 1.10.0 Jetson TX2 tf version 1.6.0-rc1 I m new to Ubuntu and Tensorflow so go easy on me :) Thanks EDIT: It seems like line 546, in _apply_op_helper is some sort of error handling line. I tried to fix this error with following edit. Added these. In /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py Added these to line 236 just after define statement import tensorflow as tf y = tf.cast(y, x.dtype) This created some other error message which is solved by editing /home/nvidia/tensorflow/models/research/object_detection/matchers/argmax_matcher.py line 203-204 to these indicator = tf.cast(1-indicator, x.dtype) return tf.add(tf.multiply(x, indicator), val * indicator) But i m still getting error Traceback (most recent call last): File \"object_detection/model_main.py\", line 101, in &lt;module&gt; tf.app.run() File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 126, in run _sys.exit(main(argv)) File \"object_detection/model_main.py\", line 97, in main tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0]) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 425, in train_and_evaluate executor.run() File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 504, in run self.run_local() File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 636, in run_local hooks=train_hooks) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 355, in train loss = self._train_model(input_fn, hooks, saving_listeners) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 824, in _train_model features, labels, model_fn_lib.ModeKeys.TRAIN, self.config) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 805, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File \"/home/nvidia/tensorflow/models/research/object_detection/model_lib.py\", line 287, in model_fn prediction_dict, features[fields.InputDataFields.true_image_shape]) File \"/home/nvidia/tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py\", line 686, in loss keypoints, weights) File \"/home/nvidia/tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py\", line 859, in _assign_targets groundtruth_weights_list) File \"/home/nvidia/tensorflow/models/research/object_detection/core/target_assigner.py\", line 481, in batch_assign_targets anchors, gt_boxes, gt_class_targets, unmatched_class_label, gt_weights) File \"/home/nvidia/tensorflow/models/research/object_detection/core/target_assigner.py\", line 180, in assign match = self._matcher.match(match_quality_matrix, **params) File \"/home/nvidia/tensorflow/models/research/object_detection/core/matcher.py\", line 239, in match return Match(self._match(similarity_matrix, **params), File \"/home/nvidia/tensorflow/models/research/object_detection/matchers/argmax_matcher.py\", line 190, in _match _match_when_rows_are_non_empty, _match_when_rows_are_empty) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func return func(*args, **kwargs) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2047, in cond orig_res_t, res_t = context_t.BuildCondBranch(true_fn) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1897, in BuildCondBranch original_result = fn() File \"/home/nvidia/tensorflow/models/research/object_detection/matchers/argmax_matcher.py\", line 153, in _match_when_rows_are_non_empty -1) File \"/home/nvidia/tensorflow/models/research/object_detection/matchers/argmax_matcher.py\", line 203, in _set_values_using_indicator indicator = tf.cast(1-indicator, x.dtype) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 983, in r_binary_op_wrapper x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name=\"x\") File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 950, in convert_to_tensor as_ref=False) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1040, in internal_convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 235, in _constant_tensor_conversion_function return constant(v, dtype=dtype, name=name) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 214, in constant value, dtype=dtype, shape=shape, verify_shape=verify_shape)) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 433, in make_tensor_proto _AssertCompatible(values, dtype) File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 344, in _AssertCompatible (dtype.name, repr(mismatch), type(mismatch).__name__)) TypeError: Expected bool, got 1 of type 'int' instead. And this one is out of my leage I think there is a huge compatability issues and i will just install tf 1.1 instead I m open to new ideas though",
        "answers": [
            [
                "The key to your problem is here(bottom of traceback): TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'. your y has type float32, but argument - x(place where you pass this y to), needs to be int32. Try using tf.cast(y, tf.int32) or something like that. Sometimes there are some changes in tf/you use some older model versions. So this may happen from time to time. So just open File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\" find line 546, and do that cast. (using sudo vim, i guess)"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am new to Digits and TX2. I am trying to create object detection model using the tutorial from: https://github.com/dusty-nv/jetson-inference I created dataset sucessfully. The issue is with the model While creating a model, I am getting the following error. Memory required for data: 3268934784 creating layer bbox_loss Creating Layer bbox_loss bbox_loss &lt;- bboxes-obj-masked-norm bbox_loss &lt;- bbox-obj-label-norm bbox_loss -&gt; loss_bbox Setting up bbox_loss Top shape: (1) with loss weight 2 Memory required for data: 3268934788 Creating layer coverage_loss Creating Layer coverage_loss coverage_loss &lt;- coverage_coverage/sig_0_split_0 coverage_loss &lt;- coverage-label_slice-label_4_split_0 coverage_loss -&gt; loss_coverage Setting up coverage_loss Top shape: (1) with loss weight 1 Memory required for data: 3268934792 Creating layer cluster The job directory information on the left is: Job Directory /home/nvidia/DIGITS/digits/jobs/20180816-161051-e67a Disk Size 0 B Network (train/val) train_val.prototxt Network (deploy) deploy.prototxt Network (original) original.prototxt Solver solver.prototxt Raw caffe output caffe_output.log Pretrained Model /home/nvidia/bvlc_googlenet.caffemodel.4 Visualizations Tensorboard The error on the server is 2018-08-16 16:10:53 [20180816-161051-e67a] [INFO ] Task subprocess args: \"/home/nvidia/Caffe/caffe/build/tools/caffe train --solver=/home/nvidia/DIGITS/digits/jobs/20180816-161051-e67a/solver.prototxt --gpu=0 --weights=/home/nvidia/bvlc_googlenet.caffemodel.4\" 2018-08-16 16:11:00 [20180816-161051-e67a] [ERROR] Train Caffe Model task failed with error code 1 I have no idea on how to free up memory as I have more than 2 gb available in the job directory. Please help me. Thanks in advance.",
        "answers": [
            [
                "Had the same issue for the last few days, maybe it will help someone in the future. Firstly, make sure that you have the right version of protobuf. You can check it with: protoc --version If it's 2.* you have to update to 3.*, for example to build it as listed here https://github.com/NVIDIA/DIGITS/blob/digits-6.0/docs/BuildProtobuf.md, and then rebuild the Caffe. Also, make sure that you have the compatible version of pip package of protobuf. For me the following version is working well right now for Digits and Caffe from the tutorial https://github.com/dusty-nv/jetson-inference : pip install --user --upgrade protobuf==3.1.0.post1"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm currently trying to run TensorFlow on Nvidia's Jetson Tegra TX1 (running Ubuntu 16.04). On a usual Ubuntu 16.04 the installation of TensorFlow is as easy as pip install tensorflow. But due to the TX1's arm64 (aka aarch64) hardware architecture, this is not possible on the TX1. I managed to install a prebuilt wheel-file of TensorFlow, but I only found some outdated versions: This pip package tensorflow-aarch64 only provides v1.2 This Post by JetsonHacks provides v1.3 This lherman-cs/tensorflow-aarch64 github repo provides v1.4 What I am looking for is a more recent version of TensorFlow ready to install. Does anyone have information on that? (I also tried to build from source, which failed in several errors and takes some hours)",
        "answers": [
            [
                "TensorFlow 1.10 Nvidia itself suggests officially to use the current build from this repo: https://github.com/peterlee0127/tensorflow-nvJetson And I also found another repo providing pre-built wheels: https://github.com/JasonAtNvidia/JetsonTFBuild"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to get the TensorFlow Object Detection API to train a Mask RCNN model. Since I am using a Nvidia Jetson TX1, I installed TensorFlow 1.4 using the prebuilt wheel from this github repo. While trying to run cd &lt;tensorflow&gt;/models/research python object_detection/model_main.py \\ --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\ --model_dir=${MODEL_DIR} \\ --num_train_steps=${NUM_TRAIN_STEPS} \\ --num_eval_steps=${NUM_EVAL_STEPS} \\ --alsologtostderr I hit the following the error: [libprotobuf FATAL google/protobuf/stubs/common.cc:61] This program requires version 3.4.0 of the Protocol Buffer runtime library, but the installed version is 2.6.1. Please update your library. If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library. (Version verification failed in \"external/protobuf_archive/src/google/protobuf/any.pb.cc\".) terminate called after throwing an instance of 'google::protobuf::FatalException' It's telling me to update the protobuf library, but I have no clue how to do that, because I already tried to install the newest version (see below). Actually I don't even know who exactly is throwing this error. Hopefully someone can help me on this. Thanks in advance! I installed the Object Detection API following the official tutorial, especially I installed a newer version of protobuf via # remove old version sudo apt purge protobuf-compiler # download prebuilt protoc cd ~/protobuf wget https://github.com/google/protobuf/releases/download/v3.6.0/protoc-3.6.0-linux-aarch_64.zip unzip protobuf-python*.zip export PATH=$PATH:~/protobuf/bin I think this installation works, because I get the desired output: $ protoc --version libprotoc 3.6.0 $ which protoc /home/&lt;username&gt;/protobuf/bin/protoc But also if I type $ sudo find . -name \"*libprotobuf*\" /usr/lib/aarch64-linux-gnu/libprotobuf-lite.so.9.0.1 /usr/lib/aarch64-linux-gnu/libprotobuf.so.9 /usr/lib/aarch64-linux-gnu/libprotobuf.so.9.0.1 /usr/lib/aarch64-linux-gnu/libprotobuf-lite.so.9 there is something installed by apt. Not sure if that has something to do with the error. Running on: Ubuntu 16.04 Python 3.5 TensorFlow 1.4",
        "answers": [],
        "votes": []
    },
    {
        "question": "When I set per_process_gpu_memory from 0.5 to 1.0, there is not enough memory and it will crashed. 1) So, any ideas or suggestions to make it work? 2) Does convert tensorflow code to tensorRT will improve the performance (not for training, only for prediction)?",
        "answers": [
            [
                "Don't set memory usage to 1.0. Remember, the TX2 is a SoC and the CPU cores and GPU all share a common pool of memory. If the GPU is using 100% of that memory, there is no memory left for the CPU and if I recall correctly, the default OS is not setup for any swap space. There are a few benchmarks that show improvement when using TensorRT over just doing inference in TensorFlow. In theory, TensorRT is more optimized for inference on the GPU and runs a special set of kernels that it selects when building the engine. See here for a few benchmarks: https://github.com/NVIDIA-Jetson/tf_to_trt_image_classification"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am using an NVIDIA Jetson TX2. I am trying to generate an \".so\" file using \"make\" for the DynamixelSDK. But I am getting this Error: mkdir -p ./.objects/ gcc -O2 -O3 -DLINUX -D_GNU_SOURCE -Wall -c -I../../include/dynamixel_sdk -m64 -fPIC -g -c ../../src/dynamixel_sdk/group_bulk_read.c -o .objects/group_bulk_read.o gcc: error: unrecognized command line option \u2018-m64\u2019 Makefile:114: recipe for target '.objects/group_bulk_read.o' failed make: *** [.objects/group_bulk_read.o] Error 1 You can access the make file at- https://pastebin.com/zz9MNnqp Here's a part of the MakeFile : #--------------------------------------------------------------------- # C COMPILER, COMPILER FLAGS, AND TARGET PROGRAM NAME #--------------------------------------------------------------------- DIR_DXL = ../.. DIR_OBJS = ./.objects INSTALL_ROOT = /usr/local MAJ_VERSION = 2 MIN_VERSION = 0 REV_VERSION = 0 TARGET = libdxl_x64_c.so TARGET1 = $(TARGET).$(MAJ_VERSION) TARGET2 = $(TARGET).$(MAJ_VERSION).$(MIN_VERSION) TARGET3 = $(TARGET).$(MAJ_VERSION).$(MIN_VERSION).$(REV_VERSION) CHK_DIR_EXISTS = test -d PRINT = echo STRIP = strip AR = ar ARFLAGS = cr LD = g++ LDFLAGS = -shared -fPIC $(FORMAT)#-Wl,-soname,dxl LD_CONFIG = ldconfig CP = cp CP_ALL = cp -r RM = rm RM_ALL = rm -rf SYMLINK = ln -s MKDIR = mkdir CC = gcc CX = g++ CCFLAGS = -O2 -O3 -DLINUX -D_GNU_SOURCE -Wall -c $(INCLUDES) $(FORMAT) -fPIC -g CXFLAGS = -O2 -O3 -DLINUX -D_GNU_SOURCE -Wall -c $(INCLUDES) $(FORMAT) -fPIC -g FORMAT = -m64 INCLUDES += -I$(DIR_DXL)/include/dynamixel_sdk #--------------------------------------------------------------------- Tried Both the 32 and 64 bit Versions of the MakeFile (for linux). I don't know hoe to solve this error. Any help would be appreciated.",
        "answers": [
            [
                "The makefile assumes that the target is the x86-64 architecture. As a first step, you can simply remove the -m64 option from the FORMAT line in order to get further in the build. However, if the project has never been ported to another architecture, there could well be other target dependencies."
            ],
            [
                "This error usually arises when the march, that is, the architecture of the target machine is not defined correctly. The -m64 line represents that it is being compiled for a 64bit architecture. If you see the Makefile for 32bit, it would be -m32. Try changing the makefile such that it reads something like : ... FORMAT = -march=armv8-a+crypto -mcpu=cortex-a57+crypto .... This is march is usually used for Jetson TX2. Also, for TX2, GCC options to keep in mind are : Use latest GCC toolchain 7.2 Use CLANG llvm front end an alternative to GCC -march=armv8.a+crypto+simd, this enables SIMD, crypto and floating point instruction set and may help."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I dont know if someone can help me here. I also asked in the ros forum. So here is my problem: Im working on a project and im a very beginner of ROS. I have a sensor which calculates the distance between two decanodes (range). The name of these two decanodes are AGV and SV. The AGV is the module which calculates the distance. Thats the key difference between AVG and SV. The AGV has to be connected to the PC via UART to USB, in my case its connected to the Jetson board. You have insight to the data of AGV with the docklight software. Its only for windows. If everything is set up the sensor begins to collect the data (in meters). My question is: How can I have access to these data. There is no information about that i should install a driver or something like that or a package. So if i connect it to the Jetson and if i look to the rosnode list i cant find anything which leads me to the data. I dont know what i have to do. I asked other people and they didnt know how to access the data if it is via UART to USB. So it would be really nice if someone could write a helpful answer.",
        "answers": [
            [
                "If I understand correctly, I think you will need to find documentation on the data format or protocol that your sensor is sending over its serial connection, or, if the manufacturer of the sensor provides a library or SDK with an API for receiving the data. Then write a new ROS node that receives the data and publishes it to the ROS system."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "It used to be that I could easily get USB camera input to Jetson TX2 but somehow the same method and code does not seem to be working and I have no clue what the reason might be. First, I try the Ubuntu USB cam command $ cheese It does not recognize a camera being plugged. However, there is a USB device listed when I type $ lsusb But nothing is shown under $ ls /dev/video* Above all, the actual problem is, I can see this USB cam from Ubuntu 16.04 desktop, but not in Jetson TX2. Absouletly clueless what to do. Help is appriciated.",
        "answers": [
            [
                "I found out the problem after a long research and experimentation. First of all, the USB cam that I meant here was actually an analog camera, plugged with a USB converter. As can be seen from this link analog to digital video conversion is possible through a device called EasyCap. When EasyCap is plugged to a computer (or Jetson TX2) it recognizes it as a USB cam. The problem I faced was that there are actually 4 manufacturers of EasyCap, all doing this by a different chipset; yet all of them are called \"EasyCap\" in the market. STK1160 EasyCAP is in Windows: (Syntek) STK1160 or STK1150 Empia EasyCAP is in Windows: USB 2861 Device or EMP Somagic EasyCAP is in Windows: SM-USB 007 or SMI Grabber Device UTV007 based EasyCAP is in Windows: USBTV007 Jetson TX2, more accurately Ubuntu Tegra, is only able to recognize 1 of all the 4 EasyCap chipsets. And the recommended libraries for other 3 chipsets do not compile in Ubuntu Tegra. So what I did was to buy all of the 4 EasyCap devices, and finally one of them worked. The one that worked is Empia chipset, in my case. Hope I can save someone else the time,"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Righto, I'm getting this issue... cv::cuda::getDeviceEnabledCount() == 0 Platform Specifications: -Jetson TX2 -Ubuntu 16.04 -OpenCV 3.4.2 I have a usr/local/cuda folder. I just reinstalled OpenCV by grabbing the newest release from GitHub, cmaking and then making it. If I manage to figure it out, I'll be sure to update this. Any tips would be greatly appreciated.",
        "answers": [
            [
                "I set the cmake \"WITH_CUDA\" flag to on in the opencv folder I DL'd. Fixed me up m880s"
            ]
        ],
        "votes": [
            -0.9999999
        ]
    },
    {
        "question": "I have a simple program where I am trying to get video out of the camera and run it through a omxh264enc. The element linking is shown below: gst_element_link (source, cFilter0); gst_element_link (cFilter0, encoder); gst_element_link (encoder, cFilter1); gst_element_link (cFilter1, autovs); However I get the errors as shown below: Framerate set to : 30 at NvxVideoEncoderSetParameter0:00:00.609624260 3087 0x683850 ERROR omxh264enc gstomxh264enc.c:280:gst_omx_h264_enc_set_format:&lt;264encoder&gt; Empty caps 0:00:00.609644260 3087 0x683850 ERROR omxvideoenc gstomxvideoenc.c:1815:gst_omx_video_enc_set_format:&lt;264encoder&gt; Subclass failed to set the new format 0:00:00.609656996 3087 0x683850 WARN videoencoder gstvideoencoder.c:623:gst_video_encoder_setcaps:&lt;264encoder&gt; rejected caps video/x-raw(memory:NVMM), width=(int)640, height=(int)480, format=(string)I420, framerate=(fraction)30/1 0:00:00.609696708 3087 0x683850 INFO basesrc gstbasesrc.c:2843:gst_base_src_loop:&lt;camerasrc&gt; marking pending DISCONT 0:00:00.609804004 3087 0x683850 FIXME videoencoder gstvideoencoder.c:606:gst_video_encoder_setcaps:&lt;264encoder&gt; GstVideoEncoder::reset() is deprecated Framerate set to : 30 at NvxVideoEncoderSetParameter0:00:00.609876579 3087 0x683850 ERROR omxh264enc gstomxh264enc.c:280:gst_omx_h264_enc_set_format:&lt;264encoder&gt; Empty caps 0:00:00.609890819 3087 0x683850 ERROR omxvideoenc gstomxvideoenc.c:1815:gst_omx_video_enc_set_format:&lt;264encoder&gt; Subclass failed to set the new format 0:00:00.609903779 3087 0x683850 WARN videoencoder gstvideoencoder.c:623:gst_video_encoder_setcaps:&lt;264encoder&gt; rejected caps video/x-raw(memory:NVMM), width=(int)640, height=(int)480, format=(string)I420, framerate=(fraction)30/1 0:00:00.609955843 3087 0x683850 WARN basesrc gstbasesrc.c:2948:gst_base_src_loop:&lt;camerasrc&gt; error: Internal data flow error. 0:00:00.609971683 3087 0x683850 WARN basesrc gstbasesrc.c:2948:gst_base_src_loop:&lt;camerasrc&gt; error: streaming task paused, reason not-negotiated (-4) 0:00:00.610001219 3087 0x683850 INFO GST_ERROR_SYSTEM gstelement.c:1879:gst_element_message_full:&lt;camerasrc&gt; posting message: Internal data flow error. 0:00:00.610046498 3087 0x683850 INFO GST_ERROR_SYSTEM gstelement.c:1902:gst_element_message_full:&lt;camerasrc&gt; posted error message: Internal data flow error. 0:00:00.610141602 3087 0x683850 FIXME videoencoder gstvideoencoder.c:606:gst_video_encoder_setcaps:&lt;264encoder&gt; GstVideoEncoder::reset() is deprecated 0:00:00.610159906 3087 0x683850 ERROR omx gstomx.c:256:gst_omx_component_handle_messages:&lt;264encoder&gt; encoder port 0 was not flushing 0:00:00.610175682 3087 0x683850 ERROR omx gstomx.c:256:gst_omx_component_handle_messages:&lt;264encoder&gt; encoder port 1 was not flushing Framerate set to : 30 at NvxVideoEncoderSetParameter0:00:00.610238113 3087 0x683850 ERROR omxh264enc gstomxh264enc.c:280:gst_omx_h264_enc_set_format:&lt;264encoder&gt; Empty caps 0:00:00.610250977 3087 0x683850 ERROR omxvideoenc gstomxvideoenc.c:1815:gst_omx_video_enc_set_format:&lt;264encoder&gt; Subclass failed to set the new format 0:00:00.610262593 3087 0x683850 WARN videoencoder gstvideoencoder.c:623:gst_video_encoder_setcaps:&lt;264encoder&gt; rejected caps video/x-raw(memory:NVMM), width=(int)640, height=(int)480, format=(string)I420, framerate=(fraction)30/1 0:00:00.610340385 3087 0x683850 FIXME videoencoder gstvideoencoder.c:606:gst_video_encoder_setcaps:&lt;264encoder&gt; GstVideoEncoder::reset() is deprecated 0:00:00.610353953 3087 0x683850 ERROR omx gstomx.c:256:gst_omx_component_handle_messages:&lt;264encoder&gt; encoder port 0 was not flushing 0:00:00.610367521 3087 0x683850 ERROR omx gstomx.c:256:gst_omx_component_handle_messages:&lt;264encoder&gt; encoder port 1 was not flushing Framerate set to : 30 at NvxVideoEncoderSetParameter0:00:00.610422529 3087 0x683850 ERROR omxh264enc gstomxh264enc.c:280:gst_omx_h264_enc_set_format:&lt;264encoder&gt; Empty caps 0:00:00.610434753 3087 0x683850 ERROR omxvideoenc gstomxvideoenc.c:1815:gst_omx_video_enc_set_format:&lt;264encoder&gt; Subclass failed to set the new format 0:00:00.610447360 3087 0x683850 WARN videoencoder gstvideoencoder.c:623:gst_video_encoder_setcaps:&lt;264encoder&gt; rejected caps video/x-raw(memory:NVMM), width=(int)640, height=(int)480, format=(string)I420, framerate=(fraction)30/1 0:00:00.610519680 3087 0x683850 INFO GST_STATES gstbin.c:3238:bin_handle_async_done:&lt;avs&gt; committing state from READY to PAUSED, old pending PAUSED 0:00:00.610551008 3087 0x683850 INFO GST_STATES gstbin.c:3258:bin_handle_async_done:&lt;avs&gt; completed state change, pending VOID 0:00:00.610564192 3087 0x683850 INFO GST_STATES gstelement.c:2277:_priv_gst_element_state_changed:&lt;avs&gt; notifying about state-changed READY to PAUSED (VOID_PENDING pending) 0:00:00.610609216 3087 0x683ed0 INFO task gsttask.c:318:gst_task_func:&lt;264encoder:src&gt; Task resume from paused 0:00:00.610625792 3087 0x683ed0 ERROR omx gstomx.c:256:gst_omx_component_handle_messages:&lt;264encoder&gt; encoder port 0 was not flushing 0:00:00.610641536 3087 0x683ed0 ERROR omx gstomx.c:256:gst_omx_component_handle_messages:&lt;264encoder&gt; encoder port 1 was not flushing After extensively observing the caps, and using gst-inspect-1.0, it looks like the caps between the source and the encoder should be accepted, especially since I have set up a caps filter too. However, this isn't happening, and I have no idea as to why this is the case.",
        "answers": [
            [
                "Use this stream: uri = 'rtsp://admin:123456@192.168.0.123:554/mpeg4cif' gst_str = (\"rtspsrc location={} latency={} ! rtph264depay ! h264parse ! omxh264dec ! nvvidconv ! video/x-raw, width=(int){}, height=(int){}, format=(string)BGRx ! videoconvert ! appsink sync=false\").format(uri, 200, 3072, 2048) #3072,2048 is frame size cap= cv2.VideoCapture(gst_str,cv2.CAP_GSTREAMER) Now, you can use it simple like any video reading with _,frame=cap.read()"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "When Running a model trained for object detection I'm running out of memory when calling tf.run() 2018-06-26 18:32:16.914049: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2018-06-26 18:32:17.393037: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.31GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2018-06-26 18:32:23.825495: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.31GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2018-06-26 18:32:24.659582: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.11GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2018-06-26 18:32:29.902840: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2018-06-26 18:32:30.955526: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.29GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2018-06-26 18:32:37.434223: W tensorflow/core/framework/op_kernel.cc:1328] OP_REQUIRES failed at where_op.cc:286 : Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true / nonzero indices. temp_storage_bytes: 1, status: too many resources requested for launch Is there some type of process of training a model to ensure a model doesn't require a large amount of RAM for inference? Is there some way I'm able to convert my model to use less memory? I've tried some of the graph transforms but they didn't seem to do much. I've also set the GPU limit to be 40% of ram but that didn't help either. I should have about 4gb-5gb of ram available. These are the main problems I think I might have. 1) The model that was trained off of inception V3 instead of a mobile model. 2) Images of somewhat of a large size were labeled and used for transfer learning. -EDIT It appears to be due to poor allocation of memory with tensorflow and Cuda on ARM architectures.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm looking into converting a pre-trained object detection model with TensorRT to try it out on my NVIDIA Jetson TX2 but every model I find has layers that are not yet supported by TensorRT. So far I tried SSD with MobileNet and Faster R-CNN but they both have operations such as Identity that are not supported by TensorRT and I can't find many other TensorFlow models out there. Thank you",
        "answers": [
            [
                "This repo has the pre-trained models you are looking for, along with instructions on how to take a model and build a TensorRT engine: https://github.com/NVIDIA-Jetson/tf_to_trt_image_classification#download"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I would like to create a VR so I created a RTSP server to link to my Zedmini. It is working if I use a h265 encoder, but the bad thing is the RTSP only works if I use Iphone7 VLC app or computer window 8 VLC software, my Android phone huawei p7 Onvifer app cannot use this RTSP address at all. I need to use huawei p7 for my project as I am going to create the app and link to this RTSP server. Based on my checking, Some Android device do not support h265 encoder, so I decided to use h264 and I have been googling a lot for few weeks but became frustrated for not finding a solution to use h264. This is the code which I amend from test-readme.c------&gt; #include &lt;gst/gst.h&gt; #include &lt;gst/rtsp-server/rtsp-server.h&gt; int main (int argc, char *argv[]) { GMainLoop *loop; GstRTSPServer *server; GstRTSPMountPoints *mounts; GstRTSPMediaFactory *factory; gst_init (&amp;argc, &amp;argv); loop = g_main_loop_new (NULL, FALSE); /* create a server instance */ server = gst_rtsp_server_new (); /* get the mount points for this server, every server has a default object that be used to map uri mount points to media factories */ mounts = gst_rtsp_server_get_mount_points (server); /* make a media factory for a test stream. The default media factory can use gst-launch syntax to create pipelines. any launch line works as long as it contains elements named pay%d. Each element with pay%d names will be a stream */ factory = gst_rtsp_media_factory_new (); //working case for streaming video //gst_rtsp_media_factory_set_launch (factory,\"( videotestsrc is-live=1 ! x264enc ! rtph264pay name=pay0 pt=96 )\"); //working case for external camera //gst_rtsp_media_factory_set_launch (factory,\"( v4l2src is-live=1 device=/dev/video1 ! video/x-raw, width=(int)720, height=(int)480 framerate=30/1 format=I420 ! timeoverlay ! omxh265enc ! rtph265pay name=pay0 pt=96 )\"); //working case for JX2 camera //gst_rtsp_media_factory_set_launch (factory,\"( nvcamerasrc sensor-id=0 ! video/x-raw(memory:NVMM), width=1920, height=1080, framerate=30/1, format=I420 ! nvvidconv flip-method=4 !video/x-raw, width=(int)720, height=(int)480 framerate=30/1 format=I420 ! timeoverlay ! omxh265enc ! rtph265pay name=pay0 pt=96 )\"); //Fail or not working case for Zed mini camera testing FOR H264 gst_rtsp_media_factory_set_launch (factory,\"(v4l2src is-live=1 device=/dev/video1 ! video/x-raw, width=2560, height=720, framerate=30/1, format=I420 ! nvvidconv !video/x-raw, width=(int)720, height=(int)480, framerate=30/1, format=NV12 ! omxh264enc bitrate=10000000 ! rtph264pay name=pay0 pt=96 )\"); //working case for Zed mini camera FOR H265 //gst_rtsp_media_factory_set_launch (factory,\"(v4l2src is-live=1 device=/dev/video1 ! video/x-raw, width=2560, height=720, framerate=30/1, format=I420 ! nvvidconv !video/x-raw, width=(int)720, height=(int)480 framerate=30/1 format=I420 ! timeoverlay ! omxh265enc ! rtph265pay name=pay0 pt=96 )\"); gst_rtsp_media_factory_set_shared (factory, TRUE); /* attach the test factory to the /test url */ gst_rtsp_mount_points_add_factory (mounts, \"/test\", factory); /* don't need the ref to the mapper anymore */ g_object_unref (mounts); /* attach the server to the default maincontext */ gst_rtsp_server_attach (server, NULL); /* start serving */ g_print (\"stream ready at rtsp://172.16.124.75:8554/test\\n\"); g_main_loop_run (loop); return 0; } This code is working on streaming video, JX2 camera, simple USB camera (low end), also zedmini camera but using h265. I need the code to run using h264, there must be some element missed out here or wrong. gst_rtsp_media_factory_set_launch (factory,\"(v4l2src is-live=1 device=/dev/video1 ! video/x-raw, width=2560, height=720, framerate=30/1, format=I420 ! nvvidconv !video/x-raw, width=(int)720, height=(int)480, framerate=30/1, format=NV12 ! omxh264enc bitrate=10000000 ! rtph264pay name=pay0 pt=96 )\");",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a rather large code base. It is all here https://github.com/mpkuse/nap/tree/master-desktop/src It is actually a ros package which has to be compiled with catkin_make which is an layer over cmake. My code base compiles fine on my ubuntu-desktop (gcc version 5.4.0). However, as I move it to Nvidia-TX2, I get very long error list. A snippet of it as below. The class Node is defined in Node.h and included in other classes. /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:73:68: error: template argument 2 is invalid /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:73:68: error: template argument 1 is invalid /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:73:68: error: template argument 2 is invalid /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:73:56: error: invalid use of template-name \u2018std::vector\u2019 without an argument list LocalBundle( const nap::NapMsg::ConstPtr&amp; msg, const vector&lt;Node*&gt;&amp; global_nodes, const PinholeCamera&amp; camera ); ^ /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:73:62: error: expected \u2018,\u2019 or \u2018...\u2019 before \u2018&lt;\u2019 token LocalBundle( const nap::NapMsg::ConstPtr&amp; msg, const vector&lt;Node*&gt;&amp; global_nodes, const PinholeCamera&amp; camera ); ^ In file included from /home/nvidia/catkin_ws/src/nap/src/DataManager.h:82:0, from /home/nvidia/catkin_ws/src/nap/src/DataManager_rviz_visualization.cpp:1: /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:127:45: error: template argument 1 is invalid int find_indexof_node( const vector&lt;Node*&gt;&amp; global_nodes, ros::Time stamp ); ^ /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:127:45: error: template argument 2 is invalid /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:127:45: error: template argument 1 is invalid /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:127:45: error: template argument 2 is invalid /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:127:45: error: template argument 1 is invalid /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:127:45: error: template argument 2 is invalid /home/nvidia/catkin_ws/src/nap/src/LocalBundle.h:127:45: error: template argument 1 is invalid What is going on here? Any suggestions? I tried to isolate the problem. Here is what I did, I completely removed the classes LocalBundle and Corvus. They were being used in DataManager_core.cpp/place_recog_callback(). Also removed corresponding cmake entries. Now the code can compile. I tried adding a dummy class Suse // Suse.h #pragma once #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;vector&gt; #include &lt;math.h&gt; using namespace std; class Suse { public: Suse(); void sayHi(); }; ; // Suse.cpp #include \"Suse.h\" Suse::Suse() { } void Suse::sayHi() { cout &lt;&lt; \"Hi from Suse\\n\"; } Then tried to use this class in exact same place (ie. in place_recog_callback()) like: Suse sy = Suse(); sy.sayHi(); Also added Suse.cpp in cmake compile list and included Suse.h in DataManager.h. With this it compiles successfully. However, as I include opencv in Suse.h something like: // //opencv #include &lt;opencv2/core/core.hpp&gt; using namespace cv; I get a long error message, first few lines of which look similar to original error. [ 81%] Building CXX object nap/CMakeFiles/pose_graph_opt_node.dir/src/DataManager_core.cpp.o [ 83%] Building CXX object nap/CMakeFiles/pose_graph_opt_node.dir/src/DataManager_rviz_visualization.cpp.o [ 85%] Building CXX object nap/CMakeFiles/pose_graph_opt_node.dir/src/pose_graph_opt_node.cpp.o [ 87%] Building CXX object nap/CMakeFiles/pose_graph_opt_node.dir/src/DataManager_utils.cpp.o [ 88%] Building CXX object nap/CMakeFiles/pose_graph_opt_node.dir/src/Suse.cpp.o In file included from /home/nvidia/catkin_ws/src/nap/src/DataManager_core.cpp:1:0: /home/nvidia/catkin_ws/src/nap/src/DataManager.h:171:21: error: template argument 1 is invalid const vector&lt;Node*&gt;&amp; getNodesRef() { return nNodes; } ^ /home/nvidia/catkin_ws/src/nap/src/DataManager.h:171:21: error: template argument 2 is invalid /home/nvidia/catkin_ws/src/nap/src/DataManager.h:185:15: error: template argument 1 is invalid vector&lt;Node*&gt; nNodes; //list of notes ^ /home/nvidia/catkin_ws/src/nap/src/DataManager.h:185:15: error: template argument 2 is invalid In file included from /home/nvidia/catkin_ws/src/nap/src/DataManager_rviz_visualization.cpp:1:0: /home/nvidia/catkin_ws/src/nap/src/DataManager.h:171:21: error: template argument 1 is invalid const vector&lt;Node*&gt;&amp; getNodesRef() { return nNodes; } ^ /home/nvidia/catkin_ws/src/nap/src/DataManager.h:171:21: error: template argument 2 is invalid /home/nvidia/catkin_ws/src/nap/src/DataManager.h:185:15: error: template argument 1 is invalid vector&lt;Node*&gt; nNodes; //list of notes ^ /home/nvidia/catkin_ws/src/nap/src/DataManager.h:185:15: error: template argument 2 is invalid",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am optimizing the hyper-parameters of my neural-network, for which I am recursively training the network using different hyper-parameters. It works as expected until after some iterations, when creating a new network for training, it dies with the error \"Segmentation fault (core dumped)\". Furthermore, I am using GPU for training and I am doing this on a Nvidia Jetson TX2 and Python3.5. Also, I am using Keras with TensorFlow backend.",
        "answers": [
            [
                "If you run K.clearsession() on a GPU with Keras 2, you may get a segmentation fault. If you have this in your code, try removing it!"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a Jetson TX2 running a machine vision algorithm, and I'd like to communicate the output from this board to a Windows 10 PC in some way. The data being sent is tiny - on the scale of a vector of ~100 floats at worst, and the refresh rate I need is nothing crazy - the limiting factor will be that the frames from the camera going to the Jetson board are being grabbed at ~60 FPS. I'm open to suggestions using either Ethernet cables (preferred) or USB cables. The code on the Jetson is written in Python, and the output will be picked up by a C# application running on the Windows PC. Neither of the computers will be connected to the internet in general. Since I'm not scaling up my system (it'll only ever be 1 Jetson TX2 -&gt; 1 PC) I'd appreciate the easiest possible solution, rather than most technically robust! Thanks",
        "answers": [
            [
                "How about using rabitmq (message broker) to control sending and receiving messages (your data). It will look like this picture. You sender (Jetson) will store message in message queue. And your receiver (window PC) will keep connecting to the queue through local IP. When new message come, receiver will read it and do further processing. There is example about using dotnet at https://www.rabbitmq.com/tutorials/tutorial-one-dotnet.html . So it's very easy to implement. Wouldn't take more than 2 hours. Hope that help"
            ],
            [
                "The Jetson itself doesn't do communication. It's up to its carrier board. Most of them provide an Ethernet port. Hence, it's quite easy to implement almost any transport over IP. I'd choose something like mere TCP socket to do it."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "So I am getting \"not a supported wheel on this platform\" message even though I have installed earlier versions of these wheels from the same repository. Dump: nvidia@tegra-ubuntu:~/Downloads$ sudo -H pip install tensorflow-1.8.0-cp27-cp27mu-linux_aarch64.whl tensorflow-1.8.0-cp27-cp27mu-linux_aarch64.whl is not a supported wheel on this platform. Digging a little deeper I dumped the PEP supported tags. nvidia@tegra-ubuntu:~/Downloads$ python -c \"from pip import pep425tags;print(pep425tags.supported_tags)\" [('cp27', 'cp27mu', 'linux_aarch64'), ('cp27', 'none', 'linux_aarch64'), ('py2', 'none', 'linux_aarch64'), ('cp27', 'none', 'any'), ('cp2', 'none', 'any'), ('py27', 'none', 'any'), ('py2', 'none', 'any'), ('py26', 'none', 'any'), ('py25', 'none', 'any'), ('py24', 'none', 'any'), ('py23', 'none', 'any'), ('py22', 'none', 'any'), ('py21', 'none', 'any'), ('py20', 'none', 'any')] The wheel file name seems to match the very first tag (PEP 425 standard), so as far as I can tell it should install it. Maybe the definitive tags are stored internally and are different? But I could not find a utility to dump them from a wheel, and I also get the same error message when I try on the old 1.6 wheel that I successfully installed a few months back. I am thinking something broke when I upgraded my pip to version 10 yesterday (duh... but that pesky message kept coming up), but then the dumped supported tags should have changed then too. I don't really want to flatten this machine which would surely fix this. Anyone have some ideas how I can get pip to install this wheel? More information on how pip finds the tags (does it really just look at the file name?) would help as well. I tried messing with the tags in the name and that did not change a thing. Update: I am running Linux4Tegra on a Nvidia Jetson TX2 - Jetpack 3.2 - it seems L4T is very Ubuntu-like, I am guessing it is kind of based on that. Here is the uname output: nvidia@tegra-ubuntu:~/Downloads$ uname -a Linux tegra-ubuntu 4.4.38-tegra #1 SMP PREEMPT Fri Dec 1 06:08:28 PST 2017 aarch64 aarch64 aarch64 GNU/Linux",
        "answers": [
            [
                "Following a suggestion I received here, I reinstalled pip from bootstrap.pypa.io which is more authoritative (I think) than whatever apt-get finds and then it worked fine. Too bad in a way, would have liked to know what was broken. But at least I can on with TensorFlowing... This is what worked: $ curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py $ sudo python get.pip.py Docs here: https://pip.pypa.io/en/stable/installing/"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am unexpectedly getting a very low FPS (~16 fps) while capturing from the internal webcam (1280x720 @ 30fps) of a recent Dell XPS 9560. This is the trivial code I'm using ( python3, OpenCV 3.4.0 ) import cv2, time cam = cv2.VideoCapture(0) n_frames = 0 execution_time = 0 while True: t_start = time.time() rv, frame = cam.read() n_frames+=1 if rv: #also tried to comment imshow. Same FPS. cv2.imshow('window', frame) if cv2.waitKey(1) &gt;= 0: break pass else: print('Cannot read Frame') t_end = time.time() execution_time += (t_end-t_start)*1000 if execution_time &gt; 10000: print ('avg FPS in 10 seconds: %.2f' % (n_frames*1000/execution_time)) n_frames = 0 execution_time = 0 I tried to write the same simple program in C++ and got the same result, the same ~16 FPS. Occasionally, both the C++ and Python program can generate a higher FPS for a shorter amount of time. By monitoring CPU usage with i7z, I could see that all 4 cores where running at a very low frequency, close to the minimum, for most of the time, with occasional spikes that seemed not to affect very much the average FPS. I then transferred the exact same code to a Jetson TX1. For those who don't know it, it's an ARM-based system on a chip, running a dedicated Ubuntu 16.10. It is connected to an USB 2.0 camera, 1920x1080 @ 25fps. Needless to say, I got exactly 25 FPS as expected. Can anybody explain this behaviour? Is it something related to differences at operating system level? How to get full FPS in any case? Thanks for your help EDIT: after VTT comment, I attached the same external (supposed 30 fps) USB camera to both systems, and I get 15 FPS on both. This points to crappy cameras/usb bus. I will need to dismantle the jetson device internal camera and connect it to the laptop to double-check this is camera-related.",
        "answers": [
            [
                "I dropped this problem out of frustration only to discover its answer some days ago while I was chasing another one... I just had to turn on the lights to find the answer! It turns out that my camera automatically lowers its FPS when in poor light condition. When you think about it, it may surely happen when the exposure time needs to be longer than the 1/30s. So, in the end it looks like the 30fps should be intended as \"Maximum fps=30\". But it can and will be lower than that. Very annoying in my opinion, too bad I had to learn it the hard way."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am working on the autonomous driving system of a university-made Mars rover with ROS. I am trying to communicate NVIDIA jetson (master system) with Arduino (slave system) to control ESC that is connected with a motor with platformio through USB. However, after I finished inputting the command platformio run --target upload the following error shows: errors I have tried all solutions online, but none of them work and everytime led me to restore the Jetson. So please help! Many thanks in advance",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am currently using a windows computer with gitbash to ssh into an nvidia jetson tx2 with an onboard camera: ssh nvidia@'my ip address\" Then I am activating the camera on the jetson tx2 with the command: gst-launch-1.0 -ev nvcamerasrc ! nvoverlaysink I have a python script ready to go to view the camera stream: import numpy as np import cv2 cap=cv2.VideoCapture('/dev/video0') while(True): # Capture frame-by-frame ret, frame = cap.read() # Our operations on the frame come here gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Display the resulting frame cv2.imshow('frame',gray) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break # When everything done, release the capture cap.release() cv2.destroyAllWindows() When I run this script directly on my jetson tx2 python distribution I have installed this script displays the camera feed. My question is, how can I make this script work on the machine I'm using to ssh into the jetson tx2? I believe it has to do with the line: cap=cv2.VideoCapture('/dev/video0') how does the camera get called when I am sshing into the jetson?",
        "answers": [
            [
                "You'll need to X11 forward the display. Right now it doesn't know what to do with the output since there is no display connected. If you are running on windows you'll need to download xming. Run that (just double click the desktop shortcut) and then: ssh -X nvidia@ip_address Keep in mind that X forwarding is bandwidth limited so it might slow down your process, whatever that may end up being."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "So I'm using NVIDIA's Jetson toolchain in a Docker container. The toolchain seems to have a folder structure like this: gcc-4.8.5-aarch64 install aarch-unknown-linux-gnu bin include lib lib64 sysroot etc lib sbin usr var bin include lib libexec share There are nested directories with three lib dirs. I can compile my libraries e.g. Boost and PCL just fine but linking PCL to my application gives: /install/bin/../lib/gcc/aarch64-unknown-linux-gnu/4.8.5/../../../.. /aarch64-unknown-linux-gnu/bin/ld: warning: libgomp.so.1, needed by /install/aarch64-unknown-linux-gnu/sysroot/lib/libpcl_common.so, not found (try using -rpath or -rpath-link) /install/aarch64-unknown-linux-gnu/sysroot/lib/libpcl_common.so: undefined reference to `GOMP_loop_dynamic_next@GOMP_1.0' The library is there: ./gcc-4.8.5-aarch64/install/aarch64-unknown-linux-gnu/lib64/libgomp.so.1 I have just extracted the toolchain tarball and set CMake SYSROOT to the enclosed sysroot. I'm still wondering if that's how it's supposed to by used. Should I, for example, move libgomp.so.1 and other libraries from their current locations to the sysroot dir? What is the correct way to make linker find libgomp.so.1 here?",
        "answers": [
            [
                "I eventually solved this by moving /install/aarch64-unknown-linux-gnu/lib64 under /install/aarch64-unknown-linux-gnu/sysroot/. Not sure if that's 100% right, but everything now compiles and links beautifully."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have installed Tensorflow and Tflearn on my Jetson Tx1. Tensorflow works and the program I'm trying to run works on my mac. But I get this error when I run it on my jetson. Traceback (most recent call last): File \"net.py\", line 164, in &lt;module&gt; net = tflearn.regression(net, optimizer='adam', learning_rate=0.00001) File \"/usr/local/lib/python3.5/dist-packages/tflearn/layers/estimator.py\", line 174, in regression loss = objectives.get(loss)(incoming, placeholder) File \"/usr/local/lib/python3.5/dist-packages/tflearn/objectives.py\", line 66, in categorical_crossentropy keepdims=True) TypeError: reduce_sum() got an unexpected keyword argument 'keepdims' The code for the neural net # Network building net = tflearn.input_data([None, 25]) net = tflearn.embedding(net, input_dim=len(words), output_dim=256) #Embedding instead of one hot encoding. net = tflearn.lstm(net, 256, dropout=0.9) #0.9, 0.00001, 30 was good --&gt;63% net = tflearn.fully_connected(net, 2, activation='softmax') net = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy', learning_rate=0.00001) # Training model = tflearn.DNN(net, tensorboard_verbose=0) model.fit(x_train, y_train, n_epoch=15, validation_set=(x_test, y_test), show_metric=True, batch_size=30) model.save('mod.model')",
        "answers": [
            [
                "For Tensorflow v1.4 or below, the parameter to preserve dimensions is written keep_dims (with underscore). The change (to keepdims, currently with retro-compatibility) was introduced in v1.5. It is thus possible that your TFlearn version is too recent for your Tensorflow. Upgrading the latter may solve your problem."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm trying to change the image being displayed in an Fl_Box. static Fl_RGB_Image *greenRgb; static Fl_RGB_Image *redRgb; static Fl_RGB_Image *blackRgb; Fl_Box *makeTristate(char const *name) { Fl_Box *ret = new Fl_Box(0, 0, 300, 32, name); ret-&gt;labelsize(24); ret-&gt;align(FL_ALIGN_RIGHT | FL_ALIGN_IMAGE_NEXT_TO_TEXT); ret-&gt;image(whiteRgb); return ret; } void setTristate(Fl_Box *tris, int state) { Fl_RGB_Image *wanted; if (state &lt; 0) { wanted = redRgb; } else if (state &gt; 0) { wanted = greenRgb; } else { wanted = blackRgb; } if (wanted != tris-&gt;image()) { LOG_PRINTF(\"changing %p (%s) from %p to %p\", tris, tris-&gt;label(), tris-&gt;image(), wanted); tris-&gt;image(wanted); tris-&gt;redraw(); } } The images are properly loaded. Whichever image is first set on the Fl_Box when it is created, keeps being displayed forever, even though the printf() clearly shows that I change images to some other good image. It's as if the redraw() didn't actually cause the box to realize it has a new image and redraw itself. The main loop is running; I have another image (a video capture) that I re-create (delete the old, create a new) 30 times a second, and that box/image re-displays itself just fine. The Fl_Box-es that I create with makeTristate() and update with setTristate() are in themselves inside a Fl_Pack. In desperation, I've tried to call uncache() on the wanted image before assigning it, but that doesn't help. What am I doing wrong; how can I get the image of the Fl_Box to be a different image and have it re-display? The system is Ubuntu 16.04 running on a Jetson TX2. I'm using FLTK 1.3.4. I can't update to anything newer, because this is the latest NVIDIA has released (\"JetPack 3.2\") for the hardware I'm running on. The problem happens both on a natively attached display, driven by the on-board NVIDIA driver, and when running across a network with a different DISPLAY.",
        "answers": [
            [
                "So, it turns out I had forgotten the FLTK behavior that causes this. redraw() will only invalidate the box of the widget, not its label area. Because I didn't set FL_ALIGN_INSIDE in the call to align() the label and image ended up outside the label area, but I hadn't noticed because there wasn't a lot to line up with these things in the window. Adding FL_ALIGN_INSIDE to the call to align() fixed the problem. And, more generally, remember that redraw() doesn't invalidate the label area of a widget, if the label is outside the widget."
            ]
        ],
        "votes": [
            1e-07
        ]
    }
]