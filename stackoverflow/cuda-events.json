[
    {
        "question": "I need to pause the execution of all calls in a stream from a certain point in one part of the program until another part of the program decides to unpause this stream at an arbitrary time. This is the requirement of the application I'm working on, I can't work around that. Ideally I want to use the graph API (e.g. cudaGraphAddMemcpyNode), but regular async calls (e.g. cudaMemcpyAsync) are acceptable too if graphs can't do this for some reason. From reading CUDA's documentation I thought that there is an obvious way to do this, but it turned out to be way more complicated. This is my first attempt, distilled to a simple example: cudaGraphCreate(&amp;cuda_graph_cpy, 0); cudaGraphAddMemcpyNode1D(&amp;memcpy_h2d_node, cuda_graph_cpy, NULL, 0, device_buf, host_buf, BUF_SIZE * sizeof(char), cudaMemcpyDefault); cudaGraphAddEventWaitNode(&amp;wait_node, cuda_graph_cpy, &amp;memcpy_h2d_node, 1, cuda_event); cudaGraphAddMemcpyNode1D(&amp;memcpy_d2h_node, cuda_graph_cpy, &amp;wait_node, 1, host_buf, device_buf, BUF_SIZE * sizeof(char), cudaMemcpyDefault); cudaGraphInstantiate(&amp;cuda_graph_exec_cpy, cuda_graph_cpy, NULL, NULL, 0); cudaGraphCreate(&amp;cuda_graph_set, 0); cudaGraphAddMemsetNode(&amp;memset_node, cuda_graph_set, NULL, 0, &amp;memset_params); cudaGraphAddEventRecordNode(&amp;record_set_node, cuda_graph_set, &amp;memset_node, 1, cuda_event); cudaGraphInstantiate(&amp;cuda_graph_exec_set, cuda_graph_set, NULL, NULL, 0); cudaGraphLaunch(cuda_graph_exec_cpy, cuda_stream_cpy); cudaGraphLaunch(cuda_graph_exec_set, cuda_stream_set); cudaStreamSynchronize(cuda_stream_cpy); So I create and instantiate one linear graph that: does a host-to-device copy, waits for cuda_event, does a device-to-host copy. Then I create and instantiate another linear graph that: does a memset on the device memory, records cuda_event. After that I launch the first graph on cuda_stream_cpy, then launch the second graph on cuda_stream_set, then synchronize on cuda_stream_cpy. In the end I expected to modify host_buf, but instead it is left untouched because the first graph/stream didn't actually wait for anything and proceeded with the second copy immediately. After rewriting the code with regular async calls instead of graphs and getting the same behavior, reading everything I could find in Google on this topic, and experimenting with flags and adding more cudaEventRecord/cudaGraphAddEventRecordNode calls in different places, I realized that the event semantics of CUDA doesn't seem to be capable of the behavior I need? The issue seems to be that both record and wait calls have to be made around the same time, and it's impossible to decouple them. If there is no event record enqueued yet, the wait async call or graph node doesn't block the stream, and the stream keeps going. So what I would like to do is to either replace cudaGraphAddEventWaitNode/cudaGraphAddEventRecordNode in the code sample above, or add something to the sample so that the code works the way I described: the wait node actually blocks the stream until the record node (or its replacement?) unblocks it. I also found in CUDA something called the \"external semaphores\" that could be doing what I want (with cudaGraphAddExternalSemaphoresWaitNode/cudaGraphAddExternalSemaphoresSignalNode instead) but they seem to be impossible to create without also using Vulkan or DirectX, which is something I can't bring into the application. I tried to pass a shared memory object's file descriptor to cudaImportExternalSemaphore for cudaExternalSemaphoreHandleTypeOpaqueFd, but that didn't work. EDIT 1: I tried to integrate the wait kernel suggested by @RobertCrovella into my prototype, but it gets stuck on the first graph's launch. Here's the reproducer: #include \"cuda_runtime_api.h\" #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #define BUF_SIZE 1024 #define TEST_POS_OLD 512 #define TEST_POS_NEW 10 #define OLD_VAL 5 #define NEW_VAL 23 #define CUDA_CHKERR(x) res = x; if (res != cudaSuccess) goto fail; __global__ void wait_kernel(volatile unsigned char *event, unsigned char val) { while (*event == val); } int main() { cudaError_t res = cudaSuccess; const char *err_str = NULL; const char *err_name = NULL; cudaStream_t cuda_stream_cpy; cudaStream_t cuda_stream_set; cudaGraph_t cuda_graph_cpy; cudaGraphExec_t cuda_graph_exec_cpy; cudaGraph_t cuda_graph_set; cudaGraphExec_t cuda_graph_exec_set; cudaGraphNode_t memcpy_h2d_node; cudaGraphNode_t memcpy_d2h_node; cudaGraphNode_t memset_node; cudaGraphNode_t signal_node; cudaGraphNode_t wait_node; unsigned char *event; unsigned char test = 0; dim3 grid(1,1,1); dim3 block(1,1,1); struct cudaKernelNodeParams kernel_node_params = {}; struct cudaMemsetParams memset_params = {}; void *wait_kernel_args[2] = {(void *) &amp;event, (void *) &amp;test}; char *host_buf = NULL; void *device_buf = NULL; printf(\"Creating the event...\\n\"); CUDA_CHKERR(cudaMalloc(&amp;event, sizeof(event[0]))); printf(\"cudaMalloc\\n\"); CUDA_CHKERR(cudaMemset(event, 0, sizeof(event[0]))); printf(\"cudaMemset\\n\"); printf(\"Allocating the host buffer and setting the test value...\\n\"); host_buf = (char *) malloc(BUF_SIZE * sizeof(char)); for (int i = 0; i &lt; BUF_SIZE; i++) { host_buf[i] = OLD_VAL; } CUDA_CHKERR(cudaMalloc(&amp;device_buf, BUF_SIZE * sizeof(char))); printf(\"cudaMalloc\\n\"); CUDA_CHKERR(cudaStreamCreate(&amp;cuda_stream_cpy)); printf(\"cudaStreamCreate cpy\\n\"); CUDA_CHKERR(cudaStreamCreate(&amp;cuda_stream_set)); printf(\"cudaStreamCreate set\\n\"); CUDA_CHKERR(cudaGraphCreate(&amp;cuda_graph_cpy, 0)); printf(\"cudaGraphCreate cpy\\n\"); CUDA_CHKERR(cudaGraphAddMemcpyNode1D(&amp;memcpy_h2d_node, cuda_graph_cpy, NULL, 0, device_buf, host_buf, BUF_SIZE * sizeof(char), cudaMemcpyDefault)); printf(\"cudaGraphAddMemcpyNode1D H2D\\n\"); memset(&amp;kernel_node_params, 0, sizeof(cudaKernelNodeParams)); kernel_node_params.func = (void *)wait_kernel; kernel_node_params.gridDim = grid; kernel_node_params.blockDim = block; kernel_node_params.sharedMemBytes = 0; kernel_node_params.kernelParams = wait_kernel_args; kernel_node_params.extra = NULL; CUDA_CHKERR(cudaGraphAddKernelNode(&amp;wait_node, cuda_graph_cpy, &amp;memcpy_h2d_node, 1, &amp;kernel_node_params)); printf(\"cudaGraphAddKernelNode (wait)\\n\"); CUDA_CHKERR(cudaGraphAddMemcpyNode1D(&amp;memcpy_d2h_node, cuda_graph_cpy, &amp;wait_node, 1, host_buf, device_buf, BUF_SIZE * sizeof(char), cudaMemcpyDefault)); printf(\"cudaGraphAddMemcpyNode1D D2H\\n\"); CUDA_CHKERR(cudaGraphInstantiate(&amp;cuda_graph_exec_cpy, cuda_graph_cpy, NULL, NULL, 0)); printf(\"cudaGraphInstantiate cpy\\n\"); CUDA_CHKERR(cudaGraphCreate(&amp;cuda_graph_set, 0)); printf(\"cudaGraphCreate set\\n\"); memset(&amp;memset_params, 0, sizeof(cudaMemsetParams)); memset_params.dst = device_buf; memset_params.value = NEW_VAL; memset_params.pitch = 0; memset_params.elementSize = sizeof(char); memset_params.width = 512; memset_params.height = 1; CUDA_CHKERR(cudaGraphAddMemsetNode(&amp;memset_node, cuda_graph_set, NULL, 0, &amp;memset_params)); printf(\"cudaGraphAddMemsetNode\\n\"); memset(&amp;memset_params, 0, sizeof(cudaMemsetParams)); memset_params.dst = event; memset_params.value = 1; memset_params.pitch = 0; memset_params.elementSize = 1; memset_params.width = 1; memset_params.height = 1; CUDA_CHKERR(cudaGraphAddMemsetNode(&amp;signal_node, cuda_graph_set, &amp;memset_node, 1, &amp;memset_params)); printf(\"cudaGraphAddMemsetNode (signal)\\n\"); CUDA_CHKERR(cudaGraphInstantiate(&amp;cuda_graph_exec_set, cuda_graph_set, NULL, NULL, 0)); printf(\"cudaGraphInstantiate set\\n\"); CUDA_CHKERR(cudaGraphLaunch(cuda_graph_exec_cpy, cuda_stream_cpy)); printf(\"cudaGraphLaunch cpy\\n\"); CUDA_CHKERR(cudaGraphLaunch(cuda_graph_exec_set, cuda_stream_set)); printf(\"cudaGraphLaunch set\\n\"); CUDA_CHKERR(cudaStreamSynchronize(cuda_stream_cpy)); printf(\"cudaStreamSynchronize cpy\\n\"); CUDA_CHKERR(cudaGraphExecDestroy(cuda_graph_exec_cpy)); printf(\"cudaGraphExecDestroy\\n\"); CUDA_CHKERR(cudaGraphExecDestroy(cuda_graph_exec_set)); printf(\"cudaGraphExecDestroy\\n\"); CUDA_CHKERR(cudaGraphDestroy(cuda_graph_cpy)); printf(\"cudaGraphDestroy\\n\"); CUDA_CHKERR(cudaGraphDestroy(cuda_graph_set)); printf(\"cudaGraphDestroy\\n\"); CUDA_CHKERR(cudaStreamDestroy(cuda_stream_cpy)); printf(\"cudaStreamDestroy cpy\\n\"); CUDA_CHKERR(cudaStreamDestroy(cuda_stream_set)); printf(\"cudaStreamDestroy set\\n\"); if (host_buf[TEST_POS_OLD] == OLD_VAL) { printf(\"host_buf[TEST_POS_OLD] is correct.\\n\"); } else { printf(\"host_buf[TEST_POS_OLD] is not correct!\\n\"); } if (host_buf[TEST_POS_NEW] == NEW_VAL) { printf(\"host_buf[TEST_POS_NEW] is correct.\\n\"); } else { printf(\"host_buf[TEST_POS_NEW] is not correct!\\n\"); if (host_buf[TEST_POS_OLD] == host_buf[TEST_POS_NEW]) printf(\"They are equal!\\n\"); } return 0; fail: err_name = cudaGetErrorName(res); err_str = cudaGetErrorString(res); printf(\"%s: %s\\n\", err_name, err_str); return 1; } EDIT 2: The issue was indeed the host memory allocation, with that correction my code works properly.",
        "answers": [
            [
                "Although the comments say otherwise, you are effectively building a dependency between the two graphs. With a bit of refactoring, my suggestion would be to combine those activities into one graph, and express the dependency using the mechanisms available in graph capture. However with the goals: Two graphs (apparently) using the graph API (not stream capture) we could realize this in a fashion similar to mentioned stream memops (in the comments, only available in the driver API). Basically, we create a kernel that is waiting on device memory location, to synchronize one graph to another. The graph that is waiting will launch the kernel to synchronize. The other graph signals via a memset node. Here is an example: $ cat t2217.cu #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;cstdio&gt; #include &lt;cstdlib&gt; #define cudaCheckErrors(msg) \\ do { \\ cudaError_t __err = cudaGetLastError(); \\ if (__err != cudaSuccess) { \\ fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\ msg, cudaGetErrorString(__err), \\ __FILE__, __LINE__); \\ fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\ exit(1); \\ } \\ } while (0) __global__ void calc1kernel(float *data, float val, size_t n){ size_t idx = threadIdx.x+blockDim.x*blockIdx.x; while (idx &lt; n){ data[idx] += val; idx += gridDim.x*blockDim.x;} } __global__ void calc2kernel(float *data, float val, size_t n){ size_t idx = threadIdx.x+blockDim.x*blockIdx.x; while (idx &lt; n){ data[idx] *= val; idx += gridDim.x*blockDim.x;} } __global__ void waitkernel(volatile unsigned char *signal, unsigned char val){ while (*signal == val); } // CUDA Graph 1: // calc1kernelnode // | // memsetnode // CUDA Graph 2: // waitkernel // | // calc2kernelnode int main(int argc, char *argv[]){ size_t data_size = 32; cudaStream_t s1, s2; cudaGraph_t g1, g2; float *data, val; unsigned char *sig; // allocate for data on the device cudaMalloc(&amp;data, data_size*sizeof(data[0])); cudaCheckErrors(\"CUDAMalloc failure\"); cudaMalloc(&amp;sig, sizeof(sig[0])); cudaCheckErrors(\"CUDAMalloc failure\"); cudaMemset(sig, 0, sizeof(sig[0])); cudaCheckErrors(\"CUDAMemset failure\"); cudaMemset(data, 0, data_size*sizeof(data[0])); cudaCheckErrors(\"CUDAMemset failure\"); // create the graph cudaGraphCreate(&amp;g1, 0); cudaCheckErrors(\"CUDAGraphCreate failure\"); cudaGraphCreate(&amp;g2, 0); cudaCheckErrors(\"CUDAGraphCreate failure\"); cudaStreamCreate(&amp;s1); cudaCheckErrors(\"CUDAStreamCreate failure\"); cudaStreamCreate(&amp;s2); cudaCheckErrors(\"CUDAStreamCreate failure\"); dim3 grid(1,1,1); dim3 block(1,1,1); cudaGraphNode_t calc1kernelnode, calc2kernelnode, waitkernelnode, memsetnode; // add nodes and their dependencies to the first graph cudaKernelNodeParams kernelNodeParams = {0}; // first add calc1kernelnode, which has no dependencies val = 3.0f; memset(&amp;kernelNodeParams, 0, sizeof(cudaKernelNodeParams)); void *kernelargs[3] = {(void *)&amp;data, (void *)&amp;val, (void *)&amp;data_size}; kernelNodeParams.func = (void *)calc1kernel; kernelNodeParams.gridDim = grid; kernelNodeParams.blockDim = block; kernelNodeParams.sharedMemBytes = 0; kernelNodeParams.kernelParams = kernelargs; kernelNodeParams.extra = NULL; cudaGraphAddKernelNode(&amp;calc1kernelnode, g1, NULL, 0, &amp;kernelNodeParams); cudaCheckErrors(\"CUDAGraphAddKernelNode failure\"); // now add the memsetnode, which has 1 dependency on calc1kernelnode cudaMemsetParams memsetParams = {0}; memset(&amp;memsetParams, 0, sizeof(cudaMemsetParams)); memsetParams.dst = sig; memsetParams.elementSize = 1; memsetParams.height = 1; memsetParams.pitch = 1; memsetParams.value = 1; memsetParams.width = 1; cudaGraphAddMemsetNode(&amp;memsetnode, g1, &amp;calc1kernelnode, 1, &amp;memsetParams); cudaCheckErrors(\"CUDAGraphAddMemsetNode failure\"); // graph 1 is now defined, next step is to instantiate an executable version of it size_t num_nodes = 0; cudaGraphNode_t *nodes1 = NULL; cudaGraphGetNodes(g1, nodes1, &amp;num_nodes); cudaCheckErrors(\"CUDAGraphGetNodes failure\"); printf(\"graph 1 num nodes: %lu\\n\", num_nodes); cudaGraphExec_t graphExec1, graphExec2; cudaGraphInstantiate(&amp;graphExec1, g1, NULL, NULL, 0); cudaCheckErrors(\"CUDAGraphInstantiate failure\"); // add nodes and their dependencies to the second graph // first add waitkernelnode, which has no dependencies unsigned char test = 0; memset(&amp;kernelNodeParams, 0, sizeof(cudaKernelNodeParams)); void *waitkernelargs[2] = {(void *) &amp;sig, (void *) &amp;test }; kernelNodeParams.func = (void *)waitkernel; kernelNodeParams.gridDim = grid; kernelNodeParams.blockDim = block; kernelNodeParams.sharedMemBytes = 0; kernelNodeParams.kernelParams = waitkernelargs; kernelNodeParams.extra = NULL; cudaGraphAddKernelNode(&amp;waitkernelnode, g2, NULL, 0, &amp;kernelNodeParams); cudaCheckErrors(\"CUDAGraphAddKernelNode failure\"); // now add the calc2kernelnode, which has 1 dependency on waitkernelnode memset(&amp;kernelNodeParams, 0, sizeof(cudaKernelNodeParams)); kernelNodeParams.func = (void *)calc2kernel; kernelNodeParams.gridDim = grid; kernelNodeParams.blockDim = block; kernelNodeParams.sharedMemBytes = 0; kernelNodeParams.kernelParams = kernelargs; kernelNodeParams.extra = NULL; cudaGraphAddKernelNode(&amp;calc2kernelnode, g2, &amp;waitkernelnode, 1, &amp;kernelNodeParams); cudaCheckErrors(\"CUDAGraphAddKernelNode failure\"); // graph 2 is now defined, next step is to instantiate an executable version of it cudaGraphNode_t *nodes2 = NULL; cudaGraphGetNodes(g2, nodes2, &amp;num_nodes); cudaCheckErrors(\"CUDAGraphGetNodes failure\"); printf(\"graph 2 num nodes: %lu\\n\", num_nodes); cudaGraphInstantiate(&amp;graphExec2, g2, NULL, NULL, 0); cudaCheckErrors(\"CUDAGraphInstantiate failure\"); // now launch the graphs cudaGraphLaunch(graphExec2, s2); cudaCheckErrors(\"CUDAGraphLaunch failure\"); cudaGraphLaunch(graphExec1, s1); cudaCheckErrors(\"CUDAGraphLaunch failure\"); cudaStreamSynchronize(s1); cudaCheckErrors(\"graph execution failure\"); cudaStreamSynchronize(s2); cudaCheckErrors(\"graph execution failure\"); float *result = new float[data_size]; cudaMemcpy(result, data, data_size*sizeof(float), cudaMemcpyDeviceToHost); std::cout &lt;&lt; \"result[0] = \" &lt;&lt; result[0] &lt;&lt; std::endl; // clean up cudaFree(data); cudaStreamDestroy(s1); cudaGraphDestroy(g1); cudaGraphExecDestroy(graphExec1); cudaStreamDestroy(s2); cudaGraphDestroy(g2); cudaGraphExecDestroy(graphExec2); } $ nvcc -o t2217 t2217.cu $ ./t2217 graph 1 num nodes: 2 graph 2 num nodes: 2 result[0] = 9 $ The result of 9 indicates that even though graph 2 was launched first, it successfully waited until the synchronization point in graph 1, before it allowed its calc kernel to run. The given example (in the question) shows use of the runtime API, as does my answer. If you want to use the driver API, as already indicated in the comments, it should be possible to do this directly via batched memops using cuGraphAddBatchMemOpNode. A memset node, or similar, is also still needed. This kind of interlock is something that can give rise to hangs and deadlocks if used improperly. Note the various warnings given: Warning: Improper use of this API may deadlock the application. Synchronization ordering established through this API is not visible to CUDA. CUDA tasks that are (even indirectly) ordered by this API should also have that order expressed with CUDA-visible dependencies such as events. ... Regarding your EDIT 1: If I change this: host_buf = (char *) malloc(BUF_SIZE * sizeof(char)); to this: CUDA_CHKERR(cudaHostAlloc(&amp;host_buf, BUF_SIZE*sizeof(char), cudaHostAllocDefault)); your code runs correctly for me. In CUDA, in order for a D-&gt;H or H-&gt;D transfer to be (guaranteed to be) asynchronous and non-blocking, the host buffer must be a pinned buffer. When we apply this to graphs, the requirement is more stringent: General requirements: Memcpy nodes: Only copies involving device memory and/or pinned device-mapped host memory are permitted. (emphasis added) memory allocated with malloc is neither device memory, nor is it pinned device-mapped host memory."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Suppose I call cuEventRecord(0, my_event_handle). cuEventRecord() requires the stream and the event to belong to the same context. Now, one can interpret the 0 as \"the default stream in the appropriate context\" - the requirements are satisfied and this should work; but one can also interpret it as \"the default stream in the current context\" - in which case if the current context is not the event's context - this should fail. Or it may all just be undefined/inconsistent behavior. My question: Is cuEventRecord() guaranteed to prefer one interpretation over the other?",
        "answers": [
            [
                "My personal tinkering suggests that the CUDA driver expects the current context to be the event and the stream's context. Perhaps it even expects that for any stream, not just the default one. Try this program: #include &lt;cuda/api.hpp&gt; #include &lt;iostream&gt; #include &lt;stdexcept&gt; int main() { cuda::initialize_driver(); auto pc_0 = cuda::device::primary_context::detail_::obtain_and_increase_refcount(0); auto pc_1 = cuda::device::primary_context::detail_::obtain_and_increase_refcount(1); cuda::context::current::detail_::push(pc_1); CUevent eh = cuda::event::detail_::create_raw_in_current_context(0); cuda::context::current::pop(); // At this point, the context stack is empty cuda::context::current::detail_::push(pc_0); CUstream default_stream_handle = nullptr; cuda::event::detail_::enqueue_in_current_context(default_stream_handle, eh); } with this commit of my cuda-api-wrappers library to see for yourself; if you replace pc_0 with pc_1 - it all works."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Suppose I have a struct: typedef enum {ON_CPU,ON_GPU,ON_BOTH} memLocation; typedef struct foo *foo; struct foo { cudaEvent_t event; float *deviceArray; float *hostArray; memLocation arrayLocation; }; a function: void copyArrayFromCPUToGPUAsync(foo bar, cudaStream_t stream) { cudaStreamWaitEvent(stream, bar-&gt;event); if (bar-&gt;arrayLocation == ON_CPU) { // ON_CPU means !ON_GPU and !ON_BOTH cudaMemcpyAsync(cudaMemcpyHostToDevice, stream); bar-&gt;arrayLocation = ON_BOTH; } cudaEventRecord(bar-&gt;event, stream); } void doWorkOnGPUAsync(foo bar, cudaStream_t stream) { cudaStreamWaitEvent(stream, bar-&gt;event); // do async work cudaEventRecord(bar-&gt;event, stream); } And the following scenario (with a lion, witch, and wardrobe fitting in somewhere as well): // stream1, stream2, and stream3 have no prior work // assume bar-&gt;arrayLocation = ON_GPU doWorkOnGPUAsync(bar, stream1); copyArrayFromCPUToGPUAsync(bar, stream2); // A no-op doWorkOnGPUAsync(bar, stream3); Is the above safe? I.e. will stream2 still wait on stream1 to finish its \"work\" if it itself does no work? And will the resulting recorded cudaEvent reflect this, such that stream3 will not start until stream1 finishes?",
        "answers": [
            [
                "This should be safe. There is no mention anywhere (that I know) of some kind \"event cancellation\" due to lack of other work between a wait-on-event and the recording of another event. And it doesn't matter that you're re-using the same event object in the cudaEventRecord() call, since as the Runtime API docs say: cudaEventRecord() can be called multiple times on the same event and will overwrite the previously captured state. Other APIs such as cudaStreamWaitEvent() use the most recently captured state at the time of the API call, and are not affected by later calls to cudaEventRecord(). Additional notes: With your apparent use-case, you may also want to consider the possibility of using managed memory instead of manually copying back and forth. You should check for the success of your various operations, not just assume they succeeded."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a sparse triangular solver that works with 4 Tesla V100 GPUs. I completed implementation and all things work well in terms of accuracy. However, I am using a CPU timer to calculate elapsed time. I know that the CPU timer is not the perfect choice for calculating elapsed time, since I can use CUDA Events. But the thing is, I do not know how to implement CUDA Events for multi GPU. As I saw from NVIDIA tutorials, they use events for inter-GPU synchronization, i.e. waiting for other GPUs to finish dependencies. Anyway, I define events like; cudaEvent_t start_events[num_gpus] cudaEvent_t end_events[num_gpus] I can also initialize these events in a loop by setting the current GPU iteratively. And my kernel execution is like; for(int i = 0; i &lt; num_gpus; i++) { CUDA_FUNC_CALL(cudaSetDevice(i)); kernel&lt;&lt;&lt;&gt;&gt;&gt;() } for(int i = 0; i &lt; num_devices; i++) { CUDA_FUNC_CALL(cudaSetDevice(i)); CUDA_FUNC_CALL(cudaDeviceSynchronize()); } My question is, how should I use these events to record elapsed times for each GPU separately?",
        "answers": [
            [
                "You need to create two events per GPU, and record the events before and after the kernel call on each GPU. It could look something like this: cudaEvent_t start_events[num_gpus]; cudaEvent_t end_events[num_gpus]; for(int i = 0; i &lt; num_gpus; i++) { CUDA_FUNC_CALL(cudaSetDevice(i)); CUDA_FUNC_CALL(cudaEventCreate(&amp;start_events[i])); CUDA_FUNC_CALL(cudaEventCreate(&amp;end_events[i])); } for(int i = 0; i &lt; num_gpus; i++) { CUDA_FUNC_CALL(cudaSetDevice(i)); // In cudaEventRecord, ommit stream or set it to 0 to record // in the default stream. It must be the same stream as // where the kernel is launched. CUDA_FUNC_CALL(cudaEventRecord(start_events[i], stream)); kernel&lt;&lt;&lt;&gt;&gt;&gt;() CUDA_FUNC_CALL(cudaEventRecord(end_events[i], stream)); } for(int i = 0; i &lt; num_devices; i++) { CUDA_FUNC_CALL(cudaSetDevice(i)); CUDA_FUNC_CALL(cudaDeviceSynchronize()); } for(int i = 0; i &lt; num_devices; i++) { //the end_event must have happened to get a valid duration //In this example, this is true because of previous device synchronization float time_in_ms; CUDA_FUNC_CALL(cudaEventElapsedTime(&amp;time_in_ms, start_events[i], end_events[i])); printf(\"Elapsed time on device %d: %f ms\\n\", i, time_in_ms) } for(int i = 0; i &lt; num_gpus; i++) { CUDA_FUNC_CALL(cudaSetDevice(i)); CUDA_FUNC_CALL(cudaEventDestroy(start_events[i])); CUDA_FUNC_CALL(cudaEventDestroy(end_events[i])); }"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "This question is about notion of a CUDA stream (Stream) and the apparent anomaly with CUDA events (Event) recorded on a stream. Consider the following code demonstrating this anamoly, cudaEventRecord(eventStart, stream1) kernel1&lt;&lt;&lt;..., stream1&gt;&gt;&gt;(...) cudaEventRecord(eventBetween, stream1) kernel2&lt;&lt;&lt;..., stream1&gt;&gt;&gt;(...) cudaEventRecord(eventEnd, stream1) Since all work on a Stream is sequential, the following sequence of events should've happened, \"eventStart\" is recorded to be complete kernel1 runs and completes \"eventBetween\" is recorded to have completed kernel2 runs and completes \"eventEnd\" is recorded to have completed After synchronizing the host thread with the device, the time taken between eventStart and eventBetween ( measured by cudaEventElapsedTime ) should be run-time of kernel1 and the time taken between eventBetween and eventEnd should be run-time of kernel2. But according to NVIDIA's documentation on cudaStreamWaitEvent, Makes all future work submitted to stream wait until event reports completion before beginning execution. and blogpost, You can also synchronize operations within a single stream on a specific event using cudaStreamWaitEvent(event) Events needn't report completion before all work that's scheduled after recording the Event on the same Stream. This comes as a surprise since all work scheduled on a Stream is supposed to run in a sequential fashion. Can someone clarify this asynchronous behavior of Events within a Stream ?",
        "answers": [
            [
                "It appears the cause of your confusion is conflating host-stream synchronisation and stream-stream synchronisation. In short: Streams are FIFO, and all operations within a stream are synchronous with respect to one another. The documentation you are referring to is describing a use case where the programmer wishes to synchronise between events in different streams. The blog post you refer to is discussing synchronisation between the calling host thread and an event enqueued in a stream. There are three levels of host-device synchronisation granularity available -- the whole device, a whole stream, or an event within a stream. The blog is describing the latter. There is no contradiction between any of the sources you cited."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am new to CUDA and got a little confused with cudaEvent. I now have a code sample that goes as follows: float elapsedTime; cudaEvent_t start, stop; CUDA_ERR_CHECK(cudaEventCreate(&amp;start)); CUDA_ERR_CHECK(cudaEventCreate(&amp;stop)); CUDA_ERR_CHECK(cudaEventRecord(start)); // Kernel functions go here ... CUDA_ERR_CHECK(cudaEventRecord(stop)); CUDA_ERR_CHECK(cudaEventSynchronize(stop)); CUDA_ERR_CHECK(cudaEventElapsedTime(&amp;elapsedTime, start, stop)); CUDA_ERR_CHECK(cudaDeviceSynchronize()); I have two questions regarding this code: 1.Is the last cudaDeviceSynchronize necessary? Because according to the documentation for cudaEventSynchronize, its functionality is Wait until the completion of all device work preceding the most recent call to cudaEventRecord(). So given that we have already called cudaEventSynchronize(stop), do we need to call cudaDeviceSynchronize once again? 2.How different is the above code compared to the following implementation: #include &lt;chrono&gt; auto tic = std::chrono::system_clock::now(); // Kernel functions go here ... CUDA_ERR_CHECK(cudaDeviceSynchronize()); auto toc = std::chrono::system_clock:now(); float elapsedTime = std::chrono::duration_cast &lt; std::chrono::milliseconds &gt; (toc - tic).count() * 1.0;",
        "answers": [
            [
                "Just to flesh out comments so that this question has an answer and will fall off the unanswered queue: No, the cudaDeviceSynchronize() call is not necessary. In fact, in many cases where asynchronous API calls are being used in multiple streams, it is incorrect to use a global scope synchronization call, because you will break the features of event timers which allow accurate timing of operations in streams. They are completely different. One is using host side timing, the other is using device driver timing. In the simplest cases, the time measured by both will be comparable. However, in the host side timing version, if you put a host CPU operation which consumes a significant amount of time in the host timing section, your measurement of time will not reflect the GPU time used when the GPU operations take less time than the host operations."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "When looking at the name of the performance counters in NVIDIA Fermi architecture (the file Compute_profiler.txt in the doc folder of cuda), I noticed that for L2 cache misses, there are two performance counters, l2_subp0_read_sector_misses and l2_subp1_read_sector_misses. They said that these are for two slices of L2. Why do they have two slices of L2? Is there any relation with the Streaming Multi-processor architecture? What would be the effect of this division to the performance? Thanks",
        "answers": [
            [
                "I don't think there is any direct relation with the streaming multiprocessor. I just think that slice is equivalent of bank memory. Just sum the values of the two to get the \"total\" L2 read misses."
            ],
            [
                "The CUDA C Programming Guide describes the architecture of the multiprocessor. The document states that each Fermi multiprocessor has two warp schedulers. I assume that the L2 cache is split to allow concurrent caching. I haven't looked at L2 read misses for the Kepler architecture, but Kepler multiprocessors have four warp processors. So, this assumption may be validated if there are four performance counters reported for Kepler compilation."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "cudaEventRecord takes an event ID and a stream ID as parameters. The Runtime API reference does not say whether the stream is required to be associated with the current device - and I can't test whether that's the case since I only have one GPU at most on any system I have access to right now. Assuming that it must be a stream on the current device: what happens if it gets a stream on another device? Assuming that it can be a stream on any device: What happens when it gets the (current device's) default stream's ID? After all, all devices' default streams have the same (null) ID? Is there any difference in behavior based on whether the stream's device is current or not?",
        "answers": [
            [
                "Combining the information from @Talonmies' answer and the Stream and Event Behavior section of the CUDA C Programming Guide which @RobertCrovella linked to in his comment. Must the stream be associated with the current device? No, it can be any device. However, event recording does require that the stream and the event be associated with the same device. Is there any difference in behavior based on whether the stream's device is current or not? Typically, no, but... What happens when it gets the (current device's) default stream's ID? ... the Default stream is an exception to that rule. Since (each device's own) default stream has the same ID, passing the null ID to cudaEventRecord means that it will check what device is currently set to determine which stream to record the event on (and this needs to be the same device the event is associated with)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Is it possible to synchronize two CUDA streams without blocking the host? I know there's cudaStreamWaitEvent, which is non-blocking. But what about the creation and destruction of the events using cudaEventCreate and cudaEventDestroy. The documentation for cudaEventDestroy says: In case event has been recorded but has not yet been completed when cudaEventDestroy() is called, the function will return immediately and the resources associated with event will be released automatically once the device has completed event. What I don't understand here is what the difference is between a recorded event and a completed event. Also this seems to imply that the call is blocking if the event has not yet been recorded. Anyone who can shed some light on this?",
        "answers": [
            [
                "You're on the right track by using cudaStreamWaitEvent. Creating events does carry some cost, but they can be created during your application start-up to prevent the creation time from being costly during your GPU routines. An event is recorded when you you put the event into a stream. It is completed after all activity that was put into the stream before the event has completed. Recording the event basically puts a marker into your stream, which is the thing that enables cudaStreamWaitEvent to stop forward progress on the stream until the event has completed."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "Is there any way to fire an event (for benchmarking purposes, similar to cudaEvents in the CPU code) from a device kernel in CUDA? E.g. suppose I would like to measure the time passed from kernel start to the first thread ever that starts a computation and the time passed from the last thread that leaves the computation to the CPU return. Can I do that?",
        "answers": [
            [
                "The device runtime API (used with dynamic parallelism) does have limited stream and events support, but event timing is not supported. So, no you can't do that."
            ],
            [
                "An ugly workaround would be writing to some managed-memory location, and having a host-side thread poll it and fire the event when the value changes."
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "I have a processing loop on the host, where I record an event in a GPU stream. Then another stream waits for that event (waits for event's state \"set\" or \"true\"). Will this function (cudaStreamWaitEvent) unset this event (so, switching it to \"unset\" or \"false\")? If not, what CUDA function I can use to unset this event?",
        "answers": [
            [
                "This sounds very much like an XY question. You might be better off describing at a higher level what it is you are trying to accomplish, or what problem you are facing or think you are facing. cudaStreamWaitEvent does not \"unset\" an event. When the event is encountered in the stream, then cudaStreamWaitEvent will unblock, and any subsequent calls to cudaStreamWaitEvent on the same event will immediately unblock (assuming no cudaEventRecord has again been issued for that event). This behavior is easy to prove with a trivial code sample. The function that \"unsets\" a cudaEvent is cudaEventRecord(). Any cudaStreamWaitEvent calls issued after that event gets recorded will wait again, until it is encountered again. You may want to read the runtime API documentation for cudaEventRecord and cudaStreamWaitEvent. Note the following excerpts: cudaEventRecord: If cudaEventRecord() has previously been called on event, then this call will overwrite any existing state in event. Any subsequent calls which examine the status of event will only examine the completion of this most recent call to cudaEventRecord(). cudaStreamWaitEvent: The stream stream will wait only for the completion of the most recent host call to cudaEventRecord() on event."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to implement the following kind of pipeline on the GPU with CUDA: I have four streams with each a Host2Device copy, a kernel call and a Device2Host copy. However, the kernel calls have to wait for the Host2Device copy of the next stream to finish. I intended to use cudaStreamWaitEvent for synchronization. However, according to the documentation, this only works if cudaEventRecord has been called earlier for the according event. And this is not the case in this scenario. The streams are managed by separate CPU threads which basically look as follows: Do some work ... cudaMemcpyAsync H2D cudaEventRecord (event_copy_complete[current_stream]) cudaStreamWaitEvent (event_copy_complete[next_stream]) call kernel on current stream cudaMemcpyAsync D2H Do some work ... The CPU threads are managed to start the streams in the correct order. Thus, cudaStreamWaitEvent for the copy complete event of stream 1 is called (in stream 0) before cudaEventRecord of that very event (in stream 1). This results in a functional no-op. I have the feeling that events can't be used this way. Is there another way to achieve the desired synchronization? Btw, I can't just reverse the stream order because there are some more dependencies. API call order As requested, here is the order in which CUDA calls are issued: //all on stream 0 cpy H2D cudaEventRecord (event_copy_complete[0]) cudaStreamWaitEvent (event_copy_complete[1]) K&lt;&lt;&lt; &gt;&gt;&gt; cpy D2H //all on stream 1 cpy H2D cudaEventRecord (event_copy_complete[1]) cudaStreamWaitEvent (event_copy_complete[2]) K&lt;&lt;&lt; &gt;&gt;&gt; cpy D2H //all on stream 2 cpy H2D cudaEventRecord (event_copy_complete[2]) cudaStreamWaitEvent (event_copy_complete[3]) K&lt;&lt;&lt; &gt;&gt;&gt; cpy D2H ... As can be seen, the call to cudaStreamWaitEvent is always earlier than the call to cudaEventRecord.",
        "answers": [
            [
                "If at all possible, you should be dispatching all this GPU work from a single CPU thread. That way, (at the risk of stating the obvious), the order in which the API calls are performed can be inferred from the order in which they appear in your code. Because the cudaEventRecord() and cudaStreamWaitEvent() calls both operate on progress values associated with the CUDA context, the exact order of API calls is important. cudaEventRecord() records the current progress value, then increments it; cudaStreamWaitEvent() emits a command for the current GPU to wait on the event's current progress value. (That's why if you reverse the order of the calls, the wait becomes an effective no-op.) If the API calls are being made from different threads, you will have to do a lot of thread synchronization to generate the desired result, which also negatively impacts performance. In fact, if you need the multiple CPU threads for performance reasons, you may want to restructure your code to delegate CUDA calls onto a single CPU thread to enforce the ordering."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am attempting to write a small demo program that has two cuda streams progressing and, governed by events, waiting for each other. So far this program looks like this: // event.cu #include &lt;iostream&gt; #include &lt;cstdio&gt; #include &lt;cuda_runtime.h&gt; #include &lt;cuda.h&gt; using namespace std; __global__ void k_A1() { printf(\"\\tHi! I am Kernel A1.\\n\"); } __global__ void k_B1() { printf(\"\\tHi! I am Kernel B1.\\n\"); } __global__ void k_A2() { printf(\"\\tHi! I am Kernel A2.\\n\"); } __global__ void k_B2() { printf(\"\\tHi! I am Kernel B2.\\n\"); } int main() { cudaStream_t streamA, streamB; cudaEvent_t halfA, halfB; cudaStreamCreate(&amp;streamA); cudaStreamCreate(&amp;streamB); cudaEventCreate(&amp;halfA); cudaEventCreate(&amp;halfB); cout &lt;&lt; \"Here is the plan:\" &lt;&lt; endl &lt;&lt; \"Stream A: A1, launch 'HalfA', wait for 'HalfB', A2.\" &lt;&lt; endl &lt;&lt; \"Stream B: Wait for 'HalfA', B1, launch 'HalfB', B2.\" &lt;&lt; endl &lt;&lt; \"I would expect: A1,B1, (A2 and B2 running concurrently).\" &lt;&lt; endl; k_A1&lt;&lt;&lt;1,1,0,streamA&gt;&gt;&gt;(); // A1! cudaEventRecord(halfA,streamA); // StreamA triggers halfA! cudaStreamWaitEvent(streamA,halfB,0); // StreamA waits for halfB. k_A2&lt;&lt;&lt;1,1,0,streamA&gt;&gt;&gt;(); // A2! cudaStreamWaitEvent(streamB,halfA,0); // StreamB waits, for halfA. k_B1&lt;&lt;&lt;1,1,0,streamB&gt;&gt;&gt;(); // B1! cudaEventRecord(halfB,streamB); // StreamB triggers halfB! k_B2&lt;&lt;&lt;1,1,0,streamB&gt;&gt;&gt;(); // B2! cudaEventDestroy(halfB); cudaEventDestroy(halfA); cudaStreamDestroy(streamB); cudaStreamDestroy(streamA); cout &lt;&lt; \"All has been started. Synchronize!\" &lt;&lt; endl; cudaDeviceSynchronize(); return 0; } My grasp of CUDA streams is the following: A stream is a kind of list to which I can add tasks. These tasks are tackled in series. So in my program I can rest assured that streamA would in order Call kernel k_A1 Trigger halfA Wait for someone to trigger halfB Call kernel k_A2 and streamB would Wait for someone to trigger halfA Call kernel k_B1 Trigger halfB Call kernel k_B2 Normally both streams might run asynchronous to each other. However, I would like to block streamB until A1 is done and then block streamA until B1 is done. This appears not to be as simple. On my Ubuntu with Tesla M2090 (CC 2.0) the output of nvcc -arch=sm_20 event.cu &amp;&amp; ./a.out is Here is the plan: Stream A: A1, launch 'HalfA', wait for 'HalfB', A2. Stream B: Wait for 'HalfA', B1, launch 'HalfB', B2. I would expect: A1,B1, (A2 and B2 running concurrently). All has been started. Synchronize! Hi! I am Kernel A1. Hi! I am Kernel A2. Hi! I am Kernel B1. Hi! I am Kernel B2. And I really would have expected B1 to be completed before the cudaEventRecord(halfB,streamB). Nevertheless stream A obviously does not wait for the completion of B1 and so not for the recording of halfB. What's more: If I altogether delete the cudaEventRecord commands I would expect the program to lock down on the cudaStreamWait commands. But it does not and produces the same output. What am I overlooking here?",
        "answers": [
            [
                "I think this is because \"cudaStreamWaitEvent(streamA,halfB,0); \" was called before \"halfB\" was recorded (cudaEventRecord(halfB,streamB);). It's likely that the cudaStreamWaitEvent call was searching for the closed \"halfB\" prior to it; since it was not found, it just quietly moved forward. See the following documentation: The stream stream will wait only for the completion of the most recent host call to cudaEventRecord() on event. Once this call has returned, any functions (including cudaEventRecord() and cudaEventDestroy()) may be called on event again, and the subsequent calls will not have any effect on stream. I could not find a solution if you have to do a depth-first coding; however, the following code may lead to what you want: k_A1&lt;&lt;&lt;1,1,0,streamA&gt;&gt;&gt;(d); // A1! cudaEventRecord(halfA,streamA); // StreamA triggers halfA! cudaStreamWaitEvent(streamB,halfA,0); // StreamB waits, for halfA. k_B1&lt;&lt;&lt;1,1,0,streamB&gt;&gt;&gt;(d); // B1! cudaEventRecord(halfB,streamB); // StreamB triggers halfB! cudaStreamWaitEvent(streamA,halfB,0); // StreamA waits for halfB. k_A2&lt;&lt;&lt;1,1,0,streamA&gt;&gt;&gt;(d); // A2! k_B2&lt;&lt;&lt;1,1,0,streamB&gt;&gt;&gt;(d); // B2! which is confirmed by the profiling: Note that I changed the kernel interfaces."
            ],
            [
                "From the docs: If cudaEventRecord() has not been called on event, this call acts as if the record has already completed, and so is a functional no-op. https://www.cs.cmu.edu/afs/cs/academic/class/15668-s11/www/cuda-doc/html/group__CUDART__STREAM_gfe68d207dc965685d92d3f03d77b0876.html#gfe68d207dc965685d92d3f03d77b0876 So we need to sort these lines so that the record is in the program before the eventwait. That is, for the stream of the event wait to be forced to run before the record, the record must be earlier in the code! Here's the original code: k_A1&lt;&lt;&lt;1,1,0,streamA&gt;&gt;&gt;(); // A1! cudaEventRecord(halfA,streamA); // StreamA triggers halfA! cudaStreamWaitEvent(streamA,halfB,0); // StreamA waits for halfB. k_A2&lt;&lt;&lt;1,1,0,streamA&gt;&gt;&gt;(); // A2! cudaStreamWaitEvent(streamB,halfA,0); // StreamB waits, for halfA. k_B1&lt;&lt;&lt;1,1,0,streamB&gt;&gt;&gt;(); // B1! cudaEventRecord(halfB,streamB); // StreamB triggers halfB! k_B2&lt;&lt;&lt;1,1,0,streamB&gt;&gt;&gt;(); // B2! We see that the record of halfB is called on the second to last line but the wait is called above, on the third line. No good. So we re-order. The first thing on streamB is that wait and our only requirement is that is happen after the record. So that line can move up to be the third line. Likewise, the k_B1 can follow it directly. And then the cudaEventRecord for halfB can be moved up before the waitevent. Hmm, does this prevent deadlock I wonder? k_A1&lt;&lt;&lt;1,1,0,streamA&gt;&gt;&gt;(); // A1! cudaEventRecord(halfA,streamA); // StreamA triggers halfA! cudaStreamWaitEvent(streamB,halfA,0); // StreamB waits, for halfA. k_B1&lt;&lt;&lt;1,1,0,streamB&gt;&gt;&gt;(); // B1! cudaEventRecord(halfB,streamB); // StreamB triggers halfB! cudaStreamWaitEvent(streamA,halfB,0); // StreamA waits for halfB. k_A2&lt;&lt;&lt;1,1,0,streamA&gt;&gt;&gt;(); // A2! k_B2&lt;&lt;&lt;1,1,0,streamB&gt;&gt;&gt;(); // B2!"
            ]
        ],
        "votes": [
            8.0000001,
            1.0000001
        ]
    },
    {
        "question": "I understand that cudaEventSynchronize will block the host until the event has been triggered. However, what about cudaStreamWaitEvent? Will cudaStreamWaitEvent block only the specified stream whereas the host will proceed, or the host will be blocked as well?",
        "answers": [
            [
                "It only blocks the specified stream processing, at the point at which the WaitEvent was inserted into the stream queue. The host processing will continue. You can use cudaStreamSynchronize() to force the host to wait on completion of the processing in a particular stream."
            ]
        ],
        "votes": [
            7.0000001
        ]
    }
]