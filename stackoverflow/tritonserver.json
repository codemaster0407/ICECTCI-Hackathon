[
    {
        "question": "I have been looking examples and ran into this from aws, https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-triton/ensemble/sentence-transformer-trt/examples/ensemble_hf/bert-trt/config.pbtxt. based on this example , we need to define the input and output , data types for those input and output. the example is not clear on what the dims ( probably dimensions) represent , is it number of elements in an array of inputs ? also , what Is max_batch_size ? and at the bottom , we have to specify instance group and kind is set to KIND_GPU, I assume if we are using a cpu based instance , we can change this to cpu. do we need to specify , how many cpu we want to use? name: \"bert-trt\" platform: \"tensorrt_plan\" max_batch_size: 16 input [ { name: \"token_ids\" data_type: TYPE_INT32 dims: [128] }... ] output [ { name: \"output\" data_type: TYPE_FP32 dims: [128, 384] }... ] instance_group [ { kind: KIND_GPU } ] I have tested the given example , but if we want to use a text based input and do tokenization in the server, how does this config.pbtxt file look like?",
        "answers": [],
        "votes": []
    },
    {
        "question": "The problem I am trying to deploy a Machine translation Model from the M2M family in a production setting using the Triton server. What I have tried so far. I have exported my model to onnx format and quantized them, and I have the encoder, decoder, and decoder with past models on the onnx format. I would like to find the optimal way to deploy the model in production. I was thinking about an ensemble model with this pipeline: Raw Text -&gt; Preprocessing -&gt; Encoder -&gt; Decoder -&gt; Probability for next Token \u2191 \u2193 Top k Tokens &lt;- Beam Search I already have an ensemble model with the tokenizer and the encoder part. It uses the encoder's quantized version leverage the onnxruntime on triton server and returns the last input state of the model. What I want I want to implement the decoder as a Python server on Triton that takes the output of the encoder and other decoder input and starts the generation process using beam search. Load the decoder_quantized, and then call generate on the decoder and return the generated token. How can I achieve this? I have looked at the code for the generate method on hugginface but too much and I got lost. Does anyone have an easy way to implement this? What I can do. I can also have a TritonModel using a python runtime and make it load the model with the encoder and decoder and then perform the generation. But with this, I will use the advantage of the onnxruntime on triton server. My question is this optimal/fast compared to the approach I want to describe? This seems to be the approach used in this repository which seems to work.",
        "answers": [],
        "votes": []
    },
    {
        "question": "setting up a python backend to test out multi model endpoints in aws sagemaker, came up with minimal client code to invoke/process the request/response for the inference with multi model endpoint. the example uses tritonclient.http , see below inputs.append(httpclient.InferInput('input_ids', [token_ids.shape[0], 512], \"INT32\")) can someone point me to documentation on this library/function. i would like to understand what InferInput does (line above) . and also , sorry for the basic question on numpy, once the prediction is generated ,we call precition.as_numpy, i am assuming this converts into some array that numpy library represents. import numpy as np import tritonclient.http as httpclient from utils import preprocess, postprocess triton_client = httpclient.InferenceServerClient(url='localhost:8000') def text_process(text): return tokenizer.encode(text, maxlen=512) if __name__ == \"__main__\": text_list = ['some text here'] token_ids= text_process(text_list) inputs = [] inputs.append(httpclient.InferInput('input_ids', [token_ids.shape[0], 512], \"INT32\")) inputs[0].set_data_from_numpy(token_ids.astype(np.int32), binary_data=False) outputs = [] outputs.append(httpclient.InferRequestedOutput('output', binary_data=False)) prediction = triton_client.infer('sentence_classification', inputs=inputs, outputs=outputs) final_output = prediction.as_numpy(\"output\")",
        "answers": [],
        "votes": []
    },
    {
        "question": "I run nvcr.io/nvidia/tritonserver:23.01-py3 docker image with the following command docker run --gpus=0 --rm -it --net=host -v ${PWD}/models:/models nvcr.io/nvidia/tritonserver:23.01-py3 tritonserver --model-repository=/models i was compiled yolov8n.pt to engine format, by nvidia-tensorrt 8.4.1.5 and in model folder: . \u251c\u2500\u2500 yolov8 \u2502 \u251c\u2500\u2500 1 \u2502 \u2502 \u2514\u2500\u2500 model.plan \u2502 \u2514\u2500\u2500 config.pbtxt and i received this: I0629 08:04:00.307095 1 tensorrt.cc:211] TRITONBACKEND_ModelInitialize: yolov8 (version 1) I0629 08:04:01.044353 1 logging.cc:49] Loaded engine size: 169 MiB E0629 08:04:01.239643 1 logging.cc:43] 1: [stdArchiveReader.cpp::StdArchiveReader::32] Error Code 1: Serialization (Serialization assertion magicTagRead == kMAGIC_TAG failed.Magic tag does not match) E0629 08:04:01.250591 1 logging.cc:43] 4: [runtime.cpp::deserializeCudaEngine::66] Error Code 4: Internal Error (Engine deserialization failed.) I0629 08:04:01.273851 1 tensorrt.cc:237] TRITONBACKEND_ModelFinalize: delete model state humm, may be model.plan was broke when i try to compile it, how to solve that problem.",
        "answers": [],
        "votes": []
    },
    {
        "question": "My model accepts data in the shape(1, 32, 32, 3), I am looking for a way to pass the data using np.array from numpy. Any help on this will be appreciated please",
        "answers": [
            [
                "Your data right now is a one-dimensional array you want to convert your input data to a 4-dimensional array? Because a 4d array is a matrice of matrices the data input you have right now looks like a singular index within a matrix of the bigger matrix of matrixes. to create a 4d array it just: a = np.random.rand(1, 32, 32, 3) to get something better we need the structure of your data"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "My triton model config.pbtxt file looks like below. How can I pass inputs and outputs using tritonclient and perform an infer request. name: \u201ccifar10\u201d platform: \u201ctensorflow_savedmodel\u201d max_batch_size: 10000 input [ { name: \u201cinput_1\u201d data_type: TYPE_FP32 dims: [ 32, 32, 3 ] } ] output [ { name: \u201cfc10\u201d data_type: TYPE_FP32 dims: [ 10 ] } ] Any help is appreciated as I am very new to using tritonclient python package please",
        "answers": [
            [
                "See example for your case, but it was not tested as I don't have script which creates your model and hence minor issues might arise with some explicit exceptions. You can use this code snippet as reference and baseline for you solution. Before executing - make sure you have running triton server on localhost port 8000 or change URI address appropriately. import tritonclient.http as httpclient import argparse import numpy as np def test_infer(model_name, input_data, output_data): r = triton_client.infer( model_name=model_name, inputs=input_data, outputs=output_data ) return r payload_input = [] payload_output = [] payload_input.append(httpclient.InferInput(\"input_1\", [32, 32, 3], \"FP32\")) print(\"-----------fill inputs with data------------\") input1_data = np.full(shape=(32, 32, 3), fill_value=\"trousers\", dtype=np.float32) payload_input[0].set_data_from_numpy(input1_data, binary_data=False) if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument( '-u', '--url', default='localhost:8000', help='Endpoint URL' ) params = parser.parse_args() # setup client: triton_client = httpclient.InferenceServerClient(params.url, verbose=True) # send request: results = test_infer(\"\u201ccifar10\u201d\", payload_input, payload_output) print(\"----------_RESPONSE_-----------\") print(results.get_response()) print(results.as_numpy(\"\u201cfc10\u201d\"))"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to deploy XGBoost model on kserve. I deployed it on default serving runtime. But I want to try it on kserve-tritonserver. I know kserve told me kserve-tritonserver supports Tensorflow, ONNX, PyTorch, TensorRT. And NVIDIA said triton inference server supported XGBoost model. so.. is there a way to deploy kserve inference service using XGBoost model on kserve-tritonserver? k apply -n kserve-test -f - &lt;&lt;EOF apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"digits-classification-xgboost\" spec: predictor: model: modelFormat: name: xgboost protocolVersion: v2 storageUri: \"s3://.../digits_classification_model\" runtime: kserve-tritonserver EOF I tried it. But I got this description Status: Model Status: Last Failure Info: Message: Specified runtime does not support specified framework/version Reason: NoSupportingRuntime States: Active Model State: Target Model State: FailedToLoad Transition Status: InvalidSpec Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning InternalError 65s (x19 over 9m9s) v1beta1Controllers specified runtime kserve-tritonserver does not support specified framework/version",
        "answers": [],
        "votes": []
    },
    {
        "question": "examples here (https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-triton/nlp_bert/triton_nlp_bert.ipynb) show , that instead of sending text and tokenizing text in the server, it is done in the client side and tokenized input is send to triton server. is it possible to pass in text directly for the nlp Bert model like in the example and tokenize the test in the server? the default model.py only accepts a list of pb_utils.InferenceRequest as input. I assume , this will need to overridden or customized to accept the types. model.py class TritonPythonModel: \"\"\"Your Python model must use the same class name. Every Python model that is created must have \"TritonPythonModel\" as the class name. \"\"\" def initialize(self, args): def execute(self, requests): \"\"\"`execute` must be implemented in every Python model. `execute` function receives a list of pb_utils.InferenceRequest as the only argument. This function is called when an inference is requested for this model. Depending on the batching configuration (e.g. Dynamic Batching) used, `requests` may contain multiple requests. Every Python model, must create one pb_utils.InferenceResponse for every pb_utils.InferenceRequest in `requests`. If there is an error, you can set the error argument when creating a pb_utils.InferenceResponse. Parameters ---------- requests : list A list of pb_utils.InferenceRequest Returns ------- list A list of pb_utils.InferenceResponse. The length of this list must be the same as `requests` \"\"\" text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\" input_ids, attention_mask = tokenize_text(text_triton) payload = { \"inputs\": [ {\"name\": \"INPUT__0\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": input_ids}, {\"name\": \"INPUT__1\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": attention_mask}, ] } response = client.invoke_endpoint( EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload) ) print(json.loads(response[\"Body\"].read().decode(\"utf8\"))) created and tested the multi model endpoint model in aws sagemaker with Nvidia triton server, with tokenized text as inputs.",
        "answers": [],
        "votes": []
    },
    {
        "question": "based on documentation here, https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/nlp/realtime/triton/multi-model/bert_trition-backend/bert_pytorch_trt_backend_MME.ipynb, I have set up a multi model utilizing gpu instance type and nvidia triton container. looking at the set up in the link, the model is invoked by passing tokens instead of passing text directly to the model. is it possible to pass text directly to the model, given the input type is set to string data type in the config.pbtxt (sample code below) . looking for any examples around this. config.pbtxt name: \"...\" platform: \"...\" max_batch_size : 0 input [ { name: \"INPUT_0\" data_type: TYPE_STRING ... } ] output [ { name: \"OUTPUT_1\" .... } ] multi-model invocation text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\" input_ids, attention_mask = tokenize_text(text_triton) payload = { \"inputs\": [ {\"name\": \"token_ids\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": input_ids}, {\"name\": \"attn_mask\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": attention_mask}, ] } response = client.invoke_endpoint( EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload), TargetModel=f\"bert-{i}.tar.gz\", )",
        "answers": [
            [
                "If you want you could make use of an ensemble model in Triton where the first model tokenizes the text and passes it onto the model. Take a look at this link that describes the strategy: https://blog.ml6.eu/triton-ensemble-model-for-deploying-transformers-into-production-c0f727c012e3"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I currently have a triton server with a python backend that serves a model. The machine I am running the inference on is a g4dn.xlarge machine. The instance count provided for the GPU in the config.pbtxt is varied between 1 to 3. I am using perf_analyzer to see if my model scales well for concurrent requests but I get the following results when instance_count is set to 3: Inferences/Second vs. Client Average Batch Latency Concurrency: 1, throughput: 72.2694 infer/sec, latency 13838 usec Concurrency: 2, throughput: 85.3758 infer/sec, latency 23419 usec Concurrency: 3, throughput: 91.5349 infer/sec, latency 32754 usec From the above results, its seen that the average throughput does increase when the instance count is set to 3, but does not scale linearly. Using nvidia-smi, I was able to verify that the GPU capacity is not utilized completely (goes upto 70%). I have a couple of questions: Is it possible to load the model into GPU memory once, and do inference on multiple threads/process which share the single model copy Does triton have capability to automatically to batch requests that arrive at the same time individually from multiple clients?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I converted my model into Onnx and then onnxruntime transformer optimization step is also done. Model is successfully loading and logits values are being matched with the native model as well. I moved this model to Triton server but facing following error on model loading step: Unrecognized attribute: mask_filter_value for operator Attention Library information is as: onnx: 1.13.1 onnxruntime: 1.14.1 torch: 1.13.1 onnxruntime-tools:1.7.0 onnxconverter-common: 1.13.0 opset_version: 11 I tried two versions of triton inference server. Both gave the same errors: nvcr.io/nvidia/tritonserver:21.04-py3 nvcr.io/nvidia/tritonserver:23.02-py3 Could there be something still wrong in onnx runtime but logits are matching exactly. Did anyone else face this error?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Is it possible to configure Triton Server for serving multiple models with different input shapes in such a way that just a single \"collective\" (features lists union) request can service all these models (instead of multiple requests - one per every deployed model)? This would presumably have to be a JSON request, as we could no longer rely on the sequence of unnamed inputs as with numpy arrays / tensors. This could yield significant performance improvements in our use case due to large (90%) overlap of the features lists among the deployed models. From the info I've collected it seems it would only be possible for a special case where all models had the same inputs (shapes, feature names). In such a case one could set up an ensemble (an extra meta-model of the platform: \"ensemble\" type), redistributing input data to all deployed models in parallel (as defined in ensemble_scheduling section of the config file).",
        "answers": [],
        "votes": []
    },
    {
        "question": "I had two 2 docker container in the server. One is Triton Client Server whose GRPC port I set is 1747. Triton Client Server port had a TorchScript model running on it. The other container is where I want to call grpcclient.InferenceServerClient to the Triton container but I got the error: Traceback (most recent call last): File \"/home/user/miniconda/bin/uvicorn\", line 8, in &lt;module&gt; sys.exit(main()) File \"/home/user/miniconda/lib/python3.8/site-packages/click/core.py\", line 1130, in __call__ return self.main(*args, **kwargs) File \"/home/user/miniconda/lib/python3.8/site-packages/click/core.py\", line 1055, in main rv = self.invoke(ctx) File \"/home/user/miniconda/lib/python3.8/site-packages/click/core.py\", line 1404, in invoke return ctx.invoke(self.callback, **ctx.params) File \"/home/user/miniconda/lib/python3.8/site-packages/click/core.py\", line 760, in invoke return __callback(*args, **kwargs) File \"/home/user/miniconda/lib/python3.8/site-packages/uvicorn/main.py\", line 404, in main run( File \"/home/user/miniconda/lib/python3.8/site-packages/uvicorn/main.py\", line 569, in run server.run() File \"/home/user/miniconda/lib/python3.8/site-packages/uvicorn/server.py\", line 60, in run return asyncio.run(self.serve(sockets=sockets)) File \"/home/user/miniconda/lib/python3.8/asyncio/runners.py\", line 43, in run return loop.run_until_complete(main) File \"/home/user/miniconda/lib/python3.8/asyncio/base_events.py\", line 616, in run_until_complete return future.result() File \"/home/user/miniconda/lib/python3.8/site-packages/uvicorn/server.py\", line 67, in serve config.load() File \"/home/user/miniconda/lib/python3.8/site-packages/uvicorn/config.py\", line 477, in load self.loaded_app = import_from_string(self.app) File \"/home/user/miniconda/lib/python3.8/site-packages/uvicorn/importer.py\", line 21, in import_from_string module = importlib.import_module(module_str) File \"/home/user/miniconda/lib/python3.8/importlib/__init__.py\", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"&lt;frozen importlib._bootstrap&gt;\", line 1014, in _gcd_import File \"&lt;frozen importlib._bootstrap&gt;\", line 991, in _find_and_load File \"&lt;frozen importlib._bootstrap&gt;\", line 975, in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 671, in _load_unlocked File \"&lt;frozen importlib._bootstrap_external&gt;\", line 783, in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 219, in _call_with_frames_removed File \"./api_control.py\", line 5, in &lt;module&gt; import helpers File \"./helpers.py\", line 79, in &lt;module&gt; timi_triton = TritonInferTTS(url=TRITON_URL,model_name=TIMI_MODEL_NAME,model_version=TIMI_MODEL_VERSION) File \"./triton_client.py\", line 22, in __init__ self.triton_client = grpcclient.InferenceServerClient(url=self.url, verbose=VERBOSE) File \"/home/user/miniconda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\", line 273, in __init__ self._channel = grpc.insecure_channel(url, options=channel_opt) File \"/home/user/miniconda/lib/python3.8/site-packages/grpc/__init__.py\", line 1977, in insecure_channel return _channel.Channel(target, () if options is None else options, None, File \"/home/user/miniconda/lib/python3.8/site-packages/grpc/_channel.py\", line 1479, in __init__ _common.encode(target), _augment_options(core_options, compression), File \"/home/user/miniconda/lib/python3.8/site-packages/grpc/_common.py\", line 74, in encode return s.encode('utf8') AttributeError: 'NoneType' object has no attribute 'encode' Exception ignored in: &lt;function InferenceServerClient.__del__ at 0x7f41eb398550&gt; Traceback (most recent call last): File \"/home/user/miniconda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\", line 286, in __del__ File \"/home/user/miniconda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\", line 293, in close File \"/home/user/miniconda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\", line 1649, in stop_stream AttributeError: 'InferenceServerClient' object has no attribute '_stream' Here is the code: import os import numpy as np import sys from functools import partial import tritonclient.grpc as grpcclient from tritonclient.utils import * VERBOSE = False TRITON_URL = 'localhost:1747' TIMI_MODEL_NAME = os.getenv(\"TIMI_MODEL_NAME\") TIMI_MODEL_VERSION = os.getenv('TIMI_MODEL_VERSION') class TritonInferTTS(): def __init__(self, url, model_name, model_version): self.url = url self.model_name = model_name self.model_version = model_version self.input_name = ['INPUT__0', 'INPUT__1','INPUT__2','INPUT__3','INPUT__4'] self.output_name = ['OUTPUT__0'] self.triton_client = grpcclient.InferenceServerClient(url=self.url, verbose=VERBOSE) model_metadata = self.triton_client.get_model_metadata(model_name=self.model_name, model_version=self.model_version) model_config = self.triton_client.get_model_config(model_name=self.model_name, model_version=self.model_version) def triton_infer_grpc(self, x, x_length,noise_scale,length_scale,noise_scale_w): # x = np.array([x]) # alpha = np.array([alpha], dtype=np.float32) input0 = InferInput(self.input_name[0], x.shape, 'INT64') input0 = x input1 = InferInput(self.input_name[1], x_length.shape, 'INT64') input1 = x_length input2 = InferInput(self.input_name[2], noise_scale.shape, 'FP32') input2 = noise_scale input3 = InferInput(self.input_name[3], length_scale.shape, 'FP32') input3 = length_scale input4 = InferInput(self.input_name[4], noise_scale_w.shape, 'FP32') input4 = noise_scale_w output0 = grpcclient.ssInferRequestedOutput(self.output_name[0]) # output1 = InferRequestedOutput(self.output_name[1]) response = self.triton_client.infer(self.model_name, model_version=self.model_version, inputs=[input0, input1,input2,input3,input4], outputs=[output0]) audio = response[0] return audio Please help me. I'm so desperate I changed the Triton Url to 'host.docker.internal:1747' and '&lt;my_public_ip&gt;:1747' but i got the same error",
        "answers": [],
        "votes": []
    },
    {
        "question": "Description Trying to deploy the triton docker image as container on kubernetes cluster Triton Information What version of Triton are you using? -&gt; 22.10 Are you using the Triton container or did you build it yourself? I used the server repo with following command: python3 compose.py --backend onnxruntime --backend python --backend tensorflow2 --repoagent checksum --container-version 22.10 then again created new triton image with following dockerfile: FROM tritonserver:latest RUN apt install python3-pip -y RUN pip install tensorflow==2.7.0 RUN pip install transformers==2.11.0 RUN pip install tritonclient RUN pip install tritonclient[all] and dockerfile is being with following command: docker build -t customtritonimage -f ./DockerFiles/DockerFile . To Reproduce directory structure: parent directory -&gt; tritonnludeployment files in it -&gt; DockerFiles (folder containing docker files), k8_trial.yaml, model_repo_triton (all the models here in triton-supported directory shape and has required files) I am using this 'k8_trial.yaml' file for starting kubectl deployment apiVersion: apps/v1 kind: Deployment metadata: name: flower labels: app: flower spec: replicas: 3 selector: matchLabels: app: flower template: metadata: labels: app: flower spec: volumes: - name: models hostPath: # server: 216.48.183.17 path: /root/Documents/tritonnludeployment # readOnly: false type: Directory containers: - name: flower ports: - containerPort: 8000 name: http-triton - containerPort: 8001 name: grpc-triton - containerPort: 8002 name: metrics-triton image: \"customtritonimage:latest\" imagePullPolicy: Never volumeMounts: - mountPath: /root/Documents/tritonnludeployment name: models command: [\"/bin/sh\", \"-c\"] args: [\"cd /models /opt/tritonserver/bin/tritonserver --model-repository=/models/model_repo_triton --allow-gpu-metrics=false --strict-model-config=false\"] # resources: # requests: # memory: \"500Mi\" # cpu: \"500Mi\" # limits: # memory: \"900Mi\" # cpu: \"900Mi\" # nvidia.com/gpu: 1 Describe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well). Expected behavior kubectl deployment should start, with triton container as one of the pods Which step i am doing wrong!",
        "answers": [
            [
                "And what is the error message you are getting? Some of the issues I noticed: use the expected file name know to docker, i.e. Dockerfile not DockerFile make sure base image exists (tritonserver:latest does not, you probably want one of these) first update the sources (RUN apt install ... -&gt; RUN apt update &amp;&amp; apt install ...) reduce layers number by installing multiple python packages at once tritonclient[all] already includes tritonclient don't run containers as root (tritonserver does not require it anyway) make sure you pull the image first time (imagePullPolicy: Never -&gt; IfNotPresent) remove multiple and unnecessary commands from args (such as cd /models) tritonserver can import all subfolders, so --model-repository=/models is probably better"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "We need to setup Nvidia Triton Inference Server on a Windows 2019 server and utilize the Tesla GPU for inferencing the client applications using python. For the ways that we came across we found that we need to it with docker and to use docker in Windows server as per my knowledge we need to do it with WSL, but, We don\u2019t want to setup a WSL on the Windows system. Can someone please share the steps for the same. Is there a way to setup docker without WSL? if yes kindly do share the reference for triton inference server.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Building the windows-based triton server image. Building the Dockerfile.win10.min for triton server version 22.11 was not working as base image required for building the server image was not available for downloading. To build the image downgraded the triton server version to 22.10. Also needed to download the appropriate CUDNN and TENSORT version for building the image. Successfully built the base image using below command for 22.10 server version. docker build -t win10-py3-min -f Dockerfile.win10.min . Once base image is built. To build the triton server image used below command with appropriate container tag and required backend. python build.py --cmake-dir=&lt;path/to/repo&gt;/build --build-dir=/tmp/citritonbuild --no-container-pull --image=base,win10-py3-min --enable-logging --enable-stats --enable-tracing --enable-gpu --endpoint=grpc --endpoint=http --repo-tag=common:&lt;container tag&gt; --repo-tag=core:&lt;container tag&gt; --repo-tag=backend:&lt;container tag&gt; --repo-tag=thirdparty:&lt;container tag&gt; --backend=ensemble --backend=tensorrt:&lt;container tag&gt; --backend=onnxruntime:&lt;container tag&gt; --backend=openvino:&lt;container tag&gt; Tried different arguments with build.py command to build the triton server image was unsuccessful. As there were certain issues related to CMAKE, Rapid Json. Certain issues on github suggested to try with the latest stable release which is 22.12. For building the base image it required the same os as per 22.11. So, to build the base image used the Dockerfile.win10.min file from 22.10 version which worked earlier. After passing different arguments to build.py file was able to build the tritonserver image. However, initially built images did not have the tritonserver.exe which is required for starting the server. In the end was able to build the tritonserver which had the required tritonserver.exe. built the image using below command. python build.py -v --no-container-pull --image=base,win10-py3-min --enable-logging --enable-stats --enable-tracing --enable-gpu --endpoint=grpc --endpoint=http --repo-tag=common:r22.12 --repo-tag=core:r22.12 --repo-tag=backend:r22.12 --repo-tag=thirdparty:r22.12 --backend=ensemble To start the triton server, need to mount the local model repository to the docker image. Command to start the server is as below. docker run -it -v C:/Users/Desktop/model_repository:C:/opt/tritonserver/models tritonserver:latest bin/tritonserver.exe --model-repository=C/opt/tritonserver/models While starting the triton server getting the below error. failed to resize tty, using default size",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have followed the steps mentioned here. I am able to build the win10-py3-min image. After that I am trying to build the Triton Server as mentioned here Command: python build.py -v --no-container-pull --image=gpu-base,win10-py3-min --enable-logging --enable-stats --enable-tracing --enable-gpu --endpoint=grpc --endpoint=http --repo-tag=common:r22.10 --repo-tag=core:r22.10 --repo-tag=backend:r22.10 --repo-tag=thirdparty:r22.10 --backend=ensemble --backend=tensorrt:r22.10 I am getting error as below. cmake : The term 'cmake' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again. At C:\\workspace\\build\\cmake_build.ps1:20 char:1 + cmake \"-DTRT_VERSION4{env:TRT_VERSION}\" \"-DCMAKE_TOOLCHAIN_FILE=${en +..\u2014+ Categorylnfo : ObjectNotFound: (cmake:String) [], CommandNotFoundException + FullyQualifiedErrorld : CommandNotFoundException DEBUG: 86+ &gt;&gt;&gt;&gt; ExitWithCode 1; DEBUG: 6+ function ExitWithCode($exitcode) &gt;&gt;&gt;&gt; DEBUG: 7+ &gt;&gt;&gt;&gt; $host.SetShouldExit($exitcode) DEBUG: 8+ &gt;&gt;&gt;&gt; exit $exitcode DEBUG: 33+ if ( &gt;&gt;&gt;&gt; $LASTEXITCODE -ne 0) DEBUG: 34+ &gt;&gt;&gt;&gt; Write-Output \"exited with status code $LASTEXITCODE-; exited with status code 1 DEBUG: 35+ &gt;&gt;&gt;&gt; ExitWithCode 1; DEBUG: 6+ function ExitWithCode($exitcode) &gt;&gt;&gt;&gt; DEBUG: 7+ &gt;&gt;&gt;&gt; $host.SetShouldExit($exitcode) DEBUG: 8+ &gt;&gt;&gt;&gt; exit $exitcode error: build failed and for below command python build.py -v --no-container-pull --image=base,win10-py3-min --enable-logging --enable-stats --enable-tracing --enable-gpu --endpoint=grpc --endpoint=http --repo-tag=common:r22.10 --repo-tag=core:r22.10 --repo-tag=backend:r22.10 --repo-tag=thirdparty:r22.10 --backend=ensemble --backend=tensorrt:r22.10 getting error as below. \"C:\\tmp\\tritonbuild\\tritonserver\\build\\install.vcxproj\" (default target) (1) -&gt; \"C:\\tmp\\tritonbuild\\tritonserver\\build\\ALL_BUILO.vcxproj\" (default target) (3) -&gt; \"C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core.vcxproj\" (default target) (5) -&gt; (CustomBuild target) -&gt; C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core\\_deps\\repo-common-src\\include\\triton/common/triton_json.h(641,35): error 02039: 'GetObjectA': is not a member of 'rapidjson::GenericValue&lt;rapidjson::UTF8&lt;c har&gt;,rapidjson::MemoryPoolAllocator&lt;rapidjson::CrtAllocator&gt;&gt;1 [C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core\\triton-core.vcxproll [C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core .vcxproj] C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core\\_deps\\repo-common-src\\include\\triton/common/triton_json.h(641,1): error 02530: 'm': references must be initialized [C:\\tmp\\tritonbuild\\tritonserver\\build\\_ deps\\repo-core-build\\triton-core\\triton-core.vcxproj] [C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core.vcxproj] C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core\\_deps\\repo-common-src\\include\\triton/common/triton_json.h(641,1): error C3531: 'm': a symbol whose type contains 'auto' must have an initializer [C:\\tmp\\tr itonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core\\triton-core.vcxproj] [C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core.vcxproj] C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core\\_deps\\repo-common-src\\include\\triton/common/triton_json.h(641,26): error C2143: syntax error: missing ';' before ':' [C:\\tmp\\tritonbuild\\tritonserver\\build \\_deps\\repo-core-build\\triton-core\\triton-core.vcxproj] [C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core.vcxproj] C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core\\_deps\\repo-common-src\\include\\triton/common/triton_json.h(641,46): error C2143: syntax error: missing ';' before 'y [C:\\tmp\\tritonbuild\\tritonserver\\build \\_deps\\repo-core-build\\triton-core\\triton-core.vcxproj] [C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\triton-core.vcxproj] C:\\BuildTools\\MSBuild\\Microsoft\\VC\\v160\\Microsoft.CppCommon.targets(238,5): error MSB8066: Custom build for 'C:\\tmp\\tritonbuild\\tritonserver\\build\\CMakeFiles\\6f6d31a7577427f4fd89bcde8fd28163\\triton-core-mkdir.rule;C:\\tmp\\triton build\\tritonserver\\build\\CMakeFiles\\6f6d31a7577427f4fd89bcde8fd28163\\triton-core-download.rule;C:\\tmp\\tritonbuild\\tritonserver\\build\\CMakeFiles\\6f6d31a7577427f4fd89bcde8fd28163\\triton-core-update.rule;C:\\tmp\\tritonbuild\\tritonser ver\\build\\CMakeFiles\\6f6d310577427f4fd89bcde8fd28163\\triton-core-patch.rule;C:\\tmp\\tritonbuild\\tritonserver\\build\\CMakeFiles\\6f6d310577427f4fd89bcde8fd28163\\triton-core-configure.rule;C:\\tmp\\tritonbuild\\tritonserver\\build\\CMake Files\\6f6d31a7577427f4fd89bcde8fd28163\\triton-core-build.rule;C:\\tmp\\tritonbuild\\tritonserver\\build\\CMakeFiles\\6f6d31a7577427f4fd89bcde8fd28163\\triton-core-install.rule;C:\\tmp\\tritonbuild\\tritonserver\\build\\CMakeFiles\\e0e8eabd6eb cadfabbd7ced13e471b12\\triton-core-complete.rule;C:\\tmp\\tritonbuild\\tritonserver\\build\\CMakeFiles\\d677bfcd41cd12f160cbc1390c778655\\triton-core.rule' exited with code 1. [C:\\tmp\\tritonbuild\\tritonserver\\build\\_deps\\repo-core-build\\ triton-core.vcxproj] 2021 Warning(s) 6 Error(s)",
        "answers": [
            [
                "Run Visual Studio Installer and ensure you have installed C++ CMake tools for Windows. Run x64 Native Tools Command Prompt for VS2019 Run in the command prompt: python build.py -v --no-container-pull --image=gpu-base,win10-py3-min --enable-logging --enable-stats --enable-tracing --enable-gpu --endpoint=grpc --endpoint=http --repo-tag=common:r22.10 --repo-tag=core:r22.10 --repo-tag=backend:r22.10 --repo-tag=thirdparty:r22.10 --backend=ensemble --backend=tensorrt:r22.10"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to Deploying the Nvidia Triton Inference Server behind AWS Internal Application Load Balancer My Triton Application Running ubuntu 20.04 with Docker triton image nvcr.io/nvidia/tritonserver:22.08-py3 tritonserver on Docker version 20.10.12, build e91ed57 Here, we use port 8000 to listen to HTTP requestsm for health check (/v2/health/ready) Port 8001 for GRPC, and 8002 for metrics as needed. But when a going to attach my Triton Machine behind target group my application throw this errorgRPC: 14 UNAVAILABLE: failed to connect to all addresses My Alb Target Group Setting as displayed in Screen Shot I want to server my Triton Grpc Request via ALB",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am having a pytorch artifact named model.pth but the Triton server is looking only for model.pt file which is by default here https://github.com/triton-inference-server/pytorch_backend/blob/4a971e6b6789310609ca84cf1c532084c1c6edc9/src/libtorch.cc#L209-L215 So, how do I override this config from config.pbtxt file so that I can point it to my .pth file. Which parameter to use in config.pbtxt file to do this ?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am currently running a Triton server in production on AWS Cloud using a standard GPU enabled EC2 (very expensive). I have seen these new GPU enabled Graviton instances can be 40% cheaper to run. However, they run on ARM (not AMD). Does this mean I can run the standard version of Triton server on this instance? Looking at Triton server release notes, I have seen it can run on jetson nano, which is nvidia gpu ARM https://github.com/triton-inference-server/server/releases/tag/v1.12.0 Does this method reduce my costs? Can I run Triton server on these graviton instances? Does performance drop using these instances?",
        "answers": [
            [
                "Looking at Nvidia's NGC container repository there are containers built for Arm64 for the most recent version. On the surface it appears it should work on G5g. I would recommend trying the container and testing if it suits your needs. Without testing your specific workload, it is impossible to know up front what the performance would be and by extension if its cheaper."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have installed triton inference server with docker, docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v /mnt/data/nabil/triton_server/models:/models nvcr.io/nvidia/tritonserver:22.08-py3 tritonserver --model-repository=/models I have also created the torchscript model from my pytorch model, using from model_ecapatdnn import ECAPAModel import soundfile as sf import torch model_1 = ECAPAModel.ECAPAModel(lr = 0.001, lr_decay = 0.97, C = 1024, n_class = 18505, m = 0.2, s = 30, test_step = 3, gpu = -1) model_1.load_parameters(\"/ecapatdnn/model.pt\") model = model_1.speaker_encoder # Switch the model to eval model model.eval() # An example input you would normally provide to your model's forward() method. example = torch.rand(1, 48000) # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing. traced_script_module = torch.jit.trace(model, example) # Save the TorchScript model traced_script_module.save(\"traced_ecapatdnn_bangasianeng.pt\") Now, as you can see, my model takes a tensor with shape (BxN), where B is the batch size. How do I write the config.pbtxt for this model?",
        "answers": [
            [
                "So, found the answer. Have to just specify the shape in config file. Here is the config that works for me. name: \"ecapatdnn_bangasianeng\" platform: \"pytorch_libtorch\" max_batch_size: 1 input[ { name: \"INPUT__0\" data_type: TYPE_FP32 dims: [-1] } ] output:[ { name: \"OUTPUT__0\" data_type: TYPE_FP32 dims: [512] } ]"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "NVIDIA Triton vs TorchServe for SageMaker inference? When to recommend each? Both are modern, production grade inference servers. TorchServe is the DLC default inference server for PyTorch models. Triton is also supported for PyTorch inference on SageMaker. Anyone has a good comparison matrix for both?",
        "answers": [
            [
                "Important notes to add here where both serving stacks differ: TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one. if you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it). There is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code/handlers instead of just being able to serve a model's forward function. Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization. Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination."
            ],
            [
                "Because I don't have enough reputation for replying in comments, I write in answer. MME is Multi-model endpoints. MME enables sharing GPU instances behind an endpoint across multiple models and dynamically loads and unloads models based on the incoming traffic. You can read it further in this link"
            ]
        ],
        "votes": [
            4.0000001,
            -0.9999999
        ]
    },
    {
        "question": "I am trying to add a new accelerator to the Nvidia Triton inference server. One of the last thing I need to do it add a new constant like this one (kOpenVINOExecutionAccelerator) but for some reason I cannot find where it is defined: https://github.com/triton-inference-server/onnxruntime_backend/search?q=kOpenVINOExecutionAccelerator I'm quite new to cmake, is this some kind of cmake trick?",
        "answers": [
            [
                "It's in the Triton Inference Server Backend here."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Recently, I have come across a solution of the triton serving config file disable flag \"--strict-model-config=false\" while running the inferencing server. This would enable to create its own config file while loading the model from the model repository. sudo docker run --rm --net=host -p 8000:8000 -p 8001:8001 -p 8002:8002 \\ -v /home/rajesh/custom_repository:/models nvcr.io/nvidia/tritonserver:22.06-py3 \\ tritonserver --model-repository=/models --strict-model-config=false I would like to get the generated config file from the triton inferencing server since we can play around with the batch config and other parameters. Is there a way to get the inbuilt generated config.pbtxt file for the models I have loaded in the server so that I can play around the batch size and other parameters.",
        "answers": [
            [
                "As per Triton docs (source), the loaded model configuration can be found by curl'ing the /config endpoint: Command: curl localhost:8000/v2/models/&lt;model_name&gt;/config [source]"
            ],
            [
                "The above answer which the uses curl command would return the json response. If the results should be in the protobuf format, try loading the model using triton inferencing server with strict model config as false and fetch the results by using the below python script which would return the results in necessary protobuf format. Use this to get the format of the model and edit it easily as per the needs in config pbtxt file instead of cnoverting json to protobuf results. import tritonclient.grpc as grpcclient triton_client = grpcclient.InferenceServerClient(url=&lt;triton_server_url&gt;) model_config = triton_client.get_model_config(model_name=&lt;model_name&gt;, model_version=&lt;model_version&gt;)"
            ]
        ],
        "votes": [
            3.0000001,
            2.0000001
        ]
    },
    {
        "question": "I try to run NVIDIA\u2019s Triton Inference Server. I pulled the pre-built container nvcr.io/nvidia/pytorch:22.06-py3 and then run it with the command run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/F/models:/models nvcr.io/nvidia/pytorch:22.06-py3 tritonserver --model-repository=/models and got the error /opt/nvidia/nvidia_entrypoint.sh: line 49: exec: tritonserver: not found I googled and have not found something to catch this. I tried to change tritonserver to trtserver as recommended but it did not help. Please give some advice how it can be solved.",
        "answers": [
            [
                "Looks like you're trying to run a tritonserver using a pytorch image but according to the triton-server quick start guide, the image should be: $ docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/docs/examples/model_repository:/models nvcr.io/nvidia/tritonserver:&lt;xx.yy&gt;-py3 tritonserver --model-repository=/models Where &lt;xx.yy&gt; is the version of Triton that you want to use In your case it should be nvcr.io/nvidia/tritonserver:22.06-py3 and the full command: run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/F/models:/models nvcr.io/nvidia/tritonserver:22.06-py3 tritonserver --model-repository=/models"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm considering Cog and Triton Inference Server for inference in production. Does someone know what is the difference in capabilities as well as in run times between the two, especially on AWS?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to deploy a simple model on the Triton Inference Server. It is loaded well but I'm having trouble formatting the input to do a proper inference request. My model has a config.pbtxt set up like this max_batch_size: 1 input: [ { name: \"examples\" data_type: TYPE_STRING format: FORMAT_NONE dims: [ -1 ] is_shape_tensor: false allow_ragged_batch: false optional: false } ] I've tried using a pretty straightforward python code to setup the input data like this (the outputs are not written but are setup correctly) bytes_data = [input_data.encode('utf-8')] bytes_data = np.array(bytes_data, dtype=np.object_) bytes_data = bytes_data.reshape([-1, 1]) inputs = [ httpclient.InferInput('examples', bytes_data.shape, \"BYTES\"), ] inputs[0].set_data_from_numpy(bytes_data) But I keep getting the same error message tritonclient.utils.InferenceServerException: Could not parse example input, value: '[my text input here]' [[{{node ParseExample/ParseExampleV2}}]] I've tried multiple ways of encoding the input, as bytes or even as TFX serving used to ask like this { \"instances\": [{\"b64\": \"CjEKLwoJdXR0ZXJhbmNlEiIKIAoecmVuZGV6LXZvdXMgYXZlYyB1biBjb25zZWlsbGVy\"}]} I'm not exactly sure where the problems comes from if anyone knows?",
        "answers": [
            [
                "If anyone gets this same problem, this solved it. I had to create a tf.train.Example() and set the data correctly example = tf.train.Example() example_bytes = str.encode(input_data) example.features.feature['utterance'].bytes_list.value.extend([example_bytes]) inputs = [ httpclient.InferInput('examples', [1], \"BYTES\"), ] inputs[0].set_data_from_numpy(np.asarray(example.SerializeToString()).reshape([1]), binary_data=False)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This article explains how to do image decoding and preprocessing on server side with Dali while using triton-inference-server. I am trying to find something similar for doing video decoding from h.264 encoded bytes array on server side, before the input \"NTHWC\" array is passed to any of the video recognition models like in mmaction2 or swin-transformer, using ensemble model. All I can find is how to load video from files, but nothing on loading videos from external_source. Also, as a workaround, I guess I can do the desired thing using python-backend by writing the encoded video bytes to a file, and preprocess the video, but that will not inherently support batch processing, and I will either have to handle the batch sequentially or by starting multiprocess pools for processing each batch. highly un-optimal I guess. Any help is highly appreciated.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using Triton Inference Server with python backend, at moment send single grpc request does anybody know how we can use the python backend with streaming, because I didn't find any example or anything related to streaming the documentation.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am struggling with running pose models in NVIDIA Triton inference server. The model (open pose , alpha pose , HRNet ... etc ) load normally but the post processing is the problem",
        "answers": [
            [
                "You can refer to the post-processing script in the docs. They have given an example for an image classifier: image_client.py def postprocess(results, output_name, batch_size, batching): \"\"\" Post-process results to show classifications. \"\"\" output_array = results.as_numpy(output_name) if len(output_array) != batch_size: raise Exception(\"expected {} results, got {}\".format( batch_size, len(output_array))) # Include special handling for non-batching models for results in output_array: if not batching: results = [results] for result in results: if output_array.dtype.type == np.object_: cls = \"\".join(chr(x) for x in result).split(':') else: cls = result.split(':') print(\" {} ({}) = {}\".format(cls[0], cls[1], cls[2]))"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I went through the mmdetection documentation to convert a pytorch model to onnx here link All installations are correct and i'm using onnxruntime==1.8.1, custom operators for ONNX Runtime MMCV_WITH_OPS. I'm using the configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py for faster rcnn link and using R-5-FPN pretrained model link I used this to convert the pretrained model to onnx and successfully saved an onnx file named fasterrcnn.onnx python tools/deployment/pytorch2onnx.py \\ configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\ checkpoints/faster_rcnn/faster_rcnn_r50_fpn_mstrain_3x_coco_20210524_110822-e10bd31c.pth \\ --output-file checkpoints/faster_rcnn/fasterrcnn.onnx \\ --input-img demo/demo.jpg \\ --test-img tests/data/color.jpg \\ --shape 608 608 \\ --dynamic-export \\ --cfg-options \\ model.test_cfg.deploy_nms_pre=-1 \\ I am using that onnx file to host the model in NVIDIA triton model server. fasterrcnn_model | 1 | READY The model summary of the onnx model from Triton is shown below { \"name\": \"fasterrcnn_model\", \"platform\": \"onnxruntime_onnx\", \"backend\": \"onnxruntime\", \"version_policy\": { \"latest\": { \"num_versions\": 1 } }, \"max_batch_size\": 1, \"input\": [ { \"name\": \"input\", \"data_type\": \"TYPE_FP32\", \"dims\": [ 3, -1, -1 ] } ], \"output\": [ { \"name\": \"labels\", \"data_type\": \"TYPE_INT64\", \"dims\": [ -1 ] }, { \"name\": \"dets\", \"data_type\": \"TYPE_FP32\", \"dims\": [ -1, 5 ] } ], \"batch_input\": [], \"batch_output\": [], \"optimization\": { \"priority\": \"PRIORITY_DEFAULT\", \"input_pinned_memory\": { \"enable\": true }, \"output_pinned_memory\": { \"enable\": true }, \"gather_kernel_buffer_threshold\": 0, \"eager_batching\": false }, \"instance_group\": [ { \"name\": \"fasterrcnn_model\", \"kind\": \"KIND_CPU\", \"count\": 1, \"gpus\": [], \"profile\": [] } ], \"default_model_filename\": \"model.onnx\", \"cc_model_filenames\": {}, \"metric_tags\": {}, \"parameters\": {}, \"model_warmup\": [] } The summary outlines that the output has the categories \"labels\" and \"dets\" After sending an inference request with a sample image to triton I am getting the following responses. labels [[ 0. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 0. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19.]] dets [[[-1.0000e+00 -1.0000e+00 -1.0000e+00 -1.0000e+00 0.0000e+00] [-3.0000e+02 -3.0000e+02 -3.0000e+02 -3.0000e+02 0.0000e+00] [-5.9900e+02 -5.9900e+02 -5.9900e+02 -5.9900e+02 0.0000e+00] [-8.9800e+02 -8.9800e+02 -8.9800e+02 -8.9800e+02 0.0000e+00] [-1.1970e+03 -1.1970e+03 -1.1970e+03 -1.1970e+03 0.0000e+00] [-1.4960e+03 -1.4960e+03 -1.4960e+03 -1.4960e+03 0.0000e+00] [-1.7950e+03 -1.7950e+03 -1.7950e+03 -1.7950e+03 0.0000e+00] [-2.0940e+03 -2.0940e+03 -2.0940e+03 -2.0940e+03 0.0000e+00] [-2.3930e+03 -2.3930e+03 -2.3930e+03 -2.3930e+03 0.0000e+00] [-2.6920e+03 -2.6920e+03 -2.6920e+03 -2.6920e+03 0.0000e+00] [-2.9910e+03 -2.9910e+03 -2.9910e+03 -2.9910e+03 0.0000e+00] [-3.2900e+03 -3.2900e+03 -3.2900e+03 -3.2900e+03 0.0000e+00] [-3.5890e+03 -3.5890e+03 -3.5890e+03 -3.5890e+03 0.0000e+00] [-3.8880e+03 -3.8880e+03 -3.8880e+03 -3.8880e+03 0.0000e+00] [-4.1870e+03 -4.1870e+03 -4.1870e+03 -4.1870e+03 0.0000e+00] [-4.4860e+03 -4.4860e+03 -4.4860e+03 -4.4860e+03 0.0000e+00] [-4.7850e+03 -4.7850e+03 -4.7850e+03 -4.7850e+03 0.0000e+00] [-5.0840e+03 -5.0840e+03 -5.0840e+03 -5.0840e+03 0.0000e+00] [-5.3830e+03 -5.3830e+03 -5.3830e+03 -5.3830e+03 0.0000e+00] [-5.6820e+03 -5.6820e+03 -5.6820e+03 -5.6820e+03 0.0000e+00] [-5.9810e+03 -5.9810e+03 -5.9810e+03 -5.9810e+03 0.0000e+00] [-6.2800e+03 -6.2800e+03 -6.2800e+03 -6.2800e+03 0.0000e+00] [-6.5790e+03 -6.5790e+03 -6.5790e+03 -6.5790e+03 0.0000e+00] [-6.8780e+03 -6.8780e+03 -6.8780e+03 -6.8780e+03 0.0000e+00] [-7.1770e+03 -7.1770e+03 -7.1770e+03 -7.1770e+03 0.0000e+00] [-7.4760e+03 -7.4760e+03 -7.4760e+03 -7.4760e+03 0.0000e+00] [-7.7750e+03 -7.7750e+03 -7.7750e+03 -7.7750e+03 0.0000e+00] [-8.0740e+03 -8.0740e+03 -8.0740e+03 -8.0740e+03 0.0000e+00] [-8.3730e+03 -8.3730e+03 -8.3730e+03 -8.3730e+03 0.0000e+00] [-8.6720e+03 -8.6720e+03 -8.6720e+03 -8.6720e+03 0.0000e+00] [-8.9710e+03 -8.9710e+03 -8.9710e+03 -8.9710e+03 0.0000e+00] [-9.2700e+03 -9.2700e+03 -9.2700e+03 -9.2700e+03 0.0000e+00] [-9.5690e+03 -9.5690e+03 -9.5690e+03 -9.5690e+03 0.0000e+00] [-9.8680e+03 -9.8680e+03 -9.8680e+03 -9.8680e+03 0.0000e+00] [-1.0167e+04 -1.0167e+04 -1.0167e+04 -1.0167e+04 0.0000e+00] [-1.0466e+04 -1.0466e+04 -1.0466e+04 -1.0466e+04 0.0000e+00] [-1.0765e+04 -1.0765e+04 -1.0765e+04 -1.0765e+04 0.0000e+00] [-1.1064e+04 -1.1064e+04 -1.1064e+04 -1.1064e+04 0.0000e+00] [-1.1363e+04 -1.1363e+04 -1.1363e+04 -1.1363e+04 0.0000e+00] [-1.1662e+04 -1.1662e+04 -1.1662e+04 -1.1662e+04 0.0000e+00] [-1.1961e+04 -1.1961e+04 -1.1961e+04 -1.1961e+04 0.0000e+00] [-1.2260e+04 -1.2260e+04 -1.2260e+04 -1.2260e+04 0.0000e+00] [-1.2559e+04 -1.2559e+04 -1.2559e+04 -1.2559e+04 0.0000e+00] [-1.2858e+04 -1.2858e+04 -1.2858e+04 -1.2858e+04 0.0000e+00] [-1.3157e+04 -1.3157e+04 -1.3157e+04 -1.3157e+04 0.0000e+00] [-1.3456e+04 -1.3456e+04 -1.3456e+04 -1.3456e+04 0.0000e+00] [-1.3755e+04 -1.3755e+04 -1.3755e+04 -1.3755e+04 0.0000e+00] [-1.4054e+04 -1.4054e+04 -1.4054e+04 -1.4054e+04 0.0000e+00] [-1.4353e+04 -1.4353e+04 -1.4353e+04 -1.4353e+04 0.0000e+00] [-1.4652e+04 -1.4652e+04 -1.4652e+04 -1.4652e+04 0.0000e+00] [-1.4951e+04 -1.4951e+04 -1.4951e+04 -1.4951e+04 0.0000e+00] [-1.5250e+04 -1.5250e+04 -1.5250e+04 -1.5250e+04 0.0000e+00] [-1.5549e+04 -1.5549e+04 -1.5549e+04 -1.5549e+04 0.0000e+00] [-1.5848e+04 -1.5848e+04 -1.5848e+04 -1.5848e+04 0.0000e+00] [-1.6147e+04 -1.6147e+04 -1.6147e+04 -1.6147e+04 0.0000e+00] [-1.6446e+04 -1.6446e+04 -1.6446e+04 -1.6446e+04 0.0000e+00] [-1.6745e+04 -1.6745e+04 -1.6745e+04 -1.6745e+04 0.0000e+00] [-1.7044e+04 -1.7044e+04 -1.7044e+04 -1.7044e+04 0.0000e+00] [-1.7343e+04 -1.7343e+04 -1.7343e+04 -1.7343e+04 0.0000e+00] [-1.7642e+04 -1.7642e+04 -1.7642e+04 -1.7642e+04 0.0000e+00] [-1.7941e+04 -1.7941e+04 -1.7941e+04 -1.7941e+04 0.0000e+00] [-1.8240e+04 -1.8240e+04 -1.8240e+04 -1.8240e+04 0.0000e+00] [-1.8539e+04 -1.8539e+04 -1.8539e+04 -1.8539e+04 0.0000e+00] [-1.8838e+04 -1.8838e+04 -1.8838e+04 -1.8838e+04 0.0000e+00] [-1.9137e+04 -1.9137e+04 -1.9137e+04 -1.9137e+04 0.0000e+00] [-1.9436e+04 -1.9436e+04 -1.9436e+04 -1.9436e+04 0.0000e+00] [-1.9735e+04 -1.9735e+04 -1.9735e+04 -1.9735e+04 0.0000e+00] [-2.0034e+04 -2.0034e+04 -2.0034e+04 -2.0034e+04 0.0000e+00] [-2.0333e+04 -2.0333e+04 -2.0333e+04 -2.0333e+04 0.0000e+00] [-2.0632e+04 -2.0632e+04 -2.0632e+04 -2.0632e+04 0.0000e+00] [-2.0931e+04 -2.0931e+04 -2.0931e+04 -2.0931e+04 0.0000e+00] [-2.1230e+04 -2.1230e+04 -2.1230e+04 -2.1230e+04 0.0000e+00] [-2.1529e+04 -2.1529e+04 -2.1529e+04 -2.1529e+04 0.0000e+00] [-2.1828e+04 -2.1828e+04 -2.1828e+04 -2.1828e+04 0.0000e+00] [-2.2127e+04 -2.2127e+04 -2.2127e+04 -2.2127e+04 0.0000e+00] [-2.2426e+04 -2.2426e+04 -2.2426e+04 -2.2426e+04 0.0000e+00] [-2.2725e+04 -2.2725e+04 -2.2725e+04 -2.2725e+04 0.0000e+00] [-2.3024e+04 -2.3024e+04 -2.3024e+04 -2.3024e+04 0.0000e+00] [-2.3323e+04 -2.3323e+04 -2.3323e+04 -2.3323e+04 0.0000e+00] [-2.3622e+04 -2.3622e+04 -2.3622e+04 -2.3622e+04 0.0000e+00] [-1.0000e+00 -1.0000e+00 -1.0000e+00 -1.0000e+00 0.0000e+00] [-3.0000e+02 -3.0000e+02 -3.0000e+02 -3.0000e+02 0.0000e+00] [-5.9900e+02 -5.9900e+02 -5.9900e+02 -5.9900e+02 0.0000e+00] [-8.9800e+02 -8.9800e+02 -8.9800e+02 -8.9800e+02 0.0000e+00] [-1.1970e+03 -1.1970e+03 -1.1970e+03 -1.1970e+03 0.0000e+00] [-1.4960e+03 -1.4960e+03 -1.4960e+03 -1.4960e+03 0.0000e+00] [-1.7950e+03 -1.7950e+03 -1.7950e+03 -1.7950e+03 0.0000e+00] [-2.0940e+03 -2.0940e+03 -2.0940e+03 -2.0940e+03 0.0000e+00] [-2.3930e+03 -2.3930e+03 -2.3930e+03 -2.3930e+03 0.0000e+00] [-2.6920e+03 -2.6920e+03 -2.6920e+03 -2.6920e+03 0.0000e+00] [-2.9910e+03 -2.9910e+03 -2.9910e+03 -2.9910e+03 0.0000e+00] [-3.2900e+03 -3.2900e+03 -3.2900e+03 -3.2900e+03 0.0000e+00] [-3.5890e+03 -3.5890e+03 -3.5890e+03 -3.5890e+03 0.0000e+00] [-3.8880e+03 -3.8880e+03 -3.8880e+03 -3.8880e+03 0.0000e+00] [-4.1870e+03 -4.1870e+03 -4.1870e+03 -4.1870e+03 0.0000e+00] [-4.4860e+03 -4.4860e+03 -4.4860e+03 -4.4860e+03 0.0000e+00] [-4.7850e+03 -4.7850e+03 -4.7850e+03 -4.7850e+03 0.0000e+00] [-5.0840e+03 -5.0840e+03 -5.0840e+03 -5.0840e+03 0.0000e+00] [-5.3830e+03 -5.3830e+03 -5.3830e+03 -5.3830e+03 0.0000e+00] [-5.6820e+03 -5.6820e+03 -5.6820e+03 -5.6820e+03 0.0000e+00]]] The labels response looks like regular COCO classes (80) but I'm having a hard time decoding the dets response. Which looks like bounding boxes coordinates 4 and confidence threshold 1. Making the shape (1,100,5). Any idea on what the dets category is supposed to represent? The output usually depends on the model itself but I think the onnx conversion is changing the output to say labels and dets",
        "answers": [
            [
                "Looking at the conversion script seems like dets is a combo of boxes plus score boxes (Tensor): The bounding boxes of shape [N, num_boxes, 4] scores (Tensor): The detection scores of shape [N, num_boxes, num_classes] tuple[Tensor, Tensor]: dets of shape [N, num_det, 5] and class labels of shape [N, num_det]. dets = torch.cat([boxes, scores], dim=2) https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/export/onnx_helper.py#L197"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I need a little advice with deploying Triton inference server with explicit model control. From the looks of it, this mode gives the user the most control to which model goes live. But the problem I\u2019m not able to solve is how to load models in case the server goes down in production which triggers a new instance to spawn up. The only solution I can think of is to have a service poll the server at regular time intervals, constantly check if my live models are actually live and if not, load them. But this seems like quite a complicated process. I would like to know how others have solved this problem. Thanks in advance",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to compile triton inference server on centos/rhel instead of ubuntu. One problem I encounter is that I'll get the following error for some packages (e.g. protobuf, prometheus-cpp): Could not find a package configuration file provided by \"Protobuf\" with any of the following names: ProtobufConfig.cmake protobuf-config.cmake Add the installation prefix of \"Protobuf\" to CMAKE_PREFIX_PATH or set \"Protobuf_DIR\" to a directory containing one of the above files. If \"Protobuf\" provides a separate development package or SDK, be sure it has been installed. I already figured out that on ubuntu many packages get installed under .../lib/cmake/.../*config.cmake while on centos/rhel they get installed to .../lib64/cmake../*config.cmake. I'm wondering if there is an easy way to tell cmake to install everything under lib/cmake also on centos. I've also tried to \"go the other way around\" and set search paths for specific packages to lib64 (e.g. I changed this and this line of code from .../lib/cmake/... to .../lib64/cmake/...) but when third party modules from other repositories are cloned I face the same issue. So ideally, I'd like an easy way to tell cmake in the \"main\" CMakeLists.txt (there are multiple CMakeLists.txt and some get generated through make later in the build process) that everything should be installed to lib. Thanks!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to use a model in my Triton Inference Server model repository in another custom Python model that I have in the same repository. Is it possible? If yes, how to do that? I guess it could be done with Building Custom Python Backend Stub, but I was wondering if there is a simpler way.",
        "answers": [
            [
                "Yes. You can construct InferenceRequest and call exec() method to use another model in the model repository. Here is code snippet: inference_request = pb_utils.InferenceRequest( model_name='model_name', requested_output_names=['output0', 'output1'], inputs=[pb_utils.Tensor('input0', input0.astype(np.float32))] ) inference_response = inference_request.exec() output0 = pb_utils.get_output_tensor_by_name(inference_response, 'output0') output1 = pb_utils.get_output_tensor_by_name(inference_response, 'output1') Here is a relatively complete example. import numpy as np import triton_python_backend_utils as pb_utils import utils class facenet(object): def __init__(self): self.Facenet_inputs = ['input_1'] self.Facenet_outputs = ['Bottleneck_BatchNorm'] def calc_128_vec(self, img): face_img = utils.pre_process(img) inference_request = pb_utils.InferenceRequest( model_name='facenet', requested_output_names=[self.Facenet_outputs[0]], inputs=[pb_utils.Tensor(self.Facenet_inputs[0], face_img.astype(np.float32))] ) inference_response = inference_request.exec() pre = utils.pb_tensor_to_numpy(pb_utils.get_output_tensor_by_name(inference_response, self.Facenet_outputs[0])) pre = utils.l2_normalize(np.concatenate(pre)) pre = np.reshape(pre, [128]) return pre You can find more reference here: https://github.com/triton-inference-server/python_backend#business-logic-scripting-beta"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Having problems with implementing triton service into gitlab CI. As I noticed in the triton github https://github.com/triton-inference-server/server, they don't have any exposed port by default in Dockerfile and I'm not really able to access the service in any way? Is there any workaround for accessing the triton service? Thanks! .triton_service: &amp;triton_service name: nvcr.io/nvidia/tritonserver:21.06-py3 command: ['tritonserver', '--model-repository=$(S3_TRITON_BUCKET)', '--model-control-mode=explicit', '--allow-http=true', '--allow-metrics=false', '--allow-gpu-metrics=false', '--log-verbose=true', '--log-info=true', '--log-warning=true', '--log-error=true'] alias: triton services: - *triton_service script: - curl -v http://triton:8000/v2/health/ready",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm currently trying use perf_analyzer of Nvidia Triton Inference Server with Deep Learning model which take as input a numpy array (which is an image).* I followed the steps to use real data from the documentation but my input are rejected by the perf_analyzer : \"error: unsupported input data provided perf_analyzer\". This is my input config: \"input\": [ { \"name\": \"input_1:0\", \"data_type\": \"TYPE_FP32\", \"format\": \"FORMAT_NONE\", \"dims\": [ -1, -1, 3 ], \"is_shape_tensor\": false, \"allow_ragged_batch\": false } ], And an example of JSON which I want to load in my perf_analyzer : { \"data\": [ { \"input_1:0\": { \"content\": [ [ [ [ 1.8207893371582031, 2.0784313678741455, 2.4482789039611816 ],..., [ -2.1179039478302, -2.0357143878936768, -1.804444432258606 ] ] ] ], \"shape\": [ 1024, 1024, 3 ] } } ] } Do you have any idea how to pass images in my type_FP32 through JSON? Thanks",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to feed a very large image into Triton server. I need to divide the input image into patches and feed the patches one by one into a tensorflow model. The image has a variable size, so the number of patches N is variable for each call. I think a Triton ensemble model that calls the following steps would do the job: A python model (pre-process) to create the patches The segmentation model Finally another python model (post-process) to merge the output patches into a big output mask However, for this, I would have to write a config. pbtxt file with 1:N and N:1 relation, meaning the ensemble scheduler needs to call the 2nd step multiple times and the 3rd once with the aggregated output. Is this possible, or do I need to use some other technique?",
        "answers": [
            [
                "Disclaimer The below answer isn't the actual solution to the above question. I misunderstood the above query. But I'm leaving this response in case of future readers find it useful. Input import cv2 import matplotlib.pyplot as plt input_img = cv2.imread('/content/2.jpeg') print(input_img.shape) # (719, 640, 3) plt.imshow(input_img) Slice and Stitch The following functionality is adopted from here. More details and discussion can be found here.. Apart from the original code, we bring together the necessary functionality and put them in a single class (ImageSliceRejoin). # ref: https://github.com/idealo/image-super-resolution class ImageSliceRejoin: def pad_patch(self, image_patch, padding_size, channel_last=True): \"\"\" Pads image_patch with padding_size edge values. \"\"\" if channel_last: return np.pad( image_patch, ((padding_size, padding_size), (padding_size, padding_size), (0, 0)), 'edge', ) else: return np.pad( image_patch, ((0, 0), (padding_size, padding_size), (padding_size, padding_size)), 'edge', ) # function to split the image into patches def split_image_into_overlapping_patches(self, image_array, patch_size, padding_size=2): \"\"\" Splits the image into partially overlapping patches. The patches overlap by padding_size pixels. Pads the image twice: - first to have a size multiple of the patch size, - then to have equal padding at the borders. Args: image_array: numpy array of the input image. patch_size: size of the patches from the original image (without padding). padding_size: size of the overlapping area. \"\"\" xmax, ymax, _ = image_array.shape x_remainder = xmax % patch_size y_remainder = ymax % patch_size # modulo here is to avoid extending of patch_size instead of 0 x_extend = (patch_size - x_remainder) % patch_size y_extend = (patch_size - y_remainder) % patch_size # make sure the image is divisible into regular patches extended_image = np.pad(image_array, ((0, x_extend), (0, y_extend), (0, 0)), 'edge') # add padding around the image to simplify computations padded_image = self.pad_patch(extended_image, padding_size, channel_last=True) xmax, ymax, _ = padded_image.shape patches = [] x_lefts = range(padding_size, xmax - padding_size, patch_size) y_tops = range(padding_size, ymax - padding_size, patch_size) for x in x_lefts: for y in y_tops: x_left = x - padding_size y_top = y - padding_size x_right = x + patch_size + padding_size y_bottom = y + patch_size + padding_size patch = padded_image[x_left:x_right, y_top:y_bottom, :] patches.append(patch) return np.array(patches), padded_image.shape # joing the patches def stich_together(self, patches, padded_image_shape, target_shape, padding_size=4): \"\"\" Reconstruct the image from overlapping patches. After scaling, shapes and padding should be scaled too. Args: patches: patches obtained with split_image_into_overlapping_patches padded_image_shape: shape of the padded image contructed in split_image_into_overlapping_patches target_shape: shape of the final image padding_size: size of the overlapping area. \"\"\" xmax, ymax, _ = padded_image_shape # unpad patches patches = patches[:, padding_size:-padding_size, padding_size:-padding_size, :] patch_size = patches.shape[1] n_patches_per_row = ymax // patch_size complete_image = np.zeros((xmax, ymax, 3)) row = -1 col = 0 for i in range(len(patches)): if i % n_patches_per_row == 0: row += 1 col = 0 complete_image[ row * patch_size: (row + 1) * patch_size, col * patch_size: (col + 1) * patch_size, : ] = patches[i] col += 1 return complete_image[0: target_shape[0], 0: target_shape[1], :] Initiate Slicing import numpy as np isr = ImageSliceRejoin() padding_size = 1 patches, p_shape = isr.split_image_into_overlapping_patches( input_img, patch_size=220, padding_size=padding_size ) patches.shape, p_shape, input_img.shape ((12, 222, 222, 3), (882, 662, 3), (719, 640, 3)) Verify n = np.ceil(patches.shape[0] / 2) plt.figure(figsize=(20, 20)) patch_size = patches.shape[1] for i in range(patches.shape[0]): patch = patches[i] ax = plt.subplot(n, n, i + 1) patch_img = np.reshape(patch, (patch_size, patch_size, 3)) plt.imshow(patch_img.astype(\"uint8\")) plt.axis(\"off\") Inference I'm using the Image-Super-Resolution model for demonstration. # import model from ISR.models import RDN model = RDN(weights='psnr-small') # number of patches that will pass to model for inference: # here, batch_size &lt; len(patches) batch_size = 2 for i in range(0, len(patches), batch_size): # get some patches batch = patches[i: i + batch_size] # pass them to model to give patches output batch = model.model.predict(batch) # save the output patches if i == 0: collect = batch else: collect = np.append(collect, batch, axis=0) Now, the collect holds the output of each patch from the model. patches.shape, collect.shape ((12, 222, 222, 3), (12, 444, 444, 3)) Rejoin Patches scale = 2 padded_size_scaled = tuple(np.multiply(p_shape[0:2], scale)) + (3,) scaled_image_shape = tuple(np.multiply(input_img.shape[0:2], scale)) + (3,) sr_img = isr.stich_together( collect, padded_image_shape=padded_size_scaled, target_shape=scaled_image_shape, padding_size=padding_size * scale, ) Verify print(input_img.shape, sr_img.shape) # (719, 640, 3) (1438, 1280, 3) fig, ax = plt.subplots(1,2) fig.set_size_inches(18.5, 10.5) ax[0].imshow(input_img) ax[1].imshow(sr_img.astype('uint8'))"
            ]
        ],
        "votes": [
            2.0000001
        ]
    }
]