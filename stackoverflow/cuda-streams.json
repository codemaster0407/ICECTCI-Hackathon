[
    {
        "question": "I'm supposed to write a fast GPU solution for 1-bit images (C++). In my opinion my code is correct, but for some reason when I submit my answer the system says /box/is.cu:3:10: fatal error: cudacheck.h: No such file or directory 3 | #include \"cudacheck.h\" | ^~~~~~~~~~~~~ compilation terminated. The code is below. Can you see the possible mistake I'm making? #include \"is.h\" #include &lt;cuda_runtime.h&gt; #include \"cudacheck.h\" #include &lt;vector&gt; static inline int jakolasku(int alku, int loppu) { return (alku + loppu - 1)/loppu; } std::vector&lt;float&gt; summienlasku(int ny, int nx, int pny, int pnx, const float* data){ std::vector&lt;float&gt; sums(pnx*pny, 0.f); for(int alk=0; alk&lt;ny; ++alk){ for(int lop=0; lop&lt;nx; ++lop){ sums[(lop+1) + pnx*(alk+1)] = data[3 * (lop+nx*alk)] + sums[(lop+1) + pnx*alk] + sums[lop + pnx*(alk+1)] - sums[lop + pnx*alk]; } } return sums; } __global__ void nelio(int ny, int nx, int size, int pny, int pnx, const float* sums, float* mitat){ int leveys = threadIdx.x + blockIdx.x * blockDim.x; int korkeus = threadIdx.y + blockIdx.y * blockDim.y; if( !(0 &lt; leveys &amp;&amp; leveys &lt;= nx) || !(0 &lt; korkeus &amp;&amp; korkeus &lt;=ny) ) return; int xsize = korkeus * leveys; int ysize = size - xSize; float xluku = 1.0f / (float) xSize; float yluku = ySize == 0 ? 0.f : 1.0f / (float) ySize; float lk = sums[pnx*pny-1]; float L = 0.f; for(int y0=0; y0&lt;=ny-korkeus; ++y0){ int y1 = y0 + korkeus; for(int x0=0; x0&lt;=nx-leveys; ++x0){ int x1 = x0 + leveys; float s1 = sums[y1*pnx + x1]; float s2 = sums[y1*pnx + x0]; float s3 = sums[y0*pnx + x1]; float s4 = sums[y0*pnx + x0]; float xtoin = s1 - s2 - s3 + s4; float ytoin = lk - xtoin; float l = xtoin * xtoin * xluku + ytoin * ytoin * yluku; if(l &gt; L) L = l; } } mitat[korkeus*pnx + leveys] = L; } struct Rectangle{ int width; int height; int size; }; Rectangle loydanelikulmio(int ny, int nx, int pnx, const float* rectdims){ float L = 0.f; int width = 0, height = 0; for(int l=1; l&lt;=ny; ++l){ for(int v=1; v&lt;=nx; ++v){ float lu = rectdims[l*pnx+v]; if (lu &gt; L){ L = lu; width = v; height = l; } } } Rectangle rect = {width, height, width*height}; return rect; } struct SegmentResult{ int y0; int x0; int y1; int x1; float outer[3]; float inner[3]; }; SegmentResult loydaSegmentti(int ny, int nx, int pny, int pnx, Rectangle* rect, const float* sums){ int size = nx*ny; float vluku = sums[pnx*pny-1]; int korkeus = rect-&gt;height; int leveys = rect-&gt;width; int xkoko = rect-&gt;size; int ykoko = size - xkoko; float xx = 1.0f / (float) xkoko; float yy = ykoko == 0 ? 0.f : 1.0f / (float) ykoko; float K = 0.f; float nL = 0.f, bL = 0.f; int xx0 = 0, xx1 = 0, yy0 = 0, yy1 = 0; for(int y0=0; y0&lt;=ny-korkeus; ++y0){ for(int x0=0; x0&lt;=nx-leveys; ++x0){ int y1 = y0 + korkeus; int x1 = x0 + leveys; float s1 = sums[y1*pnx + x1]; float s2 = sums[y1*pnx + x0]; float s3 = sums[y0*pnx + x1]; float s4 = sums[y0*pnx + x0]; float vlukuu = s1 - s2 - s3 + s4; float ylukuu = vluku - vlukuu; float k = vlukuu * vlukuu * xx + ylukuu * ylukuu * yy; if(k &gt; K){ K = k; nL = xlukuu; bL = ylukuu; xx0 = x0; xx1 = x1; yy0 = y0; yy1 = y1; } } } nL *= xx; bL *= yy; SegmentResult tulos = { yy0, xx0, yy1, xx1, { bL, bL, bL }, {nL, nL, nL } }; return tulos; } Result segment(int ny, int nx, const float* data){ int laskux = nx+1, laskuy = ny+1; std::vector&lt;float&gt; summa = summienlasku(ny, nx, laskuy, laskux, data); float* smuuttuja = NULL; CHECK(cudaMalloc((void**)&amp;smuuttuja, laskux*laskuy*sizeof(float))); float* hmuuttuja = NULL; CHECK(cudaMalloc((void**)&amp;hmuuttuja, laskux*laskuy*sizeof(float))); CHECK(cudaMemcpy(smuuttuja, summa.data(), laskux*laskuy*sizeof(float), cudaMemcpyHostToDevice)); { dim3 dimBlock(16, 16); dim3 dimGrid(jakolasku(nx, dimBlock.x), jakolasku(ny, dimBlock.y)); nelio&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(ny, nx, nx*ny, laskuy, laskux, smuuttuja, hmuuttuja); CHECK(cudaGetLastError()); } std::vector&lt;float&gt; rectdims(lasku*laskuy); CHECK(cudaMemcpy(rectdims.data(), hmuuttuja, laskux*laskuy*sizeof(float), cudaMemcpyDeviceToHost)); Rectangle rect = loydaSegmentti(ny, nx, laskux, rectdims.data()); SegmentResult sr = loydaSegmentti(ny, nx, laskuy, laskux, &amp;rect, summa.data()); Result result { sr.y0, sr.x0, sr.y1, sr.x1, { sr.outer[0], sr.outer[1], sr.outer[2] }, { sr.inner[0], sr.inner[1], sr.inner[2] } }; CHECK(cudaFree(smuuttuja)); CHECK(cudaFree(hmuuttuja)); return result; } I'm not sure if the mistake is about the \"cuda\" rows here in my code? Why does it give an error witht the #include \"cudacheck.h\" when I'm trying to submit my code?",
        "answers": [
            [
                "From the error message, it looks like the compiler could not find the codacheck.h header file in the directory that contains the C++ source file or paths that are specified by the INCLUDE environment variable. Are you supposed to put the header file in the project directory or install it somehow? Another possibility is that the filename or the path in the include statement is incorrect."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "The CUDA documentation for scheduling the launching a host function (cuLaunchHostFunc) says: Completion of the function does not cause a stream to become active except as described above. I couldn't quite figure out what's \"described above\". As far as I understand how streams work - the next consecutive piece of work scheduled on the stream after the host function should begin right after the host function execution concludes, i.e. the stream should \"become active\". Am I missing something? Perhaps I'm misunderstanding what being \"active\" means?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I`m trying to overlap the computation and memory operation with HuggingFace SwitchTransformer. Here\u2019s a detailed explanation. The memory operation is for data movement from CPU to GPU, and its size is 4MB per block. The number of blocks is variable (typically from 2 to 6 in total). The computation operation comprises several very small computation operations like GEMM, which takes 10s to 100s microseconds per each. I'm trying to use CudaStream, so I created two different Cuda streams and pushed memory operation and computation operation to each of them. But it had not been overlapped. s_0 = torch.cuda.Stream() # Create a new stream. s_1 = torch.cuda.Stream() # Create a new stream. with torch.cuda.stream(s_0): this_gate_info = router_mask, router_probs, router_logits router_mask = router_mask.bool() idx_mask = router_mask.transpose(1,2) idx_mask = torch.cat(torch.split(idx_mask, 1, dim=0), dim=2) idx_mask = idx_mask.sum(dim=2) idx_mask = idx_mask.squeeze() if next_blk is not None: active_idx = torch.nonzero(idx_mask, as_tuple=True) for idx in active_idx[0]: tmp = getattr(next_blk.layer[-1].mlp.experts, \"expert_{}\".format(idx)) tmp.prefetching() ## THIS IS MEMORY OPERATION COLORED GREEN IN THE FIGURE with torch.cuda.stream(s_1): delayed_router_mask, delayed_router_probs, delayed_router_logits = delayed_gate_info delayed_expert_index = torch.argmax(delayed_router_mask, dim=-1) delayed_router_mask = delayed_router_mask.bool() delayed_idx_mask = delayed_router_mask.transpose(1,2) delayed_idx_mask = torch.cat(torch.split(delayed_idx_mask, 1, dim=0), dim=2) delayed_idx_mask = delayed_idx_mask.sum(dim=2) delayed_idx_mask = delayed_idx_mask.squeeze() for idx, expert in enumerate(self.experts.values()): if delayed_idx_mask[idx] != 0: expert_counter = expert_counter + 1 next_states[delayed_router_mask[:, :, idx]] = expert(hidden_states[delayed_router_mask[:, :, idx]], None, None, None) And here's my question. Firstly, I'd learn that to overlap the memory operation (CPU-&gt;GPU) and computation operation, the memory in the CPU should be pinned. But in my case, as can be seen in the figure, it is pageable memory, not pinned. Is it a reason that this cannot be overlapped? Second, I conducted an experiment to prove it with a simple example (overlapping GEMM with CPU-&gt;GPU memory operation), and here`s the output. import torch import torch.nn as nn import torch.cuda.nvtx as nvtx_cuda torch.cuda.cudart().cudaProfilerStart() cuda = torch.device('cuda') nvtx_cuda.range_push(\"STREAM INIT\") s_0 = torch.cuda.Stream() # Create a new stream. s_1 = torch.cuda.Stream() # Create a new stream. nvtx_cuda.range_pop() A = torch.rand(size=(1024*4, 1024*4), device=\"cuda\") B = torch.rand(size=(1024*4, 1024*4), device=\"cuda\") C = torch.rand(size=(1024*4, 1024*4), device=\"cuda\") D = torch.rand(size=(1024*4, 1024*4), device=\"cuda\") E = torch.rand(size=(1024*4, 1024*4), device=\"cuda\") F = torch.rand(size=(1024*4, 1024*4), device=\"cuda\") a = torch.rand(size=(1024*4, 1024*4), pin_memory=False) b = torch.rand(size=(1024*4, 1024*4), device=\"cuda\") iter = 10 for i in range(iter): with torch.cuda.stream(s_0): nvtx_cuda.range_push(\"S0\") C = A.matmul(B) F = D.matmul(E) nvtx_cuda.range_pop() with torch.cuda.stream(s_1): nvtx_cuda.range_push(\"S1\") nvtx_cuda.range_pop() b = a.to(cuda) torch.cuda.cudart().cudaProfilerStop() This is pageable memory. This is pinned memory. It seems like pageable memory also can be overlapped. Then, what is the reason that my application is not overlapping?",
        "answers": [
            [
                "Generally speaking, for arbitrary sizes and situations, in order to overlap a D-&gt;H or H-&gt;D copy operation with a kernel execution, its necessary to: use cudaMemcpyAsync() use a created stream, not the default stream use a pinned pointer/buffer for the host allocation The kernel launch in question would also need to be launched into a different, non-null stream. (Yes, modifying stream default behavior could affect some of this. I am assuming the default null stream behavior.) Regarding the last item (3), this is a general statement. Support for this comes from several places in the documentation, including here and here. However, a D-&gt;H or H-&gt;D copy from a non-pinned buffer proceeds in stages. The CUDA runtime creates its own pinned buffers that are used for all pageable copy operations, and for small enough transfers that fit within the buffers maintained by the CUDA runtime (size is not specified anywhere), the transfer operation may be asynchronous. In that case, its possible to witness overlap from a pageable buffer. Since this is not formally specified, and sizes and whatnot necessary for sensible use are unspecified, in practice people typically do not depend on such behavior, and the usual advice is to use pinned memory to achieve overlap. That doesn't describe your 2nd case, however, where the profiler appears to indicate overlap for a presumably \"large\" transfer. However the staged transfer is key to understanding this as well. When a transfer operation satisfies items 1 and 2 (above), but not 3, and is of large enough size to not fit entirely in the staging buffer, the CUDA runtime will break the transfer operation into pieces that will fit in the staging buffer. It then transfers the data on the host from your pageable buffer to a pinned buffer. The pinned buffer contents are then transferred to the device. This operation is repeated until all data is transferred. The cudaMemcpyAsync() operation itself does not return, i.e. does not unblock the CPU thread, until the final chunk is transferred to the (pinned) staging buffer. So considering that, if you launch a kernel, and then initiate a pageable transfer (exactly your test case) you may indeed witness transfer activity while the kernel is still executing (ie. overlap). However, as also indicated in your traces, the cudaMemcpyAsync() operation is not returning (unblocking the CPU thread) until the transfer operation is complete or nearly complete. And this is a problem. This behavior (CPU blocking) is disastrous for trying to issue carefully orchestrated concurrent/asynchronous work to the GPU. So while you may be able to witness some overlap for a carefully constructed test case, in the general case, using pageable buffers makes it quite difficult to launch work that is not intended to take place until sometime in the future. It makes it essentially impossible to issue a large tranche of asynchronous work to the GPU. As a simple example, your particular test-case pageable transfer is being overlapped because the transfer was issued after the kernel launch in question. The kernel launch is always non-blocking to the CPU thread, so the CPU thread can begin the pageable transfer, thus the overlap. If we had reversed the order of execution, however, for the most part that particular transfer could not overlap with that particular kernel, because the CPU thread is blocked during the transfer, and cannot proceed to launch the kernel. This appears to be what is happening in your original case: (Yes, I understand your test case has a loop, so it may well overlap with some other kernel launch.) So the general recommendation is to use pinned buffers, when issuing asynchronous work to the GPU where overlap is desired. Anything else is very difficult to rely on for efficient work execution."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Want to implement a custom plugin which process only the GPU frames (memory:CUDAMemory) and also update the frame (Consider creating an overlay on the video). $./gst-launch-1.0 videotestsrc ! cudaupload ! 'video/x-raw(memory:CUDAMemory)' ! **myplugin** ! cudascale ! cudadownload ! autovideosink /* chain function * this function does the actual processing */ static GstFlowReturn gst_test_chain (GstPad * pad, GstObject * parent, GstBuffer * buf) { GstTest *filter; filter = GST_TEST (parent); if (filter-&gt;silent == FALSE) g_print (\"I'm plugged, therefore I'm in.\\n\"); GstMemory *mem; if (gst_buffer_n_memory (buf) == 1 &amp;&amp; (mem = gst_buffer_peek_memory (buf, 0)) &amp;&amp; gst_is_cuda_memory (mem)) { //Issue: gst_is_cuda_memory failed here GstCudaMemory *cmem = GST_CUDA_MEMORY_CAST (mem); //TODO Do processing } /* just push out the incoming buffer without touching it */ return gst_pad_push (filter-&gt;srcpad, buf); } So, I set gst_pad_set_chain_function to sinkpad and getting GstBuffer data one by one, but when I try to Convert GstBuffer to GstCudaMemory then We got this error message. (gst-launch-1.0:47481): GLib-GObject-WARNING **: 18:20:20.641: cannot register existing type 'GstCudaAllocator' (gst-launch-1.0:47481): GLib-CRITICAL **: 18:20:20.641: g_once_init_leave: assertion 'result != 0' failed Any idea how to process with CUDA memory buffer ?",
        "answers": [],
        "votes": []
    },
    {
        "question": "One of the attributes of CUDA memory pools is CU_MEMPOOL_ATTR_REUSE_ALLOW_OPPORTUNISTIC, described in the doxygen as follows: Allow reuse of already completed frees when there is no dependency between the free and allocation. If a free (a cuFreeAsync() I presume) depends on an allocation - how can that free be completed when the allocation needs to happen? Or - am I misunderstanding what this attribute allows?",
        "answers": [
            [
                "This flag is explained in the CUDA programming guide. 11.9.2. cudaMemPoolReuseAllowOpportunistic According to the cudaMemPoolReuseAllowOpportunistic policy, the allocator examines freed allocations to see if the free\u2019s stream order semantic has been met (such as the stream has passed the point of execution indicated by the free). When this is disabled, the allocator will still reuse memory made available when a stream is synchronized with the CPU. Disabling this policy does not stop the cudaMemPoolReuseFollowEventDependencies from applying. cudaMallocAsync(&amp;ptr, size, originalStream); kernel&lt;&lt;&lt;..., originalStream&gt;&gt;&gt;(ptr, ...); cudaFreeAsync(ptr, originalStream); // after some time, the kernel finishes running wait(10); // When cudaMemPoolReuseAllowOpportunistic is enabled this allocation request // can be fulfilled with the prior allocation based on the progress of originalStream. cudaMallocAsync(&amp;ptr2, size, otherStream);"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Investigating possible solutions for this problem, I thought about using CUDA graphs' host execution nodes (cudaGraphAddHostNode). I was hoping to have the option to block and unblock streams on the host side instead of the device side with the wait kernel, while still using graphs. I made a test program with two graphs. One graph does a host-to-device copy, calls a host function (with a host execution node) that waits in a loop for the \"event\" variable to stop being equal to 0 (i.e. waits for the \"event\"), then does a device-to-host copy. Another graph does a memset on the device memory, then calls a host function that sets the \"event\" variable to 1 (i.e. signals the \"event\"). I launch the first graph on one stream, the second on another, then synchronize on the first stream. The result was that both graphs were launched as expected, the \"wait\" host function was executed, and the first stream was blocked successfully. However, even though the second graph was launched, the \"signal\" host function was never executed. I realized that CUDA's implementation is likely serializing all host execution nodes in the context, so the \"signal\" node is forever waiting for the \"wait\" node to finish before executing. The documentation is even saying that \"host functions without a mandated order (such as in independent streams) execute in undefined order and may be serialized\". I also tried launching the graphs from separate host threads, but that didn't work. Is there some kind of way to make host execution nodes on different streams concurrent that I'm missing?",
        "answers": [
            [
                "No, this isn't a reliable method. It is evident that additional thread(s) are spun up by the CUDA runtime to handle host callbacks, but there is no detail or specification. In order for such a thing to work, you would need the two synchronizing agents to each have their own thread, running concurrently. That way, if the waiting agent spun up first, the signaling agent would still be able to execute and deliver a signal. But for cudaLaunchHostFunc (and we can surmise the same thing with graph host nodes) it is explicitly stated: Host functions without a mandated order (such as in independent streams) execute in undefined order and may be serialized. (emphasis added) Serialization of host functions would make such a scheme not workable. Is there some kind of way to make host execution nodes on different streams concurrent that I'm missing? There aren't any additional controls or specification for this, that I am aware of."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I need to pause the execution of all calls in a stream from a certain point in one part of the program until another part of the program decides to unpause this stream at an arbitrary time. This is the requirement of the application I'm working on, I can't work around that. Ideally I want to use the graph API (e.g. cudaGraphAddMemcpyNode), but regular async calls (e.g. cudaMemcpyAsync) are acceptable too if graphs can't do this for some reason. From reading CUDA's documentation I thought that there is an obvious way to do this, but it turned out to be way more complicated. This is my first attempt, distilled to a simple example: cudaGraphCreate(&amp;cuda_graph_cpy, 0); cudaGraphAddMemcpyNode1D(&amp;memcpy_h2d_node, cuda_graph_cpy, NULL, 0, device_buf, host_buf, BUF_SIZE * sizeof(char), cudaMemcpyDefault); cudaGraphAddEventWaitNode(&amp;wait_node, cuda_graph_cpy, &amp;memcpy_h2d_node, 1, cuda_event); cudaGraphAddMemcpyNode1D(&amp;memcpy_d2h_node, cuda_graph_cpy, &amp;wait_node, 1, host_buf, device_buf, BUF_SIZE * sizeof(char), cudaMemcpyDefault); cudaGraphInstantiate(&amp;cuda_graph_exec_cpy, cuda_graph_cpy, NULL, NULL, 0); cudaGraphCreate(&amp;cuda_graph_set, 0); cudaGraphAddMemsetNode(&amp;memset_node, cuda_graph_set, NULL, 0, &amp;memset_params); cudaGraphAddEventRecordNode(&amp;record_set_node, cuda_graph_set, &amp;memset_node, 1, cuda_event); cudaGraphInstantiate(&amp;cuda_graph_exec_set, cuda_graph_set, NULL, NULL, 0); cudaGraphLaunch(cuda_graph_exec_cpy, cuda_stream_cpy); cudaGraphLaunch(cuda_graph_exec_set, cuda_stream_set); cudaStreamSynchronize(cuda_stream_cpy); So I create and instantiate one linear graph that: does a host-to-device copy, waits for cuda_event, does a device-to-host copy. Then I create and instantiate another linear graph that: does a memset on the device memory, records cuda_event. After that I launch the first graph on cuda_stream_cpy, then launch the second graph on cuda_stream_set, then synchronize on cuda_stream_cpy. In the end I expected to modify host_buf, but instead it is left untouched because the first graph/stream didn't actually wait for anything and proceeded with the second copy immediately. After rewriting the code with regular async calls instead of graphs and getting the same behavior, reading everything I could find in Google on this topic, and experimenting with flags and adding more cudaEventRecord/cudaGraphAddEventRecordNode calls in different places, I realized that the event semantics of CUDA doesn't seem to be capable of the behavior I need? The issue seems to be that both record and wait calls have to be made around the same time, and it's impossible to decouple them. If there is no event record enqueued yet, the wait async call or graph node doesn't block the stream, and the stream keeps going. So what I would like to do is to either replace cudaGraphAddEventWaitNode/cudaGraphAddEventRecordNode in the code sample above, or add something to the sample so that the code works the way I described: the wait node actually blocks the stream until the record node (or its replacement?) unblocks it. I also found in CUDA something called the \"external semaphores\" that could be doing what I want (with cudaGraphAddExternalSemaphoresWaitNode/cudaGraphAddExternalSemaphoresSignalNode instead) but they seem to be impossible to create without also using Vulkan or DirectX, which is something I can't bring into the application. I tried to pass a shared memory object's file descriptor to cudaImportExternalSemaphore for cudaExternalSemaphoreHandleTypeOpaqueFd, but that didn't work. EDIT 1: I tried to integrate the wait kernel suggested by @RobertCrovella into my prototype, but it gets stuck on the first graph's launch. Here's the reproducer: #include \"cuda_runtime_api.h\" #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #define BUF_SIZE 1024 #define TEST_POS_OLD 512 #define TEST_POS_NEW 10 #define OLD_VAL 5 #define NEW_VAL 23 #define CUDA_CHKERR(x) res = x; if (res != cudaSuccess) goto fail; __global__ void wait_kernel(volatile unsigned char *event, unsigned char val) { while (*event == val); } int main() { cudaError_t res = cudaSuccess; const char *err_str = NULL; const char *err_name = NULL; cudaStream_t cuda_stream_cpy; cudaStream_t cuda_stream_set; cudaGraph_t cuda_graph_cpy; cudaGraphExec_t cuda_graph_exec_cpy; cudaGraph_t cuda_graph_set; cudaGraphExec_t cuda_graph_exec_set; cudaGraphNode_t memcpy_h2d_node; cudaGraphNode_t memcpy_d2h_node; cudaGraphNode_t memset_node; cudaGraphNode_t signal_node; cudaGraphNode_t wait_node; unsigned char *event; unsigned char test = 0; dim3 grid(1,1,1); dim3 block(1,1,1); struct cudaKernelNodeParams kernel_node_params = {}; struct cudaMemsetParams memset_params = {}; void *wait_kernel_args[2] = {(void *) &amp;event, (void *) &amp;test}; char *host_buf = NULL; void *device_buf = NULL; printf(\"Creating the event...\\n\"); CUDA_CHKERR(cudaMalloc(&amp;event, sizeof(event[0]))); printf(\"cudaMalloc\\n\"); CUDA_CHKERR(cudaMemset(event, 0, sizeof(event[0]))); printf(\"cudaMemset\\n\"); printf(\"Allocating the host buffer and setting the test value...\\n\"); host_buf = (char *) malloc(BUF_SIZE * sizeof(char)); for (int i = 0; i &lt; BUF_SIZE; i++) { host_buf[i] = OLD_VAL; } CUDA_CHKERR(cudaMalloc(&amp;device_buf, BUF_SIZE * sizeof(char))); printf(\"cudaMalloc\\n\"); CUDA_CHKERR(cudaStreamCreate(&amp;cuda_stream_cpy)); printf(\"cudaStreamCreate cpy\\n\"); CUDA_CHKERR(cudaStreamCreate(&amp;cuda_stream_set)); printf(\"cudaStreamCreate set\\n\"); CUDA_CHKERR(cudaGraphCreate(&amp;cuda_graph_cpy, 0)); printf(\"cudaGraphCreate cpy\\n\"); CUDA_CHKERR(cudaGraphAddMemcpyNode1D(&amp;memcpy_h2d_node, cuda_graph_cpy, NULL, 0, device_buf, host_buf, BUF_SIZE * sizeof(char), cudaMemcpyDefault)); printf(\"cudaGraphAddMemcpyNode1D H2D\\n\"); memset(&amp;kernel_node_params, 0, sizeof(cudaKernelNodeParams)); kernel_node_params.func = (void *)wait_kernel; kernel_node_params.gridDim = grid; kernel_node_params.blockDim = block; kernel_node_params.sharedMemBytes = 0; kernel_node_params.kernelParams = wait_kernel_args; kernel_node_params.extra = NULL; CUDA_CHKERR(cudaGraphAddKernelNode(&amp;wait_node, cuda_graph_cpy, &amp;memcpy_h2d_node, 1, &amp;kernel_node_params)); printf(\"cudaGraphAddKernelNode (wait)\\n\"); CUDA_CHKERR(cudaGraphAddMemcpyNode1D(&amp;memcpy_d2h_node, cuda_graph_cpy, &amp;wait_node, 1, host_buf, device_buf, BUF_SIZE * sizeof(char), cudaMemcpyDefault)); printf(\"cudaGraphAddMemcpyNode1D D2H\\n\"); CUDA_CHKERR(cudaGraphInstantiate(&amp;cuda_graph_exec_cpy, cuda_graph_cpy, NULL, NULL, 0)); printf(\"cudaGraphInstantiate cpy\\n\"); CUDA_CHKERR(cudaGraphCreate(&amp;cuda_graph_set, 0)); printf(\"cudaGraphCreate set\\n\"); memset(&amp;memset_params, 0, sizeof(cudaMemsetParams)); memset_params.dst = device_buf; memset_params.value = NEW_VAL; memset_params.pitch = 0; memset_params.elementSize = sizeof(char); memset_params.width = 512; memset_params.height = 1; CUDA_CHKERR(cudaGraphAddMemsetNode(&amp;memset_node, cuda_graph_set, NULL, 0, &amp;memset_params)); printf(\"cudaGraphAddMemsetNode\\n\"); memset(&amp;memset_params, 0, sizeof(cudaMemsetParams)); memset_params.dst = event; memset_params.value = 1; memset_params.pitch = 0; memset_params.elementSize = 1; memset_params.width = 1; memset_params.height = 1; CUDA_CHKERR(cudaGraphAddMemsetNode(&amp;signal_node, cuda_graph_set, &amp;memset_node, 1, &amp;memset_params)); printf(\"cudaGraphAddMemsetNode (signal)\\n\"); CUDA_CHKERR(cudaGraphInstantiate(&amp;cuda_graph_exec_set, cuda_graph_set, NULL, NULL, 0)); printf(\"cudaGraphInstantiate set\\n\"); CUDA_CHKERR(cudaGraphLaunch(cuda_graph_exec_cpy, cuda_stream_cpy)); printf(\"cudaGraphLaunch cpy\\n\"); CUDA_CHKERR(cudaGraphLaunch(cuda_graph_exec_set, cuda_stream_set)); printf(\"cudaGraphLaunch set\\n\"); CUDA_CHKERR(cudaStreamSynchronize(cuda_stream_cpy)); printf(\"cudaStreamSynchronize cpy\\n\"); CUDA_CHKERR(cudaGraphExecDestroy(cuda_graph_exec_cpy)); printf(\"cudaGraphExecDestroy\\n\"); CUDA_CHKERR(cudaGraphExecDestroy(cuda_graph_exec_set)); printf(\"cudaGraphExecDestroy\\n\"); CUDA_CHKERR(cudaGraphDestroy(cuda_graph_cpy)); printf(\"cudaGraphDestroy\\n\"); CUDA_CHKERR(cudaGraphDestroy(cuda_graph_set)); printf(\"cudaGraphDestroy\\n\"); CUDA_CHKERR(cudaStreamDestroy(cuda_stream_cpy)); printf(\"cudaStreamDestroy cpy\\n\"); CUDA_CHKERR(cudaStreamDestroy(cuda_stream_set)); printf(\"cudaStreamDestroy set\\n\"); if (host_buf[TEST_POS_OLD] == OLD_VAL) { printf(\"host_buf[TEST_POS_OLD] is correct.\\n\"); } else { printf(\"host_buf[TEST_POS_OLD] is not correct!\\n\"); } if (host_buf[TEST_POS_NEW] == NEW_VAL) { printf(\"host_buf[TEST_POS_NEW] is correct.\\n\"); } else { printf(\"host_buf[TEST_POS_NEW] is not correct!\\n\"); if (host_buf[TEST_POS_OLD] == host_buf[TEST_POS_NEW]) printf(\"They are equal!\\n\"); } return 0; fail: err_name = cudaGetErrorName(res); err_str = cudaGetErrorString(res); printf(\"%s: %s\\n\", err_name, err_str); return 1; } EDIT 2: The issue was indeed the host memory allocation, with that correction my code works properly.",
        "answers": [
            [
                "Although the comments say otherwise, you are effectively building a dependency between the two graphs. With a bit of refactoring, my suggestion would be to combine those activities into one graph, and express the dependency using the mechanisms available in graph capture. However with the goals: Two graphs (apparently) using the graph API (not stream capture) we could realize this in a fashion similar to mentioned stream memops (in the comments, only available in the driver API). Basically, we create a kernel that is waiting on device memory location, to synchronize one graph to another. The graph that is waiting will launch the kernel to synchronize. The other graph signals via a memset node. Here is an example: $ cat t2217.cu #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;cstdio&gt; #include &lt;cstdlib&gt; #define cudaCheckErrors(msg) \\ do { \\ cudaError_t __err = cudaGetLastError(); \\ if (__err != cudaSuccess) { \\ fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\ msg, cudaGetErrorString(__err), \\ __FILE__, __LINE__); \\ fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\ exit(1); \\ } \\ } while (0) __global__ void calc1kernel(float *data, float val, size_t n){ size_t idx = threadIdx.x+blockDim.x*blockIdx.x; while (idx &lt; n){ data[idx] += val; idx += gridDim.x*blockDim.x;} } __global__ void calc2kernel(float *data, float val, size_t n){ size_t idx = threadIdx.x+blockDim.x*blockIdx.x; while (idx &lt; n){ data[idx] *= val; idx += gridDim.x*blockDim.x;} } __global__ void waitkernel(volatile unsigned char *signal, unsigned char val){ while (*signal == val); } // CUDA Graph 1: // calc1kernelnode // | // memsetnode // CUDA Graph 2: // waitkernel // | // calc2kernelnode int main(int argc, char *argv[]){ size_t data_size = 32; cudaStream_t s1, s2; cudaGraph_t g1, g2; float *data, val; unsigned char *sig; // allocate for data on the device cudaMalloc(&amp;data, data_size*sizeof(data[0])); cudaCheckErrors(\"CUDAMalloc failure\"); cudaMalloc(&amp;sig, sizeof(sig[0])); cudaCheckErrors(\"CUDAMalloc failure\"); cudaMemset(sig, 0, sizeof(sig[0])); cudaCheckErrors(\"CUDAMemset failure\"); cudaMemset(data, 0, data_size*sizeof(data[0])); cudaCheckErrors(\"CUDAMemset failure\"); // create the graph cudaGraphCreate(&amp;g1, 0); cudaCheckErrors(\"CUDAGraphCreate failure\"); cudaGraphCreate(&amp;g2, 0); cudaCheckErrors(\"CUDAGraphCreate failure\"); cudaStreamCreate(&amp;s1); cudaCheckErrors(\"CUDAStreamCreate failure\"); cudaStreamCreate(&amp;s2); cudaCheckErrors(\"CUDAStreamCreate failure\"); dim3 grid(1,1,1); dim3 block(1,1,1); cudaGraphNode_t calc1kernelnode, calc2kernelnode, waitkernelnode, memsetnode; // add nodes and their dependencies to the first graph cudaKernelNodeParams kernelNodeParams = {0}; // first add calc1kernelnode, which has no dependencies val = 3.0f; memset(&amp;kernelNodeParams, 0, sizeof(cudaKernelNodeParams)); void *kernelargs[3] = {(void *)&amp;data, (void *)&amp;val, (void *)&amp;data_size}; kernelNodeParams.func = (void *)calc1kernel; kernelNodeParams.gridDim = grid; kernelNodeParams.blockDim = block; kernelNodeParams.sharedMemBytes = 0; kernelNodeParams.kernelParams = kernelargs; kernelNodeParams.extra = NULL; cudaGraphAddKernelNode(&amp;calc1kernelnode, g1, NULL, 0, &amp;kernelNodeParams); cudaCheckErrors(\"CUDAGraphAddKernelNode failure\"); // now add the memsetnode, which has 1 dependency on calc1kernelnode cudaMemsetParams memsetParams = {0}; memset(&amp;memsetParams, 0, sizeof(cudaMemsetParams)); memsetParams.dst = sig; memsetParams.elementSize = 1; memsetParams.height = 1; memsetParams.pitch = 1; memsetParams.value = 1; memsetParams.width = 1; cudaGraphAddMemsetNode(&amp;memsetnode, g1, &amp;calc1kernelnode, 1, &amp;memsetParams); cudaCheckErrors(\"CUDAGraphAddMemsetNode failure\"); // graph 1 is now defined, next step is to instantiate an executable version of it size_t num_nodes = 0; cudaGraphNode_t *nodes1 = NULL; cudaGraphGetNodes(g1, nodes1, &amp;num_nodes); cudaCheckErrors(\"CUDAGraphGetNodes failure\"); printf(\"graph 1 num nodes: %lu\\n\", num_nodes); cudaGraphExec_t graphExec1, graphExec2; cudaGraphInstantiate(&amp;graphExec1, g1, NULL, NULL, 0); cudaCheckErrors(\"CUDAGraphInstantiate failure\"); // add nodes and their dependencies to the second graph // first add waitkernelnode, which has no dependencies unsigned char test = 0; memset(&amp;kernelNodeParams, 0, sizeof(cudaKernelNodeParams)); void *waitkernelargs[2] = {(void *) &amp;sig, (void *) &amp;test }; kernelNodeParams.func = (void *)waitkernel; kernelNodeParams.gridDim = grid; kernelNodeParams.blockDim = block; kernelNodeParams.sharedMemBytes = 0; kernelNodeParams.kernelParams = waitkernelargs; kernelNodeParams.extra = NULL; cudaGraphAddKernelNode(&amp;waitkernelnode, g2, NULL, 0, &amp;kernelNodeParams); cudaCheckErrors(\"CUDAGraphAddKernelNode failure\"); // now add the calc2kernelnode, which has 1 dependency on waitkernelnode memset(&amp;kernelNodeParams, 0, sizeof(cudaKernelNodeParams)); kernelNodeParams.func = (void *)calc2kernel; kernelNodeParams.gridDim = grid; kernelNodeParams.blockDim = block; kernelNodeParams.sharedMemBytes = 0; kernelNodeParams.kernelParams = kernelargs; kernelNodeParams.extra = NULL; cudaGraphAddKernelNode(&amp;calc2kernelnode, g2, &amp;waitkernelnode, 1, &amp;kernelNodeParams); cudaCheckErrors(\"CUDAGraphAddKernelNode failure\"); // graph 2 is now defined, next step is to instantiate an executable version of it cudaGraphNode_t *nodes2 = NULL; cudaGraphGetNodes(g2, nodes2, &amp;num_nodes); cudaCheckErrors(\"CUDAGraphGetNodes failure\"); printf(\"graph 2 num nodes: %lu\\n\", num_nodes); cudaGraphInstantiate(&amp;graphExec2, g2, NULL, NULL, 0); cudaCheckErrors(\"CUDAGraphInstantiate failure\"); // now launch the graphs cudaGraphLaunch(graphExec2, s2); cudaCheckErrors(\"CUDAGraphLaunch failure\"); cudaGraphLaunch(graphExec1, s1); cudaCheckErrors(\"CUDAGraphLaunch failure\"); cudaStreamSynchronize(s1); cudaCheckErrors(\"graph execution failure\"); cudaStreamSynchronize(s2); cudaCheckErrors(\"graph execution failure\"); float *result = new float[data_size]; cudaMemcpy(result, data, data_size*sizeof(float), cudaMemcpyDeviceToHost); std::cout &lt;&lt; \"result[0] = \" &lt;&lt; result[0] &lt;&lt; std::endl; // clean up cudaFree(data); cudaStreamDestroy(s1); cudaGraphDestroy(g1); cudaGraphExecDestroy(graphExec1); cudaStreamDestroy(s2); cudaGraphDestroy(g2); cudaGraphExecDestroy(graphExec2); } $ nvcc -o t2217 t2217.cu $ ./t2217 graph 1 num nodes: 2 graph 2 num nodes: 2 result[0] = 9 $ The result of 9 indicates that even though graph 2 was launched first, it successfully waited until the synchronization point in graph 1, before it allowed its calc kernel to run. The given example (in the question) shows use of the runtime API, as does my answer. If you want to use the driver API, as already indicated in the comments, it should be possible to do this directly via batched memops using cuGraphAddBatchMemOpNode. A memset node, or similar, is also still needed. This kind of interlock is something that can give rise to hangs and deadlocks if used improperly. Note the various warnings given: Warning: Improper use of this API may deadlock the application. Synchronization ordering established through this API is not visible to CUDA. CUDA tasks that are (even indirectly) ordered by this API should also have that order expressed with CUDA-visible dependencies such as events. ... Regarding your EDIT 1: If I change this: host_buf = (char *) malloc(BUF_SIZE * sizeof(char)); to this: CUDA_CHKERR(cudaHostAlloc(&amp;host_buf, BUF_SIZE*sizeof(char), cudaHostAllocDefault)); your code runs correctly for me. In CUDA, in order for a D-&gt;H or H-&gt;D transfer to be (guaranteed to be) asynchronous and non-blocking, the host buffer must be a pinned buffer. When we apply this to graphs, the requirement is more stringent: General requirements: Memcpy nodes: Only copies involving device memory and/or pinned device-mapped host memory are permitted. (emphasis added) memory allocated with malloc is neither device memory, nor is it pinned device-mapped host memory."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "CUDA 12 introduces two new API calls, cuStreamGetId() and cuCtxGetId() which return \"unique ID\"s associated with a stream or a context respectively. I'm struggling to understand why this is useful, or how this would be used. Are the handles for streams and contexts not unique? i.e. does CUDA create copies of CUstream_st and CUctx_st structures with the same values, or which describe the same entities? If it does - under what circumstances?",
        "answers": [],
        "votes": []
    },
    {
        "question": "A CUDA stream is a queue of tasks: memory copies, event firing, event waits, kernel launches, callbacks... But - these queues don't have infinite capacity. In fact, empirically, I find that this limit is not super-high, e.g. in the thousands, not millions. My questions: Is the size/capacity of a CUDA stream fixed in terms of any kind of enqueued items, or does the capacity behave differently based on what kind of actions/tasks you enqueue? How can I determine this capacity other than enqueuing more and more stuff until I can no longer fit any?",
        "answers": [
            [
                "Is the size/capacity of a CUDA stream fixed in terms of any kind of enqueued items, or does the capacity behave differently based on what kind of actions/tasks you enqueue? The \"capacity\" behaves differently based on actions/tasks you enqueue. Here is a demonstration: If we enqueue a single host function/callback in the midst of a number of kernel calls, on a Tesla V100 on CUDA 11.4 I observe a \"capacity\" for ~1000 enqueued items. However if I alternate kernel calls and host functions, I observe a capacity for ~100 enqueued items. // test case with alternating kernels and callbacks $ cat t2042a.cu #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;mutex&gt; #include &lt;condition_variable&gt; #define CUDACHECK(x) x // empty kernel __global__ void NoOpKernel() {} // for blocking stream to wait for host signal class Event { private: std::mutex mtx_condition_; std::condition_variable condition_; bool signalled = false; public: void Signal() { { std::lock_guard&lt;decltype(mtx_condition_)&gt; lock(mtx_condition_); signalled = true; } condition_.notify_all(); } void Wait() { std::unique_lock&lt;decltype(mtx_condition_)&gt; lock(mtx_condition_); while (!signalled) { condition_.wait(lock); } } }; void CUDART_CB block_op_host_fn(void* arg) { Event* evt = (Event*)arg; evt-&gt;Wait(); } int main() { cudaStream_t stream; CUDACHECK(cudaStreamCreate(&amp;stream)); int num_events = 60; // 50 is okay, 60 will hang std::vector&lt;std::shared_ptr&lt;Event&gt;&gt; event_vec; for (int i = 0; i &lt; num_events; i++) { std::cout &lt;&lt; \"Queuing NoOp \" &lt;&lt; i &lt;&lt; std::endl; NoOpKernel&lt;&lt;&lt;1, 128, 0, stream&gt;&gt;&gt;(); // HERE : is where it hangs std::cout &lt;&lt; \"Queued NoOp \" &lt;&lt; i &lt;&lt; std::endl; event_vec.push_back(std::make_shared&lt;Event&gt;()); cudaLaunchHostFunc(stream, block_op_host_fn, event_vec.back().get()); std::cout &lt;&lt; \"Queued block_op \" &lt;&lt; i &lt;&lt; std::endl; } for (int i = 0; i &lt; num_events; i++) { event_vec[i]-&gt;Signal(); } // clean up CUDACHECK(cudaDeviceSynchronize()); CUDACHECK(cudaStreamDestroy(stream)); return 0; } $ nvcc -o t2042a t2042a.cu $ ./t2042a Queuing NoOp 0 Queued NoOp 0 Queued block_op 0 Queuing NoOp 1 Queued NoOp 1 Queued block_op 1 Queuing NoOp 2 Queued NoOp 2 Queued block_op 2 Queuing NoOp 3 Queued NoOp 3 Queued block_op 3 Queuing NoOp 4 Queued NoOp 4 Queued block_op 4 Queuing NoOp 5 Queued NoOp 5 Queued block_op 5 Queuing NoOp 6 Queued NoOp 6 Queued block_op 6 Queuing NoOp 7 Queued NoOp 7 Queued block_op 7 Queuing NoOp 8 Queued NoOp 8 Queued block_op 8 Queuing NoOp 9 Queued NoOp 9 Queued block_op 9 Queuing NoOp 10 Queued NoOp 10 Queued block_op 10 Queuing NoOp 11 Queued NoOp 11 Queued block_op 11 Queuing NoOp 12 Queued NoOp 12 Queued block_op 12 Queuing NoOp 13 Queued NoOp 13 Queued block_op 13 Queuing NoOp 14 Queued NoOp 14 Queued block_op 14 Queuing NoOp 15 Queued NoOp 15 Queued block_op 15 Queuing NoOp 16 Queued NoOp 16 Queued block_op 16 Queuing NoOp 17 Queued NoOp 17 Queued block_op 17 Queuing NoOp 18 Queued NoOp 18 Queued block_op 18 Queuing NoOp 19 Queued NoOp 19 Queued block_op 19 Queuing NoOp 20 Queued NoOp 20 Queued block_op 20 Queuing NoOp 21 Queued NoOp 21 Queued block_op 21 Queuing NoOp 22 Queued NoOp 22 Queued block_op 22 Queuing NoOp 23 Queued NoOp 23 Queued block_op 23 Queuing NoOp 24 Queued NoOp 24 Queued block_op 24 Queuing NoOp 25 Queued NoOp 25 Queued block_op 25 Queuing NoOp 26 Queued NoOp 26 Queued block_op 26 Queuing NoOp 27 Queued NoOp 27 Queued block_op 27 Queuing NoOp 28 Queued NoOp 28 Queued block_op 28 Queuing NoOp 29 Queued NoOp 29 Queued block_op 29 Queuing NoOp 30 Queued NoOp 30 Queued block_op 30 Queuing NoOp 31 Queued NoOp 31 Queued block_op 31 Queuing NoOp 32 Queued NoOp 32 Queued block_op 32 Queuing NoOp 33 Queued NoOp 33 Queued block_op 33 Queuing NoOp 34 Queued NoOp 34 Queued block_op 34 Queuing NoOp 35 Queued NoOp 35 Queued block_op 35 Queuing NoOp 36 Queued NoOp 36 Queued block_op 36 Queuing NoOp 37 Queued NoOp 37 Queued block_op 37 Queuing NoOp 38 Queued NoOp 38 Queued block_op 38 Queuing NoOp 39 Queued NoOp 39 Queued block_op 39 Queuing NoOp 40 Queued NoOp 40 Queued block_op 40 Queuing NoOp 41 Queued NoOp 41 Queued block_op 41 Queuing NoOp 42 Queued NoOp 42 Queued block_op 42 Queuing NoOp 43 Queued NoOp 43 Queued block_op 43 Queuing NoOp 44 Queued NoOp 44 Queued block_op 44 Queuing NoOp 45 Queued NoOp 45 Queued block_op 45 Queuing NoOp 46 Queued NoOp 46 Queued block_op 46 Queuing NoOp 47 Queued NoOp 47 Queued block_op 47 Queuing NoOp 48 Queued NoOp 48 Queued block_op 48 Queuing NoOp 49 Queued NoOp 49 Queued block_op 49 Queuing NoOp 50 Queued NoOp 50 Queued block_op 50 Queuing NoOp 51 Queued NoOp 51 Queued block_op 51 Queuing NoOp 52 Queued NoOp 52 Queued block_op 52 Queuing NoOp 53 Queued NoOp 53 Queued block_op 53 Queuing NoOp 54 Queued NoOp 54 Queued block_op 54 Queuing NoOp 55 Queued NoOp 55 Queued block_op 55 Queuing NoOp 56 Queued NoOp 56 Queued block_op 56 Queuing NoOp 57 ^C $ // test case with a single callback and many kernels $ cat t2042.cu #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;mutex&gt; #include &lt;condition_variable&gt; #include &lt;cstdlib&gt; #define CUDACHECK(x) x // empty kernel __global__ void NoOpKernel() {} // for blocking stream to wait for host signal class Event { private: std::mutex mtx_condition_; std::condition_variable condition_; bool signalled = false; public: void Signal() { { std::lock_guard&lt;decltype(mtx_condition_)&gt; lock(mtx_condition_); signalled = true; } condition_.notify_all(); } void Wait() { std::unique_lock&lt;decltype(mtx_condition_)&gt; lock(mtx_condition_); while (!signalled) { condition_.wait(lock); } } }; void CUDART_CB block_op_host_fn(void* arg) { Event* evt = (Event*)arg; evt-&gt;Wait(); } int main(int argc, char *argv[]) { cudaStream_t stream; CUDACHECK(cudaStreamCreate(&amp;stream)); int num_loops = 2000; // 50 is okay, 60 will hang int num_events = 0; std::vector&lt;std::shared_ptr&lt;Event&gt;&gt; event_vec; if (argc &gt; 1) num_loops = atoi(argv[1]); for (int i = 0; i &lt; num_loops; i++) { std::cout &lt;&lt; \"Queuing NoOp \" &lt;&lt; i &lt;&lt; std::endl; NoOpKernel&lt;&lt;&lt;1, 128, 0, stream&gt;&gt;&gt;(); // HERE : is where it hangs std::cout &lt;&lt; \"Queued NoOp \" &lt;&lt; i &lt;&lt; std::endl; if (i == 0){ num_events++; event_vec.push_back(std::make_shared&lt;Event&gt;()); cudaLaunchHostFunc(stream, block_op_host_fn, event_vec.back().get()); std::cout &lt;&lt; \"Queued block_op \" &lt;&lt; i &lt;&lt; std::endl;} } for (int i = 0; i &lt; num_events; i++) { event_vec[i]-&gt;Signal(); } // clean up CUDACHECK(cudaDeviceSynchronize()); CUDACHECK(cudaStreamDestroy(stream)); return 0; } $ nvcc -o t2042 t2042.cu $ nvcc -o t2042 t2042.cu $ ./t2042 ... &lt;snip&gt; Queuing NoOp 1019 Queued NoOp 1019 Queuing NoOp 1020 Queued NoOp 1020 Queuing NoOp 1021 Queued NoOp 1021 Queuing NoOp 1022 ^C $ (the code hangs when the queue becomes \"full\", and I terminate at that point with ctrl-C) How can I determine this capacity other than enqueuing more and more stuff until I can no longer fit any? Currently, there is no specification for this in CUDA, nor any explicit method to query for this at runtime."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I know how to time the execution of one CUDA kernel using CUDA events, which is great for simple cases. But in the real world, an algorithm is often made up of a series of kernels (CUB::DeviceRadixSort algorithms, for example, launch many kernels to get the job done). If you're running your algorithm on a system with a lot of other streams and kernels also in flight, it's not uncommon for the gaps between individual kernel launches to be highly variable based on what other work gets scheduled in-between launches on your stream. If I'm trying to make my algorithm work faster, I don't care so much about how long it spends sitting waiting for resources. I care about the time it spends actually executing. So the question is, is there some way to do something like the event API and insert a marker in the stream before the first kernel launches, and read it back after your last kernel launches, and have it tell you the actual amount of time spent executing on the stream, rather than the total end-to-end wall-clock time? Maybe something in CUPTI can do this?",
        "answers": [
            [
                "You can use Nsight Systems or Nsight Compute. (https://developer.nvidia.com/tools-overview) In Nsight Systems, you can profile timelines of each stream. Also, you can use Nsight Compute to profile details of each CUDA kernel. I guess Nsight Compute is better because you can inspect various metrics about GPU performances and get hints about the kernel optimization."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am using cuda graph stream capture API to implement a small demo with multi streams. Referenced by the CUDA Programming Guide here, I wrote the complete code. In my knowledge, kernelB should execute on stream1, but with nsys I found kernelB is executed on a complete new stream. It is under-control. The scheduling graph is showed below: Here is my code: #include &lt;iostream&gt; __global__ void kernelA() {} __global__ void kernelB() {} __global__ void kernelC() {} int main() { cudaStream_t stream1, stream2; cudaStreamCreate(&amp;stream1); cudaStreamCreate(&amp;stream2); cudaGraphExec_t graphExec = NULL; cudaEvent_t event1, event2; cudaEventCreate(&amp;event1); cudaEventCreate(&amp;event2); for (int i = 0; i &lt; 10; i++) { cudaGraph_t graph; cudaGraphExecUpdateResult updateResult; cudaGraphNode_t errorNode; cudaStreamBeginCapture(stream1, cudaStreamCaptureModeGlobal); kernelA&lt;&lt;&lt;512, 512, 0, stream1&gt;&gt;&gt;(); cudaEventRecord(event1, stream1); cudaStreamWaitEvent(stream2, event1, 0); kernelB&lt;&lt;&lt;256, 512, 0, stream1&gt;&gt;&gt;(); kernelC&lt;&lt;&lt;16, 512, 0, stream2&gt;&gt;&gt;(); cudaEventRecord(event2, stream2); cudaStreamWaitEvent(stream1, event2, 0); cudaStreamEndCapture(stream1, &amp;graph); if (graphExec != NULL) { cudaGraphExecUpdate(graphExec, graph, &amp;errorNode, &amp;updateResult); } if (graphExec == NULL || updateResult != cudaGraphExecUpdateSuccess) { if (graphExec != NULL) { cudaGraphExecDestroy(graphExec); } cudaGraphInstantiate(&amp;graphExec, graph, NULL, NULL, 0); } cudaGraphDestroy(graph); cudaGraphLaunch(graphExec, stream1); cudaStreamSynchronize(stream1); } }",
        "answers": [
            [
                "\"An operation may be scheduled at any time once the nodes on which it depends are complete. Scheduling is left up to the CUDA system.\" Here."
            ],
            [
                "I also ask in Nvidia Forums, Robert answered this question which help me a lot. Someone who are interested in the scheduling of cuda graph can also reference to this answer here."
            ]
        ],
        "votes": [
            2.0000001,
            2.0000001
        ]
    },
    {
        "question": "Suppose I call cuEventRecord(0, my_event_handle). cuEventRecord() requires the stream and the event to belong to the same context. Now, one can interpret the 0 as \"the default stream in the appropriate context\" - the requirements are satisfied and this should work; but one can also interpret it as \"the default stream in the current context\" - in which case if the current context is not the event's context - this should fail. Or it may all just be undefined/inconsistent behavior. My question: Is cuEventRecord() guaranteed to prefer one interpretation over the other?",
        "answers": [
            [
                "My personal tinkering suggests that the CUDA driver expects the current context to be the event and the stream's context. Perhaps it even expects that for any stream, not just the default one. Try this program: #include &lt;cuda/api.hpp&gt; #include &lt;iostream&gt; #include &lt;stdexcept&gt; int main() { cuda::initialize_driver(); auto pc_0 = cuda::device::primary_context::detail_::obtain_and_increase_refcount(0); auto pc_1 = cuda::device::primary_context::detail_::obtain_and_increase_refcount(1); cuda::context::current::detail_::push(pc_1); CUevent eh = cuda::event::detail_::create_raw_in_current_context(0); cuda::context::current::pop(); // At this point, the context stack is empty cuda::context::current::detail_::push(pc_0); CUstream default_stream_handle = nullptr; cuda::event::detail_::enqueue_in_current_context(default_stream_handle, eh); } with this commit of my cuda-api-wrappers library to see for yourself; if you replace pc_0 with pc_1 - it all works."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Suppose we have two CUDA streams running two CUDA kernels on a GPU at the same time. How can I pause the CUDA kernel running with the instruction I putting in the host code and resume it with the instruction in the host code? I have no idea how to write a sample code in this case, for example, to continue this question. Exactly my question is whether there is an instruction in CUDA that can pause a CUDA kernel running in a CUDA stream and then resume it?",
        "answers": [
            [
                "You can use dynamic parallelism with parameters for communication with host for the signals. Then launch a parent kernel with only 1 cuda thread and let it launch child kernels continuously until work is done or signal is received. If child kernel does not fully occupy the GPU, then it will lose performance. __global__ void parent(int * atomicSignalPause, int * atomicSignalExit, Parameters * prm) { int progress = 0; while(checkSignalExit(atomicSignalExit) &amp;&amp; progress&lt;100) { while(checkSignalPause(atomicSignalPause)) { child&lt;&lt;&lt;X,Y&gt;&gt;&gt;(prm,progress++); cudaDeviceSynchronize(); } } } There is no command to pause a stream. For multiple GPUs, you should use unified memory allocation for the communication (between GPUs). To overcome the gpu utilization issue, you may invent a task queue for child kernels. It pushes work N times (roughly enough to keep GPU efficient in power/compute), then for every completed child kernel it increments a dedicated counter in the parent kernel and pushes a new work, until all work is complete (while trying to keep concurrent kernels at N). Maybe something like this: // producer kernel // N: number of works that make gpu fully utilized while(hasWork) { // concurrency is a global parameter while(checkConcurrencyAtomic(concurrency)&lt;N) { incrementConcurrencyAtomic(concurrency); // a \"consumer\" parent kernel will get items from queue // it will decrement concurrency when a work is done bool success = myQueue.tryPush(work, concurrency); if(success) { // update status of whole work or signal the host } } // synchronization once per ~N work cudaDeviceSynchronize(); ... then check for pause signals and other tasks } If total work takes more than a few seconds, these atomic value updates shouldn't be a performance problem but if you have way too many child kernels to launch then you can launch more producer/consumer (parent) cuda-threads."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am beginner in CUDA. I am using NVIDIA Geforce GTX 1070 and CUDA toolkit 11.3 and ubuntu 18.04. As shown in the code below, I use two CPU threads to send two kernels in the form of two streams to a GPU. I want exactly these two kernels to be sent to the GPU at the same time. Is there a way to do this? Or at least better than what I did. Thank you in advance. My code: //Headers pthread_cond_t cond; pthread_mutex_t cond_mutex; unsigned int waiting; cudaStream_t streamZero, streamOne; //Kernel zero defined here __global__ void kernelZero(){...} //Kernel one defined here __global__ void kernelOne(){...} //This function is defined to synchronize two threads when sending kernels to the GPU. void threadsSynchronize(void) { pthread_mutex_lock(&amp;cond_mutex); if (++waiting == 2) { pthread_cond_broadcast(&amp;cond); } else { while (waiting != 2) pthread_cond_wait(&amp;cond, &amp;cond_mutex); } pthread_mutex_unlock(&amp;cond_mutex); } void *threadZero(void *_) { // ... threadsSynchronize(); kernelZero&lt;&lt;&lt;blocksPerGridZero, threadsPerBlockZero, 0, streamZero&gt;&gt;&gt;(); cudaStreamSynchronize(streamZero); // ... return NULL; } void *threadOne(void *_) { // ... threadsSynchronize(); kernelOne&lt;&lt;&lt;blocksPerGridOne, threadsPerBlockOne, 0, streamOne&gt;&gt;&gt;(); cudaStreamSynchronize(streamOne); // ... return NULL; } int main(void) { pthread_t zero, one; cudaStreamCreate(&amp;streamZero); cudaStreamCreate(&amp;streamOne); // ... pthread_create(&amp;zero, NULL, threadZero, NULL); pthread_create(&amp;one, NULL, threadOne, NULL); // ... pthread_join(zero, NULL); pthread_join(one, NULL); cudaStreamDestroy(streamZero); cudaStreamDestroy(streamOne); return 0; }",
        "answers": [
            [
                "Actually witnessing concurrent kernel behavior on a GPU has a number of requirements which are covered in other questions here on the SO cuda tag, so I'm not going to cover that ground. Let's assume your kernels have the possibility to run concurrently. In that case, you're not going to do any better than this, whether you use threading or not: cudaStream_t s1, s2; cudaStreaCreate(&amp;s1); cudaStreamCreate(&amp;s2); kernel1&lt;&lt;&lt;...,s1&gt;&gt;&gt;(...); kernel2&lt;&lt;&lt;...,s2&gt;&gt;&gt;(...); If your kernels have a \"long\" duration (much longer than the kernel launch overhead, approximately 5-50us) then they will appear to start at \"nearly\" the same time. You won't do better than this by switching to threading. The reason for this is not published as far as I know, so I will simply say that my own observations suggest to me that kernel launches to the same GPU are serialized by the CUDA runtime, somehow. You can find anecdotal evidence of this on various forums, and its fine if you don't believe me. There's also no reason to assume, with CPU threading mechanisms that I am familiar with, that CPU threads execute in lockstep. Therefore there is no reason to assume that a threading system will cause the kernel launch in two different threads to even be reached by the host threads at the same instant in time. You might do a small amount better by using the cudaLaunchKernel for kernel launch, rather than the triple-chevron launch syntax: &lt;&lt;&lt;...&gt;&gt;&gt;, but there really is no documentation to support this claim. YMMV. Keep in mind that the GPU is doing its best work as a throughput processor. There are no explicit mechanisms to ensure simultaneous kernel launch, and its unclear why you would need that."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Suppose I have a struct: typedef enum {ON_CPU,ON_GPU,ON_BOTH} memLocation; typedef struct foo *foo; struct foo { cudaEvent_t event; float *deviceArray; float *hostArray; memLocation arrayLocation; }; a function: void copyArrayFromCPUToGPUAsync(foo bar, cudaStream_t stream) { cudaStreamWaitEvent(stream, bar-&gt;event); if (bar-&gt;arrayLocation == ON_CPU) { // ON_CPU means !ON_GPU and !ON_BOTH cudaMemcpyAsync(cudaMemcpyHostToDevice, stream); bar-&gt;arrayLocation = ON_BOTH; } cudaEventRecord(bar-&gt;event, stream); } void doWorkOnGPUAsync(foo bar, cudaStream_t stream) { cudaStreamWaitEvent(stream, bar-&gt;event); // do async work cudaEventRecord(bar-&gt;event, stream); } And the following scenario (with a lion, witch, and wardrobe fitting in somewhere as well): // stream1, stream2, and stream3 have no prior work // assume bar-&gt;arrayLocation = ON_GPU doWorkOnGPUAsync(bar, stream1); copyArrayFromCPUToGPUAsync(bar, stream2); // A no-op doWorkOnGPUAsync(bar, stream3); Is the above safe? I.e. will stream2 still wait on stream1 to finish its \"work\" if it itself does no work? And will the resulting recorded cudaEvent reflect this, such that stream3 will not start until stream1 finishes?",
        "answers": [
            [
                "This should be safe. There is no mention anywhere (that I know) of some kind \"event cancellation\" due to lack of other work between a wait-on-event and the recording of another event. And it doesn't matter that you're re-using the same event object in the cudaEventRecord() call, since as the Runtime API docs say: cudaEventRecord() can be called multiple times on the same event and will overwrite the previously captured state. Other APIs such as cudaStreamWaitEvent() use the most recently captured state at the time of the API call, and are not affected by later calls to cudaEventRecord(). Additional notes: With your apparent use-case, you may also want to consider the possibility of using managed memory instead of manually copying back and forth. You should check for the success of your various operations, not just assume they succeeded."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "If I have a __constant__ value __constant__ float constVal; Which may or may not be initialized by MPI ranks on non-blocking streams: cudaMemcpyToSymbolAsync((void*)&amp;constVal,deviceValue,sizeof(float),0,cudaMemcpyDeviceToDevice,stream); Is this: Safe to be accessed by multiple MPI ranks simultaneously within kernels? I.e. do ranks share the same instance of val or do MPI semantics (they all have a private copy) still hold? If the above is safe, is it safe to be initialized by multiple MPI ranks?",
        "answers": [
            [
                "Safe to be accessed by multiple MPI ranks simultaneously within kernels? I.e. do ranks share the same instance of val or do MPI semantics (they all have a private copy) still hold? Neither. CUDA contexts are not shared amongst processes. If you have multiple processes you get multiple contexts, and each context has its own copy of all the statically defined symbols and code. This behaviour is independent of MPI semantics. If you are imagining that multiple processes in an MPI communicator are sharing the same GPU context and state, they aren't. If the above is safe, is it safe to be initialized by multiple MPI ranks? It isn't only safe, it is mandatory."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a single kernel which is feeling data to two parameters (dev_out_1 and dev_out_2) using single stream. I wanted to copy back the data from the device to host in parallel. my requirement is to use single stream and copy back to the host in parallel. How do you manage this kind of issues ? SomeCudaCall&lt;&lt;&lt;25,34&gt;&gt;&gt;(input, dev_out_1,dev_out_2); cudaMemcpyAsync(toHere_1, dev_out_1, sizeof(int), cudaMemcpyDeviceToHost,0); cudaMemcpyAsync(toHere_2, dev_out_2, sizeof(int), cudaMemcpyDeviceToHost,0);",
        "answers": [
            [
                "I wanted to copy back the data from the device to host in parallel That is not possible. NVIDIA GPUs can only use one DMA engine for device to host transfers (even in the case where there are more than one DMA engine), and the DMA engine can only perform a single transfer at a time. So \"parallel\" copies in the same direction over the PCI express bus are not possible."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "So this guide here shows the general way to overlap kernel execution and data transfer. cudaStream_t streams[nStreams]; for (int i = 0; i &lt; nStreams; ++i) { cudaStreamCreate(&amp;streams[i]); int offset = ...; cudaMemcpyAsync(&amp;d_a[offset], &amp;a[offset], streamBytes, cudaMemcpyHostToDevice, stream[i]); kernel&lt;&lt;&lt;streamSize/blockSize, blockSize, 0, stream[i]&gt;&gt;&gt;(d_a, offset); // edit: no deviceToHost copy } However, the kernel is serial. So it must process 0-&gt;1000, then 1000-&gt;2000, ... In short, the order to correctly perform this kernel while overlapping data transfer is: copy[a-&gt;b] must happen before kernel[a-&gt;b] kernel [a-&gt;b] must happen before kernel[b-&gt;c], where c &gt; a, b Is it possible to do this without using cudaDeviceSynchronize() ? If not, what's the fastest way to do it?",
        "answers": [
            [
                "So each kernel is dependent on (cannot begin until): The associated H-&gt;D copy is complete The previous kernel execution is complete Ordinary stream semantics won't handle this case (2 separate dependencies, from 2 separate streams), so we'll need to put an extra interlock in there. We can use a set of events and cudaStreamWaitEvent() to handle it. For the most general case (no knowledge of the total number of chunks) I would recommend something like this: $ cat t1783.cu #include &lt;iostream&gt; #include &lt;time.h&gt; #include &lt;sys/time.h&gt; #define USECPSEC 1000000ULL unsigned long long dtime_usec(unsigned long long start){ timeval tv; gettimeofday(&amp;tv, 0); return ((tv.tv_sec*USECPSEC)+tv.tv_usec)-start; } template &lt;typename T&gt; __global__ void process(const T * __restrict__ in, const T * __restrict__ prev, T * __restrict__ out, size_t ds){ for (size_t i = threadIdx.x+blockDim.x*blockIdx.x; i &lt; ds; i += gridDim.x*blockDim.x){ out[i] = in[i] + prev[i]; } } const int nTPB = 256; typedef int mt; const int chunk_size = 1048576; const int data_size = 10*1048576; const int ns = 3; int main(){ mt *din, *dout, *hin, *hout; cudaStream_t str[ns]; cudaEvent_t evt[ns]; for (int i = 0; i &lt; ns; i++) { cudaStreamCreate(str+i); cudaEventCreate( evt+i);} cudaMalloc(&amp;din, sizeof(mt)*data_size); cudaMalloc(&amp;dout, sizeof(mt)*data_size); cudaHostAlloc(&amp;hin, sizeof(mt)*data_size, cudaHostAllocDefault); cudaHostAlloc(&amp;hout, sizeof(mt)*data_size, cudaHostAllocDefault); cudaMemset(dout, 0, sizeof(mt)*chunk_size); // for first loop iteration for (int i = 0; i &lt; data_size; i++) hin[i] = 1; cudaEventRecord(evt[ns-1], str[ns-1]); // this event will immediately \"complete\" unsigned long long dt = dtime_usec(0); for (int i = 0; i &lt; (data_size/chunk_size); i++){ cudaStreamSynchronize(str[i%ns]); // so we can reuse event safely cudaMemcpyAsync(din+i*chunk_size, hin+i*chunk_size, sizeof(mt)*chunk_size, cudaMemcpyHostToDevice, str[i%ns]); cudaStreamWaitEvent(str[i%ns], evt[(i&gt;0)?(i-1)%ns:ns-1], 0); process&lt;&lt;&lt;(chunk_size+nTPB-1)/nTPB, nTPB, 0, str[i%ns]&gt;&gt;&gt;(din+i*chunk_size, dout+((i&gt;0)?(i-1)*chunk_size:0), dout+i*chunk_size, chunk_size); cudaEventRecord(evt[i%ns]); cudaMemcpyAsync(hout+i*chunk_size, dout+i*chunk_size, sizeof(mt)*chunk_size, cudaMemcpyDeviceToHost, str[i%ns]); } cudaDeviceSynchronize(); dt = dtime_usec(dt); for (int i = 0; i &lt; data_size; i++) if (hout[i] != (i/chunk_size)+1) {std::cout &lt;&lt; \"error at index: \" &lt;&lt; i &lt;&lt; \" was: \" &lt;&lt; hout[i] &lt;&lt; \" should be: \" &lt;&lt; (i/chunk_size)+1 &lt;&lt; std::endl; return 0;} std::cout &lt;&lt; \"elapsed time: \" &lt;&lt; dt &lt;&lt; \" microseconds\" &lt;&lt; std::endl; } $ nvcc -o t1783 t1783.cu $ ./t1783 elapsed time: 4366 microseconds Good practice here would be to use a profiler to verify the expected overlap scenarios. However, we can take a shortcut based on the elapsed time measurement. The loop is transferring a total of 40MB of data to the device, and 40MB back. The elapsed time is 4366us. This gives an average throughput for each direction of (40*1048576)/4366 or 9606 bytes/us which is 9.6GB/s. This is basically saturating the Gen3 link in both directions, therefore my chunk processing is approximately back-to-back, and I have essentially full overlap of D-&gt;H with H-&gt;D memcopies. The kernel here is trivial so it shows up as just slivers in the profile. For your case, you indicated you didn't need the D-&gt;H copy, but it adds no extra complexity so I chose to show it. The desired behavior still occurs if you comment that line out of the loop (although this affects results checking later). A possible criticism of this approach is that the cudaStreamSynchronize() call, which is necessary so we don't \"overrun\" the event interlock, means that the loop will only proceed to ns number of iterations beyond the one that is currently executing on the device. So it is not possible to launch more work asynchronously than that. If you wanted to launch all the work at once and go on and do something else on the CPU, this method will not fully allow that (the CPU will proceed past the loop when the stream processing has reach ns iterations from the last one). The code is presented to illustrate an approach, conceptually. It is not guaranteed to be defect free, nor do I claim it is suitable for any particular purpose."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "By default, the kernel will use all available SMs of the device (if enough blocks). However, now I have 2 stream with one computational-intense and one memory-intense, and I want to limit the maximal SMs used for 2 stream respectively (after setting the maximal SMs, the kernel in one stream will use up to maximal SMs, like 20SMs for computational-intense and 4SMs for memory-intense), is it possible to do so? (if possible, which API should I use)",
        "answers": [
            [
                "In short, no there is no way to do what you envisage. The CUDA execution model doesn't provide that sort of granularity, and that isn't an accident. By abstracting that level of scheduling and work distribution away, it means (within reason) any code you can run on the smallest GPU of a given architecture can also run on the largest without any modification. That is important from a portability and interoperability point of view."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to perform some image dilation using OpenCV &amp; CUDA. I invoke two calls to filter-&gt;apply(...) with a different filter object and on a different Mat, after each other, every time specifying a different stream to work with. They DO get executed in different streams, as can be seen from the attached nvvp profiling info, but they run sequentially, instead of in parallel. This seems to be caused, for some reason, by the CPU waiting for the stream (cudaStreamSynchronize). Why could OpenCV do that? I'm not calling the wait for the stream explicitly or anything, what else could be wrong? Here's the actual code: cv::Mat hIm1, hIm2; cv::imread(\"/path/im1.png\", cv::IMREAD_GRAYSCALE).convertTo(hIm1, CV_32FC1); cv::imread(\"/path/im2.png\", cv::IMREAD_GRAYSCALE).convertTo(hIm2, CV_32FC1); cv::cuda::GpuMat dIm1(hIm1); cv::cuda::GpuMat dIm2(hIm2); cv::cuda::Stream stream1, stream2; const cv::Mat strel1 = cv::getStructuringElement(cv::MORPH_ELLIPSE, cv::Size(41, 41)); cv::Ptr&lt;cv::cuda::Filter&gt; filter1 = cv::cuda::createMorphologyFilter(cv::MORPH_DILATE, dIm1.type(), strel1); const cv::Mat strel2 = cv::getStructuringElement(cv::MORPH_ELLIPSE, cv::Size(41, 41)); cv::Ptr&lt;cv::cuda::Filter&gt; filter2 = cv::cuda::createMorphologyFilter(cv::MORPH_DILATE, dIm2.type(), strel2); cudaDeviceSynchronize(); filter1-&gt;apply(dIm1, dIm1, stream1); filter2-&gt;apply(dIm2, dIm2, stream2); cudaDeviceSynchronize(); The images are sized 512\u00d7512; I tried it with smaller ones (down to 64\u00d764) but to no avail!",
        "answers": [
            [
                "It was user responsibility to run the application sequentially. Few Best Practices: Pipeline your code so that both CPU and GPU utilized at the same time. Make GPU call as asynchronous. GPU Need resources to run sequentially. If the filter1() utilized 100% of GPU, then the filter2() will wait in the pipeline until filter1() completes. Please check with the GPU utilization data in profiler for more details."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to overlap data transfers and kernel executions in a form like this: int numStreams = 3; int size = 10; for(int i = 0; i &lt; size; i++) { cuMemcpyHtoDAsync( _bufferIn1, _host_memoryIn1 ), _size * sizeof(T), cuda_stream[i % numStreams]); cuMemcpyHtoDAsync( _bufferIn2, _host_memoryIn2, _size * sizeof(T), cuda_stream[i % numStreams]); cuLaunchKernel( _kernel, gs.x(), gs.y(), gs.z(), bs.x(), bs.y(), bs.z(), _memory_size, cuda_stream[i % numStreams], _kernel_arguments, 0 ); cuEventRecord(event[i], cuda_stream); } for(int i = 0; i &lt; size; i++) { cuEventSynchronize(events[i]); cuMemcpyDtoHAsync( _host_memoryOut, _bufferOut, _size * sizeof(T), cuda_stream[i % numStreams]); } Is overlapping possible in this case? Currently only the HtoD-transfers overlap with the kernel executions. The first DtoH-transfer is executed after the last kernel execution.",
        "answers": [
            [
                "Overlapping is possible only when the operations are executed on different streams. CUDA operations in the same stream are executed sequentially by the host calling order so that the copy from the device to host at the end will be executed once all the operations on corresponding streams are completed. The overlap doesn't happen because both the last kernel and the first copy are executed on stream 0, so the copy has to wait for the kernel to finish. Since you are synchronizing with an event at each loop iteration, the other copies on the other streams (stream 1 and 2) are not called yet."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "When I am trying to capture stream execution to build CUDA graph, call to thrust::reduce causes a runtime error cudaErrorStreamCaptureUnsupported: operation not permitted when stream is capturing. I have tried returning the reduction result to both host and device variables, and I am calling reduction in a proper stream by the means of thrust::cuda::par.on(stream). Is there any way I can add thrust functions execution to CUDA graphs?",
        "answers": [
            [
                "Thrust's reduction operation is a blocking operation on the host side. I am assuming that you are using the result of reduction as a parameter to one of your following kernels. So that when you are capturing a CUDA graph, it cannot instantiate the graph executable because you are dependent on a variable that is on the host side but not available until the reduction kernel finishes execution. As a solution, you can try adding a host node to your graph that returns the result of the reduction."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to make some changes to the ResNet-18 model in PyTorch to invoke the execution of another auxiliary trained model which takes in the ResNet intermediate layer output at the end of each ResNet block as an input and makes some auxiliary predictions during the inference phase. I want to be able to do the auxiliary computation after the computation of a block in parallel to the computation of the next ResNet block so as to reduce the end-to-end latency of the entire pipeline execution on GPU. I have a base code that works correctly from the functionality perspective, but the execution of the auxiliary model is serial to the computation of the ResNet block. I verified this in two ways - By adding print statements and verifying the order of execution. By instrumenting the running time of the original ResNet model (say time t1) and the auxiliary model (say time t2). My execution time is currently t1+t2. The original ResNet block code (This is the BasicBlock since I am experimenting on ResNet-18). The entire code is available here class BasicBlock(nn.module): ... def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out This is my modification which works in a serial fashion - def forward(self, x): if len(x[0]) == self.auxiliary_prediction_size: # Got an Auxiliary prediction earlier return x # Do usual block computation residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) # Try to make an auxiliary prediction # First flatten the tensor (also assume for now that batch size is 1) batchSize = x.shape[0] intermediate_output = out.view(batchSize, -1) # Place the flattened on GPU device = torch.device(\"cuda:0\") input = intermediate_output.to(device) # Make auxiliary prediction auxiliary_input = out.float() auxiliary_prediction = self.auxiliary_model(auxiliary_input) if auxiliary_prediction meets some condition: return auxiliary_prediction # If no auxiliary prediction, then return intermediate output return out Understandably, the above code causes a data dependency between the execution of the auxiliary model and the next block and hence things to happen serially. The first solution I tried was to check if breaking this data dependency reduces latency. I tried doing so by allowing the auxiliary model to execute but not having the auxiliary_prediction return if the condition is met (Note that this would break functionality but this experiment was purely to understand the behavior). Essentially, what I did was - batchSize = x.shape[0] intermediate_output = out.view(batchSize, -1) # Place the flattened on GPU device = torch.device(\"cuda:0\") input = intermediate_output.to(device) # Make auxiliary prediction auxiliary_input = out.float() auxiliary_prediction = self.auxiliary_model(auxiliary_input) if auxiliary_prediction meets some condition: # Comment out return to break data dependency #return auxiliary_prediction # If no auxiliary prediction, then return intermediate output return out However, this did not work and upon researching further, I stumbled upon CUDA streams at this Stack Overflow link. I tried incorporating the idea of CUDA streams to solve my problem in the below way - def forward(self, x): if len(x[0]) == self.auxiliary_prediction_size: # Got an Auxiliary prediction earlier return x s1 = torch.cuda.Stream() s2 = torch.cuda.Stream() with torch.cuda.Stream(s1): # Do usual block computation residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) with torch.cuda.Stream(s2): # Try to make an auxiliary prediction # First flatten the tensor (also assume for now that batch size is 1) out_detach = out.detach() # Detach from backprop flow and from computational graph dependency batchSize = x.shape[0] intermediate_output = out_detach.view(batchSize, -1) # Place the flattened on GPU device = torch.device(\"cuda:0\") input = intermediate_output.to(device) # Make auxiliary prediction auxiliary_input = out_detach.float() auxiliary_prediction = self.auxiliary_model(auxiliary_input) if auxiliary_prediction meets some condition: return auxiliary_prediction # If no auxiliary prediction, then return intermediate output return out However, the output from Nvidia Visual Profiler still indicates that all work is still being done on the default stream and still serialized. Note that I did verify with a small CUDA program that CUDA streams is supported by the CUDA version I am using. My questions - Why does breaking the data dependency not cause PyTorch to schedule the computations in parallel? I thought this was the point of the dynamic computation graphs in PyTorch. Why does using CUDA streams not delegate the computation to non-default streams? Are there alternative approaches to execute the auxiliary model asynchronously/parallelly to the ResNet block computation?",
        "answers": [],
        "votes": []
    },
    {
        "question": "My CUDA application performs an associative reduction over a volume. Essentially each thread computes values which are atomically added to overlapping locations of the same output buffer in global memory. Is it possible to concurrently launch this kernel with different input parameters and the same output buffer? In other words, each kernel would share the same global buffer and write to it atomically. All kernels are running on the same GPU.",
        "answers": [
            [
                "Yes, it's possible. atomic operations to global memory are device-wide. They will be atomic with respect to any code running on the device."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am developing a Multi-GPU accelerated Flow solver. Currently I am trying to implement communication hiding. That means, while data is exchanged the GPU computes the part of the mesh, that is not involved in communication and computes the rest of the mesh, once communication is done. I am trying to solve this by having one stream (computeStream) for the long run time kernel (fluxKernel) and one (communicationStream) for the different phases of communication. The computeStream has a very low priority, in order to allow kernels on the communicationStream to interleave the fluxKernel, even though it uses all resources. These are the streams I am using: int priority_high, priority_low; cudaDeviceGetStreamPriorityRange(&amp;priority_low , &amp;priority_high ) ; cudaStreamCreateWithPriority (&amp;communicationStream, cudaStreamNonBlocking, priority_high ); cudaStreamCreateWithPriority (&amp;computeStream , cudaStreamNonBlocking, priority_low ); The desired cocurrency pattern looks like this: I need synchronization of the communicationStream before I send the data via MPI, to ensure that the data is completely downloaded, before I send it on. In the following listing I show the structure of what I am currently doing. First I start the long run time fluxKernel for the main part of the mesh on the computeStream. Then I start a sendKernel that collects the data that should be send to the second GPU and subsequently download it to the host (I cannot use cuda-aware MPI due to hardware limitations). The data is then send non-blocking per MPI_Isend and blocking receive (MPI_recv) is used subsequently. When the data is received the procedure is done backwards. First the data is uploaded to the device and then spread to the main data structure by recvKernel. Finally the fluxKernel is called for the remaining part of the mesh on the communicationStream. Note, that before and after the shown code kernels are run on the default stream. { ... } // Preparations // Start main part of computatation on first stream fluxKernel&lt;&lt;&lt; ..., ..., 0, computeStream &gt;&gt;&gt;( /* main Part */ ); // Prepare send data sendKernel&lt;&lt;&lt; ..., ..., 0, communicationStream &gt;&gt;&gt;( ... ); cudaMemcpyAsync ( ..., ..., ..., cudaMemcpyDeviceToHost, communicationStream ); cudaStreamSynchronize( communicationStream ); // MPI Communication MPI_Isend( ... ); MPI_Recv ( ... ); // Use received data cudaMemcpyAsync ( ..., ..., ..., cudaMemcpyHostToDevice, communicationStream ); recvKernel&lt;&lt;&lt; ..., ..., 0, communicationStream &gt;&gt;&gt;( ... ); fluxKernel&lt;&lt;&lt; ..., ..., 0, communicationStream &gt;&gt;&gt;( /* remaining Part */ ); { ... } // Rest of the Computations I used nvprof and Visual Profiler to see, whether the stream actually execute concurrently. This is the result: I observe that the sendKernel (purple), upload, MPI communication and download are concurrent to the fluxKernel. The recvKernel (red) only starts ofter the other stream is finished, though. Turning of the synchronization does not solve the problem: For my real application I have not only one communication, but multiple. I tested this with two communications as well. The procedure is: sendKernel&lt;&lt;&lt; ..., ..., 0, communicationStream &gt;&gt;&gt;( ... ); cudaMemcpyAsync ( ..., ..., ..., cudaMemcpyDeviceToHost, communicationStream ); cudaStreamSynchronize( communicationStream ); MPI_Isend( ... ); sendKernel&lt;&lt;&lt; ..., ..., 0, communicationStream &gt;&gt;&gt;( ... ); cudaMemcpyAsync ( ..., ..., ..., cudaMemcpyDeviceToHost, communicationStream ); cudaStreamSynchronize( communicationStream ); MPI_Isend( ... ); MPI_Recv ( ... ); cudaMemcpyAsync ( ..., ..., ..., cudaMemcpyHostToDevice, communicationStream ); recvKernel&lt;&lt;&lt; ..., ..., 0, communicationStream &gt;&gt;&gt;( ... ); MPI_Recv ( ... ); cudaMemcpyAsync ( ..., ..., ..., cudaMemcpyHostToDevice, communicationStream ); recvKernel&lt;&lt;&lt; ..., ..., 0, communicationStream &gt;&gt;&gt;( ... ); The result is similar to the one with one communication (above), in the sense that the second kernel invocation (this time it is a sendKernel) is delayed till the kernel on the computeStream is finished. Hence the overall observation is, that the second kernel invocation is delayed, independent of which kernel this is. Can you explain, why the GPU is synchronizing in this way, or how I can get the second Kernel on communicationStream to also run concurrently to the computeStream? Thank you very much. Edit 1: complete rework of the question Minimal Reproducible Example I built a minimal reproducible Example. In the end the code plots the int data to the terminal. The correct last value would be 32778 (=(32*1024-1) + 1 + 10). At the beginning I added an option integer to test 3 different options: 0: Intended version with synchronisation before CPU modification of data 1: Same as 0, but without synchronization 2: dedicated stream for memcpys and no syncronization #include &lt;iostream&gt; #include &lt;cuda.h&gt; #include &lt;cuda_runtime.h&gt; #include &lt;device_launch_parameters.h&gt; const int option = 0; const int numberOfEntities = 2 * 1024 * 1024; const int smallNumberOfEntities = 32 * 1024; __global__ void longKernel(float* dataDeviceIn, float* dataDeviceOut, int numberOfEntities) { int index = blockIdx.x * blockDim.x + threadIdx.x; if(index &gt;= numberOfEntities) return; float tmp = dataDeviceIn[index]; #pragma unroll for( int i = 0; i &lt; 2000; i++ ) tmp += 1.0; dataDeviceOut[index] = tmp; } __global__ void smallKernel_1( int* smallDeviceData, int numberOfEntities ) { int index = blockIdx.x * blockDim.x + threadIdx.x; if(index &gt;= numberOfEntities) return; smallDeviceData[index] = index; } __global__ void smallKernel_2( int* smallDeviceData, int numberOfEntities ) { int index = blockIdx.x * blockDim.x + threadIdx.x; if(index &gt;= numberOfEntities) return; int value = smallDeviceData[index]; value += 10; smallDeviceData[index] = value; } int main(int argc, char **argv) { cudaSetDevice(0); float* dataDeviceIn; float* dataDeviceOut; cudaMalloc( &amp;dataDeviceIn , sizeof(float) * numberOfEntities ); cudaMalloc( &amp;dataDeviceOut, sizeof(float) * numberOfEntities ); int* smallDataDevice; int* smallDataHost; cudaMalloc ( &amp;smallDataDevice, sizeof(int) * smallNumberOfEntities ); cudaMallocHost( &amp;smallDataHost , sizeof(int) * smallNumberOfEntities ); cudaStream_t streamLong; cudaStream_t streamSmall; cudaStream_t streamCopy; int priority_high, priority_low; cudaDeviceGetStreamPriorityRange(&amp;priority_low , &amp;priority_high ) ; cudaStreamCreateWithPriority (&amp;streamLong , cudaStreamNonBlocking, priority_low ); cudaStreamCreateWithPriority (&amp;streamSmall, cudaStreamNonBlocking, priority_high ); cudaStreamCreateWithPriority (&amp;streamCopy , cudaStreamNonBlocking, priority_high ); ////////////////////////////////////////////////////////////////////////// longKernel &lt;&lt;&lt; numberOfEntities / 32, 32, 0, streamLong &gt;&gt;&gt; (dataDeviceIn, dataDeviceOut, numberOfEntities); ////////////////////////////////////////////////////////////////////////// smallKernel_1 &lt;&lt;&lt; smallNumberOfEntities / 32, 32, 0 , streamSmall &gt;&gt;&gt; (smallDataDevice, smallNumberOfEntities); if( option &lt;= 1 ) cudaMemcpyAsync( smallDataHost, smallDataDevice, sizeof(int) * smallNumberOfEntities, cudaMemcpyDeviceToHost, streamSmall ); if( option == 2 ) cudaMemcpyAsync( smallDataHost, smallDataDevice, sizeof(int) * smallNumberOfEntities, cudaMemcpyDeviceToHost, streamCopy ); if( option == 0 ) cudaStreamSynchronize( streamSmall ); // some CPU modification of data for( int i = 0; i &lt; smallNumberOfEntities; i++ ) smallDataHost[i] += 1; if( option &lt;= 1 ) cudaMemcpyAsync( smallDataDevice, smallDataHost, sizeof(int) * smallNumberOfEntities, cudaMemcpyHostToDevice, streamSmall ); if( option == 2 ) cudaMemcpyAsync( smallDataDevice, smallDataHost, sizeof(int) * smallNumberOfEntities, cudaMemcpyHostToDevice, streamCopy ); smallKernel_2 &lt;&lt;&lt; smallNumberOfEntities / 32, 32, 0 , streamSmall &gt;&gt;&gt; (smallDataDevice, smallNumberOfEntities); ////////////////////////////////////////////////////////////////////////// cudaDeviceSynchronize(); cudaMemcpy( smallDataHost, smallDataDevice, sizeof(int) * smallNumberOfEntities, cudaMemcpyDeviceToHost ); for( int i = 0; i &lt; smallNumberOfEntities; i++ ) std::cout &lt;&lt; smallDataHost[i] &lt;&lt; \"\\n\"; return 0; } With code I see the same behavior as described above: Option 0 (correct result): Option 1 (wrong reslut, +1 from CPU missing): Option 2 (completely wrong result, all 10, dowload before smallKernel_1) Solutions: Running Option 0 under Linux (on the suggestion in Roberts answere), brings the expected behavior!",
        "answers": [
            [
                "Here's how I would try to accomplish this. Use a high-priority/low-priority stream arrangement as you suggest. Only 2 streams should be needed Make sure to pin host memory to allow compute/copy overlap Since you don't intend to use cuda-aware MPI, your MPI transactions are purely host activity. Therefore we can use a stream callback to insert this host activity into the high-priority stream. To allow the high-priority kernels to easily insert themselves into the low-priority kernels, I choose a design strategy of grid-stride-loop for the high priority copy kernels, but non-grid-stride-loop for low priority kernels. We want the low priority kernels to have a larger number of blocks, so that blocks are launching and retiring all the time, easily allowing the GPU block scheduler to insert high-priority blocks as they become available. The work issuance per \"frame\" uses no synchronize calls of any kind. I am using a cudaDeviceSynchronize() once per loop/frame, to break (separate) the processing of one frame from the next. Arrangement of activities within a frame is handled entirely with CUDA stream semantics, to enforce serialization for activities which depend on each other, but to allow concurrency for activities that don't. Here's a sample code that implements these ideas: #include &lt;iostream&gt; #include &lt;unistd.h&gt; #include &lt;cstdio&gt; #define cudaCheckErrors(msg) \\ do { \\ cudaError_t __err = cudaGetLastError(); \\ if (__err != cudaSuccess) { \\ fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\ msg, cudaGetErrorString(__err), \\ __FILE__, __LINE__); \\ fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\ exit(1); \\ } \\ } while (0) typedef double mt; const int nTPB = 512; const size_t ds = 100ULL*1048576; const size_t bs = 1048576ULL; const int my_intensity = 1; const int loops = 4; const size_t host_func_delay_us = 100; const int max_blocks = 320; // chosen based on GPU, could use runtime calls to set this via cudaGetDeviceProperties template &lt;typename T&gt; __global__ void fluxKernel(T * __restrict__ d, const size_t n, const int intensity){ size_t idx = ((size_t)blockDim.x) * blockIdx.x + threadIdx.x; if (idx &lt; n){ T temp = d[idx]; for (int i = 0; i &lt; intensity; i++) temp = sin(temp); // just some dummy code to simulate \"real work\" d[idx] = temp; } } template &lt;typename T&gt; __global__ void sendKernel(const T * __restrict__ d, const size_t n, T * __restrict__ b){ for (size_t idx = ((size_t)blockDim.x) * blockIdx.x + threadIdx.x; idx &lt; n; idx += ((size_t)blockDim.x)*gridDim.x) b[idx] = d[idx]; } template &lt;typename T&gt; __global__ void recvKernel(const T * __restrict__ b, const size_t n, T * __restrict__ d){ for (size_t idx = ((size_t)blockDim.x) * blockIdx.x + threadIdx.x; idx &lt; n; idx += ((size_t)blockDim.x)*gridDim.x) d[idx] = b[idx]; } void CUDART_CB MyCallback(cudaStream_t stream, cudaError_t status, void *data){ printf(\"Loop %lu callback\\n\", (size_t)data); usleep(host_func_delay_us); // simulate: this is where non-cuda-aware MPI calls would go, operating on h_buf } int main(){ // get the range of stream priorities for this device int priority_high, priority_low; cudaDeviceGetStreamPriorityRange(&amp;priority_low, &amp;priority_high); // create streams with highest and lowest available priorities cudaStream_t st_high, st_low; cudaStreamCreateWithPriority(&amp;st_high, cudaStreamNonBlocking, priority_high); cudaStreamCreateWithPriority(&amp;st_low, cudaStreamNonBlocking, priority_low); // allocations mt *h_buf, *d_buf, *d_data; cudaMalloc(&amp;d_data, ds*sizeof(d_data[0])); cudaMalloc(&amp;d_buf, bs*sizeof(d_buf[0])); cudaHostAlloc(&amp;h_buf, bs*sizeof(h_buf[0]), cudaHostAllocDefault); cudaCheckErrors(\"setup error\"); // main processing loop for (unsigned long i = 0; i &lt; loops; i++){ // issue low-priority fluxKernel&lt;&lt;&lt;((ds-bs)+nTPB)/nTPB, nTPB,0,st_low&gt;&gt;&gt;(d_data+bs, ds-bs, my_intensity); // issue high-priority sendKernel&lt;&lt;&lt;max_blocks,nTPB,0,st_high&gt;&gt;&gt;(d_data, bs, d_buf); cudaMemcpyAsync(h_buf, d_buf, bs*sizeof(h_buf[0]), cudaMemcpyDeviceToHost, st_high); cudaStreamAddCallback(st_high, MyCallback, (void*)i, 0); cudaMemcpyAsync(d_buf, h_buf, bs*sizeof(h_buf[0]), cudaMemcpyHostToDevice, st_high); recvKernel&lt;&lt;&lt;max_blocks,nTPB,0,st_high&gt;&gt;&gt;(d_buf, bs, d_data); fluxKernel&lt;&lt;&lt;((bs)+nTPB)/nTPB, nTPB,0,st_high&gt;&gt;&gt;(d_data, bs, my_intensity); cudaDeviceSynchronize(); cudaCheckErrors(\"loop error\"); } return 0; } Here is the visual profiler timeline output (on linux, Tesla V100): Note that arranging complex concurrency scenarios can be quite challenging on Windows WDDM. I would recommend avoiding that, and this answer does not intend to discuss all the challenges there. I suggest using linux or Windows TCC GPUs to do this. If you try this code on your machine, you may need to adjust some of the various constants to get things to look like this."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm working on a CUDA matrix multiplication, but I did some modifications to observe how they affect performances. I want to observe the behavior and performances of a matrix multiplication kernel, making some changes. I'm measuring the changes in GPU events time, I'm testing it in two speicific different conditions: I have an amount of matrices (say matN) for A, B and C, then I transfer (H2D) one matrix for A, one for B and multply them, to transfer back (D2H) one C; I have matN for A, B and C, but I transfer &gt;1(say chunk) matrices for A and for B, I compute exactly chunk multiplications, and transfer back chunk result matrices C. In the first case (chunk = 1) all works as expected, but in the second case (chunk &gt; 1) I get some of Cs are correct, while others are not. But if I put a cudaDeviceSynchronize() after the cudaMemcpyAsync, I get correct results. Here's the code doing what I've just described: /**** main.cpp ****/ int chunk = matN/iters; #ifdef LOWPAR GRIDx= 1; GRIDy= 1; label=\"LOW\"; #else int sizeX = M; int sizeY = N; GRIDx = ceil((sizeX)/BLOCK); GRIDy = ceil((sizeY)/BLOCK); label=\"\"; #endif const int bytesA = M*K*sizeof(float); const int bytesB = K*N*sizeof(float); const int bytesC = M*N*sizeof(float); //device mem allocation float *Ad, *Bd, *Cd; gpuErrchk( cudaMalloc((void **)&amp;Ad, bytesA*chunk) ); gpuErrchk( cudaMalloc((void **)&amp;Bd, bytesB*chunk) ); gpuErrchk( cudaMalloc((void **)&amp;Cd, bytesC*chunk) ); //host pinned mem allocation float *A, *B, *C; gpuErrchk( cudaMallocHost((void **)&amp;A, bytesA*matN) ); gpuErrchk( cudaMallocHost((void **)&amp;B, bytesB*matN) ); gpuErrchk( cudaMallocHost((void **)&amp;C, bytesC*matN) ); //host data init for(int i=0; i&lt;matN; ++i){ randomMatrix(M, K, A+(i*M*K)); randomMatrix(K, N, B+(i*K*N)); } //event start createAndStartEvent(&amp;startEvent, &amp;stopEvent); if (square) { label += \"SQUARE\"; int size = N*N; for (int i = 0; i &lt; iters; ++i) { int j = i%nStream; int idx = i*size*chunk; newSquareMatMulKer(A+idx, B+idx, C+idx, Ad, Bd, Cd, N, chunk, stream[j]); } } else { ... } msTot = endEvent(&amp;startEvent, &amp;stopEvent); #ifdef MEASURES printMeasures(square, label, msTot, millis.count(), matN, iters, devId); #else float *_A, *_B, *_C, *tmpC; tmpC = (float *)calloc(1,bytesC*chunk); for (int s=0; s&lt;matN; ++s) { _A = A+(s*M*K); _B = B+(s*K*N); _C = C+(s*M*N); memset(tmpC, 0, bytesC*chunk); hostMatMul(_A, _B, tmpC, M, K, N); checkMatEquality(_C, tmpC, M, N); } #endif /**** matmul.cu ****/ __global__ void squareMatMulKernel(float* A, float* B, float* C, int N, int chunk) { int ROW = blockIdx.x*blockDim.x+threadIdx.x; int COL = blockIdx.y*blockDim.y+threadIdx.y; if (ROW&lt;N &amp;&amp; COL&lt;N) { int size=N*N; int offs = 0; float tmpSum=0.0f; for (int s=0; s&lt;chunk; ++s) { offs = s*size; tmpSum = 0.0f; for (int i = 0; i &lt; N; ++i) { tmpSum += A[offs+(ROW*N)+i] * B[offs+(i*N)+COL]; } C[offs+(ROW*N)+COL] = tmpSum; } } return ; } void newSquareMatMulKer(float *A, float *B, float *C, float *Ad, float *Bd, float *Cd, int n, int chunk, cudaStream_t strm) { int size = n*n; int bytesMat = size*sizeof(float); dim3 dimBlock(BLOCK,BLOCK,1); dim3 dimGrid(GRIDx, GRIDy,1); gpuErrchk( cudaMemcpyAsync(Ad, A, bytesMat*chunk, cudaMemcpyHostToDevice, strm) ); gpuErrchk( cudaMemcpyAsync(Bd, B, bytesMat*chunk, cudaMemcpyHostToDevice, strm) ); #ifdef LOWPAR squareMatMulGridStrideKer&lt;&lt;&lt;dimGrid, dimBlock, 0, strm&gt;&gt;&gt;(Ad, Bd, Cd, n, chunk); #else squareMatMulKernel&lt;&lt;&lt;dimGrid, dimBlock, 0, strm&gt;&gt;&gt;(Ad, Bd, Cd, n, chunk); #endif squareMatMulKernel&lt;&lt;&lt;dimGrid, dimBlock, 0, strm&gt;&gt;&gt;(Ad, Bd, Cd, n, chunk); gpuErrchk( cudaMemcpyAsync( C, Cd, bytesMat*chunk, cudaMemcpyDeviceToHost, strm) ); cudaDeviceSynchronize(); ^ ^ ^ ^ ^ ^ } I tried to debug using cuda-gdb but nothing strange showed up, gpuErrchk doesn't throw errors in CUDA API calls. I run the code using memcheck too, both with and without cudaDeviceSynchronize and I got no error. I think it can be a synchronization issue, but I can't understand the reason behind that. Can someone spot where I'm wrong? Other code advices are really appreciated too.",
        "answers": [
            [
                "If you are using multiples streams, you may override Ad and Bd before using them. Example with iters = 2 and nStream = 2 : for (int i = 0; i &lt; iters; ++i) { int j = i%nStream; int idx = i*size*chunk; newSquareMatMulKer(A+idx, B+idx, C+idx, Ad, Bd, Cd, N, chunk, stream[j]); } From this loop, you will call newSquareMatMulKer(A, B, C, Ad, Bd, Cd, N, chunk, stream[0]); // call 0 newSquareMatMulKer(A+idx, B+idx, C+idx, Ad, Bd, Cd, N, chunk, stream[1]); // call 1 As you are using the same memory area on device for both call, you may have several synchronizations issues: call 1 start to copy A and B on device before call 0:squareMatMulKernel end, so you may use incorrect values of A and/or B to compute your first iteration. call 1:squareMatMulKernel start before you retrieve the values of C from call 0, so you may override C with values from call 1. To fix this problem, I see two approaches: Using synchronization as in your example with cudaDeviceSynchronize();. You can allocate more memory two device side (one workspace per stream), for example. '' //device mem allocation float *Ad, *Bd, *Cd; gpuErrchk( cudaMalloc((void **)&amp;Ad, bytesA*chunk*nStream) ); gpuErrchk( cudaMalloc((void **)&amp;Bd, bytesB*chunk*nStream) ); gpuErrchk( cudaMalloc((void **)&amp;Cd, bytesC*chunk*nStream) ); /* code here */ for (int i = 0; i &lt; iters; ++i) { int j = i%nStream; int idx = i*size*chunk; int offset_stream = j*size*chunk; newSquareMatMulKer(A+idx, B+idx, C+idx, Ad + offset_stream , Bd + offset_stream , Cd + offset_stream , N, chunk, stream[j]); } In this case you don't need synchronization before the end of the loop."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I always thought that Hyper-Q technology is nothing but the streams in GPU. Later I found I was wrong(Am I?). So I was doing some reading about Hyper-Q and got confused more. I was going through one article and it had these two statements: A. Hyper-Q is a flexible solution that allows separate connections from multiple CUDA streams, from multiple Message Passing Interface (MPI) processes, or even from multiple threads within a process B. Hyper-Q increases the total number of connections (work queues) between the host and the GK110 GPU by allowing 32 simultaneous, hardware-managed connections (compared to the single connection available with Fermi) In aforementioned points, Point B says that there can be multiple connected created to a single GPU from host. Does it mean I can create multiple context on a simple GPU through different applications? Does it mean that I will have to execute all applications on different streams?What if all my connections are memory and compute resource consuming, who manages the resource (memory/cores) scheduling?",
        "answers": [
            [
                "Think of HyperQ as streams implemented in hardware on the device side. Before the arrival of HyperQ, e.g. on Fermi, commands (kernel launches, memory transfers, etc.) from all streams were placed in a single work queue by the driver on the host. That meant that commands could not overtake each other, and you had to be careful issuing them in the right order on the host to achieve best overlap. On the GK110 GPU and later devices with HyperQ, there are (at least) 32 work queues on the device. This means that commands from different queues can be reordered relative to each other until they start execution. So both orderings in the example linked above lead to good overlap on a GK110 device. This is particularly important for multithreaded host code, where you can't control the order without additional synchronization between threads. Note that of the 32 hardware queues only 8 are used by default to save resources. Set the CUDA_\u200bDEVICE_\u200bMAX_\u200bCONNECTIONS environment variable to a higher value if you need more."
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "I am using OpenCV 3.4 with cuda libraries to process video images. Image is grabbed and uploaded over the device using GpuMat::upload(). Afterward the image is thresholded twice to create 2 different binary images (Th1 and Th2). My first question is that: Can I use cuda streams in both threshold functions to run at the same time? Is that a problem since they are both using the same GpuMat as input. After the thresholding I will be using both binary GpuMats to do more processing on them using other cv::cuda functions. The second question is: Should I use Stream::waitForCompletion() to wait for the thresholding streams to be finished before using Th1 and Th2 for further processing? or this data dependency is automatically detected? basically I am trying to process this 2 binary images from here in parallel, not to process first Th1 and then th2. They are going to be processed exactly with similar cuda functions but with different values.. I am using cuda 9.0. Is it still a problem if same operation is enqueued twice with different data to different streams? an example of my code is shown below: src.upload(IMG); //upload cv::Mat to device //threashold src with different values and write them in dst0 and dst1 // Can I have 2 stream on the same functions that are both using src as input? cv::cuda::threshold(src, dst0, 10, 1, THRESH_BINARY, s0); //Stream s0 cv::cuda::threshold(src, dst1, 40, 1, THRESH_BINARY, s1); // Stream S1 s0.waitForCompletion();// I dont know if this is necessary cv::cuda::reduce(dst0, Vx0, 0, CV_REDUCE_SUM, CV_32S); s1.waitForCompletion();// also this one cv::cuda::reduce(dst1, Vx1, 0, CV_REDUCE_SUM, CV_32S);",
        "answers": [],
        "votes": []
    },
    {
        "question": "My program is a pipeline, which contains multiple kernels and memcpys. Each task will go through the same pipeline with different input data. The host code will first chooses a Channel, an encapsulation of scratchpad memory and CUDA objects, when it process a task. And after the last stage, I will record an event then will go to process next task. The main pipeline logic is in the following. The problem is that operations in different streams are not overlapping. I attached the timeline of processing 10 tasks. You can see none operations in streams are overlapped. For each kernel, there is 256 threads in a block and 5 blocks in a grid. All buffers used for memcpy are pinned, I am sure that I have meet those requirements for overlapping kernel execution and data transfers. Can someone help me figure out the reason? Thanks. Environment information GPU: Tesla K40m (GK110) Max Warps/SM: 64 Max Thread Blocks/SM: 16 Max Threads/SM: 2048 CUDA version: 8.0 void execute_task_pipeline(int stage, MyTask *task, Channel *channel) { assert(channel-&gt;taken); assert(!task-&gt;finish()); GPUParam *para = &amp;channel-&gt;para; assert(para-&gt;col_num &gt; 0); assert(para-&gt;row_num &gt; 0); // copy vid_list to device CUDA_ASSERT( cudaMemcpyAsync(para-&gt;vid_list_d, task-&gt;vid_list.data(), sizeof(uint) * para-&gt;row_num, cudaMemcpyHostToDevice, channel-&gt;stream) ); k_get_slot_id_list&lt;&lt;&lt;WK_GET_BLOCKS(para-&gt;row_num), WK_CUDA_NUM_THREADS, 0, channel-&gt;stream&gt;&gt;&gt;( vertices_d, para-&gt;vid_list_d, para-&gt;slot_id_list_d, config.num_buckets, para-&gt;row_num); k_get_edge_list&lt;&lt;&lt;WK_GET_BLOCKS(para-&gt;row_num), WK_CUDA_NUM_THREADS, 0, channel-&gt;stream&gt;&gt;&gt;( vertices_d, para-&gt;slot_id_list_d, para-&gt;edge_size_list_d, para-&gt;offset_list_d, para-&gt;row_num); k_calc_prefix_sum(para, channel-&gt;stream); k_update_result_table_k2u&lt;&lt;&lt;WK_GET_BLOCKS(para-&gt;row_num), WK_CUDA_NUM_THREADS, 0, channel-&gt;stream&gt;&gt;&gt;( edges_d, para-&gt;vid_list_d, para-&gt;updated_result_table_d, para-&gt;prefix_sum_list_d, para-&gt;offset_list_d, para-&gt;col_num, para-&gt;row_num); para-&gt;col_num += 1; // copy result back to host CUDA_ASSERT( cudaMemcpyAsync(&amp;(channel-&gt;num_new_rows), para-&gt;prefix_sum_list_d + para-&gt;row_num - 1, sizeof(uint), cudaMemcpyDeviceToHost, channel-&gt;stream) ); // copy result to host memory CUDA_ASSERT( cudaMemcpyAsync(channel-&gt;h_buf, para-&gt;updated_result_table_d, channel-&gt;num_new_rows * (para-&gt;col_num + 1), cudaMemcpyDeviceToHost, channel-&gt;stream) ); // insert a finish event in the end of pipeline CUDA_ASSERT( cudaEventRecord(channel-&gt;fin_event, channel-&gt;stream) ); }",
        "answers": [
            [
                "are you trying to overlap treatments which are during 82microsecs ? Since you have profiled your application, the clue can be in the big orange box between two kernel execution (wich is not readable in your image). If this is a synchronisation remove it. If this is a trace like cudaLaunch_KernelName, try to make your treatments bigger (more datas or more computations) because you take more time sending an order to the GPU than it takes to execute it, so you can't make parallel computations into these different streams."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am having the following performance problem with CUDA. When I run a simple sample code on a Titan V and Titan X card, the running times are fine as expected. Titan X: 0.269299 ms Titan V: 0.111766 ms Now, when I add another kernel in the code, which uses dynamic parallelism, but still do not call it or use it at all, the performance in Volta GPU goes down drastically but on other cards the performance is not affected. Titan X: 0.270602 ms Titan V: 1.999299 ms It is important to put emphasis on the fact that this second kernel is not used at all, it just sits next to the rest of the code, i.e., it is only compiled with the rest of the code. One can also comment the recursive kernel calls along with the stream creation, and see that the running times for Volta become good again. I suspect that the presence of dynamic parallelism has a negative effect on the code, even when it is not used at all ar runtime. Any ideas on how to approach this problem? #include &lt;cuda.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;time.h&gt; #include &lt;math.h&gt; __global__ void MetNormal(int *a_d,const int N ); __global__ void rec(int *a_d ,int Nm,int xi, int yi, int zi, const int BlockSize){ int x=blockIdx.x*blockDim.x+threadIdx.x+xi; int y=blockIdx.y*blockDim.y+threadIdx.y+yi; int z=blockIdx.z*blockDim.z+threadIdx.z+zi; int Nbloques= (Nm+BlockSize-1)/BlockSize; dim3 b(BlockSize,BlockSize,BlockSize); dim3 g(Nbloques,Nbloques,Nbloques); cudaStream_t s1;//line of code to comment cudaStreamCreateWithFlags(&amp;s1,cudaStreamNonBlocking);// line of code to comment rec&lt;&lt;&lt;g,b,0,s1&gt;&gt;&gt;(a_d,Nm,xi,yi,zi,BlockSize);//line of code to comment } __global__ void MetNormal(int *a_d,const int N){ int x= blockIdx.x*blockDim.x+threadIdx.x; int y= blockIdx.y*blockDim.y+threadIdx.y; int z= blockIdx.z*blockDim.z+threadIdx.z; int ind=z*N*N+y*N+x; a_d[ind]=1; } int main(int argc ,char **argv){ if (argc !=4){ fprintf(stderr,\"Error, run program as ./prog N rep device\\n\"); exit(EXIT_FAILURE); } unsigned long N=atoi(argv[1]); unsigned long rep=atoi(argv[2]); cudaSetDevice(atoi(argv[3])); int *a,*a_d, xi=0, yi=0,zi=0; int BSize=8; a=(int*)malloc(sizeof(int)*N*N*N); cudaMalloc((void ** ) &amp;a_d,N*N*N*sizeof(int)); dim3 Bloque(BSize,BSize,BSize); float NB=(float)N/(float)(2*BSize); int B=(int) ceil(NB); dim3 GridBruto((N+BSize-1)/BSize,(N+BSize-1)/BSize,(N+BSize-1)/BSize); fflush(stdout); for(int i=0;i&lt;N;i++){ for (int j=0;j&lt;N;j++){ for(int k=0;k&lt;N;k++){ a[N*N*k+i*N+j]=0; } } } cudaEvent_t start, stop; cudaEventCreate(&amp;start); cudaEventCreate(&amp;stop); cudaMemcpy(a_d,a,N*N*N*sizeof(int),cudaMemcpyHostToDevice); cudaEventRecord(start); for(int i =0;i&lt;rep;i++){ MetNormal&lt;&lt;&lt;GridBruto,Bloque&gt;&gt;&gt;(a_d,N); cudaDeviceSynchronize(); } cudaEventRecord(stop); cudaEventSynchronize(stop); float milliseconds = 0; cudaEventElapsedTime(&amp;milliseconds, start, stop); printf(\"Time %f ms\\n\", milliseconds/(rep)); fflush(stdout); cudaDeviceSynchronize(); cudaMemcpy(a,a_d,N*N*N*sizeof(int),cudaMemcpyDeviceToHost); return 0; } compilation line: nvcc -O3 -std=c++11 -lm -arch sm_60 -rdc=true -lcudadevrt prog.cu -o prog",
        "answers": [],
        "votes": []
    },
    {
        "question": "Glancing from the official NVIDIA Multi-Process Server docs, it is unclear to me how it interacts with CUDA streams. Here's an example: App 0: issues kernels to logical stream 0; App 1: issues kernels to (its own) logical stream 0. In this case, 1) Does / how does MPS \"hijack\" these CUDA calls? Does it have full knowledge of , for each application, what streams are used and what kernels are in which streams? 2) Does MPS create its own 2 streams, and place the respective kernels into the right streams? Or does MPS potentially enable kernel concurrency via mechanisms other than streams? If it helps, I'm interested in how MPS work on Volta, but information with respect to older architecture is appreciated as well.",
        "answers": [
            [
                "A way to think about MPS is that it acts as a funnel for CUDA activity, emanating from multiple processes, to take place on the GPU as if they emanated from a single process. One of the specific benefits of MPS is that it is theoretically possible for kernel concurrency even if the kernels emanate from separate processes. The \"ordinary\" CUDA multi-process execution model would serialize such kernel executions. Since kernel concurrency in a single process implies that the kernels in question are issued to separate streams, it stands to reason that conceptually, MPS is treating the streams from the various client processes as being completely separate. Naturally, then, if you profile such a MPS setup, the streams will show up as being separate from each other, whether they are separate streams associated with a single client process, or streams across several client processes. In the pre-Volta case, MPS did not guarantee process isolation between kernel activity from separate processes. In this respect, it was very much like a funnel, taking activity from several processes and issuing it to the GPU as if it were issued from a single process. In the Volta case, activity from separate processes behaves from an execution standpoint (e.g. concurrency, etc.) as if it were from a single process, but activity from separate processes still carry process isolation (e.g. independent address spaces). 1) Does / how does MPS \"hijack\" these CUDA calls? Does it have full knowledge of , for each application, what streams are used and what kernels are in which streams? Yes, CUDA MPS understands separate streams from a given process, as well as the activity issued to each, and maintains such stream semantics when issuing work to the GPU. The exact details of how CUDA calls are handled by MPS are unpublished, to my knowledge. 2) Does MPS create its own 2 streams, and place the respective kernels into the right streams? Or does MPS potentially enable kernel concurrency via mechanisms other than streams? MPS maintains all stream activity, as well as CUDA stream semantics, across all clients. Activity issued into a particular CUDA stream will be serialized. Activity issued to independent streams may possibly run concurrently. This is true regardless of the origin of the streams in question, be they from one process or several."
            ]
        ],
        "votes": [
            9.0000001
        ]
    },
    {
        "question": "I have inherited some code that basically does stuff like this: void *stream; cudaStreamCreate((cudaStream_t *)&amp;stream); Looking at targets/x86_64-linux/driver_types.h for CUDA 8, I see: typedef __device_builtin__ struct CUStream_st *cudaStream_t; As far as I understand it, the cast will work, but I worry about how future-proof this may be, and also if it is safe when the code is ported to ARM. How dangerous is the above code? Does the __device_builtin__ affect anything? (Note: I plan to talk to the developer directly and tell them to use cudaStream_t throughout and #include &lt;cuda_runtime.h&gt;, so I am hoping to clarify the technical issues here.)",
        "answers": [
            [
                "How big is a cudaStream_t? Like you've observed, typedef __device_builtin__ struct CUStream_st *cudaStream_t; So it's a pointer, and has the size of a pointer, i.e. 64 bits on typical architectures today but different sizes on other architectures. But do you really need to utilize that information? I would guess not. As far as I understand it, the cast will work, but I worry about how future-proof this may be Then make it: cudaStream_t stream; cudaStreamCreate(&amp;stream); or use the C++'ish API wrappers, e.g.: auto device = cuda::device::current::get(); auto stream = device.create_stream(cuda::stream::sync); where that's abstracted away, and stream_t is a wrapper, not a pointer, anyway (caveat: I'm the author of the wrapper library.) What I'd worry about is not incompatibility, but rather avoiding invalid assumptions. And, indeed, you should not assume cudaStream_t is a pointer - just treat it as something opaque. and also if it is safe when the code is ported to ARM. How dangerous is the above code? It's dangerous, but not because of the porting, but like I said, because of the invalid assumption. It would be less dangerous with, say, static_assert(sizeof(void*) == sizeof(cudaStream_t), \"Unexpected size of cudaStream_t - not the same as void *\"); but why are you insisting on the void *, really?"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "In CUDA(driver API) documentation, it says The start of execution of a callback has the same effect as synchronizing an event recorded in the same stream immediately prior to the callback. It thus synchronizes streams which have been \"joined\" prior to the callback. Does this mean that if I have a pipeline with callbacks after each critical point to signal host, I don't need any cuStreamSynchronize for those points to be able to access output arrays? Very simple code like cuda memcpy host to device cuda launch kernel cuda memcpy device to host add callback callback() { here, safe to access host \"results\" array? (considering no more cuda commands on these arrays) }",
        "answers": [
            [
                "CUDA streams have some fairly simple semantics. One of those is that all activity issued into a stream will execute in-order. Item B, issued into a particular stream, will not begin to execute until item A, issued into that stream prior to B, has completed. So, yes, the callback, issued into a particular stream, will not begin to execute until all prior activity in that stream has completed. If you wanted this characteristic in \"ordinary\" host code (i.e. that code that is not wrapped in a CUDA callback) it would require some sort of explicit activity, such as cuStreamSynchronize or cuEventSynchronize, or cuMemcpy, or similar. For the purposes of this discussion, I'm ignoring CUDA managed memory, and assuming you are doing an explicit copy of data from device memory to host memory, as you have laid out."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This question is about notion of a CUDA stream (Stream) and the apparent anomaly with CUDA events (Event) recorded on a stream. Consider the following code demonstrating this anamoly, cudaEventRecord(eventStart, stream1) kernel1&lt;&lt;&lt;..., stream1&gt;&gt;&gt;(...) cudaEventRecord(eventBetween, stream1) kernel2&lt;&lt;&lt;..., stream1&gt;&gt;&gt;(...) cudaEventRecord(eventEnd, stream1) Since all work on a Stream is sequential, the following sequence of events should've happened, \"eventStart\" is recorded to be complete kernel1 runs and completes \"eventBetween\" is recorded to have completed kernel2 runs and completes \"eventEnd\" is recorded to have completed After synchronizing the host thread with the device, the time taken between eventStart and eventBetween ( measured by cudaEventElapsedTime ) should be run-time of kernel1 and the time taken between eventBetween and eventEnd should be run-time of kernel2. But according to NVIDIA's documentation on cudaStreamWaitEvent, Makes all future work submitted to stream wait until event reports completion before beginning execution. and blogpost, You can also synchronize operations within a single stream on a specific event using cudaStreamWaitEvent(event) Events needn't report completion before all work that's scheduled after recording the Event on the same Stream. This comes as a surprise since all work scheduled on a Stream is supposed to run in a sequential fashion. Can someone clarify this asynchronous behavior of Events within a Stream ?",
        "answers": [
            [
                "It appears the cause of your confusion is conflating host-stream synchronisation and stream-stream synchronisation. In short: Streams are FIFO, and all operations within a stream are synchronous with respect to one another. The documentation you are referring to is describing a use case where the programmer wishes to synchronise between events in different streams. The blog post you refer to is discussing synchronisation between the calling host thread and an event enqueued in a stream. There are three levels of host-device synchronisation granularity available -- the whole device, a whole stream, or an event within a stream. The blog is describing the latter. There is no contradiction between any of the sources you cited."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This program: #include &lt;string&gt; #include &lt;stdexcept&gt; struct buffers_t { void* host_buffer; void* device_buffer; }; void ensure_no_error(std::string message) { auto status = cudaGetLastError(); if (status != cudaSuccess) { throw std::runtime_error(message + \": \" + cudaGetErrorString(status)); } } void my_callback(cudaStream_t stream, cudaError_t status, void* args) { auto buffers = (buffers_t *) args; cudaMemcpyAsync( buffers-&gt;host_buffer, buffers-&gt;device_buffer, 1, cudaMemcpyDefault, stream); ensure_no_error(\"after cudaMemcpyAsync\"); } int main() { cudaStream_t stream; cudaStreamCreateWithFlags(&amp;stream, cudaStreamNonBlocking); buffers_t buffers; cudaMallocHost(&amp;buffers.host_buffer, 1); cudaMalloc(&amp;buffers.device_buffer, 1); cudaStreamAddCallback(stream, my_callback, &amp;buffers, 0); ensure_no_error(\"after enqueue callback\"); cudaStreamSynchronize(stream); ensure_no_error(\"after sync\"); } yields: terminate called after throwing an instance of 'std::runtime_error' what(): after cudaMemcpyAsync: operation not permitted Aborted That's kind of weird, because the API reference for cudaMemcpyAsync does not list cudaErrorNotPermitted as one of the potential errors. Is there really a problem with scheduling an async copy from a callback? Note: My machine has a GTX 650 Ti (CC 3.0), CUDA 9.0, Linux kernel 4.8.0, driver 384.59.",
        "answers": [
            [
                "Is there really a problem with scheduling an async copy from a callback? From the documentation on stream callbacks: A callback must not make CUDA API calls (directly or indirectly), as it might end up waiting on itself if it makes such a call leading to a deadlock."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Consider the following program: #include &lt;iostream&gt; #include &lt;array&gt; #include &lt;unistd.h&gt; using clock_value_t = long long; __device__ void gpu_sleep(clock_value_t sleep_cycles) { clock_value_t start = clock64(); clock_value_t cycles_elapsed; do { cycles_elapsed = clock64() - start; } while (cycles_elapsed &lt; sleep_cycles); } __global__ void dummy(clock_value_t duration_in_cycles) { gpu_sleep(duration_in_cycles); } int main() { const clock_value_t duration_in_clocks = 1e7; const size_t buffer_size = 2e7; constexpr const auto num_streams = 8; std::array&lt;char*, num_streams&gt; host_ptrs; std::array&lt;char*, num_streams&gt; device_ptrs; std::array&lt;cudaStream_t, num_streams&gt; streams; for (auto i=0; i&lt;num_streams; i++) { cudaMallocHost(&amp;host_ptrs[i], buffer_size); cudaMalloc(&amp;device_ptrs[i], buffer_size); cudaStreamCreateWithFlags(&amp;streams[i], cudaStreamNonBlocking); } cudaDeviceSynchronize(); for (auto i=0; i&lt;num_streams; i++) { cudaMemcpyAsync(device_ptrs[i], host_ptrs[i], buffer_size, cudaMemcpyDefault, streams[i]); dummy&lt;&lt;&lt;128, 128, 0, streams[i]&gt;&gt;&gt;(duration_in_clocks); cudaMemcpyAsync(host_ptrs[i], device_ptrs[i], buffer_size, cudaMemcpyDefault, streams[i]); } usleep(50000); for (auto i=0; i&lt;num_streams; i++) { cudaStreamSynchronize(streams[i]); } for (auto i=0; i&lt;num_streams; i++) { cudaFreeHost(host_ptrs[i]); cudaFree(device_ptrs[i]); } } I'm running it on an GTX Titan X, with CUDA 8.0.61, on Fedora 25, with driver 375.66. The timeline I'm seeing is this: There a few things wrong with this picture: As far as I can recall there can only be one HtoD transfer at a time. All of the memory transfers should take basically the same amount of time - they're of the same amount of data; and there's nothing else interesting going on with the PCIe bus to affect transfer rates so much. Some DtoH bars like like they're stretched out until something happens on another stream. There's this huge gap in which there seems to be no Computer and no real I/O. And even if the DtoH for all previously-completed kernels was to occupy that gap, that would still leave a very significant amount of time. That actually looks like a scheduling issue rather than a profiling error. So, how should I interpret this timeline? And where does the problem lie? (Hopefully not with the programmer...) I should mention that with less streams (e.g. 2) the timeline looks very nice on the same SW+HW:",
        "answers": [],
        "votes": []
    },
    {
        "question": "The following program: #include &lt;iostream&gt; #include &lt;array&gt; using clock_value_t = long long; __device__ void gpu_sleep(clock_value_t sleep_cycles) { clock_value_t start = clock64(); clock_value_t cycles_elapsed; do { cycles_elapsed = clock64() - start; } while (cycles_elapsed &lt; sleep_cycles); } __global__ void dummy(clock_value_t duration_in_cycles) { gpu_sleep(duration_in_cycles); } int main() { const clock_value_t duration_in_clocks = 1e7; const size_t buffer_size = 5e7; constexpr const auto num_streams = 2; std::array&lt;char*, num_streams&gt; host_ptrs; std::array&lt;char*, num_streams&gt; device_ptrs; std::array&lt;cudaStream_t, num_streams&gt; streams; for (auto i=0; i&lt;num_streams; i++) { cudaMallocHost(&amp;host_ptrs[i], buffer_size); cudaMalloc(&amp;device_ptrs[i], buffer_size); cudaStreamCreateWithFlags(&amp;streams[i], cudaStreamNonBlocking); } cudaDeviceSynchronize(); for (auto i=0; i&lt;num_streams; i++) { cudaMemcpyAsync(device_ptrs[i], host_ptrs[i], buffer_size, cudaMemcpyDefault, streams[i]); dummy&lt;&lt;&lt;128, 128, 0, streams[i]&gt;&gt;&gt;(duration_in_clocks); cudaMemcpyAsync(host_ptrs[i], device_ptrs[i], buffer_size, cudaMemcpyDefault, streams[i]); } for (auto i=0; i&lt;num_streams; i++) { cudaStreamSynchronize(streams[i]); } for (auto i=0; i&lt;num_streams; i++) { cudaFreeHost(host_ptrs[i]); cudaFree(device_ptrs[i]); } } should result in overlapping I/O and Compute between the work on the first and second streams: When the first stream's Host-to-Device ends, the first stream's kernel can start, but so can the second stream's Host-to-Device transfer. Instead, I get the following timeline, with no overlap: I think I've covered my bases to ensure overlap. The streams are non-blocking (and indeed the enqueueing of work concludes well before the first HtoD does); the host memory is pinned... so what's missing for me to see overlap? Using CUDA 8.0.61 on GNU/Linux Mint 18.2 with an NVIDIA GTX 650 Ti Boost. But the driver is v384.59.",
        "answers": [
            [
                "Ok, it must be something with my GPU model, because with Fedora 25, and a GTX Titan X, I get:"
            ]
        ],
        "votes": [
            -1.9999999
        ]
    },
    {
        "question": "Consider the following program for enqueueing some work on a non-blocking GPU stream: #include &lt;iostream&gt; using clock_value_t = long long; __device__ void gpu_sleep(clock_value_t sleep_cycles) { clock_value_t start = clock64(); clock_value_t cycles_elapsed; do { cycles_elapsed = clock64() - start; } while (cycles_elapsed &lt; sleep_cycles); } void callback(cudaStream_t, cudaError_t, void *ptr) { *(reinterpret_cast&lt;bool *&gt;(ptr)) = true; } __global__ void dummy(clock_value_t sleep_cycles) { gpu_sleep(sleep_cycles); } int main() { const clock_value_t duration_in_clocks = 1e6; const size_t buffer_size = 1e7; bool callback_executed = false; cudaStream_t stream; auto host_ptr = std::unique_ptr&lt;char[]&gt;(new char[buffer_size]); char* device_ptr; cudaMalloc(&amp;device_ptr, buffer_size); cudaStreamCreateWithFlags(&amp;stream, cudaStreamNonBlocking); cudaMemcpyAsync(device_ptr, host_ptr.get(), buffer_size, cudaMemcpyDefault, stream); dummy&lt;&lt;&lt;128, 128, 0, stream&gt;&gt;&gt;(duration_in_clocks); cudaMemcpyAsync(host_ptr.get(), device_ptr, buffer_size, cudaMemcpyDefault, stream); cudaStreamAddCallback( stream, callback, &amp;callback_executed, 0 /* fixed and meaningless */); snapshot = callback_executed; std::cout &lt;&lt; \"Right after we finished enqueuing work, the stream has \" &lt;&lt; (snapshot ? \"\" : \"not \") &lt;&lt; \"concluded execution.\" &lt;&lt; std::endl; cudaStreamSynchronize(stream); snapshot = callback_executed; std::cout &lt;&lt; \"After cudaStreamSynchronize, the stream has \" &lt;&lt; (snapshot ? \"\" : \"not \") &lt;&lt; \"concluded execution.\" &lt;&lt; std::endl; } The size of the buffers and the length of the kernel sleep in cycles are high enough, that as they execute in parallel with the CPU thread, it should finish the enqueueing well before they've concluded (8ms+8ms for copying and 20 ms for the kernel). And yet, looking at the trace below, it seems the two cudaMemcpyAsync() are actually synchronous, i.e. they block until the (non-blocking) stream has actually concluded the copying. Is this intended behavior? It seems to contradict the relevant section of the CUDA Runtime API documentation. How does that make sense? Trace: (numbered lines, time in useconds): 1 \"Start\" \"Duration\" \"Grid X\" \"Grid Y\" \"Grid Z\" \"Block X\" \"Block Y\" \"Block Z\" 104 14102.830000 59264.347000 \"cudaMalloc\" 105 73368.351000 19.886000 \"cudaStreamCreateWithFlags\" 106 73388.and 20 ms for the kernel). And yet, looking at the trace below, it seems the two cudaMemcpyAsync()'s are actually synchronous, i.e. they block until the (non-blocking) stream has actually concluded the copying. Is this intended behavior? It seems to contradict the relevant section of the CUDA Runtime API documentation. How does it make sense? 850000 8330.257000 \"cudaMemcpyAsync\" 107 73565.702000 8334.265000 47.683716 5.587311 \"Pageable\" \"Device\" \"GeForce GTX 650 Ti BOOST (0)\" \"1\" 108 81721.124000 2.394000 \"cudaConfigureCall\" 109 81723.865000 3.585000 \"cudaSetupArgument\" 110 81729.332000 30.742000 \"cudaLaunch (dummy(__int64) [107])\" 111 81760.604000 39589.422000 \"cudaMemcpyAsync\" 112 81906.303000 20157.648000 128 1 1 128 1 1 113 102073.103000 18736.208000 47.683716 2.485355 \"Device\" \"Pageable\" \"GeForce GTX 650 Ti BOOST (0)\" \"1\" 114 121351.936000 5.560000 \"cudaStreamSynchronize\"",
        "answers": [
            [
                "This seemed weird, so I contacted someone from the CUDA driver team, who confirmed the documentation is correct. I was also able to confirm it: #include &lt;iostream&gt; #include &lt;memory&gt; using clock_value_t = long long; __device__ void gpu_sleep(clock_value_t sleep_cycles) { clock_value_t start = clock64(); clock_value_t cycles_elapsed; do { cycles_elapsed = clock64() - start; } while (cycles_elapsed &lt; sleep_cycles); } void callback(cudaStream_t, cudaError_t, void *ptr) { *(reinterpret_cast&lt;bool *&gt;(ptr)) = true; } __global__ void dummy(clock_value_t sleep_cycles) { gpu_sleep(sleep_cycles); } int main(int argc, char* argv[]) { cudaFree(0); struct timespec start, stop; const clock_value_t duration_in_clocks = 1e6; const size_t buffer_size = 2 * 1024 * 1024 * (size_t)1024; bool callback_executed = false; cudaStream_t stream; void* host_ptr; if (argc == 1){ host_ptr = malloc(buffer_size); } else { cudaMallocHost(&amp;host_ptr, buffer_size, 0); } char* device_ptr; cudaMalloc(&amp;device_ptr, buffer_size); cudaStreamCreateWithFlags(&amp;stream, cudaStreamNonBlocking); clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &amp;start); cudaMemcpyAsync(device_ptr, host_ptr, buffer_size, cudaMemcpyDefault, stream); clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &amp;stop); double result = (stop.tv_sec - start.tv_sec) * 1e6 + (stop.tv_nsec - start.tv_nsec) / 1e3; std::cout &lt;&lt; \"Elapsed: \" &lt;&lt; result / 1000 / 1000&lt;&lt; std::endl; dummy&lt;&lt;&lt;128, 128, 0, stream&gt;&gt;&gt;(duration_in_clocks); clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &amp;start); cudaMemcpyAsync(host_ptr, device_ptr, buffer_size, cudaMemcpyDefault, stream); clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &amp;stop); result = (stop.tv_sec - start.tv_sec) * 1e6 + (stop.tv_nsec - start.tv_nsec) / 1e3; std::cout &lt;&lt; \"Elapsed: \" &lt;&lt; result / 1000 / 1000 &lt;&lt; std::endl; cudaStreamAddCallback( stream, callback, &amp;callback_executed, 0 /* fixed and meaningless */); auto snapshot = callback_executed; std::cout &lt;&lt; \"Right after we finished enqueuing work, the stream has \" &lt;&lt; (snapshot ? \"\" : \"not \") &lt;&lt; \"concluded execution.\" &lt;&lt; std::endl; cudaStreamSynchronize(stream); snapshot = callback_executed; std::cout &lt;&lt; \"After cudaStreamSynchronize, the stream has \" &lt;&lt; (snapshot ? \"\" : \"not \") &lt;&lt; \"concluded execution.\" &lt;&lt; std::endl; } This is basically your code, with a few modifications: Time measurement A switch to allocate from pageable or pinned memory A buffer size of 2 GiB to ensure a measurable copy time cudaFree(0) to force CUDA lazy initialisation. Here are the results: $ nvcc -std=c++11 main.cu -lrt $ ./a.out # using pageable memory Elapsed: 0.360828 # (memcpyDtoH pageable -&gt; device, fully async) Elapsed: 5.20288 # (memcpyHtoD device -&gt; pageable, sync) $ ./a.out 1 # using pinned memory Elapsed: 4.412e-06 # (memcpyDtoH pinned -&gt; device, fully async) Elapsed: 7.127e-06 # (memcpyDtoH device -&gt; pinned, fully async) It is slower when copying from pageable to device, but it is really async. I'm sorry for my mistake. I deleted my previous comments to avoid confusing people."
            ],
            [
                "It so happens that CUDA memory copies are only asynchronous under strict conditions, as @RobinThoni has kindly indicated. For the code in question, the issue is mostly the use of unpinned (that is, paged) host memory. To quote from a separate section of the Runtime API documentation (emphasis mine): 2. API synchronization behavior The API provides memcpy/memset functions in both synchronous and asynchronous forms, the latter having an \"Async\" suffix. This is a misnomer as each function may exhibit synchronous or asynchronous behavior depending on the arguments passed to the function. ... Asynchronous For transfers from device memory to pageable host memory, the function will return only once the copy has completed. and that's just the half of it! It's actually true that For transfers from pageable host memory to device memory, the data will first be staged in pinned host memory, then copied to the device; and the function will return only after the staging has occurred."
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "We are having performance issues when using the CUDA Dynamic Parallelism. At this moment, CDP is performing at least 3X slower than a traditional approach. We made the simplest reproducible code to show this issue, which is to increment the value of all elements of an array by +1. i.e., a[0,0,0,0,0,0,0,.....,0] --&gt; kernel +1 --&gt; a[1,1,1,1,1,1,1,1,1] The point of this simple example is just to see if CDP can perform as the others, or if there are serious overheads. The code is here: #include &lt;stdio.h&gt; #include &lt;cuda.h&gt; #define BLOCKSIZE 512 __global__ void kernel_parent(int *a, int n, int N); __global__ void kernel_simple(int *a, int n, int N, int offset); // N is the total array size // n is the worksize for a kernel (one third of N) __global__ void kernel_parent(int *a, int n, int N){ cudaStream_t s1, s2; cudaStreamCreateWithFlags(&amp;s1, cudaStreamNonBlocking); cudaStreamCreateWithFlags(&amp;s2, cudaStreamNonBlocking); int tid = blockIdx.x * blockDim.x + threadIdx.x; if(tid == 0){ dim3 block(BLOCKSIZE, 1, 1); dim3 grid( (n + BLOCKSIZE - 1)/BLOCKSIZE, 1, 1); kernel_simple&lt;&lt;&lt; grid, block, 0, s1 &gt;&gt;&gt; (a, n, N, n); kernel_simple&lt;&lt;&lt; grid, block, 0, s2 &gt;&gt;&gt; (a, n, N, 2*n); } a[tid] += 1; } __global__ void kernel_simple(int *a, int n, int N, int offset){ int tid = blockIdx.x * blockDim.x + threadIdx.x; int pos = tid + offset; if(pos &lt; N){ a[pos] += 1; } } int main(int argc, char **argv){ if(argc != 3){ fprintf(stderr, \"run as ./prog n method\\nn multiple of 32 eg: 1024, 1048576 (1024^2), 4194304 (2048^2), 16777216 (4096^2)\\nmethod:\\n0 (traditional) \\n1 (dynamic parallelism)\\n2 (three kernels using unique streams)\\n\"); exit(EXIT_FAILURE); } int N = atoi(argv[1])*3; int method = atoi(argv[2]); // init array as 0 int *ah, *ad; printf(\"genarray of 3*N = %i.......\", N); fflush(stdout); ah = (int*)malloc(sizeof(int)*N); for(int i=0; i&lt;N; ++i){ ah[i] = 0; } printf(\"done\\n\"); fflush(stdout); // malloc and copy array to gpu printf(\"cudaMemcpy:Host-&gt;Device..........\", N); fflush(stdout); cudaMalloc(&amp;ad, sizeof(int)*N); cudaMemcpy(ad, ah, sizeof(int)*N, cudaMemcpyHostToDevice); printf(\"done\\n\"); fflush(stdout); // kernel launch (timed) cudaStream_t s1, s2, s3; cudaStreamCreateWithFlags(&amp;s1, cudaStreamNonBlocking); cudaStreamCreateWithFlags(&amp;s2, cudaStreamNonBlocking); cudaStreamCreateWithFlags(&amp;s3, cudaStreamNonBlocking); cudaEvent_t start, stop; float rtime = 0.0f; cudaEventCreate(&amp;start); cudaEventCreate(&amp;stop); printf(\"Kernel...........................\", N); fflush(stdout); if(method == 0){ // CLASSIC KERNEL LAUNCH dim3 block(BLOCKSIZE, 1, 1); dim3 grid( (N + BLOCKSIZE - 1)/BLOCKSIZE, 1, 1); cudaEventRecord(start, 0); kernel_simple&lt;&lt;&lt; grid, block &gt;&gt;&gt; (ad, N, N, 0); cudaDeviceSynchronize(); cudaEventRecord(stop, 0); } else if(method == 1){ // DYNAMIC PARALLELISM dim3 block(BLOCKSIZE, 1, 1); dim3 grid( (N/3 + BLOCKSIZE - 1)/BLOCKSIZE, 1, 1); cudaEventRecord(start, 0); kernel_parent&lt;&lt;&lt; grid, block, 0, s1 &gt;&gt;&gt; (ad, N/3, N); cudaDeviceSynchronize(); cudaEventRecord(stop, 0); } else{ // THREE CONCURRENT KERNEL LAUNCHES USING STREAMS dim3 block(BLOCKSIZE, 1, 1); dim3 grid( (N/3 + BLOCKSIZE - 1)/BLOCKSIZE, 1, 1); cudaEventRecord(start, 0); kernel_simple&lt;&lt;&lt; grid, block, 0, s1 &gt;&gt;&gt; (ad, N/3, N, 0); kernel_simple&lt;&lt;&lt; grid, block, 0, s2 &gt;&gt;&gt; (ad, N/3, N, N/3); kernel_simple&lt;&lt;&lt; grid, block, 0, s3 &gt;&gt;&gt; (ad, N/3, N, 2*(N/3)); cudaDeviceSynchronize(); cudaEventRecord(stop, 0); } printf(\"done\\n\"); fflush(stdout); printf(\"cudaMemcpy:Device-&gt;Host..........\", N); fflush(stdout); cudaMemcpy(ah, ad, sizeof(int)*N, cudaMemcpyDeviceToHost); printf(\"done\\n\"); fflush(stdout); printf(\"checking result..................\"); fflush(stdout); for(int i=0; i&lt;N; ++i){ if(ah[i] != 1){ fprintf(stderr, \"bad element: a[%i] = %i\\n\", i, ah[i]); exit(EXIT_FAILURE); } } printf(\"done\\n\"); fflush(stdout); cudaEventSynchronize(stop); cudaEventElapsedTime(&amp;rtime, start, stop); printf(\"rtime: %f ms\\n\", rtime); fflush(stdout); return EXIT_SUCCESS; } Can be compiled with nvcc -arch=sm_35 -rdc=true -lineinfo -lcudadevrt -use_fast_math main.cu -o prog This example can compute the result with 3 methods: Simple Kernel: Just a single classic kernel +1 pass on the array. Dynamic Parallelism: from main(), call a parent kernel which does +1 on the range [0,N/3), and also calls two child kernels. The first child does +1 in the range [N/3, 2*N/3), the second child in the range [2*N/3,N). Childs are launched using different streams so they can be concurrent. Three Streams from Host: This one just launches three non-blocking streams from main(), one for each third of the array. I get the following profile for method 0 (simple kernel): The following for method 1 (dynamic parallelism): And the following for method 2 (Three Streams from Host) The running times are like this: \u279c simple-cdp git:(master) \u2717 ./prog 16777216 0 genarray of 3*N = 50331648.......done cudaMemcpy:Host-&gt;Device..........done Kernel...........................done cudaMemcpy:Device-&gt;Host..........done checking result..................done rtime: 1.140928 ms \u279c simple-cdp git:(master) \u2717 ./prog 16777216 1 genarray of 3*N = 50331648.......done cudaMemcpy:Host-&gt;Device..........done Kernel...........................done cudaMemcpy:Device-&gt;Host..........done checking result..................done rtime: 5.790048 ms \u279c simple-cdp git:(master) \u2717 ./prog 16777216 2 genarray of 3*N = 50331648.......done cudaMemcpy:Host-&gt;Device..........done Kernel...........................done cudaMemcpy:Device-&gt;Host..........done checking result..................done rtime: 1.011936 ms The main problem, visible from the pictures, is that in the Dynamic Parallelism method the parent kernel is taking excessive amount of time to close after the two child kernels have finished, which is what is making it take 3X or 4X times more. Even when considering the worst case, if all three kernels (parent and two childs) run in serial, it should take much less. I.e., there is N/3 of work for each kernel, so the whole parent kernel should take approx 3 child kernels long, which is much less. Is there a way to solve this problem? EDIT: The serialization phenomenon of the child kernels, as well as for method 2, have been explained by Robert Crovella in the comments (many thanks). The fact that the kernels did run in serial do not invalidate the problem described in bold text (not for now at least).",
        "answers": [
            [
                "Calls into the device runtime are \"expensive\", just like calls into the host runtime are expensive. In this case, it seems that you are calling into the device runtime to create streams for every thread, even though this code only requires them for thread 0. By modifying your code to only request the stream creation for thread 0, we can produce timing parity between the case where we are using separate streams for the child kernel launch, and the case where we are not using separate streams for the child kernel launch: $ cat t370.cu #include &lt;stdio.h&gt; #define BLOCKSIZE 512 __global__ void kernel_parent(int *a, int n, int N); __global__ void kernel_simple(int *a, int n, int N, int offset); // N is the total array size // n is the worksize for a kernel (one third of N) __global__ void kernel_parent(int *a, int n, int N){ int tid = blockIdx.x * blockDim.x + threadIdx.x; if(tid == 0){ dim3 block(BLOCKSIZE, 1, 1); dim3 grid( (n + BLOCKSIZE - 1)/BLOCKSIZE, 1, 1); #ifdef USE_STREAMS cudaStream_t s1, s2; cudaStreamCreateWithFlags(&amp;s1, cudaStreamNonBlocking); cudaStreamCreateWithFlags(&amp;s2, cudaStreamNonBlocking); kernel_simple&lt;&lt;&lt; grid, block, 0, s1 &gt;&gt;&gt; (a, n, N, n); kernel_simple&lt;&lt;&lt; grid, block, 0, s2 &gt;&gt;&gt; (a, n, N, 2*n); #else kernel_simple&lt;&lt;&lt; grid, block &gt;&gt;&gt; (a, n, N, n); kernel_simple&lt;&lt;&lt; grid, block &gt;&gt;&gt; (a, n, N, 2*n); #endif // these next 2 lines add noticeably to the overall timing cudaError_t err = cudaGetLastError(); if (err != cudaSuccess) printf(\"oops1: %d\\n\", (int)err); } a[tid] += 1; } __global__ void kernel_simple(int *a, int n, int N, int offset){ int tid = blockIdx.x * blockDim.x + threadIdx.x; int pos = tid + offset; if(pos &lt; N){ a[pos] += 1; } } int main(int argc, char **argv){ if(argc != 3){ fprintf(stderr, \"run as ./prog n method\\nn multiple of 32 eg: 1024, 1048576 (1024^2), 4194304 (2048^2), 16777216 (4096^2)\\nmethod:\\n0 (traditional) \\n1 (dynamic parallelism)\\n2 (three kernels using unique streams)\\n\"); exit(EXIT_FAILURE); } int N = atoi(argv[1])*3; int method = atoi(argv[2]); // init array as 0 int *ah, *ad; printf(\"genarray of 3*N = %i.......\", N); fflush(stdout); ah = (int*)malloc(sizeof(int)*N); for(int i=0; i&lt;N; ++i){ ah[i] = 0; } printf(\"done\\n\"); fflush(stdout); // malloc and copy array to gpu printf(\"cudaMemcpy:Host-&gt;Device..........\", N); fflush(stdout); cudaMalloc(&amp;ad, sizeof(int)*N); cudaMemcpy(ad, ah, sizeof(int)*N, cudaMemcpyHostToDevice); printf(\"done\\n\"); fflush(stdout); // kernel launch (timed) cudaStream_t s1, s2, s3; cudaStreamCreateWithFlags(&amp;s1, cudaStreamNonBlocking); cudaStreamCreateWithFlags(&amp;s2, cudaStreamNonBlocking); cudaStreamCreateWithFlags(&amp;s3, cudaStreamNonBlocking); cudaEvent_t start, stop; float rtime = 0.0f; cudaEventCreate(&amp;start); cudaEventCreate(&amp;stop); printf(\"Kernel...........................\", N); fflush(stdout); if(method == 0){ // CLASSIC KERNEL LAUNCH dim3 block(BLOCKSIZE, 1, 1); dim3 grid( (N + BLOCKSIZE - 1)/BLOCKSIZE, 1, 1); cudaEventRecord(start, 0); kernel_simple&lt;&lt;&lt; grid, block &gt;&gt;&gt; (ad, N, N, 0); cudaDeviceSynchronize(); cudaEventRecord(stop, 0); } else if(method == 1){ // DYNAMIC PARALLELISM dim3 block(BLOCKSIZE, 1, 1); dim3 grid( (N/3 + BLOCKSIZE - 1)/BLOCKSIZE, 1, 1); cudaEventRecord(start, 0); kernel_parent&lt;&lt;&lt; grid, block, 0, s1 &gt;&gt;&gt; (ad, N/3, N); cudaDeviceSynchronize(); cudaEventRecord(stop, 0); } else{ // THREE CONCURRENT KERNEL LAUNCHES USING STREAMS dim3 block(BLOCKSIZE, 1, 1); dim3 grid( (N/3 + BLOCKSIZE - 1)/BLOCKSIZE, 1, 1); cudaEventRecord(start, 0); kernel_simple&lt;&lt;&lt; grid, block, 0, s1 &gt;&gt;&gt; (ad, N/3, N, 0); kernel_simple&lt;&lt;&lt; grid, block, 0, s2 &gt;&gt;&gt; (ad, N/3, N, N/3); kernel_simple&lt;&lt;&lt; grid, block, 0, s3 &gt;&gt;&gt; (ad, N/3, N, 2*(N/3)); cudaDeviceSynchronize(); cudaEventRecord(stop, 0); } printf(\"done\\n\"); fflush(stdout); printf(\"cudaMemcpy:Device-&gt;Host..........\", N); fflush(stdout); cudaMemcpy(ah, ad, sizeof(int)*N, cudaMemcpyDeviceToHost); printf(\"done\\n\"); fflush(stdout); printf(\"checking result..................\"); fflush(stdout); for(int i=0; i&lt;N; ++i){ if(ah[i] != 1){ fprintf(stderr, \"bad element: a[%i] = %i\\n\", i, ah[i]); exit(EXIT_FAILURE); } } printf(\"done\\n\"); fflush(stdout); cudaEventSynchronize(stop); cudaEventElapsedTime(&amp;rtime, start, stop); printf(\"rtime: %f ms\\n\", rtime); fflush(stdout); return EXIT_SUCCESS; } $ nvcc -arch=sm_52 -rdc=true -lcudadevrt -o t370 t370.cu $ ./t370 16777216 1 genarray of 3*N = 50331648.......done cudaMemcpy:Host-&gt;Device..........done Kernel...........................done cudaMemcpy:Device-&gt;Host..........done checking result..................done rtime: 6.925632 ms $ nvcc -arch=sm_52 -rdc=true -lcudadevrt -o t370 t370.cu -DUSE_STREAMS $ ./t370 16777216 1 genarray of 3*N = 50331648.......done cudaMemcpy:Host-&gt;Device..........done Kernel...........................done cudaMemcpy:Device-&gt;Host..........done checking result..................done rtime: 6.673568 ms $ Although not included in the test output above, according to my testing, this also brings the CUDA dynamic parallelism (CDP) case (1) into \"approximate parity\" with the non-CDP cases (0, 2). Note that we can shave about 1 ms (!) off the above time by forgoing the call to cudaGetLastError() in the parent kernel (which I added to your code)."
            ],
            [
                "#include &lt;stdio.h&gt; #include &lt;thrust/host_vector.h&gt; #include &lt;thrust/device_vector.h&gt; using thrust::host_vector; using thrust::device_vector; #define BLOCKSIZE 512 __global__ void child(int* a) { if (threadIdx.x == 0 &amp;&amp; blockIdx.x == 0) a[0]++; } __global__ void parent(int* a) { if (threadIdx.x == 0 &amp;&amp; blockIdx.x == 0) child&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(a); } #define NBLOCKS 1024 #define NTHREADS 1024 #define BENCHCOUNT 1000 template&lt;typename Lambda&gt; void runBench(Lambda arg, int* rp, const char* name) { // \"preheat\" the GPU for (int i = 0; i &lt; 100; i++) child&lt;&lt;&lt;dim3(NBLOCKS,1,1), dim3(NTHREADS,1,1)&gt;&gt;&gt;(rp); cudaEvent_t start, stop; float rtime = 0.0f; cudaEventCreate(&amp;start); cudaEventCreate(&amp;stop); cudaEventRecord(start, 0); for (int i = 0; i &lt; BENCHCOUNT; i++) arg(); cudaEventRecord(stop, 0); cudaEventSynchronize(stop); cudaEventElapsedTime(&amp;rtime, start, stop); printf(\"=== %s ===\\n\", name); printf(\"time: %f ms\\n\", rtime/BENCHCOUNT); fflush(stdout); cudaEventDestroy(start); cudaEventDestroy(stop); cudaDeviceSynchronize(); } int main(int argc, char **argv) { host_vector&lt;int&gt; hv(1); hv[0] = 0xAABBCCDD; device_vector&lt;int&gt; dv(1); dv = hv; int* rp = thrust::raw_pointer_cast(&amp;dv[0]); auto benchFun = [&amp;](void) { child&lt;&lt;&lt;dim3(NBLOCKS,1,1), dim3(NTHREADS,1,1)&gt;&gt;&gt;(rp); }; runBench(benchFun, rp, \"Single kernel launch\"); auto benchFun2 = [&amp;](void) { for (int j = 0; j &lt; 2; j++) child&lt;&lt;&lt;dim3(NBLOCKS,1,1), dim3(NTHREADS,1,1)&gt;&gt;&gt;(rp); }; runBench(benchFun2, rp, \"2x sequential kernel launch\"); auto benchFunDP = [&amp;](void) { parent&lt;&lt;&lt;dim3(NBLOCKS,1,1), dim3(NTHREADS,1,1)&gt;&gt;&gt;(rp); }; runBench(benchFunDP, rp, \"Nested kernel launch\"); } To build/run: Copy/paste code above to dpar.cu nvcc -arch=sm_52 -rdc=true -std=c++11 -lcudadevrt -o dpar dpar.cu ./dpar On my p5000 laptop it prints: === Single kernel launch === time: 0.014297 ms === 2x sequential kernel launch === time: 0.030468 ms === Nested kernel launch === time: 0.083820 ms So the overhead is quite large.. looks like in my case 43 microseconds."
            ]
        ],
        "votes": [
            6.0000001,
            2.0000001
        ]
    },
    {
        "question": "I want to make two CUBLAS APIs(eg.cublasDgemm) really execute concurrently in two cudaStreams. As we know, the CUBLAS API is asynchronous,level 3 routines like cublasDgemm don't block the host,that means the following codes (in default cudaStream) will run on concurrently: cublasDgemm(); cublasDgemm(); BUT,when I profile the program with \"NVIDIA Visual Profiler\" , it shows that they run on orderly. Then,I try to make them bind to different cudaStreams,the pseudocode is: // Create a stream for every DGEMM operation cudaStream_t *streams = (cudaStream_t *) malloc(batch_count*sizeof(cudaStream_t)); for(i=0; i&lt;batch_count; i++) cudaStreamCreate(&amp;streams[i]); // Set matrix coefficients double alpha = 1.0; double beta = 1.0; // Launch each DGEMM operation in own CUDA stream for(i=0; i&lt;batch_count; i++){ // Set CUDA stream cublasSetStream(handle, streams[i]); // DGEMM: C = alpha*A*B + beta*C cublasDgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, dim, dim, dim, &amp;alpha, d_A[i], dim, d_B[i], dim, &amp;beta, d_C[i], dim); } When the batch_count=5, the result showed by \"NVIDIA Visual Profiler \" is : Multi-CublasDegmm Rountines Execution Result With Multi-Streams The result shows that they still run on orderly. How to make multi cublas apis run on really concurrently in multi cudaStreams,like this: Multi-Kernels Execution Result With Multi-Streams,They Run on Really Concurrnently Does anybody has any idea ? Thanks.",
        "answers": [
            [
                "Firstly, thanks for @Robert Crovella's comments. According to @Robert Crovella's help and my research,we can run multi CUBLAS APIs(e.g. cublasDgemm) concurrently in some special cases, but most of cases can not. CASE 1\uff1aWhen I execute cublasDgemm with large dims of (m=n=k=1024*8) on K40, the profiler show the result as following: cublasDgemm with dims of (M=N=K=1024*8) CASE 2\uff1aWhen I execute cublasDgemm with small dims of (m=n=k=64) on K40, the profiler show the result as following: cublasDgemm with dims of (M=N=K=64) CASE 3\uff1aBUT when I execute cublasDgemm with dims of (m=n=k=256) on K40, the profiler show the result as following: cublasDgemm with dims of (M=N=K=256) From the result of CASE 1 and CASE 2 ,it shows that we can not, not only with large dims and also small dims, run CUBLAS APIs concurrently. The reason for case 1 is that the gpu resources have been used up,so no left room to run another routines, and for case 2, it is the latency of two kernels launch that cause it's difficulty to see con."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm working on a video stream in real time that I try to process with a GeForce GTX 960M. (Windows 10, VS 2013, CUDA 8.0) Each frame has to be captured, lightly blured, and whenever I can, I need to do some hard-work calculations on the 10 latest frames. So I need to capture ALL the frames at 30 fps, and I expect to get the hard-work result at 5 fps. My problems is that I cannot keep the capture running at the right pace : it seems that the hard-work calculation slows down the capture of frames, either at CPU level or at GPU level. I miss some frames... I tried many solutions. None worked: I tried to set-up jobs on 2 streams (image below): the host gets a frame First stream (called Stream2) : cudaMemcpyAsync copies the frame on the Device. Then, a first kernel does the basic bluring calculations. (In the attached image, bluring appears as a short slot at 3.07 s and 3.085 s. And then nothing... until the big part has finished) the host checks if the second stream is \"available\" thanks to a CudaEvent, and lauches it if possible. Practically, the stream is available 1/2 of tries. Second stream (called Stream4) : starts hard-work calculations in a kernel ( kernelCalcul_W2), outputs the result, and records an Event. NSight capture Practically, I wrote : cudaStream_t sHigh, sLow; cudaStreamCreateWithPriority(&amp;sHigh, cudaStreamNonBlocking, priority_high); cudaStreamCreateWithPriority(&amp;sLow, cudaStreamNonBlocking, priority_low); cudaEvent_t event_1; cudaEventCreate(&amp;event_1); if (frame has arrived) { cudaMemcpyAsync(..., sHigh); // HtoD, to upload images in the GPU blur_Image &lt;&lt;&lt;... , sHigh&gt;&gt;&gt; (...) if (cudaEventQuery(event_1)==cudaSuccess)) hard_work(sLow); else printf(\"Event 2 not ready\\n\"); } void hard_work( cudaStream_t sLow_) { kernelCalcul_W2&lt;&lt;&lt;... , sLow_&gt;&gt;&gt; (...); cudaMemcpyAsync(... the result..., sLow_); //DtoH cudaEventRecord(event_1, sLow_); } I tried to use only one stream. It's the same code as above, but change 1 parameter while launching hard_work. host gets a frame Stream: cudaMemcpyAsync copies the frame on the Device. Then, the kernel does the basic bluring calculations. Then, if the CudaEvent Event_1 is ok, I lauch the hard-work, and I add an Event_1 to get the status on next round. Practically, the stream is ALWAYS available: I never fall in the \"else\" part. This way, while the hard-work is running, I expected to \"buffer\" all the frames to copy, and not to lose any. But I do lose some: it turns out that each time I get a frame and I copy it, Event_1 seems ok so I launch the hard-work, and only get the the next frame very late. I tried to put the two streams in two different threads (in C). Not better (even worse). So the question is: how to ensure that the first stream captures ALL frames? I really have the feeling that the different streams block the CPU. I display the images with OpenGL. Would it interfere? Any idea of ways to improve this? Thanks a lot! EDIT: As requested, I put here a MCVE. There is a parameter you can tune (#define ADJUST) to see what's happening. Basically, the main procedure sends CUDA requests in Async mode, but it seems to block the main thread. As you will see in the image, I have \"memory access\" (i.e. images captured ) every 30 ms except when the hard-work is running (then, I just don't get images). Last detail: I'm using CUDA 7.5 to run this. I tried to install 8.0 but apparently the compiler is still 7.5 #define _USE_MATH_DEFINES 1 #define _CRT_SECURE_NO_WARNINGS 1 #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;time.h&gt; #include &lt;Windows.h&gt; #define ADJUST 400 // adjusting this paramter may make the problem occur. // Too high =&gt; probably watchdog will stop the kernel // too low =&gt; probably the kernel will run smothly unsigned short * images_as_Unsigned_in_Host; unsigned short * Images_as_Unsigned_in_Device; unsigned short * camera; float * images_as_Output_in_Host; float * Images_as_Float_in_Device; float * imageOutput_in_Device; unsigned short imageWidth, imageHeight, totNbOfImages, imageSlot; unsigned long imagePixelSize; unsigned short lastImageFromCamera; cudaStream_t s1, s2; cudaEvent_t event_2; clock_t timeRef; // Basically, in the middle of the image, I average the values. I removed the logic behind to make it simpler. // This kernel runs fast, and that's the point. __global__ void blurImage(unsigned short * Images_as_Unsigned_in_Device_, float * Images_as_Float_in_Device_, unsigned short imageWidth_, unsigned long imagePixelSize_, short blur_distance) { // we start from 'blur_distance' from the edge // p0 is the point we will calculate. p is a pointer which will move around for average unsigned long p0 = (threadIdx.x + blur_distance) + (blockIdx.x + blur_distance) * imageWidth_; unsigned long p = p0; unsigned short * us; if (p &gt;= imagePixelSize_) return; unsigned long tot = 0; short a, b, n, k; k = 0; // p starts from the top edge and will move to the right-bottom p -= blur_distance + blur_distance * imageWidth_; us = Images_as_Unsigned_in_Device_ + p; for (a = 2 * blur_distance; a &gt;= 0; a--) { for (b = 2 * blur_distance; b &gt;= 0; b--) { n = *us; if (n &gt; 0) { tot += n; k++; } us++; } us += imageWidth_ - 2 * blur_distance - 1; } if (k &gt; 0) Images_as_Float_in_Device_[p0] = (float)tot / (float)k; else Images_as_Float_in_Device_[p0] = 128.f; } __global__ void kernelCalcul_W2(float *inputImage, float *outputImage, unsigned long imagePixelSize_, unsigned short imageWidth_, unsigned short slot, unsigned short totImages) { // point the pixel and crunch it unsigned long p = threadIdx.x + blockIdx.x * imageWidth_; if (p &gt;= imagePixelSize_) { return; } float result; long a, b, n, n0; float input; b = 3; // this is not the right algorithm (which is pretty complex). // I know this is not optimal in terms of memory management. Still, I want a \"long\" calculation here so I don't care... for (n = 0; n &lt; 10; n++) { n0 = slot - n; if (n0 &lt; 0) n0 += totImages; input = inputImage[p + n0 * imagePixelSize_]; for (a = 0; a &lt; ADJUST ; a++) result += pow(input, inputImage[a + n0 * imagePixelSize_]) * cos(input); } outputImage[p] = result; } void hard_work( cudaStream_t s){ cudaError err; // launch the hard work printf(\"Hard work is launched after image %d is captured ==&gt; \", imageSlot); kernelCalcul_W2 &lt;&lt; &lt;340, 500, 0, s &gt;&gt; &gt;(Images_as_Float_in_Device, imageOutput_in_Device, imagePixelSize, imageWidth, imageSlot, totNbOfImages); err = cudaPeekAtLastError(); if (err != cudaSuccess) printf( \"running error: %s \\n\", cudaGetErrorString(err)); else printf(\"running ok\\n\"); // copy the result back to Host //printf(\" %p %p \\n\", images_as_Output_in_Host, imageOutput_in_Device); cudaMemcpyAsync(images_as_Output_in_Host, imageOutput_in_Device, sizeof(float) * imagePixelSize, cudaMemcpyDeviceToHost, s); cudaEventRecord(event_2, s); } void createStorageSpace() { imageWidth = 640; imageHeight = 480; totNbOfImages = 300; imageSlot = 0; imagePixelSize = 640 * 480; lastImageFromCamera = 0; camera = (unsigned short *)malloc(imagePixelSize * sizeof(unsigned short)); for (int i = 0; i &lt; imagePixelSize; i++) camera[i] = rand() % 255; // storing the images in the Host memory. I know I could optimize with cudaHostAllocate. images_as_Unsigned_in_Host = (unsigned short *) malloc(imagePixelSize * sizeof(unsigned short) * totNbOfImages); images_as_Output_in_Host = (float *)malloc(imagePixelSize * sizeof(float)); cudaMalloc(&amp;Images_as_Unsigned_in_Device, imagePixelSize * sizeof(unsigned short) * totNbOfImages); cudaMalloc(&amp;Images_as_Float_in_Device, imagePixelSize * sizeof(float) * totNbOfImages); cudaMalloc(&amp;imageOutput_in_Device, imagePixelSize * sizeof(float)); int priority_high, priority_low; cudaDeviceGetStreamPriorityRange(&amp;priority_low, &amp;priority_high); cudaStreamCreateWithPriority(&amp;s1, cudaStreamNonBlocking, priority_high); cudaStreamCreateWithPriority(&amp;s2, cudaStreamNonBlocking, priority_low); cudaEventCreate(&amp;event_2); } void releaseMapFile() { cudaFree(Images_as_Unsigned_in_Device); cudaFree(Images_as_Float_in_Device); cudaFree(imageOutput_in_Device); free(images_as_Output_in_Host); free(camera); cudaStreamDestroy(s1); cudaStreamDestroy(s2); cudaEventDestroy(event_2); } void putImageCUDA(const void * data) { // We put the image in a round-robin. The slot to put the image is imageSlot printf(\"\\nDealing with image %d\\n\", imageSlot); // Copy the image in the Round Robin cudaMemcpyAsync(Images_as_Unsigned_in_Device + imageSlot * imagePixelSize, data, sizeof(unsigned short) * imagePixelSize, cudaMemcpyHostToDevice, s1); // We will blur the image. Let's prepare the memory to get the results as floats cudaMemsetAsync(Images_as_Float_in_Device + imageSlot * imagePixelSize, 0., sizeof(float) * imagePixelSize, s1); // blur image blurImage &lt;&lt; &lt;imageHeight - 140, imageWidth - 140, 0, s1 &gt;&gt; &gt; (Images_as_Unsigned_in_Device + imageSlot * imagePixelSize, Images_as_Float_in_Device + imageSlot * imagePixelSize, imageWidth, imagePixelSize, 3); // launches the hard-work if (cudaEventQuery(event_2) == cudaSuccess) hard_work(s2); else printf(\"Hard_work still running, so unable to process after image %d\\n\", imageSlot); imageSlot++; if (imageSlot &gt;= totNbOfImages) { imageSlot = 0; } } int main() { createStorageSpace(); printf(\"The following loop is supposed to push images in the GPU and do calculations in Async mode, and to wait 30 ms before the next image, so we should have the output on the screen in 10 x 30 ms. But it's far slower...\\nYou may adjust a #define ADJUST parameter to see what's happening.\"); for (int i = 0; i &lt; 10; i++) { putImageCUDA(camera); // Puts an image in the GPU, does the bluring, and tries to do the hard-work Sleep(30); // to simulate Camera } releaseMapFile(); getchar(); }",
        "answers": [
            [
                "The primary issue here is that cudaMemcpyAsync is only a properly non-blocking async operation if the host memory involved is pinned, i.e. allocated using cudaHostAlloc. This characteristic is covered in several places, including the API documentation and the relevant programming guide section. The following modification to your code (to run on linux, which I prefer) demonstrates the behavioral difference: $ cat t33.cu #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;time.h&gt; #include &lt;unistd.h&gt; #define ADJUST 400 // adjusting this paramter may make the problem occur. // Too high =&gt; probably watchdog will stop the kernel // too low =&gt; probably the kernel will run smothly unsigned short * images_as_Unsigned_in_Host; unsigned short * Images_as_Unsigned_in_Device; unsigned short * camera; float * images_as_Output_in_Host; float * Images_as_Float_in_Device; float * imageOutput_in_Device; unsigned short imageWidth, imageHeight, totNbOfImages, imageSlot; unsigned long imagePixelSize; unsigned short lastImageFromCamera; cudaStream_t s1, s2; cudaEvent_t event_2; clock_t timeRef; // Basically, in the middle of the image, I average the values. I removed the logic behind to make it simpler. // This kernel runs fast, and that's the point. __global__ void blurImage(unsigned short * Images_as_Unsigned_in_Device_, float * Images_as_Float_in_Device_, unsigned short imageWidth_, unsigned long imagePixelSize_, short blur_distance) { // we start from 'blur_distance' from the edge // p0 is the point we will calculate. p is a pointer which will move around for average unsigned long p0 = (threadIdx.x + blur_distance) + (blockIdx.x + blur_distance) * imageWidth_; unsigned long p = p0; unsigned short * us; if (p &gt;= imagePixelSize_) return; unsigned long tot = 0; short a, b, n, k; k = 0; // p starts from the top edge and will move to the right-bottom p -= blur_distance + blur_distance * imageWidth_; us = Images_as_Unsigned_in_Device_ + p; for (a = 2 * blur_distance; a &gt;= 0; a--) { for (b = 2 * blur_distance; b &gt;= 0; b--) { n = *us; if (n &gt; 0) { tot += n; k++; } us++; } us += imageWidth_ - 2 * blur_distance - 1; } if (k &gt; 0) Images_as_Float_in_Device_[p0] = (float)tot / (float)k; else Images_as_Float_in_Device_[p0] = 128.f; } __global__ void kernelCalcul_W2(float *inputImage, float *outputImage, unsigned long imagePixelSize_, unsigned short imageWidth_, unsigned short slot, unsigned short totImages) { // point the pixel and crunch it unsigned long p = threadIdx.x + blockIdx.x * imageWidth_; if (p &gt;= imagePixelSize_) { return; } float result; long a, n, n0; float input; // this is not the right algorithm (which is pretty complex). // I know this is not optimal in terms of memory management. Still, I want a \"long\" calculation here so I don't care... for (n = 0; n &lt; 10; n++) { n0 = slot - n; if (n0 &lt; 0) n0 += totImages; input = inputImage[p + n0 * imagePixelSize_]; for (a = 0; a &lt; ADJUST ; a++) result += pow(input, inputImage[a + n0 * imagePixelSize_]) * cos(input); } outputImage[p] = result; } void hard_work( cudaStream_t s){ #ifndef QUICK cudaError err; // launch the hard work printf(\"Hard work is launched after image %d is captured ==&gt; \", imageSlot); kernelCalcul_W2 &lt;&lt; &lt;340, 500, 0, s &gt;&gt; &gt;(Images_as_Float_in_Device, imageOutput_in_Device, imagePixelSize, imageWidth, imageSlot, totNbOfImages); err = cudaPeekAtLastError(); if (err != cudaSuccess) printf( \"running error: %s \\n\", cudaGetErrorString(err)); else printf(\"running ok\\n\"); // copy the result back to Host //printf(\" %p %p \\n\", images_as_Output_in_Host, imageOutput_in_Device); cudaMemcpyAsync(images_as_Output_in_Host, imageOutput_in_Device, sizeof(float) * imagePixelSize/2, cudaMemcpyDeviceToHost, s); cudaEventRecord(event_2, s); #endif } void createStorageSpace() { imageWidth = 640; imageHeight = 480; totNbOfImages = 300; imageSlot = 0; imagePixelSize = 640 * 480; lastImageFromCamera = 0; #ifdef USE_HOST_ALLOC cudaHostAlloc(&amp;camera, imagePixelSize*sizeof(unsigned short), cudaHostAllocDefault); cudaHostAlloc(&amp;images_as_Unsigned_in_Host, imagePixelSize*sizeof(unsigned short)*totNbOfImages, cudaHostAllocDefault); cudaHostAlloc(&amp;images_as_Output_in_Host, imagePixelSize*sizeof(unsigned short), cudaHostAllocDefault); #else camera = (unsigned short *)malloc(imagePixelSize * sizeof(unsigned short)); images_as_Unsigned_in_Host = (unsigned short *) malloc(imagePixelSize * sizeof(unsigned short) * totNbOfImages); images_as_Output_in_Host = (float *)malloc(imagePixelSize * sizeof(float)); #endif for (int i = 0; i &lt; imagePixelSize; i++) camera[i] = rand() % 255; cudaMalloc(&amp;Images_as_Unsigned_in_Device, imagePixelSize * sizeof(unsigned short) * totNbOfImages); cudaMalloc(&amp;Images_as_Float_in_Device, imagePixelSize * sizeof(float) * totNbOfImages); cudaMalloc(&amp;imageOutput_in_Device, imagePixelSize * sizeof(float)); int priority_high, priority_low; cudaDeviceGetStreamPriorityRange(&amp;priority_low, &amp;priority_high); cudaStreamCreateWithPriority(&amp;s1, cudaStreamNonBlocking, priority_high); cudaStreamCreateWithPriority(&amp;s2, cudaStreamNonBlocking, priority_low); cudaEventCreate(&amp;event_2); cudaEventRecord(event_2, s2); } void releaseMapFile() { cudaFree(Images_as_Unsigned_in_Device); cudaFree(Images_as_Float_in_Device); cudaFree(imageOutput_in_Device); cudaStreamDestroy(s1); cudaStreamDestroy(s2); cudaEventDestroy(event_2); } void putImageCUDA(const void * data) { // We put the image in a round-robin. The slot to put the image is imageSlot printf(\"\\nDealing with image %d\\n\", imageSlot); // Copy the image in the Round Robin cudaMemcpyAsync(Images_as_Unsigned_in_Device + imageSlot * imagePixelSize, data, sizeof(unsigned short) * imagePixelSize, cudaMemcpyHostToDevice, s1); // We will blur the image. Let's prepare the memory to get the results as floats cudaMemsetAsync(Images_as_Float_in_Device + imageSlot * imagePixelSize, 0, sizeof(float) * imagePixelSize, s1); // blur image blurImage &lt;&lt; &lt;imageHeight - 140, imageWidth - 140, 0, s1 &gt;&gt; &gt; (Images_as_Unsigned_in_Device + imageSlot * imagePixelSize, Images_as_Float_in_Device + imageSlot * imagePixelSize, imageWidth, imagePixelSize, 3); // launches the hard-work if (cudaEventQuery(event_2) == cudaSuccess) hard_work(s2); else printf(\"Hard_work still running, so unable to process after image %d\\n\", imageSlot); imageSlot++; if (imageSlot &gt;= totNbOfImages) { imageSlot = 0; } } int main() { createStorageSpace(); printf(\"The following loop is supposed to push images in the GPU and do calculations in Async mode, and to wait 30 ms before the next image, so we should have the output on the screen in 10 x 30 ms. But it's far slower...\\nYou may adjust a #define ADJUST parameter to see what's happening.\"); for (int i = 0; i &lt; 10; i++) { putImageCUDA(camera); // Puts an image in the GPU, does the bluring, and tries to do the hard-work usleep(30000); // to simulate Camera } cudaError_t err = cudaGetLastError(); if (err != cudaSuccess) printf(\"some CUDA error: %s\\n\", cudaGetErrorString(err)); releaseMapFile(); } $ nvcc -arch=sm_52 -lineinfo -o t33 t33.cu $ time ./t33 The following loop is supposed to push images in the GPU and do calculations in Async mode, and to wait 30 ms before the next image, so we should have the output on the screen in 10 x 30 ms. But it's far slower... You may adjust a #define ADJUST parameter to see what's happening. Dealing with image 0 Hard work is launched after image 0 is captured ==&gt; running ok Dealing with image 1 Hard work is launched after image 1 is captured ==&gt; running ok Dealing with image 2 Hard work is launched after image 2 is captured ==&gt; running ok Dealing with image 3 Hard work is launched after image 3 is captured ==&gt; running ok Dealing with image 4 Hard work is launched after image 4 is captured ==&gt; running ok Dealing with image 5 Hard work is launched after image 5 is captured ==&gt; running ok Dealing with image 6 Hard work is launched after image 6 is captured ==&gt; running ok Dealing with image 7 Hard work is launched after image 7 is captured ==&gt; running ok Dealing with image 8 Hard work is launched after image 8 is captured ==&gt; running ok Dealing with image 9 Hard work is launched after image 9 is captured ==&gt; running ok real 0m2.790s user 0m0.688s sys 0m0.966s $ nvcc -arch=sm_52 -lineinfo -o t33 t33.cu -DUSE_HOST_ALLOC $ time ./t33 The following loop is supposed to push images in the GPU and do calculations in Async mode, and to wait 30 ms before the next image, so we should have the output on the screen in 10 x 30 ms. But it's far slower... You may adjust a #define ADJUST parameter to see what's happening. Dealing with image 0 Hard work is launched after image 0 is captured ==&gt; running ok Dealing with image 1 Hard_work still running, so unable to process after image 1 Dealing with image 2 Hard_work still running, so unable to process after image 2 Dealing with image 3 Hard_work still running, so unable to process after image 3 Dealing with image 4 Hard_work still running, so unable to process after image 4 Dealing with image 5 Hard_work still running, so unable to process after image 5 Dealing with image 6 Hard_work still running, so unable to process after image 6 Dealing with image 7 Hard work is launched after image 7 is captured ==&gt; running ok Dealing with image 8 Hard_work still running, so unable to process after image 8 Dealing with image 9 Hard_work still running, so unable to process after image 9 real 0m1.721s user 0m0.028s sys 0m0.629s $ In the USE_HOST_ALLOC case above, the launch pattern for the low-priority kernel is intermittent, as expected, and the overall run time is considerably shorter. In short, if you want the expected behavior out of cudaMemcpyAsync, make sure any participating host allocations are page-locked. A pictorial (profiler) example of the effect that pinning can have on multi-stream behavior can be seen in this answer."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am currently learning CUDA streams through the computation of a dot product between two vectors. The ingredients are a kernel function that takes in vectors x and y and returns a vector result of size equal to the number of blocks, where each block contributes its own reduced sum. I also have a host function dot_gpu that calls the kernel and reduces the vector result to the final dot product value. The synchronous version does just this: // copy to device copy_to_device&lt;double&gt;(x_h, x_d, n); copy_to_device&lt;double&gt;(y_h, y_d, n); // kernel double result = dot_gpu(x_d, y_d, n, blockNum, blockSize); while the async one goes like: double result[numChunks]; for (int i = 0; i &lt; numChunks; i++) { int offset = i * chunkSize; // copy to device copy_to_device_async&lt;double&gt;(x_h+offset, x_d+offset, chunkSize, stream[i]); copy_to_device_async&lt;double&gt;(y_h+offset, y_d+offset, chunkSize, stream[i]); // kernel result[i] = dot_gpu(x_d+offset, y_d+offset, chunkSize, blockNum, blockSize, stream[i]); } for (int i = 0; i &lt; numChunks; i++) { finalResult += result[i]; cudaStreamDestroy(stream[i]); } I am getting worse performance when using streams and was trying to investigate the reasons. I tried to pipeline the downloads, kernel calls and uploads, but with no results. // accumulate the result of each block into a single value double dot_gpu(const double *x, const double* y, int n, int blockNum, int blockSize, cudaStream_t stream=NULL) { double* result = malloc_device&lt;double&gt;(blockNum); dot_gpu_kernel&lt;&lt;&lt;blockNum, blockSize, blockSize * sizeof(double), stream&gt;&gt;&gt;(x, y, result, n); #if ASYNC double* r = malloc_host_pinned&lt;double&gt;(blockNum); copy_to_host_async&lt;double&gt;(result, r, blockNum, stream); CudaEvent copyResult; copyResult.record(stream); copyResult.wait(); #else double* r = malloc_host&lt;double&gt;(blockNum); copy_to_host&lt;double&gt;(result, r, blockNum); #endif double dotProduct = 0.0; for (int i = 0; i &lt; blockNum; i ++) { dotProduct += r[i]; } cudaFree(result); #if ASYNC cudaFreeHost(r); #else free(r); #endif return dotProduct; } My guess is that the problem is inside the dot_gpu() functions that doesn't only call the kernel. Tell me if I understand correctly the following stream executions foreach stream { cudaMemcpyAsync( device[stream], host[stream], ... stream ); LaunchKernel&lt;&lt;&lt;...stream&gt;&gt;&gt;( ... ); cudaMemcpyAsync( host[stream], device[stream], ... stream ); } The host executes all the three instructions without being blocked, since cudaMemcpyAsync and kernel return immediately (however on the GPU they will execute sequentially as they are assigned to the same stream). So host goes on to the next stream (even if stream1 who knows what stage it is at, but who cares.. it's doing his job on the GPU, right?) and executes the three instructions again without being blocked.. and so on and so forth. However, my code blocks the host before it can process the next stream, somewhere inside the dot_gpu() function. Is it because I am allocating &amp; freeing stuff, as well as reducing the array returned by the kernel to a single value?",
        "answers": [
            [
                "Assuming your objectified CUDA interface does what the function and method names suggest, there are three reasons why work from subsequent calls to dot_gpu() might not overlap: Your code explicitly blocks by recording an event and waiting for it. If it weren't blocking for 1. already, your code would block on the pinned host side allocation and deallocation, as you suspected. If your code weren't blocking for 2. already, work from subsequent calls to dot_gpu() might still not overlap depending on compute capbility. Devices of compute capability 3.0 or lower do not reorder operations even if they are enqueued to different streams. Even for devices of compute capability 3.5 and higher the number of streams whose operations can be reordered is limited by the CUDA_\u200bDEVICE_\u200bMAX_\u200bCONNECTIONS environment variable, which defaults to 8 and can be set to values as large as 32."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to use streams to run H2D copy and kernel run in parallel. To do this, I created 2 streams with cudaStreamNonBlocking flag. In a loop I do the following pseudocode: // pseudocode cudaMemcpy(data[0]); streamIdx = 0; while(1) { // prepare for next loop cudaMemcpyAsync(dData[!streamIdx], hData[!streamIdx], stream[!streamIdx]); // run current loop cudaStreamSynchronize(stream[streamIdx]); kernel1&lt;stream[streamIdx]&gt;(); kernel2&lt;stream[streamIdx]&gt;(); streamIdx = !streamIdx; } The host memory is pinned. The result is that each second cudaMemcpyAsync is delayed and this causes the code to run slower. See diagram: I managed to trick the copy engine by running a fake kernel on the same stream of the copy just to make it run immediately. Is there any NORMAL way to make the GPU execute the cudaMemcpyAsync immediately? I use GeForce GTX 1060 6GB GPU",
        "answers": [
            [
                "cudaStreamSynchronize forces the CPU to block waiting for the stream to be idle. In your case, the CPU has no need to block, instead just keep feeding the GPU. Restructure your code like this: while(1) { // prepare for next loop cudaMemcpyAsync(dData[streamIdx], hData[streamIdx], stream[streamIdx]); // run current loop kernel1&lt;stream[streamIdx]&gt;(); kernel2&lt;stream[streamIdx]&gt;(); streamIdx = !streamIdx; }"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have created the Scan Algorithm for CUDA from scratch and trying to use it for smaller data amounts less than 80,000 bytes. Two separate instances were created where, one runs the kernels using streams where possible and the other runs only in the default stream. What Ive observed is that for this range of data sizes, running with streams takes longer time to complete the task compared with the other method. When analysed using the nvprofiler, what was observed is that for smaller amount of data sizes, running in streams will not provide parallelism for separate kernals Without Streams With Streams But when the data size is increased some kind of parallelism could be obtained With Streams for 400,000bytes My problem is, is there some additional parameters to reduce this kernel invoking time delays or is it normal to have this kind of behavior for smaller data sizes where using streams are disadvantageous UPDATE : I've included the Runtime API calls timeline as well to clarify the answer",
        "answers": [
            [
                "Generally your data is too small to fully utilize the GPU in your first case. If you check the timeline of 'Runtime API' in nvvp, which you did not show in your figures, you will find launching a kernel take a few microseconds. If your first kernel in stream 13 is too short, the second kernel in stream 14 may not be launched yet, thus there's no parallelism across streams. Because of these overheads, you may find it even faster to run your program on CPU if the data is small."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to call the same kernel on CUDA (with one different input parameter) more times, but it executes only the first one and doesn't follow with other kernel calls. Assume the inputs arrays are new_value0=[123.814935276; 234; 100; 166; 203.0866414; 383; 186; 338; 173.0984233] and new_value1=[186.221113; 391; 64; 235; 195.7454998; 275; 218; 121; 118.0333872] part of output is: entra entra entra 334 549 524 alpha1.000000 alpha1.000000 alpha1.000000 in 2 idx-j 0-0 Value 123.814934 - m=334 - k=0 mlx -1618.175171 in 1 idx-j 0-1 Value 234.000000 - m=334 k=1 mlx -571.983032 in 1 idx-j 0-2 Value 100.000000 - m=334 k=2 mlx -208.243652 in 1 idx-j 1-0 Value 166.000000 - m=549 k=3 mlx 477.821777 in 2 idx-j 1-1 Value 203.086639 - m=549 - k=4 mlx -2448.556396 in 1 idx-j 1-2 Value 383.000000 - m=549 k=5 mlx -549.565674 in 1 idx-j 2-0 Value 186.000000 - m=524 k=6 mlx 239.955444 in 1 idx-j 2-1 Value 338.000000 - m=524 k=7 mlx 1873.975708 in 2 idx-j 2-2 Value 173.098419 - m=524 - k=8 mlx -835.600220 mlx =-835.600220 bs = -835.600220 . esci esci esci It is from the first kernel call. This is the kernel: __global__ void calculateMLpa( int N, float *bs, float *Value, float alphaxixj, float tauxi, const int sz, int dim, int *m){ int idx = blockIdx.x * blockDim.x + threadIdx.x; printf(\"entra\\n\"); if(idx&lt;N){ bs[idx]=0; int i,k=0; float mlx = 0; float v; float alphaxi; m[idx]=0; int state[9]; int p, j, t; int cont=0; if(idx==0){ m[idx]=Value[idx+1]+Value[idx+2]; } else if(idx==1){ m[idx]=Value[idx+2]+Value[idx+4]; }else{ m[idx]=Value[idx+4]+Value[idx+5]; } printf(\"%d \\n\",m[idx]); alphaxi = alphaxixj * (((float) sz) - 1.0); alphaxi = alphaxixj; printf(\"alpha%f \\n\",alphaxi); if(idx==0){ for(i=0;i&lt;sz;i++){ for (j = 0; j &lt; sz; j++) { // xi!=xj if (i!=j){ if(j==0) { k=i*3; } else if(j==1){ k=i*3+1; } else if(j==2) { k=i*3+2; } mlx = mlx + lgamma(alphaxixj + Value[k]) - lgamma(alphaxixj); printf(\"in 1 idx-j %d-%d Value %f - m=%d k=%d \\n\",i,j,Value[k],m[i],k); printf(\" mlx %f \\n\",mlx); //k++; } // xi else { if(j==0) { k=i*3; } else if(j==1){ k=i*3+1; } else if(j==2) { k=i*3+2; } mlx = mlx + lgamma(alphaxi) - lgamma(alphaxi + m[i]); mlx = mlx + lgamma(alphaxi + m[i] + 1.0)+ (alphaxi + 1.0) * log(tauxi); mlx = mlx - lgamma(alphaxi + 1.0)- (alphaxi + m[i] + 1.0) * log(tauxi + Value[k]); printf(\"in 2 idx-j %d-%d Value %f - m=%d - k=%d \\n\",i,j,Value[k],m[i],k); printf(\" mlx %f \\n\",mlx); //k++; } } } printf(\"mlx =%f \\n\",mlx); bs[idx]=mlx; printf(\"bs = %f .\\n\",bs[idx]); } } printf(\"esci\\n\"); } Here is the code: int main (void){ printf(\"START\"); FILE *pf; const int N=9; char fName[2083]; char *parents[3]={\"0\",\"1\",\"2\"}; char *traject[9]={\"0-0\",\"0-1\",\"0-2\",\"1-0\",\"1-1\",\"1-2\",\"2-0\",\"2-1\",\"2-2\"}; size_t parents_len; size_t traject_len; parents_len=sizeof(char)/sizeof(parents[0]); traject_len=sizeof(char)/sizeof(traject[0]); //possibile malloc //pointer host to memory char **parents_dev; char **traject_dev; //allocate on device cudaMalloc((void **)&amp;parents_dev,sizeof(char**)*parents_len); cudaMalloc((void **)&amp;traject_dev,sizeof(char**)*traject_len); //host to Device cudaMemcpy(parents_dev,parents,sizeof(char**)*parents_len,cudaMemcpyHostToDevice); cudaMemcpy(traject_dev,traject,sizeof(char**)*traject_len,cudaMemcpyHostToDevice); //Loop start int file,Epoca; float *bs; float *bs_dev; int file_size0=28; int file_size1=55; int file_size3=109; //size_t size = N * sizeof(float); bs=(float *)malloc(N * sizeof(float)); cudaMalloc((void **)&amp;bs_dev, N * sizeof(float)); float *new_value0,*new_value0_dev; new_value0=(float *)malloc(file_size0*N/3); cudaMalloc((void **)&amp;new_value0_dev, N * file_size0/3); // float *new_value1,*new_value1_dev; new_value1=(float *)malloc(file_size0*N/3); cudaMalloc((void **)&amp;new_value1_dev, N * file_size0/3); // float *new_value2,*new_value2_dev; new_value2=(float *)malloc(file_size0*N/3); cudaMalloc((void **)&amp;new_value2_dev, N * file_size0/3); // //one parent 1,2 float *new_value00,*new_value00_dev; new_value00=(float *)malloc(file_size1*N/6); cudaMalloc((void **)&amp;new_value00_dev, N * file_size1/6); // float *new_value01,*new_value01_dev; new_value01=(float *)malloc(file_size1*N/6); cudaMalloc((void **)&amp;new_value01_dev, N * file_size1/6); // float *new_value10,*new_value10_dev; new_value10=(float *)malloc(file_size1*N/6); cudaMalloc((void **)&amp;new_value10_dev, N * file_size1/6); // float *new_value11,*new_value11_dev; new_value11=(float *)malloc(file_size1*N/6); cudaMalloc((void **)&amp;new_value11_dev, N * file_size1/6); // float *new_value20,*new_value20_dev; new_value20=(float *)malloc(file_size1*N/6); cudaMalloc((void **)&amp;new_value20_dev, N * file_size1/6); // float *new_value21,*new_value21_dev; new_value21=(float *)malloc(file_size1*N/6); cudaMalloc((void **)&amp;new_value21_dev, N * file_size1/6); // //double parent float *new_value000,*new_value000_dev; new_value000=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value000_dev, N * file_size3/12); // float *new_value001,*new_value001_dev; new_value001=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value001_dev, N * file_size3/12); // float *new_value010,*new_value010_dev; new_value010=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value010_dev, N * file_size3/12); // float *new_value011,*new_value011_dev; new_value011=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value011_dev, N * file_size3/12); // float *new_value100,*new_value100_dev; new_value100=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value100_dev, N * file_size3/12); // float *new_value101,*new_value101_dev; new_value101=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value101_dev, N * file_size3/12); // float *new_value110,*new_value110_dev; new_value110=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value110_dev, N * file_size3/12); // float *new_value111,*new_value111_dev; new_value111=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value111_dev, N * file_size3/12); // float *new_value200,*new_value200_dev; new_value200=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value200_dev, N * file_size3/12); // float *new_value201,*new_value201_dev; new_value201=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value201_dev, N * file_size3/12); // float *new_value210,*new_value210_dev; new_value210=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value210_dev, N * file_size3/12); // float *new_value211,*new_value211_dev; new_value211=(float *)malloc(file_size3*N/12); cudaMalloc((void **)&amp;new_value211_dev, N * file_size3/12); //int file; for(file=0;file&lt;4;file++){ int f, i, j, file_size=0, kk=0; //file IO sprintf(fName, \"//home//user//prova%d.csv\",file); pf=fopen(fName,\"r\"); char *X; char *PaX; int Time; char *pa; char *xixj; float val; char buffer[BUFSIZ], *ptr; if (pf) { /* * Read each line from the file. */ while(fgets(buffer, sizeof buffer, pf)){ file_size++; } fclose(pf); } //variabile per kernel float *Value, *Value_dev; Value=(float *)malloc(file_size*N); cudaMalloc((void **)&amp;Value_dev, N * file_size); // pf=fopen(fName,\"r\"); if(pf) { printf(\"\\nnumero righe file %d = %d\\n\",file,file_size); char *state[file_size]; while(fgets(buffer, sizeof buffer, pf)) { //printf(\"start csv \\n\"); char *token; char *ptr = buffer; const char end[2]=\",\";//fgets(buffer, sizeof buffer, pf); token = strtok(ptr, end); f=0; /* walk through other tokens */ while( token != NULL ) { if(f==0){ X=token; // printf( \"X %s\\n\", token ); }else if(f==1){ PaX=token; // printf( \"PaX %s\\n\", token ); } else if(f==2){ Time=strtod(token,NULL); // printf( \"Time %f \\n\", token ); } else if(f==3){ pa=token; // printf( \"pa %s \\n\", token ); } else if(f==4){ xixj=(token); // printf( \"xixj %s \\n\", token ); } else{ Value[kk]=strtod(&amp;token[1], NULL); // printf(\"Value %f \\n\", Value[kk]); kk++; } token = strtok(NULL, end); f++; } } // //insert in variable if (file==0){ for (i=0;i&lt;(file_size0-1)/3;++i){ new_value0[i]=Value[i+1]; cudaMemcpy(new_value0_dev,new_value0,N*sizeof(file_size0), cudaMemcpyHostToDevice); new_value1[i]=Value[i + 1+((file_size0-1)/3)]; cudaMemcpy(new_value1_dev,new_value1,N*sizeof(file_size0), cudaMemcpyHostToDevice); new_value2[i]=Value[i + (1+ 2*(file_size0-1)/3)]; cudaMemcpy(new_value2_dev,new_value2,N*sizeof(file_size0), cudaMemcpyHostToDevice); // printf(\" new_value- %d - %f - %f - %f \\n\",i,new_value0[i],new_value1[i],new_value2[i]); } }else if(file==1 || file==2){ for (i=0; i&lt;(file_size1-1)/6;++i) { new_value00[i]=Value[i+1]; cudaMemcpy(new_value00_dev,new_value00,N*sizeof(file_size0), cudaMemcpyHostToDevice); new_value01[i]=Value[i+ ((file_size0-1)/3)+1]; cudaMemcpy(new_value01_dev,new_value01,N*sizeof(file_size1), cudaMemcpyHostToDevice); new_value10[i]=Value[i+ (2*(file_size1-1)/6)+1]; cudaMemcpy(new_value10_dev,new_value10,N*sizeof(file_size1), cudaMemcpyHostToDevice); new_value11[i]=Value[i+ (3*(file_size1-1)/6)+1]; cudaMemcpy(new_value11_dev,new_value11,N*sizeof(file_size1), cudaMemcpyHostToDevice); new_value20[i]=Value[i+ (4*(file_size1-1)/6)+1]; cudaMemcpy(new_value20_dev,new_value20,N*sizeof(file_size1), cudaMemcpyHostToDevice); new_value21[i]=Value[i+ (5*(file_size1-1)/6)+1]; cudaMemcpy(new_value21_dev,new_value21,N*sizeof(file_size1), cudaMemcpyHostToDevice); // printf(\" new_value- %d - %f - %f - %f - %f - %f - %f \\n\",i,new_value00[i],new_value01[i],new_value10[i],new_value11[i],new_value20[i],new_value21[i]); } }else{ for (i=0; i&lt;(file_size3-1)/12;++i) { new_value000[i]=Value[i+1]; cudaMemcpy(new_value000_dev,new_value000,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value001[i]=Value[i+ ((file_size3-1)/12)+1]; cudaMemcpy(new_value001_dev,new_value001,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value010[i]=Value[i+ (2*(file_size3-1)/12)+1]; cudaMemcpy(new_value010_dev,new_value010,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value011[i]=Value[i+ (3*(file_size3-1)/12)+1]; cudaMemcpy(new_value011_dev,new_value011,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value100[i]=Value[i+ (4*(file_size3-1)/12)+1]; cudaMemcpy(new_value100_dev,new_value100,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value101[i]=Value[i+ (5*(file_size3-1)/12)+1]; cudaMemcpy(new_value101_dev,new_value101,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value110[i]=Value[i+ (6*(file_size3-1)/12)+1]; cudaMemcpy(new_value110_dev,new_value110,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value111[i]=Value[i+ (7*(file_size3-1)/12)+1]; cudaMemcpy(new_value111_dev,new_value111,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value200[i]=Value[i+ (8*(file_size3-1)/12)+1]; cudaMemcpy(new_value200_dev,new_value200,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value201[i]=Value[i+ (9*(file_size3-1)/12)+1]; cudaMemcpy(new_value201_dev,new_value201,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value210[i]=Value[i+ (10*(file_size3-1)/12)+1]; cudaMemcpy(new_value210_dev,new_value210,N*sizeof(file_size3), cudaMemcpyHostToDevice); new_value211[i]=Value[i+ (11*(file_size3-1)/12)+1]; cudaMemcpy(new_value211_dev,new_value211,N*sizeof(file_size3), cudaMemcpyHostToDevice); // printf(\" new_value- %d - %f - %f - %f - %f - %f - %f - %f - %f - %f - %f - %f - %f \\n\",i,new_value000[i],new_value001[i],new_value010[i],new_value011[i],new_value100[i],new_value101[i],new_value110[i],new_value111[i],new_value200[i],new_value201[i],new_value210[i],new_value211[i]); } } } } //cudaMemcpy(Value_dev,Value,N*sizeof(file_size), cudaMemcpyHostToDevice); //variable of kernel //no parent //START computation printf(\"\\nPRE KERNEL\\n\"); const int sz=(sizeof(parents)/sizeof(*(parents))); const int dim=(sizeof(traject)/sizeof(*(traject))); printf(\"%d - %d \\n\",sz, dim); //chiamata kernel int block_size = 3; int n_blocks =1 ; int *m, *m_dev; m=(int *)malloc(sz*N); cudaMalloc((void **)&amp;m_dev, N * sz); float *trns_dev; cudaMalloc((void **)&amp;trns_dev, N * dim); int i; for(i=0;i&lt;(file_size0-1)/3;i++){ printf(\" new_value- %d - %f - %f - %f \\n\",i,new_value0[i],new_value1[i],new_value2[i]); } printf(\"\\n\"); for(i=0;i&lt;(file_size1-1)/6;i++){ printf(\" new_value- %d - %f - %f - %f - %f - %f - %f \\n\",i,new_value00[i],new_value01[i],new_value10[i],new_value11[i],new_value20[i],new_value21[i]); } printf(\"\\n\"); for(i=0;i&lt;(file_size3-1)/12;i++){ printf(\" new_value- %d - %f - %f - %f - %f - %f - %f - %f - %f - %f - %f - %f - %f \\n\",i,new_value000[i],new_value001[i],new_value010[i],new_value011[i],new_value100[i],new_value101[i],new_value110[i],new_value111[i],new_value200[i],new_value201[i],new_value210[i],new_value211[i]); } for(Epoca=0; Epoca&lt;3; Epoca++){ bs=0; float bf=0; cudaMalloc((void **)&amp;bf, N * sz); cudaMemcpy(bs_dev,bs,N*sizeof(float), cudaMemcpyHostToDevice); if(Epoca==0){ calculateMLpa&lt;&lt;&lt;n_blocks, block_size &gt;&gt;&gt;(N,bs_dev,new_value0_dev,1.0,0.1,sz,dim,m_dev); cudaDeviceSynchronize(); cudaMemcpy(bs,bs_dev,N*sizeof(float), cudaMemcpyDeviceToHost); cudaMemcpy(m,m_dev,N*sizeof(float), cudaMemcpyDeviceToHost); bf =+ bs[0]; printf(\"score= %f m0 = %d, m1 = %d, m2 = %d \\n\\n\", bf, m[0], m[1], m[2]); calculateMLpa&lt;&lt;&lt;n_blocks, block_size &gt;&gt;&gt;(N,bs_dev,new_value00_dev,1.0,0.1,sz,dim,m_dev); cudaDeviceSynchronize(); cudaMemcpy(bs,bs_dev,N*sizeof(float), cudaMemcpyDeviceToHost); cudaMemcpy(m,m_dev,N*sizeof(float), cudaMemcpyDeviceToHost); bf =+ bs[0]; printf(\"score= %f \\n\", bf); } printf(\"score %d= %f \\n\",Epoca, bf); } free(bs_dev); } I think that I can parallelize this with stream but I have never used it before. I have watched this for start.",
        "answers": [
            [
                "It sounds like you should use parallel CUDA streams. An interesting option: CUDA 7 introduces a new option, the per-thread default stream, that has two effects. First, it gives each host thread its own default stream. This means that commands issued to the default stream by different host threads can run concurrently. Also noteworthy: As described by the CUDA C Programming Guide, asynchronous commands return control to the calling host thread before the device has finished the requested task (they are non-blocking). These commands are: Kernel launches; Memory copies between two addresses to the same device memory; Memory copies from host to device of a memory block of 64 KB or less; Memory copies performed by functions with the Async suffix; Memory set function calls."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a CUDA stream which someone handed to me - a cudaStream_t value. The CUDA Runtime API does not seem to indicate how I can obtain the index of the device with which this stream is associated. Now, I know that cudaStream_t is just a pointer to a driver-level stream structure, but I'm hesitant to delve into the driver too much. Is there an idiomatic way to do this? Or some good reason not to want to do it? Edit: Another aspect to this question is whether the stream really is associated with a device in a way in which the CUDA driver itself can determine that device's identity given the pointed-to structure.",
        "answers": [
            [
                "Yes, streams are device-specific. In CUDA, streams are specific to a context, and contexts are specific to a device. Now, with the runtime API, you don't \"see\" contexts - you use just one context per device. But if you consider the driver API - you have: CUresult cuStreamGetCtx ( CUstream hStream, CUcontext* pctx ); CUstream and cudaStream_t are the same thing - a pointer. So, you can get the context. Then, you set or push that context to be the current context (read about doing that elsewhere), and finally, you use: CUresult cuCtxGetDevice ( CUdevice* device ) to get the current context's device. So, a bit of a hassle, but quite doable. My approach to easily determining a stream's device My workaround for this issue is to have the (C++'ish) stream wrapper class keep (the context and) the device among the member variables, which means that you can write: auto my_device = cuda::device::get(1); auto my_stream = my_device.create_stream(); /* using some default param values here */ assert(my_stream.device() == my_device()); and not have to worry about it (+ it won't trigger the extra API calls since, at construction, we know what the current context is and what its device is). Note: The above snippet is for a system with at least two CUDA devices, otherwise there is no device with index 1..."
            ],
            [
                "Regarding to the explicit streams, it is up to the implementation (to the best of my knowledge) there is no API providing this potential query capability to the users; I don't know about the capabilities that the driver can provide for you in this front, however, you can always query the stream. By using cudaStreamQuery, you can query your targeted stream on your selected device, if it returns cudaSuccess or cudaErrorNotReady it means that the stream does exist on that device and if it returns cudaErrorInvalidResourceHandle, it means that it does not."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "In CUDA's driver_types.h we have: typedef __device_builtin__ struct CUstream_st *cudaStream_t; And in cuda_runtime.h we have, in many places, default-initialized stream parameters. For example: template&lt;class T&gt; static __inline__ __host__ cudaError_t cudaLaunchKernel( const T *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem = 0, cudaStream_t stream = 0 ) How safe is it to assume the default stream is (cudaStream) nullptr?",
        "answers": [
            [
                "This is documented to be the case in multiple places: Programming guide: Kernel launches and host &lt;-&gt; device memory copies that do not specify any stream parameter, or equivalently that set the stream parameter to zero, are issued to the default stream. For example, cudaMemcpyAsync: The copy can optionally be associated to a stream by passing a non-zero stream argument. It seems quite safe to assume that the default stream is equivalent to (cudaStream_t) 0 Note that you can call out the default stream specifically with an argument other than zero, using cudaStreamLegacy (or cudaStreamPerThread) as described here. Interestingly, in CUDA 11.4, cudaStreamLegacy is a #define in driver_types.h as follows: #define cudaStreamLegacy ((cudaStream_t)0x1) This probably makes sense, since it is always associated with the legacy default stream, whereas a stream argument of 0 will reference the current system-defined default stream, whether that happens to be the legacy default stream or the per-thread default stream, as indicated in the previously linked blog. Similarly, cudaStreamPerThread is defined to be (cudaStream_t)2."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I am currently designing a short tutorial exhibiting various aspects and capabilities of Thrust template library. Unfortunately, it seems that there is a problem in a code that I have written in order to show how to use copy/compute concurrency using cuda streams. My code could be found here, in the asynchronousLaunch directory: https://github.com/gnthibault/Cuda_Thrust_Introduction/tree/master/AsynchronousLaunch Here is an abstract of the code that generates the problem: //STL #include &lt;cstdlib&gt; #include &lt;algorithm&gt; #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;functional&gt; //Thrust #include &lt;thrust/device_vector.h&gt; #include &lt;thrust/host_vector.h&gt; #include &lt;thrust/execution_policy.h&gt; #include &lt;thrust/scan.h&gt; //Cuda #include &lt;cuda_runtime.h&gt; //Local #include \"AsynchronousLaunch.cu.h\" int main( int argc, char* argv[] ) { const size_t fullSize = 1024*1024*64; const size_t halfSize = fullSize/2; //Declare one host std::vector and initialize it with random values std::vector&lt;float&gt; hostVector( fullSize ); std::generate(hostVector.begin(), hostVector.end(), normalRandomFunctor&lt;float&gt;(0.f,1.f) ); //And two device vector of Half size thrust::device_vector&lt;float&gt; deviceVector0( halfSize ); thrust::device_vector&lt;float&gt; deviceVector1( halfSize ); //Declare and initialize also two cuda stream cudaStream_t stream0, stream1; cudaStreamCreate( &amp;stream0 ); cudaStreamCreate( &amp;stream1 ); //Now, we would like to perform an alternate scheme copy/compute for( int i = 0; i &lt; 10; i++ ) { //Wait for the end of the copy to host before starting to copy back to device cudaStreamSynchronize(stream0); //Warning: thrust::copy does not handle asynchronous behaviour for host/device copy, you must use cudaMemcpyAsync to do so cudaMemcpyAsync(thrust::raw_pointer_cast(deviceVector0.data()), thrust::raw_pointer_cast(hostVector.data()), halfSize*sizeof(float), cudaMemcpyHostToDevice, stream0); cudaStreamSynchronize(stream1); //second copy is most likely to occur sequentially after the first one cudaMemcpyAsync(thrust::raw_pointer_cast(deviceVector1.data()), thrust::raw_pointer_cast(hostVector.data())+halfSize, halfSize*sizeof(float), cudaMemcpyHostToDevice, stream1); //Compute on device, here inclusive scan, for histogram equalization for instance thrust::transform( thrust::cuda::par.on(stream0), deviceVector0.begin(), deviceVector0.end(), deviceVector0.begin(), computeFunctor&lt;float&gt;() ); thrust::transform( thrust::cuda::par.on(stream1), deviceVector1.begin(), deviceVector1.end(), deviceVector1.begin(), computeFunctor&lt;float&gt;() ); //Copy back to host cudaMemcpyAsync(thrust::raw_pointer_cast(hostVector.data()), thrust::raw_pointer_cast(deviceVector0.data()), halfSize*sizeof(float), cudaMemcpyDeviceToHost, stream0); cudaMemcpyAsync(thrust::raw_pointer_cast(hostVector.data())+halfSize, thrust::raw_pointer_cast(deviceVector1.data()), halfSize*sizeof(float), cudaMemcpyDeviceToHost, stream1); } //Full Synchronize before exit cudaDeviceSynchronize(); cudaStreamDestroy( stream0 ); cudaStreamDestroy( stream1 ); return EXIT_SUCCESS; } Here are the results of one instance of the program, observed through nvidia visual profile: As yo can see, cudamemcopy (in brown) are both issued to stream 13 and 14, but kernels generated by Thrust from thrust::transform are issued to default stream (in blue in the capture) By the way, I am using cuda toolkit version 7.0.28, with a GTX680 and gcc 4.8.2. I would be grateful if someone could tell me what is wrong with my code. Thank you in advance Edit: here is the code that I consider as a solution: //STL #include &lt;cstdlib&gt; #include &lt;algorithm&gt; #include &lt;iostream&gt; #include &lt;functional&gt; #include &lt;vector&gt; //Thrust #include &lt;thrust/device_vector.h&gt; #include &lt;thrust/host_vector.h&gt; #include &lt;thrust/execution_policy.h&gt; //Cuda #include &lt;cuda_runtime.h&gt; //Local definitions template&lt;typename T&gt; struct computeFunctor { __host__ __device__ computeFunctor() {} __host__ __device__ T operator()( T in ) { //Naive functor that generates expensive but useless instructions T a = cos(in); for(int i = 0; i &lt; 350; i++ ) { a+=cos(in); } return a; } }; int main( int argc, char* argv[] ) { const size_t fullSize = 1024*1024*2; const size_t nbOfStrip = 4; const size_t stripSize = fullSize/nbOfStrip; //Allocate host pinned memory in order to use asynchronous api and initialize it with random values float* hostVector; cudaMallocHost(&amp;hostVector,fullSize*sizeof(float)); std::fill(hostVector, hostVector+fullSize, 1.0f ); //And one device vector of the same size thrust::device_vector&lt;float&gt; deviceVector( fullSize ); //Declare and initialize also two cuda stream std::vector&lt;cudaStream_t&gt; vStream(nbOfStrip); for( auto it = vStream.begin(); it != vStream.end(); it++ ) { cudaStreamCreate( &amp;(*it) ); } //Now, we would like to perform an alternate scheme copy/compute in a loop using the copyToDevice/Compute/CopyToHost for each stream scheme: for( int i = 0; i &lt; 5; i++ ) { for( int j=0; j!=nbOfStrip; j++) { size_t offset = stripSize*j; size_t nextOffset = stripSize*(j+1); cudaStreamSynchronize(vStream.at(j)); cudaMemcpyAsync(thrust::raw_pointer_cast(deviceVector.data())+offset, hostVector+offset, stripSize*sizeof(float), cudaMemcpyHostToDevice, vStream.at(j)); thrust::transform( thrust::cuda::par.on(vStream.at(j)), deviceVector.begin()+offset, deviceVector.begin()+nextOffset, deviceVector.begin()+offset, computeFunctor&lt;float&gt;() ); cudaMemcpyAsync(hostVector+offset, thrust::raw_pointer_cast(deviceVector.data())+offset, stripSize*sizeof(float), cudaMemcpyDeviceToHost, vStream.at(j)); } } //On devices that do not possess multiple queues copy engine capability, this solution serializes all command even if they have been issued to different streams //Why ? Because in the point of view of the copy engine, which is a single ressource in this case, there is a time dependency between HtoD(n) and DtoH(n) which is ok, but there is also // a false dependency between DtoH(n) and HtoD(n+1), that preclude any copy/compute overlap //Full Synchronize before testing second solution cudaDeviceSynchronize(); //Now, we would like to perform an alternate scheme copy/compute in a loop using the copyToDevice for each stream /Compute for each stream /CopyToHost for each stream scheme: for( int i = 0; i &lt; 5; i++ ) { for( int j=0; j!=nbOfStrip; j++) { cudaStreamSynchronize(vStream.at(j)); } for( int j=0; j!=nbOfStrip; j++) { size_t offset = stripSize*j; cudaMemcpyAsync(thrust::raw_pointer_cast(deviceVector.data())+offset, hostVector+offset, stripSize*sizeof(float), cudaMemcpyHostToDevice, vStream.at(j)); } for( int j=0; j!=nbOfStrip; j++) { size_t offset = stripSize*j; size_t nextOffset = stripSize*(j+1); thrust::transform( thrust::cuda::par.on(vStream.at(j)), deviceVector.begin()+offset, deviceVector.begin()+nextOffset, deviceVector.begin()+offset, computeFunctor&lt;float&gt;() ); } for( int j=0; j!=nbOfStrip; j++) { size_t offset = stripSize*j; cudaMemcpyAsync(hostVector+offset, thrust::raw_pointer_cast(deviceVector.data())+offset, stripSize*sizeof(float), cudaMemcpyDeviceToHost, vStream.at(j)); } } //On device that do not possess multiple queues in the copy engine, this solution yield better results, on other, it should show nearly identic results //Full Synchronize before exit cudaDeviceSynchronize(); for( auto it = vStream.begin(); it != vStream.end(); it++ ) { cudaStreamDestroy( *it ); } cudaFreeHost( hostVector ); return EXIT_SUCCESS; } Compiled using nvcc ./test.cu -o ./test.exe -std=c++11",
        "answers": [
            [
                "There are 2 things I would point out. Both of these are (now) referenced in this related question/answer which you may wish to refer to. The failure of thrust to issue the underlying kernels to non-default streams in this case seems to be related to this issue. It can be rectified (as covered in the comments to the question) by updating to the latest thrust version. Future CUDA versions (beyond 7) will probably include a fixed thrust as well. This is probably the central issue being discussed in this question. The question seems to also suggest that one of the goals is overlap of copy and compute: in order to show how to use copy/compute concurrency using cuda streams but this won't be achievable, I don't think, with the code as currently crafted, even if item 1 above is fixed. Overlap of copy with compute operations requires the proper use of cuda streams on the copy operation (cudaMemcpyAsync) as well as a pinned host allocation. The code proposed in the question is lacking any use of a pinned host allocation (std::vector does not use a pinned allocator by default, AFAIK), and so I would not expect the cudaMemcpyAsync operation to overlap with any kernel activity, even if it should be otherwise possible. To rectify this, a pinned allocator should be used, and one such example is given here. For completeness, the question is otherwise lacking an MCVE, which is expected for questions of this type. This makes it more difficult for others to attempt to test your issue, and is explicitly a close reason on SO. Yes, you provided a link to an external github repo, but this behavior is frowned on. The MCVE requirement explicitly states that the necessary pieces should be included in the question itself (not an external reference.) Since the only lacking piece, AFAICT, is \"AsynchronousLaunch.cu.h\", it seems like it would have been relatively straightforward to include this one additional piece in your question. The problem with external links is that when they break in the future, the question becomes less useful for future readers. (And, forcing others to navigate an external github repo looking for specific files is not conducive to getting help, in my opinion.)"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm in trying to improve the performance of my code using asynchronous memory transfer overlapped with GPU computation. Formerly I had a code where I created an FFT plan, and then make use of it multiple times. In such situation the time invested in creating the CUDA FFT plan is negligible althought according to this earlier post it could be quite significant. Now that I move to streams, what I'm doing is creating the \"same\" plan \"multiple times\" and then setting the CUDA FFT stream. According to the answers given by some of you in this other post this is wasteful. But, is there any other way to do it? NOTE: I'm acquiring the data in real-time, so launching a \"batch\" CUDA FFT is out of the question. What I'm doing is to create and lauch a new CUDA stream as a result of a complete pulse transmission. NOTE 2: I was also considering using a \"pool\" of \"CUDA Streams/FFT Plans\" instead but I don't think that would be an elegant, sensible solution, any thoughts? Is there otherwise a way to \"copy\" an \"existent\" fft plan before I assign the CUDA Stream? Thanks guys!/gals? Hopefully meet some of you in San Jose. =) Omar",
        "answers": [
            [
                "What I'm doing is to create and lauch a new CUDA stream as a result of a complete pulse transmission. Re-use the streams, rather than creating a new stream each time. Then you can re-use the plan created for that stream ahead of time, and you have no need to recreate the \"same\" plan on-the-fly. Perhaps this is what you mean by the pool of streams method. Your criticism is that it is not \"elegant\" or \"sensible\". I have no idea what that means. Stream re-use in pipelined algorithms is a common tactic, if for no other reason than to avoid the cudaStreamCreate overhead (whatever it may be, large or small). A cufft plan has a stream associated with it. You cannot copy a plan without the stream association. A plan is an opaque container."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "In nvidia's cuda technology. There are two concepts :the concept of stream in cuda programming, and the concept of Stream Multiprocessors(also called SMM in Maxwell Architecture,sometimes shorted by SM) ,how to understand the two? Case I:i just use the default stream to execute a kernel,and the block number is large enough. In this case , will all my 5 Stream Multiprocessors (GTX 750 Ti has 5 SMM,which is 640 core) be engaged in the processing the blocks,or just one Stream Multiprocessors is engaged in processing the one default stream Case II:i use cudaStreamCreate() to create 5 (or more) streams,and i use them to execute 5 different kernels,where all the 5 kernels are independent ,will the 5 kernels be parallel processed physically?",
        "answers": [
            [
                "There is no connection between cuda streams and Streaming Multiprocessors. Regardless of which stream arrangement you use to launch a kernel, all the SMs will participate in executing that kernel, if there are enough blocks. If you launch 5 kernels in 5 separate streams, most likely your kernels will execute approximately sequentially, unless all the kernels are very small in terms or resource usage, in which case they may execute at the same time."
            ],
            [
                "There are two concepts :the concept of stream in cuda programming, and the concept of Stream Multiprocessors(also called SMM in Maxwell Architecture,sometimes shorted by SM) ,how to understand the two? Despite the similar terminology, both concepts are unrelated. A streaming multiprocessor is a hardware component composed of several streaming processors that execute your kernel in a SIMD fashion. A stream is just a command queue on which you queue commands (yeah...) such as kernel executions or memory copies. Streams execute in parallel, so if you have two independent kernels, you may want to execute them in separate streams for (possibly) improved performance. You may also overlap kernel execution and data transfers if your device supports it. Case I:i just use the default stream to execute a kernel,and the block number is large enough. In this case , will all my 5 Stream Multiprocessors (GTX 750 Ti has 5 SMM,which is 640 core) be engaged in the processing the blocks,or just one Stream Multiprocessors is engaged in processing the one default stream Assuming the block number is large enough, all SMs will be busy. Case II:i use cudaStreamCreate() to create 5 (or more) streams,and i use them to execute 5 different kernels,where all the 5 kernels are independent ,will the 5 kernels be parallel processed physically? That's up to the scheduler. If your kernel computations can overlap (they don't fully utilize the GPU), then they most certainly will."
            ]
        ],
        "votes": [
            2.0000001,
            2.0000001
        ]
    },
    {
        "question": "Is it possible, using streams, to have multiple unique kernels on the same streaming multiprocessor in Kepler 3.5 GPUs? I.e. run 30 kernels of size &lt;&lt;&lt;1,1024&gt;&gt;&gt; at the same time on a Kepler GPU with 15 SMs?",
        "answers": [
            [
                "On a compute capability 3.5 device, it might be possible. Those devices support up to 32 concurrent kernels per GPU and 2048 threads peer multi-processor. With 64k registers per multi-processor, two blocks of 1024 threads could run concurrently if their register footprint was less than 16 per thread, and less than 24kb shared memory per block. You can find all of this is the hardware description found in the appendices of the CUDA programming guide."
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "Currently I have one pixel buffer and I process the data in it with a single kernel call: dim3 threadsPerBlock(32, 32) dim3 blocks(screenWidth / threadsPerBlock.x, screenHeight / threadsPerBlock.y); kernel&lt;&lt;&lt;blocks, threadsPerBlock&gt;&gt;&gt;(); The pixel buffer contains all the pixels in a window with dimensions screenWidth x screenHeight. My idea is to divide the window in 2 or 4 parts and to process the pixel data simultaneously. Can this be done, and if it can - how ? I've read little about streams but from what I understood two streams cannot work on a single piece of data (e.g. my pixelBuffer), or am I wrong ? Edit: My graphics card is with compute capability 3.0 Edit 2: I use SDL to do the drawing and I have a single GPU, and I use user defined data array: main.cu Color vfb_linear[VFB_MAX_SIZE * VFB_MAX_SIZE]; // array on the Host Color vfb[VFB_MAX_SIZE][VFB_MAX_SIZE] // 2D array used for SDL extern \"C\" void callKernels(Color* dev_vfb); int main() { Color* dev_vfb; // pixel array used on the GPU // allocate memory for dev_vfb on the GPU cudaMalloc((void**)&amp;dev_vfb, sizeof(Color) * RES_X * RES_Y); // memcpy HostToDevice cudaMemcpy(dev_vfb, vfb_linear, sizeof(Color) * RES_X * RES_Y, cudaMemcpyHostToDevice); callKernels(dev_vfb); // wrapper function that calls the kernels // memcpy DeviceToHost cudaMemcpy(vfb_linear, dev_vfb, sizeof(Color) * RES_X * RES_Y, cudaMemcpyDeviceToHost); // convert vfb_linear into 2D array so it can be handled by SDL convertDeviceToHostBuffer(); display(vfb); // render pixels on screen with SDL } cudaRenderer.cu __global__ void kernel(Color* dev_vfb) { int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; if (offset &lt; RES_X * RES_Y) { dev_vfb[offset] = getColorForPixel(); } } extern \"C\" callKernels(Color* dev_vfb) { dim3 threadsPerBlock(32, 32) dim3 blocks(screenWidth / threadsPerBlock.x, screenHeight / threadsPerBlock.y); kernel&lt;&lt;&lt;blocks, threadsPerBlock&gt;&gt;&gt;(dev_vfb); } contents of display(vfb): void display(Color vfb[VFB_MAX_SIZE][VFB_MAX_SIZE]) { // screen is pointer to SDL_Surface int rs = screen-&gt;format-&gt;Rshift; int gs = screen-&gt;format-&gt;Gshift; int bs = screen-&gt;format-&gt;Bshift; for (int y = 0; y &lt; screen-&gt;h; ++y) { Uint32* row = (Uint32*) ((Uint8*) screen-&gt;pixels + y * screen-&gt;pitch); for (int x = 0; x &lt; screen-&gt;w; ++x) row[x] = vfb[y][x].toRGB32(rs, gs, bs); } SDL_Flip(screen); } This is a simple example of what I am doing in my project. It is a raytracer and maybe SDL is the worst choice for interop with CUDA but I don't know if I will have time to change it.",
        "answers": [
            [
                "There's nothing that prevents two streams from working on the same piece of data in global memory of one device. As I said in the comments, I don't think this is a sensible approach to make things run faster. However, the modifications to your code would be something like this (coded in browser, not tested): __global__ void kernel(Color* dev_vfb, int slices) { int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; int offset = x + y * blockDim.x * gridDim.x; if (offset &lt; (RES_X * RES_Y/slices) { dev_vfb[offset] = getColorForPixel(); } } extern \"C\" callKernels(Color* dev_vfb) { int num_slices=2; cudaStream_t streams[num_slices]; for (int i = 0; i &lt; num_slices; i++) cudaStreamCreate(&amp;(streams[i])); dim3 threadsPerBlock(32, 32) dim3 blocks(screenWidth / threadsPerBlock.x, screenHeight / (num_slices*threadsPerBlock.y)); for (int i = 0; i &lt; num_slices; i++){ int off = i * (screenWidth*screenHeight/num_slices); kernel&lt;&lt;&lt;blocks, threadsPerBlock, 0, streams[i]&gt;&gt;&gt;(dev_vfb+off, num_slices); } }"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Considering the following case: //thread 0 on device 0: cudaMemcpyAsync(Dst0, Src0, ..., stream0);//stream0 is on Device 0; ... //thread 1 on device 1: cudaMemcpyAsync(Dst1, Src1, ..., stream1);//stream1 is on Device 1; Can the two memcpy operations occur concurrently and get doubled host-device bandwidth (as long as the host memory bandwidth is sufficient)? if the answer is yes the is there an upper limit of such concurrency? I plan to write some program for many (6-8) GPUs in a single compute node, so that will be quite critical for performance.",
        "answers": [
            [
                "Only one cudaMemcpy operation can actually be transferring data at any given time, per direction, across a PCIE link. Other operations can be queued up of course, but only one can be actually using the link. Some CUDA GPUs have one DMA engine, others have 2. Those that have two can be simultaneously transferring data in both directions. That is the limit of concurrency in data transfers: one per direction (assuming dual copy engines). Assuming the data transfers are not trivially small, then for the duration of each transfer, the PCIE bus will be fully utilized (in that direction)."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "This is a continuation of this post. It seems as though a special case has been solved by adding volitile but now something else has broken. If I add anything between the two kernel calls, the system reverts back to the old behavior, namely freezing and printing everything at once. This behavior is shown by adding sleep(2); between set_flag and read_flag. Also, when put in another program, this causes the GPU to lock up. What am I doing wrong now? Thanks again.",
        "answers": [
            [
                "There is an interaction with X and the display driver, as well as the standard output queue and it's interaction with the graphical display driver. A few experiments you can try, (with the sleep(2); added between the set_flag and read_flag kernels): Log into your machine over the network via ssh from another machine. I think your program will work. (X is not involved in the display in this case) comment out the line that prints out \"Starting...\" I think your program will then work. (This avoids the display driver/ print queue deadlock, see below). add a sleep(2); in between the \"Starting...\" print line and the first kernel. I think your program will then work. (This allows the display driver to fully service the first printout before the first kernel is launched, so no CPU thread stall.) Stop X and run from a console. I think your program will work. When the GPU is both hosting an X display and also running CUDA tasks, it has to switch between the two. For the duration of the CUDA task, ordinary display processing is suspended. You can read more about this here. The problem here is that when running X, the first printout is getting sent to the print queue but not actually displayed before the first kernel is launched. This is evident because you don't see the printout before the display freeze. After that, the CPU thread is getting stalled waiting for the display of the text. The second kernel is not starting. The intervening sleep(2); and it's interaction with the OS is enough for this stall to occur. And the executing first kernel has the display driver \"stopped\" for ordinary display tasks, so the OS never gets past it's stall, so the 2nd kernel doesn't get launched, leading to the apparent hang. Note that options 1,2, or 3 in the linked custhelp article would be effective in your case. Option 4 would not."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to set a flag in one kernel function and read it in another. Basically, I'm trying to do the following. #include &lt;iostream&gt; #include &lt;cuda.h&gt; #include &lt;cuda_runtime.h&gt; #define FLAGCLEAR 0 #define FLAGSET 1 using namespace std; __global__ void set_flag(int *flag) { *flag = FLAGSET; // Wait for flag to reset. while (*flag == FLAGSET); } __global__ void read_flag(int *flag) { // wait for the flag to set. while (*flag != FLAGSET); // Clear it for next time. *flag = FLAGCLEAR; } int main(void) { // Setup memory for flag int *flag; cudaMalloc(&amp;flag, sizeof(int)); // Setup streams cudaStream_t stream0, stream1; cudaStreamCreate(&amp;stream0); cudaStreamCreate(&amp;stream1); // Print something to let me know that we started. cout &lt;&lt; \"Starting the flagging\" &lt;&lt; endl; // do the flag test set_flag &lt;&lt;&lt;1,1,0,stream0&gt;&gt;&gt;(flag); read_flag &lt;&lt;&lt;1,1,0,stream1&gt;&gt;&gt;(flag); // Wait for the streams cudaDeviceSynchronize(); // Getting here is a painful process! cout &lt;&lt; \"Finished the flagging\" &lt;&lt; endl; // Clean UP! cudaStreamDestroy(stream0); cudaStreamDestroy(stream1); cudaFree(flag); } I eventually get the second printout, but only after the computer freezes for 15 seconds, and I get both printouts at the same time. These streams are supposed to run in parallel, and not bog the system down. What am I doing wrong? How can I fix this? Thanks. EDIT It seems as though a special case has been solved by adding volitile but now something else has broken. If I add anything between the two kernel calls, the system reverts back to the old behavior, namely freezing and printing everything at once. This behavior is shown by adding sleep(2); between set_flag and read_flag. Also, when put in another program, this causes the GPU to lock up. What am I doing wrong now? Thanks again.",
        "answers": [
            [
                "The compiler is allowed to do fairly aggressive optimization. Furthermore, the L1 caches on Fermi devices are not guaranteed to be coherent. To work around these issues, try adding the volatile keyword to your functions usage of the flag variable like so: __global__ void set_flag(volatile int *flag) and __global__ void read_flag(volatile int *flag) Generally speaking, when used on a variable resident in global memory, this will cause the compiler to issue loads that bypass the L1 cache and will also generally speaking prevent optimizations of these variables into registers, for example. I think you'll have better results. The code you've posted has the potential to deadlock due to these issues. Therefore, the observation you're seeing may actually be the OS (e.g. windows TDR) interrupting your program."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "In the CUDA documentation, it is mentioned that if we use 2 streams (stream0 and stream1) like this way: we copy data in stream0 then we launch the first kernel in stream0 , then we recuperate data from the device in stream0, and then the same operations are made in stream1, this way, like mentioned in the book \"CUDA by example 2010\", doesn't offer the concurrent execution, but in the \"concurrent kernels sample\" this method is used and offers the concurrent execution. So can you help me please to understand the difference between the two examples?",
        "answers": [
            [
                "Overlapped data transfer depends on many factors including compute capability version and coding styles. This blog may provide more info. https://developer.nvidia.com/content/how-overlap-data-transfers-cuda-cc"
            ],
            [
                "I'm just expanding Eric's answer. In the CUDA C Programming Guide, the example is reported of using 2 streams, say stream0 and stream1, to do the following CASE A memcpyHostToDevice --- stream0 kernel execution --- stream0 memcpyDeviceToHost --- stream0 memcpyHostToDevice --- stream1 kernel execution --- stream1 memcpyDeviceToHost --- stream1 In other words, all the operations of stream0 are issued first and then those regarding stream1. The same example is reported in the \"CUDA By Example\" book, Section 10.5, but it is \"apparently\" concluded (in \"apparent\" contradition with the guide) that in this way concurrency is not achieved. In Section 10.6 of \"CUDA By Example\", the following alternative use of streams is proposed CASE B memcpyHostToDevice --- stream0 memcpyHostToDevice --- stream1 kernel execution --- stream0 kernel execution --- stream1 memcpyDeviceToHost --- stream0 memcpyDeviceToHost --- stream1 In other words, the mem copy operations and kernel executions of stream0 and stream1 are now interlaced. The book points how with this solution concurrency is achieved. Actually, there is no contradition between the \"CUDA By Example\" book and the CUDA C Programming guide, since the discussion in the book has been carried out with particular reference to a GTX 285 card while, as already pointed out by Eric and in the quoted blog post How to Overlap Data Transfers in CUDA C/C++, concurrency can be differently achieved on different architectures, as a result of dependencies and copy engines available. For example, the blog considers two cards: C1060 and C2050. The former has one kernel engine and one copy engine which can issue only one memory transaction (H2D or D2H) at a time. The latter has one kernel engine and two copy engines which can simultaneously issue two memory transactions (H2D and D2H) at a time. What happens for the C1060, having only one copy engine, is the following CASE A - C1060 - NO CONCURRENCY ACHIEVED Stream Kernel engine Copy engine Comment stream0 ---- memcpyHostToDevice ---- stream0 ---- kernel execution ---- Depends on previous memcpy stream0 ---- memcpyDeviceToHost ---- Depends on previous kernel stream1 ---- memcpyHostToDevice ---- stream1 ---- kernel execution ---- Depends on previous memcpy stream1 ---- memcpyDeviceToHost ---- Depends on previous kernel CASE B - C1060 - CONCURRENCY ACHIEVED Stream Kernel engine Copy engine Comment stream0 ---- memcpyHostToDevice 0 ---- stream0/1 ---- Kernel execution 0 ---- memcpyHostToDevice 1 ---- stream0/1 ---- Kernel execution 1 ---- memcpyDeviceToHost 0 ---- stream1 ---- memcpyDeviceToHost 1 ---- Concerning the C2050 and considering the case of 3 streams, in CASE A concurrency is now achieved, opposite to C1060. CASE A - C2050 - CONCURRENCY ACHIEVED Stream Kernel engine Copy engine H2D Copy engine D2H stream0 ---- memcpyHostToDevice 0 ---- stream0/1 ---- kernel execution 0 ---- memcpyHostToDevice 1 ---- stream0/1/2 ---- kernel execution 1 ---- memcpyHostToDevice 2 ---- memcpyDeviceToHost 0 stream0/1/2 ---- kernel execution 2 ---- memcpyDeviceToHost 1 stream2 ---- memcpyDeviceToHost 2"
            ]
        ],
        "votes": [
            2.0000001,
            2.0000001
        ]
    },
    {
        "question": "In CUDA, how is stream 0 related to other streams? Does stream 0 (default stream) execute concurrently with other streams in a context or not? Considering the following example: cudaMemcpy(Dst, Src, sizeof(float)*datasize, cudaMemcpyHostToDevice);//stream 0; cudaStream_t stream1; /...creating stream1.../ somekernel&lt;&lt;&lt;blocks, threads, 0, stream1&gt;&gt;&gt;(Dst);//stream 1; In the above code, can the compiler ensure somekernel always launches AFTER cudaMemcpy finishes or will somekernel execuate concurrently with cudaMemcpy?",
        "answers": [
            [
                "cudaMemcpy call is (in all but a particular case) a synchronous call. The host thread running that code blocks until the memory transfer to the host. It cannot proceed to launch the kernel until the cudaMemcpy call has returned, it that doesn't happen until the copy operation is completed. More generally, the default stream (0 or null) implicitly serializes operations on the GPU whenever an operation is active in that stream. If you create streams and push operations into them at the same time as an operation is being performed in default stream, all concurrency in those streams is lost until the default stream is idle."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "If I want to benefit from Kepler GK110's Hyper-Q mechanism, i.e., to make two streams be put into two different hardware work queues to avoid some false dependencies, is it necessary for me to create the two streams with two CPU threads or the process forementioned will be undertaken automatically by CUDA driver or something else like CUDA work distributor? Or by what means I can testify the consideration above?",
        "answers": [
            [
                "As for other new CUDA features (e.g., dynamic parallelism, shuffle operations), Hyper-Q is equipped and very well explained and illustrated by the simpleHyperQ SDK example and the related documentation."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I would like to use streams in order to parallelize the execution of kernels that work on separate device data arrays. Data were allocated on the device and filled from previous kernels. I have written the following program that shows I can't reach my goal so far. Indeed, the kernels on two non-default streams execute sequentially in their respective streams. The same behaviour is observed on 2 Intel machines with latest Debian linux version. One has a Tesla C2075 with CUDA 4.2 and the other has a Geforce 460GT with CUDA 5.0. The Visual Profiler shows sequential execution in both the 4.2 and also 5.0 CUDA version. Here is the code: #include &lt;iostream&gt; #include &lt;stdio.h&gt; #include &lt;ctime&gt; #include &lt;curand.h&gt; using namespace std; // compile and run this way: // nvcc cuStreamsBasics.cu -arch=sm_20 -o testCuStream -lcuda -lcufft -lcurand // testCuStream 1024 512 512 /* -------------------------------------------------------------------------- */ // \"useful\" macros /* -------------------------------------------------------------------------- */ #define MSG_ASSERT( CONDITION, MSG ) \\ if (! (CONDITION)) \\ { \\ std::cerr &lt;&lt; std::endl &lt;&lt; \"Dynamic assertion `\" #CONDITION \"` failed in \" &lt;&lt; __FILE__ \\ &lt;&lt; \" line \" &lt;&lt; __LINE__ &lt;&lt; \": &lt;\" &lt;&lt; MSG &lt;&lt; \"&gt;\" &lt;&lt; std::endl; \\ exit( 1 ); \\ } \\ #define ASSERT( CONDITION ) \\ MSG_ASSERT( CONDITION, \" \" ) // allocate data on the GPU memory, unpinned #define CUDALLOC_GPU( _TAB, _DIM, _DATATYPE ) \\ MSG_ASSERT( \\ cudaMalloc( (void**) &amp;_TAB, _DIM * sizeof( _DATATYPE) ) \\ == cudaSuccess , \"failed CUDALLOC\" ); /* -------------------------------------------------------------------------- */ // the CUDA kernels /* -------------------------------------------------------------------------- */ // finds index in 1D array from sequential blocks #define CUDAINDEX_1D \\ blockIdx.y * ( gridDim.x * blockDim.x ) + \\ blockIdx.x * blockDim.x + \\ threadIdx.x; \\ __global__ void kernel_diva(float* data, float value, int array_size) { int i = CUDAINDEX_1D if (i &lt; array_size) data[i] /= value; } __global__ void kernel_jokea(float* data, float value, int array_size) { int i = CUDAINDEX_1D if (i &lt; array_size) data[i] *= value + sin( double(i)) * 1/ cos( double(i) ); } /* -------------------------------------------------------------------------- */ // usage /* -------------------------------------------------------------------------- */ static void usage(int argc, char **argv) { if ((argc -1) != 3) { printf(\"Usage: %s &lt;dimx&gt; &lt;dimy&gt; &lt;dimz&gt; \\n\", argv[0]); printf(\"do stuff\\n\"); exit(1); } } /* -------------------------------------------------------------------------- */ // main program, finally! /* -------------------------------------------------------------------------- */ int main(int argc, char** argv) { usage(argc, argv); size_t x_dim = atoi( argv[1] ); size_t y_dim = atoi( argv[2] ); size_t z_dim = atoi( argv[3] ); cudaStream_t stream1, stream2; ASSERT( cudaStreamCreate( &amp;stream1 ) == cudaSuccess ); ASSERT( cudaStreamCreate( &amp;stream2 ) == cudaSuccess ); size_t size = x_dim * y_dim * z_dim; float *data1, *data2; CUDALLOC_GPU( data1, size, float); CUDALLOC_GPU( data2, size, float); curandGenerator_t gen; curandCreateGenerator(&amp;gen, CURAND_RNG_PSEUDO_DEFAULT); /* Set seed */ curandSetPseudoRandomGeneratorSeed(gen, 1234ULL); /* Generate n floats on device */ curandGenerateUniform(gen, data1, size); curandGenerateUniform(gen, data2, size); dim3 dimBlock( z_dim, 1, 1); dim3 dimGrid( x_dim, y_dim, 1); clock_t start; double diff; cudaDeviceSynchronize(); start = clock(); kernel_diva &lt;&lt;&lt; dimGrid, dimBlock&gt;&gt;&gt;( data1, 5.55f, size); kernel_jokea&lt;&lt;&lt; dimGrid, dimBlock&gt;&gt;&gt;( data1, 5.55f, size); kernel_diva &lt;&lt;&lt; dimGrid, dimBlock&gt;&gt;&gt;( data2, 5.55f, size); kernel_jokea&lt;&lt;&lt; dimGrid, dimBlock&gt;&gt;&gt;( data2, 5.55f, size); cudaDeviceSynchronize(); diff = ( std::clock() - start ) / (double)CLOCKS_PER_SEC; cout &lt;&lt; endl &lt;&lt; \"sequential: \" &lt;&lt; diff; cudaDeviceSynchronize(); start = clock(); kernel_diva &lt;&lt;&lt; dimGrid, dimBlock, 0, stream1 &gt;&gt;&gt;( data1, 5.55f, size); kernel_diva &lt;&lt;&lt; dimGrid, dimBlock, 0, stream2 &gt;&gt;&gt;( data2, 5.55f, size); kernel_jokea&lt;&lt;&lt; dimGrid, dimBlock, 0, stream1 &gt;&gt;&gt;( data1, 5.55f, size); kernel_jokea&lt;&lt;&lt; dimGrid, dimBlock, 0, stream2 &gt;&gt;&gt;( data2, 5.55f, size); cudaDeviceSynchronize(); diff = ( std::clock() - start ) / (double)CLOCKS_PER_SEC; cout &lt;&lt; endl &lt;&lt; \"parallel: \" &lt;&lt; diff; cudaStreamDestroy( stream1 ); cudaStreamDestroy( stream2 ); return 0; } Typically, the dimension of the arrays is 512^3 single float. I usually just cut the array in blocks of (512,1,1) threads that I put on a grid of size (1&lt;&lt;15, (rest), 1). Thank you in advance for any hint or comment. Best regards.",
        "answers": [
            [
                "I'm trying to provide an interpretation to why you do not see execution overlap of your two kernels. To this end, I have constructed the code reported below, which uses your two kernels and monitors which Streaming Multiprocessor (SM) each block runs on. I'm using CUDA 6.5 (Release Candidate) and I'm running on a GT540M card, which has only 2 SMs, so it provides a simple playground to work with. The blockSize choice is delegated to the new CUDA 6.5 cudaOccupancyMaxPotentialBlockSize facility. THE CODE #include &lt;stdio.h&gt; #include &lt;time.h&gt; //#define DEBUG_MODE /********************/ /* CUDA ERROR CHECK */ /********************/ #define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); } inline void gpuAssert(cudaError_t code, char *file, int line, bool abort=true) { if (code != cudaSuccess) { fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line); if (abort) exit(code); } } /**************************************************/ /* STREAMING MULTIPROCESSOR IDENTIFICATION NUMBER */ /**************************************************/ __device__ unsigned int get_smid(void) { unsigned int ret; asm(\"mov.u32 %0, %smid;\" : \"=r\"(ret) ); return ret; } /************/ /* KERNEL 1 */ /************/ __global__ void kernel_1(float * __restrict__ data, const float value, int *sm, int N) { int i = threadIdx.x + blockIdx.x * blockDim.x; if (i &lt; N) { data[i] = data[i] / value; if (threadIdx.x==0) sm[blockIdx.x]=get_smid(); } } //__global__ void kernel_1(float* data, float value, int N) //{ // int start = blockIdx.x * blockDim.x + threadIdx.x; // for (int i = start; i &lt; N; i += blockDim.x * gridDim.x) // { // data[i] = data[i] / value; // } //} /************/ /* KERNEL 2 */ /************/ __global__ void kernel_2(float * __restrict__ data, const float value, int *sm, int N) { int i = threadIdx.x + blockIdx.x*blockDim.x; if (i &lt; N) { data[i] = data[i] * (value + sin(double(i)) * 1./cos(double(i))); if (threadIdx.x==0) sm[blockIdx.x]=get_smid(); } } //__global__ void kernel_2(float* data, float value, int N) //{ // int start = blockIdx.x * blockDim.x + threadIdx.x; // for (int i = start; i &lt; N; i += blockDim.x * gridDim.x) // { // data[i] = data[i] * (value + sin(double(i)) * 1./cos(double(i))); // } //} /********/ /* MAIN */ /********/ int main() { const int N = 10000; const float value = 5.55f; const int rep_num = 20; // --- CPU memory allocations float *h_data1 = (float*) malloc(N*sizeof(float)); float *h_data2 = (float*) malloc(N*sizeof(float)); float *h_data1_ref = (float*) malloc(N*sizeof(float)); float *h_data2_ref = (float*) malloc(N*sizeof(float)); // --- CPU data initializations srand(time(NULL)); for (int i=0; i&lt;N; i++) { h_data1[i] = rand() / RAND_MAX; h_data2[i] = rand() / RAND_MAX; } // --- GPU memory allocations float *d_data1, *d_data2; gpuErrchk(cudaMalloc((void**)&amp;d_data1, N*sizeof(float))); gpuErrchk(cudaMalloc((void**)&amp;d_data2, N*sizeof(float))); // --- CPU -&gt; GPU memory transfers gpuErrchk(cudaMemcpy(d_data1, h_data1, N*sizeof(float), cudaMemcpyHostToDevice)); gpuErrchk(cudaMemcpy(d_data2, h_data2, N*sizeof(float), cudaMemcpyHostToDevice)); // --- CPU data initializations srand(time(NULL)); for (int i=0; i&lt;N; i++) { h_data1_ref[i] = h_data1[i] / value; h_data2_ref[i] = h_data2[i] * (value + sin(double(i)) * 1./cos(double(i))); } // --- Stream creations cudaStream_t stream1, stream2; gpuErrchk(cudaStreamCreate(&amp;stream1)); gpuErrchk(cudaStreamCreate(&amp;stream2)); // --- Launch parameters configuration int blockSize1, blockSize2, minGridSize1, minGridSize2, gridSize1, gridSize2; cudaOccupancyMaxPotentialBlockSize(&amp;minGridSize1, &amp;blockSize1, kernel_1, 0, N); cudaOccupancyMaxPotentialBlockSize(&amp;minGridSize2, &amp;blockSize2, kernel_2, 0, N); gridSize1 = (N + blockSize1 - 1) / blockSize1; gridSize2 = (N + blockSize2 - 1) / blockSize2; // --- Allocating space for SM IDs int *h_sm_11 = (int*) malloc(gridSize1*sizeof(int)); int *h_sm_12 = (int*) malloc(gridSize1*sizeof(int)); int *h_sm_21 = (int*) malloc(gridSize2*sizeof(int)); int *h_sm_22 = (int*) malloc(gridSize2*sizeof(int)); int *d_sm_11, *d_sm_12, *d_sm_21, *d_sm_22; gpuErrchk(cudaMalloc((void**)&amp;d_sm_11, gridSize1*sizeof(int))); gpuErrchk(cudaMalloc((void**)&amp;d_sm_12, gridSize1*sizeof(int))); gpuErrchk(cudaMalloc((void**)&amp;d_sm_21, gridSize2*sizeof(int))); gpuErrchk(cudaMalloc((void**)&amp;d_sm_22, gridSize2*sizeof(int))); // --- Timing individual kernels float time; cudaEvent_t start, stop; cudaEventCreate(&amp;start); cudaEventCreate(&amp;stop); cudaEventRecord(start, 0); for (int i=0; i&lt;rep_num; i++) kernel_1&lt;&lt;&lt;gridSize1, blockSize1&gt;&gt;&gt;(d_data1, value, d_sm_11, N); cudaEventRecord(stop, 0); cudaEventSynchronize(stop); cudaEventElapsedTime(&amp;time, start, stop); printf(\"Kernel 1 - elapsed time: %3.3f ms \\n\", time/rep_num); cudaEventRecord(start, 0); for (int i=0; i&lt;rep_num; i++) kernel_2&lt;&lt;&lt;gridSize2, blockSize2&gt;&gt;&gt;(d_data1, value, d_sm_21, N); cudaEventRecord(stop, 0); cudaEventSynchronize(stop); cudaEventElapsedTime(&amp;time, start, stop); printf(\"Kernel 2 - elapsed time: %3.3f ms \\n\", time/rep_num); // --- No stream case cudaEventRecord(start, 0); kernel_1&lt;&lt;&lt;gridSize1, blockSize1&gt;&gt;&gt;(d_data1, value, d_sm_11, N); #ifdef DEBUG_MODE gpuErrchk(cudaPeekAtLastError()); gpuErrchk(cudaDeviceSynchronize()); gpuErrchk(cudaMemcpy(h_data1, d_data1, N*sizeof(float), cudaMemcpyDeviceToHost)); // --- Results check for (int i=0; i&lt;N; i++) { if (h_data1[i] != h_data1_ref[i]) { printf(\"Kernel1 - Error at i = %i; Host = %f; Device = %f\\n\", i, h_data1_ref[i], h_data1[i]); return; } } #endif kernel_2&lt;&lt;&lt;gridSize2, blockSize2&gt;&gt;&gt;(d_data1, value, d_sm_21, N); #ifdef DEBUG_MODE gpuErrchk(cudaPeekAtLastError()); gpuErrchk(cudaDeviceSynchronize()); #endif kernel_1&lt;&lt;&lt;gridSize1, blockSize1&gt;&gt;&gt;(d_data2, value, d_sm_12, N); #ifdef DEBUG_MODE gpuErrchk(cudaPeekAtLastError()); gpuErrchk(cudaDeviceSynchronize()); gpuErrchk(cudaMemcpy(d_data2, h_data2, N*sizeof(float), cudaMemcpyHostToDevice)); #endif kernel_2&lt;&lt;&lt;gridSize2, blockSize2&gt;&gt;&gt;(d_data2, value, d_sm_22, N); #ifdef DEBUG_MODE gpuErrchk(cudaPeekAtLastError()); gpuErrchk(cudaDeviceSynchronize()); gpuErrchk(cudaMemcpy(h_data2, d_data2, N*sizeof(float), cudaMemcpyDeviceToHost)); for (int i=0; i&lt;N; i++) { if (h_data2[i] != h_data2_ref[i]) { printf(\"Kernel2 - Error at i = %i; Host = %f; Device = %f\\n\", i, h_data2_ref[i], h_data2[i]); return; } } #endif cudaEventRecord(stop, 0); cudaEventSynchronize(stop); cudaEventElapsedTime(&amp;time, start, stop); printf(\"No stream - elapsed time: %3.3f ms \\n\", time); // --- Stream case cudaEventRecord(start, 0); kernel_1&lt;&lt;&lt;gridSize1, blockSize1, 0, stream1 &gt;&gt;&gt;(d_data1, value, d_sm_11, N); #ifdef DEBUG_MODE gpuErrchk(cudaPeekAtLastError()); gpuErrchk(cudaDeviceSynchronize()); #endif kernel_1&lt;&lt;&lt;gridSize1, blockSize1, 0, stream2 &gt;&gt;&gt;(d_data2, value, d_sm_12, N); #ifdef DEBUG_MODE gpuErrchk(cudaPeekAtLastError()); gpuErrchk(cudaDeviceSynchronize()); #endif kernel_2&lt;&lt;&lt;gridSize2, blockSize2, 0, stream1 &gt;&gt;&gt;(d_data1, value, d_sm_21, N); #ifdef DEBUG_MODE gpuErrchk(cudaPeekAtLastError()); gpuErrchk(cudaDeviceSynchronize()); #endif kernel_2&lt;&lt;&lt;gridSize2, blockSize2, 0, stream2 &gt;&gt;&gt;(d_data2, value, d_sm_22, N); #ifdef DEBUG_MODE gpuErrchk(cudaPeekAtLastError()); gpuErrchk(cudaDeviceSynchronize()); #endif cudaEventRecord(stop, 0); cudaEventSynchronize(stop); cudaEventElapsedTime(&amp;time, start, stop); printf(\"Stream - elapsed time: %3.3f ms \\n\", time); cudaStreamDestroy(stream1); cudaStreamDestroy(stream2); printf(\"Test passed!\\n\"); gpuErrchk(cudaMemcpy(h_sm_11, d_sm_11, gridSize1*sizeof(int), cudaMemcpyDeviceToHost)); gpuErrchk(cudaMemcpy(h_sm_12, d_sm_12, gridSize1*sizeof(int), cudaMemcpyDeviceToHost)); gpuErrchk(cudaMemcpy(h_sm_21, d_sm_21, gridSize2*sizeof(int), cudaMemcpyDeviceToHost)); gpuErrchk(cudaMemcpy(h_sm_22, d_sm_22, gridSize2*sizeof(int), cudaMemcpyDeviceToHost)); printf(\"Kernel 1: gridSize = %i; blockSize = %i\\n\", gridSize1, blockSize1); printf(\"Kernel 2: gridSize = %i; blockSize = %i\\n\", gridSize2, blockSize2); for (int i=0; i&lt;gridSize1; i++) { printf(\"Kernel 1 - Data 1: blockNumber = %i; SMID = %d\\n\", i, h_sm_11[i]); printf(\"Kernel 1 - Data 2: blockNumber = %i; SMID = %d\\n\", i, h_sm_12[i]); } for (int i=0; i&lt;gridSize2; i++) { printf(\"Kernel 2 - Data 1: blockNumber = %i; SMID = %d\\n\", i, h_sm_21[i]); printf(\"Kernel 2 - Data 2: blockNumber = %i; SMID = %d\\n\", i, h_sm_22[i]); } cudaDeviceReset(); return 0; } KERNEL TIMINGS FOR N = 100 and N = 10000 N = 100 kernel_1 0.003ms kernel_2 0.005ms N = 10000 kernel_1 0.011ms kernel_2 0.053ms So, kernel 1 is more computationally expensive than kernel 2. RESULTS FOR N = 100 Kernel 1: gridSize = 1; blockSize = 100 Kernel 2: gridSize = 1; blockSize = 100 Kernel 1 - Data 1: blockNumber = 0; SMID = 0 Kernel 1 - Data 2: blockNumber = 0; SMID = 1 Kernel 2 - Data 1: blockNumber = 0; SMID = 0 Kernel 2 - Data 2: blockNumber = 0; SMID = 1 In this case, each kernel is launched with only one block and this is the timeline. As you can see, the overlap occurs. By looking at the above outcomes, the scheduler delivers the single blocks of the two calls to kernel 1 in parallel to the two available SMs and then does the same for kernel 2. This seems to be the main reason why overlap occurs. RESULTS FOR N = 10000 Kernel 1: gridSize = 14; blockSize = 768 Kernel 2: gridSize = 10; blockSize = 1024 Kernel 1 - Data 1: blockNumber = 0; SMID = 0 Kernel 1 - Data 2: blockNumber = 0; SMID = 1 Kernel 1 - Data 1: blockNumber = 1; SMID = 1 Kernel 1 - Data 2: blockNumber = 1; SMID = 0 Kernel 1 - Data 1: blockNumber = 2; SMID = 0 Kernel 1 - Data 2: blockNumber = 2; SMID = 1 Kernel 1 - Data 1: blockNumber = 3; SMID = 1 Kernel 1 - Data 2: blockNumber = 3; SMID = 0 Kernel 1 - Data 1: blockNumber = 4; SMID = 0 Kernel 1 - Data 2: blockNumber = 4; SMID = 1 Kernel 1 - Data 1: blockNumber = 5; SMID = 1 Kernel 1 - Data 2: blockNumber = 5; SMID = 0 Kernel 1 - Data 1: blockNumber = 6; SMID = 0 Kernel 1 - Data 2: blockNumber = 6; SMID = 0 Kernel 1 - Data 1: blockNumber = 7; SMID = 1 Kernel 1 - Data 2: blockNumber = 7; SMID = 1 Kernel 1 - Data 1: blockNumber = 8; SMID = 0 Kernel 1 - Data 2: blockNumber = 8; SMID = 1 Kernel 1 - Data 1: blockNumber = 9; SMID = 1 Kernel 1 - Data 2: blockNumber = 9; SMID = 0 Kernel 1 - Data 1: blockNumber = 10; SMID = 0 Kernel 1 - Data 2: blockNumber = 10; SMID = 0 Kernel 1 - Data 1: blockNumber = 11; SMID = 1 Kernel 1 - Data 2: blockNumber = 11; SMID = 1 Kernel 1 - Data 1: blockNumber = 12; SMID = 0 Kernel 1 - Data 2: blockNumber = 12; SMID = 1 Kernel 1 - Data 1: blockNumber = 13; SMID = 1 Kernel 1 - Data 2: blockNumber = 13; SMID = 0 Kernel 2 - Data 1: blockNumber = 0; SMID = 0 Kernel 2 - Data 2: blockNumber = 0; SMID = 0 Kernel 2 - Data 1: blockNumber = 1; SMID = 1 Kernel 2 - Data 2: blockNumber = 1; SMID = 1 Kernel 2 - Data 1: blockNumber = 2; SMID = 1 Kernel 2 - Data 2: blockNumber = 2; SMID = 0 Kernel 2 - Data 1: blockNumber = 3; SMID = 0 Kernel 2 - Data 2: blockNumber = 3; SMID = 1 Kernel 2 - Data 1: blockNumber = 4; SMID = 1 Kernel 2 - Data 2: blockNumber = 4; SMID = 0 Kernel 2 - Data 1: blockNumber = 5; SMID = 0 Kernel 2 - Data 2: blockNumber = 5; SMID = 1 Kernel 2 - Data 1: blockNumber = 6; SMID = 1 Kernel 2 - Data 2: blockNumber = 6; SMID = 0 Kernel 2 - Data 1: blockNumber = 7; SMID = 0 Kernel 2 - Data 2: blockNumber = 7; SMID = 1 Kernel 2 - Data 1: blockNumber = 8; SMID = 1 Kernel 2 - Data 2: blockNumber = 8; SMID = 0 Kernel 2 - Data 1: blockNumber = 9; SMID = 0 Kernel 2 - Data 2: blockNumber = 9; SMID = 1 This is the timeline: In this case, no overlap occurs. According to the above outcomes, this does not mean that the two SMs are not simultaneously exploited, but (I think) that, due to the larger number of blocks to be launched, assigning two blocks of different kernels or the two blocks of the same kernel does not make much difference in terms of performance and thus the scheduler chooses the second option. I have tested that, considering more work done per thread, the behavior keeps the same."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "Using steams to overlap data transfer with kernel execution is not working in my system. Hello I want to use Overlapping computation and data transfers in CUDA ,but I can't. NVIDIA help document say Overlapping computation and data transfers is possible if you use streams. but my system has not being working Please help me. My system is below OS : Window 7 64bit CUDA : ver 5.0.7 Develp kit : Visual studion 2008 GPU : GTX 680 I get a profile View is Like this I am not getting overlapping, the code is below: -new pinned memory cudaHostAlloc((void **)&amp;apBuffer, sizeof(BYTE)*lBufferSize,cudaHostAllocDefault); -call function //Input Data for(int i=0;i&lt;m_n3DChannelCnt*m_nBucket;++i) { cudaErrorChk_Return(cudaMemcpyAsync(d_ppbImg[i],ppbImg[i],sizeof(BYTE)*m_nImgWidth*m_nImgHeight,cudaMemcpyHostToDevice,m_pStream[i/m_nBucket])); } //Call Function for(int i=0;i&lt;m_n3DChannelCnt ;++i) {KernelGetVis8uObjPhsPhs&lt;&lt;&lt;nBlockCnt,nThreadCnt,0,m_pStream[i]&gt;&gt;&gt;(d_ppbVis[i],d_ppbAvg[i],d_ppfPhs[i],d_ppfObj[i],d_ppbAmp[i] ,nTotalSize,d_ppstRefData[i],d_ppbImg[i*m_nBucket],d_ppbImg[i*m_nBucket+1],d_ppbImg[i*m_nBucket+2],d_ppbImg[i*m_nBucket+3] ,fSclFloatVis2ByteVis); } //OutputData for(int i=0;i&lt;m_n3DChannelCnt;++i) { if(ppbVis &amp;&amp; ppbVis[i]) cudaErrorChk_Return(cudaMemcpyAsync(ppbVis[i],d_ppbVis[i],sizeof(BYTE)*m_nImgWidth*m_nImgHeight,cudaMemcpyDeviceToHost,m_pStream[i])); if(ppbAvg &amp;&amp; ppbAvg[i]) cudaErrorChk_Return(cudaMemcpyAsync(ppbAvg[i],d_ppbAvg[i],sizeof(BYTE)*m_nImgWidth*m_nImgHeight,cudaMemcpyDeviceToHost,m_pStream[i])); if(ppfPhs &amp;&amp; ppfPhs[i]) cudaErrorChk_Return(cudaMemcpyAsync(ppfPhs[i],d_ppfPhs[i],sizeof(float)*m_nImgWidth*m_nImgHeight,cudaMemcpyDeviceToHost,m_pStream[i])); if(ppfObj &amp;&amp; ppfObj[i]) cudaErrorChk_Return(cudaMemcpyAsync(ppfObj[i],d_ppfObj[i],sizeof(float)*m_nImgWidth*m_nImgHeight,cudaMemcpyDeviceToHost,m_pStream[i])); if(ppbAmp &amp;&amp; ppbAmp[i]) cudaErrorChk_Return(cudaMemcpyAsync(ppbAmp[i],d_ppbAmp[i],sizeof(BYTE)*m_nImgWidth*m_nImgHeight,cudaMemcpyDeviceToHost,m_pStream[i])); } Please let me know about why the profiler doesn't show overlapping of kernel execution and data transfer.",
        "answers": [
            [
                "You need to invoke cudaMemcpyAsync() and kernel launches in the right order. Before compute capability 3.5 there was only a single queue for invoking device-side operations, and they do not get reordered. Combine the \"Call Function\" and \"OutputData\" phases to something like //Call Function and OutputData for(int i=0;i&lt;m_n3DChannelCnt ;++i) {KernelGetVis8uObjPhsPhs&lt;&lt;&lt;nBlockCnt,nThreadCnt,0,m_pStream[i]&gt;&gt;&gt;(d_ppbVis[i],d_ppbAvg[i],d_ppfPhs[i],d_ppfObj[i],d_ppbAmp[i] ,nTotalSize,d_ppstRefData[i],d_ppbImg[i*m_nBucket],d_ppbImg[i*m_nBucket+1],d_ppbImg[i*m_nBucket+2],d_ppbImg[i*m_nBucket+3] ,fSclFloatVis2ByteVis); if(ppbVis &amp;&amp; ppbVis[i]) cudaErrorChk_Return(cudaMemcpyAsync(ppbVis[i],d_ppbVis[i],sizeof(BYTE)*m_nImgWidth*m_nImgHeight,cudaMemcpyDeviceToHost,m_pStream[i])); if(ppbAvg &amp;&amp; ppbAvg[i]) cudaErrorChk_Return(cudaMemcpyAsync(ppbAvg[i],d_ppbAvg[i],sizeof(BYTE)*m_nImgWidth*m_nImgHeight,cudaMemcpyDeviceToHost,m_pStream[i])); if(ppfPhs &amp;&amp; ppfPhs[i]) cudaErrorChk_Return(cudaMemcpyAsync(ppfPhs[i],d_ppfPhs[i],sizeof(float)*m_nImgWidth*m_nImgHeight,cudaMemcpyDeviceToHost,m_pStream[i])); if(ppfObj &amp;&amp; ppfObj[i]) cudaErrorChk_Return(cudaMemcpyAsync(ppfObj[i],d_ppfObj[i],sizeof(float)*m_nImgWidth*m_nImgHeight,cudaMemcpyDeviceToHost,m_pStream[i])); if(ppbAmp &amp;&amp; ppbAmp[i]) cudaErrorChk_Return(cudaMemcpyAsync(ppbAmp[i],d_ppbAmp[i],sizeof(BYTE)*m_nImgWidth*m_nImgHeight,cudaMemcpyDeviceToHost,m_pStream[i])); } You will only be able to overlap kernel launches with the first or last of the memory copies though, as you have five cudaMemcpyAsync() calls within the stream which again don't get reordered. Allocate all five arrays contiguously in memory so that you can transfer them with a single cudaMemcpyAsync(). Overall I notice however that the data transfers takes much longer than the kernels run, so overlapping compute and copy will provide only a minor speedup in your case."
            ],
            [
                "You might want to check if your code works as expected (i.e., with overlapping) in LINUX. I've just incurred the same problem and found that the WINDOWS might have some problems (either in NVIDIA's driver or Windows itself), which interfere the overlapping in CUDA streaming. You can try and check if the \"simpleStreams\" example in SDK works with overlapping in your machine. For my case, the \"simpleStream\" running on Windows does not have overlap at all, but it runs perfectly in Linux. To be specific, I am using CUDA 5.0 + VS2010, on a Fermi GTX570."
            ],
            [
                "TL;DR: The issue is caused by the WDDM TDR delay option in Nsight Monitor! When set to false, the issue appears. Instead, if you set the TDR delay value to a very high number, and the \"enabled\" option to true, the issue goes away. Read below for other (older) steps followed until i came to the solution above, and some other possible causes. I just recently were able to mostly solve this problem! It is specific to windows and aero i think. Please try these steps and post your results to help others! I have tried it on GTX 650 and GT 640. Before you do anything, consider using both onboard gpu(as display) and the discrete gpu (for computations), because there are verified issues with the nvidia driver for windows! When you use onboard gpu, said drivers don't get fully loaded, so many bugs are evaded. Also, system responsiveness is maintained while working! Make sure your concurrency problem is not related to other issues like old drivers (incl. bios version), wrong code, incapable device, etc. Go to computer&gt;properties Select advanced system settings on the left side Go to the Advanced tab On Performance click settings In the Visual Effects tab, select the \"adjust for best performance\" bullet. This will disable aero and almost all visual effects. If this configuration works, you can try enabling one-by-one the boxes for visual effects until you find the precise one that causes problems! Alternatively, you can: Right click on desktop, select personalize Select a theme from basic themes, that doesn't have aero. This will also work as the above, but with more visual options enabled. For my two devices, this setting also works, so i kept it. Please, when you try these solutions, come back here and post your findings! For me, it solved the problem for most cases (a tiled dgemm i have made),but NOTE THAT i still can't run \"simpleStreams\" properly and achieve concurrency... UPDATE: The problem is fully solved with a new windows installation!! The previous steps improved the behavior for some cases, but a fresh install solved all the problems! I will try to find a less radical way of solving this problem, maybe restoring just the registry will be enough."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07,
            -1.9999999
        ]
    },
    {
        "question": "I'm working on CUDA and I have a doubt about global memory and streams CUDA. Let: __device__ float Aux[32]; __global__ void kernel1(...) { [...] Aux[threadIdx.y] = 0; [...] } So, if I run this kernel on different streams GPU. Is Aux the same for all streams? or there is a Aux variable for all streams being Aux global variable? I can't find that information in the guide cuda. Thanks in advance.",
        "answers": [
            [
                "It's the same for all streams. Streams control the (partial) order in which kernels are executed. They do not create new namespaces."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Hi a few questions regarding Cuda stream processing for multiple kernels. Assume s streams and a kernels in a 3.5 capable kepler device, where s &lt;= 32. kernel uses a dev_input array of size n and a dev output array of size s*n. kernel reads data from input array, stores its value in a register, manipulates it and writes its result back to dev_output at the position s*n + tid. We aim to run the same kernel s times using one of the n streams each time. Similar to the simpleHyperQ example. Can you comment if and how any of the following affects concurrency please? dev_input and dev_output are not pinned; dev_input as it is vs dev_input size s*n, where each kernel reads unique data (no read conflicts) kernels read data from constant memory 10kb of shared memory are allocated per block. kernel uses 60 registers Any good comments will be appreciated...!!! cheers, Thanasio Robert, thanks a lot for your detailed answer. It has been very helpful. I edited 4, it is 10kb per block. So in my situation, i launch grids of 61 blocks and 256 threads. The kernels are rather computationally bound. I launch 8 streams of the same kernel. Profile them and then i see a very good overlap between the first two and then it gets worse and worse. The kernel execution time is around 6ms. After the first two streams execute almost perfectly concurrent the rest have a 3ms distance between them. Regarding 5, i use a K20 which has a 255 register file. So i would not expect drawbacks from there. I really cannot understand why i do not achieve concurrency equivalent to what is specified for gk110s.. Please take a look at the following link. There is an image called kF.png .It shows the profiler output for the streams..!!! https://devtalk.nvidia.com/default/topic/531740/cuda-programming-and-performance/concurrent-streams-and-hyperq-for-k20/",
        "answers": [
            [
                "Concurrency amongst kernels depends upon a number of factors, but one that many people overlook is simply the size of the kernel (i.e. number of blocks in the grid.) Kernels that are of a size that can effectively utilize the GPU by themselves will not generally run concurrently to a large degree, and there would be little throughput advantage even if they did. The work distributor inside the GPU will generally begin distributing blocks as soon as a kernel is launched, so if one kernel is launched before another, and both have a large number of blocks, then the first kernel will generally occupy the GPU until it is nearly complete, at which point blocks of the second kernel will then get scheduled and executed, perhaps with a small amount of \"concurrent overlap\". The main point is that kernels that have enough blocks to \"fill up the GPU\" will prevent other kernels from actually executing, and apart from scheduling, this isn't any different on a compute 3.5 device. In addition, rather than just specifying a few parameters for the kernel as a whole, also specifying launch parameters and statistics (such as register usage, shared mem usage, etc.) at the block level are helpful for providing crisp answers. The benefits of the compute 3.5 architecture in this area will still mainly come from \"small\" kernels of \"few\" blocks, attempting to execute together. Compute 3.5 has some advantages there. You should also review the answer to this question. When global memory used by the kernel is not pinned, it affects the speed of data transfer, and also the ability to overlap copy and compute but does not affect the ability of two kernels to execute concurrently. Nevertheless, the limitation on copy and compute overlap may skew the behavior of your application. There shouldn't be \"read conflicts\", I'm not sure what you mean by that. Two independent threads/blocks/grids are allowed to read the same location in global memory. Generally this will get sorted out at the L2 cache level. As long as we are talking about just reads there should be no conflict, and no particular effect on concurrency. Constant memory is a limited resource, shared amongst all kernels executing on the device (try running deviceQuery). If you have not exceeded the total device limit, then the only issue will be one of utilization of the constant cache, and things like cache thrashing. Apart from this secondary relationship, there is no direct effect on concurrency. It would be more instructive to identify the amount of shared memory per block rather than per kernel. This will directly affect how many blocks can be scheduled on a SM. But answering this question would be much crisper also if you specified the launch configuration of each kernel, as well as the relative timing of the launch invocations. If shared memory happened to be the limiting factor in scheduling, then you can divide the total available shared memory per SM by the amount used by each kernel, to get an idea of the possible concurrency based on this. My own opinion is that number of blocks in each grid is likely to be a bigger issue, unless you have kernels that use 10k per grid but only have a few blocks in the whole grid. My comments here would be nearly the same as my response to 4. Take a look at deviceQuery for your device, and if registers became a limiting factor in scheduling blocks on each SM, then you could divide available registers per SM by the register usage per kernel (again, it makes a lot more sense to talk about register usage per block and the number of blocks in the kernel) to discover what the limit might be. Again, if you have reasonable sized kernels (hundreds or thousands of blocks, or more) then the scheduling of blocks by the work distributor is most likely going to be the dominant factor in the amount of concurrency between kernels. EDIT: in response to new information posted in the question. I've looked at the kF.png First let's analyze from a blocks per SM perspective. CC 3.5 allows 16 \"open\" or currently scheduled blocks per SM. If you are launching 2 kernels of 61 blocks each, that may well be enough to fill the \"ready-to-go\" queue on the CC 3.5 device. Stated another way, the GPU can handle 2 of these kernels at a time. As the blocks of one of those kernels \"drains\" then another kernel is scheduled by the work distributor. The blocks of the first kernel \"drain\" sufficiently in about half the total time, so that the next kernel gets scheduled about halfway through the completion of the first 2 kernels, so at any given point (draw a vertical line on the timeline) you have either 2 or 3 kernels executing simultaneously. (The 3rd kernel launched overlaps the first 2 by about 50% according to the graph, I don't agree with your statement that there is a 3ms distance between each successive kernel launch). If we say that at peak we have 3 kernels scheduled (there are plenty of vertical lines that will intersect 3 kernel timelines) and each kernel has ~60 blocks, then that is about 180 blocks. Your K20 has 13 SMs and each SM can have at most 16 blocks scheduled on it. This means at peak you have about 180 blocks scheduled (perhaps) vs. a theoretical peak of 16*13 = 208. So you're pretty close to max here, and there's not much more that you could possibly get. But maybe you think you're only getting 120/208, I don't know. Now let's take a look from a shared memory perspective. A key question is what is the setting of your L1/shared split? I believe it defaults to 48KB of shared memory per SM, but if you've changed this setting that will be pretty important. Regardless, according to your statement each block scheduled will use 10KB of shared memory. This means we would max out around 4 blocks scheduled per SM, or 4*13 total blocks = 52 blocks max that can be scheduled at any given time. You're clearly exceeding this number, so probably I don't have enough information about the shared memory usage by your kernels. If you're really using 10kb/block, this would more or less preclude you from having more than one kernel's worth of threadblocks executing at a time. There could still be some overlap, and I believe this is likely to be the actual limiting factor in your application. The first kernel of 60 blocks gets scheduled. After a few blocks drain (or perhaps because the 2 kernels were launched close enough together) the second kernel begins to get scheduled, so nearly simultaneously. Then we have to wait a while for about a kernel's worth of blocks to drain before the 3rd kernel can get scheduled, this may well be at the 50% point as indicated in the timeline. Anyway I think the analyses 1 and 2 above clearly suggest you're getting most of the capability out of the device, based on the limitations inherent in your kernel structure. (We could do a similar analysis based on registers to discover if that is a significant limiting factor.) Regarding this statement: \"I really cannot understand why i do not achieve concurrency equivalent to what is specified for gk110s..\" I hope you see that the concurrency spec (e.g. 32 kernels) is a maximum spec, and in most cases you are going to run into some other kind of machine limit before you hit the limit on the maximum number of kernels that can execute simultaneously. EDIT: regarding documentation and resources, the answer I linked to above from Greg Smith provides some resource links. Here are a few more: The C programming guide has a section on Asynchronous Concurrent Execution. GPU Concurrency and Streams presentation by Dr. Steve Rennich at NVIDIA is on the NVIDIA webinar page"
            ],
            [
                "My experience with HyperQ so far is 2-3 (3.5) times parallellization of my kernels, as the kernels usually are larger for a little more complex calculations. With small kernels its a different story, but usually the kernels are more complicated. This is also answered by Nvidia in their cuda 5.0 documentation that more complex kernels will take down the amount of parallellization. But still, GK110 has a great advantage just allowing this."
            ]
        ],
        "votes": [
            6.0000001,
            1e-07
        ]
    },
    {
        "question": "The way I see both Process One &amp; Process Two (below), are equivalent in that they take the same amount of time. Am I wrong? allOfData_A= data_A1 + data_A2 allOfData_B= data_B1 + data_B2 allOFData_C= data_C1 + data_C2 Data_C is the output of the kernel operation of both Data_A &amp; Data_B. (Like C=A+B) The HW supports one DeviceOverlap (concurrent) operation. Process One: MemcpyAsync data_A1 stream1 H-&gt;D MemcpyAsync data_A2 stream2 H-&gt;D MemcpyAsync data_B1 stream1 H-&gt;D MemcpyAsync data_B2 stream2 H-&gt;D sameKernel stream1 sameKernel stream2 MemcpyAsync result_C1 stream1 D-&gt;H MemcpyAsync result_C2 stream2 D-&gt;H Process Two: (Same operation, different order) MemcpyAsync data_A1 stream1 H-&gt;D MemcpyAsync data_B1 stream1 H-&gt;D sameKernel stream1 MemcpyAsync data_A2 stream2 H-&gt;D MemcpyAsync data_B2 stream2 H-&gt;D sameKernel stream2 MemcpyAsync result_C1 stream1 D-&gt;H MemcpyAsync result_C2 stream2 D-&gt;H",
        "answers": [
            [
                "Using CUDA streams allows the programmer to express work dependencies by putting dependent operations in the same stream. Work in different streams is independent and can be executed concurrently. On GPUs without HyperQ (compute capability 1.0 to 3.0) you can get false dependencies because the work for a DMA engine or for computation gets put into a single hardware pipe. Compute capability 3.5 brings HyperQ which allows for multiple hardware pipes and there you shouldn't get the false dependencies. The simpleHyperQ example illustrates this, and the documentation shows diagrams to explain what is going on more clearly. Putting it simply, on devices without HyperQ you would need to do a breadth-first launch of your work to get maximum concurrency, whereas for devices with HyperQ you can do a depth-first launch. Avoiding the false dependencies is pretty easy, but not having to worry about it is easier!"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "From K20 different streams becomes fully concurrent(used to be concurrent on the edge). However My program need the old way. Or I need to do a lot of synchronization to solve the dependency problem. Is it possible to switch stream management to the old way?",
        "answers": [
            [
                "CUDA C Programming Guide section on Asynchronous Current Execution A stream is a sequence of commands (possibly issued by different host threads) that execute in order. Different streams, on the other hand, may execute their commands out of order with respect to one another or concurrently; this behavior is not guaranteed and should therefore not be relied upon for correctness (e.g., inter-kernel communication is undefined). If the application relied on Compute Capability 2.* and 3.0 implementation of streams then the program violates the definition of streams and any change to the CUDA driver (e.g. queuing of per stream requests) or new hardware will break the program. If you need a temporary workaround then I would suggest moving all work to a single user defined stream. This may impact performance but it is likely the only temporary workaround."
            ],
            [
                "Can you express the kernel dependencies with cudaEvent_t objects? The Streams and Concurrency Webinar shows some quick code snippets on how to use events. Some of the details of that presentation are only applicable to pre-Kepler hardware, but I'm assuming from the original question that you're familiar with how things have changed since Fermi now that there are multiple command queues."
            ]
        ],
        "votes": [
            6.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am trying to understand CUDA streams and I have made my first program with streams, but It is slower than usual kernel function... why is this code slower cudaMemcpyAsync(pole_dev, pole, size, cudaMemcpyHostToDevice, stream_1); addKernel&lt;&lt;&lt;count/100, 100, 0, stream_1&gt;&gt;&gt;(pole_dev); cudaMemcpyAsync(pole, pole_dev, size, cudaMemcpyDeviceToHost, stream_1); cudaThreadSynchronize(); // I don't know difference between cudaThreadSync and cudaDeviceSync cudaDeviceSynchronize(); // it acts relatively same... than: cudaMemcpy(pole_dev, pole, size, cudaMemcpyHostToDevice); addKernel&lt;&lt;&lt;count/100, 100&gt;&gt;&gt;(pole_dev); cudaMemcpy(pole, pole_dev, size, cudaMemcpyDeviceToHost); I thounght that it should run faster ... value of variable count is 6 500 000 (maximum) ... first source code takes 14 millisecconds and second source code takes 11 milliseconds. Can anybody explain it to me, please?",
        "answers": [
            [
                "In this snippet you like dealing with only a single stream (stream_1), but that's actually what CUDA automatically does for you when you don't explicitely manipulate streams. To take advantage of streams and asynchronous memory transfers, you need to use several streams, and split your data and computations through each of them."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "When running concurrent copy &amp; kernel operations: If I have a kernel runTime that is twice as long as a dataCopy operation, will I get 2 copies per kernel run? The stream examples I'm seeing show a 1:1 relationship. (Time of copy = time of kernel run.) I'm wondering what happens when there is something different. Is there always one copy operation (max) for every kernel launch? Or does the copy operation run independent of the kernel launch? i.e. I could possibly complete 5 copy operations for every kernel launch, if the run &amp; copy time work out that way. (I'm trying to figure out how many copy operations to queue up before a kernel launch.) One to one: (time to copy = kernel run time) &lt;--stream1Copy--&gt;&lt;--stream2Copy--&gt; ..............................&lt;-stream1Kernel-&gt; Two to one: (time to copy = 1/2 kernel run time) &lt;-stream1Copy-&gt;&lt;-stream2Copy-&gt;&lt;-stream3Copy-&gt; ............................&lt;----------stream1Kernel------------&gt;",
        "answers": [
            [
                "You can have more than one copy per kernel launch. Only one copy (per direction on devices with dual copy engines) can be running at a particular time to a particular GPU, but once that one is complete, another can be started immediately. Asynchronous copies issued in streams other than the kernel launch stream in question will run completely asynchronously to that kernel launch, assuming niether stream is stream 0. (This also assumes you are using pinned memory i.e. cudaHostAlloc to create the relevant host-side buffers.) You may want to read the relevant section in the best practices guide. The reason you frequently see a 1:1 analysis of compute and copy is that it is assumed the copied data will be consumed by (or is produced by) the kernel call, and so logically we can think of the block of data this way. But if it's easier to structure your code as a sequence of copies, there should be no problem with that. Naturally if you can batch up all your data into a single cudaMemcpy call, that will be slightly more efficient that a sequence of copies that are transferring the same data. The visual profiler will help you see exactly what is going on comparing data copy operations to kernel operations, in a timeline fashion."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Can somebody explain the data independency requirement in concurrent Cuda streams? Assume i want to run the following kernel in 8 concurrent streams Kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(float *readOnlyInput, float *output); can all streams read the same *readOnlyInput and write on different *output arrays? Or in order to achieve concurrency they need to read data from different memory locations as well? will the above pseudocode snippet be executed concurrently, or it needs *readOnlyInput+i*size to ensure concurrency? cudaStream_t stream[8]; int size = 1000;//some array size int blocks =2, threads=256;//some grid dims for (int i = 0; i &lt; 8; ++i){ cudaStreamCreate(&amp;stream[i]); } for (int i = 0; i &lt; 8; ++i){ Kernel&lt;&lt;&lt;blocks, threads, stream[i]&gt;&gt;&gt;(float *readOnlyInput, float *output + i*size); }",
        "answers": [
            [
                "You can safely read the same data from multiple independent kernels in different streams, as long as there is sufficient synchronization in place to make sure the data is written entirely before any of the kernels starts, and the data is not being overwritten again before all of the kernels have finished."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "For my CUDA development, I am using a machine with 16 cores, and 1 GTX 580 GPU with 16 SMs. For the work that I am doing, I plan to launch 16 host threads (1 on each core), and 1 kernel launch per thread, each with 1 block and 1024 threads. My goal is to run 16 kernels in parallel on 16 SMs. Is this possible/feasible? I have tried to read as much as possible about independent contexts, but there does not seem to be too much information available. As I understand it, each host thread can have its own GPU context. But, I am not sure whether the kernels will run in parallel if I use independent contexts. I can read all the data from all 16 host threads into one giant structure and pass it to GPU to launch one kernel. However, it will be too much copying and it will slow down the application.",
        "answers": [
            [
                "You can only have one context on a GPU at a time. One way to achieve the sort of parallelism you require would be to use CUDA streams. You can create 16 streams inside the context, and launch memcopies and kernels into streams by name. You can read more in a quick webinar on using streams at : http://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf. The full API reference is in the CUDA toolkit manuals. The CUDA 4.2 manual is available at http://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/CUDA_Toolkit_Reference_Manual.pdf."
            ],
            [
                "While a multi-threaded application can hold multiple CUDA contexts simultaneously on the same GPU, those contexts cannot perform operations concurrently. When active, each context has sole use of the GPU, and must yield before another context (which could include operations with a rendering API or a display manager) can have access to the GPU. So in a word, no this strategy can't work with any current CUDA versions or hardware."
            ]
        ],
        "votes": [
            6.0000001,
            4.0000001
        ]
    },
    {
        "question": "I expected that GTX 680 (which is one of the latest version of GPUs) is capable of concurrent data transfer (concurrent data transfer in both direction). But when I run cuda SDK \"Device Query\", the test result of the term \"Concurrent copy and execution\" is \"Yes with 1 copy engine\", and it means that the GPU can not do concurrent data transfer. I wonder if my testing result also happens to you? And can you share with me which devices are capable of concurrent data transfer? Thanks!",
        "answers": [
            [
                "Dual copy engines are available on Tesla cards and modules: http://www.nvidia.com/object/why-choose-tesla.html http://www.nvidia.com/docs/IO/43395/NV-DS-Tesla-C2075.pdf Also, some Quadro models provide dual copy engines, e.g.: http://www.nvidia.com/docs/IO/40049/Dual_copy_engines.pdf http://www.nvidia.com/object/quadro-k5000.html"
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I implemented a RNS Montgomery exponentiation in Cuda. Everything nice everything fine. It runs on just one SM. BUT, so far I focus on parallelization of just a single exp. What I want to do now is test with several exp on fly. That is, I want that the i-th next exp is assign to a free SM. I tried, and the final time was always growing linearly, that is all the exp were assign to the same SM. Then I switched to streams, but nothing changed. However I have never used them, so maybe I am doing something wrong.. This is the code: void __smeWrapper() { cudaEvent_t start, stop; cudaStream_t stream0, stream1, stream2; float time; unsigned int j, i, tmp; cudaEventCreate(&amp;start); cudaEventCreate(&amp;stop); dim3 threadsPerBlock(SET_SIZE, (SET_SIZE+1)/2); setCudaDevice(); s_transferDataToGPU(); if(cudaDeviceSetCacheConfig(cudaFuncCachePreferL1) != cudaSuccess) printf(\"cudaDeviceSetCacheConfig ERROR!\"); cudaEventRecord( start, 0 ); //for(i=0; i&lt;EXPONENTIATION_NUMBER; i++) { i=0; __me&lt;&lt;&lt; 1, threadsPerBlock, 0, stream0 &gt;&gt;&gt;(&amp;__s_x[i*(2*SET_SIZE + 1)], __B2modN, __bases, __mmi_NinB, __mmi_Bimodbi, __Bi_inAUar, __dbg, __NinAUar, __mmi_BinAUar, __mmi_Ajmodaj, __Ajmodar, __mmi_Armodar, __AjinB, __minusAinB, &amp;__z[i*(2*SET_SIZE + 1)], __e); i=1; __me&lt;&lt;&lt; 1, threadsPerBlock, 0, stream1 &gt;&gt;&gt;(&amp;__s_x[i*(2*SET_SIZE + 1)], __B2modN, __bases, __mmi_NinB, __mmi_Bimodbi, __Bi_inAUar, __dbg, __NinAUar, __mmi_BinAUar, __mmi_Ajmodaj, __Ajmodar, __mmi_Armodar, __AjinB, __minusAinB, &amp;__z[i*(2*SET_SIZE + 1)], __e); i=2; __me&lt;&lt;&lt; 1, threadsPerBlock, 0, stream2 &gt;&gt;&gt;(&amp;__s_x[i*(2*SET_SIZE + 1)], __B2modN, __bases, __mmi_NinB, __mmi_Bimodbi, __Bi_inAUar, __dbg, __NinAUar, __mmi_BinAUar, __mmi_Ajmodaj, __Ajmodar, __mmi_Armodar, __AjinB, __minusAinB, &amp;__z[i*(2*SET_SIZE + 1)], __e); //printf(\"\\n%s\\n\\n\", cudaGetErrorString(cudaGetLastError())); //} cudaEventRecord( stop, 0 ); cudaEventSynchronize( stop ); cudaEventElapsedTime( &amp;time, start, stop ); printf(\"GPU %f \u00b5s : %f ms\\n\", time*1000, time); cudaEventDestroy( start ); cudaEventDestroy( stop ); Ubuntu 11.04 64b, Cuda 5 RC, 560 Ti (8 SM)",
        "answers": [
            [
                "All threads from a block always run on a same SM. You need to start more then one block to use other SMs. There seems to be something wrong with your streams - do you call cudaStreamCreate for every stream? On my system it crashes with SEGFAULT if I don't use one though."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "This question is related to using cuda streams to run many kernels In CUDA there are many synchronization commands cudaStreamSynchronize, CudaDeviceSynchronize, cudaThreadSynchronize, and also cudaStreamQuery to check if streams are empty. I noticed when using the profiler that these synchronize commands introduce a large delay to the program. I was wondering if anyone knows any means to reduce this latency apart from of course using as few synchronisation commands as possible. Also is there any figures to judge the most effecient synchronisation method. that is consider 3 streams used in an application and two of them need to complete for me to launch a forth streams should i use 2 cudaStreamSyncs or just one cudaDeviceSync what will incur less loss ?",
        "answers": [
            [
                "The main difference between synchronize methods is \"polling\" and \"blocking.\" \"Polling\" is the default mechanism for the driver to wait for the GPU - it waits for a 32-bit memory location to attain a certain value written by the GPU. It may return the wait more quickly after the wait is resolved, but while waiting, it burns a CPU core looking at that memory location. \"Blocking\" can be requested by calling cudaSetDeviceFlags() with cudaDeviceScheduleBlockingSync, or calling cudaEventCreate() with cudaEventBlockingSync. Blocking waits cause the driver to insert a command into the DMA command buffer that signals an interrupt when all preceding commands in the buffer have been executed. The driver can then map the interrupt to a Windows event or a Linux file handle, enabling the synchronization commands to wait without constantly burning CPU, as do the default polling methods. The queries are basically a manual check of that 32-bit memory location used for polling waits; so in most situations, they are very cheap. But if ECC is enabled, the query will dive into kernel mode to check if there are any ECC errors; and on Windows, any pending commands will be flushed to the driver (which requires a kernel thunk)."
            ]
        ],
        "votes": [
            12.0000001
        ]
    },
    {
        "question": "I have something very similar to the code: int k, no_streams = 4; cudaStream_t stream[no_streams]; for(k = 0; k &lt; no_streams; k++) cudaStreamCreate(&amp;stream[k]); cudaMalloc(&amp;g_in, size1*no_streams); cudaMalloc(&amp;g_out, size2*no_streams); for (k = 0; k &lt; no_streams; k++) cudaMemcpyAsync(g_in+k*size1/sizeof(float), h_ptr_in[k], size1, cudaMemcpyHostToDevice, stream[k]); for (k = 0; k &lt; no_streams; k++) mykernel&lt;&lt;&lt;dimGrid, dimBlock, 0, stream[k]&gt;&gt;&gt;(g_in+k*size1/sizeof(float), g_out+k*size2/sizeof(float)); for (k = 0; k &lt; no_streams; k++) cudaMemcpyAsync(h_ptr_out[k], g_out+k*size2/sizeof(float), size2, cudaMemcpyDeviceToHost, stream[k]); cudaThreadSynchronize(); cudaFree(g_in); cudaFree(g_out); 'h_ptr_in' and 'h_ptr_out' are arrays of pointers allocated with cudaMallocHost (with no flags). The problem is that the streams do not overlap. In the visual profiler I can see the kernel execution from the first stream overlapping with the copy (H2D) from the second stream but nothing else overlaps. I may not have resources to run 2 kernels (I think I do) but at least the kernel execution and copy should be overlaping, right? And if I put all 3 (copy H2D, kernel execution, copy D2H) within the same for-loop none of them overlap... Please HELP, what can be causing this? I'm running on: Ubuntu 10.04 x64 Device: \"GeForce GTX 460\" (CUDA Driver Version: 3.20, CUDA Runtime Version: 3.20, CUDA Capability Major/Minor version number: 2.1, Concurrent copy and execution: Yes, Concurrent kernel execution: Yes)",
        "answers": [
            [
                "According to this post on the NVIDIA forums, the profiler will serialize streaming to get accurate timing data. If you think your timings are off, make sure you're using CUDA events... I've been experimenting with streaming lately, and I found the \"simpleMultiCopy\" example from the SDK to be really helpful, particularly with the appropriate logic and synchronizations."
            ],
            [
                "If you want to see the kernels overlap with kernels (concurrent kernels) you need to make use of CUDA Visual profiler 5.0 that comes with CUDA 5.0 Toolkit. I don't think previous profilers are capable of this. It should also show kernel and memcpy overlap."
            ]
        ],
        "votes": [
            7.0000001,
            3.0000001
        ]
    },
    {
        "question": "I wrote a code which uses many host (OpenMP) threads per one GPU. Each thread has its own CUDA stream to order it requests. It looks very similar to below code: #pragma omp parallel for num_threads(STREAM_NUMBER) for (int sid = 0; sid &lt; STREAM_NUMBER; sid++) { cudaStream_t stream; cudaStreamCreate(&amp;stream); while (hasJob()) { //... code to prepare job - dData, hData, dataSize etc cudaError_t streamStatus = cudaStreamQuery(stream); if (streamStatus == cudaSuccess) { cudaMemcpyAsync(dData, hData, dataSize, cudaMemcpyHostToDevice, stream); doTheJob&lt;&lt;&lt;gridDim, blockDim, smSize, stream&gt;&gt;&gt;(dData, dataSize); else { CUDA_CHECK(streamStatus); } cudaStreamSynchronize(stream); } cudaStreamDestroy(stream); } And everything were good till I got many small jobs. In that case, from time to time, cudaStreamQuery returns cudaErrorNotReady, which is for me unexpected because I use cudaStreamSynchronize. Till now I were thinking that cudaStreamQuery will always return cudaSuccess if it is called after cudaStreamSynchronize. Unfortunately it appeared that cudaStreamSynchronize may finish even when cudaStreamQuery still returns cudaErrorNotReady. I changed the code into the following and everything works correctly. #pragma omp parallel for num_threads(STREAM_NUMBER) for (int sid = 0; sid &lt; STREAM_NUMBER; sid++) { cudaStream_t stream; cudaStreamCreate(&amp;stream); while (hasJob()) { //... code to prepare job - dData, hData, dataSize etc cudaError_t streamStatus; while ((streamStatus = cudaStreamQuery(stream)) == cudaErrorNotReady) { cudaStreamSynchronize(); } if (streamStatus == cudaSuccess) { cudaMemcpyAsync(dData, hData, dataSize, cudaMemcpyHostToDevice, stream); doTheJob&lt;&lt;&lt;gridDim, blockDim, smSize, stream&gt;&gt;&gt;(dData, dataSize); else { CUDA_CHECK(streamStatus); } cudaStreamSynchronize(stream); } cudaStreamDestroy(stream); } So my question is.... is it a bug or a feature? EDIT: it is similar to JAVA synchronize { while(waitCondition) { wait(); } }",
        "answers": [
            [
                "What is under //... code to prepare job - dData, hData, dataSize etc Do you have any functions of kind cudaMemcpyAsync there, or the only memory transfer is in the code you have shown? Those are asynchronous functions may exit early, even when the code is not at the destination yet. When that happens cudaStreamQuery will return cudaSuccess only when memory transfers succeed. Also, does hasJob() uses any of the host-CUDA functions? If I am not mistaken, in a single stream, it is not possible to execute both kernel and memory transfers. Therefore, calling cudaStreamQuery is necessary only when a kernel depends on the data transferred by a different stream."
            ],
            [
                "Didn't notice it earlier: cudaStreamSynchronize() should take a parameter (stream). I am not sure which stream you are synchronising when parameter is ommited, could be that it defaults to stream 0."
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "I looking for a way how to get rid of busy waiting in host thread in fallowing code (do not copy that code, it only shows an idea of my problem, it has many basic bugs): cudaStream_t steams[S_N]; for (int i = 0; i &lt; S_N; i++) { cudaStreamCreate(streams[i]); } int sid = 0; for (int d = 0; d &lt; DATA_SIZE; d+=DATA_STEP) { while (true) { if (cudaStreamQuery(streams[sid])) == cudaSuccess) { //BUSY WAITING !!!! cudaMemcpyAssync(d_data, h_data + d, DATA_STEP, cudaMemcpyHostToDevice, streams[sid]); kernel&lt;&lt;&lt;gridDim, blockDim, smSize streams[sid]&gt;&gt;&gt;(d_data, DATA_STEP); break; } sid = ++sid % S_N; } } Is there a way to idle host thread and wait somehow to some stream to finish, and then prepare and run another stream? EDIT: I added while(true) into the code, to emphasize busy waiting. Now I execute all the streams, and check which of them finished to run another new one. cudaStreamSynchronize waits for particular stream to finish, but I want to wait for any of the streams which as a first finished the job. EDIT2: I got rid of busy-waiting in fallowing way: cudaStream_t steams[S_N]; for (int i = 0; i &lt; S_N; i++) { cudaStreamCreate(streams[i]); } int sid = 0; for (int d = 0; d &lt; DATA_SIZE; d+=DATA_STEP) { cudaMemcpyAssync(d_data, h_data + d, DATA_STEP, cudaMemcpyHostToDevice, streams[sid]); kernel&lt;&lt;&lt;gridDim, blockDim, smSize streams[sid]&gt;&gt;&gt;(d_data, DATA_STEP); sid = ++sid % S_N; } for (int i = 0; i &lt; S_N; i++) { cudaStreamSynchronize(streams[i]); cudaStreamDestroy(streams[i]); } But it appears to be a little bit slower than the version with busy-waiting on host thread. I think it is because, now I statically distribute the jobs on streams, so when the one stream finishes work it is idle till each of the stream finishes the work. The previous version dynamically distributed the work to the first idle stream, so it was more efficient, but there was busy-waiting on the host thread.",
        "answers": [
            [
                "The real answer is to use cudaThreadSynchronize to wait for all previous launches to complete, cudaStreamSynchronize to wait for all launches in a certain stream to complete, and cudaEventSynchronize to wait for only a certain event on a certain stream to be recorded. However, you need to understand how streams and sychronization work before you will be able to use them in your code. What happens if you do not use streams at all? Consider the following code: kernel &lt;&lt;&lt; gridDim, blockDim &gt;&gt;&gt; (d_data, DATA_STEP); host_func1(); cudaThreadSynchronize(); host_func2(); The kernel is launched and the host moves on to execute host_func1 and kernel concurrently. Then, the host and the device are synchronized, ie the host waits for kernel to finish before moving on to host_func2(). Now, what if you have two different kernels? kernel1 &lt;&lt;&lt;gridDim, blockDim &gt;&gt;&gt; (d_data + d1, DATA_STEP); kernel2 &lt;&lt;&lt;gridDim, blockDim &gt;&gt;&gt; (d_data + d2, DATA_STEP); kernel1 is launched asychronously! the host moves on, and kernel2 is launched before kernel1 finishes! however, kernel2 will not execute until after kernel1 finishes, because they have both been launched on stream 0 (the default stream). Consider the following alternative: kernel1 &lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt; (d_data + d1, DATA_STEP); cudaThreadSynchronize(); kernel2 &lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt; (d_data + d2, DATA_STEP); There is absolutely no need to do this because the device already synchronizes kernels launched on the same stream. So, I think that the functionality that you are looking for already exists... because a kernel always waits for previous launches in the same stream to finish before starting (even though the host passes by). That is, if you want to wait for any previous launch to finish, then simply don't use streams. This code will work fine: for (int d = 0; d &lt; DATA_SIZE; d+=DATA_STEP) { cudaMemcpyAsync(d_data, h_data + d, DATA_STEP, cudaMemcpyHostToDevice, 0); kernel&lt;&lt;&lt;gridDim, blockDim, smSize, 0&gt;&gt;&gt;(d_data, DATA_STEP); } Now, on to streams. you can use streams to manage concurrent device execution. Think of a stream as a queue. You can put different memcpy calls and kernel launches into different queues. Then, kernels in stream 1 and launches in stream 2 are asynchronous! They may be executed at the same time, or in any order. If you want to be sure that only one memcpy/kernel is being executed on the device at a time, then don't use streams. Similarly, if you want kernels to be executed in a specific order, then don't use streams. That said, keep in mind that anything put into a stream 1, is executed in order, so don't bother synchronizing. Synchronization is for synchronizing host and device calls, not two different device calls. So, if you want to execute several of your kernels at the same time because they use different device memory and have no effect on each other, then use streams. Something like... cudaStream_t steams[S_N]; for (int i = 0; i &lt; S_N; i++) { cudaStreamCreate(streams[i]); } int sid = 0; for (int d = 0; d &lt; DATA_SIZE; d+=DATA_STEP) { cudaMemcpyAsync(d_data, h_data + d, DATA_STEP, cudaMemcpyHostToDevice, streams[sid]); kernel&lt;&lt;&lt;gridDim, blockDim, smSize streams[sid]&gt;&gt;&gt;(d_data, DATA_STEP); sid = ++sid % S_N; } No explicit device synchronization necessary."
            ],
            [
                "My idea to solve that problem is to have one host thread per one stream. That host thread would invoke cudaStreamSynchronize to wait till the stream commands are completed. Unfortunately it is not possible in CUDA 3.2 since it allows only one host thread deal with one CUDA context, it means one host thread per one CUDA enabled GPU. Hopefully, in CUDA 4.0 it will be possible: CUDA 4.0 RC news EDIT: I have tested in CUDA 4.0 RC, using open mp. I created one host thread per cuda stream. And it started to work."
            ],
            [
                "There is: cudaEventRecord(event, stream) and cudaEventSynchronize(event). The reference manual http://developer.download.nvidia.com/compute/cuda/3_2/toolkit/docs/CUDA_Toolkit_Reference_Manual.pdf has all the details. Edit: BTW streams are handy for concurrent execution of kernels and memory transfers. Why do you want to serialize the execution by waiting on the current stream to finish?"
            ],
            [
                "Instead of cudaStreamQuery, you want cudaStreamSynchronize int sid = 0; for (int d = 0; d &lt; DATA_SIZE; d+=DATA_STEP) { cudaStreamSynchronize(streams[sid]); cudaMemcpyAssync(d_data, h_data + d, DATA_STEP, cudaMemcpyHostToDevice, streams[sid]); kernel&lt;&lt;&lt;gridDim, blockDim, smSize streams[sid]&gt;&gt;&gt;(d_data, DATA_STEP); sid = ++sid % S_N; } (You can also use cudaThreadSynchronize to wait for launches across all streams, and events with cudaEventSynchronize for more advanced host/device synchronization.) You can further control the type of waiting that occurs with these synchronization functions. Look at the reference manual for the cudaDeviceBlockingSync flag and others. The default is probably what you want, though."
            ],
            [
                "You need to copy the data-chunk and execute kernel on that data-chunk in different for loops. That'll be more efficient. like this: size = N*sizeof(float)/nStreams; for (i=0; i&lt;nStreams; i++){ offset = i*N/nStreams; cudaMemcpyAsync(a_d+offset, a_h+offset, size, cudaMemcpyHostToDevice, stream[i]); } for (i=0; i&lt;nStreams; i++){ offset = i*N/nStreams; kernel&lt;&lt;&lt;N(nThreads*nStreams), nThreads, 0, stream[i]&gt;&gt;&gt; (a_d+offset); } In this way the memory copy doesn't have to wait for kernel execution of previous stream and vice versa."
            ]
        ],
        "votes": [
            6.0000001,
            4.0000001,
            3.0000001,
            2.0000001,
            1.0000001
        ]
    }
]