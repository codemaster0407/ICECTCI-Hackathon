[
    {
        "question": "PyTorch2.0 introduced a compiler--Inductor, and Inductor generage Triton DSL for generating ptx code. I am curious about why Triton DSL, but not any other DSL that can be compiled to PTX code, was selected as the backend language for Inductor. Is it for extensibility or performance reasons? I cannot find any clue to answer this question. Could anyone help me with this? Thanks.",
        "answers": [],
        "votes": []
    },
    {
        "question": "set up - multi model endpoint in aws sagemaker with nvidia triton server. based on the documentation provided here -&gt; https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/nlp/realtime/triton/multi-model/t5_pytorch_python-backend/t5_pytorch_python-backend.ipynb, we construct a request payload, which is of type httpclient class provided by tritonclient.http =&gt; httpclient.InferenceServerClient.genreate_request_body(inputs, outputs=outputs... examples i have seen pass list for both inputs and outputs parameter. any examples of passing just one input instead of list ? also, on the backend , the code that process this request is model.py file (code below) , looks like it only accepts , list of inputs rather than just one input. is there a way to override that . import tritonclient.http as httpclient import numpy as np def get_text_payload_binary(model_name, text): inputs = [] outputs = [] input_ids, attention_mask = tokenize_text(model_name, text) inputs.append(httpclient.InferInput(\"input_ids\", input_ids.shape, \"INT32\")) inputs.append(httpclient.InferInput(\"attention_mask\", attention_mask.shape, \"INT32\")) inputs[0].set_data_from_numpy(input_ids.astype(np.int32), binary_data=True) inputs[1].set_data_from_numpy(attention_mask.astype(np.int32), binary_data=True) output_name = \"output\" if model_name == \"t5-small\" else \"logits\" request_body, header_length = httpclient.InferenceServerClient.generate_request_body( inputs, outputs=outputs ) return request_body, header_length model.py import numpy as np import sys import os import json from pathlib import Path import torch import triton_python_backend_utils as pb_utils class TritonPythonModel: def initialize(self, args): ... def execute(self, requests): \"\"\"`execute` must be implemented in every Python model. `execute` function receives a list of pb_utils.InferenceRequest as the only argument. This function is called when an inference is requested for this model. Parameters ---------- requests : list A list of pb_utils.InferenceRequest Returns ------- list A list of pb_utils.InferenceResponse. The length of this list must be the same as `requests` \"\"\" responses = [] for request in requests: input_ids = pb_utils.get_input_tensor_by_name(request, \"input_ids\") input_ids = input_ids.as_numpy() input_ids = torch.as_tensor(input_ids).long().cuda() attention_mask = pb_utils.get_input_tensor_by_name(request, \"attention_mask\") attention_mask = attention_mask.as_numpy() attention_mask = torch.as_tensor(attention_mask).long().cuda() inputs = {'input_ids': input_ids, 'attention_mask': attention_mask} translation = self.model.generate(**inputs, num_beams=1) np_translation = translation.cpu().int().detach().numpy() inference_response = pb_utils.InferenceResponse( output_tensors=[ pb_utils.Tensor( \"output\", np_translation.astype(self.output_dtype) ) ] ) responses.append(inference_response) return responses",
        "answers": [],
        "votes": []
    },
    {
        "question": "My triton model config.pbtxt file looks like below. How can I pass inputs and outputs using tritonclient and perform an infer request. name: \u201ccifar10\u201d platform: \u201ctensorflow_savedmodel\u201d max_batch_size: 10000 input [ { name: \u201cinput_1\u201d data_type: TYPE_FP32 dims: [ 32, 32, 3 ] } ] output [ { name: \u201cfc10\u201d data_type: TYPE_FP32 dims: [ 10 ] } ] Any help is appreciated as I am very new to using tritonclient python package please",
        "answers": [
            [
                "See example for your case, but it was not tested as I don't have script which creates your model and hence minor issues might arise with some explicit exceptions. You can use this code snippet as reference and baseline for you solution. Before executing - make sure you have running triton server on localhost port 8000 or change URI address appropriately. import tritonclient.http as httpclient import argparse import numpy as np def test_infer(model_name, input_data, output_data): r = triton_client.infer( model_name=model_name, inputs=input_data, outputs=output_data ) return r payload_input = [] payload_output = [] payload_input.append(httpclient.InferInput(\"input_1\", [32, 32, 3], \"FP32\")) print(\"-----------fill inputs with data------------\") input1_data = np.full(shape=(32, 32, 3), fill_value=\"trousers\", dtype=np.float32) payload_input[0].set_data_from_numpy(input1_data, binary_data=False) if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument( '-u', '--url', default='localhost:8000', help='Endpoint URL' ) params = parser.parse_args() # setup client: triton_client = httpclient.InferenceServerClient(params.url, verbose=True) # send request: results = test_infer(\"\u201ccifar10\u201d\", payload_input, payload_output) print(\"----------_RESPONSE_-----------\") print(results.get_response()) print(results.as_numpy(\"\u201cfc10\u201d\"))"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to deploy XGBoost model on kserve. I deployed it on default serving runtime. But I want to try it on kserve-tritonserver. I know kserve told me kserve-tritonserver supports Tensorflow, ONNX, PyTorch, TensorRT. And NVIDIA said triton inference server supported XGBoost model. so.. is there a way to deploy kserve inference service using XGBoost model on kserve-tritonserver? k apply -n kserve-test -f - &lt;&lt;EOF apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"digits-classification-xgboost\" spec: predictor: model: modelFormat: name: xgboost protocolVersion: v2 storageUri: \"s3://.../digits_classification_model\" runtime: kserve-tritonserver EOF I tried it. But I got this description Status: Model Status: Last Failure Info: Message: Specified runtime does not support specified framework/version Reason: NoSupportingRuntime States: Active Model State: Target Model State: FailedToLoad Transition Status: InvalidSpec Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning InternalError 65s (x19 over 9m9s) v1beta1Controllers specified runtime kserve-tritonserver does not support specified framework/version",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to adapt pytriton to host multiple models for a multi-model sagemaker setup. In my case, I am trying to get it to load all models that are hosted in the SAGEMAKER_MULTI_MODEL_DIR folder. I could not find any relevnt example here for a multimodel use case, so I am trying with this code below. Is this the right approach? import logging import numpy as np from pytriton.decorators import batch from pytriton.model_config import ModelConfig, Tensor from pytriton.triton import Triton logger = logging.getLogger(\"examples.multiple_models_python.server\") logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(name)s: %(message)s\") # assume these are custom pytorch models # loaded from SAGEMAKER_MULTI_MODEL_DIR using a custom function models = [model1, model2] @batch def _infer(input, model): # do processing return [result] with Triton() as triton: logger.info(\"Loading models\") for model in models: triton.bind( model_name=model.name, infer_func=_infer, inputs=[ Tensor(name=\"multiplicand\", dtype=np.float32, shape=(-1,)), model ], outputs=[ Tensor(name=\"product\", dtype=np.float32, shape=(-1,)), ], config=ModelConfig(max_batch_size=8), ) triton.serve() However, this does not work due to the models not existing on loadtime for pytriton. Is there anymore documentation to using pytriton in a multimodel setup?",
        "answers": [
            [
                "If you are trying to use Triton Inference Server as the model server with SageMaker MME, please reference this example: https://aws.amazon.com/blogs/machine-learning/host-ml-models-on-amazon-sagemaker-using-triton-tensorrt-models/. You need to package your tarball in the appropriate format: model.py, config.pbtxt, and model artifacts. Dumping all these different tarballs in a common S3 location will then enable an MME endpoint which you can specify in the create_model API call. container = { \"Image\": triton_image_uri, \"ModelDataUrl\": model_data_uri, \"Mode\": \"MultiModel\", } create_model_response = sm.create_model( ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container )"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a custom python backend that works well with AWS sagemaker MMS (multimodel server) using an S3 model repository. I want to adapt this backend to work with Triton python backend. I have a example dockerfile that runs the triton server with my requirements. I also have a model_handler.py file that is based on this example, but I do not understand where to place this file to test it's functionality. Using classic sagemaker with MMS for example, I would import the handler in the dockerd-entrypoint. However with triton, I do not understand where this file should be imported. I understand I can use pytriton, but there is absolutely no documentation that I can comprehend. Can someone point me in the right direction please?",
        "answers": [
            [
                "For Triton a custom inference script is expected in the form of a model.py file. This model.py implements the initialize method (model loading), execute, and finalize methods where you can implement your pre/post processing logic. For the custom python backend you can install any additional dependencies by using conda-pack to define the environment and install any dependencies. In your config.pbtxt you can point towards this environment you have defined and created. Example: https://github.com/aws/amazon-sagemaker-examples/tree/main/inference/nlp/realtime/triton/single-model/t5_pytorch_python-backend"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "For some reason, I didn't update the cuda driver version of my environment, currently using 470.42.01 But I wanted to use the latest triton-influence-server\uff0823.04, Requires NVIDIA CUDA 12.1.0 by default, so I tried something like this: FROM nvcr.io/nvidia/tritonserver:23.04-py3 COPY cuda-compat /cuda-compat RUN dpkg -i /cuda-compat/cuda-compat-12-1_530.30.02-1_amd64.deb RUN LD_LIBRARY_PATH=\"/usr/local/cuda-12.1/compat:${LD_LIBRARY_PATH}\" Then build an image and run. Got an error that looks like it's not supported, I don't know if there's something wrong with my approach, or is it just not supported? ERROR: The NVIDIA Driver is present, but CUDA failed to initialize. GPU functionality will not be available. [[ System has unsupported display driver / cuda driver combination (error 803) ]] W0520 02:02:08.644624 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: system has unsupported display driver / cuda driver combination E0520 02:02:08.644693 1 server.cc:230] Failed to initialize CUDA memory manager: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination W0520 02:02:08.644699 1 server.cc:237] failed to enable peer access for some device pairs I0520 02:02:08.650682 1 model_lifecycle.cc:459] loading: densenet_onnx:1 I0520 02:02:08.650712 1 model_lifecycle.cc:459] loading: inception_graphdef:1 I0520 02:02:08.650734 1 model_lifecycle.cc:459] loading: simple_int8:1 I0520 02:02:08.650753 1 model_lifecycle.cc:459] loading: simple_sequence:1 I0520 02:02:08.650771 1 model_lifecycle.cc:459] loading: simple:1 I0520 02:02:08.650791 1 model_lifecycle.cc:459] loading: simple_dyna_sequence:1 I0520 02:02:08.650812 1 model_lifecycle.cc:459] loading: simple_identity:1 I0520 02:02:08.650854 1 model_lifecycle.cc:459] loading: simple_string:1 I0520 02:02:08.651782 1 onnxruntime.cc:2504] TRITONBACKEND_Initialize: onnxruntime I0520 02:02:08.651801 1 onnxruntime.cc:2514] Triton TRITONBACKEND API version: 1.12 I0520 02:02:08.651806 1 onnxruntime.cc:2520] 'onnxruntime' TRITONBACKEND API version: 1.12 I0520 02:02:08.651811 1 onnxruntime.cc:2550] backend configuration: {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"default-max-batch-size\":\"4\"}} E0520 02:02:08.663226 1 model_lifecycle.cc:597] failed to load 'densenet_onnx' version 1: Internal: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination I0520 02:02:08.883231 1 tensorflow.cc:2565] TRITONBACKEND_Initialize: tensorflow I0520 02:02:08.883260 1 tensorflow.cc:2575] Triton TRITONBACKEND API version: 1.12 I0520 02:02:08.883265 1 tensorflow.cc:2581] 'tensorflow' TRITONBACKEND API version: 1.12 I0520 02:02:08.883269 1 tensorflow.cc:2605] backend configuration: {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"default-max-batch-size\":\"4\"}} E0520 02:02:08.883301 1 model_lifecycle.cc:597] failed to load 'simple_int8' version 1: Internal: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination E0520 02:02:08.883308 1 model_lifecycle.cc:597] failed to load 'inception_graphdef' version 1: Internal: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination E0520 02:02:08.883314 1 model_lifecycle.cc:597] failed to load 'simple' version 1: Internal: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination E0520 02:02:08.883334 1 model_lifecycle.cc:597] failed to load 'simple_identity' version 1: Internal: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination E0520 02:02:08.883333 1 model_lifecycle.cc:597] failed to load 'simple_sequence' version 1: Internal: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination E0520 02:02:08.883347 1 model_lifecycle.cc:597] failed to load 'simple_dyna_sequence' version 1: Internal: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination E0520 02:02:08.883365 1 model_lifecycle.cc:597] failed to load 'simple_string' version 1: Internal: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination I0520 02:02:08.883525 1 server.cc:610] +-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Backend | Path | Config | +-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+ | onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"default-max-batch-size\":\"4\"}} | | tensorflow | /opt/tritonserver/backends/tensorflow/libtriton_tensorflow.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"default-max-batch-size\":\"4\"}} | +-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+ I0520 02:02:08.883603 1 server.cc:653] +----------------------+---------+------------------------------------------------------------------------------------------------------------------------------+ | Model | Version | Status | +----------------------+---------+------------------------------------------------------------------------------------------------------------------------------+ | densenet_onnx | 1 | UNAVAILABLE: Internal: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination | | inception_graphdef | 1 | UNAVAILABLE: Internal: unable to get number of CUDA devices: system has unsupported display driver / cuda driver combination | +----------------------+---------+------------------------------------------------------------------------------------------------------------------------------+ W0520 02:02:08.919379 1 metrics.cc:792] Cannot get CUDA device count, GPU metrics will not be available I0520 02:02:08.919635 1 metrics.cc:701] Collecting CPU metrics I0520 02:02:08.919802 1 server.cc:284] Waiting for in-flight requests to complete. I0520 02:02:08.919810 1 server.cc:300] Timeout 30: Found 0 model versions that have in-flight inferences I0520 02:02:08.919817 1 server.cc:315] All models are stopped, unloading models I0520 02:02:08.919823 1 server.cc:322] Timeout 30: Found 0 live models and 0 in-flight non-inference requests error: creating server: Internal - failed to load all models",
        "answers": [
            [
                "I see, this is the minor version number of the cuda driver that I ignored I use the cuda driver of 470.42 But cuda-compat requires 470.57+"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "examples here (https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-triton/nlp_bert/triton_nlp_bert.ipynb) show , that instead of sending text and tokenizing text in the server, it is done in the client side and tokenized input is send to triton server. is it possible to pass in text directly for the nlp Bert model like in the example and tokenize the test in the server? the default model.py only accepts a list of pb_utils.InferenceRequest as input. I assume , this will need to overridden or customized to accept the types. model.py class TritonPythonModel: \"\"\"Your Python model must use the same class name. Every Python model that is created must have \"TritonPythonModel\" as the class name. \"\"\" def initialize(self, args): def execute(self, requests): \"\"\"`execute` must be implemented in every Python model. `execute` function receives a list of pb_utils.InferenceRequest as the only argument. This function is called when an inference is requested for this model. Depending on the batching configuration (e.g. Dynamic Batching) used, `requests` may contain multiple requests. Every Python model, must create one pb_utils.InferenceResponse for every pb_utils.InferenceRequest in `requests`. If there is an error, you can set the error argument when creating a pb_utils.InferenceResponse. Parameters ---------- requests : list A list of pb_utils.InferenceRequest Returns ------- list A list of pb_utils.InferenceResponse. The length of this list must be the same as `requests` \"\"\" text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\" input_ids, attention_mask = tokenize_text(text_triton) payload = { \"inputs\": [ {\"name\": \"INPUT__0\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": input_ids}, {\"name\": \"INPUT__1\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": attention_mask}, ] } response = client.invoke_endpoint( EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload) ) print(json.loads(response[\"Body\"].read().decode(\"utf8\"))) created and tested the multi model endpoint model in aws sagemaker with Nvidia triton server, with tokenized text as inputs.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I converted my model into Onnx and then onnxruntime transformer optimization step is also done. Model is successfully loading and logits values are being matched with the native model as well. I moved this model to Triton server but facing following error on model loading step: Unrecognized attribute: mask_filter_value for operator Attention Library information is as: onnx: 1.13.1 onnxruntime: 1.14.1 torch: 1.13.1 onnxruntime-tools:1.7.0 onnxconverter-common: 1.13.0 opset_version: 11 I tried two versions of triton inference server. Both gave the same errors: nvcr.io/nvidia/tritonserver:21.04-py3 nvcr.io/nvidia/tritonserver:23.02-py3 Could there be something still wrong in onnx runtime but logits are matching exactly. Did anyone else face this error?",
        "answers": [],
        "votes": []
    },
    {
        "question": "The tutorials on deployment GPT-like models inference to Triton looks like: Preprocess our data as input_ids = tokenizer(text)[\"input_ids\"] Feed input to Triton inference server and get outputs_ids = model(input_ids) Postprocess outputs like outputs = outputs_ids.logits.argmax(axis=2) outputs = tokenizer.decode(outputs) I use finetuned GPT2 model and this method gives incorrect result. The correct result will be obtained by model.decode(input_ids) method. There is the way to deploy finetuned GPT-like huggingface model to Triton with inference model.decode(input_ids) not model(input_ids)?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have installed triton inference server with docker, docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v /mnt/data/nabil/triton_server/models:/models nvcr.io/nvidia/tritonserver:22.08-py3 tritonserver --model-repository=/models I have also created the torchscript model from my pytorch model, using from model_ecapatdnn import ECAPAModel import soundfile as sf import torch model_1 = ECAPAModel.ECAPAModel(lr = 0.001, lr_decay = 0.97, C = 1024, n_class = 18505, m = 0.2, s = 30, test_step = 3, gpu = -1) model_1.load_parameters(\"/ecapatdnn/model.pt\") model = model_1.speaker_encoder # Switch the model to eval model model.eval() # An example input you would normally provide to your model's forward() method. example = torch.rand(1, 48000) # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing. traced_script_module = torch.jit.trace(model, example) # Save the TorchScript model traced_script_module.save(\"traced_ecapatdnn_bangasianeng.pt\") Now, as you can see, my model takes a tensor with shape (BxN), where B is the batch size. How do I write the config.pbtxt for this model?",
        "answers": [
            [
                "So, found the answer. Have to just specify the shape in config file. Here is the config that works for me. name: \"ecapatdnn_bangasianeng\" platform: \"pytorch_libtorch\" max_batch_size: 1 input[ { name: \"INPUT__0\" data_type: TYPE_FP32 dims: [-1] } ] output:[ { name: \"OUTPUT__0\" data_type: TYPE_FP32 dims: [512] } ]"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Recently, I have come across a solution of the triton serving config file disable flag \"--strict-model-config=false\" while running the inferencing server. This would enable to create its own config file while loading the model from the model repository. sudo docker run --rm --net=host -p 8000:8000 -p 8001:8001 -p 8002:8002 \\ -v /home/rajesh/custom_repository:/models nvcr.io/nvidia/tritonserver:22.06-py3 \\ tritonserver --model-repository=/models --strict-model-config=false I would like to get the generated config file from the triton inferencing server since we can play around with the batch config and other parameters. Is there a way to get the inbuilt generated config.pbtxt file for the models I have loaded in the server so that I can play around the batch size and other parameters.",
        "answers": [
            [
                "As per Triton docs (source), the loaded model configuration can be found by curl'ing the /config endpoint: Command: curl localhost:8000/v2/models/&lt;model_name&gt;/config [source]"
            ],
            [
                "The above answer which the uses curl command would return the json response. If the results should be in the protobuf format, try loading the model using triton inferencing server with strict model config as false and fetch the results by using the below python script which would return the results in necessary protobuf format. Use this to get the format of the model and edit it easily as per the needs in config pbtxt file instead of cnoverting json to protobuf results. import tritonclient.grpc as grpcclient triton_client = grpcclient.InferenceServerClient(url=&lt;triton_server_url&gt;) model_config = triton_client.get_model_config(model_name=&lt;model_name&gt;, model_version=&lt;model_version&gt;)"
            ]
        ],
        "votes": [
            3.0000001,
            2.0000001
        ]
    },
    {
        "question": "I try to run NVIDIA\u2019s Triton Inference Server. I pulled the pre-built container nvcr.io/nvidia/pytorch:22.06-py3 and then run it with the command run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/F/models:/models nvcr.io/nvidia/pytorch:22.06-py3 tritonserver --model-repository=/models and got the error /opt/nvidia/nvidia_entrypoint.sh: line 49: exec: tritonserver: not found I googled and have not found something to catch this. I tried to change tritonserver to trtserver as recommended but it did not help. Please give some advice how it can be solved.",
        "answers": [
            [
                "Looks like you're trying to run a tritonserver using a pytorch image but according to the triton-server quick start guide, the image should be: $ docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/full/path/to/docs/examples/model_repository:/models nvcr.io/nvidia/tritonserver:&lt;xx.yy&gt;-py3 tritonserver --model-repository=/models Where &lt;xx.yy&gt; is the version of Triton that you want to use In your case it should be nvcr.io/nvidia/tritonserver:22.06-py3 and the full command: run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/F/models:/models nvcr.io/nvidia/tritonserver:22.06-py3 tritonserver --model-repository=/models"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm considering Cog and Triton Inference Server for inference in production. Does someone know what is the difference in capabilities as well as in run times between the two, especially on AWS?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I run a model in triton serving with shared memory and it works correctly. In order to simulate backend structure I wrote a Fast API for my model and run it with gunicorn with 6 workers. Then I wrote anthor Fast API to route locust requests to my first Fast Fast API as below image(pseudo code). my second Fast API runs with uvicorn. but the problem is when I used multiple workers for my uvicorn, triton serving failed to shared memory. Note: without shared memory every thing works but my response time is much longer than the shared memory option. so I need to use shared memory option. here is my triton client code: I have a functions in my client code named predict function which used the requestGenerator to shared input_simple and output_simple spaces. this is my requestGenerator generator: def requestGenerator(self, triton_client, batched_img_data, input_name, output_name, dtype, batch_data): triton_client.unregister_system_shared_memory() triton_client.unregister_cuda_shared_memory() output_simple = \"output_simple\" input_simple = \"input_simple\" input_data = np.ones( shape=(batch_data, 3, self.width, self.height), dtype=np.float32) input_byte_size = input_data.size * input_data.itemsize output_byte_size = input_byte_size * 2 shm_op0_handle = shm.create_shared_memory_region( output_name, output_simple, output_byte_size) triton_client.register_system_shared_memory( output_name, output_simple, output_byte_size) shm_ip0_handle = shm.create_shared_memory_region( input_name, input_simple, input_byte_size) triton_client.register_system_shared_memory( input_name, input_simple, input_byte_size) inputs = [] inputs.append( httpclient.InferInput(input_name, batched_img_data.shape, dtype)) inputs[0].set_data_from_numpy(batched_img_data, binary_data=True) outputs = [] outputs.append( httpclient.InferRequestedOutput(output_name, binary_data=True)) inputs[-1].set_shared_memory(input_name, input_byte_size) outputs[-1].set_shared_memory(output_name, output_byte_size) yield inputs, outputs, shm_ip0_handle, shm_op0_handle this is my predict function: def predict(self, triton_client, batched_data, input_layer, output_layer, dtype): responses = [] results = None for inputs, outputs, shm_ip_handle, shm_op_handle in self.requestGenerator( triton_client, batched_data, input_layer, output_layer, type, len(batched_data)): self.sent_count += 1 shm.set_shared_memory_region(shm_ip_handle, [batched_data]) responses.append( triton_client.infer(model_name=self.model_name, inputs=inputs, request_id=str(self.sent_count), model_version=\"\", outputs=outputs)) output_buffer = responses[0].get_output(output_layer) if output_buffer is not None: results = shm.get_contents_as_numpy( shm_op_handle, triton_to_np_dtype(output_buffer['datatype']), output_buffer['shape']) triton_client.unregister_system_shared_memory() triton_client.unregister_cuda_shared_memory() shm.destroy_shared_memory_region(shm_ip_handle) shm.destroy_shared_memory_region(shm_op_handle) return results Any help would be appreciated to help me how to use multiple uvicorn workers to send multiple requests concurrently to my triton code without failing.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have deployed T5 tensorrt model on nvidia triton server and below is the config.pbtxt file, but facing problem while inferencing the model using triton client. As per the config.pbtxt file there should be 4 inputs to the tensorrt model along with the decoder ids. But how can we send decoder as input to the model I think decoder is to be generated from models output. name: \"tensorrt_model\" platform: \"tensorrt_plan\" max_batch_size: 0 input [ { name: \"input_ids\" data_type: TYPE_INT32 dims: [ -1, -1 ] }, { name: \"attention_mask\" data_type: TYPE_INT32 dims: [-1, -1 ] }, { name: \"decoder_input_ids\" data_type: TYPE_INT32 dims: [ -1, -1] }, { name: \"decoder_attention_mask\" data_type: TYPE_INT32 dims: [ -1, -1 ] } ] output [ { name: \"last_hidden_state\" data_type: TYPE_FP32 dims: [ -1, -1, 768 ] }, { name: \"input.151\" data_type: TYPE_FP32 dims: [ -1, -1, -1 ] } ] instance_group [ { count: 1 kind: KIND_GPU } ]",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am deploying a triton inference server on the Amazon Elastic Kubernetes Service (Amazon EKS) and using Nginx Open-Source Load Balancer for load-balancing. Our EKS Cluster is private (EKS Nodes are in private subnets) so that no one can access it from the outside world. Since, triton inference server has three endpoints:- port 8000: for HTTP requests port 8001: for grpc requests port 8002: Prometheus metrics server First of all, I have created a deployment for Triton on AWS EKS and exposed it using clusterIP = None, so that all the replicas endpoints are exposed and identified by NGINX Load Balancer. apiVersion: v1 kind: Service metadata: name: triton labels: app: triton spec: clusterIP: None ports: - protocol: TCP port: 8000 name: http targetPort: 8000 - protocol: TCP port: 8001 name: grpc targetPort: 8001 - protocol: TCP port: 8002 name: metrics targetPort: 8002 selector: app: triton Then, I have created a image for nginx opensource load balancer using the below configuration. Configuration file for NGINX on EKS node at the location /etc/nginx/conf.d/nginx.conf. resolver kube-dns.kube-system.svc.cluster.local valid=5s; upstream backend { zone upstream-backend 64k; server triton.default.svc.cluster.local:8000; } upstream backendgrpc { zone upstream-backend 64k; server triton.default.svc.cluster.local:8001; } server { listen 80; location / { proxy_pass http://backend/; } } server { listen 89 http2; location / { grpc_pass grpc://backendgrpc; } } server { listen 8080; root /usr/share/nginx/html; location = /dashboard.html { } location = / { return 302 /dashboard.html; } } Dockerfile for Nginx Opensource LB is:- FROM nginx RUN rm /etc/nginx/conf.d/default.conf COPY /etc/nginx/conf.d/nginx.conf /etc/nginx/conf.d/default.conf I have created a ReplicationController for NGINX. To pull the image from the private registry, Kubernetes needs credentials. The imagePullSecrets field in the configuration file specifies that Kubernetes should get the credentials from a Secret named ecr-cred. The nginx-rc file looks like:- apiVersion: v1 kind: ReplicationController metadata: name: nginx-rc spec: replicas: 1 selector: app: nginx template: metadata: labels: app: nginx spec: imagePullSecrets: - name: ecr-cred containers: - name: nginx command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"nginx; while true; do sleep 30; done;\" ] imagePullPolicy: IfNotPresent image: &lt;Image URL with tag&gt; ports: - name: http containerPort: 80 hostPort: 8085 - name: grpc containerPort: 89 hostPort: 8087 - name: http-alt containerPort: 8080 hostPort: 8086 - name: triton-svc containerPort: 8000 hostPort: 32309 Now, the issue which I am facing is, when the pods are increasing, the nginx load balancer is not doing the load balancing between those newly added pods. Can anyone help me?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a Triton server on EKS listening on 3 ports, 8000 is for http requests, 8001 is for gRPC and 8002 is for prometheus metrics. So, I have created a Triton deployment on EKS which is exposed through NodePort service of EKS. I am also using ALB ingress which is creating an application load balancer to balance the load of Triton servers on these ports. But the traffic is not flowing correctly as it is showing same output for all the 3 ports but it should be different. So, now do I have to create 3 Application Load Balancers for 3 ports or is it possible to manage all ports with a single Application Load Balancer? Yaml file for ALB Ingress looks like:- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: triton annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\":8000}, {\"HTTP\":8001}, {\"HTTP\":8002}]' alb.ingress.kubernetes.io/healthcheck-port: traffic-port spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Prefix backend: service: name: triton port: number: 8000 - http: paths: - path: /v2 pathType: Prefix backend: service: name: triton port: number: 8001 - http: paths: - path: /metrics pathType: Prefix backend: service: name: triton port: number: 8002",
        "answers": [
            [
                "Based on the Load Balancer controller's documentation, the listen ports specified on the annotation alb.ingress.kubernetes.io/listen-ports is merged across all Ingresses, so the rules to all of the ports are going to be applied on all the listeners. This is going to require a deployment of multiple Ingresses in the Kubernetes cluster. By default, if you deploy multiple Ingress Controllers in the EKS, then whenever you create an Ingress, a race condition will occur between all those controllers in which each controller tries to update Ingress status fields. Hence Kubernetes provides the capability where different Ingresses can be implemented by different Ingress controllers. This is achieved by using IngressClass. Than we can share the same ALB by specifying the alb.ingress.kubernetes.io/group.name annotation. The code should be something like this: --- apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: alb spec: controller: ingress.k8s.aws/alb --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: triton annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\":8000}]' alb.ingress.kubernetes.io/healthcheck-port: traffic-port alb.ingress.kubernetes.io/group.name: triton-group spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Prefix backend: service: name: triton port: number: 8000 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: triton annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\":8001}]' alb.ingress.kubernetes.io/healthcheck-port: traffic-port alb.ingress.kubernetes.io/group.name: triton-group spec: ingressClassName: alb rules: - http: paths: - path: /v2 pathType: Prefix backend: service: name: triton port: number: 8001 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: triton annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\":8002}]' alb.ingress.kubernetes.io/healthcheck-port: traffic-port alb.ingress.kubernetes.io/group.name: triton-group spec: ingressClassName: alb rules: - http: paths: - path: /metrics pathType: Prefix backend: service: name: triton port: number: 8002"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Having problems with implementing triton service into gitlab CI. As I noticed in the triton github https://github.com/triton-inference-server/server, they don't have any exposed port by default in Dockerfile and I'm not really able to access the service in any way? Is there any workaround for accessing the triton service? Thanks! .triton_service: &amp;triton_service name: nvcr.io/nvidia/tritonserver:21.06-py3 command: ['tritonserver', '--model-repository=$(S3_TRITON_BUCKET)', '--model-control-mode=explicit', '--allow-http=true', '--allow-metrics=false', '--allow-gpu-metrics=false', '--log-verbose=true', '--log-info=true', '--log-warning=true', '--log-error=true'] alias: triton services: - *triton_service script: - curl -v http://triton:8000/v2/health/ready",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a tensorflow model with a string parameter as input. Whats the type to use for strings in the Triton Java api? Eg. Model definition { \"name\":\"test_model\", \"platform\":\"tensorflow_savedmodel\", \"backend\":\"tensorflow\", \"version_policy\":{ \"latest\":{ \"num_versions\":1 } }, \"max_batch_size\":8, \"input\":[{ \"name\":\"input_text\", \"data_type\":\"TYPE_STRING\", \"format\":\"FORMAT_NONE\", \"dims\":[1],\"reshape\":{ \"shape\":[]},\"is_shape_tensor\":false, \"allow_ragged_batch\":false }] Client code String text = \"the text\"; InferTensorContents.Builder input0_data = InferTensorContents.newBuilder(); input0_data ... how to set",
        "answers": [
            [
                "Triton uses google protobufs, so this is they way by using ByteString String text = \"textstring\"; InferTensorContents.Builder input0input_text = InferTensorContents.newBuilder(); final ByteString input = ByteString.copyFrom(text, Charset.forName(\"UTF8\")); System.out.println(input.size()); input0input_text.addByteContents(input);"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to install tritonclient according to client_libraries.md on win10. Errors happened when I install nvidia-pyindex. How to solve it? Thanks! (py38trtc250) G:\\client_py&gt;pip install --user nvidia-pyindex Looking in indexes: https://mirrors.aliyun.com/pypi/simple Collecting nvidia-pyindex Using cached https://mirrors.aliyun.com/pypi/packages/64/4c/dd413559179536b9b7247f15bf968f7e52b5f8c1d2183ceb3d5ea9284776/nvidia-pyindex-1.0.5.tar.gz (6.1 kB) Building wheels for collected packages: nvidia-pyindex Building wheel for nvidia-pyindex (setup.py) ... error ERROR: Command errored out with exit status 1: command: 'D:\\Anaconda\\envs\\py38trtc250\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\15151\\\\AppData\\\\Local\\\\Temp\\\\pip-install-a7e_bahh\\\\nvidia-pyindex\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\15151\\\\AppData\\\\Local\\\\Temp\\\\pip-install-a7e_bahh\\\\nvidia-pyindex\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\15151\\AppData\\Local\\Temp\\pip-wheel-cho7jhkv' cwd: C:\\Users\\15151\\AppData\\Local\\Temp\\pip-install-a7e_bahh\\nvidia-pyindex\\ Complete output (25 lines): running bdist_wheel running build running build_py creating build creating build\\lib creating build\\lib\\nvidia_pyindex copying nvidia_pyindex\\cmdline.py -&gt; build\\lib\\nvidia_pyindex copying nvidia_pyindex\\utils.py -&gt; build\\lib\\nvidia_pyindex copying nvidia_pyindex\\__init__.py -&gt; build\\lib\\nvidia_pyindex running egg_info writing nvidia_pyindex.egg-info\\PKG-INFO writing dependency_links to nvidia_pyindex.egg-info\\dependency_links.txt writing entry points to nvidia_pyindex.egg-info\\entry_points.txt writing top-level names to nvidia_pyindex.egg-info\\top_level.txt reading manifest file 'nvidia_pyindex.egg-info\\SOURCES.txt' reading manifest template 'MANIFEST.in' writing manifest file 'nvidia_pyindex.egg-info\\SOURCES.txt' installing to build\\bdist.win-amd64\\wheel running install '\"nvidia_pyindex uninstall\"' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f \u6216\u6279\u5904\u7406\u6587\u4ef6\u3002 error: [WinError 2] \u7cfb\u7edf\u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6587\u4ef6\u3002 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% COMMAND: InstallCommand %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ---------------------------------------- ERROR: Failed building wheel for nvidia-pyindex Running setup.py clean for nvidia-pyindex Failed to build nvidia-pyindex Installing collected packages: nvidia-pyindex Running setup.py install for nvidia-pyindex ... error ERROR: Command errored out with exit status 1: command: 'D:\\Anaconda\\envs\\py38trtc250\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\15151\\\\AppData\\\\Local\\\\Temp\\\\pip-install-a7e_bahh\\\\nvidia-pyindex\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\15151\\\\AppData\\\\Local\\\\Temp\\\\pip-install-a7e_bahh\\\\nvidia-pyindex\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\15151\\AppData\\Local\\Temp\\pip-record-ma8cx0c0\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\15151\\AppData\\Roaming\\Python\\Python38\\Include\\nvidia-pyindex' cwd: C:\\Users\\15151\\AppData\\Local\\Temp\\pip-install-a7e_bahh\\nvidia-pyindex\\ Complete output (7 lines): running install '\"nvidia_pyindex uninstall\"' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f \u6216\u6279\u5904\u7406\u6587\u4ef6\u3002 error: [WinError 2] \u7cfb\u7edf\u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6587\u4ef6\u3002 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% COMMAND: InstallCommand %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ---------------------------------------- ERROR: Command errored out with exit status 1: 'D:\\Anaconda\\envs\\py38trtc250\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\15151\\\\AppData\\\\Local\\\\Temp\\\\pip-install-a7e_bahh\\\\nvidia-pyindex\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\15151\\\\AppData\\\\Local\\\\Temp\\\\pip-install-a7e_bahh\\\\nvidia-pyindex\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\15151\\AppData\\Local\\Temp\\pip-record-ma8cx0c0\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\15151\\AppData\\Roaming\\Python\\Python38\\Include\\nvidia-pyindex' Check the logs for full command output.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I need something like that for x86 arch: mov edi, dword ptr [0x7fc70000] add edi, 0x11 sub edi, 0x33F0B753 After Z3 simplification I have got (memory 0x7FC70000 is symbolized): bvadd (_ bv3423553726 32) MEM_0x7FC70000 The last step is converting Z3's AST into ASM code to get result like this: mov edi, dword ptr [0x7fc70000] add edi, 0xCC0F48BE Is there any efficient way to perform last step? Should I parse SMT formula and convert it manually (bv -&gt; mov...)?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers. We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 3 years ago. Improve this question There is an example: mov edi, dword ptr [0x7fc70000] add edi, 0x11 sub edi, 0x33F0B753 After Z3 simplification I have got (memory 0x7FC70000 is symbolized): bvadd (_ bv3423553726 32) MEM_0x7FC70000 Now I need to convert Z3 into ASM to get result like this: mov edi, 0xCC0F48BE add edi, dword ptr [0x7fc70000] Or like that: mov edi, dword ptr [0x7fc70000] add edi, 0xCC0F48BE",
        "answers": [
            [
                "There is no such tool publically available, to the best of my knowledge. And it's not quite clear how one would design one, as it would have to be very specific for a given machine's assembly language. (X86 assumption can take you far I suppose.) It might be better to compile it to straight-line C, and then use the ubiquitously available gnu-c toolchain to go the last mile. But of course, it all depends on your specific use case and needs. Take a look at this page: http://smtlib.cs.uiowa.edu/utilities.shtml Perhaps the tools listed there can give you a starting point if you go down the path of developing one. And if you do develop such a tool, please do advertise it there as well."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I've set my first steps into the world of terraform, I'm trying to deploy infrastructure on Joyent triton. After setup, I wrote my first .tf (well, copied from the examples) and hit terraform apply. All seems to go well, it doesn't break on errors, but it doesn't actually provision my container ?? I doublechecked in the triton web gui and with \"triton instance list\". Nothing there. Any ideas what's going on here ? provider \"triton\" { account = \"tralala\" key_id = \"my-pub-key\" url = \"https://eu-ams-1.api.joyentcloud.com\" } resource \"triton_machine\" \"test-smartos\" { name = \"test-smartos\" package = \"g4-highcpu-128M\" image = \"842e6fa6-6e9b-11e5-8402-1b490459e334\" tags { hello = \"world\" role = \"database\" } cns { services = [\"web\", \"frontend\"] } }",
        "answers": [],
        "votes": []
    },
    {
        "question": "I cannot get any Java stack with dtrace in a Joyent SmartOS instance. I tried the java:15.1.1 image and a plain SmartOS 'base64' image, where I installed openjdk 8. I most basic example: cat Loop.java [root@7e8c2a25-c852-4967-b60c-7b4fbd9a1de5 /demo]# cat Loop.java class Loop { public static void main(String[] args) throws InterruptedException { while (true) { System.out.println(\"Sleepy\"); Thread.sleep(2000); } } } [root@7e8c2a25-c852-4967-b60c-7b4fbd9a1de5 /demo]# javac Loop.java [root@7e8c2a25-c852-4967-b60c-7b4fbd9a1de5 /demo]# java Loop I added the libdtrace_forceload.so as recommended. export LD_AUDIT_64=/usr/lib/dtrace/64/libdtrace_forceload.so It is a 64 bit JVM: [root@7e8c2a25-c852-4967-b60c-7b4fbd9a1de5 /demo]# java -version openjdk version \"1.7.0-internal\" OpenJDK Runtime Environment (build 1.7.0-internal-pkgsrc_2015_05_29_19_05-b00) OpenJDK 64-Bit Server VM (build 24.76-b04, mixed mode) When I run dtrace, and use jstack, I get the C-stacks. However, the JAVA frames are the raw addresses, quite useless: [root@7e8c2a25-c852-4967-b60c-7b4fbd9a1de5 ~]# pgrep -fn \"java Loop\" 32597 [root@7e8c2a25-c852-4967-b60c-7b4fbd9a1de5 ~]# dtrace -n 'syscall:::entry/pid == 32597/ { @num[ustack(20)] = count(); }' dtrace: description 'syscall:::entry' matched 237 probes ^C libc.so.1`__write+0xa libjvm.so`_ZN2os5writeEiPKvj+0x128 libjvm.so`JVM_Write+0x34 libjava.so`writeBytes+0x1b5 libjava.so`Java_java_io_FileOutputStream_writeBytes+0x1f 0xffffbf7ffa612d98 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa606058 0xffffbf7ffa6004e7 libjvm.so`_ZN9JavaCalls11call_helperEP9JavaValueP12methodHandleP17JavaCallArgumentsP6Thread+0x31d *snip* I do see hotspot probes are available: [root@7e8c2a25-c852-4967-b60c-7b4fbd9a1de5 ~]# dtrace -l | grep hotspot | more 6103 hotspot32597 libjvm.so _ZN17VM_GenCollectFull4doitEv gc-begin 6104 hotspot32597 libjvm.so _ZN15VM_GC_Operation13notify_gc_endEv gc-end 6105 hotspot32597 libjvm.so _ZN26VM_GenCollectForAllocation4doitEv gc-end 6106 hotspot32597 libjvm.so _ZN35VM_GenCollectForPermanentAllocation4doitEv gc-end 6107 hotspot32597 libjvm.so _ZN17VM_GenCollectFull4doitEv gc-end 6132 hotspot32597 libjvm.so _ZN13instanceKlass15initialize_implE19instanceKlassHandleP6Thread class-initialization-end 6133 hotspot32597 libjvm.so _ZN13instanceKlass15initialize_implE19instanceKlassHandleP6Thread class-initialization-erroneous 6441 hotspot_jni32597 libjvm.so jni_DeleteLocalRef DeleteLocalRef-entry 6442 hotspot_jni32597 libjvm.so jni_DeleteLocalRef DeleteLocalRef-return 6443 hotspot_jni32597 libjvm.so jni_DeleteWeakGlobalRef DeleteWeakGlobalRef-entry 6444 hotspot_jni32597 libjvm.so jni_DeleteWeakGlobalRef DeleteWeakGlobalRef-return 6445 hotspot_jni32597 libjvm.so jni_DestroyJavaVM DestroyJavaVM-entry 6446 hotspot_jni32597 libjvm.so jni_DestroyJavaVM DestroyJavaVM-return Question: Is there a way to list ustack helpers and if they are loaded? Any way to get the Java stack?",
        "answers": [
            [
                "In your example with untranslated Java stack frames, you appear to be using the ustack() action. In order to get translated frames, I believe you ought to be using the jstack() action instead."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Recently some alternatives for running docker containers or even the app container have developed. I know that there is rkt from coreos (https://coreos.com/blog/rocket/) and triton from joyent (https://www.joyent.com/) How do these two approaches compare? Edit Maybe I should re-phrase my question after these good comments from @ Lakatos Gyula How does Triton compare to coreos or kubernetes for running docker-containers at scale?",
        "answers": [
            [
                "So in a way, this is an apples to oranges to grapes comparison. CoreOS is an operating system, Kubernetes is open source container orchestration software, and Triton is a PaaS. So CoreOS, it's a minimal operating system with a focus on security. I've been using this in production for several months now at work, haven't found a reason to not like it yet. It does not have a package manager, but it comes preinstalled with both rkt and Docker. You can run both docker and rkt just fine on there. It also comes with Etcd, which is a distributed key-value store, and it happens that kubernetes is backed by it. It also comes with Flannel which is a networking program for networking between containers and machines in your cluster. CoreOS also ships with Fleet, which you can think of like a distributed version of systemd, which systemd is CoreOS' init system. And as of recently, CoreOS ships with Kubernetes itself. Kubernetes is a container orchestration software that is made up of a few main components. There are masters, which use the APIServer, controller and scheduler to manage the cluster. And there are nodes which use the \"kubelet\" and kube-proxy\". Through these components, Kubernetes schedules and manages where to run your containers on your cluster. As of v1.1 Kubernetes also can auto-scale your containers. I also have been using this in production as long as I have been using CoreOS, and the two go together very well. Triton is Joyent's Paas for Docker. Think of it like Joyent's traditional service, but instead of BSD jails (similar concept to Linux containers) and at one point Solaris Zones (could be wrong on that one, that was just something I heard from word of mouth), you're using Docker containers. This does abstract away a lot of the work you'd have to do with setting up CoreOS and Kubernetes, that said there are services that'll do the same and use kubernetes under the hood. Now I haven't used Triton like I have used Kubernetes and CoreOS, but it definitely seems to be quite well engineered. Ultimately, I'd say it's about your needs. Do you need flexibility and visibility, then something like CoreOS makes sense, particularly with Kubernetes. If you want that abstracted away and have these things handled for you, I'd say Triton makes sense."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I am trying to setup jwilder/nginx-proxy docker container on joyent's triton platform. This container needs access to docker.sock to read information about its environment. Basically it needs to do docker up -v /var/run/docker.sock:/tmp/docker.sock:ro ... On triton this fails like Invalid host volume paths found: must be in the form \"http://host/path:/container_path\": [\"/var/run/docker.sock:/tmp/docker.sock:rw\"] Any ideas how I could inject docker.sock into my container on triton?",
        "answers": [
            [
                "Triton doesn't currently support docker.sock. You have to go through the DOCKER_HOST TCP port. If you would like this to be a feature, send them a tweet, email or github ticket. Full disclosure: I work for Joyent."
            ]
        ],
        "votes": [
            1e-07
        ]
    }
]