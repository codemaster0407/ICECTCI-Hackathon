[
    {
        "question": "I am working on how to offload some workload to GPU using CUDA in SpringBoot project. To help me explain my question better, let me suppose that we want to implement a REST API to do matrix-vector multiplication in SpringBoot application. We need to load some matrices with various sizes to GPU's memory on application launch, then accept user's request with vector data and find the corresponding matrix inside GPU to do matrix-vector multiplication, and finally return the multiplication result to user. We have already implemented the kernel using JCuda. In this scenario, we want to process users' requests concurrently, so there are several questions I am interested in: How to avoid CUDA out of memory error when there are lots of REST API calls? If we use explicit cuda streams to improve application's throughput, how to determine the number of cuda streams? If we also need to do CUD operations to matrices in GPU's memory while processing REST API calls, how to make these operations and matrix-vector multiplication operations atomic?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am comletelly new to JAVA GPU thing (JCuda), trying to test and understand how JCuda works and if it can be any useful to my needs (JAVA renderer), so I downloaded JCuda examples and loaded them into NetBeans 8. But when I try to run any of the example files, let's say JCudaDriverVolumeRendererJOGL.java, I get warning/errors in output window, specifically this one for the above class (but I got errors on most of those example files, only examples that are creating no GUI are working, like example JCudaRuntimeMemoryBandwidths): run: aug 13, 2021 12:00:25 PM jcuda.samples.utils.JCudaSamplesUtils invokeNvcc INFO: Creating ptx file for src/main/resources/kernels/JCudaDriverVolumeRendererKernel.cu aug 13, 2021 12:00:25 PM jcuda.samples.utils.JCudaSamplesUtils invokeNvcc INFO: Executing nvcc -m64 -ptx src/main/resources/kernels/JCudaDriverVolumeRendererKernel.cu -o src/main/resources/kernels/JCudaDriverVolumeRendererKernel.ptx Exception in thread \"AWT-EventQueue-0-AWTAnimator#00\" com.jogamp.opengl.util.AnimatorBase$UncaughtAnimatorException: java.lang.RuntimeException: com.jogamp.opengl.GLException: Caught CudaException: Could not create ptx file on thread AWT-EventQueue-0 at com.jogamp.opengl.util.AWTAnimatorImpl.display(AWTAnimatorImpl.java:92) at com.jogamp.opengl.util.AnimatorBase.display(AnimatorBase.java:452) at com.jogamp.opengl.util.Animator$MainLoop.run(Animator.java:204) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: com.jogamp.opengl.GLException: Caught CudaException: Could not create ptx file on thread AWT-EventQueue-0 at com.jogamp.common.util.awt.AWTEDTExecutor.invoke(AWTEDTExecutor.java:58) at jogamp.opengl.awt.AWTThreadingPlugin.invokeOnOpenGLThread(AWTThreadingPlugin.java:103) at jogamp.opengl.ThreadingImpl.invokeOnOpenGLThread(ThreadingImpl.java:201) at com.jogamp.opengl.Threading.invokeOnOpenGLThread(Threading.java:202) at com.jogamp.opengl.Threading.invoke(Threading.java:221) at com.jogamp.opengl.awt.GLCanvas.display(GLCanvas.java:505) at com.jogamp.opengl.util.AWTAnimatorImpl.display(AWTAnimatorImpl.java:81) ... 3 more Caused by: com.jogamp.opengl.GLException: Caught CudaException: Could not create ptx file on thread AWT-EventQueue-0 at com.jogamp.opengl.GLException.newGLException(GLException.java:76) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1327) at jogamp.opengl.GLDrawableHelper.invokeGL(GLDrawableHelper.java:1147) at com.jogamp.opengl.awt.GLCanvas$12.run(GLCanvas.java:1438) at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:301) at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:758) at java.awt.EventQueue.access$500(EventQueue.java:97) at java.awt.EventQueue$3.run(EventQueue.java:709) at java.awt.EventQueue$3.run(EventQueue.java:703) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.awt.EventQueue.dispatchEvent(EventQueue.java:728) at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:205) at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116) at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93) at java.awt.EventDispatchThread.run(EventDispatchThread.java:82) Caused by: jcuda.CudaException: Could not create ptx file at jcuda.samples.utils.JCudaSamplesUtils.invokeNvcc(JCudaSamplesUtils.java:176) at jcuda.samples.utils.JCudaSamplesUtils.preparePtxFile(JCudaSamplesUtils.java:51) at jcuda.driver.gl.samples.JCudaDriverVolumeRendererJOGL.initCuda(JCudaDriverVolumeRendererJOGL.java:476) at jcuda.driver.gl.samples.JCudaDriverVolumeRendererJOGL.init(JCudaDriverVolumeRendererJOGL.java:455) at jogamp.opengl.GLDrawableHelper.init(GLDrawableHelper.java:644) at jogamp.opengl.GLDrawableHelper.init(GLDrawableHelper.java:667) at com.jogamp.opengl.awt.GLCanvas$10.run(GLCanvas.java:1407) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1291) ... 16 more Caused by: java.io.IOException: Cannot run program \"nvcc\": CreateProcess error=2, The system cannot find the file specified at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at java.lang.Runtime.exec(Runtime.java:621) at java.lang.Runtime.exec(Runtime.java:451) at java.lang.Runtime.exec(Runtime.java:348) at jcuda.samples.utils.JCudaSamplesUtils.invokeNvcc(JCudaSamplesUtils.java:148) ... 23 more Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified at java.lang.ProcessImpl.create(Native Method) at java.lang.ProcessImpl.&lt;init&gt;(ProcessImpl.java:453) at java.lang.ProcessImpl.start(ProcessImpl.java:140) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ... 27 more Exception in thread \"AWT-EventQueue-0\" com.jogamp.opengl.GLException: Caught CudaException: CUDA_ERROR_INVALID_VALUE on thread AWT-EventQueue-0 at com.jogamp.opengl.GLException.newGLException(GLException.java:76) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1327) at jogamp.opengl.GLDrawableHelper.invokeGL(GLDrawableHelper.java:1147) at com.jogamp.opengl.awt.GLCanvas$12.run(GLCanvas.java:1438) at com.jogamp.opengl.Threading.invoke(Threading.java:223) at com.jogamp.opengl.awt.GLCanvas.display(GLCanvas.java:505) at com.jogamp.opengl.awt.GLCanvas.paint(GLCanvas.java:559) at sun.awt.RepaintArea.paintComponent(RepaintArea.java:264) at sun.awt.RepaintArea.paint(RepaintArea.java:240) at sun.awt.windows.WComponentPeer.handleEvent(WComponentPeer.java:358) at java.awt.Component.dispatchEventImpl(Component.java:4965) at java.awt.Component.dispatchEvent(Component.java:4711) at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:760) at java.awt.EventQueue.access$500(EventQueue.java:97) at java.awt.EventQueue$3.run(EventQueue.java:709) at java.awt.EventQueue$3.run(EventQueue.java:703) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:84) at java.awt.EventQueue$4.run(EventQueue.java:733) at java.awt.EventQueue$4.run(EventQueue.java:731) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.awt.EventQueue.dispatchEvent(EventQueue.java:730) at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:205) at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116) at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93) at java.awt.EventDispatchThread.run(EventDispatchThread.java:82) Caused by: jcuda.CudaException: CUDA_ERROR_INVALID_VALUE at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:396) at jcuda.driver.JCudaDriver.cuMemcpyHtoD(JCudaDriver.java:4797) at jcuda.driver.gl.samples.JCudaDriverVolumeRendererJOGL.display(JCudaDriverVolumeRendererJOGL.java:732) at jogamp.opengl.GLDrawableHelper.displayImpl(GLDrawableHelper.java:692) at jogamp.opengl.GLDrawableHelper.display(GLDrawableHelper.java:674) at com.jogamp.opengl.awt.GLCanvas$11.run(GLCanvas.java:1424) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1293) ... 28 more Exception in thread \"AWT-EventQueue-0\" com.jogamp.opengl.GLException: Caught CudaException: CUDA_ERROR_INVALID_VALUE on thread AWT-EventQueue-0 at com.jogamp.opengl.GLException.newGLException(GLException.java:76) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1327) at jogamp.opengl.GLDrawableHelper.invokeGL(GLDrawableHelper.java:1147) at com.jogamp.opengl.awt.GLCanvas$12.run(GLCanvas.java:1438) at com.jogamp.opengl.Threading.invoke(Threading.java:223) at com.jogamp.opengl.awt.GLCanvas.display(GLCanvas.java:505) at com.jogamp.opengl.awt.GLCanvas.paint(GLCanvas.java:559) at sun.awt.RepaintArea.paintComponent(RepaintArea.java:264) at sun.awt.RepaintArea.paint(RepaintArea.java:240) at sun.awt.windows.WComponentPeer.handleEvent(WComponentPeer.java:358) at java.awt.Component.dispatchEventImpl(Component.java:4965) at java.awt.Component.dispatchEvent(Component.java:4711) at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:760) at java.awt.EventQueue.access$500(EventQueue.java:97) at java.awt.EventQueue$3.run(EventQueue.java:709) at java.awt.EventQueue$3.run(EventQueue.java:703) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:84) at java.awt.EventQueue$4.run(EventQueue.java:733) at java.awt.EventQueue$4.run(EventQueue.java:731) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.awt.EventQueue.dispatchEvent(EventQueue.java:730) at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:205) at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116) at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93) at java.awt.EventDispatchThread.run(EventDispatchThread.java:82) Caused by: jcuda.CudaException: CUDA_ERROR_INVALID_VALUE at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:396) at jcuda.driver.JCudaDriver.cuMemcpyHtoD(JCudaDriver.java:4797) at jcuda.driver.gl.samples.JCudaDriverVolumeRendererJOGL.display(JCudaDriverVolumeRendererJOGL.java:732) at jogamp.opengl.GLDrawableHelper.displayImpl(GLDrawableHelper.java:692) at jogamp.opengl.GLDrawableHelper.display(GLDrawableHelper.java:674) at com.jogamp.opengl.awt.GLCanvas$11.run(GLCanvas.java:1424) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1293) ... 28 more I have all the required files in the right paths as you can see here below on the project printscreen: Anyone can explain to me what is wrong here (I did not add any code of mine anywhere, just original files coming in the examples zip file)? EDIT In this specific example, it looks like the problem is missing \"nvcc\" (whatever that is), so I was searching for that \"nvcc\", and I found out it is a part of the CUDA Toolkit 10 (latest version for Win7 x64 - my case) that needs to be downloaded (1.7GB) and installed on local machine, which I am dong right now, so I will update this post once I got it done and tested. But why there is no mention about this thing in JCuda examples is strange, one have to find out the hard way like this on his own, it seems. UPDATE After installing that CUDA Toolkit the previous error is gone, but new one pops up: nvcc fatal : Cannot find compiler 'cl.exe' in PATH Like,. seriously: are we required to have MS VisualStudio installed as well (several useless gigabytes, when we only need relatively tiny cl.exe) just to be able using JCuda for JAVA?! Cos it looks that cl.exe is part of that MS software and there is no other way how to obtain it...well, unless I am mistaken. UPDATE 2 I ended up downloading and installing the MSVC 2010 Express for Desktop Windows, which was the 1st one - oldest one, pretty hard to find actually (= smallest install files size, around 500MB + the minimum of not-needed garbage that comes with it, remember: I just needed cl.exe) - having integrated x64 files support (as I am compiling on Win7 x64 machine with x64 NetBeans. I also had to manually add path to cl.exe into system environment variable PATH (else it would still not be found despite having it installed, one would expect installer should set that sys variable automatically by itself, right?) + manually create new directory called amd64 inside the VC\\bin of the MSVC install directory and one simple bat file there too, else it would end up in another error this time complaining about Microsoft Visual Studio configuration file 'vcvars64.bat' could not be found for installation at 'C:/Program Files (x86)/Microsoft Visual Studio 11.0/VC/bin/ ( according to this post's 1st answer ) YET I AM STILL GETTING ANOTHER BUNCH OF COMPILING ERRORS (!!!), now it cannot find another files, specifically crtdefs.h, I don't know but I am slowly giving up any other trying...I really do wonder: is there abnyone in this world in these days that could actually successfully run those example files of JCuda?! Unbelievable... run: aug 13, 2021 9:20:53 PM jcuda.samples.utils.JCudaSamplesUtils invokeNvcc INFO: Creating ptx file for src/main/resources/kernels/JCudaDriverVolumeRendererKernel.cu aug 13, 2021 9:20:53 PM jcuda.samples.utils.JCudaSamplesUtils invokeNvcc INFO: Executing nvcc -m64 -ptx src/main/resources/kernels/JCudaDriverVolumeRendererKernel.cu -o src/main/resources/kernels/JCudaDriverVolumeRendererKernel.ptx aug 13, 2021 9:20:53 PM jcuda.samples.utils.JCudaSamplesUtils invokeNvcc SEVERE: nvcc process exitValue 2 aug 13, 2021 9:20:53 PM jcuda.samples.utils.JCudaSamplesUtils invokeNvcc SEVERE: errorMessage: c:\\program files\\nvidia gpu computing toolkit\\cuda\\v10.0\\include\\crt/host_config.h(219) : fatal error C1083: Cannot open include file: 'crtdefs.h': No such file or directory aug 13, 2021 9:20:53 PM jcuda.samples.utils.JCudaSamplesUtils invokeNvcc SEVERE: outputMessage: nvcc warning : nvcc support for Microsoft Visual Studio 2012 and earlier has been deprecated and is no longer being maintained JCudaDriverVolumeRendererKernel.cu Exception in thread \"AWT-EventQueue-0-AWTAnimator#00\" com.jogamp.opengl.util.AnimatorBase$UncaughtAnimatorException: java.lang.RuntimeException: com.jogamp.opengl.GLException: Caught CudaException: Could not create ptx file: c:\\program files\\nvidia gpu computing toolkit\\cuda\\v10.0\\include\\crt/host_config.h(219) : fatal error C1083: Cannot open include file: 'crtdefs.h': No such file or directory on thread AWT-EventQueue-0 at com.jogamp.opengl.util.AWTAnimatorImpl.display(AWTAnimatorImpl.java:92) at com.jogamp.opengl.util.AnimatorBase.display(AnimatorBase.java:452) at com.jogamp.opengl.util.Animator$MainLoop.run(Animator.java:204) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: com.jogamp.opengl.GLException: Caught CudaException: Could not create ptx file: c:\\program files\\nvidia gpu computing toolkit\\cuda\\v10.0\\include\\crt/host_config.h(219) : fatal error C1083: Cannot open include file: 'crtdefs.h': No such file or directory on thread AWT-EventQueue-0 at com.jogamp.common.util.awt.AWTEDTExecutor.invoke(AWTEDTExecutor.java:58) at jogamp.opengl.awt.AWTThreadingPlugin.invokeOnOpenGLThread(AWTThreadingPlugin.java:103) at jogamp.opengl.ThreadingImpl.invokeOnOpenGLThread(ThreadingImpl.java:201) at com.jogamp.opengl.Threading.invokeOnOpenGLThread(Threading.java:202) at com.jogamp.opengl.Threading.invoke(Threading.java:221) at com.jogamp.opengl.awt.GLCanvas.display(GLCanvas.java:505) at com.jogamp.opengl.util.AWTAnimatorImpl.display(AWTAnimatorImpl.java:81) ... 3 more Caused by: com.jogamp.opengl.GLException: Caught CudaException: Could not create ptx file: c:\\program files\\nvidia gpu computing toolkit\\cuda\\v10.0\\include\\crt/host_config.h(219) : fatal error C1083: Cannot open include file: 'crtdefs.h': No such file or directory on thread AWT-EventQueue-0 at com.jogamp.opengl.GLException.newGLException(GLException.java:76) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1327) at jogamp.opengl.GLDrawableHelper.invokeGL(GLDrawableHelper.java:1147) at com.jogamp.opengl.awt.GLCanvas$12.run(GLCanvas.java:1438) at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:301) at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:758) at java.awt.EventQueue.access$500(EventQueue.java:97) at java.awt.EventQueue$3.run(EventQueue.java:709) at java.awt.EventQueue$3.run(EventQueue.java:703) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.awt.EventQueue.dispatchEvent(EventQueue.java:728) at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:205) at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116) at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93) at java.awt.EventDispatchThread.run(EventDispatchThread.java:82) Caused by: jcuda.CudaException: Could not create ptx file: c:\\program files\\nvidia gpu computing toolkit\\cuda\\v10.0\\include\\crt/host_config.h(219) : fatal error C1083: Cannot open include file: 'crtdefs.h': No such file or directory at jcuda.samples.utils.JCudaSamplesUtils.invokeNvcc(JCudaSamplesUtils.java:170) at jcuda.samples.utils.JCudaSamplesUtils.preparePtxFile(JCudaSamplesUtils.java:51) at jcuda.driver.gl.samples.JCudaDriverVolumeRendererJOGL.initCuda(JCudaDriverVolumeRendererJOGL.java:476) at jcuda.driver.gl.samples.JCudaDriverVolumeRendererJOGL.init(JCudaDriverVolumeRendererJOGL.java:455) at jogamp.opengl.GLDrawableHelper.init(GLDrawableHelper.java:644) at jogamp.opengl.GLDrawableHelper.init(GLDrawableHelper.java:667) at com.jogamp.opengl.awt.GLCanvas$10.run(GLCanvas.java:1407) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1291) ... 16 more Exception in thread \"AWT-EventQueue-0\" com.jogamp.opengl.GLException: Caught CudaException: CUDA_ERROR_INVALID_VALUE on thread AWT-EventQueue-0 at com.jogamp.opengl.GLException.newGLException(GLException.java:76) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1327) at jogamp.opengl.GLDrawableHelper.invokeGL(GLDrawableHelper.java:1147) at com.jogamp.opengl.awt.GLCanvas$12.run(GLCanvas.java:1438) at com.jogamp.opengl.Threading.invoke(Threading.java:223) at com.jogamp.opengl.awt.GLCanvas.display(GLCanvas.java:505) at com.jogamp.opengl.awt.GLCanvas.paint(GLCanvas.java:559) at sun.awt.RepaintArea.paintComponent(RepaintArea.java:264) at sun.awt.RepaintArea.paint(RepaintArea.java:240) at sun.awt.windows.WComponentPeer.handleEvent(WComponentPeer.java:358) at java.awt.Component.dispatchEventImpl(Component.java:4965) at java.awt.Component.dispatchEvent(Component.java:4711) at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:760) at java.awt.EventQueue.access$500(EventQueue.java:97) at java.awt.EventQueue$3.run(EventQueue.java:709) at java.awt.EventQueue$3.run(EventQueue.java:703) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:84) at java.awt.EventQueue$4.run(EventQueue.java:733) at java.awt.EventQueue$4.run(EventQueue.java:731) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.awt.EventQueue.dispatchEvent(EventQueue.java:730) at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:205) at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116) at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93) at java.awt.EventDispatchThread.run(EventDispatchThread.java:82) Caused by: jcuda.CudaException: CUDA_ERROR_INVALID_VALUE at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:396) at jcuda.driver.JCudaDriver.cuMemcpyHtoD(JCudaDriver.java:4797) at jcuda.driver.gl.samples.JCudaDriverVolumeRendererJOGL.display(JCudaDriverVolumeRendererJOGL.java:732) at jogamp.opengl.GLDrawableHelper.displayImpl(GLDrawableHelper.java:692) at jogamp.opengl.GLDrawableHelper.display(GLDrawableHelper.java:674) at com.jogamp.opengl.awt.GLCanvas$11.run(GLCanvas.java:1424) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1293) ... 28 more Exception in thread \"AWT-EventQueue-0\" com.jogamp.opengl.GLException: Caught CudaException: CUDA_ERROR_INVALID_VALUE on thread AWT-EventQueue-0 at com.jogamp.opengl.GLException.newGLException(GLException.java:76) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1327) at jogamp.opengl.GLDrawableHelper.invokeGL(GLDrawableHelper.java:1147) at com.jogamp.opengl.awt.GLCanvas$12.run(GLCanvas.java:1438) at com.jogamp.opengl.Threading.invoke(Threading.java:223) at com.jogamp.opengl.awt.GLCanvas.display(GLCanvas.java:505) at com.jogamp.opengl.awt.GLCanvas.paint(GLCanvas.java:559) at sun.awt.RepaintArea.paintComponent(RepaintArea.java:264) at sun.awt.RepaintArea.paint(RepaintArea.java:240) at sun.awt.windows.WComponentPeer.handleEvent(WComponentPeer.java:358) at java.awt.Component.dispatchEventImpl(Component.java:4965) at java.awt.Component.dispatchEvent(Component.java:4711) at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:760) at java.awt.EventQueue.access$500(EventQueue.java:97) at java.awt.EventQueue$3.run(EventQueue.java:709) at java.awt.EventQueue$3.run(EventQueue.java:703) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:84) at java.awt.EventQueue$4.run(EventQueue.java:733) at java.awt.EventQueue$4.run(EventQueue.java:731) at java.security.AccessController.doPrivileged(Native Method) at java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:74) at java.awt.EventQueue.dispatchEvent(EventQueue.java:730) at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:205) at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:116) at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:105) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:93) at java.awt.EventDispatchThread.run(EventDispatchThread.java:82) Caused by: jcuda.CudaException: CUDA_ERROR_INVALID_VALUE at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:396) at jcuda.driver.JCudaDriver.cuMemcpyHtoD(JCudaDriver.java:4797) at jcuda.driver.gl.samples.JCudaDriverVolumeRendererJOGL.display(JCudaDriverVolumeRendererJOGL.java:732) at jogamp.opengl.GLDrawableHelper.displayImpl(GLDrawableHelper.java:692) at jogamp.opengl.GLDrawableHelper.display(GLDrawableHelper.java:674) at com.jogamp.opengl.awt.GLCanvas$11.run(GLCanvas.java:1424) at jogamp.opengl.GLDrawableHelper.invokeGLImpl(GLDrawableHelper.java:1293) ... 28 more",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm writing a program to compare the speed of JBlas and JCublas. When I call the below function the first time, everything works fine and v contains the correct eigenvectors. When I call it a second time, it takes a lot less time to compute but only returns the inputted symmetric matrix a, as if d_A's value was never changed. It seems that the function only works as expected on odd-numbered calls. I have a hunch that this error is due to something in the GPU memory not getting cleared properly, but I can't find it. public static void getSymEigenGPU(cusolverDnHandle handle, DoubleMatrix a) { int n2 = a.length; int n = a.rows; double[] a1d = to1d(a); double[] v = new double[n2]; double[] w = new double[n]; Pointer h_A = Pointer.to(a1d); Pointer h_V = Pointer.to(v); Pointer h_W = Pointer.to(w); Pointer d_A = new Pointer(); Pointer d_V = new Pointer(); Pointer d_W = new Pointer(); Pointer d_work = new Pointer(); JCuda.cudaMalloc(d_A, (long) n2 * Sizeof.DOUBLE); JCuda.cudaMalloc(d_V, (long) n2 * Sizeof.DOUBLE); JCuda.cudaMalloc(d_W, n * Sizeof.DOUBLE); int jobz = CUSOLVER_EIG_MODE_VECTOR; int uplo = CUBLAS_FILL_MODE_UPPER; JCuda.cudaMemcpy(d_A, h_A, (long) n2 * Sizeof.DOUBLE, cudaMemcpyHostToDevice); int[] lworkl = new int[1]; JCusolverDn.cusolverDnDsyevd_bufferSize(handle, jobz, uplo, n, d_A, n, d_W, lworkl); int lwork = lworkl[0]; JCuda.cudaMalloc(d_work, (long) lwork * Sizeof.DOUBLE); NanoStopWatch sw = NanoStopWatch.sw(); JCusolverDn.cusolverDnDsyevd(handle, jobz, uplo, n, d_A, n, d_W, d_work, n2, new Pointer()); System.out.println(\"sw.stop() = \" + sw.stop()); JCuda.cudaMemcpy(h_W, d_W, Sizeof.DOUBLE * n, cudaMemcpyDeviceToHost); JCuda.cudaMemcpy(h_V, d_A, (long) Sizeof.DOUBLE * n2, cudaMemcpyDeviceToHost); pp(from1d(v)); JCuda.cudaFree(d_A); JCuda.cudaFree(d_V); JCuda.cudaFree(d_W); JCuda.cudaFree(d_work); }",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a rather strange observation on the following code snippet. When I do both - copy memory to device and copy results back to host the streams seem to be synronized - i.e. they execute the kernel sequentially. Once I remove the copy to host and keep copy the parameters to the device the streams execute in parallel, once I remove copying the parameters and keep copying the results the streams also execute in parallel. Any Idea why? and how to solve the problem? for (int j=0; j&lt;n_streams; j++) { cuMemcpyHtoDAsync(gpu_parameters[j], parameters[j].asPointer(), (parameterCount) * Sizeof.FLOAT, stream[j]); Pointer kernelParameters1 = Pointer.to( Pointer.to(new int[]{0}), Pointer.to(new int[] {10000}), Pointer.to(gpu_data), Pointer.to(gpu_results[j]), Pointer.to(gpu_parameters[j]) ); cuLaunchKernel(function[j], s_grid, 1, 1, // Grid dimension s_block, 1, 1, // Block dimension 0, stream[j], // Shared memory size and stream kernelParameters1, null // Kernel- and extra parameters ); cuMemcpyDtoHAsync(results[j].asPointer(), gpu_results[j], (results[j].size()) * Sizeof.FLOAT, stream[j]); }",
        "answers": [
            [
                "No Idea why ... but changing the sequence removed the problem - and is executing in parallel.... for (int j=0; j&lt;n_streams; j++) { cuMemcpyHtoDAsync(gpu_parameters[j], parameters[j].asPointer(), (parameterCount) * Sizeof.FLOAT, stream[j]); } for (int j=0; j&lt;n_streams; j++) { Pointer kernelParameters1 = Pointer.to( Pointer.to(new int[]{0}), Pointer.to(new int[] {getNPrices()}), Pointer.to(get_gpu_prices()), Pointer.to(gpu_results[j]), Pointer.to(gpu_parameters[j]) //,Pointer.to(new int[]{0}) ); cuLaunchKernel(function[j], s_grid, 1, 1, // Grid dimension s_block, 1, 1, // Block dimension 0, stream[j], // Shared memory size and stream kernelParameters1, null // Kernel- and extra parameters ); } for (int j=0; j&lt;n_streams; j++) { cuMemcpyDtoHAsync(results[j].asPointer(), gpu_results[j], (results[j].size()) * Sizeof.FLOAT, stream[j]); }"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "When I call the function, cuLaunchKernel(), my program crashes with the error, CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES. Is this because I'm passing 29 parameters to my kernel? If so, is there any work around? I am pretty certain that I need these parameters for calculations. Pointer kernelParams = Pointer.to( Pointer.to(new int[] {n}), Pointer.to(new int[] {xRes}), Pointer.to(new int[] {yRes}), Pointer.to(new double[] {camX}), Pointer.to(new double[] {camY}), Pointer.to(new double[] {camZ}), Pointer.to(new double[] {camforX}), Pointer.to(new double[] {camforY}), Pointer.to(new double[] {camforZ}), Pointer.to(new double[] {camupX}), Pointer.to(new double[] {camupY}), Pointer.to(new double[] {camupZ}), Pointer.to(new double[] {fov}), Pointer.to(new double[] {aspectRatio}), Pointer.to(in14), Pointer.to(in15), Pointer.to(in16), Pointer.to(in17), Pointer.to(in18), Pointer.to(in19), Pointer.to(in20), Pointer.to(in21), Pointer.to(in22), Pointer.to(new double[] {meshposX}), Pointer.to(new double[] {meshposY}), Pointer.to(new double[] {meshposZ}), Pointer.to(out0), Pointer.to(out1), Pointer.to(out2) ); cuLaunchKernel(function, 32,32,1, 32,32,1, 0,null, kernelParams,null );",
        "answers": [
            [
                "Here is my naive answer: As suggested in comments, the problem was to do with the block size. Reducing the block size fixed the error. After reducing the block size, I received the error: CUDA_ERROR_ILLEGAL_ADDRESS. Reducing the grid size solved that problem."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I work on the code from JCuda documentation. Currently, it's just adding vectors on GPU. What should I do to reuse function add on CPU (host)? I know that, I have to change __global__ to __host__ __device__ but I have no idea how can I call it in my main function. I suspect that I have to use another nvcc option. My goal is to run this same function on GPU and CPU and check execution time (I know how to check it). .cu file (compiled with nvcc -ptx file.cu -o file.ptx extern \"C\" __global__ void add(int n, float *a, float *b, float *sum) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i&lt;n) { sum[i] = a[i] + b[i]; } } fragment of main function public static void main(String[] args) { cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); CUmodule module = new CUmodule(); cuModuleLoad(module, \"kernels/JCudaVectorAdd.ptx\"); CUfunction function = new CUfunction(); cuModuleGetFunction(function, module, \"add\"); ... Pointer kernelParameters = Pointer.to( Pointer.to(new int[]{numElements}), Pointer.to(deviceInputA), Pointer.to(deviceInputB), Pointer.to(deviceOutput) );",
        "answers": [
            [
                "You can't and probably will never be able do this in JCUDA, because of the API interface it uses to interact with CUDA. While CUDA can now \"launch\" a host function into a stream, that API isn't exposed by JCUDA at present, and it wouldn't work the way that device code does now (this restriction would apply to PyCUDA and other driver API based frameworks as well). You would likely need use JNI or some other way to retrieve the host function from a library and call it that way."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm migrating my app from Java 1.8 to Java 11, got the following error on one of my external jar at runtime. Error occurred during initialization of boot layer java.lang.module.ResolutionException: Module image.viewer contains package jcuda, module jcuda exports package jcuda to image.viewer Here is my Module.info class module image.viewer { exports com.viewer; exports com.viewer.backup; exports com.viewer.backup.analyse; exports com.viewer.compare; exports com.viewer.controls; exports com.viewer.database; exports com.viewer.discovery; exports com.viewer.events; exports com.viewer.exif; exports com.viewer.exportWorker; exports com.viewer.external; exports com.viewer.faceidentification; exports com.viewer.gpuworker; exports com.viewer.importers; exports com.viewer.keypoints; exports com.viewer.location; exports com.viewer.model; exports com.viewer.panes; exports com.viewer.pool; exports com.viewer.referenceDatabase; exports com.viewer.registration; exports com.viewer.renderers; exports com.viewer.report; exports com.viewer.search; exports com.viewer.search.imfilters; exports com.viewer.statistics; exports com.viewer.surf; exports com.viewer.tags; exports com.viewer.thumbnail; exports com.viewer.thumbnails; exports com.viewer.util; exports com.viewer.worker; exports com.viewer.xmlWorker; requires commons.dbcp; requires ehcache; requires eventbus; requires transitive java.desktop; requires java.sql; requires java.activation; requires java.xml; requires java.xml.bind; requires jna; requires logback.classic; requires logback.core; requires lucene.core; requires miglayout.swing; requires slf4j.api; requires snakeyaml; requires com.fasterxml.jackson.core; requires com.fasterxml.jackson.databind; requires commons.io; requires commons.lang3; requires jdk.unsupported; requires jgoodies.forms; requires commons.collections4; requires java.prefs; requires jfreechart; requires synthetica.base; requires synthetica.theme.aluoxide; requires synthetica.theme.blackeye; requires synthetica.theme.dark; requires synthetica.theme.plain; requires WMI4Java; requires jdk.security.auth; requires commons.cli; requires synthetica.addons.swingx; requires synthetica.addons.base; requires exiftool.lib; requires javacsv; requires poi; requires poi.ooxml; requires poi.scratchpad; requires xmlbeans; requires itextpdf; requires mapsforge.core; requires mapsforge.map; requires mapsforge.map.awt; requires mapsforge.map.reader; requires opencv; requires image.group.mk5; requires image.face.age; requires image.object; requires image.zoning; requires roi.detector; requires rot.profile.age; requires zt.zip; requires webp.io; requires opencsv; requires psd.analizer; requires xmlworker; requires java.management; requires jdk.management; requires batik.dom; requires batik.transcoder; requires jackson.annotations; requires org.glassfish.java.json; requires gson; requires OOXML; requires image.jcuda; opens com.viewer.model to snakeyaml, java.xml.bin; }",
        "answers": [],
        "votes": []
    },
    {
        "question": "My java project is working properly on classical architecture, i.e. Von Neumann architecture. However, maven is unable to resolve dependencies on my Jetson AGX, i.e. ARM architecture. What do I need to change to compile my project on the ARM architecture? Are the corresponding packages available on maven? The pom.xml: &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt; &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt; &lt;classifier&gt;aarch64&lt;/classifier&gt; &lt;version&gt;1.0.0-beta4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.nd4j&lt;/groupId&gt; &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt; &lt;classifier&gt;aarch64&lt;/classifier&gt; &lt;version&gt;1.0.0-beta4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.mail&lt;/groupId&gt; &lt;artifactId&gt;mail&lt;/artifactId&gt; &lt;classifier&gt;aarch64&lt;/classifier&gt; &lt;version&gt;1.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt; &lt;classifier&gt;aarch64&lt;/classifier&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jcuda&lt;/groupId&gt; &lt;artifactId&gt;jcuda&lt;/artifactId&gt; &lt;classifier&gt;aarch64&lt;/classifier&gt; &lt;version&gt;10.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;classifier&gt;aarch64&lt;/classifier&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;classifier&gt;aarch64&lt;/classifier&gt; &lt;version&gt;4.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; The following error shows up: Downloading from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-api/maven-metadata.xml Downloaded from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-api/maven-metadata.xml (1.5 kB at 577 B/s) Downloading from central: https://repo.maven.apache.org/maven2/junit/junit/maven-metadata.xml Downloaded from central: https://repo.maven.apache.org/maven2/junit/junit/maven-metadata.xml (1.1 kB at 2.2 kB/s) Downloading from central: https://repo.maven.apache.org/maven2/org/nd4j/nd4j-native-platform/1.0.0-beta4/nd4j-native-platform-1.0.0-beta4-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/org/deeplearning4j/deeplearning4j-core/1.0.0-beta4/deeplearning4j-core-1.0.0-beta4-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/org/nd4j/nd4j-native/1.0.0-beta4/nd4j-native-1.0.0-beta4-linux-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/org/bytedeco/openblas/0.3.5-1.5/openblas-0.3.5-1.5-linux-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/org/bytedeco/mkl/2019.3-1.5/mkl-2019.3-1.5-linux-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/org/bytedeco/mkl-dnn/0.18.1-1.5/mkl-dnn-0.18.1-1.5-linux-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/javax/mail/mail/1.4/mail-1.4-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/org/junit/jupiter/junit-jupiter-api/5.5.1/junit-jupiter-api-5.5.1-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/org/jcuda/jcuda/10.1.0/jcuda-10.1.0-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/org/jcuda/jcuda-natives/10.1.0/jcuda-natives-10.1.0-linux-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/junit/junit/4.13-beta-3/junit-4.13-beta-3-aarch64.jar Downloading from central: https://repo.maven.apache.org/maven2/com/esotericsoftware/kryo/4.0.1/kryo-4.0.1-aarch64.jar Downloading from maven-restlet: http://maven.restlet.org/org/nd4j/nd4j-native/1.0.0-beta4/nd4j-native-1.0.0-beta4-linux-aarch64.jar Downloading from maven-restlet: http://maven.restlet.org/org/bytedeco/openblas/0.3.5-1.5/openblas-0.3.5-1.5-linux-aarch64.jar Downloading from maven-restlet: http://maven.restlet.org/org/bytedeco/mkl-dnn/0.18.1-1.5/mkl-dnn-0.18.1-1.5-linux-aarch64.jar Downloading from maven-restlet: http://maven.restlet.org/org/bytedeco/mkl/2019.3-1.5/mkl-2019.3-1.5-linux-aarch64.jar [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Total time: 18.554 s [INFO] Finished at: 2019-08-15T10:07:47+02:00 [INFO] ------------------------------------------------------------------------ [ERROR] Failed to execute goal on project cnn-cpca-kwta: Could not resolve dependencies for project com.github.champib:cnn-cpca-kwta:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.deeplearning4j:deeplearning4j-core:jar:aarch64:1.0.0-beta4, org.nd4j:nd4j-native-platform:jar:aarch64:1.0.0-beta4, org.nd4j:nd4j-native:jar:linux-aarch64:1.0.0-beta4, org.bytedeco:openblas:jar:linux-aarch64:0.3.5-1.5, org.bytedeco:mkl:jar:linux-aarch64:2019.3-1.5, org.bytedeco:mkl-dnn:jar:linux-aarch64:0.18.1-1.5, javax.mail:mail:jar:aarch64:1.4, org.junit.jupiter:junit-jupiter-api:jar:aarch64:RELEASE, org.jcuda:jcuda:jar:aarch64:10.1.0, org.jcuda:jcuda-natives:jar:linux-aarch64:10.1.0, junit:junit:jar:aarch64:RELEASE, com.esotericsoftware:kryo:jar:aarch64:4.0.1: Could not find artifact org.deeplearning4j:deeplearning4j-core:jar:aarch64:1.0.0-beta4 in central (https://repo.maven.apache.org/maven2) -&gt; [Help 1]",
        "answers": [
            [
                "Try removing your local m2 directory completely and run mvn clean install OR try \" mvn clean install -U\" command, This will force update the snapshot dependency ."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have defined a new data type like this in GPU side (cuda): typedef union { int i; double d; long l; char s[16]; } data_unit; data_unit *d_array; And in Java, we have an array of one of the kinds available in the defined union. Normally, we can do the following in Java (JCuda) if we have an array of int type for example: import static jcuda.driver.JCudaDriver.*; int data_size; CUdeviceptr d_array; int[] h_array = new int[data_size]; cuMemAlloc(d_array, data_size * Sizeof.INT); cuMemcpyHtoD(d_array, Pointer.to(h_array), data_size * Sizeof.INT); But how can it be done if there is an array on device that its type is our union? (assume that still the h_array is of the type int) int data_size; CUdeviceptr d_array; int[] h_array = new int[data_size]; cuMemAlloc(d_array, data_size * Sizeof.?); // Here we should have some type of alignment (?) cuMemcpyHtoD(d_array, Pointer.to(h_array), data_size * Sizeof.?);",
        "answers": [
            [
                "I believe there is a fundamental misunderstanding of what a union is. Lets think about it. What makes a union different from a struct? It can store different types of data at different times. How does it accomplish this feat? Well one could use some sort of separate variable to dynamically specify the type or how much memory it takes up, but a Union does not do this, it relies on the programmer knowing exactly what type they want to retrieve and when. So the only alternative, if the type is only actually known by the programmer at any given point in time, is to merely make sure there is enough space allocated for your union variable that one could always use it for what ever type. Indeed, this is what a union does, see here (yes I know it is C/C++, but this also applies to CUDA as well). What does that mean for you? It means that the size of your union array should be the size of its largest member x the number of elements, since the size of a union is the size of its largest member. Lets look at your union to see how to figure it out. typedef union { int i; double d; long l; char s[16]; } data_unit; Your union has: int i, which we assume to be 4 bytes double d, which is 8 bytes long l, which is confusing because depending on the compiler/platform can either be 4 or 8 bytes, we assume 8 bytes for now. char s[16], easy, 16 bytes So the largest number of bytes any member takes up is your char s[16] variable, 16 bytes. This means that you will need to change your code to: int data_size; int union_size = 16; CUdeviceptr d_array; // copying this to the device will not result in what you expect with out over allocating // if you just copy over integers, which occupy 4 bytes each, your integers will fill less space than the number of unions // we need to make sure that there is a \"stride\" here if we want to actually copy real data from host to device. // union_size / Sizeof.INT = 4, so there will be 4 x as many ints, 4 for each union. int[] h_array = new int[data_size * (union_size / Sizeof.INT)]; // here we aren't looking for size of int to allocate, but the size of our union. cuMemAlloc(d_array, data_size * union_size); // we are copying, again, data_size * union_size bytes cuMemcpyHtoD(d_array, Pointer.to(h_array), data_size * union_size); NOTE If you want to copy ints over, this basically means you will need to assign every 4th int to the actual int you want for that index. int 0 is h_array[0], int 1 is h_array[4] int 2 is h_array[8] int n is h_array[n * 4] etc.."
            ],
            [
                "I did the alignment and padding with a little bit of dirty coding. Also, it is important to pay attention to byte order difference between compilers. Java seems to store bytes in BIG_ENDIAN format. So here I had to change it to LITTLE_ENDIAN to get it done. It took me 2 hours to debug. This is how it looks now: int data_size; int union_size = 16; // Device Array CUdeviceptr d_array; // Host Array int[] h_array = new int[data_size]; byte[] h_array_bytes = new byte[data_size * union_size]; // Data allocation on GPU memory cuMemAlloc(d_array, data_size * union_size); // Alignment and padding byte[] tempBytes; for(int i = 0; i &lt; data_size; i++){ tempBytes = ByteBuffer.allocate(Integer.BYTES).order(ByteOrder.LITTLE_ENDIAN) .putInteger(h_array[i]).array(); int start = i * union_size; for(int j = start, k = 0; k &lt; union_size; k++, j++){ if(k &lt; tempBytes.length){ h_array_bytes[j] = tempBytes[k]; } else { h_array_bytes[j] = 0; } } } // And then simply do the copy cuMemcpyHtoD(d_array, Pointer.to(h_array_bytes), data_size * union_size);"
            ]
        ],
        "votes": [
            5.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am trying to set up JCuda and execute a Sample addition kernel. When I try to execute JCudaVectorAdd after compiling JCudaVectorAdd.java, I am getting the following error: Exception in thread \"main\" java.lang.NoClassDefFoundError: jcuda/driver/JCudaDriver at JCudaVectorAdd.main(JCudaVectorAdd.java:38) Caused by: java.lang.ClassNotFoundException: jcuda.driver.JCudaDriver at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 1 more I have created a bash file with the steps I have performed to get to where I am: #!/bin/bash # This system has multiple Cuda instances, so we need to load the correct one module load cuda-9.2 # Try to remove any jcuda zip files that may have been created previously rm $HOME/jcuda.zip # Get jcuda zip file from online, storing into $HOME directory wget http://www.jcuda.org/downloads/JCuda-All-0.9.2.zip -O $HOME/jcuda.zip # Remove the 0.9.2 directory in case it exists to get ready for a clean install rm -rf $HOME/jcuda/JCuda-All-0.9.2 # Unzip the file and store within jcuda directory. NOTE: The version number will be maintained as jcuda/JCuda-ALL-0.9.2.zip, so multiple versions of jcuda can be installed using this script unzip $HOME/jcuda.zip -d $HOME/jcuda # Remove the zipped file now that it is no longer needed rm $HOME/jcuda.zip # Move into the newly create jcuda directory cd $HOME/jcuda/JCuda-All-0.9.2/ # Get the example Main program for Vector addition from Jcuda site wget http://www.jcuda.org/samples/JCudaVectorAdd.java # Create a sample kernel echo 'extern \"C\"' &gt; JCudaVectorAddKernel.cu echo '__global__ void add(int n, float *a, float *b, float *sum)' &gt;&gt; JCudaVectorAddKernel.cu echo '{' &gt;&gt; JCudaVectorAddKernel.cu echo ' int i = blockIdx.x * blockDim.x + threadIdx.x;' &gt;&gt; JCudaVectorAddKernel.cu echo ' if (i&lt;n)' &gt;&gt; JCudaVectorAddKernel.cu echo ' {' &gt;&gt; JCudaVectorAddKernel.cu echo ' sum[i] = a[i] + b[i];' &gt;&gt; JCudaVectorAddKernel.cu echo ' }' &gt;&gt; JCudaVectorAddKernel.cu echo '}' &gt;&gt; JCudaVectorAddKernel.cu # Create a .ptx file from the cuda kernel to be consumed by the Main java program later # The sample Main program also performs this action, but we have it here as well nvcc -ptx JCudaVectorAddKernel.cu -o JCudaVectorAddKernel.ptx # Try to generate a class file from the example Main .java file javac -cp \".:jcuda-0.9.2.jar:jcuda-natives-0.9.2-linux-x86_64.jar\" JCudaVectorAdd.java # Run the compiled executable java JCudaVectorAdd It appears that I am missing a step somewhere, although I am not familiar enough with java or cuda/jcuda to identify what I am missing. Can anyone point me in the direction of how to resolve this issue, whether it be additional steps are modifying the steps I have performed? EDIT: It appears that I still need to reference the other .jar files in my execution (like they do in http://www.jcuda.org/tutorial/TutorialIndex.html basic test), so my last command may have been wrong. Changing it to the following has shown slightly different results: java JCudaVectorAdd -&gt; java -cp \".:jcuda-0.9.2.jar:jcuda-natives-0.9.2-linux-x86_64.jar\" JCudaVectorAdd Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Error while loading native library \"JCudaDriver-0.9.2-linux-x86_64\" Operating system name: Linux Architecture : amd64 Architecture bit size: 64 ---(start of nested stack traces)--- Stack trace from the attempt to load the library as a file: java.lang.UnsatisfiedLinkError: no JCudaDriver-0.9.2-linux-x86_64 in java.library.path at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867) at java.lang.Runtime.loadLibrary0(Runtime.java:870) at java.lang.System.loadLibrary(System.java:1122) at jcuda.LibUtils.loadLibrary(LibUtils.java:143) at jcuda.driver.JCudaDriver.&lt;clinit&gt;(JCudaDriver.java:296) at JCudaVectorAdd.main(JCudaVectorAdd.java:38) Stack trace from the attempt to load the library as a resource: java.lang.UnsatisfiedLinkError: /tmp/libJCudaDriver-0.9.2-linux-x86_64.so: libcuda.so.1: cannot open shared object file: No such file or directory at java.lang.ClassLoader$NativeLibrary.load(Native Method) at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) at java.lang.Runtime.load0(Runtime.java:809) at java.lang.System.load(System.java:1086) at jcuda.LibUtils.loadLibraryResource(LibUtils.java:260) at jcuda.LibUtils.loadLibrary(LibUtils.java:158) at jcuda.driver.JCudaDriver.&lt;clinit&gt;(JCudaDriver.java:296) at JCudaVectorAdd.main(JCudaVectorAdd.java:38) ---(end of nested stack traces)--- at jcuda.LibUtils.loadLibrary(LibUtils.java:193) at jcuda.driver.JCudaDriver.&lt;clinit&gt;(JCudaDriver.java:296) at JCudaVectorAdd.main(JCudaVectorAdd.java:38)",
        "answers": [
            [
                "It seems you need to tell the JRE where to find the corresponding OS library to run cuda. If you look at the output, it's stating it cannot find libcuda.so.1. Find where in your filesystem these libraries are, and set your java.library.path property to point there. java -Djava.library.path=/path/to/cudalibdir/ &lt;rest of commandline&gt; If your system does not have the library, you might need to install it or compile it locally. Disclaimer: I do not have experience with cuda specifically but have worked with other jars that require OS level libraries to function."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I want to compile and run my jcuda codes in an IDE (Netbeans) on ubuntu 16.04. I've already installed Netbeans, Maven, java and cuda 8.0. For example, I want to run sample of vector add that the java file is JCudaVectorAdd.java and the cuda kernel is JCudaVectorAddKernel.cu. How can I compile and run this simple example using Netbeans?",
        "answers": [
            [
                "You can create a maven project and add dependencies to the pom.xml file of the project: &lt;dependency&gt; &lt;groupId&gt;org.jcuda&lt;/groupId&gt; &lt;artifactId&gt;jcuda&lt;/artifactId&gt; &lt;version&gt;0.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jcuda&lt;/groupId&gt; &lt;artifactId&gt;jcublas&lt;/artifactId&gt; &lt;version&gt;0.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jcuda&lt;/groupId&gt; &lt;artifactId&gt;jcufft&lt;/artifactId&gt; &lt;version&gt;0.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jcuda&lt;/groupId&gt; &lt;artifactId&gt;jcusparse&lt;/artifactId&gt; &lt;version&gt;0.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jcuda&lt;/groupId&gt; &lt;artifactId&gt;jcusolver&lt;/artifactId&gt; &lt;version&gt;0.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jcuda&lt;/groupId&gt; &lt;artifactId&gt;jcurand&lt;/artifactId&gt; &lt;version&gt;0.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jcuda&lt;/groupId&gt; &lt;artifactId&gt;jnvgraph&lt;/artifactId&gt; &lt;version&gt;0.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jcuda&lt;/groupId&gt; &lt;artifactId&gt;jcudnn&lt;/artifactId&gt; &lt;version&gt;0.9.2&lt;/version&gt; &lt;/dependency&gt; then add the JCudaVectorAdd.java to the project and give the address of JCudaVectorAddKernel.cu file in the java file: String ptxFileName = preparePtxFile(\"JCudaVectorAddKernel.cu\"); now you can build your project successfully."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Im calculting skintone of an image in java. convert the pixel of Image in yCbCR. check if image pixel is in specific range, then its a skin color. calculate percentage by dividing it by total pixel. Its working fine in CPU code, but when i convert it to GPU code, The pixel percentage is not coming right. The confusing part for me was send the pixel data to GPU and get its r, g, b value in GPU. So i follow JCuda Pixel Invert Example example to send pixel data. The difference is the example send pixel data in int[] array and I'm sending it in byte[] array. Here the code. import static jcuda.driver.JCudaDriver.cuCtxCreate; import static jcuda.driver.JCudaDriver.cuCtxSynchronize; import static jcuda.driver.JCudaDriver.cuDeviceGet; import static jcuda.driver.JCudaDriver.cuInit; import static jcuda.driver.JCudaDriver.cuLaunchKernel; import static jcuda.driver.JCudaDriver.cuMemAlloc; import static jcuda.driver.JCudaDriver.cuMemFree; import static jcuda.driver.JCudaDriver.cuMemcpyDtoH; import static jcuda.driver.JCudaDriver.cuMemcpyHtoD; import java.awt.image.BufferedImage; import java.awt.image.DataBuffer; import java.awt.image.DataBufferByte; import java.awt.image.Raster; import java.io.File; import java.io.IOException; import javax.imageio.ImageIO; import ij.IJ; import jcuda.Pointer; import jcuda.Sizeof; import jcuda.driver.CUcontext; import jcuda.driver.CUdevice; import jcuda.driver.CUdeviceptr; import jcuda.driver.CUfunction; import jcuda.driver.JCudaDriver; import jcuda.nvrtc.JNvrtc; public class SkinTone { public static void CalculateSKintoneGPU(File file) throws IOException { BufferedImage bufferedImage = ImageIO.read(file); if (bufferedImage == null || bufferedImage.getData() == null) return; Raster raster = bufferedImage.getData(); DataBuffer dataBuffer = raster.getDataBuffer(); DataBufferByte dataBufferInt = (DataBufferByte)dataBuffer; byte[] pixels = dataBufferInt.getData(); int totalPixels = raster.getHeight() * raster.getWidth(); CUfunction kernelFunction = initlize(); int output[] = execute(kernelFunction, pixels, raster.getWidth(), raster.getHeight()); // Flushing memory raster = null; bufferedImage.flush(); bufferedImage = null; long skintoneThreshold = Math.round(output[0] / (double) totalPixels * 100.0); System.err.println(\"Skintone Using GPU: \" + output[0]); System.err.println(\"Total Pixel Of GPU: \" + totalPixels); System.err.println(\"SKinTone Percentage Using GPU: \" + skintoneThreshold + \"%\"); } static int[] execute(CUfunction kernelFunction, byte[] pixels, int w, int h) { // Allocate memory on the device, and copy the host data to the device int size = w * h * Sizeof.BYTE; CUdeviceptr pointer = new CUdeviceptr(); cuMemAlloc(pointer, size); cuMemcpyHtoD(pointer, Pointer.to(pixels), size); int numElements = 9; int s = 0; // Allocate device output memory CUdeviceptr deviceOutput = new CUdeviceptr(); cuMemAlloc(deviceOutput, numElements * Sizeof.INT); // Set up the kernel parameters: A pointer to an array // of pointers which point to the actual values. Pointer kernelParameters = Pointer.to(Pointer.to(pointer), Pointer.to(new int[] { w }), Pointer.to(new int[] { h }), Pointer.to(deviceOutput)); // Call the kernel function int blockSize = 16; int gridSize = (Math.max(w, h) + blockSize - 1) / blockSize; cuLaunchKernel(kernelFunction, gridSize, gridSize, 1, // Grid dimension blockSize, blockSize, 1, // Block dimension 0, null, // Shared memory size and stream kernelParameters, null // Kernel- and extra parameters ); cuCtxSynchronize(); // Allocate host output memory and copy the device output // to the host. int hostOutput[] = new int[numElements]; cuMemcpyDtoH(Pointer.to(hostOutput), deviceOutput, numElements * Sizeof.INT); // Clean up. cuMemFree(deviceOutput); cuMemFree(pointer); return hostOutput; } public static CUfunction initlize() { // Enable exceptions and omit all subsequent error checks JCudaDriver.setExceptionsEnabled(true); JNvrtc.setExceptionsEnabled(true); // Initialize the driver and create a context for the first device. cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); // Obtain the CUDA source code from the CUDA file String cuFileName = \"Skintone.cu\"; String sourceCode = CudaUtils.readResourceAsString(cuFileName); if (sourceCode == null) { IJ.showMessage(\"Error\", \"Could not read the kernel source code\"); } // Create the kernel function return CudaUtils.createFunction(sourceCode, \"skintone\"); } public static void CalculateSKintoneCPU(File file) throws IOException { BufferedImage bufferedImage = ImageIO.read(file); if (bufferedImage == null || bufferedImage.getData() == null) return; Raster raster = bufferedImage.getData(); float[] rgb = new float[4]; int totalPixels = raster.getHeight() * raster.getWidth(); int skinTonePixels = 0; for (int x = 0; x &lt; raster.getWidth(); x++) { for (int y = 0; y &lt; raster.getHeight(); y++) { raster.getPixel(x, y, rgb); if (skintone(rgb)) { skinTonePixels++; } } } // Flushing memory raster = null; rgb = null; bufferedImage.flush(); bufferedImage = null; long skintoneThreshold = Math.round(skinTonePixels / (double) totalPixels * 100.0); System.err.println(\"Skintone Using CPU: \" + skinTonePixels); System.err.println(\"Total Pixel Of CPU: \" + totalPixels); System.err.println(\"SKinTone Percentage Using CPU: \" + skintoneThreshold + \"%\"); } private static boolean skintone(float[] rgb) { float yCbCr[] = (float[]) convertRGBtoYUV(rgb); if ((yCbCr[1] &gt;= 80 &amp;&amp; yCbCr[1] &lt;= 120) &amp;&amp; (yCbCr[2] &gt;= 133 &amp;&amp; yCbCr[2] &lt;= 173)) { return true; } return false; } private static float[] convertRGBtoYUV(float[] rgb) { final float[] yCbCr = new float[3]; float r = rgb[0]; float g = rgb[1]; float b = rgb[2]; yCbCr[0] = 16 + (0.299f * r) + (0.587f * g) + (0.144f * b); yCbCr[1] = 128 + (-0.169f * r) - (0.331f * g) + (0.5f * b); yCbCr[2] = 128 + (0.5f * r) - (0.419f * g) - (0.081f * b); return yCbCr; } public static void main(String[] args) throws IOException { File file = new File(\"C:\\\\Users\\\\Aqeel\\\\git\\\\jcuda-imagej-example\\\\src\\\\test\\\\resources\\\\lena512color.png\"); CalculateSKintoneCPU(file); CalculateSKintoneGPU(file); } } Kernal File extern \"C\" __global__ void skintone(uchar4* data, int w, int h, int* output) { int x = threadIdx.x+blockIdx.x*blockDim.x; int y = threadIdx.y+blockIdx.y*blockDim.y; if (x &lt; w &amp;&amp; y &lt; h) { float r, g, b; float cb, cr; int index = y*w+x; uchar4 pixel = data[index]; r = pixel.x; g = pixel.y; b = pixel.z; cb = 128 + (-0.169f * r) - (0.331f * g) + (0.5f * b); cr = 128 + (0.5f * r) - (0.419f * g) - (0.081f * b); if((cb &gt;= 80 &amp;&amp; cb &lt;= 120) &amp;&amp; (cr &gt;= 133 &amp;&amp; cr &lt;= 173)) { atomicAdd(&amp;output[0], 1); } } } Complete Example src, Machine Need Nvida Card, Cuda Toolkit V9 and Graphics Drivers",
        "answers": [
            [
                "I solve the problem by hit and trial method. In the kernel i change the position of r with b, and the problem resolved, also instead of byte i have to send the code in int array in java. extern \"C\" __global__ void skintone(uchar4* data, int w, int h, int* output) { int x = threadIdx.x+blockIdx.x*blockDim.x; int y = threadIdx.y+blockIdx.y*blockDim.y; if (x &lt; w &amp;&amp; y &lt; h) { float b, g, r; float cb, cr; int index = y*w+x; uchar4 pixel = data[index]; b = (float)pixel.x; g = (float)pixel.y; r = (float)pixel.z; cb = 128 + (-0.169f * r) - (0.331f * g) + (0.5f * b); cr = 128 + (0.5f * r) - (0.419f * g) - (0.081f * b); if((cb &gt;= 80 &amp;&amp; cb &lt;= 120) &amp;&amp; (cr &gt;= 133 &amp;&amp; cr &lt;= 173)) { atomicAdd(&amp;output[0], 1); } } } Java Code Changes. public static void calculateSkintoneGPU() throws IOException { BufferedImage img = ImageIO.read(SkinTone.class.getClassLoader().getResource(\"images.jpg\")); if (img == null || img.getData() == null) return; int width = img.getWidth(null); int height = img.getHeight(null); int[] pixels = new int[width * height]; PixelGrabber pg = new PixelGrabber(img, 0, 0, width, height, pixels, 0, width); try { pg.grabPixels(); } catch (InterruptedException e){}; int totalPixels = width * height; CUfunction kernelFunction = initlize(); int output[] = execute(kernelFunction, pixels, width, height); // Flushing memory img.flush(); img = null; long skintoneThreshold = Math.round(output[0] / (double) totalPixels * 100.0); System.err.println(\"Skintone Using GPU: \" + output[0]); System.err.println(\"Total Pixel Of GPU: \" + totalPixels); System.err.println(\"SKinTone Percentage Using GPU: \" + skintoneThreshold + \"%\"); }"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have imported the following libraries in Gradle: compile group: 'org.jcuda', name: 'jcuda-natives', version: '0.9.2' compile group: 'org.jcuda', name: 'jcublas-natives', version: '0.9.2' compile group: 'org.jcuda', name: 'jcublas', version: '0.9.2' and copy pasted JCublasSampleexample from JCuda page. Unfortunately, I am getting the following error: Creating input data... Performing Sgemm with Java... Performing Sgemm with JCublas... Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Error while loading native library \"JCudaRuntime-0.9.2-windows-x86_64\" Operating system name: Windows 10 Architecture : amd64 Architecture bit size: 64 ---(start of nested stack traces)--- Stack trace from the attempt to load the library as a file: java.lang.UnsatisfiedLinkError: no JCudaRuntime-0.9.2-windows-x86_64 in java.library.path at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867) at java.lang.Runtime.loadLibrary0(Runtime.java:870) at java.lang.System.loadLibrary(System.java:1122) at jcuda.LibUtils.loadLibrary(LibUtils.java:143) at jcuda.runtime.JCuda.initialize(JCuda.java:422) at jcuda.runtime.JCuda.&lt;clinit&gt;(JCuda.java:406) at jcuda.jcublas.JCublas.initialize(JCublas.java:93) at jcuda.jcublas.JCublas.&lt;clinit&gt;(JCublas.java:81) ... Of course, library is absent. The question is how to link it Maven/Gradle? Site says all DLLs should be inside JARs. CUDA is installed, but I didn't specify it's version anywhere as I was to do with nd4j.",
        "answers": [
            [
                "It was my fault: cublas DLL of correct CUDA version was not in the PATH. DLL of JCublas was loading successfully without any Gradle tricks, because code was copying it into tmp directory froume CLASSPATH. Simultaneously, error message was not informative, suggesting me to look for problems in other place and also Gradle example was confusing. Summarizing what is required: to have CUDA of the same version as JCublas installed (and PATH correct) and required \"native\" JARs on classpath."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I run CUDA through JCuda API. When I use NVIDIA control panel to set optimization for computing (that is off by default) for participating JVM executable (java.exe), the program fails to create context in the call to cuCtxCreate. For optimization set off, the program runs just fine. The only problem was too high deviation of elapsed time in the kernel (~50%) from run to run. I was searching for ways to get CUDA cores in something like monopoly mode, just to get more stable result. Do I misunderstand this parameter in control panel? Exception in thread \"main\" jcuda.CudaException: CUDA_ERROR_UNKNOWN at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:353) at jcuda.driver.JCudaDriver.cuCtxCreate(JCudaDriver.java:1606) at com.varankin.cuda.CudaContext.(CudaContext.java:21) Configuration: Intel i7-4510U with integrated graphics, that is set as default GPU. NVIDIA GeForce 840M on shared memory space. Windows 10 Java 8, rev. 152 JCuda-All-0.9.0d-bin-x86_64 CUDA from cuda_9.2.148_win10.exe",
        "answers": [
            [
                "For optimization set off, the program runs just fine. The only problem was too high deviation of elapsed time in the kernel (~50%) from run to run. That is likely cause by WDDM driver batching of computing commands, If you search around you will find some suggestions you can try to minimize its negative effect. I was searching for ways to get CUDA cores in something like monopoly mode, just to get more stable result. That type of mode of operation only exists for Tesla and certain Quadro GPUs using TCC mode on Windows. Your GPU cannot be used in that way. Do I misunderstand this parameter in control panel? Yes. Quoting from the documentation: Windows 10, Maxwell GPUs and later. Offers significant improvement for some Compute applications. Care should be taken when turning this setting ON, as there can be unpredictable effects with some applications and graphics features. Giving that warning, the safe thing to do is leave that setting off."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am attempting to load the dll for the JCudaDriver which I extracted with the jar.exe tool. System.loadLibrary(\"JCudaDriver-0.9.2-windows-x86_64\") That driver is definitely in my java.libary.path because if I remove it manually, I get a not-found type error instead. Now I receive the following error. Exception in thread \"main\" java.lang.UnsatisfiedLinkError: myPath.JCudaDriver-0.9.2-windows-x86_64.dll.dll: The specified procedure could not be found My understanding is that this specified procedure is located in some missing dependency on another dll or there is a version clash. I used the windows utility function on the command line as dumpbin /dependents xx.dll to find the dependent dlls. They are as follows. Dump of file JCudaDriver-0.9.2-windows-x86_64.dll Image has the following dependencies: nvcuda.dll ADVAPI32.dll KERNEL32.dll I can load the nvcuda.dll without error but the latter two are a problem. fun main(args: Array&lt;String&gt;) { //System.loadLibrary(\"nvcuda\") System.loadLibrary(\"ADVAPI32\") //System.loadLibrary(\"KERNEL32\") } This is the error when trying to load ADVAPI32.dll Exception in thread \"main\" java.lang.UnsatisfiedLinkError: C:\\aaa_eric\\code\\lib\\dlls_x64\\advapi32.dll: %1 is not a valid Win32 application",
        "answers": [
            [
                "After some effort by the supporter of JCuda, Marco13 who left a comment above, the problem was identified as having updated the graphics driver after having installed Cuda. That changed some of the dlls. The fix was to remove and reinstall Cuda. I'll reference here the forum link where we iterated to the solution."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a JCuda project that's encountering an access violation whenever it tries to create a texture object using the driver API. Java HotSpot claims that the error is coming from nvcuda.dll. The underlying CUarray from which the texture is being created seems to be populated correctly; copying its contents back into a host-side float array results in an array that's identical to the initial host-side data. That means that the error itself has to be something in the texture declaration, right? Running the code using cuda-memcheck reveals no errors. Here is the code that's encountering the error: import jcuda.Pointer; import jcuda.Sizeof; import jcuda.driver.*; public class Main { public static void main(String[] args) { init(); float[] hostArray = new float[]{0, 1, 2, 3, 4, 5, 6, 7}; int[] dims = new int[]{2,2,2}; CUdeviceptr deviceArray = new CUdeviceptr(); JCudaDriver.cuMemAlloc(deviceArray, hostArray.length * Sizeof.FLOAT); JCudaDriver.cuMemcpyHtoD(deviceArray, Pointer.to(hostArray), hostArray.length * Sizeof.FLOAT); // initialize the opaque array object to represent the texture's data CUarray cuArray = makeCudaArray(dims); // populate the opaque array object copyDataIntoCudaArray(deviceArray, cuArray, dims); JCudaDriver.cuMemFree(deviceArray); // create the various descriptors CUDA_RESOURCE_DESC resourceDescriptor = makeResourceDescriptor(cuArray); CUDA_TEXTURE_DESC textureDescriptor = makeTextureDescriptor(); CUDA_RESOURCE_VIEW_DESC resourceViewDescriptor = makeResourceViewDescriptor(dims); CUtexObject texture = new CUtexObject(); System.out.println(\"About to hit an access violation:\"); JCudaDriver.cuTexObjectCreate(texture, resourceDescriptor, textureDescriptor, resourceViewDescriptor); } static void init() { JCudaDriver.setExceptionsEnabled(true); JCudaDriver.cuInit(0); int[] deviceCount = new int[1]; JCudaDriver.cuDeviceGetCount(deviceCount); CUdevice currentDevice = new CUdevice(); JCudaDriver.cuDeviceGet(currentDevice, 0); CUcontext currentContext = new CUcontext(); JCudaDriver.cuCtxCreate(currentContext, 0, currentDevice); } static CUarray makeCudaArray(int[] dims) { CUarray array = new CUarray(); CUDA_ARRAY3D_DESCRIPTOR arrayDescriptor = new CUDA_ARRAY3D_DESCRIPTOR(); arrayDescriptor.Width = dims[0]; arrayDescriptor.Height = dims[1]; arrayDescriptor.Depth = dims[2]; arrayDescriptor.Format = CUarray_format.CU_AD_FORMAT_FLOAT; arrayDescriptor.NumChannels = 1; arrayDescriptor.Flags = 0; JCudaDriver.cuArray3DCreate(array, arrayDescriptor); return array; } static void copyDataIntoCudaArray(CUdeviceptr deviceArray, CUarray array, int[] dims) { CUDA_MEMCPY3D copyParams = new CUDA_MEMCPY3D(); copyParams.srcMemoryType = CUmemorytype.CU_MEMORYTYPE_DEVICE; copyParams.srcDevice = deviceArray; copyParams.srcXInBytes = 0; copyParams.srcY = 0; copyParams.srcZ = 0; copyParams.srcPitch = (long) dims[0] * Sizeof.FLOAT; copyParams.srcHeight = dims[1]; copyParams.srcLOD = 0; copyParams.dstMemoryType = CUmemorytype.CU_MEMORYTYPE_ARRAY; copyParams.dstArray = array; copyParams.dstXInBytes = 0; copyParams.dstY = 0; copyParams.dstZ = 0; copyParams.dstLOD = 0; copyParams.WidthInBytes = (long) dims[0] * Sizeof.FLOAT; copyParams.Height = dims[1]; copyParams.Depth = dims[2]; JCudaDriver.cuMemcpy3D(copyParams); } static CUDA_RESOURCE_DESC makeResourceDescriptor(CUarray cuArray) { CUDA_RESOURCE_DESC resourceDescriptor = new CUDA_RESOURCE_DESC(); resourceDescriptor.resType = CUresourcetype.CU_RESOURCE_TYPE_ARRAY; resourceDescriptor.array_hArray = cuArray; resourceDescriptor.flags = 0; return resourceDescriptor; } static CUDA_TEXTURE_DESC makeTextureDescriptor() { CUDA_TEXTURE_DESC textureDescriptor = new CUDA_TEXTURE_DESC(); textureDescriptor.addressMode = new int[]{ CUaddress_mode.CU_TR_ADDRESS_MODE_CLAMP, CUaddress_mode.CU_TR_ADDRESS_MODE_CLAMP, CUaddress_mode.CU_TR_ADDRESS_MODE_CLAMP }; textureDescriptor.filterMode = CUfilter_mode.CU_TR_FILTER_MODE_LINEAR; textureDescriptor.flags = 0; textureDescriptor.maxAnisotropy = 1; textureDescriptor.mipmapFilterMode = CUfilter_mode.CU_TR_FILTER_MODE_POINT; textureDescriptor.mipmapLevelBias = 0; textureDescriptor.minMipmapLevelClamp = 0; textureDescriptor.maxMipmapLevelClamp = 0; return textureDescriptor; } static CUDA_RESOURCE_VIEW_DESC makeResourceViewDescriptor(int[] dims) { CUDA_RESOURCE_VIEW_DESC resourceViewDescriptor = new CUDA_RESOURCE_VIEW_DESC(); resourceViewDescriptor.format = CUresourceViewFormat.CU_RES_VIEW_FORMAT_FLOAT_1X32; resourceViewDescriptor.width = dims[0]; resourceViewDescriptor.height = dims[1]; resourceViewDescriptor.depth = dims[2]; resourceViewDescriptor.firstMipmapLevel = 0; resourceViewDescriptor.lastMipmapLevel = 0; resourceViewDescriptor.firstLayer = 0; resourceViewDescriptor.lastLayer = 0; return resourceViewDescriptor; } } What am I doing wrong here?",
        "answers": [
            [
                "The reason for this access violation was a bug in JCuda 0.9.0. The texture handle was erroneously passed to the native function as a NULL pointer. This is fixed in this commit, and the fix will be part of the next release. A test case based on the code in the question has been added. Update: This issue is fixed in JCuda 0.9.0d."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Note: The question has been updated to address the questions that have been raised in the comments, and to emphasize that the core of the question is about the interdependencies between the Runtime- and Driver API The CUDA runtime libraries (like CUBLAS or CUFFT) are generally using the concept of a \"handle\" that summarizes the state and context of such a library. The usage pattern is quite simple: // Create a handle cublasHandle_t handle; cublasCreate(&amp;handle); // Call some functions, always passing in the handle as the first argument cublasSscal(handle, ...); // When done, destroy the handle cublasDestroy(handle); However, there are many subtle details about how these handles interoperate with Driver- and Runtime contexts and multiple threads and devices. The documentation lists several, scattered details about context handling: The general description of contexts in the CUDA Programming Guide at http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#context The handling of multiple contexts, as described in the CUDA Best Practices Guide at http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#multiple-contexts The context management differences between runtime and driver API, explained at http://docs.nvidia.com/cuda/cuda-driver-api/driver-vs-runtime-api.html The general description of CUBLAS contexts/handles at http://docs.nvidia.com/cuda/cublas/index.html#cublas-context and their thread safety at http://docs.nvidia.com/cuda/cublas/index.html#thread-safety2 However, some of information seems to be not entirely up to date (for example, I think one should use cuCtxSetCurrent instead of cuCtxPushCurrent and cuCtxPopCurrent?), some of it seems to be from a time before the \"Primary Context\" handling was exposed via the driver API, and some parts are oversimplified in that they only show the most simple usage patterns, make only vague or incomplete statements about multithreading, or cannot be applied to the concept of \"handles\" that is used in the runtime libraries. My goal is to implement a runtime library that offers its own \"handle\" type, and that allows usage patterns that are equivalent to the other runtime libraries in terms of context handling and thread safety. For the case that the library can internally be implemented solely using the Runtime API, things may be clear: The context management is solely in the responsibility of the user. If he creates an own driver context, the rules that are stated in the documentation about the Runtime- and Driver context management will apply. Otherwise, the Runtime API functions will take care of the handling of primary contexts. However, there may be the case that a library will internally have to use the Driver API. For example, in order to load PTX files as CUmodule objects, and obtain the CUfunction objects from them. And when the library should - for the user - behave like a Runtime library, but internally has to use the Driver API, some questions arise about how the context handling has to be implemented \"under the hood\". What I have figured out so far is sketched here. (It is \"pseudocode\" in that it omits the error checks and other details, and ... all this is supposed to be implemented in Java, but that should not be relevant here) 1. The \"Handle\" is basically a class/struct containing the following information: class Handle { CUcontext context; boolean usingPrimaryContext; CUdevice device; } 2. When it is created, two cases have to be covered: It can be created when a driver context is current for the calling thread. In this case, it should use this context. Otherwise, it should use the primary context of the current (runtime) device: Handle createHandle() { cuInit(0); // Obtain the current context CUcontext context; cuCtxGetCurrent(&amp;context); CUdevice device; // If there is no context, use the primary context boolean usingPrimaryContext = false; if (context == nullptr) { usingPrimaryContext = true; // Obtain the device that is currently selected via the runtime API int deviceIndex; cudaGetDevice(&amp;deviceIndex); // Obtain the device and its primary context cuDeviceGet(&amp;device, deviceIndex); cuDevicePrimaryCtxRetain(&amp;context, device)); cuCtxSetCurrent(context); } else { cuCtxGetDevice(device); } // Create the actual handle. This might internally allocate // memory or do other things that are specific for the context // for which the handle is created Handle handle = new Handle(device, context, usingPrimaryContext); return handle; } 3. When invoking a kernel of the library, the context of the associated handle is made current for the calling thread: void someLibraryFunction(Handle handle) { cuCtxSetCurrent(handle.context); callMyKernel(...); } Here, one could argue that the caller is responsible for making sure that the required context is current. But if the handle was created for a primary context, then this context will be made current automatically. 4. When the handle is destroyed, this means that cuDevicePrimaryCtxRelease has to be called, but only when the context is a primary context: void destroyHandle(Handle handle) { if (handle.usingPrimaryContext) { cuDevicePrimaryCtxRelease(handle.device); } } From my experiments so far, this seems to expose the same behavior as a CUBLAS handle, for example. But my possibilities for thoroughly testing this are limited, because I only have a single device, and thus cannot test the crucial cases, e.g. of having two contexts, one for each of two devices. So my questions are: Are there any established patterns for implementing such a \"Handle\"? Are there any usage patterns (e.g. with multiple devices and one context per device) that could not be covered with the approach that is sketched above, but would be covered with the \"handle\" implementations of CUBLAS? More generally: Are there any recommendations of how to improve the current \"Handle\" implementation? Rhetorical: Is the source code of the CUBLAS handle handling available somewhere? (I also had a look at the context handling in tensorflow, but I'm not sure whether one can derive recommendations about how to implement handles for a runtime library from that...) (An \"Update\" has been removed here, because it was added in response to the comments, and should no longer be relevant)",
        "answers": [
            [
                "I'm sorry I hadn't noticed this question sooner - as we might have collaborated on this somewhat. Also, it's not quite clear to me whether this question belongs here, on codereview.SX or on programmers.SX, but let's ignore all that. I have now done what you were aiming to do, and possibly more generally. So, I can offer both an example of what to do with \"handles\", and moreover, suggest the prospect of not having to implement this at all. The library is an expanding of cuda-api-wrappers to also cover the Driver API and NVRTC; it is not yet release-grade, but it is in the testing phase, on this branch. Now, to answer your concrete question: Pattern for writing a class surrounding a raw \"handle\" Are there any established patterns for implementing such a \"Handle\"? Yes. If you read: What is the difference between: Handle, Pointer and Reference you'll notice a handle is defined as an \"opaque reference to an object\". It has some similarity to a pointer. A relevant pattern, therefore, is a variation on the PIMPL idiom: In regular PIMPL, you write an implementation class, and the outwards-facing class only holds a pointer to the implementation class and forwards method calls to it. When you have an opaque handle to an opaque object in some third-party library or driver - you use the handle to forward method calls to that implementation. That means, that your outwards-facing class is not a handle, it represents the object to which you have a handle. Generality and flexibility Are there any usage patterns (e.g. with multiple devices and one context per device) that could not be covered with the approach that is sketched above, but would be covered with the \"handle\" implementations of CUBLAS? I'm not sure what exactly CUBLAS does under the hood (and I have almost never used CUBLAS to be honest), but if it were well-designed and implemented, it would create its own context, and try to not to impinge on the rest of your code, i.e. it would alwas do: Push our CUBLAS context onto the top of the stack Do actual work Pop the top of the context stack. Your class doesn't do this. More generally: Are there any recommendations of how to improve the current \"Handle\" implementation? Yes: Use RAII whenever it is possible and relevant. If your creation code allocates a resource (e.g. via the CUDA driver) - the destructor for the object you return should safely release those resources. Allow for both reference-type and value-type use of Handles, i.e. it may be the handle I created, but it may also be a handle I got from somewhere else and isn't my responsibility. This is trivial if you leave it up to the user to release resources, but a bit tricky if you take that responsibility You assume that if there's any current context, that's the one your handle needs to use. Says who? At the very least, let the user pass a context in if they want to. Avoid writing the low-level parts of this on your own unless you really must. You are quite likely to miss some things (the push-and-pop is not the only thing you might be missing), and you're repeating a lot of work that is actually generic and not specific to your application or library. I may be biased here, but you can now use nice, RAII-ish, wrappers for CUDA contexts, streams, modules, devices etc. without even known about raw handles for anything. Rhetorical: Is the source code of the CUBLAS handle handling available somewhere? To the best of my knowledge, NVIDIA hasn't released it."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Is there any way to convert cudaStream_t object to CUStream? I found this hack but I don't think its safe to use. Or, is there any way to call cudaLaunchKernel in JCuda application using cudaStream_t object in CUDA 7.5 only?",
        "answers": [
            [
                "At a C level within the runtime and driver APIs, cudaStream_t and CUStream are the same type and can be used interchangeably in either API. At a JCUDA level, it appears that CUstream has a specialization of its constructor for initializing an instance with an existing cudaStream_t instance. The provision of this alternative constructor eliminates the need for the pointer swap you linked to in the question."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "How do I get the CUDA cores count in jcuda? I've tried this but it doesn't produce the right output: int cudacount = cudaDeviceAttr.cudaDevAttrMultiProcessorCount; It returns 16 but I have 1 Nvidia GPU with 640 cudacores. The JavaDoc for the above property is available here. Any help will be appreciated.",
        "answers": [
            [
                "It seems that this answer does almost exactly what you want. It's written in C, and the types are slightly different, so here's a Java version (it's hardly any different): int getSPCount() { final int mp = cudaDeviceAttr.cudaDevAttrMultiProcessorCount; final int major = cudaDeviceAttr.cudaDevAttrComputeCapabilityMajor; final int minor = cudaDeviceAttr.cudaDevAttrComputeCapabilityMinor; switch (major) { case 2: // Fermi return (minor == 1) ? mp * 48 : mp * 32; case 3: // Kepler return mp * 192; case 5: // Maxwell return mp * 128; case 6: // Pascal if (minor == 1) { return mp * 128; } else if (minor == 0) { return mp * 64; } } throw new RuntimeException(\"Unknown device type\"); } Use this function like so: int cudacount = getSPCount();"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "http://www.jcuda.org/tutorial/TutorialIndex.html /* * To change this license header, choose License Headers in Project Properties. * To change this template file, choose Tools | Templates * and open the template in the editor. */ package jcudavectoradd; /** * * @author Sanjula */ /* * JCuda - Java bindings for NVIDIA CUDA driver and runtime API * http://www.jcuda.org * * Copyright 2011 Marco Hutter - http://www.jcuda.org */ import static jcuda.driver.JCudaDriver.*; import java.io.*; import jcuda.*; import jcuda.driver.*; /** * This is a sample class demonstrating how to use the JCuda driver * bindings to load and execute a CUDA vector addition kernel. * The sample reads a CUDA file, compiles it to a PTX file * using NVCC, loads the PTX file as a module and executes * the kernel function. &lt;br /&gt; */ public class JCudaVectorAdd { /** * Entry point of this sample * * @param args Not used * @throws IOException If an IO error occurs */ public static void main(String args[]) throws IOException { // Enable exceptions and omit all subsequent error checks JCudaDriver.setExceptionsEnabled(true); // Create the PTX file by calling the NVCC String ptxFileName = preparePtxFile(\"JCudaVectorAddKernel.cu\"); //String ptxFileName = \"JCudaVectorAddKernel.ptx\"; // Initialize the driver and create a context for the first device. cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); // Load the ptx file. CUmodule module = new CUmodule(); cuModuleLoad(module, ptxFileName); // Obtain a function pointer to the \"add\" function. CUfunction function = new CUfunction(); cuModuleGetFunction(function, module, \"add\"); int numElements = 100000; // Allocate and fill the host input data float hostInputA[] = new float[numElements]; float hostInputB[] = new float[numElements]; for(int i = 0; i &lt; numElements; i++) { hostInputA[i] = (float)i; hostInputB[i] = (float)i; } // Allocate the device input data, and copy the // host input data to the device CUdeviceptr deviceInputA = new CUdeviceptr(); cuMemAlloc(deviceInputA, numElements * Sizeof.FLOAT); cuMemcpyHtoD(deviceInputA, Pointer.to(hostInputA), numElements * Sizeof.FLOAT); CUdeviceptr deviceInputB = new CUdeviceptr(); cuMemAlloc(deviceInputB, numElements * Sizeof.FLOAT); cuMemcpyHtoD(deviceInputB, Pointer.to(hostInputB), numElements * Sizeof.FLOAT); // Allocate device output memory CUdeviceptr deviceOutput = new CUdeviceptr(); cuMemAlloc(deviceOutput, numElements * Sizeof.FLOAT); // Set up the kernel parameters: A pointer to an array // of pointers which point to the actual values. Pointer kernelParameters = Pointer.to( Pointer.to(new int[]{numElements}), Pointer.to(deviceInputA), Pointer.to(deviceInputB), Pointer.to(deviceOutput) ); // Call the kernel function. int blockSizeX = 256; int gridSizeX = (int)Math.ceil((double)numElements / blockSizeX); cuLaunchKernel(function, gridSizeX, 1, 1, // Grid dimension blockSizeX, 1, 1, // Block dimension 0, null, // Shared memory size and stream kernelParameters, null // Kernel- and extra parameters ); cuCtxSynchronize(); // Allocate host output memory and copy the device output // to the host. float hostOutput[] = new float[numElements]; cuMemcpyDtoH(Pointer.to(hostOutput), deviceOutput, numElements * Sizeof.FLOAT); // Verify the result boolean passed = true; for(int i = 0; i &lt; numElements; i++) { float expected = i+i; if (Math.abs(hostOutput[i] - expected) &gt; 1e-5) { System.out.println( \"At index \"+i+ \" found \"+hostOutput[i]+ \" but expected \"+expected); passed = false; break; } } System.out.println(\"Test \"+(passed?\"PASSED\":\"FAILED\")); // Clean up. cuMemFree(deviceInputA); cuMemFree(deviceInputB); cuMemFree(deviceOutput); } /** * The extension of the given file name is replaced with \"ptx\". * If the file with the resulting name does not exist, it is * compiled from the given file using NVCC. The name of the * PTX file is returned. * * @param cuFileName The name of the .CU file * @return The name of the PTX file * @throws IOException If an I/O error occurs */ private static String preparePtxFile(String cuFileName) throws IOException { int endIndex = cuFileName.lastIndexOf('.'); if (endIndex == -1) { endIndex = cuFileName.length()-1; } String ptxFileName = cuFileName.substring(0, endIndex+1)+\"ptx\"; File ptxFile = new File(ptxFileName); if (ptxFile.exists()) { return ptxFileName; } File cuFile = new File(cuFileName); if (!cuFile.exists()) { throw new IOException(\"Input file not found: \"+cuFileName); } String modelString = \"-m\"+System.getProperty(\"sun.arch.data.model\"); String command = \"nvcc \" + modelString + \" -ptx \"+ cuFile.getPath()+\" -o \"+ptxFileName; System.out.println(\"Executing\\n\"+command); Process process = Runtime.getRuntime().exec(command); String errorMessage = new String(toByteArray(process.getErrorStream())); String outputMessage = new String(toByteArray(process.getInputStream())); int exitValue = 0; try { exitValue = process.waitFor(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new IOException( \"Interrupted while waiting for nvcc output\", e); } if (exitValue != 0) { System.out.println(\"nvcc process exitValue \"+exitValue); System.out.println(\"errorMessage:\\n\"+errorMessage); System.out.println(\"outputMessage:\\n\"+outputMessage); throw new IOException( \"Could not create .ptx file: \"+errorMessage); } System.out.println(\"Finished creating PTX file\"); return ptxFileName; } /** * Fully reads the given InputStream and returns it as a byte array * * @param inputStream The input stream to read * @return The byte array containing the data from the input stream * @throws IOException If an I/O error occurs */ private static byte[] toByteArray(InputStream inputStream) throws IOException { ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte buffer[] = new byte[8192]; while (true) { int read = inputStream.read(buffer); if (read == -1) { break; } baos.write(buffer, 0, read); } return baos.toByteArray(); } } extern \"C\" __global__ void add(int n, float *a, float *b, float *sum) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i&lt;n) { sum[i] = a[i] + b[i]; } } when I compile this code I get this error. I am using the NetBeans 8.2 and I installed the Cuda. it is perfectly working in the visual studio 2015 . but it not working with java.",
        "answers": [
            [
                "i added visual studio cl.exe path to Environment Variables C:\\Program Files\\Microsoft Visual Studio 10.0\\VC\\bin go to My Computer -&gt; Properties -&gt; Advanced System Settings -&gt; Environment Variables. Here look for \"PATH\" in the list, and add the path above (or whatever is the location of your cl.exe)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am learning JCuda and studying with JCuda samples. When I studied a KMeans algorithm code using JCuda, I got a \"CUDA_ERROR_ILLEGAL_ADDRESS\" when executed line cuCtxSynchronize(); It confused me a lot. How can I solve it? Here is KMeansKernel.cu extern \"C\" __global__ void add(int n, float *a, float *b, float *sum) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i&lt;n) { sum[i] = a[i] + b[i]; } } Main method(my class named \"CUDA\"): public static void main(String[] args){ // omit some code which input kinds of parameters try { // Open image file BufferedImage bi = ImageIO.read(picFiles); if (bi == null) { System.out.println(\"ERROR: File input error.\"); return; } // Read image data int length = bi.getWidth() * bi.getHeight(); int[] imageProperty = new int[length*5]; int[] pixel; int count = 0; for (int y = 0; y &lt; bi.getHeight(); y++) { for (int x = 0; x &lt; bi.getWidth(); x++) { pixel = bi.getRaster().getPixel(x, y, new int[4]); imageProperty[count*5 ] = pixel[0]; imageProperty[count*5+1] = pixel[1]; imageProperty[count*5+2] = pixel[2]; imageProperty[count*5+3] = x; imageProperty[count*5+4] = y; count++; } } //setup JCudaDriver.setExceptionsEnabled(true); // Create the PTX file String ptxFileName; try { ptxFileName = preparePtxFile(\"KmeansKernel.cu\"); } catch (IOException e) { System.out.println(\"Warning...\"); System.out.println(e.getMessage()); System.out.println(\"Exiting...\"); return; } cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); CUmodule module = new CUmodule(); cuModuleLoad(module, ptxFileName); CUfunction kmeansFunction = new CUfunction(); System.out.println(\"x\"); cuModuleGetFunction(kmeansFunction, module, \"add\"); //copy host input to device CUdeviceptr imageDevice = new CUdeviceptr(); cuMemAlloc(imageDevice, imageProperty.length * Sizeof.INT); cuMemcpyHtoD(imageDevice, Pointer.to(imageProperty), imageProperty.length * Sizeof.INT); int blockSizeX = 256; int gridSizeX = (int) Math.ceil((double)(imageProperty.length / 5) / blockSizeX); long et = System.currentTimeMillis(); System.out.println(((double)(et-st)/1000.0) + \"s\"); for (int k = startClusters; k &lt;= endClusters; k++) { long startTime = System.currentTimeMillis(); int[] clusters = new int[length]; int[] c = new int[k*5]; int h = 0; for(int i = 0; i &lt; k; i++) { c[i*5] = imageProperty[h*5]; c[i*5+1] = imageProperty[h*5+1]; c[i*5+2] = imageProperty[h*5+2]; c[i*5+3] = imageProperty[h*5+3]; c[i*5+4] = imageProperty[h*5+4]; h += length / k; } double tolerance = 1e-4; **//got warning in following line CUDA.KmeansKernel(kmeansFunction, imageDevice, imageProperty, clusters, c, k, tolerance, distanceWeight, colorWeight, blockSizeX, gridSizeX);** int[] output = calculateAveragePixels(imageProperty, clusters); BufferedImage outputImage = new BufferedImage(bi.getWidth(), bi.getHeight(), BufferedImage.TYPE_INT_RGB); for (int i = 0; i &lt; length; i++) { int rgb = output[i*5]; rgb = (rgb * 256) + output[i*5+1]; rgb = (rgb * 256) + output[i*5+2]; outputImage.setRGB(i%bi.getWidth(), i/bi.getWidth(), rgb); } String fileName = (picFiles.getName()) + \".bmp\"; File outputFile = new File(\"output/\" + fileName); ImageIO.write(outputImage, \"BMP\", outputFile); long runTime = System.currentTimeMillis() - startTime; System.out.println(\"Completed iteration k=\" + k + \" in \" + ((double)runTime/1000.0) + \"s\"); } System.out.println(\"Files saved to \" + outputDirectory.getAbsolutePath() + \"\\\\\"); cuMemFree(imageDevice); } catch (IOException e) { e.printStackTrace(); } } Method KmeansKernel: private static void KmeansKernel(CUfunction kmeansFunction, CUdeviceptr imageDevice, int[] imageProperty, int[] clusters, int[] c, int k, double tolerance, double distanceWeight, double colorWeight, int blockSizeX, int gridSizeX) { CUdeviceptr clustersDevice = new CUdeviceptr(); cuMemAlloc(clustersDevice, clusters.length * Sizeof.INT); // Alloc device output CUdeviceptr centroidPixels = new CUdeviceptr(); cuMemAlloc(centroidPixels, k * 5 * Sizeof.INT); CUdeviceptr errorDevice = new CUdeviceptr(); cuMemAlloc(errorDevice, Sizeof.DOUBLE * clusters.length); int[] c1 = new int[k*5]; cuMemcpyHtoD(centroidPixels, Pointer.to(c), Sizeof.INT * 5 * k); // begin algorithm int[] counts = new int[k]; double old_error, error = Double.MAX_VALUE; int l = 0; do { l++; old_error = error; error = 0; Arrays.fill(counts, 0); Arrays.fill(c1, 0); cuMemcpyHtoD(centroidPixels, Pointer.to(c), k * 5 * Sizeof.INT); Pointer kernelParameters = Pointer.to( Pointer.to(new int[] {clusters.length}), Pointer.to(new int[] {k}), Pointer.to(new double[] {colorWeight}), Pointer.to(new double[] {distanceWeight}), Pointer.to(errorDevice), Pointer.to(imageDevice), Pointer.to(centroidPixels), Pointer.to(clustersDevice) ); cuLaunchKernel(kmeansFunction, gridSizeX, 1, 1, blockSizeX, 1, 1, 0, null, kernelParameters, null ); **cuCtxSynchronize(); //got warning here.why?** cuMemcpyDtoH(Pointer.to(clusters), clustersDevice, Sizeof.INT*clusters.length); for (int i = 0; i &lt; clusters.length; i++) { int cluster = clusters[i]; counts[cluster]++; c1[cluster*5] += imageProperty[i*5]; c1[cluster*5+1] += imageProperty[i*5+1]; c1[cluster*5+2] += imageProperty[i*5+2]; c1[cluster*5+3] += imageProperty[i*5+3]; c1[cluster*5+4] += imageProperty[i*5+4]; } for (int i = 0; i &lt; k; i++) { if (counts[i] &gt; 0) { c[i*5] = c1[i*5] / counts[i]; c[i*5+1] = c1[i*5+1] / counts[i]; c[i*5+2] = c1[i*5+2] / counts[i]; c[i*5+3] = c1[i*5+3] / counts[i]; c[i*5+4] = c1[i*5+4] / counts[i]; } else { c[i*5] = c1[i*5]; c[i*5+1] = c1[i*5+1]; c[i*5+2] = c1[i*5+2]; c[i*5+3] = c1[i*5+3]; c[i*5+4] = c1[i*5+4]; } } double[] errors = new double[clusters.length]; cuMemcpyDtoH(Pointer.to(errors), errorDevice, Sizeof.DOUBLE*clusters.length); error = sumArray(errors); System.out.println(\"\" + l + \" iterations\"); } while (Math.abs(old_error - error) &gt; tolerance); cuMemcpyDtoH(Pointer.to(clusters), clustersDevice, clusters.length * Sizeof.INT); cuMemFree(errorDevice); cuMemFree(centroidPixels); cuMemFree(clustersDevice); } Stack trace: Exception in thread \"main\" jcuda.CudaException: CUDA_ERROR_ILLEGAL_ADDRESS at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:330) at jcuda.driver.JCudaDriver.cuCtxSynchronize(JCudaDriver.java:1938) at com.test.CUDA.KmeansKernel(CUDA.java:269) at com.test.CUDA.main(CUDA.java:184)",
        "answers": [
            [
                "As @talonmies mentions, the kernelParameters you are passing to the cuLaunchKernel method are not in line with add kernel function signature. You get the error at cuCtxSynchronize because CUDA execution model is asynchronous: cuLaunchKernel returns immediately and actual execution of the kernel on the device is asynchronous. cuCtxSynchronize documentation reads: Note that this function may also return error codes from previous, asynchronous launches. The second kernelParameters entry is an int k, where the second parameter of add method is a pointer to float, hence most probably the illegal access error."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I wrote some Java code that uses JCuda to execute some CUDA kernels. I would like to profile the application in order to understand how streams are overlapped and whatnot. I am able to use cuda event calls such as cudaEventElpasedTime to get the execution time of a kernel, but I do not know how to get the starting and ending timestamps for the same kernel. I know nvprof can generate such results and display the timelines, but I do not find a way to run nvprof with a Java application. Edit: Now I understand how to use nvprof to profile a Java application thanks to the answers. I still prefer getting the starting and ending times using cudaEvent calls so I would have more control. It seems nvprof can get that information but there is no APIs for an end user to do so?",
        "answers": [
            [
                "There are two ways to do this: If you can run your JCuda application via the command-line, you can profile it using the command nvprof --profile-child-processes &lt;command to run your JCuda application&gt; If you cannot run your application via the command-line, open a terminal and run nvprof using the command nvprof --profile-all-processes. Nvprof will go into daemon mode and keep waiting for CUDA activity to happen. Now launch your application as usual from your IDE, and once CUDA activity happens and the application exits, nvprof will print results in its terminal session."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm doing matrix multiplication on a GTX1080 GPU using JCuda, version 0.8.0RC with CUDA 8.0. I load two matrices A and B into the device in row-major vector form, and read the product matrix from the device. But I'm finding that I run out of device memory earlier than I would expect. For example, if matrix A is dimensioned 100000 * 5000 = 500 million entries = 2GB worth of float values, then: cuMemAlloc(MatrixA, 100000 * 5000 * Sizeof.FLOAT); works fine. But if I increase the number or rows to 110000 from 100000, I get the following error on this call (which is made before the memory allocations for matrices B and C, so those are not part of the problem): Exception in thread \"main\" jcuda.CudaException: CUDA_ERROR_OUT_OF_MEMORY at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:344) at jcuda.driver.JCudaDriver.cuMemAlloc(JCudaDriver.java:3714) at JCudaMatrixMultiply.main(JCudaMatrixMultiply.java:84) (my code) The issue is that allocating a matrix of this size on the device should take only about 2.2GB, and the GTX1080 has 8GB of memory, so I don't see why I'm running out of memory. Does anyone have any thoughts on this? It's true that I'm using the JCuda 0.8.0RC with the release version of CUDA 8, but I tried downloading the RC version of CUDA 8 (8.0.27) to use with JCuda 0.8.0RC and had some problems getting it to work. If versions compatibility is likely to be the issue, however, I can try again. Matrices of 100000 * 5000 are pretty big, of course, and I won't need to work with larger matrices for a while on my neural network project, but I would like to be confident that I can use all 8GB of memory on this new card. Thanks for any help.",
        "answers": [
            [
                "tl;dr: When calling cuMemAlloc(MatrixA, (long)110000 * 5000 * Sizeof.FLOAT); // ^ cast to long here or alternatively cuMemAlloc(MatrixA, 110000L * 5000 * Sizeof.FLOAT); // ^ use the \"long\" literal suffix here it should work. The last argument to cuMemAlloc is of type size_t. This is an implementation-specific unsigned integer type for \"arbitrary\" sizes. The closest possible primitive type in Java for this is long. And in general, every size_t in CUDA is mapped to long in JCuda. In this case, the Java long is passed as a jlong into the JNI layer, and this is simply cast to size_t for the actual native call. (The lack of unsigned types in Java and the odd plethora of integer types in C can still cause problems. Sometimes, the C types and the Java types just don't match. But as long as the allocation is not larger than 9 Million Terabytes (!), a long should be fine here...) But the comment by havogt lead to the right track. What happens here is indeed an integer overflow: The computation of the actual value 110000 * 5000 * Sizeof.FLOAT = 2200000000 is by default done using the int type in Java, and this is where the overflow happens: 2200000000 is larger than Integer.MAX_VALUE. The result will be a negative value. When this is cast to the (unsigned) size_t value in the JNI layer, it will become a ridiculosly large positive value, that clearly causes the error. When doing the computation using long values, either by explicitly casting to long or by appending the L suffix to one of the literals, the value is passed to CUDA as the proper long value of 2200000000."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "In the source code's comments of JCublas2.cublasSdot, it's commented that the 'result' parameter can be a 'host or device pointer'. public static int cublasSdot( cublasHandle handle, int n, Pointer x, int incx, Pointer y, int incy, Pointer result)/** host or device pointer */ { return checkResult(cublasSdotNative(handle, n, x, incx, y, incy, result)); } However, I can use only a host pointer like Pointer.to(fs) with float[] fs ={0}. If I use a device pointer like 'CUdeviceptr devicePtr = new CUdeviceptr(); JCudaDriver.cuMemAlloc(devicePtr, 100 * Sizeof.FLOAT);', the program crashes with console messages like: # # A fatal error has been detected by the Java Runtime Environment: # # EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x000007fed93af2a3, pid=9376, tid=0x0000000000003a7c # ..... Minimization of data transfer between host and device saves time. How to use device Pointer as the 'result' argument for this method, as well as other JCuda methods with result Pointer commented with /** host or device pointer **/?",
        "answers": [
            [
                "CUBLAS can write the results of certain computations (like the dot product) either to host or to device memory. The target memory type has to be set explicitly, using cublasSetPointerMode. An example of how this can be used is shown in the JCublas2PointerModes sample. It once writes the result of the dot product computation to host memory (which is also the default, when no pointer mode is set explicitly): // Set the pointer mode to HOST cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_HOST); // Prepare the pointer for the result in HOST memory float hostResult[] = { -1.0f }; Pointer hostResultPointer = Pointer.to(hostResult); // Execute the 'dot' function cublasSdot(handle, n, deviceData, 1, deviceData, 1, hostResultPointer); And then changes the pointer mode and calls the function again, this time writing the result to device memory: cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_DEVICE); // Prepare the pointer for the result in DEVICE memory Pointer deviceResultPointer = new Pointer(); cudaMalloc(deviceResultPointer, Sizeof.FLOAT); // Execute the 'dot' function cublasSdot(handle, n, deviceData, 1, deviceData, 1, deviceResultPointer);"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am novice to jCUDA. I am trying to compile a vector addition program from NVIDIA samples using eclipse on windows. Here is my program: import static jcuda.driver.JCudaDriver.*; import java.io.*; import jcuda.*; import jcuda.driver.*; /** * This is a sample class demonstrating how to use the JCuda driver * bindings to load and execute a CUDA vector addition kernel. * The sample reads a CUDA file, compiles it to a PTX file * using NVCC, loads the PTX file as a module and executes * the kernel function. &lt;br /&gt; */ public class TestSampleCuda { /** * Entry point of this sample * * @param args Not used * @throws IOException If an IO error occurs */ public static void main(String args[]) throws IOException { // Enable exceptions and omit all subsequent error checks JCudaDriver.setExceptionsEnabled(true); // Create the PTX file by calling the NVCC String ptxFileName = preparePtxFile(\"C:\\\\Users\\\\590943\\\\workspace\\\\Assignments\\\\TestSampleCudaKernel.cu\"); // String ptxFileName = \"C:\\\\Users\\\\590943\\\\workspace\\\\Assignments\\\\JCudaVectorAddKernel.ptx\"; // Initialize the driver and create a context for the first device. cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); // Load the ptx file. CUmodule module = new CUmodule(); cuModuleLoad(module, ptxFileName); // Obtain a function pointer to the \"add\" function. CUfunction function = new CUfunction(); cuModuleGetFunction(function, module, \"add\"); int numElements = 1000000; // Allocate and fill the host input data float hostInputA[] = new float[numElements]; float hostInputB[] = new float[numElements]; for(int i = 0; i &lt; numElements; i++) { hostInputA[i] = (float)i; hostInputB[i] = (float)i; } // Allocate the device input data, and copy the // host input data to the device CUdeviceptr deviceInputA = new CUdeviceptr(); cuMemAlloc(deviceInputA, numElements * Sizeof.FLOAT); cuMemcpyHtoD(deviceInputA, Pointer.to(hostInputA), numElements * Sizeof.FLOAT); CUdeviceptr deviceInputB = new CUdeviceptr(); cuMemAlloc(deviceInputB, numElements * Sizeof.FLOAT); cuMemcpyHtoD(deviceInputB, Pointer.to(hostInputB), numElements * Sizeof.FLOAT); // Allocate device output memory CUdeviceptr deviceOutput = new CUdeviceptr(); cuMemAlloc(deviceOutput, numElements * Sizeof.FLOAT); // Set up the kernel parameters: A pointer to an array // of pointers which point to the actual values. Pointer kernelParameters = Pointer.to( Pointer.to(new int[]{numElements}), Pointer.to(deviceInputA), Pointer.to(deviceInputB), Pointer.to(deviceOutput) ); // Call the kernel function. int blockSizeX = 256; int gridSizeX = (int)Math.ceil((double)numElements / blockSizeX); cuLaunchKernel(function, gridSizeX, 1, 1, // Grid dimension blockSizeX, 1, 1, // Block dimension 0, null, // Shared memory size and stream kernelParameters, null // Kernel- and extra parameters ); cuCtxSynchronize(); // Allocate host output memory and copy the device output // to the host. float hostOutput[] = new float[numElements]; cuMemcpyDtoH(Pointer.to(hostOutput), deviceOutput, numElements * Sizeof.FLOAT); // Verify the result boolean passed = true; for(int i = 0; i &lt; numElements; i++) { float expected = i+i; if (Math.abs(hostOutput[i] - expected) &gt; 1e-5) { System.out.println( \"At index \"+i+ \" found \"+hostOutput[i]+ \" but expected \"+expected); passed = false; break; } } System.out.println(\"Test \"+(passed?\"PASSED\":\"FAILED\")); // Clean up. cuMemFree(deviceInputA); cuMemFree(deviceInputB); cuMemFree(deviceOutput); } /** * The extension of the given file name is replaced with \"ptx\". * If the file with the resulting name does not exist, it is * compiled from the given file using NVCC. The name of the * PTX file is returned. * * @param cuFileName The name of the .CU file * @return The name of the PTX file * @throws IOException If an I/O error occurs */ private static String preparePtxFile(String cuFileName) throws IOException { int endIndex = cuFileName.lastIndexOf('.'); if (endIndex == -1) { endIndex = cuFileName.length()-1; } String ptxFileName = cuFileName.substring(0, endIndex+1)+\"ptx\"; //File ptxFile = new File(ptxFileName); File ptxFile = new File(ptxFileName); System.out.println(ptxFile.getCanonicalPath()); if (ptxFile.exists()) { return ptxFileName; } File cuFile = new File(cuFileName); System.out.println(cuFile.getCanonicalPath()); if (!cuFile.exists()) { throw new IOException(\"Input file not found: \"+cuFileName); } String modelString = \"-m\"+System.getProperty(\"sun.arch.data.model\"); String command = \"nvcc \" + modelString + \" -ptx \"+ cuFile.getPath()+\" -o \"+ptxFileName; System.out.println(\"Executing\\n\"+command); Process process = Runtime.getRuntime().exec(command); String errorMessage = new String(toByteArray(process.getErrorStream())); String outputMessage = new String(toByteArray(process.getInputStream())); int exitValue = 0; try { exitValue = process.waitFor(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new IOException( \"Interrupted while waiting for nvcc output\", e); } if (exitValue != 0) { System.out.println(\"nvcc process exitValue \"+exitValue); System.out.println(\"errorMessage:\\n\"+errorMessage); System.out.println(\"outputMessage:\\n\"+outputMessage); throw new IOException( \"Could not create .ptx file: \"+errorMessage); } System.out.println(\"Finished creating PTX file\"); return ptxFileName; } /** * Fully reads the given InputStream and returns it as a byte array * * @param inputStream The input stream to read * @return The byte array containing the data from the input stream * @throws IOException If an I/O error occurs */ private static byte[] toByteArray(InputStream inputStream) throws IOException { ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte buffer[] = new byte[8192]; while (true) { int read = inputStream.read(buffer); if (read == -1) { break; } baos.write(buffer, 0, read); } return baos.toByteArray(); } } Error message: C:\\Users\\590943\\workspace\\Assignments\\TestSampleCudaKernel.ptx Exception in thread \"main\" C:\\Users\\590943\\workspace\\Assignments\\TestSampleCudaKernel.cu java.io.IOException: Input file not found: C:\\Users\\590943\\workspace\\Assignments\\TestSampleCudaKernel.cu at TestSampleCuda.preparePtxFile(TestSampleCuda.java:169) at TestSampleCuda.main(TestSampleCuda.java:46) Can you please help me?",
        "answers": [],
        "votes": []
    },
    {
        "question": "On http://www.jcuda.org/samples/samples.html, currently there is JCusolverSamples20151013.zip for download. It seems that a current version of jcuda like 0.7.5 or 0.7 fits it. However, JCusolverSp_LinearSolver_Direct.java, one of the two java files, cannot compile, and it contains 5 errors wrt 3 different methods. All of the error messages are similar to the following message about discrepancy of the last argument, 'Pointer' instead of 'int[]': The method cusolverSpDcsrlsvchol(cusolverSpHandle, int, int, cusparseMatDescr, Pointer, Pointer, Pointer, Pointer, double, int, Pointer, Pointer) in the type JCusolverSp is not applicable for the arguments (cusolverSpHandle, int, int, cusparseMatDescr, Pointer, Pointer, Pointer, Pointer, double, int, Pointer, int[]) I have also checked JCuda 0.7.0, and the APIs regarding the methods are the same. I am new to Jcuda so I don't know how to modify the sample JCusolverSp_LinearSolver_Direct java file. How to make it work?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to setup mavenized JCUDA for a project that I am working on and am running into issues with telling java where to locate the library files. Setting up and getting maven to build the .jar and .dll files has worked fine, I can see the correctly named .dll files in project\\target\\lib and I am setting my native library location to this folder. The error I get when trying to run one of the programs from JCUDA JCublasSample.java (www.jcuda.org/samples/JCublasSample.java) is: Creating input data... Performing Sgemm with Java... Performing Sgemm with JCublas... Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Could not load the native library. Error while loading native library \"JCublas-windows-x86_64\" with base name \"JCublas\" Operating system name: Windows 7 Architecture : amd64 Architecture bit size: 64 Stack trace from the attempt to load the library as a resource: java.lang.NullPointerException: No resource found with name '/lib/JCublas-windows-x86_64.dll' at jcuda.LibUtils.loadLibraryResource(LibUtils.java:149) at jcuda.LibUtils.loadLibrary(LibUtils.java:83) at jcuda.jcublas.JCublas.initialize(JCublas.java:93) at jcuda.jcublas.JCublas.(JCublas.java:81) at JCublasSample.sgemmJCublas(JCublasSample.java:64) at JCublasSample.testSgemm(JCublasSample.java:49) at JCublasSample.main(JCublasSample.java:25) Stack trace from the attempt to load the library as a file: java.lang.UnsatisfiedLinkError: C:\\Users\\kristoffer.bernhem\\git\\SMlocalizer\\target\\lib\\JCublas-windows-x86_64.dll: Can't find dependent libraries at java.lang.ClassLoader$NativeLibrary.load(Native Method) at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1857) at java.lang.Runtime.loadLibrary0(Runtime.java:870) at java.lang.System.loadLibrary(System.java:1122) at jcuda.LibUtils.loadLibrary(LibUtils.java:94) at jcuda.jcublas.JCublas.initialize(JCublas.java:93) at jcuda.jcublas.JCublas.(JCublas.java:81) at JCublasSample.sgemmJCublas(JCublasSample.java:64) at JCublasSample.testSgemm(JCublasSample.java:49) at JCublasSample.main(JCublasSample.java:25) at jcuda.LibUtils.loadLibrary(LibUtils.java:128) at jcuda.jcublas.JCublas.initialize(JCublas.java:93) at jcuda.jcublas.JCublas.(JCublas.java:81) at JCublasSample.sgemmJCublas(JCublasSample.java:64) at JCublasSample.testSgemm(JCublasSample.java:49) at JCublasSample.main(JCublasSample.java:25) As explained by Guenther, the problem lies in supporting .dll files that are lacking. How would I go about sorting this error out? This is being run in windows 7 (64bit) and run with JDK1.8.0_91.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm getting a CUDA_ERROR_ILLEGAL_ADDRESS exception when trying to run a kernel used for calculating Buddhabrot fractal orbits. extern \"C\" __global__ void exec(int iterations, int size, float* inputR, float* inputI, // Real/Imaginary input int* output // Output image in one dimension ) { int i = blockIdx.x * blockDim.x + threadIdx.x; float cR = inputR[i]; float cI = inputI[i]; float x = 0; float y = 0; float outX[1000]; float outY[1000]; for (int j = 0; j &lt; iterations; j++) { outX[j] = x; outY[j] = y; float xNew = (x * x) - (y * y) + cR; float yNew = (2 * x * y) + cI; if (xNew * xNew + yNew * yNew &gt; 4) { for (int k = 1; k &lt; j; k++) { int curX = (outX[k] + 2 ) * size / 4; int curY = (outY[k] + 2 ) * size / 4; int idx = curX + size * curY; output[idx]++; // &lt;- exception here } return; } x = xNew; y = yNew; } } I've tried multiple things now and the error doesn't even seem to be stemming from the array contrary to what I first thought. For example, output[0] = 0; will work just fine. However, when I tried to debug idx (Remember I first thought the error was related to the array), i found out that I can neither assign idx like so output[0] = idx; nor use it in a printf statement if (i == 0) { printf(\"%d\\n\", idx); } I've tried the same with curX and curY which also refuse to work, however cR for example will work without any error. There seems to be a problem with the variables assigned inside the innermost loop (I also can't assign k), so I tried declaring idx outside of all loops at the start of the function, but to no avail. Still the same error. Stack trace: Exception in thread \"main\" jcuda.CudaException: CUDA_ERROR_ILLEGAL_ADDRESS at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:330) at jcuda.driver.JCudaDriver.cuCtxSynchronize(JCudaDriver.java:1938) at fractal.Buddhabrot.&lt;init&gt;(Buddhabrot.java:96) at controller.Controller.&lt;init&gt;(Controller.java:10) at Main.main(Main.java:8) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) Constants: block size 512*1*1 grid size 64 *1*1 iterations 1000 size 256 inputR, inputI length 64*512 output length 256*256 MCVE: import jcuda.Pointer; import jcuda.Sizeof; import jcuda.driver.*; import java.io.File; import java.util.Random; import static jcuda.driver.JCudaDriver.*; public class Stackoverflow { public static final int SIZE = 256; public static final long NUM_POINTS = 128 * 128 * 128; public static final int ITERATIONS = 10000; public static final int BLOCK_SIZE = 512; public static final int SIM_THREADS = BLOCK_SIZE * 64; public static final Random random = new Random(); public static void main(String[] args) { File ptxFile = new File(\"Buddha.ptx\"); setExceptionsEnabled(true); cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); CUmodule module = new CUmodule(); cuModuleLoad(module, ptxFile.getAbsolutePath()); CUfunction function = new CUfunction(); cuModuleGetFunction(function, module, \"exec\"); cuCtxSetLimit(CUlimit.CU_LIMIT_PRINTF_FIFO_SIZE, 4096); float[] inR = new float[SIM_THREADS]; float[] inI = new float[SIM_THREADS]; int[] out = new int[SIZE * SIZE]; CUdeviceptr deviceInputR = new CUdeviceptr(); cuMemAlloc(deviceInputR, inR.length * Sizeof.FLOAT); CUdeviceptr deviceInputI = new CUdeviceptr(); cuMemAlloc(deviceInputI, inI.length * Sizeof.FLOAT); CUdeviceptr deviceOutput = new CUdeviceptr(); cuMemAlloc(deviceOutput, out.length * Sizeof.INT); for (long i = 0; i &lt; NUM_POINTS; i += SIM_THREADS) { for (int j = 0; j &lt; SIM_THREADS; j++) { inR[j] = random.nextFloat() * 4f - 2f; inI[j] = random.nextFloat() * 4f - 2f; } System.out.println(\"GPU START\"); cuMemcpyHtoD(deviceInputR, Pointer.to(inR), inR.length * Sizeof.FLOAT); cuMemcpyHtoD(deviceInputI, Pointer.to(inI), inI.length * Sizeof.FLOAT); Pointer kernelParameters = Pointer.to( Pointer.to(new int[]{ITERATIONS}), Pointer.to(new int[]{SIZE}), Pointer.to(deviceInputR), Pointer.to(deviceInputI), Pointer.to(deviceOutput) ); int gridSize = (int) Math.ceil(((double) SIM_THREADS) / BLOCK_SIZE); cuLaunchKernel(function, gridSize, 1, 1, BLOCK_SIZE, 1, 1, 0, null, kernelParameters, null ); cuCtxSynchronize(); System.out.println(\"GPU END\"); } cuMemcpyDtoH(Pointer.to(out), deviceOutput, out.length * Sizeof.INT); } }",
        "answers": [
            [
                "In your \"constants\" section you had indicated this: iterations 1000 but in your java code (after you provided the MCVE) you have this: public static final int ITERATIONS = 10000; That can clearly cause this section of your kernel code to break: float outX[1000]; float outY[1000]; for (int j = 0; j &lt; iterations; j++) { outX[j] = x; outY[j] = y; since 10000 for iterations is indexing out of bounds. (The extent of this loop is actually data dependent, but for some data input patterns, the loop will traverse past 1000, as written). When I change this: public static final int ITERATIONS = 10000; to this: public static final int ITERATIONS = 1000; your code runs correctly for me: $ cuda-memcheck java -cp \".:jcuda-0.7.5b.jar\" so1 ========= CUDA-MEMCHECK GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END GPU START GPU END ========= ERROR SUMMARY: 0 errors $"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am trying to implement jcuda code on hadoop,and it worked in local mode,but when I run the job on hadoop cluster,it gives me a error:the container was killed here is the specific error report: 16/04/29 10:18:07 INFO mapreduce.Job: Task Id : attempt_1461835313661_0014_r_000009_2, Status : FAILED Container [pid=19894,containerID=container_1461835313661_0014_01_000021] is running beyond virtual memory limits. Current usage: 197.5 MB of 1 GB physical memory used; 20.9 GB of 2.1 GB virtual memory used. Killing container. the input data is just 200MB,but the job ask for 20.9GB virtual memory I don't konw why.and I have tried to increase the virtual memory ,and the configuration is in yarn-site.xml: &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ration&lt;/name&gt; &lt;value&gt;12&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; it is not working ,I don't konw to slove it,and I'm sorry for my poor English.",
        "answers": [
            [
                "Please check the following parameters and set it if not set to the values below: In mapred-site.xml: mapreduce.map.memory.mb: 4096 mapreduce.reduce.memory.mb: 8192 mapreduce.map.java.opts: -Xmx3072m mapreduce.reduce.java.opts: -Xmx6144m Hope this solves your issue"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am working on an image segmentation program which uses JCuda. The project is a Maven project, the dependencies for JCuda however are stored in dll files and are not managed with maven. Since I got a runtime error in my Cuda kernel (*.ptx), which is invoked by Jcuda, I want to start to debug, which is relatively difficult using a Java wrapper for Cuda. Thus, I want to use \u201cmemcheck\u201d, which requires to have a *.bat file which invokes my Java program. This is the point where I get lost. I do not manage to create a runnable jar file, which I can run with the java command in my *.bat file. My Question is, is there another way I could debug my Cuda code, but without creating a runnable jar file? \u2013If not, how can I create a runnable jar file from maven? I am aware that other people had that question too, but no answer worked for me, which makes me suspect that the dll files have to do with it. I tried to package the project with maven (maven built\u2026 -&gt; goal: package). When I navigate to the target directory where my jar file was created via my command prompt the program does not run: java in cmd prompt The reply means that no main manifest attribute can be found. Here is the snapshot of my pom file which is supposed to define the main class: Build in pom file Thank you for your help. This is my first post here on stack overflow, so please point out inconsistencies in my question.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to understand how to use Cuda in Java. I am using jCuda. Everything was fine until I came across an example containing the code: // Set up the kernel parameters: A pointer to an array // of pointers which point to the actual values. Pointer kernelParameters = Pointer.to( Pointer.to(new int[]{numElements}), Pointer.to(deviceInputA), Pointer.to(deviceInputB), Pointer.to(deviceOutput) ); The kernel function prototype is: __global__ void add(int n, float *a, float *b, float *sum) The question is: In terms of c, does it not seem that we are passing something like? (***n, ***a, ***b, ***sum) So basically, do we always have to have: Pointer kernelParameters = Pointer.to( double pointer, double pointer, ...)??? Thank you",
        "answers": [
            [
                "The cuLaunchKernel function of JCuda corresponds to the cuLaunchKernel function of CUDA. The signature of this function in CUDA is CUresult cuLaunchKernel( CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream, void** kernelParams, void** extra) where the kernelParams is the only parameter that is relevant for this question. The documentation says Kernel parameters can be specified via kernelParams. If f has N parameters, then kernelParams needs to be an array of N pointers. Each of kernelParams[0] through kernelParams[N-1] must point to a region of memory from which the actual kernel parameter will be copied. The key point here is the last sentence: The elements of the kernelParams array are not the actual kernel parameters. They only point to the actual kernel parameters. And indeed, this has the odd effect that for a kernel that receives a single float *pointer, you could basically set up the kernel parameters as follows: float *pointer= allocateSomeDeviceMemory(); float** pointerToPointer = &amp;pointer; float*** pointerToPointerToPointer = &amp;pointerToPointer; void **kernelParams = pointerToPointerToPointer; (This is just to make clear that this is indeed a pointer to a pointer to a pointer - in reality, wou wouldn't write it like that) Now, the \"structure\" of the kernel parameters is basically the same for JCuda and for CUDA. Of course you can not take \"the address of a pointer\" in Java, but the number of indirections is the same. Imagine you have a kernel like this: __global__ void example(int value, float *pointer) In the CUDA C API, you can then define the kernel parameters as follows: int value = 123; float *pointer= allocateSomeDeviceMemory(); int* pointerToValue = &amp;value; float** pointerToPointer = &amp;pointer; void **kernelParams = { pointerToValue, pointerToPointer }; The setup is done analogously in the JCuda Java API: int value = 123; Pointer pointer= allocateSomeDeviceMemory(); Pointer pointerToValue = Pointer.to(new int[]{value}); float** pointerToPointer = Pointer.to(pointer); Pointer kernelParameters = Pointer.to( pointerToValue, pointerToPointer ); The main difference that is relevant here is that you can write this a bit more concisely in C, using the address operator &amp;: void **kernelParams = { &amp;value, // This can be imagined as a pointer to an int &amp;pointer // This can be imagined as a pointer to a pointer }; But this is basically the same as in the example that you provided: Pointer kernelParameters = Pointer.to( Pointer.to(new int[]{value}), // A pointer to an int Pointer.to(pointer) // A pointer to a pointer ); Again, the key point is that with something like void **kernelParams = { &amp;value, }; or Pointer kernelParameters = Pointer.to( Pointer.to(new int[]{value}), ); you are not passing the value to the kernel directly. Instead, you are telling CUDA: \"Here is an array of pointers. The first pointer points to an int value. Copy the value from this memory location, and use it as the actual value for the kernel call\"."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am trying to use cuModuleLoad() in JCuda to load a vectorAdd.ptx file from /src/main/resources. The code is as follows: cuModuleLoad(module, getClass.getResource(\"vectorAdd.ptx\").getPath()) But the cuModuleLoad() doesn't pick up this file. It only works when I pass in the absolute path of the ptx file. But I would like to have the ptx file shipped with compile jar files. Is there any way to accomplish this?",
        "answers": [
            [
                "The cuModuleLoad function in JCuda is a direct mapping to the corresponding cuModuleLoad function in CUDA. It expects a file name as the second argument. The problem is: cuModuleLoad can not load the PTX file, because the PTX file simply does not exist for CUDA! The PTX file is hidden inside the JAR file. When you obtain a resource from a JAR file using someClass.getResource(), then it will point to the resource in the JAR file. When you do something like System.out.println(getClass().getResource(\"vectorAdd.ptx\").getPath()); and run this (as a JAR file), then you will see an output like this: file:/U:/YourWorkspace/YourJarFile.jar!/vectorAdd.ptx Note the .jar! part: This path is not a path to a real file, but only a path to a resource in the JAR. In order to load the PTX file from a JAR, you have to read the PTX file from the JAR into a byte[] array on Java side, and then pass it to the cuModuleLoadData function of JCuda (which corresponds to the cuModuleLoadData function of CUDA). Here is an example that loads the PTX data from a JAR file into a byte array, representing the zero-terminated string that can be passed to cuModuleLoadData: import static jcuda.driver.JCudaDriver.cuCtxCreate; import static jcuda.driver.JCudaDriver.cuDeviceGet; import static jcuda.driver.JCudaDriver.cuInit; import static jcuda.driver.JCudaDriver.cuModuleGetFunction; import static jcuda.driver.JCudaDriver.cuModuleLoadData; import static jcuda.runtime.JCuda.cudaDeviceReset; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.io.InputStream; import jcuda.driver.CUcontext; import jcuda.driver.CUdevice; import jcuda.driver.CUfunction; import jcuda.driver.CUmodule; import jcuda.driver.JCudaDriver; public class JCudaPtxInJar { public static void main(String args[]) throws IOException { // Initialization JCudaDriver.setExceptionsEnabled(true); cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); // Read the PTX data into a zero-terminated string byte array byte ptxData[] = toZeroTerminatedStringByteArray( JCudaPtxInJar.class.getResourceAsStream( \"JCudaVectorAddKernel.ptx\")); // Load the module data CUmodule module = new CUmodule(); cuModuleLoadData(module, ptxData); // Obtain a function pointer to the \"add\" function // and print a simple test/debug message CUfunction function = new CUfunction(); cuModuleGetFunction(function, module, \"add\"); System.out.println(\"Got function \"+function); cudaDeviceReset(); } /** * Read the contents of the given input stream, and return it * as a byte array containing the ZERO-TERMINATED string data * from the stream. The caller is responsible for closing the * given stream. * * @param inputStream The input stream * @return The ZERO-TERMINATED string byte array * @throws IOException If an IO error occurs */ private static byte[] toZeroTerminatedStringByteArray( InputStream inputStream) throws IOException { ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte buffer[] = new byte[8192]; while (true) { int read = inputStream.read(buffer); if (read == -1) { break; } baos.write(buffer, 0, read); } baos.write(0); return baos.toByteArray(); } } Compiling this and packing it into a JAR (together with the /resources/JCudaVectorAddKernel.ptx PTX file, of course) will allow you to start the program and obtain the example function from the PTX in the JAR."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I said in this question that I had some problem loading ptx modules in JCuda and after @talonmies's idea, I implemented a JCuda version of his solution to load multiple ptx files and load them as a single module. Here is the related part of the code: import static jcuda.driver.JCudaDriver.cuLinkAddFile; import static jcuda.driver.JCudaDriver.cuLinkComplete; import static jcuda.driver.JCudaDriver.cuLinkCreate; import static jcuda.driver.JCudaDriver.cuLinkDestroy; import static jcuda.driver.JCudaDriver.cuModuleGetFunction; import static jcuda.driver.JCudaDriver.cuModuleLoadData; import jcuda.driver.CUjitInputType; import jcuda.driver.JITOptions; import jcuda.driver.CUlinkState; import jcuda.driver.CUfunction; public class JCudaTestJIT{ private CUmodule module; private CUfunction functionKernel; public void prepareModule(){ String ptxFileName4 = \"file4.ptx\"; String ptxFileName3 = \"file3.ptx\"; String ptxFileName2 = \"file2.ptx\"; String ptxFileName1 = \"file1.ptx\"; CUlinkState linkState = new CUlinkState(); JITOptions jitOptions = new JITOptions(); cuLinkCreate(jitOptions, linkState); cuLinkAddFile(linkState, CUjitInputType.CU_JIT_INPUT_PTX, ptxFileName4, jitOptions); cuLinkAddFile(linkState, CUjitInputType.CU_JIT_INPUT_PTX, ptxFileName3, jitOptions); cuLinkAddFile(linkState, CUjitInputType.CU_JIT_INPUT_PTX, ptxFileName2, jitOptions); cuLinkAddFile(linkState, CUjitInputType.CU_JIT_INPUT_PTX, ptxFileName1, jitOptions); long sizeOut = 32768; byte[] image = new byte[32768]; Pointer cubinOut = Pointer.to(image); cuLinkComplete(linkState, cubinOut, (new long[]{sizeOut})); module = new CUmodule(); // Load the module from the image buffer cuModuleLoadData(module, cubinOut.getByteBuffer(0, 32768).array()); cuLinkDestroy(linkState); functionKernel = new CUfunction(); cuModuleGetFunction(functionKernel, module, \"kernel\"); } // Other methods } But I got the error of CUDA_ERROR_INVALID_IMAGE at calling cuModuleLoadData method. While debugging it, I saw that after calling cuLinkComplete method and pass the image array as the output, the array is still unchanged and clear. Am I passing the output parameter correctly? Is this how one can pass a variable by reference in JCuda?",
        "answers": [
            [
                "I had never written a single line of Java code until 30 minutes ago, let alone used JCUDA before, but an almost literal line-by-line translation of the native C++ code I gave you here seems to work perfectly: import static jcuda.driver.JCudaDriver.*; import java.io.*; import jcuda.*; import jcuda.driver.*; public class JCudaRuntimeTest { public static void main(String args[]) { JCudaDriver.setExceptionsEnabled(true); cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); CUlinkState linkState = new CUlinkState(); JITOptions jitOptions = new JITOptions(); cuLinkCreate(jitOptions, linkState); String ptxFileName2 = \"test_function.ptx\"; String ptxFileName1 = \"test_kernel.ptx\"; cuLinkAddFile(linkState, CUjitInputType.CU_JIT_INPUT_PTX, ptxFileName2, jitOptions); cuLinkAddFile(linkState, CUjitInputType.CU_JIT_INPUT_PTX, ptxFileName1, jitOptions); long sz[] = new long[1]; Pointer image = new Pointer(); cuLinkComplete(linkState, image, sz); System.out.println(\"Pointer: \" + image); System.out.println(\"CUBIN size: \" + sz[0]); CUmodule module = new CUmodule(); cuModuleLoadDataEx(module, image, 0, new int[0], Pointer.to(new int[0])); cuLinkDestroy(linkState); CUfunction functionKernel = new CUfunction(); String kernelname = \"_Z6kernelPfS_S_S_\"; cuModuleGetFunction(functionKernel, module, kernelname); System.out.println(\"Function: \" + functionKernel); } } which works like this: &gt; nvcc -ptx -arch=sm_21 test_function.cu test_function.cu &gt; nvcc -ptx -arch=sm_21 test_kernel.cu test_kernel.cu &gt; javac -cp \".;jcuda-0.7.0a.jar\" JCudaRuntimeTest.java &gt; java -cp \".;jcuda-0.7.0a.jar\" JCudaRuntimeTest Pointer: Pointer[nativePointer=0xa5a13a8,byteOffset=0] CUBIN size: 5924 Function: CUfunction[nativePointer=0xa588160] The key here seems to be to use cuModuleLoadDataEx, noting that the return values from cuLinkComplete are a system pointer to the linked CUBIN and the size of the image returned as a long[]. As per the C++ code, the pointer is just passed directly to the module data load. As a final comment, it would have been much simpler and easier if you had posted a proper repro case that could be been directly hacked on, rather than making me learn the rudiments of JCUDA and Java before I could create a useful repro case and get it to work. The documentation for JCUDA is basic, but complete, and against the working C++ example already provided, it only took a couple of minutes of reading to see how to do this."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "In jCuda one can load cuda files as PTX or CUBIN format and call(launch) __global__ functions (kernels) from Java. With keeping that in mind, I want to develop a framework with JCuda that gets user's __device__ function in a .cu file at run-time, loads and runs it. And I have already implemented a __global__ function, in which each thread finds out the start point of its related data, perform some computation, initialization and then call user's __device__ function. Here is my kernel pseudo code: extern \"C\" __device__ void userFunc(args); extern \"C\" __global__ void kernel(){ // initialize userFunc(args); // rest of the kernel } And user's __device__ function: extern \"C\" __device__ void userFunc(args){ // do something } And in Java side, here is the part that I load the modules(modules are made from ptx files which are successfully created from cuda files with this command: nvcc -m64 -ptx path/to/cudaFile -o cudaFile.ptx) CUmodule kernelModule = new CUmodule(); // 1 CUmodule userFuncModule = new CUmodule(); // 2 cuModuleLoad(kernelModule, ptxKernelFileName); // 3 cuModuleLoad(userFuncModule, ptxUserFuncFileName); // 4 When I try to run it I got error at line 3 : CUDA_ERROR_NO_BINARY_FOR_GPU. After some searching I get that my ptx file has some syntax error. After running this suggested command: ptxas -arch=sm_30 kernel.ptx I got: ptxas fatal : Unresolved extern function 'userFunc' Even when I replace line 3 with 4 to load userFunc before kernel I get this error. I got stuck at this phase. Is this the correct way to load multiple modules that need to be linked together in JCuda? Or is it even possible? Edit: Second part of the question is here",
        "answers": [
            [
                "The really short answer is: No, you can't load multiple modules into a context in the runtime API. You can do what you want, but it requires explicit setup and execution of a JIT linking call. I have no idea how (or even whether) that has been implemented in JCUDA, but I can show you how to do it with the standard driver API. Hold on... If you have a device function in one file, and a kernel in another, for example: // test_function.cu #include &lt;math.h&gt; __device__ float mathop(float &amp;x, float &amp;y, float &amp;z) { float res = sin(x) + cos(y) + sqrt(z); return res; } and // test_kernel.cu extern __device__ float mathop(float &amp; x, float &amp; y, float &amp; z); __global__ void kernel(float *xvals, float * yvals, float * zvals, float *res) { int tid = threadIdx.x + blockIdx.x * blockDim.x; res[tid] = mathop(xvals[tid], yvals[tid], zvals[tid]); } You can compile them to PTX as usual: $ nvcc -arch=sm_30 -ptx test_function.cu $ nvcc -arch=sm_30 -ptx test_kernel.cu $ head -14 test_kernel.ptx // // Generated by NVIDIA NVVM Compiler // // Compiler Build ID: CL-19324607 // Cuda compilation tools, release 7.0, V7.0.27 // Based on LLVM 3.4svn // .version 4.2 .target sm_30 .address_size 64 // .globl _Z6kernelPfS_S_S_ .extern .func (.param .b32 func_retval0) _Z6mathopRfS_S_ At runtime, your code must create a JIT link session, add each PTX to the linker session, then finalise the linker session. This will give you a handle to a compiled cubin image which can be loaded as a module as usual. The simplest possible driver API code to put this together looks like this: #include &lt;cstdio&gt; #include &lt;cuda.h&gt; #define drvErrChk(ans) { drvAssert(ans, __FILE__, __LINE__); } inline void drvAssert(CUresult code, const char *file, int line, bool abort=true) { if (code != CUDA_SUCCESS) { fprintf(stderr, \"Driver API Error %04d at %s %d\\n\", int(code), file, line); exit(-1); } } int main() { cuInit(0); CUdevice device; drvErrChk( cuDeviceGet(&amp;device, 0) ); CUcontext context; drvErrChk( cuCtxCreate(&amp;context, 0, device) ); CUlinkState state; drvErrChk( cuLinkCreate(0, 0, 0, &amp;state) ); drvErrChk( cuLinkAddFile(state, CU_JIT_INPUT_PTX, \"test_function.ptx\", 0, 0, 0) ); drvErrChk( cuLinkAddFile(state, CU_JIT_INPUT_PTX, \"test_kernel.ptx\" , 0, 0, 0) ); size_t sz; char * image; drvErrChk( cuLinkComplete(state, (void **)&amp;image, &amp;sz) ); CUmodule module; drvErrChk( cuModuleLoadData(&amp;module, image) ); drvErrChk( cuLinkDestroy(state) ); CUfunction function; drvErrChk( cuModuleGetFunction(&amp;function, module, \"_Z6kernelPfS_S_S_\") ); return 0; } You should be able to compile and run this as posted and verify it works OK. It should serve as a template for a JCUDA implementation, if they have JIT linking support implemented."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have a CUDA kernel that takes a list of structs. kernel&lt;&lt;&lt;blockCount,blockSize&gt;&gt;&gt;(MyStruct *structs); Each struct contains 3 pointers. typedef struct __align(16)__ { float* pointer1; float* pointer2; float* pointer3; } I have three device arrays containing floats and each pointer within the struct points to a float within one of the three device array. The list of structs represents a tree/graph structure which allows the kernel to execute recursive operations, depending on the order of the list of structs that is sent to the kernel. (This bit works in C++ so is not associated to my problem) What I would like to do is be able to send my struct of pointers from JCuda. I understand that this isn't natively possible unless it is flattened to a padded array as in this post. I understand all the issues with alignment and padding that may happen when sending a list of structs, it's essentially a repeating padded array which I am fine with. The bit I am not sure how to do, is populate my flattened struct buffer with pointers, for example, I would think i can do something like this: Pointer A = ....(underlying device array1) Pointer B = ....(underlying device array2) Pointer C = ....(underlying device array3) ByteBuffer structListBuffer = ByteBuffer.allocate(16*noSteps); for(int x = 0; x&lt;noSteps; x++) { // Get the underlying pointer values long pointer1 = A.withByteOffset(getStepOffsetA(x)).someGetUnderlyingPointerValueFunction(); long pointer2 = B.withByteOffset(getStepOffsetB(x)).someGetUnderlyingPointerValueFunction(); long pointer3 = C.withByteOffset(getStepOffsetC(x)).someGetUnderlyingPointerValueFunction(); // Build the struct structListBuffer.asLongBuffer().append(pointer1); structListBuffer.asLongBuffer().append(pointer2); structListBuffer.asLongBuffer().append(pointer3); structListBuffer.asLongBuffer().append(0); //padding } structListBuffer would then contain a list of structs in the way that the kernel would expect it. So is there any way to do the someGetUnderlyingPointerValueFunction() from a ByteBuffer?",
        "answers": [
            [
                "If I understood everything correctly, the main point of the question is whether there is such a magic function like long address = pointer.someGetUnderlyingPointerValueFunction(); that returns the address of the native pointer. The short answer: No, there is no such function. (Side note: A similar functionality was already requested in quite a while ago, but I have not yet added it. Mainly because such a function does not make sense for pointers to Java arrays or (non-direct) byte buffers. Additionally, manually handling structs with their paddings and alignments, and pointers with different sizes on 32 and 64 bit machines, and buffers that are big- or little endian is an endless source of headaches. But I see the point, and the possible application case, and so I'll most likely add something like a getAddress() function. Maybe only to the CUdeviceptr class, where it definitely makes sense - at least more than in the Pointer class. People will use this method to do odd things, and they will do things that will cause nasty crashes of the VM, but JCuda itself is such a thin abstraction layer that there is no safety net in this regard anyhow...) That said, you can work around the current limitation, with a method like this: private static long getPointerAddress(CUdeviceptr p) { // WORKAROUND until a method like CUdeviceptr#getAddress exists class PointerWithAddress extends Pointer { PointerWithAddress(Pointer other) { super(other); } long getAddress() { return getNativePointer() + getByteOffset(); } } return new PointerWithAddress(p).getAddress(); } Of course, this is ugly and clearly contradicts the intention of making the getNativePointer() and getByteOffset() methods protected. But it might eventually be replaced with some \"official\" method: private static long getPointerAddress(CUdeviceptr p) { return p.getAddress(); } and until now, this is probably the solution that is closest to what you can do on the C side. Here is an example that I wrote for testing this. The kernel is only a dummy kernel, that fills the structure with \"identifiable\" values (to see whether they end up in the right place), and is supposed to be launched with 1 thread only: typedef struct __declspec(align(16)) { float* pointer1; float* pointer2; float* pointer3; } MyStruct; extern \"C\" __global__ void kernel(MyStruct *structs) { structs[0].pointer1[0] = 1.0f; structs[0].pointer1[1] = 1.1f; structs[0].pointer1[2] = 1.2f; structs[0].pointer2[0] = 2.0f; structs[0].pointer2[1] = 2.1f; structs[0].pointer2[2] = 2.2f; structs[0].pointer3[0] = 3.0f; structs[0].pointer3[1] = 3.1f; structs[0].pointer3[2] = 3.2f; structs[1].pointer1[0] = 11.0f; structs[1].pointer1[1] = 11.1f; structs[1].pointer1[2] = 11.2f; structs[1].pointer2[0] = 12.0f; structs[1].pointer2[1] = 12.1f; structs[1].pointer2[2] = 12.2f; structs[1].pointer3[0] = 13.0f; structs[1].pointer3[1] = 13.1f; structs[1].pointer3[2] = 13.2f; } This kernel is launched in the following program (Note: The compilation of the PTX file is done here on the fly, with settings that may not match your application case. In doubt, you may compile your PTX file manually). The pointer1, pointer2 and pointer3 pointers of each struct are initialized so that they point to consecutive elements of the device buffers A, B and C, respectively, each with an offset that allows identifying the values that are written by the kernel. (Note that I tried to handle the two possible cases of running this either on a 32bit- or a 64bit machine, which implies different pointer sizese - although, currently, I can only test the 32bit version) import static jcuda.driver.JCudaDriver.*; import java.io.ByteArrayOutputStream; import java.io.File; import java.io.IOException; import java.io.InputStream; import java.nio.ByteBuffer; import java.nio.ByteOrder; import java.nio.IntBuffer; import java.nio.LongBuffer; import java.util.Arrays; import jcuda.Pointer; import jcuda.Sizeof; import jcuda.driver.CUcontext; import jcuda.driver.CUdevice; import jcuda.driver.CUdeviceptr; import jcuda.driver.CUfunction; import jcuda.driver.CUmodule; import jcuda.driver.JCudaDriver; public class JCudaPointersInStruct { public static void main(String args[]) throws IOException { JCudaDriver.setExceptionsEnabled(true); String ptxFileName = preparePtxFile(\"JCudaPointersInStructKernel.cu\"); cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); CUmodule module = new CUmodule(); cuModuleLoad(module, ptxFileName); CUfunction function = new CUfunction(); cuModuleGetFunction(function, module, \"kernel\"); int numElements = 9; CUdeviceptr A = new CUdeviceptr(); cuMemAlloc(A, numElements * Sizeof.FLOAT); cuMemsetD32(A, 0, numElements); CUdeviceptr B = new CUdeviceptr(); cuMemAlloc(B, numElements * Sizeof.FLOAT); cuMemsetD32(B, 0, numElements); CUdeviceptr C = new CUdeviceptr(); cuMemAlloc(C, numElements * Sizeof.FLOAT); cuMemsetD32(C, 0, numElements); int numSteps = 2; int sizeOfStruct = Sizeof.POINTER * 4; ByteBuffer hostStructsBuffer = ByteBuffer.allocate(numSteps * sizeOfStruct); if (Sizeof.POINTER == 4) { IntBuffer b = hostStructsBuffer.order( ByteOrder.nativeOrder()).asIntBuffer(); for(int x = 0; x&lt;numSteps; x++) { CUdeviceptr pointer1 = A.withByteOffset(getStepOffsetA(x)); CUdeviceptr pointer2 = B.withByteOffset(getStepOffsetB(x)); CUdeviceptr pointer3 = C.withByteOffset(getStepOffsetC(x)); //System.out.println(\"Step \"+x+\" pointer1 is \"+pointer1); //System.out.println(\"Step \"+x+\" pointer2 is \"+pointer2); //System.out.println(\"Step \"+x+\" pointer3 is \"+pointer3); b.put((int)getPointerAddress(pointer1)); b.put((int)getPointerAddress(pointer2)); b.put((int)getPointerAddress(pointer3)); b.put(0); } } else { LongBuffer b = hostStructsBuffer.order( ByteOrder.nativeOrder()).asLongBuffer(); for(int x = 0; x&lt;numSteps; x++) { CUdeviceptr pointer1 = A.withByteOffset(getStepOffsetA(x)); CUdeviceptr pointer2 = B.withByteOffset(getStepOffsetB(x)); CUdeviceptr pointer3 = C.withByteOffset(getStepOffsetC(x)); //System.out.println(\"Step \"+x+\" pointer1 is \"+pointer1); //System.out.println(\"Step \"+x+\" pointer2 is \"+pointer2); //System.out.println(\"Step \"+x+\" pointer3 is \"+pointer3); b.put(getPointerAddress(pointer1)); b.put(getPointerAddress(pointer2)); b.put(getPointerAddress(pointer3)); b.put(0); } } CUdeviceptr structs = new CUdeviceptr(); cuMemAlloc(structs, numSteps * sizeOfStruct); cuMemcpyHtoD(structs, Pointer.to(hostStructsBuffer), numSteps * sizeOfStruct); Pointer kernelParameters = Pointer.to( Pointer.to(structs) ); cuLaunchKernel(function, 1, 1, 1, 1, 1, 1, 0, null, kernelParameters, null); cuCtxSynchronize(); float hostA[] = new float[numElements]; cuMemcpyDtoH(Pointer.to(hostA), A, numElements * Sizeof.FLOAT); float hostB[] = new float[numElements]; cuMemcpyDtoH(Pointer.to(hostB), B, numElements * Sizeof.FLOAT); float hostC[] = new float[numElements]; cuMemcpyDtoH(Pointer.to(hostC), C, numElements * Sizeof.FLOAT); System.out.println(\"A \"+Arrays.toString(hostA)); System.out.println(\"B \"+Arrays.toString(hostB)); System.out.println(\"C \"+Arrays.toString(hostC)); } private static long getStepOffsetA(int x) { return x * Sizeof.FLOAT * 4 + 0 * Sizeof.FLOAT; } private static long getStepOffsetB(int x) { return x * Sizeof.FLOAT * 4 + 1 * Sizeof.FLOAT; } private static long getStepOffsetC(int x) { return x * Sizeof.FLOAT * 4 + 2 * Sizeof.FLOAT; } private static long getPointerAddress(CUdeviceptr p) { // WORKAROUND until a method like CUdeviceptr#getAddress exists class PointerWithAddress extends Pointer { PointerWithAddress(Pointer other) { super(other); } long getAddress() { return getNativePointer() + getByteOffset(); } } return new PointerWithAddress(p).getAddress(); } //------------------------------------------------------------------------- // Ignore this - in practice, you'll compile the PTX manually private static String preparePtxFile(String cuFileName) throws IOException { int endIndex = cuFileName.lastIndexOf('.'); if (endIndex == -1) { endIndex = cuFileName.length()-1; } String ptxFileName = cuFileName.substring(0, endIndex+1)+\"ptx\"; File cuFile = new File(cuFileName); if (!cuFile.exists()) { throw new IOException(\"Input file not found: \"+cuFileName); } String modelString = \"-m\"+System.getProperty(\"sun.arch.data.model\"); String command = \"nvcc \" + modelString + \" -ptx -arch sm_11 -lineinfo \"+ cuFile.getPath()+\" -o \"+ptxFileName; System.out.println(\"Executing\\n\"+command); Process process = Runtime.getRuntime().exec(command); String errorMessage = new String(toByteArray(process.getErrorStream())); String outputMessage = new String(toByteArray(process.getInputStream())); int exitValue = 0; try { exitValue = process.waitFor(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new IOException( \"Interrupted while waiting for nvcc output\", e); } if (exitValue != 0) { System.out.println(\"nvcc process exitValue \"+exitValue); System.out.println(\"errorMessage:\\n\"+errorMessage); System.out.println(\"outputMessage:\\n\"+outputMessage); throw new IOException( \"Could not create .ptx file: \"+errorMessage); } System.out.println(\"Finished creating PTX file\"); return ptxFileName; } private static byte[] toByteArray(InputStream inputStream) throws IOException { ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte buffer[] = new byte[8192]; while (true) { int read = inputStream.read(buffer); if (read == -1) { break; } baos.write(buffer, 0, read); } return baos.toByteArray(); } } The result is, as expected/desired: A [1.0, 1.1, 1.2, 0.0, 11.0, 11.1, 11.2, 0.0, 0.0] B [0.0, 2.0, 2.1, 2.2, 0.0, 12.0, 12.1, 12.2, 0.0] C [0.0, 0.0, 3.0, 3.1, 3.2, 0.0, 13.0, 13.1, 13.2]"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Well I have installed Nvidia driver and cuda 7 successfully and already built and run many jcuda (not mavenized) projects in eclipse. But I wanted to migrate to the mavenized version one. So, I cloned this repository and tried to build it. I am on Ubuntu 14.04 x64 and I have set up nexus, too. But when I want to build it with mvn clean package I got this: Downloading: http://localhost:8081/nexus/content/groups/public/jcuda/jcublas/0.7.0a/jcublas-0.7.0a.pom [WARNING] The POM for jcuda:jcublas:jar:0.7.0a is missing, no dependency information available for all of the *.poms and after these warnings, the build get finished unsuccessfully with: Could not resolve dependencies for project org.mystic:mavenized- jcuda:jar:0.1.2: The following artifacts could not be resolved: jcuda:jcublas:jar:0.7.0a, jcuda:jcuda:jar:0.7.0a ... and it lists all of the jcuda *.jars and *.sos Have I forgotten something? UPDATE: I used option -Dmaven.repo.local=repo with mvn clean package and every thing is ok.",
        "answers": [
            [
                "Yes, I'm 80% sure, that this is because of your local Nexus Failure to find jcuda:jcublas:jar:0.7.0a in http://localhost:8081/nexus/content/groups/public was cached in the local repository, resolution will not be reattempted until the update interval of nexus has elapsed or updates are force (taken from your log file) Could you turn it off? It should resolve the issue. P.S. Current situation with mavenized-jcuda project is, that it takes artifacts from local repo, since this artifacts are not presented in Maven Central or other hosted repositories."
            ],
            [
                "It was nexus problem. And I figured it out in a better way than turn it off. In the maven settings.xml (~/.m2/settings.xml) the mirrorOf pattern of * causes any repository request to be redirected to this mirror and to my single repository group. Another option is external:* which matches all repositories except those using localhost or file based repositories. Putting external:* instead of * solved my problem."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm trying to do something very very simple with shared memory in jcuda. My kernel: __shared__ int testMe; extern \"C\" __global__ void test() { testMe = 5; } Making shared memory global allows me to use it in device functions, unfortunately, I can't even declare shared memory global. Cuda just crashes with kernel cannot launch issue. I've also tried doing: extern __shared__ int test However nvcc compiler gives, cannot externally link shared memory error. I really want to get this to work to make writing kernels easier, but I don't see a way.",
        "answers": [
            [
                "You can't define static shared memory at translation unit scope, you have to declare it at kernel scope. If you do so, your shared memory will be the same memory for every thread in the same block. If you want to use memory that is same for every thread in all blocks, you have to use global memory (call cudaMalloc() in host-code and pass a pointer of the memory to your kernel)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have already looked at this http://www.javacodegeeks.com/2011/10/gpgpu-with-jcuda-good-bad-and-ugly.html which says I must modify my kernel to take only single dimensional arrays. However I refuse to believe that it is impossible to create a struct and copy it to device memory in JCuda. I would imagine the usual implementation would be to create a case class (scala terminology) that extends some native api, which can then be turned into a struct that can be safely passed into the kernel. Unfortunately I haven't found anything on google, hence the question.",
        "answers": [
            [
                "(The author of JCuda here (not \"JCUDA\", please)) As mentioned in the forum post linked from the comment: It is not impossible to use structs in CUDA kernels and fill them from JCuda side. It is just very complicated, and rarely beneficial. For the reason of why it is rarely beneficial to use structs at all in GPU programming, you will have to refer to the results that you'll find when you search about the differences between \"Array Of Structures\" versus \"Structure Of Arrays\". Usually, the latter is preferred for GPU computations, due to improved memory coalescing, but this is beyond what I can profoundly summarize in this answer. Here, I will only summarize why using structs in GPU computing is a bit difficult in general, and particularly difficult in JCuda/Java. In plain C, structs are (theoretically!) very simple, regarding the memory layout. Imagine a structure like struct Vertex { short a; float x; float y; float z; short b; }; Now you can create an array of these structs: Vertex* vertices = (Vertex*)malloc(n*sizeof(Vertex)); These structs will be guaranteed to be are laid out as one contiguous memory block: | vertices[0] || vertices[1] | | || | vertices -&gt; [ a| x | y | z | b][ a| x | y | z | b].... Since the CUDA kernel and the C code are compiled with the same compiler, there is not much room for musinderstandings. The host side says \"Here is some memory, interpret this as Vertex objects\", and the kernel will receive the same memory and work with it. Still, even in plain C, there is in practice some potential for unexpected problems. Compilers will often introduce paddings into these structs, to achieve certain alignments. The example structure might thus in fact have a layout like this: struct Vertex { short a; // 2 bytes char PADDING_0 // Padding byte char PADDING_1 // Padding byte float x; // 4 bytes float y; // 4 bytes float z; // 4 bytes short b; // 2 bytes char PADDING_2 // Padding byte char PADDING_3 // Padding byte }; Something like this may done in order to make sure that the structures are aligned to 32bit (4byte) word boundaries. Moreover, there are certain pragmas and compiler directives that may influence this alignment. CUDA additionally prefers certain memory alignments, and therefore these directives are used heavily in the CUDA headers. For short: When you define a struct in C, and then print the sizeof(YourStruct) (or the actual layout of the struct) to the console, you will have a hard time to predict what it will actually print. Expect some surprises. In JCuda/Java, the world is different. There simply are no structs. When you create a Java class like class Vertex { short a; float x; float y; float z; short b; } and then create an array of these Vertex vertices[2] = new Vertex[2]; vertices[0] = new Vertex(); vertices[1] = new Vertex(); then the these Vertex objects may be arbirarily scattered in memory. You don't even know how large one Vertex object is, and will hardly be able to find it out. Thus, trying to create an array of structures in JCuda and pass it to a CUDA kernel simply does not make sense. However, as mentioned above: It is still possible, in some form. If you know the memory layout that your structures will have in the CUDA kernel, then you can create a memory block that is \"compatible\" with this structure layout, and fill it from Java side. For something like the struct Vertex mentioned above, this could roughly (involving some pseudocode) look like this: // 1 short + 3 floats + 1 short, no paddings int sizeOfVertex = 2 + 4 + 4 + 4 + 2; // Allocate data for 2 vertices ByteBuffer data = ByteBuffer.allocateDirect(sizeOfVertex * 2); // Set vertices[0].a and vertices[0].x and vertices[0].y data.position(0).asShortBuffer().put(0, a0); data.position(2).asFloatBuffer().put(0, x0); data.position(2).asFloatBuffer().put(1, y0); // Set vertices[1].a and vertices[1].x and vertices[1].y data.position(sizeOfVertex+0).asShortBuffer().put(0, a1); data.position(sizeOfVertex+2).asFloatBuffer().put(0, x1); data.position(sizeOfVertex+2).asFloatBuffer().put(1, y1); // Copy the Vertex data to the device cudaMemcpy(deviceData, Pointer.to(data), cudaMemcpyHostToDevice); It basically boils down to keeping the memory in a ByteBuffer, and to manually access the memory regions that correspond to the desired fields of the desired structs. However, a warning: You have to consider the possibility that this will not be perfectly portable among several CUDA-C compiler versions or platforms. When you compile your kernel (that contains the struct definition) once on a 32bit Linux machine and once on a 64 bit Windows machine, then the structure layout might be different (and your Java code would have to be aware of this). (Note: One could define interface to simplify these accesses. And for JOCL, I tried to create utility classes that feel a bit more like C structs and automate the copying process to some extent. But in any case, it will be inconvenient (and not achieve a really good performance) compared to plain C)"
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I have a very simple scala jcuda program that adds a very large array. Everything compiles and runs just fine until I want to copy more than 4 bytes from my device to host. I am getting CUDA_ERROR_INVALID_VALUE when I try to copy more than 4 bytes. // This does pukes and gives CUDA_ERROR_INVALID_VALUE var hostOutput = new Array[Int](numElements) cuMemcpyDtoH( Pointer.to(hostOutput), deviceOutput, 8 ) // This runs just fine var hostOutput = new Array[Int](numElements) cuMemcpyDtoH( Pointer.to(hostOutput), deviceOutput, 4 ) To give better context of the actual program bellow is my kernel code which compiles and runs just fine: extern \"C\" __global__ void add(int n, int *a, int *b, int *sum) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i&lt;n) { sum[i] = a[i] + b[i]; } } Also I then translated some java sample code into my scala code. Anyway bellow is the entire program that runs: package dev import jcuda.driver.JCudaDriver._ import jcuda._ import jcuda.driver._ import jcuda.runtime._ /** * Created by dev on 6/7/15. */ object TestCuda { def init = { JCudaDriver.setExceptionsEnabled(true) // Input vector // Output vector // Load module // Load the ptx file. val kernelPath = \"/home/dev/IdeaProjects/jniopencl/src/main/resources/kernels/JCudaVectorAddKernel30.cubin\" cuInit(0) val device = new CUdevice cuDeviceGet(device, 0) val context = new CUcontext cuCtxCreate(context, 0, device) // Create and load module val module = new CUmodule() cuModuleLoad(module, kernelPath) // Obtain a function pointer to the kernel function. var add = new CUfunction() cuModuleGetFunction(add, module, \"add\") val numElements = 100000 val hostInputA = 1 to numElements toArray val hostInputB = 1 to numElements toArray val SI: Int = Sizeof.INT.asInstanceOf[Int] // Allocate the device input data, and copy // the host input data to the device var deviceInputA = new CUdeviceptr cuMemAlloc(deviceInputA, numElements * SI) cuMemcpyHtoD( deviceInputA, Pointer.to(hostInputA), numElements * SI ) var deviceInputB = new CUdeviceptr cuMemAlloc(deviceInputB, numElements * SI) cuMemcpyHtoD( deviceInputB, Pointer.to(hostInputB), numElements * SI ) // Allocate device output memory val deviceOutput = new CUdeviceptr() cuMemAlloc(deviceOutput, SI) // Set up the kernel parameters: A pointer to an array // of pointers which point to the actual values. val kernelParameters = Pointer.to( Pointer.to(Array[Int](numElements)), Pointer.to(deviceInputA), Pointer.to(deviceInputB), Pointer.to(deviceOutput) ) // Call the kernel function val blockSizeX = 256 val gridSizeX = Math.ceil(numElements / blockSizeX).asInstanceOf[Int] cuLaunchKernel( add, gridSizeX, 1, 1, blockSizeX, 1, 1, 0, null, kernelParameters, null ) cuCtxSynchronize // **** Code pukes here with that error // If I comment this out the program runs fine var hostOutput = new Array[Int](numElements) cuMemcpyDtoH( Pointer.to(hostOutput), deviceOutput, numElements ) hostOutput.foreach(print(_)) } } Anyway, just to let you know the specs of my computer. I'm running Ubuntu 14.04 on an optimus setup with a GTX 770M card which is compute 3.0 capable. I'm also running NVCC version 5.5. Lastly I'm running scala version 2.11.6 with Java 8. I'm a noob and would greatly appreciate any help.",
        "answers": [
            [
                "Here val deviceOutput = new CUdeviceptr() cuMemAlloc(deviceOutput, SI) you are allocating SI bytes - which is 4 bytes, as the size of one int. Writing more than 4 bytes to this device pointer will mess up things. It should be cuMemAlloc(deviceOutput, SI * numElements) And similarly, I think that the call in question should be cuMemcpyDtoH( Pointer.to(hostOutput), deviceOutput, numElements * SI ) (note the * SI for the last parameter)."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm trying to optimize a process which is to calculate all possible combinations of players to form partitions. To understand my problem, I use the following example. For exampe we have a set of players N = {1,2,3,4,5} , this players are regrouped like this {1,2},{3,4},{5}. It means player 1 will play with player 2 as a single player, and so one. Each group of players has a set of strategies or choices. Each player chooses the group to which he wants to belong, for example: the group {1,2} has these possibilities {{1,2};{1,2,3,4}}; i.e the players {1,2} either choose to stay together or to join the group {3,4}. The same explanation for the rest of players: {3,4}=&gt;{{3,4};{3,4,5};{1,2,3,4}} {5}=&gt;{{5};{3,4,5}} Now, the group of players choosing the same strategy will form a new group (coalition). For example, group {1,2} chose the strategy {1,2,3,4} i.e. the players {1,2} want to form a new group with players {3,4}. Players {3,4} choose the strategy {3,4,5}, player {5} choose the strategy {3,4,5}. The players that choose the same strategy will be grouped together to form a final partition of players like this: {1,2},{3,4,5}; players {3,4,5} have chosen the same strategy, so they are grouped together, players {1,2} chose a different strategy so they stay alone. I have programmed this process as a recursive function to get all admissible partitions of players. Another problem here: my function generate all possible partitions and I get only the admissibles which takes a lot of time. Now my question is if it is possible to resolve this problem without using a recursive function; i.e. in sequential form in order to use parallelism with JCUDA, especially when we have many players and so many partitions. What is the ideal solution here, MPI or JCUDA? import java.util.ArrayList; import java.util.Hashtable; public class Partitions { public Hashtable&lt;ArrayList&lt;Integer&gt;, ArrayList&lt;ArrayList&lt;Integer&gt;&gt;&gt; HashTab = new Hashtable&lt;ArrayList&lt;Integer&gt;,ArrayList&lt;ArrayList&lt;Integer&gt;&gt;&gt;(); // The key is the chosen strategy(coalition) and the value is the group of players have chosen the same coalition(strategy). // create partitions combination public void CalculStrategiesCombination (int index, ArrayList&lt;ObjectsCoalitions&gt; PlayerCoalitions, int K,int L) { index = index +1; if(index &lt; PlayerCoalitions.size()) { for(int j =0; j&lt; PlayerCoalitions.get(index).Coaltions.size(); j++) { if(!this.HashTab.containsKey(PlayerCoalitions.get(index).Coaltions.get(j))) { ArrayList&lt;ArrayList&lt;Integer&gt;&gt; e = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); e.add(PlayerCoalitions.get(index).Objects); this.HashTab.put(PlayerCoalitions.get(index).Coaltions.get(j), e); } else { this.HashTab.get(PlayerCoalitions.get(index).Coaltions.get(j)).add(PlayerCoalitions.get(index).Objects); } if(this.HashTab.size()&lt;K) { CalculStrategiesCombination (index, PlayerCoalitions,K,L); } if(this.HashTab.get(PlayerCoalitions.get(index).Coaltions.get(j)).size()==1) { this.HashTab.remove(PlayerCoalitions.get(index).Coaltions.get(j)); } else { this.HashTab.get(PlayerCoalitions.get(index).Coaltions.get(j)).remove(this.HashTab.get(PlayerCoalitions.get(index).Coaltions.get(j)).size()-1); } } } else { // Treatment....... } } } public class ObjectsCoalitions { public ArrayList&lt;Integer&gt;Objects = new ArrayList&lt;Integer&gt;(); // for example Objects = {1,2} public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; Coaltions = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt; (); // coalitions (strategis)={{1,2};{1,2,3,4}} }",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to implement JCUDA code on Hadoop. However while running the application, it gives unsatisfied link error: can not load the native library \"JCudaDriver-linux-x86_64\" with base name \"JCudaDriver\"' I have tried added the path to the library in LIBJARS, LD_LIBRARY_PATH, etc. I have tried setting the variable in the Java code using System.setProperty(). I have also tried the variable -Djava.library.path. However nothing seems to be working. Operating System: Linux Jcuda version: 0.7.0 CUDA version: 7.0 Hadoop version: 1.2.1",
        "answers": [],
        "votes": []
    },
    {
        "question": "I just developped my first program with JCUDA (on eclipse) and i want to execute it on other machines that don't have the same nvidia-driver and compilers i used to develop it. How can I export just the executable to another machine and execute it without recompiling the code ? Any help will be appreciated",
        "answers": [
            [
                "(Sorry, this may not fully answer your question if you expected a step-by-step-manual. I'll consider doing some experiments here, and maybe extend this answer or add the information on the website) For the driver API, the CUDA version on the target system must at least be that of the corresponding JCuda version - otherwise, newer CUDA functions would simply not be supported. Regarding the runtime libraries, quoting from the CUDA FAQ: Q: What do I need to distribute my CUDA application? Applications that use the driver API only need the CUDA driver library (\"nvcuda.dll\" under Windows), which is included as part of the standard NVIDIA driver install. Applications that use the runtime API also require the runtime library (\"cudart.dll\" under Windows), which is included in the CUDA Toolkit. It is permissible to distribute this library with your application under the terms of the End User License Agreement included with the CUDA Toolkit. So in doubt, you may include the \"cudart.dll\" that you used for compiling your application into the distribution package. Newer versions of CUDA also support static linking against the runtime DLL, but this is not (yet?) done for the JCuda runtime library."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using jCuda and tried to compile and run one of its examples(JCudppHashSample) available on jcuda on Ubuntu 14.04 with CUDA 7.0. But at compile time the library jcudpp says that there is no libcudpp available. And I am wondering is it missing from cuda sdk or not? I don't like the idea of getting its source and make it manually.",
        "answers": [
            [
                "It's not part of the CUDA toolkit or CUDA SDK. You will have to get it from its source. The JCuda pages you linked indicate: JCudpp is only a Java binding for CUDPP. That means, in order to use JCudpp, you need the CUDPP library. This library can be compiled from the source code that is available at the CUDPP home page"
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I'm new with JCuda. I try to write a simple example in Eclipse on Linux. I have this error, but I haven't understand what it means. Here the code: import jcuda.Pointer; import jcuda.runtime.JCuda; public class cudaTest { public static void main(String[] args) { Pointer pointer = new Pointer(); JCuda.cudaMalloc(pointer, 4); System.out.println(\"Pointer: \" + pointer); JCuda.cudaFree(pointer); } } I add the Jcuda.jar from the Java Build Path, and edit the Native library location by selecting the extracted JCuda file. The error is: Error while loading native library \"JCudaRuntime-linux-x86_64\" with base name \"JCudaRuntime\" Operating system name: Linux Architecture : amd64 Architecture bit size: 64 Stack trace from the attempt to load the library as a resource: java.lang.NullPointerException: No resource found with name '/lib/libJCudaRuntime-linux-x86_64.so' at jcuda.LibUtils.loadLibraryResource(LibUtils.java:151) at jcuda.LibUtils.loadLibrary(LibUtils.java:83) at jcuda.runtime.JCuda.initialize(JCuda.java:303) at jcuda.runtime.JCuda.&lt;clinit&gt;(JCuda.java:290) at cudaTest.main(cudaTest.java:8) Stack trace from the attempt to load the library as a file: java.lang.UnsatisfiedLinkError: /home/Faith/JCuda-All-0.4.2-bin-linux-x86_64/libJCudaRuntime-linux-x86_64.so: libcudart.so.4: Ne peut ouvrir le fichier d'objet partag\u00e9: Aucun fichier ou dossier de ce type at java.lang.ClassLoader$NativeLibrary.load(Native Method) at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1965) at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1890) at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1880) at java.lang.Runtime.loadLibrary0(Runtime.java:849) at java.lang.System.loadLibrary(System.java:1088) at jcuda.LibUtils.loadLibrary(LibUtils.java:94) at jcuda.runtime.JCuda.initialize(JCuda.java:303) at jcuda.runtime.JCuda.&lt;clinit&gt;(JCuda.java:290) at cudaTest.main(cudaTest.java:8) Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Could not load the native library at jcuda.LibUtils.loadLibrary(LibUtils.java:129) at jcuda.runtime.JCuda.initialize(JCuda.java:303) at jcuda.runtime.JCuda.&lt;clinit&gt;(JCuda.java:290) at cudaTest.main(cudaTest.java:8) Could tou help me please, I'm really need the solution. Thank you in advance.",
        "answers": [
            [
                "Look for libJCudaRuntime-linux-x86_64.so file in your system, and make a symoblic link to /lib/libJCudaRuntime-linux-x86_64.so OR add the path of the directory where you find it to your ldconfig"
            ],
            [
                "Copy the .so files to lib directory: $ sudo cp *.so /lib/"
            ],
            [
                "One of the possible idea - is to give a try to my project called Mavenized JCuda https://github.com/MysterionRise/mavenized-jcuda It's specifically created for the purpose of not fighting against classpath and other problems. It's really easy to use it, all you need to do - is to put needed version of JCuda in pom.xml, then run mvn clean package and then mvn exec:exec. Full HowTo is available on Github page. Feel free to ask me question about that or raise question about the project"
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "What is the best way to operate with complex numbers using jCuda? Should I use cuComplex format or is there any other solution (like an array with real and imaginary parts going one after another)? I would really appreciate examples of java code with this type of calculations. As my purpose is to solve big systems of linear equations with complex numbers using GPU, I would not like to attach to jCuda only. What are the alternative ways to conduct such calculations with GPU?",
        "answers": [
            [
                "First, concerning your question about computations with Java on the GPU in general, I wrote a few words about this here. Your application case seems to be very specific. You should probably describe in more detail what your actual intention is, because this will govern all design decisions. Until now, I only can give some basic hints. Deciding which is the most appropriate solution is up to you. One of the main difficulties when bridging between the Java world and the GPU world is the fundamentally different memory handling. Memory layout in C/C++ The cuComplex struct in CUDA is defined as typedef float2 cuFloatComplex typedef cuFloatComplex cuComplex; where float2 is basically something like a struct float2 { float x; float y; }; (with some additional specifiers for alignment etc.) Now, When you allocate an array of cuComplex values in a C/C++ program, then you'll just write something like cuComplex *c = new cuComplex[100]; In this case, it is guaranteed that the memory of all these cuComplex values will be a single, contiguous memory block. This memory block just consists of all the x and y values of the complex numbers, one after the other: _____________________________ c -&gt; | x0 | y0 | x1 | y1 | x2 | y2 |... |____|____|____|____|____|____| This contiguous memory block can easily be copied to the device: One takes the pointer, and invokes a call like cudaMemcpy(device, c, sizeof(cuComplex)*n, cudaMemcpyHostToDevice); Memory layout in Java Consider the case where you create a Java class that is structurally equal to the cuComplex struct, and allocate an array of these: class cuComplex { public float x; public float y; } cuComplex c[] = new cuComplex[100]; Then you do not have a single contiguous memory block of float values. Instead, you have an array of references to cuComplex objects, and the respective x and y values are scattered all around: ____________________ c -&gt; | c0 | c1 | c2 |... |______|______|______| | | | v v v [x,y] [x,y] [x,y] The key point here is: You can't copy an (Java) array of cuComplex objects to the device!. This has several implications. In the comments, you already referred to the cublasSetVector method that takes a cuComplex array as an argument, and where I tried to emphasize that this is not the most efficient solution, but only there for convenience. In fact, this method works by internally creating a new ByteBuffer in order to have a contiguous memory block, filling this ByteBuffer with the values from the cuComplex[] array, and then copying this ByteBuffer to the device. Of course, this imposes an overhead that you'd most likely want to avoid in performance-critical applications. There are several options for tackling this problem. Fortunately, for complex numbers, the solution is comparatively easy: Don't use the cuComplex structure to represent arrays of complex numbers Instead, you should represent your array of complex numbers as a single, contiguous memory block, where the real and imaginary parts of the complex numbers are interleaved, each being a single float or double value, respectively. This will allow the greatest interoperability between different backends (leaving out certain details, like alignment requirements). Unfortunately, this may cause some inconveniences and raise some questions, and there is no one-fits-all solution for this. If one tried to generalize this, to not only refer to complex numbers, but to \"structures\" in general, then there is a \"pattern\" that could be applied: One could create interfaces for the structures, and create a collection of these structures that is a list of instances of classes implementing this interface, which are all backed by a contiguous memory block. This may be appropriate for certain cases. But for complex numbers, the memory overhead of having a Java object for each complex number may be prohibitively large. The other extreme, of only handling raw float[] or double[] arrays may also not be the best solution. For example: If you have an array of float values that represents complex numbers, then how could you multiply one of these complex numbers with another one? One \"intermediate\" solution could to create an interface that allows accessing the real and imaginary parts of complex numbers. In the implementation, these complex numbers are stored in a single array, as described above. I sketched such an implementation here. NOTE: This is only intended as an example, to show the basic idea, and to show how it may work together with something like JCublas. For you, a different strategy may be more appropriate, depending on your actual goals: Which other backends (besides JCuda) should there be? How \"convenient\" should it be to handle the complex numbers on Java side? What should the structures (classes/interfaces) for handling the complex numbers on the Java side look like? For short: You should have a very clear idea about what your application/library should be able to do, before proceeding with the implementation. import static jcuda.jcublas.JCublas2.*; import static jcuda.jcublas.cublasOperation.CUBLAS_OP_N; import static jcuda.runtime.JCuda.*; import java.util.Random; import jcuda.*; import jcuda.jcublas.cublasHandle; import jcuda.runtime.cudaMemcpyKind; // An interface describing an array of complex numbers, residing // on the host, with methods for accessing the real and imaginary // parts of the complex numbers, as well as methods for copying // the underlying data from and to the device interface cuComplexHostArray { int size(); float getReal(int i); float getImag(int i); void setReal(int i, float real); void setImag(int i, float imag); void set(int i, cuComplex c); void set(int i, float real, float imag); cuComplex get(int i, cuComplex c); void copyToDevice(Pointer devicePointer); void copyFromDevice(Pointer devicePointer); } // A default implementation of a cuComplexHostArray, backed // by a single float[] array class DefaultCuComplexHostArray implements cuComplexHostArray { private final int size; private final float data[]; DefaultCuComplexHostArray(int size) { this.size = size; this.data = new float[size * 2]; } @Override public int size() { return size; } @Override public float getReal(int i) { return data[i+i]; } @Override public float getImag(int i) { return data[i+i+1]; } @Override public void setReal(int i, float real) { data[i+i] = real; } @Override public void setImag(int i, float imag) { data[i+i+1] = imag; } @Override public void set(int i, cuComplex c) { data[i+i+0] = c.x; data[i+i+1] = c.y; } @Override public void set(int i, float real, float imag) { data[i+i+0] = real; data[i+i+1] = imag; } @Override public cuComplex get(int i, cuComplex c) { float real = getReal(i); float imag = getImag(i); if (c != null) { c.x = real; c.y = imag; return c; } return cuComplex.cuCmplx(real, imag); } @Override public void copyToDevice(Pointer devicePointer) { cudaMemcpy(devicePointer, Pointer.to(data), size * Sizeof.FLOAT * 2, cudaMemcpyKind.cudaMemcpyHostToDevice); } @Override public void copyFromDevice(Pointer devicePointer) { cudaMemcpy(Pointer.to(data), devicePointer, size * Sizeof.FLOAT * 2, cudaMemcpyKind.cudaMemcpyDeviceToHost); } } // An example that performs a \"gemm\" with complex numbers, once // in Java and once in JCublas2, and verifies the result public class JCublas2ComplexSample { public static void main(String args[]) { testCgemm(500); } public static void testCgemm(int n) { cuComplex alpha = cuComplex.cuCmplx(0.3f, 0.2f); cuComplex beta = cuComplex.cuCmplx(0.1f, 0.7f); int nn = n * n; System.out.println(\"Creating input data...\"); Random random = new Random(0); cuComplex[] rhA = createRandomComplexRawArray(nn, random); cuComplex[] rhB = createRandomComplexRawArray(nn, random); cuComplex[] rhC = createRandomComplexRawArray(nn, random); random = new Random(0); cuComplexHostArray hA = createRandomComplexHostArray(nn, random); cuComplexHostArray hB = createRandomComplexHostArray(nn, random); cuComplexHostArray hC = createRandomComplexHostArray(nn, random); System.out.println(\"Performing Cgemm with Java...\"); cgemmJava(n, alpha, rhA, rhB, beta, rhC); System.out.println(\"Performing Cgemm with JCublas...\"); cgemmJCublas(n, alpha, hA, hB, beta, hC); boolean passed = isCorrectResult(hC, rhC); System.out.println(\"testCgemm \"+(passed?\"PASSED\":\"FAILED\")); } private static void cgemmJCublas( int n, cuComplex alpha, cuComplexHostArray A, cuComplexHostArray B, cuComplex beta, cuComplexHostArray C) { int nn = n * n; // Create a CUBLAS handle cublasHandle handle = new cublasHandle(); cublasCreate(handle); // Allocate memory on the device Pointer dA = new Pointer(); Pointer dB = new Pointer(); Pointer dC = new Pointer(); cudaMalloc(dA, nn * Sizeof.FLOAT * 2); cudaMalloc(dB, nn * Sizeof.FLOAT * 2); cudaMalloc(dC, nn * Sizeof.FLOAT * 2); // Copy the memory from the host to the device A.copyToDevice(dA); B.copyToDevice(dB); C.copyToDevice(dC); // Execute cgemm Pointer pAlpha = Pointer.to(new float[]{alpha.x, alpha.y}); Pointer pBeta = Pointer.to(new float[]{beta.x, beta.y}); cublasCgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, n, n, n, pAlpha, dA, n, dB, n, pBeta, dC, n); // Copy the result from the device to the host C.copyFromDevice(dC); // Clean up cudaFree(dA); cudaFree(dB); cudaFree(dC); cublasDestroy(handle); } private static void cgemmJava( int n, cuComplex alpha, cuComplex A[], cuComplex B[], cuComplex beta, cuComplex C[]) { for (int i = 0; i &lt; n; ++i) { for (int j = 0; j &lt; n; ++j) { cuComplex prod = cuComplex.cuCmplx(0, 0); for (int k = 0; k &lt; n; ++k) { cuComplex ab = cuComplex.cuCmul(A[k * n + i], B[j * n + k]); prod = cuComplex.cuCadd(prod, ab); } cuComplex ap = cuComplex.cuCmul(alpha, prod); cuComplex bc = cuComplex.cuCmul(beta, C[j * n + i]); C[j * n + i] = cuComplex.cuCadd(ap, bc); } } } private static cuComplex[] createRandomComplexRawArray( int n, Random random) { cuComplex c[] = new cuComplex[n]; for (int i = 0; i &lt; n; i++) { float real = random.nextFloat(); float imag = random.nextFloat(); c[i] = cuComplex.cuCmplx(real, imag); } return c; } private static cuComplexHostArray createRandomComplexHostArray( int n, Random random) { cuComplexHostArray c = new DefaultCuComplexHostArray(n); for (int i = 0; i &lt; n; i++) { float real = random.nextFloat(); float imag = random.nextFloat(); c.setReal(i, real); c.setImag(i, imag); } return c; } private static boolean isCorrectResult( cuComplexHostArray result, cuComplex reference[]) { float errorNormX = 0; float errorNormY = 0; float refNormX = 0; float refNormY = 0; for (int i = 0; i &lt; result.size(); i++) { float diffX = reference[i].x - result.getReal(i); float diffY = reference[i].y - result.getImag(i); errorNormX += diffX * diffX; errorNormY += diffY * diffY; refNormX += reference[i].x * result.getReal(i); refNormY += reference[i].y * result.getImag(i); } errorNormX = (float) Math.sqrt(errorNormX); errorNormY = (float) Math.sqrt(errorNormY); refNormX = (float) Math.sqrt(refNormX); refNormY = (float) Math.sqrt(refNormY); if (Math.abs(refNormX) &lt; 1e-6) { return false; } if (Math.abs(refNormY) &lt; 1e-6) { return false; } return (errorNormX / refNormX &lt; 1e-6f) &amp;&amp; (errorNormY / refNormY &lt; 1e-6f); } } (By the way: I'll probably take parts of this answer and extend them to become samples and/or \"How To...\" pages for JCuda. The task of providing some information like this has already been on my \"todo\" list for quite a while)."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Is there any way to debug cuda functions called by java (jcuda) ? My program launch this exception: Exception in thread \"main\" jcuda.CudaException:CUDA_ERROR_ILLEGAL_ADDRESS at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:288) at jcuda.driver.JCudaDriver.cuCtxSynchronize(JCudaDriver.java:1852) at CalculateurGPU.updateAndCompute(rGPUcalculator.java:129) at Test.main(Test.java:90) I have a very long cuda code, and i can't find any information in that to help me find where the error is. Another question: when i click on \"JCudaDriver.java:288\" i don't have access to it, it says \"source not found\". How can i attach thesesources to my project ? Thanks in advance",
        "answers": [
            [
                "The best way I have been able to debug my JCuda executable code is by putting printf statements everywhere. Obviously delete them after you are done but you have to start somewhere. Add the following to your .java code that is calling your kernel code. JCudaDriver.cuCtxSetLimit(CUlimit.CU_LIMIT_PRINTF_FIFO_SIZE, 4096); Then inside your CUDA kernel, use printf(\"\"); statements in your CUDA kernel code to determine where the problem exists. If I had a guess it would be due to you access a piece of memory that you shouldn't be. I would also suggest wrapping the printf with a if(gtid == 0){printf(\"hello\\n\");} otherwise you are going to have every thread trying to perform the printf. Source: http://forum.byte-welt.net/byte-welt-projekte-projects/swogl-jcuda-jocl/jcuda/3255-gpu-printf-compute-capility-2-0-a.html EDIT: I would like to add that there has been a debugging section added on the JCuda website. It suggests using a tool called \"cuda-memcheck\" to aid in debugging your CUDA applications. http://www.jcuda.org/debugging/Debugging.html"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Closed. This question needs to be more focused. It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 8 years ago. Improve this question I want to improve a JAVA program performances using GPU programming. After some research on the internet I found that it is possible if i use jcuda or jocl, but the problem is that the kernel code must be written in C in both cases. and the algorithm that i want every thread to execute is very complicated to be written in C (it does some computations to know if there will be an accident between two aircrafts) so an object-oriented language is necessery. Is there a solution ? or must we translate the whole project to C++ ? Thanks for your help !",
        "answers": [
            [
                "Simple rule: if it needs object orientation, it looses its performance. Even if you are using GPU acceleration. I would advise you to identify the parallel parts of your program code. You do not have to transfer all of your algorithm to the GPU device. Is there any aspect of paralellization, e.g. arrays or grids that are filled? What kind is your simulation message exchange? Is it explicit, i.e. sending messages around your kernels, or implicit via synchronization. You should at least give us some more information about you algorithm and its data layout."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using JCUDA and would like to know if the JNI objects are smart enough to deallocate when they are garbage collected? I can understand why this may not work in all situations, but I know it will work in my situation, so my followup question is: how can I accomplish this? Is there a \"mode\" I can set? Will I need to build a layer of abstraction? Or maybe the answer really is \"no don't ever try that\" so then why not? EDIT: I'm referring only to native objects created via JNI, not Java objects. I am aware that all Java objects are treated equally W.R.T. garbage collection.",
        "answers": [
            [
                "Usually, such libraries do not deallocate memory due to garbage collection. Particularly: JCuda does not do this, and has no option or \"mode\" where this can be done. The reason is quite simple: It does not work. You'll often have a pattern like this: void doSomethingWithJCuda() { CUdeviceptr data = new CUdeviceptr(); cuMemAlloc(data, 1000); workWith(data); // *(See notes below) } Here, native memory is allocated, and the Java object serves as a \"handle\" to this native memory. At the last line, the data object goes out of scope. Thus, it becomes eligible for garbage collection. However, there are two issues: 1. The garbage collector will only destroy the Java object, and not free the memory that was allocated with cuMemAlloc or any other native call. So you'll usually have to free the native memory, by explicitly calling cuMemFree(data); before leaving the method. 2. You don't know when the Java object will be garbage collected - or whether it will be garbage collected at all. A common misconception is that an object becomes garbage collected when it is no longer reachable, but this is not necessarily true. As bmargulies pointed out in his answer: One means is to have a Java object with a finalizer that makes the necessary JNI call to free native memory. It may look like a viable option to simply override the finalize() method of these \"handle\" objects, and do the cuMemFree(this) call there. This has been tried, for example, by the authors of JavaCL (a library that also allows using the GPU with Java, and thus, is conceptually somewhat similar to JCuda). But it simply does not work: Even if a Java object is no longer reachable, this does not mean that it will be garbage collected immediately. You simply don't know when the finalize() method will be called. This can easily cause nasty errors: When you have 100 MB of GPU memory, you can use 10 CUdeviceptr objects, each allocating 10MB. Your GPU memory is full. But for Java, these few CUdeviceptr objects only occupy a few bytes, and the finalize() method may not be called at all during the runtime of the application, because the JVM simply does not need to reclaim these few bytes of memory. (Omitting discussions about hacky workarounds here, like calling System.gc() or so - the bottom line is: It does not work). So answering your actual question: JCuda is a very low-level library. This means that you have the full power, but also the full responsibilities of manual memory management. I know that this is \"inconvenient\". When I started creating JCuda, I originally intended it as a low-level backend for an object-oriented wrapper library. But creating a robust, stable and universally applicable abstraction layer for a complex general-purpose library like CUDA is challenging, and I did not dare to tackle such a project - last but not least because of the complexities that are implied by ... things like garbage collection..."
            ],
            [
                "Java objects created in JNI are equal to all other Java objects, and are garbage collected and destroyed when their time comes. To keep such objects from being destroyed too early, we often use JNI function env-&gt;NewGlobalRef() (but its usage is by no ways limited to objects created in native). On the other hand, native objects are not subject to garbage collection."
            ],
            [
                "There are two cases here. Native code allocates Java Objects. These objects are GC's like all other Java objects. If the native goofs up and holds strong references, it can prevent GC. Native code allocates Native memory. The GC knows nothing about it; it's up to the library to arrange to free it. One means is to have a Java object with a finalizer that makes the necessary JNI call to free native memory."
            ]
        ],
        "votes": [
            2.0000001,
            2.0000001,
            2.0000001
        ]
    },
    {
        "question": "I'm doing a real-to-complex FFT with the org.apache.commons.math3.transform library as following: private Complex[] fft(double[] values) { FastFourierTransformer ffTransformer = new FastFourierTransformer(DftNormalization.STANDARD); Complex[] result = ffTransformer.transform(values, TransformType.FORWARD); return result; } This gives me a org.apache.commons.math3.complex array with the result. This works fine. Now I want to perform exactly the same with the JCufft library. I tried to do to it as following: private Complex[] fft(double[] values) { double inputJCufft[] = values.clone(); double outputJCufft[] = new double[values.length * 2]; cufftHandle plan = new cufftHandle(); JCufft.cufftPlan1d(plan, values.length, cufftType.CUFFT_D2Z, 1); JCufft.cufftExecD2Z(plan, inputJCufft, outputJCufft); JCufft.cufftDestroy(plan); Complex[] result = BaseHelper.getComplexArray(outputJCufft); return result; } public static Complex[] getComplexArray(double[] input) { List&lt;Complex&gt; result = new ArrayList&lt;Complex&gt;(); for (int i = 0; i &lt; input.length - 1; i = i + 2) { result.add(new Complex(input[i], input[i + 1])); } return result.toArray(new Complex[result.size()]); } However, when I'm comparing the results, they differ from each other. What I have not taken into account, what am I doing wrong? Thanks for your help.",
        "answers": [
            [
                "Ok, it was my lack of understanding the FFT... I changed the getComplexArray method to the following and now it works fine: public static Complex[] getComplexArray(double[] input) { Deque&lt;Complex&gt; deque = new LinkedList&lt;Complex&gt;(); int size = (input.length / 4 + 1) * 2; for (int i = 0; i &lt; size; i = i + 2) { deque.add(new Complex(input[i], input[i + 1])); } List&lt;Complex&gt; result = new ArrayList&lt;Complex&gt;(deque); deque.removeLast(); while (deque.size() &gt; 1) { result.add(deque.removeLast().conjugate()); } return result.toArray(new Complex[result.size()]); }"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm working on a business project that is done in Java, and it needs huge computation power to compute business markets. Simple math, but with huge amount of data. We ordered some CUDA GPUs to try it with and since Java is not supported by CUDA, I'm wondering where to start. Should I build a JNI interface? Should I use JCUDA or are there other ways? I don\u2019t have experience in this field and I would like if someone could direct me to something so I can start researching and learning.",
        "answers": [
            [
                "First of all, you should be aware of the fact that CUDA will not automagically make computations faster. On the one hand, because GPU programming is an art, and it can be very, very challenging to get it right. On the other hand, because GPUs are well-suited only for certain kinds of computations. This may sound confusing, because you can basically compute anything on the GPU. The key point is, of course, whether you will achieve a good speedup or not. The most important classification here is whether a problem is task parallel or data parallel. The first one refers, roughly speaking, to problems where several threads are working on their own tasks, more or less independently. The second one refers to problems where many threads are all doing the same - but on different parts of the data. The latter is the kind of problem that GPUs are good at: They have many cores, and all the cores do the same, but operate on different parts of the input data. You mentioned that you have \"simple math but with huge amount of data\". Although this may sound like a perfectly data-parallel problem and thus like it was well-suited for a GPU, there is another aspect to consider: GPUs are ridiculously fast in terms of theoretical computational power (FLOPS, Floating Point Operations Per Second). But they are often throttled down by the memory bandwidth. This leads to another classification of problems. Namely whether problems are memory bound or compute bound. The first one refers to problems where the number of instructions that are done for each data element is low. For example, consider a parallel vector addition: You'll have to read two data elements, then perform a single addition, and then write the sum into the result vector. You will not see a speedup when doing this on the GPU, because the single addition does not compensate for the efforts of reading/writing the memory. The second term, \"compute bound\", refers to problems where the number of instructions is high compared to the number of memory reads/writes. For example, consider a matrix multiplication: The number of instructions will be O(n^3) when n is the size of the matrix. In this case, one can expect that the GPU will outperform a CPU at a certain matrix size. Another example could be when many complex trigonometric computations (sine/cosine etc) are performed on \"few\" data elements. As a rule of thumb: You can assume that reading/writing one data element from the \"main\" GPU memory has a latency of about 500 instructions.... Therefore, another key point for the performance of GPUs is data locality: If you have to read or write data (and in most cases, you will have to ;-)), then you should make sure that the data is kept as close as possible to the GPU cores. GPUs thus have certain memory areas (referred to as \"local memory\" or \"shared memory\") that usually is only a few KB in size, but particularly efficient for data that is about to be involved in a computation. So to emphasize this again: GPU programming is an art, that is only remotely related to parallel programming on the CPU. Things like Threads in Java, with all the concurrency infrastructure like ThreadPoolExecutors, ForkJoinPools etc. might give the impression that you just have to split your work somehow and distribute it among several processors. On the GPU, you may encounter challenges on a much lower level: Occupancy, register pressure, shared memory pressure, memory coalescing ... just to name a few. However, when you have a data-parallel, compute-bound problem to solve, the GPU is the way to go. A general remark: Your specifically asked for CUDA. But I'd strongly recommend you to also have a look at OpenCL. It has several advantages. First of all, it's an vendor-independent, open industry standard, and there are implementations of OpenCL by AMD, Apple, Intel and NVIDIA. Additionally, there is a much broader support for OpenCL in the Java world. The only case where I'd rather settle for CUDA is when you want to use the CUDA runtime libraries, like CUFFT for FFT or CUBLAS for BLAS (Matrix/Vector operations). Although there are approaches for providing similar libraries for OpenCL, they can not directly be used from Java side, unless you create your own JNI bindings for these libraries. You might also find it interesting to hear that in October 2012, the OpenJDK HotSpot group started the project \"Sumatra\": http://openjdk.java.net/projects/sumatra/ . The goal of this project is to provide GPU support directly in the JVM, with support from the JIT. The current status and first results can be seen in their mailing list at http://mail.openjdk.java.net/mailman/listinfo/sumatra-dev However, a while ago, I collected some resources related to \"Java on the GPU\" in general. I'll summarize these again here, in no particular order. (Disclaimer: I'm the author of http://jcuda.org/ and http://jocl.org/ ) (Byte)code translation and OpenCL code generation: https://github.com/aparapi/aparapi : An open-source library that is created and actively maintained by AMD. In a special \"Kernel\" class, one can override a specific method which should be executed in parallel. The byte code of this method is loaded at runtime using an own bytecode reader. The code is translated into OpenCL code, which is then compiled using the OpenCL compiler. The result can then be executed on the OpenCL device, which may be a GPU or a CPU. If the compilation into OpenCL is not possible (or no OpenCL is available), the code will still be executed in parallel, using a Thread Pool. https://github.com/pcpratts/rootbeer1 : An open-source library for converting parts of Java into CUDA programs. It offers dedicated interfaces that may be implemented to indicate that a certain class should be executed on the GPU. In contrast to Aparapi, it tries to automatically serialize the \"relevant\" data (that is, the complete relevant part of the object graph!) into a representation that is suitable for the GPU. https://code.google.com/archive/p/java-gpu/ : A library for translating annotated Java code (with some limitations) into CUDA code, which is then compiled into a library that executes the code on the GPU. The Library was developed in the context of a PhD thesis, which contains profound background information about the translation process. https://github.com/ochafik/ScalaCL : Scala bindings for OpenCL. Allows special Scala collections to be processed in parallel with OpenCL. The functions that are called on the elements of the collections can be usual Scala functions (with some limitations) which are then translated into OpenCL kernels. Language extensions http://www.ateji.com/px/index.html : A language extension for Java that allows parallel constructs (e.g. parallel for loops, OpenMP style) which are then executed on the GPU with OpenCL. Unfortunately, this very promising project is no longer maintained. http://www.habanero.rice.edu/Publications.html (JCUDA) : A library that can translate special Java Code (called JCUDA code) into Java- and CUDA-C code, which can then be compiled and executed on the GPU. However, the library does not seem to be publicly available. https://www2.informatik.uni-erlangen.de/EN/research/JavaOpenMP/index.html : Java language extension for for OpenMP constructs, with a CUDA backend Java OpenCL/CUDA binding libraries https://github.com/ochafik/JavaCL : Java bindings for OpenCL: An object-oriented OpenCL library, based on auto-generated low-level bindings http://jogamp.org/jocl/www/ : Java bindings for OpenCL: An object-oriented OpenCL library, based on auto-generated low-level bindings http://www.lwjgl.org/ : Java bindings for OpenCL: Auto-generated low-level bindings and object-oriented convenience classes http://jocl.org/ : Java bindings for OpenCL: Low-level bindings that are a 1:1 mapping of the original OpenCL API http://jcuda.org/ : Java bindings for CUDA: Low-level bindings that are a 1:1 mapping of the original CUDA API Miscellaneous http://sourceforge.net/projects/jopencl/ : Java bindings for OpenCL. Seem to be no longer maintained since 2010 http://www.hoopoe-cloud.com/ : Java bindings for CUDA. Seem to be no longer maintained"
            ],
            [
                "From the research I have done, if you are targeting Nvidia GPUs and have decided to use CUDA over OpenCL, I found three ways to use the CUDA API in java. JCuda (or alternative)- http://www.jcuda.org/. This seems like the best solution for the problems I am working on. Many of libraries such as CUBLAS are available in JCuda. Kernels are still written in C though. JNI - JNI interfaces are not my favorite to write, but are very powerful and would allow you to do anything CUDA can do. JavaCPP - This basically lets you make a JNI interface in Java without writing C code directly. There is an example here: What is the easiest way to run working CUDA code in Java? of how to use this with CUDA thrust. To me, this seems like you might as well just write a JNI interface. All of these answers basically are just ways of using C/C++ code in Java. You should ask yourself why you need to use Java and if you can't do it in C/C++ instead. If you like Java and know how to use it and don't want to work with all the pointer management and what-not that comes with C/C++ then JCuda is probably the answer. On the other hand, the CUDA Thrust library and other libraries like it can be used to do a lot of the pointer management in C/C++ and maybe you should look at that. If you like C/C++ and don't mind pointer management, but there are other constraints forcing you to use Java, then JNI might be the best approach. Though, if your JNI methods are just going be wrappers for kernel commands you might as well just use JCuda. There are a few alternatives to JCuda such as Cuda4J and Root Beer, but those do not seem to be maintained. Whereas at the time of writing this JCuda supports CUDA 10.1. which is the most up-to-date CUDA SDK. Additionally there are a few java libraries that use CUDA, such as deeplearning4j and Hadoop, that may be able to do what you are looking for without requiring you to write kernel code directly. I have not looked into them too much though."
            ],
            [
                "I'd start by using one of the projects out there for Java and CUDA: http://www.jcuda.org/"
            ],
            [
                "Marco13 already provided an excellent answer. In case you are in search for a way to use the GPU without implementing CUDA/OpenCL kernels, I would like to add a reference to the finmath-lib-cuda-extensions (finmath-lib-gpu-extensions) http://finmath.net/finmath-lib-cuda-extensions/ (disclaimer: I am the maintainer of this project). The project provides an implementation of \"vector classes\", to be precise, an interface called RandomVariable, which provides arithmetic operations and reduction on vectors. There are implementations for the CPU and GPU. There are implementation using algorithmic differentiation or plain valuations. The performance improvements on the GPU are currently small (but for vectors of size 100.000 you may get a factor &gt; 10 performance improvements). This is due to the small kernel sizes. This will improve in a future version. The GPU implementation use JCuda and JOCL and are available for Nvidia and ATI GPUs. The library is Apache 2.0 and available via Maven Central."
            ],
            [
                "There is not much information on the nature of the problem and the data, so difficult to advise. However, would recommend to assess the feasibility of other solutions, that can be easier to integrate with java and enables horizontal as well as vertical scaling. The first I would suggest to look at is an open source analytical engine called Apache Spark https://spark.apache.org/ that is available on Microsoft Azure but probably on other cloud IaaS providers too. If you stick to involving your GPU then the suggestion is to look at other GPU supported analytical databases on the market that fits in the budget of your organisation."
            ]
        ],
        "votes": [
            504.0000001,
            4.0000001,
            3.0000001,
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am trying to implement radial average of matrix element in CUDA, where I have to find and print average of all neighbouring elements (including itself) of every matrix element. Following is what I ended up with(for radius = 1): __global__ void matrix_avg(float *ad,float *bd) { float t1, t2, t3, t4, t5, t6, t7, t8, t9, avg; int i = blockIdx.y * blockDim.y + threadIdx.y; int j = blockIdx.x * blockDim.x + threadIdx.x; int k = (i*N)+(j); if(i==0) { if(j==0) { bd[k]=(ad[k+1]+ad[k+N]+ad[k+N+1]+ad[k])/4; } else if(j==N-1) { bd[k]=(ad[k-1]+ad[k+N]+ad[k+N-1]+ad[k])/4; } else { bd[k]=(ad[k-1]+ad[k+1]+ad[k+N-1]+ad[k+N]+ad[k+N+1]+ad[k])/6; } } else if(i==N-1) { if(j==0) { bd[k]=(ad[k+1]+ad[k-N]+ad[k-N+1]+ad[k])/4; } else if(j==N-1) { bd[k]=(ad[k-1]+ad[k-N]+ad[k-N-1]+ad[k])/4; } else { bd[k]=(ad[k-1]+ad[k+1]+ad[k-N-1]+ad[k-N]+ad[k-N+1]+ad[k])/6; } } else if(j==0) { bd[k]=(ad[k-N]+ad[k-N+1]+ad[k+1]+ad[k+N]+ad[k+N+1]+ad[k])/6; } else if(j==N-1) { bd[k]=(ad[k-N-1]+ad[k-N]+ad[k-1]+ad[k+N-1]+ad[k+N]+ad[k])/6; } else { t1=ad[k-N-1]; t2=ad[k-N]; t3=ad[k-N+1]; t4=ad[k-1]; t5=ad[k+1]; t6=ad[k+N-1]; t7=ad[k+N]; t8=ad[k+N+1]; t9=ad[k]; avg=(t1+t2+t3+t4+t5+t6+t7+t8+t9)/9; bd[k]=avg; } } My above code checks for the conditions of top row, bottom row, right most and left most column elements, for which it has to calculate average of 6 elements. Ans also for 4 corner elements for which it has to calculate average of 4 elements. For remaining inner elements it has to calculate average of 9 elements. Above code is just a simple conversion C to CUDA program. I am looking for most efficient way without using shared memory to write the program., for any given radius. Any algorithm, pseudo code or suggestion will do. Thanks in advance.",
        "answers": [
            [
                "This is how I implemented radial average of matrix elements without using shared memory: __global__ void matrix_avg(float *ad,float *bd, int radius, int N) { int counter =0,i,j; float sum=0.0; int globalRow = blockIdx.y * blockDim.y + threadIdx.y; int globalCol = blockIdx.x * blockDim.x + threadIdx.x; for(i=-radius;i&lt;=radius;i++) { for(j=-radius;j&lt;=radius;j++) { if(((globalRow+i)&lt;0) || ((globalCol+j)&lt;0) || ((globalRow+i)&gt;=N) || ((globalCol+j)&gt;=N)) { sum = sum + 0; } else { sum = sum + ad[(globalRow+i)*N+(globalCol+j)]; counter++; } } } bd[globalRow*N+globalCol]=sum/counter; } ad - Input matrix. bd - Output matrix N - Matrix dimension. (I kept it as square matrix for now) radius - radius to perform range for average calculation"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to implement simple matrix multiplication program using shared memory in JCuda. Following is my JCudaSharedMatrixMul.java code: import static jcuda.driver.JCudaDriver.cuCtxCreate; import static jcuda.driver.JCudaDriver.cuCtxSynchronize; import static jcuda.driver.JCudaDriver.cuDeviceGet; import static jcuda.driver.JCudaDriver.cuInit; import static jcuda.driver.JCudaDriver.cuLaunchKernel; import static jcuda.driver.JCudaDriver.cuMemAlloc; import static jcuda.driver.JCudaDriver.cuMemFree; import static jcuda.driver.JCudaDriver.cuMemcpyDtoH; import static jcuda.driver.JCudaDriver.cuMemcpyHtoD; import static jcuda.driver.JCudaDriver.cuModuleGetFunction; import static jcuda.driver.JCudaDriver.cuModuleLoad; import static jcuda.runtime.JCuda.cudaEventCreate; import static jcuda.runtime.JCuda.cudaEventRecord; import static jcuda.runtime.JCuda.*; import java.io.ByteArrayOutputStream; import java.io.File; import java.io.IOException; import java.io.InputStream; import java.util.Scanner; import jcuda.Pointer; import jcuda.Sizeof; import jcuda.driver.CUcontext; import jcuda.driver.CUdevice; import jcuda.driver.CUdeviceptr; import jcuda.driver.CUfunction; import jcuda.driver.CUmodule; import jcuda.driver.JCudaDriver; import jcuda.runtime.cudaEvent_t; public class JCudaSharedMatrixMul { public static void main(String[] args) throws IOException { // Enable exceptions and omit all subsequent error checks JCudaDriver.setExceptionsEnabled(true); // Create the PTX file by calling the NVCC String ptxFilename = preparePtxFile(\"JCudaSharedMatrixMulKernel.cu\"); //Initialize the driver and create a context for the first device. cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet (device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); //Load PTX file CUmodule module = new CUmodule(); cuModuleLoad(module,ptxFilename); //Obtain a function pointer to the Add function CUfunction function = new CUfunction(); cuModuleGetFunction(function, module, \"jCudaSharedMatrixMulKernel\"); int numRows = 16; int numCols = 16; //Allocate and fill Host input Matrices: float hostMatrixA[] = new float[numRows*numCols]; float hostMatrixB[] = new float[numRows*numCols]; float hostMatrixC[] = new float[numRows*numCols]; for(int i = 0; i&lt;numRows; i++) { for(int j = 0; j&lt;numCols; j++) { hostMatrixA[i*numCols+j] = (float) 1; hostMatrixB[i*numCols+j] = (float) 1; } } // Allocate the device input data, and copy the // host input data to the device CUdeviceptr devMatrixA = new CUdeviceptr(); cuMemAlloc(devMatrixA, numRows * numCols * Sizeof.FLOAT); //This is the part where it gives me the error cuMemcpyHtoD(devMatrixA, Pointer.to(hostMatrixA), numRows * numCols * Sizeof.FLOAT); CUdeviceptr devMatrixB = new CUdeviceptr(); cuMemAlloc(devMatrixB, numRows * numCols * Sizeof.FLOAT); //This is the part where it gives me the error cuMemcpyHtoD(devMatrixB, Pointer.to(hostMatrixB ), numRows * numCols * Sizeof.FLOAT); //Allocate device matrix C to store output CUdeviceptr devMatrixC = new CUdeviceptr(); cuMemAlloc(devMatrixC, numRows * numCols * Sizeof.FLOAT); // Set up the kernel parameters: A pointer to an array // of pointers which point to the actual values. Pointer kernelParameters = Pointer.to( Pointer.to(new int[]{numCols}), Pointer.to(devMatrixA), Pointer.to(devMatrixB), Pointer.to(devMatrixC)); //Kernel thread configuration int blockSize = 16; int gridSize = 1; cudaEvent_t start = new cudaEvent_t(); cudaEvent_t stop = new cudaEvent_t(); cudaEventCreate(start); cudaEventCreate(stop); long start_nano=System.nanoTime(); cudaEventRecord(start, null); cuLaunchKernel(function, gridSize, 1, 1, blockSize, 16, 1, 250, null, kernelParameters, null); cuCtxSynchronize(); cudaEventRecord(stop, null); long end_nano=System.nanoTime(); float elapsedTimeMsArray[] = { Float.NaN }; cudaEventElapsedTime(elapsedTimeMsArray, start, stop); float elapsedTimeMs = elapsedTimeMsArray[0]; System.out.println(\"Time Required (Using cudaevent elapsed time) = \" + \" \" +elapsedTimeMs+ \"Time Required (Using nanotime)= \"+(end_nano-start_nano)/1000000); // Allocate host output memory and copy the device output // to the host. //This is the part where it gives me the error cuMemcpyDtoH(Pointer.to(hostMatrixC), devMatrixC, numRows * numCols * Sizeof.FLOAT); //verify the result for (int i =0; i&lt;numRows; i++) { for (int j =0; j&lt;numRows; j++) { System.out.print(\" \"+ hostMatrixC[i*numCols+j]); } System.out.println(\"\"); } cuMemFree(devMatrixA); cuMemFree(devMatrixB); cuMemFree(devMatrixC); } private static String preparePtxFile(String cuFileName) throws IOException { int endIndex = cuFileName.lastIndexOf('.'); if (endIndex == -1) endIndex = cuFileName.length()-1; { } String ptxFileName = cuFileName.substring(0, endIndex+1)+\"ptx\"; File ptxFile = new File(ptxFileName); if (ptxFile.exists()) { return ptxFileName; } File cuFile = new File(cuFileName); if (!cuFile.exists()) { throw new IOException(\"Input file not found: \"+cuFileName); } String modelString = \"-m\"+System.getProperty(\"sun.arch.data.model\"); String command = \"nvcc \" + modelString + \" -ptx \"+ cuFile.getPath()+\" -o \"+ptxFileName; System.out.println(\"Executing\\n\"+command); Process process = Runtime.getRuntime().exec(command); String errorMessage = new String(toByteArray(process.getErrorStream())); String outputMessage = new String(toByteArray(process.getInputStream())); int exitValue = 0; try { exitValue = process.waitFor(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new IOException( \"Interrupted while waiting for nvcc output\", e); } if (exitValue != 0) { System.out.println(\"nvcc process exitValue \"+exitValue); System.out.println(\"errorMessage:\\n\"+errorMessage); System.out.println(\"outputMessage:\\n\"+outputMessage); throw new IOException( \"Could not create .ptx file: \"+errorMessage); } System.out.println(\"Finished creating PTX file\"); return ptxFileName; } private static byte[] toByteArray(InputStream inputStream) throws IOException { ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte buffer[] = new byte[8192]; while (true) { int read = inputStream.read(buffer); if (read == -1) { break; } baos.write(buffer, 0, read); } return baos.toByteArray(); } } Following is my JCudaSharedMatrixMulKernel.cu code: extern \"C\" __global__ void jCudaSharedMatrixMulKernel(int N,float *ad,float *bd,float *cd) { float pvalue=0; int TILE=blockDim.x; int ty=threadIdx.y; int tx=threadIdx.x; __shared__ float ads[4][4]; __shared__ float bds[4][4]; int Row = blockIdx.y * blockDim.y + threadIdx.y; int Col = blockIdx.x * blockDim.x + threadIdx.x; for(int i=0;i&lt; N/TILE;++i) { ads[ty][tx] = ad[Row * N + (i * TILE) + tx]; bds[ty][tx] = bd[(i * TILE + ty) * N + Col]; __syncthreads(); for(int k=0;k&lt;TILE;k++) pvalue += ads[ty][k] * bds[k][tx]; __syncthreads(); } cd[Row * N + Col] = pvalue; } In my above example total shared memory used per block is 2*4*4*4 = 128 bytes. In the cuLaunchKernel when I define sharedMemBytes parameter as 0(zero) then it gives me following error: **Exception in thread \"main\" jcuda.CudaException: CUDA_ERROR_LAUNCH_FAILED at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:282) at jcuda.driver.JCudaDriver.cuCtxSynchronize(JCudaDriver.java:1795) at JCudaSharedMatrixMul.main(JCudaSharedMatrixMul.java:121)** When I define it as 128 then it gives the same above error. But when I make it as 129 then it gives me correct output! When I give any value between 129 to 49024 then it gives me the correct result. My question is why I am not able to get the correct output when I am defining it as 128? Also what is the maximum shared memory can be defined? Why this 129-49024 range is working here?",
        "answers": [
            [
                "You're launching blocks of 16x16 threads: cuLaunchKernel(function, gridSize, 1, 1, blockSize, 16, 1, &lt;-- the first two params are block.x and block.y 250, null, kernelParameters, null); so __shared__ float ads[4][4]; should not be working at all. For example, these lines of kernel code would be accessing those shared arrays out-of-bounds for some threads: ads[ty][tx] = ad[Row * N + (i * TILE) + tx]; bds[ty][tx] = bd[(i * TILE + ty) * N + Col]; ^ ^ | tx goes from 0..15 for a 16x16 threadblock ty goes from 0..15 for a 16x16 threadblock Your code is broken in this respect. If you run your code with cuda-memcheck it may catch these out-of-bounds accesses, even in your \"passing\" case. Looking at the matrixMulDrv cuda sample code, will be instructive, and you'll see that the shared memory allocation is 2*block_size*block_size, as it should be for your case as well, but your shared memory definitions should be [16][16] not [4][4] It may be that the shared memory allocation granularity just happens to work when you exceed 128 bytes, but there is a defect in your code. Your shared definitions should be: __shared__ float ads[16][16]; __shared__ float bds[16][16]; Since the above allocations are static allocations, and the sharedMemBytes parameter is defined as dynamic shared memory allocation, for this example you don't need to allocate any (0 is OK) dynamic shared memory, and it still works. The difference between static and dynamic is covered here. The maximum shared memory per block is available in the documentation, or if you run the cuda deviceQuery sample code. It is 48K bytes for cc2.0 and newer devices."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am new to jCUDA and java as well. I am trying to compile a vector addition program from NVIDIA samples using eclipse on Redhat Linux. Steps I followed: 1. Enter: nvcc -ptx JCudaVectorAddKernel.cu -&gt; It generates JCudaVectorAddKernel.ptx file 2. Execute following program: JCudaVectorAdd.java: package JCudaVectorAdd; import static jcuda.driver.JCudaDriver.*; import java.io.*; import jcuda.*; import jcuda.driver.*; public class JCudaVectorAdd { public static void main(String[] args) throws IOException { // Enable exceptions and omit all subsequent error checks JCudaDriver.setExceptionsEnabled(true); // Create the PTX file by calling the NVCC String ptxFileName = preparePtxFile(\"JCudaVectorAddKernel.cu\"); // Initialize the driver and create a context for the first device. cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); // Load the ptx file. CUmodule module = new CUmodule(); cuModuleLoad(module, ptxFileName); // Obtain a function pointer to the \"add\" function. CUfunction function = new CUfunction(); cuModuleGetFunction(function, module, \"add\"); int numElements = 100000; // Allocate and fill the host input data float hostInputA[] = new float[numElements]; float hostInputB[] = new float[numElements]; for(int i = 0; i &lt; numElements; i++) { hostInputA[i] = (float)i; hostInputB[i] = (float)i; } // Allocate the device input data, and copy the // host input data to the device CUdeviceptr deviceInputA = new CUdeviceptr(); cuMemAlloc(deviceInputA, numElements * Sizeof.FLOAT); cuMemcpyHtoD(deviceInputA, Pointer.to(hostInputA), numElements * Sizeof.FLOAT); CUdeviceptr deviceInputB = new CUdeviceptr(); cuMemAlloc(deviceInputB, numElements * Sizeof.FLOAT); cuMemcpyHtoD(deviceInputB, Pointer.to(hostInputB), numElements * Sizeof.FLOAT); // Allocate device output memory CUdeviceptr deviceOutput = new CUdeviceptr(); cuMemAlloc(deviceOutput, numElements * Sizeof.FLOAT); // Set up the kernel parameters: A pointer to an array // of pointers which point to the actual values. Pointer kernelParameters = Pointer.to( Pointer.to(new int[]{numElements}), Pointer.to(deviceInputA), Pointer.to(deviceInputB), Pointer.to(deviceOutput) ); // Call the kernel function. int blockSizeX = 256; int gridSizeX = (int)Math.ceil((double)numElements / blockSizeX); cuLaunchKernel(function, gridSizeX, 1, 1, // Grid dimension blockSizeX, 1, 1, // Block dimension 0, null, // Shared memory size and stream kernelParameters, null // Kernel- and extra parameters ); cuCtxSynchronize(); // Allocate host output memory and copy the device output // to the host. float hostOutput[] = new float[numElements]; cuMemcpyDtoH(Pointer.to(hostOutput), deviceOutput, numElements * Sizeof.FLOAT); // Verify the result boolean passed = true; for(int i = 0; i &lt; numElements; i++) { float expected = i+i; if (Math.abs(hostOutput[i] - expected) &gt; 1e-5) { System.out.println( \"At index \"+i+ \" found \"+hostOutput[i]+ \" but expected \"+expected); passed = false; break; } } System.out.println(\"Test \"+(passed?\"PASSED\":\"FAILED\")); // Clean up. cuMemFree(deviceInputA); cuMemFree(deviceInputB); cuMemFree(deviceOutput); } private static String preparePtxFile(String cuFileName) throws IOException { int endIndex = cuFileName.lastIndexOf('.'); if (endIndex == -1) { endIndex = cuFileName.length()-1; } String ptxFileName = cuFileName.substring(0, endIndex+1)+\"ptx\"; File ptxFile = new File(ptxFileName); if (ptxFile.exists()) { return ptxFileName; } File cuFile = new File(cuFileName); if (!cuFile.exists()) { throw new IOException(\"Input file not found: \"+cuFileName); } String modelString = \"-m\"+System.getProperty(\"sun.arch.data.model\"); String command = \"nvcc \" + modelString + \" -ptx \"+ cuFile.getPath()+\" -o \"+ptxFileName; System.out.println(\"Executing\\n\"+command); Process process = Runtime.getRuntime().exec(command); String errorMessage = new String(toByteArray(process.getErrorStream())); String outputMessage= new String(toByteArray(process.getInputStream())); int exitValue = 0; try { exitValue = process.waitFor(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new IOException( \"Interrupted while waiting for nvcc output\", e); } if (exitValue != 0) { System.out.println(\"nvcc process exitValue \"+exitValue); System.out.println(\"errorMessage:\\n\"+errorMessage); System.out.println(\"outputMessage:\\n\"+outputMessage); throw new IOException( \"Could not create .ptx file: \"+errorMessage); } System.out.println(\"Finished creating PTX file\"); return ptxFileName; } private static byte[] toByteArray(InputStream inputStream) throws IOException { ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte buffer[] = new byte[8192]; while (true) { int read = inputStream.read(buffer); if (read == -1) { break; } baos.write(buffer, 0, read); } return baos.toByteArray(); } } JCudaVectorAddKernel.cu: extern \"C\" __global__ void add(int n, float *a, float *b, float *sum) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i&lt;n) { sum[i] = a[i] + b[i]; } } Both 'JCudaVectorAddKernel.cu' and 'JCudaVectorAddKernel.java' are in on the same path: /home/sandeep/workspace1/jCuda/jCudaVectorAdd/src/jCudaVectorAdd When I execute the program in eclipse it gives me following error: Exception in thread \"main\" java.io.IOException: Input file not found: JCudaVectorAddKernel.cu at JCudaVectorAdd.JCudaVectorAdd.preparePtxFile(JCudaVectorAdd.java:128) at JCudaVectorAdd.JCudaVectorAdd.main(JCudaVectorAdd.java:20) Is there anything to do related to the compile command? or ptx/.cu file path? Please guide me if I am going in the wrong direction.",
        "answers": [
            [
                "The sample is trying to compile the PTX file at runtime, and it prints the command that it is trying to execute. When you are compiling the PTX file manually, this may actually not be necessary, and you can change String ptxFileName = preparePtxFile(\"JCudaVectorAddKernel.cu\"); to String ptxFileName = \"JCudaVectorAddKernel.ptx\"; In any case, you may print the file names File ptxFile = new File(ptxFileName); System.out.println(ptxFile.getCanonicalPath()); and File cuFile = new File(cuFileName); System.out.println(cuFile.getCanonicalPath()); to see whether they match the expected directories. But the PTX file (and the CU file) should probably be located in /home/sandeep/workspace1/jCuda/jCudaVectorAdd/ (that is, in the \"root\" directory of your project)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am NEW to java programming and trying to code a matrix multiplication program in jCUDA. While transferring the data from host to device and vice versa I use: cuMemcpyHtoD(devMatrixA, Pointer.to(hostMatrixA), numRows * numCols * Sizeof.FLOAT); cuMemcpyHtoD(devMatrixB, Pointer.to(hostMatrixA), numRows * numCols * Sizeof.FLOAT); cuMemcpyDtoH(Pointer.to(hostMatrixC), devMatrixC, numRows * numCols * Sizeof.FLOAT); Here, the devMatrixA, devMatrixB and devMatrixC are the matrices to be stored on device memory. And hostMatrixA, hostMatrixB and hostMatrixC are the matrices stored on my Host memory. When I call above functions for data transfer, it gives me following error 'The method to(byte[]) in the type Pointer is not applicable for the arguments (float[][])' with 'to' in 'Pointer.to(' is red underlined. I am using eclipse. I have given my complete code as below. Pardon my java knowledge, and please suggest if I am going into wrong direction. Package JCudaMatrixAddition; import static jcuda.driver.JCudaDriver.*; import java.io.*; import jcuda.*; import jcuda.driver.*; import jcuda.Pointer; import jcuda.Sizeof; public class JCudaMatrixAddition { public static void main(String[] args) throws IOException { // Enable exceptions and omit all subsequent error checks JCudaDriver.setExceptionsEnabled(true); // Create the PTX file by calling the NVCC String ptxFilename = preparePtxFile(\"JCudaMatrixAdditionKernel.cu\"); //Initialize the driver and create a context for the first device. cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet (device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); //Load PTX file CUmodule module = new CUmodule(); cuModuleLoad(module,ptxFilename); //Obtain a function pointer to the Add function CUfunction function = new CUfunction(); cuModuleGetFunction(function, module, \"add\"); int numRows = 32; int numCols = 32; //Allocate and fill Host input Matrices: float hostMatrixA[][] = new float[numRows][numCols]; float hostMatrixB[][] = new float[numRows][numCols]; float hostMatrixC[][] = new float[numRows][numCols]; for(int i = 0; i&lt;numRows; i++) { for(int j = 0; j&lt;numCols; j++) { hostMatrixA[i][j] = (float) 1.0; hostMatrixB[i][j] = (float) 1.0; } } // Allocate the device input data, and copy the // host input data to the device CUdeviceptr devMatrixA = new CUdeviceptr(); cuMemAlloc(devMatrixA, numRows * numCols * Sizeof.FLOAT); //This is the part where it gives me the error cuMemcpyHtoD(devMatrixA, Pointer.to(hostMatrixA), numRows * numCols * Sizeof.FLOAT); CUdeviceptr devMatrixB = new CUdeviceptr(); cuMemAlloc(devMatrixB, numRows * numCols * Sizeof.FLOAT); //This is the part where it gives me the error cuMemcpyHtoD(devMatrixB, Pointer.to(hostMatrixA), numRows * numCols * Sizeof.FLOAT); //Allocate device matrix C to store output CUdeviceptr devMatrixC = new CUdeviceptr(); cuMemAlloc(devMatrixC, numRows * numCols * Sizeof.FLOAT); // Set up the kernel parameters: A pointer to an array // of pointers which point to the actual values. Pointer kernelParameters = Pointer.to(Pointer.to(new int[]{numRows}), Pointer.to(new int[]{numRows}), Pointer.to(devMatrixA), Pointer.to(devMatrixB), Pointer.to(devMatrixC)); //Kernel thread configuration int blockSize = 32; int gridSize = 1; cuLaunchKernel(function, gridSize, 1, 1, blockSize, 32, 1, 0, null, kernelParameters, null); cuCtxSynchronize(); // Allocate host output memory and copy the device output // to the host. //This is the part where it gives me the error cuMemcpyDtoH(Pointer.to(hostMatrixC), devMatrixC, numRows * numCols * Sizeof.FLOAT); //verify the result for (int i =0; i&lt;numRows; i++) { for (int j =0; j&lt;numRows; j++) { System.out.print(\" \"+ hostMatrixB[i][j]); } System.out.println(\"\"); } cuMemFree(devMatrixA); cuMemFree(devMatrixB); cuMemFree(devMatrixC); }",
        "answers": [
            [
                "You can not copy a float[][] array from the host to the device directly. When you create a float[][] array, then this is not a large array of float values. Instead, it is an array of arrays. Imagine that you could even create an array like float array[][] = new float[3]; array[0] = new float[42]; array[1] = null; array[2] = new float[1234]; This is simply not a contiguous memory block, and thus, such an array can not be copied to the device. When handling matrices in CUDA (not only in JCuda, but in CUDA in general), they are usually represented as 1-dimensional arrays. So in this case, you could declare your matrices as float hostMatrixA[] = new float[numRows*numCols]; In order to access the matrix elements, you have to compute the appropriate index: int row = ...; int col = ...; hostMatrix[col+row*numCols] = 123.0f; // Column-major // Or hostMatrix[row+col*numRows] = 123.0f; // Row-major The difference between the last two lines is that one assumes column-major order, and the other assumes row-major order. See the Wikipedia site about row-major order for details. Some side notes: The CUDA matrix libraries like CUBLAS use column-major ordering, so it is probably a good idea to follow the same convention. Particularly when you later want to use CUBLAS/JCublas functions. For example, the cublasSgeam function already offers the functionality to perform a matrix addition. When you only want to do a matrix addition, you will not see a speedup when using CUDA/JCuda. I wrote a summary about this in this answer. And BTW: Technically, it is possible to use \"2D arrays\". The JCudaDriverSample shows how this can be done. But it is rather inconvenient and not recommended for matrix operations."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "In CUDA we can get to know about errors simply by checking return type of functions such as cudaMemcpy(), cudaMalloc() etc. which is cudaError_t with cudaSuccess. Is there any method available in JCuda to check error for functions such as cuMemcpyHtoD(), cuMemAlloc(), cuLaunchKernel() etc.",
        "answers": [
            [
                "First of all, the methods of JCuda (should) behave exactly like the corresponding CUDA functions: They return an error code in form of an int. These error codes are also defined in... the cudaError class for the Runtime API the CUresult class for the Driver API the cublasStatus class for JCublas the cufftResult class for JCufft the curandStatus class for JCurand the cusparseStatus class for JCusparse and are the same error codes as in the respective CUDA library. All these classes additionally have a static method called stringFor(int) - for example, cudaError#stringFor(int) and CUresult#stringFor(int). These methods return a human-readable String representation of the error code. So you could do manual error checks, for example, like this: int error = someCudaFunction(); if (error != 0= { System.out.println(\"Error code \"+error+\": \"+cudaError.stringFor(error)); } which might print something like Error code 10: cudaErrorInvalidDevice But... ...the error checks may be a hassle. You might have noticed in the CUDA samples that NVIDIA introduced some macros that simplify the error checks. And similarly, I added optional exception checks for JCuda: All the libraries offer a static method called setExceptionsEnabled(boolean). When calling JCudaDriver.setExceptionsEnabled(true); then all subsequent method calls for the Driver API will automatically check the method return values, and throw a CudaException when there was any error. (Note that this method exists separately for all libraries. E.g. the call would be JCublas.setExceptionsEnabled(true) when using JCublas) The samples usually enable exception checks right at the beginning of the main method. And I'd recommend to also do this, at least during the development phase. As soon as it is clear that the program does not contain any errors, one could disable the exceptions, but there's hardly a reason to do so: They conveniently offer clear information about which error occurred, whereas otherwise, the calls may fail silently."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I was able to compile the native 0.6.0 libraries of JCuda on a Mac OSX 10.9.3 64-bit system (http://www.jcuda.org/downloads/downloads.html). Unfortunately Im not smart enough to package everything into a jar. Can someone help me to do so? I created a Netbeans project and copied the contents of all provided Java src folders into it. Then I packaged everything into a jar. Using Jarsplice 0.4.0 I then tried to build a Jar file containing all the native library files listed below (except the first .a file). Unfortunately when I linked the resulting jar into my project and run it, it wasnt working since a libJCudaRuntime-apple-x86_64.jnilib was missing. Thank you in advance Radek My lib folder contains: libCommonJNI.a libJCublas-apple-x86_64.dylib libJCublas2-apple-x86_64.dylib libJCudaDriver-apple-x86_64.dylib libJCudaRuntime-apple-x86_64.dylib libJCufft-apple-x86_64.dylib libJCurand-apple-x86_64.dylib libJCusparse-apple-x86_64.dylib",
        "answers": [
            [
                "Just to have this answered here as well: The native libraries can not be packed directly into a JAR (or rather: They can currently not be loaded if they are only contained in the JAR). Although the infrastructure of the JCuda LibUtils class basically offers the possibility to do this, it is not \"officially\" supported. One reason for that is that this would only make sense when all native libraries for all operating systems were available (in 32 and 64 bit). Then it would be possible to put all these libraries into a JAR, and unpack and load them transparently at runtime. It usually takes a while until all native libraries are available. The problem may be alleviated with things like https://github.com/MysterionRise/mavenized-jcuda , which manages the dependencies to the native libraries."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using CUDA to generate this ABGR output image. The image in question is stored in a uchar4 array. Each element of the array represents the color of each pixel in the image. Obviously, this output array is a 2D image but it is allocated in CUDA as a linear memory of interleaved bytes. I know that CUDA can easily map this array to an OpenGL Vertex Buffer Object. My question is, assuming that I have the RGB value of every pixel in an image, along with the width and height of the image, how can I draw this image to screen using OpenGL? I know that some kind of shader must be involved but since my knowledge is very little, I have no idea how a shader can use the color of each pixel, but map it to correct screen pixels. I know I should increase my knowledge in OpenGL, but this seems like a trivial task. If there is an easy way for me to draw this image, I'd rather not spend much time learning OpenGL.",
        "answers": [
            [
                "I finally figured out an easy way to do what I wanted. Unfortunately, I did not know about the existence of the sample that Robert was talking about on NVIDIA's website. Long story short, the easiest way to draw the image was to define a Pixel Buffer Object in OpenGL, register the buffer with CUDA and pass it as an output array of uchar4 to the CUDA kernel. Here is a quick pseudo-code based on JOGL and JCUDA that shows the steps involved. Most of the code was obtained from the sample on NVIDIA's website: 1) Creaing the OpenGL buffers GL2 gl = drawable.getGL().getGL2(); int[] buffer = new int[1]; // Generate buffer gl.glGenBuffers(1, IntBuffer.wrap(buffer)); glBuffer = buffer[0]; // Bind the generated buffer gl.glBindBuffer(GL2.GL_ARRAY_BUFFER, glBuffer); // Specify the size of the buffer (no data is pre-loaded in this buffer) gl.glBufferData(GL2.GL_ARRAY_BUFFER, imageWidth * imageHeight * 4, (Buffer)null, GL2.GL_DYNAMIC_DRAW); gl.glBindBuffer(GL2.GL_ARRAY_BUFFER, 0); // The bufferResource is of type CUgraphicsResource and is defined as a class field this.bufferResource = new CUgraphicsResource(); // Register buffer in CUDA cuGraphicsGLRegisterBuffer(bufferResource, glBuffer, CUgraphicsMapResourceFlags.CU_GRAPHICS_MAP_RESOURCE_FLAGS_NONE); 2) Initialize the texture and set texture parameters GL2 gl = drawable.getGL().getGL2(); int[] texture = new int[1]; gl.glGenTextures(1, IntBuffer.wrap(texture)); this.glTexture = texture[0]; gl.glBindTexture(GL2.GL_TEXTURE_2D, glTexture); gl.glTexParameteri(GL2.GL_TEXTURE_2D, GL2.GL_TEXTURE_MIN_FILTER, GL2.GL_LINEAR); gl.glTexParameteri(GL2.GL_TEXTURE_2D, GL2.GL_TEXTURE_MAG_FILTER, GL2.GL_LINEAR); gl.glTexImage2D(GL2.GL_TEXTURE_2D, 0, GL2.GL_RGBA8, imageWidth, imageHeight, 0, GL2.GL_BGRA, GL2.GL_UNSIGNED_BYTE, (Buffer)null); gl.glBindTexture(GL2.GL_TEXTURE_2D, 0); 3) Run the CUDA kernel and display the results in OpenGL's display loop. this.runCUDA(); GL2 gl = drawable.getGL().getGL2(); gl.glBindBuffer(GL2.GL_PIXEL_UNPACK_BUFFER, glBuffer); gl.glBindTexture(GL2.GL_TEXTURE_2D, glTexture); gl.glTexSubImage2D(GL2.GL_TEXTURE_2D, 0, 0, 0, imageWidth, imageHeight, GL2.GL_RGBA, GL2.GL_UNSIGNED_BYTE, 0); //The last argument must be ZERO! NOT NULL! :-) gl.glBindBuffer(GL2.GL_PIXEL_PACK_BUFFER, 0); gl.glBindBuffer(GL2.GL_PIXEL_UNPACK_BUFFER, 0); gl.glBindTexture(GL2.GL_TEXTURE_2D, glTexture); gl.glEnable(GL2.GL_TEXTURE_2D); gl.glDisable(GL2.GL_DEPTH_TEST); gl.glDisable(GL2.GL_LIGHTING); gl.glTexEnvf(GL2.GL_TEXTURE_ENV, GL2.GL_TEXTURE_ENV_MODE, GL2.GL_REPLACE); gl.glMatrixMode(GL2.GL_PROJECTION); gl.glPushMatrix(); gl.glLoadIdentity(); gl.glOrtho(-1.0, 1.0, -1.0, 1.0, -1.0, 1.0); gl.glMatrixMode(GL2.GL_MODELVIEW); gl.glLoadIdentity(); gl.glViewport(0, 0, imageWidth, imageHeight); gl.glBegin(GL2.GL_QUADS); gl.glTexCoord2f(0.0f, 1.0f); gl.glVertex2f(-1.0f, -1.0f); gl.glTexCoord2f(1.0f, 1.0f); gl.glVertex2f(1.0f, -1.0f); gl.glTexCoord2f(1.0f, 0.0f); gl.glVertex2f(1.0f, 1.0f); gl.glTexCoord2f(0.0f, 0.0f); gl.glVertex2f(-1.0f, 1.0f); gl.glEnd(); gl.glMatrixMode(GL2.GL_PROJECTION); gl.glPopMatrix(); gl.glDisable(GL2.GL_TEXTURE_2D); 3.5) The CUDA call: public void runCuda(GLAutoDrawable drawable) { devOutput = new CUdeviceptr(); // Map the OpenGL buffer to a resource and then obtain a CUDA pointer to that resource cuGraphicsMapResources(1, new CUgraphicsResource[]{bufferResource}, null); cuGraphicsResourceGetMappedPointer(devOutput, new long[1], bufferResource); // Setup the kernel parameters making sure that the devOutput pointer is passed to the kernel Pointer kernelParams = . . . . int gridSize = (int) Math.ceil(imageWidth * imageHeight / (double)DESC_BLOCK_SIZE); cuLaunchKernel(function, gridSize, 1, 1, DESC_BLOCK_SIZE, 1, 1, 0, null, kernelParams, null); cuCtxSynchronize(); // Unmap the buffer so that it can be used in OpenGL cuGraphicsUnmapResources(1, new CUgraphicsResource[]{bufferResource}, null); } PS: I thank Robert for providing the link to the sample. I also thank the people who downvoted my question without any useful feedback!"
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "CUdeviceptr deviceInputA = new CUdeviceptr(); cuMemAlloc(deviceInputA, size * Sizeof.DOUBLE); cuMemcpyHtoD(deviceInputA, Pointer.to(inputfingerprint), size * Sizeof.DOUBLE); cuMemFree(deviceInputA); Here cuMemFree is not releasing the GPU memory .",
        "answers": [
            [
                "Finally , this problem is resolved . I had a 2D output array and was not deallocating the inner pointers of the array on the GPU . Hence GPU memory was not being released . After deallocation , it is releasing the GPU memory . cuMemFree is working absolutely fine in JCuda ."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "How does one index in, lets say a 1D \"Result\" array, using the function pointer.to(int[ ]) in jCuda. I want to write a chunk of data into the first n locations of \"Result\" and the next chunk of data into Result[0 + chunk] onwards, and so on. Unlike C, i cannot say Result+chunk and get on with life. how do i index to an intermediate location then?",
        "answers": [
            [
                "Assuming that you are refering to JCuda from jcuda.org: When you created a Pointer to an int array using Pointer#to(int[]), you can create a Pointer with a certain byte offset using the Pointer#withByteOffset(long) method. So in this example: Pointer p = Pointer.to(intArray); Pointer next = p.withByteOffset(chunkSize * Sizeof.INT); This method creates only a \"view\" on the particular position in the array. It does not copy any data or so. The resulting pointer will just point to the 'chunkSize'th element of the array. Thus, it is the \"Java version\" of the C construct int *p = ... int *next = p + chunkSize; that you mentioned. Important: Make sure to really multiply the intended offset by the size of the elements in the array! It has to be a BYTE offset, so the 'chunkSize' has to be multiplied with Sizeof.INT to really point to the right position in the int[] array. (In C, this multiplication is done implicitly, based on the pointer type. But since the Pointer in Java has no associated type, you always have to specify the BYTE offset) Hint: When you frequently need such offset-pointers of a particular type, the readability can be increased with a helper method like this private static Pointer at(Pointer p, int offsetInInts) { int offsetInBytes = offsetInInts * Sizeof.INT; return p.withByteOffset(offsetInBytes); } that you can use inside a method call: // C version: cudaMemcpy(p + chunkSize * i, ...); // Java version: cudaMemcpy(at(p, chunkSize * i), ...);"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Using the driver api precludes the usage of the runtime api in the same application ([1]) . Unfortunately cublas, cufft, etc are all based on the runtime api. If one wants dynamic kernel definition as in cuModuleLoad and cublas at the same time, what are the options? I have these in mind, but maybe there are more: A. Wait for compute capability 3.5 that's rumored to support peaceful coexistence of driver and runtime apis in the same application. B. Compile the kernels to an .so file and dlopen it. Do they get unloaded on dlcose? C. Attempt to use cuModuleLoad from the driver api, but everything else from the runtime api. No idea if there is any hope for this. I'm not holding my breath, because jcuda or pycuda are in pretty much the same bind and they probably would have figured it out already. [1] CUDA Driver API vs. CUDA runtime",
        "answers": [
            [
                "To summarize, you are tilting at windmills here. By relying on extremely out of date information, you seem to have concluded that runtime and driver API interoperability isn't supported in CUDA, when, in fact, it has been since the CUDA 3.0 beta was released in 2009. Quoting from the release notes of that version: The CUDA Toolkit 3.0 Beta is now available. Highlights for this release include: CUDA Driver / Runtime Buffer Interoperability, which allows applications using the CUDA Driver API to also use libraries implemented using the CUDA C Runtime. There is documentation here which succinctly describes how the driver and runtime API interact. To concretely answer your main question: If one wants dynamic kernel definition as in cuModuleLoad and cublas at the same time, what are the options? The basic approach goes something like this: Use the driver API to establish a context on the device as you would normally do. Call the runtime API routine cudaSetDevice(). The runtime API will automagically bind to the existing driver API context. Note that device enumeration is identical and common between both APIs, so if you establish context on a given device number in the driver API, the same number will select the same GPU in the driver API You are now free to use any CUDA runtime API call or any library built on the CUDA runtime API. Behaviour is the same as if you relied on runtime API \"lazy\" context establishment"
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "Well I decided I prefer to use GPU over CPU especially since I'm working on a game and FPS will increase I expect. The thing is i'm not sure where to start. I can easily implement JOCL or JCUDA but after that I wouldnt know where to replace it from using CPU to GPU. Help is appreciated :)",
        "answers": [
            [
                "What kind of computations are you after? If those are compute intensive such as N-body gravity experiments, then you can simply copy variables to gpu then compute then copy results back to main memory. If your objects have big data but small computation such as fluid dynamics or collision detection, then you should add interoperability between your graphics api and compute api. Then you can do only computations withouth any copying of data.(speed-up is like your GPU ram bandwidth divided by your pci-e bandwidth. For a HD7870, it is like 25x if compute power is not saturated already) I used jocl and lwjgl using gl/cl interoperability in java and they were working very well. Some neural network is trained with CPU(Encog) but used by GPU(jocl) to generate a map and drawn by LWJGL :(neuron weigths are changed a little to have some more randomizing effect) Very important part is: Start a GL context. Use the GL context's handle variables to start an inter-operable CL context Create GL buffers Create CL buffers with the interoperable cl context. Dont forget calling clFinish() when opencl is done and gl is ready to start Dont forget calling glFinish() when opengl is done and cl is ready to start Using an opencl kernel builder/table class and a buffer scheduler class would help when you have tens of different kernels many different buffers between gl and cl and you need them run in an order. Example: // clh is a fictional class that binds oepncl to opengl through interoperability // registering needed kernels to this object clh.addKernel( kernelFactory.fluidDiffuse(1024,1024), // enumaration is fluid1 kernelFactory.fluidAdvect(1024,1024), // enumeration is fluid2 kernelFactory.rigidBodySphereSphereInteracitons(2048,32,32), kernelFactory.fluidRigidBodyInteractions(false), // fluidRigid kernelFactory.rayTracingShadowForFluid(true), kernelFactory.rayTracingBulletTargetting(true), kernelFactory.gravity(G), kernelFactory.gravitySphereSphere(), // enumeration is fall kernelFactory.NNBotTargetting(3,10,10,2,numBots) // Encog ); clh.addBuffers( // enumeration is buf1 and is used as fluid1, fluid2 kernels' arguments bufferFactory.fluidSurfaceVerticesPosition(1024,1024, fluid1, fluid2), // enumeration is buf2, used by fluid1 and fluid2 bufferFactory.fluidSurfaceColors(1024,1024,fluid1, fluid2), // enumeration is buf3, used by network bufferFactory.NNBotTargetting(numBots*25, Encog) ) Running kernels: // shortcut of a sequence of kernels int [] fluidCalculations = new int[]{fluid1,fluid2,fluidRigid, fluid1} clh.run(fluidCalculations); // runs the registered kernels // diffuses, advects, sphere-fluid interaction, diffuse again //When any update of GPU-buffer from main-memory is needed: clh.sendData(cpuBuffer, buf1); // updates fluid surface position from main-memory. Changing a cpu code to a opencl code can be done automatically by APARAPI but Im not sure if it has interoperability. If you need to do it yourself, then it is as easy as: From Java: for(int i=0;i&lt;numParticles;i++) { for(int j=0;j&lt;numParticles;j++) { particle.get(i).calculateAndAddForce(particle.get(j)); } } To a Jocl kernel string(actually very similar to calculateAndAddForce's inside): \"__kernel void nBodyGravity(__global float * positions,__global float *forces)\" + \"{\" + \" int indis=get_global_id(0);\" + \" int totalN=\" + n + \"; \"+ \" float x0=positions[0+3*(indis)];\"+ \" float y0=positions[1+3*(indis)];\"+ \" float z0=positions[2+3*(indis)];\"+ \" float fx=0.0f;\" + \" float fy=0.0f;\" + \" float fz=0.0f;\" + \" for(int i=0;i&lt;totalN;i++)\" + \" { \"+ \" float x1=positions[0+3*(i)];\" + \" float y1=positions[1+3*(i)];\" + \" float z1=positions[2+3*(i)];\" + \" float dx = x0-x1;\" + \" float dy = y0-y1;\" + \" float dz = z0-z1;\" + \" float r=sqrt(dx*dx+dy*dy+dz*dz+0.01f);\" + \" float tr=0.1f/r;\" + \" float tr2=tr*tr*tr;\" + \" fx+=tr2*dx*0.0001f;\" + \" fy+=tr2*dy*0.0001f;\" + \" fz+=tr2*dz*0.0001f;\" + \" } \"+ \" forces[0+3*(indis)]+=fx; \" + \" forces[1+3*(indis)]+=fy; \" + \" forces[2+3*(indis)]+=fz; \" + \"}\""
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I've been working with JCuda for some months now and I can't copy a multidimensional array from device memory to host memory. The funny thing is that I have no problems in doing so in the opposite direction (I can invoke my kernel with multidimensional arrays and everything works with the correct values). In a few words, I put the results of my kernel in a bi-dimensional array of shorts, where the first dimension of such array is the number of threads, so that each one can write in different locations. Here an example: CUdeviceptr pointer_dev = new CUdeviceptr(); cuMemAlloc(pointer_dev, Sizeof.POINTER); // in this case, as an example, it's an array with one element (one thread), but it doesn't matter // Invoke kernel with pointer_dev as parameter. Now it should contain some results CUdeviceptr[] arrayPtr = new CUdeviceptr[1]; // It will point to the result arrayPtr[0] = new CUdeviceptr(); short[] resultArray = new short[3]; // an array of 3 shorts was allocated in the kernel cuMemAlloc(arrayPtr[0], 3 * Sizeof.SHORT); cuMemcpyDtoH(Pointer.to(arrayPtr), pointer_dev, Sizeof.POINTER); // Its seems, using the debugger, that the value of arrayPtr[0] isn't changed here! cuMemcpyDtoH(Pointer.to(resultArray), arrayPtr[0], 3 * Sizeof.SHORT); // Not the expected values in resultArray, probably because of the previous instruction What am I doing wrong? EDIT: Apparently, there are some limitations that doesn't allow device allocated memory to be copied back to host, as stated in this (and many more) threads: link Any workaround? I'm using CUDA Toolkit v5.0",
        "answers": [
            [
                "Here we are copying a two dimensional array of integers from the device to host. First, create a single dimensional array with size equal to size of another single dimension array (here blockSizeX). CUdeviceptr[] hostDevicePointers = new CUdeviceptr[blockSizeX]; for (int i = 0; i &lt; blockSizeX; i++) { hostDevicePointers[i] = new CUdeviceptr(); cuMemAlloc(hostDevicePointers[i], size * Sizeof.INT); } Allocate device memory for the array of pointers that point to the other array, and copy array pointers from the host to the device. CUdeviceptr hostDevicePointersArray = new CUdeviceptr(); cuMemAlloc(hostDevicePointersArray, blockSizeX * Sizeof.POINTER); cuMemcpyHtoD(hostDevicePointersArray, Pointer.to(hostDevicePointers), blockSizeX * Sizeof.POINTER); Launch the kernel. kernelLauncher.call(........, hostDevicePointersArray); Transfer the output from the device to host. int hostOutputData[] = new int[numberofelementsInArray * blockSizeX]; cuMemcpyDtoH(Pointer.to(hostOutputData), hostDevicePointers[i], numberofelementsInArray * blockSizeX * Sizeof.INT); for (int j = 0; j &lt; size; j++) { sum = sum + hostOutputData[j]; }"
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I'd like to know what is the best debugger for a JCuda project (if exists). I know that for normal CUDA applications there are tools like cuda-gdb and cuda-memcheck and I wonder if I can use any of them or similar ones for a JCuda project. Thank you!",
        "answers": [
            [
                "You should be able to use regular CUDA debugger (e.g. Nsight or cuda-gdb) to debug the GPU code in your JCuda application. You can use Java debuggers (e.g. Eclipse) to debug Java code. There is no debugger that can seamlessly debug Java and CUDA code though."
            ],
            [
                "On Linux, you can use Nvidia Nsight (cuda-gdb). To set up a debugging session that launches your executable: launch Nsight as root (surprising, but necessary for me) go to Run menu &gt; Debug Configurations... Right-click \"C/C++ Application\", choose New Under Main tab Set the executable (field C/C++ Application) to /usr/bin/java or whatever the correct path Under Arguments tab set the startup arguments, eg -jar target/MyApp.jar set the working directory Under Debugger tab set the \"CUDA GDB init file\" path (see below) set additional options, like enabling CUDA memcheck Because Java uses segmentation faults internally for some too-clever purposes, you will need to create a file with the following GDB option and point to it as the \"CUDA GDB init file\". Note that by setting this option you won't be able to catch segmentation fault bugs inside your own JNI code. This shouldn't be a problem. handle SIGSEGV nostop noprint pass If you compile your kernels with debug symbols, you'll be able to debug them."
            ]
        ],
        "votes": [
            4.0000001,
            1.0000001
        ]
    },
    {
        "question": "JCuda + GEForce Gt640 Question: I'm trying to reduce the latency associated with copying memory from Device to Host after the result has been computed by the GPU. Doing the simple Vector Add program I found that the bulk of the latency is indeed copying the result buffer back to the Host side. The transfer latency of the source buffers to the Device side is negligible ~.30ms while copying the result back is on the order of 20ms. I did the research an found that a better alternative to copying out the results is to use pinned memory. From what I learned, this memory is allocated on the host side but the kernel would have direct access to it over the pci-e and in turn yielding a higher speed than copying the result after the computation in bulk. I'm using the following example but the results aren't yielding what I expect. Kernel: {Simple Example to illustrate point, Launching 1 block 1 thread only} extern \"C\" __global__ void add(int* test) { test[0]=1; test[1]=2; test[2]=3; test[3]=4; test[4]=5; } Java: import java.io.*; import jcuda.*; import jcuda.runtime.*; import jcuda.driver.*; import static jcuda.runtime.cudaMemcpyKind.*; import static jcuda.driver.JCudaDriver.*; public class JCudaTest { public static void main(String args[]) { // Initialize the driver and create a context for the first device. cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); // Load the ptx file. CUmodule module = new CUmodule(); JCudaDriver.cuModuleLoad(module, \"JCudaKernel.ptx\"); // Obtain a function pointer to the kernel function. CUfunction function = new CUfunction(); JCudaDriver.cuModuleGetFunction(function, module, \"add\"); Pointer P = new Pointer(); JCudaDriver.cuMemAllocHost(P, 5*Sizeof.INT); Pointer kernelParameters = Pointer.to(P); // Call the kernel function with 1 block, 1 thread: JCudaDriver.cuLaunchKernel(function, 1, 1, 1, 1, 1, 1, 0, null, kernelParameters, null); int [] T = new int[5]; JCuda.cudaMemcpy(Pointer.to(T), P, 5*Sizeof.INT, cudaMemcpyHostToHost); // Print the results: for(int i=0; i&lt;5; i++) System.out.println(T[i]); } } 1.) Build the Kernel: root@NVS295-CUDA:~/JCUDA/MySamples# nvcc -ptx JCudaKernel.cu root@NVS295-CUDA:~/JCUDA/MySamples# ls -lrt | grep ptx -rw-r--r-- 1 root root 3295 Mar 27 17:46 JCudaKernel.ptx 2.) Build the Java: root@NVS295-CUDA:~/JCUDA/MySamples# javac -cp \"../JCuda-All-0.5.0-bin-linux-x86/*:.\" JCudaTest.java 3.) Run the code: root@NVS295-CUDA:~/JCUDA/MySamples# java -cp \"../JCuda-All-0.5.0-bin-linux-x86/*:.\" JCudaTest 0 0 0 0 0 Expecting: 1 2 3 4 5 Note: I'm using JCuda0.5.0 for x86 if that matters. Please let me know what I'm doing wrong and thanks in advance: Ilir",
        "answers": [
            [
                "The problem here is that the device may not access host memory directly. Admittedly, the documentation sounds misleading here: cuMemAllocHost Allocates bytesize bytes of host memory that is page-locked and accessible to the device... This sounds like a clear statement. However, \"accessible\" here does not mean that the memory may be used directly as a kernel parameter in all cases. This is only possible on devices that support Unified Addressing. For all other devices, it is necessary to obtain a device pointer that corresponds to the allocated host pointer, with cuMemHostGetDevicePointer. The key point of page-locked host memory is that the data transfer between the host and device is faster. An example of how this memory may be used in JCuda can be seen in the JCudaBandwidthTest sample (this is for the runtime API, but for the driver API, it works analogously). EDIT: Note that the new Unified Memory feature of CUDA 6 actually supports what you originally intended to do: With cudaMallocManaged you can allocate memory that is directly accessible to the host and the device (in the sense that it can, for example, be passed to a kernel, written by the device, and afterwards read by the host without additional effort). Unfortunately, this concept does not map very well to Java, because the memory is still managed by CUDA - and this memory can not replace the memory that is, for example, used by the Java VM for a float[] array or so. But at least it should be possible to create a ByteBuffer from the memory that was allocated with cudaMallocManaged, so that you may access this memory, for example, as a FloatBuffer."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using Java with Jcuda library to access CUDA.. I was able to run a simple application ( like vectorAdd).. However, the code of my Kernel stated getting very large.. is there a way to use device function in the kerenel.cu file? I knew that it is possible if I use CUDA C , but with Java I got an error? can any one help thanks in advance",
        "answers": [
            [
                "I think the error you get is most likely CUDA_ERROR_NOT_FOUND. Please check if you put \"extern C\" before your __device__ function. Otherwise, it won't be found in the symbol table."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, visit the help center for guidance. Closed 10 years ago. Me mother tongue is Java (I know I know, but you can't change who you are). So I wanted to get a pros / cons of doing CUDA vs. JCUDA. I just found out there is a jcuda.org Thanks in advance.",
        "answers": [
            [
                "cuda represents language extensions to C or C++ (or Fortran). JCuda presents a binding API so that library calls (e.g. cublas for linear algebra, or cufft for fft's, etc.) and arbitrary cuda functions written in C or C++ can be called from Java. If you want to write java kernel code that runs on the GPU, you may also be interested in root beer. Root beer allows you to write kernel code that runs on the thread processors in a GPU directly, without any requirement for CUDA C. However, you'll still need to install the CUDA toolkit to enable the Root Beer \"Java Compiler\"."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have some CUDA code I made in C and it seems to be working fine (it's plain old C and not C++). I\u2019m running a Hadoop cluster and wanted to consolidate my code so ideally I\u2019m looking to run it within Java (long story short: system is too complex). Currently the C program parses a log file, takes a few thousand lines, processes each line in parallel on the GPU, saves specific errors/transactions into a linked list, and writes them to the drive. What is the best approach to do this? Is JCUDA a perfect mapping to C CUDA or is it totally different? Or does it make sense to call C code from Java and share results (would the linked list be accessible)?",
        "answers": [
            [
                "IMO? JavaCPP. For example, here is a port to Java of the example displayed on the main page of Thrust's Web site: import com.googlecode.javacpp.*; import com.googlecode.javacpp.annotation.*; @Platform(include={\"&lt;thrust/host_vector.h&gt;\", \"&lt;thrust/device_vector.h&gt;\", \"&lt;thrust/generate.h&gt;\", \"&lt;thrust/sort.h&gt;\", \"&lt;thrust/copy.h&gt;\", \"&lt;thrust/reduce.h&gt;\", \"&lt;thrust/functional.h&gt;\", \"&lt;algorithm&gt;\", \"&lt;cstdlib&gt;\"}) @Namespace(\"thrust\") public class ThrustTest { static { Loader.load(); } public static class IntGenerator extends FunctionPointer { static { Loader.load(); } protected IntGenerator() { allocate(); } private native void allocate(); public native int call(); } @Name(\"plus&lt;int&gt;\") public static class IntPlus extends Pointer { static { Loader.load(); } public IntPlus() { allocate(); } private native void allocate(); public native @Name(\"operator()\") int call(int x, int y); } @Name(\"host_vector&lt;int&gt;\") public static class IntHostVector extends Pointer { static { Loader.load(); } public IntHostVector() { allocate(0); } public IntHostVector(long n) { allocate(n); } public IntHostVector(IntDeviceVector v) { allocate(v); } private native void allocate(long n); private native void allocate(@ByRef IntDeviceVector v); public IntPointer begin() { return data(); } public IntPointer end() { return data().position((int)size()); } public native IntPointer data(); public native long size(); public native void resize(long n); } @Name(\"device_ptr&lt;int&gt;\") public static class IntDevicePointer extends Pointer { static { Loader.load(); } public IntDevicePointer() { allocate(null); } public IntDevicePointer(IntPointer ptr) { allocate(ptr); } private native void allocate(IntPointer ptr); public native IntPointer get(); } @Name(\"device_vector&lt;int&gt;\") public static class IntDeviceVector extends Pointer { static { Loader.load(); } public IntDeviceVector() { allocate(0); } public IntDeviceVector(long n) { allocate(n); } public IntDeviceVector(IntHostVector v) { allocate(v); } private native void allocate(long n); private native void allocate(@ByRef IntHostVector v); public IntDevicePointer begin() { return data(); } public IntDevicePointer end() { return new IntDevicePointer(data().get().position((int)size())); } public native @ByVal IntDevicePointer data(); public native long size(); public native void resize(long n); } public static native @MemberGetter @Namespace IntGenerator rand(); public static native void copy(@ByVal IntDevicePointer first, @ByVal IntDevicePointer last, IntPointer result); public static native void generate(IntPointer first, IntPointer last, IntGenerator gen); public static native void sort(@ByVal IntDevicePointer first, @ByVal IntDevicePointer last); public static native int reduce(@ByVal IntDevicePointer first, @ByVal IntDevicePointer last, int init, @ByVal IntPlus binary_op); public static void main(String[] args) { // generate 32M random numbers serially IntHostVector h_vec = new IntHostVector(32 &lt;&lt; 20); generate(h_vec.begin(), h_vec.end(), rand()); // transfer data to the device IntDeviceVector d_vec = new IntDeviceVector(h_vec); // sort data on the device (846M keys per second on GeForce GTX 480) sort(d_vec.begin(), d_vec.end()); // transfer data back to host copy(d_vec.begin(), d_vec.end(), h_vec.begin()); // compute sum on device int x = reduce(d_vec.begin(), d_vec.end(), 0, new IntPlus()); } } Your code in C should be easier to map though. We can get this compiled and running on Linux x86_64 with these commands, or on other supported platforms by modifying the -properties option appropriately: $ javac -cp javacpp.jar ThrustTest.java $ java -jar javacpp.jar ThrustTest -properties linux-x86_64-cuda $ java -cp javacpp.jar ThrustTest"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have the following code in Java: float in[][] = new float[2][2]; float B[] = new float[2]; float A[] = new float[2]; float res[] = new float[A[0].length]; for(float r : res){ r = 0; } for (int i = 0; i &lt; A[0].length; i++) { for (int j = 0; j &lt; B[0].length; j++) { res[i] += A[j] * in[j][i]; } I simplified it at most, so you should not search for a real logic in there :). I struggle for some hours converting this in CUDA because of the += statement in the loop. I started with something like this : extern \"C\" __global__ void filter(float* in, float* A, float* B, float* res, int in_size){ unsigned int x = blockIdx.x*blockDim.x + threadIdx.x; unsigned int y = blockIdx.y*blockDim.y + threadIdx.y; res[x] = A[y] * in[x + y * in_width]; } but quickly realized it couldn't work because of all the threads trying to set the same variable. I read the example of the dot product in this presentation, but I don't really see how to adapt that with my need of two dimensions for in. I don't ask for a complete solution, but any direction would definitely be appreciated. Thx,",
        "answers": [
            [
                "Too much CUDA killed my head. I found a partial solution by unrolling one of the loops inside my kernel. Here it what it looks like right now : extern \"C\" __global__ void filter(float* in, float* A, float* res, const int in_width, const int sizeB){ unsigned int x = blockIdx.x*blockDim.x + threadIdx.x; int i = 0; for(i = 0; i &lt; sizeB; i++){ res[x] += A[i] * in[i + x * in_width]; } } I am sure I can find better, but I think I'll stick with this for today :)"
            ],
            [
                "You can split up the multiplication job A[j] * in[j][i] in A[0].length*B[0].length threads and can sum up the results of multiplication as like reduction sample in NVIDIA sdk using shared memory"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm implementing GPU calculation in a program already written in Java. For that purpose I'm using jcuda bindings. I need a fast host to device memory transfer of, sometimes, relatively large arrays. If I want to use streams, I have to use pinned memory. The problem is if I want to allocate host pinned memory larger than cca 600 Mbs of RAM, I get \"CUDA_ERROR_OUT_OF_MEMORY\" exception. This is the code I used to test size of the available pinned memory: public static void main(String[] args) { //Init GPU JCudaDriver.setExceptionsEnabled(true); // Initialize the device and create device context cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); Pointer p = new Pointer(); int Kb = 1024; int Mb = 1024 * Kb; int Gb = 1024 * Mb; int sequenceSize = 172*Mb; // times 4 for float float[] expecteds = new float[sequenceSize]; float[] actuals = new float[sequenceSize]; Arrays.fill(expecteds, 3.33f); int i = 0; try { JCudaDriver.cuMemAllocHost(p, sequenceSize* Sizeof.FLOAT); FloatBuffer fb = p.getByteBuffer(0, sequenceSize* Sizeof.FLOAT). order(ByteOrder.nativeOrder()). asFloatBuffer(); fb.position(0); fb.put(expecteds); fb.position(0); fb.get(actuals); JCudaDriver.cuMemFreeHost(p); } catch (Exception e) { e.printStackTrace(); JCudaDriver.cuMemFreeHost(p); } } Now, I'm aware that OS can prevent me to use too much pinned memory since it's non-pageable. The thing is that I have 48Gb (45Gb free) of physical memory and I need a way of forcing OS to give me more of it. Is there a way to do this (elegantly if possible)? EDIT: OS is 64-bit Windows 7 Professional SP1",
        "answers": [
            [
                "Check that you are running Java in 64-bit mode. The FAQ suggests the default is 32-bit, even with the 64-bit downloads. The linked FAQ also tells you how to run in 64-bit mode, you'll need to use the 64-bit DLLs etc. too. @ArchaeSoftware's suggestion of using cuMemHostRegister()/cuMemHostUnregister() to pin smaller sections of the memory is a sensible alternative."
            ],
            [
                "It seems an old page but without Answer.. I guess you are not utilizing your RAM properly as by default Java does not allocate much memory for heap by itself. You can force JVM to use minimum and maximum memory by -Xms and -Xmx respectively and as you are working on 64-bit architechture use \"-d64\" to after \"-Xms\" or \"-Xmx\""
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm using a dynamic language (Clojure) to create CUDA contexts in a interactive development way using JCuda. Often I will call an initializer that includes the call to jcuda.driver.JCudaDriver/cuInit. Is it safe to call cuInit multiple times? In addition, is there something like a destroy method for cuInit? I ask since its possible for an error code CUDA_ERROR_DEINITIALIZED to be returned.",
        "answers": [
            [
                "To answer the question, yes it is probably safe to call cuInit multiple times. I haven't noticed any side effects from doing so. Note, however, that cuInit only triggers one-time initialisation processes inside the API. It doesn't do anything with devices, or contexts and it definitely can't return CUDA_ERROR_DEINITIALIZED. Doing the steps you would do after calling cuInit in an application (ie. creating a context) would have real implications - doing so creates a new context each time you call it and resource exhaustion will occur if contexts are not actively destroyed. There is no equivalent deinitialisation call for the API. I guess the intention is that once intialised, the runtime API is expected to stay in that state until an application terminates."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have started using JCuda.When I compile the program I get \"jcuda.cuda exception cuda_error_launch_failed\" error.I have followed the steps given in the installation guide but doesn't work.Very few resources are available online.So,Please help me.This is my code: extern \"C\" __global__ void reduce(float *g_idata, float *g_odata, unsigned int n) { extern __shared__ float sdata[]; unsigned int tid = threadIdx.x; unsigned int i = blockIdx.x*blockDim.x*2 + threadIdx.x; unsigned int gridSize = blockDim.x*2*gridDim.x; float mySum = 0; while (i &lt; n) { mySum += g_idata[i]; if (i + blockDim.x &lt; n) mySum += g_idata[i+blockDim.x]; i += gridSize; } sdata[tid] = mySum; __syncthreads(); if (blockDim.x &gt;= 512) { if (tid &lt; 256) { sdata[tid] = mySum = mySum + sdata[tid + 256]; } __syncthreads(); } if (blockDim.x &gt;= 256) { if (tid &lt; 128) { sdata[tid] = mySum = mySum + sdata[tid + 128]; } __syncthreads(); } if (blockDim.x &gt;= 128) { if (tid &lt; 64) { sdata[tid] = mySum = mySum + sdata[tid + 64]; } __syncthreads(); } if (tid &lt; 32) { volatile float* smem = sdata; if (blockDim.x &gt;= 64) { smem[tid] = mySum = mySum + smem[tid + 32]; } if (blockDim.x &gt;= 32) { smem[tid] = mySum = mySum + smem[tid + 16]; } if (blockDim.x &gt;= 16) { smem[tid] = mySum = mySum + smem[tid + 8]; } if (blockDim.x &gt;= 8) { smem[tid] = mySum = mySum + smem[tid + 4]; } if (blockDim.x &gt;= 4) { smem[tid] = mySum = mySum + smem[tid + 2]; } if (blockDim.x &gt;= 2) { smem[tid] = mySum = mySum + smem[tid + 1]; } } if (tid == 0) g_odata[blockIdx.x] = sdata[0]; } I have nvidia 460 gtx gpu and cuda drivers and toolkit installed.",
        "answers": [
            [
                "Use cudaFree(deviceId); or cudaFree(deviceId); to free memory and try again.Hope this helps you."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I've been trying to use cublasSgemmBatched() function in jcuda for matrix multiplication and I'm not sure how to properly handle pointer passing and vectors of batched matrices. I will be really thankful if someone knows how to modify my code to properly handle this problem. In this example, C array stays unchanged after cublasGetVector. public static void SsmmBatchJCublas(int m, int n, int k, float A[], float B[]){ // Create a CUBLAS handle cublasHandle handle = new cublasHandle(); cublasCreate(handle); // Allocate memory on the device Pointer d_A = new Pointer(); Pointer d_B = new Pointer(); Pointer d_C = new Pointer(); cudaMalloc(d_A, m*k * Sizeof.FLOAT); cudaMalloc(d_B, n*k * Sizeof.FLOAT); cudaMalloc(d_C, m*n * Sizeof.FLOAT); float[] C = new float[m*n]; // Copy the memory from the host to the device cublasSetVector(m*k, Sizeof.FLOAT, Pointer.to(A), 1, d_A, 1); cublasSetVector(n*k, Sizeof.FLOAT, Pointer.to(B), 1, d_B, 1); cublasSetVector(m*n, Sizeof.FLOAT, Pointer.to(C), 1, d_C, 1); Pointer[] Aarray = new Pointer[]{d_A}; Pointer AarrayPtr = Pointer.to(Aarray); Pointer[] Barray = new Pointer[]{d_B}; Pointer BarrayPtr = Pointer.to(Barray); Pointer[] Carray = new Pointer[]{d_C}; Pointer CarrayPtr = Pointer.to(Carray); // Execute sgemm Pointer pAlpha = Pointer.to(new float[]{1}); Pointer pBeta = Pointer.to(new float[]{0}); cublasSgemmBatched(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, pAlpha, AarrayPtr, Aarray.length, BarrayPtr, Barray.length, pBeta, CarrayPtr, Carray.length, Aarray.length); // Copy the result from the device to the host cublasGetVector(m*n, Sizeof.FLOAT, d_C, 1, Pointer.to(C), 1); // Clean up cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); cublasDestroy(handle); }",
        "answers": [
            [
                "I asked on official jcuda forum and quickly received the answer here."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a matrix of 1 million columns x 1 million rows. My algoritm needs to do: Matrix m = Matrix(rows,cols) for (colB: cols){ vector currColA = m.getcolumn(colA) for (colB: cols){ vector currColB = m.getcolumn(colB) result = currColA.dotProduct(colB) return result; }} or you could also say: Vectors [] v = Vectors[] for (i: v.length){ vector v1 = v[i] for (i: v.length){ vector v2 = v[i] result = v1.dotProduct(v2) return result; }} My question: what is the proper way to allocate memory and initialize the memory for this problem: - Should I allocate the memory for the full matrix, initialize it with the full matrix, and then run the algo? - or should I allocate the memory for a list of vectors, and then loop through this list? - or else?? My concern is that I would like to minimize transfer times to the gpu. I have tried this sort of computations by modifying the JCublas hello world example for a sgemm operation on 2 vectors but when doing it on my large number of vectors, ended up having transfer times deleting the benefits of the gpu acceleration. Thx! PS: implementation could be in any Java library",
        "answers": [
            [
                "It sounds like you are enforcing a 1-at-a-time restriction. CPU-&gt;GPU copy, wait, compute, GPU-&gt;CPU copy, wait. Most people don't realize the implicit waits that memory copies can cause. Can you pipeline your operations? In other words, does your loop consist of the following? CPU-&gt;GPU copy GPU compute GPU-&gt;CPU copy To pipeline this you would use (for example) 4 separate (in order) command queues, issue a non-blocking transfer to the GPU on each queue, issue the kernel execution on each queue, and issue the GPU-&gt;CPU copy on each queue, in that order. You have to guarantee both buffers remain valid until the wait (described later). This will allow the GPU to begin computing while the subsequent memory transfers are taking place. Also, never use blocking memory transfers, always use non blocking. Every so many (8?) transfers, get an event object for the GPU-&gt;CPU copy, but wait for the last event object first if this isn't the first iteration. This will throttle your queues and allow you to reuse buffers, but overlapping the operations keeps the transfers and compute overlapped. We're waiting for the transfer 8 iterations ago, so we're not draining the queue. It is important to manage queue depth, excessive workitems cause laggy GUI and workitem starvation."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a trouble working with JCUDA. I have a task to make 1D FFT using CUFFT library, but the result should be multiply on 2. So I decided to make 1D FFT with type CUFFT_R2C. Class responsible for this going next: public class FFTTransformer { private Pointer inputDataPointer; private Pointer outputDataPointer; private int fftType; private float[] inputData; private float[] outputData; private int batchSize = 1; public FFTTransformer (int type, float[] inputData) { this.fftType = type; this.inputData = inputData; inputDataPointer = new CUdeviceptr(); JCuda.cudaMalloc(inputDataPointer, inputData.length * Sizeof.FLOAT); JCuda.cudaMemcpy(inputDataPointer, Pointer.to(inputData), inputData.length * Sizeof.FLOAT, cudaMemcpyKind.cudaMemcpyHostToDevice); outputDataPointer = new CUdeviceptr(); JCuda.cudaMalloc(outputDataPointer, (inputData.length + 2) * Sizeof.FLOAT); } public Pointer getInputDataPointer() { return inputDataPointer; } public Pointer getOutputDataPointer() { return outputDataPointer; } public int getFftType() { return fftType; } public void setFftType(int fftType) { this.fftType = fftType; } public float[] getInputData() { return inputData; } public int getBatchSize() { return batchSize; } public void setBatchSize(int batchSize) { this.batchSize = batchSize; } public float[] getOutputData() { return outputData; } private void R2CTransform() { cufftHandle plan = new cufftHandle(); JCufft.cufftPlan1d(plan, inputData.length, cufftType.CUFFT_R2C, batchSize); JCufft.cufftExecR2C(plan, inputDataPointer, outputDataPointer); JCufft.cufftDestroy(plan); } private void C2CTransform(){ cufftHandle plan = new cufftHandle(); JCufft.cufftPlan1d(plan, inputData.length, cufftType.CUFFT_C2C, batchSize); JCufft.cufftExecC2C(plan, inputDataPointer, outputDataPointer, fftType); JCufft.cufftDestroy(plan); } public void transform(){ if (fftType == JCufft.CUFFT_FORWARD) { R2CTransform(); } else { C2CTransform(); } } public float[] getFFTResult() { outputData = new float[inputData.length + 2]; JCuda.cudaMemcpy(Pointer.to(outputData), outputDataPointer, outputData.length * Sizeof.FLOAT, cudaMemcpyKind.cudaMemcpyDeviceToHost); return outputData; } public void releaseGPUResources(){ JCuda.cudaFree(inputDataPointer); JCuda.cudaFree(outputDataPointer); } public static void main(String... args) { float[] inputData = new float[65536]; for(int i = 0; i &lt; inputData.length; i++) { inputData[i] = (float) Math.sin(i); } FFTTransformer transformer = new FFTTransformer(JCufft.CUFFT_FORWARD, inputData); transformer.transform(); float[] result = transformer.getFFTResult(); HilbertSpectrumTicksKernelInvoker.multiplyOn2(transformer.getOutputDataPointer(), inputData.length+2); transformer.releaseGPUResources(); } } Method which responsible for multiplying uses cuda kernel function. Java method code: public static void multiplyOn2(Pointer inputDataPointer, int dataSize){ // Enable exceptions and omit all subsequent error checks JCudaDriver.setExceptionsEnabled(true); // Create the PTX file by calling the NVCC String ptxFileName = null; try { ptxFileName = FileService.preparePtxFile(\"resources\\\\HilbertSpectrumTicksKernel.cu\"); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } // Initialize the driver and create a context for the first device. cuInit(0); CUdevice device = new CUdevice(); cuDeviceGet(device, 0); CUcontext context = new CUcontext(); cuCtxCreate(context, 0, device); // Load the ptx file. CUmodule module = new CUmodule(); cuModuleLoad(module, ptxFileName); // Obtain a function pointer to the \"add\" function. CUfunction function = new CUfunction(); cuModuleGetFunction(function, module, \"calcSpectrumSamples\"); // Set up the kernel parameters: A pointer to an array // of pointers which point to the actual values. int N = (dataSize + 1) / 2 + 1; int pair = (dataSize + 1) % 2 &gt; 0 ? 1 : -1; Pointer kernelParameters = Pointer.to(Pointer.to(inputDataPointer), Pointer.to(new int[] { dataSize }), Pointer.to(new int[] { N }), Pointer.to(new int[] { pair })); // Call the kernel function. int blockSizeX = 128; int gridSizeX = (int) Math.ceil((double) dataSize / blockSizeX); cuLaunchKernel(function, gridSizeX, 1, 1, // Grid dimension blockSizeX, 1, 1, // Block dimension 0, null, // Shared memory size and stream kernelParameters, null // Kernel- and extra parameters ); cuCtxSynchronize(); // Allocate host output memory and copy the device output // to the host. float freq[] = new float[dataSize]; cuMemcpyDtoH(Pointer.to(freq), (CUdeviceptr)inputDataPointer, dataSize * Sizeof.FLOAT); And the kernel function is next: extern \"C\" __global__ void calcSpectrumSamples(float* complexData, int dataSize, int N, int pair) { int i = threadIdx.x + blockIdx.x * blockDim.x; if(i &gt;= dataSize) return; complexData[i] = complexData[i] * 2; } But when I'm trying to pass the pointer which points to the result of FFT (in device memory) to the multiplyOn2 method, it throws the exception on cuCtxSynchronize() call. Exception: Exception in thread \"main\" jcuda.CudaException: CUDA_ERROR_UNKNOWN at jcuda.driver.JCudaDriver.checkResult(JCudaDriver.java:263) at jcuda.driver.JCudaDriver.cuCtxSynchronize(JCudaDriver.java:1709) at com.ifntung.cufft.HilbertSpectrumTicksKernelInvoker.multiplyOn2(HilbertSpectrumTicksKernelInvoker.java:73) at com.ifntung.cufft.FFTTransformer.main(FFTTransformer.java:123) I was trying to do the same using Visual Studion C++ and there no problems with this. Could you please help me. P.S. I can solve this prolem, but I need to copy data from device memory to host memory and then copy back with creating new pointers every time before calling new cuda functions, which slows my program executing.",
        "answers": [
            [
                "Where exactly does the error occurs at which line? The Cuda error can also be a previous error. Why do you use Pointer.to(inputDataPointer), you already have that device pointer. Now you pass a pointer to the device pointer to the device? Pointer kernelParameters = Pointer.to(Pointer.to(inputDataPointer), I also recommend to use \"this\" qualifier or any other marking to detect instance variables. I hate and refuse to look through code, especially as nested and long as your example if I cannot see which scope the variable in methods have trying to debug it by just reading it. I don't wanna ask myself always where the hell comes this variable from. If a complex code in a question at SO is not formatted properly I don't read it."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "The question lays out all the pertinent versions. I'm trying to run one of the sample files at http://www.jcuda.org: JCublasMatrixInvert.java. I've got the NVIDIA CUDA drivers installed; I've downloaded the JCUDA 0.4.1 bindings and added their /lib directory to my java.library.path. Here are the JVM parameters I set when I run: -ea -XX:+UseParNewGC -XX:+UseNUMA -XX:PermSize=128m -XX:MaxPermSize=256m -Xmx1024m -Djava.library.path=F:\\Projects\\Java\\learning\\out\\production\\learning Yet when I try to run the class I get the following stack trace: Error while loading native library \"JCublas-windows-x86_64\" with base name \"JCublas\" Operating system name: Windows 7 Architecture : amd64 Architecture bit size: 64 Stack trace from the attempt to load the library as a resource: java.lang.NullPointerException: No resource found with name '/lib/JCublas-windows-x86_64.dll' at jcuda.LibUtils.loadLibraryResource(LibUtils.java:151) at jcuda.LibUtils.loadLibrary(LibUtils.java:83) at jcuda.jcublas.JCublas.initialize(JCublas.java:82) at jcuda.jcublas.JCublas.&lt;clinit&gt;(JCublas.java:70) at matrix.jcuda.JCublasMatrixInvert.main(JCublasMatrixInvert.java:34) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120) Stack trace from the attempt to load the library as a file: java.lang.UnsatisfiedLinkError: F:\\Projects\\Java\\learning\\out\\production\\learning\\JCublas-windows-x86_64.dll: Can't find dependent libraries at java.lang.ClassLoader$NativeLibrary.load(Native Method) at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1928) at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1854) at java.lang.Runtime.loadLibrary0(Runtime.java:845) at java.lang.System.loadLibrary(System.java:1084) at jcuda.LibUtils.loadLibrary(LibUtils.java:94) at jcuda.jcublas.JCublas.initialize(JCublas.java:82) at jcuda.jcublas.JCublas.&lt;clinit&gt;(JCublas.java:70) at matrix.jcuda.JCublasMatrixInvert.main(JCublasMatrixInvert.java:34) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120) Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Could not load the native library at jcuda.LibUtils.loadLibrary(LibUtils.java:129) at jcuda.jcublas.JCublas.initialize(JCublas.java:82) at jcuda.jcublas.JCublas.&lt;clinit&gt;(JCublas.java:70) at matrix.jcuda.JCublasMatrixInvert.main(JCublasMatrixInvert.java:34) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120) Process finished with exit code 1 I can see the allegedly missing dll at the path F:\\Projects\\Java\\learning\\out\\production\\learning\\JCublas-windows-x86_64.dll What have I missed? What am I doing wrong? Can anyone who has had success with running this example advise me? Thanks.",
        "answers": [
            [
                "If standard way is too hard - I recommend you to use mavenized version of JCuda. You could clone standard project here - https://github.com/MysterionRise/mavenized-jcuda it's working automatically for Windows/Linux 32/64 bit All you need to do is to run: You need to install Cuda 5.5 for your platform here - https://developer.nvidia.com/cuda-downloads Call mvn clean package to build project Call mvn exec:exec to run main class with \"Hello, JCuda\" sample :) After that you could replace main class with your JCuda code"
            ],
            [
                "Download that file from the jCuda download section and put it into your library path."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm working on my game project (tower defense) and I'm trying to compute the distance between all critters and a tower with JCuda using shared memory. For each tower I run 1 block with N threads, where N equals the number of critters on the map. I'm computing the distance between all critters and that tower for a given block, and I store the smallest found distance so far in the block's shared memory. My current code looks like that: extern \"C\" __global__ void calcDistance(int** globalInputData, int size, int critters, int** globalQueryData, int* globalOutputData) { //shared memory __shared__ float minimum[2]; int x = threadIdx.x + blockIdx.x * blockDim.x; int y = blockIdx.y; if (x &lt; critters) { int distance = 0; //Calculate the distance between tower and criter for (int i = 0; i &lt; size; i++) { int d = globalInputData[x][i] - globalQueryData[y][i]; distance += d * d; } if (x == 0) { minimum[0] = distance; minimum[1] = x; } __syncthreads(); if (distance &lt; minimum[0]) { minimum[0] = distance; minimum[1] = x; } __syncthreads(); globalOutputData[y * 2] = minimum[0]; globalOutputData[y] = minimum[1]; } } The problem is if I rerun the code using the same input multiple times (I free all the memory on both host and device after each run) I get different output each time I the code gets executed for blocks (tower) number &gt; 27... I'm fairly sure it has something to do with the shared memory and the way I'm dealing with it, as rewriting the code to use global memory gives the same result whenever the code gets executed. Any ideas?",
        "answers": [
            [
                "There is a memory race problem (so read-after-write correctness) in that kernel here: if (distance &lt; minimum[0]) { minimum[0] = distance; minimum[1] = x; } When executed, every thread in the block is going to try and simultaneously read and write the value of minimum. There are no guarantees what will happen when multiple threads in a warp try writing to the same shared memory location, and there are no guarantees what values that other warps in the same block will read when loading from a memory location to which is being written. Memory access is not atomic, and there is no locking or serialization which would ensure that code performed the type of reduction operation you seem to be trying to do. A milder version of the same problem applies to the write back to global memory at the end of the kernel: __syncthreads(); globalOutputData[y * 2] = minimum[0]; globalOutputData[y] = minimum[1]; The barrier before the writes ensures that the writes to minimum will be completed prior that a \"final\" (although inconsistent) value will be stored in minimum, but then every thread in the block will execute the write. If your intention is to have each thread compute a distance, and then for the minimum of the distance values over the block to get written out to global memory, you will have to either use atomic memory operations (for shared memory this is supported on compute 1.2/1.3 and 2.x devices only), or write an explicit shared memory reduction. After that, only one thread should execute the write back to global memory. Finally, you also have a potential synchronization correctness problem that could cause the kernel to hang. __syncthreads() (which maps to the PTX bar instruction) demands that every thread in the block arrive and execute the instruction prior to the kernel continuing. Having this sort of control flow: if (x &lt; critters) { .... __syncthreads(); .... } will cause the kernel to hang if some threads in the block can branch around the barrier and exit while others wait at the barrier. There should never be any branch divergence around a __syncthreads() call to ensure execution correctness of a kernel in CUDA. So, in summary, back to the drawing board on at least three issues in the current code."
            ]
        ],
        "votes": [
            1.0000001
        ]
    }
]