{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb508e6-029a-425d-9f3a-bd244d471c0d",
   "metadata": {},
   "source": [
    "## Notebook for inference of user queries \n",
    "\n",
    "### Run all the cells (Requires atleast 18 GB of GPU Memory), paste your question in the query variable and select the kind of question you are interested to ask. (Generic OR Code based)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4be580e-94ce-4e7d-8c5d-7ed541739547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c9078-7d43-4094-9ce6-fcdd80f69754",
   "metadata": {},
   "source": [
    "## Loading the generic model (finetuned on Nvidia tech blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0beda9-3dfd-4d84-aa23-266377c00b38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=torch.float32 with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280c98e70ddd4f08b33431c5e973638a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PEFT_MODEL_gen = \"llama2_qat_r32_a32_gen3\"\n",
    "PEFT_MODEL_gen = \"FALCON7B_r32_a64_gen_tot\"\n",
    "\n",
    "config_gen = PeftConfig.from_pretrained(PEFT_MODEL_gen)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit = True\n",
    ")\n",
    "\n",
    "\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(\n",
    "    config_gen.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True,\n",
    "    # use_auth_token = True\n",
    ")\n",
    "\n",
    "tokenizer_gen = AutoTokenizer.from_pretrained(config_gen.base_model_name_or_path)\n",
    "tokenizer_gen.pad_token = tokenizer_gen.eos_token\n",
    "\n",
    "model_gen = PeftModel.from_pretrained(model_gen, PEFT_MODEL_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45e640-39da-4389-b1f1-c352cb719402",
   "metadata": {},
   "source": [
    "### Setting up the generation parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a930f8b1-068f-4d87-8799-df0e688f2559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config_gen = model_gen.generation_config\n",
    "generation_config_gen.max_new_tokens = 100\n",
    "generation_config_gen.temperature = 0.005\n",
    "generation_config_gen.top_p = 1\n",
    "generation_config_gen.num_return_sequences = 1\n",
    "generation_config_gen.pad_token_id = tokenizer_gen.eos_token_id\n",
    "generation_config_gen.eos_token_id = tokenizer_gen.eos_token_id\n",
    "generation_config_gen.repetition_penalty = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7855af9-31d0-452c-92b7-3dad4d8e823e",
   "metadata": {},
   "source": [
    "## Loading the code model (Fine-tuned on stackover flow's queries about Nvidia SDKs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "420e0b66-ab4e-47b4-abcb-a7d597e6a676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97edc7ad925a45588d483e9ed12918e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PEFT_MODEL_code = \"pup_gorilla_model\"\n",
    "\n",
    "config_code = PeftConfig.from_pretrained(PEFT_MODEL_code)\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit = True\n",
    ")\n",
    "\n",
    "model_code = AutoModelForCausalLM.from_pretrained(\n",
    "    config_code.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    # use_auth_token = True\n",
    ")\n",
    "\n",
    "tokenizer_code = AutoTokenizer.from_pretrained(config_code.base_model_name_or_path)\n",
    "tokenizer_code.pad_token = tokenizer_code.eos_token\n",
    "\n",
    "model_code = PeftModel.from_pretrained(model_code, PEFT_MODEL_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4115c-0db4-44ab-b544-40dcbbd20088",
   "metadata": {},
   "source": [
    "### Setting up the generation parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be939fc2-504f-4288-9b25-1b0e3069a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config_code = model_code.generation_config\n",
    "generation_config_code.max_new_tokens = 100\n",
    "generation_config_code.temperature = 0.005\n",
    "generation_config_code.top_p = 1\n",
    "generation_config_code.num_return_sequences = 1\n",
    "generation_config_code.pad_token_id = tokenizer_code.eos_token_id\n",
    "generation_config_code.eos_token_id = tokenizer_code.eos_token_id\n",
    "generation_config_code.repetition_penalty = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a573fc-af83-40c8-a387-e3cc9143e2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f9cfaed-ae64-4965-b88e-c3a9b335c10a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:59\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "\n",
    "# question = '''How can I install the\n",
    "# NVIDIA CUDA Toolkit on\n",
    "# Windows?'''\n",
    "\n",
    "question = '''What is the NVIDIA CUDA Toolkit?'''\n",
    "\n",
    "ans_true = '''The CUDA Toolkit from NVIDIA provides everything\n",
    "you need to develop GPU-accelerated applications.\n",
    "The CUDA Toolkit includes GPU-accelerated libraries,\n",
    "a\n",
    "compiler, development tools and the CUDA runtime.'''\n",
    "\n",
    "\n",
    "# question = '''What are the key features of the NVIDIA TensorRT?'''\n",
    "# ans_true = '''NVIDIA TensorRT, an SDK for high-performance\n",
    "# deep learning inference, includes a deep learning\n",
    "# inference optimizer and runtime that delivers low\n",
    "# latency and high throughput for inference\n",
    "# applications.Some key features of TensorRT include:\n",
    "# - NVIDIA TensorRT-based applications perform up\n",
    "# to 36X faster than CPU-only platforms during\n",
    "# inference, enabling you to optimize neural network\n",
    "# models\n",
    "# - TensorRT, built on the NVIDIA CUDA®\n",
    "# parallel programming model, enables you to optimize\n",
    "# inference using techniques such as quantization, layer\n",
    "# and tensor fusion, kernel tuning, and others on\n",
    "# NVIDIA GPUs.'''\n",
    "\n",
    "\n",
    "question = '''What is the difference between\n",
    "NVIDIA's BioMegatron and Megatron\n",
    "530B LLM?'''\n",
    "\n",
    "ans_true = '''BioMegatron focuses specifically on biomedical NLP\n",
    "tasks and has been trained on relevant biomedical data,\n",
    "Megatron 530B LLM is a more general-purpose\n",
    "language model trained on a wide variety of text from\n",
    "different domains. The choice between the two models\n",
    "depends on the specific requirements and domain of your\n",
    "NLP task.'''\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Answer the following question about Nvidea's product and services in detail: \\n{question}\n",
    "\\nAnswer:-\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# Answer the following question about Nvidea's product and services: What is NVIDEA's Omniverse? What is the purpose of it? Explain breifly \\n\\n\n",
    "# \\n\\nAnswer:-:\n",
    "# \"\"\".strip()\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac1c992-f17f-4212-977e-9897e31f0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reponse(query, query_type):\n",
    "    \n",
    "    device = \"cuda:0\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        Answer the following question about Nvidia's product and services in detail: \\n{query}\n",
    "        \\nAnswer:-\n",
    "        \"\"\".strip()\n",
    "    \n",
    "    if query_type == \"code\":\n",
    "        \n",
    "        print(\"Code\")\n",
    "\n",
    "        \n",
    "        encoding = tokenizer_code(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model_code.generate(\n",
    "              input_ids = encoding.input_ids,\n",
    "              attention_mask = encoding.attention_mask,\n",
    "              generation_config = generation_config_code\n",
    "          )\n",
    "            \n",
    "        response = tokenizer_code.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        \n",
    "        return response\n",
    "        \n",
    "\n",
    "\n",
    "    elif query_type == \"generic\":\n",
    "        \n",
    "        print(\"Generic\")\n",
    "        \n",
    "        encoding = tokenizer_gen(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model_gen.generate(\n",
    "              input_ids = encoding.input_ids,\n",
    "              attention_mask = encoding.attention_mask,\n",
    "              generation_config = generation_config_gen\n",
    "          )\n",
    "            \n",
    "        response = tokenizer_gen.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        \n",
    "        return response\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('''Please choose query_type to be one of these :- {\"code\", \"generic\"}''')\n",
    "        return \n",
    "            \n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b1d69f-a3b9-49bc-83c0-5fa2527f0c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic\n"
     ]
    }
   ],
   "source": [
    "generation_config_gen = model_gen.generation_config\n",
    "generation_config_gen.max_new_tokens = 100\n",
    "generation_config_gen.temperature = 0.005\n",
    "generation_config_gen.top_p = 1\n",
    "generation_config_gen.num_return_sequences = 1\n",
    "generation_config_gen.pad_token_id = tokenizer_gen.eos_token_id\n",
    "generation_config_gen.eos_token_id = tokenizer_gen.eos_token_id\n",
    "generation_config_gen.repetition_penalty = 2.5\n",
    "\n",
    "\n",
    "# op = get_reponse(query = ''' What is Nvidia RAPIDS? what is it used for? '''\n",
    "            \n",
    "#             , query_type = \"generic\")\n",
    "\n",
    "\n",
    "query = \"How to install NVIDIA on vmware?\"\n",
    "\n",
    "op = get_reponse(query = query\n",
    "            \n",
    "            , query_type = \"generic\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8295dd5-15b8-4014-8c31-a021b2a198d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question about Nvidia's product and services in detail: \n",
      "How to install NVIDIA on vmware?\n",
      "        \n",
      "Answer:-: To install NVIDIA drivers for Linux guests, you must first enable virtualization support in your VMware Workstation or Fusion settings. Then download and run the installer package from nvidia.com/drivers. After installation, reboot your virtual machine. Once it restarts, log into GNOME Shell as root with sudo su -c sh -e '/usr/bin/nvdia_installer'. This will launch a graphical user interface that guides you through installing CUDA Toolkit components. You can also use this method to update existing installations of CUDA software packages.\n"
     ]
    }
   ],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd9085b-d991-4d0a-849c-f2260acdda26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic\n",
      "Answer the following question about Nvidia's product and services in detail: \n",
      "What is the NVIDIA CUDA Toolkit?\n",
      "        \n",
      "Answer:-: The NVIDIA CUDA Toolkit provides a unified, efficient programming environment for developing GPU-accelerated applications. It includes libraries, compilers, sample code, documentation, tools, drivers, performance tuning guides, profiling utilities, tutorials, support forums, and more.\n",
      "CPU times: user 5.48 s, sys: 2.93 ms, total: 5.48 s\n",
      "Wall time: 5.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What is the NVIDIA CUDA Toolkit?\"\n",
    "\n",
    "op = get_reponse(query = query\n",
    "            \n",
    "            , query_type = \"generic\")\n",
    "\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc026f38-cae6-4c7a-be75-e7536fbde95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic\n",
      "Answer the following question about Nvidia's product and services in detail: \n",
      "What are the key features of the NVIDIA TensorRT?\n",
      "        \n",
      "Answer:-: The NVIDIA® TensorRT™ inference engine is a high-performance, production ready library that accelerates deep learning inferencing on GPUs. It provides optimized implementations for popular neural networks such as ResNet50V2, MobileNetsv2/3, Inception v4, XGBoost, DensePose, etc., with support for multiple backends including CUDA, OpenCL, DirectX Compute Shaders, Intel SGDR, Microsoft CUDNN, Arm NNAPI,\n",
      "CPU times: user 10.3 s, sys: 2.66 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What are the key features of the NVIDIA TensorRT?\"\n",
    "\n",
    "op = get_reponse(query = query\n",
    "            \n",
    "            , query_type = \"generic\")\n",
    "\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32b6e6d0-8bf3-4c04-9485-54760bafb2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic\n",
      "Answer the following question about Nvidia's product and services in detail: \n",
      "What is the nvidia-smi command used for?\n",
      "        \n",
      "Answer:-: The nvidia-smi command displays information on GPUs, memory usage, fan speeds, power consumption, temperatures, clocks, errors, events, etc. It can be run from a terminal or as an interactive session via SSH to view real-time statistics.\n",
      "CPU times: user 5.52 s, sys: 1.19 ms, total: 5.53 s\n",
      "Wall time: 5.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What is the nvidia-smi command used for?\"\n",
    "\n",
    "op = get_reponse(query = query\n",
    "            , query_type = \"generic\")\n",
    "\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1a2ac97-32cf-43a2-93e2-aaa90cb878bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code\n",
      "Answer the following question about Nvidia's product and services in detail: \n",
      "What is the nvidia-smi command used for?\n",
      "        \n",
      "Answer:-: The NVIDIA System Management Interface, or nvidia-smi command, provides information on GPU utilization, temperature, power consumption, memory usage, fan speed, clock rate, core voltage, PCIe lane allocation, CUDA device availability, installed driver version, display output configuration, supported OpenCL platforms, and more. It can also be used to monitor real-time performance metrics of a running application with an interactive interface that displays graphs based on collected data.\n",
      "CPU times: user 9.49 s, sys: 9.78 ms, total: 9.5 s\n",
      "Wall time: 9.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What is the nvidia-smi command used for?\"\n",
    "\n",
    "op = get_reponse(query = query\n",
    "            , query_type = \"code\")\n",
    "\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf9306-c605-4804-a310-e7356595edf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df188cb-fb1b-4587-b26c-6f34dbd9516c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
