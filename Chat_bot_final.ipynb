{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb508e6-029a-425d-9f3a-bd244d471c0d",
   "metadata": {},
   "source": [
    "## Notebook for inference of user queries \n",
    "\n",
    "### Run all the cells (Requires atleast 18 GB of GPU Memory), paste your question in the query variable and select the kind of question you are interested to ask. (Generic OR Code based)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4be580e-94ce-4e7d-8c5d-7ed541739547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c9078-7d43-4094-9ce6-fcdd80f69754",
   "metadata": {},
   "source": [
    "## Loading the generic model (finetuned on Nvidia tech blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0beda9-3dfd-4d84-aa23-266377c00b38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=torch.float32 with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698282a0af554f9a828a58dd3ad0a25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PEFT_MODEL_gen = \"llama2_qat_r32_a32_gen3\"\n",
    "PEFT_MODEL_gen = \"FALCON7B_r32_a64_gen_tot\"\n",
    "\n",
    "config_gen = PeftConfig.from_pretrained(PEFT_MODEL_gen)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit = True\n",
    ")\n",
    "\n",
    "\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(\n",
    "    config_gen.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True,\n",
    "    # use_auth_token = True\n",
    ")\n",
    "\n",
    "tokenizer_gen = AutoTokenizer.from_pretrained(config_gen.base_model_name_or_path)\n",
    "tokenizer_gen.pad_token = tokenizer_gen.eos_token\n",
    "\n",
    "model_gen = PeftModel.from_pretrained(model_gen, PEFT_MODEL_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45e640-39da-4389-b1f1-c352cb719402",
   "metadata": {},
   "source": [
    "### Setting up the generation parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a930f8b1-068f-4d87-8799-df0e688f2559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config_gen = model_gen.generation_config\n",
    "generation_config_gen.max_new_tokens = 100\n",
    "generation_config_gen.temperature = 0.005\n",
    "generation_config_gen.top_p = 1\n",
    "generation_config_gen.num_return_sequences = 1\n",
    "generation_config_gen.pad_token_id = tokenizer_gen.eos_token_id\n",
    "generation_config_gen.eos_token_id = tokenizer_gen.eos_token_id\n",
    "generation_config_gen.repetition_penalty = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7855af9-31d0-452c-92b7-3dad4d8e823e",
   "metadata": {},
   "source": [
    "## Loading the code model (Fine-tuned on stackover flow's queries about Nvidia SDKs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420e0b66-ab4e-47b4-abcb-a7d597e6a676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95edb60fb71b452a967a0141ce73b25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PEFT_MODEL_code = \"pup_gorilla_model\"\n",
    "\n",
    "config_code = PeftConfig.from_pretrained(PEFT_MODEL_code)\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit = True\n",
    ")\n",
    "\n",
    "model_code = AutoModelForCausalLM.from_pretrained(\n",
    "    config_code.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    # use_auth_token = True\n",
    ")\n",
    "\n",
    "tokenizer_code = AutoTokenizer.from_pretrained(config_code.base_model_name_or_path)\n",
    "tokenizer_code.pad_token = tokenizer_code.eos_token\n",
    "\n",
    "model_code = PeftModel.from_pretrained(model_code, PEFT_MODEL_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4115c-0db4-44ab-b544-40dcbbd20088",
   "metadata": {},
   "source": [
    "### Setting up the generation parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be939fc2-504f-4288-9b25-1b0e3069a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config_code = model_code.generation_config\n",
    "generation_config_code.max_new_tokens = 100\n",
    "generation_config_code.temperature = 0.005\n",
    "generation_config_code.top_p = 1\n",
    "generation_config_code.num_return_sequences = 1\n",
    "generation_config_code.pad_token_id = tokenizer_code.eos_token_id\n",
    "generation_config_code.eos_token_id = tokenizer_code.eos_token_id\n",
    "generation_config_code.repetition_penalty = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a573fc-af83-40c8-a387-e3cc9143e2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cfaed-ae64-4965-b88e-c3a9b335c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "\n",
    "# question = '''How can I install the\n",
    "# NVIDIA CUDA Toolkit on\n",
    "# Windows?'''\n",
    "\n",
    "# question = '''What is the NVIDIA CUDA Toolkit?'''\n",
    "\n",
    "# ans_true = '''The CUDA Toolkit from NVIDIA provides everything\n",
    "# you need to develop GPU-accelerated applications.\n",
    "# The CUDA Toolkit includes GPU-accelerated libraries,\n",
    "# a\n",
    "# compiler, development tools and the CUDA runtime.'''\n",
    "\n",
    "\n",
    "# question = '''What are the key features of the NVIDIA TensorRT?'''\n",
    "# ans_true = '''NVIDIA TensorRT, an SDK for high-performance\n",
    "# deep learning inference, includes a deep learning\n",
    "# inference optimizer and runtime that delivers low\n",
    "# latency and high throughput for inference\n",
    "# applications.Some key features of TensorRT include:\n",
    "# - NVIDIA TensorRT-based applications perform up\n",
    "# to 36X faster than CPU-only platforms during\n",
    "# inference, enabling you to optimize neural network\n",
    "# models\n",
    "# - TensorRT, built on the NVIDIA CUDAÂ®\n",
    "# parallel programming model, enables you to optimize\n",
    "# inference using techniques such as quantization, layer\n",
    "# and tensor fusion, kernel tuning, and others on\n",
    "# NVIDIA GPUs.'''\n",
    "\n",
    "\n",
    "question = '''What is the difference between\n",
    "NVIDIA's BioMegatron and Megatron\n",
    "530B LLM?'''\n",
    "\n",
    "ans_true = '''BioMegatron focuses specifically on biomedical NLP\n",
    "tasks and has been trained on relevant biomedical data,\n",
    "Megatron 530B LLM is a more general-purpose\n",
    "language model trained on a wide variety of text from\n",
    "different domains. The choice between the two models\n",
    "depends on the specific requirements and domain of your\n",
    "NLP task.'''\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Answer the following question about Nvidea's product and services in detail: \\n{question}\n",
    "\\nAnswer:-\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# Answer the following question about Nvidea's product and services: What is NVIDEA's Omniverse? What is the purpose of it? Explain breifly \\n\\n\n",
    "# \\n\\nAnswer:-:\n",
    "# \"\"\".strip()\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac1c992-f17f-4212-977e-9897e31f0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reponse(query, query_type):\n",
    "    \n",
    "    device = \"cuda:0\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        Answer the following question about Nvidia's product and services in detail: \\n{query}\n",
    "        \\nAnswer:-\n",
    "        \"\"\".strip()\n",
    "    \n",
    "    if query_type == \"code\":\n",
    "        \n",
    "        print(\"Code\")\n",
    "\n",
    "        \n",
    "        encoding = tokenizer_code(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model_code.generate(\n",
    "              input_ids = encoding.input_ids,\n",
    "              attention_mask = encoding.attention_mask,\n",
    "              generation_config = generation_config_code\n",
    "          )\n",
    "            \n",
    "        response = tokenizer_code.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        \n",
    "        return response\n",
    "        \n",
    "\n",
    "\n",
    "    elif query_type == \"generic\":\n",
    "        \n",
    "        print(\"Generic\")\n",
    "        \n",
    "        encoding = tokenizer_gen(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model_gen.generate(\n",
    "              input_ids = encoding.input_ids,\n",
    "              attention_mask = encoding.attention_mask,\n",
    "              generation_config = generation_config_gen\n",
    "          )\n",
    "            \n",
    "        response = tokenizer_gen.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        \n",
    "        return response\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('''Please choose query_type to be one of these :- {\"code\", \"generic\"}''')\n",
    "        return \n",
    "            \n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b1d69f-a3b9-49bc-83c0-5fa2527f0c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic\n"
     ]
    }
   ],
   "source": [
    "generation_config_gen = model_gen.generation_config\n",
    "generation_config_gen.max_new_tokens = 500\n",
    "generation_config_gen.temperature = 0.005\n",
    "generation_config_gen.top_p = 1\n",
    "generation_config_gen.num_return_sequences = 1\n",
    "generation_config_gen.pad_token_id = tokenizer_gen.eos_token_id\n",
    "generation_config_gen.eos_token_id = tokenizer_gen.eos_token_id\n",
    "generation_config_gen.repetition_penalty = 2.5\n",
    "\n",
    "\n",
    "# op = get_reponse(query = ''' What is Nvidia RAPIDS? what is it used for? '''\n",
    "            \n",
    "#             , query_type = \"generic\")\n",
    "\n",
    "\n",
    "query = \"How to install NVIDIA on vmware?\"\n",
    "\n",
    "op = get_reponse(query = query\n",
    "            \n",
    "            , query_type = \"generic\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8295dd5-15b8-4014-8c31-a021b2a198d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9085b-d991-4d0a-849c-f2260acdda26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc026f38-cae6-4c7a-be75-e7536fbde95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6e6d0-8bf3-4c04-9485-54760bafb2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
