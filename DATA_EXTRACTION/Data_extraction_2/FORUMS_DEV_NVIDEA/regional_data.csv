,query,data
0,nvidia-ai-generating-motion-capture-animation-without-hardware-or-motion-data,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ai-generating-motion-capture-animation-without-hardware-or-motion-data/
NVIDIA researchers developed a framework to build motion capture animation without the use of hardware or motion data by simply using video capture and AI.NVDA Shareholder with an idea for development:
Holograms:Picture this and then imagine…A special occasion
is coming up, (Birthday,anniverary,wedding,graduation)
and you make a reservation for dinner at a personalized
venue with multiple choices for your entertainment.
An evening at this dinner event goes like this:
A pre selected famous figure chosen by you (a hologram of
John Wayne) guides you to your parking space.
Upon entering the venue, a Hostess greets you and asks for your
reservation name which includes your list of options
for the evening. A hologram of Frankenstein appears and
escorts you to your dinner room of choice.
A hologram of Star Wars R2D2 shows you to your table.
You have chosen the Storm room venue. A hologram of an
approaching thunderstorm begins to appear and fills the
room with clouds, thunder and lightning complete with
sound effects.
A hologram of a much larger than life flying hummingbird slowly
materializes above your table to take your order.
After dinner you all decide to stop by the wild west bar
for a drink. Various holograms of wild west characters having
gun fights appear in different areas of the room.
You sit down at the bar with a full mirrored back bar and in
the mirror various figures (chosen by you beforehand) appear
in the mirror on the bar stool next to you.Imagine:
The ultimate 3D experience…Hologram fantasies of choice.
nvidia is one of very few company’s with the technological
abilities to make this happen.
Sincerely,
Bryan Mailliard
B &C Mailliard
480 694 1367Looks great! Any plans to release source code or a implementation of this Motion Capture research?Hey @Sephiroth_FF, thanks for jumping into the forums! Currently the researchers are not planning on releasing the source code, the method currently relies on an unreleased project that will be presented a bit later this year. Keep an eye out for new research from the authors and a potential source code release afterwards!I have been watching for updates for this project on github and on the research project page. Any idea when we will get updates and a source code release? Thanks!Unfortunately, no additional plans have been made to progress with releasing the code. I will update this thread when I have any news related to this project.Powered by Discourse, best viewed with JavaScript enabled"
1,explainer-what-is-a-digital-twin,"Originally published at:			What Is a Digital Twin? | NVIDIA Blog
A digital twin is a virtual representation synchronized with physical things, people, or processes.Powered by Discourse, best viewed with JavaScript enabled"
2,scaling-ai-with-mlops-and-the-nvidia-partner-ecosystem,"Originally published at:			https://developer.nvidia.com/blog/scaling-ai-with-mlops-and-the-nvidia-partner-ecosystem/
AI is impacting every industry, from improving customer service and streamlining supply chains to accelerating cancer research.  As enterprises invest in AI to stay ahead of the competition, they often struggle with finding the strategy and infrastructure for success. Many AI projects are rapidly evolving, which makes production at scale especially challenging. We believe in…Our MLOps partner ecoystem is growing daily, making it easier than ever to deploye production AI at scale. The MLOps sessions and the partner panel at GTC are going to cover so much new ad usefully information to help get enterprises off the ground. I highly recommend joining to hear from the experts!Powered by Discourse, best viewed with JavaScript enabled"
3,explainer-what-is-edge-ai-and-how-does-it-work,"Originally published at:			What Is Edge AI and How Does It Work? | NVIDIA Blog
Edge AI is the deployment of AI applications in devices throughout the physical world. The AI computation is done near the user at the edge of the network, close to where the data is located.Powered by Discourse, best viewed with JavaScript enabled"
4,developing-an-end-to-end-auto-labeling-pipeline-for-autonomous-vehicle-perception,"Originally published at:			https://developer.nvidia.com/blog/developing-an-end-to-end-auto-labeling-pipeline-for-autonomous-vehicle-perception/
Learn about a GPU-powered automated labeling pipeline developed as a part of Tata’s AI-based autonomous vehicle platform.Powered by Discourse, best viewed with JavaScript enabled"
5,how-to-overlap-data-transfers-in-cuda-c-c,"Originally published at:			https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/
In our last CUDA C/C++ post we discussed how to transfer data efficiently between the host and device.  In this post, we discuss how to overlap data transfers with computation on the host, computation on the device, and in some cases other data transfers between the host and device. Achieving overlap between data transfers and other…Thanks for the great article.I suspect your cudaMemcpyAsync() invocations in the first example are missing the ""kind"" argument.Thanks for noticing! I fixed this.Nice article. Small suggestion: the behavior of the default stream with respect to synchronization has changed throught different CUDA versions sync the article has been written (e.g. no more implicit sync with CUDA 7). It would be useful to add a small recap of the behavior according to the version.I added mentions of the CUDA 7 behavior along with a link to the post GPU Pro Tip: CUDA 7 Streams Simplify Concurrency.Hi. I want to also overlap/hide the memory copy from host pageable to host pinned (following the model in your last post) but cudaMemcpyAsync with hosttohost does not do. it also destroy the overlapping of memcpyasync from hosttodevice. do you have any idea why?Hi Mark,I have a question, if the time required for the host-to-device transfer, kernel execution, and device-to-host transfer are not the same, as contrary to the above post. Then is there any formula to compute ""the optimal number of streams to be created"", for example on a Tesla K40 GPU?To be more precise, if the time to transfer from HtoD (input data) is much higher than DtoH (result data) and the time for kernel execution is even higher than both the memory transfer, then is there any formula to compute the optimal number of streams to be created to achieve maximum performance.If there is any documentation or papers on this it would be of great help, if you can cite them here.Thanks in advanceHi, thank you for this great article. I have some observation with Quadro K420. When using multiple streams on their own CPU threads and synchronizing(after each copy + kernel + copy) within their own CPU threads, they get serialized in timeline. When I enqueue many copy + kernel + copy per stream and synchronize only once at the end, they all overlap. Why would cuStreamSynchronize(streamHandle) stop other streams overlapping with this? I tried changing synchronization policies such as spin wait, block and yield. They all do same. How can I copy+kernel+copy+synchronize on different threads with their own streams and expect them to overlap in time? It does this on only first sync but can't overlap anything on next syncs.Maybe this is possible with only hyper-q?Note: all CPU threads I mentioned are completely free of each other. They don't wait for any specific order. They just issue commands to their own streams as soon as possible(maybe not a good practice but) then expect drivers to handle the overlapping.- Tested with both WDDM and TCC mode (I have 2 of same card)- Using driver api equivalent commands (with async suffix).- If arrays are not pinned, they do overlap but nearly %30 slower overall- kernel is just vecAdd and data is 1M unsigned char elements per stream (for a,b,c arrays)- arrays are same but regions are 1M leaping per stream- 3 streams- tried with and without #define CUDA_API_PER_THREAD_DEFAULT_STREAMIt's really hard to help without more detailed information, and it's hard to debug programs in the comments of a blog post. May I suggest you post your question, along with a test program, on either the cuda tag of StackOverflow or on devtalk.nvidia.com forums? The experts on those sites are likely to be able to help find the issue. Thanks!Thank you very much. I'll prepare a retriggerable version and post them.Issue was environment variable for cuda max connections. Setting it to 16 and using TCC mode solved the problem.Thanks for sharing your solution!Forgot to say this was windows.In linux, all are ok with or without max connections setting.Maybe windows is not so focuesd on computing.Hi,Nice article. Definitely a good read.I had a clarification which may not have been considered by some. This article and method assumes that all the data would fit on the GPU to be run in a single stream (stream0) right? In other words, this method would not work if I were trying to overlap processing and data transfer for a workload which does not fit in the GPU main memory all at once. Is there a way to signal the next memory preload as soon as the current main memory data is moved to local scratch pad memory? I am imagining this optimization for something like PiRNA which takes an enormous amount of memory to process.ThanksThanks a lot for the article Mark. I notice that the kernel executed in sequential version uses 4x more threads compared to the kernels executed in the asynchronous versions. However, the each of the kernels in the asynchronous version only spent 1/4 time compared to the kernel in sequential version. I was expecting the they should almost be the same. Could you please explain why? Thank you very much.First of all, I would like to thank for coming up with such an elaborative and wonderful article.I just had one query. As per this article, ""When multiple kernels are issued back-to-back in different streams, the scheduler tries to enable concurrent execution of these kernels and as a result delays a signal that normally occurs after each kernel completion"". I would just like to ask whether this property is followed in all the different types of GPU devices or this is a device specific property. If it is a device specific property, do we have any method to find out whether this occurs in any particular GPU device.Hi @user34605 , there is a device property you can query called concurrentKernels.  See the cudaDevAttrConcurrentKernels attribute and the cudaDeviceGetAttribute API.At this time, most modern CUDA GPUs support concurrent kernels. However some Tegra (embedded) GPUs may not (I’m not sure about this), especially small GPUs with only a single multiprocessor.Glad you liked the post, thanks!Currently, I am using a volta V100-32GB GPU. The value of the variable cudaDeviceProp:asyncEngineCount for this GPU is 6.So, can it allow 6 concurrent copying operation ( 3 in D2H direction and other 3 in H2D direction) ?Assume that a GPU device allows 4 concurrent copying operations (2 in D2H direction and 2 in H2D direction).I issue following two copying operations in the same direction and in different streams:cudaMemcpyAsync(A, dA, n, cudaMemcpyDeviceToHost,stream[i])cudaMemcpyAsync(B, dB, n, cudaMemcpyDeviceToHost,stream[i+1])Will these two copy operations happen concurrently?In some of the V100-32GB GPUs, the value of cudaDeviceProp:asyncEngineCount is 5.What is the interpretation of having an odd number of copy engines? Does it mean that 2 copy engines are used for copying in D2H direction, the other 2 engines are used for copying in H2D direction and the remaining one is used for copying in D2D direction?Powered by Discourse, best viewed with JavaScript enabled"
6,speech-ai-spotlight-visualizing-spoken-language-and-sounds-on-ar-glasses,"Originally published at:			https://developer.nvidia.com/blog/speech-ai-spotlight-visualizing-spoken-language-and-sounds-on-ar-glasses/
Audio can include a wide range of sounds, from human speech to non-speech sounds like barking dogs and sirens. When designing accessible applications for people with hearing difficulties, the application should be able to recognize sounds and understand speech. Such technology would help deaf or hard-of-hearing individuals with visualizing speech, like human conversations and non-speech…I really enjoyed the article!! Making a generalized sound AI system that covers both speech and non-speech sounds is definitely a challenging goal, but we were able to demonstrate this futuristic goal using NVIDIA Riva with Cochl.Sense!Powered by Discourse, best viewed with JavaScript enabled"
7,accelerating-ai-development-with-nvidia-tao-toolkit-and-weights-amp-biases,"Originally published at:			https://developer.nvidia.com/blog/accelerating-ai-development-with-nvidia-tao-toolkit-and-weights-biases/
Learn how to integrate NVIDIA TAO Toolkit and the Weights and Biases MLOps platform to accelerate common AI tasks.Powered by Discourse, best viewed with JavaScript enabled"
8,nvidia-on-demand-top-data-science-sessions-from-gtc-2023,"Originally published at:			Playlist | Top Data Science Sessions | NVIDIA On-Demand
Learn from experts about how to optimize a data pipeline or use machine learning for anomaly detection with these 15 educational sessions.Powered by Discourse, best viewed with JavaScript enabled"
9,transferring-industrial-robot-assembly-tasks-from-simulation-to-reality,"Originally published at:			https://developer.nvidia.com/blog/transferring-industrial-robot-assembly-tasks-from-simulation-to-reality/
Simulation is an essential tool for robots learning new skills. These skills include perception (understanding the world from camera images), planning (formulating a sequence of actions to solve a problem), and control (generating motor commands to change a robot’s position and orientation).  Robotic assembly is ubiquitous in the automotive, aerospace, electronics, and medical device industries.…Powered by Discourse, best viewed with JavaScript enabled"
10,in-game-gpu-profiling-for-directx-12-using-setbackgroundprocessingmode,"Originally published at:			https://developer.nvidia.com/blog/in-game-gpu-profiling-for-dx12-using-setbackgroundprocessingmode/
Leanr best practices for performing in-game GPU profiling while monitoring the state of the background driver optimizations, using the DX12 SetBackgroundProcessingMode API on NVIDIA GPUs.Powered by Discourse, best viewed with JavaScript enabled"
11,event-cuda-12-2-youtube-premiere,"Originally published at:			CUDA Toolkit12.2: New Accelerated Computing and Security Enhancements Revealed - YouTube
On July 6, join experts for a deep dive into CUDA 12.2, including support for confidential computing.Powered by Discourse, best viewed with JavaScript enabled"
12,explainer-what-is-a-smartnic,"Originally published at:			What Is a SmartNIC? | NVIDIA Blog
A SmartNIC is a programmable accelerator that makes data center networking, security and storage efficient and flexible.Powered by Discourse, best viewed with JavaScript enabled"
13,modeling-earth-s-atmosphere-with-spherical-fourier-neural-operators,"Originally published at:			https://developer.nvidia.com/blog/modeling-earths-atmosphere-with-spherical-fourier-neural-operators/
Machine learning-based weather prediction has emerged as a promising complement to traditional numerical weather prediction (NWP) models. Models such as NVIDIA FourCastNet have demonstrated that the computational time for generating weather forecasts can be reduced from hours to mere seconds, a significant improvement to current NWP-based workflows. Traditional methods are formulated from first principles and…Powered by Discourse, best viewed with JavaScript enabled"
14,develop-ai-powered-robots-smart-vision-systems-and-more-with-nvidia-jetson-orin-nano-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/develop-ai-powered-robots-smart-vision-systems-and-more-with-nvidia-jetson-orin-nano-developer-kit/
Discover how easy it can be to develop robotics and edge AI applications with the new NVIDIA Jetson Orin Nano Developer Kit.Jetson Orin Nano devkit does not have a hardware encoder and I will need use cpu to encode CSI camera inputs? This seems like a step back from Jetson Nano to me. I would expect hardware encode for anything related to computer vision and robot vision these days.Where to buy? :)   I kept looking for an Amazon link or even the gear store but still seeing the previous nano there.  It would be great if the article could link to a few of the resellers.All I see is coming soon. Nothing in stock. So… Where is that Orin Nano? You get an email that says Now Available except its notThe Jetson Orin Nano Developer Kit is now available to buy from some distributors and will be available from others soon. Please see the Buy Link that is on Jetson Orin Page: NVIDIA Jetson OrinPowered by Discourse, best viewed with JavaScript enabled"
15,speech-ai-spotlight-reimagine-customer-service-with-virtual-agents,"Originally published at:			https://developer.nvidia.com/blog/speech-ai-spotlight-reimagine-customer-service-with-virtual-agents/
Virtual agents or voice-enabled assistants have been around for quite some time. But in the last decade, their usefulness and popularity have exploded with the use of AI. According to Gartner, virtual assistants will automate up to 75% of tasks for call center agents by 2025–up from 30% in 2021. This translates to a better…Powered by Discourse, best viewed with JavaScript enabled"
16,dynamic-graphs,"Have you come across any scenarios where users need their graph to change dynamically? What are those scenarios? For example, the user has a large existing graph and modifications in the form of edge insertions or deletions are streamed in?A number of customers have been discussing the notion of a dynamic graph.  There are two main thrusts we have seen:
•        A fixed graph with time attributes where we run algorithms that filter data based on different time windows
•        Graphs where the graph is actually being updated over time.  That is, we want to create a graph, run some algorithms against that graph and then add/update/delete edges from the graph before running more algorithms.We have been looking at options for implementing these.Thank you : ).Can you please say something more about the second bullet? What are the applications and/or what are the algorithms being run?We have customers in the Financial sector and in the Cyber Security sector that have been interested in dynamic graphs.  Most of these are for running GNN workflows… do some inferencing, append some new financial or cyber transactions and then do more inferencing.Powered by Discourse, best viewed with JavaScript enabled"
17,dialed-into-5g-nvidia-cloudxr-4-0-brings-enhanced-flexibility-and-scalability-for-xr-deployment,"Originally published at:			https://developer.nvidia.com/blog/dialed-into-5g-cloudxr-4-0-brings-enhanced-flexibility-and-scalability-for-xr-deployment/
At GTC 2023, NVIDIA announced the latest release of NVIDIA CloudXR that enables you to customize this SDK for your applications and customers, and scale extended reality (XR) deployment across the cloud, 5G Mobile Compute Edge (MEC), and corporate networks. NVIDIA CloudXR 4.0 introduces new APIs that deliver enhanced flexibility for server and client application…Powered by Discourse, best viewed with JavaScript enabled"
18,cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility/
The Kepler architecture introduces texture objects, a new feature that makes textures easier to use and higher performance. Texture References Textures are likely a familiar concept to anyone who’s done much CUDA programming. A feature from the graphics world, textures are images that are stretched, rotated and pasted on polygons to form the 3D graphics…Hi,There are a couple of bugs in this code: viz. the case of token cudatextureObject_t should actually be cudaTextureObject_t, there are invalid symbols such as a double quote in kernel launch code.Importantly, upon trying this code my compiler complains :error: more than one instance of overloaded function ""tex1Dfetch"" matches the argument list:Making a similar setup for 2D texture, I get:error : more than one instance of overloaded function ""tex2D"" matches the argument list:My platform is:Windows 7, Visual Studio 2012, NVIDIA Nsight 3.2, CUDA 5.5 Runtime Project, Debug Compile, compute capability set to compute_35,sm_35GK110 Card (GTX 780), Intel i7.Hi,After I added a template type to ""tex1Dfetch"", I could compile the code above.I mean,  float x = tex1Dfetch(tex, i);should be  float x = tex1Dfetch<float>(tex, i);I hope this helps you :-)My platform is:Windows 7, Visual Studio 2012, CUDA 5.5, Release Compile, compute capability set to compute_35,sm_35The code snippets in the blog post have now been rectified.  Thanks for posting.Hey,We just bought a PC with NVIDIA card and are trying to run different example code from different sources. We ran into trouble now with the command:cudaCreateTextureObject(&tex, &resDesc, &texDesc, NULL);It seems that in our computer the function fails to assign a value to ""tex"". When adding an assertion in the next line:assert(tex != 0);the assertion fails. We have also tried to surround the object creation command with checkCudaErrors()  in hope to get more information, but it gives no error message (so the program thinks everything is fine).Could you help us sort it out? We really have run out of ideas here and we have not found any hints online either.Our system specs:Ubuntu 14.04GeForce GTX 750Driver Version: 331.62Cuda toolkit 6.0compiling with parameter  ""--gpu-architecture sm_50""PS! Also worth mentioning that we have a large-scale example using Cuda via python/pylearn2/Theano running without problems. So in other cases out Cuda works fine and thus it seems a very specific issue.I have the same problem on MacBook Pro with GTX 750M. Cuda 6.5Submitted a bug report in Cuda Zone, also mentioned that it does not work on your system. I will let you know if we get help from there. Hope for the best.Can you provide the bug ID number?Bug ID is 1562891I looked at the bug, and our QA was able to reproduce the problem. It is now being investigated.  Sorry for the inconvenience, we'll resolve it as soon as we can.Please inform us as soon as any progress is made on fixing the bug, so we could plan our work.This “old” article on texture usage and how to migrate from the texture template reference format to the (for many years) recommended usage will be much appreciated now that the texture template is obsolete (as of CUDA 12.0). Posting this message in the hope that it will help others finding it in the search function.
I needed to be referred to it my a human.  :-)
[https://developer.nvidia.com/blog/cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility](Texture article)Powered by Discourse, best viewed with JavaScript enabled"
19,low-code-building-blocks-for-speech-ai-robotics,"Originally published at:			Low-Code Building Blocks for Speech AI Robotics | NVIDIA Technical Blog
Learn how to add speech AI skills, like speech-to-text (STT) and text-to-speech (TTS), to your intelligent robotics application using low-code building blocks.Powered by Discourse, best viewed with JavaScript enabled"
20,reducing-development-time-for-intelligent-virtual-assistants-in-contact-centers,"Originally published at:			https://developer.nvidia.com/blog/reducing-development-time-for-intelligent-virtual-assistants-in-contact-centers/
As the global service economy grows, companies rely increasingly on contact centers to drive better customer experiences, increase customer satisfaction, and lower costs with increased efficiencies. Customer demand has increased far more rapidly than contact center employment ever could. Combined with the high agent churn rate, customer demand creates a need for more automated real-time…Powered by Discourse, best viewed with JavaScript enabled"
21,towards-environment-specific-base-stations-ai-ml-driven-neural-5g-nr-multi-user-mimo-receiver,"Originally published at:			https://developer.nvidia.com/blog/towards-environment-specific-base-stations-ai-ml-driven-neural-5g-nr-multi-user-mimo-receiver/
At this year’s Mobile World Congress (MWC), NVIDIA showcased a neural receiver​ for a 5G New Radio (NR) uplink multi-user MIMO scenario, which could be seen as​ a blueprint for possible 6G physical-layer architectures. For the first time, NVIDIA demonstrated a research prototype of a trainable neural network-based receiver that learns to replace significant parts…Powered by Discourse, best viewed with JavaScript enabled"
22,giving-virtual-dressing-rooms-a-makeover-with-computer-vision,"Originally published at:			https://developer.nvidia.com/blog/giving-virtual-dressing-rooms-a-makeover-with-computer-vision/
With the help of AI, a new fashion startup offers online retailers a scalable virtual dressing room, capable of cataloging over a million garment images weekly.Powered by Discourse, best viewed with JavaScript enabled"
23,optimizing-memory-with-nvidia-nsight-systems,"Originally published at:			https://developer.nvidia.com/blog/optimizing-memory-with-nvidia-nsight-systems/
NVIDIA Nsight Systems is a comprehensive tool for tracking application performance across CPU and GPU resources. It helps ensure that hardware is being efficiently used, traces API calls, and gives insight into inter-node network communication by describing how low-level metrics sum to application performance and finding where it can be improved. Nsight Systems can scale…Thanks for reading! Questions? Comments? Need help getting started? Let us know!Powered by Discourse, best viewed with JavaScript enabled"
24,deploying-diverse-ai-model-categories-from-public-model-zoo-using-nvidia-triton-inference-server,"Originally published at:			https://developer.nvidia.com/blog/deploying-diverse-ai-model-categories-from-public-model-zoo-using-nvidia-triton-inference-server/
Nowadays, a huge number of implementations of state-of-the-art (SOTA) models and modeling solutions are present for different frameworks like TensorFlow, ONNX, PyTorch, Keras, MXNet, and so on. These models can be used for out-of-the-box inference if you are interested in categories already in the datasets, or they can be embedded to custom business scenarios with…We hope you find this post a helpful starting point for AI model inference. If you have any questions or comments, let us knowGreat blog post!  I wanted to get the example code, but the link in the post appears to be broken.  How can I get the code?Thank you for the feedback. The code should be accessible now; please check it again.Powered by Discourse, best viewed with JavaScript enabled"
25,creating-custom-ai-models-using-nvidia-tao-toolkit-with-azure-machine-learning,"Originally published at:			https://developer.nvidia.com/blog/creating-custom-ai-models-using-nvidia-tao-toolkit-with-azure-machine-learning/
Learn how to accelerate your vision AI model development using NVIDIA TAO Toolkit and deploy it for inference with NVIDIA Triton Inference Server—all on the Azure platform.Powered by Discourse, best viewed with JavaScript enabled"
26,simplifying-ai-development-with-nvidia-base-command-platform,"Originally published at:			https://developer.nvidia.com/blog/simplifying-ai-development-with-base-command-platform/
The NVIDIA Base Command Platform enables an intuitive, fully featured development experience for AI applications. It was built to serve the needs of the internal NVIDIA research and product development teams. Now, it has become an essential method for accessing on-demand compute resources to train neural network models and execute other accelerated computing experiments.  Base Command Platform…Hi! I’m Joe, one of the authors of this blog - we’ve been working with Base Command Platform internally for a while now, and we’re excited to share it outside of NVIDIA! I’m happy to answer any questions that come up - please give the blog a read and let us know what you think, we’d be happy to hear from anyone interested in Base Command Platform!Powered by Discourse, best viewed with JavaScript enabled"
27,leverage-3d-geospatial-data-for-immersive-environments-with-cesium,"Originally published at:			Leverage 3D Geospatial Data for Immersive Environments with Cesium | NVIDIA Technical Blog
With Cesium for Omniverse, you can jump-start 3D geospatial app development with tiling pipelines for streaming your own content.Powered by Discourse, best viewed with JavaScript enabled"
28,what-is-a-pretrained-ai-model,"Originally published at:			What Is a Pretrained AI Model? | NVIDIA Blog
A pretrained AI model is a deep learning model that’s trained on large datasets to accomplish a specific task, and it can be used as is or customized to suit application requirements across multiple industries.Powered by Discourse, best viewed with JavaScript enabled"
29,spotlight-chaos-enscape-streamlines-vr-quality-and-performance-with-nvidia-vcr,"Originally published at:			https://developer.nvidia.com/blog/spotlight-chaos-enscape-streamlines-vr-quality-and-performance-with-vcr/
Precisely reproducing VR experiences is critical to many workflows, yet it is extremely challenging. But VR testing is critical for many teams, especially when they’re looking to troubleshoot a VR experience, or want to gain more insights into what customers see when they put on the headset. Chaos Enscape is using NVIDIA VR Capture and…Powered by Discourse, best viewed with JavaScript enabled"
30,how-to-watch,"how to watch? this forum engine is from 2002Yes, how to watch?Post your question here and the team will respond real time. There is no video to watch.We will have other events which have a live stream aspect but they will be hosted on our nvidia developer discord.
Sorry for any confusionNo problem, chat can be pretty easy. How does Chat GPT4 understand 3d layouts?No live, just chat. I’m out, can’t type fast enough maybe next time.No worries thanks for your interest.In this paper, Large Language Models as Tool Makers, we see GPT4 being used to generate tools in Python, and GPT3.5 being used as the consumer of those tools along with validation. Do we anticipate a time when GPT4 could make tools inside of Omniverse?Recent research shows the potential of enhancing the problem-solving ability
of large language models (LLMs) through the use of external tools. However,
prior work along this line depends on the availability of existing tools. In
this work, we take...Don’t worry, take your time with any question, we will also try to answer after the event is over.Thanks Mark, I have something I want to ask as part of a bigger question but looks like I’ll need bit more time on  my end.still trying to attend.
went on discord.
went on omniverse
went to livestream.
nothing?
what’s wrong?It is just this chat, here in the forum. Different format so far.ok. which chat?
livestream?
always returns to…
Build Custom AI Tools With ChatGPT and NVIDIA Omniverse : AMA June 28, 9am PDT - Connect With Experts - AMA / ChatGPT and Omniverse: AMA June 28th, 2023 - NVIDIA Developer ForumsLook at the directory in which this post is - and you will see other folks posting questions and getting answers. Sorry for any confusionUse this link to get to the directory if you like : ChatGPT and Omniverse: AMA June 28th, 2023 - NVIDIA Developer ForumsHey @zia_s_ideas, did you want to post this as a question?How does Chat GPT4 understand 3d layouts?We might have missed it in this thread, so please go ahead and re-post it in the main category.The LLM related one below is already “in the works”.It’s an interesting paper. If you’re a tool developer and want to incorporate AI, it’s a good one to look at. I think there will be many LLMs and LLM centric tools that can make tools inside of Omniverse. Already, ChatGPT is pretty good with Python and USD - both foundations of Omniverse extension and scene building. Because of this, I am able to use ChatGPT in my tool building workflow now. That said, it’s going to get much better. It’s easy to imagine LLMs helping with more of my existing work, and I’ve also started thinking that LLMs will become useful for temporary tools that I only need while working on a specific task. For example, maybe I want to make a temporary tool to create a UI that lists all of my lights and a brightness slider for each. I also think that tools that mix procedural algorythms + validation + AI are interesting for tool builders to explore.is it 28th or 29th? When will the event go live??Its live right now - folks are posting  questions  and getting answers . Look at the parent directory of this post.Hi @prateekha3 and welcome to the NVIDIA
developer forums.The event is live right now, here in this forum. Check the main category ChatGPT and Omniverse: AMA June 28th, 2023 - NVIDIA Developer Forums and you will see live questions and answers.Hey Paul! Thanks for the reply.
Yeah, that would be very cool. Using prompts to generate UI’s, or in my case, I’d rather skip UI’s all together and just have prompts drive the input. I think GUI’s have been necessary so average people don’t have to assiduously type in commands but now I think with the constraints of exacting syntax being relaxed, artists could start to bypass the need to learn sequences of GUI steps.
This could be especially useful for things like rendering which can get pretty involved.
Prompt driven rendering could be something like:“Render out this sequence from selected camera at 30 frames per second, 2k resolution, add the bloom filter, set to 6 percent in post and add an edge vignette with 6% inset. Also set the motion blur samples to low. Send a notification when the render is half done and totally done.”This way, we know what we want, but we don’t have to know how to get it.
Initially, we don’t have to dig through settings experimentally, nor dig through documentation or someone’s youtube tutorial. As an idea.Powered by Discourse, best viewed with JavaScript enabled"
31,understanding-and-measuring-pc-latency,"Originally published at:			https://developer.nvidia.com/blog/understanding-and-measuring-pc-latency/
Learn about PC Latency and how to leverage PCL Stats to accurately track, measure, and improve the latency within your rendering pipeline.Powered by Discourse, best viewed with JavaScript enabled"
32,new-course-introduction-to-robotic-simulations-in-isaac-sim,"Originally published at:			Courses – NVIDIA
Learn how to use NVIDIA Isaac Sim to tap into the simulation loop of a 3D engine and initialize experiments with objects, robots, and physics logic.Powered by Discourse, best viewed with JavaScript enabled"
33,high-performance-storage-on-nvidia-dgx-cloud-with-oracle-cloud-infrastructure,"Originally published at:			https://developer.nvidia.com/blog/high-performance-storage-on-nvidia-dgx-cloud-with-oracle-cloud-infrastructure/
Learn how NVIDIA partnered with Oracle Compute Infrastructure to build high-performance storage for NVIDIA DGX Cloud with NVMesh software.Powered by Discourse, best viewed with JavaScript enabled"
34,data-structures,"What data structures do your customers ask for? Or do they care? Like, do you get requests that say “I’d really like to use X data structure, can you support that”?We support CSR and COO input formats within cuGraph.  Internally we reformat the data to an optimized DCSR hybrid format  which is highly optimized for our graph primitives and algorithm use.Do customers ask for anything else? (Like, is anything else on your radar where you say “we should think about supporting that”?)Vertex/edge masking/insertion/deletion. We’re planning to update our graph data structures to support these features.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
35,an-even-easier-introduction-to-cuda,"Originally published at:			https://developer.nvidia.com/blog/even-easier-introduction-cuda/
Learn more with these hands-on DLI courses: Fundamentals of Accelerated Computing with CUDA C/C++ Fundamentals of Accelerated Computing with CUDA Python This post is a super simple introduction to CUDA, the popular parallel computing platform and programming model from NVIDIA. I wrote a previous “Easy Introduction” to CUDA in 2013 that has been very popular over…Hi Sir, thanks for the tutorial. I tried this code on Tesla T4, and found that using 4096 blocks does not improve the performance to us level compare to 1 block 256 threads. What might be the reason ? thanks.Thanks for the post Mark.  To get accurate profiling, is it still a good idea to put cudaDeviceReset() just prior to exiting?  https://devblogs.nvidia.com...Also, is it possible to get this level of timing via code?  cudaEventElapsedTime does not seem to have this same level of precision.Thanks Mark. I tried this and it did not work as a straight copy and paste. The cudaMallocManaged did not appear to do anything. This is on a Titan X with up to date drivers and NSight. I replaced the cudaMallocManaged functionality with the relevant cudaMalloc and cudaMempy, which sorted it. Am I missing something wrt cudaMallocManaged?It's a really good post btw. Thanks - I have learned much.What do you mean did not appear to do anything? Did you get an error? Incorrect results? What CUDA version do you have installed? Is it an ""NVIDIA Titan X"" (Pascal) or ""GeForce GTX Titan X"" (Maxwell)?I think the Windows tools are more dependent on cudaDeviceReset(). I kept it out of this post to keep things simple. cudaEventElapsedTime() should have the same level of precision, but in more complex apps you may get things in your timing that you didn't intend.I believe the most reliable way to accurately time is to run your kernel many times in a loop, followed by cudaDeviceSynchronize() or cudaStreamSynchronize(), and use a high precision CPU timer (like std::chrono) to wrap the whole loop and the sync. Then divide by the number of iterations.CUDA 8 latest. It's the Pascal card. After executing the cudaMallocManaged function the variable pointed to address 0x0. I'll put error checking into your original code in the morning and try to get more diagnostic information.I cannot access variable assigned using 'cudaMallocManaged' on host. It throws '0xC0000005: Access violation writing location 0x00000000' error. I can access them fine on device kernel. Am I missing something ? I am using MS Visual Studio Community 2015 with CUDA Runtime 8.0 on GTX 1070.I think I had the same problem. Var pointed to '0x0' and it was not accessible by host. However accessible by device. I know it works the old was using separate var for host and device. But it would be good if we can make it work without all those memCpy like this example does. Tell me if you have any luck.Did you guys change the program at all? If you share your changes I can try to diagnose.Did you change the code, or are you getting the error in the initialization loop?I didn't change the code at all. Just copied it to visual studio. Yes, I got error on initialization loop. So I put initialization in kernel. Got error at verification as expected. Removed verification and it ran fine. From this I concluded that it's not accessible by host. Further mode in VS debugging watch it shows it the same was as var allocated by cudaMalloc, some can't read or something indicating that it's not accessible to CPU as I understand. And the address is 0x0. Sorry for long response. I really appreciate your help.I just put the unchanged original on a box with a GT 755M and it fails similarly. What have I done wrong?:::#include ""cuda_runtime.h""#include ""device_launch_parameters.h""#include <iostream>#include <math.h>// Kernel function to add the elements of two arrays__global__void add(int n, float *x, float *y){    for (int i = 0; i < n; i++)        y[i] = x[i] + y[i];}int main(void){    int N = 1 << 20;    float *x, *y;    // Allocate Unified Memory ñ accessible from CPU or GPU    cudaMallocManaged(&x, N*sizeof(float));    cudaMallocManaged(&y, N*sizeof(float));    // initialize x and y arrays on the host    for (int i = 0; i < N; i++) {        x[i] = 1.0f;        y[i] = 2.0f;    }    // Run kernel on 1M elements on the GPU    add << <1, 1 >> >(N, x, y);    // Wait for GPU to finish before accessing on host    cudaDeviceSynchronize();    // Check for errors (all values should be 3.0f)    float maxError = 0.0f;    for (int i = 0; i < N; i++)        maxError = fmax(maxError, fabs(y[i] - 3.0f));    std::cout << ""Max error: "" << maxError << std::endl;    // Free memory    cudaFree(x);    cudaFree(y);    return 0;}It is not sufficient to change the the target to x64  in the CUDA properties solution properties and Active(x64) in the properties - you must change it in the solution configuration manager. It then execs fine. The error is simply a 'not supported' one.Thanks Mark - suspected it was my bad.You need to change the solution properties to x64 in the Configuration Manager. Changin it in the CUDA props and in the Active platform dropdown don't get it done.Thanks, that worked !any chance you can post a screenshot of this, @mike_agius:disqus? Thanks!I have used this tech way back and am quite pleased with the results .The speedup is so adictive that i swear by my programI encorage all to give cuda a must try to solve problems even if it requires nvidia specific hardware.This is my code developed when i was grad student http://bit.ly/2ko6rNbSure https://uploads.disquscdn.c...Powered by Discourse, best viewed with JavaScript enabled"
36,sparsity-in-int8-training-workflow-and-best-practices-for-nvidia-tensorrt-acceleration,"Originally published at:			https://developer.nvidia.com/blog/sparsity-in-int8-training-workflow-and-best-practices-for-tensorrt-acceleration/
The training stage of deep learning (DL) models consists of learning numerous dense floating-point weight matrices, which results in a massive amount of floating-point computations during inference. Research has shown that many of those computations can be skipped by forcing some weights to be zero, with little impact on the final accuracy. In parallel to…Powered by Discourse, best viewed with JavaScript enabled"
37,upcoming-event-the-next-frontier-of-computer-vision-simulation-and-synthetic-data,"Originally published at:			https://nvda.ws/NextFrontierofComputerVision#new_tab
Learn how simulation and synthetic data are transforming vision AI applications at the NVIDIA Metropolis meetup on February 22 and 23.Powered by Discourse, best viewed with JavaScript enabled"
38,data-storytelling-best-practices-for-data-scientists-and-ai-practitioners,"Originally published at:			https://developer.nvidia.com/blog/data-storytelling-best-practices-for-data-scientists-and-ai-practitioners/
Learn how you can use data storytelling to more effectively communicate with clients, project stakeholders, team members, and other business entities.Hi Guys,Do have a read of this article and let me know if you have any questions regards to this content.Interested in hearing what topics around Data Storytelling you would like me to cover next.Regards,
RichmondPowered by Discourse, best viewed with JavaScript enabled"
39,how-language-neutralization-is-transforming-customer-service-contact-centers,"Originally published at:			https://developer.nvidia.com/blog/how-language-neutralization-is-transforming-customer-service-contact-centers/
The powerful language neutralization offered by Infosys Cortex and based on NVIDIA Riva speech and translation enables contact center agents to communicate effectively with customers.Powered by Discourse, best viewed with JavaScript enabled"
40,seven-things-you-might-not-know-about-numba,"Originally published at:			Seven Things You Might Not Know about Numba | NVIDIA Technical Blog
One of my favorite things is getting to talk to people about GPU computing and Python. The productivity and interactivity of Python combined with the high performance of GPUs is a killer combination for many problems in science and engineering. There are several approaches to accelerating Python with GPUs, but the one I am most…Wow, this is super cool!!! Thank you for sharing!Hi All,Here is what I did with Numba:I just got published an blog article ""High Performance Big Data Analysis Using NumPy, Numba and Python Asynchronous Programming"" in Dataconomy media (http://dataconomy.com/). Here is the link: http://dataconomy.com/2017/...Let me know what you think?ThanksErnest Bonat, Ph.D.Senior Software EngineerSenior Data ScientistDid you run it on a GPU, or just the CPU?Hi Mark,I run it in CPU laptop! (32 RAMs)ThanksErnest Bonat, Ph.D.Senior Software EngineerSenior Data ScientistDid you consider compiling it to run on GPU?Yes, I would like to do that. I wish I can have a GPU laptop for it!ThanksErnest Bonat, Ph.D.Senior Software EngineerSenior Data ScientistHi Stanley,Looks like a typo snuck into your Cuda C++ clamp example, patched thusly:{code} __host__ __device__ float clamp(float x, float xmin, float xmax) {     if (x < xmin){         return xmin;-    } else if (x > xmin) {+    } else if (x > xmax) {         return xmax;     } else {         return x;{code}Thanks! Fixed.Probably a very late response but:
Regarding “Numba + Jupyter”: I find that Jupyter somehow limits the sched_affinity such that Numba is only using 2 cores when 96 are available on my cluster node.  I’d love to know how to get around that.  Just setting “os.environ[‘NUMBA_NUM_THREADS’] = ‘96’” within the notebook doesn’t seem to do it; still only uses 2.Powered by Discourse, best viewed with JavaScript enabled"
41,speech-ai-technology-enables-natural-interactions-with-service-robots,"Originally published at:			https://developer.nvidia.com/blog/speech-ai-technology-enables-natural-interactions-with-service-robots/
From taking your order and serving you food in a restaurant to playing poker with you, service robots are becoming increasingly prevalent. Globally, you can find these service robots at hospitals, airports, and retail stores. According to Gartner, by 2030, 80% of humans will engage with smart robots daily, due to smart robot advancements in…Powered by Discourse, best viewed with JavaScript enabled"
42,upcoming-workshop-building-conversational-ai-applications,"Originally published at:			​ - Building Conversational AI Applications (EMEA)
On May 23 at 9 am CEST learn to build and deploy production-quality conversational AI applications with real-time transcription and natural language processing capabilities.Powered by Discourse, best viewed with JavaScript enabled"
43,getting-started-with-nvidia-omniverse-ace-early-access,"Originally published at:			https://developer.nvidia.com/blog/getting-started-with-nvidia-omniverse-ace-early-access/
NVIDIA just announced at CES 2023 that early access is now available for NVIDIA Omniverse Avatar Cloud Engine (ACE). Developers and teams building avatars and virtual assistants can register to join the program, which includes access to the Omniverse ACE suite of cloud-native AI microservices for faster, easier development of interactive avatars. Early partners include…Powered by Discourse, best viewed with JavaScript enabled"
44,explainer-what-is-robotics-simulation,"Originally published at:			What Is Robotics Simulation? | NVIDIA Blog
Robotics simulation enables virtual training and programming that can use physics-based digital representations of environments, robots, machines, objects, and other assets.Powered by Discourse, best viewed with JavaScript enabled"
45,developers-look-to-openusd-in-era-of-ai-and-industrial-digitalization,"Originally published at:			Developers Look to OpenUSD in Era of AI and Industrial Digitalization | NVIDIA Blog
A new paradigm for data modeling and interchange is unlocking possibilities for 3D workflows and virtual worlds.Powered by Discourse, best viewed with JavaScript enabled"
46,updated-course-integrating-sensors-with-nvidia-drive,"Originally published at:			Courses – NVIDIA
Learn how to integrate your sensor of choice for NVIDIA DRIVE. This updated self-paced course now uses DriveWorks 5.8 and includes lidar examples.Powered by Discourse, best viewed with JavaScript enabled"
47,accelerating-the-suricata-ids-ips-with-nvidia-bluefield-dpus,"Originally published at:			https://developer.nvidia.com/blog/accelerating-the-suricata-ids-ips-with-nvidia-bluefield-dpus/
Deep packet inspection (DPI) is a critical technology for network security that enables the inspection and analysis of data packets as they travel across a network. By examining the content of these packets, DPI can identify potential security threats such as malware, viruses, and malicious traffic, and prevent them from infiltrating the network. However, the…Are you interested in developing solutions for Suricata or have any questions? If so, drop a note below.Powered by Discourse, best viewed with JavaScript enabled"
48,upcoming-event-jetpack-5-0-2-walkthrough-for-jetson-orin-based-modules,"Originally published at:			JetPack 5 Webinar Series
Join us on October 4 to explore new features in JetPack 5.0.2. Learn how to develop for any Jetson Orin module using emulation support on the Jetson AGX Orin Developer Kit.Powered by Discourse, best viewed with JavaScript enabled"
49,new-app-automatically-classifies-your-instagram-photos,"Originally published at:			New App Automatically Classifies Your Instagram Photos | NVIDIA Technical Blog
Have you ever wondered what your photos say about how you look at the world and who you are? Pictograph, developed by a machine intelligence research company, uses deep learning on GPUs in the Amazon cloud to analyze your Instagram photos and creates a visualization of what you like to photograph. The app is ready…It’s a genius idea! I’ve wanted to start a blog about photography and videography for a long time. I’ve created Instagram and TikTok accounts several times, but I haven’t been able to get many followers.Powered by Discourse, best viewed with JavaScript enabled"
50,ai-helps-verify-id-documents,"Originally published at:			AI Helps Verify ID Documents | NVIDIA Technical Blog
Researchers at the University of Michigan developed a deep learning based-system that performs real-time facial recognition and verifies the photo against the corresponding passport and government-issued IDs. The method has the potential to help law enforcement prevent fraud, could serve as an alternative payment method, and could also keep known criminals from entering a sensitive…Powered by Discourse, best viewed with JavaScript enabled"
51,control-of-prompt-based-3d-content-creation,"Hi,Does creating these 3D environment using prompt can adhere to strict constraints like scale of an obj file has to be a specific value. And can we ask the prompt use a specific obj file? How much control do we have on the domain.There are two places in the process where you can add constraints. The first is in your prompt engineering and the second is in your post-processing of the prompt results.When you craft your prompt, you could give GPT a list of acceptable responses and then ask it to only use those responses. You can see an example of this in prompts.py starting on line 32 where it says:""For each object you need to store:You can use this type of pattern to apply other constraints to any of those properties.When you get the results back from GPT you can enforce any other constraints you might want. The limit here is only what you can code in python!Thanks for your response!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
52,fast-large-scale-agent-based-simulations-on-nvidia-gpus-with-flame-gpu,"Originally published at:			https://developer.nvidia.com/blog/fast-large-scale-agent-based-simulations-on-nvidia-gpus-with-flame-gpu/
The COVID-19 pandemic has brought the focus of agent-based modeling and simulation (ABMS) to the public’s attention. It’s a powerful computational technique for the study of behavior, whether epidemiological, biological, social, or otherwise. The process is conceptually simple: Behaviors of individuals are described (the model). Inputs to the model are provided. The simulation of many…Author of this work here. So pleased to see it on the NVIDIA blog. If people have questions about it then feel free to ask. We also have a GitHub discussions board for technical questions about using the software: Discussions · FLAMEGPU/FLAMEGPU2 · GitHubPowered by Discourse, best viewed with JavaScript enabled"
53,explainer-what-is-conversational-ai,"Originally published at:			What Is Conversational AI? | NVIDIA Blog
Real-time natural language understanding will transform how we interact with intelligent machines and applications.Powered by Discourse, best viewed with JavaScript enabled"
54,ai-tackles-offensive-language-on-social-media,"Originally published at:			AI Tackles Offensive Language on Social Media | NVIDIA Technical Blog
IBM researchers are working to fix one of the internet’s most significant problems — offensive, and abusive language. With the help of deep learning, the team trained their neural network to automatically turn offensive comments into non-offensive ones.   “The use of offensive language is a common problem of abusive behavior on online social media…That’s interesting! Have there been any updates on it? I wish AI could do SM promoting, btw…Wow, it’s such an importany step forward. Dealing with offensive and abusive comments can be a real challenge online, so it’s great to see IBM researchers using deep learning to address this issue. Kudos to them!Powered by Discourse, best viewed with JavaScript enabled"
55,saving-apache-spark-big-data-processing-costs-on-google-cloud-dataproc,"Originally published at:			https://developer.nvidia.com/blog/saving-apache-spark-big-data-processing-costs-on-google-cloud-dataproc/
According to IDC, the volume of data generated each year is growing exponentially.  IDC’s Global DataSphere projects that the world will generate 221 ZB of data by 2026. This data holds fantastic information. But as the volume of data grows, so does the processing cost. As a data scientist or engineer, you’ve certainly felt the…Powered by Discourse, best viewed with JavaScript enabled"
56,explainer-what-is-a-transformer-model,"Originally published at:			What Is a Transformer Model? | NVIDIA Blogs
A transformer model is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence.Powered by Discourse, best viewed with JavaScript enabled"
57,release-nvidia-rtx-branch-of-unreal-engine-5-2,"Originally published at:			RTX Branch of Unreal Engine 4 (NvRTX) | NVIDIA Developer
This release offers Unreal Engine, NVIDIA RTX, and neural rendering advancements. Powered by Discourse, best viewed with JavaScript enabled"
58,accelerate-whole-exome-analysis-with-deep-learning-at-70-cost-reduction-using-nvidia-parabricks,"Originally published at:			https://developer.nvidia.com/blog/accelerate-whole-exome-analysis-with-deep-learning-at-70-cost-reduction-using-nvidia-parabricks/
NVIDIA Parabricks is a suite of accelerated genomic analysis applications for high-throughput data that can be used for exome analysis.Powered by Discourse, best viewed with JavaScript enabled"
59,explainer-what-s-the-difference-between-level-2-and-level-5-autonomy,"Originally published at:			What’s the Difference Between Level 2, Level 3, Level 4, and Level 5 Autonomy?
To define the path to fully realized autonomy, SAE International detailed six categories of autonomous capability to establish clear benchmarks.Powered by Discourse, best viewed with JavaScript enabled"
60,physics-informed-machine-learning-platform-nvidia-modulus-is-now-open-source,"Originally published at:			https://developer.nvidia.com/blog/physics-ml-platform-modulus-is-now-open-source/
Physics-informed machine learning (physics-ML) is transforming high-performance computing (HPC) simulation workflows across disciplines, including computational fluid dynamics, structural mechanics, and computational chemistry. Because of its broad applications, physics-ML is well suited for modeling physical systems and deploying digital twins across industries ranging from manufacturing to climate sciences. NVIDIA Modulus is a state-of-the-art physics-ML platform that…Powered by Discourse, best viewed with JavaScript enabled"
61,accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores,"Originally published at:			Accelerating Matrix Multiplication with Block Sparse Format and NVIDIA Tensor Cores | NVIDIA Technical Blog
Sparse-matrix dense-matrix multiplication (SpMM) is a fundamental linear algebra operation and a building block for more complex algorithms such as finding the solutions of linear systems, computing eigenvalues through the preconditioned conjugate gradient, and multiple right-hand sides Krylov subspace iterative solvers. SpMM is also an important kernel used in many domains such as fluid dynamics,…Hi, I am trying to use this new blocked-ELL SpMM, having issues understanding how blocked-ELL is constructed. Does cusparse or any other library provide a dense matrix to blocked-ELL conversion (much like CSR or other sparse-formats in cusparse). It is really intuitive to understand what CSR and COO are and do, but a construction of ELL seems implementation based, and it is not necessarily clear to me how that is done with a single paragraph and a picture.Thanks!Hi, my first suggestion is to take a look at the figure in the documentation https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-spmat-create-blockedell that highlights the memory layout. You can also find an example of usage in the Github samples page https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuSPARSE/spmm_blockedell. Finally, we are going to provide in the upcoming release a conversion routine from dense to BlockedEll by using the routine DenseToSparse https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-dense2sparseThank you! I have seen those resources, I guess what I am struggling with is how the column-idx array for Blocked-ELL is created, and the memory layout or the very small example doesn’t seem super sufficient to understand how I can take an existing dense matrix, write values in this blocked-format to a values array, and somehow generate the column indices for those.The most useful thing I found was the figure in this blog, any more context or open-source code that you have available that I can read/look through?Also looking forward to DenseToSparse support for ELL! Really cool stuff, thank you so much!Just to further add to that, even if you had a larger example hand-coded with a visualization to go along with it (like the one in the blog) that will be great!I also had some other questions:Again, thanks!thanks for the feedback. Batched Ell-SpMM is currently not supported. We will consider this feature in the future. While about bached SpMM for standard format, it is supported but the sample is not available. I will add it in the next few days.Hi @fbusato , I wonder do we support SpMM on transposed blocked-ell format as well?Hi. No, it is not supported. You can set the op(B) operation and change the layout of B and C matrices, but op(A) must be non-transposedI see, but in back-propagation, we need to transpose A, which is not necessarily a blocked-ellpack.transpose A is supported by all other formats for SpMMThe API reference guide for cuSPARSE, the CUDA sparse matrix library.Yeah I understand, but they do not support acceleration w/ TensorCore, is that correct?yes, TensorCore-Accelerated SpMM with op(A) == T is not available. Maybe you can take a look at cuSPARSELt cuSPARSELt: A High-Performance CUDA Library for Sparse Matrix-Matrix Multiplication — NVIDIA cuSPARSELt 0.3.0 documentation. Otherwise, the straightforward solution is to transpose A in a preprocessing stepThanks @fbusato , I think cuSPARSELt is a toolkit designed for leveraging Ampere architecture’s Sparse TensorCore (which requires a 2:4 sparse pattern) to accelerate general GeMM workloads (by pruning A to be 2:4 sparse).Transpose A would break the requirement that each row needs to have an equal number of blocks, but A^T is still a BSR matrix.btw, do you have any plans to accelerate SpmmBsr with TensorCore? I think it’s feasible but I don’t know how good it is compared to block-ellpack (which looks more load-balancing).Yes, we have a plan to accelerate BSR SpMM with Tensor Cores, probably in one of the next releases.awsome, looking forward to seeing that.Hi @fbusato ,I was evaluating the use of “cusparseSpMM”, coupled with the blocked ELLPACK format, to see if it was able to outperform the cuBLAS gemm counterpart.The results I’ve got in terms of performance are not what I expected. Let me present the problem setting first.My goal is to compute AB + C.Now, given A’s structure as well as the figures shown at Accelerating Matrix Multiplication with Block Sparse Format and NVIDIA Tensor Cores | NVIDIA Technical Blog, Figure 3, I was expecting cusparseSpMM + blocked ELL to yield a noticeable speedup w.r.t. cuBLAS gemm on the Tesla V100.Consequently, what I did was to compute the “sparse” multiplication with cusparseSpMM (and A compressed with the blocked ELL format), while the “dense” one was computed with cublasHgemm.Well, in the best case scenario, i.e., cusparseSpMM() + Blocked ELLPACK with block size = 32, I was able to get the same execution time to that I get from cublasHgemm.This is not what I expected, as the problem setting I’ve introduced above should heavily favor cusparseSpMM() + Blocked ELLPACK (and thus observe a noticeable speedup vs cublasHgemm).I have double checked the results generated both from cusparseSpMM and cublasHgemm, and they are the same, so it doesn’t appear I’m doing something wrong…it simply seems that cusparseSpMM() + Blocked ELLPACK is not delivering in terms of promised performance.Am I missing something?Thanks for any reply!Hi, my feeling is that you are doing everything in the right way. There are two points to consider for the performance.There is nothing wrong with these results. We didn’t claim that Blocked-ELL SpMM is faster than cuBLAS in all cases. We will optimize this functionality in the future but I don’t expect to substitute cuBLAS. The user should always consider both alternatives depending on the given problemHi All,
if you want to use Nvidia Tensor cores and CuBLAS implementation for Sparse Matrix times dense Matrix product, you may check our compression algorithm to build a sparse block format.
It is transparent and it basically works for any arbitrary sparse matrix in CSR format.
Paper → [2202.05868] Blocking Techniques for Sparse Matrix Multiplication on Tensor Accelerators
Code → GitHub - LACSFUB/SPARTA: SParse AcceleRation on Tensor Architecture
Please reach us if you will experience any issueHi. I am wondering is there any way to set a Stridedbatch to the Blocked_Ell format? Like ‘cusparseCsrSetStridedBatch’ or ‘cusparseCooSetStridedBatch’ API.
Thank you!Hi, unfortunately, we don’t provide batch computation for BlockedELL format at the moment. Is there any particular application that you want to support?Powered by Discourse, best viewed with JavaScript enabled"
62,is-nvidia-also-thinking-about-actual-3d-images,"Hello,Are generative or predictive models being considered for images that are actually 3D in size? For example, 3D scans of objects or human bodies (e.g., gray-scale 3D bodies of [256, 256, 256, 1])? Such problems usually require fully 3D-aware models.If so, what are the challenges on top of those of 2D images?This will have significant applications in scientific fields such as medicine and geoscience.Many thanks,
RezaHi, GET3D is able to genearte 3D meshes as it output, but generating actual 3D voxels is not considered at the moment. It would also be possible try to use similar idea from GET3D for this task (e.g. rendering the 3D bodies into 2D images and apply discriminator on the 2D images for supervision) we’d love to see how this can work!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
63,top-data-science-sessions-at-nvidia-gtc-2023,"Originally published at:			Data Science Conference Sessions | GTC 2023 Spring | NVIDIA
Learn about the latest AI and data science breakthroughs from leading data science teams at NVIDIA GTC 2023.Powered by Discourse, best viewed with JavaScript enabled"
64,open-source-healthcare-ai-innovation-continues-to-expand-with-monai-v1-0,"Originally published at:			https://developer.nvidia.com/blog/open-source-healthcare-ai-innovation-continues-to-expand-with-monai-1-0/
Learn about MONAI, the domain-specific, open-source medical AI framework that drives research breakthroughs and accelerates AI into clinical impact.Powered by Discourse, best viewed with JavaScript enabled"
65,what-kind-of-output-format-does-get3d-create,"I might have overlooked some details in the ReadMes or the research paper, but what  kind of format does Get3D output? Is it some special interchange format or raw triangle meshes? And can I use the output directly in tools like Blender, 3DSMax or Unreal Engine?Thanks!Get3D outputs triangular meshes and texture maps so it is directly usable in standard rendering engines such as Blender, 3DSMax and Unreal Engine, OmniverseThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
66,deploying-ai-accelerated-medical-devices-with-nvidia-clara-holoscan,"Originally published at:			https://developer.nvidia.com/blog/deploying-ai-accelerated-medical-devices-with-nvidia-clara-holoscan/
NVIDIA Clara Holoscan accelerates deployment of production-quality medical applications by providing a set of OpenEmbedded build recipes and reference configurations.Powered by Discourse, best viewed with JavaScript enabled"
67,cvpr-2020-synthesizing-high-resolution-images-with-gans,"CVPR 2020 dcv20
Presenters: Tech Demo Team, NVIDIA
Abstract
Developed by NVIDIA Researchers, StyleGAN2 yields state-of-the-art results in data-driven unconditional generative image modeling.Watch this session
Join in the conversation below.Supposing this proposed method will eventually work well on photographic inputs, artists going forward will eventually no longer make a profit from generating new subjects from their initial style.Now, I understand the intention here is to speed up time-consuming processes in order to optimize profit margins at the enterprise level. I help automate processes for companies as well. That is the essential value we all provide as employees.I’m curious if the intention of the creators of this project is to annihilate the essence of the phrase “artistic style” and reduce stylistic families to a meditation as representational art has become in the place of photography for all of time?And if this wasn’t the intention because the thought was not raised, what can we all do now for all the artists who will be affected?It’s always a good idea to have constructive debate on how new technologies might be used, for art or any other application. Other research into new capabilities for creative applications has drawn widespread praise (Over Half-Million Images Created with GauGAN AI Art Tool | NVIDIA Blog) from both the research and artistic communities, and artists have commented positively that new tools have allowed them to think differently and create their visions in new ways. We are certainly open to input from our developer community, so if others have questions or concerns you can contact me directly at gestes@nvidia.comThe GAN you are referring to is a concept-generator tool as opposed to the tool this post is presenting which will directly replace existing job functions. Moreover, Creative Directors (who were the ones who have commented positively in your article) are the project manager equivalents to development teams who are not themselves the artists doing the work, and can hire and fire artists to do work as they please.I do not mean at all to oppose but to raise the question that when a tool is released, what kind of instruction manual will be released with it? What resources will be created alongside your development of these tools for those who will eventually be impacted to let them know to pivot their job functions?A lot of people complain about the rich getting richer and the poor getting poorer. If we are to continue to complain, I hope that when we create tools we are not waving it around legitimizing the greatness of it only because of the technical achievements it provides, but we are also thinking about the long term impacts.This concludes all that I would like to express on the topic.Hello. Is it possible to play around with the paintings GAN tool demonstrated in the video ? I’m not referring to GauGAN, but this specific one. Is it very curious, but I can’t find any additional information on it except this video.Hi,Last year I was playing with GauGAN and I had an idea to generate animated input maps to create an AI visualization in animated format. I made a YouTube video detailing my process and in the last week have had almost 1 million views! The output was fascinating but I had to click the load, process and save buttons 700 times to create a 30 second video. I’m excited to do more but could anyone create a method to batch process PNGs into the web client or a desktop client?Check out my video at around the 6 minute mark to really be amazed!Thanks,
Jonjonwarlick@gmail.comAssuming this proposed method becomes successful with photographic inputs, it is possible that artists may no longer generate profits from creating new subjects using their initial style. While I understand the desire to optimize profit margins at the enterprise level and to automate processes, as someone who also automates processes for companies, I am curious if the creators of this project intend to erode the meaning of “artistic style” and reduce stylistic diversity to a mere representation of art, akin to photography throughout history. If this was not the intended outcome, what steps can we take to support the artists who will inevitably be impacted?Powered by Discourse, best viewed with JavaScript enabled"
68,explainer-what-s-a-cloud-native-supercomputer,"Originally published at:			What's a Cloud-Native Supercomputer? | NVIDIA Blogs
The University of Cambridge, in the UK, and an NVIDIA DGX SuperPOD point way to the next generation of secure, efficient HPC clouds.Powered by Discourse, best viewed with JavaScript enabled"
69,rapidly-generate-3d-assets-for-virtual-worlds-with-generative-ai,"Originally published at:			https://developer.nvidia.com/blog/rapidly-generate-3d-assets-for-virtual-worlds-with-generative-ai/
To accelerate the development of 3D worlds and the metaverse, NVIDIA has launched numerous AI research projects to help creators across industries unlock new possibilities with generative AI. Generative AI will touch every aspect of the metaverse and it is already being leveraged for use cases like bringing AI avatars to life with Omniverse ACE.…Powered by Discourse, best viewed with JavaScript enabled"
70,benefits-of-using-pull-requests-for-collaboration-and-code-review,"Originally published at:			https://developer.nvidia.com/blog/benefits-of-using-pull-requests-for-collaboration-and-code-review/
Learn about the benefits of using pull requests for code review and collaboration.Powered by Discourse, best viewed with JavaScript enabled"
71,updating-the-cuda-linux-gpg-repository-key,"Originally published at:			Updating the CUDA Linux GPG Repository Key | NVIDIA Technical Blog
NVIDIA is updating and rotating the signing keys used by apt, dnf/yum, and zypper package managers beginning April 27, 2022.I have followed instructions posted in Updating the CUDA Linux GPG Repository Key | NVIDIA Technical Blog but still running into issue trying to install content form the repo sles15/x86_64 repo.Digest verification failed when trying to install cuda-license-10-2.Sample output:602dd1eef4d7:/ # zypper in -y --repo cuda-sles15-x86_64 cuda-license-10-2
Loading repository data…
Reading installed packages…
Resolving package dependencies…The following NEW package is going to be installed:
cuda-license-10-2The following package has no support information from its vendor:
cuda-license-10-21 new package to install.
Overall download size: 22.0 KiB. Already cached: 0 B. After the operation, additional 59.6 KiB will be used.
Continue? [y/n/v/…? shows all options] (y): y
Retrieving package cuda-license-10-2-10.2.89-1.x86_64                                 (1/1),  22.0 KiB ( 59.6 KiB unpacked)
Retrieving: cuda-license-10-2-10.2.89-1.x86_64.rpm …[done]Warning: Digest verification failed for file ‘cuda-license-10-2-10.2.89-1.x86_64.rpm’
[/var/tmp/AP_0xwFNjG5/cuda-license-10-2-10.2.89-1.x86_64.rpm]expected c1e357bd47b05a401bd3fc9358ab00647937d9276ae14c11d275178003b8f6fe
but got  36b7b27765abac583e4244f6f13938b782516f47536f2f15f3916f024f8056faAccepting packages with wrong checksums can lead to a corrupted system and in extreme cases even to a system compromise.However if you made certain that the file with checksum ‘36b7…’ is secure, correct
and should be used within this operation, enter the first 4 characters of the checksum
to unblock using this file on your own risk. Empty input will discard the file.Unblock or discard? [36b7/…? shows all options] (discard): discard
Package cuda-license-10-2-10.2.89-1.x86_64 (cuda-sles15-x86_64) seems to be corrupted during transfer. Do you want to retry retrieval?
Abort, retry, ignore? [a/r/i] (a): a
Problem occurred during or after installation or removal of packages:
Installation has been aborted as directed.
Please see the above error message for a hint.@roarmstrong @kmittman The instructions from Updating the CUDA Linux GPG Repository Key | NVIDIA Technical Blog are not working for me. PLEASE HELP ASAP!!I wanted to rebuild a Docker image, which is based on nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04 by adding following instructions:However apt-get update fails with:Full log below:Hi @user152836
We are still experiencing 48 stale metadata files on the CDN for some repositories including sles15/x86_64.Hi @someuser
By default the add-apt-repository command appends repos to /etc/apt/sources.list rather than creating a new .list file in /etc/apt/sources.list.d/. Can you try something like this in your Dockerfile?China CDN is not updated correctly:
https://developer.download.nvidia.cn/compute/cuda/repos/ubuntu2004/x86_64/

image1920×1800 237 KB

https://developer.download.nvidia.cn/compute/cuda/repos/debian11/x86_64/

image1920×1757 254 KB
We can see both old pkgs and cannot find the new key.
Downloading seems fine for the key, so could be an mismatch on the html page.IMO big change like this should really be made transactionally.Besides, the announcement suggest to remove old keys, but looks like other parts in the cuda ecosystem, e.g. cudnn/tensorrt/nvidia-docker/…, still rely on the old key, so as those pkgs in the old machine-learning repos.@roarmstrong @kmittman
I am facing same issue as well. I started NVIDIA VM in azure and not able to run apt update because of publickey error. Also tried running sudo apt-key adv --recv-keys --keyserver keys.gnupg.net A4B469963BF863CC to get keys but its giving rror for No data… How do i fix it ?
I am using ubuntu 20.04i tried your method as well:
steps I followed:Even after following all the steps, it still gives error. Please explain how to fix the issue.Hi @Eyshika
Please delete the duplicate cuda .list file or entry in /etc/apt/sources.listSee my comment #11 in https://forums.developer.nvidia.com/t/the-repository-https-developer-download-nvidia-com-compute-cuda-repos-ubuntu1804-x86-64-release-is-not-signed/193764/11Or the more comprehensive guide on GitHub: https://github.com/NVIDIA/cuda-repo-management/issues/4Thanks @kmittman !
Actually I had to do the following instead (i.e. manipulate subfolder /etc/apt/sources.list.d instead)Is there an ETA for when the nvidia base docker images (e.g. nvidia/cuda:10.2-devel-ubuntu18.04) will be updated with the new key? Is there currently any nvidia docker image online that works?I installed cuda-keyring, but I found out that cuda-repository-pin-600
and cuda-ubuntu2004-x86_64.list were not installed. Is it the correct behavior?When doing dnf reposync on CentOS 7, only half of the packages (~2.4k/~4k) are updated.
All the rest ~1.6k are skipped.
We curl the pkg, and sha256 does not match what we had locally.
Wondering if the metadata on nvidia side wasn’t updated correctly to include all new pkgs.update: Seems to be working again as of this morning. Thanks NVidia :-)Hi @eldad.klaiman
The container images update is tracked on GitLab: Cuda Repo Signing Key Change is causing package repo update failures (#158) · Issues · nvidia / container-images / cuda · GitLabHi @daisuke.nishimatsu
Can you provide any more information? I’m not able to reproduceHi @xkszltl
Was this starting from a previous mirror snapshot? For RPM distros, all of the packages have been re-signed as part of the key rotation. I have validated that the packages match their metadataFollowing the instructions, I did the following:wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deband  received the following response:–2022-05-02 12:51:11--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb
Resolving developer.download.nvidia.com (developer.download.nvidia.com)… 152.195.19.142
Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443… connected.
HTTP request sent, awaiting response… 200 OK
Length: 4332 (4.2K) [application/x-deb]
Saving to: ‘cuda-keyring_1.0-1_all.deb’cuda-keyring_1.0-1_ 100%[===================>]   4.23K  --.-KB/s    in 0s2022-05-02 12:51:11 (92.5 MB/s) - ‘cuda-keyring_1.0-1_all.deb’ saved [4332/4332]Then I :rusty@apricot-dev:~$ sudo dpkg -i cuda-keyring_1.0-1_all.deband got:
Selecting previously unselected package cuda-keyring.
(Reading database … 302165 files and directories currently installed.)
Preparing to unpack cuda-keyring_1.0-1_all.deb …
Unpacking cuda-keyring (1.0-1) …
Setting up cuda-keyring (1.0-1) …But when I :sudo apt updateI get errors :
Get:1 file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  InRelease
Ign:1 file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  InRelease
Get:2 file:/var/cuda-repo-ubuntu1804-10-2-local  InRelease
Ign:2 file:/var/cuda-repo-ubuntu1804-10-2-local  InRelease
Get:3 file:/var/visionworks-repo  InRelease
Ign:3 file:/var/visionworks-repo  InRelease
Get:4 file:/var/visionworks-sfm-repo  InRelease
Ign:4 file:/var/visionworks-sfm-repo  InRelease
Get:5 file:/var/visionworks-tracking-repo  InRelease
Ign:5 file:/var/visionworks-tracking-repo  InRelease
Get:6 file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  Release [563 B]
Get:7 file:/var/cuda-repo-ubuntu1804-10-2-local  Release [564 B]
Get:8 file:/var/visionworks-repo  Release [1,999 B]
Get:9 file:/var/visionworks-sfm-repo  Release [2,003 B]
Get:6 file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  Release [563 B]
Get:10 file:/var/visionworks-tracking-repo  Release [2,008 B]
Get:7 file:/var/cuda-repo-ubuntu1804-10-2-local  Release [564 B]
Get:8 file:/var/visionworks-repo  Release [1,999 B]
Get:9 file:/var/visionworks-sfm-repo  Release [2,003 B]
Get:10 file:/var/visionworks-tracking-repo  Release [2,008 B]
Err:11 file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  Release.gpg
The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
Err:12 file:/var/cuda-repo-ubuntu1804-10-2-local  Release.gpg
The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
Get:13 https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64  InRelease [1,484 B]
Get:14 Index of /compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,575 B]
Hit:17 Index of /ubuntu bionic InRelease
Hit:18 Index of linux/ubuntu/ bionic InRelease
Get:20 Index of /ubuntu bionic-updates InRelease [88.7 kB]
Get:21 Index of /compute/cuda/repos/ubuntu1804/x86_64  Packages [709 kB]
Get:22 Index of /ubuntu bionic-security InRelease [88.7 kB]
Get:23 Index of /ubuntu bionic-backports InRelease [74.6 kB]
Get:24 Index of /ubuntu bionic-updates/main i386 Packages [1,464 kB]
Get:25 Index of /ubuntu xenial-security InRelease [99.8 kB]
Get:26 Index of /ubuntu bionic-updates/main amd64 Packages [2,544 kB]
Get:27 Index of /ubuntu bionic-updates/main amd64 DEP-11 Metadata [297 kB]
Get:28 Index of /ubuntu bionic-updates/universe i386 Packages [1,606 kB]
Get:29 Index of /ubuntu bionic-updates/universe amd64 Packages [1,806 kB]
Get:30 Index of /ubuntu bionic-security/main amd64 Packages [2,201 kB]
Get:31 Index of /ubuntu bionic-updates/universe amd64 DEP-11 Metadata [301 kB]
Get:32 Index of /ubuntu bionic-updates/multiverse amd64 DEP-11 Metadata [2,468 B]
Get:33 Index of /ubuntu bionic-backports/universe amd64 DEP-11 Metadata [9,272 B]
Get:34 Index of /ubuntu bionic-security/main i386 Packages [1,162 kB]
Get:35 Index of /ubuntu bionic-security/main amd64 DEP-11 Metadata [55.2 kB]
Get:36 Index of /ubuntu bionic-security/universe amd64 Packages [1,193 kB]
Get:37 Index of /ubuntu bionic-security/universe i386 Packages [1,016 kB]
Get:38 Index of /ubuntu bionic-security/universe amd64 DEP-11 Metadata [59.8 kB]
Get:39 Index of /ubuntu bionic-security/multiverse amd64 DEP-11 Metadata [2,464 B]
Get:40 Index of /ubuntu xenial-security/main amd64 DEP-11 Metadata [93.6 kB]
Fetched 14.9 MB in 3s (5,329 kB/s)
Reading package lists… Done
Building dependency tree
Reading state information… Done
24 packages can be upgraded. Run ‘apt list --upgradable’ to see them.
W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  Release: The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: file:/var/cuda-repo-ubuntu1804-10-2-local  Release: The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1
W: Target Translations (en_US) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1
W: Target Translations (en) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1
W: Failed to fetch file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local/Release.gpg  The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
W: Failed to fetch file:/var/cuda-repo-ubuntu1804-10-2-local/Release.gpg  The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
W: Some index files failed to download. They have been ignored, or old ones used instead.
W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1
W: Target Translations (en_US) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1
W: Target Translations (en) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1As you can imagine,sudo apt upgradedidn’t go well either:sudo apt upgrade
Reading package lists… Done
Building dependency tree
Reading state information… Done
Calculating upgrade… Done
The following package was automatically installed and is no longer required:
linux-hwe-5.4-headers-5.4.0-105
Use ‘sudo apt autoremove’ to remove it.
The following packages have been kept back:
cuda-drivers cuda-drivers-450 libnvidia-cfg1-470 libnvidia-compute-470 libnvidia-decode-470 libnvidia-encode-470 libnvidia-extra-470 libnvidia-fbc1-470 libnvidia-gl-470 libnvidia-ifr1-470
nvidia-compute-utils-470 nvidia-dkms-470 nvidia-driver-470 nvidia-kernel-common-470 nvidia-kernel-source-470 nvidia-utils-470 xserver-xorg-video-nvidia-470
The following packages will be upgraded:
libkeyutils1 libnvidia-common-470 libxnvctrl0 nsight-systems-2021.1.3 nvidia-modprobe nvidia-settings ubuntu-advantage-tools
7 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.
Need to get 249 MB of archives.
After this operation, 52.2 kB of additional disk space will be used.
Do you want to continue? [Y/n] y
Get:1 Index of /compute/cuda/repos/ubuntu1804/x86_64  libnvidia-common-470 470.103.01-0ubuntu1 [10.3 kB]
Get:2 Index of /compute/cuda/repos/ubuntu1804/x86_64  libxnvctrl0 510.47.03-0ubuntu1 [20.5 kB]
Get:3 Index of /compute/cuda/repos/ubuntu1804/x86_64  nsight-systems-2021.1.3 2021.1.3.14-b695ea9 [248 MB]
Get:4 Index of /ubuntu bionic-updates/main amd64 ubuntu-advantage-tools amd64 27.7~18.04.1 [712 kB]
Get:5 Index of /ubuntu bionic-updates/main amd64 libkeyutils1 amd64 1.5.9-9.2ubuntu2.1 [8,764 B]
Get:6 Index of /compute/cuda/repos/ubuntu1804/x86_64  nvidia-settings 510.47.03-0ubuntu1 [894 kB]
Get:7 Index of /compute/cuda/repos/ubuntu1804/x86_64  nvidia-modprobe 510.47.03-0ubuntu1 [19.8 kB]
Fetched 249 MB in 7s (36.2 MB/s)
Preconfiguring packages …
(Reading database … 302170 files and directories currently installed.)
Preparing to unpack …/0-ubuntu-advantage-tools_27.7~18.04.1_amd64.deb …
Unpacking ubuntu-advantage-tools (27.7~18.04.1) over (27.6~18.04.1) …
Preparing to unpack …/1-libkeyutils1_1.5.9-9.2ubuntu2.1_amd64.deb …
Unpacking libkeyutils1:amd64 (1.5.9-9.2ubuntu2.1) over (1.5.9-9.2ubuntu2) …
Preparing to unpack …/2-libnvidia-common-470_470.103.01-0ubuntu1_all.deb …
Unpacking libnvidia-common-470 (470.103.01-0ubuntu1) over (470.103.01-0ubuntu0.18.04.1) …
Preparing to unpack …/3-libxnvctrl0_510.47.03-0ubuntu1_amd64.deb …
Unpacking libxnvctrl0:amd64 (510.47.03-0ubuntu1) over (470.57.01-0ubuntu0.18.04.1) …
Preparing to unpack …/4-nsight-systems-2021.1.3_2021.1.3.14-b695ea9_amd64.deb …
update-alternatives: removing manually selected alternative - switching nsys to auto mode
update-alternatives: removing manually selected alternative - switching nsys-ui to auto mode
update-alternatives: using /opt/nvidia/nsight-systems/2021.2.3/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode
Unpacking nsight-systems-2021.1.3 (2021.1.3.14-b695ea9) over (2021.1.3.12-5261246) …
Preparing to unpack …/5-nvidia-settings_510.47.03-0ubuntu1_amd64.deb …
Unpacking nvidia-settings (510.47.03-0ubuntu1) over (470.57.01-0ubuntu0.18.04.1) …
Preparing to unpack …/6-nvidia-modprobe_510.47.03-0ubuntu1_amd64.deb …
Unpacking nvidia-modprobe (510.47.03-0ubuntu1) over (450.115-0ubuntu1) …
Setting up ubuntu-advantage-tools (27.7~18.04.1) …
Installing new version of config file /etc/logrotate.d/ubuntu-advantage-tools …
Setting up nsight-systems-2021.1.3 (2021.1.3.14-b695ea9) …
update-alternatives: using /opt/nvidia/nsight-systems/2021.1.3/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode
update-alternatives: error: alternative path /opt/nvidia/nsight-systems/2021.1.3/host-linux-x64/nsight-sys doesn’t exist
update-alternatives: error: no alternatives for nsight-sys
update-alternatives: using /opt/nvidia/nsight-systems/2021.1.3/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in manual mode
Setting up libnvidia-common-470 (470.103.01-0ubuntu1) …
Setting up nvidia-modprobe (510.47.03-0ubuntu1) …
Setting up libkeyutils1:amd64 (1.5.9-9.2ubuntu2.1) …
Setting up libxnvctrl0:amd64 (510.47.03-0ubuntu1) …
Setting up nvidia-settings (510.47.03-0ubuntu1) …
Processing triggers for man-db (2.8.3-2ubuntu0.1) …
Processing triggers for gnome-menus (3.13.3-11ubuntu1.1) …
Processing triggers for mime-support (3.60ubuntu1) …
Processing triggers for desktop-file-utils (0.23-1ubuntu3.18.04.2) …
Processing triggers for libc-bin (2.27-3ubuntu1.5) …
rusty@apricot-dev:~$ sudo apt update
Get:1 file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  InRelease
Ign:1 file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  InRelease
Get:2 file:/var/cuda-repo-ubuntu1804-10-2-local  InRelease
Ign:2 file:/var/cuda-repo-ubuntu1804-10-2-local  InRelease
Get:3 file:/var/visionworks-repo  InRelease
Ign:3 file:/var/visionworks-repo  InRelease
Get:4 file:/var/visionworks-sfm-repo  InRelease
Ign:4 file:/var/visionworks-sfm-repo  InRelease
Get:5 file:/var/visionworks-tracking-repo  InRelease
Ign:5 file:/var/visionworks-tracking-repo  InRelease
Get:6 file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  Release [563 B]
Get:7 file:/var/cuda-repo-ubuntu1804-10-2-local  Release [564 B]
Get:8 file:/var/visionworks-repo  Release [1,999 B]
Get:9 file:/var/visionworks-sfm-repo  Release [2,003 B]
Get:6 file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  Release [563 B]
Get:10 file:/var/visionworks-tracking-repo  Release [2,008 B]
Get:7 file:/var/cuda-repo-ubuntu1804-10-2-local  Release [564 B]
Get:8 file:/var/visionworks-repo  Release [1,999 B]
Get:9 file:/var/visionworks-sfm-repo  Release [2,003 B]
Get:10 file:/var/visionworks-tracking-repo  Release [2,008 B]
Get:11 https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64  InRelease [1,484 B]
Hit:12 Index of /compute/cuda/repos/ubuntu1804/x86_64  InRelease
Hit:13 Index of /ubuntu bionic InRelease
Hit:14 Index of linux/ubuntu/ bionic InRelease
Err:15 file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  Release.gpg
The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
Hit:16 Index of /ubuntu bionic-updates InRelease
Hit:17 Index of /ubuntu bionic-backports InRelease
Err:18 file:/var/cuda-repo-ubuntu1804-10-2-local  Release.gpg
The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
Hit:19 Index of /ubuntu bionic-security InRelease
Hit:22 Index of /ubuntu xenial-security InRelease
Fetched 1,484 B in 1s (992 B/s)
Reading package lists… Done
Building dependency tree
Reading state information… Done
17 packages can be upgraded. Run ‘apt list --upgradable’ to see them.
W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local  Release: The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: file:/var/cuda-repo-ubuntu1804-10-2-local  Release: The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1
W: Target Translations (en_US) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1
W: Target Translations (en) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1
W: Failed to fetch file:/var/cuda-repo-cross-aarch64-ubuntu1804-10-2-local/Release.gpg  The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
W: Failed to fetch file:/var/cuda-repo-ubuntu1804-10-2-local/Release.gpg  The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY F60F4B3D7FA2AF80
W: Some index files failed to download. They have been ignored, or old ones used instead.
W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1
W: Target Translations (en_US) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1
W: Target Translations (en) is configured multiple times in /etc/apt/sources.list:54 and /etc/apt/sources.list.d/cuda-ubuntu1804-x86_64.list:1None of these issues appeared prior to the GPG key being changed.
How does this get fixed?ThanksHi @user152836
Can you try accessing the sles15/x86_64 again? It should be working on the CDN since this weekend.Hi @xkszltl
Right, the files exist on the CDN but the index HTML page was not re-generated, I’ll try to get that fixed today.Hi @rusty2
The local repo (cuda-repo-cross-aarch64-ubuntu1804-10-2-local) already installed on your system continues to use the 7fa2af80 key.So I suggest  removing the local cross repo from your system ( sudo apt-get remove --purge ""cuda-repo-cross-aarch64-ubuntu1804-10-2-local*"" ), otherwise you will need to re-enroll the old key.Thanks. Will deleting the local repo require further action to continue programming my TX2?Can nvidia update ALL old docker images with the right key? I am building tensorflow serving and it usesand addedIt doesn’t work though as I am hittingAny ideas?Hi @tianhaitong
TensorRT (libnvinfer) version 8.x and newer is available in the CUDA repository (3bf863cc.pub)
However TensorRT version 7.x is only available in the defunct Machine Learning repository (7fa2af80.pub)Upgrading to a newer version is recommended. However, if TensorRT 7.x is still required then do not include:Updating imagePullPolicy: Always for GPU operator doesn’t seem to have worked for me, I still get the error:When inspecting the manifest, it looks like the clusterpolicy was applied correctly.And when I describe the pod, it seems to have been pulling correctly.Edit for more info from pod description (actual container hash):Fix is to also update the image tag by updating the version defined in clusterpolicy to version: 470-signedPowered by Discourse, best viewed with JavaScript enabled"
72,nvidia-grace-hopper-superchip-architecture-in-depth,"Originally published at:			https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/
The NVIDIA Grace Hopper Superchip Architecture is the first true heterogeneous accelerated platform for high-performance computing (HPC) and AI workloads. It accelerates applications with the strengths of both GPUs and CPUs while providing the simplest and most productive distributed heterogeneous programming model to date. Scientists and engineers can focus on solving the world’s most important…Hello! I would be very interested in trying out the chip for AI inference over encrypted data (see Sentiment Analysis on Encrypted Data with FHE - a Hugging Face Space by zama-fhe for example). Is the chip available via a cloud solution already?Powered by Discourse, best viewed with JavaScript enabled"
73,just-released-nvidia-hpc-sdk-v23-3,"Originally published at:			Release Notes Version 23.3
Version 23.3 expands platform support and provides minor updates to the NVIDIA HPC SDK.Powered by Discourse, best viewed with JavaScript enabled"
74,new-research-highlights-speed-and-cost-savings-of-clara-parabricks-for-genomic-analyses,"Originally published at:			https://developer.nvidia.com/blog/new-research-highlights-speed-and-cost-savings-of-clara-parabricks-for-genomic-analyses/
Learn about two recent research papers highlighting the speed, accuracy, and cost savings of Clara Parabricks for genomic analyses.Powered by Discourse, best viewed with JavaScript enabled"
75,ama-is-live-now-please-bring-your-questions-now,"We are excited to answer any questions about the exciting new features of CUDA 12Do you have any collection of information about CUDA with recipe for various computing solutions?Please post questions  as new topics not as replys.
I will repost this question for you.
Thanks for joining usPowered by Discourse, best viewed with JavaScript enabled"
76,structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines,"Originally published at:			https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/
Deep learning is achieving significant success in various fields and areas, as it has revolutionized the way we analyze, understand, and manipulate data. There are many success stories in computer vision, natural language processing (NLP), medical diagnosis and health care, autonomous vehicles, recommendation systems, and climate and weather modeling. In an era of ever-growing neural…Powered by Discourse, best viewed with JavaScript enabled"
77,cutlass-fast-linear-algebra-in-cuda-c,"Originally published at:			https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/
Update May 21, 2018: CUTLASS 1.0 is now available as Open Source software at the CUTLASS repository. CUTLASS 1.0 has changed substantially from our preview release described in the blog post below. We have decomposed the structure of the GEMM computation into deeper, structured primitives for loading data, computing predicate masks, streaming data at each level…Thanks for the great tutorial! I am trying to understand better what ""fusing element-wise operation"" means. I implement lot's of custom LSTM with pytorch (and fusing is a big problem if I understand stuff correctly). I don't write CUDA codes, so the explanation in the tutorial about the gemm::epilogue_op are hard to follow for me. I am just looking for a theoretical understanding.Please let me know if the following is correct:Suppose I want to compute ReLU(A*B). Without fusing the pointwise operation, this means that I launch a GEMM kernel to compute A*B. Once the kernel is finished it will send the product C=A*B back to global memory. Then I will launch a kernel to compute ReLU(C). To do this I will need to go to fetch the matrix C in global memory, send it to shared memory, and then threshold all the entries of C. Obviously in this last step, all the time is spent in fetching the matrix C from global memory. The goal of fusing is to eliminate this unnecessary fetching time.In the ""fusing scenario"" we launch a single GEMM kernel, with simply an extra line at the end of the kernel code to threshold each entry of C once they become available available.Did I understand correctly what ""fusing element-wise operation"" means?ThanksHi Alain,That's correct.   Although, ReLU(C) wouldn't need to stage through shared memory since each element is only accessed once.   But it saves a load and store of C.NiallThanks for the write up!But I don't quite get the essence of the thread tile. In figure 5, it seems that one thread is responsible for calculating the outer product for 4 locations in the warp accumulator, I don't understand where the 8x8 matrix (on the right of fig 5) comes from?Also, to my understanding, threads cannot share their register space, how is the thread tile achieving O(mn) computations with only O(m+n) loads (as illustrated in the slides)? Are you using the __shfl_sync function to build another caching layer?Thanks!Hello Andrew. This is Isaac from GTC who had a fortune of talking to you about CUTLASS. Your explanation of CUTLASS was extremely helpful. Thanks so much.In the ""Complete GEMM"" block code, this line:accumulator[thread_x][thread_y] += frag_a[y]*frag_b[x];seems to contain a typo. Should y be replaced with thread_y and x with thread_x?Hello Andrew, I'm somewhat confused as to how you're getting simultaneous global load and computation in the same CTA (Software Pipelining) when those sections are separated by a syncthreads in your GEMM pseudo-code.  My understanding was that in the following setup, all threads in a CTA must either be in section A or section B.for(...){   //Section A   shared[i] = global[i];   syncthreads();  //Section B  result = compute(shared[i]);  syncthreads();}Am I missing something fundamental about what syncthreads actually does?  My apologies if this is the wrong place to ask this.  However this is the only place I've seen anyone suggest that loading from global into shared can be pipelined with computation using only a couple syncthreads.ThanksReplying for Andrew:Two buffers in shared memory are allocated. One is actively being written by values fetched from global memory loads (the threadblock tile), while the other SMEM buffer is being loaded from into registers (the warp-scoped tile). At the appropriate point in the mainloop body, all data has been written to one SMEM buffer and all data has been loaded from the other and issued to multiply-add instructions, so the threadblock issues a barrier and then exchanges pointers.Because one buffer is only being written to by the threads of a threadblock, and one thread is only being loaded from, there is no hazard. This permits a single barrier and latency tolerance of global memory.Here’s pseudocode mirroring your example:__shared__ float shared[2][N];  // two buffersint write_buffer = 0;int read_buffer = 1;for (...) {  tmp_registers = global[i];           // global load  result = compute(shared[read_buffer][j]);  // math instructions  shared[write_buffer][i] = tmp_registers;   // shared memory store  syncthreads();  swap(read_buffer, write_buffer);  // exchange pointers}While this doesn't directly answer my question, I like the solution you have presented here.  It addresses the main issue we have with using shared memory.  All of the simple examples that I've seen that use shared memory seem to throw away the nice latency hiding feature of the GPU with naive usage of syncthreads.  Double buffering the shared memory is such a simple and elegant way regaining performance that I'm surprised it isn't presented as a standard way of using shared memory.Thank you very much Jen.Thanks for the feedback! I'll pass this along to Andrew.Sorry for digging this up, but I am really confused by Figure 5.
1600×1031 65 KB
In particular, I don’t understand how we got the “8-by-8 overall thread tile”. There are a total of 4*8=32 threads in the wrap, each computing a 2-by-2 block (there are 4 green cells on the left). How do we get 8-by-8 from that?Additionally, does thread tile mean “a part of the wrap tile as seen from a single thread’s point of view”?Hi! I am deeply impressed by the “different policy for different type of SGEMM”. But I can not find to decide which size is which type, such as “tall”, “large”, and so on. Is it possible to provide me a link for the code to decide the SGEMM type? Thank you!!!I have the exact question for that. How did we get 8x8 from that?Confused too，any idea about that？Powered by Discourse, best viewed with JavaScript enabled"
78,top-speech-ai-developer-day-sessions-at-nvidia-gtc-2023,"Originally published at:			Custom Domain by Bitly
Explore the latest advances in accurate and customizable automatic speech recognition, multi-language translation, and text-to-speech.Powered by Discourse, best viewed with JavaScript enabled"
79,upcoming-event-healthcare-life-sciences-developer-summit-november-10-2022,"Originally published at:			EMEA HCLS Dev Summit
A virtual event designed for healthcare developers and startups, this summit on November 10, 2022 offers a full day of technical talks to reach developers and technical leaders in the EMEA region. Get best practices and insights for applications, from biopharma to medical imaging.Powered by Discourse, best viewed with JavaScript enabled"
80,generative-ai-sparks-life-into-virtual-characters-with-nvidia-ace-for-games,"Originally published at:			https://developer.nvidia.com/blog/generative-ai-sparks-life-into-virtual-characters-with-ace-for-games/
Use NVIDIA ACE for Games to build and deploy customized speech, conversation, and animation AI models in software and games.Powered by Discourse, best viewed with JavaScript enabled"
81,accelerating-inference-with-sparsity-using-the-nvidia-ampere-architecture-and-nvidia-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/
○	TensorRT is an SDK for high-performance deep learning inference, and TensorRT 8.0 introduces support for sparsity that uses sparse tensor cores on NVIDIA Ampere GPUs. It can accelerate networks by reducing the computation of zeros present in GEMM operations in neural networks. You get a performance gain compared to dense networks by just following the steps in this post.Hi ,I test resnext101_32x8d_sparse_onnx_v1/resnext101_32x8d_dense_onnx_v1 model with the trtexec as the blog said.
The sparse result as:
[07/28/2021-08:45:44] [I] Throughput: 501.618 qps
[07/28/2021-08:45:44] [I] Latency: min = 1.81982 ms, max = 3.1843 ms, mean = 2.05464 ms, median = 2.02905 ms, percentile(99%) = 2.29761 msThe dense result as:
[07/28/2021-08:48:35] [I] Throughput: 498.367 qps
[07/28/2021-08:48:35] [I] Latency: min = 1.78882 ms, max = 2.14081 ms, mean = 2.06509 ms, median = 2.06612 ms, percentile(99%) = 2.10205 msI also use nsys to profile the kernel trace, but fail to see any different with the kernel used by sparse with the one used by dense model…I am doing the experiment over 3090 with nvcr.io/nvidia/pytorch:21.07-py3
docker image. Any idea?ThxHello @leiwen, thanks for your comment. There was a mistake in the code snippet. ngc registry model download-version nvidia/resnext101_32x8d_dense_onnx:1 command downloads the dense model and NOT the sparse model. You can change this to ngc registry model download-version nvidia/resnext101_32x8d_sparse_onnx:1 and then run the trtexec command with sparsity enabled for exporting the onnx model to trt engine. Please let me know if that works thanks!Hi @asawarkar ,I redownload the onnx file, and here are the md5sum of the two files:
c962aeafd8a7000f3c72bbfcd2165572  resnext101_32x8d_sparse_onnx_v1/resnext101_32x8d_pyt_torchvision_sparse.onnx
49beb2920f6f6e42eb20b874a30eab98  resnext101_32x8d_dense_onnx_v1/resnext101_32x8d_pyt_torchvision_dense.onnxBut still I cannot see any different for the performance improve for the sparse onnx model.When trt build the sparse one, it print below message:
[08/07/2021-09:14:02] [I] [TRT] (Sparsity) Layers eligible for sparse math: Conv_3 + Relu_4, Conv_7, Conv_8 + Add_9 + Relu_10, Conv_11 + Relu_12, Conv_15 + Add_16 + Relu_17, Conv_18 + Relu_19, Conv_22 + Add_23 + Relu_24, Conv_25 + Relu_26, Conv_29, Conv_30 + Add_31 + Relu_32, Conv_33 + Relu_34, Conv_37 + Add_38 + Relu_39, Conv_40 + Relu_41, Conv_44 + Add_45 + Relu_46, Conv_47 + Relu_48, Conv_51 + Add_52 + Relu_53, Conv_54 + Relu_55, Conv_58, Conv_59 + Add_60 + Relu_61, Conv_62 + Relu_63, Conv_66 + Add_67 + Relu_68, Conv_69 + Relu_70, Conv_73 + Add_74 + Relu_75, Conv_76 + Relu_77, Conv_80 + Add_81 + Relu_82, Conv_83 + Relu_84, Conv_87 + Add_88 + Relu_89, Conv_90 + Relu_91, Conv_94 + Add_95 + Relu_96, Conv_97 + Relu_98, Conv_101 + Add_102 + Relu_103, Conv_104 + Relu_105, Conv_108 + Add_109 + Relu_110, Conv_111 + Relu_112, Conv_115 + Add_116 + Relu_117, Conv_118 + Relu_119, Conv_122 + Add_123 + Relu_124, Conv_125 + Relu_126, Conv_129 + Add_130 + Relu_131, Conv_132 + Relu_133, Conv_136 + Add_137 + Relu_138, Conv_139 + Relu_140, Conv_143 + Add_144 + Relu_145, Conv_146 + Relu_147, Conv_150 + Add_151 + Relu_152, Conv_153 + Relu_154, Conv_157 + Add_158 + Relu_159, Conv_160 + Relu_161, Conv_164 + Add_165 + Relu_166, Conv_167 + Relu_168, Conv_171 + Add_172 + Relu_173, Conv_174 + Relu_175, Conv_178 + Add_179 + Relu_180, Conv_181 + Relu_182, Conv_185 + Add_186 + Relu_187, Conv_188 + Relu_189, Conv_192 + Add_193 + Relu_194, Conv_195 + Relu_196, Conv_199 + Add_200 + Relu_201, Conv_202 + Relu_203, Conv_206 + Add_207 + Relu_208, Conv_209 + Relu_210, Conv_213 + Add_214 + Relu_215, Conv_216 + Relu_217, Conv_220, Conv_221 + Add_222 + Relu_223, Conv_224 + Relu_225, Conv_228 + Add_229 + Relu_230, Conv_231 + Relu_232, Conv_235 + Add_236 + Relu_237
[08/07/2021-09:14:02] [I] [TRT] (Sparsity) TRT inference plan picked sparse implementation for layers: Conv_3 + Relu_4, Conv_7, Conv_8 + Add_9 + Relu_10, Conv_11 + Relu_12, Conv_15 + Add_16 + Relu_17, Conv_18 + Relu_19, Conv_22 + Add_23 + Relu_24, Conv_25 + Relu_26, Conv_29, Conv_30 + Add_31 + Relu_32, Conv_33 + Relu_34, Conv_37 + Add_38 + Relu_39, Conv_40 + Relu_41, Conv_44 + Add_45 + Relu_46, Conv_47 + Relu_48, Conv_51 + Add_52 + Relu_53, Conv_54 + Relu_55, Conv_58, Conv_59 + Add_60 + Relu_61, Conv_62 + Relu_63, Conv_66 + Add_67 + Relu_68, Conv_69 + Relu_70, Conv_73 + Add_74 + Relu_75, Conv_76 + Relu_77, Conv_80 + Add_81 + Relu_82, Conv_83 + Relu_84, Conv_87 + Add_88 + Relu_89, Conv_90 + Relu_91, Conv_94 + Add_95 + Relu_96, Conv_97 + Relu_98, Conv_101 + Add_102 + Relu_103, Conv_104 + Relu_105, Conv_108 + Add_109 + Relu_110, Conv_111 + Relu_112, Conv_115 + Add_116 + Relu_117, Conv_118 + Relu_119, Conv_122 + Add_123 + Relu_124, Conv_125 + Relu_126, Conv_129 + Add_130 + Relu_131, Conv_132 + Relu_133, Conv_136 + Add_137 + Relu_138, Conv_139 + Relu_140, Conv_143 + Add_144 + Relu_145, Conv_146 + Relu_147, Conv_150 + Add_151 + Relu_152, Conv_153 + Relu_154, Conv_157 + Add_158 + Relu_159, Conv_160 + Relu_161, Conv_164 + Add_165 + Relu_166, Conv_167 + Relu_168, Conv_171 + Add_172 + Relu_173, Conv_174 + Relu_175, Conv_178 + Add_179 + Relu_180, Conv_181 + Relu_182, Conv_185 + Add_186 + Relu_187, Conv_188 + Relu_189, Conv_192 + Add_193 + Relu_194, Conv_195 + Relu_196, Conv_199 + Add_200 + Relu_201, Conv_202 + Relu_203, Conv_206 + Add_207 + Relu_208, Conv_209 + Relu_210, Conv_213 + Add_214 + Relu_215, Conv_216 + Relu_217, Conv_220, Conv_224 + Relu_225, Conv_231 + Relu_232I assume after enabling structual sparsity, it would at last gain twice speed up with the non sparse kernel?
But from the nsys profile, no big improment is seen.
Could you help list the performance you assume that  resnext101_32x8d_pyt_torchvision_sparse.onnx could reach over 3090 platform with sparsity turn on or off? And would twice speed up assumption could be hold for this case?Thx,
LeiHi @leiwenThe assumption of double the performance gain due to structured sparsity is incorrect. We don’t have numbers for 3090 but on A100, the performance gain for ResNeXt101 32x8d should be in the range of 1% to 8% end to end in INT8. If FP16 is used, then sparse vs dense perf gap is larger.I think to compare the performance shall take single kernel as example. In previous experience, when switch from fp16 to int8, the same shape convolution would be accelerated upto twice of the origin speed.As the article also mention that, in ampere, dense int8 has 624Tops, while sparse has 1248 Tops, I think if the kernel is implemented corrected, its performance also shall be twice speed up?Thx@asawarkar I am planning to test sparsity gains for bert models and want to know that bert-large onnx also available in the ngc registry like below resnet model ?
ngc registry model download-version nvidia/resnext101_32x8d_dense_onnx:1Hi,As described in the slides, I used the following script to covert pretrained resnet50 model to prune the weights but it’s been more than 24 hours and still pruning process hasn’t completed. Could someone help on this ?Script used:
import torch
import torchvision
import torch.optim as optim
from torchvision import datasets, transforms, models
try:
from apex.contrib.sparsity import ASP
except ImportError:
raise RuntimeError(“Failed to import ASP. Please install Apex from https:// GitHub - NVIDIA/apex: A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch .”)
device = torch.device(‘cuda’)
print(‘Is cuda available: ’ + str(torch.cuda.is_available()))
model = models.resnet50(pretrained=True) # Define model structure
model.load_state_dict(torch.load(’/home/ubuntu/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth’))
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # Define optimizer
ASP.prune_trained_model(model, optimizer)
torch.save(model.state_dict(), ‘pruned_resnetmodel.pth’) # checkpoint has weights and masksI have also attached the log file.
resnet50_asp_pruning_log.txt (386.3 KB)Hi ,  I have run the resnext101_32x8d dense and sparse models for inference for  fp16 as described in the following paper and I don’t see any improvement in the inference for sparse model. Could some one look in to it ?Dense model:
trt generation: tuser@fde6b05b597a:/workspace/TensorRT/build/out/trtexec --onnx=resnext101_32x8d_pyt_torchvision_dense.onnx --saveEngine=resnext101_dense_engine_pytorch.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw  --fp16
Batch size: 32000
Precision: fp16
Processing time for 1 loop:4.3sSparse model:
trt generation: trtuser@fde6b05b597a:/workspace/TensorRT/build/out/trtexec --onnx=resnext101_32x8d_pyt_torchvision_sparse.onnx --saveEngine=resnext101_sparse_engine_pytorch.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16 --sparsity=enable
Batch size: 32000
Precision: fp16
Processing time for 1 loop:4.2sHi, I’m using TensorRT to test with the ResNeXt101. But the problem is as follows:[07/19/2022-09:37:20] [I] [TRT] (Sparsity) Layers eligible for sparse math: Conv_3 + Relu_4, Conv_7, Conv_8 + Add_9 + Relu_10, Conv_11 + Relu_12, Conv_15 + Add_16 + Relu_17, Conv_18 + Relu_19, Conv_22 + Add_23 + Relu_24, Conv_25 + Relu_26,
Conv_29, Conv_30 + Add_31 + Relu_32, Conv_33 + Relu_34, Conv_37 + Add_38 + Relu_39, Conv_40 + Relu_41, Conv_44 + Add_45 + Relu_46, Conv_47 + Relu_48, Conv_51 + Add_52 + Relu_53, Conv_54 + Relu_55, Conv_58, Conv_59 + Add_60 + Relu_61, Conv
_62 + Relu_63, Conv_66 + Add_67 + Relu_68, Conv_69 + Relu_70, Conv_73 + Add_74 + Relu_75, Conv_76 + Relu_77, Conv_80 + Add_81 + Relu_82, Conv_83 + Relu_84, Conv_87 + Add_88 + Relu_89, Conv_90 + Relu_91, Conv_94 + Add_95 + Relu_96, Conv_97 + Relu_98, Conv_101 + Add_102 + Relu_103, Conv_104 + Relu_105, Conv_108 + Add_109 + Relu_110, Conv_111 + Relu_112, Conv_115 + Add_116 + Relu_117, Conv_118 + Relu_119, Conv_122 + Add_123 + Relu_124, Conv_125 + Relu_126, Conv_129 + Add_130 +
Relu_131, Conv_132 + Relu_133, Conv_136 + Add_137 + Relu_138, Conv_139 + Relu_140, Conv_143 + Add_144 + Relu_145, Conv_146 + Relu_147, Conv_150 + Add_151 + Relu_152, Conv_153 + Relu_154, Conv_157 + Add_158 + Relu_159, Conv_160 + Relu_161,
Conv_164 + Add_165 + Relu_166, Conv_167 + Relu_168, Conv_171 + Add_172 + Relu_173, Conv_174 + Relu_175, Conv_178 + Add_179 + Relu_180, Conv_181 + Relu_182, Conv_185 + Add_186 + Relu_187, Conv_188 + Relu_189, Conv_192 + Add_193 + Relu_194,
Conv_195 + Relu_196, Conv_199 + Add_200 + Relu_201, Conv_202 + Relu_203, Conv_206 + Add_207 + Relu_208, Conv_209 + Relu_210, Conv_213 + Add_214 + Relu_215, Conv_216 + Relu_217, Conv_220, Conv_221 + Add_222 + Relu_223, Conv_224 + Relu_225,
Conv_228 + Add_229 + Relu_230, Conv_231 + Relu_232, Conv_235 + Add_236 + Relu_237, MatMul_240[07/19/2022-09:37:20] [I] [TRT] (Sparsity) TRT inference plan picked sparse implementation for layers:
[07/19/2022-09:37:20] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 3117, GPU 1787 (MiB)
[07/19/2022-09:37:20] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 3117, GPU 1797 (MiB)
[07/19/2022-09:37:20] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +8, GPU +339, now: CPU 8, GPU 339 (MiB)
[07/19/2022-09:37:20] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will a
lways return 1.
[07/19/2022-09:37:20] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will a
lways return 1.There are all the layers that are eligible for sparse math but none in TRT inference plan picked sparse implementation for layers. Could you help me check why this could happen?I’ve solved this problem by adding enable FP16 flag.
However, I’ve noticed that the sparsity module only support convolution operation but barely linear layers(fully connected), is that the case or we need other flag to enable linear layer acceleration?I tested on A100, latest TensorRT and I got:
sparse model qps 885
dense mode qps 870It seems that the advantage of using sparsity is very little.Hi,
I tested the inference speed but I did not find how to test GPU power consumption.(RTX-A6000)
Could you tell me how to check the power consumption?ThxI am trying to prune the model to 2:4 sparsity and convert it to tensorRT and run on orin nano Ampere architecture. But Iam stuck at the ONNX conversion step.
I was able to prune the model using the ‘ASP.prune_trained_model(model, optimizer)’ command. But the model I got as the output was a mix of pruned weights and masks. Then when I tried to convert it to onnx its not working as the model. eval() is not working. can you please provide the instructions to convert it to onnx?These are the things I tried.
#. Prune the model
print(“Pruning the model”)
ASP.prune_trained_model(model, optimizer)
for epoch in range(1, args.epochs+1):
print(“\n---- Training Model ----”)torch.save(model.state_dict(), “yolov3_pruned.pth”)#ONNX conversionimport torchpruned_model = torch.load(‘yolov3_pruned.pth’)
dummy_input=torch.randn(1, 3, 224, 224)
torch.onnx.export(pruned_model , dummy_input, “yolov3.onnx”, verbose=False)But when converting to ONNX this is the error Iam getting this error.
File “/home/ashish/code/pruning/PyTorch-YOLOv3/test.py”, line 5, in 
torch.onnx.export(pruned_model , dummy_input, “yolov3.onnx”, verbose=False)
File “/home/ashish/venvs/py310/lib/python3.10/site-packages/torch/onnx/utils.py”, line 506, in export
_export(
File “/home/ashish/venvs/py310/lib/python3.10/site-packages/torch/onnx/utils.py”, line 1525, in _export
with exporter_context(model, training, verbose):
File “/usr/lib/python3.10/contextlib.py”, line 135, in enter
return next(self.gen)
File “/home/ashish/venvs/py310/lib/python3.10/site-packages/torch/onnx/utils.py”, line 178, in exporter_context
with select_model_mode_for_export(
File “/usr/lib/python3.10/contextlib.py”, line 135, in enter
return next(self.gen)
File “/home/ashish/venvs/py310/lib/python3.10/site-packages/torch/onnx/utils.py”, line 139, in disable_apex_o2_state_dict_hook
for module in model.modules():
AttributeError: ‘collections.OrderedDict’ object has no attribute ‘modules’Powered by Discourse, best viewed with JavaScript enabled"
82,just-released-nvidia-nsight-compute-2023-1,"Originally published at:			https://developer.nvidia.com/blog/just-released-nvidia-nsight-compute-2023-1/
NVIDIA Nsight Compute 2023.1 adds more metrics and usability to the source view, a sample for shared memory banks, and improves the application replay collection mode.Powered by Discourse, best viewed with JavaScript enabled"
83,demystifying-enterprise-mlops,"Originally published at:			https://developer.nvidia.com/blog/demystifying-enterprise-mlops/
Learn about enterprise MLOps with this introductory overview.Great overview from William Benton on MLOps! I think this is recommended reading for anyone just getting started/curious about what’s required for production AI at scale. Can’t wait for the GTC session!Powered by Discourse, best viewed with JavaScript enabled"
84,upcoming-event-recommender-systems-sessions-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Learn about transformer-powered personalized online advertising, cross-framework model evaluation, the NVIDIA Merlin ecosystem, and more with these featured GTC 2022 sessions.Powered by Discourse, best viewed with JavaScript enabled"
85,gtc-2020-intelligent-end-to-end-ai-chatbot-with-audio-driven-facial-animation,"GTC 2020 D2S39
Presenters: Tech Demo Team,NVIDIA
Abstract
During the GTC 2020 virtual keynote, NVIDIA CEO and founder Jensen Huang interacted with Misty, a conversational AI weather chatbot, to demonstrate an end-to-end pipeline to create an AI driven 3D digital avatar. The NVIDIA Jarvis multimodal application framework includes pre-trained conversational AI models, and optimized end-to-end services for speech, vision, and natural language understanding (NLU) tasks. It also includes Audio2Face, an audio driven AI-based technology, to automatically create the real-time facial animation from the synthesized speech of Jarvis. All of this is rendered in real-time with NVIDIA Omniverse, a powerful, multi-GPU, real-time simulation and collaboration platform for 3D production pipelines.

Learn more about NVIDIA Jarvis
Learn more about NVIDIA Omniverse

Watch the entire GTC 2020 keynoteWatch this session
Join in the conversation below.HeyPowered by Discourse, best viewed with JavaScript enabled"
86,upcoming-event-why-gpus-are-important-to-ai,"Originally published at:			NVIDIA Emerging Chapters Education Series
Join us on October 20 to learn how NVIDIA GPUs can dramatically accelerate your machine learning workloads.Powered by Discourse, best viewed with JavaScript enabled"
87,top-generative-ai-sessions-at-nvidia-gtc-2023,"Originally published at:			https://register.nvidia.com/events/widget/nvidia/gtcspring2023/1674855518111001o2Ur/?nvid=nv-int-bnr-463583
See how recent breakthroughs in generative AI are transforming media, content creation, personalized experiences, and more. Powered by Discourse, best viewed with JavaScript enabled"
88,seamlessly-develop-vision-ai-applications-with-nvidia-deepstream-sdk-6-2,"Originally published at:			https://developer.nvidia.com/blog/seamlessly-develop-vision-ai-applications-with-nvidia-deepstream-sdk-6-2/
NVIDIA announced the general availability of the NVIDIA DeepStream SDK 6.2, an AI analytics toolkit for building high-performance video analytics and streaming applications. The update adds new features including enhanced multi-object trackers, support for new sensors, integration with REST APIs, updated Graph Composer, and enterprise-grade support through NVIDIA AI Enterprise.  Download the DeepStream SDK 6.2…Powered by Discourse, best viewed with JavaScript enabled"
89,is-there-a-way-to-control-the-number-of-triangles-in-the-output-mesh,"Content creators are very often on a tight budget regarding triangle count. So is it possible to control that up-front with Get3D to avoid too much post-processing work on the generated meshes?The number of triangles in the resulting mesh can be controlled by changing the grid resolution of DMTet. However, this may come at the cost of compromising quality. We’re actively exploring ways to reduce the triangle count while maintaining high-quality meshes.Thanks!Looking forward to further developments!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
90,ask-me-anything-unleashing-the-power-of-nvidia-rtx-path-tracing,"Originally published at:			Unleashing the Power of RTX Path Tracing : AMA June 6, 2023 - Path Tracing : AMA June 6, 2023 - NVIDIA Developer Forums
Join us on June 6 and learn how to mimic real-world lighting for lifelike 3D graphics with the NVIDIA RTX Path Tracing SDK.Powered by Discourse, best viewed with JavaScript enabled"
91,the-technology-behind-the-viral-prisma-photo-app,"Originally published at:			The Technology Behind the Viral Prisma Photo App | NVIDIA Technical Blog
The new Prisma mobile app that transforms your photos into a work of art has been downloaded nearly 17 million times since it was released last month on iOS devices and is now surpassing 2 million a day after it was made available for Android users this week. People of all ages and celebrities around…Powered by Discourse, best viewed with JavaScript enabled"
92,new-cublas-12-0-features-and-matrix-multiplication-performance-on-nvidia-hopper-gpus,"Originally published at:			https://developer.nvidia.com/blog/new-cublas-12-0-features-and-matrix-multiplication-performance-on-nvidia-hopper-gpus/
Explore the NVIDIA cuBLAS library in CUDA 12.0, including the recently-introduced FP8 format, GEMM performance on NVIDIA Hopper GPUs, and user experience improvements.Powered by Discourse, best viewed with JavaScript enabled"
93,top-professional-visualization-sessions-at-nvidia-gtc-2023,"Originally published at:			Developer Conference Session Catalog | NVIDIA GTC
Explore professional visualization developer tools including NVIDIA NeuralVDB, NVIDIA OptiX, and NVIDIA Video Codec.Powered by Discourse, best viewed with JavaScript enabled"
94,top-robotics-sessions-at-nvidia-gtc-2023,"Originally published at:			https://nvda.ws/3IISW7q
Get to know the NVIDIA technologies and software development tools powering the latest in robotics and edge AI.Powered by Discourse, best viewed with JavaScript enabled"
95,massively-improved-multi-node-nvidia-gpu-scalability-with-gromacs,"Originally published at:			https://developer.nvidia.com/blog/massively-improved-multi-node-nvidia-gpu-scalability-with-gromacs/
GROMACS, a scientific software package widely used for simulating biomolecular systems, plays a crucial role in comprehending important biological processes important for disease prevention and treatment. GROMACS can use multiple GPUs in parallel to run each simulation as quickly as possible. Over the past several years, NVIDIA and the core GROMACS developers have collaborated on…Powered by Discourse, best viewed with JavaScript enabled"
96,optimizing-fraud-detection-in-financial-services-with-graph-neural-networks-and-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/optimizing-fraud-detection-in-financial-services-with-graph-neural-networks-and-nvidia-gpus/
Learn an end-to-end workflow showcasing best practices for detecting financial services fraud using GNNs and GPUs.What batch sizes were used while scaling from 1 to 8 GPUs on the MAG240M dataset?We used batch size of 8192 and this batch size gave us the best classification accuracy. We see similar speedups with lower batch sizes as well.Hi!
Can you share the full end-to-end code for fraud detection (including R-GCN building, training and downstream XGBoost applying)?Hi I know I am a bit late to the topic, but I have some questions I was wondering if you could answer.So far, I have conducted preprocessing, and the dataset now contains 20 numerical features. As the article suggests, I have saved the bulk of the data on the edges between nodes, leaving the nodes featureless, besides their distinct IDs. Moreover, from my perspective, it would seem that these transactions only have one relationship, which is “Credit card purchases from Merchant”. Now, I have some questions regarding the suggestions of the articleI am having a pretty hard time understanding this article, and the methods for that matter, and I would really appreciate some clarification.Hi there nthqhai2002!Thanks for reading this blog and for your awesome questions!it would seem that these transactions only have one relationship, which is “Credit card purchases from Merchant”In order to make the edge undirected and to allow message propogation between both classes of the graph, we also add a second reverse edge type, in your cases “Merchant has purchase from Credit Card”As such, wouldn’t it be ineffective to conduct node embeddings?The IDs themselves in a transductive setting are valuable as well, as a learned user embedding encodes the generalized structural behavioral embedding of the user. You can also aggregate adjacent edge features per node to user as a node featureYou are correct that an architecture that propogates edge information would likely be useful here. The purpose of the blog post was mainly to show baseline usefulness, but if you’re interested in edge-inclusive papers, you can refer to: Exploiting Edge Features for Graph Neural Networks | IEEE Conference Publication | IEEE XploreI have also seen the article suggest using Link Prediction as part of the approach, but I do not understand how it helps with detecting fraudulent transactions.Link Prediction in this case is used to generate robust representations of nodes, which can be used downstream in the direct prediction of the fraud label. Often in the fraud detection domain, labels are noisy and generally weak. Training representations on non-noisy labels (transaction presence) often has more consistent convergence properties.Hi kkranen!
Thanks a lot for the response, your answers have helped me a lot !
I have another question regarding this workflow, if you do not mind. Suppose I have trained the model on data from 2015 until 2021, and new influx of data from 2022 comes in. At this stage, I would follow the workflow in order to generate robust node embeddings, then attach them to the 2022 tabular data according to the unique node IDs, then conduct predictions. My question is should the node embeddings of 2022 be generated by submitting the entire dataset from 2015 to 2022 to the workflow, or do I only need to exclusively use data of 2022?Hello,
Could you find the reproducible source code?Powered by Discourse, best viewed with JavaScript enabled"
97,new-asynchronous-programming-model-library-now-available-with-nvidia-hpc-sdk-v22-11,"Originally published at:			https://developer.nvidia.com/blog/new-asynchronous-programming-model-library-now-available-with-nvidia-hpc-sdk-v22-11/
Find out what’s in the NVIDIA HPC SDK v22.11 release, including a preview of an innovative library for standardizing and asynchronously scheduling C++ work.Powered by Discourse, best viewed with JavaScript enabled"
98,a-comprehensive-overview-of-regression-evaluation-metrics,"Originally published at:			https://developer.nvidia.com/blog/a-comprehensive-overview-of-regression-evaluation-metrics/
As a data scientist, evaluating machine learning model performance is a crucial aspect of your work. To do so effectively, you have a wide range of statistical metrics at your disposal, each with its own unique strengths and weaknesses. By developing a solid understanding of these metrics, you are not only better equipped to choose…I’m curious to know if you’re using any other regression evaluation metrics. If you are, which ones do you use?Powered by Discourse, best viewed with JavaScript enabled"
99,upcoming-event-improve-your-cybersecurity-posture-with-ai,"Originally published at:			https://info.nvidia.com/improve-your-cybersecurity-posture-with-ai.html?nvid=nv-int-tblg-746753-vt22#cid=dl24_nv-int-tblg_en-us
Find out how federal agencies are adopting AI to improve cybersecurity in this November 16 webinar featuring Booz Allen Hamilton.Powered by Discourse, best viewed with JavaScript enabled"
100,smart-spaces-end-to-end-ai-application-boosts-factory-efficiency,"Originally published at:			https://developer.nvidia.com/blog/smart-spaces-end-to-end-ai-application-boosts-factory-efficiency/
Smart spaces are delivering unprecedented value, creating a continuous flow of information between physical and digital worlds.  By incorporating technologies such as the Internet of Things (IoT), cloud computing, machine learning, and AI at the edge, world-class businesses can capture digital data and turn them into actionable insights.  However, the process is complicated with edge…Powered by Discourse, best viewed with JavaScript enabled"
101,improve-shader-performance-and-in-game-frame-rates-with-shader-execution-reordering,"Originally published at:			https://developer.nvidia.com/blog/improve-shader-performance-and-in-game-frame-rates-with-shader-execution-reordering/
Learn about Shader Execution Reordering (SER), a performance optimization that unlocks the potential for better ray and memory coherency in ray tracing shaders.Powered by Discourse, best viewed with JavaScript enabled"
102,top-video-streaming-and-conferencing-sessions-at-nvidia-gtc-2023,"Originally published at:			Applied AI Session Catalog | NVIDIA GTC
Learn about advancements in video conferencing that have transformed how we communicate.Powered by Discourse, best viewed with JavaScript enabled"
103,evolving-record-fast-optoelectronic-chips-for-data-center-networks,"Originally published at:			https://developer.nvidia.com/blog/evolving-record-fast-optoelectronic-chips-for-data-center-networks/
The innovative technologies developed in the plaCMOS project provide the foundation for the evolution of optical interconnects in data center networks.Powered by Discourse, best viewed with JavaScript enabled"
104,explainer-what-is-extended-reality,"Originally published at:			https://developer.nvidia.com/blog/explainer-what-is-extended-reality/
Extended reality, or XR, is a collective term that refers to immersive technologies, including virtual reality, augmented reality, and mixed reality.Powered by Discourse, best viewed with JavaScript enabled"
105,upcoming-webinar-build-modern-recommender-systems-using-nvidia-merlin,"Originally published at:			https://info.nvidia.com/Recommender-Systems-Using-NVIDIA-Merlin-webinar.html
Join this webinar on January 17, or catch it on-demand, for a technical overview and a demo of NVIDIA Merlin—an end-to-end framework for building a modern recommenders pipeline.Powered by Discourse, best viewed with JavaScript enabled"
106,qhack-results-highlight-quantum-computing-applications-and-tools-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/qhack-results-highlight-quantum-computing-applications-and-tools-on-gpus/
Participants in QHack 2023, the world’s largest quantum machine learning hackathon, used the NVIDIA Quantum Platform to create useful tools and innovations, and novel algorithm development.Powered by Discourse, best viewed with JavaScript enabled"
107,maximizing-gromacs-throughput-with-multiple-simulations-per-gpu-using-mps-and-mig,"Originally published at:			https://developer.nvidia.com/blog/maximizing-gromacs-throughput-with-multiple-simulations-per-gpu-using-mps-and-mig/
In this post, we demonstrate the benefits of running multiple simulations per GPU for GROMACS and show how MPS can achieve up to 1.8X overall improvement in throughput.Hello Alan and Szilárd,Thanks for the very useful post. I have tried implementing the MPS on V100s and have seen a massive improvement in the performance.I am facing an issue with using MPS on nodes with multiple GPUs (two GPUs). I would like your help with the same:I am using a job scheduler (qsub) to submit a gromacs simulation run. Each job requests 1xV100+4xCPUs. I run one independent simulation on each CPU core. Therefore I have 4 parallel ongoing runs. The sample command is as follows:The above command runs perfect and I am able to see nvidia-cuda-mps-server and four gmx_mpi processes on GPU 0.However, the issue arises when the job scheduler assigns another job on the same node. Note that nodes have 2 GPUs, therefore it assigns the jobs to the other GPU (GPU 1). (The same resources are requested and the same script as above was run).The messages/errors on using gmx mdrun are as follows:I also notice this in the log file:The CUDA runtime shows N/A. While for the first job, it shows 11.0.Please note the following:I would really appreciate it if you can help resolve my query. Let me know if you need additional details.Akshay,
PhD student,Thanks,I am new to parallelization and cannot understand running the same job multiple times on the same GPU. I want to run multiple jobs on the same GPU and thus cannot comprehend $INPUT file manipulation.
I hope you can help me.Hi Akshay, can you please provide me with your simulation.sh scriptHi Akshay,In your simulation.sh script, are you setting the CUDA_VISIBLE_DEVICES environment variable? If so, please can you try removing that. When you launch each job with 1xV100, I expect that each device will be available as (the default) GPU 0 in each of the jobs (you can check this with nvidia-smi), such that setting CUDA_VISIBLE_DEVICES to any other value would case the error your see. If you still get the error, then I’m not sure of the cause at the moment but can try and reproduce internally.One other thing to try is launching jobs with multiple GPUs in each job, and using CUDA_VISIBLE_DEVICES in a similar way to that shown in the script in the blog.Best regards,AlanHi Ravis,The relevant lines in the first script given in the blog are L45-51, where I create a new directory specific to each simulation, and copy the (same) input file into that directory. The directory naming structure I use is gpu${i}_sim${j}, such that e.g. for 2 simulations on each of 2 GPUs we would have 4 directories, gpu0_sim0, gpu0_sim1, gpu1_sim0 and gpu1_sim1.In your case, of course you will want to use a different input file in each directory. I suggest to set up these directories in advance, each with the appropriate input file(s), and then for each simulation, simply “cd” into the appropriate pre-existing directory to run the simulation (i.e. remove lines 47,48 and 51 but keep lines 46 and 49).Best regards,AlanHello Dr Alan,I appreciate your response to my queries.1)
No, I am not setting the CUDA_VISIBLE_DEVICES environment variable.
(Though I had also tried running mdrun after setting this as detailed in your blog.)The simulation.sh file solely consists of:2)
I have tried launching jobs with multiple GPUs and used the CUDA_VISIBLE_DEVICES variable. This had worked as expected without errors.  The simulations were running on GPU_ID 0 or 1 based on our CUDA_VISIBLE_DEVICES variable used with gmx mdrun.Some observations:I am attaching the tpr file, in case you would like to test them at your end.
md.tpr (6.1 MB)Thank you,
Akshay.Hi Akshay,Thanks for the info. It looks like you just need to set a unique MPS pipe directory for each job, before launching MPS.To do this for your first job (using, e.g. /tmp/mps1 for the directory):export CUDA_MPS_PIPE_DIRECTORY=/tmp/mps1
mkdir -p $CUDA_MPS_PIPE_DIRECTORY
nvidia-cuda-mps-control -dThen the second job on the same node should be able to use its GPU OK, and can also use MPS in a similar way, as long as it uses a different directory (e.g. /tmp/mps2).Best regards,AlanHello Dr Alan,Thank you very much for the suggestion. I will update you in case the problem remains unresolved.Regards,AkshayI have almost reproduced the results that are given in this post for launching multiple simulations on the A100 GPU using MPS and MIG. Now, I am trying to use the Nvidia Nsights profiler to get a deeper understanding of the system state, GPUs, memory copies, kernel execution times etc. However I am not finding any resource or article that could help in creating the profiles. I tried browsing through the documentation of the nsight tool and tried a lot of different methods to create profiles, however nvtx events are not captured along with some other events. Is there a blog/article which explains how this can be done? Thanks in advance for the help!Hi Dr. Alan,I am able to run the script and get an output of 32 .xtc files. I want to concatenate these separate files into one xtc file that covers the entire simulation. I’ve tried this using the gromacs trjcat utility as well as concatenating using VMD, but the xtc files do not appear to be in chronological order where gpu0_sim0 is the first portion of the trajectory and gpu0_sim1 is the next and so on. The position of the protein makes a large jump when moving from the last frame of one xtc file to the first frame of the next file.  Is there a way to stitch all the separate xtc files together so it is one contiguous trajectory?Thank you,
ReubenPowered by Discourse, best viewed with JavaScript enabled"
108,upcoming-webinar-transforming-transportation-with-the-metaverse-and-ai,"Originally published at:			https://info.nvidia.com/transforming-transportation-with-the-metaverse-and-ai.html#new_tab
Learn how NVIDIA Omniverse and NVIDIA DRIVE Sim are used to create digital twin environments to train, test, and validate autonomous driving systems.Powered by Discourse, best viewed with JavaScript enabled"
109,adapting-llms-to-downstream-tasks-using-federated-learning-on-distributed-datasets,"Originally published at:			https://developer.nvidia.com/blog/adapting-llms-to-downstream-tasks-using-federated-learning-on-distributed-datasets/
Learn how LLMs can be adapted to downstream tasks using distributed datasets and federated learning to preserve privacy and enhance model performance.Powered by Discourse, best viewed with JavaScript enabled"
110,accelerating-data-center-and-hpc-performance-analysis-with-nvidia-nsight-systems,"Originally published at:			https://developer.nvidia.com/blog/accelerating-data-center-and-hpc-performance-analysis-with-nvidia-nsight-systems/
NVIDIA Nsight Systems 2023.2 previews profiling for multinode systems alongside support for profiling Python, networking hardware metrics, and a new analysis framework.Powered by Discourse, best viewed with JavaScript enabled"
111,data-center-interconnect-reference-design-guide,"Originally published at:			Data Center Interconnect Reference Design Guide | Cumulus Networks Guides
This reference guide on Data Center Interconnect with NVIDIA Cumulus Linux covers L2 and L3 extension and design aspects.Powered by Discourse, best viewed with JavaScript enabled"
112,webinar-empower-your-industrial-edge-ai-applications-with-nvidia-jetson,"Originally published at:			https://nvda.ws/3pGXqEY
Gain insights from advanced AI use cases powered by the NVIDIA Jetson Orin in ruggedized environments.Powered by Discourse, best viewed with JavaScript enabled"
113,optimizing-large-scale-sparse-volumetric-data-with-nvidia-neuralvdb-early-access,"Originally published at:			https://developer.nvidia.com/blog/optimizing-large-scale-sparse-volumetric-data-with-nvidia-neuralvdb-early-access/
Building on the past decade’s development of OpenVDB, the introduction of NVIDIA NeuralVDB is a game-changer for developers and researchers working with extremely large and complex datasets. The pre-release version of NVIDIA NeuralVDB brings AI and GPU optimization to OpenVDB, delivering up to a 100x reduction in memory footprint for smoke, clouds, and other sparse…This is incredible. Posted to twitter.Powered by Discourse, best viewed with JavaScript enabled"
114,custom-datasets,"Does the license allow us to train our own dataset?  Can we provide a bunch of 3D human characters and train it on those 3D models?
As mentioned in another thread… that would also be useful to come with a standard game engine skeleton (unreal’s skeleton) and yes eventually also trained with the text description data at the same time.Yes, you can use the released code to train the model on your own dataset. However, please make sure to first check the intended use is in agreement with Nvidia Source Code license (GET3D/LICENSE.txt at master · nv-tlabs/GET3D · GitHub)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
115,webinar-empower-telco-contact-center-agents-with-multi-language-speech-ai-customized-agent-assists,"Originally published at:			Empower Telco Contact Center Agents with Multi-Language Speech-AI-Customized Agent Assists
Join Infosys, NVIDIA and Quantiphi on June 7 to learn how to use speech and translation AI to improve agent assist solutions in multiple languages.Powered by Discourse, best viewed with JavaScript enabled"
116,explainer-what-is-mlops,"Originally published at:			What is MLOps? | NVIDIA Blog
Machine learning operations, MLOps, are best practices for businesses to run AI successfully with help from an expanding smorgasbord of software products and cloud services.Powered by Discourse, best viewed with JavaScript enabled"
117,what-are-foundation-models,"Originally published at:			What Are Foundation Models? | NVIDIA Blogs
Foundation models are AI neural networks trained on massive unlabeled datasets to handle a wide variety of jobs from translating text to analyzing medical images.A foundation model refers to a base or core AI model that serves as the starting point for developing more specialized or domain-specific models. It is a large-scale pre-trained language model that has been trained on vast amounts of text data to understand and generate human-like language. Foundation models, such as GPT-3, are trained using unsupervised learning techniques and can perform a wide range of language-related tasks, including text generation, summarization, translation, and more. These models provide a strong foundation for building more targeted and specific AI applications by fine-tuning them on narrower datasets or adding task-specific prompts and instructions.Thanks for chiming in!Powered by Discourse, best viewed with JavaScript enabled"
118,accelerate-enterprise-apps-with-microsoft-azure-stack-hci-and-nvidia-bluefield-dpus,"Originally published at:			https://developer.nvidia.com/blog/accelerate-enterprise-apps-with-microsoft-azure-stack-hci-and-nvidia-bluefield-dpus/
Learn how the Microsoft Azure Stack HCI on NVIDIA BlueField DPUs prototype delivers network throughput gains and CPU core savings.Powered by Discourse, best viewed with JavaScript enabled"
119,improve-human-connection-in-video-conferences-with-nvidia-maxine-eye-contact,"Originally published at:			https://developer.nvidia.com/blog/improve-human-connection-in-video-conferences-with-nvidia-maxine-eye-contact/
Video conferencing is at the heart of several streaming use cases such as vlogging, vtubing, webcasting, and even video streaming for remote work. To create an increased sense of presence and pick up on both verbal and non-verbal cues video conferencing technology must enable users to see and hear clearly. Eye contact plays a key…Powered by Discourse, best viewed with JavaScript enabled"
120,job-statistics-with-nvidia-data-center-gpu-manager-and-slurm,"Originally published at:			Job Statistics with NVIDIA Data Center GPU Manager and SLURM | NVIDIA Technical Blog
Resource management software, such as SLURM, PBS, and Grid Engine, manages access for multiple users to shared computational resources. The basic unit of resource allocation is the “job”, a set of resources allocated to a particular user for a period of time to run a particular task. Job level GPU usage and accounting enables both users…Using -c allgpus the solution proposed does not work for non exclusive nodes.You may want to build upon these instead:prolog:… it would be great if dcgmi group -c would do json as well.epilog:… this requires jp.Powered by Discourse, best viewed with JavaScript enabled"
121,accelerating-standard-c-with-gpus-using-stdpar,"Originally published at:			https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/
Historically, accelerating your C++ code with GPUs has not been possible in Standard C++ without using language extensions or additional libraries: CUDA C++ requires the use of host and device attributes on functions and the triple-chevron syntax for GPU kernel launches.OpenACC uses #pragmas to control GPU acceleration.Thrust lets you express parallelism portably but uses language…This was a great article! It appears that all the discussions and example are based on accelerating standard C++ (code) without any need for CUDA programming but only on one single GPU.From my work so far on multi-GPU programming, invoking two GPUs and partitioning the data in between always needs some CUDA related code – for instance, binding a MPI rank or a thread to one of the GPUs, or using CUDA Streams for simultaneous use of multiple GPUs and probably other approaches to enable accelerations on multi-GPUs all need selecting the device one way or another which needs CUDA.All of these are in the opposite direction of “Accelerating Standard C++ with a GPU Using stdpar”, where the goal is to not change the CPU-based code (with no CUDA runtime API, etc.) and compile the code simply with NVC++. So I’m very curious if there any way around this currently, and if not is this something to look forward to in the future? I’d appreciate any insights here.Yes, multi-GPU stdpar support is on the roadmap.Great to hear! Here in August 2021 is there any new developments on the NVC++ compiler using multiple GPUs?Not yet I’m afraid, stay tuned!I understand that the containers in use must be using the heap, not the stack, in order for unified memory to have the data visible to both CPU and GPU.  My question is whether or when it will be possible for the containers memory to be a mmap pointer instead of a RAM pointer?Hi, thanks for the question. We are working on enabling more memory types such as stack memory for use with the parallel algorithms. mmap memory is not on our near-term roadmap, but I have forwarded your inquiry on to the team.I’ve created a benchmark for Standard C++ Parallel STL functions. When compiling it with nvc++ with -stdpar these functions run much slower than the serial (single-core CPU) versions and sort() along with stable_sort produce Segmentation Fault (when sorting a vector of 100 Million 32-bit integers). This is running on a Dell Alienware laptop with GeForce RTX 3060 GPU and 12-th Gen Intel 14-core CPU.Are there certain compiler switches that should be used to produce results that accelerate these functions? I use -stdpar and -O3 for the nvc++When compiling (using nvc++) without -stdpar all benchmarks, including sort() and stable_sort(), run to completion without segmentation fault, executing on a single-core of the Intel CPU.Thank you,
-VictorPowered by Discourse, best viewed with JavaScript enabled"
122,webinar-ai-enabled-cybersecurity-for-financial-services,"Originally published at:			AI-Enabled Cybersecurity for Financial Services
Learn how financial firms can build automated, real-time fraud and threat detection solutions with NVIDIA Morpheus.Powered by Discourse, best viewed with JavaScript enabled"
123,upcoming-webinar-kick-start-deep-learning-on-the-cloud,"Originally published at:			Event Registration
Join experts from NVIDIA and Microsoft on November 30 to discover the latest development in deep learning through hands-on demos and practical guidance for getting started on the cloud.Powered by Discourse, best viewed with JavaScript enabled"
124,state-of-the-art-language-modeling-using-megatron-on-the-nvidia-a100-gpu,"Originally published at:			State-of-the-Art Language Modeling Using Megatron on the NVIDIA A100 GPU | NVIDIA Technical Blog
Recent work has demonstrated that larger language models dramatically advance the state of the art in natural language processing (NLP) applications such as question-answering, dialog systems, summarization, and article completion. However, during training, large models do not fit in the available memory of a single accelerator, requiring model parallelism to split the parameters across multiple…Could I expect to be able to run this Megatron Q&A model on a Jetson Xavier NX device if it was the only model loaded?Powered by Discourse, best viewed with JavaScript enabled"
125,new-course-develop-customize-and-publish-in-nvidia-omniverse-with-extensions,"Originally published at:			Courses – NVIDIA
Learn how to create and customize the NVIDIA Omniverse experience with extensions using Python code in this free hands-on and self-paced course.Powered by Discourse, best viewed with JavaScript enabled"
126,webinar-cybersecurity-and-ai-in-retail,"Originally published at:			https://nvda.ws/44L3bRZ
​Join NVIDIA on Tuesday, June 6, 2023 at 9AM PT, for a webinar on cybersecurity and AI in retail. We discuss how AI can bring a new level of information security to the data center, cloud, and edge.Powered by Discourse, best viewed with JavaScript enabled"
127,advanced-api-performance-cpus,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-cpus/
To get the best performance from your NVIDIA GPU, pair it with efficient work delegation on the CPU.Powered by Discourse, best viewed with JavaScript enabled"
128,exploiting-and-securing-jenkins-instances-at-scale-with-groovywaiter,"Originally published at:			https://developer.nvidia.com/blog/exploiting-and-securing-jenkins-instances-at-scale-with-groovywaiter/
Learn how to secure Jenkins instances with the GroovyWaiter Python script.Powered by Discourse, best viewed with JavaScript enabled"
129,overcoming-communication-congestion-for-hpc-applications-with-nvidia-nvtags,"Originally published at:			Overcoming Communication Congestion for HPC Applications with NVIDIA NVTAGS | NVIDIA Technical Blog
HPC applications are critical to solving the biggest computational challenges to further scientific research. There is a constant need to drive efficiencies in hardware and software stacks to run larger scientific models and speed up simulations. High communication costs between GPUs prevents you from maximizing performance from your existing hardware. To address this, we’ve built…Hi, Does NVTAGS support running jobs through LSF (Load Sharing Facility)? Also, NVTAGS seems to be for distributed MPI+GPU, would it work without MPI? Currently, for my application, it seems that both the device and the socket are randomly and independently selected – Is there a way to grab / bind the closest socket automatically without setting CUDA_VISIBLE_DEVICES using NVTAGS? Thanks!Hi,At the moment NVTAGS has support for Slurm and direct MPI calls (with mpirun/mpiexec). NVTAGS is also only supported with MPI. It relies on detecting MPI ranks of processes to assign selected resources.Setting CUDA_VISIBLE_DEVICES is necessary as otherwise NVTAGS will not know which devices are assigned to each process, so that it can select CPU socket/cores and NICs that are within the affinity of the selected device.Running NVTAGS in run mode without prior tuning will set CUDA_VISIBLE_DEVICES to GPUs 0,1,2,… The assumption here is that process with rank 0,1,2,… will pick them in order(which is the typical default GPU selection). Next, CPU sockets/cores and NIC within the affinity of selected devices will be selected for each process.In short, you need to know which GPU is selected by each process so that you can select resources that are within its affinity. Hope this helpsHi @imanfaraji
Regarding “Running NVTAGS in run mode without prior tuning will set CUDA_VISIBLE_DEVICES to GPUs 0,1,2,…” – is this due to the fact that this environment variable is set to FASTEST_FIRST by default? Then, it could potentially generate different IDs for the devices compared to PCI_BUS_ID if the devices happen to have different speeds. Is there a way to set the IDs so that they are consistent with the IDs in nvidia-smi? Also, if we were to run with IBM LSF spectrum, is there a way to set CUDA_VISIBLE_DEVICES?
Thanks!Yes, FASTEST_FIRST is default behavior but what tools like NVTAGS does is trying to resolve is optimizing total GPU communication time. So, this ordering doesn’t mean GPU3<->GPU4 connection is faster than GPU4<->GPU5 connection. BW/Latency benchmarking needs to be done here to figure out connection speed.Moreover, let’s say GPU1<->GPU2 and GPU5<->GPU6 are fastest GPU pairs in your applications, however what matters here most is that GPU processing pairs that are communicating the most to use them. Let’s say Process1<->Process2 and Process5<->Process6 have little or no GPU communication, then default GPU ordering will be a bad GPU selection for your application. What NVTAGS does it reorder GPUs so highly communicating processes use fastest GPU pairs.Again, you need Latency/BW benchmarks to know which processing pairs are the fastest.Please, note that you first need to know that your application GPU communication pattern is non-uniform. If all of the GPUs are communicating the same amount of data with each other, there won’t any benefit in reordering GPUs. Having said that, you can still pick the NIC and CPU cores/socket within the affinity of your selected GPU and that should help.I am not sure how setting affinity can be done in IBML LSF, but I find setting CUDA_VISIBLE_DEVIES env var and using numactl/tasket best option for this. Please take a look at nvtags_set_env.sh and nvtags_run scripts. This will give you an idea how this is done, I think you should be able to replicate that for IBM LSF.Powered by Discourse, best viewed with JavaScript enabled"
130,realizing-the-power-of-real-time-network-processing-with-nvidia-doca-gpunetio,"Originally published at:			https://developer.nvidia.com/blog/realizing-the-power-of-real-time-network-processing-with-nvidia-doca-gpunetio/
NVIDIA DOCA GPUNetIO library can be adopted in a wide range of applications from different contexts, providing huge improvements for latency, throughput, and system resource utilization.Powered by Discourse, best viewed with JavaScript enabled"
131,training-your-jetbot-in-nvidia-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/training-your-jetbot-in-isaac-sim/
How do you teach your JetBot new tricks? In this post, we highlight NVIDIA Isaac Sim simulation and training capabilities by walking you through how to train the JetBot in Isaac Sim with reinforcement learning (RL) and test this trained RL model on NVIDIA Jetson Nano with the real JetBot. Figure 1. Which one is…Hi there, for training on omniverse, besides the isaac sim, do we need any other libs? i check your post and the source code in github, the readme.txt in the isaccsim_patch says:
Please copy these files over to your Isaac Sim 2020.2 folder before training.
Specifically, syntheticdata.py goes to source/extensions/omni.isaac/synthetic_utils/python/scripts/
The other 4 files go to source/python_samples/jetbotso where is the omniverse extension, where can we download the extension?ls/python/scripts/
The other 4 files go to source/python_samples/jetbotso where is the omniverse extension, where can we download the extension?You can register and download Isaac Sim here, as stated in the blog post.
https://developer.nvidia.com/isaac-sim“First, download Isaac Sim. Running Isaac Sim requires the following resources:”Hi HaiLocLu,
thanks for the reply, I already downloaded the Isaac Sim, and could not find the extensions in this location: extensions/omni.isaac.
Instead , i found out another directory, looks like it is what I want:
isaac-sim/_build/linux-x86_64/release/exts
I will do the sample training later.isaac-sim/_build/linux-x86_64/release/extsThanks much. You’re right. I was referring to the internal path, while the path for your public build should be in the _build folder. I updated the instructions on my github.Thanks
image1626×1000 81.3 KB
maybe gym version error…how to solve it?Looks like a gym version error. Which version of stable_baselines did you use? You can also check the Isaac Sim’s python_samples/requirements.txt.
At the time of the blogpost published, it was
python3 -m pip install stable_baselines3==0.8.0So the versions in Isaac Sim need to match what’s installed on the real JetBot. Thanks.I am struggling to train using given material from issac sim patch to isaac simulation windows 11. When I trained its saying omnikithelper errorHiya Naresh!Thank you for your interest in our blog post and RL in Omniverse with Isaac-sim! Unfortunately, much of the codebase has changed since this post was released and it is no longer compatible with current versions of Isaac-sim.  In short, addressing this issue would not only mean maintaining depreciated code, but also maintaining a demonstration of a workflow that may not even be valid anymore.That said, we are still developing tools for building and running RL environments in Omniverse with Isaac-sim!  Keep your eyes peeled for new developments in the near future! ;)Sorry I couldn’t be of more help.  Good luck in your work!
GusThanks for the reply, I am happy to collaborate with you guys to make new blog post based on the latest information on the deployment of Deep Reinforcement learning to robotics kit like jetbotThanks for the offer, @ac.nareshkumar1993! While Gus might not be able to work on a new post right now (GTC is coming right up!), we’re always willing to get new posts from other people who have success stories or solutions. Let us know if you’d like to submit a post and we’ll work with you on the process.I am willing to work with you , and write a post soonGreetings! I found the blog and started to follow along, but ran into some snags. Is it still currently possible to train the Jetbot in Isaac Sim through the Omniverse, or is this no longer supported?If it is supported, is it possible to do on Windows? I tried placing the patch files in a few different directories but no luck getting Jetson options to show up in the software.If it’s not supported, is there a roadmap for when support will return? Or alternatives to use? I am looking to teach AI + robotics and would like to leverage simulation to speed up some elements like model training.Hiya jbflot!  welcome to the forums :DThis is a question I get asked a lot! The bad news first: Unfortunately that code is depreciated.The good news is we have Omni Isaac Gym Environments (OIGE) GitHub - NVIDIA-Omniverse/OmniIsaacGymEnvs: Reinforcement Learning Environments for Omniverse Isaac Gym
Which leverages the RL games library for training, and might meet your immediate teaching needs.This might actually make another good topic for a live stream :3Thanks for the response and the link to the Gym Environments. They’re very cool, but after further investigation they seem out-of-reach for the graphics cards in our computers. The Viewport wasn’t loading in Isaac Sim and the console was throwing an error that said I needed to do a clean install of my drivers - I did, but the problem persisted. I have a Quadro T2000 which seems like it’s not supported. Thanks again.Edit: Maybe update the blog that this is based on, if possible, to reflect the deprecated support? It’s still quite high in search rankings and makes it seem like it’s something supported until you get into the weeds.You are not the only person who has pointed this out! Thank you!  I want to do an updated version of this RL blog post with updated techniques, but modern techniques are very different.  There’s a lot to discuss actually…   I will at the very least add a depreciation warning at the top of the postPowered by Discourse, best viewed with JavaScript enabled"
132,top-data-center-energy-efficiency-sessions-at-nvidia-gtc-2023,"Originally published at:			https://nvda.ws/3lVqnea
Learn how accelerated computing can reduce your total carbon footprint and support your organization’s energy efficiency efforts.Powered by Discourse, best viewed with JavaScript enabled"
133,improving-machine-learning-security-skills-at-a-def-con-competition,"Originally published at:			https://developer.nvidia.com/blog/improving-machine-learning-security-skills-at-a-def-con-competition/
NVIDIA recently helped run an innovative competition at DEF CON 30, providing an opportunity for security and data professions to improve their machine learning security skills.Powered by Discourse, best viewed with JavaScript enabled"
134,getting-started-with-nvidia-instant-nerfs,"Originally published at:			https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/
Johnathan Stephens provides a walkthrough of how he started using Instant NeRF.Hi
What/Where is the right place in this forum for technical assistance/exchange  regarding NeRFs ?Hi,
I was trying to build the NeRF ‘program’ but I got an error.
The build stops because it can’t find the file :
\instant-ngp\build\CMakeFiles\3.25.0\CompilerIdCUDA\CompilerIdCUDA.vcxproj
Does anyone have an idea how to solve this issue ?
Thank you all for your answers,Warmest regardsIf running without GUI, how to generate animation videos? When run .\build\testbed --scene data\nerf\fox --no-gui ( without GUI) it won’t stop the training…
Thanks,
JimPowered by Discourse, best viewed with JavaScript enabled"
135,open-source-simulation-expands-with-nvidia-physx-5-release,"Originally published at:			Open Source Simulation Expands with NVIDIA PhysX 5 Release | NVIDIA Technical Blog
Learn about the latest version of the NVIDIA PhysX SDK, the primary physics engine and a key foundational technology pillar of NVIDIA Omniverse.Powered by Discourse, best viewed with JavaScript enabled"
136,writing-ray-tracing-applications-in-python-using-the-numba-extension-for-pyoptix,"Originally published at:			https://developer.nvidia.com/blog/writing-ray-tracing-apps-in-python-using-numba-for-pyoptix/
Using Numba and PyOptiX, NVIIDA enables you to configure ray tracing pipeline and write kernels in Python compatible with the OptiX pipeline.Thank you for reading the post! The example outlined in this blog post was adapted from the triangle example in the OptiX package. Several modifications was made to the kernel code to address the differences in python and CUDA C++. The extension currently have several hard coded constraints on some of the API calls, such as the number of payload registers for optix.Trace. While it’s trivial to expand the lowering of these methods to support more overloads, I’m exploring a more programmatic way to provide a full support to all APIs available in optix kernel. In general, this work is in active development and I’d love to hear from you while you are playing with the demo repository.I have a few higher level questions, my background is previously using CUDA for doing ray tracing based simulations for medical imaging and computed tomography projections.Does OptiX lend itself to these applications? Typically summing a path integral as a trilinear interpolation through a voxellized space.Any idea what kind of performance difference with this and a CUDA implementation?To what numerical precision does OptiX support (float or double)?Hi Michael!If you are able to say, is this work still in active development?  Using Numba to access the OptiX API would be a perfect workflow for a new project that I am starting.  I know it is not ready for prime time yet, but it would be useful to know whether there is value in planning my work to slot in to OptiX using Numba in the future, or whether I should assume that if I want to use OptiX’s features then I should assume I’ll only ever be able to use the c++ API directly.I ask because I noticed that there’s been no development in the PyOptiX repository since before this post was made.Powered by Discourse, best viewed with JavaScript enabled"
137,accelerated-data-analytics-speed-up-data-exploration-with-rapids-cudf,"Originally published at:			https://developer.nvidia.com/blog/accelerated-data-analytics-speed-up-data-exploration-with-rapids-cudf/
This post is part of a series on accelerated data analytics: Accelerated Data Analytics: Faster Time Series Analysis with RAPIDS cuDF walks you through the common steps of time series data processing with RAPIDS cuDF. This post discusses how the pandas library provides efficient, expressive functions in Python. Digital advancements in climate modeling, healthcare, finance,…Powered by Discourse, best viewed with JavaScript enabled"
138,favorite-path-tracing-color,"What’s your favorite path tracing color?Green. Obvs. Pantone #76B900Powered by Discourse, best viewed with JavaScript enabled"
139,getting-the-most-out-of-the-nvidia-a100-gpu-with-multi-instance-gpu,"Originally published at:			https://developer.nvidia.com/blog/getting-the-most-out-of-the-a100-gpu-with-multi-instance-gpu/
With the third-generation Tensor Core technology, NVIDIA recently unveiled A100 Tensor Core GPU that delivers unprecedented acceleration at every scale for AI, data analytics, and high-performance computing. Along with the great performance increase over prior generation GPUs comes another groundbreaking innovation, Multi-Instance GPU (MIG). With MIG, each A100 GPU can be partitioned up to seven…I have followed all the instructions  referred in the MIG Manual , however, when I run “sudo nvidia-smi mig -cgi 9,3g.20gb -C”, it turns out to be
Option “-C” is not recognized.
How should I solve this problem?
And without the “-C” option, though I can find the MIGs by “nvidia-smi mig -lgi”, but neither can I get it through “nvidia-smi” nor “ls -l /proc/driver/nvidia/capabilities/gpu1/mig/gi*”
What should I do with this problem?Hi ryy19Option “-C” is not recognized.As mentioned in the software pre-requisites, are you running at least R450.80.02 as the driver version for A100? The “-C” option is only available starting with this driver version.but neither can I get it through “nvidia-smi” nor “ls -l /proc/driver/nvidia/capabilities/gpu1/mig/gi*”Can you please provide more information on what you’re not able to see? MIG devices once created can be accessed either through “nvidia-smi -L” or “nvidia-smi mig -lgi”hi, is there a good way for users without sudo rights to use the MIG functionality? I think running multiple scripts in parallel on the same A100 sounds very interesting, but it needs to work without admin rights (at least after the admin has enabled MIG on the GPU).is there a way to do that?Hi @mikkelsen.kaare - not today. We expect that clusters with A100 GPUs are configured in desired MIG geometries - the configurations can be static (a priori by the infra team) or dynamic (using a systemd service for example as nodes are brought online when used in an autoscaler environment). We have created tooling that can be used for these purposes.Please check this project for a declarative way to create the desired MIG geometries: https://github.com/nvidia/mig-parted and the associated systemd service that can be used in conjunction with provisioning nodes: https://github.com/NVIDIA/mig-parted/tree/master/deployments/systemd. We expect that these tools be used instead of nvidia-smi commands, which can be error prone when used in a production environment. Hope these are useful.Hi,
whether the A100 GPU with Multi-Instance GPU (MIG) allow users to set the application clock (graphic or memory) for a specific GPU instance? Or when we set the application clock via Nvidia-smi, it applies to all instances within the GPU.Hi @kz181, it should apply to all MIG instances, as all MIG instances share a single clock and power limit.is it possible to enumerate multiple MIG compute instances?For example, can I pass the UUIDs of multiple compute instances of MIG as the CUDA_VISIABLE_DEVICES or --gpus for the docker, such that my program or docker container can find those MIG GPU devices and using the cudaSetDevice to index them by number, such as 0,1,2 for three different compute instances?Thanks!Under “single” strategy,  “num_gpus” doesn’t work. It always uses one MIG device.
python tf_cnn_benchmarks.py --num_gpus=2 --batch_size=64 --model=resnet50 --use_fp16Hi,
When multiple users log into the same A100 (linux ssh), how to allocate the MIG GPUs to each so that one user does not step on to other user’s GPU slices? Lets say we use 3g.20GB i.e. each A100 GPU is split into 2 slices. So totally 16 slices are available. There are device ids now uuids. Is there a way to allocate devices to individual users? @maggiez @chetantekurIs MIG meant for only docker containers? Can multiple users ssh directly to the VM and use ?CUDA_VISIBLE_DEVICES, I am not sure whether this could help you.Powered by Discourse, best viewed with JavaScript enabled"
140,breaking-mlperf-training-records-with-nvidia-h100-gpus,"Originally published at:			https://developer.nvidia.com/blog/breaking-mlperf-training-records-with-nvidia-h100-gpus/
In MLPerf Training v3.0, the NVIDIA AI platform powered by the NVIDIA H100 Tensor Core GPU set new performance records.Powered by Discourse, best viewed with JavaScript enabled"
141,ultra-realism-made-accessible-with-nvidia-ai-and-path-tracing-technologies,"Originally published at:			https://developer.nvidia.com/blog/ultra-realism-made-accessible-with-ai-and-path-tracing-technologies/
At GDC 2023, NVIDIA released new tools that make real-time path tracing more accessible to developers while accelerating the creation of ultra-realistic game worlds. Video 1. NVIDIA at GDC 2023: Frame Generation and Path Tracing Tools Now Available Generate frames with the latest breakthrough in AI rendering Announced with the NVIDIA Ada Lovelace architecture, DLSS…Powered by Discourse, best viewed with JavaScript enabled"
142,get3d-rights-protection,"Will Get3d be programmed with a certificate system or something to let it know whether it can use an image or not?GET3D is a research project, not for commercial use., hence we do not plan to provide any certificate systemThank you for answering. I had the wrong program in mind obviously.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
143,building-an-intelligent-robot-dog-with-the-nvidia-isaac-sdk,"Originally published at:			Building an Intelligent Robot Dog with the NVIDIA Isaac SDK | NVIDIA Technical Blog
Figure 1. Some of the mobile robots running Isaac SDK for autonomous navigation. From left to right, NVIDIA Carter, BMW STR, NVIDIA Kaya, and Laikago. The navigation stack supports different bases, brains, and sensors. The modular and easy-to-use navigation stack of the NVIDIA Isaac SDK continues to accelerate the development of various mobile robots. Isaac…Is the TX1 powerful enough to run the software? I have one from a previous project.what chances do we have to control Go Pro 1 [ non EDU] ?Powered by Discourse, best viewed with JavaScript enabled"
144,is-cuda-12toolkit-with-tensorflow-and-nvidia-driver-535-compatible-on-nvidia-a100-i-seem-to-run-into-issues,"tf.config.list_physical_devices(‘GPU’)the above command returns and empty list instead of showing devicesmy os is ubuntu 20.4also can you help me know what tensorflow version are supported by cuda 12 and nvidia driver 535 .Yes, CUDA 12.x and R535 are compatible with the A100 GPUs on Ubuntu 20.04. You may want to double check that the GPU is visible through nvidia-smi. If it’s not, you may have a driver installation issue, or if it is you may have installed TensorFlow without GPU support or without the required dependencies.If you want further help debugging this it might be good to post on the general forum, we’re not able to get into specific issue debug here in this AMA.Thank you lastly one qestion like does running the code inside a  conda enviroment affect the command output for detecting tensorflow  also can you refer me  a guide which i could follow for proper driver and installation with cuda 12 it will  be great help for me.One great option for you might be to use one of the developer containers from NGC that comes with TensorFlow and other machine learning frameworks already set up in a Docker container ready-to-use. You can sidestep a lot of the environment setup and get straight to the good stuff: TensorFlow | NVIDIA NGCThese containers are usable on both Linux and Windows. But, for a specific TensorFlow setup guide using Anaconda I am unfortunately not the right person to ask, you might check on our ML forums for more information or refer to the TensorFlow documentation.unfournatley i cant use the container as we run our codes on virtual server and there are conda enviroment set up i seem to list the gou using driver 470 and cuda 11.8 also i can try it without the conda enviroment can you provide me some documents to follow to properly install latest nvidia driver and cuda 12 properly without the conda enviroment on nvidia a100Jump on over to this download page:Get the latest feature updates to NVIDIA's proprietary compute stack.Here you’ll find the Ubuntu 20.04 deb package, as well as instructions to get it installed. If you follow those, you wll end up with your system configured for 12.2 CUDA (paired with r535 driver), which is currently the latest versions released.Thank youPowered by Discourse, best viewed with JavaScript enabled"
145,distributed-deep-learning-made-easy-with-spark-3-4,"Originally published at:			https://developer.nvidia.com/blog/distributed-deep-learning-made-easy-with-spark-3-4/
With the release of Spark 3.4, users now have access to built-in APIs for both distributed model training and model inference at scale.Powered by Discourse, best viewed with JavaScript enabled"
146,debugging-cuda-more-efficiently-with-nvidia-compute-sanitizer,"Originally published at:			https://developer.nvidia.com/blog/debugging-cuda-more-efficiently-with-nvidia-compute-sanitizer/
Debugging code is a crucial aspect of software development but can be both challenging and time-consuming. Parallel programming with thousands of threads can introduce new dimensions to the already complex debugging process. There are various tools and techniques available to developers to help make debugging simpler and more efficient. This post looks at one such…Great news!  I look forward to using this.  Does this also now work with address sanitizer and thread sanitizer?No, the compute sanitizer tools use binary patching at runtime, so they work independently from compiler-assisted tools such as asan or tsan.This looks very useful! Do you know if these tools will detect problems within user Cuda code which are compiled into larger OptiX kernels? Thanks.Yes, the compute-sanitizer tools support OptiX applications since CUDA 11.6.See Compute Sanitizer User Manual :: Compute Sanitizer Documentation for more information.Powered by Discourse, best viewed with JavaScript enabled"
147,ai-app-predicts-the-popularity-of-social-media-posts,"Originally published at:			AI App Predicts the Popularity of Social Media Posts | NVIDIA Technical Blog
Cornea AI takes the guesswork out of photo sharing by using deep learning to predict how well your photo will do on social platforms. Users can either take a photo, or import one from their gallery and the AI will score the photo — it then suggests popular filters and hashtags to increase the popularity…While I believe quality content and genuine engagement are more important than simply amassing a large following, building a social media presence can be challenging, and any little help can be appreciated.Powered by Discourse, best viewed with JavaScript enabled"
148,designing-digital-twins-with-flexible-workflows-on-nvidia-base-command-platform,"Originally published at:			https://developer.nvidia.com/blog/designing-digital-twins-with-flexible-workflows-on-nvidia-base-command-platform/
Creating high-fidelity digital twins across teams and locations using NVIDIA Modulus with NVIDIA Base Command Platform is the newest tool available for HPC workflows.Hi! I’m Joe, one of the authors of this blog - we’re excited to show off some of the hard work from our Modulus team on Base Command Platform, powering DGX Cloud. I’m happy to answer any questions that come to mind after reading our post!Hello,Hi, Joe, …
I’ll have further questions on the expectable output from DGX Cloud interactions, …I watched videos, read much of the material (glance-wise), and now am hoping simply to “Anchor” said process, with personal interact… (The following, mostly brief, …tweet-length circumstance/definition.)Link, to Document Remainder [Multiple Links]:Access Google Docs with a personal Google account or Google Workspace account (for business use).J.Powered by Discourse, best viewed with JavaScript enabled"
149,thank-you-for-joining-us-for-our-path-tracing-ama-we-appreciate-all-the-questions,"Thanks for joining us for this AMAThanks from both Filip and I enjoyed answering the questions
Here are some additional resources for Path Tracing:
Path Tracing SDK overview page: RTX Path Tracing SDK | NVIDIA Developer
Overview Session from GTC Spring: How to Build a Real-time Path Tracer | NVIDIA On-DemandPowered by Discourse, best viewed with JavaScript enabled"
150,evaluating-applications-using-the-nvidia-arm-hpc-development-kit,"Originally published at:			https://developer.nvidia.com/blog/evaluating-applications-using-the-nvidia-arm-hpc-development-kit/
The Oak Ridge National Laboratory Leadership Computing Facility integrated the NVIDIA Arm HPC Developer Kit into their Wombat test cluster and tested different HPC applications.Powered by Discourse, best viewed with JavaScript enabled"
151,archigan-a-generative-stack-for-apartment-building-design,"Originally published at:			ArchiGAN: a Generative Stack for Apartment Building Design | NVIDIA Technical Blog
Figure 1. GAN-Generated masterplan AI will soon massively empower architects in their day-to-day practice. This potential is around the corner and my work provides a proof of concept. The framework used in my work offers a springboard for discussion, inviting architects to start engaging with AI, and data scientists to consider Architecture as a field of investigation.…Interesting - I am also working on a 3D-gan space planing, in particular to generate small modular dwellings but I got stucked when I generate vectors in a given space. As oposed to learning creatively from images, i want to develop a network of boundaries but i dont know how to represent them in 3d. In 2d space makes sense, you touched a very sensitive part of space planning. Congrats!  https://uploads.disquscdn.c...Hi, Can you please provide a link to the dataset used for training here?would be happy to see it implemented on FOSS tools like Blender…
see osarch.orgTake (i) a floor plate (Gross Square footage; length and width of building; assume same square footage of both sides of center-line); and, (ii) a set of standardized apartment floor plans:Use a modular approach to Unit design:A Module’s dimensions are (must be):
(i) length equal to the length of 1/2 of the building width
(ii) width - a number that when multiplied by the width is divisible by 1/4 and 1/2, each of which result in planned room or space usage
(iii) that is, all room spaces consume either: 100%, are some combination of quarters (75%; 50% or 25%) of a standard module dimension
EX: 40 foot width by 16.26 foot = a 325 SF module
(a) Studio - one module
(b) 1BR 1 Bath - two modules
(c) 2BR 1 Bath - three modules
(d) many variations using 1/2 and 1/4 modules added to the above
Then:Use a given for (a) corridor space, (b) elevator shaft space, (c) stairwell space
Use the set of Apt Units selected for the building
Factor Two Story units for Town-homes (if selected)Requirement - Determine the Unit mix that consumes/maximizes the floor plate square footageHow can I train myself to use AI for architecture? Where do I even start?  I was behind the curve when Revit gained the edge over AutoCAD. It has been a painful process to catch up.  I would love to be in front of the curve with adapting AI. Thank youPowered by Discourse, best viewed with JavaScript enabled"
152,harnessing-the-nvidia-ada-architecture-for-frame-rate-up-conversion-in-the-nvidia-optical-flow-sdk,"Originally published at:			https://developer.nvidia.com/blog/harnessing-the-nvidia-ada-architecture-for-frame-rate-up-conversion-in-the-nvidia-optical-flow-sdk/
The NVIDIA Optical Flow SDK 4.0 is now available, enabling you to fully harness the new NVIDIA Optical Flow Accelerator on the NVIDIA Ada architecture with NvOFFRUC. Optical flow on the NVIDIA Ada Lovelace architecture Starting from the NVIDIA Turing architecture, NVIDIA GPUs have dedicated hardware for optical flow computation between a pair of frames.…Powered by Discourse, best viewed with JavaScript enabled"
153,the-9am-ama-session-is-closed-but-the-team-will-be-back-live-at-6pm-pst-again,"Please feel free to post new questions and topics and the team will answer them when they return at 6pm (PST) later today
Thanks so muchPowered by Discourse, best viewed with JavaScript enabled"
154,changing-cybersecurity-with-natural-language-processing,"Originally published at:			https://developer.nvidia.com/blog/changing-cybersecurity-with-natural-language-processing/
NLP can be leveraged in cybersecurity workflows to assist in breach protection, identification, and scale and scope analysis.Powered by Discourse, best viewed with JavaScript enabled"
155,extension-for-chatgpt,"
jcass358
Posted this question as a reply - so I am re-posting it here so everyone can see.Hello, I am definitely interested in using a tool like ChatGPT, would this event show us how to buld the extension? also how feasible would it be to perform transfer learning on the LLM with all of the NVIDIA information and software publicly available to finetune a model to help out beginners like myself.
thank youthank youYour are most welcome - sorry for any confusion - our guys will have a response for you very soonHi, while we will not be presenting today, there is a GitHub project that demonstrates how to implement the ChatGPT + Omniverse integration. You can find it here: GitHub - NVIDIA-Omniverse/kit-extension-sample-airoomgenerator: A tool used to create 3D content for rooms by calling OpenAI's API. As per the question on fine-tuning the model, while you can’t fine-tune GPT-4, you can fine tune other LLMs and then connect your fine-tuned model into Omniverse in a similar way we did in our Github project.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
156,webinar-nvidia-dlss-3-and-unreal-engine-5-2,"Originally published at:			Level Up with NVIDIA
On July 26, walkthrough DLSS 3 features within Unreal Engine 5.2 and learn how to best use the latest updates.Powered by Discourse, best viewed with JavaScript enabled"
157,nvidia-and-snowflake-collaboration-boosts-data-cloud-ai-capabilities,"Originally published at:			NVIDIA and Snowflake Collaboration Boosts Data Cloud AI Capabilities | NVIDIA Technical Blog
NVIDIA and Snowflake announced a new partnership bringing accelerated computing to the Data Cloud with the new Snowpark Container Services (private preview), a runtime for developers to manage and deploy containerized workloads. By integrating the capabilities of GPUs and AI into the Snowflake platform, customers can enhance ML performance and efficiently fine-tune LLMs. They achieve…As an enthusiast in LLM, I’d like to suggest a hands-on demo with the Data model for training while using Nvidia’s 90-day trial in this environment to help us understand how to build Data Cloud AI using the environment capabilities.Powered by Discourse, best viewed with JavaScript enabled"
158,google-colabs-pay-as-you-go-offers-more-access-to-powerful-nvidia-compute-for-machine-learning,"Originally published at:			Colab's ‘Pay As You Go’ Offers More Access to Powerful NVIDIA Compute for Machine Learning — The TensorFlow Blog
Colab’s new Pay As You Go option helps you accomplish more with machine learning. Access additional time on NVIDIA GPUs with the ability to upgrade to NVIDIA A100 Tensor Core GPUs when you need more power for your ML project.Powered by Discourse, best viewed with JavaScript enabled"
159,new-gpu-library-lowers-compute-costs-for-apache-spark-ml,"Originally published at:			https://developer.nvidia.com/blog/new-gpu-library-lowers-compute-costs-for-apache-spark-ml/
Spark MLlib is a key component of Apache Spark for large-scale machine learning and provides built-in implementations of many popular machine learning algorithms. These implementations were created a decade ago, but do not leverage modern computing accelerators, such as NVIDIA GPUs. To address this gap, we have recently open-sourced Spark RAPIDS ML (NVIDIA/spark-rapids-ml), a Python…Powered by Discourse, best viewed with JavaScript enabled"
160,av1-encoding-and-fruc-video-performance-boosts-and-higher-fidelity-on-the-nvidia-ada-architecture,"Originally published at:			AV1 Encoding and FRUC: Video Performance Boosts and Higher Fidelity on the NVIDIA Ada Architecture | NVIDIA Technical Blog
AV1 Encoding comes to Video Codec SDK 12.0 on NVIDIA Ada, while frame rate up conversion (FRUC) doubles video frame rates with interpolation in Optical Flow 4.0.Hi
Can you tell us what is the underlying algorithms behind nvidia optical flow method.
Thanks
IlanHi, is it possible to use greater than 2X framerate enhancement using FRUC? I.e. generate, say, 2 or more intermediate frames, instead of just 1.You would only need to do the forward / back motion vector estimates once, plus filling in the missing gaps, and use those end points to generate several intermediary frames instead of just 1. I’d love to see a 5x or even 10x enhancement (like 24 FPS → 120 FPS or even 240).thanks!Any anticipated release date for NVOF SDK 4.0? Front page said “in October” ;)Thanks.I would welcome an adjustable interpolation. Having a 0-1 parameter as the distance between the frames. 0 would be completely first frame and 1 the second one. 0.5 would be the interpolated exactly in between.Available Now!!! :)Hi,
We do not provide these details publicly but are happy working under NDA with companies requiring this information when needed. Please reach out to your NVIDIA contact if this is needed.Current implementation of FRUC library support only 2x framerate in one run.
FRUC library is capable to interpolate a frame anywhere between two frames e.g. given two frames at time stamp 0 and 1 you can interpolate frame at any arbitrary time stamp like 0.25, 0.33, 0.75 and so on.
Frame rate of more than 2x can be achieved using FRUC library multiple times on same input stream.  This solution is not optimal and efficient but will allow you to test and check on your end.Powered by Discourse, best viewed with JavaScript enabled"
161,benchmarking-deep-neural-networks-for-low-latency-trading-and-rapid-backtesting-on-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/benchmarking-deep-neural-networks-for-low-latency-trading-and-rapid-backtesting-on-nvidia-gpus/
NVIDIA GPUs enable electronic trading applications to run inference in real time on very large LSTM models serving some of today’s fastest-moving markets.Thanks for the post; just a question. Would you mind refrencing the result for the same task on FPGA/ASIC?Other submissions for the STAC ML Inference Benchmarks can be found on the STAC website. For instance,Powered by Discourse, best viewed with JavaScript enabled"
162,webinar-performance-measurement-of-robotics-applications-with-ros2-benchmark,"Originally published at:			Isaac ROS Webinar Series
Register now for this Isaac ROS webinar on May 4th to learn how to run and customize ros2_benchmark to measure your robotics application graphs of nodes.Powered by Discourse, best viewed with JavaScript enabled"
163,step-into-the-future-of-industrial-grade-edge-ai-with-nvidia-jetson-agx-orin-industrial,"Originally published at:			https://developer.nvidia.com/blog/step-into-the-future-of-industrial-grade-edge-ai-with-nvidia-jetson-agx-orin-industrial/
Embedded edge AI is transforming industrial environments by introducing intelligence and real-time processing to even the most challenging settings. Edge AI is increasingly being used in agriculture, construction, energy, aerospace, satellites, the public sector, and more. With the NVIDIA Jetson edge AI and robotics platform, you can deploy AI and compute for sensor fusion in…Powered by Discourse, best viewed with JavaScript enabled"
164,new-data-science-client-and-wsl2-for-data-science-development-on-workstations,"Originally published at:			https://developer.nvidia.com/blog/how-to-jumpstart-data-science-workflows/
Data science development faces many challenges in the areas of: Exploration and model developmentTraining and evaluationModel scoring and inference Some estimates point to 70%-90% of the time is spent on experimentation – much of which will run fast and efficiently on GPU-enabled mobile and desktop workstations. Running on a Linux mobile workstation, for example, presents…Powered by Discourse, best viewed with JavaScript enabled"
165,asynchronous-error-reporting-when-printf-just-won-t-do,"Originally published at:			Asynchronous Error Reporting: When printf Just Won’t Do | NVIDIA Technical Blog
Some programming situations call for reporting “soft” errors asynchronously. While printf can be a useful tool, it can increase register use and impact performance. In this post, we present an alternative, including a header library for generating custom error and warning messages on the GPU without a hard stop to your kernel. Often error reporting…Powered by Discourse, best viewed with JavaScript enabled"
166,upgrading-multi-gpu-interconnectivity-with-the-third-generation-nvidia-nvswitch,"Originally published at:			https://developer.nvidia.com/blog/upgrading-multi-gpu-interconnectivity-with-the-third-generation-nvidia-nvswitch/
NVIDIA NVSwitch is designed to provide connectivity within a node or to GPUs external to the node for the NVLink Switch System.Is the Custom-FW OSFP optical or electrical?Powered by Discourse, best viewed with JavaScript enabled"
167,validating-active-sensors-in-nvidia-drive-sim,"Originally published at:			https://developer.nvidia.com/blog/validating-active-sensors-in-nvidia-drive-sim/
Autonomous vehicle development is all about scale. Engineers must collect and label massive amounts of data to train self-driving neural networks.  This data is then used to test and validate the AV system, which is also an immense undertaking to ensure robustness.  Simulation is an important tool to reach this level of scale, but accuracy…Powered by Discourse, best viewed with JavaScript enabled"
168,smarter-retail-data-analytics-with-gpu-accelerated-apache-spark-workloads-on-google-cloud-dataproc,"Originally published at:			https://developer.nvidia.com/blog/smarter-retail-data-analytics-with-gpu-accelerated-apache-spark-workloads-on-google-cloud-dataproc/
A retailer’s supply chain includes the sourcing of raw materials or finished goods from suppliers; storing them in warehouses or distribution centers; and transporting them to stores or customers; managing sales. They also collect, store, and analyze data to optimize supply chain performance. Retailers have teams responsible for managing each stage of the supply chain,…Powered by Discourse, best viewed with JavaScript enabled"
169,accelerated-data-analytics-machine-learning-with-gpu-accelerated-pandas-and-scikit-learn,"Originally published at:			https://developer.nvidia.com/blog/accelerated-data-analytics-machine-learning-with-gpu-accelerated-pandas-and-scikit-learn/
Learn how GPU-accelerated machine learning with cuDF and cuML can drastically speed up your data science pipelines.Powered by Discourse, best viewed with JavaScript enabled"
170,cuda-12-0-compiler-support-for-runtime-lto-using-nvjitlink-library,"Originally published at:			https://developer.nvidia.com/blog/cuda-12-0-compiler-support-for-runtime-lto-using-nvjitlink-library/
CUDA Toolkit 12.0 introduces a new nvJitLink library for Just-in-Time Link Time Optimization (JIT LTO) support.Hi,
some problems have annoyed me,like following statement:
""JIT LTO minimizes the impact on binary size by enabling the cuFFT library to build LTO optimized speed-of-light (SOL) kernels for any parameter combination, at runtime. This is achieved by shipping the building blocks of FFT kernels instead of specialized FFT kernels. ""
can you explain what ”the building blocks of FFT kernels“ means？ThanksThanks for the question.  I am not the FFT developer, but in general what they have done is decompose their algorithm into individual pieces.  Previously the library was very large because they provided all permutations of an algorithm.  Now they just have a handful of building blocks which they can combine into a specific permutation at runtime.  They gave a GTC talk about the work they have done which has some more details.Looking at cuFFTDx library (C++ header only) can give good insight on what can be considered FFT building blocks. Bit more summarized view from another point of view would be SIAM PP22 presentation (slide 10).Links:Powered by Discourse, best viewed with JavaScript enabled"
171,running-docker-containers-directly-on-nvidia-drive-agx-orin,"Originally published at:			https://developer.nvidia.com/blog/running-docker-containers-directly-on-nvidia-drive-agx-orin/
Learn how to run a few sample applications inside a Docker container running on the target NVIDIA DRIVE AGX hardware.https://developer.nvidia.com/blog/running-docker-containers-directly-on-nvidia-drive-agx-orin/
Not Found@weijia.li  – Sorry about that! There was a delay in the release announcement but we’ll get this post back out early next year.Hi, @jwitsoeFriendly ping, but is this feature of running docker containers directly on drive agx orin available for now 2023-03-10 for the sdk version of 6.0.4?Thanks and looking forward to your reply!Hi @lizhensheng,
The feature is available from SDK version 6.0.5 onwards.The feature is available from SDK version 6.0.5 onwards.Thank you for your quick reply!I know about the runtime-container for jetson agx which is l4t-jetpack NVIDIA L4T JetPack | NVIDIA NGC that containing all sdk provided by nvidia in the jetson container.Is there a runtime-container for drive agx products like l4t-jetpack?
Is there any tutorial about building the runtime-container in the drive agx orin that can make use of drive os sdk? (perhaps the name would be v5l-drivesdk)Thanks.Only host side DRIVE OS SDK docker containers with are available as of now. There is no tutorial as of now, as we recommend keeping the development only on the host machine. With that, currently the blog post will help with running sample applications within docker containers on the target.Hey @kchemudupati  , from the blog you sharing, seems we can only run docker container on orin target with runtime mode, is they any possible we can create personal docker image base on the running runtime container, so we can exec it and do some debug? ThanksYou should be able to run the containers without --runtime flag as well. It is used when access to the GPU is required.You should also be able to build your own docker images and run them just like on a host system.build your own docker images and run them just like on a host system.Thanks. What confused us is that we need to run program on aarch64 architecture ,but we cannot find an aarch64 architecture already with DRIVE OS base docker image in https://catalog.ngc.nvidia.com/, and we cannot get the critical deb package since we are develper so we cannot build our personal image?So do you have any good idea about that? Thanks for your favor~Yes, there are currently no aarch64 architecture DRIVE OS base docker images, as we currently recommend the building and the development to be done on the host machine itself.You should be able to mount the targetfs to an aarch64 QEMU environment on host machine to build your image there, or you can flash the target and build your image directly on the target itself. Additionally, as mentioned in the blog post you should be able to edit the two csv files to make any libraries and drivers available inside the container.Hi @kchemudupati, I saw you built the sample code directly on target, how can I do that? I don’t have nvcc on my target.I saw you built the sample code directly on target, how can I do that? I don’t have nvcc on my target.As there is no any compile-debug toolchain in target-orin-kit, the sample code is cross-compiled in the host machine with deb/docker of DriveSDK development environment.@kchemudupati  am I right?Thanks.Yes, there are currently no aarch64 architecture DRIVE OS base docker images, as we currently recommend the building and the development to be done on the host machine itself.@kchemudupatiDo you know any roadmap about this?Thanks.The blogpost showed compiling a small CUDA sample directly on the target for simplicity purposes. The ideal recommended method is to cross-compile on the host.You can run the sample compilation command on the target after flashing the DRIVE AGX Orin with DRIVE OS SDK. You should then be able to find the CUDA toolchain and samples directly on the target at /usr/local/cuda-11.4/ as shown in the blogpost.As there is no any compile-debug toolchain in target-orin-kitI wonder if you agree with the priciples above? @kchemudupatiIf you agree I would have the qst to ensure that any cross-compiled program from host-driveos-docker would running well in the target-orin-docker.Thanks!@kchemudupatiWhat’s the internal mechnisim of /etc/nvidia-container-runtime/host-files-for-container.d/? Is it something like docker run -v  filesystem mapping?Thanks.What’s the internal mechnisim of /etc/nvidia-container-runtime/host-files-for-container.d/? Is it something like docker run -v  filesystem mapping?After searching and reading, I know this is the csv mode of nvidia-container-runtime.@kchemudupatiCould you help me with this topic when I use the target-docker-container with non-root-user?[BUG] target-docker-container running cuda-samples require unintended extra permission - DRIVE AGX Orin / DRIVE AGX Orin General - NVIDIA Developer ForumsThanks!Powered by Discourse, best viewed with JavaScript enabled"
172,introducing-qoda-the-platform-for-hybrid-quantum-classical-computing,"Originally published at:			https://developer.nvidia.com/blog/introducing-qoda-the-platform-for-hybrid-quantum-classical-computing/
NVIDIA introduces QODA, a new platform for hybrid quantum-classical computing, enabling easy programming of integrated CPU, GPU, and QPU systems.Hi, any timeframe, when QODA will be available?Am member of NVIDIA developer program, and when I click “apply for early interest” button on QODA for Hybrid Quantum-Classical Computing | NVIDIA Developer page, it says “membership required”.  So, is there some kind of problem here, or you’re just not ready to accept applications for this program yet?  I’d like to be able at least to browse docs and read some tutorials, if available, even if SDK is not yet released.Thanks.I’m experiencing the same thing too.QODA will be available early next yearThanks for bringing this to our attention. The early access application page should now be working again with your developer login.Powered by Discourse, best viewed with JavaScript enabled"
173,hmm-linux-kernel-support,"Can a full list of the Linux Kernels that support HMM be provided as NVIDIA CUDA Toolkit 12.2 Unleashes Powerful Features for Boosting Applications | NVIDIA Technical Blog only states 6.1.24+ or 6.2.11+ (what about 6.3, 6.4, etc.)? Which executables in the extras/demo suite can be used to test HMM? Also, can the executables that are distributed with the demo suite linked against libglut be linked against libglut.so and not libglut.so.3 so a symbolic link does not have to be added for it such as in /usr/lib/x86_64-linux-gnu? Additionally, why is the “options nvidia NVreg_OpenRmEnableUnsupportedGpus=1” not automatically added by the current driver .run installer to a config file in /etc/modprobe.d/ since it seems to be necessary to get the open kernel driver to work currently per  XFree86/Linux-x86_64/535.86.05/README/kernel_open.html and given that a blacklist file is already being added there for the noveau driver? (The last url should have  us. download nvidia com in front of it, but is being blocked because of some stupid one URL per post as this account is deemed a new user.In general we don’t have a specific list of kernels, but anything after our upstream HMM patch set was applied should work: 6.1.24+, 6.2.11+, or 6.3+. For samples there’s no specific HMM sample, but you can use for example general vector add with malloc() and free() instead of CUDA-specific memory APIs.Regardling libglut.so that shouldn’t be the case, is that in a specific sample? We will investigate and get it fixed.nbody, oceanFFT & randomFogThanks for sharing thatPowered by Discourse, best viewed with JavaScript enabled"
174,deepsearch-license-question,"You mention in the blog that this feature requires DeepSearch with Nucleus. But it seems this only comes with the Enterprise license. Is it somehow possible for a single developer with the standard free license to use this extension?Yes, if you don’t have access to an Omniverse Enterprise Nucleus instance with DeepSearch enabled, you can uncheck the DeepSearch option in the extension and it will place boxes for the results.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
175,evaluating-hidden-costs-when-building-or-buying-an-edge-management-platform,"Originally published at:			https://developer.nvidia.com/blog/evaluating-hidden-costs-when-building-or-buying-an-edge-management-platform/
Edge computing and edge AI are powering the digital transformation of business processes. But, as a growing field, there are still many questions about what exactly needs to be in an edge management platform. The benefits of edge computing include low latency for real-time responses, using local area networks for higher bandwidth, and storage at…Powered by Discourse, best viewed with JavaScript enabled"
176,end-to-end-ai-for-workstation-transitioning-ai-models-with-onnx,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-workstation-transitioning-ai-models-with-onnx/
This post is the second in a series about optimizing end-to-end AI for workstations. For more information, see part 1, End-to-End AI for Workstation: An Introduction, and part 3, End-to-End AI for Workstation: ONNX Runtime and Optimization. In this post, I discuss how to use ONNX to transition your AI models from research to production…Powered by Discourse, best viewed with JavaScript enabled"
177,machine-learning-in-practice-ml-workflows,"Originally published at:			https://developer.nvidia.com/blog/machine-learning-in-practice-ml-workflows/
This series looks at the development and deployment of machine learning (ML) models. This post gives an overview of the ML workflow, considering the stages involved in using machine learning and data science to deliver business value. In part 2, you train an ML model and save that model so it can be deployed as…Powered by Discourse, best viewed with JavaScript enabled"
178,navigating-generative-ai-for-network-admins,"Originally published at:			https://developer.nvidia.com/blog/navigating-generative-ai-for-network-admins/
We all know that AI is changing the world. For network admins, AI can improve day-to-day operations in some amazing ways: Automation of repetitive tasks: This includes monitoring, troubleshooting, and upgrades, saving time while lowering the risk of human errors. Network security: AI can help detect and respond to security threats in real time. For…Powered by Discourse, best viewed with JavaScript enabled"
179,ai-animation-support-sam-walks-up-to-hank-and-slaps-him-on-the-back,"NVIDIA has some features like path animations (get a character to walk along a path). Any plans to have navigation support (walk around objects in room like tables) so you can use ChatGPT or similar to generate an animation clip from text such as “Sam walks up to Hank and slaps him on the back”? Obviously you have to tell it the starting point, but there isE.g. with “digital humans” effort, how far is that likely to go? There were some early videos doing text to animation - is that work continuing?What a great scenario. That sounds like a great extension that someone should make. If you’d like to make it, I think you’d find enthusiastic support in the Omniverse discord channel.There was an old NVIDIA video about generating a single animation clip from multiple movements. Sounds like that is not an active area of development then?Powered by Discourse, best viewed with JavaScript enabled"
180,anyone-can-build-metaverse-applications-with-new-beta-release-of-omniverse,"Originally published at:			Anyone Can Build Metaverse Applications With New Beta Release of Omniverse | NVIDIA Technical Blog
Learn about the new beta release of NVIDIA Omniverse, which includes major updates to core reference applications and tools for developers looking to build metaverse applications.@jwitsoe Sounds interesting, Is there a workflow or some kind of plugin/application with or for Blender and macOs ?Powered by Discourse, best viewed with JavaScript enabled"
181,upcoming-webinar-deep-learning-demystified,"Originally published at:			NVIDIA Emerging Chapters Education Series
Join NVIDIA on December 1 at 3 pm GMT to learn the fundamentals of accelerated data analytics, high-level use cases, and problem-solving methods.Powered by Discourse, best viewed with JavaScript enabled"
182,model-parallelism-virtual-workshop,"Originally published at:			​ - Model Parallelism: Building and Deploying Large Neural Networks (NALA)
Learn to build and deploy large neural networks to production with this virtual workshop on May 3 from the NVIDIA Deep Learning Institute.Powered by Discourse, best viewed with JavaScript enabled"
183,achieving-100x-faster-single-cell-modality-prediction-with-nvidia-rapids-cuml,"Originally published at:			https://developer.nvidia.com/blog/achieving-100x-faster-single-cell-modality-prediction-with-nvidia-rapids-cuml/
Single-cell measurement technologies have advanced rapidly, revolutionizing the life sciences. We have scaled from measuring dozens to millions of cells and from one modality to multiple high dimensional modalities. The vast amounts of information at the level of individual cells present a great opportunity to train machine learning models to help us better understand the…Powered by Discourse, best viewed with JavaScript enabled"
184,how-to-manage-virtual-environments-and-automate-testing-with-tox,"Originally published at:			https://developer.nvidia.com/blog/how-to-manage-virtual-environments-and-automate-testing-with-tox/
Learn how to use tox to automate workflows and manage virtual environments, as well as standardize and automate tests in Python.Powered by Discourse, best viewed with JavaScript enabled"
185,get3d-takes-3d-cad-data-in-training,"Can GET3D takes 3D CAD data in training and generate new 3D CAD models?Can GET3D directly take 3D CAD data for training?
In 3D CAD (computer aided design)  software industry, there are many finished mechanical models. The ideas to use those existing 3D models to training GET3D and then generate new 3D models.Assuming that you have triangulated 3D CAD models in mind, the answer is yes. Get3D could be trained on a dataset of 3D CAD models and then used to generate new ones.Yes, I have triangulated 3D CAD models for training input.
Is there any example/showcase for this application? Thanks.You could follow the examples on how to train Get3D from our Github repository (GitHub - nv-tlabs/GET3D). In principle, you would need to: i) render images of your models in a rendering engine (e.g. Omnvierse or Blender), and then ii) use those images to train Get3D. We provide instructions and example scripts for both in our repository.Do you mean I need to convert 3D CAD models to images and rendering them for GET3D training input? Can I directly feed 3D CAD data (triangulates) to GET3D for training?An example of my use case is like I have a separate 3D car model and a street model. I want to generate a model with the car (or two cars) on the street.Powered by Discourse, best viewed with JavaScript enabled"
186,event-jensen-huang-nvidia-keynote-at-siggraph-2023,"Originally published at:			NVIDIA at SIGGRAPH 2023 Conference
 On Aug. 8, Jensen Huang features new NVIDIA technologies and award-winning research for content creation.Powered by Discourse, best viewed with JavaScript enabled"
187,a-startup-s-guide-to-success-in-central-and-eastern-europe,"Originally published at:			https://developer.nvidia.com/blog/a-startups-guide-to-success-in-central-and-eastern-europe/
Central and Eastern Europe (CEE) is quickly gaining recognition as one of the world’s most important rising technology ecosystems. A highly skilled workforce, government support, proximity to key markets, and a history of entrepreneurship are all factors that have led to a significant increase in funding to the region over the past several years. In…Powered by Discourse, best viewed with JavaScript enabled"
188,boost-ai-development-with-pretrained-models-and-the-nvidia-tao-toolkit,"Originally published at:			https://developer.nvidia.com/blog/boost-ai-development-with-pretrained-models-and-the-nvidia-tao-toolkit/
The latest version of the NVIDIA TAO Toolkit 4.0 boosts developer productivity with all-new AutoML capability, integration with third-party MLOPs services, and new pretrained vision AI models. The enterprise version now includes access to the full source code and model weights for pretrained models.  The toolkit enables efficient model training for vision and conversational AI.…will the output of these models work on the Jetson class devices ? If so what versions of Jetpack are supported ?TAO toolkit models work on the NVIDIA Jetson edge AI platform and are supported all the way up to the latest version of Jetpack 5.0.2.Powered by Discourse, best viewed with JavaScript enabled"
189,advanced-api-performance-pipeline-state-objects,"Originally published at:			Advanced API Performance: Pipeline State Objects | NVIDIA Technical Blog
Pipeline state objects (PSOs) define how input data is interpreted and rendered by the hardware when submitting work to the GPUs. Proper management of PSOs is essential for optimal usage of system resources and smooth gameplay. Recommended: Create PSOs on worker threads asynchronously. PSO creation is where shaders compilation and related stalls happen. Start with…Powered by Discourse, best viewed with JavaScript enabled"
190,new-ebook-a-beginners-guide-to-large-language-models,"Originally published at:			An Enterprises’ Guide to Large Language Models | NVIDIA
Download this free eBook to learn more about LLMs and how they are powering use cases such as chatbots, global translation, and summarization.Powered by Discourse, best viewed with JavaScript enabled"
191,cuda-heterogeneous-memory-management-hmm-support,"How will the CUDA 12.2 HMM impact my A100-based system?A good way to think about HMM is that it’s like UVM, but better. So where UVM gives you the ability to do things like using managed memory to make data migration easier, oversubscribe memory, etc. when you allocate memory through specific UVM APIs like cudaMallocManaged(), HMM gives you all of that with standard malloc() / free(). It makes it so much easier to work with large applications and libraries that might have their own internal memory management, but still use your GPU for acceleration.Powered by Discourse, best viewed with JavaScript enabled"
192,unlocking-speech-ai-technology-for-global-language-users-top-q-amp-as,"Originally published at:			https://developer.nvidia.com/blog/unlocking-speech-ai-technology-for-global-language-users-top-qas/
This post summarizes the top questions asked during Unlocking Speech AI Technology for Global Language Users, a recorded talk from the Speech AI Summit 2022.Powered by Discourse, best viewed with JavaScript enabled"
193,amgx-v1-0-enabling-reservoir-simulation-with-classical-amg,"Originally published at:			https://developer.nvidia.com/blog/amgx-v1-0-enabling-reservoir-simulation-with-classical-amg/
Back in January I wrote a post about the public beta availability of AmgX, a linear solver library for large-scale industrial applications.  Since then, AmgX has grown up!  Now we can solve problems that were impossible for us before, due to the addition of “classical” Algebraic Multi-Grid (often called Ruge-Stueben AMG).  V1.0 comes complete with…Those are very impressive results! Did you use double or single precision when computing the speed up? And what smoother did you use in the plot when comparing to HYPRE? I just implemented a solver for the inhomogeneous poisson equation in my own simulation code using HYPRE. Reading this, it is very tempting to try out AmgX :)These are all double precision results.  SPE10 has such a wider range of coefficient values that single precision misses a lot of the action.  For HYPRE, we used Gauss-Seidel smoothing with 1 pre and post sweep. For AmgX we used a Jacobi variant called Jacobi-L1.  This takes the sum of the absolute value of each entry in each row, and uses the inverse of this as a damping factor. Both solvers used D2 interpolation and a truncated prolongation (4 elements are kept).Joe, could you please share with us the actual timings for build and solve stages for the 5M and 10M SPE10 benchmarks? Also, since these seem to be modified problems compared to the basic SPE10, would you be able to provide the matrix and RHS for our internal benchmarking purposes?Thanks in advance; adrinHello, Sir i am working on CUDA and i want to use AmgX with OpenFOAM but due to lack of resources i am not able to identify feasibility with OpenFOAM. can u suggest me some guidelines.Hello, Sir I’m facing same issue, I want to use AmgX with OpenFOAM , I am also working on Cuda. can u share me some guidelines.Powered by Discourse, best viewed with JavaScript enabled"
194,hello-world-robot-responds-to-human-gestures,"Originally published at:			Hello World! Robot Responds to Human Gestures | NVIDIA Technical Blog
By: Madeleine Waldie, Abhinav Ayalur, Jackson Moffet, and Nikhil Suresh This summer a team of four high school interns, the Neural Ninjas, developed a gesture recognition neural network using Python and C++ to teach a robot to recognize a human wave. Working with robots is familiar territory for them. They’re all members of the FIRST…I’m wondering could this be adapted to react to an ADS (Aim Down Sight) pose? I have an Airsoft arena and I have an idea for a dummy turret that reacts to people ADSing in the waiting area.Powered by Discourse, best viewed with JavaScript enabled"
195,build-high-performance-robotic-applications-with-nvidia-isaac-ros-developer-preview-3,"Originally published at:			https://developer.nvidia.com/blog/build-high-performance-robotic-applications-with-nvidia-isaac-ros-developer-preview-3/
NVIDIA Isaac ROS DP3 includes major updates and enhancements, enabling the ROS community to benefit from hardware acceleration.Powered by Discourse, best viewed with JavaScript enabled"
196,webinar-how-telcos-transform-customer-experiences-with-conversational-ai,"Originally published at:			How Telcos Transform Customer Experiences with Conversational AI
Join Infosys, Quantiphi, Talkmap, and NVIDIA on May 31 for a live webinar to learn how telecommunications companies are using AI to improve operational efficiency and enhance customer engagement.Powered by Discourse, best viewed with JavaScript enabled"
197,improving-gpu-application-performance-with-nvidia-cuda-11-2-device-link-time-optimization,"Originally published at:			https://developer.nvidia.com/blog/improving-gpu-app-performance-with-cuda-11-2-device-lto/
CUDA 11.2 features the powerful link time optimization (LTO) feature for device code in GPU-accelerated applications. Device LTO brings the performance advantages of device code optimization that were only possible in the nvcc whole program compilation mode to the nvcc separate compilation mode, which was introduced in CUDA 5.0.Separate compilation mode allows CUDA device kernel…Figure 2 seems to be wrong, it’s the same as Figure 1. Also it would be nice to get the figures in a higher resolution.@rkobus – Sorry about that! It’s fixed now. Hope the larger size helps as well. Thanks for the feedback!Is the MonteCarlo benchmark in the CUDA 11.2 sample code?No.  It was used for some internal benchmarking.  Unfortunately most of the sample code does not involve separate compilation so are not good tests for LTO.Good question.  We are working on support for JIT LTO, but in 11.2 it is not supported.  So in the example you give at JIT time it will JIT each individual PTX to cubin and then do a cubin link.  This is the same as we have always done for JIT linking.  But we should have more support for JIT LTO in future releases.@mmurphy1 Thanks for the reply - I look forward to seeing more information in the future :)Would you also be able to shed any light on the following: Using device link-time optimization results in much larger fatbinaries@mmurphy1 Are there any reasons that DLTO cannot achieve the same runtime performance as the whole program compilation? Performing DLTO should be able to inline and optimize all functions thus will generate the same code as the whole program compilation, unless the linker does not always inline and optimize the code (since DLTO doesn’t have enough memory to perform the linking?).DLTO should provide the same runtime performance as whole program.  If doing “partial LTO” where some objects were not compiled with -dlto then the scope of optimization will be smaller.but in 11.2 it is not supported.Ok, is it supported in 11.7?JIT LTO is supported as of 11.4, but only as a preview feature.  There will be a change to the interface in 12.0 to better support our compatibility guarantees.Thanks! However, judging by the release notes for 11.4, it looks to be more for manual JIT (i.e. explicitly invoking nvcc), whereas I was thinking more about the “automatic” JIT that the NVIDIA GPU driver performs if a fatbinary doesn’t include SASS for the target GPU arch. Do you know how/if the driver handles DLTO JIT?That is correct, JIT LTO is only supported manually at this time, not as part of the automatic or implicit runtime. JIT linking at the ELF level is supported in the runtime. By default when you compile with -dlto -dc it stores both LTO-IR and PTX in the fatbinary, so if you update your chip it will then do JIT compile and link of the PTX and it will work functionally, but you won’t get the LTO optimization from that. This is something that we may release later, depending on customer feedback.Thanks for the clarification :) Having the driver automatically perform LTO when JIT compiling/linking from PTX/LTO-IR would be a great feature from our point of view, so fingers crossed!Powered by Discourse, best viewed with JavaScript enabled"
198,explainer-what-is-denoising,"Originally published at:			What Is Denoising? | NVIDIA Blog
Denoising is an advanced technique used to decrease grainy spots and discoloration in images while minimizing the loss of quality.Powered by Discourse, best viewed with JavaScript enabled"
199,maximizing-performance-with-massively-parallel-hash-maps-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/maximizing-performance-with-massively-parallel-hash-maps-on-gpus/
Learn the fundamentals of hash maps and how their memory access patterns make them well suited for GPU acceleration.Powered by Discourse, best viewed with JavaScript enabled"
200,optimizing-production-ai-performance-and-efficiency-with-nvidia-ai-enterprise-3-0,"Originally published at:			https://developer.nvidia.com/blog/optimizing-production-ai-performance-and-efficiency-with-nvidia-ai-enterprise-3-0/
Organizations can reduce development time of production AI with the performance and efficiency optimizations in NVIDIA AI Enterprise 3.0.Powered by Discourse, best viewed with JavaScript enabled"
201,upcoming-workshop-model-parallelism-building-and-deploying-large-neural-networks-emea,"Originally published at:			Personal Information - Model Parallelism: Building and Deploying Large Neural Networks (EMEA)
Learn how to train the largest of neural networks and deploy them to production.Powered by Discourse, best viewed with JavaScript enabled"
202,real-time-object-detection-in-10-lines-of-python-on-jetson-nano,"Originally published at:			https://developer.nvidia.com/blog/realtime-object-detection-in-10-lines-of-python-on-jetson-nano/
To help you get up-and-running with deep learning and inference on NVIDIA’s Jetson platform, today we are releasing a new video series named Hello AI World to help you get started.  In the first episode Dustin Franklin, Developer Evangelist on the Jetson team at NVIDIA, shows you how to perform real-time object detection on the…I have ran this on my Jetson Nano!  Awesome!!!  Now I need to run in on a dGPU. What are the libraries that need to be changed?  What else?  Is there an example of doing this project on a dGPU?Powered by Discourse, best viewed with JavaScript enabled"
203,simulating-intelligent-robots-of-the-future-with-nvidia-isaac-sim-2022-2,"Originally published at:			https://developer.nvidia.com/blog/simulating-intelligent-robots-of-the-future-with-isaac-sim-2022-2/
NVIDIA announces the availability of the 2022.2 release of NVIDIA Isaac Sim. As a robotics simulation and synthetic data generation (SDG) tool, this NVIDIA Omniverse application accelerates the development, testing, training, and deployment of intelligent robots. With NVIDIA Isaac Sim, you can easily import the robot model of your choice. Use it to build realistic…Powered by Discourse, best viewed with JavaScript enabled"
204,simplifying-cuda-upgrades-for-nvidia-jetson-users,"Originally published at:			https://developer.nvidia.com/blog/simplifying-cuda-upgrades-for-nvidia-jetson-users/
With CUDA Toolkit 11.8 and NVIDIA JetPack 5.0, you can upgrade to the latest CUDA release without updating NVIDIA JetPack or Jetson Linux BSP software.Powered by Discourse, best viewed with JavaScript enabled"
205,new-nvidia-metropolis-microservices-fast-tracks-cloud-native-vision-ai-development,"Originally published at:			https://developer.nvidia.com/blog/new-nvidia-metropolis-microservices-fast-tracks-cloud-native-vision-ai-development/
Vision AI-powered applications are exploding in terms of value and adoption across industries. They’re being developed both by sophisticated AI developers and those totally new to AI.  Both types of developers are being challenged with more complex solution requirements and faster time to market. Building these vision AI solutions requires a scalable, distributed architecture and…Does Metropolis support on-premise processing?Powered by Discourse, best viewed with JavaScript enabled"
206,upcoming-event-level-up-with-nvidia-dlss-dlaa-and-nvidia-image-scaling-in-unreal-engine-5,"Originally published at:			Level Up with NVIDIA
Join us for the second episode of our webinar series, Level Up with NVIDIA. You learn how to use the latest NVIDIA RTX technology in Unreal Engine 5, followed by a live Q&A session where you can ask NVIDIA experts about your game integrations. Powered by Discourse, best viewed with JavaScript enabled"
207,open-source-fleet-management-tools-for-autonomous-mobile-robots,"Originally published at:			https://developer.nvidia.com/blog/open-source-fleet-management-tools-for-autonomous-mobile-robots/
The newest Isaac ROS release includes new cloud– and edge-to-robot task management and monitoring software for autonomous mobile robot fleets.Powered by Discourse, best viewed with JavaScript enabled"
208,cuda-toolkit-12-0-released-for-general-availability,"Originally published at:			https://developer.nvidia.com/blog/cuda-toolkit-12-0-released-for-general-availability/
NVIDIA announces the newest CUDA Toolkit software release, 12.0. This release is the first major release in many years and it focuses on new programming models and CUDA application acceleration through new hardware capabilities. You can now target architecture-specific features and instructions in the NVIDIA Hopper and NVIDIA Ada Lovelace architectures with CUDA custom code,…Powered by Discourse, best viewed with JavaScript enabled"
209,upcoming-workshop-applications-of-ai-for-anomaly-detection,"Originally published at:			Deep Learning Instructor-led Remote Training | NVIDIA
Learn to detect data abnormalities before they impact your business by using XGBoost, autoencoders, and GANs.Powered by Discourse, best viewed with JavaScript enabled"
210,how-to-create-a-custom-language-model,"Originally published at:			https://developer.nvidia.com/blog/how-to-create-a-custom-language-model/
Large language models are powerful and versatile, yet zero-shot and few-shot prompting techniques may not fully leverage their power. Parameter-efficient customization techniques offer a solution.Powered by Discourse, best viewed with JavaScript enabled"
211,just-released-nvidia-drive-os-6-0-5-now-available,"Originally published at:			https://developer.nvidia.com/drive/downloads#?tx=$product,drive_orin
The latest NVIDIA DRIVE OS release includes customization and safety updates for supercharging autonomous vehicle development.Powered by Discourse, best viewed with JavaScript enabled"
212,upcoming-workshop-computer-vision-for-industrial-inspection,"Originally published at:			Computer Vision for Industrial Inspection Workshop | NVIDIA
Learn how to create an end-to-end hardware-accelerated industrial inspection pipeline to automate defect detection in this workshop on January 18 (CET).Powered by Discourse, best viewed with JavaScript enabled"
213,scaling-large-graphs,"I created an analytic on a small graph, how easy is it to scale my analytic to large graph?At the python layer, cuGraph leverages dask to distribute the work/data accross multiple GPUs. For that transition, one has to setup the dask cluster and the MG API almost matches the SG one. Checkout more here: Multi-GPU with cuGraph — cugraph 23.02.00 documentationThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
214,bootstrapping-object-detection-model-training-with-3d-synthetic-data,"Originally published at:			https://developer.nvidia.com/blog/bootstrapping-object-detection-model-training-with-3d-synthetic-data/
Learn step by step how to use NVIDIA Omniverse to generate your own synthetic dataset. Then fine-tune your computer vision model deployed in NVIDIA Triton for inference.I’m a bit confused about which direction to take with Replicator, the way described in this tutorial or using Replicator Composer.  Composer appears to be a higher level tool but I haven’t been able to find how to set the semantic labels of the items drawn.  I don’t see anything in the Replicator Composer manual related to semantics.Any pointers anyone?Thanks,
DaveHi Dave, thanks for this question! Composer as it is now will be deprecated soon so utilizing a Python script with the Replicator API is how we would suggest you can get started today with generating your semantic labels on your dataset. However, there will be a Composer update coming soon that you might be interested in checking out! Hope that helps.Thanks for the answer.  In the meantime however I got the Replicator Composer (RC) warehouse demo working and was able to swap in my own assets, get annotations, etc.So now that I have two ways of running Replicator, do you still recommend I stick with the Python more manual mode?I am happy to hear that Replicator Composer is working for you. I think it depends on the timeline you are looking at in regard to your projects. Python API is well developed and can be used now which is a big benefit. Composer will see changes in the future which might affect your workflow as it stands now. Up to you which works best for your specific case though!The problem is, and you can see in my other very long thread to tech support, the VS Code debugger is great but it crashes Code after a few iterations.  So I’m stuck with no python debugger.I REALLY REALLY wish you guys would prioritize fixing the stability of Code when using your own suggested extension debugging method.  Not sure where it is on the queue but it’s incredibly painful to have to reboot Code after each try of my Replicator debug session.Something I’ve noticed is the time to render grows exponentially with image number.
image3352×1318 335 KB
The only changes to the code I made were output image size (224x224), output directory and number of frames generated.I doubt exponential growth in render times is NVIDIA’s intention. For example I generated 1000 224x224 images and it takes about 3 minutes for 100 frames.  But took 17 hours for 1000 frames.I am building the synthetic data following this article. However, I was stuck when I was coding the following code:
bbox2d_loose_file_name = “bounding_box_2d_tight_0.npy”
data = np.load(os.path.join(out_dir, bbox2d_tight_file_name))bbox2d_tight_labels_file_name = “bounding_box_2d_tight_labels_0.json”
with open(os.path.join(out_dir, bbox2d_tight_labels_file_name), “r”) as json_data:
bbox2d_loose_id_to_labels = json.load(json_data)colorize_bbox_2d(rgb_path, data, bbox2d_loose_id_to_labels, os.path.join(vis_out_dir, “bbox2d_tight.png”))
The output noticed me that there is no bounding_box_2d_tight_0.npy despite the fact that I already made BasicWritter code before. Can you show me how can I get that .npy file ?Thanks for this question, it looks like you are trying to visualize the data. Do you have any files ending in .npy, or are you only seeing the .json and .png files?Linking to workaround posted on the other blog: Randomizer based Replicator code gets slower every frame - #5 by jlaflecheThat workaround doesn’t work as far as I can tell.  See the end of that other thread.I did find .npy, .json and .png in my created folder. Are they generated by the above codes, right? So .npy and .json shall be link via path to my folder and the .png are, too ?Yes all three types of files should be in the same folder! The BasicWriter code is where the files are generated. The code snippet you included above is actually us labelling and example image with the generated bounding boxes and labels.However, there will be a Composer update coming soonHas this been released?Powered by Discourse, best viewed with JavaScript enabled"
215,ace-r-d-testing-options,"Dear devs,I’m leading a start up that will benefit from the ACE solutions. At the moment we’re doing extensive R&D on other technologies, but we’re on time to chose a different path. Then I’m afraid it would be late to re-code: is there a way to have a talk for doing some testing with your solution?ThanksAre you a part of the Inception Program? If so, you should reach out to your inception account manager. They have resources to help with this kind of request. Join NVIDIA Inception for StartupsReading the requirements we are not fully compliant, and this one sounds weird - as a startupMinimum requirements
[…]
** Your startup needs to be incorporated and have been incorporated in the last 10 years.*Please contact me directly - and I can put you in touch with our Inception team who can adviseOk, done thanksPowered by Discourse, best viewed with JavaScript enabled"
216,new-course-introduction-to-physics-informed-machine-learning-with-modulus,"Originally published at:			Courses – NVIDIA
Learn the basics of physics-informed deep learning and how to use NVIDIA Modulus, the physics machine learning platform, in this self-paced online course.Powered by Discourse, best viewed with JavaScript enabled"
217,nvidia-hopper-architecture-in-depth,"Originally published at:			https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/
Everything you want to know about the new H100 GPU.A question, arose with reading the news, is related to L2 (L1) bandwidth for sustained 3TB/s memory bandwidth on that latency levels of Grace and Hopper architectures.
Are there suggestions or real numbers (from cpu/gpu analysis) for to sort in these high numbers on cache and memory transfer bw (thx)?Any news on this GPU accelerator?Where (in the cloud?) can I test it? More specifically, I would like to test its TEE capabilities. Is there any SDK/documentation available for this GPU?Powered by Discourse, best viewed with JavaScript enabled"
218,best-approach-for-text-to-3d-scene-assembly,"I was trying out ChatGPT to generate scene assembly based on the NVIDIA demo app, but it would often get the height wrong. E.g. putting a TV on a TV stand if I understand the code, it would find a TV stand from the assets then add a TV, but the “add TV” step did not use the height of the TV stand - everything ended up at height zero.Are there best practices or a better way to generate a scene description so the geometry of objects put into a scene are taken into account?There are a few ways to achieve this, and include prompt engineering, result control and pre-object disposition checks. In the demo AI Room Generator we just take the XYZ position of objects and don’t provide specific instructions on how the Y axis should behave. You can be more strict in your prompt engineering about it, for example you could try adding specific behaviour requirements like “If you are placing a tabletop object or an object that usually is on a wall or on top of furniture, make sure the Y axis value is not 0”. Also you can provide a few examples of desired result, for example provide an example where a TV is placed on top of a TV stand. Once you receive the final results, you can also run your check to make sure that the desired objects are correctly placed with the right offsets ahead of placing them in the scene.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
219,driving-5g-era-innovation-with-ai-and-accelerated-computing,"Originally published at:			https://developer.nvidia.com/blog/driving-5g-era-innovation-with-ai-and-accelerated-computing/
Telcos are starting to use AI and accelerated computing to address key industry challenges in the 5G era. NVIDIA accelerated computing and AI platforms are making this possible.Powered by Discourse, best viewed with JavaScript enabled"
220,best-in-class-quantum-circuit-simulation-at-scale-with-nvidia-cuquantum-appliance,"Originally published at:			https://developer.nvidia.com/blog/best-in-class-quantum-circuit-simulation-at-scale-with-nvidia-cuquantum-appliance/
Performance evaluations highlight ultra-fast, full state vector quantum circuit simulations at scale using the NVIDIA cuQuantum Appliance on AIST’s ABCI 2.0 Supercomputer.Powered by Discourse, best viewed with JavaScript enabled"
221,pinterest-sharpens-its-visual-search-skills,"Originally published at:			Pinterest Sharpens its Visual-Search Skills | NVIDIA Technical Blog
The photo-sharing website Pinterest just rolled out a visual search tool that lets you zoom in on a specific object in a pinned image (or “Pin”) and discover visually similar objects, colors, patterns and more. For example, see a lamp that you like in a Pin of a living room? Tap the search tool in…Pinterest, the popular social media platform known for its visual discovery and bookmarking features, has recently improved its visual-search skills through the introduction of several new features. These include a revamped search engine that uses machine learning to better understand search queries and offer more personalized results, as well as a “Complete the Look” feature that suggests similar items to the ones in a user’s search query. Additionally, Pinterest has implemented a “Lens Your Look” feature that allows users to take a photo of an outfit and receive recommendations for similar items. These updates demonstrate Pinterest’s commitment to enhancing its visual-search capabilities and providing users with a more seamless and enjoyable experience.Powered by Discourse, best viewed with JavaScript enabled"
222,end-to-end-ai-for-nvidia-based-pcs-onnx-and-directml,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-nvidia-based-pcs-onnx-and-directml/
This post is part of a series about optimizing end-to-end AI. While NVIDIA hardware can process the individual operations that constitute a neural network incredibly fast, it is important to ensure that you are using the tools correctly. Using the respective tools such as ONNX Runtime or TensorRT out of the box with ONNX usually…Powered by Discourse, best viewed with JavaScript enabled"
223,explainer-what-is-accelerated-computing,"Originally published at:			What Is Accelerated Computing? | NVIDIA Blog
Accelerated computing uses parallel processing to speed up work on demanding applications, from AI and data analytics to simulations and visualizations.Powered by Discourse, best viewed with JavaScript enabled"
224,just-released-nvidia-hpc-sdk-v23-5,"Originally published at:			Release Notes Version 23.5
This update expands platform support and provides minor updates.Powered by Discourse, best viewed with JavaScript enabled"
225,new-nvidia-maxine-microservices-enhance-real-time-audio-and-video-effects-for-conferences,"Originally published at:			https://developer.nvidia.com/blog/new-maxine-microservices-enhance-real-time-audio-and-video-effects-for-video-conferences-at-scale/
At CES 2023, NVIDIA Maxine announced SDK updates and new microservices, enabling clear communications in video conferences through private or public clouds. NVIDIA Maxine is a suite of GPU-accelerated AI SDKs and cloud-native microservices for deploying optimized and accelerated AI features that enhance audio, video, and augmented reality (AR) effects for real-time communications. Powered by…Powered by Discourse, best viewed with JavaScript enabled"
226,fast-3d-reconstruction-using-get3d,"Hey there. I was wondering if GET3D can be readily used for 3D reconstruction using slices or projections from medical imaging (DICOM, tiff, or NIFTI files)? If so, would it be available to students for free? Thanks.Thanks for the question! Currently we do not support shape generation conditioned on such information due to time constraint.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
227,new-video-series-what-developers-need-to-know-about-universal-scene-description,"Originally published at:			https://developer.nvidia.com/blog/new-video-series-what-developers-need-to-know-about-universal-scene-description/
Universal Scene Description (OpenUSD) is an open and extensible framework for creating, editing, querying, rendering, collaborating, and simulating within 3D worlds. Invented by Pixar Animation Studios, USD is much more than a file format. It is an ecosystem and interchange paradigm that models, labels, classifies, and combines a wide range of data sources into a…Powered by Discourse, best viewed with JavaScript enabled"
228,enabling-dynamic-control-flow-in-cuda-graphs-with-device-graph-launch,"Originally published at:			https://developer.nvidia.com/blog/enabling-dynamic-control-flow-in-cuda-graphs-with-device-graph-launch/
CUDA device graph launch offers a performant way to enable dynamic control flow within CUDA kernels.Hi, from the article it’s not clear but I suppose we cannot update the graph on device, am I right?
I mean, changing parameters like the number of threads with cudaGraphExecKernelNodeSetParams() or enable/device kernels with cudaGraphNodeSetEnabled() for exampleFrom the CUDA toolkit documentation the “cudaGraphLaunch” is still tagged as ""__host__​ "", is it a mistake? It should be possible to use that function on device nowLast question is about cudaGraphLaunch performances, with “graph length=100” do you mean you’re testing a graph with 100kernels or a graph with 100 sequential kernels?
I mean, if I have 10 straight lines with 10 sequential kernels… it’s a “graph length” equal to 100 or 10?
ThanksYou cannot update the graph from the device, that is correct. Updates can only be performed from the host (this applies both to parameter updates as well as node enable/disable), and the graph must also be re-uploaded for the changes to take effect in subsequent device launches. We are thinking of adding device-side update functionality in a future release, though.Yes, cudaGraphLaunch can be called from both the host and device. That does indeed appear to be a documentation error, thanks for bringing it to our attention! We’ll work on getting that fixed.The length is the sequential length, not the general size. For the single-entry parallel straight-line and straight-line graphs, that means that the straight-line sections are 100 sequential kernels each; and for the fork join case, it is 100 sequential iterations of forking out into 2 nodes and then joining back into a single node.I hope that answers all your questions.Powered by Discourse, best viewed with JavaScript enabled"
229,how-to-deploy-an-ai-model-in-python-with-pytriton,"Originally published at:			https://developer.nvidia.com/blog/how-to-deploy-an-ai-model-in-python-with-pytriton/
Learn how to use NVIDIA Triton Inference Server to serve models within your Python code and environment using the new PyTriton interface.Powered by Discourse, best viewed with JavaScript enabled"
230,ask-me-anything-nvidia-cuda-toolkit-12,"Originally published at:			AddEvent
On July 26, connect with CUDA product team experts on the latest NVIDIA CUDA Toolkit 12.Powered by Discourse, best viewed with JavaScript enabled"
231,just-released-cuda-toolkit-12-1,"Originally published at:			https://developer.nvidia.com/blog/just-released-cuda-toolkit-12-1/
Available now for download, the CUDA Toolkit 12.1 release provides support for NVIDIA Hopper and NVIDIA Ada Lovelace architecture.Powered by Discourse, best viewed with JavaScript enabled"
232,deep-learning-is-transforming-asr-and-tts-algorithms,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-is-transforming-asr-and-tts-algorithms/
Speech is one of the primary means to communicate with an AI-powered application. From virtual assistants to digital avatars, voice-based interfaces are changing how we typically interact with smart devices. Deep learning techniques for speech recognition and speech synthesis are helping improve the user experience—think human-like responses and natural-sounding tones. If you plan to build…Powered by Discourse, best viewed with JavaScript enabled"
233,post-your-questions-in-this-category,"You are at the right place to post questions for the AMAPost your questions to this category - NOT as a reply to this posting - thanksGoodnight.
I would like to know if it is possible to animate the mouth of a realistic photo and synchronize it with the output of an audio in real time, as in a conversation.
I also wanted to know how I would be charged for this functionality.
Thank you very much.Yes, we have a product called Maxine. One of the feature is Live portrait. We have the ability to use video and audio to animate the 2D portrait. See more here NVIDIA Maxine | NVIDIA DeveloperPowered by Discourse, best viewed with JavaScript enabled"
234,enhancing-architectural-3d-modeling-collaboration-with-universal-scene-description,"Originally published at:			https://developer.nvidia.com/blog/enhancing-architectural-3d-modeling-collaboration-with-universal-scene-description/
Learn about the possibilities and benefits of using USD with NVIDIA Omniverse for architecture, design, engineering, and construction projects.Powered by Discourse, best viewed with JavaScript enabled"
235,cugraph-and-entity-alignment-models,"Hello, I wanted to know if implementation of some entity alignment models on graphs e.g. RDGCN are available on cuGraph?We do not support entity alignment models as of now, but we have been gradually adding most requested models from users/customers.Thank you so much. I hope it happens very soon.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
236,build-generative-ai-pipelines-for-drug-discovery-with-nvidia-bionemo-service,"Originally published at:			https://developer.nvidia.com/blog/build-generative-ai-pipelines-for-drug-discovery-with-bionemo-service/
Creating new drug candidates is a heroic endeavor, often taking over 10 years to bring a drug to market. New supercomputing-scale large language models (LLMs) that understand biology and chemistry text are helping scientists understand proteins, small molecules, DNA, and biomedical text. These state-of-the-art AI models help generate de novo proteins and molecules and predict…Powered by Discourse, best viewed with JavaScript enabled"
237,mobile-app-helps-find-furniture-you-see-in-the-real-world,"Originally published at:			Mobile App Helps Find Furniture You See in the Real World | NVIDIA Technical Blog
Take a photo of a chair and a new mobile app will tell you where to buy it, and show you pictures of how it will look in various rooms. “It seems a lot of people want to buy things they see in someone else’s home or in a photo, but they don’t know where…I’m excited to see how this app develops and how it can help make the furniture-buying process easier for people.Powered by Discourse, best viewed with JavaScript enabled"
238,reinforcing-the-value-of-simulation-by-teaching-dexterity-to-a-real-robot-hand,"Originally published at:			https://developer.nvidia.com/blog/reinforcing-the-value-of-simulation-by-teaching-dexterity-to-a-real-robot-hand/
The human hand is one of the most remarkable outcomes of millions of years of evolution. The ability to pick up all sorts of objects and use them as tools is a crucial differentiator enabling us to shape our world. For robots to work in the everyday human world, the ability to deftly interact with…What a wonderful work! (“What a time to be alive!” ;-)Now the proof is made that the training of humanoid robotic hand in simulation (and sim2real) can be carried out on a few GPUs and are no longer reserved for a few happy people who have access to unlimited funds …The gap between the training time, with and without randomization (for sim2real), seems to be in par (proportionally) with the previous work (cf. OpenAI).So, if I understood correctly, we should be able to train / try various tasks in fast simulations on a single GPU (as, say: 1-6h / A100 [1], and even 2h / RTX 3090 ! [2]) and then, when we are satisfied, we can train them with randomization with a more powerful workstation.The article talks about 1.5/2.5 days on AWS g5.48xlarge (8 x A10G GPU) so if things scale linearly it should be possible to train with randomization (sim2real) on a “simple” workstation with 4 x RTX 3080Ti/3090 within 3/5 days… Amazing!Also, maybe we will see the emergence of a kind of “transfer learning” for humanoid robotic hand…[1] https://arxiv.org/pdf/2108.10470.pdf , “Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning”
[2] ttps://sites.google.com/view/isaacgymIndeed that’s right, and this is one of the reasons we are so pleased to be able to help democratize this capability.You can see a demo here of sim2real transfer of a simpler problem (quadruped locomotion) that’s fast enough to be run live on stage in a handful of minutes: Keynote speech Marco Hutter ICRA 2022 - YouTubeGlad you enjoyed our post!Take care,
-GavThanks for the feedback and the interesting link.Definitely, being able to see the progress of training directly on the robot in “almost” real time is very appreciable.
On this point, I suppose that it is also a question of computing power that you can put. But it should be feasible even for more complex tasks. At least it deserves to be tested.keep up the good work!-)
I’m looking forward to your code …
Best wishes for the holiday season!Powered by Discourse, best viewed with JavaScript enabled"
239,train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-run-ai,"Originally published at:			https://developer.nvidia.com/blog/train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-runai/
Organizations are increasingly adopting hybrid and multi-cloud strategies to access the latest compute resources, consistently support worldwide customers, and optimize cost. However, a major challenge that engineering teams face is operationalizing AI applications across different platforms as the stack changes. This requires MLOps teams to familiarize themselves with different environments and developers to customize applications…Powered by Discourse, best viewed with JavaScript enabled"
240,year-in-review-trending-posts-of-2022,"Originally published at:			https://developer.nvidia.com/blog/year-in-review-trending-posts-of-2022/
Marking a year of new and evolving technologies, 2022 produced wide-ranging advancements and AI-powered solutions across industries. These include boosting HPC and AI workload power, research breakthroughs, and new capabilities in 3D graphics, gaming, simulation, robotics, and more. In a record-breaking year, the NVIDIA Technical Blog published nearly 550 posts and received over 2 million…Powered by Discourse, best viewed with JavaScript enabled"
241,improve-accuracy-and-robustness-of-vision-ai-apps-with-vision-transformers-and-nvidia-tao,"Originally published at:			https://developer.nvidia.com/blog/improve-accuracy-and-robustness-of-vision-ai-apps-with-vision-transformers-and-nvidia-tao/
Vision Transformers (ViTs) are taking computer vision by storm, offering incredible accuracy, robust solutions for challenging real-world scenarios, and improved generalizability. The algorithms are playing a pivotal role in boosting computer vision applications and NVIDIA is making it easy to integrate ViTs into your applications using NVIDIA TAO Toolkit and NVIDIA L4 GPUs. How ViTs…Powered by Discourse, best viewed with JavaScript enabled"
242,explainer-what-is-explainable-ai,"Originally published at:			What Is Explainable AI (XAI)? | NVIDIA Blog
Our trust in AI will largely depend on how well we understand it — explainable AI, or XAI, helps shine a flashlight into the “black box” of complexity in AI models.Powered by Discourse, best viewed with JavaScript enabled"
243,tips-on-scaling-storage-for-ai-training-and-inferencing,"Originally published at:			https://developer.nvidia.com/blog/tips-on-scaling-storage-for-ai-training-and-inferencing/
There are many benefits of GPUs in scaling AI, ranging from faster model training to GPU-accelerated fraud detection. While planning AI models and deployed apps, scalability challenges—especially performance and storage—must be accounted for.  Regardless of the use case, AI solutions have four elements in common:  Training model Inferencing app Data storage  Accelerated compute  Of these elements, data storage…While writing this blog, I spoke with AI solutions creators and IT professionals. As a result, I learned of several important factors that are not always considered in deployments. This includes storage scalability, availability, and adaptability as these are not always fully evaluated. Additionally, I learned that even a well-designed POC does not necessarily address future adaptability and challenges. For those fully aware of the points made in this blog, I’m hopeful this will serve as a good checklist for future reference. For others, it’s my hope this will cause existing plans to be re-evaluated in the light of storage scalability for training and inference. I’d love to hear your comments and/or questions!Powered by Discourse, best viewed with JavaScript enabled"
244,icymi-exploring-challenges-posed-by-biased-datasets-using-rapids-cudf,"Originally published at:			There and Back Again… a RAPIDS Tale - KDnuggets
Read about an innovative GPU solution that solves limitations posed by small biased datasets using RAPIDS cuDF.Powered by Discourse, best viewed with JavaScript enabled"
245,animated-models,"Hey there!
It is probably a difficult feat, but given tools s.a. OpenPose for humanoid body poses - are you envisioning this featuring auto-rigging or even AI-made animations coming along with the models themselves? Even auto-rigging would probably be immensely valuable, given that animations for i.e. dogs could be applied to any AI-generated type of dog!Cheers,
FlorianYes, enabling the rigging is a great feature to support in the future, as it can enabling the animation. It’s possible to combine the human/animal rigging model (e.g. SMPL model) with GET3D, such that the generated 3D human body can have skinning weights for animation, we wold be interested to add in the future!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
246,explainer-what-is-a-machine-learning-model,"Originally published at:			What Is a Machine Learning Model? | NVIDIA Blogs
Fueled by data, ML models are the mathematical engines of AI, expressions of algorithms that find patterns and make predictions faster than a human can.Powered by Discourse, best viewed with JavaScript enabled"
247,how-nerfs-helped-me-re-imagine-the-world,"Originally published at:			https://developer.nvidia.com/blog/how-nerfs-helped-me-re-imagine-the-world/
Why have images not evolved past two dimensions yet? Why are we satisfied with century-old technology? What if the technology already exists that’s ready to push the content world forward and only requires a camera? That technology is neural radiance fields (NeRFs). Although this technology was created only a few years ago (2020), the pace…Powered by Discourse, best viewed with JavaScript enabled"
248,explainer-what-is-active-learning,"Originally published at:			How Can Active Learning Help Train Autonomous Vehicles? | NVIDIA Blog
Finding the right self-driving training data doesn’t have to take a swarm of human labelers.Powered by Discourse, best viewed with JavaScript enabled"
249,apache-airflow-for-authoring-workflows-in-nvidia-base-command-platform,"Originally published at:			Apache Airflow for Authoring Workflows in NVIDIA Base Command Platform | NVIDIA Technical Blog
Integrating Apache Airflow with an AI platform such as NVIDIA Base Command, which leverages GPU acceleration, streamlines the process of training and deploying AI models.Powered by Discourse, best viewed with JavaScript enabled"
250,the-live-ama-is-now-finished,"Thanks for all the great questions.
We still have some responses we will post.
We welcome more questions but the responses will be be offline.
Thanks again .Powered by Discourse, best viewed with JavaScript enabled"
251,top-cybersecurity-sessions-at-nvidia-gtc-2023,"Originally published at:			Cybersecurity Conference Sessions | GTC 2023 Spring | NVIDIA
Learn how AI is improving your cybersecurity to detect threats faster.Powered by Discourse, best viewed with JavaScript enabled"
252,what-is-a-smart-hospital,"Originally published at:			What Is a Smart Hospital? | NVIDIA Blog
A smart hospital relies on data-driven insights, including machine learning models and AI-powered medical devices, to facilitate decision-making.Powered by Discourse, best viewed with JavaScript enabled"
253,top-hpc-sessions-at-nvidia-gtc-2023,"Originally published at:			HPC Conference Session Catalog | NVIDIA GTC
From climate modeling to quantum computing, large language models to molecular dynamics; see how HPC is transforming the world.Powered by Discourse, best viewed with JavaScript enabled"
254,machine-learning-in-practice-deploy-an-ml-model-on-google-cloud-platform,"Originally published at:			https://developer.nvidia.com/blog/machine-learning-in-practice-deploy-an-ml-model-on-google-cloud-platform/
This series looks at the development and deployment of machine learning (ML) models. In this post, you deploy ML models on Google Cloud Platform. Part 1 gave an overview of the ML workflow, considering the stages involved in using machine learning and data science to deliver business value. In part 2, you trained an ML…Powered by Discourse, best viewed with JavaScript enabled"
255,gpu-pro-tip-cuda-7-streams-simplify-concurrency,"My CUDA version is 7.5, I am using Visual Studio 2013, device is GTX 850M but when I run the program, nvvp shows no timeline. What can be the problem ?And when I run nvprof, it says :==8128== NVPROF is profiling process 8128, command: CudaTest.exe==8128== Profiling application: CudaTest.exe==8128== Profiling result:No kernels were profiled.==8128== API calls:No API activities were profiled.I test the first one code on my PC, in the linux, it works fine. But in the windows, with VS2013 the stream doesn't run concurrently, with the ""default-stream per-thread"" flag. Also, I compile the code in the command-line ""nvcc –default-stream per-thread ./stream_test.cu -o stream_per-thread"". It doesn't work concurrently too.dear Mr. Harris,I have try out your instructions above and then I come an idea like this.void threadExecute(void *input_data, int nx){cufftComplex *data = (cufftComplex*)input_data;cufftHandle plan;cufftPlan1d(&plan, nx, CUFFT_C2C, 1);cufftSetStream(plan, this_stream);cufftExecC2C(plan, data, data, CUFFT_FORWARD);cufftDestroy(plan);cudaStreamSynchronize(0);}As I understand, each CPU thread will be given it own stream on GPU. Does it correct ?If it is correct how can I get the stream that is assigned to each CPU thread so that I can pass it to the cufftSetStream().If it is not correct so how can I use cufft API with multiple CPU thread and multiple stream?Could you please help me with this?I will be very appreciate.See my answer above regarding the NPP library, which is similar. If you follow the instructions in the post and compile your code to use ""-default-stream per-thread"", you should be able to pass cudaStreamPerThread to cufftSetStream() so that it uses the default stream in each thread. Does this work?Hello Mark,I understand that as you mentioned, Enabling PTDS for your compilation units doesn't enable it for libraries that are separately compiled.But I wish to enable PTDS for thrust libraryHow to I call thrustSetStream() Thank you.To set a stream for a Thrust algorithm you need to use the .on() method on the cuda::par execution policy, like so:    thrust::sort(thrust:cuda::par.on(stream), begin, end, comparator)Hi Mark,The example above works perfectly in my ubuntu system as well.I could even obtained Figure 2 shown above using the commandnvcc --default-stream per-threadHowever, when I try to do the same thing in the Nsight editor, I do not see the effect of  --default-stream per-thread command.I have written the command "" --default-stream per-thread"" on the Command box  as ""nvcc --default-stream per-thread"" on the project properties -> settings -> Tool Settings in the NVCC Compiler.I suspect if this is the correct place to put this flag. I have even tried putting it on the Build Stages -> Preprocessor options (-Xcompiler) but that too did not work.Could you please guide me where should I put this command on the Nsight Editor.Thanks and Warm RegardsAmit GurungHi Amit,Try adding the flag in Project Properties -> Settings -> Tool Settings -> NVCC Compiler -> Expert Setting:1.  ${COMMAND} --default-stream per-thread ${FLAGS} ${OUTPUT_FLAG} ${OUTPUT_PREFIX} ${OUTPUT} ${INPUTS}Thanks a lot Mark, it is indeed an Expert advice. It worked fine.Thanks again I hope this answer will be very beneficial to all.Hi, Mark,Could you please have a look at this profile output generated using the nvprof command and comment why I am not able to see the concurrency in the kernel executions (just like the above Figure 2 or Figure 4, kernels one below the others) note that I do not even have any kernel in the Default stream, i.e., I only have kernels using 10 streams, I have also used the --default-stream per-thread flag as per your instruction (which works fine for the example that you have posted above).The profile output link ( https://drive.google.com/op... )Thanks a lot in advance.I can't access that link.I have made it public now, anyone can download it.Thanks once again.That file doesn't have an extension. I have no idea what format it is. I can't risk opening it. Suggest you post a screenshot.Sorry for the file with no extension.I have the file now with the extension .nvprof as shown in one of your post for output of nvprof. Kindly find the new link (  https://drive.google.com/op...   )I would have posted the screenshot but my profile output does not fit in a single screen and if I zoom out then it may not make any sense.I hope you can access the output file now.Thank you once again.I have edited the typo with the above link, I hope you can access it now.Hi Mark, I have a question about the multi-threading example.Each of 8 threads calls the kernel but have all the 8 threads access to the same global memory or each thread works to its own global memory (of its kernel)?In the multi-threaded example, each of the 8 threads calls launch_kernel, which allocates memory with cudaMalloc. So in this case all threads access different regions of memory.Thanks for the answer! But it is poissible to allocate once (in the global memory) and then all the other threads access this region of memory every time they call the kernel? So they can read the same data?Yes i think that should work.We can see your code every kernel just occupy one grid block: kernel<<<1, 64>>>(data, N);, so every kernel execution should fully occupy GPU resouce, however, we can see the timeline ,when concurrency comes. every kernel execution time would be much longer than serialization condition. that is why？Powered by Discourse, best viewed with JavaScript enabled"
256,top-mlops-sessions-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/top-mlops-sessions-at-nvidia-gtc-2023/
Discover how to build a robust MLOps practice for continuous delivery and automated deployment of AI workloads at scale.Powered by Discourse, best viewed with JavaScript enabled"
257,nvidia-ai-inference-performance-milestones-delivering-leading-throughput-latency-and-efficiency,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ai-inference-performance-milestones-delivering-leading-throughput-latency-and-efficiency/
Inference is where AI-based applications really go to work. Object recognition, image classification, natural language processing, and recommendation engines are but a few of the growing number of applications made smarter by AI. Recently, TensorRT 5, the latest version of NVIDIA’s inference optimizer and runtime,  became available. This version brings new features including support for…Powered by Discourse, best viewed with JavaScript enabled"
258,achieving-supercomputing-scale-quantum-circuit-simulation-with-the-nvidia-dgx-cuquantum-appliance,"Originally published at:			Achieving Supercomputing-Scale Quantum Circuit Simulation with the NVIDIA DGX cuQuantum Appliance | NVIDIA Technical Blog
Scale quantum circuit simulations on the largest supercomputers with the DGX cuQuantum Appliance.Powered by Discourse, best viewed with JavaScript enabled"
259,does-cugraph-support-graph-convolutional-networks-gcns,"I’d like to know whether cuGraph supports Graph Convolutional Networks (GCNs).Yes, cuGraph support GNN, we have GPU sampling methods and cuGraph-ops also support message passing/aggregation functions as well.
We provide accelerated GNN conv layers via a separate package called pylibcugraphops (cugraph-ops). So far, we support GraphSAGE, GAT and RGCN models.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
260,cuda-team-will-be-live-at-9am,"Bring your questions about CUDA 12Powered by Discourse, best viewed with JavaScript enabled"
261,just-released-nvidia-modulus-v23-05,"Originally published at:			Modulus Release Notes - NVIDIA Docs
This version 23.05 update to the NVIDIA Modulus platform expands support for physics-ML and provides minor updates.Powered by Discourse, best viewed with JavaScript enabled"
262,ai-classifies-galaxies-using-hubble-space-telescope-images,"Originally published at:			https://developer.nvidia.com/blog/ai-classifies-galaxies-using-hubble-space-telescope-images/
A new study published in the Astrophysical Journal this week describes how a team of researchers from all over the globe developed a deep learning system that can classify galaxies with superb accuracy.  Using NVIDIA TITAN Xp GPUs and the cuDNN-accelerated Keras and Theano deep learning frameworks, the team, made up of researchers from institutions…Yes, it is possible to classify galaxies using Hubble Space Telescope images with the help of artificial intelligence (AI). AI algorithms can be trained to recognize and classify different types of galaxies based on their visual characteristics, such as their shape, color, and texture. This can be done using machine learning techniques such as neural networks, which are designed to recognize patterns in large amounts of data.In fact, the Hubble Space Telescope has been used in several studies to classify galaxies using AI. For example, in 2019, a team of astronomers used deep learning techniques to analyze Hubble images of galaxies and classify them into different categories based on their morphologies. The results of this study demonstrated the potential of AI in helping to automate the analysis of large datasets and improve our understanding of the universe.Powered by Discourse, best viewed with JavaScript enabled"
263,gpu-accelerated-single-cell-rna-analysis-with-rapids-singlecell,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-single-cell-rna-analysis-with-rapids-singlecell/
RAPIDS-singlecell is a GPU-accelerated tool for scRNA analysis that offers seamless scverse compatibility for efficient single-cell data processing and analysis.Powered by Discourse, best viewed with JavaScript enabled"
264,explainer-what-is-zero-trust,"Originally published at:			What Is Zero Trust? | NVIDIA Blogs
Zero trust is a cybersecurity strategy for verifying every user, device, application, and transaction in the belief that no user or process should be trusted.Powered by Discourse, best viewed with JavaScript enabled"
265,explainer-what-is-confidential-computing,"Originally published at:			What Is Confidential Computing? | NVIDIA Blogs
Confidential computing is a way of processing data in a protected zone of a computer’s processor, often inside a remote edge or public cloud server, and proving that no one viewed or altered the work.Powered by Discourse, best viewed with JavaScript enabled"
266,end-to-end-ai-for-nvidia-based-pcs-optimizing-ai-by-transitioning-from-fp32-to-fp16,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-nvidia-based-pcs-optimizing-ai-by-transitioning-from-fp32-to-fp16/
This post is part of a series about optimizing end-to-end AI. The performance of AI models is heavily influenced by the precision of the computational resources being used. Lower precision can lead to faster processing speeds and reduced memory usage, while higher precision can contribute to more accurate results. Finding the right balance between precision…Powered by Discourse, best viewed with JavaScript enabled"
267,visual-foundation-models-for-medical-image-analysis,"Originally published at:			https://developer.nvidia.com/blog/visual-foundation-models-for-medical-image-analysis/
The analysis of 3D medical images is crucial for advancing clinical responses, disease tracking, and overall patient survival. Deep learning models form the backbone of modern 3D medical representation learning, enabling precise spatial context measurements that are essential for clinical decision-making. These 3D representations are highly sensitive to the physiological properties of medical imaging data,…Powered by Discourse, best viewed with JavaScript enabled"
268,uvm-support,"Does cuGraph have UVM support?Yes, through RMM (rapids memory manager) we support use of managed memory for both single-GPU and multi-GPU workflows. Obviously, performance with managed memory will not be as high as performance with device memory. In addition, cuDF has a spilling feature that can be very helpful when working with RMM pools, which are pre-allocated chunks of memory. And finally, for multi-GPU workflows, dask has its own spilling features that can be configured. There is also additional research going on for further expanding use of host memory in GPU workflows.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
269,applying-federated-learning-to-traditional-machine-learning-methods,"Originally published at:			https://developer.nvidia.com/blog/applying-federated-learning-to-traditional-machine-learning-methods/
In the era of big data and distributed computing, traditional approaches to machine learning (ML) face a significant challenge: how to train models collaboratively when data is decentralized across multiple devices or silos. This is where federated learning comes into play, offering a promising solution that decouples model training from direct access to raw training…Powered by Discourse, best viewed with JavaScript enabled"
270,thanks-for-joining-the-cuda-12-ama-is-now-closed,"If you do have questions please do post on our general Accelerated Computing Forums where the community and NVIDIA technical staff do engage and answer questions.Thanks again for participating in this AMA.Powered by Discourse, best viewed with JavaScript enabled"
271,rtx3060-along-side-quadro-p1000-for-machine-learning-in-ms-windows-11,"I just purchased an Nvidia RTX3060 to use or machine learning and artificial intelligence. I couldn’t get my freebie legacy Tesla K40c to be detected for a MS Windows install with Anaconda and WSL2 confguration for machine learning.My question to the development team is, is my RTX3060 fully compatible with the latest Nvidia GPU driver, CUDA Toolkit, CuDNN, Nvidia Control Panel, and Anaconda for machine learning development with Python and Jupyter Notebooks? Will it be detected by Python?If you have a cookbook to setup and configure my RTX3060 GPU in MS Windows 11 then I’ll be sure to follow it fully because it’s very painful to setup in Windows to get it working with my Tesla K40c headless GPU.Thanks and I really enjoy using Nvidia products like the Jetson embedded solutions and own the stock.Hello! Yes, this combination (RTX 3060) is fully supported in CUDA, WSL2, cuDNN, etc. I use an RTX 3090 on one of my personal systems for similar. However, your other GPU is a Kepler architecture GPU which is deprecated and won’t be recognized by modern drivers, so you wouldn’t be able to use that one with modern CUDA. The last driver release that supported it was R470, from back in 2021.Thank you so very much for your detailed reply and clarification to my question. I’m so glad I participated in today’s AMA and I am more confident now to get everything fully working later this week when I have time long enough to configure it and test it with your answer. :-)Thanks for participating. Feel free to take the opportunity to post new questions.Powered by Discourse, best viewed with JavaScript enabled"
272,why-automatic-augmentation-matters,"Originally published at:			https://developer.nvidia.com/blog/why-automatic-augmentation-matters/
Deep learning models require hundreds of gigabytes of data to generalize well on unseen samples. Data augmentation helps by increasing the variability of examples in datasets. The traditional approach to data augmentation dates to statistical learning when the choice of augmentation relied on the domain knowledge, skill, and intuition of the engineers that set up…Powered by Discourse, best viewed with JavaScript enabled"
273,upcoming-event-accelerate-yolov5-and-custom-ai-models-in-ros-with-nvidia-isaac,"Originally published at:			Isaac ROS Webinar Series
Learn about the NVIDIA Isaac ROS DNN Inference pipeline and how to use your own models with a YOLOv5 example in this December 1 webinar.Powered by Discourse, best viewed with JavaScript enabled"
274,decentralizing-ai-with-a-liquid-cooled-development-platform-by-supermicro-and-nvidia,"Originally published at:			https://developer.nvidia.com/blog/decentralizing-ai-with-a-liquid-cooled-development-platform-by-supermicro-and-nvidia/
AI is the topic of conversation around the world in 2023. It is rapidly being adopted by all industries including media, entertainment, and broadcasting. To be successful in 2023 and beyond, companies and agencies must embrace and deploy AI more rapidly than ever before. The capabilities of new AI programs like video analytics, ChatGPT, recommenders,…Powered by Discourse, best viewed with JavaScript enabled"
275,using-carbon-capture-and-storage-digital-twins-for-net-zero-strategies,"Originally published at:			https://developer.nvidia.com/blog/using-carbon-capture-and-storage-digital-twins-for-net-zero-strategies/
CO2 capture and storage technologies (CCS) catch CO2 from its production source, compress it, transport it through pipelines or by ships, and store it underground. CCS enables industries to massively reduce their CO2 emissions and is a powerful tool to help industrial manufacturers achieve net-zero goals. In many heavy industrial processes, greenhouse gas (GHG) emissions…An admirable concept, but the problem is creating a digital twin of an environment that is unobservable as well as being subject to numerous temporal variables that interact dynamically based on chemical, physical and geological processes.  It is an impossible task.Powered by Discourse, best viewed with JavaScript enabled"
276,developing-and-deploying-your-custom-action-recognition-application-without-any-ai-expertise-using-nvidia-tao-and-nvidia-deepstream,"Originally published at:			https://developer.nvidia.com/blog/developing-and-deploying-your-custom-action-recognition-application-without-any-ai-expertise-using-tao-and-deepstream/
Build an action recognition app with pretrained models, the TAO Toolkit, and DeepStream without large training data sets or deep AI expertise.Hello,
I was trying to recreate the pre-trained model. I trained the net with the same 5 classes  (ride bike, walk, fall floor, run and push) and the default hyperparameters. Then I use this model as a pre-trained model and train again with different classes. However, the accuracy and F1 I get with my recreated model don’t even come close to the ones I get with the downloaded pre-trained model. I thought this could be cause the hyperparameters used to train the model were different from the ones I used, but I couldn’t find any information on how the pre-trained model was trained.
I would highly appreciate if anyone could provide me with this information. The hyperparameters I am referring to are the ones in the train_rgb_3d_finetune.yaml file, such as the learning rate,  epochs, momentum,  weight_decay, dropout ratio, steps, batch size etc…Thank you!Thanks for a fine guide on how to use TAO.I am using my own data to train a model with 2 categories, but the pictures a full HD - and a 12 picture sequence from each video.When I try to train the model I get an error :“ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).”It looks like the docker image needs extra memory. Can you please advice me on how to do that?Thanks,
AndersHi, I am trying run deepstream-3d-action-recognition-app on jetson NX and it running out of memory. May this app can not run on jetson nx right?Hi @samvdh - Can you share mode details about your setup, please?ThanksHi, my issue is temporary solved at this topic Deepstream 3d action recognition app leak memory on jetson nx - #19 by samvdh. Thank you for your support.@samvdh  – Great to hear! Let us know if you have any other questions…How to collect sequence of tracked detected humans (after human detection (pgie) followed by tracker) to pass it as an input to an action classifier (sgie) ? The task is action detection of individual human beings appearing in the frame.Powered by Discourse, best viewed with JavaScript enabled"
277,implementing-path-tracing-in-justice-an-interview-with-dinggen-zhan-of-netease,"Originally published at:			Implementing Path Tracing in ‘Justice’: An Interview with Dinggen Zhan of NetEase | NVIDIA Technical Blog
We sat down with NetEase Lead Programmer Dinggen Zhan to find out how his team integrated path tracing into the open world of Justice.Powered by Discourse, best viewed with JavaScript enabled"
278,inline-gpu-packet-processing-with-nvidia-doca-gpunetio,"Originally published at:			https://developer.nvidia.com/blog/inline-gpu-packet-processing-with-nvidia-doca-gpunetio/
Learn how the new NVIDIA DOCA GPUNetIO Library can overcome some of the limitations found in the previous DPDK solution, moving a step closer to GPU-centric packet processing applications.A very interesting article. If the payloads of the packets were encrypted (e.g. they represent a TCP connection with TLS enabled) is there anything in DOCA to facilitate decryption/encryption. If not how would this be done in a CUDA kernel? ThxThanks for the feedback! I’m happy to answer your questions, however, this would currently require an NDA. You can register for Early Access here and I’ll connect with you then.Hi I am facing some difficulty.I want to use DPDK to perform burst transfer to My RTX 3090 GPU. I am unable to find any  methods. I would appreciate if someone can help me with thatPowered by Discourse, best viewed with JavaScript enabled"
279,detecting-threats-faster-with-ai-based-cybersecurity,"Originally published at:			Detecting Threats Faster with AI-Based Cybersecurity | NVIDIA Technical Blog
The latest release of NVIDIA Morpheus includes new visualization capabilities enabling cybersecurity analysts to more quickly pinpoint and react to threats.Powered by Discourse, best viewed with JavaScript enabled"
280,new-video-visualizing-census-data-with-rapids-cudf-and-plotly-dash,"Originally published at:			https://developer.nvidia.com/blog/new-video-visualizing-census-data-with-rapids-cudf-and-plotly-dash/
Gathering business insights can be a pain, especially when you’re dealing with countless data points.  It’s no secret that GPUs can be a time-saver for data scientists. Rather than wait for a single query to run, GPUs help speed up the process and get you the insights you need quickly. In this video, Allan Enemark,…If you’re looking for the accompanying Notebook to try out these data viz techniques with RAPIDS cuDF, check out the RAPIDS Visualization Guide Notebook on GitHub.Powered by Discourse, best viewed with JavaScript enabled"
281,optimizing-robot-route-planning-with-nvidia-cuopt-for-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/optimizing-robot-route-planning-with-nvidia-cuopt-for-isaac-sim/
Mailroom workers pick up mail and parcels from different stations and deliver them to various recipients. They know that some envelopes are time-sensitive so they use their knowledge to plan routes with the shortest possible delivery time. This mail delivery puzzle can be mathematically addressed by using techniques from operations research, a discipline that deals…Powered by Discourse, best viewed with JavaScript enabled"
282,improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async,"Originally published at:			https://developer.nvidia.com/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/
Today’s leading-edge high performance computing (HPC) systems contain tens of thousands of GPUs. In NVIDIA systems, GPUs are connected on nodes through the NVLink scale-up interconnect, and across nodes through a scale-out network like InfiniBand. The software libraries that GPUs use to communicate, share work, and efficiently operate in parallel are collectively called NVIDIA Magnum…Thanks Jim, Seth, Pak, Sreeram for this thoughtful piece addressing situations when applications use smaller message sizes as the workload scales to larger numbers of GPUs, its nice to see MagnumIO (GPUDIrect Async, NVSHMEM ) helps NICs to sustain high throughput on NVIDIA InfiniBand networks. Hint: GPU initiated communications bypassing the GPU bottleneck.Thank you for the interesting article.
Do you intend to publish the lower-level RDMA api for GPU (i.e., InfiniBand GPUDirect Async) for users that wish to use RDMA without the shared-memory abstraction?
All I found is the libgdsync (GitHub - gpudirect/libgdsync: GPUDirect Async support for IB Verbs) which doesn’t seem to have had any development for the last years.Best,
LassePowered by Discourse, best viewed with JavaScript enabled"
283,can-i-use-other-tools-than-chatgpt,"This is a bit of a follow-up to my earlier question.If I don’t want to use ChatGPT, do I have any other options?Omniverse is a platform and open to integrating other LLMs. You can use alternatives like NVIDIA NeMo, which allows you to train on your own data and get responses that speak in your brand’s language and reflect your specific domain.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
284,cucim-rapid-n-dimensional-image-processing-and-i-o-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/cucim-rapid-n-dimensional-image-processing-and-i-o-on-gpus/
Overview cuCIM is a new RAPIDS library for accelerated n-dimensional image processing and image I/O. The project is now publicly available under a permissive license (Apache 2.0) and welcomes community contributions. In this post, we will cover the overall motivation behind the library and some initial benchmark results. In a complementary post on the Quansight…Is there an implementation example existing for Jetson Nano?
Thx+regardsHi @amirtousa , I am sorry for the late reply.cuCIM’s image loading part (CuImage class and read_region() method) is implemented in C++ and we don’t support wheel library for Jetson Nano (though Conda package supports cuCIM for arm64 SBSA Files :: Anaconda.org (arm64 SBSA includes Jetson boards such as Jetson Xavior/Orin).If you are able to install CuPy package (Add arm64 binary for Jetson Nano · Issue #3196 · cupy/cupy · GitHub and CuPy: Arm Wheels ) in your Jetson Nano, you can install and use cuCIM’s image processing feature.
Please see [FEA] Windows compatibility · Issue #454 · rapidsai/cucim · GitHubThank you!Powered by Discourse, best viewed with JavaScript enabled"
285,genomic-llms-show-superior-performance-and-generalization-across-diverse-tasks,"Originally published at:			https://developer.nvidia.com/blog/genomic-llms-show-superior-performance-and-generalization-across-diverse-tasks/
A collaboration between InstaDeep, the Technical University of Munich (TUM), and NVIDIA has led to the development of multiple super-computing scale foundation models for genomics. These models demonstrate state-of-the-art performance across many prediction tasks, such as promoter and enhancer site predictions. The joint team of researchers showed that large language models (LLMs) trained on genomics…Powered by Discourse, best viewed with JavaScript enabled"
286,this-is-an-example-question-post-for-the-ama-with-the-get3d-team,"This would be an example of a question for the GET3D Team.
The Team will post a reply to these questions.
See you right here on May 4th.Powered by Discourse, best viewed with JavaScript enabled"
287,debugging-a-mixed-python-and-c-language-stack,"Originally published at:			https://developer.nvidia.com/blog/debugging-mixed-python-and-c-language-stack/
Debugging is difficult. Debugging across multiple languages is especially challenging, and debugging across devices often requires a team with varying skill sets and expertise to reveal the underlying problem.  Yet projects often require using multiple languages, to ensure high performance where necessary, a user-friendly experience, and compatibility where possible. Unfortunately, there is no single programming…Powered by Discourse, best viewed with JavaScript enabled"
288,explainer-what-is-the-metaverse,"Originally published at:			What Is the Metaverse? | NVIDIA Blog
The metaverse is the “next evolution of the internet, the 3D internet,” according to NVIDIA CEO Jensen Huang.Powered by Discourse, best viewed with JavaScript enabled"
289,upcoming-event-openacc-and-hackathons-summit-2022,"Originally published at:			OpenACC and Hackathons Summit 2022
Join this digital conference from August 2-4 to learn how science is being advanced through the work done at Open Hackathons or accelerated using OpenACC.Powered by Discourse, best viewed with JavaScript enabled"
290,advanced-api-performance-setstablepowerstate,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-setstablepowerstate/
This post covers best practices for using SetStablePowerState on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips.Thanks for the post!
After many tries, this use of nvidia-smi --lock functions gave us the persistent performance the project requiresIt is very useful APIs when using the GPU as a DSP that need to work without any frequency lowering / idle behaviorRelated to this post,
can you explain the different usage of the following Performance approaches:Use “Nvidia Control Panel” GUI settingsUse nvidia-smi command line callsUse nvml CUDA APII prefers to use NVML to get the GPU persistent behavior, is it possible to get such persistent behavior only using nvml on GeForce RTX 3050 ?Hi!That’s great news.Did you encounter differences between the article’s recommendations and your usage? I’d like to update it to match current uses. nvidia-smi isn’t guaranteed to be a fixed target, unfortunately and it’s been a while since I wrote this article!Thanks, RyanHello again-I’m not familiar with what you’re referring to with 1/control panel, can you elaborate?I’m also not certain about the differences between 2 and 3. I suspect on the backend they are poking the same driver components to achieve their goals, because there’s basically one place to set these things. Unfortunately, the best way in the near term to establish this is probably to do a bit of testing.I’ll try to find the maintainers of these routes and report back if/when I get definitive answers.Thanks, RyanHello,
This sequence from the post works perfectly to set GPU core & GPU memory clock frequenciesnvidia-smi --query-supported-clocks=timestamp,gpu_name,gpu_uuid,memory,graphics --format=csv`
nvidia-smi --lock-gpu-clocks=<core_clock_rate_from_csv>
nvidia-smi --lock-memory-clocks=<memory_clock_rate_from_csv>There is additional GPU performance option on:
Windows → “Nvidia Control Panel” → “3D setting” → “Manage 3D setting” → “Global Setting” / “Program Setting” → “Power management mode” = “Prefer maximum performance”Ah, I would not rely on that setting. That’s more of a behavioral suggestion, it doesn’t pin the frequencies.nvidia-smi --query-supported-clocks=timestamp,gpu_name,gpu_uuid,memory,graphics --format=csv`
nvidia-smi --lock-gpu-clocks=<core_clock_rate_from_csv>
nvidia-smi --lock-memory-clocks=<memory_clock_rate_from_csv>Excellent, so it looks like the commands are the same. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
291,sharing-cuda-resources-through-interoperability-with-nvscibuf-and-nvscisync,"Originally published at:			https://developer.nvidia.com/blog/sharing-cuda-resources-through-interoperability-with-nvscibuf-and-nvscisync/
Figure 1. Various hardware engines on the NVIDIA embedded platform. There is a growing need among embedded and HPC applications to share resources and control execution for pipelined workflows spanning multiple hardware engines and software applications. The following diagram gives an insight into the number of engines that can be supported on NVIDIA embedded platforms.…CUDA interoperability with NvSciSync / NvSciBuf has been implemented in CUDA 10.2 release with a focus on usability on safety critical applications and we have seen good performance gains with this as well as elaborated in the blog. If you have any questions or comments, let us know.As stated in this thread, nvsci does not work properly with the dGPUs of the DriveAGX Pegasus with Drive10. As this issue should be fixed in the next release, I was wondering if there is any information on when this will happen.hello @rekhamukund,
i want to know if i allocate NvSciBuf on orin, if cpu and gpu can both can access the buffer.
thank you.Can we use NvSciSync / NvSciBuf to implement CUDA and OpenGL interop? Can we use it in a headless environment like an AWS EC2 instance?Please advise.OpenGL is not a supported UMD (User Mode Driver) for NvSciBuf/NvSciSync, so CUDA-OpenGL interop cannot achieved. Other interops which can be used instead are CUDA-OpenGL (CUDA Runtime API :: CUDA Toolkit Documentation) or EGL interop (CUDA Driver API :: CUDA Toolkit Documentation).CPU access to NvSciBuf allocated buffer can be achieved with NvSciBufObjGetCpuPtr() API as described in https://docs.nvidia.com/drive/drive_os_5.1.6.1L/nvvib_docs/index.html#page/DRIVE_OS_Linux_SDK_Development_Guide/Graphics/nvsci_nvscibuf.html#wwpID0E0OK0HAPowered by Discourse, best viewed with JavaScript enabled"
292,explainer-what-is-a-pod-what-is-a-cluster,"Originally published at:			What Is a Pod? What Is a Cluster? | NVIDIA Blog
Our digital lives run on collections of computers tightly linked on high-speed networks, and the latest one is an AI supercomputer called NVIDIA DGX SuperPOD.Powered by Discourse, best viewed with JavaScript enabled"
293,hands-on-lab-digital-fingerprinting-to-detect-cyber-threats,"Originally published at:			Digital Fingerprinting to Detect Cyber Threats | NVIDIA
Learn how to use an NVIDIA AI workflow to uniquely fingerprint users and machines across your network in a new, free NVIDIA LaunchPad hands-on lab.Powered by Discourse, best viewed with JavaScript enabled"
294,evolving-from-network-simulation-to-data-center-digital-twin,"Originally published at:			https://developer.nvidia.com/blog/evolving-from-network-simulation-to-data-center-digital-twin/
Learn about the evolution and value of data center digital twins, which are attracting increasing attention across industries.Powered by Discourse, best viewed with JavaScript enabled"
295,advanced-api-performance-synchronization,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-synchronization/
Synchronization in graphics programming refers to the coordination and control of concurrent operations to ensure the correct and predictable execution of rendering tasks. Improper synchronization across the CPU and GPU can lead to slow performance, race conditions, and visual artifacts. Recommended If running workloads asynchronously, make sure that they stress different GPU units. For example,…Powered by Discourse, best viewed with JavaScript enabled"
296,explainer-what-are-graph-neural-networks,"Originally published at:			What Are Graph Neural Networks? | NVIDIA Blogs
GNNs apply the predictive power of deep learning to rich data structures that depict objects and their relationships as points connected by lines in a graph.Powered by Discourse, best viewed with JavaScript enabled"
297,whole-slide-image-analysis-in-real-time-with-monai-and-rapids,"Originally published at:			https://developer.nvidia.com/blog/whole-slide-image-analysis-in-real-time-with-monai-and-rapids/
Learn how tools such as MONAI and RAPIDS can unlock meaningful insights in the analysis of whole slide image data.Powered by Discourse, best viewed with JavaScript enabled"
298,entropy-based-methods-for-word-level-asr-confidence-estimation,"Originally published at:			https://developer.nvidia.com/blog/entropy-based-methods-for-word-level-asr-confidence-estimation/
Learn how to achieve fast, simple word-level ASR confidence estimation using entropy-based methods.Powered by Discourse, best viewed with JavaScript enabled"
299,gpt-agent-in-omniverse,"Hi!Two questions!Would it be possible to create entire environments based on GPT prompts within Replicator? The idea is to fine-tune computer vision models on synthetic datasets (models + environment)I would like to create an AI agent operating from visual feedback.
Can we use other python libraries within Omniverse (such as YOLO or SegmentAnything) and is it possible to feed camera coordinates to a GPT prompt and then update the position based on the reply from GPT, similar to the 3D object placing pipeline?Thanks!You could certainly do that, you can even have GPT create USD directly rather than json files, the problem I started to run into were with token limits. Your GPT response can only be so long and if you’re creating meshes and textures from scratch, that takes up a lot of tokens very quickly.A different approach I might take would be to compose your scene, keeping in mind those things you would like to randomize, and then ask GPT to give you random values for only those key items. That way you would be able to work with a much larger scene. That said, this is what replicator already does and I might use replicator for this specific task over GPT.That sounds like such a cool project! Can’t wait to see what you put together! You can incorporate pretty much any python package you want into your extension, the Omniverse platform is extremely flexible. You could definitely tap into one of the scene’s update callabacks, get camera coordinates and feed those into GPT, just be ready for a really, really slow frame rate because it takes a few seconds to get that reply back from GPT :DThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
300,a-new-frontier-for-5g-network-security,"Originally published at:			https://developer.nvidia.com/blog/a-new-frontier-for-5g-network-security/
Wireless technology has evolved rapidly and the 5G deployments have made good progress around the world. Up until recently, wireless RAN was deployed using closed-box appliance solutions by traditional RAN vendors. This closed-box approach is not scalable, underuses the infrastructure, and does not deliver optimal RAN TCO. It has many shortcomings. We have come to…Powered by Discourse, best viewed with JavaScript enabled"
301,uncovering-treatments-for-addiction-and-depression-with-gpu-accelerated-supercomputing,"Originally published at:			Uncovering Treatments for Addiction and Depression with GPU-Accelerated Supercomputing | NVIDIA Technical Blog
A team lead by Cornell University researchers are using the Titan Supercomputer at Oak Ridge National Laboratory to study mechanisms of sodium-powered transporters in cell-to-cell communications. Harel Weinstein’s lab at the Weill Cornell Medical College of Cornell University have constructed complex 3D molecular models of a specific family of neurotransmitter transporters called neurotransmitter sodium symporters…Wow, this sounds like an incredible research project! The use of GPU-accelerated supercomputing to uncover treatments for addiction and depression is genuinely fascinating. It’s incredible how technology enables researchers to delve deeper into the mechanisms of cell-to-cell communications. I’m excited to see how their complex 3D molecular models of neurotransmitter sodium symporters contribute to our understanding of these conditions. I also think that once this technology is proven safe and beneficial, leading rehabs (like rehab Essex) will implement it. Thanks for sharing, and keep posting exciting news!Powered by Discourse, best viewed with JavaScript enabled"
302,deep-learning-super-sampling-v2-in-unreal-engine-5,"NVIDIA Deep Learning Super Sampling (DLSS) is a plug-in that uses deep learning algorithms to upscale or “super sample” an image, and helps during GPU heavy workloads like ray tracing. NVIDIA DLSS takes a lower resolution image and increases its resolution.For a visual walkthrough on how to install and implement DLSS view the video below.This is part of a series of Unreal 5 related introductory videos first listed in our technical blog on the developer pages. https://developer.nvidia.com/blog/shaping-the-future-of-graphics-with-nvidia-technologies-in-unreal-engine-5/Powered by Discourse, best viewed with JavaScript enabled"
303,monai-reaches-1-million-download-milestone-driven-by-research-breakthroughs-and-clinical-adoption,"Originally published at:			https://developer.nvidia.com/blog/monai-reaches-1-million-download-milestone-driven-by-research-breakthroughs-and-clinical-adoption/
MONAI, the domain-specific, open-source medical imaging AI framework that drives research breakthroughs and accelerates AI into clinical impact, has now been downloaded by over 1M data scientists, developers, researchers, and clinicians. The 1M mark represents a major milestone for the medical open network for AI, which has powered numerous research breakthroughs and introduced new developer…Powered by Discourse, best viewed with JavaScript enabled"
304,generative-ai-research-empowers-creators-with-guided-image-structure-control,"Originally published at:			https://developer.nvidia.com/blog/generative-ai-research-empowers-creators-with-guided-image-structure-control/
New research is boosting the creative potential of generative AI with a text-guided image-editing tool. The innovative study presents a framework using plug-and-play diffusion features (PnP DFs) that guides realistic and precise image generation. With this work, visual content creators can transform images into visuals with just a single prompt image and a few descriptive…Powered by Discourse, best viewed with JavaScript enabled"
305,upcoming-event-retail-edge-computing-101-an-introduction-to-the-edge-for-retail,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-retail-edge-computing-101-an-introduction-to-the-edge-for-retail/
​​Join us November 9 for the Retail Edge Computing 101: An Introduction to the Edge for Retail webinar to get everything you wanted to know about the edge, from the leader in AI and accelerated computing.Powered by Discourse, best viewed with JavaScript enabled"
306,ai-models-recap-scalable-pretrained-models-across-industries,"Originally published at:			https://developer.nvidia.com/blog/ai-models-recap-scalable-pretrained-models-across-industries/
The year 2022 has thus far been a momentous, thrilling, and an overwhelming year for AI aficionados. Get3D is pushing the boundaries of generative 3D modeling, an AI model can now diagnose breast cancer from MRIs as accurately as board-certified radiologists, and state-of-the-art speech AI models have widened their horizons to extended reality. Pretrained models…Powered by Discourse, best viewed with JavaScript enabled"
307,upcoming-event-speech-ai-summit-2022,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-speech-ai-summit-2022/
Join experts from Google, Meta, NVIDIA, and more at the first annual NVIDIA Speech AI Summit. Register now!Powered by Discourse, best viewed with JavaScript enabled"
308,speech-ai-spotlight-how-pendulum-nabs-harmful-narratives-online,"Originally published at:			https://developer.nvidia.com/blog/speech-ai-spotlight-how-pendulum-nabs-harmful-narratives-online/
Over 55% of the global population uses social media, easily sharing online content with just one click. While connecting with others and consuming entertaining content, you can also spot harmful narratives posing real-life threats. That’s why VP of Engineering at Pendulum, Ammar Haris, wants his company’s AI to help clients to gain deeper insight into…Enjoyed writing this spotlight piece. I appreciate the work that Pendulum is doing to discover and call out harmful narratives, which will become even more important with the advent of text-based Generative AI.What are your thoughts on applying AI for good in other ways?Powered by Discourse, best viewed with JavaScript enabled"
309,can-we-use-nvidia-omniverse-to-build-virtual-assistant-3d-human-like-metahuman-and-talking-to-us-by-using-chatgpt-api,"Can we use NVIDIA Omniverse to build Virtual Assistant 3D human like MetaHuman and talking to us by using ChatGPT API work as custom agent?While you definitely can build your onw solution in NVIDIA Omniverse to animate virtual assistants, using technologies like Audio2Face in combination with speech synthesis tech like Riva and combine that with GPT-4, there are also 3rd party solutions in Omniverse that allow you to easily create Virtual Assistants using generative AI. One is Convai, you can check it out here: https://convai.com/ They have recently released an Extension for Omniverse that allows youto easily create your characters on their website and then interact with the in Omniverse. They were recently featured in the GTC keynote!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
310,dense-graphs,"Have you come across any interesting examples of dense graphs (e.g., a high edge to vertex ratio : most vertices are connected each other)?Our implementation is mainly tailored towards sparse graph therefore, we recommend using dense matrix libraries instead. In fact, our implementation incurs additional overhead when processing and storing low degree vertices and their properties. We did at some point support Hungrarian wich is for dense graphs but it was pushed to a more suitable package (raft)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
311,generating-ai-based-accident-scenarios-for-autonomous-vehicles,"Originally published at:			https://developer.nvidia.com/blog/generating-ai-based-accident-scenarios-for-autonomous-vehicles/
Testing autonomous vehicles in potential-accident scenarios is critical for safety. Learn about recent research that explores automatically generating accident scenarios in simulation using AI.Powered by Discourse, best viewed with JavaScript enabled"
312,gpu-dashboards-in-jupyter-lab,"Originally published at:			https://developer.nvidia.com/blog/gpu-dashboards-in-jupyter-lab/
Learn how NVDashboard in Jupyter Lab is a great open-source package to monitor system resources for all GPU and RAPIDS users to achieve optimal performance and day to day model and workflow development.The blog article was posted 3 days ago and contains long detailed “documentation” of how to install this extension using various wayBut this article is already wrong (probably outdated) and misleading for potential users.The article instructions to install:Gives:According to the official docs (GitHub - rapidsai/jupyterlab-nvdashboard: A JupyterLab extension for displaying dashboards of GPU usage.) we only need to do the pip install (like for most extension on JupyterLab 3). And the extension is only supported on JupyterLab > 3, so the whole installation process documented in this 3 days-old article already does not exist anymore@vincent.emonet Thank you for catching that. We have updated it.It seems like it is still broken. In my case installing this package automatically updated jupyterlab to v3.. and broke the Web UI. Uninstalling jupyterlab-nvdashboard and downgrading jupyterlab back to v2 solved the broken UI at least.Are there any plans to update the dashboards to work with the current version of JupyterLab?Could I also use this on VSCODE? Thanks!I’m afraid not. Currently, only Jupyter Lab and the standalone dashboard is supported.Powered by Discourse, best viewed with JavaScript enabled"
313,deploying-edge-ai-in-nvidia-headquarters,"Originally published at:			https://developer.nvidia.com/blog/deploying-edge-ai-in-nvidia-headquarters/
Learn how the NVIDIA IT team deployed an edge AI solution in NVIDIA headquarters.Powered by Discourse, best viewed with JavaScript enabled"
314,i-am-bit-confused-about-discriminator,"How are you utilizing the Discriminator in your work? Does it receive randomly rendered outputs for fake signal and real images for real signal, or does it receive the same position rendered output each time for fake signal?Yes, we’re utilizing the discriminator in GET3D. At every training iteration, it receives a rendered image (rendered at a random camera position, which is sampled from the training dataset) and classifies whether this image is real or fake. The real and fake images are not rendered in the same camera position, all of the images & cameras are randomly sampled from the training setThank You !This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
315,deep-learning-in-a-nutshell-history-and-training,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-nutshell-history-training/
This series of blog posts aims to provide an intuitive and gentle introduction to deep learning that does not rely heavily on math or theoretical constructs. The first part in this series provided an overview over the field of deep learning, covering fundamental and core concepts. The third part of the series covers sequence learning…I would like to know when did NVIDIA began production on AI products?Hi,
I am a research scholar at IIT Kanpur. I am working towards my PhD thesis on the architecture optimization of deep learning models.I was going through brief history on deep learning. I came across this blog article which has mentioned about Ivakhnenko and Lapa in 1965 (Figure 1).Can you help me to get the digital copy of this article.I am expecting your help in this regard.Thanking you,
Arun.@arun.iitkgp2k9  – You can find Ivakhnenko and Lapa’s work in the following book:  Cybernetic predicting devices. Good luck with your thesis!Hi,
I was going through brief history on deep learning. I came across this blog article which has mentioned about Ivakhnenko and Lapa in 1965 (Figure 1).Can you help me to get the digital copy of this article.I am expecting your help in this regard.Thanking you,As I mentioned to @arun.iitkgp2k9 earlier, you can find Ivakhnenko and Lapa’s work in the following book: Cybernetic predicting devices . Good luck!Powered by Discourse, best viewed with JavaScript enabled"
316,now-available-nvidia-nemo-guardrails,"Originally published at:			Nemo Framework for Generative AI - Get Started | NVIDIA Developer
Develop safe and trustworthy LLM conversational applications with NVIDIA NeMo Guardrails, an open-source toolkit that enables programmable guardrails for defining desired user interactions.Powered by Discourse, best viewed with JavaScript enabled"
317,top-synthetic-data-sessions-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/top-synthetic-data-sessions-at-nvidia-gtc-2023/
 Discover how 3D synthetic data generation is accelerating AI and simulation workflows.Powered by Discourse, best viewed with JavaScript enabled"
318,gtc-2020-nvidia-marbles-rtx,"GTC 2020 D2S38
Presenters: Tech Demo Team,NVIDIA
Abstract
During the GTC 2020 virtual keynote, NVIDIA CEO and founder Jensen Huang showcased a piece created by NVIDIA to illustrate the power of #RTX on the Omniverse Platform. Marbles was created by a distributed team of artists and engineers in Omniverse, assembling VFX+ quality assets into a fully physically simulated game level. There was no sacrifice to quality and fidelity that typically results from “gamifying” art assets to run in real-time.  Marbles runs on a single Quadro RTX 8000 simulating complex physics in a real-time ray traced world.Learn more about the NVIDIA Omniverse Platform: https://developer.nvidia.com/nvidia-omniverse-platform
Watch the entire GTC 2020 keynote: Keynote by NVIDIA CEO Jensen Huang | GTC 2022 | NVIDIAWatch this session
Join in the conversation below.Will you release this demo publicly? Lots of people wanna play with it.Same. Where to download it?Why is the shadow of the marble not refracting light?Still not released.I want to download it so much… But I dont know if my 2080Ti will be enough…I’ve downloaded the demo from the Omniverse,  but I get this error:|---------------------------------------------------------------------------------------------|
| Driver Version: 466.27      | Graphics API: D3D12
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA GeForce RTX 2060 SUPER    |        |     | 8031    MB | 10de      | d6070000… |
|     |                                  |        |     |            | 1f06      | 0          |
|=============================================================================================|
| OS: Windows, Version: 10.0 (20H2), Build: 19042
| Processor: Intel(R) Core™ i9-9900KF CPU @ 3.60GHz | Cores: 8 | Logical: 16
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 65465 | Free Memory: 54827
| Total Page/Swap (MB): 75193 | Free Page/Swap: 59952
|---------------------------------------------------------------------------------------------|
2021-05-07 00:12:39 [712ms] [Error] [gpu.foundation.plugin] No device could be created.Oh looking at the log file I see why:C:/Users/me/.nvidia-omniverse/logs/Kit/omni.app.marbles/2021.1/kit_20210506_171239.log2021-05-07 00:21:50 [708ms] [Warning] [gpu.foundation.plugin] Skipping GPU NVIDIA GeForce RTX 2060 SUPER with 8031 MB of GPU memory. Requested min 11000 of GPU memoryLooks like the docs are wrong as it says marbles demo will work with any RTX card but it seems to have a min memory requirement of 11GB.The minimum video memory requirement can be changed in:C:\Users\me\AppData\Local\ov\pkg\marbles_rtx-2021.1.5_build\windows-x86_64\release\apps\omni.app.marbles.kityou should find renderer.minGpuMemoryMB = 11000 on line 37Lowering the minimum should allow the application to run on something like a RTX 3080 or 3070, but I can’t guarantee that you would get at least reasonable performance, nor can I test it for you because I only have a RTX 3090.Hope this helps!Can some one paste link to download it? Or maybe private share?Yes updating that kit file worked! Thanks Dusty!  It runs dog slow on my RTX 2060 Super though.  That extra 3GB of memory must really help.  Konrad, you can download the marbles demo by installing NVIDIA Omniverse and installing the Marbles RTX app.i have RTX 3080 i5 9600 and 32 GB of Ram and the game does not exceed 12 FPS in 3440 x 1440 and 1080p max 20fps min 7fps.How can open this file? If I open it via doubleclick it crashesif windows: right click, ‘open with’ and select notepad. once edited, save, exit and try run marbles again. Worked for me after changing value to 10000 for my 3080 so thanks to the author. Beautiful tech; to think that quality will be mainstream in years to come, wowon a RTX2080 setting it to 720p it kind of runs but is no where near 60FPS wondering why there are no dynamic lights considering its a raytracing demo. Isn’t that the whole point??? Setting the ball on fire has the effect of adding a light but its not raytracing the fire. Is this just me? Any how Impressive demo maybe If I had a 3080 it would run as intended.Thanks Dusty, it worked for me, I have the Rtx 2080, I had to bring it down to 7000tried today but i loads  only to the instructions screen but pressing any button doesnt doo anyhthing it just stays ther in the instructions screen foreverOkay i finally figured it out. its working now.Marble Nvidia 12559×1516 441 KBMarble Nvidia 22560×1525 455 KBMarble Nvidia2560×1510 443 KBPowered by Discourse, best viewed with JavaScript enabled"
319,what-is-cugraph-dgl,"Thanks for answering my other question.
I have another newbie question what is cugraph-dgl?cugraph-dglcugraph-dgl is particularly useful for graph analytics tasks that require both graph operations and GNN modeling, such as node classification, link prediction, and graph classification. The combination of efficient graph operations in cuGraph and scalable GNN modeling in DGL allows for fast and accurate training of GNNs on large-scale graph data.Overall, cugraph-dgl is a powerful tool for graph analytics and GNN modeling on GPU-accelerated hardware, providing a comprehensive solution for graph-based machine learning applications.Thanks !Thanks, I shall have to read up on that - thanks.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
320,gpu-integration-propels-data-center-efficiency-and-cost-savings-for-taboola,"Originally published at:			https://developer.nvidia.com/blog/gpu-integration-propels-data-center-efficiency-and-cost-savings-for-taboola/
When you see a context-relevant advertisement on a web page, it’s most likely content served by a Taboola data pipeline. As the leading content recommendation company in the world, a big challenge for Taboola was the frequent need to scale Apache Spark CPU cluster capacity to address the constantly growing compute and storage requirements. Data…So much time could have been saved not to mention high potential for better performance results if they would have used our Synopsys Optimizer Studio (former Concertio) tools.Powered by Discourse, best viewed with JavaScript enabled"
321,introducing-low-level-gpu-virtual-memory-management,"If you could further elaborate in what way do they interact with the OS, it would be incredible. We could try to fine-tune our OS to try and reduce this jittering. We’re currently looking at such solutions, but we’re doing so blindly.So, the ioctls involved on your platform in cuMemSetAccess (and in this case cuMemUnmap, as it internally calls cuMemSetAccess(PROT_NONE) to unmap the memory) are the UVM ioctls, which is open source (look for UvmMapExternalMemory when extracting the nvidia display driver package, the full source of this part of the kernel mode driver is readily available), so you’re free to inspect these paths yourself.  Unfortunately, this path in particular ends up jumping into the proprietary binary blob, for which there’s not much I can give insight on in a public forum.If this was an issue in our actual use-case, I would expect to see jittering in various other ioctls, not only these three.It really depends on what other ioctls you’re looking at, as some ioctls have different locking semantics than others (some have read/write semantics, others are plain exclusive).  The fact that you said nvidia-smi makes your application slower is an indication this could possibly be your issue, but I can’t be sure without looking at your use case directly, and/or seeing a timeline profile.You had asked about paid consulting before, and that might be the direction you might want to take in order to have someone dedicated to look into your particular use case and see what we can do to help.  I’ve sent you a direct message with a contact that you can use to describe your problem in more detail with a representative more suited to guide your application development.  I’ll also be in touch with them and see if we can’t resolve your issue so we can hopefully post the solution for others to enjoy.Hey Cory,
Sorry for the late response. You’ve been extremely helpful. I’ll contact the representative.— OmriHey @jmak, I reached out but received some error from your exchange server. It couldn’t be delivered to some specific mail address (because it’s restricted). Not sure if my email arrived properly.Hi!Is there support for these APIs in Windows systems as well? I’m having trouble with mine.
I’m using Windows Server 2022 with a single Tesla T4 GPU and NVIDIA driver version 516.01 installed.At first I saw that the API cuMemAddressReserve succeeds but returns the address 0x0.
I then encountered the device attribute CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED and querying the device for this attribute returned 0 as if this is not supported by the GPU.As mentioned above, I’m using a Tesla T4 GPU which should support this feature.
I even verified that querying the same attribute for the same GPU type in a Linux machine returns 1.I haven’t found any note that these APIs do not support Windows, and it also seems that the memMapIPCDrv CUDA sample supports Windows.I wonder if this is something related to Windows or the GPU itself, and if I can somehow add support for this in my system.Thanks in advance!Hi razrotenberg!Is there support for these APIs in Windows systems as well?Yes, but unfortunately only for WDDM driver model GPUs, which your Tesla T4 being a Tesla card only supports TCC driver model IIRC (more information about WDDM/TCC driver model can be found in the link below).  This feature is supported on this GPU on native Linux platforms though if that’s an option for you.  As to when or if this feature will be supported on TCC, I cannot say at this time, but I can say we’re actively looking into some options!NVIDIA-SMI Documentation - Look for the Driver Model section.Hope this helps, good luck!Thanks @Cory.Perry for the super fast answer. Much appreciated!.Is there any place where I could see which GPUs support the WDDM driver model? Is there a general rule or should I check per GPU type?
From peeking at the nvidia-smi documentation you sent, it seems that TCC is more for compute-related GPUs while WDDM is more for graphics. Does it mean that any data-center GPU will not support the WDDM driver model?
It also seems that WDDM driver model has some disadvantages compared to TCC in performance.I really hope you’ll manage to add support for these amazing features to the TCC driver model as well.Thanks again and I’ll be waiting for any news regarding this in the future!Thanks @Cory.Perry for the super fast answer. Much appreciated!.No worries, happy to help!Is there any place where I could see which GPUs support the WDDM driver model?If you look at the nvidia-smi output, you can see the current driver model.  With admin priviledges you can change the driver model on some GPUs, but IIRC all “Tesla” branded models do not support changing the driver model, but I don’t recall if that’s entirely accurate.  I do know Quadro GPUs support switching driver models, and I believe drivers today require the GPU to have a display head (ability to connect to a screen, though an actual connection to a screen is not required) in order to use WDDM.It also seems that WDDM driver model has some disadvantages compared to TCC in performance.Most of the documentation surrounding TCC and WDDM touting lower performance are typically fairly old in my experience.  Current drivers in WDDM can achieve comparative performance on par with TCC, and newer CUDA features like WSL support, these VMM apis, and many many more are only available on WDDM.  For more up-to-date information on performance of WDDM, check out this other blog post by one of our lead CUDA Windows engineers, linked below!  This blog post is mostly related to WSL, but there is information here about WDDM performance as well, and much of the performance benefits (and more) apply to native Windows WDDM as well.  That all said, WDDM is a very different driver model from what most are used to with TCC and native Linux, and there are some caveats and pitfalls that one might run into.  Most of these issues fall outside our CUDA Programming Model as defined (like buffering launches until synchronization rather than immediately).  See the programming guide below for more information.  I believe there’s also a few GTC presentations that go over WDDM performance tips as well that might be beneficial in this regard.I really hope you’ll manage to add support for these amazing features to the TCC driver model as well.Keep an eye out :)Hope this helps, good luck and let us know if you have any other questions!Hey Cory!I am testing VMM random copy performance, and get in a strange performance problem when  I initialize requestedHandleTypes in CUmemAllocationProp  with CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR .Considering we have two array, I want to read the src array randomly and copy to the dst array.The testing kernel looks like the following:And src, dst are  1GB 1D arrays allocated by vmm api. Say their CUmemAllocationProp is prop, when intializing prop I set:Case1: prop.requestedHandleTypes = 0
Case2: prop.requestedHandleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTORI find that Case1 performance is better than Case2 4 times.  But I think they should have same performance. So my questions are:Hi User 2944419175!I am guessing you don’t hit the same issue with sequential copies?  And when you run your random copy through one of our profilers, my guess is your TLB miss  counters is unusually high? If so, then yeah, this is unfortunately a known issue with allocations that need to be importable with other user mode drivers like Vulkan.  Some of these require a smaller page size than cuda’s default internal page size, and due some driver limitations we don’t support being able to use different page sizes with different mappings of the same allocation.  This means CUDA is forced to map with a smaller page size, which puts more pressure on the gpu’s TLB.The good news is, this should be fixed in an upcoming driver update very soon, so look forward to it!  Hope this helps, good luck!Hey, thanks for you fast answer, much appreciated!I am guessing you don’t hit the same issue with sequential copies?Yes, this problem will not occur with sequential copies.And when you run your random copy through one of our profilers, my guess is your TLB miss counters is unusually high?I see nvprof document, but I didn’t find some metrics about TLB miss counter.  And I do a pointer chasing to test access latency , comfirm that TLB pressure is actually higher in Case2, and result shows that the first access (every cache miss) VMM with CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR seems to have higher latency than VMM without CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR or memory allocated by cudaMalloc?Thanks again!I see nvprof document, but I didn’t find some metrics about TLB miss counterI’m sorry, I’m not much of an expert with nvprof or nsys that I can direct you to the exact counters, but I believe many of our devblogs should be able to help guide you.  If anything, I believe PC sampling and guided analysis should be able to indicate the large access times, give details into some cache analysis, and help you confirm this is the case.  That said, based on the fact that sequential access is working fine, I would count on the TLB thrashing as being the the issue, unfortunately.So, yes, by requesting a shareable handle type, you’re opting to create this allocation such that it is compatible with other UMDs and thus needs a smaller page size.  Thus you will likely get higher latency accesses if you thrash the TLB (with random accesses across 1GiB for example) due to the smaller page size.  As I mentioned, this should be fixed in an upcoming driver release (I’m not sure which one just yet, I can try to let you know when it does).  Even so, it is always recommended, just as it is with CPUs though much more so with GPUs, to try to align your accesses in order to achieve maximum bandwidth and cache utilization whenever possible.Hope this helps, please let us know if you have further questions or if we can help further!  Good luck!Hi, We are developing a Windows application using the Virtual Memory Management APIs along with the CU_MEM_HANDLE_TYPE_WIN32 shareable handle. We have followed the memMapIpcDrv code sample and have successfully accessed the same CUDA memory on separate processes.The issue we are experiencing is that after any shared handles have been imported on the child process, the physical memory is not released until the child process exits.Do you happen to know if this is a known issue or limitation?We are making sure to unmap/release/close all device-pointers/allocation-handles/shareable-handles on both processes. All function calls exit without errors. Please see the following summary for the sequence of the api calls (error handling and minor details omitted for clarity).Any help would be greatly appreciated. Thanks!Parent process:Child process:Parent process:Hi, I try some features about cuMemSetAccess, I find if one device can not peer access another, the cuMemSetAccess will raise an cuda error when I set the access to this device. Is more detail information about the limitation of Driver API cuMemSetAccess?Hi jearnshaw, welcome back to the forums!It looks like everything you’re doing is correct… could you post a reproducer of the problem?  Do you see the same issue with the sample code or just your application?  I believe there used to be a bug in older drivers (I don’t remember what versions sorry) that did have a leak in this path, but this should be fixed in newer drivers IIRC.  Can you post what version driver you’re using?Hi 182yzh,I’m sorry, I’m not quite following your question.  I believe you’re asking if cuMemSetAccess can be used to enable access to memory physically located on a different device but does not have peer access enabled.  As described in the blog post above, this is a primary use case for this API, but it still requires that the two communicating device be peer-capable (i.e. cuDeviceCanAccessPeer or the runtime equivalent cudaDeviceCanAccessPeer returns true for the two devices).  Memory physically present on one device cannot be accessed by another device without the peer-capability between the two, whatever the underlying backend may be (PCI-E, NVLINK, NVSWITCH, etc).  Hope that answers your question, let me know if it doesn’t!  Good luck!Thanks for your quick reply @Cory.Perry. Yes I can reproduce the issue with the sample code as well.I’ve just modified memMapIpcDrv in a forked cuda-samples here to demonstrate the issue.The modifications increase the memory allocated to 2GB and add in some delays to inspect the GPU memory usage with nvidia-smi and Task Manager. Please see the following screenshots of the sample running. Our application reallocates memory and the physical memory continues to increase until the child process is exited.
Screen Shot 2022-12-06 at 1.14.38 PM1920×416 69.3 KB

Screen Shot 2022-12-06 at 1.15.17 PM1920×417 75.5 KB
The test machine is running CUDA 11.8 with nvidia driver 527.37 on Windows 10 Pro 22H2.Thanks for your reply. Through your reply I find out that the cuMemSetAccess need the capacity among devices(In other words, the cuDeviceCanAccessPeer result is true). Thanks a lot！@jearnshaw please file a support ticket following the instructions on our support site and we can look into diagnosing the issue as this is likely a bug.  Here’s a link to the nvidia support site for your convenience:
NVIDIA Support SiteHi @Cory.Perry , was the reason for @jearnshaw s problems resolved? I am experiencing a similar issue with a straightforward POC in which I continuously allocate (cuMemCreate) and free memory (cuMemUnmap/cuMemRelease/cuMemAddressFree), and I see that the memory usage keeps growing indefinitely, as if the memory is never freed.PS: I just updated my drivers to 535.86.05.@moises.jimenez Unfortunately I don’t have visibility into the exact support ticket that was filed.  If you’re seeing the same issue with the latest release drivers available, I would file your own support ticket.  It never hurts to have duplicated tickets, and in fact, helps us better prioritize issues such as this in order to build a better product.  Hope this helps!Powered by Discourse, best viewed with JavaScript enabled"
322,an-aiot-solution-for-visual-blockage-detection-at-culverts,"Originally published at:			https://developer.nvidia.com/blog/an-aiot-solution-for-visual-blockage-detection-at-culverts/
One of the key contributors in originating flash floods is the blockage of cross-drainage hydraulic structures, such as culverts, by unwanted, flood-borne debris being transported. The accumulation and interaction of debris with culverts often result in reduced hydraulic capacity, diversion of upstream flows, and structural failure. For example, the Newcastle, Australia floods in 2007, Wollongong,…Powered by Discourse, best viewed with JavaScript enabled"
323,the-cuda-team-is-back-and-ready-to-answer-questions-live-now,"Thanks for joining us - please post your CUDA questions and the team will be happy to answer.Powered by Discourse, best viewed with JavaScript enabled"
324,free-dli-mini-self-paced-course-assemble-a-simple-robot-in-nvidia-isaac-sim,"Originally published at:			Courses – NVIDIA
This self-paced, free tutorial provides a basic understanding of the Isaac Sim interface and the documentation needed to begin robot simulation projects.Powered by Discourse, best viewed with JavaScript enabled"
325,access-the-latest-in-vision-ai-model-development-workflows-with-nvidia-tao-toolkit-5-0,"Originally published at:			https://developer.nvidia.com/blog/access-the-latest-in-vision-ai-model-development-workflows-with-nvidia-tao-toolkit-5-0/
NVIDIA TAO Toolkit 5.0 features include source-open architecture, transformer-based pretrained models, AI-assisted data annotation, and the capability to deploy models on any platform.Looks promising, especially the transformer-based models and the ONNX export option.When does 5.0 become available and where can it be found?
Curious to test it on a Jetson Orin NX 16GB.Hi, TAO Toolkit 5.0 will be released mid 2023 . You can sign up here to be notified when the new version is available - Log in | NVIDIA Developer. To get started with TAO Toolkit, checkout this page - Get Started with TAO Toolkit | NVIDIA Developer.Good morning June!Any news?will any custom model from BYOM converter can be usable as pretrained model in TAO?Hello, 4th of July!!! no plan for releasing TAO 5.0? or any update on when it will be released? @jwitsoe @Fiona.Chenthe 5.0 is already there, can’t wait to use it.Powered by Discourse, best viewed with JavaScript enabled"
326,streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streams,"Originally published at:			https://developer.nvidia.com/blog/streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streams/
NVIDIA GPUs have become mainstream for accelerating a variety of workloads from machine learning, high-performance computing (HPC), content creation workflows, and data center applications. For these enterprise use cases, NVIDIA provides a software stack powered by the CUDA platform: drivers, CUDA-X acceleration libraries, CUDA-optimized applications, and frameworks. Deploying the NVIDIA driver is one of the…Hi this is Kevin, hope you’ve enjoyed reading my blog post. Be sure to check out my presentations on this subject, at NVIDIA GTC Fall 2020 and Red Hat Summit 2020. On a related note, the yum-packaging-precompiled-kmod repository on GitHub, has a detailed README and pull requests are welcome. Shout out to our friends at Red Hat, that collaborated on this project. Finally, if you have any questions or comments, please let us know.Hi,thanks for the guide. However, are you aware that these drivers have not worked since RHEL 8.3 came out, for few months? Any plans to fix the build?Hi @ilkka.tengvall_nv could you expand on “drivers have not worked”? Please fill in the blanks:I have confirmed that the precompiled driver packages are functional on RHEL 8.3, with two exceptionsAlso feel free to report such issues here: Issues · NVIDIA/yum-packaging-precompiled-kmod · GitHubThanks, I’m not at the computer now, but I have some older posts here describing the problem:I’ve had troubles with it for long.By the way, I’ve added this page: https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/precompiled/ to the repository, that includes a table of available NVIDIA kernel module packages.Hi SirWe have problem having 2 Xdisplays on rhel8, centos or stream.  yes we can create 2 X displays using nvidia-settngs but   2nd screen keep crashing using xfce, gnome, kde.example:  setenv DISPLAY :0.0 and DISPLAY :0.12nd monitor is plain black and unable to do right click or anythingpreviously on rhel7,  we can use 2 Xdisplays.  how can we achieve this on rhel8 ?Does this streamlined approach for NVIDIA driver installation work on secure boot systems? Especially upgrades are a pain on such systems, one can accidentally upgrade the kernel and the system won’t boot, if the required NVIDIA kernel modules are not available or not signed.Hi @joachima
So the precompiled modules are signed and I would like for UEFI Secure Boot to work out-of-the-box, but there are a couple of pieces missing. The big one is the bootloader shim does not trust the NVIDIA certificate, so the public key has to be manually enrolled in the MOK.As far “modules are not available”, the nvidia dnf plugin will block new kernel updates, until a new compatible precompiled kmod package is available (usually within 24 hours).Regarding system upgrades, one of the things that precompiled solves today is when the kernel-devel and kernel-headers packages do not match the target kernel, resulting in the kernel modules failing to build; and subsequently failing to load on reboot (especially booting into a new kernel). This is not an issue for precompiled streams, likewise prior to shipping, they are tested against that specific kernel version, so it is known to work.Thanks for the quick reply. This sounds good. Adding a certificate via the MOK is needed either way, even with the old way to install the driver. I’m inclined to give this a try. I’ll need to review the installation instructions to see what I need to do to get the certificate registered.Okay, going through the instructions on your Web page, I don’t see it mention which key I need to register with the MOK utility. In the instructions for building ones own custom drivers one must generate a new key, which makes sense, but where is NVIDIA’s public key?Yes, I just realized we don’t have instructions written for this nor is the public key certificate hosted externally. So congratulations on being the first person to ask! I went through the procedure tonight from a clean bare-metal install and tomorrow I will work on writing up the all the steps and making the public key available. Thank you for your patience.Hi @joachima okay here are the instructions: yum-packaging-precompiled-kmod/UEFI.md at main · NVIDIA/yum-packaging-precompiled-kmod · GitHubIf you run into any problems please file a GitHub Issue on that repository. Also please note the key is subject to change.Thank you!!!  This should also work with Rocky Linux 8 (except for secure boot until they enable it)?  Going to give it a try now.This worked. Thank You!Hi Kmittman,I followed the step to install the Nvidia latest driver on RHEL 8.4, but after the installation completed,
nvidia-smi didn’t work, the message is:NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.The RHEL8.4 is a VM on an EXSI systemHi @kmittmanThis guide works on most my systems but on an older Nvidia GeForce GTX 745 we’re experiencing:NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.Driver is version 495.29.05	and kernel is 4.18.0-348.7.1 which should’ve been supported since 12/21/2021.  Is this graphic card just too old?  Any advise, thanks.Hi @mabarkdoll
Is this in a laptop (GT 745M) or a desktop machine? This is right on the border, for GeForce cards between Kepler (470.xx / CUDA 10.2) and Maxwell architectures; the GT 745M is Kepler and the GTX 745 is Maxwell.For Kepler GPUs, the last supported CUDA version in 10.2.x and the last NVIDIA driver branch is 470.
However, if this is indeed a Maxwell GPU then it should be working.Thanks, I think this Dell Optiplex 9020 is just too old.  I’m not seeing the nvidia MOK inside mokutil --list-enrolled after a reboot.  Despite having manually enrolled the Nvidia MOK key during boot up.  Also, Rocky Linux 8 supports secure boot (it boots), but their Rocky Linux key doesn’t show up as well.  I’m not sure if the following will resolve it or it is just my hardware being too old (https://bugs.rockylinux.org/show_bug.cgi?id=174).This is a desktop GPU so I believe it is the Maxwell that you noted.
01:00.0 VGA compatible controller: NVIDIA Corporation GM107 [GeForce GTX 745] (rev a2)Anyway, I can use this with secure boot off and still have the hard disk encrypted which is probably good enough for this older machines use case.  Thanks for your help developing this driver it is much appreciated.nvidia-bug-report.log.gz (81.5 KB)I followed the step to install the Nvidia latest driver on RHEL 8.5, but after the installation completed,
nvidia-smi didn’t work, the message is:NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.The RHEL8.5 is a VM on an EXSI system
GPU: Nvidia Quadro P2000 (Configured for Passthru to the VM)lspci | grep Quadro
0b:00.0 VGA compatible controller: NVIDIA Corporation GP106GL [Quadro P2000] (rev a1)dnf module list nvidia-driver
Updating Subscription Management repositories.
Last metadata expiration check: 0:40:57 ago on Mon 21 Mar 2022 01:05:09 PM EDT.
cuda-rhel8-x86_64
Name                        Stream                       Profiles                                  Summary
nvidia-driver               latest [e]                   default [d] [i], fm, ks, src              Nvidia driver for latest branchPowered by Discourse, best viewed with JavaScript enabled"
327,x-ray-research-reveals-hazards-in-airport-luggage-using-crystal-physics,"Originally published at:			https://developer.nvidia.com/blog/x-ray-research-reveals-hazards-in-airport-luggage-using-crystal-physics/
X-ray-powered research is aiming to target sneaky hazardous materials making their way through airport security. The study, recently published in Scientific Reports, proposes a new design for a fast and powerful X-ray diffraction (XRD) technology able to identify potential threats. The work could be a notable step toward more accurate luggage scanning in airports.  “The…Powered by Discourse, best viewed with JavaScript enabled"
328,scraping-real-estate-sites-for-data-acquisition-with-scrapy,"Originally published at:			https://developer.nvidia.com/blog/scraping-real-estate-sites-for-data-acquisition-with-scrapy/
Data is one of the most valuable assets that a business can possess. It sits at the core of data science and data analysis: without data, they’re both obsolete. Businesses that actively collect data may have a competitive advantage over those that do not. With sufficient data, organizations can better determine the cause of problems…Powered by Discourse, best viewed with JavaScript enabled"
329,evaluating-the-security-of-jupyter-environments,"Originally published at:			https://developer.nvidia.com/blog/evaluating-the-security-of-jupyter-environments/
The NVIDIA AI Red Team has developed a JupyterLab extension called jupysec to automatically assess the security of Jupyter environments.Powered by Discourse, best viewed with JavaScript enabled"
330,is-there-any-inversion-method-instead-of-pti-optimization,"PTI based method is used to invert the image into the latent code but it requires time. So is there any specific encoder based on your method?This would be a great feature to have! We do not have it implemented at the moment, and would love to see other developers/researchers working on this in the future!Will Nvidia provide funding for the project of Encoder ?You can register for academic grant program using this link: Join the NVIDIA Applied Research Accelerator ProgramThank you sirPowered by Discourse, best viewed with JavaScript enabled"
331,new-mlperf-inference-network-division-showcases-nvidia-infiniband-and-gpudirect-rdma-capabilities,"Originally published at:			https://developer.nvidia.com/blog/new-mlperf-inference-network-division-showcases-infiniband-and-gpudirect-rdma-capabilities/
In MLPerf Inference v3.0, NVIDIA made its first submissions to the newly introduced Network division, which is now part of the MLPerf Inference Datacenter suite. The Network division is designed to simulate a real data center setup and strives to include the effect of networking—including both hardware and software—in end-to-end inference performance. In the Network…Powered by Discourse, best viewed with JavaScript enabled"
332,boosting-application-performance-with-gpu-memory-prefetching,"Originally published at:			https://developer.nvidia.com/blog/boosting-application-performance-with-gpu-memory-prefetching/
This CUDA post examines the effectiveness of methods to hide memory latency using explicit prefetching.The work described in this post is derived from a real application in computational finance. Please feel free to ask questions about any details that may be unclear.Hello, normally how to decide PDIST per application to hide the memory latency?Thanks for the question. It is difficult to derive an analytical expression for the proper value of PDIST, because it depends, among others, on the occupancy of the Streaming Multiprocessors (SMs), which in turn is a function of the number of registers used per thread, and the total amount of shared memory used by the kernel, as well as the memory latency. The easiest strategy would be to vary PDIST until optimal performance is achieved. A slightly more focused approach would be to compute how much shared memory there is to spare, using the occupancy view in Nsight Compute, and choosing PDIST such that it is all used for the prefetch buffer. But this is not foolproof, because sometimes it helps to reduce the number of thread blocks per SM somewhat to free up more shared memory.Hello, shared memory padding strategy is not economic for some circumstances. Does #define vsmem(index) v[threadIdx.x + PDIST*index] works better for this post?
Besides, according to cuda programming guide, for Compute Capability 5.x and later, shared memory has 32 banks with 32-bit word. So there is no way to make a conflict-free read for double type?Yes, it would work better. As I wrote in the blog: “We could actually have arrived at this performance improvement without resorting to padding by changing the indexing scheme of the array in shared memory, which is left as an exercise for the reader.” You did the exercise!
It is indeed impossible to avoid conflicts with 64b words, but the point is that the indexing you proposed minimizes conflicts.The indexing into v should be threadIdx.x + blockDim.x*index right? Each thread essentially gets its own column (would equate to a bank for 32b words).Yes, you are right, I was too quick to respond to respondent liuws’s suggestion. Thank you for pointing out my error.Powered by Discourse, best viewed with JavaScript enabled"
333,top-deep-learning-sessions-at-nvidia-gtc-2023,"Originally published at:			Developer Conference Session Catalog | NVIDIA GTC
Explore the latest tools, optimizations, and best practices for deep learning training and inference.Powered by Discourse, best viewed with JavaScript enabled"
334,federated-learning-from-simulation-to-production-with-nvidia-flare,"Originally published at:			https://developer.nvidia.com/blog/federated-learning-from-simulation-to-production-with-nvidia-flare/
Learn about the new features of NVIDIA FLARE 2.2 that reduce development time and accelerate deployment for federated learning, helping organizations cut costs for building robust AI.Powered by Discourse, best viewed with JavaScript enabled"
335,securing-llm-systems-against-prompt-injection,"Originally published at:			https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/
This post explains prompt injection and shows how the NVIDIA AI Red Team identified vulnerabilities where prompt injection can be used to exploit three plug-ins included in the LangChain library.Powered by Discourse, best viewed with JavaScript enabled"
336,top-path-tracing-sessions-at-nvidia-gtc-2023,"Originally published at:			Game Development Session Catalog | NVIDIA GTC
Learn about the latest path tracing technologies and how they’re accelerating game development.Powered by Discourse, best viewed with JavaScript enabled"
337,thanks-everyone-for-joining-ama-is-now-closed,"Thanks everyone for their interest, if there are any follow on questions please add them to the posts quickly and we will respond offline.
Thanks !Powered by Discourse, best viewed with JavaScript enabled"
338,emulating-an-nvidia-jetson-orin-nx-using-the-nvidia-jetson-agx-orin-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/emulating-an-nvidia-jetson-orin-nx-using-the-nvidia-jetson-agx-orin-developer-kit/
Discover how to use the NVIDIA Jetson AGX Orin Developer Kit to emulate natively any of the NVIDIA Jetson Orin modules, including Jetson Orin NX and Jetson Orin Nano.Powered by Discourse, best viewed with JavaScript enabled"
339,setting-new-records-in-mlperf-inference-v3-0-with-full-stack-optimizations-for-ai,"Originally published at:			https://developer.nvidia.com/blog/setting-new-records-in-mlperf-inference-v3-0-with-full-stack-optimizations-for-ai/
Learn about the innovations behind the record-setting NVIDIA performance in MLPerf Inference v3.0.Powered by Discourse, best viewed with JavaScript enabled"
340,microsoft-and-tempoquest-accelerate-wind-energy-forecasts-with-acecast,"Originally published at:			https://developer.nvidia.com/blog/microsoft-and-tempoquest-accelerate-wind-energy-forecasts-with-acecast/
Accurate weather modeling is essential for companies to properly forecast renewable energy production and plan for natural disasters. Ineffective and non-forecasted weather cost an estimated $714 billion in 2022 alone. To avoid this, companies need faster, cheaper, and more accurate weather models. In a recent GTC session, Microsoft, and TempoQuest detailed their work with NVIDIA…GPU-accelerated climate and weather simulation enables utilities to proactively plan for extreme weather events, while improving forecast accuracy of renewable energy generation. Great to see the simulation speedups and reduced costs with TempoQuest and Microsoft Azure. If you have any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
341,upcoming-workshop-fundamentals-of-accelerated-computing-with-cuda-c-c,"Originally published at:			Accelerated Computing with CUDA C/C++ Workshop
Learn the fundamental tools and techniques for accelerating C/C++ applications to run on massively parallel GPUs with CUDA in this instructor-led workshop.Powered by Discourse, best viewed with JavaScript enabled"
342,workshop-how-to-enable-your-product-with-voice-interface,"Originally published at:			Workshop – NVIDIA Riva
This hands-on workshop guides you through the process of voice-enabling your product, from familiarizing yourself with NVIDIA Riva to assessing the costs and resources required for your project.Powered by Discourse, best viewed with JavaScript enabled"
343,simplifying-and-accelerating-machine-learning-predictions-in-apache-beam-with-nvidia-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/simplifying-and-accelerating-machine-learning-predictions-in-apache-beam-with-nvidia-tensorrt/
Loading and preprocessing data for running machine learning models at scale often requires seamlessly stitching the data processing framework and inference engine together. In this post, we walk through the integration of NVIDIA TensorRT with Apache Beam SDK and show how complex inference scenarios can be fully encapsulated within a data processing pipeline. We also…Powered by Discourse, best viewed with JavaScript enabled"
344,nvidia-grace-cpu-superchip-architecture-in-depth,"Originally published at:			https://developer.nvidia.com/blog/nvidia-grace-cpu-superchip-architecture-in-depth/
The NVIDIA Grace CPU Superchip brings together two high-performance and power-efficient NVIDIA Grace CPUs with server-class LPDDR5X memory connected with NVIDIA NVLink-C2C.Powered by Discourse, best viewed with JavaScript enabled"
345,latest-discoveries-at-the-healthcare-life-sciences-developer-summit,"Originally published at:			https://developer.nvidia.com/blog/latest-discoveries-at-the-healthcare-life-sciences-developer-summit/
Humanity has seen major scientific breakthroughs directly related to discoveries that do not share the glamor of the breakthrough they enabled. Sir Alexander Fleming’s penicillin gave rise to effective treatments for infections like pneumonia, but penicillin’s importance outshines a technology known as the Petri dish, invented by a German physician. It was in a Petri…Powered by Discourse, best viewed with JavaScript enabled"
346,how-to-build-an-instant-machine-learning-web-application-with-streamlit-and-fastapi,"Originally published at:			https://developer.nvidia.com/blog/how-to-build-an-instant-machine-learning-web-application-with-streamlit-and-fastapi/
Learn how to rapidly build your own machine learning web application using Streamlit for your frontend and FastAPI for your microservice.Powered by Discourse, best viewed with JavaScript enabled"
347,migrating-from-range-profiler-to-gpu-trace-in-nsight-graphics,"Originally published at:			https://developer.nvidia.com/blog/migrating-from-range-profiler-to-gpu-trace-in-nsight-graphics/
Starting in Nsight Graphics 2023.1, the GPU Trace Profiler is the best way to profile your graphics application at the frame level. The Frame Profiler activity, and the Range Profiler tool window, have been removed. Don’t worry! The key profiling information is still available, only in a different form. This post guides you through the…Powered by Discourse, best viewed with JavaScript enabled"
348,upcoming-webinar-be-an-nvidia-isaac-ros-devops-hero-with-containerized-development,"Originally published at:			Isaac ROS Webinar Series
On February 14, get introduced to Docker and Continuous Integration, and deep dive into NVIDIA Isaac ROS, setting up a basic docker, and building Isaac ROS packages for distribution.Powered by Discourse, best viewed with JavaScript enabled"
349,how-to-train-a-defect-detection-model-using-synthetic-data-with-nvidia-omniverse-replicator,"Originally published at:			https://developer.nvidia.com/blog/how-to-train-a-defect-detection-model-using-synthetic-data-with-nvidia-omniverse-replicator/
Learn how to train an object detection model entirely with synthetic data, improve its accuracy with limited ground truth real data, and validate it against images that model has never seen before.Thanks for the article, it’s a good reference design.I started playing with Replicator using a similar method, i.e. putting all my Replicator code in an extension so I could use VS Code to debug.  The problem I have though is Omniverse Code crashes after a few iterations through the debugger.  I have a VERY simple Replicator extension and I’ve tried everything I could to free memory and such to avoid memory issues, but it still crashes.  Headless mode works fine but there’s no debugger and I’m not the best Python programmer so I rely on the VS Code debugger / Github Copilot to hold my hand.  It’s extremely frustrating when I have to restart OV Code every few tries of my Replicator code, obviously.I’m on Windows 11 with 64 GB RAM and an RTX 4090.  What kind of machine are people using to successfully work with Replicator extensions?  What machine was this extension developed on.  Should I just max out my RAM and pray or what?  Should I switch to Linux?  What’s a known reference machine specs for using Replicator with no crashes, currently?Any pointers appreciated.Thanks,
DaveYour computer specs look great, I would start by making sure that OV code, your computer, and especially your GPU drivers are all up to date.Let me know how that goes and we’ll move forward from there!EricPowered by Discourse, best viewed with JavaScript enabled"
350,explainer-what-is-agent-assist,"Originally published at:			What Is Agent Assist? | NVIDIA Blog
Agent-assist technology uses AI and ML to provide facts and make real-time suggestions that help human agents across retail, telecom, and other industries conduct conversations with customers.Powered by Discourse, best viewed with JavaScript enabled"
351,gtc-2020-interactive-8k-video-editing-on-rtx-studio-laptops,"GTC 2020 D2S24
Presenters: Tech Demo Team,NVIDIA
Abstract
Blackmagic Design has integrated GPU acceleration for advanced video editing and visual effects,
including several exciting AI-based features in DaVinci Resolve Studio 16 creative app.Video editors can now create high-quality, smooth slow motion without high-speed camera footage
using Speed Warp. They can color and refine facial features using Face Refinement and easily remove
objects from video using Object Removal and much more functions. These effects and tools are
accelerated with NVIDIA CUDA and Tensor Cores empowering editors to produce high quality videos in
seconds instead of hours and iterate faster than ever before. With RTX laptops video professionals can
take advantage of these powerful features wherever they need to be.Watch this session
Join in the conversation below.Hi, Nicely demonstrated. Thanks for the demo. By the way which laptop is used for this video editing and from what storage the 8k raw video is playing? Which RTX card is used for the demo? Thank you.Powered by Discourse, best viewed with JavaScript enabled"
352,new-course-gpu-acceleration-with-the-c-standard-library,"Originally published at:			Courses – NVIDIA
Learn how to write simple, portable, parallel-first GPU-accelerated applications using only C++ standard language features in this self-paced course from the NVIDIA Deep Learning InstitutePowered by Discourse, best viewed with JavaScript enabled"
353,efficiently-scale-llm-training-across-a-large-gpu-cluster-with-alpa-and-ray,"Originally published at:			https://developer.nvidia.com/blog/efficiently-scale-llm-training-across-a-large-gpu-cluster-with-alpa-and-ray/
When used together, Alpa and Ray offer a scalable and efficient solution to train LLMs across large GPU clusters.Powered by Discourse, best viewed with JavaScript enabled"
354,using-a-network-digital-twin-as-an-it-training-tool,"Originally published at:			https://developer.nvidia.com/blog/using-a-network-digital-twin-as-an-it-training-tool/
As organizations rely on complex network systems to support their operations, the need for well-trained network administrators is becoming increasingly important. Data center infrastructure is interconnected in ways that are not always obvious, and the points of intersection between systems are often difficult to design on paper alone.  In order to ensure that network administrators…Powered by Discourse, best viewed with JavaScript enabled"
355,rapidly-build-ai-streaming-apps-with-python-and-c,"Originally published at:			https://developer.nvidia.com/blog/rapidly-build-ai-streaming-apps-with-python-and-c/
The computational needs for AI processing of sensor streams at the edge are increasingly demanding. Edge devices must keep up with high rates of incoming data streams, processing, displaying, archiving, and streaming results or closing a control loop in real time. This requires powerful, efficient, and accurate hardware and software solutions capable of high performance…Powered by Discourse, best viewed with JavaScript enabled"
356,reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft,"Originally published at:			https://developer.nvidia.com/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/
In many data analytics and machine learning algorithms, computational bottlenecks tend to come from a small subset of steps that dominate the end-to-end performance. Reusable solutions for these steps often require low-level primitives that are non-trivial and time-consuming to write well. NVIDIA made RAPIDS RAFT to address these bottlenecks and maximize reuse when building algorithms…Powered by Discourse, best viewed with JavaScript enabled"
357,how-ai-enabled-functionality-is-transforming-5g-ran,"Originally published at:			https://developer.nvidia.com/blog/how-ai-enabled-functionality-is-transforming-5g-ran/
AI is transforming 5G RAN in four key ways: energy savings, mobility management and optimization, load balancing, and Cloud RAN.Powered by Discourse, best viewed with JavaScript enabled"
358,can-we-mix-llms-with-other-models-like-stable-diffusion-for-gen-ai,"Can we mix LLM’s with other models like Stable Diffusion for Gen AI?Yes - LLM can be used in various ways as a preprocess pipeline to connect to SD. For example, you might be able to ask LLM to to stylize a prompt based on plain English, then it can be passed to SD. Technically, it’s all about connecting APIs togetherThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
359,how-speech-recognition-improves-customer-service-in-telecommunications,"Originally published at:			https://developer.nvidia.com/blog/how-speech-recognition-improves-customer-service-in-telecommunications/
The telecommunication industry has seen a proliferation of AI-powered technologies in recent years, with speech recognition and translation leading the charge. Multi-lingual AI virtual assistants, digital humans, chatbots, agent assists, and audio transcription are technologies that are revolutionizing the telco industry. Businesses are implementing AI in call centers to address incoming requests at an accelerated…Powered by Discourse, best viewed with JavaScript enabled"
360,finding-out-where-your-application-and-network-intersect,"Originally published at:			https://developer.nvidia.com/blog/finding-out-where-your-application-and-network-intersect/
NetQ is a scalable, modern network operations toolset that provides network visibility in real-time.Powered by Discourse, best viewed with JavaScript enabled"
361,ask-me-anything-experts-answer-your-nvidia-cugraph-questions-live,"Originally published at:			AMA cuGraph: Graph analysis and GNN - NVIDIA Developer Forums
Join us April 12 and ask experts about NVIDIA cuGraph, which recently added support for GNN with accelerated aggregators, models, and extensions to DGL and PyG.Powered by Discourse, best viewed with JavaScript enabled"
362,bringing-data-center-management-features-to-the-edge,"Originally published at:			Bringing Data Center Management Features to the Edge | NVIDIA Technical Blog
NVIDIA Fleet Command announced new features giving IT administrators more advanced controls and protection for edge environments. Unlike traditional data centers with hundreds of servers in a single location, edge deployments have one or two servers in thousands of locations. Traditional IT management tools struggle to meet the needs of these distributed environments, especially when…Powered by Discourse, best viewed with JavaScript enabled"
363,boosting-application-performance-with-gpu-memory-access-tuning,"Originally published at:			https://developer.nvidia.com/blog/boosting-application-performance-with-gpu-memory-access-tuning/
In this post, we examine a method programmers can use to saturate memory bandwidth on a GPU.This blog saves the best part for the end. Users interested in tuning performance of their CUDA kernels should always try and use launch bounds first. In this particular case study that would have been enough. But not always, of course.Great walkthrough, thank you! Can you please elaborate on why the duration of all kernel variants suddenly dropped to the same value (~10 ns) starting from ~20th kernel launch (Figure 1)?  Does that mean that for an application with thousands of kernel launches all the described optimizations are actually useless?Good question. The plot of kernel durations shown actually repeats itself in the application from which it was derived. Figure 1 shows one full period of that repetition. After the kernel duration drops, it jumps back up again, and substantial overall application performance improvement is obtained from the optimizations described in the blog.Thank you, very intriguing! Is there an explanation for such a huge variation (3x-5x!) of kernel durations?Great walkthrough, thank you! Can you please elaborate on why the duration of all kernel variants suddenly dropped to the same value (~10 ns) starting from ~20th kernel launch (Figure 1)? Does that mean that for an application with thousands of kernel launches all the described optimizations are actually useless?thanks for the awesome information.Mark, the drop is due to a smaller data set being processed by the kernel. But, as I said, it’s a cyclic process, and the size increases again periodically.Thank you, robv, now it is all clear! I will give it a go for sure.пн, 8 авг. 2022 г. в 18:45, Robv via NVIDIA Developer Forums <notifications@nvidia.discoursemail.com>:Very good, Mark. Please let me know if you have more questions.I think there’s a typo in the third paragraph of the launch bounds section: “each thread can use up to 64 threads”. I think that last word should be “registers”.You are correct again, thank you. We’ll fix the typo.Fixed! Thanks, @dwatersg!Great walkthrough, thank you! Can you please elaborate on why the duration of all kernel variants suddenly dropped to the same value (~10 ns) starting from ~20th kernel launch (Figure 1)? Does that mean that for an application with thousands of kernel launches all the described optimizations are actually useless?Ometv
thanks for the awesome information.thanks my issue has been fixed.Powered by Discourse, best viewed with JavaScript enabled"
364,the-fluid-dynamics-revolution-driven-by-gpu-acceleration,"Originally published at:			https://developer.nvidia.com/blog/the-fluid-dynamics-revolution-driven-by-gpu-acceleration/
The end of 2021 and beginning of 2022 saw the two largest commercial CFD tool vendors, Ansys and Siemens, both launch versions of their flagship CFD tools with support for GPU acceleration.Interesting !
For a fair comparison, it would be great to know about the precise amount of CPU cores and CPU references for both CFD tools.
Here I read for Star-CCM+ that you compare A100 GPU over “100 cores of CPU” (I assume Intel Platinum 8380), whereas you compare to 80 Intel Xeon CPU cores (which CPU model ?) for Fluent.
Could you provide more details about this ?Thanks !CharlesHello Charles,
The Intel Xeon 80 cores referenced is Xeon Platinum 8380. AMD Rome is EPYC 7742 and AMD Milan is EPYC 7763.Baskar.I have a question about CFD, does anyone know which CFD software is compatible with the jetson AGX Xavier module?Powered by Discourse, best viewed with JavaScript enabled"
365,turn-2d-images-into-immersive-3d-scenes-with-nvidia-instant-nerf-in-vr,"Originally published at:			Turn 2D Images into Immersive 3D Scenes with NVIDIA Instant NeRF in VR | NVIDIA Technical Blog
Thousands of developers and content creators have built stunning 3D visuals with NVIDIA Instant NeRF, a rendering tool that turns a set of static images into a realistic 3D scene. Now, it is also possible to navigate Instant NeRF in VR and step into 3D creations with the latest Instant NeRF software update. Named by…Powered by Discourse, best viewed with JavaScript enabled"
366,an-introduction-to-large-language-models-prompt-engineering-and-p-tuning,"Originally published at:			https://developer.nvidia.com/blog/an-introduction-to-large-language-models-prompt-engineering-and-p-tuning/
ChatGPT has made quite an impression. Users are excited to use the AI chatbot to ask questions, write poems, imbue a persona for interaction, act as a personal assistant, and more. Large language models (LLMs) power ChatGPT, and these models are the topic of this post.  Before considering LLMs more carefully, we would first like…Powered by Discourse, best viewed with JavaScript enabled"
367,boost-your-ai-workflows-with-federated-learning-enabled-by-nvidia-flare,"Originally published at:			https://developer.nvidia.com/blog/boost-your-ai-workflows-with-federated-learning-enabled-by-nvidia-flare/
NVIDIA FLARE 2.3.0 enables you to quickly deploy to multi-cloud and explore NLP examples for LLMs, and demonstrates split learning capability.Powered by Discourse, best viewed with JavaScript enabled"
368,nvidia-tools-extension-api-nvtx-annotation-tool-for-profiling-code-in-python-and-c-c,"Originally published at:			https://developer.nvidia.com/blog/nvidia-tools-extension-api-nvtx-annotation-tool-for-profiling-code-in-python-and-c-c/
As PyData leverages much of the static language world for speed including CUDA, we need tools which not only profile and measure across languages but also devices, CPU, and GPU.  While there are many great profiling tools within the Python ecosystem: line-profilers like cProfile and profilers which can observe code execution in C-extensions like PySpy/Viztracer. …Thanks Ben Zaitlen - this is an excellent tutorial and resource!Powered by Discourse, best viewed with JavaScript enabled"
369,create-real-time-simulations-with-nvidia-omniverse-and-bentley-lumenrt,"Originally published at:			https://developer.nvidia.com/blog/create-real-time-simulations-with-nvidia-omniverse-and-bentley-lumenrt/
Organizations across industries are using LumenRT for NVIDIA Omniverse, powered by Bentley iTwin Platform, to create compelling, high-quality visualizations and project deliverables.Powered by Discourse, best viewed with JavaScript enabled"
370,end-to-end-ai-for-workstation-an-introduction,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-workstation-an-introduction/
This post is the first in a series about optimizing end-to-end AI for workstations. For more information, see part 2, End-to-End AI for Workstation: Transitioning AI Models with ONNX, and part 3, End-to-End AI for Workstation: ONNX Runtime and Optimization. The great thing about the GPU is that it offers tremendous parallelism; it allows you…We hope everyone finds this introduction to optimizing workstation AI a helpful starting point. If you have any questions or comments, let us knowPowered by Discourse, best viewed with JavaScript enabled"
371,long-term-support-for-nvidia-doca-1-5,"Originally published at:			https://developer.nvidia.com/blog/long-term-support-for-nvidia-doca-1-5/
Today, NVIDIA announced the long-term support (LTS) release of NVIDIA DOCA 1.5. NVIDIA DOCA is the open cloud SDK and acceleration framework for NVIDIA BlueField DPUs. It unlocks data center innovation by enabling you to rapidly create applications and services for BlueField DPUs by using industry-standard APIs. The new NVIDIA DOCA 1.5 release includes several…LTS shows NVIDIAs commitment to build the BlueFied DPU and DOCA ecosystem!Powered by Discourse, best viewed with JavaScript enabled"
372,bringing-far-field-objects-into-focus-with-synthetic-data-for-camera-based-av-perception,"Originally published at:			https://developer.nvidia.com/blog/bringing-far-field-objects-into-focus-with-synthetic-data-for-camera-based-av-perception/
The NVIDIA DRIVE AV team improved detection accuracy of far-field objects using synthetic camera data generated in NVIDIA DRIVE Sim, leveraging NVIDIA Omniverse Replicator.Powered by Discourse, best viewed with JavaScript enabled"
373,top-smart-spaces-sessions-at-nvidia-gtc-2023,"Originally published at:			Developer Conference Session Catalog | NVIDIA GTC
Learn how AI is enabling safer, more sustainable cities and improving operational efficiency in public spaces for our communities.Powered by Discourse, best viewed with JavaScript enabled"
374,just-released-new-updates-to-nvidia-riva,"Originally published at:			Riva Getting Started | NVIDIA Developer
Build better GPU-accelerated Speech AI applications with the latest NVIDIA Riva updates, including enterprise support.Powered by Discourse, best viewed with JavaScript enabled"
375,gaugan-wins-major-awards-at-siggraph-2019s-real-time-live-competition,"Originally published at:			https://developer.nvidia.com/blog/gaugan-wins-major-awards-at-siggraph-2019s-real-time-live-competition/
GauGAN, NVIDIA’s viral real-time AI art application just won two major SIGGRAPH 2019 awards, “Best of Show” and “Audience Choice,” at the “Real Time Live” competition at SIGGRAPH 2019Powered by Discourse, best viewed with JavaScript enabled"
376,explainer-what-is-path-tracing,"Originally published at:			What Is Path Tracing? | NVIDIA Blog
Path tracing is going real-time, unleashing interactive, photorealistic 3D environments filled with dynamic light and shadow, reflections, and refractions.Powered by Discourse, best viewed with JavaScript enabled"
377,explainer-what-is-an-ai-cockpit,"Originally published at:			What Is an AI Cockpit? | NVIDIA Blog
Intelligent interiors are transforming transportation.Powered by Discourse, best viewed with JavaScript enabled"
378,ama-with-the-cugraph-engineering-team-april-12-2023-9am-pdt,"Join us for an AMA with the engineering team behind cuGraph
When: April 12th, 9am (PDT)
Where: Right here in this forum directory
How do I participate?What is cuGraph?
cuGraph is a robust, full feature, suite of graph analytics libraries. The core cuGraph library includes about three dozen algorithms, with most support scaling to massive GPU clusters. The cuGraph PageRank algorithm, for example has been test on a 4.4 trillion edge graph spread over 2,048 GPU. Even at that scale, a single PageRank iteration only took 1.5 seconds. Recently cuGraph has added supports for GNN with accelerated aggregators, accelerated models, and extensions to both DGL and PyG.Just a reminder to join us for the live AMA with the cuGraph Engineering team leads tomorrow at 9am (PDT)
See you tomorrow.Both NVIDIA Register Buttons in the email that was sent, forward to this Forum.
https://forums.developer.nvidia.com/Can you please post here the correct working registration URL for this event?The actual AMA event happens in this sub category:Join us for an AMA with the engineering team behind cuGraph
When: April 12th, 9am (PDT)
Where: Right here in this forum directoryCan cuGraph accommodate sparse adjacency matrix as input? Can I use it as an extension of cuSparse?please repost your question here: AMA cuGraph: Graph analysis and GNN - NVIDIA Developer Forums  I don’t think this post is monitored for questions :)Powered by Discourse, best viewed with JavaScript enabled"
379,new-workshop-data-parallelism-how-to-train-deep-learning-models-on-multiple-gpus,"Originally published at:			Data Parallelism - Train Deep Learning Models on Multiple GPUs | NVIDIA
Learn how to decrease model training time by distributing data to multiple GPUs, while retaining the accuracy of training on a single GPU.Powered by Discourse, best viewed with JavaScript enabled"
380,using-the-nvidia-cuda-stream-ordered-memory-allocator-part-2,"Originally published at:			https://developer.nvidia.com/blog/using-the-nvidia-cuda-stream-ordered-memory-allocator-part-2/
In part 1 of this series, we introduced new API functions, cudaMallocAsync and cudaFreeAsync, that enable memory allocation and deallocation to be stream-ordered operations. In this post, we highlight the benefits of this new capability by sharing some big data benchmark results and provide a code migration guide for modifying your existing applications. We also…This does improve some performance, but there is a question。
Is it possible to pre-allocate a large chunk of video memory and then assign values directly to this chunk of video memory，Will this performance be better? The following part of the code：float* in_d;
float* in_d_sin[10];
cudaMalloc((void**)&in_d, 4 * 10);// sizeof(float)=4
for (int i = 0; i < 10; i++) {
in_d_sin[i] = in_d + i;
}Hey @bjhd_qcj could you elaborate a bit more on your question?I’m not sure how it relates to cudaMallocAsync or performance.Your code as written wouldn’t work because you’re allocating device memory with cudaMalloc and then attempting to write to it from host code in the for loop. For that to work you would need to use cudaMallocManaged.When the model is hot loaded or unloaded, the service experiences performance jitters for several minutes.
The initial analysis of the phenomenon is caused by the slow allocation of video memory, because the video memory grows slowly during the jitter.
Further analysis, our model will call cudaMalloc 3000 times when loading, guess it may be caused by this. So, we want to reduce cudaMalloc calls.Sounds like you should try out cudaMallocAsync and see if it makes a difference!Powered by Discourse, best viewed with JavaScript enabled"
381,running-an-ultrasound-multi-ai-pipeline-from-icardio-ai-in-real-time-with-nvidia-holoscan,"Originally published at:			https://developer.nvidia.com/blog/running-an-ultrasound-multi-ai-pipeline-from-icardio-ai-in-real-time-with-nvidia-clara-holoscan/
Discover how NVIDIA Inception member iCardio.ai leverages NVIDIA Clara Holoscan to run their multi-AI pipelines in real time, increasing the accuracy of cardiovascular diagnoses.Powered by Discourse, best viewed with JavaScript enabled"
382,just-released-nvidia-hpc-sdk-v23-7,"Originally published at:			NVIDIA HPC SDK 23.7 Downloads | NVIDIA Developer
NVIDIA HPC SDK version 23.7 is now available and provides minor updates and enhancements.Powered by Discourse, best viewed with JavaScript enabled"
383,create-realistic-robotics-simulations-with-ros-2-moveit-and-nvidia-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/create-realistic-robotics-simulations-with-ros-2-moveit-and-nvidia-isaac-sim/
Learn how to integrate MoveIt 2 with a robot simulated in NVIDIA Isaac Sim.there is a problem with this command :it should clone main branch , there is no docker compose on humble branchPowered by Discourse, best viewed with JavaScript enabled"
384,optimizing-bim-workflows-using-usd-at-every-design-phase,"Originally published at:			https://developer.nvidia.com/blog/optimizing-bim-workflows-using-usd-at-every-design-phase/
Siloed data has long been a challenge in architecture, engineering, and construction (AEC), hindering productivity and collaboration. However, new innovative solutions are transforming the way that architects, engineers, and construction managers work together on BIM (building information management) workflows, offering new possibilities for real-time collaboration. The new NVIDIA Omniverse Connector from Vectorworks exemplifies this potential,…Powered by Discourse, best viewed with JavaScript enabled"
385,near-range-obstacle-perception-with-early-grid-fusion,"Originally published at:			https://developer.nvidia.com/blog/near-range-obstacle-perception-with-early-grid-fusion/
Automatic parking assist must overcome some unique challenges when perceiving obstacles. An ego vehicle contains sensors that perceive the environment around the vehicle. During parking, the ego vehicle must be close to dynamic obstacles like pedestrians and other vehicles, as well as static obstacles such as pillars and poles. To fit into the parking spot,…Powered by Discourse, best viewed with JavaScript enabled"
386,icymi-new-and-updated-ai-workflows-announced-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/icymi-new-and-updated-ai-workflows-announced-at-gtc-2023/
At NVIDIA GTC 2023, NVIDIA showed how AI workflows can be leveraged to help you accelerate the development of AI solutions to address a range of use cases. AI workflows are cloud-native, packaged reference examples showing how NVIDIA AI frameworks can be used to efficiently build AI solutions such as intelligent virtual assistants, digital fingerprinting…Powered by Discourse, best viewed with JavaScript enabled"
387,common-challenges-with-conducting-an-edge-ai-proof-of-concept,"Originally published at:			https://developer.nvidia.com/blog/common-challenges-with-conducting-an-edge-ai-proof-of-concept/
A proof-of-concept (POC) is the first step towards a successful edge AI deployment. Companies adopt edge AI to drive efficiency, automate workflows, reduce cost, and improve overall customer experiences. As they do so, many realize that deploying AI at the edge is a new process that requires different tools and procedures than the traditional data…Powered by Discourse, best viewed with JavaScript enabled"
388,diagnosing-network-issues-faster-with-nvidia-wjh,"Originally published at:			https://developer.nvidia.com/blog/diagnosing-network-issues-faster-with-wjh/
AI has seamlessly integrated into our lives and changed us in ways we couldn’t even imagine just a few years ago. In the past, the perception of AI was something futuristic and complex. Only giant corporations used AI on their supercomputers with HPC technologies to forecast weather and make breakthrough discoveries in healthcare and science.…Powered by Discourse, best viewed with JavaScript enabled"
389,new-sdks-accelerating-ai-research-computer-vision-data-science-and-more,"Originally published at:			https://developer.nvidia.com/blog/new-sdks-accelerating-ai-research-computer-vision-data-science-and-more/
At GTC 2022, NVIDIA revealed major updates to its suite of NVIDIA AI software for developers. The updates accelerate computing in several areas, such as machine learning research with NVIDIA JAX, AI imaging and computer vision with NVIDIA CV-CUDA, and data science workloads with RAPIDS. To learn about the latest SDK advancements from NVIDIA, watch…Powered by Discourse, best viewed with JavaScript enabled"
390,sculpture-symmetry,"I am working with Get3D to try to generate novel sculpture from my existing sculptures, they look like this. I altered the generator to help it generate the complex topology, and it works: Get3D can replicate sculptures faithfully and provide small neighborhoods around them.
However, interpolated forms, and all generated forms that do not replicate dataset items, are asymmetrical even if all dataset items have the same symmetry group. I would like to add a loss term to lock in the symmetry. Any ideas how to do this?PS my version runs on one GTX1070 with 8GB, the generator has 6M parameters. That seems like enough, except for the symmetry issue it’s working, it can make motorbikes if needed. :-)Enforcing symmetry in the shape is possible by mirroring the signed distance function (SDF) values with respect to a plane of symmetry. For instance, if you’d like the shape to be symmetric with respect to the x-y plane, you can add a line after GET3D/networks_get3d.py at 2a09f5836e2a3ed49a37a6825a7139d11328446d · nv-tlabs/GET3D · GitHub to set sdf[ …, sdf.shape[-1]//2 + k] = sdf[ …,sdf.shape[-1]//2 - k], for k in range(sdf.shape[-1]//2). Additionally, you can also inference on only half of the grid to save memory.Thank you, I work with groups which include rotations and inversion transforms, in addition to reflection, but what you say can generalize.Enforcing rigid symmetry in this way, or inferencing only on the fundamental unit of the grid, will clearly work.  However it would create rigidly exact symmetric output, and I am looking for fully generated “fuzzy” symmetry which would (for example) allow interpolation between data items with different symmetry groups.I tried comparing the generated mesh vertices to themselves under symmetry transforms using sliced optimal transport, and using that distance as a loss. I also tried doing a similarity loss on images from symmetrically related cameras. In both cases the generator could not learn to reduce asymmetry below random chance. I feel like it “should” be able to learn this, but I haven’t thought of the right way to train it.Powered by Discourse, best viewed with JavaScript enabled"
391,open-beta-nvidia-cunumeric-and-nvidia-legate,"Originally published at:			cuNumeric Library Download | NVIDIA Developer
NVIDIA announces the cuNumeric and Legate beta release. The cuNumeric library provides an accelerated NumPy alternative, while Legate provides a parallel computing runtime abstraction layer.Powered by Discourse, best viewed with JavaScript enabled"
392,getting-to-know-autonomous-vehicles,"Originally published at:			https://developer.nvidia.com/blog/getting-to-know-autonomous-vehicles/
The future is autonomous, and AI is already transforming the transportation industry. But what exactly is an autonomous vehicle and how does it work? Autonomous vehicles are born in the data center. They require a combination of sensors, high-performance hardware, software, and high-definition mapping to operate without a human at the wheel. While the concept…Powered by Discourse, best viewed with JavaScript enabled"
393,run-multiple-ai-models-on-the-same-gpu-with-amazon-sagemaker-multi-model-endpoints-powered-by-nvidia-triton-inference-server,"Originally published at:			https://developer.nvidia.com/blog/run-multiple-ai-models-on-same-gpu-with-sagemaker-mme-powered-by-triton/
Last November, AWS integrated open-source inference serving software, NVIDIA Triton Inference Server, in Amazon SageMaker. Machine learning (ML) teams can use Amazon SageMaker as a fully managed service to build and deploy ML models at scale. With this integration, data scientists and ML engineers can easily use the NVIDIA Triton multi-framework, high-performance inference serving with…Powered by Discourse, best viewed with JavaScript enabled"
394,upcoming-event-nvidia-jetson-edge-ai-developer-days,"Originally published at:			Jetson Developer Day at GTC 2023
At NVIDIA GTC 2023 join robotics, edge AI, and computer vision experts for a deep dive into building next-generation AI-powered applications and autonomous machines.Powered by Discourse, best viewed with JavaScript enabled"
395,please-note-we-can-not-comment-on-unannouced-features,"Thanks for your understandingPowered by Discourse, best viewed with JavaScript enabled"
396,a-guide-to-monitoring-machine-learning-models-in-production,"Originally published at:			https://developer.nvidia.com/blog/a-guide-to-monitoring-machine-learning-models-in-production/
How can machine learning models in production be monitored effectively? What specific metrics need to be monitored? What tools are most effective? Get the answers to these questions and more.""""In the context of machine learning, monitoring refers to the process of tracking the behavior of a deployed model to analyze performance. Monitoring a machine learning model after deployment is vital, as models can break and degrade in production. Deployment is not a one-time action that you do and forget about. “”Deployment is not  — I guess it should say, monitoring is not a one time actionPowered by Discourse, best viewed with JavaScript enabled"
397,nvidia-jetson-project-of-the-month-an-ai-powered-autonomous-miniature-race-car-gets-on-track,"Originally published at:			https://developer.nvidia.com/blog/nvidia-jetson-project-of-the-month-an-ai-powered-autonomous-miniature-race-car-gets-on-track/
The 65th annual Daytona 500 will take place on February 19, 2023 and for many this elite NASCAR event is the pinnacle of the car racing world. For now, there are no plans to see an autonomous vehicle racing against cars with drivers, but it’s not too hard to imagine that scenario at a future…Powered by Discourse, best viewed with JavaScript enabled"
398,top-recommender-system-sessions-at-nvidia-gtc-2023,"Originally published at:			Restaurants & Quick-Service Conference Session Catalog | NVIDIA GTC
Get training, insights, and access to experts for the latest in recommender systems.Powered by Discourse, best viewed with JavaScript enabled"
399,regarding-generating-3d-shapes-of-cars-bikes-and-chairs,"Hi Team. I ran the inference code on colab and was able to generate 25 shapes of Cars, Bikes and chairs. Is there a way to create more? and is there way to create 3d shapes of other categories?Yes, it’s possible to generate more objects, you can simply change a grid_size to a larger number in this line GET3D/inference_3d.py at master · nv-tlabs/GET3D · GitHub e.g. something like (10,10) can give you 100 shapes. It’s also possible create other categories, but it would require us to train the GET3D for it, right now due to license restrictions, we are only able to releaase, cars, bikes, chairs and tables.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
400,getting-kubernetes-ready-for-the-nvidia-a100-gpu-with-multi-instance-gpu,"Originally published at:			https://developer.nvidia.com/blog/getting-kubernetes-ready-for-the-a100-gpu-with-multi-instance-gpu/
Multi-Instance GPU (MIG) is a new feature of the latest generation of NVIDIA GPUs, such as A100. It enables users to maximize the utilization of a single GPU by running multiple GPU workloads concurrently as if there were multiple smaller GPUs. MIG supports running multiple workloads in parallel on a single A100 GPU or allowing…Hello,
I tried the instructions in the blog, it failed at the command:
‘sudo helm install --version=0.13.0-rc.2 --generate-name --set migStrategy=none nvdp/k8s-device-plugin’
I have 0.13.0.-rc.2 devic-plugin, but it reported the following error:
‘INSTALLATION FAILED: chart “k8s-device-plugin” matching 0.13.0-rc.2 not found in nvdp index’how to continue from here?thanks,Hello,
NVIDIA has changed the version names recently - please follow this guideNVIDIA device plugin for Kubernetes. Contribute to NVIDIA/k8s-device-plugin development by creating an account on GitHub.to install the current latest version (v0.12.3)
Thanks!The 0.13.0-rc.2 version is a release candidate. The full version of 0.13.0 will be released in the next couple of weeks.If you really want to run with 0.13.0-rc.2, then you can just add --devel to your helm command line so it can find release candidate versions like this.I got it run with GPU-operator. It works to remotely config MIG. I only applied ‘single’ policy for now.
thanks,Powered by Discourse, best viewed with JavaScript enabled"
401,cooperative-groups-flexible-cuda-thread-programming,"Excellent blog, thank you so much. As a minor observation, in reduce_sum_tile_shfl 'lane' seems unused.Good article--but looking for the follow up on multiblock synchronization. The user guide only talks about synchronizing the entire grid_group, but how do I synchronize among a subset of blocks in a grid_group? For example, I want to synchronize threads in the ""Z"" dimension but not all X,Y,Z blocks.Hi David, synchronizing a subset of blocks is not currently supported. Currently there's no partitioning capability for `grid_group`.Good catch! Fixed.It is part of the GPU instruction set. https://docs.nvidia.com/cud...Hello,I don't understand the purpose of second g.sync() in        temp[lane] = val;        g.sync(); // wait for all threads to store        if (lane < i) val += temp[lane + i];        g.sync(); // wait for all threads to loadLoads are done from second half of temp, while stores are essentially done to first half of temp (second half of vals is not updated because of ""if (lane < i)""). Isn't second g.sync() unnecessary?Hi Igor, while technically your suggestion may work for this specific code, in general it's incorrect to remove one of the syncs. You would probably have to mark temp as volatile, which is a hack. The g.sync()s prevent the compiler from performing code motion optimizations across the synchronization points. Without them you have a race condition, even if the data involved in the race is beyond what is used by the algorithm. As an example, if you changed from this downward reduction to a so-called ""butterfly"" reduction (using xor rather than + for the indexing), both syncs are absolutely required.hi,is the follow up post on the grid_group's already available?Questions (1 and 2) below might be showing a misunderstanding on my part.1) Should the comment ""Each thread adds its partial sum[i] to sum[lane+i]"" be something like ""Each thread adds the partial sum[lane+i] to its accumulator sum[lane] (only lane 0 will have the full accumulated value)""?2) For sum_kernel_block(), doesn't thread_sum() assumes n is divisible by 4, and doesn't the formula for nBlocks require n/4?3) Is the optimization of loop unrolling a -O2 feature or a -O3 feature?  (My project can only use -O2.)4) Is the optimization of removing the synchronization statement for warps a -O2 feature or a -O3 feature?5) After a vector-4 load, if a device function foo() is called as foo(v.x); foo(v.y); foo(v.z); foo(v.w);, will the compiler optimize across the four invocations of foo() or will each invocation be treated as a ""basic block of optimization""?  Please consider both optimization flags of -O2 and -O3.6) Please consider updating the grid-size loop blog with a section on threads working on vectors.     .To make this reduction compatible with input that is not divisible by 4, thread_sum() needs to be modified:To fix 2), see my comment on the main article.Are the .match_any() and .match_all() methods available on all generations? I know the definition of the intrinsics came by in Volta and newer generation, but can I make a function that does a similar job on previous architectures?Hello, any follow up regarding grid sync or device sync available?I am also looking for variant of grid synchronisation like the following:I would like to achieve synchronisation among the active thread blocks scheduled on a GPU.
Is this possible to do with the current co-operative thread grouping and grid synchronisation concept ?My requirement is that a current scheduled thread blocks co-operatively load a memory segment into shared memory and then compute and then synchronize until both are complete…I found two new functions in cooperative_groups. Primarily, sync_grids and sync_warp (inside include/cooperative_groups/sync.h). I wanted to know if there are any opensource or public projects that use these primitives. Can someone point to those?the last part of atomicAggInc, shouldn’t the returnbe a int* prev ?reference code:No, it returns the value, not the pointer.  The pointer should not change.  “prev” probably isn’t a good name for the variable, though.Oh I see, It’s my fault. I misunderstood the function of atomicAdd: it’s first parameter should be a pointer not a value. Thank you for the reply.By the way, do we need to add a cg::sync() after the if statement ?  Because in theory, as far as I can see,  we should add a sync here. And I wonder if the compiler will add a sync for us and we don’t have to write it down explicitly?Hi Is it available on cuda-11 ? and can anyone tell me what is the use of vec3_to_linear call ?This looks like it was fixed but seems to have reverted; the article says cg::partition again.Powered by Discourse, best viewed with JavaScript enabled"
402,how-to-get-better-outputs-from-your-large-language-model,"Originally published at:			https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/
Large language models (LLMs) have generated excitement worldwide due to their ability to understand and process human language at a scale that is unprecedented. It has transformed the way that we interact with technology. Having been trained on a vast corpus of text, LLMs can manipulate and generate text for a wide variety of applications…Powered by Discourse, best viewed with JavaScript enabled"
403,overview-of-zero-shot-multi-speaker-tts-systems-top-q-as,"Originally published at:			https://developer.nvidia.com/blog/overview-of-zero-shot-multi-speaker-tts-systems-top-qas/
The Speech AI Summit is an annual conference that brings together experts in the field of AI and speech technology to discuss the latest industry trends and advancements. This post summarizes the top questions asked during Overview of Zero-Shot Multi-Speaker TTS System, a recorded talk from the 2022 summit featuring Coqui.ai. Synthesizing a voice with…Powered by Discourse, best viewed with JavaScript enabled"
404,creating-3d-visualizations-from-x-ray-data-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/creating-3d-visualizations-from-x-ray-data-with-deep-learning/
Using AI researchers have developed a new method for turning X-ray data into 3D visualizations, hundreds of times faster than traditional methods.In case this forum is still open, is this technology applicable to creating 3D visualizations from medical x-rays that visualize the structure of the body from  anterior-posterior and lateral views?@momentumhealth  – That would be a great question for the original researcher team!Powered by Discourse, best viewed with JavaScript enabled"
405,transforming-ipsec-deployments-with-nvidia-doca-2-0,"Originally published at:			https://developer.nvidia.com/blog/transforming-ipsec-deployments-with-nvidia-doca-2-0/
Announced in March 2023, NVIDIA DOCA 2.0, the newest release of the NVIDIA SDK for BlueField DPUs, is now available. Together, NVIDIA DOCA and BlueField DPUs accelerate the development of applications that deliver breakthrough networking, security, and storage performance with a comprehensive, open development platform. NVIDIA DOCA 2.0 includes newly added support for the BlueField-3…Powered by Discourse, best viewed with JavaScript enabled"
406,exploring-unique-applications-of-text-to-speech-technology,"Originally published at:			https://developer.nvidia.com/blog/exploring-unique-applications-of-text-to-speech-technology/
When interacting with a virtual assistant, you give a command and receive a verbal response. The technology powering this generated voice response is known as text-to-speech (TTS). TTS applications are highly useful as they enable greater content accessibility for those who use assistive devices. With the latest TTS techniques, you can generate a synthetic voice…Powered by Discourse, best viewed with JavaScript enabled"
407,using-vulkan-sc-for-safety-critical-graphics-and-real-time-gpu-processing,"Originally published at:			Using Vulkan SC for Safety-Critical Graphics and Real-time GPU Processing | NVIDIA Technical Blog
Vulkan SC (Safety Critical) is a newly-released open standard to streamline the use of GPUs in markets where functional safety and hitch-free performance are essential.Many of the NVIDIA DRIVE processes as well as hardware and software components have been recently assessed or certified compliant to ISO 26262 by TÜV SÜD.
Est. reading time: 3 minutes

This blog states: "" * Development of NVIDIA DRIVE OS 6.x is in progress and will be assessed by TÜV SÜD. This follows the recent [certification of DRIVE OS 5.2]""  If possible, can you comment on the impact of Vulkan SC being supported for 6.x impacting the certification by TUV?  This is interesting since it sounds like Vulkan SC was not possible in Drive OS 5.x, but it appears that the TUV 26262 cert was performed on 5.x.   Thank you for your insights.Powered by Discourse, best viewed with JavaScript enabled"
408,time-to-generate-3d-models,"Via Get3D I understand it can take around 30 minutes or so to generate 3D models from text description at this point.  What are the expectations on the ability to speed up that time?  Is it in the realm of possibility to watch that time decrease as quickly as the image generators did?Text-guided shape generation takes around 30min as we use a relatively simple approach of finetuning the 3D generator with a directional CLIP loss. The time to generate shapes based on the text description could be largely reduced by training a text-conditioned 3D generative model in the first place.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
409,new-languages-enhanced-cybersecurity-and-medical-ai-frameworks-unveiled-at-gtc,"Originally published at:			https://developer.nvidia.com/blog/new-languages-enhanced-cybersecurity-and-medical-ai-frameworks-unveiled-at-gtc/
Major updates to AI frameworks for building real-time speech AI apps, designing high-performing recommenders at scale, applying AI to cybersecurity challenges, and creating AI-powered medical devices.Powered by Discourse, best viewed with JavaScript enabled"
410,accelerating-load-times-for-directx-games-and-apps-with-gdeflate-for-directstorage,"Originally published at:			https://developer.nvidia.com/blog/accelerating-load-times-for-directx-games-and-apps-with-gdeflate-for-directstorage/
Load times. They are the bane of any developer trying to construct a seamless experience. Trying to hide loading in a game by forcing a player to shimmy through narrow passages or take extremely slow elevators breaks immersion. Now, developers have a better solution. NVIDIA collaborated with Microsoft and IHV partners to develop GDeflate for DirectStorage…Powered by Discourse, best viewed with JavaScript enabled"
411,cuda-toolkit-11-8-new-features-revealed,"Originally published at:			https://developer.nvidia.com/blog/cuda-toolkit-11-8-new-features-revealed/
Updated news on latest CUDA Toolkit 11.8 software release.These are some incredible features!So, I guess no changes to the core APIs…? Anyway, I’ve updated the Modern-C++ API wrappers with the information about Hopper and Lovelace, but only on the development branch for now.One of the 11.8 features, “CUDA upgrades on Jetson” has an upcoming webinar here : https://info.nvidia.com/cuda-on-jetson-webinar.html
Do register if you’d like to get more details on this feature.Thanks! We appreciate the feedback! :)Powered by Discourse, best viewed with JavaScript enabled"
412,a-guide-to-cuda-graphs-in-gromacs-2023,"Originally published at:			https://developer.nvidia.com/blog/a-guide-to-cuda-graphs-in-gromacs-2023/
This post describes how CUDA Graphs have been recently leveraged by GROMACS, a simulation package for biomolecular systems and one of the most highly used scientific software applications worldwide.Following the example in this blog, I receive the gromacs error
Inconsistency in user input:
Bonded interactions can not be computed on a GPU:
None of the bonded types are implemented on the GPU.I assumed graph support enabled these computations. My build is the current version from the gitlab provided. Is there a particular release or branch required to experiment with this?Thanks and great article.Powered by Discourse, best viewed with JavaScript enabled"
413,nvidia-ax800-delivers-high-performance-5g-vran-and-ai-services-on-one-common-cloud-infrastructure,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ax800-delivers-high-performance-5g-vran-and-ai-services-on-one-common-cloud-infrastructure/
The NVIDIA AX800 converged accelerator offers a new architectural approach to deploying 5G on commodity hardware on any cloud.Powered by Discourse, best viewed with JavaScript enabled"
414,latest-nvidia-graphics-research-advances-generative-ai-s-next-frontier,"Originally published at:			Latest NVIDIA Graphics Research Advances Generative AI’s Next Frontier | NVIDIA Blog
NVIDIA will present around 20 research papers at SIGGRAPH, the year’s most important computer graphics conference.Powered by Discourse, best viewed with JavaScript enabled"
415,ready-player-me-in-audio2face,"When will it be possible to import and use RPM avatars in A2F?You can check out ACE - Avatar cloud engine. We support Audio2Face microservice that allows you to render with any avatars and any rendering engine. Omniverse Avatar Cloud Engine ACE | NVIDIA Developer | NVIDIA Developer We have showcased a demo with RPM thereCan you export the avatar from readyplayer.me as GLB? (I thought it was possible but could not find it after some quick Google searches.)I managed recently to get a VRoid Studio avatar working with Audio2Face (exported as a VRM file, which is a GLB file), but I had to write scripts to clean up the mesh. VRoid Studio was doing things like generating lots of vertexes that were never used in any mesh.I am still working on putting it all together however (sequencer, use in combination with full body animation clips, hair physics, etc).I fixed your URL typo - the url you accidently typed was a ransom ware site. So please be careful when you share the ready player URL :)Awesome work! If you haven’t yet, you should join our Audio2Face channel on discord: NVIDIA OmniverseIt’s one of our most active channels and there are a lot of people working on similar projects there. I bet they would love to see what you’re doing and might be able to help you as you progress.Just did some googling myself and it looks like you should be able to export GLB from readyplyer.me as noted here: How to convert a GLB avatar file to FBX?. It’s not something I’ve done, however, so I would definitely check in on the discord channel.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
416,monai-drives-medical-ai-on-google-cloud-with-medical-imaging-suite,"Originally published at:			MONAI Drives Medical AI on Google Cloud with Medical Imaging Suite | NVIDIA Technical Blog
Google Cloud’s Medical Imaging Suite adopts MONAI to deliver AI-assisted annotation workflows at scale.Powered by Discourse, best viewed with JavaScript enabled"
417,creating-dynamic-lighting-solutions-for-unreal-engine-5,"RTX GI or RTX for Global Illumination allows for innovative new ways of bringing dynamic lighting to your game. It offers indirect lighting, infinite colored bounces, and soft shadows while being fast and scalable. Check out this video to see how to achieve this.
This is part of a series of Unreal 5 related introductory videos first listed in our technical blog on the developer pages. https://developer.nvidia.com/blog/shaping-the-future-of-graphics-with-nvidia-technologies-in-unreal-engine-5/Powered by Discourse, best viewed with JavaScript enabled"
418,nvidia-jetson-project-of-the-month-recognizing-birds-by-sound,"Originally published at:			https://developer.nvidia.com/blog/nvidia-jetson-project-of-the-month-recognizing-birds-by-sound/
Learn how researchers used portable devices connected to the NVIDIA Jetson Nano Developer Kit to capture audio recordings for bird identification.Powered by Discourse, best viewed with JavaScript enabled"
419,open-source-time-synchronization-services-for-data-center-operators,"Originally published at:			https://developer.nvidia.com/blog/open-source-time-synchronization-services-for-data-center-operators/
Learn how NVIDIA, Meta, and others collaborated to establish blueprints for a modern time synchronization solution that are open, reliable, and scalable.Powered by Discourse, best viewed with JavaScript enabled"
420,hybridizer-high-performance-c-on-gpus,"Originally published at:			Hybridizer: High-Performance C# on GPUs | NVIDIA Technical Blog
Figure 1. The Hybridizer Pipeline. Hybridizer is a compiler from Altimesh that lets you program GPUs and other accelerators from C# code or .NET Assembly. Using decorated symbols to express parallelism, Hybridizer generates source code or binaries optimized for multicore CPUs and GPUs. In this blog post we illustrate the CUDA target. Figure 1 shows the Hybridizer compilation…Does this work also apply for F#?From the text:""Operating on MSIL bytecode also enables support for a variety of languages built on top of the .Net virtual machine, such as VB.Net and F#.""See also older Microsoft Research's Accelerator  https://www.microsoft.com/e...Hello, are there any plans to upgrade this to CUDA 11?Thanks!Powered by Discourse, best viewed with JavaScript enabled"
421,gpu-accelerated-video-processing-with-nvidia-in-depth-support-for-vulkan-video,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-video-processing-with-nvidia-in-depth-support-for-vulkan-video/
Vulkan Video extensions for video decoding get a finalized release and support from Vulkan SDK, bringing highly tunable video processing to cross-platform applications.Discord link is useless. You have to give an invite to a discord server first, then give a link for the specific channel.This link you gave won’t work for anyone that is not part of the discord server.DiscordPlease consider this, thanks.Please use this invite link to the NVIDIA Developer discord server: https://discord.gg/nvidiadeveloperThanks! Appreciate it.
I think it should be fixed in this post tooPowered by Discourse, best viewed with JavaScript enabled"
422,ai-model-matches-radiologists-accuracy-identifying-breast-cancer-in-mris,"Originally published at:			https://developer.nvidia.com/blog/ai-model-matches-radiologists-accuracy-identifying-breast-cancer-in-mris/
Researchers from NYU Langone Health aim to improve breast cancer diagnostics with a new AI model. Recently published in Science Translational Medicine, the study outlines a deep learning framework that predicts breast cancer from MRIs as accurately as board-certified radiologists. The research could help create a foundational framework for implementing AI-based cancer diagnostic models in…Powered by Discourse, best viewed with JavaScript enabled"
423,boosting-dynamic-programming-performance-using-nvidia-hopper-gpu-dpx-instructions,"Originally published at:			https://developer.nvidia.com/blog/boosting-dynamic-programming-performance-using-nvidia-hopper-gpu-dpx-instructions/
Dynamic programming (DP) is a well-known algorithmic technique and a mathematical optimization that has been used for several decades to solve groundbreaking problems in computer science. An example DP use case is route optimization with hundreds or thousands of constraints or weights using the Floyd-Warshall all-pair shortest paths algorithm. Another use case is the alignment…Powered by Discourse, best viewed with JavaScript enabled"
424,ai-for-a-scientific-computing-revolution,"Originally published at:			https://developer.nvidia.com/blog/ai-for-a-scientific-computing-revolution/
AI and its newest subdomain generative AI are dramatically accelerating the pace of change in scientific computing research. From pharmaceuticals and materials science to astronomy, this game-changing technology is opening up new possibilities and driving progress at an unprecedented rate. In this post, we explore some new and exciting applications of generative AI in science,…Tom’s presentation on Gen AI in science to a group of supercomputing facility leaders led to this post.  I researched all the papers on the subject and learned how Gen AI is being applied to these diverse scientific research projects.  His years of experience in the field gives him the perspective to say Gen AI is driving the fastest pace of change in scientific research he has seen in his career!This report from HPCWire on some talks at the ISC Conference on AI for Science using Digital Twins was generating a LOT of buzz at the showhttps://www.hpcwire.com/2023/05/25/the-grand-challenge-of-simulating-nuclear-fusion-an-overview-with-ukaeas-rob-akers/Powered by Discourse, best viewed with JavaScript enabled"
425,introductry-research-efforts-in-generative-ai,"Hello all,
I have just started my research in the Generative AI field so I’m a complete beginner; I wanted to know if there are any open-source Nvidia software or models that I can use in my upcoming projects for text-to-3D object generation.
Also since I have just started, I would appreciate any advice on how to proceed in order to learn more about 3D object generative AI. In addition, I want to know if any team member in the Omniverse team is open to future collaborations or discussions about interesting ideas about Gen AI.
Thank you for checking my questions.Hello! Yes, we’d love to learn more about your use cases. We have a few generative AI offering. Text-to-3D is part of the Picasso service - NVIDIA Picasso In general, you will be building an OV Kit extension with any gen AI APIs.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
426,developers-design-innovative-network-security-solutions-at-the-nvidia-cybersecurity-hackathon,"Originally published at:			https://developer.nvidia.com/blog/developers-design-innovative-network-security-solutions-at-the-nvidia-cybersecurity-hackathon/
The latest NVIDIA Cybersecurity Hackathon brought together 10 teams to create exciting cybersecurity innovations using the NVIDIA Morpheus cybersecurity AI framework, NVIDIA BlueField data processing unit (DPU), and NVIDIA DOCA. The event featured seven onsite Israeli teams and three remote teams from India and the UK. Working around the clock for 24 hours, the teams…In this Hackathon I personally met with diverse teams and learned about the great solutions they invented. It was great seeing how quickly developers adopt DOCA and start developing their own apps. Kudos to all the teams and developers!
If you have any comments or questions, please let us know.Powered by Discourse, best viewed with JavaScript enabled"
427,power-your-ai-inference-with-new-nvidia-triton-and-nvidia-tensorrt-features,"Originally published at:			https://developer.nvidia.com/blog/power-your-ai-inference-with-new-nvidia-triton-and-nvidia-tensorrt-features/
NVIDIA Triton now offers native Python support with PyTriton, model analyzer support for model ensembles, and more.Powered by Discourse, best viewed with JavaScript enabled"
428,artificial-intelligence-helps-identify-molecular-structures,"Originally published at:			Artificial Intelligence Helps Identify Molecular Structures | NVIDIA Technical Blog
University of San Diego researchers developed a deep learning-based method to identify the molecular structures of natural products such as soil microorganisms, terrestrial plants and, marine life forms. According to the researchers, SMART (Small Molecule Accurate Recognition) “has the potential to accelerate the molecular structure identification process ten-fold. This development could represent a paradigm shift…Powered by Discourse, best viewed with JavaScript enabled"
429,using-gans-to-improve-chair-design,"Originally published at:			https://developer.nvidia.com/blog/using-gans-to-improve-chair-design/
Developers Philipp Schmitt and Steffen Weiss recently unveiled a new system that uses a generative adversarial network (GAN) to generate classic 20th-century chair designs. “Many famous designers have designed chairs, and one could argue that there have been enough chair designs in the world for quite a while. Yet, designers still design chairs. They are…Are there any chairs made using this technology?Powered by Discourse, best viewed with JavaScript enabled"
430,building-a-multi-camera-media-server-for-ai-processing-on-the-nvidia-jetson-platform,"Originally published at:			Building a Multi-Camera Media Server for AI Processing on the NVIDIA Jetson Platform | NVIDIA Technical Blog
A media server provides multimedia all-in-one features, such as video capture, processing, streaming, recording, and, in some cases, the ability to trigger actions under certain events, for example, automatically taking a snapshot. For you to make the best out of a media server, it must be scalable, modular, and easy to integrate with other processes.…Hello , Can I use jetson nano carrier board with raspberry pi2 camera for this type of application ?Need some sample application for the Multi camera media server.@sachin.gaikwad yes you can use the Raspberry Pi Camera Module v2 (IMX219) with the Jetson Nano carrier board.   You can test it with nvgstcapture-1.0 test program or a simple script like this that uses the nvarguscamerasrc GStreamer element:   GitHub - JetsonHacksNano/CSI-Camera: Simple example of using a CSI-Camera (like the Raspberry Pi Version 2 camera) with the NVIDIA Jetson Developer KitDeepStream also supports it - I would review the latest DeepStream documentation, as this article you commented on is from 2020.Powered by Discourse, best viewed with JavaScript enabled"
431,top-ai-for-creative-applications-sessions-at-nvidia-gtc-2023,"Originally published at:			Developer Conference Session Catalog | NVIDIA GTC
Learn how AI is boosting creative applications for creators during NVIDIA GTC 2023, March 20-23.Powered by Discourse, best viewed with JavaScript enabled"
432,solving-ai-inference-challenges-with-nvidia-triton,"Originally published at:			https://developer.nvidia.com/blog/solving-ai-inference-challenges-with-nvidia-triton/
Understand the challenges in AI inference and how Triton Inference Server helps address them. The blog also discusses the recently added features to Triton and new customer success stories.Powered by Discourse, best viewed with JavaScript enabled"
433,building-an-end-to-end-retail-analytics-application-with-nvidia-deepstream-and-nvidia-tao-toolkit,"Originally published at:			https://developer.nvidia.com/blog/building-an-end-to-end-retail-analytics-application-with-nvidia-deepstream-and-nvidia-tao-toolkit/
Learn how to build a sample application that can perform real-time intelligent video analytics (IVA) in the retail domain using NVIDIA DeepStream SDK and NVIDIA TAO Toolkit.Powered by Discourse, best viewed with JavaScript enabled"
434,synchronizing-present-calls-between-applications-on-distributed-systems-with-directx-12,"Originally published at:			Synchronizing Present Calls Between Applications on Distributed Systems with DirectX 12 | NVIDIA Technical Blog
Present barrier provides an easy way of synchronizing present calls between application windows on the same machine, as well as on distributed systems.Is there any drawback with this approach when Synchronizing multiple computers with Quadro Sync II and each one having a Mosaic enabled?
In other words: is ist possible to sync across multiple Mosaic computers?Thanks for the post!
Cheers,
LucasPowered by Discourse, best viewed with JavaScript enabled"
435,new-course-scaling-gpu-accelerated-applications-with-the-c-standard-library,"Originally published at:			Courses – NVIDIA
Learn how to write scalable GPU-accelerated hybrid applications using C++ standard language features alongside MPI in this interactive hands-on self-paced course.Powered by Discourse, best viewed with JavaScript enabled"
436,nvidia-announces-cloud-native-metropolis-microservices-and-retail-ai-workflows-for-theft-prevention,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-cloud-native-metropolis-microservices-and-retail-ai-workflows-for-theft-prevention/
NVIDIA is releasing a suite of microservices, along with Retail AI Workflows, to help software developers accelerate the development of retail loss prevention solutions.When is the tentative release date of MTMC? I have questions specific to that.Powered by Discourse, best viewed with JavaScript enabled"
437,how-can-i-build-an-ai-powered-extension-for-nvidia-omniverse-using-chatgpt-and-gpt-4,"As the title says - what’s the starting point?You can find the example project AI Room Generator on GitHub: GitHub - NVIDIA-Omniverse/kit-extension-sample-airoomgenerator: A tool used to create 3D content for rooms by calling OpenAI's APIGreat - thanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
438,top-ai-video-analytics-sessions-at-nvidia-gtc-2023,"Originally published at:			https://nvda.ws/41wQ3OI
Explore the latest software and developer tools to build, deploy, and scale vision AI and IoT apps.can you please share that softwares that helps us?Powered by Discourse, best viewed with JavaScript enabled"
439,explainer-what-is-computer-vision,"Originally published at:			What Is Computer Vision? | NVIDIA Blog
Computer vision is achieved with convolutional neural networks that can use images and video to perform segmentation, classification and detection for many applications.Powered by Discourse, best viewed with JavaScript enabled"
440,upcoming-event-level-up-with-nvidia-nsight-graphics-and-optimize-your-game,"Originally published at:			Level Up with NVIDIA
Learn how to use the latest NVIDIA RTX technology in NVIDIA Nsight Graphics and get your questions answered in a live Q&A session with experts.Powered by Discourse, best viewed with JavaScript enabled"
441,virtual-agent-understands-your-social-cues,"Originally published at:			Virtual Agent Understands Your Social Cues | NVIDIA Technical Blog
A researcher from Carnegie Mellon University developed S.A.R.A. (Socially Aware Robot Assistant) that not only comprehends what you say, but also understands facial expressions and head movements. Using CUDA, GTX 1080 GPUs and cuDNN with TensorFlow to train the deep learning models, S.A.R.A. will reply differently if she detects a smile than someone frowning and…Powered by Discourse, best viewed with JavaScript enabled"
442,gpus-for-etl-run-faster-less-costly-workloads-with-nvidia-rapids-accelerator-for-apache-spark-and-databricks,"Originally published at:			GPUs for ETL? Run Faster, Less Costly Workloads with NVIDIA RAPIDS Accelerator for Apache Spark and Databricks | NVIDIA Technical Blog
We were stuck. Really stuck. With a hard delivery deadline looming, our team needed to figure out how to process a complex extract-transform-load (ETL) job on trillions of point-of-sale transaction records in a few hours. The results of this job would feed a series of downstream machine learning (ML) models that would make critical retail…Powered by Discourse, best viewed with JavaScript enabled"
443,top-game-development-sessions-at-nvidia-gtc-2023,"Originally published at:			https://register.nvidia.com/events/widget/nvidia/gtcspring2023/1674888988405001Evjl/?nvid=nv-int-bnr-463583#new_tab
Join us for the latest on NVIDIA RTX and neural rendering technologies, and learn how they are accelerating game development.Powered by Discourse, best viewed with JavaScript enabled"
444,sensing-new-frontiers-with-neural-lidar-fields-for-autonomous-vehicle-simulation,"Originally published at:			https://developer.nvidia.com/blog/sensing-new-frontiers-with-neural-lidar-fields-for-autonomous-vehicle-simulation/
Autonomous vehicle (AV) development requires massive amounts of sensor data for perception development. Developers typically get this data from two sources—replay streams of real-world drives or simulation. However, real-world datasets offer limited flexibility, as the data is fixed to only the objects, events, and view angles captured by the physical sensors. It is also difficult…Powered by Discourse, best viewed with JavaScript enabled"
445,nvidia-cuda-toolkit-12-2-unleashes-powerful-features-for-boosting-applications,"Originally published at:			https://developer.nvidia.com/blog/nvidia-cuda-toolkit-12-2-unleashes-powerful-features-for-boosting-applications/
The latest release of NVIDIA CUDA Toolkit 12.2 introduces a range of essential new features, modifications to the programming model, and enhanced support for hardware capabilities accelerating CUDA applications. Now out through general availability from NVIDIA, CUDA Toolkit 12.2 includes many new capabilities, both major and minor.  The following post offers an overview of many…Powered by Discourse, best viewed with JavaScript enabled"
446,improving-gameplay-latency-in-unreal-engine-5-with-nividia-reflex,"The NVIDIA Reflex plug-in reduces system latency, which is key for any title where a responsive experience is required. With native support in Unreal Engine 5, simply navigate to the plug-ins folder in UE5, search for NVIDIA Reflex, and enable.Review the video walkthrough below for information on installing and implementing NVIDIA Reflex.This is part of a series of Unreal 5 related introductory videos first listed in our technical blog on the developer pages. https://developer.nvidia.com/blog/shaping-the-future-of-graphics-with-nvidia-technologies-in-unreal-engine-5/Powered by Discourse, best viewed with JavaScript enabled"
447,webinar-unleash-the-power-of-vision-transformers,"Originally published at:			https://developer.nvidia.com/blog/webinar-unleash-the-power-of-vision-transformers/
Learn how Vision Transformers are revolutionizing AI applications, offering unprecedented image understanding and analysisPowered by Discourse, best viewed with JavaScript enabled"
448,cuda-12-1-supports-large-kernel-parameters,"Originally published at:			https://developer.nvidia.com/blog/cuda-12-1-supports-large-kernel-parameters/
CUDA 12.1 offers you the option of passing up to 32,764 bytes using kernel parameters, which can be exploited to simplify applications as well as gain performance improvements.Powered by Discourse, best viewed with JavaScript enabled"
449,new-release-nvidia-rtx-global-illumination-1-3,"Originally published at:			NVIDIA RTXGI SDK - Get Started | NVIDIA Developer
NVIDIA RTX Global Illumination (RTXGI) 1.3 includes highly requested features such as dynamic library support, an increased maximum probe count per DDGI volume by 2x, support for Shader Model 6.6 Dynamic Resources in D3D12, and more.Powered by Discourse, best viewed with JavaScript enabled"
450,cuda-refresher-the-cuda-programming-model,"Originally published at:			CUDA Refresher: The CUDA Programming Model | NVIDIA Technical Blog
This is the fourth post in the CUDA Refresher series, which has the goal of refreshing key concepts in CUDA, tools, and optimization for beginning or intermediate developers. The CUDA programming model provides an abstraction of GPU architecture that acts as a bridge between an application and its possible implementation on GPU hardware. This post…Seems that the link for "" CUDA sample code deviceQuery"" will show a 404 page for readers.Yeah, it should be: deviceQueryPowered by Discourse, best viewed with JavaScript enabled"
451,gtc-2020-high-performance-remote-scientific-visualization-in-jupyter-notebooks,"GTC 2020 S22111
Presenters: Nick Leaf,NVIDIA; Maximilian Rietmann,NVIDIA
Abstract
We’ll introduce iPyParaView, a Jupyter widget for interactive rendering in the notebook with ParaView. iPyParaView leverages the GPU in your local or remote Jupyter instance to render data without copying it back to the client. It uses ParaView’s Python bindings to expose its full capabilities — including the RTX path-tracing back end and IndeX volume rendering plugin. We’ll use live demos to show how to pre-process data with RAPIDS and render with ray-tracing, as well as how to use a Dask-MPI cluster for distributed-memory processing and visualization.Watch this session
Join in the conversation below.Really impressed with visualizations in Jupyter! Is it possible to select points?Hello, glad you like it!iPyParaView is using a custom camera model to translate mouse events into camera movement. This means that some things which work in ParaView’s desktop UI aren’t handled. Unfortunately this includes data selection, so selecting points through the viewing window in Jupyter isn’t currently supported.I’m really impressed with this!Powered by Discourse, best viewed with JavaScript enabled"
452,explaining-and-accelerating-machine-learning-for-loan-delinquencies,"Originally published at:			https://developer.nvidia.com/blog/explaining-and-accelerating-machine-learning-for-loan-delinquencies/
Machine learning (ML) can extract deep, complex insights out of data to help make decisions. In many cases, using more advanced models delivers real business value through significantly improving on traditional regression models. Unfortunately, using traditional infrastructure to explain what drove a particular decision with a more advanced model can be difficult, time-consuming, and expensive. Figure…We at NVIDIA hope this post was helpful for you: If you have any questions or comments, please let us know.excellent blog post chaps. Even I understood it.  Very excited about Nvidia in finance and looking to use julia on cuda. Great to have Dr Bennett in Chicago. We used his book to build out our first fpga laptops.We were disappointed that the Chicago Trade show bundled Dr Bennett into a panel discussion. He did a marvelous job BUT 90% of time was spent on meaningless rubbish, Dr Bennett was the only person who seemed to be addressing the future.
well DONE Nvidia for getting Dr Bennett on board. We are happy bunniesdave in ChicagoPowered by Discourse, best viewed with JavaScript enabled"
453,nvidia-jetson-community-project-spotlight-point-voxel-cnn-for-efficient-3d-deep-learning,"Originally published at:			NVIDIA Jetson Community Project Spotlight: Point-Voxel CNN for Efficient 3D Deep Learning | NVIDIA Technical Blog
3D deep learning is used in a variety of applications including robotics, AR/VR systems, and autonomous machines.  In this month’s Jetson Community Project spotlight, researchers from MIT’s Han Lab developed an efficient, 3D, deep learning method for 3D object segmentation, designed to run on edge devices.  “We present Point-Voxel CNN (PVCNN) for efficient, fast 3D…good workPowered by Discourse, best viewed with JavaScript enabled"
454,building-an-automatic-speech-recognition-model-for-the-kinyarwanda-language,"Originally published at:			Building an Automatic Speech Recognition Model for the Kinyarwanda Language | NVIDIA Technical Blog
Learn how an ASR model was trained on the Kinyarwanda language dataset that achieved state-of-the-art performance.Powered by Discourse, best viewed with JavaScript enabled"
455,find-furniture-you-love-with-houzz-s-new-visual-recognition-tool,"Originally published at:			https://developer.nvidia.com/blog/find-furniture-you-love-with-houzzs-new-visual-recognition-tool/
With 40 million monthly unique users, Houzz is leveraging deep learning technology to make it easier for people to discover and buy products and materials that inspire them. “People come to Houzz because they want to get everything they need to improve their homes in one place, from inspiration to execution,” said Alon Cohen, Houzz…Perhaps something depends on the grade of the aluminum as well. If it’s cheap, it might not last very longPowered by Discourse, best viewed with JavaScript enabled"
456,new-video-creation-and-streaming-features-accelerated-by-the-nvidia-video-codec-sdk,"Originally published at:			https://developer.nvidia.com/blog/new-video-creation-and-streaming-features-accelerated-by-the-nvidia-video-codec-sdk/
For over a decade, NVIDIA GPUs have been built with dedicated encoders and decoders called NVENC and NVDEC. They have a highly parallelized architecture, support popular codec formats, and provide direct access to GPU memory for optimized encode and decode operations. GPU-accelerated video means offloading your video processing to NVENCs and NVDECs, reducing CPU cycles…Powered by Discourse, best viewed with JavaScript enabled"
457,scaling-vasp-with-nvidia-magnum-io,"Originally published at:			https://developer.nvidia.com/blog/scaling-vasp-with-nvidia-magnum-io/
You could make an argument that the history of civilization and technological advancement is the history of the search and discovery of materials. Ages are named not for leaders or civilizations but for the materials that defined them: Stone Age, Bronze Age, and so on. The current digital or information age could be renamed the…I had the pleasure of working with the other authors to learn a lot more about VASP, quantum chemistry, and how asynchronous parallel communications helps many GPUs to scale applications.  It’s great to illustrate the benefits of Magnum IO NCCL with hard data.  If you have any question or comments, please let us know.Powered by Discourse, best viewed with JavaScript enabled"
458,confidential-compute-on-github,"I saw a new GitHub repo added with Confidential Compute - has this been released? Does it work at the Kernel level?The confidential computing (CC) features of the Hopper H100 are indeed now available in early access; make sure you have driver 535.86 or later!The confidential computing of the H100 is complementary to a system which has a CC enabled CPU (e.g., Intel TDX or AMD’s SEV-SNP). With these style CPUs, you can connect an H100 and begin to keep your code and data secure while in use.Powered by Discourse, best viewed with JavaScript enabled"
459,catapulting-enterprises-to-the-leading-edge-of-ai-with-nvidia-ai-enterprise-3-1,"Originally published at:			https://developer.nvidia.com/blog/catapulting-enterprises-to-the-leading-edge-of-ai-with-ai-enterprise-3-1/
Generative AI has marked an important milestone in the AI revolution journey. We are at a fundamental breaking point where enterprises are not only getting their feet wet but jumping into the deep end. With over 50 frameworks, pretrained models, and development tools, NVIDIA AI Enterprise, the software layer of the NVIDIA AI platform, is…Powered by Discourse, best viewed with JavaScript enabled"
460,protecting-sensitive-data-and-ai-models-with-confidential-computing,"Originally published at:			https://developer.nvidia.com/blog/protecting-sensitive-data-and-ai-models-with-confidential-computing/
Rapid digital transformation has led to an explosion of sensitive data being generated across the enterprise. That data has to be stored and processed in data centers on-premises, in the cloud, or at the edge. Examples of activities that generate sensitive and personally identifiable information (PII) include credit card transactions, medical imaging or other diagnostic…Powered by Discourse, best viewed with JavaScript enabled"
461,design-your-robot-on-hardware-in-the-loop-with-nvidia-jetson,"Originally published at:			https://developer.nvidia.com/blog/design-your-robot-on-hardware-in-the-loop-with-nvidia-jetson/
Hardware-in-the-loop (HIL) testing is a powerful tool used to validate and verify the performance of complex systems, including robotics and computer vision. This post explores how HIL testing is being used in these fields with the NVIDIA Isaac platform. The NVIDIA Isaac platform consists of NVIDIA Isaac Sim, a simulator that provides a simulated environment…is it possible to use an ethernet cable w/o a router to connect the workstation and the Jetson? I tried to follow the instructions below:Set of demo to try Isaac ROS with Isaac SIM. Contribute to NVIDIA-AI-IOT/isaac_demo development by creating an account on GitHub.Run the script at Jetson:
$ bash src/isaac_demo/scripts/run_in_docker.shAnd launch foxglove studio at workstation and try to connect jetson, it always show connection failed.If run a command on Jetson (w/o container) like:
$ python3 -m foxglove_websocket.examples.json_serverFoxglove can be connected, only issue is that messages cannot be seen on Foxglove.My questions are:Powered by Discourse, best viewed with JavaScript enabled"
462,orchestrating-accelerated-virtual-machines-with-kubernetes-using-nvidia-gpu-operator,"Originally published at:			https://developer.nvidia.com/blog/orchestrating-accelerated-virtual-machines-with-kubernetes-using-nvidia-gpu-operator/
The latest release of GPU Operator adds support for KubeVirt and OpenShift Virtualization, enabling the use of Kubernetes to orchestrate GPU-accelerated applications running as virtual machines.Powered by Discourse, best viewed with JavaScript enabled"
463,reducing-power-plant-greenhouse-gasses-using-ai-and-digital-twins,"Originally published at:			https://developer.nvidia.com/blog/reducing-power-plant-greenhouse-gasses-using-ai-and-digital-twins/
Learn how the physics-informed machine learning framework, NVIDIA Modulus, is being used to develop power plant digital twins that can help move towards net-zero carbon emissions.Is there code for this that can be share? IPowered by Discourse, best viewed with JavaScript enabled"
464,reducing-costs-with-one-pass-reverse-time-migration,"Originally published at:			https://developer.nvidia.com/blog/reducing-costs-with-one-pass-reverse-time-migration/
Reverse time migration (RTM) is a powerful seismic migration technique, providing geophysicists with the ability to create accurate 3D images of the subsurface. Steep dips? Complex salt structure? High velocity contrast? No problem. By splitting the upgoing and downgoing wavefields and combining them with an accurate velocity model, RTM can image even the most complex…While a one-pass TTI RTM is much more computationally efficient, it adds code complexity. NVIDIA has a collection of very well documented sample code that lowers the barrier to a one-pass TTI RTM. This sample code includes other tricks, such as compression on GPUs, which alleviates bandwidth bottleneck caused by snapshots. All code is freely available under NDA and our team can help with implementation/optimization. If interested, please reach out to me at reynaldog@nvidia.comCan I get a code sample? I mainly want to see the effect of A100 Cache Data Compression。Hello Sazc,Are you with a university or company? We can share the code once an NDA is in place. Email me at reynaldog@nvidia.com.Thanks,
ReyPowered by Discourse, best viewed with JavaScript enabled"
465,new-reference-applications-for-edge-ai-developers-on-holohub-with-nvidia-holoscan-v0-5,"Originally published at:			https://developer.nvidia.com/blog/new-reference-applications-for-edge-ai-developers-on-holohub-with-nvidia-holoscan-v0-5/
Edge AI applications, whether in airports, cars, military operations, or hospitals, rely on high-powered sensor streaming applications that enable real-time processing and decision-making. With its latest v0.5 release, the NVIDIA Holoscan SDK is ushering in a new wave of sensor-processing capabilities for the next generation of AI applications at the edge. This release also coincides…Powered by Discourse, best viewed with JavaScript enabled"
466,new-course-get-started-with-highly-accurate-custom-asr-for-speech-ai,"Originally published at:			Courses – NVIDIA
Learn how to build, train, customize, and deploy a GPU-accelerated automatic speech recognition service with NVIDIA Riva in this self-paced course.Powered by Discourse, best viewed with JavaScript enabled"
467,game-creation-with-ue5-omniverse-and-gpt4,"Where I should look for a Job that is gonna accept above workflow by that I mean adaptation USD in workflow as well.The NVIDIA careers place would be a great place to start! Jobs at NVIDIA | NVIDIA CareersThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
468,nvidia-steps-up-network-security-with-research-in-quantum-keys,"Originally published at:			https://developer.nvidia.com/blog/nvidia-steps-up-network-security-with-research-in-quantum-keys/
As part of NVIDIA efforts to advance research towards a more secure data center, the NVIDIA Advanced Development Goup is conducting research on quantum key distribution (QKD) technologies, along with other top organizations in Europe and in Israel. The initiatives are funded by the European Union’s Horizon 2020 program and the Israel Innovation Authority. QKD…Powered by Discourse, best viewed with JavaScript enabled"
469,how-well-does-cugraph-scale,"How well does cuGraph scale? Any information you can share would be interestingWe have tested PageRank and Louvain on 1000+ GPUs. Many other algorithms (except for few with legacy implementations) should scale as well even though we haven’t tested every algorithm for 1000+ GPUs. If you need to run very large graph analytics and if your algorithm of interest does not scale well enough, pleae submit an issue in the cuGraph github page. For more academic reference, see Analyzing Multi-trillion Edge Graphs on Large GPU Clusters: A Case Study with PageRank | IEEE Conference Publication | IEEE XploreThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
470,cugraph-and-sparse-matrices,"Thanks for being here! I am curious on the relationship between cuGraph and other CUDA libraries. My work now is highly dependent on cuSparse and cuSolver for geometry surface meshes.Can cuGraph accommodate sparse adjacency matrix as input? Can I use it as an extension of cuSparse?Good to know that! Thank you for the reply.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
471,upcoming-webinar-performant-multiphase-flow-simulation-at-leadership-class-scale-using-openacc,"Multi-phase and multi-component flow are central to a wide range of engineering problems. Attend an upcoming webinar to learn how Georgia Tech researchers implemented a three-fold approach to accelerate the open-source solver MFC on #GPUs. Register today: Performant Multiphase Flow Simulation at Leadership-Class Scale via OpenACCPowered by Discourse, best viewed with JavaScript enabled"
472,developing-smart-city-traffic-management-systems-with-openusd-and-synthetic-data,"Originally published at:			https://developer.nvidia.com/blog/developing-smart-city-traffic-management-systems-with-openusd-and-synthetic-data/
End-to-end AI engineering company SmartCow, an NVIDIA Metropolis partner, has created digital twins of traffic scenarios on NVIDIA Omniverse.Powered by Discourse, best viewed with JavaScript enabled"
473,is-there-a-sample-of-pt-sdk-with-blendshape-animation,"the sample code shows how to integrate PT sdk with skeleton animation,
is there a sample showing PT sdk with blendshape animation?Thank you for the question. There isn’t any support right now. But the main thing that the PT SDK is trying to demonstrate is that it does work with animated scene elements. The actual implementation of the animation isn’t important for demonstrating path tracing. There is nothing preventing blend shape animation being implemented in principle though.Powered by Discourse, best viewed with JavaScript enabled"
474,enhancing-digital-twin-models-and-simulations-with-nvidia-modulus-v22-09,"Originally published at:			https://developer.nvidia.com/blog/enhancing-digital-twin-models-and-simulations-with-nvidia-modulus-v22-09/
NVIDIA Modulus v22.09 is now available with greater composition flexibility for neural operator architectures, improved training convergence and performance, and improved UX and documentation.Powered by Discourse, best viewed with JavaScript enabled"
475,mastering-string-transformations-in-rapids-libcudf,"Originally published at:			https://developer.nvidia.com/blog/mastering-string-transformations-in-rapids-libcudf/
Learn how RAPIDS libcudf accelerates data science with optimized string processing algorithms and explore techniques for writing your own custom string transformations.To run the examples, first build:and then execute:Check it out!I’m happy to share that we now have a cuDF-python implementation of the “redact” strings transformation. In this case, cuDF uses the libcudf strings API to deliver excellent acceleration.Please see our new notebook for more information: GitHub - shwina/cudf-string-examples: Examples of working with strings in cuDF
image1230×1038 43.2 KB
Powered by Discourse, best viewed with JavaScript enabled"
476,gpt-4-and-omniverse,"It was not completely clear on this, can I actually use GPT-4 in Omniverse?Yes, you can connect the OpenAI API and use ChatGPT and GPT-4 APIs in Omniverse. You can use both the Python API or write a HTTPS call to their web API. There is a project that you can check out on GitHub that demonstrates using GPT-4 and Omniverse, you can find it here: GitHub - NVIDIA-Omniverse/kit-extension-sample-airoomgenerator: A tool used to create 3D content for rooms by calling OpenAI's APIThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
477,tell-us-what-new-features-you-would-like-to-see,"You are invited to tell us what what kind of new features you would like to see.Powered by Discourse, best viewed with JavaScript enabled"
478,create-xr-experiences-using-natural-language-voice-commands-test-project-mellon,"Originally published at:			https://developer.nvidia.com/blog/create-xr-experiences-using-natural-language-voice-commands-test-project-mellon/
At GTC 2023, NVIDIA announced that developers can start testing Project Mellon to explore creating hands-free extended reality (XR) experiences controlled by natural-language voice commands.Hi,
I’m trying to install Project Mellon but it seems the documentation points out to an unexisting repository:https://gitlab-master.nvidia.com/dialogue-research/mellontoolkit.gitCan you help me to find it?I can’t move forward without this component.Thanks,Powered by Discourse, best viewed with JavaScript enabled"
479,about-the-technical-blogs-events-category,"Find discussions about our technical blogs, our live connect with experts events, recorded presentations and webinars.
Discuss the topics with peers, post questions for the presenters and authors.Powered by Discourse, best viewed with JavaScript enabled"
480,path-tracing-vs-ray-tracing,"What is actually the difference between path tracing and ray tracing? Mostly they seem to be the same?Ray tracing refers to the mechanical process of tracing a ray through a scene and figuring out what it hits. There are many applications of ray tracing. Ray traced reflections, ray traced shadows, ray traced AO etc.Path tracing is also an application of ray tracing - but unlike those individual ‘effects’, path tracing addresses the whole of the rendering process in a single algorithm.
Specifically, in a path tracer, ray tracing is at least used to advance each path segment, and often for NEE (Next Event Estimation) light sampling visibility (shadow) tracing and similar. As an example, for a single pixel path traced sample, the path tracer might use anything from 5 to 50 or more ray traces depending on the properties of the encountered surfaces.Thank you!Powered by Discourse, best viewed with JavaScript enabled"
481,powering-ultra-high-speed-frame-rates-in-ai-medical-devices-with-the-nvidia-clara-holoscan-sdk,"Originally published at:			https://developer.nvidia.com/blog/powering-ultra-high-speed-frame-rates-in-ai-medical-devices-with-clara-holoscan-sdk/
NVIDIA Clara Holoscan SDK 0.3 now provides a lightning-fast frame rate of 240 Hz for 4K video, enabling the next generation of medical devices.Powered by Discourse, best viewed with JavaScript enabled"
482,optimizing-video-memory-usage-with-the-nvdecode-api-and-nvidia-video-codec-sdk,"Originally published at:			Optimizing Video Memory Usage with the NVDECODE API and NVIDIA Video Codec SDK | NVIDIA Technical Blog
The NVIDIA Video Codec SDK consists of GPU hardware-accelerated APIs for the following tasks: Video encoding, with the NVENCODE APIVideo decoding, with NVDECODE API (formerly known as nvcuvid) While writing an application using the NVDECODE or NVENCODE APIs, it is crucial to use video memory in an efficient way. If an application uses multiple decoders…Hi there.Figure 2 (A typical decoding pipeline) shows video parser as optional component.  The article also says: “These components are not dependent on each other and hence can be used independently.”
But to calculate the ulNumDecodeSurfaces the article suggests to use CUVIDEOFORMAT::min_num_decode_surfaces provided by the sequence callback from the parser.
Does it mean that usage of the video parser is actually required, not optional?
If parser is not required, how do I determine a value of the ulNumDecodeSurfaces without video parser call back and CUVIDEOFORMAT::min_num_decode_surfaces ?
Thank you!Does it mean that usage of the video parser is actually required, not optional?Hi @petr.mpp,Let me clarify it. Developers has freedom to implement their own parser and use. Parser is MUST for decoder pipeline but NVIDIA video parser can be replaced by their own implementation.ulNumDecodeSurfaces can be derived from bitstream DPB info from either their own parser or Nvidia video parser. Or can be set to MAX DPB size defined in codec spec.Thank you.
VikasPowered by Discourse, best viewed with JavaScript enabled"
483,building-the-future-of-real-time-graphics-with-nvidia-and-unreal-engine-5-1,"Originally published at:			Building the Future of Real-Time Graphics with NVIDIA and Unreal Engine 5.1 | NVIDIA Technical Blog
Learn about the Unreal Engine 5.1 release, including next-generation RTX lighting and speed increases to help you keep pace with rigorous development cycles.Powered by Discourse, best viewed with JavaScript enabled"
484,cugraph-pain-points,"Where are cuGraph users having issues and/or experiencing difficulties? For example: “doing X is slow” or “I really want to do Y, but cannot”. What are those Xs and Ys?We have a variety of users with different goals and objectives.  Their pain points vary just as widely.  Some of the pain points we have observed:
•        Running out of GPU memory.  Graph algorithms often require auxiliary memory, and sometimes it’s not obvious a priori whether there is sufficient GPU memory
•        Running in multi-node-multi-GPU can be complex.  At the C++ layer we use mpirun to launch multiple processes.  Running in multi-node multi GPU in python is accomplished via dask.  Creating the proper environment to run these tools can be a challenge.Access to working fast inerconnect across multi nodes like IB/NVLINK is a challenge for customers running especially in cloud environments is non trivialThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
485,nvidia-announces-generative-ai-services-for-language-visual-content-and-biology-applications,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-generative-ai-services-for-language-visual-content-and-biology-applications/
To enable enterprises to take advantage of the possibilities with generative AI, NVIDIA has launched NVIDIA AI Foundations and the NVIDIA NeMo framework, powered by NVIDIA DGX Cloud.Powered by Discourse, best viewed with JavaScript enabled"
486,harnessing-the-power-of-nvidia-ai-enterprise-on-azure-machine-learning,"Originally published at:			https://developer.nvidia.com/blog/harnessing-the-power-of-nvidia-ai-enterprise-on-azure-machine-learning/
NVIDIA AI Enterprise and Azure Machine Learning together create a powerful combination of GPU-accelerated computing and a comprehensive cloud-based machine learning platform.Powered by Discourse, best viewed with JavaScript enabled"
487,lazy-loading,"This is a question sent directly to me, so repeating as a post:What is lazy loading and what benefits should I expect from it.Lazy loading is a catch-all term we use to describe a few different techniques for lowering memory consumption by CUDA applications. Ultimately, they all boil down to not loading functions either into host memory or into the GPU’s memory until the first time the application needs to call them.If you think about some of our larger libraries, like cuDNN or cuBLAS, there are tens of thousands of kernels you could run although a typical application calls maybe 5% of them. By not loading the other 95%, you can see substantial savings in both the time it takes to load the application (less data transfer to the GPU) and lower memory utilization (functions that aren’t called aren’t loaded). In some applications this can be very substantial.It’s worth noting that because we don’t load functions until you call them, it does change the latency of the functions at the first invocation. That’s usually un-noticeable for most applications and the net effect will be a performance increase, but if you have an application that’s particularly latency sensitive you can switch it off wiht the CUDA_MODULE_LOADING environment variable (valid settings are EAGER to turn it off, and LAZY which will turn it on).Lazy loading is enabled by default in CUDA 12.2 for Linux and 12.3 for Windows.Powered by Discourse, best viewed with JavaScript enabled"
488,detecting-malware-with-purple-team-collaboration,"Originally published at:			https://developer.nvidia.com/blog/detecting-malware-with-purple-team-collaboration/
The NVIDIA Security Team worked with an open source developer within the information security field to help bolster the defensive capabilities of the broader community.Powered by Discourse, best viewed with JavaScript enabled"
489,detecting-obstacles-and-drivable-free-space-with-radarnet,"Originally published at:			https://developer.nvidia.com/blog/detecting-obstacles-and-drivable-free-space-with-radarnet/
Detecting drivable free space is key for robust AV perception. RadarNet DNN is an AI-based free space detection system using only radar detections as an input.Powered by Discourse, best viewed with JavaScript enabled"
490,upcoming-workshop-training-amp-tuning-text-to-speech-with-nvidia-nemo-and-w-amp-b,"Originally published at:			Speech AI workshop - NVIDIA x W&B
Learn to train an end-to-end TTS system and track experiments in this live workshop on December 8. Set up the environment, review code blocks, test the model, and more.Powered by Discourse, best viewed with JavaScript enabled"
491,a-comprehensive-guide-to-interaction-terms-in-linear-regression,"Originally published at:			https://developer.nvidia.com/blog/a-comprehensive-guide-to-interaction-terms-in-linear-regression/
Linear regression is a powerful statistical tool used to model the relationship between a dependent variable and one or more independent variables (features). An important, and often forgotten, concept in regression analysis is that of interaction terms. In short, interaction terms enable you to examine whether the relationship between the target and the independent variable…Have you ever utilized interaction terms in linear models? If you have, was it mostly for econometric models or have you also applied them in business use cases?Powered by Discourse, best viewed with JavaScript enabled"
492,large-language-models-trained-on-nvidias-omniverse-kit-api,"Hi!I’ve been using Omniverse a lot over the past 18 months. The release of LLMs has assisted quite a bit with parsing large and sometimes confusing APIs like Pixar USD.My question is: Is there a LLM that has been trained on the omniverse APIs? As a developer having a tool like that would increase my productivity greatly.I am specifically concerned with things like omni.kit and omni.ui for extension development. I find myself spending quite a bit of time parsing documentation (and often even finding the correct documentation as kit versions update and mutate - see my other posts on the forum for reference). The developer community and Mati have been incredibly helpful in resolving many of these roadblocks I’ve encountered, but a conversational AI would be invaluable for me. Obviously the infrastructure already exists, but things like OpenAI’s ChatGPT had their public-facing model’s training data cut off back in 2021, and there have been significant updates/changes to omniverse APIs since then.Thank you for your time!-MatthewWonderful idea - makes sense. Could you share some example questions that you might want to ask an LLM that’s trained in this way?Hi Paul,Sure! Here are some questions I’ve brought to the forums or asked other experts before that a LLM would probably make quick work of:‘Can you show me how to generate extra viewports in an Omniverse Kit extension using the kit viewport API? I would also like these viewports to be capable of popping out into their own windows outside of the currently running base kit application.’‘In the context of an NVIDIA Omniverse Kit extension, can you show me how to add text labels on world-space object that scale consistently to screen space?’ (this one may be outside LLM capability)‘I would like to generate an Omniverse Kit extension GUI that has the following elements and layout: (arbitrary description). Can you help me generate this using the omni.ui library?’In addition to some of these more complex things, it would also be very helpful to have it reply with the relevant library for a given task, i.e.Omniverse has so many powerful capabilities and use cases but for things like scene manipulation I find myself relying very heavily on coding directly with the USD API even if it is more verbose or low-level than omni.usd. Pixar has very elaborate (albeit dense) documentation, and though many of the omniverse libraries do have decent docs, it seems they are more subject to version changes/mismatches between the most modern codebase and the current web-based documentation. Pixar also has some very helpful wiki-like descriptions for computer graphics theory built into some of the top level classes. That kind of description goes a long way for keeping experimenting developers engaged and not lost. A LLM (to a certain degree) can circumvent the need for such details and if trained on every kit version separately, could even resolve the version confusion issue.If Pixar USD knowledge, Omniverse Kit knowledge, and generative AI capabilities were concatenated into one tool I think there would be some remarkable results. It would also make the platform more attractive to independent developers and beginners.Cheers!Mati (from Discord) said the Python bindings to all the API functions currently do not include all the argument lists for functions. It won’t be ready for 105, but will be released some time after that. I suspect that will help a lot as well. Teach it the python APIs (not the C++ ones).Its a great tool for learning. E.g. “Using the Python API, how do you bind a material to a geometry given a prim path to the material”. Sometimes you need to use UsdSkel instead of a prim node (a wrapper class) to get type safe APIs, and for Python you often just use a string instead of a token.I would train it up on Mati’s git repo of NVIDIA code samples from his live streams as well. But I shelved trying to “fine tune” a ChatGPT model until all the function prototypes were fully available. After that, I think this would be a fantastic resource for learning the APIs.“How do I load a model and make sure that it is rotated upwards and scaled to the same scale factor as the rest of the project?”
“What is the “st” property?”
“Write python code to retarget an animation clip from one character to another”
“How can I create a sequencer from python, add the UsdSkelRoot under /World/Characters/Sam to an animation track, then add the animation clip for walking from the Hank character to Sam.”Thanks, this is a great list. I’ll take this back to team and explore it.Powered by Discourse, best viewed with JavaScript enabled"
493,just-released-cuda-toolkit-12-0,"Originally published at:			https://developer.nvidia.com/cuda-downloads#new_tab
CUDA Toolkit 12.0 supports NVIDIA Hopper architecture and many new features to help developers maximize performance on NVIDIA GPU-based products.Powered by Discourse, best viewed with JavaScript enabled"
494,topic-modeling-and-image-classification-with-dataiku-and-nvidia-data-science,"Originally published at:			https://developer.nvidia.com/blog/topic-modeling-and-image-classification-with-dataiku-and-nvidia-data-science/
Learn about Dataiku and NVIDIA integrations for image classification and object detection.Powered by Discourse, best viewed with JavaScript enabled"
495,explainer-what-is-an-autonomous-truck,"Originally published at:			What Is an Autonomous Truck? | NVIDIA Blog
Autonomous trucks are commercial vehicles that use AI to automate everything from shipping yard operations to long-haul deliveries.Powered by Discourse, best viewed with JavaScript enabled"
496,similarity-in-graphs-jaccard-versus-the-overlap-coefficient,"Originally published at:			Similarity in Graphs: Jaccard Versus the Overlap Coefficient | NVIDIA Technical Blog
This post was originally published on the RAPIDS AI Blog. There is a wide range of graph applications and algorithms that I hope to discuss through this series of blog posts, all with a bias toward what is in RAPIDS cuGraph. I am assuming that the reader has a basic understanding of graph theory and…Regarding your commentary on the unintuitive results for similarity of vertices in a clique: why not just consider a node as a neighbor of itself? In a social networking application, where an edge means “knows” e.g., Mike “knows” Jesse, the it would also seem that Mike should “know” himself too. Then, in an application where you don’t have a logical connection between an entity and itself there wouldn’t be less similarity between members of a clique.I stumbled on this article because I’m curious about graph similarity. For example, if I have a graph G and I want to obfuscate it into H, to simply make it difficult for someone to learn about G’s connectivity, could I use  a metric like Jaccard or O.C. to test how close or dissimilar the transformed graph is to the original? Maybe if I take the summation of edge Jaccard or O.C. over every pair of nodes in graphs G and H.
I’m not so sure.Great question.  Sorry for the slow response.Both Jaccard and Overlap come from a non-graph background and are really related to sets.  For graphs we use the neighbors of a vertex as the formation of the set, but that doesn’t need to be the case.  Within Cyber analysis, for example, it is beneficial to compare the two-hop neighbors. So does vertex A interact with the same people as vertex B.Now your question about considering a node as a neighbor of itself.  The notion of a self-link is handled by the algorithms and self-links can impact the score.  Now if you assume every node has a self-link, then it all balances out.If you are interested in obfuscate graph G into H to make understanding about G harder, then self-links would be a start but not enough to hide similarity.  You would really need to add a large collection of random edges.As an aside you can see the work I did on creating a community detection algorithm using neighbor knowledge:  https://www.academia.edu/download/89102692/p1393.pdfPowered by Discourse, best viewed with JavaScript enabled"
497,upcoming-webinar-a-deep-dive-into-monai,"Originally published at:			An Introduction to MONAI: The Essential Framework For The Medical AI Ecosystem
Join us on October 24 for a deep dive into MONAI, the essential framework for AI workflows in healthcare—including use cases, building blocks, and more.Powered by Discourse, best viewed with JavaScript enabled"
498,neuralangelo-by-nvidia-research-reconstructs-3d-scenes-from-2d-videos,"Originally published at:			https://developer.nvidia.com/blog/neuralangelo-by-nvidia-research-reconstructs-3d-scenes-from-2d-videos/
A new model generates 3D reconstructions using neural networks, turns 2D video clips into detailed 3D structures — generating lifelike virtual replicas of buildings, sculptures and other real-world objects.Powered by Discourse, best viewed with JavaScript enabled"
499,supercharging-ai-video-and-ai-inference-performance-with-nvidia-l4-gpus,"Originally published at:			https://developer.nvidia.com/blog/supercharging-ai-video-and-ai-inference-performance-with-nvidia-l4-gpus/
NVIDIA T4 was introduced 4 years ago as a universal GPU for use in mainstream servers. T4 GPUs achieved widespread adoption and are now the highest-volume NVIDIA data center GPU. T4 GPUs were deployed into use cases for AI inference, cloud gaming, video, and visual computing. At the NVIDIA GTC 2023 keynote, NVIDIA introduced several…Thank you to all the customers who allowed us to share their experiences with the NVIDIA L4 GPU.  If you have any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
500,top-avatar-sessions-at-nvidia-gtc-2023,"Originally published at:			https://nvda.ws/3Xrxg3Y
Learn about the future of interactive avatars for gaming, the metaverse, and beyond at NVIDIA GTC 2023.Powered by Discourse, best viewed with JavaScript enabled"
501,top-manufacturing-sessions-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Discover the latest innovations in manufacturing and aerospace with GTC sessions from leaders at Siemens, Boeing, BMW, and more.Powered by Discourse, best viewed with JavaScript enabled"
502,enabling-matrix-product-state-based-quantum-circuit-simulation-with-nvidia-cuquantum,"Originally published at:			https://developer.nvidia.com/blog/enabling-matrix-product-state-based-quantum-circuit-simulation-with-nvidia-cuquantum/
Quantum circuit simulation is the best means to design quantum-ready algorithms so you can take advantage of powerful quantum computers as soon as they are available. NVIDIA cuQuantum is an SDK that enables you to leverage different ways to perform quantum circuit simulation. cuStateVec, a high-performance library built for state vector quantum simulators, relies on…Be sure to reach out to us if you have any questions, comments or concerns. We’d love to hear from you!Powered by Discourse, best viewed with JavaScript enabled"
503,accelerating-jpeg-2000-decoding-for-digital-pathology-and-satellite-images-using-the-nvjpeg2000-library,"Originally published at:			https://developer.nvidia.com/blog/accelerating-jpeg-2000-decoding-for-digital-pathology-and-satellite-images-using-the-nvjpeg2000-library/
JPEG 2000 (.jp2, .jpg2, .j2k) is an image compression standard defined by the Joint Photographers Expert Group (JPEG) as the more flexible successor to the still popular JPEG standard. Part 1 of the JPEG 2000 standard, which forms the core coding system, was first approved in August 2002. To date, the standard has expanded to…nvJPEG2000 library provides decoding for JPEG 2000 formatted images used by researcher in digital pathology, remote sensing applications and medical imaging.
Key Featuresis there encoding part?Hi, nvJPEG2000 support encoder, refer details nvJPEG2000 Documentation — nvJPEG2000 0.6.0 documentationHi, I can use nVJPEG2000 to decode j2k image (JPEG 2000 - Part1) encoded by Kakadu / OpenJpeg libraries successfully, but fail to decode j2k image (JPEG 2000 - Part1) encoded by Pegasus library of Accusoft with error code NVJPEG2K_STATUS_JPEG_NOT_SUPPORTED. Any suggestions to check which parts are missed or not matched?We are facing the same error (NVJPEG2K_STATUS_JPEG_NOT_SUPPORTED) trying to encode using the GPU in DCP-o-Matic free software.You can see the discussion  and track the bug here.@mkhadatare do you know how to get this solved? It would be great to move it forward. Nvjpeg2k enable_custom_precinctsThanks everyone :)enable_custom_precincts member of nvjpeg2kEncodeConfig_t). It looks like this isn’t currently supported (I get a NVJPEG2K_STATUS_JPEG_NOT_SUPPORTED error when I try it). Do you have plans to add this?In passing, perhaps I can report that the arrays precint_width and precint_height in nvjpeg2kEncodeConfig_t appear to have mis-spelt names (I think they should be precinct_width and precinct_height)Powered by Discourse, best viewed with JavaScript enabled"
504,metropolis-spotlight-lumeo-simplifies-vision-ai-development,"Originally published at:			https://developer.nvidia.com/blog/metropolis-spotlight-lumeo-simplifies-vision-ai-development/
Over a billion cameras are deployed in the most important spaces worldwide and these cameras are critical sources of video and data. It is becoming increasingly important to understand how to harness this data to make our spaces and processes more efficient and safer. Lumeo, an NVIDIA Metropolis partner, provides a ‘no-code’ video analytics platform…Powered by Discourse, best viewed with JavaScript enabled"
505,ask-me-anything-series-nvidia-experts-answer-your-questions-live,"Originally published at:			Ask Me Anything Series: NVIDIA Experts Answer Your Questions Live | NVIDIA Technical Blog
Join NVIDIA experts and the developer community on July 28 for our first Ask Me Anything.Hi, i use a nvidia G210M and i have a problem with my graphics driverI recently installed the latest version of my graphics driver, then it zoomed in my screen and showed an error code 43 on device managerI tried reinstalling the driver’s again but still didn’t workCan you please help me resolve this issueHi, i use a nvidia G210M and i have a problem with my graphics driverI recently installed the latest version of my graphics driver, then it zoomed in my screen and showed an error code 43 on device managerI tried reinstalling the driver’s again but still didn’t workCan you please help me resolve this issueHi @mariodemodder,Welcome to the NVIDIA Developer forums. You posted in the blogs section of the developper community. There is no support given at this section. If this is a consumer card issue, you should post to the Geforce forums. For enterprise and developer issues, please post in the drivers category.Best regards,
Tom Kwhile  running this command “!pkexec dpkg -i ~/Downloads/cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb”:
i am getting this and it is staying as it isSelecting previously unselected package cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48.
(Reading database … 279400 files and directories currently installed.)
Preparing to unpack …/cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb …
Unpacking cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48 (1.0-1) …
Setting up cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48 (1.0-1) …
what to do?Powered by Discourse, best viewed with JavaScript enabled"
506,take-ai-learning-to-the-edge-with-nvidia-jetson,"Originally published at:			https://developer.nvidia.com/blog/take-ai-learning-to-the-edge-with-jetson/
The NVIDIA Jetson Orin Nano and Jetson AGX Orin Developer Kits are now available at a discount for qualified students, educators, and researchers.Since its initial release almost 10 years ago, the NVIDIA Jetson platform has set the global standard for embedded computing and edge AI. These high-performance, low-power modules and developer kits for deep learning…Is the education discount available in Canada?You should be able to get the discount in Canada from this link:NVIDIA offers discounts on Jetson Orin Nano Developer Kit and Jetson AGX Orin Developer Kit to the students. If you are educator, then contact us for multiple Jetson Developer Kits.This will be charged in USD, but should work for you. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
507,experimental-ai-powered-hearing-aid-automatically-amplifies-who-you-want-to-hear,"Originally published at:			Experimental AI Powered Hearing Aid Automatically Amplifies Who You Want to Hear | NVIDIA Technical Blog
To help people who suffer from hearing loss, Researchers from Columbia University just developed a deep learning-based system that can help amplify specific speakers in a group, a breakthrough that could lead to better hearing aids.  “The brain area that processes sound is extraordinarily sensitive and powerful; it can amplify one voice over others, seemingly…Powered by Discourse, best viewed with JavaScript enabled"
508,a-comprehensive-guide-on-interaction-terms-in-time-series-forecasting,"Originally published at:			https://developer.nvidia.com/blog/a-comprehensive-guide-on-interaction-terms-in-time-series-forecasting/
Learn about interaction terms in the context of time series forecasting and how to effectively implement interaction terms in your models.Powered by Discourse, best viewed with JavaScript enabled"
509,3d-content-interoperability-with-topology-free-modeling,"Originally published at:			https://developer.nvidia.com/blog/3d-content-interoperability-with-topology-free-modeling/
NVIDIA Inception Program member Shapeyard is solving the metaverse 3D content interoperability challenge by automating topology generation in multiple levels of detail at exporting or streaming.Really interesting work being done here by Shapeyard. If people could use ipad and phones for 3d modelling, that would be a game changer.
Does anyone know if there will be a symmetry feature added soon? That would cut creator development time in 1/2.
Thanks,
-ZiaI had a go at this app , it does have potential however the app has a lot of bugs and issues at the moment to fix before I will give it another go.It crashed a lot throughout testing the app, so if you are going to try out this app , save your progress constantly and I mean constantly.The rotation snapping is not 100% perfect all the time as well so you may not get a perfect angle shape. check it from all axis and you will see what I mean. It sometimes snaps at offset angle so you have to judge by eye to correct it. As I said it does have potential but I will wait for updates to fix these issues before trying the app again.Powered by Discourse, best viewed with JavaScript enabled"
510,generating-ray-traced-caustic-effects-in-unreal-engine-4-part-2,"Can’t wait to see 4.26.2 rolling out :D because of metahuman :D
and I really should say your work is masterpiece … keep it going guysThank you! We are upgrading it to 4.26.2 and will release it ASAP :)Hello and thank you for sharing this very exciting work!I’ve read everything I could find, but have been struggling for a week to build this.  All of the commands under Building and running end with similar errors. I documented build errors from 4.25 here.  Also with both windows and linux I’m receiving this make error when building the NvRTX_Caustics-4.26 branch. Windows VS showed 5 build errors in total.  Searching the forums I don’t see anyone else with this issue, and this isn’t my first rodeo.  I’ve built many branches of UE, and other programs all the way down to custom Linux kernels for diskless nodes that boot from NFS drives.What could be the fix?[2/613] Compile Module.Engine.35_of_48.cpp
In file included from /mnt/2.26/Engine/Intermediate/Build/Linux/B4D820EA/UE4Editor/Development/Engine/Module.Engine.11_of_48.cpp:11:
/mnt/2.26/Engine/Source/Runtime/Engine/Private/Components/LightComponent.cpp:321:4: error: field ‘bTiledDeferredLightingSupported’ will be initialized after field ‘bAffectWaterCaustics’ [-Werror,-Wreorder-ctor]
, bTiledDeferredLightingSupported(false)
^Ubunbtu Linux Kernel 5.4.0-66-generic
NVIDIA Driver Version: 460.73.01
CUDA Version: 11.2, and an RTX 3090Thanks for any insight :)Hello, sorry for the delay. I think you get this error because you are turning on the “Treat warnings as errors” option in your compiler. To fix this issue, you could turn off the “Treat warnings as errors” option or relocate the variable initializing position in the code. Could you move the WaterCausticsPrecision,  NumWaterCausticsMapCascades & WaterCausticsMapCascadeScale ahead of the IESTexture and move the bAffectWaterCaustics ahead of bCastModulatedShadows ? This should fix the compiling warnings. And we will submit a change to our branch to fix it later, thank you for pointing out this problem !Here comes the NVRTX 4.26.2 and now I’m assuming an update for this branch will come soon. so excited :DHi there again:D
Sorry for asking too much but I just wanted to know what your plan is for uneral engine support?
do you guys have any plan for UE5  or not? cause I’m just thinking about Lumen with caustics and its blowing my mind.
please inform me if it is going to be in your plan or not thanks a lotCurrently we don’t have a clear plan for UE5. But I think we will continue our support for caustics.That is the most relaxing “not so clear plan” I’ve ever heard :D
thanks a lot and I hope you guys the best for letting the magic continue its wizardry.Huge thanks to developers at Nvidia, this ray trace looks really nice, hope UE5 will keep ray trace as an option, if not, just keep using UE4 is still not a bad choose.Currently we are still focusing on UE4, but I think we will try to move all the stuff to UE5 once it’s ready :DHey, just want to post a question here, i’m use UE 4.26.2, NvRTX_Caustics-4.26 branch, when i open the example file “MeshCausticsDemo_4.25” i can see Caustics is working, but all glass objects has no shadow. i have tried/checked everything relates to the shadow, with no success to turn on the shadow in UE viewport.  but if i run “MeshCaustics_923”  i can see everything are in there includes the beautiful Caustics and shadow. so my question is, how i can bring back shadow in “MeshCausticsDemo_4.25”?  to get the same look as we have in “MeshCaustics_923”.thanks in advance.Hey  Ossi.lutot,  i also have download the test file,  “MeshCausticsDemo_4.25”  but when i open it, i got the caustic working, but all glass objects has no shadow in my ue viewport. do you have the same issue?Hi Nran! With a quick check, everything seems to be working as it should for me. I understood that you have probably tried all the settings already, so really don’t know what might be going on - maybe if you already haven’t, try the cinematic viewport with a fresh cinecamera if there are some postprocessing settings affecting your regular viewport?cinematic viewportin my case, i can see the shadow in indirect reflection, but for direct shadow on the ground, i don’t see shadow on the ground, this is strange.Hey Youngy1,  do you guys have any plan to release Ray-traced Caustic for UE4.27?  because in 4.27, the ray-traced translucency material will cast shadow by default. this is something was not available in UE4.26,Hi Nran, we are working on the upgrading, and will release the Ray-traced Caustic UE4.27 ASAP.WOW,love it,  thanks to Nvidia!I’m using unrealcv 0.3.7 to get the images from cinematic camera. Seems that caustic light is not “visible” to this camera. Is there a way to fix this?@youngy1 Is there any progress on making this available for UE5.x? Best case would, of course, be if it can be integrated in the NvRTX 5.x.x releases! :)Wait… now I found NvRTX/UnrealEngine at NvRTX_Caustics-5.1 (github.com)So it’s there already!Is there any timeline for 5.2?Powered by Discourse, best viewed with JavaScript enabled"
511,nsight-visual-studio-edition-2019-1-released,"Originally published at:			Nsight Visual Studio Edition 2019.1 Released | NVIDIA Technical Blog
NVIDIA Nsight Visual Studio Edition 2019.1 is now available for download in the NVIDIA Registered Developer Program.   The NVIDIA Nsight Visual Studio Edition 2019.1 is now available for download. Version 2019.1 extends support to the latest Turing GPUs and Win10 RS5. The Graphics Debugger adds Vulkan Pixel History as well as OpenGL + Vulkan…Powered by Discourse, best viewed with JavaScript enabled"
512,will-pt-sdk-support-subsurface-scattering-for-human-skins,"Will PT SDK support Subsurface scattering for human skins?
if yes, in realtime mode or in reference mode?Thanks for your question, this is something we are considering and can prioritize based on feedback! It would require upgrading the BSDF and potentially denoising modifications.
If/when we implement subsurface scattering materials, will aim for support in both reference and real-time modes.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
513,mlperf-hpc-v1-0-deep-dive-into-optimizations-leading-to-record-setting-nvidia-performance,"Originally published at:			https://developer.nvidia.com/blog/mlperf-hpc-v1-0-deep-dive-into-optimizations-leading-to-record-setting-nvidia-performance/
Learn about the optimizations and techniques used across the full stack in the NVIDIA AI platform that led to a record-setting performance in MLPerf HPC v1.0.@jwitsoe , can I get any assitance on Nvidia’s MLPerf HPC v1.0 implementation on Cosmoflow , please direct me to correct forum , I rasied issue on github but looks like its a dead horsePowered by Discourse, best viewed with JavaScript enabled"
514,minimizing-storage-usage-on-jetson,"Originally published at:			https://developer.nvidia.com/blog/minimizing-storage-usage-on-jetson/
Some NVIDIA Jetson modules have limited storage space, which imposes a challenge in packing applications and libraries. Here are ways to cut down on disk usage.This breaks everything. Tried to install python3.8 packages, got this:Impossible to deploy our applications here (currently using internal limited emmc memory as stated in your example).@jwitsoe how do we use minimized configuration for adding wifi network auto-connection to the filesystem of e.g. jetson sdcard without OS booted?Hi @Andrey1984,
Unfortunately, there is no specifically built scheme to do that like how it is on RPi.
If it is using Network Manager, one may be able to manually create the .nmconnection file under /etc/NetworkManager/system-connections/, but I’m not sure how default user creation through the OEM-config affects it.For our reference, if possible, would you explain what situation you think such wifi auto-connection configuration scheme is needed?Hi @cyato
Thank you for your reply.
I think I have tried just copying existing /etc/NetworkManager/system-connections/  to nvidia supplied sdcard image filesystem after writing it to sdcard. It did not seem that the robot appeared on the wifi network after the boot. Maybe more files need to be edited or copied?Our use case:Remote team has a robot that got jetson nx devkit deep inside.
Kind of no serial or usb or other cables or ports are accessible of the nx devkit.
The only somehow accessible  thing is  sdcard slot. The robot does need to boot from sdcard
The need is to prepare sdcard at Host PC in a way, so once it is added to the robot the robot on the first boot can be accessed via wifi already. As otherwise there is no way to access it at all.Moreover, people in remote factory [ robot deployment location] can not  do more complex action than to insert sdcard to HostPC or to nx devkit[robot] in case OS needs to be deployed. The rest is remote operation.Reference findings include but are not limited to listed below:Hi @Andrey1984,
Thank you for explaining your use-case.How about setting up another Xavier NX Developer Kit by flashing the NVIDIA-provided SD card image, setting up the WLAN connection, then making the whole image of the SD card?
You can flash another microSD card using the image, send the new microSD card to your remote team, and let them boot the robot with the new microSD card, and it should connect to the wireless AP on site.When you download the NVIDIA-provided SD card image for the Xavier NX Developer Kit, be sure to select the same version of JetPack as the one that is currently running on the robot.If you need guidance on cloning the SD card, please let us know.I think I have tried just copying existing /etc/NetworkManager/system-connections/ to nvidia supplied sdcard image filesystem after writing it to sdcard. It did not seem that the robot appeared on the wifi network after the boot. Maybe more files need to be edited or copied?Thank you also for providing the result of what you tested.
According to this post, the file actually contains the MAC address of the NIC of the unit, so that needs to be replaced.
A hack would be to perform this sed operation on the first boot.Hi @cyato
Thank you for pointing out the sdcard clone method. We used it. It is known.However, it is not easily remotely applicable due to existing constraints.We are trying to figure out how to add wifi entry using nvidia sdcard image filesystem edits, or rootfs edits / BSP edits specifically. Without a need to use a second nx devkit unit. So that all edits ca be done at Host PC already , then written to sdcard. So that sdcard  can be used for one-time-bringup of a robot once added to nx devkit without a need to interface anyhow with the system but for wireless connection.
Like as it is implemented in rpi wpa_supplicant or jetson sdcard for duckietown wpa_supplicant. So that just after editing files on the sdcard filesystem first boot reqults in connection already. Without need to use second reference hardware devkit somehow.Could you confirm that you were able to combine the sed + copying the system-connection so that sdcard does boot up with connection activated already if using the nvidia supplied sdcard image for nx devkit?Powered by Discourse, best viewed with JavaScript enabled"
515,implementing-usd-for-game-development-pipelines-an-interview-with-polyphony-digital,"Originally published at:			https://developer.nvidia.com/blog/implementing-usd-for-game-development-pipelines-an-interview-with-polyphony-digital/
Polyphony Digital, a subsidiary of Sony Interactive Entertainment Inc and the creators of Gran Turismo, has exceeded 90M copies of cumulative sell-through sales of PlayStation software titles over three decades. Gran Turisimo 7, released in 2022, marked the 25th anniversary of the series’ beginning, and included implementation of Universal Scene Description (USD). USD is an…Powered by Discourse, best viewed with JavaScript enabled"
516,ran-in-the-cloud-delivering-cloud-economics-to-5g-ran,"Originally published at:			https://developer.nvidia.com/blog/ran-in-the-cloud-delivering-cloud-economics-to-5g-ran/
Realizing the cloud economics for 5G RAN and driving the co-innovation of 5G with edge AI applications requires embracing RAN-in-the-Cloud.Powered by Discourse, best viewed with JavaScript enabled"
517,what-options-do-i-have-to-get-path-tracing-in-my-application-in-ue5,"What options do I have to get path tracing in my application in UE5?Currently integration of path tracing requires fairly intensive engine changes, you should be familiar with the rendering subsystem of whatever engine you are using. We currently do not have a custom implementation of path tracing in Unreal Engine 5. However, NVIDIA’s NVRTX branch does feature many elements from the PT SDK used in non path tracing modes, including RTXDI based sampled lighting, SER, NRD and DLSS 3.Powered by Discourse, best viewed with JavaScript enabled"
518,building-transcription-and-entity-recognition-apps-using-nvidia-jarvis,"Originally published at:			https://developer.nvidia.com/blog/building-transcription-and-entity-recognition-apps-using-jarvis/
In the past several months, many of us have grown accustomed to seeing our doctors over a video call. It’s certainly convenient, but after the call ends, those important pieces of advice from your doctor start to slip away. What was that new medication I needed to take? Were there any side effects to watch…Can sample rate for Jarvis ASR be 8000hz?Thanks for your question! Yes, Jarvis supports 8 kHz input for ASR. The server will automatically upsample to 16 kHz. Just make sure you specify the sample rate as 8000 in the recognitionConfig being sent in the request.@jwitsoe
Hello
We were able to run the rive docker client service
However how exactly to replicate the demo in the article https://developer.nvidia.com/blog/building-transcription-and-entity-recognition-apps-using-riva/#entry-content-comments ?Thanks for your interest in the demo. Glad to hear you’ve been able to run the docker client. We have included instructions on running that demo from the docker here: Riva Contact — NVIDIA RivaCheers,
Chris@cparisien
Hi, thank you for your reply
Could you extend on how to start the callcenter with Medical NER German model, please?
AVHi @Andrey1984,We don’t have a medical NER model in German, and unfortunately I’m not immediately aware of a good dataset you could use. A quick search turned up this one, which might be worth a try – I can’t comment on how well it might suit your application, and you’d likely have to do some data preprocessing.The current state of adoption of well-structured electronic health records
and integration of digital methods for storing medical patient data in
structured formats can often considered as inferior compared to the use of
traditional, unstructured...If you do find a suitable dataset, you could use TAO to train or adapt an appropriate model:
https://docs.nvidia.com/tao/tao-toolkit/text/nlp/token_classification.htmlThen once you have that model, you can deploy it in Riva:
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/custom-model-deployment.htmlI hope that helps!ChrisThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.@cparisien
Hi, so what are the existing options for  german ASR?
Is there more support by now for the Medical NER ?
How do we integrate GitHub - frankkramer-lab/GERNERMED: GERNERMED is the first open neural NER model for medical entities designed for German data. ?
Could you extend on steps to get it as a proof of concept implemented by using “TAO to train or adapt an appropriate model”, please?
Thanks
AVWe do have some updates here:I hope that helps!
ChrisI was looking to replicate this demo (https://developer.nvidia.com/blog/building-transcription-and-entity-recognition-apps-using-riva/) which led me to this forum entry.  The solution link in this chat is trying to point to a “callcenter.html” demo.  After some searching I realized the demo has been renamed to “Riva Contact” (Riva Contact — NVIDIA Riva).  Hope this helps someone.@svaha  – Link fixed, thank you!Powered by Discourse, best viewed with JavaScript enabled"
519,accelerating-random-forests-up-to-45x-using-cuml,"Originally published at:			https://developer.nvidia.com/blog/accelerating-random-forests-up-to-45x-using-cuml/
This post was originally published on RAPIDsai. Random forests are a popular machine learning technique for classification and regression problems. By building multiple independent decision trees, they reduce the problems of overfitting seen with individual trees. In this post, I review the basic random forest algorithms, show how their training can be parallelized on NVIDIA…Hi,
Thanks for this helpful post. I have issues using dask_util.  I am getting the error:NameError: name 'dask_utils' is not defined.Could you please explain, where I can get the dask_util file?Best,
Mandar KulkarniPowered by Discourse, best viewed with JavaScript enabled"
520,advanced-api-performance-sampler-feedback,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-sampler-feedback/
This post covers best practices for using sampler feedback on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips. Sampler feedback is a DirectX 12 Ultimate feature for capturing and recording texture sampling information and locations. Sampler feedback was designed to provide better support for…Powered by Discourse, best viewed with JavaScript enabled"
521,powering-nvidia-certified-enterprise-systems-with-arm-cpus,"Originally published at:			https://developer.nvidia.com/blog/powering-nvidia-certified-enterprise-systems-with-arm-cpus/
NVIDIA has approved the first NVIDIA-Certified Systems with Arm CPUs, giving enterprises an easy way to select systems that are optimally configured to run accelerated computing workloads.Powered by Discourse, best viewed with JavaScript enabled"
522,explainer-what-is-edge-computing,"Originally published at:			What Is Edge Computing? | NVIDIA Blog
Edge computing is the practice of processing data physically closer to its source.Powered by Discourse, best viewed with JavaScript enabled"
523,nvidia-morpheus-helps-defend-against-spear-phishing-with-generative-ai,"Originally published at:			https://developer.nvidia.com/blog/nvidia-morpheus-helps-defend-against-spear-phishing-with-generative-ai/
The NVIDIA Morpheus cybersecurity framework helps defend against spear phishing using generative AI.Where can I get more information? I want to know how to deploy my data processing and execution, thank you very muchHi there! Thanks for reaching out! You can get more information on the latest ways you can leverage NVIDIA Morpheus, including the spear phishing use case by watching Bartley Richardson’s GTC session on demand here Learn About New AI-Based Cybersecurity Use Cases and Capabilities | NVIDIA On-DemandPowered by Discourse, best viewed with JavaScript enabled"
524,does-the-training-dataset-contain-3d-models-for-the-simpsons-character,"GET3D seems to be able to generate the Simpson styled objects. The Simpsons as idea or concept is copyrighted as far as I know. Does the training dataset contain 3D models for the Simpsons character? If so, how do you avoid the copyright claim from its owners?Thank you for the question. The dataset used to train Get3D does not contain Simpsons characters. The examples you are probably referring to are from text-guided shape generation where we use pre-trained weights of StyleGan-Nada to guide the generation process to a specific style.Thank you. Best wishes.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
525,boost-edge-ai-performance-with-the-new-nvidia-jetson-orin-nx-16gb,"Originally published at:			https://developer.nvidia.com/blog/boost-edge-ai-performance-with-the-new-nvidia-jetson-orin-nx-16gb/
Building on the momentum from last year’s expansion of NVIDIA Jetson edge AI devices, the NVIDIA Jetson Orin NX 16 GB module is now available for purchase worldwide.  The Jetson Orin NX 16 GB module is unmatched in performance and efficiency for small form-factor, low-power robots, and autonomous machines. This makes it ideal for use…Powered by Discourse, best viewed with JavaScript enabled"
526,improving-video-quality-and-performance-with-av1-and-nvidia-ada-lovelace-architecture,"Originally published at:			https://developer.nvidia.com/blog/improving-video-quality-and-performance-with-av1-and-nvidia-ada-lovelace-architecture/
NVIDIA NVENC AV1 offers substantial compression efficiency with respect to H.264 and HEVC at better performance.Would it be possible to add AV1 information to NVML?Just wondering if its possible to add support for 4:4:4 unless this is a hardware limitation?AV1 4:4:4 encoding is not supported at present. We may consider it in future hardware generations.We are investigating adding AV1 information in the next update of NVML.That would be great. I don’t have a 40 series card myself but I’m sure as they gain adoption people will be looking for that information.A suggestion regarding the API design: it would be great if all of the encoders/decoders information could be aligned behind a well-documented single function as much as possible so that application developers who don’t have the hardware are more confident that AV1 information reporting works. It would also be nice if existing NVML functions could be better documented on how they respond to different video encoders/decoders being used.Looks like work was done to fix NVML’s broken nvmlDeviceGetEncoderSessions function but nvmlEncoderType_t still only lists H264 and HEVC encoder types.  Is AV1 support still being worked on?Powered by Discourse, best viewed with JavaScript enabled"
527,turbocharging-generative-ai-workloads-with-nvidia-spectrum-x-networking-platform,"Originally published at:			https://developer.nvidia.com/blog/turbocharging-ai-workloads-with-nvidia-spectrum-x-networking-platform/
NVIDIA Spectrum-X networking platform is an end-to-end solution that combines AI-optimized networking hardware and software to provide predictable, consistent performance required by AI workloads.Powered by Discourse, best viewed with JavaScript enabled"
528,predicting-credit-defaults-using-time-series-models-with-recursive-neural-networks-and-xgboost,"Originally published at:			https://developer.nvidia.com/blog/predicting-credit-defaults-using-time-series-models-with-recursive-neural-networks-and-xgboost/
Today’s machine learning (ML) solutions are complex and rarely use just a single model. Training models effectively requires large, diverse datasets that may require multiple models to predict effectively. Also, deploying complex multi-model ML solutions in production can be a challenging task. A common example is when compatibility issues with different frameworks can lead to…Powered by Discourse, best viewed with JavaScript enabled"
529,multilingual-and-code-switched-automatic-speech-recognition-with-nvidia-nemo,"Originally published at:			https://developer.nvidia.com/blog/multilingual-and-code-switched-automatic-speech-recognition-with-nvidia-nemo/
Multilingual automatic speech recognition (ASR) models have gained significant interest because of their ability to transcribe speech in more than one language. This is fueled by the growing multilingual communities as well as by the need to reduce complexity. You only need one model to handle multiple languages. This post explains how to use pretrained…Powered by Discourse, best viewed with JavaScript enabled"
530,nvidia-on-demand-rapids-sessions-from-gtc-2023,"Originally published at:			Playlist | RAPIDS at GTC 2023 | NVIDIA On-Demand
Get the latest best practices about how to accelerate your data science projects with RAPIDS.Powered by Discourse, best viewed with JavaScript enabled"
531,programming-the-quantum-classical-supercomputer,"Originally published at:			https://developer.nvidia.com/blog/programming-the-quantum-classical-supercomputer/
This post introduces CUDA Quantum, highlights its unique features, and demonstrates how researchers can leverage it to gather momentum in day-to-day quantum algorithmic research and development.Powered by Discourse, best viewed with JavaScript enabled"
532,nvidia-ai-red-team-an-introduction,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/
Machine learning has the promise to improve our world, and in many ways it already has. However, research and lived experiences continue to show this technology has risks. Capabilities that used to be restricted to science fiction and academia are increasingly available to the public. The responsible use and development of AI requires categorizing, assessing,…Powered by Discourse, best viewed with JavaScript enabled"
533,ask-me-anything-build-custom-ai-tools-with-chatgpt-and-nvidia-omniverse,"Originally published at:			Connect with Experts - Ask Me Anything Series | NVIDIA Developer
On June 28, join us to ask our experts how to build an AI-powered extension for NVIDIA Omniverse using ChatGPT.Powered by Discourse, best viewed with JavaScript enabled"
534,rtx-path-tracing-sdk-multi-gpu-support,"Hi, I was wandering if there are any plan to add support for DirectX 12 explicit multi-GPU support to the RTX Path Tracing SDK. To my knowledge, the only mGPU capable real-time raytracing and real-time path tracing engine is Nvidia Omniverse but, it can not be used to ship a packaged game and there are no games, to my knowledge, that support mGPU with even classic real-time raytracing. Will the RTX Path Tracing SDK change this in the future and bring mGPU support to real-time path tracing?
Thank youThis is something we could consider and can prioritize based on feedback! Main path tracing is relatively trivial to parallelize to multiple GPUs via tiling or similar approach, but denoising and the rest of the post process pipeline would take more effort.Use the Like button to indicate your interestPowered by Discourse, best viewed with JavaScript enabled"
535,top-conversational-ai-sessions-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/top-conversational-ai-sessions-at-nvidia-gtc-2023/
Learn about the latest tools, trends, and technologies for building and deploying conversational AI.Powered by Discourse, best viewed with JavaScript enabled"
536,upcoming-event-nvidia-at-the-game-developers-conference-gdc,"Originally published at:			Game Developer Conference (GDC) 2023 | NVIDIA
Join us March 20-24 to discover the latest NVIDIA RTX and neural rendering technologies accelerating game development.Powered by Discourse, best viewed with JavaScript enabled"
537,no-way-to-watch-this,"no posted url to watch this event.Powered by Discourse, best viewed with JavaScript enabled"
538,diffusion-3d-model-on-consumer-level-hardware,"When might we see a path from stable diffusion into Get3d (or similar) possible to run on consumer grade hardware? Or are we going to be in ‘8 A100 class GPUs’ for the forseeable future? If the latter, will nvidia or a partner be hosting a cloud-based service to make this tech available to businesses not outfitted with that level of hardware?Hi, thanks for the questions! We can run inference for GET3D on consumer-grade GPUs if you’re following the instructions from here: GitHub - nv-tlabs/GET3D, which means you can run GET3D on any cloud service following our licence GET3D/LICENSE.txt at master · nv-tlabs/GET3D · GitHubThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
539,new-ai-app-provides-personalized-skin-care-evaluation,"Originally published at:			New AI App Provides Personalized Skin Care Evaluation | NVIDIA Technical Blog
With 65 years of skincare experience, Olay developed a deep learning-based tool that can act as your personalized beauty skin care advisor. “It’s become a real challenge to shop for skin care,” said Frauke Neuser, principal scientist at Olay. “You go into a store, and there are 50 yards of facial care products.” Olay, the…A very good app for those who want to maintain the basic beauty.Powered by Discourse, best viewed with JavaScript enabled"
540,upcoming-webinar-how-to-integrate-isaac-sim-with-ros,"Originally published at:			Isaac Sim series
Join this webinar on January 26 and learn how to integrate Isaac Sim into your ROS workflows to support robotics apps including navigation, manipulation, and more.Powered by Discourse, best viewed with JavaScript enabled"
541,generating-ray-traced-caustic-effects-in-unreal-engine-4-part-1,"Originally published at:			https://developer.nvidia.com/blog/generating-ray-traced-caustic-effects-in-unreal-engine-4-part-1/
Caustics are common optical phenomenon in the real world. From the sloshing sparkles by water surfaces to the curved highlights in the backlight of clear glass, they are everywhere. However, simulating accurate caustic in 3D graphics is not an easy job. For those who have been involved in creating ray tracers, you might know that…Find out how to access Unreal Engine source code on GitHubIt will grant you the access to Epic’s official UE4 repos and NVIDIA’s folks.The NVRTX_Caustics branch of UE4 is based on the UE4.25.3 NVRTX branch. The upgrade for 4.25.4 and 4.26 will be available soon.The sample scenes used in this post can be found at the following places:Executable demos: Tech_Demos - Google DriveAssets and projects: Demo_Projects - Google DriveIf you have any question on the algorithms, source, performance and usages of mesh caustics, please ask here.Do you have any plans to fix the bug that translucent objects are displayed incorrectly in reflections?
And what about ray tracing subsurface scattering?
Looking forward to your reply. Thx!To show translucent objects in reflections, you can use the following command lines to enable full binary tree of reflection+refraction bounces, which allow the refractions to be seen in reflections:Ray tracing subsurface scattering is still under researching stage. We are working behind it now.Has anyone seen this make error before with the NvRTX_Caustics-4.26 branch?What could be the fix?[2/613] Compile Module.Engine.35_of_48.cpp
In file included from /mnt/2.26/Engine/Intermediate/Build/Linux/B4D820EA/UE4Editor/Development/Engine/Module.Engine.11_of_48.cpp:11:
/mnt/2.26/Engine/Source/Runtime/Engine/Private/Components/LightComponent.cpp:321:4: error: field ‘bTiledDeferredLightingSupported’ will be initialized after field ‘bAffectWaterCaustics’ [-Werror,-Wreorder-ctor]
, bTiledDeferredLightingSupported(false)
^Ubunbtu Linux Kernel 5.4.0-66-generic
NVIDIA Driver Version: 460.73.01
CUDA Version: 11.2, and an RTX 3090Thanks in  advance,
GregoryReporting back that building on Windows with different hardware produces the exact same errors.  I wonder how people are successfully building this?This error message is indicating that the field ‘bTiledDeferredLightingSupported’ is being initialized after the field ‘bAffectWaterCaustics’, which is causing a reordering of the constructor initialization list.A possible fix for this issue would be to reorder the fields in the constructor initialization list so that ‘bAffectWaterCaustics’ is initialized before ‘bTiledDeferredLightingSupported’.Here’s an example of how the constructor could look after making this change:I downloaded NvRTX_Caustics-5.1 source code and Ran Setup.bat.
Then It showed below error.Checking dependencies…
Updating dependencies:   0% (0/97588)…
Unhandled exception. System.PlatformNotSupportedException: Thread abort is not supported on this platform.
at System.Threading.Thread.Abort()
at GitDependencies.Program.DownloadDependencies(String RootPath, IEnumerable1 RequiredFiles, IEnumerable1 Blobs, IEnumerable1 Packs, Int32 NumThreads, Int32 MaxRetries, Uri Proxy, String CachePath)    at GitDependencies.Program.UpdateWorkingTree(Boolean bDryRun, String RootPath, HashSet1 ExcludeFolders, Int32 NumThreads, Int32 MaxRetries, Uri Proxy, OverwriteMode Overwrite, String CachePath, Single CacheSizeMultiplier, Int32 CacheDays)
at GitDependencies.Program.Main(String Args)I tried this,but it didn’t work.What should i do?by the way,
In the NvRTX_Caustics-4.27, It showed below error.Failed to download ': The remote server returned an error: (403) Forbidden. (WebException)Getting exact same error. Is there a solution?@yaobino @jwitsoe :
Are there any plans to integrate this functionality in more recent Unreal Engine versions? It would be amazing if it was put in a plugin, like DLSS. :):):)Or, I guess another question is: Did any of this functionality make it into the new NvRTX 5.2.0-1?Powered by Discourse, best viewed with JavaScript enabled"
542,accelerated-data-analytics-faster-time-series-analysis-with-rapids-cudf,"Originally published at:			https://developer.nvidia.com/blog/accelerated-data-analytics-faster-time-series-analysis-with-rapids-cudf/
This post walks you through the common steps of time series data processing with RAPIDS cuDF.Powered by Discourse, best viewed with JavaScript enabled"
543,extending-nvidia-performance-leadership-with-mlperf-inference-1-0-results,"Originally published at:			Extending NVIDIA Performance Leadership with MLPerf Inference 1.0 Results | NVIDIA Technical Blog
Inference is where we interact with AI. Chat bots, digital assistants, recommendation engines, fraud protection services, and other applications that you use every day—all are powered by AI. Those deployed applications use inference to get you the information that you need. Given the wide array of usages for AI inference, evaluating performance poses numerous challenges…This is very exciting, I am a small time developer  just getting on my feet but it seems clear that team green will remain supreme. I really hope I can raise the money to get my hands on this technology when it’s ready.It seems to me when Nvidia choose Arm architecture it snowballed into the biggest jump start on the next level of innovation since x86 started it’s runEven if the above tests were hand picked, the graphs are too impressive to ignore. I want to know EVERYTHING!!!Hi Ronald. We’re pleased with our MLPerf Inference 1.0 results, and we submitted across ALL usages: CV, medical imaging, natural language processing, translation and recommender systems.  Our Triton Inference Server software did very well, as did our MIG technology to show the completeness of our data center platform.Cheers,
DeeGreat to hear about NVIDIA’s performance leadership in MLPerf Inference 1.0 results! They continue to push boundaries in the field.Powered by Discourse, best viewed with JavaScript enabled"
544,creating-faster-molecular-dynamics-simulations-with-gromacs-2020,"Originally published at:			https://developer.nvidia.com/blog/creating-faster-molecular-dynamics-simulations-with-gromacs-2020/
GROMACS logo GROMACS—one of the most widely used HPC applications— has received a major upgrade with the release of GROMACS 2020. The new version includes exciting new performance improvements resulting from a long-term collaboration between NVIDIA and the core GROMACS developers. As a simulation package for biomolecular systems, GROMACS evolves particles using the Newtonian equations…Dear all,I have tried to apply all the suggestions of this super interesting article but I got some problems.
I perform simulations with Gromacs 2020.2 on an HPC which has 4xV100 for each node (Driver Version: 450.51.06; CUDA Version: 11.0).as reported in the article, before starting the run I used the commands:export GMX_GPU_DD_COMMS=true
export GMX_GPU_PME_PP_COMMS=true
export GMX_FORCE_UPDATE_DEFAULT_GPU=trueand in the mdrun I specified:-nb gpu -bonded gpu -pme gpu -npme 1Moreover, the tpr file has been built specifying in the .mdp “constraints = h-bonds”.When the simulation starts I have these messages:Update task on the GPU was required, by the GMX_FORCE_UPDATE_DEFAULT_GPU environment variable, but the following condition(s) were not satisfied:
Domain decomposition without GPU halo exchange is not supported.
With separate PME rank(s), PME must use direct communication.
Will use CPU version of update.Any idea to solve the problem?Thank you so muchBest regards,FedericoHi Federico,I can see that the “GPU Update” feature is not being activated because the “GPU communication” features are not active, even though you are correctly setting the environment variables. My best guess therefore is that you are using an external MPI library rather than the GROMACS-internal thread MPI library - GPU communications are only supported in this release for the latter (where we now have support for the former merged into the master branch, to be included in GROMACS 2022). If so, please can you re-build with thread MPI and try again. If that’s not the case, please can you provide the full “md.log” file and I’ll take a look.AlanDear Alan,thank you so much for your reply! Your guess is right, the error appears when I use the Spectrum MPI library for multi-node/multi-replica simulations. I re-build a thread MPI version of GROMACS for a single-node run, and in that case everything works flawless.Thank youbest regards,FedericoHi Alan,Can you please give some pointers on the 3 systems you use (like how to get them/download them)? Basically we want to reproduce the same result in our system.Thanks!Maybe give the PDB ID or something like that?Hi,
Please can you provide your email address in this temporary form and I’ll  get in touch to help you get set up with this
GROMACS query - Google FormsThanks,AlanI followed the same steps but it throws the following error. Even though I increased the equilibration time but did not work.
Step 100: The total potential energy is nan, which is not finite. The LJ and
electrostatic contributions to the energy are 0 and -1.18497e+07,
respectively. A non-finite potential energy can be caused by overlapping
interactions in bonded interactions or very large or Nan coordinate values.
Usually this is caused by a badly- or non-equilibrated initial configuration,
incorrect interactions or parameters in the topology.Hi,Please can you re-try with the latest GROMACS version (2022). If you still get the error, please can you isolate which of the options described in the blog is triggering it, and then create an issue at GROMACS / GROMACS · GitLab (including your input files and command line options), and the dev team will take a look.Best regards,Alanhi, do you have similar benchmarks for Gromacs 2022 for comparison with 2020? thanksHi,Please see slide 4 of my presentation available at Presentation – PASC Program for some comparisons across versions 2019, 2020, 2021 and 2022, each running on the same A100 hardware. Please note that for the multi-GPU results, we tune for the optimal number of MPI tasks (in particular, running 2 MPI tasks per GPU can be often faster than a single task).Best regards,Alanthank you!Hi,I tried gromacs 2021.6 built using gcc-10.3.0 and cuda-11.1.
The system consisted of 81,743 atoms (slightly smaller than ADHD).
I exploited AMD EPYC™ 7763 (64 cores) and 4 x NVIDIA A100-SXM4.With simple:
$gmx -nt 64 -pin on -v -deffnm mdtest
performance: 93ns/dayWith your setup/parameters, using 64 physical cores:
$gmx -v -pin on -ntmpi 4 -ntomp 16 -nb gpu -bonded gpu -pme gpu -npme 1 -nstlist 400 -deffnm mdtest 
-nsteps 100000 -resetstep 90000 -noconfout
perfomance: 341ns/dayBased on your benchmark I expect no less than 450ns/day.Would you be so kind to commentThanks for your help and suggestions,
TamasHi,GROMACS performance is not simply influenced by the system size, but also quite strongly by specific details of the scientific system such and geometry and atom arrangement. So it’s possible that you are already getting good performance for your specific system. Here are some things to try/check:Mapping of GPU IDs to the 4 GPU tasks in the 4 ranks on this node:
PP:0,PP:1,PP:2,PME:3
PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU
PP task will update and constrain coordinates on the GPU
PME tasks will do all aspects on the GPU
GPU direct communication will be used between MPI ranks.Best regards,AlanDear Alan,Thanks for the suggestions.I have not thought about the effects of simulation system properties on performance.
Are not your tpr files available for benchmarking? I think that this would be very helpful not only for me but also for the community.All other issues (e.g. setting the environment variables, output of GPU task assignments) seem to be OK.
NVIDIA Nsight Systems profiler tool: I might try it. However, its application seems to be complicated for me, especially in an HPC environment.Best regards,
TamasHi,The STMV and ADHD  input files are available for download as the Supplementary Information archive for the paper Heterogeneous parallelization and acceleration of molecular dynamics simulations in GROMACSBest Regards,AlanPowered by Discourse, best viewed with JavaScript enabled"
545,getting-started-with-nvidia-nvue-api,"Originally published at:			NVUE API | Cumulus Linux 5.5
Learn how NVIDIA NVUE API automates data center network operations with sample code for Curl commands, Python Code, and NVUE CLI.Powered by Discourse, best viewed with JavaScript enabled"
546,developing-streaming-sensor-applications-with-holohub-from-nvidia-holoscan,"Originally published at:			https://developer.nvidia.com/blog/developing-streaming-sensor-applications-with-holohub-from-nvidia-holoscan/
The average car contains over 100 sensors to both monitor and respond to vital information. Ranging from an overheating engine to low tire pressure and erratic steering, sensors exist to provide automatic data, insight, and control. They make the act of driving safer for both the passengers and everyone else on the road. Of course,…Powered by Discourse, best viewed with JavaScript enabled"
547,top-telecommunications-sessions-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Join us for sessions from AT&T, Verizon, T-Mobile, Ericsson, and more to discover the latest innovations in telecom.Powered by Discourse, best viewed with JavaScript enabled"
548,new-ai-imaging-technique-reconstructs-photos-with-realistic-results,"Originally published at:			https://developer.nvidia.com/blog/new-ai-imaging-technique-reconstructs-photos-with-realistic-results/
Researchers from NVIDIA, led by Guilin Liu, introduced a state-of-the-art deep learning method that can edit images or reconstruct a corrupted image, one that has holes or is missing pixels. The method can also be used to edit images by removing content and filling in the resulting holes. The method, which performs a process called…Powered by Discourse, best viewed with JavaScript enabled"
549,training-your-nvidia-jetbot-to-avoid-collisions-using-nvidia-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/training-your-nvidia-jetbot-to-avoid-collisions-using-nvidia-isaac-sim/
Collecting a variety of data is important for AI model generalization. A good dataset consists of objects with different perspectives, backgrounds, colors, and sometimes obstructed views. The model should learn how to handle outliers or unseen scenarios. This makes the data collection and labeling process hard. In this post, we showcase sim2real capabilities of NVIDIA…
image1842×1727 265 KB

Hi Expert,
I am learning to use Isaac SIM . from this blog, I found no DR selection in Isaac SIM version 2022.2.0, could you help to teach me how to do this data generation like author  described in this blog?  thanks.Allan (ahou@nvidia.com)Powered by Discourse, best viewed with JavaScript enabled"
550,explain-your-machine-learning-model-predictions-with-gpu-accelerated-shap,"Originally published at:			https://developer.nvidia.com/blog/explain-your-machine-learning-model-predictions-with-gpu-accelerated-shap/
Learn how to train an XGBoost model, implement the SHAP technique in Python using a CPU and GPU, and compare results between the two.Thanks for checking out my blog on GPU accelerated SHAP values. Let me know if you have any questions about the blog or the code implementation.First off, thanks a lot for such a good blog!
I think I should check something what I missed.At Summary_plot,
it raiseand the progress never stop. what should I do?Hi @94cogus ,Firstly, apologies for replying late. I inadvertently missed the notification to your query. As for your issue, I tried it on my end but I encountered no error. Here is the link to the code that I tried let me know if you are still having issues reproducing the results : Google ColabHi! Can I use a summary plot with the implementation of SHAP on RAPIDS? I already tried, but it doesn’t results. Are there any way? Thank you!Powered by Discourse, best viewed with JavaScript enabled"
551,training-like-an-ai-pro-using-nvidia-tao-automl,"Originally published at:			https://developer.nvidia.com/blog/training-like-an-ai-pro-using-tao-automl/
There has been tremendous growth in AI over the years. With that, comes a larger demand for AI models and applications. Creating production-quality AI requires expertise in AI and data science and can still be intimidating for many developers. To develop accurate AI, you must choose what model architecture to use, what data to collect,…The one click deploy file is not a valid link: “Download the one-click deploy tar file and untar the package:”
Could you provide a valid link for tar file? Thanks!@behna.rahimi Can you try again, we have updated the linksPowered by Discourse, best viewed with JavaScript enabled"
552,new-video-composition-and-layering-with-universal-scene-description,"Originally published at:			https://developer.nvidia.com/blog/new-video-composition-and-layering-with-universal-scene-description/
Developers are using Universal Scene Description (OpenUSD) to push the boundaries of 3D workflows. As an ecosystem and interchange paradigm, OpenUSD models, labels, classifies, and combines a wide range of data sources into a composed ground truth. It is also highly extensible with four key features that help developers meet the demands of virtual worlds.…Powered by Discourse, best viewed with JavaScript enabled"
553,q-amp-a-with-remedy-entertainment-adopting-usd-into-the-game-development-pipeline,"Originally published at:			https://developer.nvidia.com/blog/qa-with-remedy-entertainment-adopting-usd-into-the-game-development-pipeline/
Mika Vehkala has worked in the game development industry for nearly three decades and contributed to projects such as Horizon Zero Dawn and The Walking Dead: No Man’s Land. Now, he’s director of technology at Remedy Entertainment, the studio that created Alan Wake and Control. Vehkala spoke with NVIDIA about embracing USD into its game…Powered by Discourse, best viewed with JavaScript enabled"
554,customize-your-own-carrier-board-with-nvidia-sdk-manager,"Originally published at:			https://developer.nvidia.com/blog/customize-your-own-carrier-board-with-nvidia-sdk-manager/
NVIDIA SDK Manager is the go-to tool for installing the NVIDIA JetPack SDK on NVIDIA Jetson Developer Kits. It provides a guided and simple way to install the development environment and get started with the developer kits in a matter of minutes. SDK Manager handles the dependencies between the components and brings the latest software…Powered by Discourse, best viewed with JavaScript enabled"
555,new-video-what-runs-chatgpt,"Originally published at:			https://developer.nvidia.com/blog/new-video-what-runs-chatgpt/
Some years ago, Jensen Huang, founder and CEO of NVIDIA, hand-delivered the world’s first NVIDIA DGX AI system to OpenAI. Fast forward to the present and OpenAI’s ChatGPT has taken the world by storm, highlighting the benefits and capabilities of artificial intelligence (AI) and how it can be applied in every industry and business, small…Powered by Discourse, best viewed with JavaScript enabled"
556,enabling-enterprise-ai-transformations-for-telcos-with-nvidia-and-vmware,"Originally published at:			https://developer.nvidia.com/blog/enabling-enterprise-ai-transformations-for-telcos-with-nvidia-and-vmware/
AI has the power to transform every industry, but transformation takes time, and it’s rarely easy. For enterprises across industries to be as successful as possible in their own transformations, they need access to AI-ready technology platforms. They also must be able to use 5G connectivity at the edge to harness valuable data and inform…Powered by Discourse, best viewed with JavaScript enabled"
557,building-state-of-the-art-biomedical-and-clinical-nlp-models-with-bio-megatron,"Originally published at:			https://developer.nvidia.com/blog/building-state-of-the-art-biomedical-and-clinical-nlp-models-with-bio-megatron/
With the advent of new deep learning approaches based on transformer architecture, natural language processing (NLP) techniques have undergone a revolution in performance and capabilities. Cutting-edge NLP models are becoming the core of modern search engines, voice assistants, chatbots, and more. Modern NLP models can synthesize human-like text and answer questions posed in natural language.…Before BIOMegatron, there was Optimus Prime, M.D.:von Davier, M. (2019). Training Optimus Prime, M.D.: Generating Medical Certification Items by Fine-Tuning OpenAI’s GPT-2 Transformer Model. [1908.08594] Training Optimus Prime, M.D.: Generating Medical Certification Items by Fine-Tuning OpenAI's gpt2 Transformer ModelPowered by Discourse, best viewed with JavaScript enabled"
558,state-of-the-art-real-time-multi-object-trackers-with-nvidia-deepstream-sdk-6-2,"Originally published at:			https://developer.nvidia.com/blog/state-of-the-art-real-time-multi-object-trackers-with-nvidia-deepstream-sdk-6-2/
When you observe something over a period of time, you can find trends or patterns that enable predictions. With predictions, you can, for example, proactively alert yourself to take appropriate action. More specifically, when you observe moving objects, the trajectory is one of the most important ways to understand the target object behavior, through which…Very impressive work! Likeee!Very cool, I am assuming this could work very well with player tracking for an AI camera recording a sporting event for example?If you have any info related to motion tracking and sports please share.ThanksLooks very impressive! Is it possible to run that vehicle tracking example from the article with the NvDCF tracker on Jetson Nano in real time? And what maximum input resolution is it able to process?Powered by Discourse, best viewed with JavaScript enabled"
559,uplifting-optimizations-debugging-and-performance-tuning-with-nvidia-nsight-developer-tools,"Originally published at:			https://developer.nvidia.com/blog/uplifting-optimizations-debugging-and-performance-tuning-with-nvidia-nsight-developer-tools/
When developing on NVIDIA platforms, the hardware should be transparent to you. GPUs can feel like magic, but in the interest of optimized and performant games, it’s best to have an understanding of low-level processes behind the curtain. NVIDIA Nsight Developer Tools are built for this very reason. Imagine a proud homeowner who lives in…Powered by Discourse, best viewed with JavaScript enabled"
560,best-avatars-to-use-for-lipsyncing,"What kind of avatars (metahuman etc.) would you recommend to connect to gpt?
Especially when it comes to A2FAs a developer platform, we welcome the use of any ecosystem APIs. Your usecase requirements decide on which which avatar system you’d like to use. For example, if you need real-time performance, then you’d select avatar systems (3D, 2D) accordingly. We have showcased a demo at Computex using LLMs with Avatars by Convai. Computex 2023 NVIDIA Generative AI Sparks Life into Virtual Characters - YouTube. It’s using Unreal engine and Metahuman.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
561,how-are-the-objects-aligned-in-your-example,"The objects in your demo look perfectly arranged at right angles, was that already part of the ChatGPT response?
Or is that done by the the placement script? If so, how?GPT-4 has a good spatial awareness and can place objects at right positions and even rotations. A great way to have more control on this is to include the rotation angle on X, Y, Z, axis in your request to the LLM as in, for example “include the X,Y,Z rotation for each object” and store that information in the desired result (for example, have the LLM store the info in a JSON variable).This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
562,building-xr-applications-on-pc-for-high-quality-graphics,"Originally published at:			https://developer.nvidia.com/blog/building-xr-applications-on-pc-for-high-quality-graphics/
When it comes to creating immersive, virtual environments, users want the experience to look as realistic and lifelike as possible. And while AIO headsets provide mobility and freedom for VR users, the headsets don’t always have enough power to render photorealistic scenes with accurate physics and lighting. Using the cloud and professional GPUs, you can…Powered by Discourse, best viewed with JavaScript enabled"
563,boosting-inline-packet-processing-using-dpdk-and-gpudev-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/optimizing-inline-packet-processing-using-dpdk-and-gpudev-with-gpus/
Inline processing of network packets using GPUs is a packet analysis technique useful to a number of different applications.This was a really interesting article. Looking at the DPDK links I noticed the only supported cards the driver supports are  V100/A100 Tesla class GPU’s. Would there be any intention to change this in the future to lower spec Quadro cards? Also is this technique specific to infiniband or can it be used for vanilla Ethernet too?I recently extended the support for more GPUs dpdk/devices.h at main · DPDK/dpdk · GitHub if your Tesla or Quadro GPU is not there please let me know and I will add it.
You can use whatever card supports GPUDirect RDMA to receive packets in GPU memory but so far this solution has been tested with ConnectX cards only.Great - my card (A4000) is there now - thanks! I’m intending to test this with a ConnectX-5 NIC to process ethernet packets. One more question - is a multi GPU setup required for this - I am assuming for persistent kernels this is a requirement, but how about for method 4 ?  I would like to try a dev setup with the A4000 card as both my display device and a packet processor.Thank you for an interesting article. How is the data consistency issue (as described in [1]) resolved for the persistent kernel (method 3)? As I understand the GDRCopy translates to RDMA operations, but afaik ordering between RDMA ops aren’t ensured from the perspective of a concurrently running GPU kernel.[1] GPUDirect RDMA :: CUDA Toolkit DocumentationNo you don’t need a multi-GPU setup with any of the methods described in the post. As an example, in case of persistent kernel you need to tune the number of CUDA blocks (i.e. persistent kernel occupancy) to not occupy the entire GPU and have SMs available for other processing kernelsTo address the data consistency issue you can use the rte_gpu_wmb() function before notifying to the running CUDA kernel that a new set of packets is ready. You can find an example hereI have the same question about the A4000.  I modified the cuda.c driver to include the A4000
device id, but I’m getting a crash in the mlx5 driver (I’m using a ConnectX-4):#0  0x00005555563b35f7 in mlx5_tx_burst_mti ()
#1  0x0000555556233c09 in rte_eth_tx_burst (nb_pkts=, tx_pkts=, queue_id=2, port_id=0)
at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:570
#2  tx_core (arg=0x2) at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:584
#3  0x0000555555763f04 in eal_thread_loop.cold () at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:757
#4  0x00007ffff7f16609 in start_thread (arg=) at pthread_create.c:477
#5  0x00007ffff7a85133 in clone () at …/sysdeps/unix/sysv/linux/x86_64/clone.S:95I have the same question about the A4000.  I modified the cuda.c driver to include the A4000
device id, but I’m getting a crash in the mlx5 driver:#0  0x00005555563b35f7 in mlx5_tx_burst_mti ()
#1  0x0000555556233c09 in rte_eth_tx_burst (nb_pkts=, tx_pkts=, queue_id=2, port_id=0)
at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:570
#2  tx_core (arg=0x2) at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:584
#3  0x0000555555763f04 in eal_thread_loop.cold () at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:757
#4  0x00007ffff7f16609 in start_thread (arg=) at pthread_create.c:477
#5  0x00007ffff7a85133 in clone () at …/sysdeps/unix/sysv/linux/x86_64/clone.S:95ConnectX4 is an old card and it’s been a while since I tested this solution on it. Can you build DPDK in debug mode and provide more info about the problematic line in mlx5_tx_burst_mti() function?Re-ran with debug enabled in dpdk lib (below).I ordered a ConnectX-5, which should arrive next week or so, i.e. hopefully newer board.hread 9 “lcore-worker-5” received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fffd7fff000 (LWP 527851)]
0x00005555567a5be4 in mlx5_tx_eseg_data (olx=83, tso=0, inlen=18, vlan=0, wqe=0x7ff7ff9a3000,
loc=0x7fffd7ff95d0, txq=0x7ff7ff9e4380) at …/drivers/net/mlx5/mlx5_tx.h:1016
1016		es->inline_data = *(unaligned_uint16_t *)psrc;
(gdb) where
#0  0x00005555567a5be4 in mlx5_tx_eseg_data (olx=83, tso=0, inlen=18, vlan=0,
wqe=0x7ff7ff9a3000, loc=0x7fffd7ff95d0, txq=0x7ff7ff9e4380)
at …/drivers/net/mlx5/mlx5_tx.h:1016
#1  mlx5_tx_burst_single_send (olx=83, loc=0x7fffd7ff95d0, pkts_n=64, pkts=0x7ff7f34a8b48,
txq=0x7ff7ff9e4380) at …/drivers/net/mlx5/mlx5_tx.h:3222
#2  mlx5_tx_burst_single (olx=83, loc=0x7fffd7ff95d0, pkts_n=64, pkts=0x7ff7f34a8b40,
txq=0x7ff7ff9e4380) at …/drivers/net/mlx5/mlx5_tx.h:3366
#3  mlx5_tx_burst_tmpl (olx=83, pkts_n=64, pkts=0x7ff7f34a8b40, txq=0x7ff7ff9e4380)
at …/drivers/net/mlx5/mlx5_tx.h:3564
#4  mlx5_tx_burst_mti (txq=0x7ff7ff9e4380, pkts=0x7ff7f34a8b40, pkts_n=64)
at …/drivers/net/mlx5/mlx5_tx_nompw.c:27
#5  0x00005555556f99e9 in rte_eth_tx_burst (nb_pkts=, tx_pkts=0x7ff7f34a8b40,
queue_id=2, port_id=0) at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:570
#6  tx_core (arg=0x2) at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:588
#7  0x000055555aa8926f in eal_thread_loop (arg=0x0) at …/lib/eal/linux/eal_thread.c:140
#8  0x00007ffff7f16609 in start_thread (arg=) at pthread_create.c:477
#9  0x00007ffff7a78133 in clone () at …/sysdeps/unix/sysv/linux/x86_64/clone.S:95Did you disable the tx inlining when launching l2fwd-nv? As an example, -a b5:00.1,txq_inline_max=0Yes:$L2FWD/build/l2fwdnv -l 0-9 -n 1 -a 06:00.0,txq_inline_max=0 -a 01:00.0 – -m 1 -w 0 -b 64 -p 4 -v 0 -z 0Interestingly, if I reduce the number of cores and the number of pipelines, the app
doesn’t segv, and seems to behave.   This might be a learning curve issue, sorry for the false alarm.Thanks for the reply. I was rather wondering how I could best ensure the consistency with pure RDMA (and not DPDK). Can I simply issue a local RDMA write from the CPU to GPU after the CPU has detected the new RDMA message on the GPU (e.g., through an RDMA completion event). When the GPU detects the flag, is it then ensured that the original RDMA message is completely consistent on the GPU? even for concurrently running GPU kernels?Great blog post and very good sample code in l2fwd-nv with GPU. Do you have it working with DPDK/DOCA on convergent DPU with GPU?  My version of DPU uses different version of DPDK so I was not sure if all dependencies will be satisfied or updated version of DPDK will soon enough make to converged DPU.The DPDK you find in the DPU already has this gpudev library installed so you can just use it. Anyway if you want your own DPDK version with gpudev you can just download the upstream DPDK from github and build it for arm64 on your DPU.Thank you for this great article. btw I have a question on you posting. Form figure 13, peak I/O performance throughput for the CPU and GPU are the same. In my understanding, using CPU memory means vanilla DPDK right? If so, afaik ConnectX6-Dx with DPDK can achieve the line rate at 64-byte packets also. But, from your posting the throughput for 64-bytes packets are just below 20 Gbps… why is that?Below is the DPDK performance report for ConnectX6-Dx.
https://fast.dpdk.org/doc/perf/DPDK_20_11_Mellanox_NIC_performance_report.pdfHello, may I ask why the l2fw-nv project uses a 60B digital packet as a segmentation data? Is there any basis for this?Powered by Discourse, best viewed with JavaScript enabled"
564,accelerating-nvidia-hpc-software-with-sve-on-aws-graviton3,"Originally published at:			Accelerating NVIDIA HPC Software with SVE on AWS Graviton3 | NVIDIA Technical Blog
The NVIDIA HPC SDK 22.7 now supports the AWS Gravition3 with auto-vectorization for the Scalable Vector Extension to the Arm architecture.I am trying to reproduce the SPEC cpu2017 fpspeed results in this blog. Can you please share the build flags used for these runs? Thanks.All results we measured on a single c7g-16xlarge AWS Graviton3 instance which has 64 cores.For the GNU results, I started with the “Example-gcc-linux-aarch64.cfg” config file included with CPU2017 updating the optimization flags to:OPTIMIZE         = -Ofast -fallow-argument-mismatch -fopenmp -march=armv8.4-a+crypto+rcpc+sha3+sm4+sve+nodotprod -lm -fpermissiveFor the NVHPC runs, I used the following config:Let me know if you have issues or questions.-MatPowered by Discourse, best viewed with JavaScript enabled"
565,get-started-with-ai-on-the-nvidia-jetson-nano-dli-course,"Originally published at:			https://developer.nvidia.com/blog/get-started-with-ai-on-the-nvidia-jetson-nano-dli-course/
Looking to get started with AI but don’t know how? NVIDIA has just published a new self-paced Deep Learning Institute course that uses the newly released Jetson Nano Developer Kit to get up and running fast.  In the course, students will learn to collect image data and use it to train, optimize, and deploy AI…Powered by Discourse, best viewed with JavaScript enabled"
566,explainer-what-is-quantum-computing,"Originally published at:			What Is Quantum Computing? | NVIDIA Blog
Quantum computers, still in their infancy, are influencing a new generation of simulations already running on classical computers, and now accelerated with the NVIDIA cuQuantum SDK.Powered by Discourse, best viewed with JavaScript enabled"
567,serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models,"Originally published at:			https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/
Learn the steps to create an end-to-end inference pipeline with multiple models using NVIDIA Triton Inference Server and different framework backends.Thanks for the detailed tutorial, very useful!However, this doesn’t seem like an apple-to-apple comparison. What if we do the pre and post processing locally using GPU, then the latency should be the same?Powered by Discourse, best viewed with JavaScript enabled"
568,autoscaling-nvidia-riva-deployment-with-kubernetes-for-speech-ai-in-production,"Originally published at:			https://developer.nvidia.com/blog/autoscaling-nvidia-riva-deployment-with-kubernetes-for-speech-ai-in-production/
Learn how to deploy NVIDIA Riva servers on a large scale with Kubernetes for autoscaling and Traefik for load balancing.Powered by Discourse, best viewed with JavaScript enabled"
569,metropolis-spotlight-nota-is-transforming-traffic-management-systems-with-ai,"Originally published at:			https://developer.nvidia.com/blog/metropolis-spotlight-nota-is-transforming-traffic-management-systems-with-ai/
Nota, an NVIDIA Metropolis partner, is using AI to make roadways safer and more efficient with NVIDIA’s edge GPUs and deep learning SDKs.TRAFFIC JUMP Problem: Omnidirectional hardware technology 360BALL DRIVE (Ballbot 2050) ProjectPowered by Discourse, best viewed with JavaScript enabled"
570,how-does-cugraph-support-gnns,"Thanks for hosting this - sorry if this is a dumb question but I am interested to know how exactly does cuGraph support GNN’sThanks again for your attention.Thanks !This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
571,power-up-your-skills-and-credentials-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/power-up-your-skills-and-credentials-at-nvidia-gtc-2023/
Last August, I wrote a post about GTC that asked, ‘What if you could spend 8 hours with an AI legend while getting hands-on experience using some of the most advanced GPU and DPU technology available?”  My point still stands: This is exactly why you should attend training at GTC. The virtual conference offers hands-on…Powered by Discourse, best viewed with JavaScript enabled"
572,oci-accelerates-hpc-ai-and-database-using-roce-and-nvidia-connectx,"Originally published at:			https://developer.nvidia.com/blog/oci-accelerates-hpc-ai-and-database-using-roce-and-nvidia-connectx/
Oracle Cluster Infrastructure uses an innovative approach to deliver scalable, RDMA-powered networking on Ethernet for a multitude of distributed workloads, providing higher performance and value.Powered by Discourse, best viewed with JavaScript enabled"
573,explainer-what-is-a-smart-city,"Originally published at:			What’s a Smart City? | NVIDIA Blogs
Examples of what a smart city is can be found in metro IoT deployments from Singapore to Seat Pleasant, Maryland.Powered by Discourse, best viewed with JavaScript enabled"
574,autodmp-optimizes-macro-placement-for-chip-design-with-ai-and-gpus,"Originally published at:			https://developer.nvidia.com/blog/autodmp-optimizes-macro-placement-for-chip-design-with-ai-and-gpus/
Macro placement has a tremendous impact on the landscape of the chip, directly affecting many design metrics, such as area and power consumption.Powered by Discourse, best viewed with JavaScript enabled"
575,how-will-machine-learning-change-ecommerce,"Originally published at:			How Will Machine Learning Change ECommerce? | NVIDIA Technical Blog
Sentient Technologies is using machine learning and GPUs to create a Sales Associate with artificial intelligence for online stores. The company recently announced a collaboration with Shoes.com that uses a ‘Visual Filter’ and machine learning to make it the world’s first artificial intelligence-powered shopping experience. The new technology called “Sentient Aware” is being deployed on…Powered by Discourse, best viewed with JavaScript enabled"
576,automating-data-center-networks-with-nvidia-cumulus-linux,"Originally published at:			https://developer.nvidia.com/blog/automating-data-center-networks-with-nvidia-cumulus-linux/
With evolving and ever-growing data centers, the days of simple networks that remained mostly unchanged are gone. Back then, when a configuration change was needed, it was simple for the network administrator to make the changes device per device, line-by-line. As data centers evolve from physical on-premises to digitized cloud infrastructures, the traditional networks have…Powered by Discourse, best viewed with JavaScript enabled"
577,deploying-a-1-3b-gpt-3-model-with-nvidia-nemo-megatron,"Originally published at:			https://developer.nvidia.com/blog/deploying-a-1-3b-gpt-3-model-with-nvidia-nemo-megatron/
Large language models (LLMs) are some of the most advanced deep learning algorithms that are capable of understanding written language. Many modern LLMs are built using the transformer network introduced by Google in 2017 in the Attention Is All You Need research paper. NVIDIA NeMo Megatron is an end-to-end GPU-accelerated framework for training and deploying…I loved deploying NeMo Megatron locally to power language-based applications and look forward to seeing exciting new ways to use LLMs. Let me know if there are any questions and I will be happy to help!Hey that was a great article! It worked well (20b param) but I’m having no luck in changing the temperature. Tried changing it where it was defined and no luck, tried adding it to the argparser, and still no luck… what am I missing?!? and still no luck… what am I missing?!? and still no luck… what am I missing?!? and still nojkjk thank you for your patience and maybe your guidance :DThanks for the excellent tutorial!  Everything worked well.    I did have one question about the tokenizer.   Why is the tokenizer GPT2 even though the model is GPT3 ?Powered by Discourse, best viewed with JavaScript enabled"
578,long-read-sequencing-workflows-and-higher-throughputs-in-nvidia-parabricks-4-1,"Originally published at:			https://developer.nvidia.com/blog/long-read-sequencing-workflows-and-higher-throughputs-in-nvidia-parabricks-4-1/
The upcoming 4.1 release of NVIDIA Parabricks, a suite of accelerated genomic analysis applications, goes further than ever before in accelerating sequencing alignment and increasing the accuracy of deep learning variant calling. The release includes a new workflow for PacBio long-read data, featuring an accelerated Minimap2 tool and Google’s DeepVariant for full GPU-enabled, end-to-end analysis…Powered by Discourse, best viewed with JavaScript enabled"
579,lower-end-hardware,"What ongoing work is NVIDIA looking into to make path tracing possible on lower spec hardware?Improvements to path tracing generally fall under ‘hardware’ or ‘software’ categories. Future hardware innovations won’t help improve the performance of existing hardware. But software (algorithmic) improvements generally make things work better across all hardware - including existing hardware and lower spec hardware.Powered by Discourse, best viewed with JavaScript enabled"
580,offline-to-online-feature-storage-for-real-time-recommendation-systems-with-nvidia-merlin,"Originally published at:			https://developer.nvidia.com/blog/offline-to-online-feature-storage-for-real-time-recommendation-systems-with-nvidia-merlin/
Recommendation models have progressed rapidly in recent years due to advances in deep learning and the use of vector embeddings. The growing complexity of these models demands robust systems to support them, which can be challenging to deploy and maintain in production. In the paper Monolith: Real Time Recommendation System With Collisionless Embedding Table, ByteDance…Powered by Discourse, best viewed with JavaScript enabled"
581,create-high-quality-computer-vision-applications-with-superb-ai-suite-and-nvidia-tao-toolkit,"Originally published at:			https://developer.nvidia.com/blog/create-high-quality-computer-vision-applications-with-superb-ai-suite-and-nvidia-tao-toolkit/
Superb AI has introduced a revolutionary way for computer vision teams to drastically decrease the time it takes to deliver high-quality training datasets.Powered by Discourse, best viewed with JavaScript enabled"
582,upcoming-event-get-to-know-nvidia-jetson-ecosystem-partners-at-embedded-world,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-get-to-know-nvidia-jetson-ecosystem-partners-at-embedded-world/
Check out object detection solutions, 360° camera views, and more from NVIDIA Jetson ecosystem partners this week at Embedded World in Germany.Powered by Discourse, best viewed with JavaScript enabled"
583,runtime-decisions,"Does cuGraph make any runtime decisions based on various graph/input properties to determine the best parallelization strategy for tackling a certain task, or is it up to the user to determine this?Yes, mainly we consider average vertex degree and the number of GPUs to decide whether to cache edge source/destination property values in a contiguous array or (key, value) pairs. More detailed answer is avaliable in Analyzing Multi-trillion Edge Graphs on Large GPU Clusters: A Case Study with PageRank | IEEE Conference Publication | IEEE Xplore If I just briefly outline the key idea, with 2D partitioning and assuming V vertices and P GPUs, the range of edge source/destination values scales V/sqrt(P). Say we have E edges, the number of edges per partition is E/P. And we access E/P or fewer source/destination property values. If E/P is smaller than V/sqrt(P), storing edge source/destination property values in (key, value) pairs saves memory. Otherwise, storing in a contiguous memory is more efficient in both space & time. We also make concurrency/memory footprint trade-offs in multiple places.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
584,nvidia-certified-next-generation-computing-platforms-for-ai-video-and-data-analytics-performance,"Originally published at:			https://developer.nvidia.com/blog/nvidia-certified-next-generation-computing-platforms-for-ai-video-and-data-analytics-performance/
A new generation of computing technologies designed to address increasingly complex compute demands is emerging.Powered by Discourse, best viewed with JavaScript enabled"
585,end-to-end-ai-for-workstation-onnx-runtime-and-optimization,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-workstation-onnx-runtime-and-optimization/
This post is the third in a series about optimizing end-to-end AI for workstations. For more information, see part 1, End-to-End AI for Workstation: An Introduction, and part 2, End-to-End AI for Workstation: Transitioning AI Models with ONNX. When your model has been converted to the ONNX format, there are several ways to deploy it,…Powered by Discourse, best viewed with JavaScript enabled"
586,leading-mlperf-training-2-1-with-full-stack-optimizations-for-ai,"Originally published at:			https://developer.nvidia.com/blog/leading-mlperf-training-2-1-with-full-stack-optimizations-for-ai/
As AI becomes increasingly capable and pervasive, MLPerf benchmarks, developed by MLCommons, have emerged as an invaluable tool for organizations to evaluate the performance of AI infrastructure across a wide range of popular AI-based workloads. MLPerf Training v2.1—the seventh iteration of this AI training-focused benchmark suite—tested performance across a breadth of popular AI use cases,…Powered by Discourse, best viewed with JavaScript enabled"
587,new-courses-for-building-metaverse-tools-on-nvidia-omniverse,"Originally published at:			https://developer.nvidia.com/blog/new-courses-for-building-metaverse-tools-in-omniverse/
Check out three new NVIDIA Omniverse self-paced, hands-on courses for developers and technical artists who build tools to create virtual worlds.Powered by Discourse, best viewed with JavaScript enabled"
588,nvidia-enables-trustworthy-safe-and-secure-large-language-model-conversational-systems,"Originally published at:			https://developer.nvidia.com/blog/nvidia-enables-trustworthy-safe-and-secure-large-language-model-conversational-systems/
Large language models (LLMs) are incredibly powerful and capable of answering complex questions, performing feats of creative writing, developing, debugging source code, and so much more. You can build incredibly sophisticated LLM applications by connecting them to external tools, for example reading data from a real-time source, or enabling an LLM to decide what action…Powered by Discourse, best viewed with JavaScript enabled"
589,from-neuroscience-to-data-science-my-road-into-cybersecurity,"Originally published at:			https://developer.nvidia.com/blog/from-neuroscience-to-data-science-my-road-into-cybersecurity/
If you asked a group of cybersecurity professionals how they got into the field, you might be surprised by the answers that you receive. With military officers, program managers, technical writers, and IT practitioners, their backgrounds are varied. There is no single path into a cybersecurity career, let alone one that incorporates both cybersecurity and…Powered by Discourse, best viewed with JavaScript enabled"
590,choosing-nvidia-spectrum-for-microsoft-azure-sonic,"Originally published at:			https://developer.nvidia.com/blog/choosing-spectrum-for-microsoft-azure-sonic/
NVIDIA supports open Ethernet and contributes innovations to the SONiC developer community project.Powered by Discourse, best viewed with JavaScript enabled"
591,predict-protein-structures-and-properties-with-biomolecular-large-language-models,"Originally published at:			https://developer.nvidia.com/blog/predict-protein-structures-and-properties-with-biomolecular-large-language-models/
The NVIDIA BioNeMo service is now available for early access. With the BioNeMo service, scientists and researchers now have access to pretrained biomolecular LLMs through a cloud API.Powered by Discourse, best viewed with JavaScript enabled"
592,end-to-end-ai-for-nvidia-based-pcs-cuda-and-tensorrt-execution-providers-in-onnx-runtime,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-nvidia-based-pcs-cuda-and-tensorrt-execution-providers-in-onnx-runtime/
This post is the fourth in a series about optimizing end-to-end AI. The last post described the higher-level idea behind ONNX and ONNX Runtime. As explained in the previous post in the End-to-End AI for NVIDIA-Based PCs series, there are multiple execution providers (EPs) in ONNX Runtime that enable the use of hardware-specific features or optimizations…Thanks for the great blog post. Assuming a previously generated TRT engine, will ONNX with a TensorRT EP achieve the same runtime performance as running the engine directly through TensorRT APIs? In other words, is there any performance penalty to use TensorRT through ONNX runtime?If your engine is not split up by ONNX Runtime the performance should be the same. Essentially if an ONNX file is not able to compile to a single engine ONNXRuntime will slice up the network and fallback to CUDA Execution provider for unsupported ops.
There are a few things to watch out for:Powered by Discourse, best viewed with JavaScript enabled"
593,accelerate-your-edge-ai-journey-with-nvidia-igx-orin-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/accelerate-your-edge-ai-journey-with-nvidia-igx-orin-developer-kit/
NVIDIA IGX Orin is the first platform to combine industrial-grade hardware with enterprise-level software and support for edge AI management.Powered by Discourse, best viewed with JavaScript enabled"
594,designing-an-optimal-ai-inference-pipeline-for-autonomous-driving,"Originally published at:			https://developer.nvidia.com/blog/designing-an-optimal-ai-inference-pipeline-for-autonomous-driving/
Electric vehicle manufacturer NIO optimized its AI inference pipeline with NVIDIA Triton on GPUs.Powered by Discourse, best viewed with JavaScript enabled"
595,webinar-accelerate-ai-model-inference-at-scale-for-financial-services,"Originally published at:			Accelerate AI Model Inference at Scale for Financial Services
Learn how AI is transforming financial services across use cases such as fraud detection, risk prediction models, contact centers, and more.Powered by Discourse, best viewed with JavaScript enabled"
596,webinar-performant-multiphase-flow-simulation-at-leadership-class-scale,"Originally published at:			Performant Multiphase Flow Simulation at Leadership-Class Scale via OpenACC
On June 6, learn how researchers use OpenACC for GPU acceleration of multiphase and compressible flow solvers that obtain speedups at scale.Powered by Discourse, best viewed with JavaScript enabled"
597,accelerating-digital-pathology-workflows-using-cucim-and-nvidia-gpudirect-storage,"Originally published at:			https://developer.nvidia.com/blog/accelerating-digital-pathology-workflows-using-cucim-and-nvidia-gpudirect-storage/
Learn how GPU-accelerated toolkits improve the input/output performance and image processing tasks for digital pathology workflows.Use of AI in Whole Slide Imaging (WSI) applications is growing. Please share what I/O or computational bottlenecks that you encounter in your use cases for digital pathology.Powered by Discourse, best viewed with JavaScript enabled"
598,sharpen-your-edge-ai-and-robotics-skills-with-the-nvidia-jetson-nano-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/sharpen-your-edge-ai-and-robotics-skills-with-the-nvidia-jetson-nano-developer-kit/
Are you interested in getting started with edge AI and robotics but not sure where to begin?  Look at the relaunched NVIDIA Jetson Nano Developer Kit available for purchase from partners starting November 25, 2022 in the US and worldwide in December. Introduced 3 years ago, the NVIDIA Jetson Nano is a low-cost, entry-level AI…is it the same as the jetson nano released a few years ago on the hardware? I thought it’s EOL.it’s unclear about the software support though, are we stuck with ubuntu 18.04 for good with jetson nano?Does this post mean that we can expect JetPack updates to continue to support the Jetson Nano?  You really should do that, it is an excellent platform for makers and not doing so will make obsolete the equipment we purchased.To partially answer my own question:
Jetson Nano 2GB is EOL.
Jetson Nano 4GB is revived and we can continue to use.
But the software will be stuck with Jetpack4 and ubuntu 18.04(which will be EOL in 4 months).Jetson Nano Developer Kit V3 has two 15-pin 2-lane MIPI CSI-2 camera connectors vs. one on old Jetson Nano DK.Wutt??I thought this was EOL??  I have a few of these and was dissapointed you EOL them after only a year after I bought it…Does this mean latest JetPack will be available for Nano 4GB??I probably spent too much money on a Jetson Xavier due to the EOL on the Nano.  If you’re now going to keep this great little module alive, then please get Jetpack up to 5.0.Please, please, please support the Nano! Nano & its bus(es) are for AI what RPi is for general computing. Thank you for leading the way, but it is very DEC-ish (Digital Equipment Company) of you to redefine the Nano bus(es), Nano eXtension bus(es). Now Orin Nano (thank you) & what should be called Orin eXtension (OX)? All of these buses make my hardware obsolete!?! The Orin NX is a physical standard and not a bus standard? Really? You are telling me that my credit card is NX compatible!! It is almost as NX compatible after all. I must acknowledge bus fluttering, though it is exclusionary and appears reactionary, has specific purpose. Please stabilize eXtensions for Entry Levels and us Life Long Learners, including buses and software updates (Cuda, JetPack, Ubuntu). SoMs have no value without a carrier board, a big effort for a one off. My frustration stems from planning to populate my Turing Pi 2s, only to find more power planned away from me and years away at that (TPi3 . . ?).We do owe thanks to DEC for motivating Thomson, Ritchie & Kernighan. In turn Linus was motivated. We do not know where the next Linus, Jenson . . . will come from. Notably, NVIDIA is not on Luxonis RAE, special sarcasm in their video as RAE runs circles around a CORAL box.0:48:12 to 1:01:40?@jwitsoe - any comment on your Jetson Nano “Re-Launch” announcement and questions on Nano support for latest Jetpack?Or was NVidia just looking to offload old stock and does not intend to support it?Hi everyone, Nano will continue to be supported with updates to JetPack 4.6 - please refer to Jetson Software Roadmap for 2H-2021 and 2022 for more info.   The latest update was released earlier this month (JetPack 4.6.3/L4T 32.7.3 released).  JetPack 5 is for Xavier and Orin only and there aren’t plans to backport it to Nano/TX1/TX2.Nano 4GB wasn’t EOL and thanks for your patience while it was out of stock due to supply-chain issues (for EOL announcements see the Jetson Product Lifecycle page).  Please direct further questions to the Nano forum where they will be more visible to the Jetson team so that we can reply in a more timely manner.Powered by Discourse, best viewed with JavaScript enabled"
599,accelerated-data-analytics-a-guide-to-data-visualization-with-rapids,"Originally published at:			https://developer.nvidia.com/blog/accelerated-data-analytics-a-guide-to-data-visualization-with-rapids/
Learn how to use RAPIDS to integrate powerful visualizations into your workflows.Powered by Discourse, best viewed with JavaScript enabled"
600,new-on-ngc-sdks-for-large-language-models-digital-twins-digital-biology-and-more,"Originally published at:			https://developer.nvidia.com/blog/new-on-ngc-sdks-for-llms-metaverse-computer-vision-and-more/
NVIDIA announces new SDKs available in the NGC catalog, a hub of GPU-optimized deep learning, machine learning, and HPC applications. With highly performant software containers, pretrained models, industry-specific SDKs, and Jupyter notebooks available, AI developers and data scientists can simplify and reduce complexities in their end-to-end workflows. This post provides an overview of new and…Powered by Discourse, best viewed with JavaScript enabled"
601,zero-to-data-science-making-data-science-teams-productive-with-kubernetes-and-rapids,"Originally published at:			Zero to Data Science: Making Data Science Teams Productive with Kubernetes and RAPIDS | NVIDIA Technical Blog
  Data collected on a vast scale has fundamentally changed the way organizations do business, driving demand for teams to provide meaningful data science, machine learning, and deep learning-based business insights quickly. Data science leaders, plus the Dev Ops and IT teams supporting them, constantly look for ways to make their teams productive while optimizing their costs…Powered by Discourse, best viewed with JavaScript enabled"
602,accelerating-python-on-gpus-with-nvc-and-cython,"Originally published at:			https://developer.nvidia.com/blog/accelerating-python-on-gpus-with-nvc-and-cython/
The C++ standard library contains a rich collection of containers, iterators, and algorithms that can be composed to produce elegant solutions to complex problems. Most importantly, they are fast, making C++ an attractive choice for writing highly performant code. NVIDIA recently introduced stdpar: a way to automatically accelerate the execution of C++ standard library algorithms…Nice post – it’s great to see access to GPU from Python/Cython.I’m trying to use the jacobi_solver example in the post as a starting point to write a GPU-accelerated function to perform convolution using the for_each algorithm from standard C++ library.I don’t want to reinvent the wheel and was wondering if basic convolution of a kernel with a 2D array (e.g., using two 1D convolutions)  might already be implemented on GPU in a way similar to the example given in the jacobi_solver.Thanks!Thanks for your message!What you’re proposing sounds very much similar to the Jacobi example. You would need to write a functor (similar to avg) that encapsulates your kernel.In fact, if I’m understanding correctly, couldn’t the Jacobi solver be thought of as repeatedly applying the following kernel?@ashwint -  Thanks for the reply – that clarifies it a lot.In trying to write the code, I’ve noticed  NVIDIA HPC SDK doesn’t appear to be available for Windows yet.  Any idea when it will reach Windows?Thanks!Hi! Thanks again for your interest. We plan to have Windows support for the HPC SDK later this year.Hi @ashwint, thanks a lot for this great post! I have a quick question regarding the Figure showing the speedup over numpy sort: why is serial CPU processing doing better at smaller sample sizes? I understand that GPU parallel processing capacities are not enfolding their power at small sample sizes - but what creates the overhead?Thanks, @boehmvanessa. Likely, it’s the cost of transferring data from the host (GPU) to the device (GPU) and back.stdpar:Hi, thanks for the article!
I’m facing following task: I have cuda code in a .cu file which relies on the cublas library, a wrapper.pyx file and a setup.py which is based on this repository. Unfortunately, I don’t have any knowledge about setuptools or creating modules in general.
I want to create a python module, however I do not know how to link my cuda code with the cublas library in this case.
Could you give me a hint how to build a module from the .pyx and the .cu file which includes the cublas funtionalities? Manual build is also fine.
Thanks for your help!Hi - the setup.py file you linked to should largely work. I think you would need to add the cublas to the library_dirs, libraries, runtime_library_dirs, and  include_dirs arguments to the Extension constructor on line 112.Thanks Ashwin for this nice article.
I am trying to run code from GitHub - anuga-community/anuga_core: ANUGA for the simulation of the shallow water equation which is having compute intensive code written in C and using cython to create extension for Python.
The memory allocation is happening through numpy.
We also have an openmp version of compute intensive code running on CPU.The challenge im seeing is about memory allocation on GPU.
Can you give us some suggestion with latest trends which will simplify memory management?Cheers,
SamirPowered by Discourse, best viewed with JavaScript enabled"
603,practical-tips-for-optimizing-ray-tracing,"Originally published at:			https://developer.nvidia.com/blog/practical-tips-for-optimizing-ray-tracing/
To achieve high efficiency with ray tracing, you must build a pipeline that scales well at every stage. This starts from mesh instance selection and their data processing towards optimized tracing and shading of every hit that you encounter. Instance data generation In a common scene, there can be far more static than dynamic objects.…Powered by Discourse, best viewed with JavaScript enabled"
604,an-it-manager-s-guide-to-deploying-an-edge-ai-solution,"Originally published at:			https://developer.nvidia.com/blog/an-it-managers-guide-to-deploying-an-edge-ai-solution/
Check out these recommendations for how to successfully deploy an edge AI solution for your organization.Powered by Discourse, best viewed with JavaScript enabled"
605,research-unveils-breakthrough-deep-learning-tool-for-understanding-neural-activity-and-movement-control,"Originally published at:			Research Unveils Breakthrough Deep Learning Tool for Understanding Neural Activity and Movement Control | NVIDIA Technical Blog
A primary goal in the field of neuroscience is understanding how the brain controls movement. By improving pose estimation, neurobiologists can more precisely quantify natural movement and in turn, better understand the neural activity that drives it. This enhances scientists’ ability to characterize animal intelligence, social interaction, and health.  Columbia University researchers recently developed a…There is nothing more rewarding than seeing how helping one of many DALI users can lead to close cooperation between open-source projects resulting in breakthroughs in neuroscience.
If you have any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
606,run-state-of-the-art-nlp-workloads-at-scale-with-rapids-huggingface-and-dask,"Originally published at:			https://developer.nvidia.com/blog/run-state-of-the-art-nlp-workloads-at-scale-with-rapids-huggingface-and-dask/
This post was originally published on the RAPIDS AI Blog. TLDR: Learn how to use RAPIDS, HuggingFace, and Dask for high-performance NLP. See how to build end-to-end NLP pipelines in a fast and scalable way on GPUs. This covers feature engineering, deep learning inference, and post-inference processing. Introduction Modern natural language processing (NLP) mixes modeling,…I think link to the workflow and code is brokenThanks for flagging! You can check out the workflow code on the rapidsai/gpu-bdb GitHub repo.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
607,graph-characteristics-taxonomy,"Your customers surely ask you for ALL KINDS of input graphs. Can you give a rough taxonomy of the kind of graphs you actually see in practice? For instance, I would say that one category is social-network graphs that are scale-free-ish, where there’s a wide range of input degrees and a wide variance, and then another category might be road-network graphs that are planar, have a small degree and very little variance in degree. How do you characterize the kind of graphs you see?We work with a wide range of customer, which means that we see a wide range of graph types. The vast majority could be classified as power-law graph, both with and without
properties. But we also see a lot of bipartite and N-partite graphs. Graphs that look like road networks, high diameter with low average degree. We are also seeing an increase in the number
of multi-graphs and have had to augment our data structure to support edge ids.I have not taken the time to count the various graph types, but that is a great
thing to do.Oh, cool, that’s very helpful. At a high level can you talk about the use cases you see for bi/N-partite graphs and multi-graphs? Like, how are people looking at them?Do you specialize the data structure for partite graphs?Can you expand on “augment our data structure to support edge ids” for multi-graphs? Are you extending COO/CSR and/or is this in your blocked data structure?Bipartite graphs come up a lot in retail and cyber.  Within retail it is a mapping of customer to products (really n-partite of customers to stores to product types to product
items, etc…).   In cyber it typically is computers outside the firewall and computers inside the firewall.    For Multigraphs, this comes up in retail and finance
where there are lot of transactions (edges) happening every minute/hour/day.
At the C/CUDA layer, we only have a single structure.  AT the Python layer we try and add additional
functions to help users interact with the various types of graphs.Related to augmenting the CSR.  We ran into the problem that if we  ran some
type of sampling or pathfinding that returned an edge, we were unable to determine
which edge was selected.  We expanded from a simple weight value in the CSR to supporting a tuple so that an edge id
could be passed through (also support passing through the edge types)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
608,hands-on-lab-learn-to-build-digital-twins-for-free-with-nvidia-modulus,"Originally published at:			Physics-Informed Machine Learning with Modulus | NVIDIA
NVIDIA Modulus is now available on NVIDIA LaunchPad. Sign-up for a free, hands-on lab that will teach you how to develop physics-informed machine-learning solutions.Powered by Discourse, best viewed with JavaScript enabled"
609,upcoming-workshop-applications-of-ai-for-anomaly-detection-emea,"Originally published at:			Personal Information - Applications of AI for Anomaly Detection (EMEA)
Learn to detect data abnormalities before they impact your business by using XGBoost, autoencoders, and GANs.Powered by Discourse, best viewed with JavaScript enabled"
610,what-short-cuts-are-needed-for-real-time-path-tracing,"The movie industry uses render farms for this level of realism, so you must cut corners to make real-time path tracing a thing, right?Well you might consider denoising to be a short-cut - but it’s not really. It allows you to extract a much better result from a poor signal. Users of offline path tracing, like the film industry, will still use denoising, but typically to a lesser degree.To extract the most performance, real time path tracing will typically use fewer bounces than offline path tracing too. As with denoising, other parts of the pipeline (like ReSTIR DI and ReSTIR GI from RTXDI) can also rely on temporal reuse of data generated in previous frames.
Likewise, we can use lower precision math, simplified materials and skip a lot of computation necessary for subtle detail that is less noticeable in real-time.Powered by Discourse, best viewed with JavaScript enabled"
611,safeguarding-networks-and-assets-with-digital-fingerprinting,"Originally published at:			https://developer.nvidia.com/blog/safeguarding-networks-and-assets-with-digital-fingerprinting/
Use of stolen or compromised credentials remains at the top of the list as the most common cause of a data breach. Because an attacker is using credentials or passwords to compromise an organization’s network, they can bypass traditional security measures designed to keep adversaries out. When they’re inside the network, attackers can move laterally…Powered by Discourse, best viewed with JavaScript enabled"
612,real-time-multiple-moving-objects-tracking-for-video-surveillance,"Originally published at:			https://developer.nvidia.com/blog/real-time-multiple-moving-objects-tracking-for-video-surveillance/
Tracking moving objects in the real-time is a complex. A new paper proposes a multi-object visual color tracking algorithm using multi-threading in real-time using GPUs. A professor from The British University in Egypt presents the integration of a proposed enhanced multi-object color tracking, Partitioned Region Matching (PRM), and Spatial Region Graph (adjacency graph) for real-time…How are you? Thanks for your post. Can you share the presentation of this solution proposal?
My email is xt@go2future.com
Best regards@xt11 – This wasn’t an NVIDIA proposal, but a research paper. Try contacting the researchers…Powered by Discourse, best viewed with JavaScript enabled"
613,power-the-next-wave-of-applications-with-nvidia-bluefield-3-dpus,"Originally published at:			https://developer.nvidia.com/blog/power-the-next-wave-of-applications-with-nvidia-bluefield-3-dpus/
​NVIDIA BlueField-3 DPUs transform traditional computing environments into efficient, high-performance, secure, and sustainable data centers, enabling the delivery of the next wave of applications.Powered by Discourse, best viewed with JavaScript enabled"
614,increasing-inference-acceleration-of-kogpt-with-nvidia-fastertransformer,"Originally published at:			https://developer.nvidia.com/blog/increasing-inference-acceleration-of-kogpt-with-fastertransformer/
Transformers are one of the most influential AI model architectures today and are shaping the direction of future AI R&D. First invented as a tool for natural language processing (NLP), transformers are now used in almost every AI task, including computer vision, automatic speech recognition, molecular structure classification, and financial data processing. In Korea, KakaoBrain…Powered by Discourse, best viewed with JavaScript enabled"
615,how-to-successfully-integrate-nvidia-dlss-3,"Originally published at:			https://developer.nvidia.com/blog/how-to-successfully-integrate-dlss-3/
NVIDIA DLSS Frame Generation is the new performance multiplier in DLSS 3 that uses AI to create entirely new frames. This breakthrough has made real-time path tracing—the next frontier in video game graphics—possible. NVIDIA has made it easier for you to take full advantage of this technology with the release of the Unreal Engine 5.2…Powered by Discourse, best viewed with JavaScript enabled"
616,recreate-high-fidelity-digital-twins-with-neural-kernel-surface-reconstruction,"Originally published at:			https://developer.nvidia.com/blog/recreate-high-fidelity-digital-twins-with-neural-kernel-surface-reconstruction/
Neural Kernel Surface Reconstruction (NKSR) is the new NVIDIA algorithm for reconstructing high-fidelity surfaces from large point clouds.Hi, I can’t turn points into objects, can I get some advice from you?Powered by Discourse, best viewed with JavaScript enabled"
617,now-available-nvidia-dlss-3-for-unreal-engine-5,"Originally published at:			https://developer.nvidia.com/blog/now-available-nvidia-dlss-3-for-unreal-engine-5/
NVIDIA DLSS 3 is a neural graphics technology that multiplies performance using AI image reconstruction and frame generation. It’s a combination of three core innovations: Super Resolution uses deep learning algorithms to upscale a lower-resolution input into a higher-resolution output, creating a sharp image with a boosted frame rate. Frame Generation uses AI rendering to…great enhancement! Is Linux support/compiling planned?FANTASTIC!  Is frame generation available on 30 series cards or 40 only?Powered by Discourse, best viewed with JavaScript enabled"
618,faster-hdbscan-soft-clustering-with-rapids-cuml,"Originally published at:			https://developer.nvidia.com/blog/faster-hdbscan-soft-clustering-with-rapids-cuml/
Discover the importance of using soft clustering to better capture nuance in downstream analysis and the performance gains possible with RAPIDS.Powered by Discourse, best viewed with JavaScript enabled"
619,unified-memory-for-cuda-beginners,"Originally published at:			https://developer.nvidia.com/blog/unified-memory-cuda-beginners/
My previous introductory post, “An Even Easier Introduction to CUDA C++“, introduced the basics of CUDA programming by showing how to write a simple program that allocated two arrays of numbers in memory accessible to the GPU and then added them together on the GPU. To do this, I introduced you to Unified Memory, which…What happens if I call cudaMemPrefetchAsync() on an array that is too big to fit in one piece on the GPU?  This might occur when oversubscribing.Hallo Mark Harris, please excuse my english i'm not a native speaker.When i understand your footnote 3) correctly. Then the unifiy memory feature with paging is not available under Windows. It is the first time i read this restriction. We checked every documentation online there whre nothing to find about a limitation to Linux 64 bit.I'm now really diasapointed becaus since Pascal was available we buyed for our work 4 Pascal cards especially beacuse of this feature and have registrated a fallback to zero copy memory using Pascal Cards with unified memory. We thought it would be a bug, like reported in the NVIDIA Developer Forum for example the last report: https://devtalk.nvidia.com/...We can't switch to Linux because we use special depth-image camera with only Windows driver available.So should we switch back to Cuda 7.5 where Pascal works like Maxwell missing the other Cuda 8.0 features  or should we sell the Pascal cards and buy old maxwell cards? Will this be fixed with cuda 9 or will be there possiblie a fix in the next months for cuda 8.0? Or is there at least any possibility to fall back to normal unified memory behaviour without paging  (would at least help for low resolutions to develop our framework in the hope that paging will work some day)?Would be nice if you could help use we really like and would need the unified programming and the paging feature.In this case only the last part of the array (as much as will fit) ) will end up in the GPU memory and the rest will be resident in the system memory. Currently the driver does not know exactly how much memory is available for eviction on the destination processor and therefore some pages could be fetched and then evicted during the prefetching operation. If this is a major performance issue for your application, please let us know more about your use case and we can follow up to improve the driver’s migration policy for oversubscription cases.I don't have a problem *yet*.  As long as the behaviour is predictable, we should be able to write our kernels such that performance degrades gracefully in the presence of oversubscribing.  However, I'm worried about what happens if oversubscription always leaves the *last* part of the array on device memory, if thread block scheduling tends to execute the *first* thread blocks first (assuming not all thread blocks fit simultaneously).  In the common case, where the first thread blocks read the first parts of the array, this would result in many page faults.It would probably help if there were some common behaviour between oversubscription and thread block scheduling.Fallback to system memory on Pascal under windows is due to a bug in CUDA 8 that should be fixed in the CUDA 9 release. So no, you should not downgrade to Maxwell. Windows support is tricky -- it requires support from Microsoft, so communicating your needs to them will help the cause.Wait seriously? How long has this bug been known? We bought a ton of servers with Pascal cards about a year ago. It took many months before we finally realized that the problem was most likely with the cards themselves, and the only solution was to use unmanaged memory. To make this work, I had to abandon a huge amount of progress that would have allowed us to iterate and experiment *very* easily, and spend a huge amount of time just making single-use code for each different scenario we needed to run as a workaround, meanwhile our tiny cash-strapped company had to pay tens of thousands of dollars in penalties while we struggled to get something usable.Maybe this issue should have been communicated somewhere among all of the hype about how much Pascal was supposed to make things *better*. And by ""this issue"" I mean ""Pascal can't use managed memory on Windows"". Seems to be worth noting.Sorry, I'm very frustrated with Pascal, and it's probably coming out pretty strongly.nvidia have a habit of releasing features half baked. it is very frustrating and causes distrust towards investment. Nvidia engineers should stand up to their marketing department for the sake of the long term success of their company.To answer your question, David, about how long this bug has been known, i can tell you that I reported it to NVIDIA at 25.01.2017. With updates to it 3.2.2017 and 15.03.2017. In my last updated I was also very harsh and threatened to change to AMD and give the 1080 to an other group in our departmend which only works with LINUX. Now the windows peding frameworks of our group a redesigned to AMD. And for furtther PASCAL cards there is an order restriction till this bug is solved. And we will never again let our frameworks be depended to NVIDIA or AMD only.At the end for use it would be enough if a pascal card would till end of this year behave under CUDA 8.0 like under 7.5 without the paging feature. Or if it would be commuicated and we wouldn't order PASCAL cards and give our MAXWELL cards to an other group. At least we could get some back.I'm sorry that i don't reported the bug directly as Cuda 8 was available and the bug hited me at the begin of October 2016. And that i didn't posted it every where in the world and sended the PASCAL cards back. We had a lot of work and concentrated on somthing else because the performance in this moment was not so important and we still had enough MAXWELL cards. And I really belived that me and my colleagues made somthing wrong after we couldn't find any answer in any forum.What are the plans for supporting Concurrent Managed Access for Pascal GPUs (I recently invested in a Titan Xp) on Windows 10?Hi there, I tried Cuda 9 RC on Windows, but still not able to use cudaMemPrefetchAsync since Concurrent Managed Access is reporting as false (1080Ti)...so, is this bug fixed?I noticed the same behaviour on my GTX 1080 Ti also using the saxpy example and kept tinkering before finding this post.  I noticed that, with 1M elements as in the original saxpy post, my bandwidth never went beyond 60GB/s, quite a bit shy of the 484GB/s that the 1080 Ti is nominally capable of.  Once I increased the size of the arrays to 1G elements, I achieved a much more respectable 390GB/s without making any changes to the code.  So, two questions: (1) Given that the saxpy code does not use cudaMallocManaged, it seems that the unified memory model also affects traditional code that uses cudaMemcpy.  Can you elaborate on why that is? (2) Why does increasing the input size lead to reasonable bandwidth even though the page misses should still be there?  Does some hardware prefetching come into play?  If so, why doesn't it kick in for smaller but still substantial sizes such as 1M elements?  Thanks.Thanks for the tutorial.  I am curious about one thing: pointer arithematic of unified memory. After ""cudaMallocManaged(&ptr, 1024x1024x1024)"", if I call ""ptr += 1024"" to increase the pointer, will the ptr still be recognized as a unified memory?Dear sr. would you mind to tell me your OS and if your GTX 1080ti shows  cudaDevAttrConcurrentManagedAccess = 1?regardsMark Harris, thank you for this tutorial series. I've tested your code in my laptop with nVidia MX150 videocard (under Linux), which has a Pascal architecture, and here are the results:add_cuda.cu - 183.55msadd_block.cu - 7.9120msadd_grid.cu - 6.0973msadd_grid_pascal.cu - 289.74usCould you please comment on the differences compared to your laptop? Maybe there are some other tricks to improve the end-result? Honestly, I was hoping for a higher speed-up. Thank you in advance!I'm not sure what the difference between add_grid and add_grid_pascal is. However, your add_grid_pascal is achieving about 43 GB/s. I looked up the MX150 and peak bandwidth is about 48 GB/s so you are getting 90% of peak.  My Pascal tests were not on a laptop, they used a server with a Tesla P100 which has MUCH higher bandwidth.Thanks for your prompt and detailed reply Mark. The file add_grid_pascal.cu is the same add_grid.cu with just your 3rd solution of Prefetching applied. Regarding to getting the 90% of peak of my video-card, is it somewhat close to optimal or it's supposed to be much higher/lower? I've looked for the answer in your page about ""How to Implement Performance Metrics in CUDA C/C++"" (https://devblogs.nvidia.com... but there's no estimate about how close those numbers should or may be. There, in your example you have a ratio of 110.37/148 for the effective and theoretical bandwidths respectively, which is 74.57% only. I suppose this efficiency may also be affected by some physical factors varying from hardware to hardware of the same model (kind of a luck?). Any comment about this aspect would be highly appreciated.The post you refer to hasn't been updated for more recent hardware. NVIDIA GPUs have improved the achievable utilization of bandwidth over time. I would say 90% of peak is pretty good! And yes, it's also affected by other factors, most importantly software / implementation factors such as memory access pattern and relative overhead compared to the runtime of the kernel. E.g. you will measure lower bandwidth for this kernel if you reduce N by a lot (due to kernel launch and other overheads) and you may see even higher bandwidth if you increase N.what is the difference between cudaMemcpy() & cudaPrefetchasync() ?I dont quite understandcudaMemcpy() copies memory from one allocated memory region to another, just like regular C memcpy(). cudaPrefetchAsync causes pages of an allocated memory region to be prefetched (in other words, populated if necessary and migrated) to the specified destination device. So for example if you allocate managed memory and initialize it on one device, but you know it will be accessed on another device later, you can prefetch it there using this method (optionally overlapping the prefetch with other CUDA work on a different stream, hence ""async""). See https://docs.nvidia.com/cud...Powered by Discourse, best viewed with JavaScript enabled"
620,accelerating-python-applications-with-cunumeric-and-legate,"Originally published at:			https://developer.nvidia.com/blog/accelerating-python-applications-with-cunumeric-and-legate/
cuNumeric is an endeavoring drop-in replacement for NumPy based on the Legion programming system.Whether cumumeric exports native BLAS API calls to be picked up in cython with cimport, the same way as numpy or scipy do (e.g. cython_blas.pxd) ?There are no plans to provide an interface at the level of the BLAS API. The BLAS API operates at the level of raw pointers and strides, and this is too low-level for cuNumeric to target effectively. cuNumeric aims to provide an implementation that can scale to multiple nodes, and thus may need to split the data across multiple address spaces, meaning that a single pointer would not be a good representation for an array. The NumPy level, which uses abstract array objects, is easier to transparently re-implement as a distributed library.We do provide an “entry point” into cuNumeric, where a cuNumeric array can be initialized from an existing memory buffer. Then you can use the NumPy API to operate on that array, and cuNumeric will take care of sharding the data, parallelizing the work etc. The final result at the end of this process can be “inline mapped”, which pulls all data to one place and provides it as a local buffer. This happens automatically if you try to do anything with a cuNumeric array that requires all the data to be in one place, e.g. printing it out.Note that it’s likely not optimal to be switching in and out of cuNumeric repeatedly, as every switch causes blocking and other overheads.Powered by Discourse, best viewed with JavaScript enabled"
621,five-tips-for-building-a-cybersecurity-career-in-the-age-of-ai,"Originally published at:			https://developer.nvidia.com/blog/five-tips-for-building-a-cybersecurity-career-in-the-age-of-ai/
Check out these five tips for pursuing a career in cybersecurity, whether you’re just starting out or are looking to make a mid-career change.Powered by Discourse, best viewed with JavaScript enabled"
622,unlocking-new-opportunities-with-ai-cloud-infrastructure-for-5g-vran,"Originally published at:			Unlocking New Opportunities with AI Cloud Infrastructure for 5G vRAN | NVIDIA Technical Blog
NVIDIA is enabling a new approach to host 5G vRAN and AI on the same cloud infrastructure to improve telecom operators’ operational efficiency and unlock new revenue opportunities.Powered by Discourse, best viewed with JavaScript enabled"
623,does-get3d-have-image-guided-shape-generation-feature,"Hey there!
in your press release (https://developer.nvidia.com/blog/rapidly-generate-3d-assets-for-virtual-worlds-with-generative-ai/) there is a phrase: “GET3D is a new generative AI model that generates 3D shapes with topology, rich geometric details, and textures from data like 2D images, text, and numbers.” - so, it seems GET3D can somehow  generate a 3D model from a picture… I mean something like that: at the Inference stage, GET3D takes a png image as a parameter, and in response generates not a bunch of random 3D models, but a 3D model similar to png picture. Is it possible to use GET3D this way, if yes - how exactly (it would be great to get any docs or advices)…
Vasil.Hi, Thanks for the questions! Due to time constraints, we are not working on image-guided shape generation for GET3D, we might explore that in the future.thanks for the answer!
is there any understanding when there will be a version with the ability to upload an image?We do not have a specific timeline for it but will be actively looking into thisFrom the point of view of technology, is it not possible to upload a picture and get a 3D model from it now? or is there such an option, but not available to us (developers)?Hi, we do not have this option at the moment, and we would encourage developers/researchers to try PTI inversion if having one image as input and wants to generate 3D shape from the image GitHub - danielroich/PTI: Official Implementation for ""Pivotal Tuning for Latent-based editing of Real Images"" (ACM TOG 2022) https://arxiv.org/abs/2106.05744This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
624,deploying-gpt-j-and-t5-with-fastertransformer-and-triton-inference-server,"Originally published at:			Deploying GPT-J and T5 with NVIDIA Triton Inference Server | NVIDIA Technical Blog
Learn step by step how to use the FasterTransformer library and Triton Inference Server to serve T5-3B and GPT-J 6B models in an optimal manner with tensor parallelism.hi @jwitsoe ,
I am from the Chinese developer community.There seems to be a picture mismatch in the results section of the article. Figure 5 should be T5-3B model inference speed-up comparison, but it shows GPT-J 6B.
微信截图_20220812082056729×567 54.1 KB
@kun.he.love.u Thanks for letting us know. We’ve updated the image.Can you please provide similar step by step guide for multi node inference example with Triton server?wow, I was waiting for such a guide for a long. Waiting for smth like that with Trition server as was asked above by aneesinaec.I tried similar exercise with a bloom model.
I have 2 GPU’s with ~10 GB Memory on each.  While trying to load a 14GB model in 2 GPU config, I keep getting out  of memory error.Fasttransformer backend supposed to split the 14 GB model in to 2 CPU’s and load…rite? What am i possibly missing here?Thank you for the feedback. We will consider it. You can also refer the document in fastertransformer_backend/t5_guide.md at main · triton-inference-server/fastertransformer_backend · GitHub first.Can you share more details and your steps?Powered by Discourse, best viewed with JavaScript enabled"
625,cuda-12-0-new-features-and-beyond-on-youtube-premiere,"Originally published at:			CUDA 12 New Features and Beyond - YouTube
Learn about the newest CUDA features such as release compatibility, dynamic parallelism, lazy module loading, and support for the new NVIDIA Hopper and NVIDIA Ada Lovelace GPU architectures.Powered by Discourse, best viewed with JavaScript enabled"
626,taking-ai-into-clinical-production-with-monai-deploy,"Originally published at:			https://developer.nvidia.com/blog/taking-ai-into-clinical-production-with-monai-deploy/
MONAI Deploy provides a set of open source tools for developing, packaging, testing, deploying, and running medical AI applications.Powered by Discourse, best viewed with JavaScript enabled"
627,fast-ai-assisted-annotation-and-transfer-learning-powered-by-the-clara-train-sdk,"Originally published at:			Fast AI Assisted Annotation and Transfer Learning Powered by the Clara Train SDK | NVIDIA Technical Blog
The growing volume of clinical data in medical imaging slows down identification and analysis of specific features in an image. This reduces the annotation speed at which radiologists and imaging technicians capture, screen, and diagnose patient data. The demand for artificial intelligence in medical image analysis has drastically grown in the last few years. AI-assisted…Hello! Actually, I am trying to reproduce the above blog at my end. I am able to run example.cpp successfully after making some changes to the code that was shared in this link (https://docs.nvidia.com/cla... and was able to get this output shown in the attachment.Can you please let me know how does MITK interact with AIAA or is there any beginner level guide for us to kind of understand how this interaction works? Or is there any tutorial end-end process? From loading images in MITK to running annotation server etc.Currently, I don't have a clear idea as to what the flow is?You can watch this video to see how to add new segmentation in MITKhttps://drive.google.com/fi...For more details on MITK workbench can be found at:http://docs.mitk.org/2018.0...You can find NVIDIA plugin in http://mitk.org/wiki/MITK_R...Hello @sachidanandalle:disqus - Thanks for the response. I have installed the MITK workbench in my desktop. We have annotation server running in remote server. As an end-user, I would like to seek your inputs on few things. Can you please help?1) Once I establish connection with the AIAA in MITK, I should be able to see the data folder under ""Data Manager"" tab. Let's say I have data segregated as training datasets and validation datasets. So what do I do from now on?Is it like I open an image file and mark certain points/annotate labels for certain images? Which part of this is automated?Once I do this annotation as shown in the video above, what happens? Can you please help us understand how does this work after loading images in MITK.Data Manager tab is nothing related to connection between AIAA and MITK.  Data Manager concept can be read over MITK User guide.For annotation, using MITK kind of tool, you pick some points as input labels for annotation.. you start with some random labels, and based on the result, you keep iterating over the different slices to complete the annotation...it's like this, if you pick the good slices/points, you are certain to see quick results and less manual work in defining the full segmentation over the organ.  it's not automation here.. we can call it as semi supervised processyou can go through the video carefully, how to load the image, how to create a label for segmentation and then shift+click points and then call the annotation actionHello @sachidanandalle:disqus - Thanks for your response. This certainly helps. I believe call to AIAA server is made each time we ""confirm points"".So, radiologist inspects the patient image and marks certain points as input labels for annotation. In this case, we saw these input points were used to identify ""spleen"", which is the label for this image. So Radiologist repeats this process for a significant portion of their dataset which can be used as training dataset. Rest of the unseen images can then be used for prediction? Is my understanding of the flow right? Radiologist may decide to repeat this procedure for creating his/her own training dataset or he/she can use NVIDIA's spleen model as well. Could you kindly correct my understanding if it's not right?In addition, I have a query regarding Nvidia-ai-assisted annotation client (code error) ?Is it the right forum to ask?https://drive.google.com/op...Possibly a good video to show-case how to select extreme points and get better results for annotation.Nvidia-ai-assisted annotation client error-code are little generic; let me know if you are facing any issue regarding.Hi Sachidanand,Can you please help me understand what does ""Nvidia SmartPoly"" does? I am not sure whether I am using it the right way. As attached in the screenshot, I am able to see some colored slices in 3D view but unable to interpret it. Can you share any resource/example on how to interpret or kindly guide me to use this the right way?https://uploads.disquscdn.c...Its like any other polygon editing.. you switch 2D and click on Nvidia SmartPoly action.This will show the polygon points for the segmented region.  If something is wrong (say for some slice) then you can do manual correction by dragging the point correctly.I followed tutorial_cxr for training classification model. But, but I'm failing at running tlt-train command.2019-05-22 02:08:14.053192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9944 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5)Traceback (most recent call last):  File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main    ""__main__"", mod_spec)  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code    exec(code, run_globals)  File ""common/scripts/train.py"", line 25, in <module>  File ""common/scripts/train.py"", line 20, in main  File ""classification/scripts/train_classification.py"", line 210, in train_classificationTypeError: 'NoneType' object is not subscriptableFaced similar issue when I ran tutorial_brats.Traceback (most recent call last):=========]  train_loss: 0.9822  train_dice_et: 0.0073  train_dice_tc: 0.0178  train_dice_wt: 0.0063  train_dice: 0.0105  time: 32.52s          File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main    ""__main__"", mod_spec)  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code    exec(code, run_globals)  File ""common/scripts/train.py"", line 25, in <module>  File ""common/scripts/train.py"", line 18, in main  File ""segmentation/scripts/train_segmentation.py"", line 316, in train_segmentation  File ""common/trainers/fitter.py"", line 500, in standard_fit  File ""common/metrics/val_classes.py"", line 165, in getTypeError: object of type 'NoneType' has no len()Thanks in Advance.TypeError: ‘NoneType’ object is not subscriptableIn general, the error means that you attempted to index an object that doesn’t have that functionality. You are trying to subscript an object which you think is a list or dict, but actually is None. NoneType is the type of the None object which represents a lack of value, for example, a function that does not explicitly return a value will return None. ‘NoneType’ object is not subscriptable is the one thrown by python when you use the square bracket notation object[key] where an object doesn’t define the getitem method .A subscriptable object is any object that implements the getitem special method (think lists, dictionaries). It is an object that records the operations done to it and it can store them as a “script” which can be replayed. You are trying to subscript an object which you think is a list or dict, but actually is None. NoneType is the type of the None object which represents a lack of value, for example, a function that does not explicitly return a value will return None.‘NoneType’ object is not subscriptable is the one thrown by python when you use the square bracket notation object[key] where an object doesn’t define the getitem method . You might have noticed that the method sort() that only modify the list have no return value printed – they return the default None. This is a design principle for all mutable data structures in Python.Powered by Discourse, best viewed with JavaScript enabled"
628,maximizing-network-performance-for-storage-with-nvidia-spectrum-ethernet,"Originally published at:			https://developer.nvidia.com/blog/maximizing-network-performance-for-storage-with-nvidia-spectrum-ethernet/
NVIDIA accelerated Ethernet can remove performance bottlenecks, enabling maximum storage performance for applications in general, and AI/ML in particular.Powered by Discourse, best viewed with JavaScript enabled"
629,partitioning-offline-online-techniques,"How do you do partitioning of a graph? Do you have offline (you have infinite time) techniques as well as online (you’re seeing the graph for the first time and have to do it quickly)?We use vertex randomization (with a hash function) based 2D partitioning. This is quick and robust to graph updates. We currently do not use a more sophisticated partitioning scheme, but we can replace the default partitioning with a more complex strategy if necessary (we don’t see the need yet). Or if you need graph partitioning as a feature, please let us know.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
630,enhancing-customer-experience-in-telecom-with-nvidia-customized-speech-ai,"Originally published at:			https://developer.nvidia.com/blog/enhancing-customer-experience-in-telecom-with-nvidia-customized-speech-ai/
Learn why conversational AI systems are essential and why it is important to have a high level of transcription accuracy for optimal performance in downstream tasks.Powered by Discourse, best viewed with JavaScript enabled"
631,exelon-uses-synthetic-data-generation-of-grid-infrastructure-to-automate-drone-inspection,"Originally published at:			https://developer.nvidia.com/blog/exelon-uses-synthetic-data-generation-of-grid-infrastructure-to-automate-drone-inspection/
Most drone inspections still require a human to manually inspect the video for defects. Training a computer vision model to automate inspection is difficult without a large pool of labeled data for every possible defect. In a recent session at NVIDIA GTC, we shared how Exelon is using synthetic data generation in NVIDIA Omniverse to…Powered by Discourse, best viewed with JavaScript enabled"
632,building-simulation-ready-usd-3d-assets-in-nvidia-omniverse,"Originally published at:			https://developer.nvidia.com/blog/building-simulation-ready-usd-3d-assets-in-nvidia-omniverse/
Many companies are embracing digital twins to improve their products and services. Digital twins can be used for complex simulations of factories and warehouses or to understand how products will look and behave in the real world. However, many businesses don’t know how to begin making their existing 3D art assets valuable within a simulation…Hey Everyone,I’ve been driving the development of our SimReady specifications internally to help users begin to understand how NVIDIA sees 3D content evolving to provide extensive value within Omniverse and the simulation experiences and tools you’re creating on top of the platform.Having been involved in the 3D industry for a long time now, I want to start a discussion to help enable talented 3D designers so you can provide content that is immediately valuable for simulation, synthetic data generation (SDG) and more. Whether you’re an individual artist, or work for a company producing goods for manufacture, we would love to engage.And as I learn more about various customer needs, I’m finding SimReady is an endless well of opportunity for creators, and this blog talks about the high-level ways we’re starting to consider the standardization of 3D asset creation (whether it comes from CAD/CAM tools or a DCC package like Blender, 3ds Max or Houdini) and how these 3D assets can be effectively leveraged within the Omniverse ecosystem.I’ll be hosting a session and Q&A at GTC in March around practical SimReady asset creation and use within our simulation tools like IsaacSim, but would love to hear from you in the interim.Let me know what you’re up to and what questions you already have around SimReady. We’re just getting started, so there is lots to explore and understand as everyone’s workflows and simulation needs are different.-=BeauI think you are exactly right. A SIMREADY 3D Object WebSite would do very well andwill be the new population objects of the Metaverse.
3D objects with PHYS added and ready to be added to a world that will behave as it would in a real world. You are perfectly placed for this type of venture. Its like yoiu unconsciously choose all the right steps to be here at this time.
Get at it.
Sim Ready 3d Objects with varing levels of complexity.Hi,I am involved in the 3D industry and simulation twins for many years in industrial machinery equipments.One of most challenging task is to convert 3d assets coming from CAD/CAM (pro-e /creo/ solidworks …) to 3D meshes , conserving frame orientations between the conversions
How do you achieve this task ?Defining Collide shapes is a challenging process too.Then, for some reasons, the final product must target a web application , from what I ve seen USD format is not web ready at the moment.  How can it compete with gltf solutions?Regards,
StéphPowered by Discourse, best viewed with JavaScript enabled"
633,upcoming-event-level-up-with-nvidia-rtx-in-unity,"Originally published at:			Level Up with NVIDIA
Learn how to leverage the latest NVIDIA RTX technology in Unity Engine and connect with experts during a live Q&A at this webinar on November 16.Powered by Discourse, best viewed with JavaScript enabled"
634,upcoming-event-conversational-ai-sessions-at-gtc-2022,"Originally published at:			Conversational AI & NLP Conference Sessions | GTC 2022 | NVIDIA
Learn about the latest tools, trends, and technologies for building and deploying conversational AI.Powered by Discourse, best viewed with JavaScript enabled"
635,detecting-objects-in-point-clouds-using-ros-2-and-tao-pointpillars,"Originally published at:			https://developer.nvidia.com/blog/detecting-objects-in-point-clouds-using-ros-2-and-tao-pointpillars/
Use this ROS 2 node for object detection from lidar in 3D scenes, an important task for robotic navigation and collision avoidance.Hello!I’m trying to use this ROS 2 node but I’m having some issues building the package. Is there any place where I can get any help or documentation to solve those problems?Thank you.Hi @francisco.cruz1,You can find ROS documentation here: ROS & ROS2 Bridge — Omniverse Robotics documentationFor posting questions, please visit the Isaac ROS forum here: Isaac ROS - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
636,increasing-throughput-and-reducing-costs-for-ai-based-computer-vision-with-cv-cuda,"Originally published at:			https://developer.nvidia.com/blog/increasing-throughput-and-reducing-costs-for-computer-vision-with-cv-cuda-2/
Real-time cloud-scale applications that involve AI-based computer vision are growing rapidly. The use cases include image understanding, content creation, content moderation, mapping, recommender systems, and video conferencing. However, the compute cost of these workloads is growing too, driven by demand for increased sophistication in the processing. The shift from still images to video is also…How faster is it compared to equivalent OpenCV operations compiled with GPU support?Sorry for the slow response.  We are working on our performance benchmarking and it will be available for a future release.  Currently, we have some performance published: CV-CUDA | NVIDIA Developer.All I see is GPU comparison which is not enough for making a switch from OpenCV. Please update this topic when you get some comparison benchmarks with other frameworks.Powered by Discourse, best viewed with JavaScript enabled"
637,cuda-context-independent-module-loading,"Originally published at:			https://developer.nvidia.com/blog/cuda-context-independent-module-loading/
Most CUDA developers are familiar with the cuModuleLoad API and its counterparts for loading a module containing device code into a CUDA context. In most cases, you want to load identical device code on all devices. This requires loading device code into each CUDA context explicitly. Moreover, libraries and frameworks that do not control context creation and…Powered by Discourse, best viewed with JavaScript enabled"
638,data-attached-to-a-graph,"I’m sure there’s a huge range, but what kind / what sizes of data do you see attached to vertices and edges? In the academic world, we rarely have more than, say, something like a weight (float) per edge, but I can imagine in practice, there might be much larger / much more interesting auxiliary data.We support any type of data, from simple integers to full tensors.
Our algorithm is generic enough to support all kind of edge and node properties and multiple properties at the same time.Is that data stored separately from the connectivity information? What kind of data structure do you use for attached data?In the generic case, data is typically stored in dataframes such as cudf or dask_cudf. If this is edge data, then connectivity is typically provided with src id and dst id, which follows the familiar “SRO” model (source, relation, object).  cuGraph also natively supports additional edge properties such as weight, edge id, and edge type.  We also offer a property graph, which can store both features and connectivity prior to extracting a structural cugraph graph to run various algorithms on.For GNNs specifically, we recommend using our feature store that can efficiently store tensors.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
639,running-the-new-toolkit-on-python-efficiently,"Hello! Hope you’re having a great day, it’s 1AM here :)I’m currently working on creating a high-performance, optimized library with gradient aggregation rules in our ML application for very large tensors/vectors, which is already speeding up our work quite well. However, some parts of the code is in PyTorch, as running CUDA with a C++ script integrated in python often doesn’t provide the best results, it requires lots of data transfer between the CPU and GPU almost every time.However, I still want to utilize CUDA more. I’m wondering, with the new Toolkit, is there a way to efficiently run CUDA in python? And if so, how to do that?Thanks for the answer in advance!
SerhanThere are a few frameworks like Numba that can help you accelerate CUDA code onto the GPU, through just-in-time compilation of either C/C++ code (usually provided through a separate file or Python string), or through compilation of Python functions directly with decorators.This is an area we’re actively investigating, though, so stay tuned for more!Thanks for the answer!I’m excited to hear the development on that side, I think it could be really beneficial in the world of AI!Powered by Discourse, best viewed with JavaScript enabled"
640,develop-smaller-speech-recognition-models-with-nvidia-s-nemo-framework,"Originally published at:			Develop Smaller Speech Recognition Models with NVIDIA’s NeMo Framework | NVIDIA Technical Blog
As computers and other personal devices have become increasingly prevalent, interest in conversational AI has grown due to its multitude of potential applications in a variety of situations. Each conversational AI framework is comprised of several more basic modules such as automatic speech recognition (ASR), and the models for these need to be lightweight in…For all the talk about the edge, the article fails to describe inference times or edge hardware requirements. Does it run on jetson nano? Rasberri pi? How much ram? EtcYes, QuartzNet inference in NeMo does run on Jetson Nano. We never tried Rasberri pi though. Note that QuartzNet is an architecture - e.g. QuartzNet15x5 has B=15 blocks with R=5 sub-blocks within each block. See https://nvidia.github.io/Ne... . To lessen memory footprint you can chose to have less blocks and/or subblocks, but then you will have to re-train yourself. Another (very effective) way to reduce memory footprint is to give it audio in shorter segments.Can I make an inference using NeMo on wav2letter? Does the library have methods to do it?We don't have wav2letter model in NeMo, but Jasper model is  similar to itSorry I did not explain myself correctly. I was referring to if the NeMo library has methods to make inference with a model?yes. we also provide high-quality pre-trained checkpoints for QuartzNetTake a look at https://github.com/NVIDIA/N...I have a question about retraining the NeMo QuartzNet 15x5. I would like to re-train on my own data set, but I don’t know any information, for example, what is the minimum and maximum size of the audio that QuartzNet 15x5 supports? What are the formats that QuartzNet 15x5 supports apart from wav?There’s no set minimum or maximum audio length for training (other than what’s limited by your GPU memory), but we tend to use a rule of thumb of 0.1s to around 17s.As for other formats, this PR introduced support for additional audio formats (including MP3, Ogg, etc.) via Pydub. Wav is the most well-tested audio format, so please let us know if you run into bugs/problems with any other formats by opening an issue.Hello everyone,Per “QuartzNet replaces Jasper’s 1D convolutions with 1D time-channel separable convolutions, which use many fewer parameters.”, what is the exact difference between the two? I might’ve gotten this wrong but 1D time-channel separable convolutions is basically two 1D convolutions with respect to time and channel, right? However, what about the regular 1D convolutions? What does it consist of?Also,…with five blocks that repeat fifteen times plus four additional convolutional layersshould be “…with five blocks that repeat three times with five subblock plus four additional convolutional layers”, right?Thanks in advance!Hi! 1D convolutions and 1D time-channel separable convolutions perform a roughly comparable operation (across time and channel of the input), but the latter splits it into two steps to save on parameters.In a normal 1D convolution, you’ll have K*c_in*c_out params since you need c_out kernels for every input channel c_in, multiplied by the number of params in the kernel.In a 1D time-channel separable convolution, we do the 1D convolution across each channel separately (c_in*K params), then a pointwise convolution for each time frame across all channels (c_in*c_out params), for a total of c_in*K + c_in*c_out parameters. It’s a little messy to try to explain without any images, so here’s a diagram I made a while ago that might help visualize this:

separable_conv1434×1014 79.1 KB
Re the 15, yep, that’s a typo… That should be five blocks that repeat three times each.It’s been a few years and things are fuzzier than I’d like, but hopefully this helps!Powered by Discourse, best viewed with JavaScript enabled"
641,limit-order-book-dataset-generation-for-accelerated-short-term-price-prediction-with-rapids,"Originally published at:			https://developer.nvidia.com/blog/limit-order-book-dataset-generation-for-accelerated-short-term-price-prediction-with-rapids/
Hardware acceleration using GPUs reduces the time required for financial ML researchers to obtain prediction results.Powered by Discourse, best viewed with JavaScript enabled"
642,accelerating-machine-learning-on-a-linux-laptop-with-an-external-gpu,"Hi Dhruv:Our company (https://kfocus.org) is working with organizations like JPL and other big data users that have interest in eGPU.  We really appreciate your post and am very interested in pursuing providing solutions for them. However, it is not clear how to use the iGPU, dGPU, and eGPU concurrently - for example, use the dGPU for display and then the eGPU for blender rendering.  Might you have recommendations on resources for that?Any help would be greatly appreciated!Sincerely, MikeHi Druv: Is this something you can help with? Cheers, MikeHi @deppman
While there isn’t a turnkey solution to the iGPU+dGPU+eGPU problem, there are a couple of ways of going about creaing a solution.
You could set the iGPU to be the X screen renderer on battery and the dGPU to be the X screen renderer on AC. There are some tools within Ubuntu for this like gpu-manager/prime-select or you can use Offloading Graphics Display with RandR 1.4.When it comes to using the eGPU, you can use it either as a PRIME Render Offload device or a Compute device, depending on the task. PRIME Render Offload is meant for applications that require the X screen to be “rendered” on a different GPU for example Blender(an older alternative was Bumblebee). Compute is meant for CUDA/Accelerated Data Science tasks.For my machine, I’m using the eGPU as my primary X renderer as I don’t have a dGPU. For a laptop with an iGPU + dGPU + eGPU, I’d imagine that wouldn’t be the case, and if it is, then you can use “AllowExternalGpus” to use the eGPU as the primary X renderer. Otherwise, you could “Prime Render Offload” Blender to the eGPU/dGPU (depending on what is connected and what is the power source) and use the eGPU as a compute device otherwise if it is connected.In case you have both the dGPU and eGPU connected and can’t use Prime Render Offload, you’d have to rely on some other way of hiding the other GPU. A way that I’ve been using has been docker with the --gpus or NVIDIA_VISIBLE_DEVICES flag/envvar. Some other applications like OBS allow you select the GPU if you happen to have multiple GPUs in you system.Here are some links to relevant information:
http://us.download.nvidia.com/XFree86/Linux-x86_64/455.28/README/optimus.html
http://us.download.nvidia.com/XFree86/Linux-x86_64/455.28/README/randr14.html
http://us.download.nvidia.com/XFree86/Linux-x86_64/455.28/README/primerenderoffload.html
https://wiki.archlinux.org/index.php/bumblebeeThanks for the great tutorial. Is it possible to use two eGPUs as well?Hi Dsinga:Sorry for the delay, but I must have missed the notification. Thank you for all your help. We will follow up on your articles. For our customers’ purposes, the use of the eGPU as an add-on compute unit is the most common use and here the CoreX is working great, and we have been able to run dGPU + eGPU to run two separate GPGPU workloads concurrently. I will report back when we resume testing.  Thanks again!Sincerely, Mikegreat info here. I have a Razer laptop with Quadro5000RTX. I’m wondering if I get a CoreX with a Desktop Quadro 5000RTX, will I be able to use both at the same time (mobile Quadro5000+ eGPUDesktopQuadro5000) for my DL training in principle?Hi Dhruv,Really appreciate the detail your post goes into. I am however unclear about the following:Make sure that the NVIDIA GPU is detected by the system and a suitable driver is loaded:$ lspci | grep -i “nvidia”$ lsmod | grep -i “nvidia”The existing driver is most likely Nouveau, an open-source driver for NVIDIA GPUs. Because Nouveau doesn’t support eGPU setups, install the NVIDIA CUDA and NVIDIA drivers instead. You must also stop the kernel from loading Nouveau.
Get the latest version of the NVIDIA CUDA Toolkit for your distribution. For Ubuntu 20.04, this toolkit is available from the standard repository:$ sudo apt-get install nvidia-cuda-toolkitDoes this mean if lsmod | grep -i ""nvidia"" returns nothing that I need to install the NVIDIA drivers using sudo apt-get install nvidia-cuda-toolkit? It seems to me from your post that the drivers and cuda toolkit should all be installed using that command (unless installing the drivers is outside the scope of your post).However, when I check the dependencies of nvidia-cuda-tookit on Ubuntu 20.04 using apt, there are no nvidia-driver dependencies. Are the drivers contained in some of the libnvidia or nvidia-cuda packages?Thanks for your time.EDIT: installed nvidia-cuda-toolkit and confirmed that the drivers are not installed. Thus the installation of CUDA and drivers is likely outside of the scope of this post.Sure! As long as you have enough PCIe lanes provided by your CPU+motherboard to your Thunderbolt ports(check with your laptop/NUC manufacturer or see if they have a engineering diagram for it) you can run 2 eGPUs. Use nvidia-settings to configure your displays and NVIDIA GPUsYes, if you’re trying to use your laptop dGPU or iGP to drive the display don’t add the “AllowExternalGpus” “True” to the 10-nvidia.conf xorg config file since that makes it so that the eGPU drives Xorg and thus the displays. Most machines also support connecting an eGPU after boot(although disconnecting while booted can cause Xorg to crash or a kernel panic).
The Desktop Quadro 5000 would show up as another GPU in your system along with your mobile Quadro 5000. Then you’d have to structure your code to leverage multi-GPU through layer or model parallelism. That said, you might find a difference in the bandwidth between your eGPU and dGPU, so in order to get the best training performance you should profile to see if you’re bandwidth bound for the eGPU and if you are, have different batch sizes based on the ability of the GPU to iterate through it.lsmod returns a list of modules loaded by the Linux kernel. If the $lspci | grep -i “nvidia” command shows that there is a NVIDIA GPU connected to the PCI bus, and $lsmod | grep -i “nvidia” returns nothing, you either don’t have the NVIDIA driver installed, or there’s something wrong with the driver installation that doesn’t allow the kernel to load it which is very unlikely. Regarding nvidia-cuda-toolkit providing drivers, it should provide you with NVIDIA driver. Once you install nvidia-cuda-toolkit, what’s the output of $nvidia-smi and $lsmod | gpre -i “nvidia”. It might be that nvidia-cuda-toolkit installed the driver but didn’t add the location of the nvidia-smi binary to PATH.So I have to be honest here and say that this isnt the right method for my system. I found as long as I enabled thunderbolt I could see both the GPU in my laptop and I could also use the eGPU be recognized for CUDA. The minute I added in the comment on “AllowExternalGpus” “True” in Grub, I lost the ability to use the HDMI input form my eGPU. This command would show up in my display setting as a second display but my mouse wouldnt move anywhere. In the end I went back to level 3 and my external monitor coming from the eGPU showed up without this command. the nvidia-smi command was showing it before hand. So I am not sure this should be kept around as an instruction…What does the command actually do? It doesnt really make sense to me. If you have a thunderbolt and you check the nvidia-smi showing both are connected. I think the blog should give a pre warning at the point of the section saying try. When I did this I know cant get my second monitor to get picked up even though it worked just before these changes. So a little annoyed… to say the least…Probably best to stick in a bit clearer that in your case (which you mention but should be clearer) that your set up is with a laptop without a gpu?edit:
As a warning,  this appears to basically wipe all the setting that nvidia run automatically such as turning off nouveau. Now having turned it off, the grub loads on the egpu hdmi but is shut down as soon as it boots to a gui.2nd time… after 30 mins of trying i just swapped the hdmi to my native laptop gpu and it seems to be working fine. At leats the egpu is free for other stuff and cuda 11.0 can see it with nvcc…Spent hours trying to get an eGPU to work with my laptop.  No dice.  Just a complete and total nightmare and frankly not worth spending days-to-weeks battling with poorly written software and random chipsets that may or may not work. LINUX has a long way to go in the eGPU area. Until someone makes a completely straight-forward, “drivers included with enclosure” system (like winblows) for LINUX (Ubuntu), an eGPU is a really mixed bag.  Distro, enclosure, drivers, chipsets, cards, laptops models => all can have a major effect on the ability to make this work.Not sure if this is a mistake or just something that varies between systems, but this didn’t work for me until I changed “Option “AllowExternalGpus” “True”” to “Option “AllowExternalGpus” “true””. Took me a while to locate the issue so I thought I’d try and save someone else the hassle!Thanks! Looking at xorg.conf, it looks like any of 1 , on , true , yes would be accepted as the boolean True when encased in quotation marks.Fun fact, the massively oversized PCI BAR on the Tesla K80 doesn’t play well with Thunderbolt 3 eGPU solutions.  I haven’t found a documented limitation in Thunderbolt 3, but it “doesn’t work anywhere I try it” lol.I fixed this; turns out it wasn’t me, but rather the PCI board that SONNET Breakaway Box 750 now uses is not LINUX compliant.  This was disappointing as SONNET was one of the eGPU enclosure manufacturers that were producing good quality compatible hardware.  Now they just produce Windows only (read you need a hacky driver) hardware.  Tried again with a Razer Core X eGPU enclosure and a MSI RTX 3060Ti 3x OC Ventus card.  Works like a charm!  The Razer Core series is fully compliant. I would really, really like to see hot-plug work at some time, but I gather the window manager Wayland is almost there (on Ubuntu).Hi @dsingalNV,Thanks for the informative post.! I have some difficulty with the setup though.I use a Sonic breakaway box 550 with a desktop 3070 RTX card attached to it. The laptop that I use has a 3050 Ti internal card. My application requires computing to be done on the egpu, while the xserver and all renders need to run on the internal 3050 Ti card.I use ubuntu 20.04 and installed the correct nvidia drivers. nvidia-smi only shows the internal graphics card and not the graphics card connected through thunderbolt. But lspci shows both cards and boltctl shows the correct thunder bolt device connected.  As I understand it, since I don’t want X to run on the eGPU, I don’t really have to change any xconfig files. I’m not sure what else to do.I don’t know why nvidia-smi doesn’t identify the eGPU. Can you help me out?Cheers,
VarunHi, Dhruv.I recently tried a configuration of a host with Thunderbolt 4 connected to a Thunderbolt 4 hub that is connected to three Thunderbolt-to-PCIe enclosures which each contain
a GTX 1060.Each GTX 1060 enumerated in Windows 11 22H2 and Ubuntu 22.04 (5.15-60) with current Nvidia drivers ( as of 2023-04-06).What a pleasant surprise!I used a parallel test case in MATLAB to confirm all GPUs were utilized.I do not know if this configuration works in general with other computer models, Thunderbolt devices and Nvidia GPU models.I work at a company that makes Thunderbolt devices.Could we get in touch to share notes?   Or could you introduce me to a colleague involved with product compatibility testing?You said,The existing driver is most likely Nouveau, an open-source driver for NVIDIA GPUs. Because Nouveau doesn’t support eGPU setups, install the NVIDIA CUDA and NVIDIA drivers instead. You must also stop the kernel from loading Nouveau.but no commands were provided to install the required driver for egpu and disable Nouveau@ramkumarkoppu you do not need to disable nouveau manually when installing the NVIDIA driver through apt-get. If you use the runfile method you will have to blacklist the nouveau kernel module through:
$ cat /etc/modprobe.d/blacklist-nvidia-nouveau.conf
blacklist nouveau
options nouveau modeset=0
That said, Ubuntu 22.04 and later ship with the NVIDIA driver which will be loaded by default instead of Nouveau.
And I would recommend GitHub - hertg/egpu-switcher: 🖥🐧 Setup script for eGPUs in Linux (X.Org) for switching between GPUs on a hybrid device going forward.Powered by Discourse, best viewed with JavaScript enabled"
643,introducing-nvidia-aerial-research-cloud-for-innovations-in-5g-and-6g,"Originally published at:			https://developer.nvidia.com/blog/introducing-aerial-research-cloud-for-innovations-in-5g-and-6g/
At NVIDIA GTC 2023, NVIDIA introduced Aerial Research Cloud, the first fully programmable 5G and 6G network research sandbox, which enables researchers to rapidly simulate, prototype, and benchmark innovative new software deployed through over-the-air networks. The platform democratizes 6G innovations with a full-stack, C-programmable 5G network, and jumpstarts ML in advanced wireless communications using NVIDIA-accelerated…Hi!
do you have any limitations because of latencies issues in your system?
I mean, 5G 1ms round trip time seems to be too tight to reach for a GPU architectureDo you support 120kHz subcarrier spacing? Thank you!Powered by Discourse, best viewed with JavaScript enabled"
644,accelerating-redis-performance-using-vmware-vsphere-8-and-nvidia-bluefield-dpus,"Originally published at:			https://developer.nvidia.com/blog/accelerating-redis-performance-using-vmware-vsphere-8-and-nvidia-bluefield-dpus/
A shift to modern distributed workloads, along with higher networking speeds, has increased the overhead of infrastructure services. There are fewer CPU cycles available for the applications that power businesses. Deploying data processing units (DPUs) to offload and accelerate these infrastructure services delivers faster performance, lower CPU utilization, and better energy efficiency. Many modern workloads…Powered by Discourse, best viewed with JavaScript enabled"
645,democratizing-and-accelerating-genome-sequencing-analysis-with-nvidia-clara-parabricks-v4-0,"Originally published at:			https://developer.nvidia.com/blog/democratizing-and-accelerating-genome-sequencing-analysis-with-clara-parabricks-v4-0/
Learn about NVIDIA Clara Parabricks v4.0, which brings significant improvements to how genomic researchers and bioinformaticians deploy and scale genome sequencing analysis pipelines.Hello, Parabricks 3.2 had a RNA pipeline. This is not present in the new Parabricsk 4.0 version. Any suggestions on how to analyze RNA data for variant identification with the 4.0 version? Thanks!3.2:
https://docs.nvidia.com/clara/parabricks/v3.2/text/rna_pipeline.htmlPowered by Discourse, best viewed with JavaScript enabled"
646,strategies-for-maximizing-data-center-energy-efficiency,"Originally published at:			https://developer.nvidia.com/blog/strategies-for-maximizing-data-center-energy-efficiency/
Data centers are an essential part of a modern enterprise, but they come with a hefty energy cost. To complicate matters, energy costs are rising and the need for data centers continues to expand, with a market size projected to grow 25% from 2023 to 2030. Globally, energy costs are already negatively affecting data centers…Powered by Discourse, best viewed with JavaScript enabled"
647,sdks-accelerating-industry-5-0-data-pipelines-computational-science-and-more-featured-at-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/sdks-accelerating-industry-5-0-data-pipelines-computational-science-and-more-featured-at-gtc-2023/
At NVIDIA GTC 2023, NVIDIA unveiled notable updates to its suite of NVIDIA AI software for developers to accelerate computing. The updates reduce costs in several areas, such as data science workloads with NVIDIA RAPIDS, model analysis with NVIDIA Triton, AI imaging and computer vision with NVIDIA CV-CUDA, and many more. To keep up with…Powered by Discourse, best viewed with JavaScript enabled"
648,aaa-games-using-path-tracing,"Can I check out some AAA games using Path Tracing today outside of Cyberpunk?Other path traced games available include Minecraft RTX and Quake II RTX. Portal RTX is coming in the future.Powered by Discourse, best viewed with JavaScript enabled"
649,developing-a-pallet-detection-model-using-openusd-and-synthetic-data,"Originally published at:			https://developer.nvidia.com/blog/developing-a-pallet-detection-model-using-openusd-and-synthetic-data/
By iteratively developing with synthetic data, our team developed a pallet detection model that works on real-world images.Hi, could you share the code based on USD Scene Construction Utilities that was used to generate the pallet scene for Replicator?Powered by Discourse, best viewed with JavaScript enabled"
650,nvidia-dlss-updates-for-super-resolution-and-unreal-engine,"Originally published at:			https://developer.nvidia.com/blog/nvidia-dlss-updates-for-super-resolution-and-unreal-engine/
Learn about the latest NVIDIA DLSS updates for Super Resolution and Unreal Engine.Powered by Discourse, best viewed with JavaScript enabled"
651,machine-learning-in-practice-build-an-ml-model,"Originally published at:			https://developer.nvidia.com/blog/machine-learning-in-practice-build-an-ml-model/
This series looks at the development and deployment of machine learning (ML) models. In this post, you train an ML model and save that model so it can be deployed as part of an ML system. Part 1 gave an overview of the ML workflow, considering the stages involved in using machine learning and data science…Powered by Discourse, best viewed with JavaScript enabled"
652,understanding-the-need-for-adaptive-temporal-antialiasing-ataa,"Originally published at:			Understanding the Need for Adaptive Temporal Antialiasing (ATAA) | NVIDIA Technical Blog
The next generation of antialiasing is called ATAA, which stands for “Adaptive Temporal Antialiasing”. This new approach solves for the weakness of TAA (temporal anti-aliasing) – blurring and ghosting artifacts – while remaining light enough to avoid introducing a significant performance hit. ATAA results are remarkably close to what a developer would achieve through 16x…Powered by Discourse, best viewed with JavaScript enabled"
653,rtx-global-illumination-part-i,"Originally published at:			RTX Global Illumination Part I | NVIDIA Technical Blog
RTX Global Illumination (RTX GI) creates changing, realistic rendering for games by computing diffuse lighting with ray tracing. It allows developers to extend their existing light probe tools, knowledge, and experience with ray tracing to eliminate bake times and avoid light leaking. Hardware-accelerated programmable ray tracing is now accessible through DXR, VulkanRT, OptiX, Unreal Engine,…Really very nice information for the global illumination and i'm waiting for the more info about it regards -  GHDSPORTS We're planning on posting part 2 in the next couple of weeks.this is a nice primer, did you ever get round to Part 2?Powered by Discourse, best viewed with JavaScript enabled"
654,time-series-forecasting-with-the-nvidia-time-series-prediction-platform-and-triton-inference-server,"Originally published at:			https://developer.nvidia.com/blog/time-series-forecasting-with-the-nvidia-time-series-prediction-platform-and-triton-inference-server/
Learn how the Time Series Prediction Platform provides an end-to-end framework that enables users to train, tune, and deploy time series models.how to use TSF with C++? And how do I customize components? Such as the loss function.Hey, thank you for you interest!  At the moment, the NVIDIA TSPP is just for Python.  In order to customize the loss function (or any component), there are 2 main things needed.Let us know if you have further issues/questions and don’t hesitate to open an issue on the github: Issues · NVIDIA/DeepLearningExamples · GitHubIs there a way to do Classification only with the TFT? I have a time series dataset that I need to do classification on and I thought I could potentially do classification on it with the TFT. Thoughts?Powered by Discourse, best viewed with JavaScript enabled"
655,gpus-and-dsls-for-life-insurance-modeling,"Originally published at:			https://developer.nvidia.com/blog/gpus-dsls-life-insurance-modeling/
The Solvency II EU Directive came into effect at the beginning of the year. It harmonizes insurance regulation in the European Union with an economic and risk based approach, which considers the full balance sheet of insurers and re-insurers. In the case of life insurers and pension funds, this requires the calculation of the economic value…Very interesting, thank you for this -complete- post. As a math person, it's always good to see a nice description and modeling of the problem.I'm currently working with a bank that uses GPUs and develop in a C# framework (nvcc & VS C# compiler), but relies on a tool provided by another company to generate multithreaded or GPU code automatically : I wasn't aware of such alternatives, or of F#.A question, though, on the C# code snippet. This simulation relies on a stochastic model which means we probably cannot just ""compute"" the result for a given contract by simply integrating the equation.Is that stochasticity somewhat hidden under the outer loop (under ""results""), that accounts for different scenarii / states of the Markov chain, or has this ""expectation"" been already computed and translated into the ""bj_ii"" factor ?As these loops are sequential by nature, I'd then infer that it's the work of a single thread. Then, what is mapped to thread blocks / grid (a contract, a single simulation, a portfolio) ?ValentinThe model directly incorporates the stochasticity of unknown remaining lifetime (through a death intensity function) but not the stochasticity of unknown death intensity (eg. longevity shocks) or unknown interest rates, nor of unknown interest rates: it assumes deterministic future interest rates, often mandated by financial regulation. A more complicated model could account for these unknowns too, or more simply, one could run the given model with a range of death intensities and interest rate curves -- which of course would require more computation time.Your are right that (the reserve of) each contract is computed in a single thread. An entire portfolio of similar contracts is mapped to  thread blocks or grids, though we are also experimenting with mapping a single many-state contract to a thread block.Powered by Discourse, best viewed with JavaScript enabled"
656,webinar-build-your-next-deep-learning-application-for-nvidia-jetson-in-matlab,"Originally published at:			Webinar: Build Your Next Deep Learning Application for NVIDIA Jetson in MATLAB | NVIDIA Technical Blog
Learn how you can use MATLAB to build your computer vision and deep learning applications and deploy them on NVIDIA Jetson. MATLAB auto-generates portable CUDA code that leverages CUDA libraries like cuBLAS and cuDNN from the MATLAB algorithm, which is then cross-compiled and deployed to Jetson. The generated code is highly optimized and benchmarks will…Hi @jwitsoe, any idea how I could access the webinar ? Tried registering on this link : https://info.nvidia.com/build-your-next-nvidia-jetson-deep-learning-application-in-matlab-reg-page.html, but the webinar page won’t load.
Thanks for your help.@AlexandreJ  – It looks like the webinar is no longer available, sorry! Maybe this video might be helpful instead?Deep Learning with MATLAB, NVIDIA Jetson, and ROS VideoGood find ! I’ll have a look :) Thank you very much for your helpPowered by Discourse, best viewed with JavaScript enabled"
657,transform-the-data-center-for-the-ai-era-with-nvidia-dpus-and-nvidia-doca,"Originally published at:			https://developer.nvidia.com/blog/transform-the-data-center-for-the-ai-era-with-nvidia-dpus-and-nvidia-doca/
NVIDIA BlueField-3 DPUs are now in full production, and have been selected by Oracle Cloud Infrastructure (OCI) to achieve higher performance, better efficiency, and stronger security.Powered by Discourse, best viewed with JavaScript enabled"
658,accelerating-ultra-realistic-game-development-with-nvidia-dlss-3-and-nvidia-rtx-path-tracing,"Originally published at:			https://developer.nvidia.com/blog/accelerating-ultrarealistic-game-development-with-nvidia-dlss-3-and-rtx-path-tracing/
Discover how NVIDIA DLSS 3 and RTX path tracing are accelerating ultra realistic game development.Powered by Discourse, best viewed with JavaScript enabled"
659,nvidia-research-auditing-ai-models-for-verified-deployment-under-semantic-specifications,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-auditing-ai-models-for-verified-deployment-under-semantic-specifications/
○	Quality assurance and audits are necessary for deep learning models. Current AI models require large data sets for training or a designed reward function that must be optimized. Algorithmically, AI is prone to optimizing behaviors that were not intended by the human designer. To help combat this, the AuditAI framework was developed to help audit these problems, which increases safety and ethical use of deep learning models during deployment.Do deep learning models need auditing? Find out about AuditAI and see how you can benefit from QA for your AI model.What is the link for additional information?
“For more information, see Auditing AI models for Verified Deployment under Semantic Specifications [LINK].”Good catch, @medgar! The paper is still under review, so I’ll add that link when the paper’s available online.The paper with technical details is now available:Auditing trained deep learning (DL) models prior to deployment is vital for
preventing unintended consequences. One of the biggest challenges in auditing
is the lack of human-interpretable specifications for the DL models that are
directly useful to...@medgar and @animeshg, the post is now updated with the link. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
660,improving-robot-motion-generation-with-motion-policy-networks,"Originally published at:			https://developer.nvidia.com/blog/improving-robot-motion-generation-with-motion-policy-networks/
Collision-free motion generation in unknown environments is a core building block for robotic applications. Generating such motions is challenging. The motion generator must be fast enough for real-time performance and reliable enough for practical deployment.  Many methods addressing these challenges have been proposed, ranging from using local controllers to global planners. However, these traditional motion…Powered by Discourse, best viewed with JavaScript enabled"
661,achieve-innovative-hyperconverged-networking-with-nvidia-spectrum-ethernet-and-microsoft-azure-stack-hci,"Originally published at:			https://developer.nvidia.com/blog/achieve-innovative-hyperconverged-networking-with-nvidia-spectrum-ethernet-and-microsoft-azure-stack-hci/
NVIDIA Spectrum Ethernet switches have been added to the Microsoft Azure Stack HCI supported device list, enabling innovative hyperconverged networking.Powered by Discourse, best viewed with JavaScript enabled"
662,develop-for-all-six-nvidia-jetson-orin-modules-with-the-power-of-one-developer-kit,"Originally published at:			Develop for All Six NVIDIA Jetson Orin Modules with the Power of One Developer Kit | NVIDIA Technical Blog
All NVIDIA Jetson Orin modules share one SoC architecture, enabling the Jetson AGX Orin Developer Kit to emulate any of them. This makes it easy to start developing your next product.Hi jwitsoe,Does the emulation apply to all HW components within the SoC such as ISP capability… Or to put it another way, is the ISP the ‘same’ for each module and do they therefore all have the same capability.Please note i am referring to the ISP processing pipeline only and not the actual camera connectivity, details regarding the C-PHY DPHY capability of modules seems well explained elsewhere.All the best
ChrisPowered by Discourse, best viewed with JavaScript enabled"
663,end-to-end-ai-for-nvidia-based-pcs-nvidia-tensorrt-deployment,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-nvidia-based-pcs-nvidia-tensorrt-deployment/
This post is the fifth in a series about optimizing end-to-end AI. NVIDIA TensorRT is a solution for speed-of-light inference deployment on NVIDIA hardware. Provided with an AI model architecture, TensorRT can be used pre-deployment to run an excessive search for the most efficient execution strategy. TensorRT optimizations include reordering operations in a graph, optimizing…Powered by Discourse, best viewed with JavaScript enabled"
664,ecommerce-and-open-ethernet-criteo-clicks-with-sonic,"Originally published at:			https://developer.nvidia.com/blog/ecommerce-and-open-ethernet-criteo-clicks-with-sonic/
When you see a browser ad for a new restaurant, or the perfect gift for that hard-to-please family member, you probably aren’t thinking about the infrastructure used to deliver that ad. However, that infrastructure is what allows advertising companies like Criteo to provide these insights. The NVIDIA networking portfolio is essential to Criteo technology stack.…Hello, and thanks for reading!  We are just witnessing the beginning of the open networking revolution, and it’s great to have this partnership with pioneers like Criteo. Let me know if you have any questions about NVIDIA, SONiC, Criteo, or anything else. Thanks again!Best,
TaylorIt was a good to read.Powered by Discourse, best viewed with JavaScript enabled"
665,announcing-nvidia-dgx-gh200-the-first-100-terabyte-gpu-memory-system,"Originally published at:			https://developer.nvidia.com/blog/announcing-nvidia-dgx-gh200-first-100-terabyte-gpu-memory-system/
At COMPUTEX 2023, NVIDIA announced NVIDIA DGX GH200, which marks another breakthrough in GPU-accelerated computing to power the most demanding giant AI workloads.Powered by Discourse, best viewed with JavaScript enabled"
666,tuning-ai-infrastructure-performance-with-mlperf-hpc-v2-0-benchmarks,"Originally published at:			https://developer.nvidia.com/blog/tuning-ai-infrastructure-performance-with-mlperf-hpc-v2-0-benchmarks/
Discover how you can tune AI infrastructure performance with MLPerf HPC v2.0 benchmarks.Powered by Discourse, best viewed with JavaScript enabled"
667,gpu-accelerated-json-data-processing-with-rapids,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-json-data-processing-with-rapids/
JSON is a widely adopted format for text-based information working interoperably between systems, most commonly in web applications. While the JSON format is human-readable, it is complex to process with data science and data engineering tools. To bridge that gap, RAPIDS cuDF provides a GPU-accelerated JSON reader (cudf.read_json) that is efficient and robust for many…Powered by Discourse, best viewed with JavaScript enabled"
668,scaling-recommendation-system-inference-with-merlin-hierarchical-parameter-server,"Originally published at:			Scaling Recommendation System Inference with Merlin Hierarchical Parameter Server | NVIDIA Technical Blog
NVIDIA Merlin introduces the Hierarchical Parameter Server (HPS), a scalable solution with multilevel adaptive storage to enable deployment of terabyte-size models under real-time latency constraints.why use rockdb in SSD rather than relational RDMS like sqlite?Powered by Discourse, best viewed with JavaScript enabled"
669,explainer-what-is-green-computing,"Originally published at:			What Is Green Computing? | NVIDIA Blog
reen computing, also called sustainable computing, aims to maximize energy efficiency and minimize environmental impact in the ways computer chips, systems, and software are designed and used.Powered by Discourse, best viewed with JavaScript enabled"
670,top-metaverse-developer-sessions-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/top-metaverse-developer-sessions-at-nvidia-gtc-2023/
Learn how developers are building metaverse applications, extensions, and microservices.Powered by Discourse, best viewed with JavaScript enabled"
671,controlled-adaption-of-speech-recognition-models-to-new-domains,"Originally published at:			https://developer.nvidia.com/blog/controlled-adaption-of-speech-recognition-models-to-new-domains/
Using adapters for parameter-efficient training reduces the effects of catastrophic forgetting of general speech recognition.Powered by Discourse, best viewed with JavaScript enabled"
672,categorical-features-in-xgboost-without-manual-encoding,"Originally published at:			https://developer.nvidia.com/blog/categorical-features-in-xgboost-without-manual-encoding/
XGBoost is a decision-tree–based, ensemble machine learning algorithm based on gradient boosting. However, until recently, it didn’t natively support categorical data. Categorical features had to be manually encoded before they could be used for training or inference. In the case of ordinal categories, for example, school grades, this is often done using label encoding where…It was exciting to explore how XGBoost’s experimental categorical support can save time and improve performance when working with categorical data. If you have any questions or comments, let us know!I was a great post, and happy to find out XGBoost supports categorical data. Do you know if this also extends to arrays of categorical data and how we should deal with them?Powered by Discourse, best viewed with JavaScript enabled"
673,model-parallelism-and-conversational-ai-workshops,"Originally published at:			​ - DLI Public Workshops June 2023
Join these upcoming workshops to learn how to train large neural networks, or how to build and deploy a conversational AI pipeline.Powered by Discourse, best viewed with JavaScript enabled"
674,new-question,"This is a repost of a question which was in the wrong location:Do you have any collection of information about CUDA with recipe for various computing solutions?I’m new to CUDA. Open to ideas of use cases for the followings:thanks@kcsham Here is where we can continue this discussion! Could you please give a little bit more information regarding what you mean regarding “computing solutions”?Currently, one thing that comes to mind is our Hopper H100 Confidential Computing Solution (NVIDIA Confidential Computing - NVIDIA Docs). This information goes over how you can ensure confidentiality (encryption and authentication) of your data from prying eyes (physically or remotely)We also have our High Performance Computing (HPC) for things like high-complexity simulations, etc.Happy to keep talking based on what you’re looking for!You can take a look at our computer vision section: Explore Computer Vision Software & Cloud Platform | NVIDIA . Personally for a hobby/getting started I recommend looking at DeepStream SDK; it is generally best for ‘getting it done fast’ as it has many pre-built plugins for you to use.For Audio transcription, you could take a look at Audio Transcription from NVIDIA LaunchPad This is a free lab where you can try our NVIDIA Riva speech recognition!For video transcoding, we have FFmpeg plugins, which can be found, built, and integrated into this open-source, industry-standard video tool. Please see this for instructions (Using FFmpeg with NVIDIA GPU Hardware Acceleration - NVIDIA Docs). This plugin you might notice is part of our larger Video Codec SDK, which is our main area for doing transcoding/modification. The main document page for that can be found here: NVIDIA Video Codec SDK v12.1 (Latest Release) - NVIDIA DocsPowered by Discourse, best viewed with JavaScript enabled"
675,mps-error-isolation,"AFAIK, MPS doesn’t provide error isolation. Are there plans to improve that?This is a complex topic because some of the error isolation and trapping happens at hardware level, so there’s an interplay between MPS and the available reporting from the GPU itself. We are exploring ways we can get better error isolation through MPS, but they’ll likely come with performance tradeoffs so they may not always be desirable.Ultimately, NVIDIA is putting a lot of engineering focus onto improving both MPS and MIG to bring better error isolation and scheduling control to MPS, and more flexibility to MIG.Powered by Discourse, best viewed with JavaScript enabled"
676,improving-gpu-performance-by-reducing-instruction-cache-misses,"Originally published at:			https://developer.nvidia.com/blog/improving-gpu-performance-by-reducing-instruction-cache-misses/
Instruction cache misses can cause performance degradation for kernels that have a large instruction footprint, which is often caused by substantial loop unrolling.Powered by Discourse, best viewed with JavaScript enabled"
677,accelerating-lidar-for-robotics-with-nvidia-cuda-based-pcl,"Originally published at:			https://developer.nvidia.com/blog/accelerating-lidar-for-robotics-with-cuda-based-pcl/
Many Jetson users choose lidars as their major sensors for localization and perception in autonomous solutions. Lidars describe the spatial environment around the vehicle as a collection of three-dimensional points known as a point cloud. Point clouds sample the surface of the surrounding objects in long range and high precision, which are well-suited for use…Hi, jwitsoe!
I am trying to use CUDA-PCL on Jetson TX2. But I have encountered a CUDA failure problem which seems to be hard for me to deal with. It would be great if you can give some suggestions.Environment: Jetson TX2 with Jetpack 4.5 (Ubuntu 18.04, CUDA-10.2, PCL 1.8.1)
Problem: When I run the built demo in each subfolder, it turn out to be a CUDA failure. One output example is given below as I run the demo in cuda-pcl/cuda-segmentation:nvidia@nvidia-tx2:~/Downloads/cuda-pcl/cuda-segmentation$ ./demo sample.pcdGPU has cuda devices: 1
----device id: 0 info----
GPU : NVIDIA Tegra X2
Capbility: 6.2
Global memory: 7850MB
Const memory: 64KB
SM in a block: 48KB
warp size: 32
threads in a block: 1024
block dim: (1024,1024,64)
grid dim: (2147483647,65535,65535)Cuda failure: no kernel image is available for execution on the device at line 310 in file cudaSegmentation.cpp error status: 209
Aborted (core dumped)I have managed several attempts to solve it.
First, I downgrade to Jetpack 4.4.1 which is the same as the official test environment. But it did not work.
Next, I followed solutions to other similar problem. Specifically, I manually add 62 (which corresponds to the compute capability 6.2 of Jetson TX2) to the SMS variable in makefile. Still, nothing changed.
Since the source code is not there, I can’t do more with it.I don’t know much about CUDA programing, but I guess the .so file is not compiled with sms=62 so it can’t be executed on Jetson TX2. I would be appreciated if you could fix it for us TX2 users.Hi triokun,
You are right that the error below means there is no kernel for CURRENT device.
This is because CUDA-PCL was not compiled for SM62.
Cuda failure: no kernel image is available for execution on the device at line 310 in file cudaSegmentation.cpp error status: 209
Aborted (core dumped)Hi, leif!
Thanks for your answering.
I’m wondering if you can recompile the library for TX2 if you have the source code. It would help me a lot.This is lib for TX2, but it has not been tested because there is no TX2 on local side.Google Drive file.I’m grateful for your help. I have tested it on TX2 and it worked perfectly!
Would you mind recompiling the other two lib (libcudafilter.so and libcudaicp.so) for TX2?
Again, thank you so much!Please check the two libs.Google Drive file.Google Drive file.Hi @leif ,Try building the CUDA-ICP example and got a usr/bin/ld: ./lib/libcudaicp.so: error adding symbols: file in wrong format error.Environment:  GTX 1050 (Ubuntu 20.04, CUDA-10.2, PCL 1.10.1)How do I get passed that?Looks like the libraries are compiled for ARM processors. Can you recompile (or provide the source code) for x86_64 ?Yes, they all work well on TX2 except for CUDA_VoxelGrid. Here is the output；---------------checking CUDA VoxelGrid---------------------
ERROR case
status = 11Jetson has a  GPU with known type but PC not.
It is hard to adjust cuda-pcl for all GPUs.
We may support X86_64 later.The VoxelGrid may be not suitable for TX2.
We will try to check it later.Hi @leif
When i run cuda-icp example and output:$~/cuda-pcl-main/cuda-icp$ ./demo
GPU has cuda devices: 1
----device id: 0 info----
GPU : NVIDIA Tegra X1
Capbility: 5.3
Global memory: 3956MB
Const memory: 64KB
SM in a block: 48KB
warp size: 32
threads in a block: 1024
block dim: (1024,1024,64)
grid dim: (2147483647,65535,65535)Loaded 859059 data points for P with the following fields: x y z rgb
Loaded 784546 data points for Q with the following fields: x y z rgb
iter.Maxiterate 20
iter.threshold 1e-12
iter.acceptrate 1Target rigid transformation : cloud_in → cloud_icp
Rotation matrix :
| 0.923880 -0.382683 0.000000 |
R = | 0.382683 0.923880 0.000000 |
| 0.000000 0.000000 1.000000 |
Translation vector :
t = < 0.000000, 0.000000, 0.200000 >matrix_icp native value
Rotation matrix :
| 1.000000 0.000000 0.000000 |
R = | 0.000000 1.000000 0.000000 |
| 0.000000 0.000000 1.000000 |
Translation vector :
t = < 0.000000, 0.000000, 0.000000 >------------checking CUDA ICP(GPU)----------------
Cuda failure: the launch timed out and was terminated at line 59 in file cudaICP.cpp error status: 702
Aborted (core dumped)Can you help me fix problem ?
Thanks you.Hi @nghiaphamsg
Error status: 702 means that :
Specified whether there is a run time limit on kernels
https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html#structcudaDeviceProp_19a63114766c4d2309f00403c1bf056c8
Could you try to boost your device firstly?Hi jwitose,
I am processing 3D points in a custom way to create 2D images. So, I am going through all the points and want to speed up the process using CUDA. The code is part a ROS node. Could you please let me know whether you have any samples or tutorials for ROS and CUDA especially for Point Cloud processing?
Thanks in advance,
AhmetHi asagllam,
Our cuda-pcl provide some libs and head files which can be used directly for any framework include ROS.Hello @leif ,Could you compile the ICP, filter, and segmentation libraries for the TX1 as well?Thanks,
RyanHi @rchen390
Please check the three libs which was compiled for TX1 with Jetpack 4.4.1.Google Drive file.Google Drive file.Google Drive file.Hi @leif,Thank you for compiling the libraries for me last time. If possible, could you also compile the ICP, filter, and segmentation libraries for the NVIDIA GeForce RTX 2080?Happy holidays!Hi @rchen390
Please check these libraries.Google Drive file.Google Drive file.Google Drive file.Google Drive file.Google Drive file.Powered by Discourse, best viewed with JavaScript enabled"
678,time-magazine-names-nvidia-instant-nerf-a-best-invention-of-2022,"Originally published at:			TIME Magazine Names NVIDIA Instant NeRF a Best Invention of 2022 | NVIDIA Technical Blog
TIME Magazine named NVIDIA Instant NeRF, a technology capable of transforming 2D images into 3D scenes, one of the Best Inventions of 2022.  “Before NVIDIA Instant NeRF, creating 3D scenes required specialized equipment, expertise, and lots of time and money. Now it just takes a few photos and a few minutes,” TIME writes in their…Powered by Discourse, best viewed with JavaScript enabled"
679,develop-a-multi-robot-environment-with-nvidia-isaac-sim-ros-and-nimbus,"Originally published at:			https://developer.nvidia.com/blog/develop-a-multi-robot-environment-with-nvidia-isaac-sim-ros-and-nimbus/
Developing a high-fidelity multi-robot simulated environment is complex and takes time, but it can be simplified with NVIDIA Isaac Sim and Nimbus.Powered by Discourse, best viewed with JavaScript enabled"
680,accelerated-motion-processing-brought-to-vulkan-with-the-nvidia-optical-flow-sdk,"Originally published at:			https://developer.nvidia.com/blog/accelerated-motion-processing-brought-to-vulkan-with-optical-flow-sdk/
The NVIDIA Optical Flow Accelerator (NVOFA) is a dedicated hardware unit on newer NVIDIA GPUs for computing optical flow between a pair of images at high performance. The NVIDIA Optical Flow SDK exposes developer APIs that enable you to leverage the power of NVOFA hardware in your applications.  We are excited to announce the availability…Powered by Discourse, best viewed with JavaScript enabled"
681,nvidia-rtx-top-3-week-of-november-9-2018,"Originally published at:			NVIDIA RTX Top 3: Week of November 9, 2018 | NVIDIA Technical Blog
Every week, we’ll be delivering 3 interesting stories coming from the world of RTX Game Development. 1 – “Final Fantasy XV” Director Takashi Aramaki shares his thoughts on DLSS Square Enix has strived to keep Final Fantasy XV on the bleeding edge of the technological curve, evolving the product over time with new game development tools.…Powered by Discourse, best viewed with JavaScript enabled"
682,latest-nvidia-optix-renders-ray-tracing-faster-than-ever-before,"Originally published at:			Latest NVIDIA OptiX Renders Ray Tracing Faster Than Ever Before | NVIDIA Technical Blog
NVIDIA OptiX 7.4 introduces parallel compilation, temporal denoising of arbitrary values, improvements to the demand loading library, enhancements to ray payloads, Catmull-Rom curves, and decreased memory for curves.My only interest is VR - I no longer play games in flat (or pancake) mode, so whenever Nvidia announces something could you please include a mention to tell us whether this is going to improve the VR experience.  I have a 2080TI which I bought especially for VR and it works well, I would have upgraded to one in the 30 series (if they had been available) but from what I have been able to work out it would not have given a significant improvement.Thank you.hello, thank you for share, but i don’t have idea how get, install and use the program to practice, maybe a tutorial or link to see how works ?
thank youThanks for the input! OptiX has a wide set of use cases from professional rendering to simulation. OptiX can even be used to simulate sound propagation, which can offer a much more immersive experience in VR.The OptiX SDK comes with a number of samples to help you get started. They range from simple to advanced and should serve as a great place for you to start. Additionally, We have a number of talks at GTC this week that would be valuable to check out. “[A31547]: RTX Ray Tracing 101: Learn How to Build Ray-tracing Applications” would be the best one to watch for beginners.First a quick question: Is it possible that the OptiX users guide hasn’t been updated? The examples in there still mention the limit of 8 payload values.A thing I find  surprising about the OptiX 7.4 SDK is how all ray tracing calls (no matter how many payloads) are now mapped to a single PTX intrinsic _optix_trace_typed_32, whereas previous versions used finer-grained wrappers like optix_trace_1 … _optix_trace_8. This makes the wrappers in include/internal/optix_7_device_impl.h super-unwieldy (both in terms of C++ code, and the PTX code that is generated).  Is this a good idea? Even the most basic one reads:My group builds JIT compilers generating OptiX code that often contain many ray tracing calls – it seems scary to generate that many unused/temporary variables when doing a few of these in a kernel.can offer a much more immersive experience in VR.Hi, @akanell ! OptiX is a pure raytracing engine, however not a renderer. Developers can use it for implementation. My main concern is real-time rendering in VR, and unfortunately a sample code/ tutorial is missing for that.Hi @wenzel.jakob, what are the benefits of using JIT compiler over NVCC compiler? Is it faster than nvcc?@_Bi2022. Speed of compilation is not the motivating factor. JIT compilation is useful in applications where you don’t even know what code to execute until at runtime, for example when the user is writing it in a different language like Python, or when there are program transformations like differentiation that change the code at runtime. See this paper for details: RGL | Dr.Jit: A Just-In-Time Compiler for Differentiable RenderingPowered by Discourse, best viewed with JavaScript enabled"
683,learn-how-to-set-up-hardware-ray-tracing-in-unreal-engine-5,"Did you ever wonder how you can get started with hardware ray tracing in Unreal Engine 5? This video will give you a short overview of how to do just that.This is part of a series of Unreal 5 related introductory videos first listed in our technical blog on the developer pages. https://developer.nvidia.com/blog/shaping-the-future-of-graphics-with-nvidia-technologies-in-unreal-engine-5/Powered by Discourse, best viewed with JavaScript enabled"
684,nvidia-maxine-elevates-video-conferencing-in-the-cloud,"Originally published at:			https://developer.nvidia.com/blog/nvidia-maxine-elevates-video-conferencing-in-the-cloud/
NVIDIA Maxine has expanded to provide microservices that can be deployed in private or public clouds, enabling developers to leverage GPU power from remote servers.Thank you for sharing this incredible development with us. NVIDIA Maxine is definitely revolutionizing the way we connect and collaborate remotely.Powered by Discourse, best viewed with JavaScript enabled"
685,how-do-dlss-and-ser-fit-in-with-path-tracing,"On your path-tracing developer page you list e.g. DLSS and SER as part of the “RTX Path Tracing core technologies”. How are they connected to path tracing?Technically, they’re unrelated to path tracing. But they are both accelerating technologies that help to make real time path tracing possible. SER in particular is designed to accelerate GPU workloads that are divergent in nature. Path tracing is by its nature a very divergent workload, so SER happens to be particularly beneficial for path tracing.Powered by Discourse, best viewed with JavaScript enabled"
686,programming-tensor-cores-in-cuda-9,"Originally published at:			https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/
Tensor cores provide a huge boost to convolutions and matrix operations. Tensor cores are programmable using NVIDIA libraries and directly in CUDA C++ code. A defining feature of the new Volta GPU Architecture is its Tensor Cores, which give the Tesla V100 accelerator a peak throughput 12 times the 32-bit floating point throughput of the previous-generation…Tensor-Cores - will it also be available within the next Gamer-GPU (Geforce) ?That's a good question !interesting possibilities for crypto currencies using tensor cores...Which NN framework did you use for Figure4 inference?This will really help the 3D porn industry, oh and cancer research, possibly...When is ""Titan W"" coming out? You know, Two Titan-V's in one card?Are you waiting for ""Windows 4K"" to be made?I concur, I think I even know the theoretical solution: For each instance of CUDA algorithm line calculation, it gets stored in Matrix A until 16 instances are filled, then Stored into B, where their computation is multiplied which ought to lower the the net computation time by 4096 per tensor core utilized.... I've already seen this applied with the cryptonight algorithm, in terms of Hash Calculation however it was insufficient to generate a nonce. Create the nonce with that version of cryptonight and you revolutionize cryptomining!10 months later, the answer is yes.Extremely helpful. Thanks as always :)Question: Do the Tensor cores run concurrently with the CUDA cores?  If I were to have my deep learning model cranking away on TCs, could I simultaneously be rendering high quality graphics?Nice blog but it misses the most important statement - ""Tensor Cores require that the Tensors be in NHWC data layout."" So if NCHW is given, it transposes it to NHWC.Can you provide information about relative performance V100 / RTX 2080 TI or V100 / RTX 2080 ?Thank youGEMMs that do not satisfy the above rules will fall back to a non-Tensor Core implementationthis sounds like a silent failure, and a really bad thing
i assume hope theres some function to assert or check that TPUs are being used?I think there is a technical error in this image https://developer-blogs.nvidia.com/wp-content/uploads/2017/12/tensor_cube_white-624x934.png. There should be 16 green layers instead of 12. As tensor core only performing multiplication of a 4x4 data, as a result 16 -4x4 array will be generated.I have a little question for tensor core code example in blog.
Since the threadblock dim3 is (128, 4), namely 16 warps, I think the loop over k is should be 4xWMMA_K which would optimize the performance.
Would this change the loop stride affect the correctness of result ?Powered by Discourse, best viewed with JavaScript enabled"
687,improving-gpu-utilization-in-kubernetes,"Originally published at:			https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes/
To improve NVIDIA GPU utilization in K8s clusters, we offer new GPU time-slicing APIs, enabling multiple GPU-accelerated workloads to time-slice and run on a single NVIDIA GPU.Hi all,I was wondering if it it is possible to enable time-slicing after having installed the NVIDIA’s operator chart.I have a fully-working k8s cluster with GPUs and I prefer not to “break” it. So I am trying the following:I create a configmap containing the configuration:(the same .yaml you use in your example)And I upgrade the operator release (after having seen its Helm’s values structure to apply config):I can see the .yaml file mounted from the configmap in /available-configs both in config-manager and nvidia-device-plugin pods of the StatefulSet. However time-slicing configuration is not applied yet.I noticed nvcr.io/nvidia/k8s-device-plugin:v0.12.2-ubi8  image is used for those pods instead of just v0.12.2.Am I missing something? Is any other approach available for running environments?Many thanks in advance!SergioHi @sergio.garcia - thanks for reading the blog and your comment!The gpu-operator Helm chart provides a default value on the devicePlugin. To set a default config across the cluster, you would need to specify a parameter of  devicePlugin.config.default=<config-name> or in your case, devicePlugin.config.default=time-slicing. If no config is set as default, then node labeling is required so that those nodes get the new plugin configuration.Also - you can also file a GitHub issue on the gpu-operator project in the future at Issues · NVIDIA/gpu-operator · GitHub for questions or issues.Hi @P_Ramarao,Many thanks for your response. I’ve been on vacation and hadn’t been able to try it.I managed to enable time-slicing using Operator’s chart. However I think the devicePlugin.config.default value (that I was leaving blank) must include the actual name of the .yaml included in the ConfigMap (dp-example-config.yaml in my previous example). Don’t you agree?
In chart’s values this option is described as “# Default config name within the ConfigMap”.Best,SergioDoes Triton also provide the same oversubscription functionality as the device plugin? It is able to run multiple models on the same device concurrently, so seems quite similar. What are the differences between the two approaches and how to choose between them?Hi @P_Ramarao  is the replica in the sharing configuration of time-slicing split the GPU memory equally, i.e for a 16 GB GPU if I specify a replica of 2 does the memory get split into half like 8GB for a single subscription by a Pod.and if the replicas is 5 would the memory be split into 16\5 for each subscription of the GPU.
Would be really great if you could clear this out.
Thankshi @adnTriton also provide the same oversubscription functionality as the device plugin? It is able to run multiple models on the same device concurrentlyTriton does not use time-slicing for oversubscription. Triton does allow multiple models to be executed concurrently - but it uses the CUDA streams API to do so (i.e. each model is executed via a different CUDA stream concurrently on the GPU). We also detailed CUDA streams in the blog - so there are tradeoffs to using CUDA streams.Hope that helps.Hi @sam137is the replica in the sharing configuration of time-slicing split the GPU memory equallyNo - the time-slicing capability does not partition memory. Each process running on the GPU has full access to the GPU - only execution contexts are swapped in & out by the scheduler. We mention in the blog that the tradeoff with using time-slicing is that you (as the application developer or devops) needs to be sure that one of the process doesn’t end up allocating all the memory on the GPU (so the other process may suffer from an OOM).The time-slicing support in the device plugin simply provides an oversubscription model on the number of GPU devices available - so that two different containers can land on the same GPU and thus time-slice (from an execution perspective).Hope that clarifies.Thanks @P_Ramarao  that clears it.Powered by Discourse, best viewed with JavaScript enabled"
688,maximizing-hpc-cluster-ethernet-fabric-performance-with-mlag,"Originally published at:			https://developer.nvidia.com/blog/maximizing-hpc-cluster-ethernet-fabric-performance-with-mlag/
MLAG can help provide physical switch-level redundancy, avoid single-point failure, and maximize overall utilization of the total available bandwidth in your Ethernet fabric.Powered by Discourse, best viewed with JavaScript enabled"
689,develop-physics-informed-machine-learning-models-with-graph-neural-networks,"Originally published at:			https://developer.nvidia.com/blog/develop-physics-informed-machine-learning-models-with-graph-neural-networks/
Modulus 23.05 brings together new capabilities, empowering the research community and industries to develop research into enterprise-grade solutions through open-source collaboration.I would love to see if the PyTorch Modulus implementation of Meshgraphnets achieves the same accuracy as the original tensorflow implementation? Are there any public experiment records on how Modulus performed, for e.g. the cylinderflow example?Hello, and thanks for your comment! Unfortunately, we do not have any quantitative comparison between the Modulus implementation and the original TF implementation of MeshGraphNet yet.Powered by Discourse, best viewed with JavaScript enabled"
690,bring-on-your-questions-we-have-oli-wright-and-filip-strugar-live-answering-questions,"Another 15mins leftPowered by Discourse, best viewed with JavaScript enabled"
691,building-cloud-native-ai-powered-avatars-with-nvidia-omniverse-ace,"Originally published at:			Building Cloud-Native, AI-Powered Avatars with NVIDIA Omniverse ACE | NVIDIA Technical Blog
Explore the AI technology that powers Violet, the cloud-native interactive avatar showcased at GTC, along with new details about NVIDIA Omniverse ACE and NVIDIA Tokkio.Got a question about NVIDIA Omniverse ACE? Let us know below!Hi. How can I start to use NVIDIA Omniverse ACE?Thanks for your interest in NVIDIA Omniverse ACE! To learn more about the platform and apply for early access, visit: https://developer.nvidia.com/omniverse-platform/aceYou can also explore related NVIDIA AI technology including Omniverse Audio2Face(Omniverse Audio2Face AI Powered Application | NVIDIA) and Riva TTS(https://developer.nvidia.com/riva). These SDKs and tools are well-suited for avatar applications that don’t need to be optimized for the cloud and are a great way to start getting familiar with the technology.Hi thank you for the answer. I have submitted form to get the ACE early access. Status show that my application is being reviewed. I guess I need to wait until it get approved then after that I will able to try the platform.Hi! Can I deploy Violet showcase or maybe some other showcase, based on Omniverse ACE or Tokkio, to my environment, e.g. AWS?Powered by Discourse, best viewed with JavaScript enabled"
