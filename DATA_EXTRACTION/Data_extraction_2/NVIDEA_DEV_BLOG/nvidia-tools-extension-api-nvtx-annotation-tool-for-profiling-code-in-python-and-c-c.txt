As PyData leverages much of the static language world for speed including CUDA, we need tools which not only profile and measure across languages but also devices, CPU, and GPU.  While there are many great profiling tools within the Python ecosystem: line-profilers like cProfile and profilers which can observe code execution in C-extensions like PySpy/Viztracer.  None of the Python profilers can profile code running on the GPU.  Developers need tooling to help debug/profile and generally understand what is happening on the GPU from Python.  Fortunately, such tooling exists — NVIDIA Tools Extension (NVTX) and Nsight Systems together are powerful tools for visualizing CPU and GPU performance.NVTX is an annotation library for code in multiple languages, including Python, C, C++.  We can use Nsight Systems to trace standard Python functions, PyData libraries like Pandas/NumPy, and even the underlying C/C++ code of those same PyData libraries!  Nsight Systems also ships with additional hooks for CUDA to give developers insight to what is happening on the device (on the GPU).  This means we can “see” CUDA calls like cudaMalloc, cudaMemcpy, etc.NVTX is a code annotation tool and can be used to mark functions or chunks of code.  Python developers can either use decorators @nvtx.annotate()or a context manager with nvtx.annotate(..): to mark code to be measured.  For example:Annotating code by itself doesn’t achieve anything. To get information about the annotated code, we typically need to run it with a third-party application such as NVIDIA Nsight Systems. In the above, we’ve annotated a function, f, and a for loop.  We can then run the program with Nsight Systems CLI:The option -t nvtx,osrt defines what nsys should capture.  In this case, nvtx annotations and OS RunTime (OSRT) functions (read/select/etc).After both nsys and the python program finish, two files are generated: a qdrep file and a sqlite database.  We can visualize the timeline of the workflow by loading the qdrep file with Nsight Systems UI (available on all OSes)At first glance this isn’t the most exciting profile — the code itself is uncomplicated.  But unlike other profiling tools, this view is a timeline.  This means we can better evaluate end-to-end workflows and introspect code as the workflow proceeds and not just how much time total we speed in loop or f().  Though nsys also present that view as wellNVTX Push-Pop Range Statistics (nanoseconds)Like py-spy, NVTX can also profile across multiple processes and threads. We can then run the program with Nsight Systems CLI:In this example, we create two processes to create a large amount of data and compute the mean.  In the first process we build a 4096×4096 matrix of random data and in the second process, a 1024×1024 matrix of random data.  What we see in the visualization are two red bars (noting the data creation time and 3 blue bars (though admittedly the blue bar attached to the small red data generation task is hard to see).  We see two very informative things here which can be hard to capture in tools which only measure time spent:An Nsight Systems’ feature called “NVTX range protection allows user to annotate across devices, specifically NVIDIA GPUs.  This means we can develop a deep understanding of not just what is happening on the GPU itself (cudaMalloc, Compute Time, Synchronization), but also leverage the GPU from Python in a diverse, complicated, multi-threaded, multi-process, multi-library landscape.  Again, let’s ease our way in.We use CuPy, a NumPy-like library targeting GPUs, to operate on the data just as we do with NumPy data. The figure above captures how long we spend allocating data on the GPU.We can also see the execution of individual cupy kernels: cupy_random, gen_sequence, generate_seed as well as where and when the kernels are executed relative to other operations.The example below calculates the squared difference between two matrices and measures the time spent performing this calculation:With this information, we can now visually understand how and where the GPU spends time.  Next, we can ask, can we do better?  The answer is YES! cudaMalloc is a non-zero time operation and creating many GPU allocations can negatively impact performance.  Instead, we can use the RAPIDS RMM pool allocator which will create a large upfront GPU memory allocation and allow us to make sub-allocations at significantly faster speeds:For this particular operation, squared_difference, we can use cupy.fuse to optimize elementwise kernels.  This is a common GPU optimization.The annotations themselves have minimal overhead and only capture information when executed with nsys. Profiling code provides useful information to users and developers that is otherwise hidden and reveals much about where and why time is being spent.  When library authors choose to annotate with NVTX, users can develop a more complete understanding of end-to-end workflows  For example, with RAPIDS cuDF, a GPU Dataframe library, kernels will often need to allocate memory as part of group by-aggregation.  As we saw before using the RMM pool allocator will reduce the amount of time spent creating GPU memory:We can also quickly see why some equivalent operations are faster than others depending on the route taken. When performing several groupby-aggregations (groupby().mean(), groupby().count(), …), we can either perform each individually or use the agg operator. Using agg will only perform the groupby operation once.In the plot above we see more than the agg-all and split-agg annotations. We see the annotations inserted by cuDF developers and can clearly see one group-agg call for agg(["count", "mean", "sum"]) and three group-agg calls for count(), mean(), sum(). Currently, annotating a function within a package requires users to make modifications to that package, or convince the maintainer(s) to annotate parts of the package. It would be convenient to be able to annotate a function without modifying its source code. It’s not immediately clear how to achieve this.Another potentially useful feature would be to annotate execution threads. Currently, the different threads of execution are named Thread 0, Thread 1, etc., which may not be as useful. This is already possible in the C layer, but not exposed in the Python layer currently.While these are just a few thoughts on our minds, we would welcome suggestions from the community. We maintain a GitHub repo with the C, C++, and Python bindings and would welcome issues on the issue tracker. If there are things missing, we welcome PRs and are happy to review. Also, just drop by and say hi, we have a Slack channel as well.It’s easy to download NVTX using pip or conda:or:The examples from this blog post (and a few others) are available at this GitHub repository. 