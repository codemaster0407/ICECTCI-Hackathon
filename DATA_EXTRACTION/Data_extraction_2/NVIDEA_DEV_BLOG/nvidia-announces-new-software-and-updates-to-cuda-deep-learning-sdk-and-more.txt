At the GPU Technology Conference, NVIDIA announced new updates and software available to download for members of the NVIDIA Developer Program.CUDA 9.2 includes updates to libraries, a new library for accelerating custom linear-algebra algorithms, and lower kernel launch latency. With CUDA 9.2, you can:Additionally, CUDA 9.2 includes bug fixes and supports new operating systems and popular development tools.
Learn more >Deep learning frameworks using cuDNN 7 and later, can leverage new features and performance of the Volta architecture to deliver up to 3x faster training performance compared to Pascal GPUs.
cuDNN 7.1 highlights include:Download now >NVIDIA Collective Communications Library (NCCL) offers multi-GPU and multi-node collective communication primitives for HPC and deep learning applications used by leading deep learning frameworks such as Caffe2, Microsoft Cognitive Toolkit, MXNet, PyTorch and TensorFlow.
NCCL 2.2 delivers faster multi-GPU training of deep neural networks on such as ResNet50 and other larger networks, with aggregated inter-GPU reduction operations. NCCL 2.2 will be available in May.
Learn more >Today we announced TensorRT 4 with capabilities for accelerating popular inference applications such as speech, recommendation systems, natural language processing (NLP) and neural machine translation.
With TensorRT 4, you also get an easy import path for popular deep learning frameworks such as Caffe 2, MxNet, CNTK, PyTorch, Chainer through the ONNX format.Learn more >TensorFlow announced integration with NVIDIA’s TensorRT programmable inference accelerator solution. With TensorFlow 1.7, developers get the full power of NVIDIA GPUs while TensorRT developers get an easy way to use TensorRT within TensorFlow. TensorFlow integrated with TensorRT performs deep learning inference 8x faster under 7ms compared to inference in TensorFlow-only on GPUs.
* Min CPU latency measured was 70 ms. It is not < 7 ms. CPU: Skylake Gold 6140, 2.5GHz, Ubuntu 16.04; 18 CPU threads. Volta V100 SXM; CUDA (384.111; v9.0.176); Batch sizes: CPU=1, V100_FP32=2, V100_TensorFlow_TensorRT=16 w/ latency=6ms
 
Read more on the NVIDIA Developer Blog >
MathWorks, the makers of MATLAB, announced MATLAB and TensorRT integration through GPU Coder Toolbox. This helps engineers and scientists automatically generate CUDA code with TensorRT integration, from MATLAB code. MATLAB users can now automatically deploy high-performance inference applications for Jetson, DRIVE and Tesla platforms.Kubernetes on NVIDIA GPUs enables enterprises to scale up training and inference deployment to multi-cloud GPU clusters seamlessly.Kubernetes on NVIDIA GPUs will be available later this year. Please sign-up on the product page to be notified when it becomes available.
Learn more >Unveiled at the GPU Technology Conference, the Isaac SDK is a collection of libraries, drivers, APIs and other tools. The SDK will save manufacturers, researchers, startups and developers hundreds of hours by making it easy to add AI into next-generation robots for perception, navigation and manipulation.Developers can register for early access to the Isaac SDK product page.
Learn more >