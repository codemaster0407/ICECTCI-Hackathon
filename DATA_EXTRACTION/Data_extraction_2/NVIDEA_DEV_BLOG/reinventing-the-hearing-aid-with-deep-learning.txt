Researchers at Ohio State University developed a GPU-accelerated program that can isolate speech from background noise and automatically adjust the volumes of each separately.
Less than 25 percent of people who need a hearing aid actually use one – with the greatest frustration among users is the hearing aid cannot distinguish sounds that occur at the same time, such as someone talking and a car driving by. The device turns the volume up on both.
Using CUDA, TITAN X and cuDNN with the TensorFlow deep learning framework, the researchers trained their models to extract features that could distinguish voices from noise based on common changes in amplitude, frequency, and the modulations of each – 85 attributes were identified.
Clean Speech: To separate speech from noise, a deep learning program breaks a noisy speech sample into a collection of elements called time-frequency units. Next, it analyzes these units to extract 85 features known to distinguish speech from other sounds. Then, the program feeds the features into a deep neural network trained to classify the units as speech or not based on past experience with similar samples. Lastly, the program applies a digital filter that tosses out all the nonspeech units to leave only separated speech.
To test the program, the researchers asked 12 hearing-impaired people and 12 with normal hearing to listen through headphones to samples of noisy sentences. People in both grounds showed a huge improvement – several went from understanding only 10 percent of words to comprehending nearly 90 percent with the program. Even the people with normal hearing were able to better understand noisy sentences with the program.
The researchers mentioned the most intriguing result of the experiment was, could people with hearing impairment who are assisted by the deep learning-based program actually outperform those with normal hearing? And, the answer is yes.
Read more >