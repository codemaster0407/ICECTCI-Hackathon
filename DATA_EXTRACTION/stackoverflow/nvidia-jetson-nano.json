[
    {
        "question": "I want to display the stream from the camera to the screen using opencv Me device: Jetson-orin-nano, Arducam IMX477 When I run this code: cap = cv2.VideoCapture(0) while(cap.isOpened()): ret, frame = cap.read() cap.set(cv2.CAP_PROP_FRAME_WIDTH, 960) cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 540) cv2.imshow('frame', frame) cv2.imwrite('frame.jpg', frame) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break cap.release() cv2.destroyAllWindows() I got black screen all of the time When I run this one: cap = cv2.VideoCapture(\"nvarguscamerasrc ! video/x-raw(memory:NVMM),width=1920, height=1080, framerate=30/1 ! nvvidconv flip-method=0 ! nvegltransform ! nveglglessink\", cv2.CAP_GSTREAMER) while(cap.isOpened()): ret, frame = cap.read() cap.set(cv2.CAP_PROP_FRAME_WIDTH, 960) cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 540) cv2.imshow('frame', frame) cv2.imwrite('frame.jpg', frame) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break cap.release() cv2.destroyAllWindows() I got the error: cv2.error: OpenCV(4.8.0) /io/opencv/modules/highgui/src/window.cpp:971: error: (-215:Assertion failed) size.width&gt;0 &amp;&amp; size.height&gt;0 in function 'imshow' How can I resolve this problem?",
        "answers": [],
        "votes": []
    },
    {
        "question": "its my first time using ubuntu on jetson nano and i encountered a problem after installing opencv. After installing opnecv 4.5.5, i went straight to check if its installed successfully. the jtop command did display that I successfully installed my opencv, but when I run it on python3 command and used \"import cv2\", it displayed \"ModuleNotFoundError: No module named 'cv2'\" and when I run the command on a normal python, it displayed my opencv version. Is there any way to make python3 detect my opencv? Details: ubuntu 18.04 jetson nano jetpack 4.6 opencv 4.5.5 i have 2 pythons, the python 2.7 default of nano &amp; python 3.6 i did try to change my default python version, and it didnt solved the problem. i used the (sudo update-alternatives --config python3) command to change the default of my python.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I recently got a Jetson Nano Developer Kit (P/N945-13450-0000-100). On this I would like to install the new Jetpack 5.1.1. Before doing this I had already installed the version 4.6.1. as a test. (jetson-nano-jp461-sd-card-image). That worked flawlessly. Next try with the (supposed) image of version 5.1.1. (jetson-nx-jp51-sd-card-image) - without success. The SD card created with this image does not the system to boot. Well, the file name suggests that this image is not intended for the Jetson Nano Developer Kit. The filename should then rather be jetson-nano-jp51-sd-card-image. A search in the WWW for such a file remains unfortunately unsuccessful. Now my questions: is there a Jetpack 5.1.1. available for the Jetson Nano Developer Kit? And if yes, where would it be found? or is the kit already EOL and no support in the form of further Jetpacks will be provided?",
        "answers": [],
        "votes": []
    },
    {
        "question": "What is the best practice and an example of how to call functions at a specific rate? At work I write code in functions that get called at 100ms or 50ms or whatever rate is needed, but I don't know how to do the low level code that makes it possible for me to just call a function at a specific rate. I am writing my code in c++ on the Jetson-Nano. I also want to be able to do the same thing for code that runs on my laptop as well (needs to have the same execution speed as the code I'm running on the nano). In my research I saw different methods using delays and interrupts, but I'm a novice at this so I wanted to make sure that I choose the best approach. Best meaning, the most professional, efficient, effective, and whatever other good adjectives fit.",
        "answers": [
            [
                "Given that the Jetson Nano runs Jetson Linux the question is not really specific to embedded systems. There is no \"one method\" used in embedded systems in any event. A simple generic solution is simply to have a continuous loop, that invokes functions when it is time to do so. You might have a table of functions and periods and implement a simple scheduler: struct { void (*periodic_function)(void) ; int period_ms ; int last_exec_time ; } tasks[] = { {task1, 50, 0 }, {task2, 100, 0 }, {task3, 10, 0 } } ; constexpr std::size_t TASK_COUNT = sizeof(tasks)/sizeof(*tasks) ; for(;;) { int now_ms = clock_ms() ; for( int t = 0; t &lt; TASK_COUNT; t++ ) { if( now - tasks[t].last_exec_time &gt;= tasks[t].period_ms ) { tasks[t].last_exec_time = now ; tasks[t].periodic_function() ; } } sleep(1) ; } Where clock_ms might be implemented in various ways, e.g: #include time.h int clock_ms() { return std::clock() / (CLOCKS_PER_SEC / 1000 ) ; } Alternatively you could simply create separate threads for each period, crudely: void periodicThread50ms() { for(;;) { periodic_function_50ms() ; sleep( 50 ) ; } } A direct solution for periodic functions given a POSIX API would be to use a POSIX timer with SIGEV_THREAD to trigger run-to-completion functions periodically. e.g: #include &lt;time.h&gt; #include &lt;signal.h&gt; void handler50ms(union sigval sv) { ... } void handler75ms(union sigval sv) { ... } int main() { timer_t timerid50ms = {0} ; sigevent event50ms = {0} ; event50ms.sigev_notify = SIGEV_THREAD ; event50ms.sigev_notify_function = handler50ms ; timer_create(CLOCK_MONOTONIC, &amp;event50ms, &amp;timerid50ms) ; itimerspec period50ms = {0} ; period50ms.it_value.tv_nsec = 50000000 ; period50ms.it_interval.tv_nsec = period50ms.it_value.tv_nsec ; timer_settime( timerid50ms, 0, &amp;period50ms, nullptr ) ; timer_t timerid75ms = {0} ; sigevent event75ms = {0} ; event75ms.sigev_notify = SIGEV_THREAD ; event75ms.sigev_notify_function = handler75ms ; timer_create(CLOCK_MONOTONIC, &amp;event75ms, &amp;timerid75ms) ; itimerspec period75ms = {0} ; period75ms.it_value.tv_nsec = 75000000 ; period75ms.it_interval.tv_nsec = period75ms.it_value.tv_nsec ; timer_settime( timerid75ms, 0, &amp;period75ms, nullptr ) ; for(;;) { // do other stuff here sleep(1) ; } return 0 ; } The POSIX timer functionality is flexible, but perhaps somewhat cumbersome. A wrapper or facade providing just the functionality you require is a good idea. For example you might define: timer_t startTimer( int period_ms, void (*handler)(sigval), sigval handler_arg ) { timer_t timerid = 0 ; sigevent event = {0} ; event.sigev_notify = SIGEV_THREAD ; event.sigev_notify_function = handler ; event.sigev_value = handler_arg ; timer_create(CLOCK_MONOTONIC, &amp;event, &amp;timerid) ; itimerspec period = {0} ; time_t seconds = period_ms / 1000 ; long nanoseconds = (period_ms % 1000) * 1000000 ; period.it_value.tv_sec = seconds ; period.it_value.tv_nsec = nanoseconds ; period.it_interval.tv_sec = seconds ; period.it_interval.tv_nsec = nanoseconds ; timer_settime( timerid, 0, &amp;period, nullptr ) ; return timerid ; } Then the previous example main() would look like: int main() { sigval arg ; arg.sival_int = 50 ; startTimer( 50, handler50ms, arg ) ; arg.sival_int = 75 ; startTimer( 75, handler75ms, arg ) ; for(;;) { // do other stuff here sleep(1) ; } return 0 ; } The handler_arg parameter is passed to the timer handler and may be used to pass information to the handler. Because it is a union with a pointer value, you can pass anything your application needs. startTimer() returns the timer ID allowing you to manipulate the timer. For example you might want to stop a timer: int stopTimer( timer_t timer_id ) { return timer_delete( timer_id ) ; }"
            ],
            [
                "It somewhat depends on the complexity of the project you're working on. If there are only a few functions you're calling, and they're all relatively quick to run (compared to the frequency you're calling them at), then interrupts are a clean, simple way to accomplish this. If your project is more complex \u2014 lots of tasks with varying priority, say, then you'd be better off using an RTOS (real-time operating system). Don't be put off by the word \"operating system\", thinking something like Linux or Windows. An RTOS is (generally) a very tiny, simple kernel to handle sharing processor time between tasks and guaranteeing consistent timing."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "Have been spending many hours on getting this working with the newest python on Jetson, default it comes with Python 3.6.9 (Jetpack 4.6.4). So I compiled Python 3.11 and registered it. It works. NOTE: pip install opencv-python is NOT the solution I am looking for, since that does not come with CUDA optimalisation. jtop says: \"It is installed\" PROBLEM: Python 3.11 import cv2 says: \"No Module named cv2\" while these did install according to \"make install\" results. - Up-to-date: /usr/lib/python3.11/site-packages/cv2/__init__.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/load_config_py2.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/load_config_py3.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/config.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/misc/__init__.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/misc/version.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/mat_wrapper/__init__.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/utils/__init__.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/gapi/__init__.py -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/python-3.11/cv2.cpython-311-aarch64-linux-gnu.so -- Up-to-date: /usr/lib/python3.11/site-packages/cv2/config-3.11.py ERror we get... Python 3.11.3 (tags/debian/3.11.3-1+bionic2-dirty:95ae635, Jun 5 2023, 15:54:34) [GCC 7.5.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import cv2 Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; ModuleNotFoundError: No module named 'cv2' opencv cmake with the recommended options for Jetson says this (among others) - NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH) -- NVIDIA GPU arch: 53 -- NVIDIA PTX archs: -- -- cuDNN: YES (ver 8.2.1) -- -- Python 3: -- Interpreter: /usr/bin/python3.11 (ver 3.11.3) -- Libraries: /usr/lib/aarch64-linux-gnu/libpython3.11.so (ver 3.11.3) -- numpy: /usr/local/lib/python3.11/dist-packages/numpy/core/include (ver 1.25.0) -- install path: /usr/lib/python3.11/site-packages/cv2/python-3.11 -- -- Python (for build): /usr/bin/python2.7",
        "answers": [
            [
                "The solution was that site-packages (some-how) is being ignored by python. When I changed the argument to dist-packages it works. cmake ...... (skip for brevity) -D PYTHON3_PACKAGES_PATH=/usr/local/lib/python3.11/dist-packages \\ now cv2 loads happily."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a Jetson Nano and I'm going to build a mobile robot. I want to control the wheels using PWM signals, use a Lidar sensor to scan the area, and use OpenCV to have an AI running that can track and follow me. My question is, how can I perform all these tasks (almost) simultaneously, reducing the latency between processes and making it as optimized as possible? I'm not asking for code, just a guide on how I could approach this, what paradigm to use, whether to use asynchronous processes or multiprocessing, things like that. Thank you for the help.",
        "answers": [
            [
                "I think that you should look at async libraries at python. Such as asyncio, and trynna to built a code with it. Or multithreading libraries :D"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to flash my jetson nano after I forgot the password to the account. I am referring to this websitetext I am using a Mac OS and when I try to follow the sdkmanager method, I am getting this error and I dont know how to get past it; zsh: command not found: sdkmanager I then tried the command line method. After I got to the point where I unzip all the necessary packages, I run the script sudo ./apply_binaries.sh But it is giving me this error Use: apply_binaries.sh [--bsp|-b PATH] [--root|-r PATH] [--target-overlay] [--help|-h] This script installs tegra binaries Options are: --bsp|-b PATH bsp location (bsp, readme, installer) --root|-r PATH install toolchain to PATH --target-overlay|-t untar NVIDIA target overlay (.tbz2) instead of pre-installing them as Debian packages --help|-h show this help I used the method of setting the environmental variable for this but it still showed this message. For the SDKmanager method, I tried setting up the path to where my .deb file is stored but it showed the same error. For the command line method, I tried setting the environmental variable LDK_ROOTFS_DIR but it still showed this message.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Before I state the issue I want to introduce the YOLOP model from pytorch see on Github The authors used Tensorrt to inference the model on Nvidia Jetson TX2. In the source code,they have provided a code for deployment and reasoning of model in ./toolkits/deploy. In my project, I wanted to deploy it in jetson nano. I have successfully generated the build files based from the instruction from github: cd toolkits/deploy mkdir build &amp;&amp; cd build cmake .. Output: -- The C compiler identification is GNU 7.5.0 -- The CXX compiler identification is GNU 7.5.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Found CUDA: /usr/local/cuda-10.2 (found version \"10.2\") -- Found OpenCV: /usr (found version \"4.1.1\") -- Configuring done -- Generating done -- Build files have been written to: /home/nvidia/Desktop/YOLOP/toolkits/deploy/build But when after the make command it outputs the errors it so long but I can provide the beginning and end of the error: nvidia@ubuntu:~/Desktop/YOLOP/toolkits/deploy/build$ make [ 25%] Building NVCC (Device) object CMakeFiles/myplugins.dir/myplugins_generated_yololayer.cu.o /home/nvidia/Desktop/YOLOP/toolkits/deploy/yololayer.h(60): error: member function declared with \"override\" does not override a base class member /home/nvidia/Desktop/YOLOP/toolkits/deploy/yololayer.h(60): warning: function \"nvinfer1::IPluginV2::enqueue(int32_t, const void *const *, void *const *, void *, cudaStream_t)\" is hidden by \"nvinfer1::YoloLayerPlugin::enqueue\" -- virtual function override intended? /home/nvidia/Desktop/YOLOP/toolkits/deploy/yololayer.h(91): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int32_t, const nvinfer1::Dims *, int32_t, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int32_t)\" is hidden by \"nvinfer1::YoloLayerPlugin::configurePlugin\" -- virtual function override intended? /home/nvidia/Desktop/YOLOP/toolkits/deploy/yololayer.h(93): error: exception specification for virtual function \"nvinfer1::YoloLayerPlugin::detachFromContext\" is incompatible with that of overridden function \"nvinfer1::IPluginV2Ext::detachFromContext\" /home/nvidia/Desktop/YOLOP/toolkits/deploy/yololayer.h(91): error: exception specification for virtual function \"nvinfer1::YoloLayerPlugin::configurePlugin\" is incompatible with that of overridden function \"nvinfer1::IPluginV2IOExt::configurePlugin(const nvinfer1::PluginTensorDesc *, int32_t, const nvinfer1::PluginTensorDesc *, int32_t)\" make[2]: *** [CMakeFiles/myplugins.dir/myplugins_generated_yololayer.cu.o] Error 1 CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/myplugins.dir/all' failed make[1]: *** [CMakeFiles/myplugins.dir/all] Error 2 Makefile:83: recipe for target 'all' failed make: *** [all] Error 2 Solution(s) I tried: -checked if packages needed are installed like ZED, Cuda, and libnvinfer. -tried to modify the yololayer.h and .cu Still it gives the error. Any comments, advice, and suggestions are great help.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a Jetson Nano 4GB (B01) SOC board and I am running out of space to install the DeepStream SDK. I want to use DeepStream for video streaming application on my Jetson Nano SOC board, but I need guidance on how to install it . What are the steps to install DeepStream SDK on the Jetson Nano?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I just received my IMX477, which I ordered for good resolution. I am able to get feed through gstreamer from the pipeline, but I don\u2019t know what it is showing. Also, the camera is working and I want to use it as a normal camera. terminal output Camera Output Pixel format is RG10 v4l2-ctl --list-formats-ext ioctl: VIDIOC_ENUM_FMT Index : 0 Type : Video Capture Pixel Format: 'RG10' Name : 10-bit Bayer RGRG/GBGB Size: Discrete 4032x3040 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 3840x2160 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 1920x1080 Interval: Discrete 0.017s (60.000 fps) videoconvert does not support RG10 dmesg kernel logs dmesg | grep imx477 [ 1.335158] imx477 7-001a: tegracam sensor driver:imx477_v2.0.6 [ 1.635508] imx477 7-001a: imx477_board_setup: error during i2c read probe (-121) [ 1.643134] imx477 7-001a: board setup failed [ 1.647608] imx477: probe of 7-001a failed with error -121 [ 1.647995] imx477 8-001a: tegracam sensor driver:imx477_v2.0.6 [ 2.224479] vi 54080000.vi: subdev imx477 8-001a bound What should I do? I just want to use it as a normal camera. No special purpose!! How can I use it as an ordinary camera?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to configure a GPIO pin on an SoC running Linux (Jetson Orin Nano), but there's no API provided to set things like pull resistors, so we have to look up the physical addresses of the control registers and write to them directly. In order to do this, we can use busybox to read and write memory-mapped I/O. The first thing I did was try out some examples. According to \"https://docs.nvidia.com/jetson/archives/r35.3.1/DeveloperGuide/text/HR/JetsonModuleAdaptationAndBringUp/JetsonAgxOrinSeries.html?highlight=switch#identifying-the-gpio-number\", I should be able to run the command \"busybox devmem 0x02430070\" in order to read the configuration state of pin \"MCLK05: SOC_GPIO33: PQ6\". Unfortunately, it reads only 0xffffffff. In fact, for absolutely any address I provide, it always reads back 0xffffffff. I even tried writing my own program that mmaps /dev/mem, but I get exactly the same result. The mmap call succeeds (it would fail if I were not running as root), but accessing memory fails. I checked dmesg, and whenever I try to perform any access to memory mapped through /dev/mem, I get a kernel error like this: [98089.048787] CPU:0, Error: cbb-fabric@0x13a00000, irq=25 [98089.054186] ************************************** [98089.059112] CPU:0, Error:cbb-fabric, Errmon:2 [98089.063596] Error Code : TIMEOUT_ERR [98089.069159] Error Code : TIMEOUT_ERR [98089.073188] MASTER_ID : CCPLEX [98089.076684] Address : 0x2340068 [98089.080275] Cache : 0x0 -- Device Non-Bufferable [98089.085466] Protection : 0x2 -- Unprivileged, Non-Secure, Data Access [98089.092444] Access_Type : Read [98089.095933] Access_ID : 0x17 [98089.095934] Fabric : cbb-fabric [98089.102828] Slave_Id : 0x35 [98089.106051] Burst_length : 0x0 [98089.109539] Burst_type : 0x1 [98089.112854] Beat_size : 0x2 [98089.116072] VQC : 0x0 [98089.118848] GRPSEC : 0x7e [98089.121895] FALCONSEC : 0x0 [98089.125114] ************************************** [98089.130151] ------------[ cut here ]------------ [98089.130170] WARNING: CPU: 0 PID: 13354 at drivers/soc/tegra/cbb/tegra234-cbb.c:577 tegra234_cbb_isr+0x130/0x170 [98089.140534] Modules linked in: fuse nvidia_modeset(O) xt_conntrack xt_MASQUERADE nf_conntrack_netlink nfnetlink xt_addrtype iptable_filter iptable_nat nf_nat nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 libcrc32c br_netfilter lzo_rle lzo_compress zram overlay ramoops reed_solomon bnep snd_soc_tegra186_asrc snd_soc_tegra210_iqc aes_ce_blk snd_soc_tegra186_dspk snd_soc_tegra210_ope snd_soc_tegra186_arad crypto_simd snd_soc_tegra210_mvc cryptd snd_soc_tegra210_dmic rtk_btusb snd_soc_tegra210_afc aes_ce_cipher snd_soc_tegra210_amx snd_soc_tegra210_adx ghash_ce btusb sha2_ce btrtl snd_soc_tegra210_mixer snd_soc_tegra210_i2s snd_soc_tegra210_admaif sha256_arm64 rtl8822ce snd_soc_tegra210_adsp snd_soc_tegra210_sfc btbcm sha1_ce snd_soc_tegra_pcm snd_soc_tegra_machine_driver btintel r8168 snd_soc_tegra_utils fusb301 snd_hda_codec_hdmi snd_soc_simple_card_utils snd_soc_spdif_tx r8169 snd_hda_tegra cfg80211 snd_soc_tegra210_ahub nvadsp realtek snd_hda_codec tegra210_adma userspace_alert [98089.140661] tegra_bpmp_thermal snd_hda_core nv_imx219 nvidia(O) spi_tegra114 binfmt_misc loop ina3221 pwm_fan nvgpu nvmap spidev ip_tables x_tables [last unloaded: mtd] [98089.140696] CPU: 0 PID: 13354 Comm: busybox Tainted: G W O 5.10.104-tegra #1 [98089.140698] Hardware name: Unknown NVIDIA Orin NX Developer Kit/NVIDIA Orin NX Developer Kit, BIOS 3.0-32616947 02/21/2023 [98089.140701] pstate: 60400089 (nZCv daIf +PAN -UAO -TCO BTYPE=--) [98089.140704] pc : tegra234_cbb_isr+0x130/0x170 [98089.140706] lr : tegra234_cbb_isr+0x10c/0x170 [98089.140707] sp : ffff800010003e10 [98089.140708] x29: ffff800010003e10 x28: ffff20b08026ab80 [98089.140711] x27: 0000000000000001 x26: 0000000000000080 [98089.140714] x25: ffffa2a2d36f9ed0 x24: ffffa2a2d404be40 [98089.140717] x23: ffffa2a2d39e7000 x22: 0000000000000019 [98089.140720] x21: ffffa2a2d3e6ef20 x20: 0000000000000002 [98089.140723] x19: ffffa2a2d3e6ef10 x18: 0000000000000010 [98089.140726] x17: 0000000000000000 x16: ffffa2a2d1fe3210 [98089.140728] x15: ffff20b08026b0f0 x14: ffffffffffffffff [98089.140731] x13: ffff800090003917 x12: ffff80001000391f [98089.140734] x11: 0101010101010101 x10: 7f7f7f7f7f7f7f7f [98089.140736] x9 : ffff800010003c30 x8 : 2a2a2a2a2a2a2a2a [98089.140739] x7 : 2a2a2a2a2a2a2a09 x6 : c0000000ffffefff [98089.140742] x5 : ffff20b1b1bb2958 x4 : ffffa2a2d3cf7968 [98089.140744] x3 : 0000000000000000 x2 : ffffa2a2d217e170 [98089.140747] x1 : ffff20b08026ab80 x0 : 0000000100010000 [98089.140751] Call trace: [98089.140753] tegra234_cbb_isr+0x130/0x170 [98089.140761] __handle_irq_event_percpu+0x68/0x2a0 [98089.140763] handle_irq_event_percpu+0x40/0xa0 [98089.140766] handle_irq_event+0x50/0xf0 [98089.140768] handle_fasteoi_irq+0xc0/0x170 [98089.140772] generic_handle_irq+0x40/0x60 [98089.140775] __handle_domain_irq+0x70/0xd0 [98089.140778] gic_handle_irq+0x68/0x134 [98089.140779] el1_irq+0xd0/0x180 [98089.140787] el0_ia+0x50/0xb0 [98089.140788] el0_sync_handler+0x90/0xb0 [98089.140790] el0_sync+0x16c/0x180 [98089.140793] ---[ end trace 71009e79516be4c4 ]--- For reference, I have posted to Nvidia's developer forum at \"https://forums.developer.nvidia.com/t/trying-to-configure-gpio-pin-direction-busybox-devmem-returns-0xffffffff-for-all-addresses/256819\", but I have a feeling that this is well outside of the sort of thing I'm going to get help on there. If anyone can give me some clues to make headway on this, that would be fantastic. Thanks!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am designing a vehicle tracking program with Deepstream SDK 6.0.1. I want to use YoloV8 as my inference model but require the cfg and weights files for that model. When you calibrate your inference model, you can specify which inference model you want to use, by specifying the location of the relevant cfg, and weights files. It is quite easy to find the pre-trained cfg files and weights of Yolov3, but I have been trying to find the weights and cfg files for other Yolo models but could not find any...I have tried to follow the steps from this website: https://wiki.seeedstudio.com/YOLOv8-DeepStream-TRT-Jetson/ , but I receive the error message 'Illegal instruction (code dumped)' when I try to use the pytorch and torchvision packages (I am using the correct versions, and system architecturs of these packages). I then implemented the steps described on the website on my personal computer (windows 11 OS), and got it running, but the python script that they specify on the website (gen_wts_yoloV8.py), is not located in the DeepsteamYolo file that had to be downloaded. The other python script that is located in the file (\"export_yoloV8.py\"), exports a onnx file, which is also not exactly what I want (except if someone know if this can be converted to a cfg and weight files). Does someone know where I could find cfg files and weights for Yolov8, or if it is possible to generate these files? I am programming on a Jetson nano, with Ubuntu 18.04 and have installed the Jetpack 4.6.1 SDK on the SD.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I apologize for the confusion. Here's the rephrased paragraph in English: I have set up a private blockchain on my PC by following the tutorial at https://docs.goquorum.consensys.net/tutorials/private-network/create-qbft-network. Now, I want to add a new node to my Quorum blockchain using an Nvidia Jetson Nano 2GB. I would like the Jetson Nano 2GB to function as a Quorum node. My objective is to send data from sensors to Quorum, and ensure that all nodes can access and view the sensor values. I followed the tutorial mentioned at https://docs.goquorum.consensys.net/tutorials/private-network/adding-removing-qbft-validators, but I encountered issues with it. I'm confused about why the tutorial suggests using the command npx quorum-genesis-tool --validators 1 --members 0 --bootnodes 0 --outputPath artifacts when I already have a genesis file on another machine. My intention is to initialize my node using the existing genesis file and configure it accordingly.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using a Jetson Nano production board to capture a live feed from a camera. If someone wants to see this live feed from the Jetson Nano production board to different devices . How can I show this live feed to different devices such as desktop computers or mobile devices. So, What is the recommended method or protocol to stream the live feed from the Jetson Nano to different devices? Are there any specific libraries, frameworks, or tools that are commonly used for live streaming from a Jetson Nano?",
        "answers": [],
        "votes": []
    },
    {
        "question": "We would like to integrate Raspberry Pi v2 CSI camera 8MP with Jetson Nano with carrier board. The end user has below requirements: Able to see the live camera feed from desktop and mobile devices without any lag. Able to run the AI inference on the live feed for object detection or classification. From hardware point of view, how are we going to support the live feed? Is it through HDMI, ethernet cable, Wifi , web server etc or through g-streamer library?",
        "answers": [
            [
                "Stack exchange is for software. You might find someone able to help you out but you're better off checking out some of the other stack exchanges e.g. https://raspberrypi.stackexchange.com/ or maybe https://hardwarerecs.stackexchange.com/"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am using the Jetson nano GPIO to read the output of an AB bi-phase rotational encoder encode, I wrote a simple python script to trigger a rising edge interrupt, the callback function prints the current state of the pin that triggered. import time import Jetson.GPIO as GPIO # Define the GPIO pins for the rotary encoders PIN = 13 # Initialize GPIO GPIO.setmode(GPIO.BOARD) GPIO.setup(PIN, GPIO.IN) def callback(PIN): print(GPIO.input(PIN)) # Add event detection for rising edges on all encoder pins GPIO.add_event_detect(PIN, GPIO.RISING, callback=callback, bouncetime=1) try: while True: pass except KeyboardInterrupt: pass # Clean up GPIO GPIO.cleanup() After a lot of debugging and many many tries, I stopped using the actual encoder, I'm using a function generator set to output a 3.3v 500hz square wave on pin 13 on the Jetson nano. The output I expect should a stream of ones, but the actual output is random mix of ones and zeros. The actual output from the terminal: 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 What am I missing or doing wrong. This my system: L4T 32.6.1 JetPack 4.6 Ubuntu 18.04.6 LTS Kernel Version: 4.9.253-tegra",
        "answers": [],
        "votes": []
    },
    {
        "question": "development environment OS : Ubuntu 18.04.5 LTS (GNU/Linux 4.9.201-tegra aarch64) Language : Python 3.6 Device: Jetson nano, Maxim MAX35104 EV Kit Issue A timeout occurs while reading HID data. In Python, the first device information is brought, but there is a problem that the data is not received. Code import usb.core vendor_id = 0x04fa product_id = 0x3900 device = usb.core.find(idVendor=vendor_id, idProduct=product_id) if device is None: raise ValueError(\"Not Found USB\") for cfg in device: print(f\"Configuration: {cfg}\") for intf in cfg: print(f\"Interface: {intf}\") for ep in intf: print(f\"Endpoint: {ep}\") device.set_configuration() def read_data(): endpoint_address = 0x81 size = 64 timeout = 1000 data = device.read(endpoint_address, size, timeout) data = read_data() print(data) what i checked The corresponding device is confirmed in dmesg, and the information of the corresponding device can be read in the Python code. dmesg python Program Reconnect the USB device Restart the USB device Run the program with sudo Change Timeout to 3 seconds",
        "answers": [
            [
                "Changes from previous issue By modifying the code, Timeout does not appear. However, all the data comes in as 0. The endpoint out is set to 0x1 and the length is set to 64 bytes. Data is received, but there is no change in the data for the sensor. Device information DEVICE ID 04fa:3900 on Bus 001 Address 008 ================= bLength : 0x12 (18 bytes) bDescriptorType : 0x1 Device bcdUSB : 0x200 USB 2.0 bDeviceClass : 0x0 Specified at interface bDeviceSubClass : 0x0 bDeviceProtocol : 0x0 bMaxPacketSize0 : 0x8 (8 bytes) idVendor : 0x04fa idProduct : 0x3900 bcdDevice : 0x2 Device 0.02 iManufacturer : 0x1 Maxim Integrated iProduct : 0x2 DS3900 Module HIDClass iSerialNumber : 0x0 bNumConfigurations : 0x1 CONFIGURATION 1: 100 mA ================================== bLength : 0x9 (9 bytes) bDescriptorType : 0x2 Configuration wTotalLength : 0x29 (41 bytes) bNumInterfaces : 0x1 bConfigurationValue : 0x1 iConfiguration : 0x0 bmAttributes : 0xc0 Self Powered bMaxPower : 0x32 (100 mA) INTERFACE 0: Human Interface Device ==================== [15/521] bLength : 0x9 (9 bytes) bDescriptorType : 0x4 Interface bInterfaceNumber : 0x0 bAlternateSetting : 0x0 bNumEndpoints : 0x2 bInterfaceClass : 0x3 Human Interface Device bInterfaceSubClass : 0x0 bInterfaceProtocol : 0x0 iInterface : 0x0 ENDPOINT 0x81: Interrupt IN ========================== bLength : 0x7 (7 bytes) bDescriptorType : 0x5 Endpoint bEndpointAddress : 0x81 IN bmAttributes : 0x3 Interrupt wMaxPacketSize : 0x40 (64 bytes) bInterval : 0x1 ENDPOINT 0x1: Interrupt OUT ========================== bLength : 0x7 (7 bytes) bDescriptorType : 0x5 Endpoint bEndpointAddress : 0x1 OUT bmAttributes : 0x3 Interrupt wMaxPacketSize : 0x40 (64 bytes) bInterval : 0x1 Modified code import usb.core import usb.util vendor_id = 0x04fa product_id = 0x3900 device = usb.core.find(idVendor=vendor_id, idProduct=product_id) if device is None: raise ValueError(\"Not Found USB\") interface = 0 endpoint_in = 0x81 endpoint_out = 0x1 device.set_configuration() try: while True: try: data = device.read(endpoint_out, 64, timeout=1000) print(\"Received data:\", data) print(\"bytes data:\" , bytes(data)) except usb.core.USBError as e: if e.errno == 110: # (110: Operation timed out) print(\"Time Out\") continue else: raise finally: usb.util.dispose_resources(device) Data output bytes data: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00' Received data: array('B', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) bytes data: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00' Received data: array('B', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) bytes data: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Closed. This question is not about programming or software development. It is not currently accepting answers. This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered. Closed last month. Improve this question I'm developing a Jetson-based HUD that reboots very quickly. During the reboot process, I noticed that it takes about 10 seconds (relatively long) just to shut down the OS when using the \"reboot\" command. Since I need an OS that can reboot within approximately 10 seconds, achieving my goal is not possible with a 10-second shutdown alone. Therefore, instead of the reboot command, I tried rebooting using the command sudo echo b &gt; /proc/sysrq-trigger, which immediately performs the reboot without any noticeable processing time. Based on my research, I found that this method does not involve synchronizing the file system or unmounting it. Could implementing the OS reboot functionality in this way pose a risk of serious errors or bugs in the system? Would using the sudo echo b &gt; /proc/sysrq-trigger command periodically cause significant issues for the system?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using BalenaEtcher to write the Jetson Nano image to Pendrive. But I am getting the following error and the flash is failed: Something went wrong while writing JetsonNano.img.xz to VendorCo ProductCode USB Device (\\\\.\\PhysicalDrive1) I am getting JetsonNano.img.xz from this link. How can I solve this error? The point to be noted here is that my Jetson Nano does not support an SD card and I have to use pendrive for storage expansion.",
        "answers": [],
        "votes": []
    },
    {
        "question": "ENVIRON: jetson nano 4g docker image: nvcr.io/nvidia/l4t-ml:r32.7.1-py3 here\u2019s my callback code, it's a main function. pad_img = np.load('./data_640x384_batch1_nonorm.npy') def my_callback(): # cv2.imread(\"./00041.jpg\") astart = time.time() runner._infer(pad_img) aend = time.time() runner_time = aend - astart print(\"runner time is %.3f\"%(runner_time)) return the runner time is 0.014 and when comment out the cv2.imread(\"./00041.jpg\"), the runner time will be 0.026, it is realy weird, hope someone can figure me out. the complete code is put here What I HAVE DONE there is no bug in the class RUNNER, no need to pay too much attention on it. And when I put load and infer process in a main function like following, it will not effect the infer time. if __name__ == '__main__': batch_size = 1 execution_time = 400 engine_path = args.engine_path engine = _get_engine(engine_path) context = RUNNER(engine, batch_size) temp = cv2.imread('./00041.jpg') temp = cv2.cvtColor(temp, cv2.COLOR_BGR2RGB) img = np.load('./data_640x384_batch1_nonorm.npy') print(\"start speed test\") for _ in range(int(execution_time/batch_size)): astart = time.time() result = context._infer(img) aend = time.time() print(aend-astart) In addition, when I use sudo jetson_clocks, it also does not effect the infer time. But I wanna why and sudo jetson_clocks is not reconmended in my project. upgrade opencv is also not allowed.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to run a python script on boot in a jetson nano. However, I think my script is not running for some reasons and I have no idea what is going on. Can any master help me out? I am trying to make a python script to run on boot on a jetson nano. Here is my python script and my service: startup.service: [Unit] Description = INTU_IPC start-uo specific script [Service] Type= idle ExecStartPre = /bin/sleep 10 ExecStart = /usr/local/bin/startup.sh User=jetbot [Install] WantedBy = multi-user.target startup.sh: #! /bin/sh sleep 10 OPENBLAS_CORETYPE=ARMV8 /usr/bin/python3 ~/py_basics/helloworld.py helloworld.py: #!/usr/bin/env python3 print(\"Hello World!\") I used the following command in terminal to enable my startup.service: sudo systemctl enable startup.service sudo systemctl start startup.service However, when I try to restart my jetson nano, and I tracked the activities of my service with the systemclt status startup.service in the terminal, I think the following lines showed my service is not running. jetbot@jetson-4-3:~$ systemctl status startup.service \u25cf startup.service - INTU_IPC start-uo specific script Loaded: loaded (/etc/systemd/system/startup.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Wed 2023-05-24 18:40:13 PDT; 1min 3s ago Process: 6345 ExecStart=/usr/local/bin/startup.sh (code=exited, status=203/EXEC) Process: 4094 ExecStartPre=/bin/sleep 10 (code=exited, status=0/SUCCESS) Main PID: 6345 (code=exited, status=203/EXEC) May 24 18:39:57 jetson-4-3 systemd[1]: Started INTU_IPC start-uo specific script. May 24 18:40:13 jetson-4-3 systemd[6345]: startup.service: Failed to execute command: Permission denied May 24 18:40:13 jetson-4-3 systemd[6345]: startup.service: Failed at step EXEC spawning /usr/local/bin/startup.sh: Permission denied May 24 18:40:13 jetson-4-3 systemd[1]: startup.service: Main process exited, code=exited, status=203/EXEC May 24 18:40:13 jetson-4-3 systemd[1]: startup.service: Failed with result 'exit-code'. Can anyone help me with this?",
        "answers": [
            [
                "Thank you for the help. I found out that is was some permission problems which I solved it with by changing the permission of the startup.sh with this code executed in the terminal: sudo chmod +x /usr/local/bin/startup.sh However, there are still some problems in importing libraries and modules while running python scripts on jetson nano."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I've trained the EfficientDet D1 model in google collab. but, when i want to load the model in jetson nano, i'm facing this error Traceback (most recent call last): File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 903, in load_internal ckpt_options, options, filters) File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 138, in _init_ meta_graph.graph_def.library, wrapper_function=_WrapperFunction)) File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 388, in load_function_def_library func_graph = function_def_lib.function_def_to_graph(copy) File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py\", line 64, in function_def_to_graph fdef, input_shapes) File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py\", line 228, in function_def_to_graph_def op_def = default_graph._get_op_def(node_def.op) # pylint: disable=protected-access File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3967, in _get_op_def buf) tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'DisableCopyOnRead' in binary running on SAMYOB-PC. Make sure the Op and Kernel are registered in the binary running in thiry running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graphare lazily regist, as contrib ops are lazily registered when the module is first accessed. During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"E:\\software\\conda2023\\envs\\older\\lib\\runpy.py\", line 193, in _run_module_as_main \"_main_\", mod_spec) File \"E:\\software\\conda2023\\envs\\older\\lib\\runpy.py\", line 85, in _run_code exec(code, run_globals) File \"E:\\software\\conda2023\\envs\\older\\Scripts\\saved_model_cli.exe\\_main_.py\", line 7, in &lt;module&gt; File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 1204, in main args.func(args) File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 729, in show _show_all(args.dir) File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 308, in _show_all _show_defined_functions(saved_model_dir) File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 188, in _show_defined_functions trackable_object = load.load(saved_model_dir) File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 864, in load result = load_internal(export_dir, tags, options)[\"root\"] File \"E:\\software\\conda2023\\envs\\older\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 906, in load_internal str(err) + \"\\n If trying to load on a different device from the \" FileNotFoundError: Op type not registered 'DisableCopyOnRead' in binary running on SAMYOB-PC. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loadn the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.cont the graph, as contrib ops are lazily registere is first accessrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhostaved_model.LoadOptions to the io_device such as '/job:localhost'. can you help me to solve this error?",
        "answers": [],
        "votes": []
    },
    {
        "question": "How to Install qt4 on Jetson Nano (ubuntu 20.04) for Hector Slam Firstly, I tried sudo add-apt-repository ppa:rock-core/qt4 this code and tried to install the needed libraries, but in my opinion it couldn't find the libraries again because of the jetson nano's architecture. Then I tried to download the debian packages from the internet and realized most of them are amd64 packages, I find one package for arm64, but this one needs dependencys as shown below tried to install downloaded package for 20.04 and arm64 When this gives an error, I tried to download the qt5 then change the version manually, as shown below it doesn't work and says run make install (don't know how to do) qt version 4.8 I can't restart with 18.04 on jetson, so much work done until now. I need to install qt4 to run Hector Slam. What should I do any suggestions?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to compile Tensorflow 2.7 with Python 3.7 in NVIDIA Jetson Nano, but after several hours the process faileed with following error. ERROR: /home/acd/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:572:11: C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:tensorflow_ops' failed (Exit 4): crosstool_wrapper_driver_is_not_gcc failed: error executing command (cd /home/acd/.cache/bazel/_bazel_acd/cde8dcf7cee5950e7dfb9a438207f0dd/execroot/org_tensorflow &amp;&amp; \\ exec env - \\ CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 \\ GCC_HOST_COMPILER_PATH=/usr/bin/aarch64-linux-gnu-gcc-7 \\ LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/lib64: \\ PATH=/usr/local/cuda-10.2/bin:/home/acd/.local/bin:/home/acd/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\ PWD=/proc/self/cwd \\ PYTHON_BIN_PATH=/usr/bin/python3.7 \\ PYTHON_LIB_PATH=/usr/local/lib/python3.7/dist-packages \\ TF2_BEHAVIOR=1 \\ TF_CUDA_COMPUTE_CAPABILITIES=5.3 \\ external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops/tf_ops.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops/tf_ops.pic.o' '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=&lt;execinfo.h&gt;' '-DLTDL_SHLIB_EXT=\".so\"' '-DLLVM_PLUGIN_EXT=\".so\"' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' '-DHAVE_STRERROR_R=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_PTHREAD_GETSPECIFIC=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_DEREGISTER_FRAME=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_LSEEK64=1' '-DHAVE_MALLINFO=1' '-DHAVE_POSIX_FALLOCATE=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=\"AArch64\"' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeAArch64AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeAArch64AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeAArch64Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeAArch64Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeAArch64TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeAArch64TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeAArch64TargetMCA' '-DLLVM_HOST_TRIPLE=\"aarch64-unknown-linux-gnu\"' '-DLLVM_DEFAULT_TARGET_TRIPLE=\"aarch64-unknown-linux-gnu\"' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/aarch64-opt/bin -iquote external/llvm-project -iquote bazel-out/aarch64-opt/bin/external/llvm-project -iquote external/llvm_terminfo -iquote bazel-out/aarch64-opt/bin/external/llvm_terminfo -iquote external/llvm_zlib -iquote bazel-out/aarch64-opt/bin/external/llvm_zlib -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/aarch64-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/aarch64-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/aarch64-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/aarch64-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/aarch64-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/aarch64-opt/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/aarch64-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/local_config_rocm -iquote bazel-out/aarch64-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/aarch64-opt/bin/external/local_config_tensorrt -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/SubElementInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/aarch64-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen -isystem external/llvm-project/llvm/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/mlir/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/mlir/include -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/aarch64-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/aarch64-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/aarch64-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/aarch64-opt/bin/external/double_conversion -isystem external/local_config_rocm/rocm -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -c tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc -o bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops/tf_ops.pic.o) Execution platform: @local_execution_config_platform//:platform aarch64-linux-gnu-gcc-7: internal compiler error: Killed (program cc1plus) Please submit a full bug report, with preprocessed source if appropriate. See &lt;file:///usr/share/doc/gcc-7/README.Bugs&gt; for instructions. Target //tensorflow/tools/pip_package:build_pip_package failed to build INFO: Elapsed time: 53520.933s, Critical Path: 27882.90s INFO: 5993 processes: 944 internal, 5049 local. FAILED: Build did NOT complete successfully Here's my configuration acd@acd-jetson:~/tensorflow$ ./configure You have bazel 3.7.2- (@non-git) installed. Please specify the location of python. [Default is /usr/bin/python3]: /usr/bin/python3.7 Found possible Python library paths: /usr/lib/python3/dist-packages /usr/local/lib/python3.7/dist-packages Please input the desired Python library path to use. Default is [/usr/lib/python3/dist-packages] Do you wish to build TensorFlow with ROCm support? [y/N]: n No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Do you wish to build TensorFlow with TensorRT support? [y/N]: y TensorRT support will be enabled for TensorFlow. Found CUDA 10.2 in: /usr/local/cuda-10.2/targets/aarch64-linux/lib /usr/local/cuda-10.2/targets/aarch64-linux/include Found cuDNN 8 in: /usr/lib/aarch64-linux-gnu /usr/include Found TensorRT 8 in: /usr/lib/aarch64-linux-gnu /usr/include/aarch64-linux-gnu Please specify a list of comma-separated CUDA compute capabilities you want to build with. You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code. Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities &gt;= 3.5 [Default is: 3.5,7.0]: 5.3 Do you want to use clang as CUDA compiler? [y/N]: n nvcc will be used as CUDA compiler. Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding \"--config=&lt;&gt;\" to your build command. See .bazelrc for more details. --config=mkl # Build with MKL support. --config=mkl_aarch64 # Build with oneDNN and Compute Library for the Arm Architecture (ACL). --config=monolithic # Config for mostly static monolithic build. --config=numa # Build with NUMA support. --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects. --config=v1 # Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features: --config=nogcp # Disable GCP support. --config=nonccl # Disable NVIDIA NCCL support. Configuration finished Cause Jetson Nano use TensorRT 8.2 which isn't supported in tensorflow 2.7 by default, I use following command based on this guide Tensorflow v2.7.0 = TensorRT8.2 build. cp tensorflow/compiler/tf2tensorrt/stub/NvInfer_8_0.inc tensorflow/compiler/tf2tensorrt/stub/NvInfer_8_2.inc sed -i '62a #elif NV_TENSORRT_MAJOR == 8 &amp;&amp; NV_TENSORRT_MINOR == 2' tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc sed -i '63a #include \"tensorflow/compiler/tf2tensorrt/stub/NvInfer_8_2.inc\"' tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc cp tensorflow/compiler/tf2tensorrt/stub/NvInferPlugin_8_0.inc tensorflow/compiler/tf2tensorrt/stub/NvInferPlugin_8_2.inc sed -i '62a #elif NV_TENSORRT_MAJOR == 8 &amp;&amp; NV_TENSORRT_MINOR == 2' tensorflow/compiler/tf2tensorrt/stub/nvinfer_plugin_stub.cc sed -i '63a #include \"tensorflow/compiler/tf2tensorrt/stub/NvInferPlugin_8_2.inc\"' tensorflow/compiler/tf2tensorrt/stub/nvinfer_plugin_stub.cc And here's my build command bazel build --config=cuda --config=nonccl --verbose_failures //tensorflow/tools/pip_package:build_pip_package How to solve this problem?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Hardware: Jetson Nano - Jetpack 4.4 - Python 3.6 - Deepstream 6.0 I'm currently working with deepstream-imagedate-multistream.py 1 example from NVIDIA-AI-IOT repo and I want to use face_recognition library 2 (of adam geitgey) in this sample to recognize faces in multiple video sources. So the question is that how can I directly process frames in this sample pipeline. I have read the deepstream document and have done many tests but still don't have any results. I hope that anybody have encounter the same problem or having similar experience could help me find a solution. I would really appreciate. Thanks.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am testing an kernel module which creates perf events on every core and count the total number of L2D_CACHE_REFILL. System details: NVIDIA Jetson nano, OS: Ubuntu 18.04.6 LTS, Kernel: 4.9.255, L4T: 32.7.3, Jetpack: 4.6.3 When I try to insert this module using insmod command, i get WARNING message in kernel log file. The screenshot of dmesg command is shown below: This warning states that at line no. 199, the function armpmu_start() is throwing an warning. I tried checking the kernel source code. The kernel code snippet for armpmu_start function from drivers/perf/arm_pmu file is as below: Also when I try to remove the module with rmmod command my jetson nano board is restarting. Here are the few perf APIs the kernel module is using: Creating attributes structure of the perf event struct perf_event_attr sched_perf_hw_attr = { .type = PERF_TYPE_RAW, .config = 0x17, .size = sizeof (struct perf_event_attr), .pinned = 1, .disabled = 1, .exclude_kernel = 1, .sample_period = budget }; Creating perf event with event_overflow_callback function event = perf_event_create_kernel_counter (&amp;sched_perf_hw_attr, cpu, NULL, event_overflow_callback, NULL ); Code to start the counter perf_event_enable (event); event-&gt;pmu-&gt;add (event, PERF_EF_START); Code to stop the counter event-&gt;pmu-&gt;stop (event, PERF_EF_UPDATE); event-&gt;pmu-&gt;del(event,0); In the event_overflow_callback function following activities are performed. Stopping counter event-&gt;pmu-&gt;stop (event, PERF_EF_UPDATE); Enable performance counter event-&gt;pmu-&gt;start (event, PERF_EF_RELOAD); Is the last statement creating an issue? I tried setting the parameters to PERF_EF_UPDATE/ PF_EF_START in the start() function. But still getting the same warning. Any help in diagnosing the issue is highly appreciated. I am new to Linux kernel module. Kindly suggest a methods of debugging Linux kernel modules on embedded devices like Nvidia jetson boards. That will help me in gaining knowledge.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have an application for artificial vision. It needs: python 3.8, numpy 1.18.4, cv2 4.2.0.34, keyboard 0.13.5 and YoloV4. How can i create a docker file to run a container with all this specific modules? I tried to install containers with any modules but i dont know how to install the others modules in the same container.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have configured Bluetooth a2dp profile and automated the connection Process in linux using bluez and the bash script, making sure the device is already paired Bluetoothctl&lt;&lt; agent NoInputNoOutput trust connect quit It works and I am able to connect from Android Phone by pressing the Connect button in Bluetooth Setting, But the problem is when the Jetson Nano reboots then the 'bluetoothctl' script does not work to create a connection directly from jetson, and still requires me to press the button from phone. When I tried the Blueman-manager to connect directly from Jetson Nano, it gives the following error message: Connection Failed: Did not receive a reply: Possible causes may be:The remote application did not send a reply, or the message bus security policy blocked the reply So how to enable the jetson to connect directly to paired device over A2DP profile, without pressing the connect button from Android phone. I am using Nvidia Jetson Nano SDK Jetpack 4.6 BlueZ Version 5.63 Android Version 11 The devices are already paired, I tried to make the Bluetooth A2DP Connection directly from Jetson Nano to Android Using Bluez. I'm expecting that when I try to connect directly from jetson to paired android device it should get connected. But It requires me to press the connect button from android bluetooth setting",
        "answers": [],
        "votes": []
    },
    {
        "question": "My mission is to plan an autonomous flight of my drone using CUAV X7+ flight controller. On my flight at certain point, there are different markers on ground which I need to detect and drop a payload there. For detection and localization, I am using Jetson nano. Now I need to write Dronekit script such as whenever a ground marker is detected, dronekit script overwrites (or whatever) my already planned auto flight, go above the marker, drop a payload and then continue its auto flight. For this purpose, I need help, which approach should I follow to perform this task. How can I overwrite CUAV auto plan for some time? Should I run script in Jetson or in Mission planner? How?",
        "answers": [],
        "votes": []
    },
    {
        "question": "\"I have set up a task on my Jetson device, where if someone presses the \"FORCE RECOVERY\" button for 15 seconds, the device will automatically recover. The pin number for the FORCE RECOVERY button is documented as pin 214, and I have used the Jetson.GPIO library in my Python code to detect the button press. However, I am getting a \"ValueError: Channel 214 is invalid\" error message when running my code. Can anyone help me understand what I am doing wrong and how I can correctly detect the button press on pin 214 using Jetson.GPIO?\" import Jetson.GPIO as GPIO #To disable the warnings GPIO.setwarnings(False) # Set the GPIO mode to BCM GPIO.setmode(GPIO.BCM) # Set pin 214 as input GPIO.setup(214, GPIO.IN) # Read the input status of pin 214 input_value = GPIO.input(214) # Print the input value to the console print(\"Input value of pin 214:\", input_value) # Clean up the GPIO configuration GPIO.cleanup() import Jetson.GPIO as GPIO #To disable the warnings GPIO.setwarnings(False) GPIO.setmode(GPIO.TEGRA_SOC) GPIO.setup('FORCE_RECOVERY', GPIO.IN) value = GPIO.input('FORCE_RECOVERY') print(value) # Clean up the GPIO configuration GPIO.cleanup() I have written this code for FC REC (Force recovery pin) but i am getting below error: WARNING: Carrier board is not from a Jetson Developer Kit.WARNNIG: Jetson.GPIO library has not been verified with this carrier board,WARNING: and in fact is unlikely to work correctly.Traceback (most recent call last):File \"frp.py\", line 7, inGPIO.setup(214, GPIO.IN)File \"/usr/lib/python3/dist-packages/Jetson/GPIO/gpio.py\", line 358, in setupch_infos = _channels_to_infos(channels, need_gpio=True)File \"/usr/lib/python3/dist-packages/Jetson/GPIO/gpio.py\", line 121, in _chansfor c in _make_iterable(channels)]File \"/usr/lib/python3/dist-packages/Jetson/GPIO/gpio.py\", line 121, infor c in _make_iterable(channels)]File \"/usr/lib/python3/dist-packages/Jetson/GPIO/gpio.py\", line 104, in _chanpraise ValueError(\"Channel %s is invalid\" % str(channel))ValueError: Channel 214 is invalid this is the link: link for the Documentation where metioned about the 214 FC REC pin",
        "answers": [],
        "votes": []
    },
    {
        "question": "I found the python script below that should be able to give a realistic humanoid voice to chat gpt,converting the text produced by it into a humanoid voice and using my voice with a mic to talk with it. In short terms I want to do the same thing that the \u201camazon echo / Alexa\u201d voice assistant does,without buying it,but using only what I already have\u2026the Jetson nano (where I have installed ubuntu 20.04). Why the Jetson nano ? Because I can move it from a place to another one within my home,like a voice assistant and because I\u2019ve already spent some money to buy it and I want to use it. This is the video tutorial where I found it : https://www.youtube.com/watch?v=8z8Cobsvc9k This is the code connected to the video tutorial : code.py : https://pastebin.ubuntu.com/p/G4wHRBNr7D/ I have configured my kinect 2 on the jetson nano and I\u2019m using it correctly as a mic to send voice messages to my whatsapp and telegram friends. They can hear my voice very well,so I\u2019m sure that it works. Did that,I have chosen \u201chw:Sensor\u201d as output device and \u201chw.Sensor,0\u201d as input device on the qjackctl panel and then I have launched again the python script : marietto@marietto-nano:/mnt/fisso/Alexa-ChatGPT$# jack_control start marietto@marietto-nano:/mnt/fisso/Alexa-ChatGPT$# python3 code.py this is what happens. Everytime I say something (not only genius),these error messages appeared ciclycally : https://pastebin.ubuntu.com/p/pXY4Ydyncq/ I tried to suppress all that errors modyfing the code with this one : code2.py : https://pastebin.ubuntu.com/p/m8j2TR4Vqw/ this is what happens when I run it : (sometime I say genius,sometimes not) https://pastebin.ubuntu.com/p/xZcfMy5cfs/ Goal : I want to reach the same result reached in this video tutorial : https://www.youtube.com/watch?v=8z8Cobsvc9k",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a Nano Jetson and flashed it with the latest available Jetpack version from here: https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit which is 4.6.1. Now when following this guide to install tensorflow: https://forums.developer.nvidia.com/t/official-tensorflow-for-jetson-nano/71770 I was not able to install h5py, I got: ERROR: Could not build wheels for h5py, which is required to install pyproject.toml-based projects , Then, I followed instruction from : https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html and tried to build specific version of h5py==3.6.0, I got an error: ERROR: Could not find a version that satisfies the requirement h5py==3.6.0 (from versions: 2.2.1, 2.3.0b1, 2.3.0, 2.3.1, 2.4.0b1, 2.4.0, 2.5.0, 2.6.0, 2.7.0rc2, 2.7.0, 2.7.1, 2.8.0rc1, 2.8.0, 2.9.0rc1, 2.9.0, 2.10.0, 3.0.0rc1, 3.0.0, 3.1.0) ERROR: No matching distribution found for h5py==3.6.0, Then I tried with h5py=3.0.0, once again I got the error, ERROR: Could not build wheels for h5py, which is required to install pyproject.toml-based projects I have included the complete error log file, Please advice agv@agv-nano:~$ sudo pip3 install -U numpy==1.19.4 future mock keras_preprocessing keras_applications gast==0.2.1 protobuf pybind11 cython pkgconfig packaging h5py==3.6.0 Keyring is skipped due to an exception: org.freedesktop.DBus.Error.NoServer: Failed to connect to socket /tmp/dbus-Uj0zn9M8en: Connection refused WARNING: The directory '/home/agv/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag. Collecting numpy==1.19.4 Downloading numpy-1.19.4-cp36-cp36m-manylinux2014_aarch64.whl (12.2 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.2 MB 4.5 MB/s Collecting future Downloading future-0.18.3.tar.gz (840 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 840 kB 4.8 MB/s Preparing metadata (setup.py) ... done Collecting mock Downloading mock-5.0.1-py3-none-any.whl (30 kB) Requirement already satisfied: keras_preprocessing in /usr/local/lib/python3.6/dist-packages (1.1.2) Collecting keras_applications Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50 kB 5.6 MB/s Collecting gast==0.2.1 Downloading gast-0.2.1.tar.gz (10 kB) Preparing metadata (setup.py) ... done Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (3.19.6) Collecting pybind11 Downloading pybind11-2.10.4-py3-none-any.whl (222 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222 kB 3.5 MB/s Requirement already satisfied: cython in ./.local/lib/python3.6/site-packages (0.29.33) Collecting pkgconfig Downloading pkgconfig-1.5.5-py3-none-any.whl (6.7 kB) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (21.3) ERROR: Could not find a version that satisfies the requirement h5py==3.6.0 (from versions: 2.2.1, 2.3.0b1, 2.3.0, 2.3.1, 2.4.0b1, 2.4.0, 2.5.0, 2.6.0, 2.7.0rc2, 2.7.0, 2.7.1, 2.8.0rc1, 2.8.0, 2.9.0rc1, 2.9.0, 2.10.0, 3.0.0rc1, 3.0.0, 3.1.0) ERROR: No matching distribution found for h5py==3.6.0 agv@agv-nano:~$ sudo pip3 install -U numpy==1.19.4 future mock keras_preprocessing keras_applications gast==0.2.1 protobuf pybind11 cython pkgconfig packaging h5py==3.0.0 Keyring is skipped due to an exception: org.freedesktop.DBus.Error.NoServer: Failed to connect to socket /tmp/dbus-Uj0zn9M8en: Connection refused WARNING: The directory '/home/agv/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag. Collecting numpy==1.19.4 Downloading numpy-1.19.4-cp36-cp36m-manylinux2014_aarch64.whl (12.2 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.2 MB 4.0 MB/s Collecting future Downloading future-0.18.3.tar.gz (840 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 840 kB 6.6 MB/s Preparing metadata (setup.py) ... done Collecting mock Downloading mock-5.0.1-py3-none-any.whl (30 kB) Requirement already satisfied: keras_preprocessing in /usr/local/lib/python3.6/dist-packages (1.1.2) Collecting keras_applications Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50 kB 2.1 MB/s Collecting gast==0.2.1 Downloading gast-0.2.1.tar.gz (10 kB) Preparing metadata (setup.py) ... done Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (3.19.6) Collecting pybind11 Downloading pybind11-2.10.4-py3-none-any.whl (222 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222 kB 2.0 MB/s Requirement already satisfied: cython in ./.local/lib/python3.6/site-packages (0.29.33) Collecting pkgconfig Downloading pkgconfig-1.5.5-py3-none-any.whl (6.7 kB) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (21.3) Collecting h5py==3.0.0 Downloading h5py-3.0.0.tar.gz (370 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 370 kB 4.3 MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Installing backend dependencies ... done Preparing metadata (pyproject.toml) ... done Collecting cached-property Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras_preprocessing) (1.15.0) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/lib/python3/dist-packages (from packaging) (2.2.0) Building wheels for collected packages: gast, h5py, future Building wheel for gast (setup.py) ... done Created wheel for gast: filename=gast-0.2.1-py3-none-any.whl size=7472 sha256=488c76a4bf32dc0e35d0018bdc73da4fb502982c8effdb5ff8b648a526edfea2 Stored in directory: /tmp/pip-ephem-wheel-cache-p8stowdg/wheels/5a/de/13/ce88a752efbdba27fa15e3047f3445128e76d6c1a1197a9d5c Building wheel for h5py (pyproject.toml) ... error ERROR: Command errored out with exit status -4: command: /usr/bin/python3 /home/agv/.local/lib/python3.6/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /tmp/tmprpijrfus cwd: /tmp/pip-install-dlr53off/h5py_f9c68c630e724b9c8225257c47447da2 Complete output (67 lines): running bdist_wheel running build running build_py creating build creating build/lib.linux-aarch64-3.6 creating build/lib.linux-aarch64-3.6/h5py copying h5py/version.py -&gt; build/lib.linux-aarch64-3.6/h5py copying h5py/_init_.py -&gt; build/lib.linux-aarch64-3.6/h5py copying h5py/ipy_completer.py -&gt; build/lib.linux-aarch64-3.6/h5py copying h5py/h5py_warnings.py -&gt; build/lib.linux-aarch64-3.6/h5py creating build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/hl/init_.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/base.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/filters.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/group.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/attrs.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/datatype.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/files.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/dataset.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/dims.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/vds.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/compat.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/selections2.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl copying h5py/_hl/selections.py -&gt; build/lib.linux-aarch64-3.6/h5py/_hl creating build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/conftest.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_attribute_create.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_errors.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_file_image.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_attrs.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/_init_.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_h5pl.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_dataset_swmr.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_attrs_data.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/common.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_dims_dimensionproxy.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_h5p.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_big_endian_file.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_datatype.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_filters.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_objects.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_dimension_scales.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_h5f.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_file.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_h5d_direct_chunk.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_h5t.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_group.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_selections.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_slicing.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_dtype.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_completions.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_dataset.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_base.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_h5.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_dataset_getitem.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests copying h5py/tests/test_file2.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests creating build/lib.linux-aarch64-3.6/h5py/tests/data_files copying h5py/tests/data_files/_init_.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests/data_files creating build/lib.linux-aarch64-3.6/h5py/tests/test_vds copying h5py/tests/test_vds/_init_.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests/test_vds copying h5py/tests/test_vds/test_virtual_source.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests/test_vds copying h5py/tests/test_vds/test_highlevel_vds.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests/test_vds copying h5py/tests/test_vds/test_lowlevel_vds.py -&gt; build/lib.linux-aarch64-3.6/h5py/tests/test_vds copying h5py/tests/data_files/vlen_string_dset.h5 -&gt; build/lib.linux-aarch64-3.6/h5py/tests/data_files copying h5py/tests/data_files/vlen_string_dset_utc.h5 -&gt; build/lib.linux-aarch64-3.6/h5py/tests/data_files copying h5py/tests/data_files/vlen_string_s390x.h5 -&gt; build/lib.linux-aarch64-3.6/h5py/tests/data_files running build_ext ---------------------------------------- ERROR: Failed building wheel for h5py Building wheel for future (setup.py) ... done Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492037 sha256=79c240fc12311f694c4be36da939fd21f541504398cd24eaa651badc9f9e4efe Stored in directory: /tmp/pip-ephem-wheel-cache-p8stowdg/wheels/63/f1/0c/e56d12b3804345ce5ba34279cbfe583ecafdd1401551457330 Successfully built gast future Failed to build h5py ERROR: Could not build wheels for h5py, which is required to install pyproject.toml-based projects",
        "answers": [
            [
                "Use below commnad $ sudo apt-get install python3-pip $ sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev $ pip3 install cython $ pip3 install h5py==2.10.0 This command was originally posted on the Nvidia Forums. But my jetson wasn't work. So I ran the below command and it worked. $ export PATH=${PATH}:/usr/local/cuda/bin"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have Ros2-galatic-Pytorch image which I got from DockerHub. Here's what I am doing. Connecting to my Nvidia Jetson Nano via ssh from my macbook Running the Ros-Gelatonic-Pytorch container image and linking the Port of the host and the container. With this command sudo docker run -it -p 8888:8888 dustynv/ros:galactic-pytorch-l4t-r32.6.1 manually installing jupyter notebook on that container instance (temporary for testing) Issue -&gt; It says the Jupyter notebook is running but when I click on the link it refuses to connect. I did checked out this Link but It don't work on my case. Maybe because I am doing ssh with Jetson nano. Here the URL: [I 09:25:22.897 NotebookApp] Jupyter Notebook 6.4.10 is running at: [I 09:25:22.897 NotebookApp] http://localhost:8888/?token=f73cd75df704164ec6043bcb54f3b3be2687134659a1a3cd [I 09:25:22.897 NotebookApp] or http://127.0.0.1:8888/?token=f73cd75df704164ec6043bcb54f3b3be2687134659a1a3cd [I 09:25:22.897 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 09:25:22.916 NotebookApp] No web browser found: could not locate runnable browser. [C 09:25:22.916 NotebookApp] To access the notebook, open this file in a browser: file:///root/.local/share/jupyter/runtime/nbserver-201-open.html Or copy and paste one of these URLs: http://localhost:8888/?token=f73cd75df704164ec6043bcb54f3b3be2687134659a1a3cd or http://127.0.0.1:8888/?token=f73cd75df704164ec6043bcb54f3b3be2687134659a1a3cd ``` I tried checking if the ports is listening on the host. It does I am not able to access the URL link even if I install a jupyter notebook image ( jupyter/mininmal-notebook) and run its instance [1]: https://i.stack.imgur.com/TWei8.png Here are the port details to check for firewall on the remote server:```opentrons@opentrons-desktop:~$ sudo lsof -i :8888 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME docker-pr 10745 root 4u IPv4 271917 0t0 TCP *:8888 (LISTEN)``` on my computer: ```COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME ssh 2117 jay 5u IPv4 0xd3cbf2494a59da73 0t0 TCP localhost:ddi-tcp-1 (LISTEN) ssh 2117 jay 6u IPv6 0xd3cbf24e1446ba8b 0t0 TCP localhost:ddi-tcp-1 (LISTEN)```",
        "answers": [],
        "votes": []
    },
    {
        "question": "My GPU is not working on Jetson Nano which I have checked using the following command: tf.config.list_physical_devices('GPU') I am using \"Jetson nano\" My Jetpack is 4.6.1 I have cuda 10.2 I have cudatoolkit and cudnn installed in my environment with following versions: enter image description here How can I enable the GPU on \"Jetson Nano\"?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to install the latest arm version of VSCode on jetsonNX with jetpack5.1, but after installation, VSCode cannot run and no response. I checked the system-monitor and there are VSCode components running.",
        "answers": [
            [
                "You would have to run code with no-sandbox flag with XavierNX: code --no-sandbox"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to run YOLOv5 with the Jetson Nano (Ubuntu 20.04 and Python 3.8) gpu. I have setup YOLOv5 with these commands git clone https://github.com/ultralytics/yolov5 cd yolov5 pip install -r requirements.txt I then attempted to run the detect.py file (that uses the base weights and testing data) and I got this error: Traceback (most recent call last): File \"detect.py\", line 37, in &lt;module&gt; import torch File \"/home/jetson/.local/lib/python3.8/site-packages/torch/__init__.py\", line 198, in &lt;module&gt; _load_global_deps() File \"/home/jetson/.local/lib/python3.8/site-packages/torch/__init__.py\", line 151, in _load_global_deps ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL) File \"/usr/lib/python3.8/ctypes/__init__.py\", line 373, in __init__ self._handle = _dlopen(self._name, mode) OSError: libcublas.so.11: cannot open shared object file: No such file or directory",
        "answers": [
            [
                "This was caused by the Pytorch version not being compatable with the Jetson Nano. To install torch on the Jetson Nano you need to install the Jetson Nano verison of torch."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm testing Yolov5n with a Jetson nano B01 device (4GB). For this purpose I'm using a docker version from this repo: repo FROM nvcr.io/nvidia/l4t-base:r32.7.1 ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update &amp;&amp; apt-get install -y \\ git \\ python3.8 python3.8-dev python3-pip \\ libopenmpi-dev libomp-dev libopenblas-dev libblas-dev libeigen3-dev RUN python3.8 -m pip install --upgrade pip RUN python3.8 -m pip install setuptools gdown # pytorch 1.11.0 RUN gdown https://drive.google.com/uc?id=1hs9HM0XJ2LPFghcn7ZMOs5qu5HexPXwM RUN python3.8 -m pip install torch-*.whl # torchvision 0.12.0 RUN gdown https://drive.google.com/uc?id=1m0d8ruUY8RvCP9eVjZw4Nc8LAwM8yuGV RUN python3.8 -m pip install torchvision-*.whl RUN git clone https://github.com/ultralytics/yolov5.git WORKDIR yolov5 RUN python3.8 -m pip install -r requirements.txt COPY is_docker.patch . RUN patch -p1 &lt; is_docker.patch I'm running docker from a default installation (SD Card, no SSD) These are the results from a list of 640x640 car and bottle images: root@25b13ebea957:/yolov5# python3.8 detect.py --weights yolov5n.pt --source data/images detect: weights=['yolov5n.pt'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1 YOLOv5 \ud83d\ude80 v7.0-116-g5c91dae Python-3.8.0 torch-1.11.0a0+gitbc2c6ed CUDA:0 (NVIDIA Tegra X1, 3964MiB) Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt to yolov5n.pt... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.87M/3.87M [00:01&lt;00:00, 4.02MB/s] Fusing layers... YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients image 1/178 /yolov5/data/images/00000002_jpg.rf.41c3bd80c13879da35da9cad8c672729.jpg: 640x640 1 parking meter, 174.0ms image 2/178 /yolov5/data/images/00000006_jpg.rf.dd3940f860d6ade63e4247555fb1e559.jpg: 640x640 1 cake, 173.9ms image 3/178 /yolov5/data/images/00000020_jpg.rf.668142e21ed24351ae6a8f5cc899817d.jpg: 640x640 (no detections), 175.3ms image 4/178 /yolov5/data/images/00000021_jpg.rf.98fce8efe88fd90a1ec0bd683243537c.jpg: 640x640 2 bottles, 173.9ms image 5/178 /yolov5/data/images/00000028_jpg.rf.03e103667071f566385b6ff03e129069.jpg: 640x640 1 person, 173.6ms image 6/178 /yolov5/data/images/00000040_jpg.rf.81e12357b61957bc7d01bb7b8fc06861.jpg: 640x640 1 surfboard, 1 bottle, 2 vases, 173.6ms image 7/178 /yolov5/data/images/00000092_jpg.rf.1c4b4f1dc7da8b6ccf574a6067651a5e.jpg: 640x640 5 bottles, 173.9ms image 8/178 /yolov5/data/images/00000132_jpg.rf.1a274621f0cf8c342962d61cefda313c.jpg: 640x640 4 bottles, 1 toothbrush, 175.4ms image 9/178 /yolov5/data/images/00000141_png.rf.0753e47a2c4614522fb06ecc9e4b10bd.jpg: 640x640 2 bottles, 173.8ms image 10/178 /yolov5/data/images/00000146_jpg.rf.ca6ce3dd4e5669fa6e15c1faf71918a4.jpg: 640x640 (no detections), 174.0ms image 11/178 /yolov5/data/images/00000163_jpg.rf.f62ee3d7bf9bbeff16f05dde8fcc2277.jpg: 640x640 1 car, 175.4ms image 12/178 /yolov5/data/images/00000184_jpg.rf.8b2df2e255a02204a885aff64f01f366.jpg: 640x640 (no detections), 173.8ms image 13/178 /yolov5/data/images/00000222_jpg.rf.c633548eabfa4e6b2532805f370933f6.jpg: 640x640 3 bottles, 2 cups, 3 vases, 175.4ms image 14/178 /yolov5/data/images/00000286_jpg.rf.da4093f2136eca656fe9d148eccec9c9.jpg: 640x640 2 bottles, 173.7ms image 15/178 /yolov5/data/images/00000291_jpg.rf.f286e4547328ad1c912e66836500aaf6.jpg: 640x640 (no detections), 173.8ms image 16/178 /yolov5/data/images/00000312_jpg.rf.cb7cfb97cdcc8e451b49eaa491235325.jpg: 640x640 (no detections), 175.2ms image 17/178 /yolov5/data/images/00000313_jpg.rf.e80becce9fa85652a528abe75132f487.jpg: 640x640 8 bottles, 173.9ms image 18/178 /yolov5/data/images/00000314_jpg.rf.e9f500142f276f218b0dacf24f6e1fb3.jpg: 640x640 1 vase, 175.3ms image 19/178 /yolov5/data/images/00000322_jpeg.rf.eee821b3dbd466ed90abd9017551a801.jpg: 640x640 2 bottles, 173.6ms image 20/178 /yolov5/data/images/00000330_jpg.rf.f5d384fb5d4bd3190b38ab48739b805b.jpg: 640x640 (no detections), 173.6ms image 21/178 /yolov5/data/images/00000332_jpg.rf.28cbcb6d54a5357cfba39d935a301bc9.jpg: 640x640 2 bottles, 1 cell phone, 175.6ms image 22/178 /yolov5/data/images/00000344_jpg.rf.9146ceacbcc8a2fb9447b6b7bb8a84ea.jpg: 640x640 (no detections), 173.9ms image 23/178 /yolov5/data/images/00000363_jpg.rf.aa01a6501cc21dcc73e3bcddfe21ed56.jpg: 640x640 1 vase, 175.5ms image 24/178 /yolov5/data/images/00000392_jpg.rf.1a206b2f0d8a62ffc20def8cfb6b9cb3.jpg: 640x640 1 toothbrush, 174.1ms image 25/178 /yolov5/data/images/00000405_jpg.rf.9f358303c0f8e65921128c7bf42c6e9f.jpg: 640x640 1 vase, 173.9ms image 26/178 /yolov5/data/images/03ea3ede0a_jpg.rf.fd4b0dd4844747e738e8784ed3ccaa20.jpg: 640x640 1 car, 1 bus, 1 parking meter, 175.4ms image 27/178 /yolov5/data/images/144a1afb9a_jpg.rf.e0d8645f9e06c85e0e8e723cbdf9e429.jpg: 640x640 (no detections), 173.7ms image 28/178 /yolov5/data/images/165347b4f5_jpg.rf.14a66411f8f715399c8509b4eaf99c22.jpg: 640x640 (no detections), 175.6ms image 29/178 /yolov5/data/images/1dea3c8341_jpg.rf.85a5126880ee0f208d392692dfd0539a.jpg: 640x640 (no detections), 173.5ms image 30/178 /yolov5/data/images/293b21c8b3_jpg.rf.9bfb3c9a1762b104a49564208e3b79b4.jpg: 640x640 3 cups, 173.5ms image 31/178 /yolov5/data/images/3b7255cc78_jpg.rf.425bae164f755ee345019c1d80c69744.jpg: 640x640 1 cup, 175.5ms image 32/178 /yolov5/data/images/496c16bd93_jpg.rf.af4a82e06fd8ae79728e56cc766b8270.jpg: 640x640 (no detections), 173.7ms image 33/178 /yolov5/data/images/72278aeed6_jpg.rf.c32eb26ebbd96f63e0eccf571d24d170.jpg: 640x640 3 cups, 1 carrot, 1 cake, 175.4ms image 34/178 /yolov5/data/images/725640810b_jpg.rf.4269e0f3c3b844d84f26fd79260c5ffa.jpg: 640x640 3 bottles, 3 cups, 173.7ms image 35/178 /yolov5/data/images/733867bbc2_jpg.rf.9b1bc09e7cd5af85d307877c6f837e76.jpg: 640x640 1 person, 173.8ms image 36/178 /yolov5/data/images/79838d8653_jpg.rf.15345de891ec0a9275ba63134eaff80b.jpg: 640x640 (no detections), 175.2ms image 37/178 /yolov5/data/images/8290e2aecd_jpg.rf.bb4f8c30ec1e7998623f89d6a587cf0b.jpg: 640x640 (no detections), 173.8ms image 38/178 /yolov5/data/images/95932a5b20_jpg.rf.49d1a36f9f48e14a4bbc4542ac3e0cac.jpg: 640x640 1 suitcase, 173.9ms image 39/178 /yolov5/data/images/95a3b3d5b0_jpg.rf.8408804a7ad35077f74310e83f5dde1c.jpg: 640x640 2 cups, 2 cakes, 173.9ms image 40/178 /yolov5/data/images/977156e83e_jpg.rf.30116803929852dabfebe2c6cbf0d69b.jpg: 640x640 3 vases, 173.8ms image 41/178 /yolov5/data/images/R_103_jpg.rf.a3140d34b7f02d1d87e414d7637a8b4b.jpg: 640x640 (no detections), 175.5ms image 42/178 /yolov5/data/images/R_107_jpg.rf.19c701f0a06a91ec34aaae9aad3f2c02.jpg: 640x640 (no detections), 173.9ms image 43/178 /yolov5/data/images/R_118_jpg.rf.64d113f463c82dc54077707e3d020d29.jpg: 640x640 (no detections), 173.8ms image 44/178 /yolov5/data/images/R_120_jpg.rf.63fd9437bc7da2b527d09a203be433f6.jpg: 640x640 (no detections), 173.8ms image 45/178 /yolov5/data/images/R_129_jpg.rf.58bd16de956b94233ae9eb7cfcbb7734.jpg: 640x640 1 person, 1 car, 173.8ms image 46/178 /yolov5/data/images/R_142_jpg.rf.1e4f57d8a3a012f746ac0b57d55f6345.jpg: 640x640 (no detections), 175.2ms image 47/178 /yolov5/data/images/R_157_jpg.rf.cb820c3f519af5c7c4fe8b56a1b77a33.jpg: 640x640 (no detections), 173.7ms image 48/178 /yolov5/data/images/R_171_jpg.rf.0d48a28aad8f47c58cea0c308d2ce18f.jpg: 640x640 (no detections), 174.2ms image 49/178 /yolov5/data/images/R_227_jpg.rf.6b3e03ba26aaa3f1ec671967c83f4f7c.jpg: 640x640 (no detections), 174.0ms image 50/178 /yolov5/data/images/R_233_jpg.rf.7bfebf3810d6e935f9743e13fa9de75a.jpg: 640x640 1 bus, 173.8ms image 51/178 /yolov5/data/images/R_242_jpg.rf.9d4fbc41af98ebd901e43305b7d92569.jpg: 640x640 (no detections), 175.6ms image 52/178 /yolov5/data/images/R_262_jpg.rf.b31a699cd4fb180dd9b7f45852640c4d.jpg: 640x640 (no detections), 174.1ms image 53/178 /yolov5/data/images/R_269_jpg.rf.d3a4231587840257972b5c5e2d10b792.jpg: 640x640 1 person, 173.6ms image 54/178 /yolov5/data/images/R_310_jpg.rf.5d46a46992b0bca830a042fdb2c77087.jpg: 640x640 (no detections), 173.7ms image 55/178 /yolov5/data/images/R_311_jpg.rf.f2917e7e573784563349e26180518184.jpg: 640x640 (no detections), 173.7ms image 56/178 /yolov5/data/images/R_323_jpg.rf.42845dc89eb03cf03c8767e41e3d4382.jpg: 640x640 (no detections), 175.8ms image 57/178 /yolov5/data/images/R_329_jpg.rf.12d35a39b88ba667e2ebd94379c739f8.jpg: 640x640 (no detections), 173.8ms image 58/178 /yolov5/data/images/R_352_jpg.rf.63c4dca15e37c5dfa22f1f6ddb46cebf.jpg: 640x640 (no detections), 175.2ms image 59/178 /yolov5/data/images/R_367_jpg.rf.c05e91c176107d403c0f7ad9cf23f824.jpg: 640x640 (no detections), 173.9ms image 60/178 /yolov5/data/images/R_395_jpg.rf.e656ac103475690d6974a5bba841a057.jpg: 640x640 1 person, 173.7ms image 61/178 /yolov5/data/images/R_410_jpg.rf.0feb4478388515af2dabe2a9a6221396.jpg: 640x640 (no detections), 175.2ms image 62/178 /yolov5/data/images/R_427_jpg.rf.1bf37eda5d130c40b9b47cf5f5b58301.jpg: 640x640 (no detections), 173.6ms image 63/178 /yolov5/data/images/R_428_jpg.rf.3c6a8af7853bf350e75718cb23946c39.jpg: 640x640 (no detections), 175.6ms image 64/178 /yolov5/data/images/R_438_jpg.rf.b286f44a546d064b4c393c18911a1fa2.jpg: 640x640 1 car, 173.5ms image 65/178 /yolov5/data/images/R_43_jpg.rf.057e62e740efa15f80c03fb97a76f457.jpg: 640x640 (no detections), 173.8ms image 66/178 /yolov5/data/images/R_466_jpg.rf.7933e3417f286bca774f8fe07edcec83.jpg: 640x640 (no detections), 175.7ms image 67/178 /yolov5/data/images/R_471_jpg.rf.d388e9b38c52938d84854ee9501bd68c.jpg: 640x640 (no detections), 173.8ms image 68/178 /yolov5/data/images/R_483_jpg.rf.50e6af4a98d3d667f37bc84e2f282bcf.jpg: 640x640 (no detections), 176.6ms image 69/178 /yolov5/data/images/R_485_jpg.rf.615c3bf5e7e3ad17dc6794b33ff161ca.jpg: 640x640 1 tv, 173.9ms image 70/178 /yolov5/data/images/R_511_jpg.rf.3826aca002b178ea3daefafe20a8f4b0.jpg: 640x640 (no detections), 173.9ms image 71/178 /yolov5/data/images/R_524_jpg.rf.6b18d068254f168c6fc6387a74c75402.jpg: 640x640 (no detections), 175.5ms image 72/178 /yolov5/data/images/R_535_jpg.rf.588fc1a12f4bbde1d96252ee2fe2b447.jpg: 640x640 (no detections), 173.7ms image 73/178 /yolov5/data/images/R_556_jpg.rf.32f69fdc9064ce3c23acd8983b06775a.jpg: 640x640 (no detections), 175.4ms image 74/178 /yolov5/data/images/R_575_jpg.rf.f7196c7768bb7e99bbde031ab44c943e.jpg: 640x640 (no detections), 174.7ms image 75/178 /yolov5/data/images/R_582_jpg.rf.16cd74748cc94163753c004f9cc17e38.jpg: 640x640 (no detections), 173.9ms image 76/178 /yolov5/data/images/R_597_jpg.rf.b5817236068d00368060dd7986e3f54c.jpg: 640x640 (no detections), 175.1ms image 77/178 /yolov5/data/images/R_617_jpg.rf.ab6e6462532e940c5a87e1e73309fe7b.jpg: 640x640 (no detections), 173.8ms image 78/178 /yolov5/data/images/R_622_jpg.rf.fcfd74bc00783ceeb6fd5596798f890d.jpg: 640x640 (no detections), 175.6ms image 79/178 /yolov5/data/images/R_654_jpg.rf.284fe8e90a07dedf9853c773d8162832.jpg: 640x640 (no detections), 173.8ms image 80/178 /yolov5/data/images/R_65_jpg.rf.337416d200a3651adeae633c1591d642.jpg: 640x640 (no detections), 173.7ms image 81/178 /yolov5/data/images/R_667_jpg.rf.b1837a9e20f53be242cc621e54cec8b0.jpg: 640x640 (no detections), 173.8ms image 82/178 /yolov5/data/images/R_67_jpg.rf.effc9c239592948931fef53b94960380.jpg: 640x640 (no detections), 173.6ms image 83/178 /yolov5/data/images/R_710_jpg.rf.9d803a6c567d964e85703d6d286b49d1.jpg: 640x640 (no detections), 175.4ms image 84/178 /yolov5/data/images/R_718_jpg.rf.610b46bbb56b7ecfbd85344078e6581c.jpg: 640x640 (no detections), 173.7ms image 85/178 /yolov5/data/images/R_719_jpg.rf.93808980de8e9f58369850821888e663.jpg: 640x640 (no detections), 173.7ms image 86/178 /yolov5/data/images/R_73_jpg.rf.ec8df56ba9aa3c5d32f67439c11def63.jpg: 640x640 (no detections), 173.9ms image 87/178 /yolov5/data/images/R_744_jpg.rf.701ad842a87d677b1236b1f417faf91f.jpg: 640x640 1 traffic light, 175.3ms image 88/178 /yolov5/data/images/R_760_jpg.rf.4879154f1d7fa0a8c9895272e82f95cc.jpg: 640x640 (no detections), 175.0ms image 89/178 /yolov5/data/images/R_789_jpg.rf.2c7e63e9182b27aa7f95150b3b66c8a0.jpg: 640x640 (no detections), 174.1ms image 90/178 /yolov5/data/images/R_796_jpg.rf.81dd7f247104c3f682d96b58582e3b2d.jpg: 640x640 (no detections), 174.2ms image 91/178 /yolov5/data/images/R_800_jpg.rf.9a599c861ee9b7a53af5ce4193377510.jpg: 640x640 (no detections), 173.7ms image 92/178 /yolov5/data/images/R_817_jpg.rf.f6df4fb60fd3717fcfb693f1d649b162.jpg: 640x640 (no detections), 175.3ms image 93/178 /yolov5/data/images/R_822_jpg.rf.c2d123175a8ea14215f02c9a1c447885.jpg: 640x640 (no detections), 173.9ms image 94/178 /yolov5/data/images/R_824_jpg.rf.13ca71bb6588f11a32f8186fead3eefc.jpg: 640x640 (no detections), 174.3ms image 95/178 /yolov5/data/images/R_828_jpg.rf.e611245ac1c4a0b2916508558938c8f4.jpg: 640x640 (no detections), 173.7ms image 96/178 /yolov5/data/images/R_836_jpg.rf.89de262510723ccbe175b790cb357c75.jpg: 640x640 (no detections), 173.8ms image 97/178 /yolov5/data/images/R_844_jpg.rf.32205fbe01479b9c76b6852672f9d255.jpg: 640x640 1 traffic light, 175.5ms image 98/178 /yolov5/data/images/R_850_jpg.rf.9bcdca851abd44dc5e973100fc6a75aa.jpg: 640x640 (no detections), 173.8ms image 99/178 /yolov5/data/images/a154e31fe6_jpg.rf.5b66585559baa1322d4b5cc71e001c0e.jpg: 640x640 (no detections), 173.7ms image 100/178 /yolov5/data/images/a4e50ecbed_jpg.rf.dde1bc6908d4c2d1471c080508d7ee6f.jpg: 640x640 (no detections), 173.8ms image 101/178 /yolov5/data/images/acca057365_jpg.rf.8ecbe957192bafc13d400d97e466a791.jpg: 640x640 (no detections), 173.9ms image 102/178 /yolov5/data/images/ba16ec227a_jpg.rf.d16312e19f402e73b6b9180b51d922b3.jpg: 640x640 (no detections), 175.5ms image 103/178 /yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, 135.9ms image 104/178 /yolov5/data/images/cd4b64e1fb_jpg.rf.b51b79f65aa4d912966e0964e82ce92c.jpg: 640x640 1 orange, 173.8ms image 105/178 /yolov5/data/images/eff91468db_jpg.rf.828693b562d8590d4d58fa655d2394fe.jpg: 640x640 (no detections), 173.6ms image 106/178 /yolov5/data/images/scene00019_png.rf.e0766b2d34d4a6f948615c3ac6963265.jpg: 640x640 1 person, 8 cars, 3 buss, 173.9ms image 107/178 /yolov5/data/images/scene09495_png.rf.ab4e13fa24076feeee2d34f48517e0d3.jpg: 640x640 2 persons, 5 cars, 1 truck, 173.7ms image 108/178 /yolov5/data/images/scene09496_png.rf.1b02bc511e308bbee1db664fa9a16ace.jpg: 640x640 1 person, 5 cars, 1 truck, 175.4ms image 109/178 /yolov5/data/images/scene09498_png.rf.5036160da3c11dbad737e770d1d3e602.jpg: 640x640 2 persons, 5 cars, 1 truck, 173.5ms image 110/178 /yolov5/data/images/scene09500_png.rf.8404d4fbbded2342ad7ba0d53773ebf2.jpg: 640x640 2 persons, 5 cars, 1 truck, 174.6ms image 111/178 /yolov5/data/images/scene09509_png.rf.b8263f519d2f424fcf6e59b41bb84f1f.jpg: 640x640 2 persons, 6 cars, 1 truck, 175.5ms image 112/178 /yolov5/data/images/scene09515_png.rf.dda67a9cea5490d5577ea1e3d3979d90.jpg: 640x640 3 persons, 5 cars, 1 umbrella, 173.8ms image 113/178 /yolov5/data/images/scene09516_png.rf.a386ffdd24fb4efd118a4aa1cad8757d.jpg: 640x640 6 cars, 1 umbrella, 175.3ms image 114/178 /yolov5/data/images/scene09518_png.rf.0f3c4c99ef3eda67ce2ff87fe4717409.jpg: 640x640 2 persons, 5 cars, 173.7ms image 115/178 /yolov5/data/images/scene09520_png.rf.782b4dd7aa3fad0595f19aa9da5bb20d.jpg: 640x640 5 cars, 1 truck, 1 umbrella, 173.7ms image 116/178 /yolov5/data/images/scene09522_png.rf.28f1821e09927a64004ddbc374f9823f.jpg: 640x640 1 person, 4 cars, 1 truck, 1 umbrella, 175.8ms image 117/178 /yolov5/data/images/scene09530_png.rf.d559251926cfcd89f9d211a8beb4f7ab.jpg: 640x640 2 persons, 3 cars, 1 truck, 173.8ms image 118/178 /yolov5/data/images/scene09531_png.rf.09abc26f32b2fc3de830bd7f239161e6.jpg: 640x640 2 persons, 3 cars, 1 truck, 173.8ms image 119/178 /yolov5/data/images/scene09532_png.rf.b0197674f666401c30a44726bcde8485.jpg: 640x640 1 person, 3 cars, 173.6ms image 120/178 /yolov5/data/images/scene09535_png.rf.de8d53cb963409a74896594c03020c2a.jpg: 640x640 2 persons, 5 cars, 174.0ms image 121/178 /yolov5/data/images/scene09536_png.rf.e910e7bf6f31c3a14e96782d28c6b860.jpg: 640x640 1 person, 3 cars, 1 airplane, 1 truck, 1 umbrella, 175.2ms image 122/178 /yolov5/data/images/scene09541_png.rf.7afde91d0f691a37ef072933bc498c4e.jpg: 640x640 1 person, 5 cars, 173.9ms image 123/178 /yolov5/data/images/scene09546_png.rf.832211b3c2645e95f2b9f81adc1e1cb4.jpg: 640x640 1 person, 3 cars, 1 truck, 173.9ms image 124/178 /yolov5/data/images/scene09547_png.rf.e0f40ab93a68a1a6bfca5043d0580ee0.jpg: 640x640 1 person, 3 cars, 1 truck, 173.7ms image 125/178 /yolov5/data/images/scene09556_png.rf.0835a0fc45c315e769adb382dfafe754.jpg: 640x640 1 person, 3 cars, 1 truck, 173.7ms image 126/178 /yolov5/data/images/scene09567_png.rf.621dbc0d981a64c5d1fbe7c8d1c76c23.jpg: 640x640 1 person, 1 car, 175.7ms image 127/178 /yolov5/data/images/scene09570_png.rf.1fdd0d8bf5a4241588bf092050d72973.jpg: 640x640 1 person, 2 cars, 174.3ms image 128/178 /yolov5/data/images/scene09572_png.rf.d33accd317e6a07f74dfc0c312c567cf.jpg: 640x640 1 person, 3 cars, 173.8ms image 129/178 /yolov5/data/images/scene09579_png.rf.c4458a73875a0c182fa962993361eb82.jpg: 640x640 1 person, 2 cars, 173.7ms image 130/178 /yolov5/data/images/scene09582_png.rf.7d2065bc8b1d2594c749ffa41d29d53c.jpg: 640x640 3 cars, 1 motorcycle, 173.7ms image 131/178 /yolov5/data/images/scene09583_png.rf.6e5a4cc3c344704b66a0da3ca2c6cc0c.jpg: 640x640 3 cars, 175.2ms image 132/178 /yolov5/data/images/scene09589_png.rf.ae8911a21a097556b71f9de5b4c8f107.jpg: 640x640 3 cars, 174.3ms image 133/178 /yolov5/data/images/scene09593_png.rf.7dcf4ed96862e0023e587d01dee5a334.jpg: 640x640 2 cars, 1 truck, 173.8ms image 134/178 /yolov5/data/images/scene09594_png.rf.4c2638da794b61ef172bf9ee718a9d48.jpg: 640x640 2 cars, 1 truck, 174.0ms image 135/178 /yolov5/data/images/scene09595_png.rf.f2ba73226ef1eb918eb6ed38079d90ea.jpg: 640x640 1 person, 3 cars, 1 truck, 173.7ms image 136/178 /yolov5/data/images/scene09598_png.rf.3d6ec787051a3895d2cc99b96300c662.jpg: 640x640 1 person, 2 cars, 1 truck, 175.3ms image 137/178 /yolov5/data/images/scene09600_png.rf.7a705dc70e42ca8f266b31d23375842f.jpg: 640x640 1 person, 4 cars, 1 truck, 174.8ms image 138/178 /yolov5/data/images/scene09609_png.rf.5be75c03fb0e183d73949eaee7aea74b.jpg: 640x640 1 person, 4 cars, 1 backpack, 173.9ms image 139/178 /yolov5/data/images/scene09610_png.rf.ffe992e5790258415830acdbc2f640c5.jpg: 640x640 2 persons, 4 cars, 1 truck, 173.7ms image 140/178 /yolov5/data/images/scene09612_png.rf.70fe9178c6e70ffb024da4a986579f22.jpg: 640x640 2 persons, 4 cars, 173.8ms image 141/178 /yolov5/data/images/scene09615_png.rf.dd434d830e71f9c35f496403120be549.jpg: 640x640 1 person, 2 cars, 1 truck, 1 traffic light, 174.1ms image 142/178 /yolov5/data/images/scene09618_png.rf.cef06e0e8001b3403ce533aa52f04c7e.jpg: 640x640 2 persons, 3 cars, 175.4ms image 143/178 /yolov5/data/images/scene09619_png.rf.ee8cda26e47b4e3fddc4702372ccfc99.jpg: 640x640 2 persons, 2 cars, 173.7ms image 144/178 /yolov5/data/images/scene09620_png.rf.8142b54d8c79ae256c91954d6429b21d.jpg: 640x640 1 person, 3 cars, 173.8ms image 145/178 /yolov5/data/images/scene09621_png.rf.bcffe3c53f7af2c8eed1921d97667437.jpg: 640x640 2 persons, 3 cars, 1 traffic light, 173.7ms image 146/178 /yolov5/data/images/scene09624_png.rf.78371bf219b7e0d391cfd615277a73c6.jpg: 640x640 2 cars, 1 traffic light, 173.8ms image 147/178 /yolov5/data/images/scene09627_png.rf.0c98638bee2e1ca83fe798704afa3258.jpg: 640x640 1 person, 4 cars, 175.6ms image 148/178 /yolov5/data/images/scene09628_png.rf.7ed1777045097c82d4556b1181e0eca9.jpg: 640x640 1 person, 3 cars, 1 traffic light, 173.8ms image 149/178 /yolov5/data/images/scene09639_png.rf.2b4f6aae5ad401e3f55bb573df415103.jpg: 640x640 2 persons, 4 cars, 173.7ms image 150/178 /yolov5/data/images/scene09642_png.rf.318f2a88a4ac410dfe591d5c514f25d1.jpg: 640x640 2 persons, 3 cars, 173.9ms image 151/178 /yolov5/data/images/scene09647_png.rf.54795643766df49f93b426f0305c5a0d.jpg: 640x640 2 persons, 2 cars, 174.5ms image 152/178 /yolov5/data/images/scene09654_png.rf.3648fca642e9404178fe30b6ae084f57.jpg: 640x640 1 person, 3 cars, 175.6ms image 153/178 /yolov5/data/images/scene09657_png.rf.1ff6a9b227119639d8992e1d5fa8eb40.jpg: 640x640 1 person, 4 cars, 173.7ms image 154/178 /yolov5/data/images/scene09658_png.rf.cf55d26e754abb729f7d656fe913e622.jpg: 640x640 1 person, 2 cars, 1 truck, 173.9ms image 155/178 /yolov5/data/images/scene09660_png.rf.67b091f2b4bcf9e622c38d6c087d5da8.jpg: 640x640 1 person, 4 cars, 176.4ms image 156/178 /yolov5/data/images/scene09664_png.rf.97685d2c31d78f8b6a10a79abdafeae1.jpg: 640x640 1 person, 4 cars, 173.6ms image 157/178 /yolov5/data/images/scene09666_png.rf.8a4eca9347354c503f5e3ef3b26ad122.jpg: 640x640 2 persons, 3 cars, 175.5ms image 158/178 /yolov5/data/images/scene09668_png.rf.5574ec124c328b1cd5892ea9477d6ff2.jpg: 640x640 4 cars, 173.9ms image 159/178 /yolov5/data/images/scene09669_png.rf.8e3d904d8bfbf01da76d5de9162955f1.jpg: 640x640 3 cars, 174.2ms image 160/178 /yolov5/data/images/scene09676_png.rf.686d9bad5be73513e12c097ec13073c8.jpg: 640x640 4 cars, 1 truck, 175.1ms image 161/178 /yolov5/data/images/scene09677_png.rf.06af75dd3863d08479a33c29b2080db6.jpg: 640x640 4 cars, 1 truck, 173.6ms image 162/178 /yolov5/data/images/scene09679_png.rf.6ff89cdcaffd78fff200ec571aae4b10.jpg: 640x640 3 cars, 175.3ms image 163/178 /yolov5/data/images/scene09681_png.rf.6c080a847fd627540ff23b9d430fed6d.jpg: 640x640 3 cars, 173.7ms image 164/178 /yolov5/data/images/scene09683_png.rf.5166c1a4004d5b9b0879cf398d9f86e2.jpg: 640x640 3 cars, 1 suitcase, 173.7ms image 165/178 /yolov5/data/images/scene09688_png.rf.d6f641ea03ab54054c3004bf0dce6e97.jpg: 640x640 4 cars, 1 traffic light, 175.5ms image 166/178 /yolov5/data/images/scene09691_png.rf.3b4c0dd805c9913e2a4e3f22fd2874cc.jpg: 640x640 4 cars, 173.9ms image 167/178 /yolov5/data/images/scene09696_png.rf.1b365e59f89261a366fbed11bb5c1770.jpg: 640x640 4 cars, 175.2ms image 168/178 /yolov5/data/images/scene09697_png.rf.d97ff2fce538d3070cc8f75d3c6e655d.jpg: 640x640 4 cars, 173.9ms image 169/178 /yolov5/data/images/scene09700_png.rf.e4e061b52d54e8f3733e576b1dc80f8a.jpg: 640x640 4 cars, 173.9ms image 170/178 /yolov5/data/images/scene09702_png.rf.87c99933afaa2329aaf72a2b7599635c.jpg: 640x640 4 cars, 175.4ms image 171/178 /yolov5/data/images/scene09703_png.rf.67a0b2d23ccefb0e7649ddcb87ff0001.jpg: 640x640 4 cars, 174.1ms image 172/178 /yolov5/data/images/scene09705_png.rf.9bd3ead27268a7db05d3e4d26f4fe341.jpg: 640x640 3 cars, 173.9ms image 173/178 /yolov5/data/images/scene09706_png.rf.10a428ee4340eada6875ec210a250015.jpg: 640x640 1 person, 3 cars, 173.6ms image 174/178 /yolov5/data/images/scene09715_png.rf.3cbd522a533436d41a62a0411b7ee820.jpg: 640x640 1 person, 3 cars, 173.7ms image 175/178 /yolov5/data/images/scene09718_png.rf.d870455f822012d818785090941c77d7.jpg: 640x640 1 person, 3 cars, 175.2ms image 176/178 /yolov5/data/images/scene15712_png.rf.fe211cdb63d359cdebc7dea53ee475d4.jpg: 640x640 12 cars, 1 bus, 1 truck, 1 parking meter, 173.6ms image 177/178 /yolov5/data/images/scene15715_png.rf.15fbe331f13ea8b104f3a699847c94dd.jpg: 640x640 10 cars, 2 buss, 173.8ms image 178/178 /yolov5/data/images/zidane.jpg: 384x640 2 persons, 1 tie, 104.7ms Speed: 3.1ms pre-process, 173.7ms inference, 10.6ms NMS per image at shape (1, 3, 640, 640) It is using CUDA, not TensorRT yet, but from this (reference from seedstudio) I expect an average of 50 ms per inference and my result has a poor performance of 170ms I have tried the same libraries without docker. Similar results: MAXN - 130ms 5W - 180ms What could be the cause of the poor performance? This is from the reference...",
        "answers": [
            [
                "The clue was to use a USB camera 640x480, this way i'm getting 0.060s average with model yolov5n. The delay was caused by the file system retrieving images from disk. I had to merge instructions from https://wiki.seeedstudio.com/YOLOv5-Object-Detection-Jetson/#inference-on-jetson-device and https://www.forecr.io/blogs/ai-algorithms/how-to-run-yolov5-real-time-object-detection-on-pytorch-with-docker-on-nvidia-jetson-modules"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm using jetson nano device that has GPU. &gt;&gt;&gt; import torch &gt;&gt;&gt; import torchvision &gt;&gt;&gt; torch.__version__ '1.8.0' &gt;&gt;&gt; torchvision.__version__ '0.9.0a0+01dfa8e' &gt;&gt;&gt; torch.cuda.is_available() True &gt;&gt;&gt; torch.cuda.device_count() 1 &gt;&gt;&gt; torch.cuda.current_device() 0 &gt;&gt;&gt; torch.cuda.get_device_name(0) 'NVIDIA Tegra X1' &gt;&gt;&gt; torch.cuda.memory_allocated(0) 0 &gt;&gt;&gt; torch.cuda.memory_reserved(0) 0 &gt;&gt;&gt; torch.cuda.max_memory_reserved(0) 0 &gt;&gt;&gt; torch.cuda.max_memory_allocated(0) 0 the issue is when I use method .to(device) the script get Killed &gt;&gt;&gt; torch.rand(3) tensor([0.6947, 0.2633, 0.7335]) &gt;&gt;&gt; device = torch.device(0 if (torch.cuda.is_available()) else \"cpu\") &gt;&gt;&gt; device device(type='cuda', index=0) &gt;&gt;&gt; torch.rand(3).to(device) Killed when I checked dmesg [3482376.053248] Out of memory: Kill process 19544 (python) score 247 or sacrifice child [3482376.067301] Killed process 19544 (python) total-vm:11339744kB, anon-rss:1200112kB, file-rss:303524kB, shmem-rss:0kB Side Note: I have 17GB available in the memory. Please advice.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to do video recording on Jetson Nano. I found that when the cv2.VideoWriter using write() the program becomes slow. How can I speed up the cv2.VideoWriter? Maybe use Gstreamer or GPU or nvidia-component can speed up the FPS. My Code: import os import time import cv2 width = 2560 height = 1440 framerate = 30 video_path = 'fps_test.mp4' gs_pipeline = f\"v4l2src device=/dev/video0 io-mode=2 \" \\ f\"! image/jpeg, width={width}, height={height}, framerate={framerate}/1, format=MJPG \" \\ f\"! nvv4l2decoder mjpeg=1 \" \\ f\"! nvvidconv flip-method=4 \" \\ f\"! video/x-raw, format=BGRx \" \\ f\"! videoconvert \" \\ f\"! video/x-raw, format=BGR \" \\ f\"! appsink drop=1\" gs_pipeline = gs_pipeline print(f\"gst-launch-1.0 {gs_pipeline}\\n\") v_cap = v_writer = None def main(): global v_cap, v_writer codec = cv2.VideoWriter_fourcc(*'mp4v') v_writer = cv2.VideoWriter(video_path, codec, 20, (width, height)) v_cap = cv2.VideoCapture(gs_pipeline, cv2.CAP_GSTREAMER) prev_frame_fetched_time = 0 while v_cap.isOpened(): ret_val, frame = v_cap.read() if not ret_val: break v_writer.write(frame) curr_frame_fetched_time = time.time() curr_fps = 1 / (curr_frame_fetched_time - prev_frame_fetched_time) prev_frame_fetched_time = curr_frame_fetched_time print(f\"FPS = {curr_fps:.3f}\") v_cap.release() cv2.destroyAllWindows() if __name__ == '__main__': try: main() except KeyboardInterrupt: # If CTRL+C is pressed, exit cleanly: pass finally: v_cap.release() cv2.destroyAllWindows() if os.path.exists(video_path): os.remove(video_path) Console: FPS = 0.000 FPS = 2.438 FPS = 3.606 FPS = 2.805 FPS = 2.741 FPS = 3.265 FPS = 3.603 FPS = 3.524 FPS = 3.290",
        "answers": [
            [
                "You may first try a pure gstreamer recording with: gst-launch-1.0 -ev v4l2src device=/dev/video0 io-mode=2 ! image/jpeg, width={width}, height={height}, framerate={framerate}/1, format=MJPG ! nvv4l2decoder mjpeg=1 ! nvvidconv flip-method=2 ! nvv4l2h264enc ! h264parse ! qtmux ! filesink location={video_path} If this works, you may try to split into opencv VideoCapture and VideoWriter, assuming you're processing BGR frames: v_cap_str = f\"v4l2src device=/dev/video0 io-mode=2 \" \\ f\"! image/jpeg, width={width}, height={height}, framerate={framerate}/1, format=MJPG \" \\ f\"! nvv4l2decoder mjpeg=1 \" \\ f\"! nvvidconv flip-method=2 ! video/x-raw,format=BGRx \" \\ f\"! videoconvert ! video/x-raw,format=BGR \" \\ f\"! queue ! appsink drop=1 \" v_cap = cv2.VideoCapture(v_cap_str, cv2.CAP_GSTREAMER) and VideoWriter: v_wrt_str = f\"appsrc ! video/x-raw, format=BGR ! queue \" \\ f\"! videoconvert ! video/x-raw,format=BGRx \" \\ f\"! nvvidconv ! video/x-raw(memory:NVMM),format=NV12 \" \\ f\"! nvv4l2h264enc \" \\ f\"! h264parse \" \\ f\"! qtmux \" \\ f\"! filesink location={video_path}\" v_wrt = cv2.VideoWriter(v_wrt_str, cv2.CAP_GSTREAMER, 0, {framerate as float}, ({width as int}, {height as int})) Check that both capture and writer are opened, then in loop read frames from capture and push these into writer."
            ],
            [
                "One way to potentially speed up the video recording with cv2.VideoWriter is to use hardware acceleration through the NVIDIA Video Codec SDK. You can try using the nvv4l2h264enc GStreamer element with OpenCV's cv2.VideoWriter to encode the video in H.264 format using the hardware encoder on the Jetson Nano. import os import time import cv2 width = 2560 height = 1440 framerate = 30 video_path = 'fps_test.mp4' gs_pipeline = f\"v4l2src device=/dev/video0 io-mode=2 \" \\ f\"! image/jpeg, width={width}, height={height}, framerate={framerate}/1, format=MJPG \" \\ f\"! nvv4l2decoder mjpeg=1 \" \\ f\"! nvvidconv flip-method=4 \" \\ f\"! video/x-raw, format=BGRx \" \\ f\"! videoconvert \" \\ f\"! video/x-raw, format=BGR \" \\ f\"! nvvidconv \" \\ f\"! 'video/x-raw(memory:NVMM), format=I420' \" \\ f\"! nvv4l2h264enc \" \\ f\"! h264parse \" \\ f\"! mp4mux \" \\ f\"! filesink location={video_path}\" print(f\"gst-launch-1.0 {gs_pipeline}\\n\") v_cap = v_writer = None def main(): global v_cap, v_writer v_cap = cv2.VideoCapture(gs_pipeline, cv2.CAP_GSTREAMER) while v_cap.isOpened(): ret_val, frame = v_cap.read() if not ret_val: break cv2.imshow('frame', frame) key = cv2.waitKey(1) v_cap.release() cv2.destroyAllWindows() if __name__ == '__main__': try: main() except KeyboardInterrupt: pass finally: v_cap.release() cv2.destroyAllWindows() if os.path.exists(video_path): os.remove(video_path)"
            ],
            [
                "You can multi-threading. So one thread for getting camera frames and another one writing the frames. As both the thread will be running independently there should be no lag/slow down."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "According to docummentation Accelerated_GStreamer_User_Guide_Release_24.2.1, nvvidconv provide 7 method to flip the frame. I want to do nvvidconv flip-method=4 and nvvidconv flip-method=6 at the same time. How should I write the gstreamer pipeline in Python with cv2 on Jetson Nano? The Code: import cv2 width = 640 height = 480 framerate = 30 gs_pipeline = f\"v4l2src device=/dev/video0 io-mode=2 \" \\ f\"! image/jpeg, width={width}, height={height}, framerate={framerate}/1, format=MJPG \" \\ f\"! nvv4l2decoder mjpeg=1 \" \\ f\"! nvvidconv flip-method=[4, 6] \" \\ f\"! video/x-raw, format=BGRx \" \\ f\"! videoconvert \" \\ f\"! video/x-raw, format=BGR \" \\ f\"! appsink drop=1\" gs_pipeline = gs_pipeline print(f\"gst-launch-1.0 {gs_pipeline}\\n\") v_cap = cv2.VideoCapture(gs_pipeline, cv2.CAP_GSTREAMER) if not v_cap.isOpened(): print(\"failed to open video capture\") v_cap.release() exit(-1) while v_cap.isOpened(): ret_val, frame = v_cap.read() if not ret_val: break cv2.imshow('', frame) input_key = cv2.waitKey(1) if input_key != -1: print(f\"input key = {input_key}\") if input_key == ord('q'): break v_cap.release() cv2.destroyAllWindows()",
        "answers": [
            [
                "Flipping horizontally and vertically would be equivalent to 180\u00b0 rotation, so just use: ... ! nvvidconv flip-method=2 ! ..."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am attempting to utilize the power of nvidia components to facilitate faster recording of video footage through the use of a usb-camera. To achieve this, I am taking advantage of the advanced acceleration capabilities that nvidia offers, which are specifically designed to enhance the performance of video recording. This should allow me to capture high-quality video quickly and efficiently, with minimal fuss. With this setup, I am confident that I will be able to record the best possible video recordings, without sacrificing on quality or speed. My Code: import cv2 width = 2560 height = 1440 framerate = 30 gs_pipeline = f\"nvv4l2camerasrc device=/dev/video0 \" \\ f\"! video/x-raw(memory:NVMM), width={width}, height={height}, format=MJPG, framerate={framerate}/1 \" \\ f\"! nvvidconv \" \\ f\"! video/x-raw, format=(string)BGRx \" \\ f\"! videoconvert \" \\ f\"! video/x-raw, format=(string)BGR \" \\ f\"! appsink\" v_cap = cv2.VideoCapture(gs_pipeline, cv2.CAP_GSTREAMER) if not v_cap.isOpened(): print(\"failed to open video capture\") exit(-1) while v_cap.isOpened(): ret_val, frame = v_cap.read() if not ret_val: break cv2.imshow('', frame) input_key = cv2.waitKey(1) if input_key != -1: print(f\"input key = {input_key}\") if input_key == ord('q'): break v_cap.release() cv2.destroyAllWindows() Error I got: [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (711) open OpenCV | GStreamer warning: Error opening bin: could not link nvv4l2camerasrc0 to nvvconv0, neither element can handle caps video/x-raw(memory:NVMM), width=(int)2560, height=(int)1440, format=(string)MJPG, framerate=(fraction)30/1",
        "answers": [
            [
                "That error occurs because nvv4l2camerasrc can't handle those caps, in specific due to format=MJPG parameter. You can figure out what caps and formats the element supports by simply inspecting on the element with the GSt Inspect tool: gst-inspect-1.0 nvv4l2camerasrc ... SRC template: 'src' Availability: Always Capabilities: video/x-raw(memory:NVMM) width: [ 1, 2147483647 ] height: [ 1, 2147483647 ] format: { (string)UYVY } interlace-mode: { (string)progressive, (string)interlaced } framerate: [ 0/1, 2147483647/1 ] As you can see it could be UYVY. So, changing MJPG to UYVY it's the what you want. Additionally, you would want to retrieve your camera caps capabilities with the following V4L2 tools command, per example: v4l2-ctl --device /dev/video0 --all ... Format Video Capture: Width/Height : 1280/960 Pixel Format : 'YUYV' (YUYV 4:2:2) Field : None Bytes per Line : 2560 Size Image : 2457600 Colorspace : sRGB Transfer Function : Rec. 709 YCbCr/HSV Encoding: ITU-R 601 Quantization : Default (maps to Limited Range) Make sure the width and height you're pulling from the camera it's accord to the output you get from this command."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "My goal is to use gstreamer pipeline instead of libcamera-still. The problem is that the frames generated by the gstreamer pipeline look concave. Gstreamer Pipeline def gstreamer_pipeline( # Issue: the sensor format used by Raspberry Pi 4B and NVIDIA Jetson Nano B01 are different # in Raspberry Pi 4B, this command # $ libcamera-still --width 1280 --height 1280 --mode 1280:1280 # uses sensor format 2328x1748. # However, v4l2-ctl --list-formats-ext do not have such format. sensor_id=0, capture_width=1920, capture_height=1080, display_width=640, display_height=360, framerate=21, flip_method=0, ): return ( \"nvarguscamerasrc sensor-id=%d ! \" \"video/x-raw(memory:NVMM),format=(string)NV12,framerate=(fraction)%d/1 ! \" \"nvvidconv flip-method=%d ! \" \"video/x-raw,width=(int)%d,height=(int)%d,format=(string)BGRx ! \" \"videoconvert ! \" \"video/x-raw,format=(string)BGR ! \" \"appsink\" % ( sensor_id, framerate, flip_method, display_width, display_height ) ) The result with gstreamer pipeline The code I run to take frame libcamera-still -t 5000 --width 1280 --height 1280 --mode 1280:1280 --autofocus-on-capture -o test.jpg The result with libcamera-still",
        "answers": [],
        "votes": []
    },
    {
        "question": "My goal is to accelerate the USB-CAMERA on JetsonNano with Python cv2 package. Trying to use some gstreaner plugin like nvv4l2decoder or nvjpegdec or (memory:NVMM). Python Code: import cv2 width = 1920 height = 1080 gs_pipeline = f\"v4l2src device=/dev/video0 io-mode=2 \" \\ f\"! image/jpeg, width={width}, height={height}\" \\ f\"! nvv4l2decoder mjpeg=1 \" \\ f\"! nvvidconv \" \\ f\"! video/x-raw(memory:NVMM) format=BGR\" \\ f\"! videoconvert \" \\ f\"! video/x-raw, format=BGR \" \\ f\"! appsink\" v_cap = cv2.VideoCapture(gs_pipeline, cv2.CAP_GSTREAMER) if not v_cap.isOpened(): print(\"failed to open video capture\") exit(-1) while v_cap.isOpened(): ret_val, frame = v_cap.read() if not ret_val: break cv2.imshow('', frame) input_key = cv2.waitKey(1) if input_key != -1: print(f\"input key = {input_key}\") if input_key == ord('q'): break Error: [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (711) open OpenCV | GStreamer warning: Error opening bin: could not parse caps \"video/x-raw(memory:NVMM) format=BGR\" [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created Another point to mention, some of the gst-launch-1.0 command can work, but it does not mean it can work in python as well. The ultimate goal is to use python cv2 to control the camera. Only Command Line Worked Example: gst-launch-1.0 v4l2src device=/dev/video0 io-mode=2 ! image/jpeg, width=1920, height=1080, framerate=30/1, format=MJPG ! nvjpegdec ! 'video/x-raw(memory:NVMM),format=I420,width=1920,height=1080,framerate=30/1' ! nvegltransform ! nveglglessink",
        "answers": [
            [
                "nvvidconv doesn't support BGR, only BGRx (thus the videoconvert for BGRx-&gt;BGR). Caps also lacks a comma. Last, videoconvert only supports system memory, so have nvvidconv to output into system memory rather than NVMM memory. So change: gs_pipeline = f\"v4l2src device=/dev/video0 io-mode=2 \" \\ f\"! image/jpeg, width={width}, height={height}, framerate=30/1, format=MJPG \" \\ f\"! nvv4l2decoder mjpeg=1 \" \\ f\"! nvvidconv \" \\ f\"! video/x-raw, format=BGRx \" \\ f\"! videoconvert \" \\ f\"! video/x-raw, format=BGR \" \\ f\"! appsink drop=1\""
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Hello I am trying to do a human-tracking drone and I am using Nvidia Jetson nano and pixhawk 4 . lately i learned that arm platforms do not support mavsdk-python. Is there any way to use Jetson nano and mavsdk together ? And I tried to install mavsdk-python to Jetson nano but I couldn't.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm new to ffmpeg but I've got an odd issue which seems to arise from using it on different Ubuntu versions. I have an NVIDIA Jetson Nano running Ubuntu 18.04.5 LTS (GNU/Linux 4.9.201-tegra aarch64). Plugged into the Nano's carrier board is an embedded camera that shows up as a Sunplus Innovation Technology Inc. USB2.0 Camera RGB when I run lsusb. When I run this ffmpeg command, I can successfully record video from the camera, which seems to be mjpeg codec with yuvj422p pixel format. The output is as follows: ffmpeg -f video4linux2 -i /dev/video0 -an -vcodec libx264 test_capture.mp4 ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers built with gcc 7 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/aarch64-linux-gnu --incdir=/usr/include/aarch64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared libavutil 55. 78.100 / 55. 78.100 libavcodec 57.107.100 / 57.107.100 libavformat 57. 83.100 / 57. 83.100 libavdevice 57. 10.100 / 57. 10.100 libavfilter 6.107.100 / 6.107.100 libavresample 3. 7. 0 / 3. 7. 0 libswscale 4. 8.100 / 4. 8.100 libswresample 2. 9.100 / 2. 9.100 libpostproc 54. 7.100 / 54. 7.100 Input #0, video4linux2,v4l2, from '/dev/video0': Duration: N/A, start: 166.142773, bitrate: N/A Stream #0:0: Video: mjpeg, yuvj422p(pc, bt470bg/unknown/unknown), 928x400, 100 fps, 100 tbr, 1000k tbn, 1000k tbc Stream mapping: Stream #0:0 -&gt; #0:0 (mjpeg (native) -&gt; h264 (libx264)) Press [q] to stop, [?] for help [libx264 @ 0x558553df70] using cpu capabilities: ARMv8 NEON [libx264 @ 0x558553df70] profile High 4:2:2, level 3.2, 4:2:2 8-bit [libx264 @ 0x558553df70] 264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00 Output #0, mp4, to 'test_capture.mp4': Metadata: encoder : Lavf57.83.100 Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj422p(pc), 928x400, q=-1--1, 100 fps, 12800 tbn, 100 tbc Metadata: encoder : Lavc57.107.100 libx264 Side data: cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1 frame= 192 fps= 65 q=-1.0 Lsize= 151kB time=00:00:01.89 bitrate= 654.4kbits/s dup=150 drop=0 speed=0.642x video:148kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 2.072495% [libx264 @ 0x558553df70] frame I:1 Avg QP:23.62 size: 36953 [libx264 @ 0x558553df70] frame P:49 Avg QP:25.25 size: 2121 [libx264 @ 0x558553df70] frame B:142 Avg QP:30.55 size: 70 [libx264 @ 0x558553df70] consecutive B-frames: 1.0% 1.0% 0.0% 97.9% [libx264 @ 0x558553df70] mb I I16..4: 10.1% 78.8% 11.1% [libx264 @ 0x558553df70] mb P I16..4: 1.1% 1.9% 0.0% P16..4: 19.1% 3.0% 2.0% 0.0% 0.0% skip:72.8% [libx264 @ 0x558553df70] mb B I16..4: 0.0% 0.0% 0.0% B16..8: 2.7% 0.0% 0.0% direct: 0.1% skip:97.2% L0:36.8% L1:62.3% BI: 0.9% [libx264 @ 0x558553df70] 8x8 transform intra:68.4% inter:77.4% [libx264 @ 0x558553df70] coded y,uvDC,uvAC intra: 42.0% 80.2% 41.3% inter: 2.2% 5.4% 0.2% [libx264 @ 0x558553df70] i16 v,h,dc,p: 50% 17% 10% 23% [libx264 @ 0x558553df70] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 21% 9% 32% 5% 7% 7% 5% 8% 5% [libx264 @ 0x558553df70] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 24% 15% 12% 5% 14% 11% 8% 5% 7% [libx264 @ 0x558553df70] i8c dc,h,v,p: 50% 12% 26% 12% [libx264 @ 0x558553df70] Weighted P-Frames: Y:28.6% UV:28.6% [libx264 @ 0x558553df70] ref P L0: 56.4% 28.2% 12.0% 2.5% 0.9% [libx264 @ 0x558553df70] ref B L0: 93.4% 5.3% 1.3% [libx264 @ 0x558553df70] ref B L1: 94.9% 5.1% [libx264 @ 0x558553df70] kb/s:628.21 Exiting normally, received signal 2. However, I'd like to replace this Nano with another that I've upgraded to Ubuntu 22.04.1 LTS (GNU/Linux 4.9.299-tegra aarch64). The device still shows up as the same type when running lsusb. Running the exact same command results in the following: ffmpeg -f video4linux2 -i /dev/video0 -an -vcodec libx264 test_capture.mp4 ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers built with gcc 11 (Ubuntu 11.2.0-19ubuntu1) configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/aarch64-linux-gnu --incdir=/usr/include/aarch64-linux-gnu --arch=arm64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared libavutil 56. 70.100 / 56. 70.100 libavcodec 58.134.100 / 58.134.100 libavformat 58. 76.100 / 58. 76.100 libavdevice 58. 13.100 / 58. 13.100 libavfilter 7.110.100 / 7.110.100 libswscale 5. 9.100 / 5. 9.100 libswresample 3. 9.100 / 3. 9.100 libpostproc 55. 9.100 / 55. 9.100 [video4linux2,v4l2 @ 0x55cab86450] Could not find codec parameters for stream 0 (Video: mjpeg, none(bt470bg/unknown/unknown), 1856x800): unspecified pixel format Consider increasing the value for the 'analyzeduration' (0) and 'probesize' (5000000) options Input #0, video4linux2,v4l2, from '/dev/video0': Duration: N/A, bitrate: N/A Stream #0:0: Video: mjpeg, none(bt470bg/unknown/unknown), 1856x800, 60 fps, 60 tbr, 1000k tbn, 1000k tbc Stream mapping: Stream #0:0 -&gt; #0:0 (mjpeg (native) -&gt; h264 (libx264)) Press [q] to stop, [?] for help Cannot determine format of input stream 0:0 after EOF Error marking filters as finished Exiting normally, received signal 2. I'm not sure why I can't read this video stream when everything seems to be the same but the OS and the ffmpeg version. I've checked the available codecs and pixel formats using ffmpeg -codecs and ffmpeg -pix_fmts: Ubuntu 18.04.5, ffmpeg version 3.4.8-0ubuntu0.2: DEVIL. mjpeg Motion JPEG (encoders: mjpeg mjpeg_vaapi ) IO... yuvj422p 3 16 Ubuntu 22.04.1, ffmpeg version 4.4.2-0ubuntu0.22.04.1: DEVIL. mjpeg Motion JPEG (decoders: mjpeg mjpeg_cuvid ) (encoders: mjpeg mjpeg_vaapi ) IO... yuvj422p 3 16 So it seems like I should be able to record video on both with this codec/pixel format combination. I've also tried forcing ffmpeg to use this combination on the 22.04 Nano with the following command: ffmpeg -f v4l2 -input_format mjpeg -framerate 100 -video_size 928x400 -pix_fmt yuvj422p -i /dev/video0 -an -vcodec libx264 test_capture.mp4 But I get the same error. I've also tried increasing the -analyzeduration and -probesize arguments to 100M, with no luck. Are there other commands or settings I should use? Should I downgrade my ffmpeg version if possible?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Actually this value is calculated by Jetson nano and send to arduino for further calculations of angles and for rotating the servos. Does anyone have any clue ? I tried using ROS-Melodic but can't understand how these kind of communication is possible.",
        "answers": [],
        "votes": []
    },
    {
        "question": "i am working on jetson nano and trying to implement the example of segnet_predict deploy to jetson nao from the matlab.but when i run the command codegen('-config ', cfg, 'segnet_predict', '-args', {img},'-report'); i get this error: STDERR: /home/remoteBuildDir/MATLAB_ws/R2021b/C/Users/DELL/Documents/MATLAB/segnet_deploy/main.cu:10:10: fatal error: opencv2/opencv.hpp: No such file or directory #include \"opencv2/opencv.hpp\" ^~~~~~~~~~~~~~~~~~~~ compilation terminated. make: *** [main.o] Error 1 make: *** Waiting for unfinished jobs.... STDOUT: make: Entering directory '/home/remoteBuildDir/MATLAB_ws/R2021b/C/Users/DELL/Documents/MATLAB/segnet_deploy/codegen/exe/segnet_predict' ------------------------------------------------------------------------ ??? Build error: C++ compiler produced errors. See the Build Log for further details. can you please help me resolving this error i tried to change the path by locating the opencv path but could not solve it.",
        "answers": [
            [
                "It seems like the issue is with the OpenCV library, which the code is trying to include but is unable to find. Check if the OpenCV library is installed on your Jetson Nano: Run the command pkg-config --modversion opencv in the terminal. If the output is a version number, OpenCV is installed. If OpenCV is not installed, install it using the command sudo apt-get install libopencv-dev Ensure that the path to the OpenCV header files is included in the compiler search path: Locate the path to the OpenCV header files (opencv2/opencv.hpp) on your system. In your MATLAB code, add the path to the OpenCV header files to the compiler search path using the \"-I\" option in the \"codegen\" function. For example: codegen('-config ', cfg, 'segnet_predict', '-args', {img},'-report', '-I /path/to/opencv/headers'); Make sure that the OpenCV libraries are in the library search path: Locate the path to the OpenCV libraries on your system. In your MATLAB code, add the path to the OpenCV libraries to the library search path using the \"-L\" option in the \"codegen\" function. For example: codegen('-config ', cfg, 'segnet_predict', '-args', {img},'-report', '-L /path/to/opencv/libraries');"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "My goal is to run DeepStream SDK 6.2 (or older version). The official SD Card Image is JP461 (I assume it's JetPack 4.6.1). However, DeepStream SDK 6.2 require JetPack 5.1. The problem is JetPack 5.1 does not mention Nano at all and I can't find older version of DeepStream SDK.",
        "answers": [
            [
                "You will not be able to see the archived versions of Deepstream without signing in. You can find the download links here Note that 6.0.1 is the last release for Jetson Nano. Incase you need docker images, you can directly docker pull on the Jetson (assuming you have Jetpack 4.6.1 installed) docker pull nvcr.io/nvidia/deepstream-l4t:6.0.1-base (You can find all the images here)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Is there any way to reset the process of setting up a headless connection of Jetson Nano to a windows laptop. I used this process for headless connection: Download and install puTTY software. Plug the following wired connections: \u2022 Type C USB cord in Jetson Nano's power connector to a power bank \u2022 Type C USB cord in Jetson Nano to Laptop \u2022 Ethernet Cable in Jetson Nano to a Globe router Find this device in Device Manager and we'll see COM (with number indicated) in Ports. In my case, COM7 Open puTTy and type in COM7 in the Hostname and 115200 in ports. A terminal window should open and set up begins with terms and conditions followed by inputting name and password, etc. Fill up and enter each step. Once done, open again puTTY and now it asks for the login username and password. This is where I can't continue because it says \"login incorrect\" everytime. I would like to repeat the process from the beginning because possibly I may have a typo in inputting the username or password in the beginning or the login is faulty but I don't know how to wipe out the saved login details. I am using a Jetson Nano called \"reComputer J1010\". It already has a pre-installed Jetpack 4.6.1. And I don't know if resetting will wipe out the pre installed which I hopefully not. I tried to reinstall puTTy but it does nothing because I open the terminal box and it asks for login username and password again and I guess the login details I'm trying to delete is in the internal storage of Jetson Nano and I can't access it.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I would like to install pytorch3d on Jetson Nano but it failed. Problem The install command I tried pip install 'git+https://github.com/facebookresearch/pytorch3d.git' and got this error: cub/cub.cuh: No such file or directory Environment nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Sun_Feb_28_22:34:44_PST_2021 Cuda compilation tools, release 10.2, V10.2.300 Build cuda_10.2_r440.TC440_70.29663091_0 cat /etc/nv_tegra_release # R32 (release), REVISION: 7.3, GCID: 31982016, BOARD: t210ref, EABI: aarch64, DATE: Tue Nov 22 17:30:08 UTC 2022 I found cub/cub.cuh at here and download it but I'm not sure how can I include it when the pip install process. https://github.com/NVlabs/cub",
        "answers": [],
        "votes": []
    },
    {
        "question": "NVIDIA is offering pre-build wheels for Pytorch leveraging the GPU under 1. But this does not cover audiotorch. It seems that it is necessary to use a matching audiotorch version, otherwise and the version having the same version number gives: OSError: ../lib/libtorchaudio.so: undefined symbol: _ZNK5torch8autograd4Node4nameEv Where can this pre-built wheel be found? I do not what to use an offered NVIDIA docker image for JetPack where this might work since this makes no sense on a small AI computer.",
        "answers": [],
        "votes": []
    },
    {
        "question": "How Do I convert the RTP (provided by jetson.utils.videoOutput(\u201crtp://@:1234\u201d)) to RTSP so that the same can ve streamed and be viewed over network. I tried with FFserver as well as Streamer, but don't have much of an expertise in those.",
        "answers": [
            [
                "The dev branch of jetson-inference/jetson-utils has native support for RTSP output. See: https://forums.developer.nvidia.com/t/convert-rtp-to-rtsp/239731/3"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to use torchvision c++ on my Jetson Nano (Jetpack 4.5). I need only torchvision::ops::nms function from torchvision. If you can give source codes of that function or an alternative implementation of that function with torch (c++) please share, and installation will not be required any more. I have tried MANY ways to install, but haven't succeed yet. For example I tried https://www.neuralception.com/settingupopencv/ this approach, but getting this error while running make and can't solve it. [ 2%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/common_jpeg.cpp.o [ 5%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_image.cpp.o In file included from /usr/include/c++/7/bits/stl_algobase.h:59:0, from /usr/include/c++/7/memory:62, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:4, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/aarch64-linux-gnu/c++/7/bits/c++config.h:250:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ /usr/include/aarch64-linux-gnu/c++/7/bits/c++config.h:438:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ In file included from /usr/include/c++/7/bits/stl_algobase.h:59:0, from /usr/include/c++/7/memory:62, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:4, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/aarch64-linux-gnu/c++/7/bits/c++config.h:563:3: error: operator '&amp;&amp;' has no right operand &amp;&amp; _GLIBCXX_USE_DUAL_ABI &amp;&amp; __cpp_transactional_memory &gt;= 201505L \\ ^~ In file included from /usr/include/c++/7/bits/stl_algobase.h:66:0, from /usr/include/c++/7/memory:62, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:4, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/bits/stl_iterator_base_funcs.h:107:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ In file included from /usr/include/c++/7/string:52:0, from /usr/include/c++/7/stdexcept:39, from /usr/include/c++/7/array:39, from /usr/include/c++/7/tuple:39, from /usr/include/c++/7/bits/unique_ptr.h:37, from /usr/include/c++/7/memory:80, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:4, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/bits/basic_string.h:56:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ In file included from /usr/include/c++/7/string:53:0, from /usr/include/c++/7/stdexcept:39, from /usr/include/c++/7/array:39, from /usr/include/c++/7/tuple:39, from /usr/include/c++/7/bits/unique_ptr.h:37, from /usr/include/c++/7/memory:80, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:4, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/bits/basic_string.tcc:50:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ In file included from /usr/include/c++/7/array:39:0, from /usr/include/c++/7/tuple:39, from /usr/include/c++/7/bits/unique_ptr.h:37, from /usr/include/c++/7/memory:80, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:4, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/stdexcept:46:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ /usr/include/c++/7/stdexcept:127:28: error: operator '||' has no left operand #if _GLIBCXX_USE_CXX11_ABI || _GLIBCXX_DEFINE_STDEXCEPT_COPY_OPS ^~ /usr/include/c++/7/stdexcept:211:28: error: operator '||' has no left operand #if _GLIBCXX_USE_CXX11_ABI || _GLIBCXX_DEFINE_STDEXCEPT_COPY_OPS ^~ In file included from /usr/include/c++/7/bits/ios_base.h:41:0, from /usr/include/c++/7/ios:42, from /usr/include/c++/7/istream:38, from /usr/include/c++/7/sstream:38, from /home/jets/Desktop/project/torch/include/c10/macros/Macros.h:228, from /home/jets/Desktop/project/torch/include/c10/core/DeviceType.h:8, from /home/jets/Desktop/project/torch/include/c10/core/Device.h:3, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:6, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/bits/locale_classes.h:354:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ In file included from /usr/include/c++/7/bits/ios_base.h:46:0, from /usr/include/c++/7/ios:42, from /usr/include/c++/7/istream:38, from /usr/include/c++/7/sstream:38, from /home/jets/Desktop/project/torch/include/c10/macros/Macros.h:228, from /home/jets/Desktop/project/torch/include/c10/core/DeviceType.h:8, from /home/jets/Desktop/project/torch/include/c10/core/Device.h:3, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:6, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/system_error:91:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ In file included from /usr/include/c++/7/ios:42:0, from /usr/include/c++/7/istream:38, from /usr/include/c++/7/sstream:38, from /home/jets/Desktop/project/torch/include/c10/macros/Macros.h:228, from /home/jets/Desktop/project/torch/include/c10/core/DeviceType.h:8, from /home/jets/Desktop/project/torch/include/c10/core/Device.h:3, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:6, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/bits/ios_base.h:230:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ /usr/include/c++/7/bits/ios_base.h:254:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ In file included from /home/jets/Desktop/project/torch/include/c10/macros/Macros.h:228:0, from /home/jets/Desktop/project/torch/include/c10/core/DeviceType.h:8, from /home/jets/Desktop/project/torch/include/c10/core/Device.h:3, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:6, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/sstream:297:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ In file included from /usr/include/c++/7/sstream:826:0, from /home/jets/Desktop/project/torch/include/c10/macros/Macros.h:228, from /home/jets/Desktop/project/torch/include/c10/core/DeviceType.h:8, from /home/jets/Desktop/project/torch/include/c10/core/Device.h:3, from /home/jets/Desktop/project/torch/include/c10/core/Allocator.h:6, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/bits/sstream.tcc:92:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ In file included from /usr/include/c++/7/locale:41:0, from /usr/include/c++/7/iomanip:43, from /home/jets/Desktop/project/torch/include/c10/util/logging_is_not_google_glog.h:8, from /home/jets/Desktop/project/torch/include/c10/util/Logging.h:28, from /home/jets/Desktop/project/torch/include/c10/core/TensorImpl.h:19, from /home/jets/Desktop/project/torch/include/ATen/core/TensorBody.h:14, from /home/jets/Desktop/project/torch/include/ATen/Tensor.h:3, from /home/jets/Desktop/project/torch/include/ATen/Context.h:4, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:9, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/bits/locale_facets_nonio.h:719:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ /usr/include/c++/7/bits/locale_facets_nonio.h:1567:33: error: operator '&amp;&amp;' has no right operand &amp;&amp; _GLIBCXX_USE_CXX11_ABI == 0 ^~ /usr/include/c++/7/bits/locale_facets_nonio.h:1590:33: error: operator '&amp;&amp;' has no right operand &amp;&amp; _GLIBCXX_USE_CXX11_ABI == 0 ^~ /usr/include/c++/7/bits/locale_facets_nonio.h:1712:33: error: operator '&amp;&amp;' has no right operand &amp;&amp; _GLIBCXX_USE_CXX11_ABI == 0 ^~ /usr/include/c++/7/bits/locale_facets_nonio.h:1747:33: error: operator '&amp;&amp;' has no right operand &amp;&amp; _GLIBCXX_USE_CXX11_ABI == 0 ^~ In file included from /usr/include/c++/7/bits/locale_facets_nonio.h:2013:0, from /usr/include/c++/7/locale:41, from /usr/include/c++/7/iomanip:43, from /home/jets/Desktop/project/torch/include/c10/util/logging_is_not_google_glog.h:8, from /home/jets/Desktop/project/torch/include/c10/util/Logging.h:28, from /home/jets/Desktop/project/torch/include/c10/core/TensorImpl.h:19, from /home/jets/Desktop/project/torch/include/ATen/core/TensorBody.h:14, from /home/jets/Desktop/project/torch/include/ATen/Tensor.h:3, from /home/jets/Desktop/project/torch/include/ATen/Context.h:4, from /home/jets/Desktop/project/torch/include/ATen/ATen.h:9, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/bits/locale_facets_nonio.tcc:352:33: error: operator '&amp;&amp;' has no right operand &amp;&amp; _GLIBCXX_USE_CXX11_ABI == 0 ^~ /usr/include/c++/7/bits/locale_facets_nonio.tcc:564:33: error: operator '&amp;&amp;' has no right operand &amp;&amp; _GLIBCXX_USE_CXX11_ABI == 0 ^~ In file included from /usr/include/c++/7/list:63:0, from /home/jets/Desktop/project/torch/include/ATen/core/dispatch/OperatorEntry.h:17, from /home/jets/Desktop/project/torch/include/ATen/core/dispatch/Dispatcher.h:6, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/runtime/operator.h:6, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/ir/ir.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/api/function_impl.h:4, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/api/method.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/api/object.h:6, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/frontend/tracer.h:9, from /home/jets/Desktop/project/torch/include/torch/csrc/autograd/generated/variable_factories.h:12, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:7, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/bits/stl_list.h:326:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ /usr/include/c++/7/bits/stl_list.h:349:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ /usr/include/c++/7/bits/stl_list.h:1886:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ /usr/include/c++/7/bits/stl_list.h:1955:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ In file included from /usr/include/c++/7/list:64:0, from /home/jets/Desktop/project/torch/include/ATen/core/dispatch/OperatorEntry.h:17, from /home/jets/Desktop/project/torch/include/ATen/core/dispatch/Dispatcher.h:6, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/runtime/operator.h:6, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/ir/ir.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/api/function_impl.h:4, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/api/method.h:7, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/api/object.h:6, from /home/jets/Desktop/project/torch/include/torch/csrc/jit/frontend/tracer.h:9, from /home/jets/Desktop/project/torch/include/torch/csrc/autograd/generated/variable_factories.h:12, from /home/jets/Desktop/project/torch/include/torch/csrc/api/include/torch/types.h:7, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.h:3, from /home/jets/Desktop/project/vision/torchvision/csrc/io/image/cpu/decode_image.cpp:1: /usr/include/c++/7/bits/list.tcc:178:27: error: #if with no expression #if _GLIBCXX_USE_CXX11_ABI ^ CMakeFiles/torchvision.dir/build.make:89: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_image.cpp.o' failed make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_image.cpp.o] Error 1 CMakeFiles/Makefile2:82: recipe for target 'CMakeFiles/torchvision.dir/all' failed make[1]: *** [CMakeFiles/torchvision.dir/all] Error 2 Makefile:135: recipe for target 'all' failed make: *** [all] Error 2 Changed the compiler, tried multiple other things but still cant fix this. Tried to install torchvision for python and link libs in cmake (worked for libtorch installation), but there is no \"include\" or \"libs\" directories in torchvision directory. Tried to install with this instructions, but when I'm running sudo -H pip3 install torchvision-0.11.0a0+fa347eb-cp36-cp36m-linux_aarch64.whl it says \"No module named Cython\", but I have cython installed, then it tries to build numpy from source and again fails (I have numpy installed). And that's not all, tried millions of other ways but still stuck (even googled in Chinese). Please, if you know something that can help, please say.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Hi all, I spent a whole lot of time today inside a docker container, installing software and making changes to the environment, setting everything up the way I need it. Then I realized none of that has been retained. Here is exactly what I have done: create directory shell script template docker_run.sh and paste the template modify to use the right tag: CONTAINER_IMAGE=\u201cdustynv/ros:galactic-pytorch-l4t-r32.7.1\u201d save and make it executable: chmod 755 docker_run.sh run it ./docker_run.sh Upon execution I get the following output: xhost: unable to open display \"\" xauth: file /home/administrator/.Xauthority does not exist xauth: (argv):1: unable to read any entries from file \"(stdin)\" running 5. above with sudo does not change anything either. Did some search on google, but only got me more confused: Seems like I have to commit changes made to the docker container, but even after reading for a solid two hours and following some other tutorials, I\u2019m still not sure how I can commit changes to the docker container. And if that needs to be done from within or outside,\u2026 As above stated, I used a docker image from here: Github ros:galactic-pytorch-l4t-r32.7.1 Please let me know how I can retain my changes, so I don\u2019t have to re-do the entire setup everytime :-/ Thanks team",
        "answers": [
            [
                "You need to use the docker image that you want to build (install software on) as the base docker image and write your own Dockerfile for the changes to persist. Example: FROM ubuntu:latest // Base image RUN apt-get update &amp;&amp; apt-get install python3 // Software installation"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I try to install microsoft/vcpkg on Jetson Nano (Ubuntu 18.04) I do $ git clone https://github.com/microsoft/vcpkg then ./vcpkg/bootstrap-vcpkg.sh And I get Unable to determine a binary release of vcpkg; attempting to build from source. Building vcpkg-tool... CMake Error: CMake was unable to find a build program corresponding to \"Ninja\". CMAKE_MAKE_PROGRAM is not set. You probably need to select a different build tool. CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage -- Configuring incomplete, errors occurred! See also \"/mnt/ALPR_Cpp/vcpkg/buildtrees/_vcpkg/build/CMakeFiles/CMakeOutput.log\". Then of course I installed ninja-build version 1.8.2 but another problem occurred there (see the problem here). Is there any other solution without using Ninja?",
        "answers": [
            [
                "sudo apt-get install ninja-build"
            ]
        ],
        "votes": [
            -1.9999999
        ]
    },
    {
        "question": "I want to install microsoft/vcpkg on jetson nano (Ubuntu 18.04). git clone https://github.com/microsoft/vcpkg Clones everything correctly, then ./vcpkg/bootstrap-vcpkg.sh The error I get Unable to determine a binary release of vcpkg; attempting to build from source. Downloading vcpkg tool sources Building vcpkg-tool... -- The CXX compiler identification is GNU 7.5.0 -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- Detecting the C++ compiler in use -- Detecting the C++ compiler in use - gcc -- Module support is disabled. -- Version: 9.1.0 -- Build type: Release -- CXX_STANDARD: 17 -- Performing Test has_std_17_flag -- Performing Test has_std_17_flag - Success -- Performing Test has_std_1z_flag -- Performing Test has_std_1z_flag - Success -- Required features: cxx_variadic_templates -- The C compiler identification is GNU 7.5.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: /usr/bin/cc - skipped -- Detecting C compile features -- Detecting C compile features - done -- Performing Test CPP_ATOMIC_BUILTIN -- Performing Test CPP_ATOMIC_BUILTIN - Success -- Looking for C++ include pthread.h -- Looking for C++ include pthread.h - found -- Performing Test CMAKE_HAVE_LIBC_PTHREAD -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed -- Check if compiler accepts -pthread -- Check if compiler accepts -pthread - yes -- Found Threads: TRUE -- Configuring done -- Generating done -- Build files have been written to: /mnt/ALPR_Cpp/vcpkg/buildtrees/_vcpkg/build [0/2] Re-checking globbed directories... [42/191] Building CXX object CMakeFile...glib.dir/src/vcpkg/binarycaching.cpp.o FAILED: CMakeFiles/vcpkglib.dir/src/vcpkg/binarycaching.cpp.o /usr/bin/c++ -DVCPKG_BASE_VERSION=2999-12-31 -DVCPKG_VERSION=unknownhash -I/mnt/ALPR_Cpp/vcpkg/buildtrees/_vcpkg/src/vcpkg-tool-2022-12-14/include -I_deps/fmt-src/include -I_cmrc/include -O3 -DNDEBUG -include /mnt/ALPR_Cpp/vcpkg/buildtrees/_vcpkg/src/vcpkg-tool-2022-12-14/include/pch.h -pthread -std=c++1z -MD -MT CMakeFiles/vcpkglib.dir/src/vcpkg/binarycaching.cpp.o -MF CMakeFiles/vcpkglib.dir/src/vcpkg/binarycaching.cpp.o.d -o CMakeFiles/vcpkglib.dir/src/vcpkg/binarycaching.cpp.o -c /mnt/ALPR_Cpp/vcpkg/buildtrees/_vcpkg/src/vcpkg-tool-2022-12-14/src/vcpkg/binarycaching.cpp In file included from /usr/include/c++/7/bits/stl_tempbuf.h:60:0, from /usr/include/c++/7/bits/stl_algo.h:62, from /usr/include/c++/7/algorithm:62, from /mnt/ALPR_Cpp/vcpkg/buildtrees/_vcpkg/src/vcpkg-tool-2022-12-14/include/pch.h:25, from &lt;command-line&gt;:0: /usr/include/c++/7/bits/stl_construct.h: In instantiation of \u2018void std::_Construct(_T1*, _Args&amp;&amp; ...) [with _T1 = std::unique_ptr&lt;vcpkg::IBinaryProvider&gt;; _Args = {const std::unique_ptr&lt;vcpkg::IBinaryProvider, std::default_delete&lt;vcpkg::IBinaryProvider&gt; &gt;&amp;}]\u2019: /usr/include/c++/7/bits/stl_uninitialized.h:83:18: required from \u2018static _ForwardIterator std::__uninitialized_copy&lt;_TrivialValueTypes&gt;::__uninit_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = __gnu_cxx::__normal_iterator&lt;const std::unique_ptr&lt;vcpkg::IBinaryProvider&gt;*, std::vector&lt;std::unique_ptr&lt;vcpkg::IBinaryProvider&gt; &gt; &gt;; _ForwardIterator = std::unique_ptr&lt;vcpkg::IBinaryProvider&gt;*; bool _TrivialValueTypes = false]\u2019 /usr/include/c++/7/bits/stl_uninitialized.h:134:15: required from \u2018_ForwardIterator std::uninitialized_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = __gnu_cxx::__normal_iterator&lt;const std::unique_ptr&lt;vcpkg::IBinaryProvider&gt;*, std::vector&lt;std::unique_ptr&lt;vcpkg::IBinaryProvider&gt; &gt; &gt;; _ForwardIterator = std::unique_ptr&lt;vcpkg::IBinaryProvider&gt;*]\u2019 /usr/include/c++/7/bits/stl_uninitialized.h:289:37: required from \u2018_ForwardIterator std::__uninitialized_copy_a(_InputIterator, _InputIterator, _ForwardIterator, std::allocator&lt;_Tp&gt;&amp;) [with _InputIterator = __gnu_cxx::__normal_iterator&lt;const std::unique_ptr&lt;vcpkg::IBinaryProvider&gt;*, std::vector&lt;std::unique_ptr&lt;vcpkg::IBinaryProvider&gt; &gt; &gt;; _ForwardIterator = std::unique_ptr&lt;vcpkg::IBinaryProvider&gt;*; _Tp = std::unique_ptr&lt;vcpkg::IBinaryProvider&gt;]\u2019 /usr/include/c++/7/bits/stl_vector.h:331:31: required from \u2018std::vector&lt;_Tp, _Alloc&gt;::vector(const std::vector&lt;_Tp, _Alloc&gt;&amp;) [with _Tp = std::unique_ptr&lt;vcpkg::IBinaryProvider&gt;; _Alloc = std::allocator&lt;std::unique_ptr&lt;vcpkg::IBinaryProvider&gt; &gt;]\u2019 /mnt/ALPR_Cpp/vcpkg/buildtrees/_vcpkg/src/vcpkg-tool-2022-12-14/include/vcpkg/base/expected.h:42:57: required from \u2018vcpkg::ExpectedHolder&lt;T&gt;::ExpectedHolder(Fwd&amp;&amp;) [with Fwd = std::vector&lt;std::unique_ptr&lt;vcpkg::IBinaryProvider&gt; &gt;&amp;; typename std::enable_if&lt;(! is_same_v&lt;vcpkg::ExpectedHolder&lt;T&gt;, typename std::remove_cv&lt;typename std::remove_reference&lt;_SrcTuple&gt;::type&gt;::type&gt;), int&gt;::type &lt;anonymous&gt; = 0; T = std::vector&lt;std::unique_ptr&lt;vcpkg::IBinaryProvider&gt; &gt;]\u2019 /mnt/ALPR_Cpp/vcpkg/buildtrees/_vcpkg/src/vcpkg-tool-2022-12-14/include/vcpkg/base/expected.h:77:85: required from \u2018vcpkg::ExpectedT&lt;T, Error&gt;::ExpectedT(ConvToT&amp;&amp;) [with ConvToT = std::vector&lt;std::unique_ptr&lt;vcpkg::IBinaryProvider&gt; &gt;&amp;; typename std::enable_if&lt;(is_convertible_v&lt;ConvToT, T&gt; &amp;&amp; (! is_same_v&lt;typename std::remove_reference&lt;_SrcTuple&gt;::type, Error&gt;)), int&gt;::type &lt;anonymous&gt; = 0; T = std::vector&lt;std::unique_ptr&lt;vcpkg::IBinaryProvider&gt; &gt;; Error = std::__cxx11::basic_string&lt;char&gt;]\u2019 /mnt/ALPR_Cpp/vcpkg/buildtrees/_vcpkg/src/vcpkg-tool-2022-12-14/src/vcpkg/binarycaching.cpp:2312:12: required from here /usr/include/c++/7/bits/stl_construct.h:75:7: error: use of deleted function \u2018std::unique_ptr&lt;_Tp, _Dp&gt;::unique_ptr(const std::unique_ptr&lt;_Tp, _Dp&gt;&amp;) [with _Tp = vcpkg::IBinaryProvider; _Dp = std::default_delete&lt;vcpkg::IBinaryProvider&gt;]\u2019 { ::new(static_cast&lt;void*&gt;(__p)) _T1(std::forward&lt;_Args&gt;(__args)...); } ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ In file included from /usr/include/c++/7/memory:80:0, from /mnt/ALPR_Cpp/vcpkg/buildtrees/_vcpkg/src/vcpkg-tool-2022-12-14/include/pch.h:35, from &lt;command-line&gt;:0: /usr/include/c++/7/bits/unique_ptr.h:383:7: note: declared here unique_ptr(const unique_ptr&amp;) = delete; ^~~~~~~~~~ [47/191] Building CXX object CMakeFiles/vcpkglib.dir/src/vcpkg/build.cpp.o ninja: build stopped: subcommand failed. ninja version I have installed 1.8.2 I have tried to reinstall ninja and tried to use --useSystemBinaries flag recommended here ./bootstrap-vcpkg.sh --useSystemBinaries but it said Warning: -useSystemBinaries no longer has any effect; ignored. Note that the VCPKG_USE_SYSTEM_BINARIES environment variable behavior is not changed. then continued the above process. Please help to solve this problem. If you need an additional information, just tell me.",
        "answers": [
            [
                "Try to compile an earlier version of vcpkg. git checkout 2022.01.31 Or you can upgrade jetpack to 5."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "ERROR Image At the beginning, the Raspberry Pi environment was 3.9.2 (armv7l). After searching the Internet, pytorch found that it only supports 3.8; after installing the conda environment, install pytorch under the (py36) python version 3.6.6; but as shown in the picture, it can't Install. Is there anyone on the forum who has encountered this problem and solved it? Thanks. At the beginning, when installing pytorch according to the official process, it will report that /home/pi/pytorch/third_party/eigen lacks the extraction file and cannot be installed. After finding and installing the pytorch.whl file on the Internet and reporting an error, try to install the conda environment; but in conda Environment can't be installed. Off topic, my second question is if I go to train mobilenet SSD v2 after success, can I directly use the lens for visual recognition? Before using the public mobilenet to detect, the number of frames was only about 4.4. Later, because I felt that the FPS was too low, I wanted to train mibilenet by myself. How about someone who has used Raspberry Pi to use FPS after training? If the FPS is too low, is it better to use NVIDIA Jetson Nano for training and detection?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a C++ code for detecting corners in OpenCV. I am trying to convert it to use NVIDIA VPI package in python to run on a Jetson Nano, but the implementations aren't equivalent, the VPI function doesn't have all of the parameters that the OpenCV function has, so I'm not sure how to go about doing this. Here is (part of) my code in C++, I am using cv::goodFeaturesToTrack with Harris Detector: int main(void){ cv::Mat image; vector&lt;Point2f&gt; points; VideoCapture cap(\"video.avi\"); cap &gt;&gt; frame; //reading frame from video, size = 1936x1216 cvtColor(cap, image, cv::COLOR_RGB2GRAY); // to grayscale cv::resize(image, image, m_ImageSizeFinal); //resize to 968x608 goodFeaturesToTrackMask = createDetectionMaskImage(); //Create mask of type CV_8UC1 cv::goodFeaturesToTrack(image, points, 500, 0.001, 15, goodFeaturesToTrackMask, 3, true); } and here is my attempt to convert it to NVIDIA VPI in Python: import vpi import cv2 import numpy as np import csv cap = cv2.VideoCapture('2021_01_25__14_05_01.avi') backend = vpi.Backend.CUDA image1, image2 = None, None with backend: writer = csv.writer(f) start = time.time(): while cap.isOpened(): now = time.time() ret, frame = cap.read() if ret == True: image = vpi.asimage(frame, vpi.Format.BGR8).convert(vpi.Format.U8) image = image.rescale((968, 608), interp=vpi.Interp.LINEAR, border=vpi.Border.ZERO) corners, scores = image.harriscorners(sensitivity=0.04) This fails to detect any points at all, the return value is always 0. What am I doing wrong? Thank you in advance.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying hard to install PyQt5 in my Jetson nano. However, there is one thing always annoys me. here's the feedback message. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for sip Failed to build sip ERROR: Could not build wheels for sip, which is required to install pyproject.toml-based projects [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error \u00d7 pip subprocess to install build dependencies did not run successfully. \u2502 exit code: 1 \u2570\u2500&gt; See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. I've tried sudo pip3 install pyqt5 and it gave me compilation terminated. error: command '/usr/bin/aarch64-linux-gnu-gcc' failed with exit code 1 [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for sip Failed to build sip ERROR: Could not build wheels for sip, which is required to install pyproject.toml-based projects [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocess-exited-with-error \u00d7 pip subprocess to install build dependencies did not run successfully. \u2502 exit code: 1 \u2570\u2500&gt; See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. I also tried sudo apt-get install python3-pyqt5. It worked but when I imported PyQt5 by from pyqtgraph.Qt import QtCore, QtGui or from PyQt5.QtCore import QObject, pyqtSignal, it gave me Traceback (most recent call last): File \"/home/llw/Desktop/project/ture file/save_pointcloud.py\", line 16, in &lt;module&gt; from pyqtgraph.Qt import QtCore, QtGui File \"/usr/local/lib/python3.7/dist-packages/pyqtgraph/__init__.py\", line 17, in &lt;module&gt; from .colors import palette File \"/usr/local/lib/python3.7/dist-packages/pyqtgraph/colors/palette.py\", line 1, in &lt;module&gt; from ..Qt import QtGui File \"/usr/local/lib/python3.7/dist-packages/pyqtgraph/Qt/__init__.py\", line 155, in &lt;module&gt; import PyQt5.QtCore ModuleNotFoundError: No module named 'PyQt5.QtCore How do I fix this problem? Any suggestions or ideas could be a help. Thank you.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm developing on Jetson Nano, and my goal is to output two overlapping videos. Here is the command I wrote first. gst-launch-1.0 v4l2src device=/dev/video0 io-mode=2 ! image/jpeg, width=1920, height=1080 ! nvjpegdec ! video/x-raw ! queue ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA' ! queue ! comp.sink_0 \\ v4l2src device=/dev/video1 io-mode=2 ! image/jpeg, width=1920, height=1080 ! nvjpegdec ! video/x-raw ! queue ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA' ! queue ! comp.sink_1 \\ alsasrc device=\"hw:1\" ! audioconvert ! audioresample ! audiorate ! \"audio/x-raw, rate=48000, channels=2\" ! queue ! faac bitrate=128000 rate-control=2 ! queue ! muxer. \\ nvcompositor name=comp sink_0::xpos=0 sink_0::ypos=0 sink_0::width=768 sink_0::height=432 sink_0::zorder=2 sink_0::alpha=1.0 sink_1::xpos=0 sink_1::ypos=0 sink_1::width=1920 sink_1::height=1080 sink_1::zorder=1 sink_1::alpha=1.0 ! \\ 'video/x-raw(memory:NVMM),format=RGBA' ! nvvidconv ! 'video/x-raw(memory:NVMM),format=(string)I420' ! nvv4l2h265enc ! h265parse ! queue ! mpegtsmux name=muxer ! filesink location=test.mp4 \\ This is my Command. and This is My C Code gst_bin_add_many (GST_BIN (gstContext-&gt;pipeline), gstContext-&gt;videosrc, gstContext-&gt;video_filter1, gstContext-&gt;jpegdec, gstContext-&gt;x_raw, gstContext-&gt;queue_video1, gstContext-&gt;video_convert2, gstContext-&gt;video_filter4, gstContext-&gt;queue_video2 , gstContext-&gt;videosrc2, gstContext-&gt;video_filter2_1, gstContext-&gt;jpegdec2, gstContext-&gt;x_raw2, gstContext-&gt;queue_video2_1, gstContext-&gt;video_convert3, gstContext-&gt;video_filter2_3, gstContext-&gt;queue_video2_2 , gstContext-&gt;video_mixer , gstContext-&gt;video_filter3, gstContext-&gt;video_convert, gstContext-&gt;video_filter2, gstContext-&gt;video_encoder, gstContext-&gt;video_pasre, gstContext-&gt;queue_video3, gstContext-&gt;muxer,gstContext-&gt;sink2, NULL); if( !gst_element_link_many(gstContext-&gt;video_mixer, gstContext-&gt;video_filter3, gstContext-&gt;video_convert, gstContext-&gt;video_filter2, gstContext-&gt;video_encoder, gstContext-&gt;video_pasre, gstContext-&gt; queue_video3, gstContext-&gt;muxer, NULL) || !gst_element_link_many(gstContext-&gt;videosrc, gstContext-&gt;video_filter1, gstContext-&gt;jpegdec, gstContext-&gt;x_raw, gstContext-&gt;queue_video1, gstContext-&gt;video_convert2, gstContext-&gt;video_filter4, gstContext-&gt;queue_video2 ,NULL) || !gst_element_link_many(gstContext-&gt;videosrc2, gstContext-&gt;video_filter2_1, gstContext-&gt;jpegdec2, gstContext-&gt;x_raw2, gstContext-&gt;queue_video2_1, gstContext-&gt;video_convert3, gstContext-&gt;video_filter2_3, gstContext-&gt;queue_video2_2 , NULL) ) { g_error(\"Failed to link elementsses!@!@!@!@\"); pthread_mutex_unlock(&amp;context-&gt;lock); return -2; } queue_video2 = gst_element_get_static_pad (gstContext-&gt;queue_video2, \"src\"); queue_video2_2 = gst_element_get_static_pad (gstContext-&gt;queue_video2_2, \"src\"); mixer2_sinkpad = gst_element_get_request_pad (gstContext-&gt;video_mixer, \"sink_%u\"); mixer1_sinkpad = gst_element_get_request_pad (gstContext-&gt;video_mixer, \"sink_%u\"); if (gst_pad_link (queue_video2, mixer2_sinkpad) != GST_PAD_LINK_OK || gst_pad_link (queue_video2_2, mixer1_sinkpad) != GST_PAD_LINK_OK) { g_printerr (\"\\n\\n\\n source0 and mixer pads could not be linked22222222222.\\n\\n\\n\"); gst_object_unref (gstContext-&gt;pipeline); return -1; } g_object_unref(queue_video2); g_object_unref(queue_video2_2); And Log 0:00:00.926466171 968 0x7f54324050 FIXME videodecoder gstvideodecoder.c:933:gst_video_decoder_drain_out:&lt;nvjpegdec0&gt; Sub-class should implement drain() 0:00:00.979159196 968 0x7f54324050 WARN basesrc gstbasesrc.c:3055:gst_base_src_loop:&lt;videosrc1&gt; error: Internal data stream error. 0:00:00.979208884 968 0x7f54324050 WARN basesrc gstbasesrc.c:3055:gst_base_src_loop:&lt;videosrc1&gt; error: streaming stopped, reason not-linked (-1) 0:00:00.979354772 968 0x7f54324050 WARN queue gstqueue.c:988:gst_queue_handle_sink_event:&lt;queue_video1&gt; error: Internal data stream error. 0:00:00.979391699 968 0x7f54324050 WARN queue gstqueue.c:988:gst_queue_handle_sink_event:&lt;queue_video1&gt; error: streaming stopped, reason not-linked (-1) ERROR: from element /GstPipeline:pipeline0/GstV4l2Src:videosrc1: Internal data stream error. Additional debug info1: gstbasesrc.c(3055): gst_base_src_loop (): /GstPipeline:pipeline0/GstV4l2Src:videosrc1: I connected the elements using requestpad. But why am I getting this error? Please let me know if I've done anything wrong The command works as soon as you run it. thank you",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to serve RTSP stream for multiple clients from CSI connected to Jetson Nano (without centralized video server) Some searching has lead me to test-launch.c. I've downloaded source code above, commented gst_rtsp_media_factory_set_enable_rtcp (factory, !disable_rtcp); line (since my GStreamer not support it) and built it via gcc test-launch.c -o test-launch $(pkg-config --cflags --libs gstreamer-1.0 gstreamer-rtsp-server-1.0) successfully. My next step is to start RTSP server: ./test-launch -p 44554 \"nvarguscamerasrc sensor-id=0 ! video/x-raw(memory:NVMM), format=NV12, width=416, height=256 ! nvv4l2h264enc insert-sps-pps=true ! h264parse ! rtph264pay name=pay0 pt=96\" after RTSP server starts I see: stream ready at rtsp://127.0.0.1:44554/test Everything seems to be fine and I can execute ffmpeg to test if it actually works: ffmpeg -i rtsp://127.0.0.1:44554/test -acodec copy -vcodec copy -y out.mp4 Everything is good again. BUT. When I'm trying to start another ffmpeg session I get next error: # This error appears till I stop first session. [rtsp @ 0x55a7fb3680] method PLAY failed: 503 Service Unavailable rtsp://127.0.0.1:44554/test: Server returned 5XX Server Error reply Is there something that I missing? Are there better ways to achieve multi-client RTSP streaming on JetsonNano+CSI camera? P.S. My setup: Camera: Arducam IMX477 GStreamer version: 1.14.5 Jetson OS: Jetpack 4.6",
        "answers": [],
        "votes": []
    },
    {
        "question": "We have a Nvidia Jetson NGX and our cuda installation broke after working for a while after accidentally updating \"sudo apt update\". We were not sure how to install cuda onto the jetson without reflashing it.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I try to use jetson nano for my research.(using analog devices' ToF sensor) I download sd card .img file at here: ad-fxtof-ebz. And I got error from Booting process: [FAILED] failed to start load kernel module but it boots and shows me GUI interface. So I open terminal and command like below: analog@analog:~$ sudo apt-get upgrade Reading package lists... Done Building dependency tree Reading state information... Done You might want to run 'apt --fix-broken install' to correct these. The following packages have unmet dependencies. cuda-drivers-450 : Depends: libnvidia-gl-450 (&gt;= 450.51.05) but it is not installed libnvidia-ifr1-450 : Depends: libnvidia-gl-450 but it is not installed nvidia-driver-450 : Depends: libnvidia-gl-450 (= 450.51.05-0ubuntu1) but it is not installed E: Unmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution). Following instructions, I try to fix problems like this: analog@analog:~$ sudo apt-get install libnvidia-gl-450 Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed libnvidia-gl-450 0 to upgrade, 1 to newly install, 0 to remove and 415 not to upgrade. 75 not fully installed or removed. Need to get 0 B/54.9 MB of archives. After this operation, 192 MB of additional disk space will be used. Get:1 file:/var/cuda-repo-ubuntu1804-11-0-local libnvidia-gl-450 450.51.05-0ubuntu1 [54.9 MB] debconf: Delaying package configuration, since apt-utils is not installed. dpkg: unrecoverable fatal error, aborting: files list file for package 'nvidia-prime' is missing final newline E: Sub-process /usr/bin/dpkg returned an error code (2) analog@analog:~$ sudo apt-get install apt-utils Reading package lists... Done Building dependency tree Reading state information... Done You might want to run 'apt --fix-broken install' to correct these. The following packages have unmet dependencies. apt-utils : Depends: apt (= 1.6.14) but 1.6.12ubuntu0.1 is to be installed cuda-drivers-450 : Depends: libnvidia-gl-450 (&gt;= 450.51.05) but it is not going to be installed libnvidia-ifr1-450 : Depends: libnvidia-gl-450 but it is not going to be installed nvidia-driver-450 : Depends: libnvidia-gl-450 (= 450.51.05-0ubuntu1) but it is not going to be installed E: Unmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution). analog@analog:~$ apt-cache policy apt apt: Installed: 1.6.12ubuntu0.1 Candidate: 1.6.14 Version table: 1.6.14 500 500 http://ports.ubuntu.com/ubuntu-ports bionic-updates/main arm64 Packages 1.6.12ubuntu0.2 500 500 http://ports.ubuntu.com/ubuntu-ports bionic-security/main arm64 Packages *** 1.6.12ubuntu0.1 100 100 /var/lib/dpkg/status 1.6.1 500 500 http://ports.ubuntu.com/ubuntu-ports bionic/main arm64 Packages My questions are below: How can I upgrade apt version? How can I solve [FAILED] failed to start load kernel module issue? I hope I could get hint from here or find solution from intelligent stackoverflowers.",
        "answers": [],
        "votes": []
    },
    {
        "question": "With the standard j4t image that should be installed to the jetson nano (in this case the 4GB dev edition) the desktop is not rendered correctly. The UI shows artifacts and does not function like expected. For example the Terminal would not start at all. The official tutorials for VNC setup do not provide a solution that works smoothly, so that one can start the Nano in headless mode (without HDMI device conncted). You can connect via VNC but an HDMI device has to be connected, so that the desktop loads correctly. Also with headless mode, I was not able to start any applications like terminal or a browser. So what can we do to connect with VNC and receive pictures / a video stream that is in a higher resolution format and also where the UI does not show artifacts?",
        "answers": [
            [
                "There is also a discussion on the official forums on this topic. For the following version: uname -a Linux jetson 4.9.253-tegra #1 SMP PREEMPT Wed Apr 20 14:25:12 PDT 2022 aarch64 aarch64 aarch64 GNU/Linux ... this is what helped me: connect to the Jetson Nano via SSH install vncserver as well as viewer: sudo apt install tightvncserver &amp;&amp; sudo apt install xtightvncviewer then run cmd vncserver and set a password also via SSH, edit the xstartup file: sudo nano ~/.vnc/xstartup Add these 3 lines and save: unset SESSION_MANAGER unset DBUS_SESSION_BUS_ADDRESS startlxde &amp; reboot the Jetson Nano and connect via SSH again, then start vncserver: vncserver Finally connect to the mentioned display device, e.g. &lt;hostname&gt;:1. Connect via e.g. xtightvncviewer."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to stream video with multiple cameras on the jetson nano. While streaming for longer period of time, one of the camera randomly kept freezing. So I check the available devices thinking there might be a connection problem but found the device id was changed. &gt; v4l2-ctl --list-devices vi-output, imx219 8-0010 (platform:54080000.vi:4): /dev/video0 USB 2.0 Camera (usb-70090000.xusb-2.1): /dev/video1 USB 2.0 Camera (usb-70090000.xusb-2.4): /dev/video2 I am using one CSI and Two usb cameras. The device id's of the usb cam was 3 and 2 before.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to write a program in Python, where the main thread will read depth frames from a RealSense camera and put them in a queue, and another thread that will run inference on them with a YoloV5 TensorRT model. The program runs on a Jetson Nano. For some reason, after reading about 15 frames the program crashes with the following error: Traceback (most recent call last): File \"test2.py\", line 59, in &lt;module&gt; img = np.asanyarray(c.colorize(DEPTH).get_data()) RuntimeError: Error occured during execution of the processing block! See the log for more info Here is the full code: from queue import Queue import numpy as np from ObjectDetection.objectDetectionV2 import ODModel, letterbox import torch import time from threading import Thread import cv2 from Camera.Realsense import RealSense # custom class for reading from Realsense camera def detect(queue): while True: if not queue.empty(): img0 = queue.get() if img0 is None: break img = letterbox(img0, 416, stride=32, auto=False)[0] # YoloV5 preprocessing img = img.transpose((2, 0, 1))[::-1] # HWC to CHW, BGR to RGB img = np.ascontiguousarray(img) print(\"loading image...\") img = torch.tensor(img) print(\"loaded image\") img = img.float() # uint8 to fp16/32 img /= 255 # 0 - 255 to 0.0 - 1.0 result = model(img) print(result) if __name__ == '__main__': queue = Queue() print(\"loading model\") model = ODModel() print(\"model loaded\") rs = RealSense() p = Thread(target=detect, args=(queue,)) c = rs.colorizer p.start() for i in range(100): RGB, DEPTH = rs.getData() img = np.asanyarray(c.colorize(DEPTH).get_data()) queue.put(img) queue.put(None) p.join() model.destroy() print(\"Exiting Main Thread\") I tried commenting everything out and checking line by line, and I think the error is because of the c.colorizer taking too much time? When I deleted it the error went away (but of course the inference failed). If I don't remove it then the error appears after the line img = np.ascontiguousarray(img). But then why is the error not on this line? If I limit the size of the queue to at most 14, the problem stops, but then the queue is blocking so everything slows down. Also the error mentions a log, but I have no idea where it is. Can anyone help me understand what I did wrong? Thank you in advance.",
        "answers": [
            [
                "this line reserves memory, and limiting the queue size also limits memory usage so you most likely ran out of memory. a possible solution is to just limit the queue size to 1 sample, you always get the most recent result that is within the timeframe of your processing time. another solution is to use a deque of say, 5 elements, your producer will append, and your consumer will pop to get the most recent item, and if the deque length is greater than 3 elements then your producer will popleft to keep the deque bounded, and the \"counting\" should be in the worker thread instead of the main thread, while the main thread will have an infinite loop to guarantee 100 images were processed before breaking out of the infinite loop. (simply switching role with the worker thread.)"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to let my gstreamer to work but I am getting this error and I donn't know what is could be. I am new to linux so i donn't really know how it al works. I have looked most of the things I did up and with this problem i could not really find a simmaler problem, meaby someone else knows something.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I received a Jetson Nano with a developer kit (B01) and a micro SD card from a previous research project that I want to continue, but unfortunately I didn't receive the needed password. My question is if there is a way to remove or change the password, so I am able to continue my research with the previous project setup. I am familiar with Raspberry Pis but not with the Jetson Nano, so I would really appreciate your help. What I tried: Since I don't want to lose the data, I first created a backup of the micro SD card using Win32 Disk Imager, like I would on a Raspberry Pi. Based on the backup and a second micro SD card, I made successfully a copy that I can work on without the risk of losing data. I tried login in with the two defaults: user: nvidia password: nvidia and user: ubuntu password: ubuntu, but it was unsuccessful. The login screen shows a custom username, so my guess is that the defaults were changed or completely deleted. Since the Jetson Nano shows a Ubuntu login screen, my initial idea was to remove the password with this guide. Unfortunately, this guide seems not applicable for the Jetson Nano because I didn't know that it doesn't use Grub. I am currently reading about the serial debug console.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Hardware: Computer -&gt; Jetson Nano 2GB B01, Camera -&gt; Arducam IMX519 MIPI/CSI Software: OS -&gt; Ubuntu v16, IDE -&gt; QT 5 Libraries: i2c, gstvideo, gstreamer-1.0 The testing that I'm performing is on a barebones project so that I can prove the concept and have it wrapped in a class that I can move to another larger project I'm working on. I figured keeping it module would allow for ruling out other interfering issues. So please keep in mind that my code is dirty and will get refactored before I export the class. gstcontroled.cpp ` GstElement *m_pipeline; GstBus *m_bus; GstStateChangeReturn m_ret; ostringstream m_launch_stream; GError *m_error = nullptr; static string m_launch_string; gstcontroled::gstcontroled() { this-&gt;m_launch_stream &lt;&lt; \"nvarguscamerasrc sensor-id=0 ! \" &lt;&lt; \"video/x-raw(memory:NVMM), format=NV12, width=(int)1920, height=(int)1080, framerate=59/1 ! \" &lt;&lt; \"nvvidconv ! \" &lt;&lt; \"video/x-raw(memory:NVMM),width=1280,height=720,framerate=59/1 ! \" &lt;&lt; \"nvoverlaysink enable-last-sample=true name=sink\"; m_launch_string = this-&gt;m_launch_stream.str(); this-&gt;m_pipeline = gst_parse_launch(m_launch_string.c_str(), &amp;this-&gt;m_error); if (this-&gt;m_pipeline == nullptr) { g_print( \"Failed to parse launch: %s\\n\", this-&gt;m_error-&gt;message); return; } if(this-&gt;m_error) g_error_free(this-&gt;m_error); this-&gt;FocalPoint(450); this-&gt;m_ret = gst_element_set_state (this-&gt;m_pipeline, GST_STATE_PLAYING); if (this-&gt;m_ret == GST_STATE_CHANGE_FAILURE) { g_printerr (\"Unable to set the pipeline to the playing state.\\n\"); gst_object_unref (this-&gt;m_pipeline); return; } this-&gt;m_bus = gst_element_get_bus (this-&gt;m_pipeline); guint bus_watch_id; bus_watch_id = gst_bus_add_watch (this-&gt;m_bus, this-&gt;my_bus_callback, NULL); } void gstcontroled::Camera_CaptureSnapshot() { g_print(\"started capture\"); GstCaps *caps; GstSample *from_sample, *to_sample; GError *err = NULL; GstBuffer *buf; GstMapInfo map_info; GstElement * v_sinkbin; v_sinkbin = gst_bin_get_by_name( GST_BIN(this-&gt;m_pipeline), \"sink\"); if(v_sinkbin == NULL){ g_print(\"Sink was empty\"); return; } g_object_get (v_sinkbin, \"last-sample\", &amp;from_sample, NULL); if (from_sample == NULL) { g_print (\"Error getting last sample from sink\"); return; } caps = gst_caps_from_string(\"image/png\"); to_sample = gst_video_convert_sample (from_sample, caps, GST_CLOCK_TIME_NONE, &amp;err); gst_caps_unref (caps); gst_sample_unref (from_sample); if (to_sample == NULL &amp;&amp; err) { g_print (\"Error converting frame: %s\", err-&gt;message); g_error_free (err); return; } buf = gst_sample_get_buffer (to_sample); if (gst_buffer_map (buf, &amp;map_info, GST_MAP_READ)) { const gchar * PIC_LOCATION = \"/home/usr/Pictures/picture110222.png\"; if (!g_file_set_contents (PIC_LOCATION, (const char *) map_info.data, 100000, &amp;err)) { g_print (\"Could not save thumbnail: %s\", err-&gt;message); g_error_free (err); } else{ g_print(\"Image saved\"); } } gst_buffer_unmap (buf, &amp;map_info); } ` The functionality that it currently has: The application opens the main window as well as the overlay stream. Because I am coding from the remote desktop I can see the from on the rdp window, and the video stream on the small touch screen connected to the Nano. I am able to adjust the motorized focus from the form interface by changing the value and clicking the \"Focus\" button. What is not working: When you click on the \"Capture\" button the function fills the sink from the pipeline, and fills the \"GstSample *from_sample\". Then returns the error \"Internal data stream error.\" when executing \"to_sample = gst_video_convert_sample (from_sample, caps, GST_CLOCK_TIME_NONE, &amp;err);\". My Troubleshooting: I have tried to modify the launch stream to have the enable-last-sample. I have tried to add another GstElement for nvvidconv with a png/jpeg/webp encoder and appsink/filesink. I also tried creating a class for just sending straight to the jpeg file but I need to adjust the focus before capturing an image because the cameras focus resets each time a new pipeline is opened. And I would send it with the initial launch string, but arducams focus hardware runs on a separate hardware line from the camera its self. Which is why I have to use the i2c for sending machine code to the camera to set the focal point. (which the reset every time confuses me just that much more that they have to be talking somehow for it to tell the thing to reset unless its a chip reset of some kind ... but I digress) What I'm expecting: When I run the function gstcontroled::Camera_CaptureSnapshot() I want an image captured (I have presumed that pulling from the sample is the correct approach.) from the playing pipeline to be saved to the designated file location. The file name will change and be fed to the function at a later time. For now I can just overwrite the image it outputs so I know its working.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to run openCV in C++ and capture the camera input. The program looks like this: #include &lt;iostream&gt; #include &lt;sstream&gt; #include &lt;new&gt; #include &lt;string&gt; #include &lt;sstream&gt; #include &lt;opencv2/opencv.hpp&gt; #include &lt;opencv2/core.hpp&gt; #include &lt;opencv2/imgcodecs.hpp&gt; #include &lt;opencv2/highgui.hpp&gt; #define INPUT_WIDTH 3264 #define INPUT_HEIGHT 2464 #define DISPLAY_WIDTH 640 #define DISPLAY_HEIGHT 480 #define CAMERA_FRAMERATE 21/1 #define FLIP 2 void DisplayVersion() { std::cout &lt;&lt; \"OpenCV version: \" &lt;&lt; cv::getVersionMajor() &lt;&lt; \".\" &lt;&lt; cv::getVersionMinor() &lt;&lt; \".\" &lt;&lt; cv::getVersionRevision() &lt;&lt; std::endl; } int main(int argc, const char** argv) { DisplayVersion(); std::stringstream ss; ss &lt;&lt; \"nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3264, height=2464, format=NV12, framerate=21/1 ! nvvidconv flip-method=2 ! video/x-raw, width=480, height=680, format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink\"; //ss &lt;&lt; \"nvarguscamerasrc ! video/x-raw(memory:NVMM), width=\" &lt;&lt; INPUT_WIDTH &lt;&lt; //\", height=\" &lt;&lt; INPUT_HEIGHT &lt;&lt; //\", format=NV12, framerate=\" &lt;&lt; CAMERA_FRAMERATE &lt;&lt; //\" ! nvvidconv flip-method=\" &lt;&lt; FLIP &lt;&lt; //\" ! video/x-raw, width=\" &lt;&lt; DISPLAY_WIDTH &lt;&lt; //\", height=\" &lt;&lt; DISPLAY_HEIGHT &lt;&lt; //\", format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink\"; cv::VideoCapture video; video.open(ss.str()); if (!video.isOpened()) { std::cout &lt;&lt; \"Unable to get video from the camera!\" &lt;&lt; std::endl; return -1; } std::cout &lt;&lt; \"Got here!\" &lt;&lt; std::endl; cv::Mat frame; while (video.read(frame)) { cv::imshow(\"Video feed\", frame); if (cv::waitKey(25) &gt;= 0) { break; } } std::cout &lt;&lt; \"Finished!\" &lt;&lt; std::endl; return 0; } When running this code I get the following outout: OpenCV version: 4.6.0 nvbuf_utils: Could not get EGL display connection Error generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst-nvarguscamera/gstnvarguscamerasrc.cpp, execute:751 Failed to create CaptureSession [ WARN:0@0.269] global /tmp/build_opencv/opencv/modules/videoio/src/cap_gstreamer.cpp (1405) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1 Got here! Finished! If I run the other commented command to video.open() I get this output: OpenCV version: 4.6.0 nvbuf_utils: Could not get EGL display connection Error generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst-nvarguscamera/gstnvarguscamerasrc.cpp, execute:751 Failed to create CaptureSession I'm currently running this from headless mode on a jetson nano. I also know that OpenCV and xlaunch works because I can use mjpeg streamer from my laptop and successfully stream my laptop camera output to my jetson nano by using video.open(http://laptop-ip:laptop-port/); and that works correctly (OpenCV is able to display a live video feed using xlaunch just fine). I think this command is telling me my camera is successfully installed: $ v4l2-ctl -d /dev/video0 --list-formats-ext ioctl: VIDIOC_ENUM_FMT Index : 0 Type : Video Capture Pixel Format: 'RG10' Name : 10-bit Bayer RGRG/GBGB Size: Discrete 3264x2464 Interval: Discrete 0.048s (21.000 fps) Size: Discrete 3264x1848 Interval: Discrete 0.036s (28.000 fps) Size: Discrete 1920x1080 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 1640x1232 Interval: Discrete 0.033s (30.000 fps) Size: Discrete 1280x720 Interval: Discrete 0.017s (60.000 fps) Any help would be much appreciated",
        "answers": [
            [
                "The warning seems to be stating that you cannot use egl, ie OpenGL when in headless mode (because there is no screen) If you run in headless mode. Would in not make more sense to not try to open a window do display? cv::imshow(\"Video feed\", frame); if (cv::waitKey(25) &gt;= 0) { break; } Remove this code and instead use cv::imwrite to write to a file, or whatever you want to do with the data. Or if you run ssh. Run ssh with -X option to show the windows on your client computer instead. Could be slow, but if you really want to use cv::imshow it could be a option."
            ],
            [
                "Well I fixed it by rebooting. I already did do a reboot but I also now have some errors whenever I run the program. I did recompile the dlib library but so I do think that when you update the gstreamer library you need to reboot your machine to successfully use it."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "Nvidia Tegra X1 (L4T) used with Linux (Ubuntu 18.04 LTS) is set by default to full range RGB (0-255) or limited range RGB (16-235)? I found that is set to limited Range, how can I modify default to full range RGB, I found nvidia control panel allows to set it in Windows, but for Linux there is an option with nvidia-settings. However, Tegra X1 doesn't allow me to use nvidia-settings. I found that Nvidia is set to limited here: https://forums.developer.nvidia.com/t/jetson-nano-graphics-display-grayscale-resolution/188829",
        "answers": [],
        "votes": []
    },
    {
        "question": "We are working on using AWS IVS to work with severeal devices but we want to know two things. Does the url link change anytime wwe use the AWS IVS to stream live video feeds to AWS. How do we associate a device ID in AWS IVS for multiple unit. Note we are currently making use of the Nvidia Jetson nano We have tested AWS IVS using OBS on jetson nano but we are limited on credits hence my ask",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am writing a code to read frames from an IP camera using OpenCV in a Jetson Nano Device with L4T R32.7.1 and JetPack 4.6.1. The version of OpenCV is 4.3.0 with Python3.6.9 The code of example that i am using is the following: import cv2 input_URI = \"rtsp://user:psw@IP_address:88/videoMain\" camera = cv2.VideoCapture(input_URI) while camera.isOpened(): ret, frame = camera.read() frame_rgba = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA) print(frame_rgba) However, i get the following error. [ WARN:0] global /tmp/pip-install-acvfpz_0/opencv-python_7732674b1cb7471d8a299216848188a1/opencv/modules/videoio/src/cap_gstreamer.cpp (1759) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module source reported: Unauthorized [ WARN:0] global /tmp/pip-install-acvfpz_0/opencv-python_7732674b1cb7471d8a299216848188a1/opencv/modules/videoio/src/cap_gstreamer.cpp (888) open OpenCV | GStreamer warning: unable to start pipeline [ WARN:0] global /tmp/pip-install-acvfpz_0/opencv-python_7732674b1cb7471d8a299216848188a1/opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created [ERROR:0] global /tmp/pip-install-acvfpz_0/opencv-python_7732674b1cb7471d8a299216848188a1/opencv/modules/videoio/src/cap.cpp (142) open VIDEOIO(CV_IMAGES): raised OpenCV exception: OpenCV(4.3.0) /tmp/pip-install-acvfpz_0/opencv-python_7732674b1cb7471d8a299216848188a1/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): rtsp://usr:psw@IP_address:88/videoMain in function 'icvExtractPattern' The code is inside a Docker container. Do you know how to fix it? It worked for a previous version of OpenCV but I updated the version and I started to receive this error. I tried to downgrade OpenCV but the error still appears for previous versions. Not sure what should i do. Thank you",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using a 8265A 8265NGW module with Jetson Nano. I am trying to connect my Jetson Nano. How to check my bluetooth UUID?",
        "answers": [
            [
                "Try running bluetoothctl info For more info, see this question on SE.U&amp;L, What are the UUIDs listed by bluetoothctl info?"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to control the leds as I want using the p9813 led driver and I am using Jetson Nano. There is a ready-made code that I found on github. I try this but I get module error. I've looked through machine modules from Pypi and I'm still getting the same error even though I've installed a few of them. Which machine module do I need to install? from machine import Pin import p9813 pin_clk = Pin(5, Pin.OUT) pin_data = Pin(4, Pin.OUT) num_leds = 10 chain = p9813.P9813(pin_clk, pin_data, num_leds) #set the first LED to red chain[0] = (255, 0, 0) #set the second LED to green chain[1] = (0, 255, 0) #rite data to all LEDs chain.write() #make all LEDs red chain.fill((255,0,0)) chain.write() #turn off all LEDs chain.reset() https://github.com/mcauser/micropython-p9813 I have micropython-p9813 (1.0.0), machine (0.0.1) and Python-Machine (1.0.2) but I get an error while installing machine-py. I got this error in other places as well, but I could not solve it. I am using pip3. Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-f1m5t3ju/matplotlib/ Or is there any other way i can control the led driver with jetson nano?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a Electron App bulid by React Run on jetson nano OS is ubuntu and I connect to TV so can use remote control to action. Question is when I adjust the volume by remote control, the screen will be affected,show a little delay render, any way to prevent it?",
        "answers": [],
        "votes": []
    },
    {
        "question": "The Jetson Nano has a 40 pin header but the pins are configured as GPIO. How does one convert this to SPI? After following the instructions in the NVidia documentation I do not find spi* devices in /dev. The instructions in Jetson hacks is out dated as well.",
        "answers": [
            [
                "The instructions in Jetson hacks is for Jetpack 4.3 - I am currently on Jetpack 4.6. To setup SPI, follow the instructions in the NVidia documentation. Then run sudo modprobe spidev. Now running ls /dev/spi* will show spidev0.0 etc."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Accidentally changed the root=/dev/mmcblk0p1 to root=/dev/sda1 on the extlinux.conf file on a Jetson Nano SUB version board and now I am stuck on a page which shows bash-4.4# when I boot the board, bash-4.4# ls, #bin dev etc init lib mnt proc root sbin sys tmp usr var , everytime that I boot the board. How can I try to change back the extlinux.conf file to add the root back to root=/dev/mmcblk0p1? I found this file after I went to cd /boot/extlinux/ and sudo cp extlinux.conf extlinux.conf.boot_emmc_backup then entered the vim extlinux.conf to modify it but now when I boot the system at bash-4.4# the command cd /boot/extlinux/ does not exists and I do not know how to get back to the previous boot loading screen.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to run a simple code below to run our CSI Camera. import cv2 camera = cv2.VideoCapture(\"nvarguscamerasrc !\" \"video/x-raw(memory:NVMM), \" \"width = (int)640, height = (int)480, \" \"format = (string)NV12, framerate = (fraction)25/1 ! \" \"nvvidconv flip-method = 2 ! \" \"video/x-raw, width = (int)640, height = '(int)480, format = (string)BGRx ! \" \"videoconvert ! \" \"video/x-raw, format = (string)BGR ! appsink\" ) while True: control,frame = camera.read() cv2.imshow(\"CSI Camera\",frame) if (cv2.waitKey(10) &amp; 0xFF == ord('q')): break However, I seem to get an error in line 16, cv2.imshow(\"CSI Camera\",frame). The error says the following: \"cv2.error: OpenCV(4.1.1) /home/nvidia/host/build_opencv/nv_opencv/modules/highgui/src/window.cpp:352: error: (-215:Assertion failed) size.width&gt;0 &amp;&amp; size.height&gt;0 in f unction imshow\" The OS I am using is the Jetbot OS, which I believe is a modified version of Ubuntu by NVIDIA. Does anyone here know what this error means and guide me on how to deal with this? Much appreciated, thanks!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to write a Python wrapper for a C++ project that uses OpenCV (my C++ knowledge is very limited). I installed OpenCV 4.5.0 using this link: https://qengineering.eu/install-opencv-4.5-on-jetson-nano.html Then I managed to compile the project on the nano and create an .so file, which I then try to load with cppyy, but I'm getting a very long list of errors when I try to load one of the headers: Traceback (most recent call last): File \"AlgoTest.py\", line 9, in &lt;module&gt; cppyy.include('NewAlgoManager.h') File \"/home/jetson/.local/lib/python3.6/site-packages/cppyy/__init__.py\", line 245, in include raise ImportError('Failed to load header file \"%s\"%s' % (header, err.err)) ImportError: Failed to load header file \"NewAlgoManager.h\" In file included from input_line_18:1: In file included from ./include/NewAlgoManager.h:4: In file included from /usr/include/opencv4/opencv2/opencv.hpp:52: In file included from /usr/include/opencv4/opencv2/core.hpp:52: In file included from /usr/include/opencv4/opencv2/core/cvdef.h:343: In file included from /usr/include/opencv4/opencv2/core/cv_cpu_dispatch.h:213: /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:38:9: error: unknown type name '__Int8x8_t' typedef __Int8x8_t int8x8_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:39:9: error: unknown type name '__Int16x4_t' typedef __Int16x4_t int16x4_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:40:9: error: unknown type name '__Int32x2_t' typedef __Int32x2_t int32x2_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:41:9: error: unknown type name '__Int64x1_t' typedef __Int64x1_t int64x1_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:42:9: error: unknown type name '__Float16x4_t' typedef __Float16x4_t float16x4_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:43:9: error: unknown type name '__Float32x2_t' typedef __Float32x2_t float32x2_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:44:9: error: unknown type name '__Poly8x8_t' typedef __Poly8x8_t poly8x8_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:45:9: error: unknown type name '__Poly16x4_t' typedef __Poly16x4_t poly16x4_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:46:9: error: unknown type name '__Uint8x8_t' typedef __Uint8x8_t uint8x8_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:47:9: error: unknown type name '__Uint16x4_t' typedef __Uint16x4_t uint16x4_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:48:9: error: unknown type name '__Uint32x2_t' typedef __Uint32x2_t uint32x2_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:49:9: error: unknown type name '__Float64x1_t' typedef __Float64x1_t float64x1_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:50:9: error: unknown type name '__Uint64x1_t' typedef __Uint64x1_t uint64x1_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:51:9: error: unknown type name '__Int8x16_t' typedef __Int8x16_t int8x16_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:52:9: error: unknown type name '__Int16x8_t' typedef __Int16x8_t int16x8_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:53:9: error: unknown type name '__Int32x4_t' typedef __Int32x4_t int32x4_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:54:9: error: unknown type name '__Int64x2_t' typedef __Int64x2_t int64x2_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:55:9: error: unknown type name '__Float16x8_t' typedef __Float16x8_t float16x8_t; ^ /usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:56:9: error: unknown type name '__Float32x4_t' typedef __Float32x4_t float32x4_t; ^ fatal error: too many errors emitted, stopping now [-ferror-limit=] I saw there's a flag ENABLE_NEON=ON that I can turn off while comiling OpenCV, but I understood that it's not recommended. Anyone got an idea what could be causing this? Thank you for the help.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to run YOLOv3 on Visual Studio 2019 using CUDA 10.2 with cuDNN v7.6.5 on Windows 10 using NVidia GeForce 930M. Here is part of the code I used. #include &lt;fstream&gt; #include &lt;sstream&gt; #include &lt;iostream&gt; #include &lt;opencv2/dnn.hpp&gt; #include &lt;opencv2/imgproc.hpp&gt; #include &lt;opencv2/highgui.hpp&gt; using namespace cv; using namespace dnn; using namespace std; int main() { // Load names of classes string classesFile = \"coco.names\"; ifstream ifs(classesFile.c_str()); string line; while (getline(ifs, line)) classes.push_back(line); // Give the configuration and weight files for the model String modelConfiguration = \"yolovs.cfg\"; String modelWeights = \"yolov3.weights\"; // Load the network Net net = readNetFromDarknet(modelConfiguration, modelWeights); net.setPreferableBackend(DNN_BACKEND_CUDA); net.setPreferableTarget(DNN_TARGET_CUDA); // Open the video file inputFile = \"vid.mp4\"; cap.open(inputFile); // Get frame from the video cap &gt;&gt; frame; // Create a 4D blob from a frame blobFromImage(frame, blob, 1 / 255.0, Size(inpWidth, inpHeight), Scalar(0, 0, 0), true, false); // Sets the input to the network net.setInput(blob); // Runs the forward pass to get output of the output layers vector&lt;Mat&gt; outs; net.forward(outs, getOutputsNames(net)); } Although I add $(CUDNN)\\include;$(cudnn)\\include; to Additional Include Directories in both C/C++ and Linker, added CUDNN_HALF;CUDNN; to C/C++&gt;Preprocessor Definitions, and added cudnn.lib; to Linker&gt;Input, I still get this warning: DNN module was not built with CUDA backend; switching to CPU and it runs on CPU instead of GPU, can anyone help me with this problem?",
        "answers": [
            [
                "I solved it by using CMake, but I had first to add this opencv_contrib then rebuilding it using Visual Studio. Make sure that these WITH_CUDA, WITH_CUBLAS, WITH_CUDNN, OPENCV_DNN_CUDA, BUILD_opencv_world are checked in CMake."
            ],
            [
                "I had a similar issue happen to me about a week ago, but I was using Python and Tensorflow. Although the languages were different compared to C++, I did get the same error. To fix this, I uninstalled CUDA 10.2 and downgraded to CUDA 10.1. From what I have found, there might be a dependency issue with CUDA, or in your case, OpenCV hasn't created support yet for the latest version of CUDA. EDIT After some further research it seems to be an issue with Opencv rather than CUDA. Referencing this github thread, if you installed Opencv with cmake, remove the arch bin version below 7 on the config file, then rebuild/reinstall Opencv. However, if that doesn't work, another option would be to remove CUDA arch bin version &lt; 5.3 and rebuild."
            ]
        ],
        "votes": [
            5.0000001,
            1e-07
        ]
    },
    {
        "question": "I set up my Jetson Nano today and installed pip and opencv. sudo apt-get install python3-pip pip install opencv-python Now I tried this code: import cv2 print(cv2.getBuildInformation()) print(cv2.__version__) and get Illegal instruction (core dumped) I wrote with gedit the code and save as .py and call it with python3 test.py",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a headless Jetson Nano setup. Setup and First Boot ls /dev/cu.usbmodem* sudo screen /dev/cu.usbmodem14216221044843 115200 It is a USB connection between the Jetson Nano and the Mac, and the above command to configure the Jetson Nano. But when I run it again it asks for user and password and just logs me in. I made a mistake in my setup and would like to do the same again. I tried initializing the SD card again with balenaEtcher, but it seems my settings are still recorded somewhere. Perhaps I can do the same thing by running some command after I log in. Can you tell me what it is? If that is difficult, can you tell me how to initialize my settings and show the system configuration again?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to connect and use a DHT11 Temp sensor on a jeston nano. I am using C_DHT.c lib Below is the kind of error msg I get: gpioSetDirection: Unknown error 517 gpioSetValue: Operation not permitted gpioSetValue: Operation not permitted gpioSetDirection: Unknown error 517 gpioSetDirection: Unknown error 517 gpioSetValue: Operation not permitted TG(-39909.0, -39909.0, 0) Build and install C_DHT.c done successfully with sudo.",
        "answers": [
            [
                "I just fixed the permission issues, it was due to wrong pin number setup. But I am still always getting (-39909.0, -39909.0, 0) as response. so no real measure. If anybody could help with correct connections, info are welcome. Thanks. I am using a 3 pin DHT11, left pin connected to 3.3v center pin to pin 37 of the jetson nano for the data right pin to grn through a 10k resistor."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "GStreamer: Multiple webcam sources, Picture in Picture to mux on a Jetson Nano, then to be used as a pipeline with belabox &gt;&gt; belabox.net I'm currently trying to pull two different usb webcams into a pipeline and create a Picture-in-Picture composite, but I keep getting an error like this: GStreamer Error: gstreamer error from v4l2src1 This is the current pipeline I'm working on that doesn't work; v4l2src device=/dev/video0 ! image/jpeg,width=1920,height=1080,framerate=60/1 ! nvvidconv ! queue ! comp.sink_0 ! v4l2src device=/dev/video1 ! image/jpeg,width=800,height=448,framerate=60/1 ! videobox left=-4 right=-4 top=-4 bottom=-4 ! nvvidconv ! queue ! comp.sink_1 ! nvcompositor name=comp sink_0::width=1920 sink_0::height=1080 sink_1::width=640 sink_1::height=360 sink_1::xpos=1266 sink_1::ypos=706 sink_1::zorder=2 ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! nvvidconv ! 'video/x-raw(memory:NVMM),format=NV12' ! queue ! identity name=v_delay signal-handoffs=TRUE ! nvv4l2decoder mjpeg=1 enable-max-performance=true ! nvvidconv ! textoverlay text='' valignment=top halignment=right font-desc=\"Monospace, 5\" name=overlay ! queue ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mux. alsasrc device=hw:2 ! identity name=a_delay signal-handoffs=TRUE ! volume volume=1.0 ! audioconvert ! opusenc bitrate=320000 ! opusparse ! queue max-size-time=10000000000 max-size-buffers=1000 ! mux. mpegtsmux name=mux ! appsink name=appsink Please note; I have two separate pipelines that can use each webcam, so I know the cameras work and can be used, I just need them to be in the same pipeline have a picture-in-picture composite and work in the belabox environment. For reference this is a working usb webcam pipeline that works with belabox: v4l2src device=/dev/video0 ! image/jpeg,width=1920,height=1080,framerate=60/1 ! identity name=v_delay signal-handoffs=TRUE ! nvv4l2decoder mjpeg=1 enable-max-performance=true ! nvvidconv ! textoverlay text='' valignment=top halignment=right font-desc=\"Monospace, 5\" name=overlay ! queue ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mux. alsasrc device=hw:2 ! identity name=a_delay signal-handoffs=TRUE ! volume volume=1.0 ! audioconvert ! voaacenc bitrate=128000 ! aacparse ! queue max-size-time=10000000000 max-size-buffers=1000 ! mux. mpegtsmux name=mux ! appsink name=appsink Any Ideas?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I use Visual Studio 2019 and connect to Jetson Nano remotely using \"Connection Manager\" in Cross Platform. I set \"/usr/include/opencv4/;$(IncludePath)\" in VC++ Directories at Project Options, so it did not give any error for libraries. But I try to build the code in Visual Studio 2019 remotely, it gives many errors like \"undefined reference to cv::xxx\" I checked several questions, it was said that it is linker issue and the only solution was to use the command: g++ -o appname appname.cpp `pkg-config --cflags --libs opencv4` When I use this command in the Jetson Nano terminal, the output file is generated and it works, but I would like to work remotely.. Any ideas on how to set up Settings for Linker in Project Properties in Visual Studio 2019?",
        "answers": [],
        "votes": []
    },
    {
        "question": "enter link description here i follow this youtuber try to install Visual Studio Code on jetson nano This happened when I used this command:./installVSCodeWithPython.sh,i recieve this: by the way the link of youtube is up,English is not very good, please forgive me! Complete output from command python setup.py egg_info: Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; File \"/tmp/pip-build-g22gfh_9/lazy-object-proxy/setup.py\", line 146, in &lt;module&gt; distclass=BinaryDistribution, File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 129, in setup return distutils.core.setup(**attrs) File \"/usr/lib/python3.6/distutils/core.py\", line 108, in setup _setup_distribution = dist = klass(attrs) File \"/usr/lib/python3/dist-packages/setuptools/dist.py\", line 372, in __init__ _Distribution.__init__(self, attrs) File \"/usr/lib/python3.6/distutils/dist.py\", line 281, in __init__ self.finalize_options() File \"/usr/lib/python3/dist-packages/setuptools/dist.py\", line 528, in finalize_options ep.load()(self, ep.name, value) File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2324, in load return self.resolve() File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2330, in resolve module = __import__(self.module_name, fromlist=['__name__'], level=0) File \"/tmp/pip-build-g22gfh_9/lazy-object-proxy/.eggs/setuptools_scm-7.0.5-py3.6.egg/setuptools_scm/__init__.py\", line 5 from __future__ import annotations ^ SyntaxError: future feature annotations is not defined ---------------------------------------- Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-g22gfh_9/lazy-object-proxy/ Collecting black* If there is more information needed, I will provide Thanks in advance!",
        "answers": [
            [
                "According to this issue. This seems to be caused by the incorrect selection of the Python interpreter. However, you are currently in the process of installing vscode, which seems to be a mixed error. I think it is more reasonable to change the download method. You can refer to the docs for details about installing vscode and python. It's more official. Here is the download page on vscode and python."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have retrained a model with tensorflow v2 and I want it to run on a Jetson Nano GPU. For that I had to save the model from .h5 as .pb then to .onnx and then to .trt (for which I also had to make the conversion to onnx with opset 12). Now when I can finally run the model, I am reusing an old code that used to work with the old .trt model but at locking a page: import pycuda.driver as cuda .... for binding in engine: size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size dtype = trt.nptype(engine.get_binding_dtype(binding)) host_mem = cuda.pagelocked_empty(size, dtype) device_mem = cuda.mem_alloc(host_mem.nbytes) This results in an error: pycuda_driver.LogicError: cuMemAlloc failed: invalid argument After further debugging it turns out cuda.pagelocked_empty(size, dtype) retuns [] at the output binding separable_conv2d_29 with size=0 and dtype=numpy.float32. With the running code, the size is &gt;0 for both input and output bindings.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I would like to convert a .h265 file into a .mp4 one with gstreamer. I know that with ffmpeg it can be done with the command: ffmpeg -framerate 30 -i video.h265 -c copy video.mp4 I'm looking for the equivalent in gstreamer to be used on a Jetson Nano. I tried with: gst-launch-1.0 filesrc location=video.h265 ! video/x-h265 ! h265parse ! mp4mux! filesink location=video.mp4 but it gives me error: Setting pipeline to PAUSED ... Pipeline is PREROLLING ... Pipeline is PREROLLED ... Setting pipeline to PLAYING ... New clock: GstSystemClock ERROR: from element /GstPipeline:pipeline0/GstMP4Mux:mp4mux0: Could not multiplex stream. Additional debug info: gstqtmux.c(4561): gst_qt_mux_add_buffer (): /GstPipeline:pipeline0/GstMP4Mux:mp4mux0: Buffer has no PTS. Execution ended after 0:00:00.016003228 Setting pipeline to PAUSED ... Setting pipeline to READY ... Setting pipeline to NULL ... Freeing pipeline ... Thank you Solution I solved by first decoding and then encoding. Also switched to h264 because it was faster on my jetson: gst-launch-1.0 filesrc location=video.h264 ! h264parse ! 'video/x-h264' ! omxh264dec! videoconvert ! omxh264enc! h264parse ! mp4mux ! filesink location=video.mp4 -e\" Still wondering if it is possible to not perform the decoding-encoding process, since to me it seems a waste of resources.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to run PyTorch on a NVIDIA Jetson Nano and my project requires me to use CUDA. I'm running on Ubuntu 18.04 and Python 3.10.6. I followed this guide to install CUDA 11.6. Then using the instructions on PyTorch.org I installed PyTorch using this command: pip install torch==1.12.0 torchvision==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu116 But then when I try to verify it, it's not available: (env) $ python Python 3.10.6 (main, Aug 2 2022, 15:11:03) [GCC 7.5.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import torch &gt;&gt;&gt; torch.cuda.is_available() False &gt;&gt;&gt; here's the CUDA version: (env) $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Fri_Dec_17_18:16:35_PST_2021 Cuda compilation tools, release 11.6, V11.6.55 Build cuda_11.6.r11.6/compiler.30794723_0 And here is some information from PyTorch. Notice how it says CUDA used to build PyTorch: Could not collect. (env) $ python -m torch.utils.collect_env PyTorch version: 1.12.0 Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (aarch64) GCC version: (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.10.2 Libc version: glibc-2.27 Python version: 3.10.6 (main, Aug 2 2022, 15:11:03) [GCC 7.5.0] (64-bit runtime) Python platform: Linux-4.9.253-tegra-aarch64-with-glibc2.27 Is CUDA available: False CUDA runtime version: 11.6.55 GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/aarch64-linux-gnu/libcudnn.so.8.2.1 /usr/lib/aarch64-linux-gnu/libcudnn_adv_infer.so.8.2.1 /usr/lib/aarch64-linux-gnu/libcudnn_adv_train.so.8.2.1 /usr/lib/aarch64-linux-gnu/libcudnn_cnn_infer.so.8.2.1 /usr/lib/aarch64-linux-gnu/libcudnn_cnn_train.so.8.2.1 /usr/lib/aarch64-linux-gnu/libcudnn_ops_infer.so.8.2.1 /usr/lib/aarch64-linux-gnu/libcudnn_ops_train.so.8.2.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.1 [pip3] torch==1.12.0 [pip3] torchvision==0.13.0 [conda] Could not collect Any help is appreciated. Thanks.",
        "answers": [
            [
                "Downloading the compiled whl file from nvidia and installing it with pip did the trick (as suggested by @FlyingTeller). wget https://nvidia.box.com/shared/static/p57jwntv436lfrd78inwl7iml6p13fzh.whl -O torch-1.8.0-cp36-cp36m-linux_aarch64.whl pip install numpy torch-1.8.0-cp36-cp36m-linux_aarch64.whl"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Question: Phrased another way. My Intel Bluetooth controller accepts data from my Polar Bluetooth LE device and places the data on the D-bus system bus. How do I use the bluez API and D-Bus to read my Polar sensors heart rate data? In an attempt to at least see the Polar sensor, I ran c code written by Parthiban Nallathambi at www.linumiz.com: https://www.linumiz.com/bluetooth-list-devices-using-gdbus/. Providing this for credit and background. The code accurately displayed the Polar sensor attributes, but no data. FYI, the 1st few executions it actually did display ManufacturerData: Address : D2:9C:2A:C8:F9:CA AddressType : random Name : Polar H9 ADAC102E Alias : Polar H9 ADAC102E Appearance : Other Paired : 1 Trusted : 1 Blocked : 0 LegacyPairing : 0 Connected : 0 UUIDs : 00001800-0000-1000-8000-00805f9b34fb 00001801-0000-1000-8000-00805f9b34fb 0000180a-0000-1000-8000-00805f9b34fb 0000180d-0000-1000-8000-00805f9b34fb 0000180f-0000-1000-8000-00805f9b34fb 0000181c-0000-1000-8000-00805f9b34fb 0000feee-0000-1000-8000-00805f9b34fb 6217ff4b-fb31-1140-ad5a-a45545d7ecf3 Adapter : Other ServicesResolved : 0 Then I ran bluetoothctl to display vendor data in ManufacturerData: steven@DEVELOPMENT-JETSON:~$ bluetoothctl [NEW] Device D2:9C:2A:C8:F9:CA Polar H9 ADAC102E [NEW] Primary Service /org/bluez/hci0/dev_D2_9C_2A_C8_F9_CA/service0045 0000feee-0000-1000-8000-00805f9b34fb Polar Electro Oy [NEW] Characteristic /org/bluez/hci0/dev_D2_9C_2A_C8_F9_CA/service000e/char000f 00002a37-0000-1000-8000-00805f9b34fb Heart Rate Measurement [bluetooth]# **connect D2:9C:2A:C8:F9:CA** Attempting to connect to D2:9C:2A:C8:F9:CA [CHG] Device D2:9C:2A:C8:F9:CA Connected: yes Connection successful [CHG] Device D2:9C:2A:C8:F9:CA ServicesResolved: yes [Polar H9 ADAC102E]# scan on Discovery started [CHG] Device D2:9C:2A:C8:F9:CA RSSI: -67 [CHG] Device D2:9C:2A:C8:F9:CA ManufacturerData Key: 0x006b [CHG] Device D2:9C:2A:C8:F9:CA ManufacturerData Value: 33 1e 33 33 3.33 I'm just baffled, I can't find any examples of c code that does the following (pseudo code): Pair to device given device ID or address Iteratively/continually read ManufacturerData where key = 0x006b Pull out heart rate data from array Not looking for someone to write the code, but for someone to point me at the bluez/dbus functions or code if you have it :-), that will accomplish this. Thanks for you time. I'm just stumped. I have already looked at the Bluetooth for Linux Developers Study Guide, but its in Python and I'm looking for a C guide.",
        "answers": [
            [
                "If you followed the example in here then you should have a function named bluez_property_value, in here you need to compare the key param with \"ManufacturerData\" and then extract that information using GVariant in this case is an a{qv} an array of dictionaries containing a q id and a v variant where this variant is an array of bytes ay. void bluez_property_value(const gchar *key, GVariant *value) { if (g_strcmp0(key, \"ManufacturerData\") != 0) return; const guint16 id; GVariant *manufacturing_data = NULL; GVariantIter *iter; g_variant_get(value, \"a{qv}\", &amp;iter); while (g_variant_iter_loop(iter, \"{qv}\", &amp;id, &amp;manufacturing_data)) { const guint8 byte; GVariantIter *bytes; g_variant_get(manufacturing_data, \"ay\", &amp;bytes); while (g_variant_iter_next(bytes, \"y\", &amp;byte)) fprintf(stdout, \"%02X\", byte); fflush(stdout) } } /** * @brief read a float from unsigned int, * Check if the first bit is 1, if it is the number is negative * so we need to negate the number with the ~ operator and extract * a the subset of the bits from the number. * * @param number the number to read from * @return double, the negative or positive value */ double read_negative_value(int number) { if ((number &amp; 0x8000) &gt; 0) return -(double) ((~number) &amp; 0x7FFF); return (double) number; } EDIT: Fix code format. EDIT2: Added a function to read negative values from manufacturer data."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have two Jetson Nano devices that need to exchange data over SPI protocol. It is my first and only project with Jetson Nano, so I am completely new to the SPI or anything regarding low-level coding. In the project, I want to use Daisy Chain to communicate, for starters I have used only two devices to test the data communication between them. Using the following link on page 4, I made one a slave. https://www.nxp.com/files-static/training_pdf/26821_68HC08_SPI_WBT.pdf What it says is that in order to select one device as a slave you need to connect SS to ground to make it low. So I have connected pin 24 (SPI_1_CS0) to pin 20 which is ground. For the master, I have put pin 24 (SPI_1_CS0) to pin 2 (5.0 VDC). I intended to set it high for it to become the master. I have used /opt/nvidia/jetson-io/jetson-io.py to configure the pins. After running sudo modprobe spidev the pins were configured. Output of ls /dev/spi* is /dev/spidev0.0 /dev/spidev0.1 /dev/spidev1.0 /dev/spidev1.1 After running the following code on the slave I received useless data. Since slave mode should have been activated, I should not be able to read data unless Master is sending some. import spidev import time spi = spidev.SpiDev() spi.open(0,0) spi.max_speed_hz = 250000 def read_spi(channel): spidata = spi.xfer2([0,(8+channel)&lt;&lt;4,0]) return ((spidata[1] &amp; 3) &lt;&lt; 8) + spidata[2] def readData(): spidata = spi.readbytes(8) return spidata try: while True: #channelData = read_spi(0) channelData = readData() print (channelData) time.sleep(.1) except KeyboardInterrupt: spi.close() channelData can be received with the other function. But it does not make the result any different. [20, 206, 54, 93, 19, 151, 211, 199] [84, 10, 89, 184, 126, 82, 49, 78] [189, 32, 110, 143, 231, 226, 76, 116] [102, 56, 174, 123, 186, 145, 148, 161] [105, 254, 152, 155, 88, 147, 191, 174] [38, 221, 219, 179, 161, 102, 107, 31] [101, 141, 98, 80, 20, 254, 25, 50] [88, 0, 0, 44, 197, 73, 32, 49] [107, 60, 44, 230, 91, 56, 172, 4] [21, 156, 120, 165, 99, 137, 245, 204] [15, 34, 164, 215, 255, 187, 34, 86] [18, 215, 67, 227, 234, 1, 237, 142] [71, 124, 36, 238, 86, 240, 105, 189] [29, 27, 63, 232, 239, 40, 189, 61] [5, 217, 209, 14, 96, 24, 181, 97] [158, 121, 125, 93, 224, 125, 97, 129] [75, 92, 95, 183, 47, 14, 111, 164] Do I need more configuration to be done if I want to make one a slave or am I doing something wrong with the coding? Any links or code example is appreciated.",
        "answers": [
            [
                "Do you use Nvidia Jetson Nano custom board? Or custom board from other vendor? If you are using Nano devkit, please see the pin out below: enter image description here Picture is from this website. spidev node location correction. There are 2 SPI channels: SPI0 and SPI1. Interesting thing is SPI0 is located at /dev/spidev1 and SPI1 is at /dev/spidev2.x. Please see their device tree setting. Depends on how you wire SPI, if you are using SPI0 and CS/SS is connected to Pin24, use /dev/spidev1.0; if CS/SS is connected pin 26, use /dev/spidev1.1 instead. device tree correction Do you have a chance to modify your device tree? If you are certain of the wire connection correctness , then check device tree setting. As I know, Jetson Nano is not allowed to change SPI slave by any command. The only way is tweak the device tree. I found a possible solution on github. See here"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "jetson nano ubantu18.04 jetpack version:4.2.2 python version:3.6.9 cuda version:10.0 pytorch:1.2.0(install from wheel v1.2.0&amp;jetpack4.2.2&amp;python3.6:https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-11-now-available/72048) pytorch can normally import, than I run: $git clone --branch v0.4.0 https://github.com/pytorch/vision torchvision $cd torchvision $python setup.py install after building, I try to import torchvision, but:error:\"SyntaxError: future feature annotations is not defined\" than I go to see source code, but in setup.py line32 is \"version = '0.4.1a0'\" than I try to reset to last commit which setup.py line32 is \"version = '0.4.0a0'\" after that, I delete the build folder and rerun $python setup.py install than I retry import but sim error: \"&gt;&gt;&gt; import torchvision Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 971, in _find_and_load File \"&lt;frozen importlib._bootstrap&gt;\", line 955, in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 656, in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 626, in _load_backward_compatible File \"/home/ucar/ucar_ws2/.venv/lib/python3.6/site-packages/torchvision-0.4.0a0+ef3ba78-py3.6-linux-aarch64.egg/torchvision/__init__.py\", line 2, in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 971, in _find_and_load File \"&lt;frozen importlib._bootstrap&gt;\", line 955, in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 656, in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 626, in _load_backward_compatible File \"/home/ucar/ucar_ws2/.venv/lib/python3.6/site-packages/torchvision-0.4.0a0+ef3ba78-py3.6-linux-aarch64.egg/torchvision/datasets/__init__.py\", line 1, in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 971, in _find_and_load File \"&lt;frozen importlib._bootstrap&gt;\", line 955, in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 656, in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 626, in _load_backward_compatible File \"/home/ucar/ucar_ws2/.venv/lib/python3.6/site-packages/torchvision-0.4.0a0+ef3ba78-py3.6-linux-aarch64.egg/torchvision/datasets/lsun.py\", line 2, in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 971, in _find_and_load File \"&lt;frozen importlib._bootstrap&gt;\", line 955, in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 656, in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 626, in _load_backward_compatible File \"/home/ucar/ucar_ws2/.venv/lib/python3.6/site-packages/Pillow-9.2.0-py3.6-linux-aarch64.egg/PIL/Image.py\", line 52, in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 971, in _find_and_load File \"&lt;frozen importlib._bootstrap&gt;\", line 951, in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 894, in _find_spec File \"&lt;frozen importlib._bootstrap_external&gt;\", line 1157, in find_spec File \"&lt;frozen importlib._bootstrap_external&gt;\", line 1131, in _get_spec File \"&lt;frozen importlib._bootstrap_external&gt;\", line 1112, in _legacy_get_spec File \"&lt;frozen importlib._bootstrap&gt;\", line 441, in spec_from_loader File \"&lt;frozen importlib._bootstrap_external&gt;\", line 544, in spec_from_file_location File \"/home/ucar/ucar_ws2/.venv/lib/python3.6/site-packages/Pillow-9.2.0-py3.6-linux-aarch64.egg/PIL/_deprecate.py\", line 1 SyntaxError: future feature annotations is not defined\" I'm so sad about it, could you help me, thank you first!!!",
        "answers": [],
        "votes": []
    },
    {
        "question": "i need help to install tensorflow on jetson nano. I follow guide at: https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html After that, my tensorflow version is: 1.15.5+nv22.3 But when i run: import tensorflow This error show up: /usr/lib/aarch64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found Pls help me because i have tried so many ways but libstdc dont update glibcxx_3.4.26 Versions: python: 3.8.0 jetpack: # R32 (release), REVISION: 7.2, GCID: 30192233, BOARD: t210ref, EABI: aarch64, DATE: Wed Apr 20 21:34:48 UTC 2022 Maybe i installed wrong version of TF ?? I tried other versions but only jetpack 5.0 installs successfully",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using Nvidia Jetson Nano and Raspberry Pi V2.1 Camera for color detection via Python and OpenCV. However, the codes that worked on my computer do not work on Jetson Nano and I keep getting errors. The camera is working, I checked it. Also, Jetson Nano is in operation but camera does not opened. I could not get the problem. Here is my code: import cv2 print(cv2.__version__) dispW=320 dispH=240 flip=2 camSet='nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3264, height=2464, format=NV12, framerate=21/1 ! nvvidconv flip-method='+str(flip)+' ! video/x-raw, width='+str(dispW)+', height='+str(dispH)+', format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink' cam=cv2.VideoCapture(camSet) lower_red = (147,46,91) upper_red = (180,255,255) while True: ret, frame=cam.read() hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) mask = cv2.inRange(hsv, lower_red,upper_red) final_frame = cv2.bitwise_and(frame,frame, mask = mask) cv2.imshow(\"Frame\", frame) cv2.imshow(\"Mask\", mask) cv2.imshow(\"Final\", final_frame) if cv2.waitKey(1) ==ord(\"q\"): break cam.release() cv2.destroyAllWindows() And here is the error: 3.2.0 OpenCV Error: Assertion failed ((scn== 3 || scn == 4) &amp;&amp; (depth == CV_8U || depth == CV_32F)) in cvtColor, file /build/opencv-XDqSFW/opencv 3.2.0+dfsg/modules/imgproc/src/color.cpp, line 9815 Traceback (most recent call last): File \"red.py\", line 15, in &lt;module&gt; hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) cv2.error: /build/opencv-XDqSFW/opencv 3.2.0+dfsg/modules/imgproc/src/color.cpp:9815: error: (-215) (scn== 3 || scn== 4) &amp;&amp; (depth == CV_8U || depth == CV_32F) in function cvtColor",
        "answers": [
            [
                "You need to make sure that OpenCV is compiled with gstreamer backend support, using print(cv2.getBuildInformation()), and than check Media I/O: section. You need to make sure that you have configured drivers using sudo /opt/nvidia/jetson-io/jetson-io.py, than you can see your cam using command ls /dev/video*. If not, may be you need to remove R8 resistor on camera board (valid for Raspberry Camera HQ (IMX477))."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to do inference with a yolov5 model on the webcam stream from a Nvidia Jetson Nano within a Docker container. On both, my Windows PC and Jetson, I get the error: qt.qpa.xcb: could not connect to display qt.qpa.plugin: Could not load the Qt platform plugin \"xcb\" in \"/usr/local/lib/python3.10/site-packages/cv2/qt/plugins\" even though it was found. This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem. Available platform plugins are: xcb. Aborted The Dockerfile looks like this: FROM python:latest WORKDIR /yolov5_aiss COPY requirements.txt . RUN pip3 install -r requirements.txt COPY ./app ./app RUN apt-get update -y RUN apt-get install -y ffmpeg libsm6 libxext6 -y RUN apt install -y libxkbcommon-x11-0 CMD [\"python\", \"./app/detect.py\", \"--weights\",\"./app/weights/best.pt\", \"--source\", \"0\"]",
        "answers": [
            [
                "This is the solution. pip uninstall opencv-python pip install opencv-python-headless Note Headless mode can't use --view-img flag of detect.py"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am customising the linux kernel. In normal Linux, the BH thread priority is set to a constant priority (MAX_USER_RT_PRIO / 2) [1], but I am changing the BH thread priority from a constant priority (MAX_USER_RT_PRIO / 2) to another value in the interrupt handler [2]. Specifically, this priority is changed in __irq_wake_thread(). #code1 action-&gt;thread-&gt;prio = prio_input or #code2 sched_setscheduler_nocheck(action-&gt;thread, SCHED_FIFO, &amp; param_input); But with such a customised kernel, the hardware crashes and reboots two seconds later. I debugged it and identified #code1, #code2 as the cause, but I don't understand what's wrong. what was the error when it crashed? It simply downed without any pop-ups like error messages. dmesg has no clues. output of command \"last\" reboot system boot 4.9.201-rt134 Sun Jul 10 11:24 still running him :1 :1 Sun Jul 10 11:21 - crash (00:02) reboot system boot 4.9.201-rt134 Fri Dec 31 20:00 still running Are priority changes ever not allowed in Interrupt? Environment Hardware: Jetson nano Power mode: 10 W Power: 5V4A Kernel: linux kernel-4.9 How to build: https://forums.developer.nvidia.com/t/applying-a-preempt-rt-patch-to-jetpack-4-5-on-jetson-nano/168428/4 [1] In setup_irq_thread() [2] In __irq_wake_thread() Reply for other comments: changing that could cause some critical BHs to not run in time and break things. I change only the priority of a few interrupts(BH), not all interrupts. It is set with care.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to use the i2c bus between Arduino nano 33 IOT and the Jetson Nano 2gb. I'm using the i2c bus and I want to send an array of integers to the Jetson Nano but when I receive data over the bus it's gibberish and destroys the data sent from the Jetson to the Arduino. Jetson Nano pins: GND, 27 (SDA), 28 (SDL) Arduino Nano 33 IoT pins: GND, A4 (SDA), A5 (SCL) Arduino code: #include &lt;Wire.h&gt; int data [4]; int x = 0; void setup() { Serial.begin(9600); Wire.begin(0x6a); Wire.onReceive(receiveData); Wire.onRequest(sendData); } void loop () { //sendData(); delay(100); } void sendData() { int arr[4] = { 0, 23, 41, 19 }; Serial.println(\"Sending Data to Jetson ...\"); //sendI2C((byte*) arr, sizeof(arr)); Wire.write( (byte*)&amp;arr, sizeof(arr)); Serial.print(\"Sent...\"); Serial.println(); } //void sendI2C(byte *data, int size) { // Wire.beginTransmission(0x6a); // for(int i = 0; i &lt; size; i++) { // Wire.write(data[i]); // } // Wire.endTransmission(); //} void receiveData(int byteCount) { while(Wire.available() &amp;&amp; x &lt; 4) { //Wire.available() returns the number of bytes available for retrieval with Wire.read(). Or it returns TRUE for values &gt;0. data[x]=Wire.read(); x++; } if(x == 4) { x = 0; } Serial.println(\"----\"); Serial.print(data[0]); Serial.print(\"\\t\"); Serial.print(data[1]); Serial.print(\"\\t\"); Serial.print(data[2]); Serial.print(\"\\t\"); Serial.println(data[3]); Serial.print(\"----\"); //sendData(); } Jetson Nano - Python3 code: import smbus import time bus = smbus.SMBus(0) address = 0x6a def writeArray(a, b, c, d): bus.write_i2c_block_data(address, a, [b, c, d]) return -1 def readArray(bytes_nr): values = bus.read_i2c_block_data(address, 0x00, bytes_nr) return values while True: writeArray(14,42,95,0) time.sleep(1) values = readArray(8) print(values) There are 2 things that happen: When I only send data from the jetson nano to the arduino, on the Serial monitor of the arduino the data is received correctly: [14, 42, 95, 0] When I try to sendData() to the Jetson Nano on the Jetson Nano console the data received 'print(values)' is like this: [0, 0, 0, 0, 0, 0, 42, 105, 0 , 0, 4, 0 , 0 ,0 ,0 , 56, 0 , 0 , 0 ,0 ,0, 187, 0 , 0 ,0, 0, 0 , 0] -- And on the Arduino console the data shifts from left to right so instead of receiving `[14, 42, 95, 0]`, It prints [8, 14, 42, 95] I'm just interested in sending an array of 4 integers from both sides. Can someone extend a helping hand? Thank you!",
        "answers": [
            [
                "Arduino code uses raw I2C protocol, Jetson uses SMBus on top of I2C. SMBus block structure implies 1 additional byte of length to be transferred."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "import cv2 print(cv2.__version__) dispW=640 dispH=480 flip=2 camSet='nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3264, height=2464, format=NV12, framerate=21/1 ! nvvidconv flip-method='+str(flip)+' ! video/x-raw, width='+str(dispW)+', height='+str(dispH)+', format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink' cam= cv2.VideoCapture(camSet) face_cascade=cv2.CascadeClassifier('/home/ppp/Desktop/test/cascade/face.xml') while True: ret, frame = cam.read() gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray,1.3,5) for(x,y,w,h) in faces: cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255),2) cv2.imshow('nanoCam',frame) cv2.moveWindow('nanoCam',0,0) if cv2.waitKey(1)==ord('q'): break cam.release() cv2.destroyAllWindows() i'm using jetson nano with pi camera at codeOSS. on python3 3.6.9, this code works very well but on change into 3.7.9,it occurs error. OpenCV(4.6.0) /io/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3264, height=2464, format=NV12, framerate=21/1 ! nvvidconv flip-method=2 ! video/x-raw, width=640, height=480, format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink in function 'icvExtractPattern' Traceback (most recent call last): File \"/home/ppp/Desktop/test/test2.py\", line 14, in gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) cv2.error: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor' \u200b and then i tried to change into cam= cv2.VideoCapture(0) it occurs again but on cmd i tried to $ gst-launch-1.0 nvarguscamerasrc sensor_mode=0 ! 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=0 ! 'video/x-raw,width=960, height=616' ! nvvidconv ! nvegltransform ! nveglglessink -e but it works every python version. how could i try",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to run my code on jetson NX5, but even after installation of it, it won't run. even tried this command but still, have the same problem. git clone --recursive https://github.com/Microsoft/vscode.git Any Idea?",
        "answers": [
            [
                "A previous version, e.g. 1.50.0 started without error. wget https://update.code.visualstudio.com/1.50.0/linux-deb-arm64/stable -O stable.deb sudo dpkg -i stable.deb"
            ],
            [
                "May be late reply, But, its worth replying for future reference: Note: This trick works for me for AGX Xavier with JP5.0.1, hope this works for you as well with NX5. There is a bug in the latest release of debian of Vs code for AGX Xavier. As, @Ali, rightly mentioned, lower release of it i.e. 1.50.* works fine. Ref: https://forums.developer.nvidia.com/t/vs-code-can-t-launch-with-jetpack-5-0/213980"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "why is this happening? [Errno 2] No Such file or directory: '/home/Documents/unetarchi.json' line 23, in json_file = open (jsonPath, 'r')",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to install the Nvidia DeepStream SDK module on my Jetson Nano running JetPack 4.6.1 with Azure IOT Edge. Following this example, but it is based on DeepStream 5.1, and since I have JetPack 4.6.1 I have DeepStream SDK 6.0.1 https://github.com/Azure-Samples/NVIDIA-Deepstream-Azure-IoT-Edge-on-a-NVIDIA-Jetson-Nano I'm getting this error when deploying the DeepStream SDK 5.1 module from the marketplace. 2022-05-21 16:52:36.053 +00:00 [ERR] - Edge agent plan execution failed. System.AggregateException: One or more errors occurred. (Error calling start module NVIDIADeepStreamSDK: Could not start module NVIDIADeepStreamSDK caused by: Could not start module NVIDIADeepStreamSDK caused by: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: error adding seccomp filter rule for syscall clone3: permission denied: unknown) ---&gt; Microsoft.Azure.Devices.Edge.Agent.Edgelet.EdgeletCommunicationException- Message:Error calling start module NVIDIADeepStreamSDK: Could not start module NVIDIADeepStreamSDK caused by: Could not start module NVIDIADeepStreamSDK caused by: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: error adding seccomp filter rule for syscall clone3: permission denied: unknown, StatusCode:500, at: at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2021_12_07.ModuleManagementHttpClient.HandleException(Exception exception, String operation) in /mnt/vss/_work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2021_12_07/ModuleManagementHttpClient.cs:line 231 at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation) in /mnt/vss/_work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/versioning/ModuleManagementHttpClientVersioned.cs:line 155 at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2021_12_07.ModuleManagementHttpClient.StartModuleAsync(String name) in /mnt/vss/_work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2021_12_07/ModuleManagementHttpClient.cs:line 179 at Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient.&lt;&gt;c__DisplayClass26_0.&lt;&lt;Throttle&gt;b__0&gt;d.MoveNext() in /mnt/vss/_work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/ModuleManagementHttpClient.cs:line 140 --- End of stack trace from previous location where exception was thrown --- Which DeepStream SDK container should I use instead on my Jetson Nano?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am on the new end of learning remote connections and I ran into a rather strange issue when connecting remotely to a machine. Host: Jetson Nano - Ubuntu Client: Asus desktop - Linux Mint I am using SSH to connect to the host machine. Once I'm in, I run my program which should open the camera that the host machine has connected via mipi connection... but it does not show a display window. Rather it displays: Gtk-WARNING **: cannot open display: lcocalhost:10.0 CONSUMER: Done Success (Argus)Error InvalidState: Argus client is exiting with 2 outstanding client threads If run the program in the machine without SSH connection, it works and the display shows what the camera is capturing. I tried changing the X11forwarding and agent to YES, and I tried export DISPLAY=localhost:10.0. That did not work as well. Any help would be appreciated. Thanks, GM",
        "answers": [
            [
                "Bear in mind that many GPU-related stuff won't work without a working display. Sadly, X11 forwarding doesn't work in those cases. At this point, it is not clear if this is your case, or if it is simply that you have the wrong DISPLAY number. You may try: Connecting a physical monitor and keyboard to the board, opening a terminal and running echo $DISPLAY. (in the keyboard/monitor session, not the SSH session). Then set that on your remote session as export DISPLAY=:X (where X is what was printed before). If you are using GStreamer then use nvoverlaysink which doesn't require X. You will need a monitor connected to the board though."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I just got a Jetson Nano and created my SD-Card with Jetpack 4.6.1. After that I installed TensorFlow like this: [Tensorflow-Install][1] Than I wanted to create an mnist Model but it seems like I cant import Keras? Any Idea ? I just install Tensorflow and upgraded all apt-get packages. &gt;&gt;&gt; import tensorflow.keras Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/usr/local/lib/python3.6/dist-packages/keras/api/_v2/keras/__init__.py\", line 12, in &lt;module&gt; from keras import __version__ File \"/usr/local/lib/python3.6/dist-packages/keras/__init__.py\", line 24, in &lt;module&gt; from keras import models File \"/usr/local/lib/python3.6/dist-packages/keras/models/__init__.py\", line 18, in &lt;module&gt; from keras.engine.functional import Functional File \"/usr/local/lib/python3.6/dist-packages/keras/engine/functional.py\", line 24, in &lt;module&gt; from keras.dtensor import layout_map as layout_map_lib File \"/usr/local/lib/python3.6/dist-packages/keras/dtensor/__init__.py\", line 22, in &lt;module&gt; from tensorflow.compat.v2.experimental import dtensor as dtensor_api # pylint: disable=g-import-not-at-top ImportError: cannot import name 'dtensor' &gt;&gt;&gt; I would appreciate any help! [1]: https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html",
        "answers": [
            [
                "try sudo pip3 uninstall keras, apparently version 2.9.0rc2 has errors, then pip3 install keras==2.7.0rc2, check compatibility, https://libraries.io/pypi/keras/2.9.0rc2, I don't know if this is the definitive solution for jetpack 4.6.1"
            ],
            [
                "Before you must delete tensorflow and keras folder at site-package. Open admin command prompt use this codes pip uninstall tensorflow pip install tensorflow --ignore-installed"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "How can I make a simultaneous CPU and GPU stress test on Jetson Xavier machine (Ubuntu 18.04, Jetpack 4.6)? The only code found is https://github.com/JTHibbard/Xavier_AGX_Stress_Test with tough package incompatibility issues. It only works for CPU. Anyone can contribute with providing another code or solve the issue with the mentioned one? A python code is preferred.",
        "answers": [
            [
                "Solution found. For CPU stress test, the above link works. It needs numba package to be installed. For GPU stress test, the samples in cuda folder of the Nvidia Jetson machines can be simply and efficiently used. The samples are in the /usr/local/cuda/samples. Choose one and compile it using sudo make. The compiled test file will be accessible in /usr/local/cuda/samples/bin/aarch64/linux/release (aarch64 may differ in different architectures). Run the test and check the performances using sudo jtop in another command line."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I downloaded jocover\u2019s git hub ffmpeg and compiled it to stream the video to the web on the Jetsonano board However, i tried ffmpeg -c:vh264_nvmpi -i /dev/video -f null - Cannot find a proper format for codec \u2018h264\u2019 (id 28), pixel format \u2018none\u2019 (id-1) Assertion *codec_id != AV_CODEC_ID_NONE failed at src/libavdevice/v4l2.c:811 I got this error and I tried ./ffmpeg -encoders |grep 264 As a result of this console, the output is as follows: v\u2026 h264_nvmpi nvmpi H.264 encoder wrapper (codec h264) v\u2026 h264_v4l2m2m V4L2 mem2mem H.264 encoder wrapper (codec h264) v\u2026 h264_vaapi H.264/AVC (VAAPI) (codec h264) The camera specs are like this maybe Sony IMX219, a model called Raspberry Pi Camera Module V2 ioctl: VIDIOC_ENUM_FMT Index : 0 Type : Video Capture Pixel Format: \u2018RG10\u2019 Name : 10-bit Bayer RGRG/GBGB I\u2019d really appreciate your help thx",
        "answers": [],
        "votes": []
    },
    {
        "question": "My goal is to have (2) RTMP sources in a Picture in Picture composition, encoding it into h265 mpegts, muxing audio from only the cam1 rtmp source, then sending it to the appsink; This is how I see it in my mind, but I'm probably wrong: [Confirmed] Working (On Device) Picture in Picture Pipeline: Devices used: Camlink 4k (Sony Action Cam FDR-x3000) and Logitech c920 v4l2src device=/dev/video0 ! nvvidconv ! queue ! comp.sink_0 v4l2src device=/dev/video1 ! video/x-raw, width=800, height=448, framerate=30/1, format=YUY2 ! videobox left=-4 right=-4 top=-4 bottom=-4 ! nvvidconv ! queue ! comp.sink_1 nvcompositor name=comp sink_0::width=1920 sink_0::height=1080 sink_1::width=640 sink_1::height=360 sink_1::xpos=1266 sink_1::ypos=706 ! queue ! identity name=v_delay signal-handoffs=TRUE ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mpegtsmux name=mux ! appsink name=appsink alsasrc device=hw:2 ! identity name=a_delay signal-handoffs=TRUE ! volume volume=1.0 ! audioconvert ! opusenc bitrate=320000 ! opusparse ! queue ! mux. [Confirmed] Working RTMP Pipeline: Device used: Samsung s10e using Larix Broadcaster to stream x264 via RTMP rtmpsrc location=rtmp://127.0.0.1/live/cam1 ! flvdemux name=demux demux.video ! identity name=v_delay signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! nvvidconv ! textoverlay text='' valignment=top halignment=right font-desc=\"Sans, 10\" name=overlay ! queue ! videorate ! video/x-raw,framerate=60/1 ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mux. demux.audio ! aacparse ! avdec_aac ! identity name=a_delay signal-handoffs=TRUE ! volume volume=1.0 ! audioconvert ! opusenc bitrate=128000 ! opusparse ! queue max-size-time=10000000000 max-size-buffers=1000 ! mux. mpegtsmux name=mux ! appsink name=appsink All my attempts have failed; These are My Attempts: Attempt 1: rtmpsrc name=cam1 location=rtmp://127.0.0.1/live/cam1 ! flvdemux name=demux0 ! queue ! demux0.video ! identity name=v_delay signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! nvvidconv ! queue ! comp.sink_0 rtmpsrc name=cam2 location=rtmp://127.0.0.1/live/cam2 ! flvdemux name=demux1 ! queue ! demux1.video ! identity signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! nvvidconv ! queue ! comp.sink_1 nvcompositor name=comp sink_0::xpos=0 sink_0::ypos=0 sink_0::width=1920 sink_0::height=1080 sink_1::xpos=0 sink_1::ypos=240 sink_1::width=320 sink_1::height=240 ! videorate ! video/x-raw,framerate=60/1 ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mux. demux0. ! queue ! audio/mpeg ! decodebin ! audioconvert ! audioresample ! autoaudiosink mpegtsmux name=mux ! appsink name=appsink Attempt 2: rtmpsrc name=cam1 location=rtmp://127.0.0.1/live/cam1 ! flvdemux name=demux0 demux0.video ! identity name=v_delay0 signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! nvvidconv ! queue ! comp.sink_0 rtmpsrc name=cam2 location=rtmp://127.0.0.1/live/cam2 ! flvdemux name=demux1 demux1.video ! identity name=v_delay1 signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! videobox left=-4 right=-4 top=-4 bottom=-4 ! nvvidconv ! queue ! comp.sink_1 nvcompositor name=comp sink_0::width=1920 sink_0::height=1080 sink_1::width=640 sink_1::height=360 sink_1::xpos=10 sink_1::ypos=10 ! queue ! identity name=v_delay0 signal-handoffs=TRUE ! nvvidconv interpolation-method=5 ! queue ! identity name=v_delay1 signal-handoffs=TRUE ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mpegtsmux name=mux ! appsink name=appsink demux0.audio ! aacparse ! avdec_aac ! identity name=a_delay signal-handoffs=TRUE ! volume volume=1.0 ! audioconvert ! opusenc bitrate=320000 ! opusparse ! queue max-size-time=10000000000 max-size-buffers=1000 ! mux. Current GStreamer Configuration: Update 1: I tried @SeB's Solution but it did not work: Here are some screenshots showing the process: videotestsrc on port 4953: videotestsrc on port 4954: full test pipeline: Update 2: The Solution: By Utilizing @SeB's answer and tinkering with it a bit, I was able to take two rtmpsrc's and compose them together, then send it to that same rtmp server under a different key, and use the rtmp pipeline that ships with the belacoder. During my testing this only works if you follow the belabox tutorial, and not with the pre-made image. Here is the pipeline that I used: gst-launch-1.0 -v \\ rtmpsrc location=rtmp://127.0.0.1/live/cam1 ! flvdemux name=demux0 \\ demux0. ! queue ! video/x-h264 ! h264parse ! nvv4l2decoder ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,width=1920,height=1080,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_0 \\ demux0. ! queue ! audio/mpeg ! mux. \\ rtmpsrc location=rtmp://127.0.0.1/live/cam2 ! flvdemux name=demux1 \\ demux1. ! queue ! video/x-h264 ! h264parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,format=YUY2,width=800,height=448,pixel-aspect-ratio=1/1 ! videobox left=-4 right=-4 top=-4 bottom=-4 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_1 \\ nvcompositor name=comp sink_0::xpos=0 sink_0::ypos=0 sink_0::width=1920 sink_0::height=1080 sink_0::zorder=1 sink_1::xpos=0 sink_1::ypos=0 sink_1::width=808,sink_1::height=456 sink_1::zorder=2 ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! nvvidconv ! 'video/x-raw(memory:NVMM),format=NV12' \\ ! nvv4l2h264enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h264parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mux. \\ flvmux name=mux ! rtmpsink location='location=rtmp://127.0.0.1/live/cam3 live=1' Then I just edited the rtmp pipeline that comes with belacoder to pull from /cam3. Here it is working in OBS Studio using belaUI + belacoder via SRTLA: This is the pipeline I used in belaUI/belacoder: rtmpsrc location=rtmp://127.0.0.1/live/cam3 ! flvdemux name=demux demux.video ! identity name=v_delay signal-handoffs=TRUE ! h264parse ! nvv4l2decoder ! nvvidconv ! textoverlay text='' valignment=top halignment=right font-desc=\"Sans, 10\" name=overlay ! queue ! nvvidconv interpolation-method=5 ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! mux. demux.audio ! aacparse ! avdec_aac ! identity name=a_delay signal-handoffs=TRUE ! volume volume=1.0 ! audioconvert ! voaacenc bitrate=128000 ! aacparse ! queue max-size-time=10000000000 max-size-buffers=1000 ! mux. mpegtsmux name=mux ! appsink name=appsink My settings are unique to the rtmp server I have running on my belabox (Jetson-nano) so keep that in mind. Here is the final pipeline selected in the belaUI: Once you have it selected all you have to do is hit start and you can utilize all of the internet connections that are connected to the belabox: Please keep in mind this is really finicky, if one of your rtmps sources crap out it ruins the whole pipeline, so this works best when all rtmp sources are in a local environment, and you have the gts-launch pipeline running as a service. If you want more information about the open-source DIY project belabox or would like to contact me, check out my profile links @ https://stackoverflow.com/users/3331416/b3ck",
        "answers": [
            [
                "Just tried simulating your sources with (I don't have a RTMP server, but should be straight forward to try adapting): # Cam 1 1920x1080@30fps with audio gst-launch-1.0 -e videotestsrc ! video/x-raw,format=NV12,width=320,height=240,framerate=30/1 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=NV12,width=1920,height=1080,pixel-aspect-ratio=1/1' ! nvv4l2h264enc ! h264parse ! queue ! flvmux name=mux audiotestsrc ! audioconvert ! voaacenc ! queue ! mux. mux. ! tcpserversink port=4953 # Cam2 with 800x448@30fps gst-launch-1.0 -e videotestsrc pattern=ball ! video/x-raw,format=NV12,width=320,height=240,framerate=30/1 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=NV12,width=800,height=448,pixel-aspect-ratio=1/1' ! nvv4l2h264enc ! h264parse ! queue ! flvmux ! tcpserversink port=4954 Then, this should output video and audio: gst-launch-1.0 -v \\ tcpclientsrc port=4953 ! flvdemux name=demux0 ! h264parse ! nvv4l2decoder ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,width=1920,height=1080,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_0 \\ tcpclientsrc port=4954 ! flvdemux name=demux1 ! h264parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,format=YUY2,width=800,height=448,pixel-aspect-ratio=1/1 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_1 \\ nvcompositor name=comp sink_0::xpos=0 sink_0::ypos=0 sink_0::width=1920 sink_0::height=1080 sink_0::zorder=1 sink_1::xpos=0 sink_1::ypos=0 sink_1::width=800,sink_1::height=448 sink_1::zorder=2 ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! nvvidconv ! autovideosink \\ demux0. ! queue ! audio/mpeg ! decodebin ! audioconvert ! audioresample ! autoaudiosink If ok, you can H265 encode composed video (note that here adding videobox the second frame will now have size 808x456) and forward mpeg audio with: gst-launch-1.0 -v \\ tcpclientsrc port=4953 ! flvdemux name=demux0 \\ demux0. ! queue ! video/x-h264 ! h264parse ! nvv4l2decoder ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,width=1920,height=1080,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_0 \\ demux0. ! queue ! audio/mpeg ! tsmux. \\ tcpclientsrc port=4954 ! flvdemux name=demux1 \\ demux1. ! queue ! video/x-h264 ! h264parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,format=YUY2,width=800,height=448,pixel-aspect-ratio=1/1 ! videobox left=-4 right=-4 top=-4 bottom=-4 ! nvvidconv ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! identity ! queue ! comp.sink_1 \\ nvcompositor name=comp sink_0::xpos=0 sink_0::ypos=0 sink_0::width=1920 sink_0::height=1080 sink_0::zorder=1 sink_1::xpos=0 sink_1::ypos=0 sink_1::width=808,sink_1::height=456 sink_1::zorder=2 ! 'video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1' ! nvvidconv ! 'video/x-raw(memory:NVMM),format=NV12' \\ ! nvv4l2h265enc control-rate=1 qp-range=\"28,50:0,38:0,50\" iframeinterval=60 preset-level=4 maxperf-enable=true EnableTwopassCBR=true insert-sps-pps=true name=venc_bps ! h265parse config-interval=-1 ! queue max-size-time=10000000000 max-size-buffers=1000 max-size-bytes=41943040 ! tsmux. \\ mpegtsmux name=tsmux ! appsink name=appsink"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to capture video from a source using gstreamer pipeline and opencv. Source has properties as follows: ioctl:VIDIOC_ENUM_FMT index : 0 Type : Video capture Pixel Format : 'YUYV' Name : YUYV 4:2:2 My receiving pipeline is : ''' pipeAnalog = 'v4l2src device=/dev/video0 ! video/x-raw,format=YUY2,interlace-mode=interleaved ! videoconvert ! video/x-raw, format=(string)BGR ! videoconvert ! appsink'''' using this pipeline, im reading the image with: '''self.currentCam = cv2.VideoCapture(self.pipeAnalog, cv2.CAP_GSTREAMER)''' After these commands, regardless of how i resize the frame using cv2.resize my frame has a black bar above it as follows : Output frame Any idea how can i get rid of this black bar? Any help would be appreciated. (I'm working on Nvidia Jetson Nano with Gstreamer Core Library version 1.14.5 and OpenCV version 4.3.0) Thanks,",
        "answers": [],
        "votes": []
    },
    {
        "question": "trying to stream from my Jetson nano with picamera 2 to youtube with gstreamer. Streaming only video works, but i need to overlay video with image using multifilesrc(image will change over time). After many hours a was not sucesfull to incorporate multifilesrc into pipeline. I have tried compositor, videomixer but all failed. Maybe using nvcompositor? Any ideas? This is what i have so far gst-launch-1.0 nvarguscamerasrc sensor-id=0 ! \\ \"video/x-raw(memory:NVMM), width=(int)1920, height=(int)1080, format=(string)NV12, framerate=(fraction)30/1\" ! omxh264enc ! \\ 'video/x-h264, stream-format=(string)byte-stream' ! \\ h264parse ! queue ! flvmux name=muxer alsasrc device=hw:1 ! \\ audioresample ! \"audio/x-raw,rate=48000\" ! queue ! \\ voaacenc bitrate=32000 ! aacparse ! queue ! muxer. muxer. ! \\ rtmpsink location=\"rtmp://a.rtmp.youtube.com/live2/x/xxx app=live2\" EDIT: tried this but not working gst-launch-1.0 \\ nvcompositor name=mix sink_0::zorder=1 sink_1::alpha=1.0 sink_1::zorder=2 ! nvvidconv ! omxh264enc ! \\ 'video/x-h264, stream-format=(string)byte-stream' ! \\ h264parse ! queue ! flvmux name=muxer alsasrc device=hw:1 ! \\ audioresample ! \"audio/x-raw,rate=48000\" ! queue ! \\ voaacenc bitrate=32000 ! aacparse ! queue ! muxer. muxer. ! \\ rtmpsink location=\"rtmp://a.rtmp.youtube.com/live2/x/xxx app=live2\" \\ nvarguscamerasrc sensor-id=0 ! \\ \"video/x-raw(memory:NVMM), width=(int)1920, height=(int)1080, format=(string)NV12, framerate=(fraction)30/1\" ! \\ nvvidconv ! video/x-raw, format=RGBA, width=1920, height=1080, framerate=30/1 ! autovideoconvert ! queue ! mix.sink_0 \\ filesrc location=logo.png ! pngdec ! alphacolor ! video/x-raw,format=RGBA ! imagefreeze ! nvvidconv ! mix.sink_1",
        "answers": [
            [
                "Although it may work in some cases without these, for using nvcompositor, I'd advise to use RGBA format in NVMM memory with pixel-aspect-ratio=1/1 for both inputs and for output. Use caps after nvvidconv for being sure in inputs pipelines, and use nvvidconv for converting nvcompositor output into NV12 (still in NVMM memory) before encoding. You may also add a queue on 2nd input for logo before compositor. Probably not mandatory, but safer. You may also set a framerate in caps after imagefreeze. Last, you may have to set xpos,ypos,width and height for all sources for a more reliable behavior."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I decode h.264 on Jetson Nano using Open-cv. I use this Code: import cv2 try: cap = cv2.VideoCapture('udp://234.0.0.0:46002', cv2.CAP_FFMPEG) print(f\"cap = {cap}\") except Exception as e: print(f\"Error: {e}\") if not cap.isOpened(): print('VideoCapture not opened') exit(-1) while True: ret, frame = cap.read() # print(f\"frame = {frame}\") try: cv2.imshow('Image', frame) except Exception as e: print(e) if cv2.waitKey(1) &amp; 0XFF == ord('q'): break cap.release() cv2.destroyAllWindows() everything works fine. now I won't try to optimize my code by decoding using GPU my question is how can I do this? I see this option: cap = cv2.VideoCapture('filesrc location=sample2.mp4 ! qtdemux ! queue ! h264parse ! omxh264dec ! nvvidconv ! video/x-raw,format=BGRx ! queue ! videoconvert ! queue ! video/x-raw, format=BGR ! appsink', cv2.CAP_GSTREAMER) but my source is URL. I would be happy to any help how to decode h.264 from URL in python using GPU.",
        "answers": [
            [
                "I use the FFmpeg command on my computer to get information about the video and I get this plot: ffmpeg -hide_banner -loglevel debug -i udp://127.0.0.0:46002 -f xv display Splitting the commandline. Reading option '-hide_banner' ... matched as option 'hide_banner' (do not show program banner) with argument '1'. Reading option '-loglevel' ... matched as option 'loglevel' (set logging level) with argument 'debug'. Reading option '-i' ... matched as input url with argument 'udp://127.0.0.0:46002'. Reading option '-f' ... matched as option 'f' (force format) with argument 'xv'. Reading option 'display' ... matched as output url. Finished splitting the commandline. Parsing a group of options: global . Applying option hide_banner (do not show program banner) with argument 1. Applying option loglevel (set logging level) with argument debug. Successfully parsed a group of options. Parsing a group of options: input url udp://127.0.0.0:46002. Successfully parsed a group of options. Opening an input file: udp://127.0.0.0:46002. [NULL @ 0000020a7c5ded80] Opening 'udp://127.0.0.0:46002' for reading [udp @ 0000020a7c5cb700] No default whitelist set [udp @ 0000020a7c5cb700] end receive buffer size reported is 393216 [h264 @ 0000020a7c5ded80] Format h264 probed with size=32768 and score=51 [h264 @ 0000020a7c5ded80] Before avformat_find_stream_info() pos: 0 bytes read:33339 seeks:0 nb_streams:1 [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [extract_extradata @ 0000020a7c60eec0] nal_unit_type: 1(Coded slice of a non-IDR picture), nal_ref_idc: 2 Last message repeated 1 times [h264 @ 0000020a7c631340] nal_unit_type: 1(Coded slice of a non-IDR picture), nal_ref_idc: 2 Last message repeated 1 times [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [h264 @ 0000020a7c631340] decode_slice_header error [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [h264 @ 0000020a7c631340] decode_slice_header error [h264 @ 0000020a7c631340] no frame! [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [extract_extradata @ 0000020a7c60eec0] nal_unit_type: 1(Coded slice of a non-IDR picture), nal_ref_idc: 2 Last message repeated 1 times [h264 @ 0000020a7c631340] nal_unit_type: 1(Coded slice of a non-IDR picture), nal_ref_idc: 2 Last message repeated 1 times [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [h264 @ 0000020a7c631340] decode_slice_header error [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [h264 @ 0000020a7c631340] decode_slice_header error [h264 @ 0000020a7c631340] no frame! [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [extract_extradata @ 0000020a7c60eec0] nal_unit_type: 1(Coded slice of a non-IDR picture), nal_ref_idc: 2 Last message repeated 1 times [h264 @ 0000020a7c631340] nal_unit_type: 1(Coded slice of a non-IDR picture), nal_ref_idc: 2 Last message repeated 1 times [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [h264 @ 0000020a7c631340] decode_slice_header error [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [h264 @ 0000020a7c631340] decode_slice_header error [h264 @ 0000020a7c631340] no frame! [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [extract_extradata @ 0000020a7c60eec0] nal_unit_type: 1(Coded slice of a non-IDR picture), nal_ref_idc: 2 Last message repeated 1 times [h264 @ 0000020a7c631340] nal_unit_type: 1(Coded slice of a non-IDR picture), nal_ref_idc: 2 Last message repeated 1 times [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [h264 @ 0000020a7c631340] decode_slice_header error [h264 @ 0000020a7c631340] non-existing PPS 0 referenced [h264 @ 0000020a7c631340] decode_slice_header error [h264 @ 0000020a7c631340] no frame! [extract_extradata @ 0000020a7c60eec0] nal_unit_type: 7(SPS), nal_ref_idc:3 [extract_extradata @ 0000020a7c60eec0] nal_unit_type: 8(PPS), nal_ref_idc:3 [extract_extradata @ 0000020a7c60eec0] nal_unit_type: 5(IDR), nal_ref_idc:3 Last message repeated 1 times [h264 @ 0000020a7c631340] nal_unit_type: 7(SPS), nal_ref_idc: 3 [h264 @ 0000020a7c631340] nal_unit_type: 8(PPS), nal_ref_idc: 3 [h264 @ 0000020a7c631340] nal_unit_type: 5(IDR), nal_ref_idc: 3 Last message repeated 1 times [h264 @ 0000020a7c631340] Format yuv420p chosen by get_format(). [h264 @ 0000020a7c631340] Reinit context to 720x576, pix_fmt: yuv420p [h264 @ 0000020a7c631340] nal_unit_type: 1(Coded slice of a non-IDR picture), nal_ref_idc: 2 Last message repeated 11 times [h264 @ 0000020a7c5ded80] max_analyze_duration 5000000 reached at 5000000 microseconds st:0 [h264 @ 0000020a7c5ded80] After avformat_find_stream_info() pos: 971047 bytes read:971495 seeks:0 frames:128 Input #0, h264, from 'udp://127.0.0.0:46002': Duration: N/A, bitrate: N/A Stream #0:0, 128, 1/1200000: Video: h264 (Constrained Baseline), 1 reference frame, yuv420p(progressive, left), 720x576, 0/1, 25 fps, 25 tbr, 1200k tbn, 50 tbc Successfully opened the file. Parsing a group of options: output url display. Applying option f (force format) with argument xv. Successfully parsed a group of options. Opening an output file: display. [NULL @ 0000020a7ce73000] Requested output format 'xv' is not a suitable output format display: Invalid argument [AVIOContext @ 0000020a7c610300] Statistics: 971495 bytes read, 0 seeks"
            ],
            [
                "You would use uridecodebin that can decode various types of urls, containers, protocols and codecs. With Jetson, the decoder selected by uridecodebin for h264 would be nvv4l2decoder, that doesn't use GPU but better dedicated HW decoder NVDEC. nvv4l2decoder outputs into NVMM memory in NV12 format, while opencv appsink expects BGR format in system memory. So you would use HW converter nvvidconv for converting and copying into system memory. Unfortunately, nvvidconv doesn't support BGR format, so first convert into supported BGRx format with nvvidconv, and finally use CPU plugin videoconvert for BGRx -&gt; BGR conversion such as: pipeline='uridecodebin uri=rtsp://127.0.0.1:8554/test ! nvvidconv ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1' cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER) This is for the general way. Though, for some streaming protocols it may not be so simple. For RTP-H264/UDP, ffmpeg backend may only work with a SDP file. For gstreamer backend you would instead use a pipeline such as: pipeline='udpsrc port=46002 multicast-group=234.0.0.0 ! application/x-rtp,encoding-name=H264 ! rtpjitterbuffer latency=500 ! rtph264depay ! h264parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1' As you can use FFMPEG, I'd speculate that received stream is using RTP-MP2T. So you would try: # Using NVDEC, but this may fail depending on sender side's codec: cap = cv2.VideoCapture('udpsrc multicast-group=234.0.0.0 port=46002 ! application/x-rtp,media=video,encoding-name=MP2T,clock-rate=90000,payload=33 ! rtpjitterbuffer latency=300 ! rtpmp2tdepay ! tsdemux ! h264parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1', cv2.CAP_GSTREAMER) # Or using CPU (may not support high pixel rate with Nano): cap = cv2.VideoCapture('udpsrc multicast-group=234.0.0.0 port=46002 ! application/x-rtp,media=video,encoding-name=MP2T,clock-rate=90000,payload=33 ! rtpjitterbuffer latency=300 ! rtpmp2tdepay ! tsdemux ! h264parse ! avdec_h264 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1', cv2.CAP_GSTREAMER) [Note that I'm not familiar with 234.0.0.0, so unsure if multicast-group should be used as I did]. If this doesn't work, you may try to get more information about received stream. You may try working ffmpeg such as: ffmpeg -hide_banner -loglevel debug -i udp://234.0.0.0:46002 -f xv display If you see: Stream #0:0, 133, 1/1200000: Video: h264 (Constrained Baseline), 1 reference frame, yuv420p(progressive, left), 720x576, 0/1, 25 fps, 25 tbr, 1200k tbn, 50 tbc you may have to change clock-rate to 1200000 (default value is 90000): application/x-rtp,media=video,encoding-name=MP2T,clock-rate=1200000 This is assuming the stream is mpeg2 ts. In this case, first lines show: ... Opening an input file: udp://127.0.0.1:5002. [NULL @ 0x55761c4690] Opening 'udp://127.0.0.1:5002' for reading [udp @ 0x55761a27c0] No default whitelist set [udp @ 0x55761a27c0] end receive buffer size reported is 131072 [mpegts @ 0x55761c4690] Format mpegts probed with size=2048 and score=47 [mpegts @ 0x55761c4690] stream=0 stream_type=1b pid=41 prog_reg_desc=HDMV [mpegts @ 0x55761c4690] Before avformat_find_stream_info() pos: 0 bytes read:26560 seeks:0 nb_streams:1 ... ffmpeg tries to guess and here found the stream was in mpegts format. You would check in your case what ffmpeg finds. Note that first guess may not be correct, you would have to check the whole log and see what it finds working. Another speculation would be that your stream is not RTP, but rather raw h264 stream. In such case you may be able to decode with something like: gst-launch-1.0 udpsrc port=46002 multicast-group=234.0.0.0 ! h264parse ! nvv4l2decoder ! autovideosink If this works, for opencv you would use: pipeline='udpsrc port=46002 multicast-group=234.0.0.0 ! h264parse ! nvv4l2decoder ! nvvidconv ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1'"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I have a jetson nano that runs 64 Ubuntu. I need to run 32bit app of python3.5. One solution is to install 32bit python version sudo dpkg --add-architecture i386 &amp;&amp; sudo apt-get install python3.5.1 but it doesnot work. Ofcourse I have already installed the libraries for i386. Another idea was to run i386/ubuntu image but a message appears about architecture incompatibility when running. sudo docker run -it i386/ubuntu``` Finally is it possible to run 32bit docker ubuntu image on Ubuntu 64bit on jetson?",
        "answers": [
            [
                "The Jetson Nano's CPU instruction set is armv8 (or arm64 or arch64), while i386 means x86. these are incompatible instruction sets. So there is no way to run i386 applications natively on the Jetson Nano. Maybe a virtual machine can help."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have 8 sensors and I need your opinion on which is better: interface all sensors directly to nvidia Jetson Nano or interface them to another microcontroller board like arduino or esp32 then send their data with serial port to th Jetson Nano for data analysis and action decision ? and why ? Thanks you in advance !",
        "answers": [
            [
                "On the Jetson, you have the GPIO pins, the I2C buses and you can use an Arduino as a sub-processor. The GPIO is convenient but requires the Jetson to do all the work. The I2C bus is common and many sensors are available and can be daisy chained. The sensor needs an I2C interface. The Arduino just needs a USB port and can offload processing. They also come in many different flavors/sizes with plenty of interface choices for different sensors. Would need to know more about your sensors to make the call."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to open 3 cameras at the same time on a Jetson Nano 2GB with python 3.6.9 and opencv 4.5.4 import cv2 Camera1 = cv2.VideoCapture(0) Camera2 = cv2.VideoCapture(1) Camera3 = cv2.VideoCapture(2) Largeur = 640 Hauteur = 480 Camera1.set(3, Largeur) Camera1.set(4, Hauteur) Camera2.set(3, Largeur) Camera2.set(4, Hauteur) Camera3.set(3, Largeur) Camera3.set(4, Hauteur) while True: ret1, IMG1 = Camera1.read() ret2, IMG2 = Camera2.read() ret3, IMG3 = Camera3.read() cv2.imshow(\"Webcam1\", IMG1) cv2.imshow(\"Webcam2\", IMG2) cv2.imshow(\"Webcam3\", IMG3) if cv2.waitKey(1) % 0xFF == ord(\"q\"): break Camera1.release() Camera2.release() Camera3.release() cv2.destroyAllWindows() But, if I try to open the 3 at the same time i get this error: GStreamer warning: Embedded video playback halted; module v4l2src2 reported: Failed to allocate required memory cv2.imshow(\"Webcam3\", IMG3) cv2.error: OpenCV(4.5.4) error: (-206:Bad flag (parameter or structure field)) Unrecognized or unsupported array type in function 'cvGetMat' I'm able to use all 3 cameras by group of 2 at the time but not the 3. I think it's because of the low 2GB of ram, but i want to have someone else opinion before doing anything. P.S. all the cameras are Logitech C925e",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to connect to a camera, and only capture a frame when an event happens (e.g. keypress). A simplified version of what I'd like to do is this: cap = cv2.VideoCapture(device_id) while True: if event: img = cap.read() preprocess(img) process(img) cv.Waitkey(10) However, cap.read seems to only capture the next frame in the queue, and not the latest. I did a lot of searching online, and there seems to be a lot of questions on this but no definitive answer. Only some dirty hacks which involve opening and closing the capture device just before and after grabbing (which won't work for me as my event might be triggered multiple times per second); or assuming a fixed framerate and reading a fixed-n times on each event (which won't work for me as my event is unpredictable and could happen at any interval). A nice solution would be: while True: if event: while capture_has_frames: img = cap.read() preprocess(img) process(img) cv.Waitkey(10) But what is capture_has_frames? Is it possible to get that info? I tried looking into CV_CAP_PROP_POS_FRAMES but it's always -1. For now I have a separate thread where the capture is running at full fps, and on my event I'm grabbing the latest image from that thread, but this seems overkill. (I'm on Ubuntu 16.04 btw, but I guess it shouldn't matter. I'm also using pyqtgraph for display)",
        "answers": [
            [
                "I think the solution mentioned in the question, namely having a separate thread that clears the buffer, is the easiest non-brittle solution for this. Here reasonably nice (I think) code for this: import cv2, queue, threading, time # bufferless VideoCapture class VideoCapture: def __init__(self, name): self.cap = cv2.VideoCapture(name) self.q = queue.Queue() t = threading.Thread(target=self._reader) t.daemon = True t.start() # read frames as soon as they are available, keeping only most recent one def _reader(self): while True: ret, frame = self.cap.read() if not ret: break if not self.q.empty(): try: self.q.get_nowait() # discard previous (unprocessed) frame except queue.Empty: pass self.q.put(frame) def read(self): return self.q.get() cap = VideoCapture(0) while True: time.sleep(.5) # simulate time between events frame = cap.read() cv2.imshow(\"frame\", frame) if chr(cv2.waitKey(1)&amp;255) == 'q': break The frame reader thread is encapsulated inside the custom VideoCapture class, and communication with the main thread is via a queue. I posted very similar code for a node.js question, where a JavaScript solution would have been better. My comments on another answer to that question give details why a non-brittle solution without separate thread seems difficult. An alternative solution that is easier but supported only for some OpenCV backends is using CAP_PROP_BUFFERSIZE. The 2.4 docs state it is \"only supported by DC1394 [Firewire] v 2.x backend currently.\" For Linux backend V4L, according to a comment in the 3.4.5 code, support was added on 9 Mar 2018, but I got VIDEOIO ERROR: V4L: Property &lt;unknown property string&gt;(38) not supported by device for exactly this backend. It may be worth a try first; the code is as easy as this: cap.set(cv2.CAP_PROP_BUFFERSIZE, 0)"
            ],
            [
                "Here's a simplified version of Ulrich's solution. OpenCV's read() function combines grab() and retrieve() in one call, where grab() just loads the next frame in memory, and retrieve decodes the latest grabbed frame (demosaicing &amp; motion jpeg decompression). We're only interested in decoding the frame we're actually reading, so this solution saves some CPU, and removes the need for a queue import cv2 import threading # bufferless VideoCapture class VideoCapture: def __init__(self, name): self.cap = cv2.VideoCapture(name) self.lock = threading.Lock() self.t = threading.Thread(target=self._reader) self.t.daemon = True self.t.start() # grab frames as soon as they are available def _reader(self): while True: with self.lock: ret = self.cap.grab() if not ret: break # retrieve latest frame def read(self): with self.lock: _, frame = self.cap.retrieve() return frame EDIT: Following Arthur Tacca's comment, added a lock to avoid simultaneous grab &amp; retrieve, which could lead to a crash as OpenCV isn't thread-safe."
            ],
            [
                "Its also possible to always get the latest frame by using cv2.CAP_GSTREAMER backend. If you have gstreamer support enabled in cv2.getBuildInformation(), you can initialize your video capture with the appsink parameters sync=false and drop=true Example: cv2.VideoCapture(\"rtspsrc location=rtsp://... ! decodebin ! videoconvert ! video/x-raw,framerate=30/1 ! appsink drop=true sync=false\", cv2.CAP_GSTREAMER)"
            ],
            [
                "On my Raspberry Pi 4, cap = cv2.VideoCapture(0) cap.set(cv2.CAP_PROP_BUFFERSIZE, 1) does work and was all that I needed for my pi camera to give me the latest frame, with a consistent 3+ second delay between the scene in front of the camera and displaying that scene in the preview image. My code takes 1.3 seconds to process an image, so I'm not sure why the other 2 seconds of delay are present, but it's consistent and works. Side note: since my code takes over a second to process an image, I also added cap.set( cv2.CAP_PROP_FPS, 2 ) in case it reduces any unneeded activity, since I can't quite get a frame a second. When I put cv2.CAP_PROP_FPS to 1, though, I got a strange output of all my frames being almost entirely dark, so setting FPS too low can cause an issue"
            ],
            [
                "If you don't want to capture the frame when there is no event happening, why are you preprocessing/processing your frame? If you do not process your frame, you can simply discard it unless the event occur. Your program should be able to capture, evaluate your condition and discard at a sufficient speed, i.e. fast enough compared to your camera FPS capture rate, to always get the last frame in the queue. If not proficient in python because I do my OpenCV in C++, but it should look similar to this: vidcap = cv.VideoCapture( filename ) while True: success, frame = vidcap.read() If Not success: break If cv.waitKey(1): process(frame) As per OpenCV reference, vidcap.read() returns a bool. If frame is read correctly, it will be True. Then, the captured frame is store in variable frame. If there is no key press, the loop keeps on going. When a key is pressed, you process your last captured frame."
            ]
        ],
        "votes": [
            44.0000001,
            9.0000001,
            2.0000001,
            1e-07,
            -1.9999999
        ]
    },
    {
        "question": "I\u2019m writing a Gstreamer pipeline using PyGST and Gst.parse_launch that plays on an Nvidia Jetson nano. The pipeline plays 4k video and fades the videos in and out at runtime. My issue is the pipeline wont play the same video consecutively. It will play different videos one after another just fine. The fade functionality is also working well. Here is a description of the problem: The pipeline works the first time with an example video such as video_1.mp4. When I play the same video again it will get stuck on the first frame, stay on that frame and exit the process after a few seconds. If I play video_1.mp4 and then play a different video like video_2.mp4 then the pipeline will work with no issues. If after this I play back video_1.mp4 then it will also play all the way through. video_1.mp4 has a duration of 10 seconds. If I stop video_1.mp4 at 5 seconds and play the video again, the first frame of video_1.mp4 will display, it will wait at that frame for 5 seconds and then continue decoding the video at the same point where the previous pipeline left off. My suspicion is the buffer running time has not been reset back to 0 when the same video plays. Im unsure of why this happens, I set the pipeline to NULL and I have tried doing a seek position 0 and flushing the elements in the pipeline after the video finishes to reset the clock time back to 0. Perhaps the syntax or the way i\u2019m implementing it is incorrect. This thread below creates the pipeline import gi import time gi.require_version('Gst', '1.0') gi.require_version('GstPbutils', '1.0') gi.require_version('GstController', '1.0') from gi.repository import GObject, Gst, GstPbutils, GstController from player_thread import PlayerThread from threading import Timer import logging import time class VideoPlayer(): def __init__(self): Gst.init(None) GObject.threads_init() self.pipeline = Gst.parse_launch (\"\"\" nvcompositor name=comp sink_0::alpha=1 ! video/x-raw(memory:NVMM),format=RGBA ! nvvidconv ! video/x-raw(memory:NVMM), format=NV12 ! autovideosink name=sinky uridecodebin name=vidsrc ! nvvidconv name=pipeconv ! video/x-raw(memory:NVMM),format=RGBA,pixel-aspect-ratio=1/1 ! queue ! comp.sink_0 \"\"\") self.loop = GObject.MainLoop() self.bus = self.pipeline.get_bus() self.bus.add_signal_watch() self.bus.connect(\"message::eos\", self.bus_call, self.loop) self.bus.connect(\"message::error\", self.bus_call, self.loop) self.sink = self.pipeline.get_by_name('sinky') self.convert = self.pipeline.get_by_name('pipeconv') self.source = self.pipeline.get_by_name('vidsrc') self.compositor = self.pipeline.get_by_name('comp') self.source.connect(\"pad-added\", self.on_pad_added) self.running = False self.pipeline.set_state(Gst.State.NULL) self.loop.run() def get_alpha_controller(self, incoming_pad): self.pad = incoming_pad self.control_source = GstController.InterpolationControlSource() self.control_source.set_property('mode', GstController.InterpolationMode.LINEAR) self.control_bind = GstController.DirectControlBinding.new(self.pad, 'alpha', self.control_source) self.pad.add_control_binding(self.control_bind) return self.control_source def fade_video_in(self): self.compositor_sink_pad = self.compositor.get_static_pad('sink_0') self.control_source = self.get_alpha_controller(self.compositor_sink_pad) self.control_source.set(0*Gst.SECOND, 0) self.control_source.set(2*Gst.SECOND, 1) def fade_video_out(self): self.pos = self.pipeline.query_position(Gst.Format.TIME).cur self.control_source.set(self.pos, 1) self.control_source.set(self.pos + 1*Gst.SECOND, 0) def on_pad_added(self, src, new_pad): print( \"Received new pad '{0:s}' from '{1:s}'\".format( new_pad.get_name(), src.get_name())) new_pad_caps = new_pad.get_current_caps() new_pad_struct = new_pad_caps.get_structure(0) new_pad_type = new_pad_struct.get_name() if new_pad_type.startswith(\"video/x-raw\"): sink_pad = self.convert.get_static_pad(\"sink\") else: print( \"It has type '{0:s}' which is not raw audio/video. Ignoring.\".format(new_pad_type)) return # if our converter is already linked, we have nothing to do here # if(sink_pad.is_linked()): # print(\"We are already linked. Ignoring.\") # return # attempt the link ret = new_pad.link(sink_pad) if not ret == Gst.PadLinkReturn.OK: print(\"Type is '{0:s}}' but link failed\".format(new_pad_type)) else: print(\"Link succeeded (type '{0:s}')\".format(new_pad_type)) return def play_video(self, video_url, scene_name): self.compositor.props.background = 1 self.running = True self.scene_name = scene_name self.video_url = video_url print(self.video_url) self.source.props.uri = self.video_url self.sink.seek_simple(Gst.Format.TIME, Gst.SeekFlags.FLUSH, 0 * Gst.SECOND) self.pipeline.set_state(Gst.State.READY) self.pipeline.get_state(Gst.CLOCK_TIME_NONE) self.pipeline.set_state(Gst.State.PAUSED) self.pipeline.set_state(Gst.State.PLAYING) def stop_video(self): self.running = False self.pipeline.set_state(Gst.State.NULL) self.pipeline.set_state(Gst.State.READY) self.pipeline.seek_simple(Gst.Format.TIME, Gst.SeekFlags.FLUSH | Gst.SeekFlags.KEY_UNIT, 0.0 * Gst.SECOND ) self.source.props.uri = \"\" def bus_call(self, bus, message, loop): t = message.type if t == Gst.MessageType.EOS: self.stop_video() elif t == Gst.MessageType.ERROR: print(message.parse_error()) elif message.type == Gst.MessageType.SEGMENT_DONE: # self.fade_video_out() pass else: # should not get here print(\"ERROR: Unexpected message received\") return True def exit(self): self.loop.quit() These are the Gstreamer Debug Logs when the error occurs 0:05:51.707439047 1756 0x7f7800e540 WARN aggregator gstaggregator.c:1717:gst_aggregator_query_latency_unlocked:&lt;comp&gt; Latency query failed 0:05:51.709119977 1756 0x7f70007460 WARN basesrc gstbasesrc.c:3583:gst_base_src_start_complete:&lt;source&gt; pad not activated yet 0:05:51.709991980 1756 0x7f70007460 WARN basesrc gstbasesrc.c:3583:gst_base_src_start_complete:&lt;source&gt; pad not activated yet sending message {\"action\": \"STATE_CHANGE\", \"body\": {\"videoActiveVideo\": \"test\"}} 0:05:51.714088392 1756 0x106e9630 WARN qtdemux qtdemux_types.c:233:qtdemux_type_get: unknown QuickTime node type pasp 0:05:51.714195686 1756 0x106e9630 WARN qtdemux qtdemux.c:3031:qtdemux_parse_trex:&lt;qtdemux11&gt; failed to find fragment defaults for stream 1 Opening in BLOCKING MODE 0:05:51.759452369 1756 0x106fae80 WARN v4l2 gstv4l2object.c:4447:gst_v4l2_object_probe_caps:&lt;nvv4l2decoder11:src&gt; Failed to probe pixel aspect ratio with VIDIOC_CROPCAP: Unknown error -1 0:05:51.759527839 1756 0x106fae80 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f64022090 Failed to determine interlace mode 0:05:51.759581956 1756 0x106fae80 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f64022090 Failed to determine interlace mode 0:05:51.759638728 1756 0x106fae80 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f64022090 Failed to determine interlace mode NvMMLiteOpen : Block : BlockType = 261 NVMEDIA: Reading vendor.tegra.display-size : status: 6 NvMMLiteBlockCreate : Block : BlockType = 261 0:05:51.865431565 1756 0x106fae80 WARN v4l2 gstv4l2object.c:4447:gst_v4l2_object_probe_caps:&lt;nvv4l2decoder11:src&gt; Failed to probe pixel aspect ratio with VIDIOC_CROPCAP: Unknown error -1 0:05:51.865703395 1756 0x106fae80 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f64022090 Failed to determine interlace mode 0:05:51.865971579 1756 0x106fae80 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f64022090 Failed to determine interlace mode 0:05:51.866240232 1756 0x106fae80 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f64022090 Failed to determine interlace mode Received new pad 'src_16' from 'vidsrc' Link succeeded (type 'video/x-raw') 0:05:51.876923175 1756 0x106fae80 WARN v4l2videodec gstv4l2videodec.c:1673:gst_v4l2_video_dec_decide_allocation:&lt;nvv4l2decoder11&gt; Duration invalid, not setting latency 0:05:51.877534077 1756 0x106fae80 WARN v4l2bufferpool gstv4l2bufferpool.c:1065:gst_v4l2_buffer_pool_start:&lt;nvv4l2decoder11:pool:src&gt; Uncertain or not enough buffers, enabling copy threshold 0:05:51.883864038 1756 0x7f5c007850 WARN v4l2bufferpool gstv4l2bufferpool.c:1512:gst_v4l2_buffer_pool_dqbuf:&lt;nvv4l2decoder11:pool:src&gt; Driver should never set v4l2_buffer.field to ANY 0:05:51.891684769 1756 0x7f7800e540 FIXME basesink gstbasesink.c:3145:gst_base_sink_default_event:&lt;sinky-actual-sink-nvoverlay&gt; stream-start event without group-id. Consider implementing group-id handling in the upstream elements 0:05:51.893670863 1756 0x7f7800e540 WARN nvcompositor gstnvcompositor.c:980:gst_nvcompositor_negotiated_caps:&lt;comp&gt; Release old pool These are logs once you play a different video and the pipeline resets (pipeline works correctly) 0:06:21.907924995 1756 0x7f580cc320 WARN aggregator gstaggregator.c:1717:gst_aggregator_query_latency_unlocked:&lt;comp&gt; Latency query failed 0:06:21.909268156 1756 0x7f70007460 WARN basesrc gstbasesrc.c:3583:gst_base_src_start_complete:&lt;source&gt; pad not activated yet 0:06:21.910099324 1756 0x7f70007460 WARN basesrc gstbasesrc.c:3583:gst_base_src_start_complete:&lt;source&gt; pad not activated yet sending message {\"action\": \"STATE_CHANGE\", \"body\": {\"videoActiveVideo\": \"test1\"}} 0:06:21.913367433 1756 0x7f7800e720 WARN qtdemux qtdemux_types.c:233:qtdemux_type_get: unknown QuickTime node type gsst 0:06:21.914455326 1756 0x7f7800e720 WARN qtdemux qtdemux_types.c:233:qtdemux_type_get: unknown QuickTime node type gstd 0:06:21.914529130 1756 0x7f7800e720 WARN qtdemux qtdemux.c:3031:qtdemux_parse_trex:&lt;qtdemux12&gt; failed to find fragment defaults for stream 1 0:06:21.914677884 1756 0x7f7800e720 WARN qtdemux qtdemux.c:3031:qtdemux_parse_trex:&lt;qtdemux12&gt; failed to find fragment defaults for stream 2 Opening in BLOCKING MODE 0:06:21.964505314 1756 0x106ee320 WARN v4l2 gstv4l2object.c:4447:gst_v4l2_object_probe_caps:&lt;nvv4l2decoder12:src&gt; Failed to probe pixel aspect ratio with VIDIOC_CROPCAP: Unknown error -1 0:06:21.964574795 1756 0x106ee320 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f5802e8e0 Failed to determine interlace mode 0:06:21.964636619 1756 0x106ee320 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f5802e8e0 Failed to determine interlace mode 0:06:21.964687871 1756 0x106ee320 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f5802e8e0 Failed to determine interlace mode NvMMLiteOpen : Block : BlockType = 261 NVMEDIA: Reading vendor.tegra.display-size : status: 6 NvMMLiteBlockCreate : Block : BlockType = 261 0:06:22.070629669 1756 0x106ee320 WARN v4l2 gstv4l2object.c:4447:gst_v4l2_object_probe_caps:&lt;nvv4l2decoder12:src&gt; Failed to probe pixel aspect ratio with VIDIOC_CROPCAP: Unknown error -1 0:06:22.070809622 1756 0x106ee320 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f5802e8e0 Failed to determine interlace mode 0:06:22.071008898 1756 0x106ee320 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f5802e8e0 Failed to determine interlace mode 0:06:22.071295416 1756 0x106ee320 WARN v4l2 gstv4l2object.c:2388:gst_v4l2_object_add_interlace_mode:0x7f5802e8e0 Failed to determine interlace mode Received new pad 'src_17' from 'vidsrc' Link succeeded (type 'video/x-raw') Received new pad 'src_18' from 'vidsrc' It has type 'audio/x-raw' which is not raw audio/video. Ignoring. 0:06:22.080439932 1756 0x106ee320 WARN v4l2videodec gstv4l2videodec.c:1673:gst_v4l2_video_dec_decide_allocation:&lt;nvv4l2decoder12&gt; Duration invalid, not setting latency 0:06:22.080866610 1756 0x106ee320 WARN v4l2bufferpool gstv4l2bufferpool.c:1065:gst_v4l2_buffer_pool_start:&lt;nvv4l2decoder12:pool:src&gt; Uncertain or not enough buffers, enabling copy threshold 0:06:22.085282770 1756 0x7f5c0078f0 WARN v4l2bufferpool gstv4l2bufferpool.c:1512:gst_v4l2_buffer_pool_dqbuf:&lt;nvv4l2decoder12:pool:src&gt; Driver should never set v4l2_buffer.field to ANY 0:06:22.091041154 1756 0x7f580cc320 FIXME basesink gstbasesink.c:3145:gst_base_sink_default_event:&lt;sinky-actual-sink-nvoverlay&gt; stream-start event without group-id. Consider implementing group-id handling in the upstream elements 0:06:22.098388747 1756 0x7f580cc320 WARN nvcompositor gstnvcompositor.c:980:gst_nvcompositor_negotiated_caps:&lt;comp&gt; Release old pool 0:06:22.108516102 1756 0x7f580cc320 ERROR omx gstomx.c:256:gst_omx_component_handle_messages:&lt;sinky-actual-sink-nvoverlay&gt; yuv420 port 0 was not flushing",
        "answers": [
            [
                "Not sure for your case, but as autovideosink on Jetson may select nvoverlaysink, you may try resetting its properties."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm currently working on a remotely controlled robot that is sending two camera streams from a Jetson Nano to a PC/Android Phone/VR Headset. I've been able to create a stable link between the robot and PC using gst-rtsp-server running this pipeline: ./test-launch nvarguscamerasrc sensor-id=1 ! video/x-raw(memory:NVMM) width=1920 height=1080 framerate=30/1 format=NV12 ! nvvidconv flip-method=2 ! omxh264enc iframeinterval=15 ! h264parse ! rtph264pay name=pay0 pt=96 And receiving it on PC using: gst-launch-1.0 -v rtspsrc location=rtspt://192.168.1.239:8554/test ! application/x-rtp, payload=96 ! rtph264depay ! avdec_h264 ! videoconvert ! autovideosink sync=false On PC, there's an excellent latency of about ~120ms, so I thought there wouldn't be a problem running that same thing on Android. Using gstreamer's prebuild binaries from here and a modification from here to be able to use rtspsrc I've succesfully managed to receive the rtsp stream. But this time the video is \"slowed down\" (probably some buffer problems, or HW acceleration?) Worked my way around that by using latency=150 drop-on-latency=true parametrs of rtspsrc which only keeps those frames with lower latency but as expected the output encoded image is trash. So my question is: Why is there such a difference between a phone and a PC receiving the stream. It seems that the gst-rtsp-stream is defaulting to sending via tcp which i tried to configure with gst_rtsp_media_factory_set_protocols(factory, GST_RTSP_LOWER_TRANS_UDP_MCAST) but doing that I can no longer receive the stream even on a PC with the same pipeline. Is there a way to force gst-rtsp-server to send via udp. Or is there a way to optimize the phone encoding performance to run as quick a the PC does? (I have an Galaxy S10+, so I guess it should be able to handle that) My goal is a clear video on Android/VR Headset with minimal latency (preferably the same ~120ms as on PC)",
        "answers": [
            [
                "The rtsp server uses TCP because your client query asked for that using rtspt where last t queries for TCP transport. Just using rstp instead should use UDP. You may have a look to protocols property of rtspsrc for more details. Full story is in the comments here and continued to solution here: Gstreamer Android HW accelerated H.264 encoding"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I had an issue like this on my Nano: profiles = [ SERIAL_PORT_PROFILE ], File \"/usr/lib/python2.7/site-packages/bluetooth/bluez.py\", line 176, in advertise_service raise BluetoothError (str (e)) bluetooth.btcommon.BluetoothError: (2, 'No such file or directory') I tried adding compatibility mode in the bluetooth.service file, reloading daemon, restarting bluetooth and then adding a serial port by doing sudo sdptool add SP These steps work fine on my ubuntu 20.04 laptop, but on jetpack 4.5.1, they don\u2019t. And I checked also, they don\u2019t work on jetson NX either. I am really curious on how to solve this issue, otherwise, another way to use bluetooth inside a python code is welcomed. Thanks",
        "answers": [
            [
                "You might want to have a look at the following article which shows how to do the connection with core Python Socket library https://blog.kevindoran.co/bluetooth-programming-with-python-3/. The way BlueZ does this now is with the Profile API. There is a Python example of using the Profile API at https://git.kernel.org/pub/scm/bluetooth/bluez.git/tree/test/test-profile hciattach, hciconfig, hcitool, hcidump, rfcomm, sdptool, ciptool, and gatttool were deprecated by the BlueZ project in 2017. If you are following a tutorial that uses them, there is a chance that it might be out of date and that Linux systems will choose not to support them."
            ],
            [
                "The solution was in the path of the bluetooth configuration file (inspired from this https://developer.nvidia.com/embedded/learn/tutorials/connecting-bluetooth-audio) this answer : bluetooth.btcommon.BluetoothError: (2, 'No such file or directory') is not enough for jetson devices (jetpack). Although I didn't test if it works without changing the file mentioned in this link. There is a .conf file that needs to be changed also : /lib/systemd/system/bluetooth.service.d/nv-bluetooth-service.conf modify : ExecStart=/usr/lib/bluetooth/bluetoothd -d --noplugin=audio,a2dp,avrcp to : ExecStart=/usr/lib/bluetooth/bluetoothd -C after that it is necessary to do: sudo systemctl daemon-reload sudo systemctl restart bluetooth Tested on jetson Nano and NX with jetpach 4.5.1 Thanks for the help !"
            ]
        ],
        "votes": [
            1e-07,
            -0.9999999
        ]
    },
    {
        "question": "Summarize the Problem: I wrote a userspace SPI driver in linux for the NRF24L01+ transceiver. My goal is to send files to a server. A jetson nano is the sender, and a raspberry pi 3b+ the receiver. Both the spi and nano are running Linux. I can consistently send packets and receive acknowledgements. However, the issue is whenever I send a packet such as 0x ff ee dd 00 cc bb aa the receiver only receives the packet 0x ff ee dd 00 00 00 00. So what is happening is that whenever the first byte encountered is zero, the rest of the packet becomes zero. This causes the files I send to become corrupted. I was able to reproduce this bug with a char array having a similar pattern. I noticed this trend when I printed out the file contents I was sending on the transmitter and receiver. What I've tried: I've tried altering my SPI read function. What I thought was happening was the chip select line was being flipped high early. This did not work, I got the same results. I've printed the packets before calling the ioctl() function from the transmitter and the packet remains intact. I've printed the return value of the ioctl() function to see how many bytes I was receiving and sending. I was sending 31 bytes from the transmitter, and receiving 32 bytes from the receiver. So it doesn't look like my reads and sends are failing. If I had a logic analyzer my next step would be to check the SPI pins on the transmitter, but unfortunately I don't have one. I've added a 10uF decoupling capacitor on the transceivers and that sped up communication. Show Some Code: Receiver side: /** * Reads the payload when data pipe * is available. * * spi_dev_fd: file descriptor for spi device. * */ int nrf_rx_read(int spi_dev_fd, char * payload, int * pipe, int * bytes) { int pipe_temp, rtn; // TODO: Add timeout. do { rtn = nrf_rx_pipe_available(spi_dev_fd, &amp;pipe_temp); }while(rtn != 0); if(rtn == 0) { char status; if(bytes != NULL) { char size; spi_read_msg(spi_dev_fd, R_RX_PL_WID, &amp;status, &amp;size, 1); *bytes = (int) size; } spi_read_msg(spi_dev_fd, R_RX_PAYLOAD , &amp;status, payload, (int) NUM_PAYLOAD_BYTES); *pipe = pipe_temp; char msg; msg = RX_DR; spi_send_msg(spi_dev_fd, W_REGISTER | STATUS, &amp;msg, 1); return 0; } return 1; } bool nrf_rx_pipe_available(int spi_dev_fd, int * pipe) { char addr = NOP; char status; spi_read_msg(spi_dev_fd, addr, &amp;status, NULL, 0); if((status &amp; RX_DR) &gt; 0) { *pipe = (status &gt;&gt; RX_P_NO) &amp; 0x07; if(*pipe &gt; 5) { return 1; } return 0; } return 1; } int spi_read_msg(int spi_dev_fd, char addr, char * status, char * copy_to, int len) { char data_buffer; char recv_buffer[len + 1]; struct spi_ioc_transfer xfer; memset(&amp;xfer, 0, sizeof(xfer)); memset(&amp;recv_buffer, 0, sizeof(recv_buffer)); data_buffer = addr; xfer.tx_buf = (unsigned long) &amp;data_buffer; xfer.rx_buf = (unsigned long) recv_buffer; xfer.len = len + 2; xfer.bits_per_word = 8; xfer.speed_hz = 1000000; xfer.cs_change = 0; xfer.rx_nbits = len * 8; xfer.tx_nbits = 8; int res = ioctl(spi_dev_fd, SPI_IOC_MESSAGE(1), xfer); if(res &gt; 0) { status[0] = recv_buffer[0]; if(copy_to != NULL) { string temp = string(recv_buffer); temp = temp.substr(1); strncpy(copy_to, temp.c_str(), len); } // debug code for(int i = 0; i &lt; len; ++i) { printf(\"copy_to: %x \\n \", copy_to[i]); } // end debug code. } return res; } Transmitter side: /** * Function to load a payload and send a packet. * * * spi_dev_fd: file descriptor for spi device. * */ int nrf_tx_send_packet(int spi_dev_fd, char * payload, int len) { int rtn; // Put low so we can add the payload. gpio_set_value((unsigned int) GPIO_CE, (unsigned int) GPIO_LVL_LOW); // Set a new payload. nrf_tx_new_payload(spi_dev_fd, payload, len); // Start tx transmission. gpio_set_value((unsigned int) GPIO_CE, (unsigned int) GPIO_LVL_HIGH); do { rtn = nrf_tx_pending_send(spi_dev_fd); if(rtn == 2) { char clr = MAX_RT; spi_send_msg(spi_dev_fd, W_REGISTER | STATUS, &amp;clr, 1); } }while(rtn != 1); // Go back to standby mode gpio_set_value((unsigned int) GPIO_CE, (unsigned int) GPIO_LVL_LOW); // Setting chip enable to 0. char reg = W_REGISTER | STATUS; char val = RX_DR | TX_DS | MAX_RT; spi_send_msg(spi_dev_fd, reg, &amp;val, 1); return 0; } int spi_send_msg(int spi_dev_fd, char addr, char * data, int len) { char data_buffer[len + 1]; char recv_buffer; struct spi_ioc_transfer xfer; memset(&amp;xfer, 0, sizeof(xfer)); memset(&amp;recv_buffer, 0, sizeof(recv_buffer)); data_buffer[0] = addr; for(int i = 1; i &lt; len + 1; ++i) { data_buffer[i] = data[i-1]; printf(\"databuffer[i]: %x \\n\", data_buffer[i]); } xfer.tx_buf = (unsigned long) data_buffer; xfer.rx_buf = (unsigned long) NULL; xfer.len = len + 1; xfer.bits_per_word = 8; xfer.speed_hz = 1000000; xfer.cs_change = 0; //xfer.rx_nbits = 8; xfer.rx_nbits = 0; xfer.tx_nbits = (8 * len) + 8; int res = ioctl(spi_dev_fd, SPI_IOC_MESSAGE(1), xfer); printf(\"res: %i \\n\", res); return res; } I tried to add all the relevant code, sorry if it is a bit much. Main thing to look at is the send and receive functions. They all work as expected until I encounter the zeroed out byte. If I am missing any information that can help someone out please let me know and I can add it. I think the send and receive functions are the most important however. I'm able to set and read the registers of the transceiver.",
        "answers": [
            [
                "I can send files now! The fix was done in spi_read_msg() function. The problem was I was converting the buffer received to a string, which caused the data to be trimmed when the byte 0x00 was encountered. This is also equivalent to the null terminating character. Receiver code: int spi_read_msg(int spi_dev_fd, char addr, char * status, char * copy_to, int len) { char data_buffer; char recv_buffer[len + 1]; struct spi_ioc_transfer xfer; memset(&amp;xfer, 0, sizeof(xfer)); memset(&amp;recv_buffer, 0, sizeof(recv_buffer)); data_buffer = addr; xfer.tx_buf = (unsigned long) &amp;data_buffer; xfer.rx_buf = (unsigned long) recv_buffer; xfer.len = len + 2; xfer.bits_per_word = 8; xfer.speed_hz = 1000000; xfer.cs_change = 0; xfer.rx_nbits = len * 8; xfer.tx_nbits = 8; int res = ioctl(spi_dev_fd, SPI_IOC_MESSAGE(1), xfer); if(res &gt; 0) { status[0] = recv_buffer[0]; if(copy_to != NULL) { for(int i = 0; i &lt; len; ++i) { copy_to[i] = recv_buffer[i + 1]; } } } return res; }"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have the problem when I open a camera with GStreamer, and the camera is not connected, I don't get an error code back from OpenCV. GStreamer returns an error in the console. When I check if the camera is open with .isOpend() the return value is true. When the camera is connected, it works without any issue. std::string pipeline = \"nvarguscamerasrc sensor_id=0 ! video/x-raw(memory:NVMM), width=(int)3264, height=(int)2464, format=(string)NV12, framerate=(fraction)21/1 ! nvvidconv flip-method=2 ! video/x-raw, width=(int)3264, height=(int)2464, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink\" cap_device_.open(pipeline, cv::CAP_GSTREAMER); bool err = cap_device_.isOpened(); if (!err) { printf(\"Not possible to open camera\"); return EXIT_FAILURE; } The GStreamer error code in the console is: (Argus) Error Timeout: (propagating from src/rpc/socket/client/SocketClientDispatch.cpp, function openSocketConnection(), line 219) (Argus) Error Timeout: Cannot create camera provider (in src/rpc/socket/client/SocketClientDispatch.cpp, function createCameraProvider(), line 106) Error generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst-nvarguscamera/gstnvarguscamerasrc.cpp, execute:720 Failed to create CameraProvider [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (933) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1 If I understand everything correct, .isOpend() should return false. If not, how can I check if the pipeline is initialized correct? My system runs with Ubuntu 18.04 on an Nvidia Jetson Nano with a MIPI-CSI camera. GStreamer version 1.14.5, OpenCV version 4.1.1",
        "answers": [
            [
                "This may just be because of a typo. nvarguscamerasrc has no property sensor_id but has sensor-id. It should work after fixing this. In not working case, cap.read() should return false."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to standardize our deployment workflow for machine vision systems. So we were thinking of the following workflow. Deployment workflow So, we want to create the prototype for the same, so we followed the workflow. So, there is no problem with GCP operation whatsoever but when we try to export models, which we train on the vertexAI it will give three models as mentioned in the workflow which is: SaveModel TFLite TFJS and we try these models to convert into the ONNX model but we failed due to different errors. SaveModel - Always getting the same error with any parameter which is as follows Error in savemodel I tried to track the error and I identified that the model is not loading inside the TensorFlow only which is wired since it is exported from the GCP vertexAI which leverages the power of TensorFlow. TFLite - Successfully converted but again the problem with the opset of ONNX but with 15 opset it gets successfully converted but then NVIDIA tensorRT ONNXparser doesn't recognize the model during ONNX to TRT conversion. TFJS - yet not tried. So we are blocked here due to these problems. We can run these models exported directly from the vertexAI on the Jetson Nano device but the problem is TF-TRT and TensorFlow is not memory-optimized on the GPU so the system gets frozen after 3 to 4 hours of running. We try this workflow with google teachable machine once and it workout well all steps are working perfectly fine so I am really confused How I conclude this full workflow since it's working on a teachable machine which is created by Google and not working on vertexAI model which is again developed by same Company. Or am I doing Something wrong in this workflow? For the background we are developing this workflow inside C++ framework for the realtime application in industrial environment.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've made a simple webpage that has to run some simple bash scripts. I run apache2 on Jetson Nano and I have three files in /var/www/html folder: index.html testexec.php test.sh First one looks like: &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;TEST&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=\"testexec.php\"&gt; &lt;input type=\"submit\" value=\"Create file\"&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; the php file: &lt;?php $output = shell_exec(\"test.sh\"); header('Location: http://192.168.25.16/index.html?success=true'); ?&gt; script: #!/bin/bash touch file.txt My problem is that everything looks good, but the scrit doesn't run. In future it will be used to run program written in python, but for now I can't run even that simple one. Is the problem with location of files or with something else? I've already tried to change php file $output = shell_exec(\"test.sh\"); with $output = exec(\"test.sh\"); with or without $output My browser (firefox) returns no errors in console. Script works fine when I run it from shell. It is executable. I've already tried to look for similar problems, but there were no solutions.",
        "answers": [
            [
                "Very often the problem is with user rights, first try installing chmod 777 test.sh to just check if this is the problem. In addition, you should go back to the exec documentation exec(string $command, array &amp;$output = null, int &amp;$result_code = null): string|false and check what $result_code is equal to. If we get echo result!: $output=null; $retval=null; exec('echo $(pwd);', $output, $retval); echo \"${retval} with value:\\n\"; print_r($output);"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to control a servo using JetsonNano and a PCA9685 board. I am on Jetpack 4.3 and using python3. After a recent update I started to end up with the following error. \"NotImplementedError: pwmio not supported for this board\" from adafruit_servokit import ServoKit kit=ServoKit(channels=16) kit.servo[0].angle=90 ## Horizontal motion kit.servo[1].angle=0 ## Vertical Motion",
        "answers": [
            [
                "I've got the same issue at JetPack 4.6. On JetPack 4.5.1 everything ok. I think something wrong with L4T 32.6.1..."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to install tensorflow on jetson nano. I have followed this document and have been able to install tensorflow. &gt;&gt;&gt; import tensorflow as tf &gt;&gt;&gt; tf.__version__ &gt;&gt;&gt; '2.5.0' I am now trying to install object-detection api. For this I have cloned the repository https://github.com/tensorflow/models.git and also installed the proto sudo apt-get install protobuf-compiler and ran below commands cd models/research protoc object_detection/protos/*.proto --python_out=. cp object_detection/packages/tf2/setup.py . pip3 install setup.py and getting error in installation of setup.py: Error: Could not find a version that satisfies the requirement setup.py (from versions: none) Error: No matching distribution found for setup.py I am not able to understand why this error is coming. I am using python3.6 with 64bit version. Can anyone please provide some suggestions on how can I install object detection api for tensorflow",
        "answers": [],
        "votes": []
    },
    {
        "question": "I installed YoloV5 on my jetson nano. I wanted to execute my object detection code when this error appeared: python3.8/site-packages/torch/lib/libgomp-d22c30c5.so.1: cannot allocate memory in static TLS block. To fix the problem I tried to put in the bashrc: export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1 It didn't work Do you have another idea? Here is my code: import cv2 import numpy as np from elements.yolo import OBJ_DETECTION Object_classes = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush' ] Object_colors = list(np.random.rand(80,3)*255) Object_detector = OBJ_DETECTION('weights/yolov5s.pt', Object_classes) def gstreamer_pipeline( capture_width=1280, capture_height=720, display_width=1280, display_height=720, framerate=60, flip_method=0, ): return ( \"nvarguscamerasrc ! \" \"video/x-raw(memory:NVMM), \" \"width=(int)%d, height=(int)%d, \" \"format=(string)NV12, framerate=(fraction)%d/1 ! \" \"nvvidconv flip-method=%d ! \" \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \" \"videoconvert ! \" \"video/x-raw, format=(string)BGR ! appsink\" % ( capture_width, capture_height, framerate, flip_method, display_width, display_height, ) ) print(gstreamer_pipeline(flip_method=0)) cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=0), cv2.CAP_GSTREAMER) if cap.isOpened(): window_handle = cv2.namedWindow(\"CSI Camera\", cv2.WINDOW_AUTOSIZE) while cv2.getWindowProperty(\"CSI Camera\", 0) &gt;= 0: ret, frame = cap.read() if ret: objs = Object_detector.detect(frame) for obj in objs: label = obj['label'] score = obj['score'] [(xmin,ymin),(xmax,ymax)] = obj['bbox'] color = Object_colors[Object_classes.index(label)] frame = cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), color, 2) frame = cv2.putText(frame, f'{label} ({str(score)})', (xmin,ymin), cv2.FONT_HERSHEY_SIMPLEX , 0.75, color, 1, cv2.LINE_AA) cv2.imshow(\"CSI Camera\", frame) keyCode = cv2.waitKey(30) if keyCode == ord('q'): break cap.release() cv2.destroyAllWindows() else: print(\"Unable to open camera\")",
        "answers": [
            [
                "export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1 is not likely to fix your problem. Because your problem occurred at python3.8/site-packages/torch/lib/libgomp-d22c30c5.so.1. So required env variable setting is below. export LD_PRELOAD=&lt;parent path to python3.8&gt;/python3.8/site-packages/torch/lib/libgomp-d22c30c5.so.1 I'm sure there's parent path to python3.8 directory. You should find it and insert it into command above. how to find it : How do I find the location of my Python site-packages directory? By default, it will be /usr/lib. In this case, required command is export LD_PRELOAD=/usr/lib/python3.8/site-packages/torch/lib/libgomp-d22c30c5.so.1"
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "I am looking if i can run Raspi and Lepton FLIR cameras simultinuesly in python code and OpenCV in jetson nano. i was able to run both cameras by this command from the terminal gst-launch-1.0 nvarguscamerasrc sensor_mode=0 ! 'video/x-raw(memory:NVMM),width=3264, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=2 ! 'video/x-raw,width=800, height=600' ! videoconvert ! ximagesink &amp; gst-launch-1.0 v4l2src device=/dev/video1 ! video/x-raw,format=UYVY ! videoscale ! video/x-raw,width=800,height=600 ! videoconvert ! ximagesink and i am looking if can implement the above command in this python / opencv code import cv2 print(cv2.__version__) dispW=640 dispH=480 flip=2 camSet='nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3264, height=2464, format=NV12, framerate=21/1 ! nvvidconv flip-method='+str(flip)+' ! video/x-raw, width='+str(dispW)+', height='+str(dispH)+', format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink' cam= cv2.VideoCapture(camSet) while True: ret, frame = cam.read() cv2.imshow('nanoCam',frame) if cv2.waitKey(1)==ord('q'): break cam.release() cv2.destroyAllWindows()",
        "answers": [
            [
                "Although I understand that your example gives some flexibility, it doesn't make much sense to capture at higher resolution than needed for processing and it can cost resources time. nvarguscamerasrc should be (in some extent) able to manage the scaling. Assuming your FLIR camera can acheive 30 fps, you would try : import cv2 print(cv2.__version__) cam0= cv2.VideoCapture('nvarguscamerasrc ! video/x-raw(memory:NVMM), width=800, height=600, format=NV12, framerate=30/1 ! nvvidconv flip-method=2 ! video/x-raw, format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink drop=1', cv2.CAP_GSTREAMER) if not cam0.isOpened(): print 'Failed to open cam0' exit cam1= cv2.VideoCapture('v4l2src device=/dev/video1 ! video/x-raw,format=UYVY,framerate=30/1 ! videoscale ! video/x-raw,width=800,height=600 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1', cv2.CAP_GSTREAMER) if not cam1.isOpened(): print 'Failed to open cam1' exit while True: ret, frame0 = cam0.read() ret, frame1 = cam1.read() cv2.imshow('Cam0',frame0) cv2.imshow('Cam1',frame1) if cv2.waitKey(1)==ord('q'): break cam.release() cv2.destroyAllWindows() [EDIT: not sure if the following works with USB cams] As your FLIR camera provides UYVY, you may try to use nvv4l2camerasrc plugin instead of v4l2src. Be sure that your sensor is in UYVY mode, and try instead for leveraging scaling and color conversion in HW : cam1= cv2.VideoCapture('nvv4l2camerasrc device=/dev/video1 ! video/x-raw(memory:NVMM),format=UYVY,framerate=30/1 ! nvvidconv ! video/x-raw,format=BGRx,width=800,height=600 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1', cv2.CAP_GSTREAMER)"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I need to setup a Jetson Nano device so that a Python script is launched everytime an Internet connection is available. So, referring to this question, I did the following: I created the 'run_when_connection_available' script: #!/bin/sh # create a dummy folder to check script execution mkdir /home /user_name/dummy_folder_00 # kill previous instances of the system pkill python3 # move to folder with python script and launch it cd /home/user_name/projects/folder /usr/bin/python3 launcher.py --arg01 --arg02 ... # create another dummy folder to check script execution mkdir /home /user_name/dummy_folder_01 I made this script executable and I copied it to /etc/network/if-up.d Now, everytime I plug the ethernet cable out and in again, I can see the dummy folders are created in /home/user_name, but the python script isn't launched (at least, it doesn't appear in the system monitor). I tried running the command in the script from the terminal, and everything works fine, the python program starts as expected. Am I doing something wrong?",
        "answers": [
            [
                "I'm trying to figure out something similar to you, but not quite the same.. This solution got my script python script running upon internet connection, I can check the logs and everything is working fine: Raspbian - Running A Script After An Internet Connection Is Established However, my script uses notify-send to send notifications to my window manager which I can't seem to get working with systemd - the script works when run inside of the user space so I assume it's something to do with systemd and Xorg. Hopefully that shouldn't be a problem for you, I hope this solves your issue. You shouldn't need a bash script in the middle, I got systemd service to run my python script with chmod u+x &lt;file&gt;.py and putting #!/usr/bin/env python3 at the top of the python file so that it's executable directly under the .service file like so: ExecStart=/path/to/file/file.py"
            ],
            [
                "Ok, I guess it was a matter of permissions, I solved it by running everything as user_name, so I modified the script as sudo -user user_name /usr/bin/python3 launcher.py --arg01 --arg02 ..."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am streaming lepton FLIR camera on jetson nano using python3 and OpenCV and I have problems that I can not resize live video. The original lepton resolution is (160 x 120) and I would like to resize it to (640 x 480). I tried to use different OpenCV commands but it does not work need help Herein the code # imports import cv2 import numpy as np import time import argparse # own modules import utills, plot confid = 0.5 thresh = 0.5 mouse_pts = [] # Function to get points for Region of Interest(ROI) and distance scale. It will take 8 points on first frame using mouse click # event.First four points will define ROI where we want to moniter social distancing. Also these points should form parallel # lines in real world if seen from above(birds eye view). Next 3 points will define 6 feet(unit length) distance in # horizontal and vertical direction and those should form parallel lines with ROI. Unit length we can take based on choice. # Points should pe in pre-defined order - bottom-left, bottom-right, top-right, top-left, point 5 and 6 should form # horizontal line and point 5 and 7 should form verticle line. Horizontal and vertical scale will be different. # Function will be called on mouse events def get_mouse_points(event, x, y, flags, param): global mouse_pts if event == cv2.EVENT_LBUTTONDOWN: if len(mouse_pts) &lt; 4: cv2.circle(image, (x, y), 5, (0, 0, 255), 10) else: cv2.circle(image, (x, y), 5, (255, 0, 0), 10) if len(mouse_pts) &gt;= 1 and len(mouse_pts) &lt;= 3: cv2.line(image, (x, y), (mouse_pts[len(mouse_pts)-1][0], mouse_pts[len(mouse_pts)-1][1]), (70, 70, 70), 2) if len(mouse_pts) == 3: cv2.line(image, (x, y), (mouse_pts[0][0], mouse_pts[0][1]), (70, 70, 70), 2) if \"mouse_pts\" not in globals(): mouse_pts = [] mouse_pts.append((x, y)) #print(\"Point detected\") #print(mouse_pts) def calculate_social_distancing(vid_path, net, output_dir, output_vid, ln1): count = 0 vs = cv2.VideoCapture(0) # Get video height, width and fps height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT)) width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH)) fps = int(vs.get(cv2.CAP_PROP_FPS)) # Set scale for birds eye view # Bird's eye view will only show ROI scale_w, scale_h = utills.get_scale(width, height) fourcc = cv2.VideoWriter_fourcc(*\"XVID\") output_movie = cv2.VideoWriter(\"./output_vid/distancing.avi\", fourcc, fps, (width, height)) bird_movie = cv2.VideoWriter(\"./output_vid/bird_eye_view.avi\", fourcc, fps, (int(width * scale_w), int(height * scale_h))) points = [] global image while True: (grabbed, frame) = vs.read() if not grabbed: print('here') break (H, W) = frame.shape[:2] # first frame will be used to draw ROI and horizontal and vertical 180 cm distance(unit length in both directions) if count == 0: while True: image = frame cv2.imshow(\"image\", image) cv2.waitKey(1) if len(mouse_pts) == 8: cv2.destroyWindow(\"image\") break points = mouse_pts # Using first 4 points or coordinates for perspective transformation. The region marked by these 4 points are # considered ROI. This polygon shaped ROI is then warped into a rectangle which becomes the bird eye view. # This bird eye view then has the property property that points are distributed uniformally horizontally and # vertically(scale for horizontal and vertical direction will be different). So for bird eye view points are # equally distributed, which was not case for normal view. src = np.float32(np.array(points[:4])) dst = np.float32([[0, H], [W, H], [W, 0], [0, 0]]) prespective_transform = cv2.getPerspectiveTransform(src, dst) # using next 3 points for horizontal and vertical unit length(in this case 180 cm) pts = np.float32(np.array([points[4:7]])) warped_pt = cv2.perspectiveTransform(pts, prespective_transform)[0] # since bird eye view has property that all points are equidistant in horizontal and vertical direction. # distance_w and distance_h will give us 180 cm distance in both horizontal and vertical directions # (how many pixels will be there in 180cm length in horizontal and vertical direction of birds eye view), # which we can use to calculate distance between two humans in transformed view or bird eye view distance_w = np.sqrt((warped_pt[0][0] - warped_pt[1][0]) ** 2 + (warped_pt[0][1] - warped_pt[1][1]) ** 2) distance_h = np.sqrt((warped_pt[0][0] - warped_pt[2][0]) ** 2 + (warped_pt[0][1] - warped_pt[2][1]) ** 2) pnts = np.array(points[:4], np.int32) cv2.polylines(frame, [pnts], True, (70, 70, 70), thickness=2) #################################################################################### # YOLO v4 blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (128, 128), swapRB=True, crop=False) net.setInput(blob) start = time.time() layerOutputs = net.forward(ln1) end = time.time() boxes = [] confidences = [] classIDs = [] for output in layerOutputs: for detection in output: scores = detection[5:] classID = np.argmax(scores) confidence = scores[classID] # detecting humans in frame if classID == 1: if confidence &gt; confid: box = detection[0:4] * np.array([W, H, W, H]) (centerX, centerY, width, height) = box.astype(\"int\") x = int(centerX - (width / 2)) y = int(centerY - (height / 2)) boxes.append([x, y, int(width), int(height)]) confidences.append(float(confidence)) classIDs.append(classID) idxs = cv2.dnn.NMSBoxes(boxes, confidences, confid, thresh) font = cv2.FONT_HERSHEY_PLAIN boxes1 = [] for i in range(len(boxes)): if i in idxs: boxes1.append(boxes[i]) x,y,w,h = boxes[i] if len(boxes1) == 0: count = count + 1 continue # Here we will be using bottom center point of bounding box for all boxes and will transform all those # bottom center points to bird eye view person_points = utills.get_transformed_points(boxes1, prespective_transform) # Here we will calculate distance between transformed points(humans) distances_mat, bxs_mat = utills.get_distances(boxes1, person_points, distance_w, distance_h) risk_count = utills.get_count(distances_mat) frame1 = np.copy(frame) # Draw bird eye view and frame with bouding boxes around humans according to risk factor bird_image = plot.bird_eye_view(frame, distances_mat, person_points, scale_w, scale_h, risk_count) img = plot.social_distancing_view(frame1, bxs_mat, boxes1, risk_count) # Show/write image and videos if count != 0: output_movie.write(img) bird_movie.write(bird_image) cv2.imshow('Bird Eye View', bird_image) cv2.imshow('Social Distancing', img) cv2.imwrite(output_dir+\"frame%d.jpg\" % count, img) cv2.imwrite(output_dir+\"bird_eye_view/frame%d.jpg\" % count, bird_image) count = count + 1 if cv2.waitKey(1) &amp; 0xFF == ord('q'): break vs.release() cv2.destroyAllWindows() if __name__== \"__main__\": # Receives arguments specified by user parser = argparse.ArgumentParser() parser.add_argument('-v', '--video_path', action='store', dest='video_path', default='./data/example.mp4' , help='Path for input video') parser.add_argument('-o', '--output_dir', action='store', dest='output_dir', default='./output/' , help='Path for Output images') parser.add_argument('-O', '--output_vid', action='store', dest='output_vid', default='./output_vid/' , help='Path for Output videos') parser.add_argument('-m', '--model', action='store', dest='model', default='./models/', help='Path for models directory') parser.add_argument('-u', '--uop', action='store', dest='uop', default='NO', help='Use open pose or not (YES/NO)') values = parser.parse_args() model_path = values.model if model_path[len(model_path) - 1] != '/': model_path = model_path + '/' output_dir = values.output_dir if output_dir[len(output_dir) - 1] != '/': output_dir = output_dir + '/' output_vid = values.output_vid if output_vid[len(output_vid) - 1] != '/': output_vid = output_vid + '/' # load Yolov4 weights weightsPath = model_path + \"yolov4-tiny-custom_best.weights\" configPath = model_path + \"yolov4-tiny-custom.cfg\" net_yl = cv2.dnn.readNetFromDarknet(configPath, weightsPath) ln = net_yl.getLayerNames() ln1 = [ln[i[0] - 1] for i in net_yl.getUnconnectedOutLayers()] # set mouse callback cv2.namedWindow(\"image\") cv2.setMouseCallback(\"image\", get_mouse_points) np.random.seed(42) calculate_social_distancing(values.video_path, net_yl, output_dir, output_vid, ln1)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to make a socket connection between my docker container and host machine. I am doing a embedded based project in a board which involves many functions. I had used one function using a docker container which was fetched from github repository ,and that function works correctly inside the container. But I couldn't take the output from that container outside of it to the board to enable a socket connection. The socket connection fails to execute inside the docker container. It is unable to make a communication from the docker container(client) to the server part which is outside the container. Kindly help me to sort this out.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm working with a jetson nano which had python2.6 and 3.6 preinstalled. I installed python3.7 through sudo apt install python3.7-dev When I tried to run the pip command, it said there was no such command. So I installed pip and pip 3 through: sudo apt update sudo apt install python3-pip Output: pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.7) So it worked fine. But pip3 and pip stopped working after I downloaded the python dependencies to install mxnet via: sudo apt-get update sudo apt-get install -y \\ build-essential \\ git \\ libopenblas-dev \\ libopencv-dev \\ python3-pip \\ python-numpy sudo pip3 install --upgrade \\ pip \\ setuptools \\ numpy It says that '-bash: /usr/bin/pip3: No such file or directory' I'm not really sure what went wrong. Even when I retry to install pip or pip3 the same error comes out.",
        "answers": [],
        "votes": []
    },
    {
        "question": "my problem is a bit complicate to explain but I am gonna do my best ( it's gonna be long) . I am trying to develop a python program to count vehicules in a traffic. For that I have a canvas on which I can draw lines to define the entry and exit of a road ( so the program counts every vehicules crossing the lines). At first it was counting from uploaded videos and it worked well on Windows but I had to use it with Jetson Nano and a webcam to have a live streaming video. s So I installed everything it needed to run and here's what happened: I have both python2.7 and python3.7 in my Jetson. But the program was built with python3.7. I tried using gstreamer to open my CSI Camera but it didn't work. I decided to see if my opencv was compatible with Gstreamer and i saw \"GStreamer: no\" and\" Python(for build): bin/python2.7 After running a script( that should open the camera) with python2.7, the camera worked well, so I decided to make my program work with python2.7. And INDEED the program could open the camera BUT there were no canvas anymore, so I couldn't draw the lines . I tried installing a virtual environment ( archiconda ) , nothing works, the cv2 in my conda env is still not working with Gstreamer. I thought about building it with Cmake but there's no CMakeList.txt in the cv2 folder in my archiconda folder. Now when I run my script (which shows my openCV build info ) with python2.7 , it shows GStreamer:Yes and Python(for build): home/archiconda/bin/python3 . But when i run it with python3.7 I get GStreamer:No and Python(for build): bin/python2.7 I must admit am so lost i don't understand what's happening. So my main problem is : How can I make my OpenCV work with GStreamer and Python3.7 on my Jetson Nano Versions: OpenCV 4.5.4 Python 3.7 GStreamer 1.14.5 Thank you if you had the patience to read me.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a project about Offloading on Jetson Nano with Python How to calculate clock rate(frequency) in In Artificial intelligence(AI) or Deep Learning Task? classically in Computer science (offloading), executed time(or CPU Time) is Product of CPU Clock cycles and Clock Cycle Time. classically Clock cycle time is inverse of frequency of CPU, but how about AI Task? Because I think in AI(or DL) GPU have also big role in Task, So should I add just each frequency? For example, CPU has 1.5GHz and GPU has 800MHz, then GPU has lower but in AI GPU is more powerful or? and secondary for estimated execute Time, I have to calculate CPU Clock Cycles, but I don't know now, how I exactly calculate this, have someone opinion about this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "SPI doesn\u2019t want to work on my Jetson Nano. In my devices from terminal I have this: val@val-desktop:~$ ls /dev/spi* /dev/spidev0.0 /dev/spidev0.1 /dev/spidev1.0 /dev/spidev1.1 And this: val@val-desktop:~$ lsmod | grep spi spidev 13282 0 When I\u2019m connecting MISO and MOSI pins(19 and 21 pins according to this picture ) and testing SPI with this code I have this: val@val-desktop:~/spi_test$ ./spi_test -v -D /dev/spidev0.0 spi mode: 0x0 bits per word: 8 max speed: 500000 Hz (500 kHz) TX | FF FF FF FF FF FF 40 00 00 00 00 95 FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF F0 0D |\u2026@\u2026| RX | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |\u2026| No response. I was trying all 0.0 0.1 1.0 1.1 variants. What am I doing wrong?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am not being able to get my recipe to copy some files into my target device. Currently the layers of my yocto project looks like this: layer path priority ========================================================================== meta /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta 5 meta-tegra /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-tegra 5 contrib /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-tegra/contrib 4 meta-oe /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-oe 6 meta-python /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-python 7 meta-networking /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-networking 5 meta-filesystems /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-filesystems 6 meta-virtualization /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-virtualization 8 meta-tegra-community /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-tegra-community 20 meta-tegra-support /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-tegra-support 40 meta-demo-ci /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-demo-ci 40 meta-tegrademo /home/juanpablo/work/yocto/tegra-demo-distro/layers/meta-tegrademo 50 workspace /home/juanpablo/work/yocto/tegra-demo-distro/build/workspace 99 meta-mine /home/juanpablo/work/yocto/meta-kwali 6 The meta-mine layer is the layer I created with a recipe to copy files inside the image I am then flashing to the sd card of a jetson-nano-devkit. The recipe log-generators_0.1.bb has the following content: DESCRIPTON = \"A template recipe to copy files from host directory to target. \\ The example is written with docker-compose files\" LICENSE = \"CLOSED\" SRC_URI = \"file://.env \\ file://docker-compose.yml \\ \" FILES_${PN} += \"/test\" inherit allarch do_install() { install -d ${D}/test install -m 0755 ${WORKDIR}/.env ${D}/test/ install -m 0755 ${WORKDIR}/docker-compose.yml ${D}/test/ } I have tried following the wiki's cookbook recipe and also 2 or 3 answers for similar questions posted in SO (e.g also defining ${S} = ${WORKDIR}, not using inherit allaarch, etc). Any suggestions or help is welcome.",
        "answers": [
            [
                "I tried your exact recipe on my setup and it seems to work correctly. bitbake log-generators produces log-generators{,-dbg,-dev}_0.1-r0_all.ipk (I happen to use ipk) packages within build/tmp/deploy/ipk/all/ directory. While inspecting log-generators_0.1-r0_all.ipk, I can see the correct files in /test inside. If you don't see the files in your target image, my best guess is that you need to reference the package in the image's install list. The simplest way is to add this to your local.conf: IMAGE_INSTALL_append = \" log-generators \""
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Trying to run object detection algorithms in jetson nano (ubuntu 18.4) and Gstreamer cannot open the video files because of this warning in the Gstreamer. I have Gstreamer 1.14.5 version. (I tried .avi, .mp4 and .MOV). It also appeared these warnings: \"Gstreamer warning:unable to start pipeline\" and \"Gstreamer warning: Embedded video playback halted; module avidemux0 reported: Internal data stream error.\" The code: if __name__ == '__main__': args = arg_parse() confidence = float(args.confidence) start = 0 CUDA = torch.cuda.is_available() num_classes = 80 CUDA = torch.cuda.is_available() videofile = args.video # start the file video stream thread and allow the buffer to # start to fill print(\"[INFO] starting video file thread...\") fvs = FileVideoStream(videofile).start() time.sleep(1.0) # start the FPS timer fps = FPS().start() # loop over frames from the video file stream while fvs.more(): try: # grab the frame from the threaded video file stream, resize # it, and convert it to grayscale (while still retaining 3 # channels) frame = fvs.read() frame = imutils.resize(frame, width=800) frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) frame = np.dstack([frame, frame, frame]) fps.update() # stop the timer and display FPS information fps.stop() print() print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed())) print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps())) print(\"[INFO] Accuracy: {:.2f}%\".format(np.clip((accuracy/(frames+1))*100,1,100))) PIL_image = Image.fromarray(np.uint8(frame)).convert('RGB') original_image = PIL_image # original_image = original_image.convert('RGB') ret = detect(original_image, min_score=confidence, max_overlap=0.1, top_k=200) ret = np.asarray(ret) cv2.imshow(\"frame\", ret) frames+=1 key = cv2.waitKey(1) if key &amp; 0xFF == ord('q'): break continue except: pass How will i fix it? Thanks.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have an spi device attached to a Jetson. The device is detected and works well provided I restrict the calls to spidev.xfer and spidev.xfer2. My device is configured according to its datasheet and works well on Arduino platforms. Connection is done this way: import spidev spi = spidev.SpiDev() spi.open(0, 0) spi.max_speed_hz = 10000000 spi.mode = 0b00 I can confirm connection by doing this test: # read from address 0, shifting by one and appending the read flag # add a dummy address of 0x00 to do the reading, expect 0xAD back addr = [0x00 &lt;&lt; 1 | 0x01, 0x00] print(spi.xfer(addr)[1] == 0xAD) # prints true if found, false if not I can lazily read the data from the device and it works fine. If I set a loop to read from the device as soon as data is available the data becomes garbled and out of order. For example: while True: status = spi.xfer([0x04 &lt;&lt; 1 | 0x01, 0x00])[1] # if the first bit is set, then there's data to read if (status &amp; 0x01) == 1: addr = [0x00] * 10 # number of bytes to read plus 1 addr[0] = 0x08 &lt;&lt; 1 | 0x01 data = spi.xfer(addr)[1:] My device is configured to deliver new data at 4 KHz, SPI is set to the device max of 10 MHz. When I count the calls for status and data I can get around 8000 and 5000 respectively per second. This tells me that status is failing at some point, because the device can't report at 5000 Hz. When I look at data itself the values will be inconsistent across the 9 bytes read. The values are integers and should be in an ascending pattern of 3 something like: [62, 192, 240, 62, 192, 240, 62, 192, 240] Instead what happens is something like this: [192, 240, 62, 192, 240, 62, 192, 240, 62] [240, 62, 192, 240, 62, 192, 240, 62, 192] The returned result is off by one or 2 places. Over time it becomes impossible to tell what data is missing and since there are more returned values than possible, some must be repeats, or partial repeats or junk. The device manufacturer specifies that reads to the data register are always consistent, so any set of 3 ascending values should always be ascending. Also status being true more than 4000 times per second leads me to believe status is corrupted or junk at some times. My conclusion is that spi.xfer is producing unexpected results. I have \"fixed\" it by forcing calls to spi.xfer to be close to the sample frequency of the device by doing this: import time import numpy as np .... freq = spi.xfer([0x28 &lt;&lt; 1 | 0x01, 0x00]) # read the frequency register addr = [0x00] * 10 # number of bytes to read plus 1 addr[0] = 0x08 &lt;&lt; 1 | 0x01 data = np.zeros((int(freq) * 2, 3)) start_time = time.time() delta_time = 1 / freq cur_time = start_time - delta_time count = 0 while count &lt; len(data): if ((time.time() - delta_time) &gt; cur_time): cur_time = cur_time + delta_time data[count] = spi.xfer(addr)[1:] count = count + 1 Using this snippet I get consistent data from the device at 4 KHz which is no surprise since it's forced to do it. I can't say that I grabbed all data points because of the timing, although I should have. Using xfer and xfer2 produce the same results. How do I properly poll the status register to read the data when it becomes available?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have been struggling because of the library adafruit_servokit has been stopping me from assigning pins. When I try to do this: from adafruit_servokit import ServoKit # Servo library that works with Jetson import RPi.GPIO as GPIO # Part of PWM DC motor control GPIO.setmode(GPIO.BOARD) # Error here It returns an error saying this: Traceback (most recent call last): File \"brew.py\", line 4, in &lt;module&gt; GPIO.setmode(GPIO.BOARD) File \"/usr/lib/python3/dist-packages/Jetson/GPIO/gpio.py\", line 317, in setmode raise ValueError(\"A different mode has already been set!\") ValueError: A different mode has already been set! I just need a way to control my servos and use my GPIO pins at the same time. I'm open to buying new parts as well.",
        "answers": [
            [
                "Turns out I just needed to use digitalio: https://learn.adafruit.com/circuitpython-on-raspberrypi-linux/digital-i-o"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "My application deals with recording video using Jetson Nano. After recording, depends on the return code of the recording function, I rename the video file. Recently, I noticed that sometime, my log shows an exception at the renaming line: 021-10-08 20:31:51,421 - root - ERROR - Exception occurred. Traceback (most recent call last): File \"/home/shozemi/.local/lib/python3.6/site-packages/aiclassroom/capture.py\", line 355, in entry sys.exit(main(args=arguments)) File \"/home/shozemi/.local/lib/python3.6/site-packages/aiclassroom/capture.py\", line 318, in main os.rename(output_path, f'{output_path}.failed') FileNotFoundError: [Errno 2] No such file or directory: '/home/uname/original/division-school_20211008_1936_classroom_75_4659.mp4' -&gt; '/home/uname/original/division-school_20211008_1936_classroom_75_4659.mp4.failed' However, when I check the folder, the file is indeed there, and has already been renamed! ~/original$ ls -la *4659* -rw-rw-r-- 1 uname uname 1428816762 10\u6708 8 20:21 division-school_20211008_1936_classroom_75_4659.mp4.failed -rw-rw-r-- 1 uname uname 1122092 10\u6708 8 20:21 division-school_20211008_1936_classroom_75_4659.png Before renaming, I also read the video file and get a frame from the video as the thumbnail on my web system, and you can see the thumbnail is created normally too. So in a nutshell: Capture video = OK Read from disk and create thumbnail = OK Rename: log says \"ERROR\" but is actually OK too! Can someone explain it for me? Additional information: OS: Ubuntu 18.04 System: Jetson Nano Python 3.6",
        "answers": [
            [
                "Considering Capture video = OK Read from disk and create thumbnail = OK Rename: log says \"ERROR\" but is actually OK too! I find probable that for some reason os.rename with same arguments is called twice, thus it is something like os.rename(\"file1\",\"file2\") # file1 exist and is successfully renamed to file2 # potentially other operations os.rename(\"file1\",\"file2\") # error as file1 no longer exist Further inquiry of /home/shozemi/.local/lib/python3.6/site-packages/aiclassroom/capture.py would be required to determine if described situation could arise"
            ],
            [
                "It turned out that my script was called multiple times due to bad locking and race condition."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "i'm trying to set the right parameter to run up a python script when the jetson nano is powered up. I'm facing a problem that the service file doesn't want to recognize the library that was installed using pip. If the code was compiled from the command line it works fine without facing any problem but with the systemd support i get this error: here the first image(upload://k2pWgcIRAB7N2OzLrascOEi1HrI.jpeg) second image The service file looks like the following figure: service file i'll be really gratefull if someone can help me.",
        "answers": [
            [
                "You have a few options, but essentially what you need to do is make sure the installed packages are available to python when run as the configured user (root by default). Next, since you've split your script into multiple files, you need to make sure those files are in locations known to python (or instruct python to look in additional directories). The following configuration should help, assuming running the script as the znvidia user from within /home/znvidia is what you had working. [Unit] Description=\"Some useful description\" [Service] User=znvidia # Or leave out for root WorkingDirectory=/home/znvidia/Desktop ExecStart=/usr/bin/python3.6 /home/znvidia/Desktop/Drowsiness_detection.py [Install] WantedBy=multi-user.target"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to run a simple multiprocessing system on a Jetson NANO device, flashed with Jetpack 4.5. I'm doing what I would usually do on a computer, so I have a main script, launcher.py launcher.py import multiprocessing as mp from multiprocessing import set_start_method, Queue, Event from camera_reader import Camera_Reader_Initializer def main(): set_start_method(\"spawn\") cam_read = mp.Process(target=Camera_Reader_Initializer, args=()) cam_read.daemon = True cam_read.start() if __name__ == \"__main__\": main() which should launch the script camera.py (actually, together with a couple of other scripts) camera.py: camera.py print(\"check 00\") def Camera_Reader_Initializer(): print('check 01') cam_read = Camera_Reader() cam_read.run() class Camera_Reader(): def __init__(self): print('check 02) self.source = \"/dev/video0\" def run(self): print('check 03') input = jetson.utils.videoSource(self.source) output = jetson.utils.videoOutput(\"\") while output.IsStreaming(): image = input.Capture(format='rgb8') output.Render(image) output.SetStatus(f\"Video Viewer | {image.width:d}x{image.height:d} | {output.GetFrameRate():.1f} FPS\") However, when running launcher.py the only output I got is: check 00 So, basically the cam_read object isn't created or run. Am I doing something wrong?",
        "answers": [
            [
                "The functionality of setting Process.daemon = True will cause the main process to call Process.terminate() when it exits. This is meant for long running child processes that can handle being closed without warning (in general you should handle the SIGTERM signal to cleanup before exiting). Your main function in \"launcher.py\" does not wait for the child to do anything, and tries to exit basically right away. There seems to be just enough time for the child to get to the print(\"check 00\") line before it's killed. This may seem somewhat consistent, but it should not be counted on even printing that. It's a race between how much the child can accomplish before the main process gets around to closing it. Fixing this will depend on how you want it to function. If you still want the child to just run in the background forever until the main process exits, you need to make sure the main process takes some time (perhaps with time.sleep). If the child is completing some finite job, it probably shouldn't be a daemon, and you should call Process.join() on it to wait for it to finish at some point in main()."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am noticing something strange when I am inferencing from a TensorsRT graph. As I inference more frames in series the overall time per frame reduces. The data is as follows: Frames Time Rate 1 frame 6sec 0.1FPS 3 frames 12sec 0.25FPS 30 frames 6sec 5FPS 100 frames 7.25sec 13.7FPS 1000 frames 31.337sec 32FPS 10000 frames 175.118sec 57FPS 100000 frames 1664.778sec 60FPS I have also calculated the time ignoring the first 15 inferences calls and it appears to follow the same pattern. So this rules out the time to initialize the graph for the first few inferences. This model is a simple MobileNetV2 and running on a jetson nano 4gb. Code snippet of inference: start_time=time.time() for i in range(n_frames): output = frozen_func(get_img_tensor(i))[0].numpy() ans_arr.append(output) end_time=time.time() print(\"time taken - \",end_time-start_time) What is happening here?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Im trying to setup a network on Jetson Nano and VM on my laptop. My version of fabric is 2.3.0. While joining channel I have two errors Error: error getting endorser client for channel: endorser client failed to connect to localhost:9051: failed to create new connection: connection error: desc = \"transport: error while dialing: dial tcp 127.0.0.1:9051: connect: connection refused\" and the same one for 10051 port. And then I cannot deploy chaincode with same errors and false approval from Org2MSP (Nano) Im following this tutorial but with updated images and binaries (and rebuild arm binaries for Nano): Hyperledger Fabric 2.0 on Multiple Hosts Can someone help me to resolve this issue? Thank you.",
        "answers": [
            [
                "you are getting above error because of you are running the peer join command from PC-1 and peers of org2 is running on PC-2 and you have exported the environment variable for CORE_PEER_ADDRESS as localhost:9051. better you should run the crate and join channel commands form cli container. and set the CORE_PEER_ADDRESS as container_name:port in place of using localhost:port for example if you are using join command for peer 0 of org1 then set the CORE_PEER_ADDRESS as following export CORE_PEER_ADDRESS=peer0.org1.example.com:7051 but before this make sure your all containers are connected through docker swarm network."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am completely new to Jetson Nano board and I need to generate 20ns delay in Jetson Nano board. But I have no idea about it. So can any one help me, how can get 20ns delay in jetson nano board. Is there any code snippet available to achieve nanosecond delay? Thanks in advance.",
        "answers": [
            [
                "If you are using C (on Linux/UNIX), there is a function called nanosleep that allows you to sleep some number of nanoseconds. However, Ubuntu is not a real-time operating system. This means other processes (parts of the OS, or anything else that is running at the same time) may interrupt the execution of your program for short periods of time. This means doing something timing-critical with short delays will not work. If you were to try to use nanosleep to delay for 20ns, your program would probably wait much longer than that. From the man page linked above: nanosleep() suspends the execution of the calling thread until either at least the time specified in *req has elapsed, or the delivery of a signal that triggers the invocation of a handler in the calling thread or that terminates the process. If you were planning on using the 20ns delay to generate some kind of output on I/O pins, you may be able to use the existing interfaces on the Jetson (like SPI) that are implemented in hardware and therefore have more consistent timing. Otherwise, consider using a different type of device to generate the signals (like a fast microcontroller or FPGA) and use I2C, SPI, or something else to communicate your intentions to it from the Jetson. If you needed this for keeping track of some amount of time passed in your program, using the system time is a much better option, and there are lots of ways to get this (though it's likely it will increment more slowly than every 20ns)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to execute a train.py script inside a colab or jupyter notebook. Before running I have to set some variables, e.g. dataset-type. I did as I would type into a terminal command but I get a SyntaxError. --dataset-type=voc --dataset-type='voc' ^ SyntaxError: can't assign to operator Executing from a terminal works fine but how do I declare the variables correctly? Here is some code of train.py: parser = argparse.ArgumentParser(description='Single Shot MultiBox Detector Training With PyTorch') # Params for datasets parser.add_argument('--dataset-type', default=\"voc\", type=str, help='Specify dataset type. Currently supports voc and open_images.') parser.add_argument('--datasets', '--data', nargs='+', default=[\"data\"], help='Dataset directory path') parser.add_argument('--balance-data', action='store_true', help=\"Balance training data by down-sampling more frequent labels.\") I tried: from sys import argv argv.append('--dataset-type=voc') This solved the SyntaxError. But I get following error in the end. There are more variables to set, but --dataset-type is still listed. usage: ipykernel_launcher.py [-h] [--dataset-type DATASET_TYPE] [--datasets DATASETS [DATASETS ...]] [--balance-data] [--net NET] [--freeze-base-net] [--freeze-net] [--mb2-width-mult MB2_WIDTH_MULT] [--base-net BASE_NET] [--pretrained-ssd PRETRAINED_SSD] [--resume RESUME] [--lr LR] [--momentum MOMENTUM] [--weight-decay WEIGHT_DECAY] [--gamma GAMMA] [--base-net-lr BASE_NET_LR] [--extra-layers-lr EXTRA_LAYERS_LR] [--scheduler SCHEDULER] [--milestones MILESTONES] [--t-max T_MAX] [--batch-size BATCH_SIZE] [--num-epochs NUM_EPOCHS] [--num-workers NUM_WORKERS] [--validation-epochs VALIDATION_EPOCHS] [--debug-steps DEBUG_STEPS] [--use-cuda USE_CUDA] [--checkpoint-folder CHECKPOINT_FOLDER] ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-f89b4ea1-0c84-4617-af88-0191a91639c0.json An exception has occurred, use %tb to see the full traceback. SystemExit: 2 /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)",
        "answers": [
            [
                "It seems the model uses sys.argv, to assign them in a notebook: from sys import argv argv.append('--dataset-type=voc') That should work the same as adding --dataset-type=voc in a terminal."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm following this guide but when I try to build the c++ library I get the following fatal error. ../../../../gcc-11.1.0/libsanitizer/asan/asan_linux.cpp: In function \u2018void __asan::AsanCheckIncompatibleRT()\u2019: ../../../../gcc-11.1.0/libsanitizer/asan/asan_linux.cpp:199:21: error: \u2018PATH_MAX\u2019 was not declared in this scope 199 | char filename[PATH_MAX]; | ^~~~~~~~ ../../../../gcc-11.1.0/libsanitizer/asan/asan_linux.cpp:200:35: error: \u2018filename\u2019 was not declared in this scope; did you mean \u2018rename\u2019? 200 | MemoryMappedSegment segment(filename, sizeof(filename)); | ^~~~~~~~ |",
        "answers": [
            [
                "This appears to be a bug in libsanitizer/asan/asan_linux.cpp. It seems to find the wrong limits.h file. I was able to work around it by modifying asan_linux.cpp as follows. -#include &lt;limits.h&gt; +#include &lt;linux/limits.h&gt;"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Hello I was working on a C++ project, and had a cmake file that was working just fine, until I tried to add cuda into the C++ project. I am building this project on the NVIDIA Jetson Nano. I get this error while building: nvlink fatal : Could not open input file 'CMakeFiles/MY_APP.dir/src/MY_APP.cpp.o' (target: sm_35) The rest of the error underneath that looks like this: CMakeFiles/MY_APP.dir/build.make:552: recipe for target 'CMakeFiles/MY_APP.dir/cmake_device_link.o' failed make[2]: *** [CMakeFiles/MY_APP.dir/cmake_device_link.o] Error 1 make[2]: Leaving directory '/home/me/Code/MyApp/build' CMakeFiles/Makefile2:127: recipe for target 'CMakeFiles/MY_APP.dir/all' failed make[1]: *** [CMakeFiles/MY_APP.dir/all] Error 2 make[1]: Leaving directory '/home/me/Code/MY_APP/build' Makefile:155: recipe for target 'all' failed make: *** [all] Error 2 make: Leaving directory '/home/me/Code/MY_APP/build' I run my cmake file using a script I called confgure.sh, which looks like this: #!/bin/sh cmake -S . -B build -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-10.2 -DCMAKE_CUDA_COMPILER=/usr/local/cuda-10.2/bin/nvcc I run my make file using a script I called build.sh, which looks like this: #!/bin/sh make -C build My Cmake File looks like this: cmake_minimum_required(VERSION 3.21.0) project(MY_APP VERSION 0.0.0) set(CMAKE_CXX_STANDARD 14) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_CXX_EXTENSIONS OFF) enable_language(CUDA) # Pass options to NVCC set( CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -O3 -gencode arch=compute_35,code=sm_35 ) set(CMAKE_CUDA_COMPILER /usr/local/cuda/bin/nvcc) FILE(GLOB_RECURSE MY_CUDA_SRCS src/*.cu) configure_file(src/MyAppConfig.h.in MyAppConfig.h) #collect cpp files FILE(GLOB_RECURSE SRC src/*.cpp) find_package(CUDA QUIET) if(CUDA_FOUND) SET(CMAKE_CUDA_COMPILER /usr/local/cuda/bin/nvcc) include_directories(${CUDA_INCLUDE_DIRS}) get_filename_component(CUDA_LIBRARY_DIR ${CUDA_CUDART_LIBRARY} DIRECTORY) set(CMAKE_EXE_LINKER_FLAGS ${CMAKE_EXE_LINKER_FLAGS} \"-L${CUDA_LIBRARY_DIR}\") SET(ALL_CUDA_LIBS ${CUDA_LIBRARIES} ${CUDA_cusparse_LIBRARY} ${CUDA_cublas_LIBRARY}) #${CUDA_CUDART_LIBRARY} #${CMAKE_CUDA_RUNTIME_LIBRARY} #) SET(LIBS ${LIBS} ${ALL_CUDA_LIBS}) message(STATUS \"CUDA_LIBRARIES: ${CUDA_INCLUDE_DIRS} ${ALL_CUDA_LIBS}\") set(CUDA_PROPAGATE_HOST_FLAGS ON) set(CUDA_SEPARABLE_COMPILATION ON) list(APPEND CUDA_NVCC_FLAGS -gencode=arch=compute_35,code=sm_35) #collect CUDA files FILE(GLOB_RECURSE CUDA_SRC src/*.cu) #build static library #CUDA_ADD_LIBRARY(my_cuda_lib ${CUDA_SRC} STATIC) cuda_compile(cuda_objs ${CUDA_SRC}) SET(SRC ${cuda_objs} ${SRC}) SET(LIBS ${LIBS} ${my_cuda_lib}) endif() link_libraries(${cuda_objs}) set_source_files_properties(${SRC} PROPERTIES LANGUAGE CUDA) message(\"using cuda_add_executable\") cuda_add_executable(${PROJECT_NAME} ${SRC}) target_include_directories(${PROJECT_NAME} PUBLIC ${PROJECT_BINARY_DIR}) target_link_libraries(${PROJECT_NAME} ${LIBS}) #DOWNLOAD ALL THE SUBMODULES find_package(Git QUIET) if (GIT_FOUND AND EXISTS \"${PROJECT_SOURCE_DIR}/.git\") # Update submodules as needed option(GIT_SUBMODULE, \"Check submodules during build\" ON) if (GIT_SUBMODULE) message(STATUS \"Submodule update\") execute_process(COMMAND ${GIT_EXECUTABLE} submodule update --init --recursvie WORKING_DIRECTORY {CMAKE_CURRENT_SOURCE_DIR} RESULT_VARIABLE_GIT_SUBMOD_RESULT) if (NOT GIT_SUBMOD_RESULT EQUAL \"0\") message(FATAL_ERROR \"git submodule update --init failed with ${GIT_SUMOD_RESULT}, please check submodule\") endif() endif() endif() #CHECK ALL THE SUBMODULES if (NOT EXISTS \"${PROJECT_SOURCE_DIR}/external/Simple-Websocket-Server/CMakeLists.txt\") message(FATAL_ERROR \"The Simple-Websocket-Server submodule was not downloaded! GIT_SUBMODULE was turned off or failed. Please update submodule\") endif() add_subdirectory(external/Simple-Websocket-Server) include_directories(PUBLIC external/Simple-Websocket-Server) find_package(PythonLibs REQUIRED) find_package(pybind11 REQUIRED) include_directories(${PYTHON_INCLUDE_DIRS}) target_link_libraries(${PROJECT_NAME} curl pthread crypto boost_system jsoncpp ${PYTHON_LIBRARIES} cudart #&lt;some-of-my-other-libraries&gt; ) install(TARGETS ${PROJECT_NAME} DESTINATION bin) install(FILES \"${PROJECT_BINARY_DIR}/MyAppConfig.h\" DESTINATION include) include(InstallRequiredSystemLibraries) set(CPACK_RESOURCE_FILE_LICENSE \"${CMAKE_CURRENT_SOURCE_DIR}/License.txt\") set(CPACK_PACKAGE_VERSION_MAJOR \"${MY_APP_VERSION_MAJOR}\") set(CPACK_PACKAGE_VERSION_MINOR \"${MY_APP_VERSION_MAJOR}\") set(CPACK_PACKAGE_VERSION_PATCH \"${MY_APP_VERSION_PATCH}\") include(CPack) set(CMAKE_CUDA_COMPILE_WHOLE_COMPILATION \"${CMAKE_CUDA_COMPILER} ${_CMAKE_CUDA_EXTRA_FLAGS} -c ${MY_CUDA_SRCS}\") message(CMAKE_CUDA_COMPILE_WHOLE_COMPILATION) I am lost on how to get CUDA added to my project that already contains a bunch of C++ files, and I need to be able to call my .cu files from a .cpp file other then main.cpp, and I need to get this building in CMake, and I am doing it on the jetson nano. Any help on solving this error?",
        "answers": [
            [
                "You are mixing up a lot of CUDA-related definitions and commands, including some from an earlier \"era\" of CUDA support in CMake. Among other things: Your CMakeLists.txt is overriding your environment setting for the CUDA compiler location. ... and actually, you shouldn't bother setting that location anyway, since you've already set the CUDA toolkit root. Don't use find_package(CUDA) with CMake versions of 3.17 or 3.18, or later. For all relevant toolkit-related paths, use find_package(CUDAToolkit)`, which does... well, less but also more. Don't use cuda_add_+suffix commands. Since CMake supports CUDA natively, you use regular add_executable, add_library etc. There are further issues with your CMakeLists.txt file - not all of them CUDA-related, but that should be enough to get you started. It may not in itself resolve the specific bottom-line problem you have, though. You may want to have a look at public repositories using CUDA and recent CMake versions to get an idea of how this is done."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "As part of a robotics project I have to communicate between an Nvidia Jetson Nano 2Gb and a ESP8266 device running Micropython, being used as a motor controller, using their UART interfaces. Allowing me to send command strings from the Jetson to the ESP. For testing purposes I'm using a CH340G UART to Serial converter to allow me to send / receive from my laptop also. I'm using the UART1 RX and TX pins on the Jetson (Pins 8 and 10) for 3.3V TTL, the ground on the UART side of the serial converter is connected to ground on the Jetson. I am able to receive messages from the ESP or my laptop successfully on the Jetson with few to no corrupted characters however when I attempt to transmit \"Hello World\" from the Jetson and receive it on my laptop all of the characters are corrupted. Both programs are using Pyserial, the Jetson is using /dev/ttyTHS1 on pins 8 and 10. Receiving On my Laptop (Windows): import serial import time baud = 4800 port = \"COM8\" # Defaults as serial.EIGHTBITS, serial.PARITY_NONE, parity.STOPBITS_ONE ser = serial.Serial(port, baud) try: while True: #ser.write(b'hello world\\n\\r') time.sleep(0.5) line = ser.read_all() print(line.decode('utf-8')) except KeyboardInterrupt: ser.close() b'\\xeb\\x8b\\xab\\xc1\\xff\\xcd\\xdb\\x1e_\\xf9' b'\\xad_\\xb1\\xbd$\\xf7\\x8b;\\xeb\\xfe' b'lm\\xb6\\x7f\\xbc\\xfd\\xb9\\xab\\xcd\\xf9\\xa5}' b'}\\x7f\\xf2o\\xfa\\xbe\\xf2\\x98\\x8dS}\\xf6\\x0b\\xd7O' b'\\xb6dp\\xb5I\\xfb\\xfb\\xbd(\\xdf\\xef\\xae\\xd1\\xd8\\xcd' b'Z\\xf1[&lt;\\xbd\\xf3\\xbe\\x96\\xfe\\xfe' b'\\xd9\\xb3\\xf6\\x83\\x7f{\\xf7\\xd9\\x9a\\xfc' b'r\\xd7m\\xe1]\\xbd\\xb9\\xb7y\\x96\\xfe' b'\\x7f\\xbf\\xfc\\x85}\\xfdr\\x7f_\\xa5\\xfe' b'}\\xb6\\xfb\\x0b_o\\x17\\xcb\\xe0[' b'\\xfc\\xf1\\xbf\\x98\\xf7\\xf9\\xf6\\x8fR' b'\\xae\\x7f\\xb7\\x8b}\\xbf\\xaf_%' These characters wont be de-coded as they don't match utf-8 Traceback (most recent call last): File \"C:\\Users\\Jackr\\Desktop\\test-serial.py\", line 16, in &lt;module&gt; print(line.decode('utf-8')) UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb1 in position 1: invalid start byte Just opening a serial terminal in Putty which usually allows me to view the strings coming through gives me this, showing the corrupt characters Transmitting on the Jetson (Linux 4 Tegra, based on Ubuntu 18.04): import serial from time import sleep baud = 4800 ser = serial.Serial('/dev/ttyTHS1', baud) print(ser) try: While True: ser.write('Hello World \\n'.encode('utf-8') sleep(0.5) except KeyboardInterrupt: print(\"closing...\") I've Tried: Nvgetty is disabled on ttyTHS1 Encoding and de-coding the message in each side with UTF-8 to ensure encoding is the same. Using a lower baud rate (4800) which did prevent corrupted characters from appearing when the Jetson was receiving, but has no effect when the Jetson is transmitting Update When I test looping back the TX from the Jetson into it's RX there is no corruption at all. Using the following code; import serial ser = serial.Serial('/dev/ttyTHS1', 9600) ser.write(b'Hello World) ser.flush() print(ser.read_all()) I have also now set up a Raspberry Pi W Zero and tested sending and receiving UART directly between the UART interfaces of this device and the Jetson. The traffic is uncorrupted going in both directions. I have also used the Pi into the serial adapter to read from the pi on my laptop and this also works fine, suggesting there may be a problem specifically between the Jetson and the serial adapter",
        "answers": [
            [
                "Unfortunatly this problem is caused by unreliabilities in the UART TX pin of the 40 Pin header on the Jetson Nano's carrier board. This causes unreliable transmission to a wide range of devices, I have personally found this with CH340G USB-tty, ESP8266 and RP2040 chips. While it was compatible with a Raspberry Pi Zero W. There is no well documented solution (that I could find) but the common work around appears to be to use an FTDI usb to serial converter to turn one of the Jetson's USB ports into a serial header. (Although as my jetson's usb ports are already in use I will be switching my microcontroller for a dedicated PWM Controller over I2C to escape thos uart nightmare) Apologies for anyone who later finds this with a similar problem, I know the frustration of only finding a work around. For more information below is what appears to be the main thread of a few on the official Nvidia Developer Forums regarding this issue; https://forums.developer.nvidia.com/t/unreliable-serial-communcation-via-the-uart-tx-rx-gpio-pins/158249/30"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to install .NET 5 on Jetson Nano that runs Ubuntu 18.04. I tried the script and I tried using snap and I tried following the manual steps. Neither of these worked, I only get this error: :dotnet -bash: /usr/local/bin/dotnet: No such file or directory There is a file there: :~/dotnet$ ls -l /usr/local/bin/dotnet lrwxrwxrwx 1 root root 18 Jun 28 13:44 /usr/local/bin/dotnet -&gt; /opt/dotnet/dotnet How do I troubleshoot this?",
        "answers": [
            [
                "Mystery solved. As a part of an installer script, a 32-bit version of .NET 5 was downloaded and installed in /opt/dotnet folder. The symlink in /usr/local/bin was pointing to that 32-bit executable. bash tells you the file doesn't exist, when it exists but it's 32-bit on 64-bit machine. file command shows the executable details: :/opt/dotnet$ file dotnet dotnet: ELF 32-bit LSB shared object, ARM, EABI5 version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-armhf.so.3, for GNU/Linux 3.2.0, BuildID[sha1]=1cfb4584e5126b53bc58927cf930f93058d1f637, stripped And that's compared to the 64-bit: :~/dotnet$ file dotnet dotnet: ELF 64-bit LSB shared object, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, for GNU/Linux 3.7.0, BuildID[sha1]=a821d5cb39af704afbb0020e6ce6f88fc2ac0c97, stripped To solve my issue, I overwrote the symlink to point to the 64-bit version :/usr/local/bin$ sudo ln -s /home/username/dotnet/dotnet dotnet"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I recently got started trying to connect a Grove PIR Motion Sensor &amp; Grove NFC Reader to my Jetson Nano, using the Grove Base HAT: I followed these two guides to get the necessary libraries set up in order to interface with the Grove devices: https://www.seeedstudio.com/blog/2019/04/19/instruction-on-how-to-use-nvidia-jetson-nano-with-grove/ https://github.com/Seeed-Studio/grove.py/ So, when I tried to test the PIR Motion Sensor, I encountered an issue. I first tried testing it using the test specified here on the sensor's Wiki page. However, all that showed up when I tried this test was the following: Hat Name = 'Grove Base Hat RPi' and then blank lines following. Next I tried using the test in the aforementioned GitHub page. However, this time, all I got was an endless stream of Watching indicating that it wasn't picking up the movement I was doing in front of it, or wasn't getting the input from the sensor. Additionally, using i2cdetect -r -y 1 with the sensor plugged in yields the following: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- 04 -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- Indicating the Grove Base HAT is being detected, but I don't know if this would also show if the sensor is connected. So, I was wondering if someone might know what the issue is, either the sensor not working, or the Jetson Nano not being able to get the input from it, and how I might be able to resolve this issue. Furthermore, none of the guides above mention anything about the NFC Reader, so I was wondering if anyone had any experience setting that up as well.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I installed anaconda but when I ran conda init doesn't work,I'm using ssh to install that on a remote jetson nano ( ARM architecture).",
        "answers": [
            [
                "I faced the similar issue while using miniconda. I tried to downgrade miniconda version from 4.10.1 to 4.9.2, and then the problem solved. Maybe you should install older version of anaconda."
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "I am trying to run a Gstreamer pipeline that duplicates a video stream so it can be used in two applications. Here is my sample pipeline that fails when run on a Jetson Nano, but works on my Ubuntu PC. I have used v4l2loopback to create 2 v4l2 devices (/dev/video1 and /dev/video2) like so: sudo modprobe v4l2loopback video_nr=1,2 PRODUCER: gst-launch-1.0 videotestsrc ! tee name=t ! queue ! v4l2sink device=/dev/video1 t. ! queue ! v4l2sink device=/dev/video2 CONSUMER: gst-launch-1.0 v4l2src device=/dev/video2 ! xvimagesink As you can see I'm trying to use tee to duplicate the stream and send it to 2 v4l2 devices that I created with v4l2loopback. When I run the consumer, it displays the first frame, then crashes with: Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... Setting pipeline to PLAYING ... New clock: GstSystemClock ERROR: from element /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: Failed to allocate a buffer Additional debug info: gstv4l2src.c(998): gst_v4l2src_create (): /GstPipeline:pipeline0/GstV4l2Src:v4l2src0 Execution ended after 0:00:00.056466348 Setting pipeline to PAUSED ... Setting pipeline to READY ... Setting pipeline to NULL ... Freeing pipeline ... Any idea on why it fails to allocate a buffer? And why doesn't this happen on my Ubuntu PC? Here is my full Jetson Nano pipeline that uses the camera and splits the stream into 2 v4l2 sinks: gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM), width=1920, height=1080, format=NV12, framerate=(fraction)30/1' ! nvvidconv ! tee name=t ! queue ! v4l2sink device=/dev/video1 t. ! queue ! v4l2sink device=/dev/video2",
        "answers": [
            [
                "The error disappears by using the max_buffers=2 argument in v4l2loopback. I can't explain why it doesn't need this option on my Ubuntu PC, must be something to do with the amount of available memory? Curiously, increasing max_buffers to something higher (eg max_buffers=8) causes the same Failed to allocate buffer error."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to know any Service in Nvidia Jetson Nano 2GB Developer Kit for accessing the board Remotely , anywhere , basically on the global Server , not on Local like we do just typing IP I tried TeamViewer but it crashes , Real VNC is not available in Jetson , any other alternative??",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to run jetracer on the 2GB version of Jetson Nano along with Waveshare's JetRacer Pro AI Kit but all the release files listed in the docs are for the 4GB version. The Waveshare wiki also features files for the 4GB version only. I've seen someone mention there being a pre-built 2GB version for Waveshare cars available on the wiki alongside the 4GB version but it's not there anymore (he meant the AI Kit instead of Pro AI Kit I have but still, it's unavailable for either anymore). Is there no way to run the AI capabilities for self driving RC cars on a 2GB jetson, then? I thought these were cross platform and worst case scenario, there'd be worse performance - but not impossibility to run it altogether. Would it make sense to flash the card by hand using Jetson's official docs and then trying to follow the Jetracer Setup Guide from step #2 instead? This seems to have a chance of working cause step #1 (the one with ready-baked files) is just an installation of Jetcard anyway so one should be able to bypass it by flashing the card yourself instead of using the provided image. At the same time, I contacted Waveshare's support and they just told me straight away that Jetracer and their kit won't work with a 2GB Jetson and that's that... Is there a possibility to run Jetracer on a 2GB Jetson then?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to execute a gstreamer pipeline using php script. I was able to get a test pipeline to work without any errors. But when I try the below pipeline in php script, I get an error from the pulsesrc element. $output = shell_exec('sudo /usr/bin/gst-launch-1.0 -e v4l2src device=/dev/video1 ! tee name=t1 t1. ! queue ! video/x-raw, width=1920, height=1080, framerate=30/1 ! nvvidconv ! queue ! nvv4l2h264enc maxperf-enable=1 bitrate=4000000 profile=4 ! queue ! h264parse ! queue ! mux. pulsesrc device=\"alsa_input.usb-VXIS_Inc_ezcap_U3_capture-02.analog-stereo\" ! queue ! audio/x-raw,width=16,depth=16,rate=44100,channel=1 ! audioconvert ! voaacenc ! aacparse ! mpegtsmux name=mux ! filesink location=feed1TS.ts t1. ! queue ! video/x-raw, width=1920, height=1080, framerate=30/1 ! nvvidconv ! queue ! \"video/x-raw(memory:NVMM),width=959,height=540,framerate=30/1,format=NV12\" ! queue ! nvoverlaysink overlay-x=0 overlay-y=270 overlay-w=959 overlay-h=540 overlay=1 v4l2src device=/dev/video0 io-mode=2 do-timestamp=true ! tee name=t2 t2. ! queue ! image/jpeg,width=1920,height=1080,framerate=30/1 ! videorate ! image/jpeg,width=1920,height=1080,framerate=30/1 ! nvv4l2decoder mjpeg=1 ! nvvidconv ! video/x-raw,width=1920,height=1080,framerate=30/1 ! nvvidconv ! queue ! \"video/x-raw(memory:NVMM),framerate=30/1,format=NV12\" ! nvv4l2h264enc maxperf-enable=1 bitrate=4000000 profile=4 ! queue ! h264parse ! queue ! mpegtsmux ! filesink location=feed2TS.ts t2. ! queue ! image/jpeg,width=1920,height=1080,framerate=30/1 ! nvv4l2decoder mjpeg=1 ! nvvidconv ! video/x-raw,framerate=30/1 ! nvvidconv ! queue ! \"video/x-raw(memory:NVMM),framerate=30/1,format=NV12\" ! queue ! nvoverlaysink overlay-x=960 overlay-y=270 overlay-w=960 overlay-h=540 overlay=2 2&gt;&amp;1 &amp;'); Below is the error Setting pipeline to PAUSED ... Opening in BLOCKING MODE Opening in BLOCKING MODE ERROR: Pipeline doesn't want to pause. ERROR: from element /GstPipeline:pipeline0/GstPulseSrc:pulsesrc0: Failed to connect: Connection refused Additional debug info: pulsesrc.c(1015): gst_pulsesrc_open (): /GstPipeline:pipeline0/GstPulseSrc:pulsesrc0 Opening in BLOCKING MODE Setting pipeline to NULL ... Freeing pipeline ... So I tried the same pipeline without pulsesrc element then the pipeline is working perfectly $output = shell_exec('sudo /usr/bin/gst-launch-1.0 -e v4l2src device=/dev/video1 ! tee name=t1 t1. ! queue ! video/x-raw, width=1920, height=1080, framerate=30/1 ! nvvidconv ! queue ! nvv4l2h264enc maxperf-enable=1 bitrate=4000000 profile=4 ! queue ! h264parse ! queue ! mpegtsmux ! filesink location=feed1TS.ts t1. ! queue ! video/x-raw, width=1920, height=1080, framerate=30/1 ! nvvidconv ! queue ! \"video/x-raw(memory:NVMM),width=959,height=540,framerate=30/1,format=NV12\" ! queue ! nvoverlaysink overlay-x=0 overlay-y=270 overlay-w=959 overlay-h=540 overlay=1 v4l2src device=/dev/video0 io-mode=2 do-timestamp=true ! tee name=t2 t2. ! queue ! image/jpeg,width=1920,height=1080,framerate=30/1 ! videorate ! image/jpeg,width=1920,height=1080,framerate=30/1 ! nvv4l2decoder mjpeg=1 ! nvvidconv ! video/x-raw,width=1920,height=1080,framerate=30/1 ! nvvidconv ! queue ! \"video/x-raw(memory:NVMM),framerate=30/1,format=NV12\" ! nvv4l2h264enc maxperf-enable=1 bitrate=4000000 profile=4 ! queue ! h264parse ! queue ! mpegtsmux ! filesink location=feed2TS.ts t2. ! queue ! image/jpeg,width=1920,height=1080,framerate=30/1 ! nvv4l2decoder mjpeg=1 ! nvvidconv ! video/x-raw,framerate=30/1 ! nvvidconv ! queue ! \"video/x-raw(memory:NVMM),framerate=30/1,format=NV12\" ! queue ! nvoverlaysink overlay-x=960 overlay-y=270 overlay-w=960 overlay-h=540 overlay=2 2&gt;&amp;1 &amp;'); I also tried to use only pulsesrc pipeline to test as below. $output = shell_exec('sudo /usr/bin/gst-launch-1.0 -e pulsesrc device=alsa_input.usb-VXIS_Inc_ezcap_U3_capture-02.analog-stereo ! queue ! audio/x-raw,width=16,depth=16,rate=44100,channel=1 ! audioconvert ! vorbisenc ! oggmux ! filesink location=HW1.ogg 2&gt;&amp;1 &amp;'); It also is not working when called from php. Pipeline output is shown as below. Setting pipeline to PAUSED ... ERROR: Pipeline doesn't want to pause. ERROR: from element /GstPipeline:pipeline0/GstPulseSrc:pulsesrc0: Failed to connect: Connection refused Additional debug info: pulsesrc.c(1015): gst_pulsesrc_open (): /GstPipeline:pipeline0/GstPulseSrc:pulsesrc0 Setting pipeline to NULL ... Freeing pipeline ... What can be the issue for this and how can I resolve this error in pulsesrc? All above pipelines are working perfectly in the commandline.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Using a jetson nano with jetpack 4.5.1 requires me to manually install opencv since the included version comes with version 4.1.1 and in order to utilize CUDA and subsequently, YOLOv5, requires 4.1.2 or later. I manually built opencv following this guide. Currently opencv works on python3.6, but my question is how do I use my built opencv on other versions of python (specifically 3.8, which is supported). Thanks in advance",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a MainWindow supposed to be always on full screen mode. A Dialog pops up when button \"Open Dialog\" is clicked. On a desktop system, Ubuntu 20.04, the application works correctly. When a Dialog pops up, the MainWindow remains at full screen mode. However, on JetsonNano Ubuntu 18.04, the Task bar pops up and the MainWindow is not at full screen mode when the Dialog is opened. Has anyone get the same problem? Why is the difference? MainWindow::MainWindow(QWidget *parent) : QMainWindow(parent), ui(new Ui::MainWindow) { ui-&gt;setupUi(this); this-&gt;showFullScreen(); } void MainWindow::on_btn_dialog_clicked() { Dialog *dialog = new Dialog(); dialog-&gt;show(); }",
        "answers": [
            [
                "Documentation says: Full-screen mode works fine under Windows, but has certain problems under X. These problems are due to limitations of the ICCCM protocol that specifies the communication between X11 clients and the window manager. ICCCM simply does not understand the concept of non-decorated full-screen windows. Therefore, the best we can do is to request a borderless window and place and resize it to fill the entire screen. Depending on the window manager, this may or may not work. The borderless window is requested using MOTIF hints, which are at least partially supported by virtually all modern window managers. An alternative would be to bypass the window manager entirely and create a window with the Qt::X11BypassWindowManagerHint flag. This has other severe problems though, like totally broken keyboard focus and very strange effects on desktop changes or when the user raises other windows. X11 window managers that follow modern post-ICCCM specifications support full-screen mode properly."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Device: Jetson Nano My experience level: very low My willingness to learn: very high Code : import cv2 print(cv2.__version__) dispW=640 dispH=480 flip=2 #Uncomment These next Two Line for Pi Camera camSet='nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3264, height=2464, format=NV12, framerate=21/1 ! nvvidconv flip-method='+str(flip)+' ! video/x-raw, width='+str(dispW)+', height='+str(dispH)+', format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink' cam= cv2.VideoCapture(camSet) #Or, if you have a WEB cam, uncomment the next line #(If it does not work, try setting to '1' instead of '0') #cam=cv2.VideoCapture(0) while True: ret, frame = cam.read() cv2.imshow('nanoCam',frame) if cv2.waitKey(1)==ord('q'): break cam.release() cv2.destroyAllWindows() error: python3 Test2.py 4.5.1 [ERROR:0] global /tmp/pip-req-build-zuuo394f/opencv/modules/videoio/src/cap.cpp (140) open VIDEOIO(CV_IMAGES): raised OpenCV exception: OpenCV(4.5.1) /tmp/pip-req-build-zuuo394f/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3264, height=2464, format=NV12, framerate=21/1 ! nvvidconv flip-method=2 ! video/x-raw, width=640, height=480, format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink in function 'icvExtractPattern' Traceback (most recent call last): File \"Test2.py\", line 15, in &lt;module&gt; cv2.imshow('nanoCam',frame) cv2.error: OpenCV(4.5.1) /tmp/pip-req-build-zuuo394f/opencv/modules/highgui/src/window.cpp:376: error: (-215:Assertion failed) size.width&gt;0 &amp;&amp; size.height&gt;0 in function 'imshow' Tutorial I'm following: https://www.youtube.com/watch?v=LXuFYRewBRY Camera module: \"IMX219-77 Camera\" I assume in function the same as a pi camera",
        "answers": [
            [
                "You are trying to read from a GStreamer input and you need to tell cv2. Try with cam = cv2.VideoCapture(camSet, cv2.CAP_GSTREAMER) Also make sure that you have GStreamer support enabled on your cv2 installation (from what I can recall it is not installed by default on some of the Jetson images). You can check with import cv2 print(cv2.getBuildInformation())"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am facing this problem. I train to use darknet on my ubuntu desktop machine. And I detect objects with the weight of the training result on desktop and jetson nano. The result from the desktop machine with a web camera is this. This is what I expected result to detect a Lego brick. And this is from Jetson nano that is not an expected result. This image shows full of tags even if there is no Lego brick in front of a camera. When I got these results, I suspected to make a mistake to send files from Desktop to Jetson nano. But the sending is completed correctly. So I suspect the possibility of this post title. I hope someone to give me some suggestions. Thank you. Desktop GPU: Nvidia Geforce RXT 2080 Super",
        "answers": [
            [
                "the answer is most definitely yes. what is the spec of the two cameras? are they producing images with the same number of pixels?"
            ],
            [
                "Instead of Yolov4-tiny, I started to use Yolov4 for Jetson Nano. Because Yolov4 performs correctly. I am not sure the reason why though. To avoid getting a slow frame rate, I use a lower number for the width and height in the config file. My config file has the numbers like this. Someone may change the numbers to balance the frame rate and the result of the detection. width=288 height=288"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "So Im trying to build a mediapipe whl file for my jetson nano by following this guide https://github.com/jiuqiant/mediapipe_python_aarch64 I am successful on building it with my raspberry pi 4. By the way this github repo also provides a whl file for mediapipe I can install it but the module for face detection is disabled (the one thing I need) So after running python3 setup.py gen_protos &amp;&amp; python3 setup.py bdist_wheel This is the error that I get Here is my pip freeze result: python 3.6.9 absl-py==0.10.0 apt-clone==0.2.1 apturl==0.5.2 asn1crypto==0.24.0 astunparse==1.6.3 attrs==20.3.0 beautifulsoup4==4.6.0 blinker==1.4 Brlapi==0.6.6 cachetools==4.2.1 certifi==2018.1.18 chardet==3.0.4 cryptography==2.1.4 cupshelpers==1.0 cycler==0.10.0 Cython==0.29.23 dataclasses==0.8 decorator==4.1.2 defer==1.0.6 distro-info===0.18ubuntu0.18.04.1 feedparser==5.2.1 future==0.18.2 futures==3.1.1 gast==0.3.3 google-auth==1.29.0 google-auth-oauthlib==0.4.4 google-pasta==0.2.0 graphsurgeon==0.4.5 grpcio==1.37.0 h5py==2.10.0 html5lib==0.999999999 httplib2==0.9.2 idna==2.6 importlib-metadata==4.0.0 Jetson.GPIO==2.0.16 Keras-Applications==1.0.8 Keras-Preprocessing==1.1.1 keyring==10.6.0 keyrings.alt==3.0 kiwisolver==1.3.1 language-selector==0.1 launchpadlib==1.10.6 lazr.restfulclient==0.13.5 lazr.uri==1.0.3 louis==3.5.0 lxml==4.2.1 macaroonbakery==1.1.3 Mako==1.0.7 Markdown==3.3.4 MarkupSafe==1.0 matplotlib==3.2.2 mock==3.0.5 numpy==1.16.1 oauth==1.0.1 oauthlib==3.1.0 onboard==1.4.1 opt-einsum==3.3.0 PAM==0.4.2 pandas==0.22.0 pbr==5.5.1 protobuf==3.16.0rc1 pyasn1==0.4.8 pyasn1-modules==0.2.8 pybind11==2.6.2 pycairo==1.16.2 pycrypto==2.6.1 pycups==1.9.73 pygobject==3.26.1 PyICU==1.9.8 PyJWT==1.5.3 pymacaroons==0.13.0 PyNaCl==1.1.2 pyparsing==2.4.7 pyRFC3339==1.0 python-apt==1.6.5+ubuntu0.3 python-dateutil==2.6.1 python-debian==0.1.32 pytz==2018.3 pyxattr==0.6.0 pyxdg==0.25 PyYAML==3.12 requests==2.25.1 requests-oauthlib==1.3.0 requests-unixsocket==0.1.5 rsa==4.7.2 scipy==0.19.1 SecretStorage==2.3.1 simplejson==3.13.2 six==1.15.0 ssh-import-id==5.7 system-service==0.3 systemd-python==234 tensorboard==2.5.0 tensorboard-data-server==0.6.0 tensorboard-plugin-wit==1.8.0 tensorflow==2.3.1+nv20.12 tensorflow-estimator==2.3.0 tensorrt==7.1.3.0 termcolor==1.1.0 testresources==2.0.1 typing-extensions==3.7.4.3 ubuntu-drivers-common==0.0.0 uff==0.6.9 unity-scope-calculator==0.1 unity-scope-chromiumbookmarks==0.1 unity-scope-colourlovers==0.1 unity-scope-devhelp==0.1 unity-scope-firefoxbookmarks==0.1 unity-scope-manpages==0.1 unity-scope-openclipart==0.1 unity-scope-texdoc==0.1 unity-scope-tomboy==0.1 unity-scope-virtualbox==0.1 unity-scope-yelp==0.1 unity-scope-zotero==0.1 urllib3==1.22 urwid==2.0.1 wadllib==1.3.2 webencodings==0.5 Werkzeug==2.0.0rc4 wrapt==1.12.1 xkit==0.0.0 youtube-dl==2018.3.14 zipp==3.4.1 zope.interface==4.3.2",
        "answers": [
            [
                "This problem solved by upgrading gcc compiler. adding gcc-8 g++-8 to the apt-get install"
            ],
            [
                "Building MediaPipe for Jetson Nano is not the easiest task ever, especially if you have decided to go for CUDA-enabled compilation. I have built MediaPipe 0.8.9 Python Wheel with CUDA 10.2 for Jetson Nano (Currently hand landmarks detection and selfie segmentation only!): https://github.com/anion0278/mediapipe-jetson I hope it will be useful for someone."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "Background I'm working on a robotics project that requires using an ESP8266 based board (Lolin D1 mini pro v2) as a microcontroller. I have Micropython on this board along with a set of functions that I have to be able to call from an Nvidia Jetson Nano over UART. Setup I have the Jetson's UART1_TXD pin wired to the D1's RX pin, and the D1's TX pin to the Jetson UART1_RXD pin. (https://developer.nvidia.com/embedded/learn/jetson-nano-2gb-devkit-user-guide#id-.JetsonNano2GBDeveloperKitUserGuidevbatuu_v1.0-40-PinHeader(J6)) On the Jetson this interface is /dev/ttyTHS1. Using machine.UART on the D1, and pyserial on the Jetson. Micropython version: esp8266-20210202-v1.14 Process so far Initially I attempted to setup a simple connection between these two devices both at baud=9600 with default settings (which I check and I believe to workout the same). I ran a while True loop on the Jetson to run print(ser.readline()), ser being the serial object I had set up. At the same time I ran a while true loop containing uart.write(b'hello world\\n') on the D1. uart being the machine.UART object initialized with the above parameters. This left the Jetson continually printing blank lines. I switched the Jetson's loop to run print(ser.read()) to read a byte at a time, to see if data was getting through at all, this gave me the same situation as before. Micropython REPL Since this wasn't working I'm now attempting to use a known interface on one side, the raw Micropython REPL. This is actually preferable to me as if I can directly send the REPL commands to execute I can import my Library on the microcontroller and run an arbitrary method. The documentation is a little hazy on whether you can access the REPL directly over UART (as opposed to through the USB serial converter) however I believe it to be possible form what I have read (If it isn't, then this problem falls back to getting the original simple UART connection working). With the above physical setup, I removed the program from the D1 which tried to use UART0 so that the REPL would be left on this port as default on boot. And using a terminal simulator on the Jetson attempted to connect to it on /dev/ttyTHS1 -b 115200. picocom -b 115200 /dev/ttyTHS1 shows the parameters of the connection but then the prompt is blank as opposed to the &gt;&gt;&gt; and wont accept any input. rshell --port /dev/ttyTHS1 --baud 115200 waits forever on connecting. screen /dev/ttyTHS1 115200 produces a black screen, no prompt. I also attempted to establish a connection to /dev/ttyTHS1 and baud 115200 using pyserial and listen for some kind of response, however this gave nothing. My Question So my question is, if it is possible, how can I connect to the raw Micropython REPL directly over UART and send commands to the D1, and otherwise how can I establish my own connection on each side to send strings between the two devices? This is the first time I'm attempting to use UART in this way and I am not overly familiar with micro python (I rarely venture out my IDE into the world of IoT projects) so I could be making some big oversights. I understand this is a rather wide problem, and I wouldn't usually post something so open ended on here but I haven't been able to narrow this down and I am limited to a schedule with the project so all help would be appreciated.",
        "answers": [
            [
                "So my question is, if it is possible, how can I connect to the raw Micropython REPL directly over UART and send commands to the D1, and otherwise how can I establish my own connection on each side to send strings between the two devices? I have MicroPython (v1.14) running on a Wemos D1 mini. I'm currently using this to connect to the REPL over the UART. I didn't have to make any configuration changes to the D1 itself; the entire process was: Connect TX wire from serial adapter to RX pin on D1 Connect RX wire from serial adapter to TX pin on D1 Run picocom -b 115200 /dev/ttyUSB0 I was also able to get it to work by connecting directly from the TX/RX pins on a Raspberry Pi (Pi TX -&gt; D1 RX, etc...), also using picocom (although in this case, the serial port on the Pi was /dev/ttyAMA0). In addition to the diagnostic steps suggested by Michael Guidry, if you have any other devices available locally (another Jetson? A Raspberry Pi? Something else?) that you can use to establish a \"known working\" configuration (that is, a configuration in which you're able to successfully communicate using the Jetson's serial pins), that gives you a good starting point."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to connect to my Jetsonnano from Windows 10 via ssh. If i use my Ubuntu Pc everything works fine, but if i use my Windows 10 Laptop I see this Error: Permission denied, please try again. After I type in the correct Password. Thank you all for your Time and Help",
        "answers": [
            [
                "Open the terminal start ssh-agent eval$(ssh-agent -s) add a key to the ssh-agent (if prompted, enter the password) ssh-add ~/.ssh/id_rsa test connection ssh -T git@github.com Clone the repo git clone git@github.com:antonykidis/Setup-ssh-for-github.git Enjoy Important: Every time you start a new Terminal instance: You will have to call ssh-agent. Add RSA key to the ssh-agent. Loop through these steps every time you close/open the terminal. Because the terminal \u201closes\u201d ssh-agent with its keys on every session. Check this information: Open C:\\Program Files\\Git\\etc\\ssh\\ssh_config (if that\u2019s where you installed Git) Add lines Host github.com or ubuntu host machine IdentityFile ~/.ssh/"
            ],
            [
                "Check for the pwsh executable path first: Get-Command pwsh | select Source this will give you the path of powershell core path Get-Command powershell | select Source this command on the other hand will return the path of earlier version of powershell i.e. powershell version 5 etc. I was also having the same issue. After I blindly copied a command from a blog post and executed it: New-ItemProperty -Path \"HKLM:\\SOFTWARE\\OpenSSH\" -Name DefaultShell -Value \"C:\\Program Files\\PowerShell\\7\\pwsh.exe\" -PropertyType String -Force I scratched my head for more than 10 hours. then I did debug run of sshd with this command on Windows 10 host: sshd -d and tried to connect from my Linux machine as usual: ssh james@192.168.1.123 I saw this line in my Windows debug prompt: User james not allowed because shell c:\\\\program files\\\\powershell\\\\7\\\\pwsh.exe does not exist so I executed this command again with modified path to Powershell 7 executable: New-ItemProperty -Path \"HKLM:\\SOFTWARE\\OpenSSH\" -Name DefaultShell -Value \"C:\\Program Files\\WindowsApps\\Microsoft.PowerShell_7.2.1.0_x64__8wekyb3d8bbwe\\pwsh.exe\" -PropertyType String -Force and it fixed my problem."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "When I install tensorflow-gpu through Conda; it gives me the following output: conda install tensorflow-gpu Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /home/psychotechnopath/anaconda3/envs/DeepLearning3.6 added / updated specs: - tensorflow-gpu The following packages will be downloaded: package | build ---------------------------|----------------- _tflow_select-2.1.0 | gpu 2 KB cudatoolkit-10.1.243 | h6bb024c_0 347.4 MB cudnn-7.6.5 | cuda10.1_0 179.9 MB cupti-10.1.168 | 0 1.4 MB tensorflow-2.1.0 |gpu_py36h2e5cdaa_0 4 KB tensorflow-base-2.1.0 |gpu_py36h6c5654b_0 155.9 MB tensorflow-gpu-2.1.0 | h0d30ee6_0 3 KB ------------------------------------------------------------ Total: 684.7 MB The following NEW packages will be INSTALLED: cudatoolkit pkgs/main/linux-64::cudatoolkit-10.1.243-h6bb024c_0 cudnn pkgs/main/linux-64::cudnn-7.6.5-cuda10.1_0 cupti pkgs/main/linux-64::cupti-10.1.168-0 tensorflow-gpu pkgs/main/linux-64::tensorflow-gpu-2.1.0-h0d30ee6_0 I see that installing tensorflow-gpu automatically triggers the installation of the cudatoolkit and cudnn. Does this mean that I no longer need to install CUDA and CUDNN manually anymore to be able to use tensorflow-gpu? Where does this conda installation of CUDA reside? I first installed CUDA and CuDNN the old way (e.g. by following these installation instructions: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html ) And then I noticed that tensorflow-gpu was also installing cuda and cudnn Do i now have two versions of CUDA/CuDNN installed and how do I check this?",
        "answers": [
            [
                "Do i now have two versions of CUDA installed and how do I check this? No. conda installs the bare minimum redistributable library components required to support the CUDA accelerated packages they offer. The package name cudatoolkit is a complete misnomer. It is nothing of the sort. Even though it is now greatly expanded in scope from what it used to be (literally 5 files -- I think at some point they must have gotten a licensing deal from NVIDIA because some of this wasn't/isn't on the official \"freely redistributable\" list AFAIK), it still is basically just a handful of libraries. You can check this for yourself: cat /opt/miniconda3/conda-meta/cudatoolkit-10.1.168-0.json { \"build\": \"0\", \"build_number\": 0, \"channel\": \"https://repo.anaconda.com/pkgs/main/linux-64\", \"constrains\": [], \"depends\": [], \"extracted_package_dir\": \"/opt/miniconda3/pkgs/cudatoolkit-10.1.168-0\", \"features\": \"\", \"files\": [ \"lib/cudatoolkit_config.yaml\", \"lib/libcublas.so\", \"lib/libcublas.so.10\", \"lib/libcublas.so.10.2.0.168\", \"lib/libcublasLt.so\", \"lib/libcublasLt.so.10\", \"lib/libcublasLt.so.10.2.0.168\", \"lib/libcudart.so\", \"lib/libcudart.so.10.1\", \"lib/libcudart.so.10.1.168\", \"lib/libcufft.so\", \"lib/libcufft.so.10\", \"lib/libcufft.so.10.1.168\", \"lib/libcufftw.so\", \"lib/libcufftw.so.10\", \"lib/libcufftw.so.10.1.168\", \"lib/libcurand.so\", \"lib/libcurand.so.10\", \"lib/libcurand.so.10.1.168\", \"lib/libcusolver.so\", \"lib/libcusolver.so.10\", \"lib/libcusolver.so.10.1.168\", \"lib/libcusparse.so\", \"lib/libcusparse.so.10\", \"lib/libcusparse.so.10.1.168\", \"lib/libdevice.10.bc\", \"lib/libnppc.so\", \"lib/libnppc.so.10\", \"lib/libnppc.so.10.1.168\", \"lib/libnppial.so\", \"lib/libnppial.so.10\", \"lib/libnppial.so.10.1.168\", \"lib/libnppicc.so\", \"lib/libnppicc.so.10\", \"lib/libnppicc.so.10.1.168\", \"lib/libnppicom.so\", \"lib/libnppicom.so.10\", \"lib/libnppicom.so.10.1.168\", \"lib/libnppidei.so\", \"lib/libnppidei.so.10\", \"lib/libnppidei.so.10.1.168\", \"lib/libnppif.so\", \"lib/libnppif.so.10\", \"lib/libnppif.so.10.1.168\", \"lib/libnppig.so\", \"lib/libnppig.so.10\", \"lib/libnppig.so.10.1.168\", \"lib/libnppim.so\", \"lib/libnppim.so.10\", \"lib/libnppim.so.10.1.168\", \"lib/libnppist.so\", \"lib/libnppist.so.10\", \"lib/libnppist.so.10.1.168\", \"lib/libnppisu.so\", \"lib/libnppisu.so.10\", \"lib/libnppisu.so.10.1.168\", \"lib/libnppitc.so\", \"lib/libnppitc.so.10\", \"lib/libnppitc.so.10.1.168\", \"lib/libnpps.so\", \"lib/libnpps.so.10\", \"lib/libnpps.so.10.1.168\", \"lib/libnvToolsExt.so\", \"lib/libnvToolsExt.so.1\", \"lib/libnvToolsExt.so.1.0.0\", \"lib/libnvblas.so\", \"lib/libnvblas.so.10\", \"lib/libnvblas.so.10.2.0.168\", \"lib/libnvgraph.so\", \"lib/libnvgraph.so.10\", \"lib/libnvgraph.so.10.1.168\", \"lib/libnvjpeg.so\", \"lib/libnvjpeg.so.10\", \"lib/libnvjpeg.so.10.1.168\", \"lib/libnvrtc-builtins.so\", \"lib/libnvrtc-builtins.so.10.1\", \"lib/libnvrtc-builtins.so.10.1.168\", \"lib/libnvrtc.so\", \"lib/libnvrtc.so.10.1\", \"lib/libnvrtc.so.10.1.168\", \"lib/libnvvm.so\", \"lib/libnvvm.so.3\", \"lib/libnvvm.so.3.3.0\" ] ..... i.e. what you get is (keeping in mind most of those \"files\" above are just symlinks) CUBLAS runtime The CUDA runtime library CUFFT runtime CUrand runtime CUsparse rutime CUsolver runtime NPP runtime nvblas runtime NVTX runtime NVgraph runtime NVjpeg runtime NVRTC/NVVM runtime The CUDNN package that conda installs is the redistributable binary distribution which is identical to what NVIDIA distribute -- which is exactly two files, a header file and a library. You would still require a supported NVIDIA driver installation to make the tensorflow which conda installs work. If you want to actually compile and build CUDA code, you need to install a separate CUDA toolkit which contains all the the development components which conda deliberately omits from their distribution."
            ]
        ],
        "votes": [
            11.0000001
        ]
    },
    {
        "question": "I have done a lot of research on how to control a brushless motor with an ESC (Electronic Speed Controller) using a nano jetson. I did not find how to connect the esc to the gpio of the jetson. I also did not find how to make a simple code to control my motor. If you could help me to clear these points. Thanks in advance. Sacha Lepresle",
        "answers": [
            [
                "Check out the Nvidia Jetson GPIO repository, and the PWM sample. You could for example initiate the PWM-signal to 100Hz like this: p = GPIO.PWM(output_pin, 100) And adjust the PWM duty cycle between 0.1-0.2% in order to generate the necessary 1-2ms high pulse PPM signal to set, like this: p.ChangeDutyCycle(0.1) # 0% p.ChangeDutyCycle(0.2) # 100% I am uncertain if the resolution of the Jetson Nano PWM-timers are good enough for this kind of small adjustments, but it is a quick thing to try."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to install boto3 for python 2.7 on my jetson nano. It is currently installed in site packages for python3.6 because of which i am able to import it in python3. However i am unable to access it in python2.7. It says module not found. jetson@jetson-desktop:~$ pip install boto3 Defaulting to user installation because normal site-packages is not writeable Requirement already satisfied: boto3 in ./.local/lib/python3.6/site-packages (1.17.43) Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in ./.local/lib/python3.6/site-packages (from boto3) (0.10.0) Requirement already satisfied: botocore&lt;1.21.0,&gt;=1.20.43 in ./.local/lib/python3.6/site-packages (from boto3) (1.20.43) Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in ./.local/lib/python3.6/site-packages (from boto3) (0.3.6) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.25.4 in ./.local/lib/python3.6/site-packages (from botocore&lt;1.21.0,&gt;=1.20.43-&gt;boto3) (1.26.4) Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in ./.local/lib/python3.6/site-packages (from botocore&lt;1.21.0,&gt;=1.20.43-&gt;boto3) (2.8.1) Requirement already satisfied: six&gt;=1.5 in ./.local/lib/python3.6/site-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.21.0,&gt;=1.20.43-&gt;boto3) (1.15.0) I want it to work for python2. I am not able to direct the installation to python2.",
        "answers": [
            [
                "try install with: $ git clone https://github.com/boto/boto3.git $ cd boto3 $ virtualenv venv $ . venv/bin/activate $ python -m pip install -r requirements.txt $ python -m pip install -e . $ python -m pip install boto3 and check https://github.com/boto/boto3"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "If using jetson for example to track the real time location of a vehicle using a GPS module what are the best practices to send the collected GPS coordinates to the central server in real time. Should it be a webserver using rest apis or is there any other alternatives? I want the data from the edge device to be sent to a spring boot application",
        "answers": [
            [
                "Are you using the Jetson device for any other application?. What Jetson device are you using? I am not sure I understand your objective. I am assuming you have a computer vision application and you want to align each inference (Detection) with a GPS coordinate, is it correct? Most GPS units use UART, Lets suppose you are using Jetson Nano. Let me know if you need help connecting the module 9What GPS module are you using?). Now, let's say you are using Adafruit (Ultimate Breakout v3) GPS module. On Jeston nano you can use the built in UART via /dev/ttyTHS1 Wire the GPS as follows: 1- Connect the Jetson Nano +3.3V pin to the Vin pin on the GPS. Connect the Jetson Nano Ground pin to the GPS Ground pin. Connect the Jetson Nano UART TX (#8) to the GPS RX pin. Connect the Jetson Nano UART RX (#10) to the GPS TX pin. The next step and for this specific example (GPS module), you need to install the Adafruit GPS CircuitPython library. Now that you have your GPS module installed properly, I strongly recommend to use Nvidia Deepstream, you should be able to use GPS data as part of the configuration file. Look at gie-kitti-output-dir (never done this but based on the documentation, I will assume that's the way). Deepstream has a specific plugins: The Message Converter Gst-nvmsgconv and Message Broker Gst-nvmsgbroker plugins in combination to send analytics data to a server in the Cloud. In my view it's the best way to do it, Deepstream provides some good example apps. I hope my answer will give you some ideas. Good luck."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have an nvidia Jetson Nano (the 4gb version). I am attempting to run this project on it: https://github.com/lucidrains/deep-daze I am attempting to run the command pip install deep-daze. However, I do not have pip so I am running pip3 install deep-daze. When I run that I get chris@chris-desktop:~$ pip3 install deep-daze Collecting deep-daze Using cached https://files.pythonhosted.org/packages/f1/ed/b3f3d9d92f5a48932b3807f683642b28da75722ae93da2f9bdc6af5f1768/deep_daze-0.7.2-py3-none-any.whl Collecting tqdm (from deep-daze) Downloading https://files.pythonhosted.org/packages/f8/3e/2730d0effc282960dbff3cf91599ad0d8f3faedc8e75720fdf224b31ab24/tqdm-4.59.0-py2.py3-none-any.whl (74kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 2.4MB/s Collecting torchvision&gt;=0.8.2 (from deep-daze) Could not find a version that satisfies the requirement torchvision&gt;=0.8.2 (from deep-daze) (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3) No matching distribution found for torchvision&gt;=0.8.2 (from deep-daze) I am pretty unfamiliar with several of the moving parts here and not sure how to fix this. I thought these version numbers may be useful in answering this question: chris@chris-desktop:~$ python3 --version Python 3.6.9 chris@chris-desktop:~$ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) chris@chris-desktop:~$ python2 --version Python 2.7.17 chris@chris-desktop:~$ pip2 --version bash: pip2: command not found chris@chris-desktop:~$ pip --version bash: pip: command not found chris@chris-desktop:~$ cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"18.04.5 LTS (Bionic Beaver)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 18.04.5 LTS\" VERSION_ID=\"18.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=bionic UBUNTU_CODENAME=bionic",
        "answers": [
            [
                "Thanks to the comment from Elbek I got this working! I was able to follow the guide here: https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-8-0-now-available/72048 Unfortunately after I got everything installed I ran into an issue with not having enough memory, but, it all got installed."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I recently bought the nvidia jetson nano microcomputer, the 4Gb. After the first boot, I created a python environment and installed some libraries like numpy, sklearn, pytorch, pandas, etc. Afterwards, I wanted to test a pre-build model for objection recognition. To be more exact I followed Nvidia's tutorial Here. Everything went according to plan and no errors, until I tried to run the programm. I think I need to specify that the following logs were the result of of the program when I ran it on both of my csi cameras: Camera 1 and Camera 2 When running it on the terminal with python my-detection.py. After running it, the programm stoped running. Here are the logs: jetson.inference -- detectNet loading build-in network 'ssd-mobilenet-v2' detectNet -- loading detection network model from: -- model networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff -- input_blob 'Input' -- output_blob 'NMS' -- output_count 'NMS_1' -- class_labels networks/SSD-Mobilenet-v2/ssd_coco_labels.txt -- threshold 0.500000 -- batch_size 1 [TRT] TensorRT version 7.1.3 [TRT] loading NVIDIA plugins... [TRT] Registered plugin creator - ::GridAnchor_TRT version 1 [TRT] Registered plugin creator - ::NMS_TRT version 1 [TRT] Registered plugin creator - ::Reorg_TRT version 1 [TRT] Registered plugin creator - ::Region_TRT version 1 [TRT] Registered plugin creator - ::Clip_TRT version 1 [TRT] Registered plugin creator - ::LReLU_TRT version 1 [TRT] Registered plugin creator - ::PriorBox_TRT version 1 [TRT] Registered plugin creator - ::Normalize_TRT version 1 [TRT] Registered plugin creator - ::RPROI_TRT version 1 [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1 [TRT] Could not register plugin creator - ::FlattenConcat_TRT version 1 [TRT] Registered plugin creator - ::CropAndResize version 1 [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1 [TRT] Registered plugin creator - ::Proposal version 1 [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1 [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1 [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1 [TRT] Registered plugin creator - ::Split version 1 [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1 [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1 [TRT] detected model format - UFF (extension '.uff') [TRT] desired precision specified for GPU: FASTEST [TRT] requested fasted precision for device GPU without providing valid calibrator, disabling INT8 [TRT] native precisions detected for GPU: FP32, FP16 [TRT] selecting fastest native precision for GPU: FP16 [TRT] attempting to open engine cache file /usr/local/bin/networks/SSD-Mobilenet- v2/ssd_mobilenet_v2_coco.uff.1.1.7103.GPU.FP16.engine [TRT] loading network plan from engine cache... /usr/local/bin/networks/SSD-Mobilenet- v2/ssd_mobilenet_v2_coco.uff.1.1.7103.GPU.FP16.engine [TRT] device GPU, loaded /usr/local/bin/networks/SSD-Mobilenet- v2/ssd_mobilenet_v2_coco.uff [TRT] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors. [TRT] Deserialize required 6072963 microseconds. [TRT] [TRT] CUDA engine context initialized on device GPU: [TRT] -- layers 117 [TRT] -- maxBatchSize 1 [TRT] -- workspace 0 [TRT] -- deviceMemory 35449856 [TRT] -- bindings 3 [TRT] binding 0 -- index 0 -- name 'Input' -- type FP32 -- in/out INPUT -- # dims 3 -- dim #0 3 (SPATIAL) -- dim #1 300 (SPATIAL) -- dim #2 300 (SPATIAL) [TRT] binding 1 -- index 1 -- name 'NMS' -- type FP32 -- in/out OUTPUT -- # dims 3 -- dim #0 1 (SPATIAL) -- dim #1 100 (SPATIAL) -- dim #2 7 (SPATIAL) [TRT] binding 2 -- index 2 -- name 'NMS_1' -- type FP32 -- in/out OUTPUT -- # dims 3 -- dim #0 1 (SPATIAL) -- dim #1 1 (SPATIAL) -- dim #2 1 (SPATIAL) [TRT] [TRT] binding to input 0 Input binding index: 0 [TRT] binding to input 0 Input dims (b=1 c=3 h=300 w=300) size=1080000 [TRT] binding to output 0 NMS binding index: 1 [TRT] binding to output 0 NMS dims (b=1 c=1 h=100 w=7) size=2800 [TRT] binding to output 1 NMS_1 binding index: 2 [TRT] binding to output 1 NMS_1 dims (b=1 c=1 h=1 w=1) size=4 [TRT] [TRT] device GPU, /usr/local/bin/networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff initialized. [TRT] W = 7 H = 100 C = 1 [TRT] detectNet -- maximum bounding boxes: 100 [TRT] detectNet -- loaded 91 class info entries [TRT] detectNet -- number of object classes: 91 [gstreamer] initialized gstreamer, version 1.14.5.0 [gstreamer] gstCamera -- attempting to create device csi://0 [gstreamer] gstCamera pipeline string: [gstreamer] nvarguscamerasrc sensor-id=0 ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, framerate=30/1, format=(string)NV12 ! nvvidconv flip-method=2 ! video/x- raw ! appsink name=mysink [gstreamer] gstCamera successfully created device csi://0 [video] created gstCamera from csi://0 ------------------------------------------------ gstCamera video options: ------------------------------------------------ -- URI: csi://0 - protocol: csi - location: 0 -- deviceType: csi -- ioType: input -- codec: raw -- width: 1280 -- height: 720 -- frameRate: 30.000000 -- bitRate: 0 -- numBuffers: 4 -- zeroCopy: true -- flipMethod: rotate-180 -- loop: 0 ------------------------------------------------ [OpenGL] glDisplay -- X screen 0 resolution: 1920x1080 [OpenGL] glDisplay -- X window resolution: 1920x1080 [OpenGL] glDisplay -- display device initialized (1920x1080) [video] created glDisplay from display://0 ------------------------------------------------ glDisplay video options: ------------------------------------------------ -- URI: display://0 - protocol: display - location: 0 -- deviceType: display -- ioType: output -- codec: raw -- width: 1920 -- height: 1080 -- frameRate: 0.000000 -- bitRate: 0 -- numBuffers: 4 -- zeroCopy: true -- flipMethod: none -- loop: 0 ------------------------------------------------ [gstreamer] opening gstCamera for streaming, transitioning pipeline to GST_STATE_PLAYING [gstreamer] gstreamer changed state from NULL to READY ==&gt; mysink [gstreamer] gstreamer changed state from NULL to READY ==&gt; capsfilter1 [gstreamer] gstreamer changed state from NULL to READY ==&gt; nvvconv0 [gstreamer] gstreamer changed state from NULL to READY ==&gt; capsfilter0 [gstreamer] gstreamer changed state from NULL to READY ==&gt; nvarguscamerasrc0 [gstreamer] gstreamer changed state from NULL to READY ==&gt; pipeline0 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; capsfilter1 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; nvvconv0 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; capsfilter0 [gstreamer] gstreamer stream status CREATE ==&gt; src [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; nvarguscamerasrc0 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; pipeline0 [gstreamer] gstreamer stream status ENTER ==&gt; src [gstreamer] gstreamer message new-clock ==&gt; pipeline0 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; capsfilter1 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; nvvconv0 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; capsfilter0 [gstreamer] gstreamer message stream-start ==&gt; pipeline0 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; nvarguscamerasrc0 Error ---------- generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst- nvarguscamera/gstnvarguscamerasrc.cpp, execute:645 No cameras available [gstreamer] gstCamera -- end of stream (EOS) (python:12329): GStreamer-CRITICAL **: 10:33:50.156: gst_mini_object_set_qdata: assertion 'object != NULL' failed I tried running cheese webcam,but the screen displayed \"No Device Found\". That was the result with both csi cameras. The camers are csi, but when I plugged them in, the infra red one, had its lights shining, but, as I said, the camera did not work! I am doing something wrong, do I need to do something I haven't done? Please Help me!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to build on CircleCi ROS packages to use on Nvidia Jetson Nano. Today I use Upboard which has amd64 arch, so it all been clear. I used a docker from dockerhub, I all worked well. Today I cannot find a way to build an arm64v8( the Jetson arch) on docker hub. When I import FROM an arm64 image it fails because the arch doesn't suit (I guess the docker is an amd64 docker). Do you know a way to create an arm64 docker on docker hub to use it on CircleCI? (ROS compitible one would be great).",
        "answers": [
            [
                "If you want to build arm64 docker image with CircleCI (then you can push it to dockerhub), there two solutions: 1/Use CircleCI machine executor and install QEMU to build multi-arch image. (or only linux/arm64 if you want) https://namiops.medium.com/build-a-multi-arch-docker-image-with-circleci-for-amd64-arm64-risc64-3ad0537a1f28 Github: https://github.com/namiops/circleci-multiarch 2/Use CircleCI arm machine executor, which simpiler and faster to build arm64 image. https://namiops.medium.com/golang-arm64-docker-image-with-circleci-arm-machine-8bebf2151b92 Github: https://github.com/namiops/circleci-arm"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Closed. This question is not about programming or software development. It is not currently accepting answers. This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered. Closed 3 months ago. Improve this question I want to set a static IP for a Jetson Nano in my local network. What I've tried: -- Changing the DHCP on the router. -- editing /etc/network/interfaces with the following: auto eth0 iface eth0 inet static address 192.168.1.80 netmask 255.255.255.0 gateway 192.168.1.1 Both this options doesn't seem to work.",
        "answers": [
            [
                "It seems the Jetson Nano has an underlying configuration for the network interface of the board. 1) edit /etc/default/networking sudo vi /etc/default/networking and set the parameter CONFIGURE_INTERFACES=no # Set to 'no' to skip interfaces configuration on boot CONFIGURE_INTERFACES=no 2) Now then the settings in /etc/network/interfaces will work sudo vi /etc/network/interfaces auto eth0 iface eth0 inet static address 192.168.1.80 netmask 255.255.255.0 gateway 192.168.1.1 3) Reboot the board"
            ],
            [
                "TLDR ; Use nmcli con mod [id] and nmcli con [down up] [id] as root. eg: nmcli con show nmcli con mod \"Wired connection 1\" \\ ipv4.addresses \"192.168.8.70/24\" \\ ipv4.gateway \"192.168.8.1\" \\ ipv4.dns \"8.8.8.8 1.1.1.1\" \\ ipv4.dns-search \"8.8.4.4\" \\ ipv4.method \"manual\" (nmcli con down id \"Wired connection 1\" &amp;&amp; nmcli con up id \"Wired connection 1\")&amp; # or reboot device LR ; Recent Jetsons nano use linux network-manager to manage network interfaces. you have to, either use the UI to modify the settings or use cli as below : show network connections with : nmcli con show nmcli con show Edit or Modify the wired network or your preferred interface with sudo nmcli con mod \"Wired connection 1\" \\ ipv4.addresses \"192.168.8.70/24\" \\ ipv4.gateway \"192.168.8.1\" \\ ipv4.dns \"8.8.8.8 1.1.1.1\" \\ ipv4.dns-search \"8.8.4.4\" \\ ipv4.method \"manual\" Edit IPs as suits you and Down/Up connection or reboot your jetson with : (nmcli con down id \"Wired connection 1\" &amp;&amp; nmcli con up id \"Wired connection 1\")&amp; or just reboot after that you'll lose access to it and with another terminal you can ping and access your jetson again."
            ]
        ],
        "votes": [
            6.0000001,
            1e-07
        ]
    },
    {
        "question": "I am using a JetsonNano with JetPack 4.4.1, Tensorflow 2.3.1 and Tensorrt 7.1.3 I have a Keras model that I converted to a TF-TRT model When performing inference on the model, I get the following error: TF-TRT Warning: Engine creation for PartitionedCall/TRTEngineOp_0_0 failed. The native segment will be used instead. Reason: Internal: Failed to build TensorRT engine During Inference I get: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:629] TF-TRT Warning: Engine retrieval for input shapes: [[1,100,68,3]] failed. Running native segment for PartitionedCall/TRTEngineOp_0_0 What does it mean? It seems like TRT is not building engines but the inference works the same. I have performed the same inference on another PC (TF-2.4.1 and TRT 7.2) and I do not get this error. However, I have compared the inference results between the Keras and TF-TRT model and they are the same (both with the error on JetsonNano and without the error on PC) Why are my results the same? How do I solve this?",
        "answers": [
            [
                "It is hard to tell what is happening without a little more information on your code. Also, I am not quite sure where exactly your two errors occur, could you elaborate a little further? As a general info: TF-TRT will fall back to a TensorFlow operation in case the specific operation is not supported by TRT. This might explain why your results are ok after all."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "hope I\u2019m in the right place to post this. I re-installed from scratch my Jetson with the latest JetPack 4.5 and Tensorflow 2.3. The simple Tensorflow sample code (MNIST) works OK. When I try to run : import numpy as np import tensorflow as tf D = tf.convert_to_tensor(np.array([[1., 2., 3.], [-3., -7., -1.], [0., 5., -2.]])) print(\"Shape of D: \", D.shape) print(tf.linalg.det(D)) I get the following error : F tensorflow/core/kernels/determinant_op_gpu.cu.cc:136] Non-OK-status: GpuLaunchKernel( DeterminantFromPivotedLUKernel&lt;Scalar, false&gt;, config.block_count, config.thread_per_block, 0, device.stream(), config.virtual_thread_count, n, lu_factor.data(), pivots, nullptr, output.data()) status: Internal: too many resources requested for launch Can anybody tell me if I\u2019m doing something wrong or if this is some bug for which I should find a workaround ?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to create a stereo vision camera on my jetson nano with 2 raspi cameras. However, I can find a lot of information and code online regarding RasPi but not jetson nano. So for example let's say I have these 2 python programs, the first for starting both cameras on Jetson nano and the second for starting both cameras on RasPi. I'm quite new to all this, so it would be great to get some advice on how I could get started on this. Thanks! Jetson (taken from JetsonHacks): # MIT License # Copyright (c) 2019,2020 JetsonHacks # See license # A very simple code snippet # Using two CSI cameras (such as the Raspberry Pi Version 2) connected to a # NVIDIA Jetson Nano Developer Kit (Rev B01) using OpenCV # Drivers for the camera and OpenCV are included in the base image in JetPack 4.3+ # This script will open a window and place the camera stream from each camera in a window # arranged horizontally. # The camera streams are each read in their own thread, as when done sequentially there # is a noticeable lag # For better performance, the next step would be to experiment with having the window display # in a separate thread import cv2 import threading import numpy as np # gstreamer_pipeline returns a GStreamer pipeline for capturing from the CSI camera # Flip the image by setting the flip_method (most common values: 0 and 2) # display_width and display_height determine the size of each camera pane in the window on the screen left_camera = None right_camera = None class CSI_Camera: def __init__ (self) : # Initialize instance variables # OpenCV video capture element self.video_capture = None # The last captured image from the camera self.frame = None self.grabbed = False # The thread where the video capture runs self.read_thread = None self.read_lock = threading.Lock() self.running = False def open(self, gstreamer_pipeline_string): try: self.video_capture = cv2.VideoCapture( gstreamer_pipeline_string, cv2.CAP_GSTREAMER ) except RuntimeError: self.video_capture = None print(\"Unable to open camera\") print(\"Pipeline: \" + gstreamer_pipeline_string) return # Grab the first frame to start the video capturing self.grabbed, self.frame = self.video_capture.read() def start(self): if self.running: print('Video capturing is already running') return None # create a thread to read the camera image if self.video_capture != None: self.running=True self.read_thread = threading.Thread(target=self.updateCamera) self.read_thread.start() return self def stop(self): self.running=False self.read_thread.join() def updateCamera(self): # This is the thread to read images from the camera while self.running: try: grabbed, frame = self.video_capture.read() with self.read_lock: self.grabbed=grabbed self.frame=frame except RuntimeError: print(\"Could not read image from camera\") # FIX ME - stop and cleanup thread # Something bad happened def read(self): with self.read_lock: frame = self.frame.copy() grabbed=self.grabbed return grabbed, frame def release(self): if self.video_capture != None: self.video_capture.release() self.video_capture = None # Now kill the thread if self.read_thread != None: self.read_thread.join() # Currently there are setting frame rate on CSI Camera on Nano through gstreamer # Here we directly select sensor_mode 3 (1280x720, 59.9999 fps) def gstreamer_pipeline( sensor_id=0, sensor_mode=3, capture_width=1280, capture_height=720, display_width=1280, display_height=720, framerate=30, flip_method=0, ): return ( \"nvarguscamerasrc sensor-id=%d sensor-mode=%d ! \" \"video/x-raw(memory:NVMM), \" \"width=(int)%d, height=(int)%d, \" \"format=(string)NV12, framerate=(fraction)%d/1 ! \" \"nvvidconv flip-method=%d ! \" \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \" \"videoconvert ! \" \"video/x-raw, format=(string)BGR ! appsink\" % ( sensor_id, sensor_mode, capture_width, capture_height, framerate, flip_method, display_width, display_height, ) ) def start_cameras(): left_camera = CSI_Camera() left_camera.open( gstreamer_pipeline( sensor_id=0, sensor_mode=3, flip_method=0, display_height=540, display_width=960, ) ) left_camera.start() right_camera = CSI_Camera() right_camera.open( gstreamer_pipeline( sensor_id=1, sensor_mode=3, flip_method=0, display_height=540, display_width=960, ) ) right_camera.start() cv2.namedWindow(\"CSI Cameras\", cv2.WINDOW_AUTOSIZE) if ( not left_camera.video_capture.isOpened() or not right_camera.video_capture.isOpened() ): # Cameras did not open, or no camera attached print(\"Unable to open any cameras\") # TODO: Proper Cleanup SystemExit(0) while cv2.getWindowProperty(\"CSI Cameras\", 0) &gt;= 0 : _ , left_image=left_camera.read() _ , right_image=right_camera.read() camera_images = np.hstack((left_image, right_image)) cv2.imshow(\"CSI Cameras\", camera_images) # This also acts as keyCode = cv2.waitKey(30) &amp; 0xFF # Stop the program on the ESC key if keyCode == 27: break left_camera.stop() left_camera.release() right_camera.stop() right_camera.release() cv2.destroyAllWindows() if __name__ == \"__main__\": start_cameras() RasPi (from https://github.com/realizator/stereopi-tutorial/blob/master/1_test.py): # Copyright (C) 2019 Eugene Pomazov, &lt;stereopi.com&gt;, virt2real team # # This file is part of StereoPi tutorial scripts. # # StereoPi tutorial is free software: you can redistribute it # and/or modify it under the terms of the GNU General Public License # as published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # StereoPi tutorial is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with StereoPi tutorial. # If not, see &lt;http://www.gnu.org/licenses/&gt;. # # Most of this code is updated version of 3dberry.org project by virt2real # # Thanks to Adrian and http://pyimagesearch.com, as there are lot of # code in this tutorial was taken from his lessons. # import picamera from picamera import PiCamera import time import cv2 import numpy as np import os from datetime import datetime # File for captured image filename = './scenes/photo.png' # Camera settimgs cam_width = 1280 cam_height = 480 # Final image capture settings scale_ratio = 0.5 # Camera resolution height must be dividable by 16, and width by 32 cam_width = int((cam_width+31)/32)*32 cam_height = int((cam_height+15)/16)*16 print (\"Used camera resolution: \"+str(cam_width)+\" x \"+str(cam_height)) # Buffer for captured image settings img_width = int (cam_width * scale_ratio) img_height = int (cam_height * scale_ratio) capture = np.zeros((img_height, img_width, 4), dtype=np.uint8) print (\"Scaled image resolution: \"+str(img_width)+\" x \"+str(img_height)) # Initialize the camera camera = PiCamera(stereo_mode='side-by-side',stereo_decimate=False) camera.resolution=(cam_width, cam_height) camera.framerate = 20 camera.hflip = True t2 = datetime.now() counter = 0 avgtime = 0 # Capture frames from the camera for frame in camera.capture_continuous(capture, format=\"bgra\", use_video_port=True, resize=(img_width,img_height)): counter+=1 t1 = datetime.now() timediff = t1-t2 avgtime = avgtime + (timediff.total_seconds()) cv2.imshow(\"pair\", frame) key = cv2.waitKey(1) &amp; 0xFF t2 = datetime.now() # if the `q` key was pressed, break from the loop and save last image if key == ord(\"q\") : avgtime = avgtime/counter print (\"Average time between frames: \" + str(avgtime)) print (\"Average FPS: \" + str(1/avgtime)) if (os.path.isdir(\"./scenes\")==False): os.makedirs(\"./scenes\") cv2.imwrite(filename, frame) break",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using a pi cam with a jetson nano. This is the result. https://i.stack.imgur.com/mAShR.jpg Any ideas on how to make it better, so it detects all the dots and not just some of them. Can't post the full code. So I shortened it down # Blur image blur=cv2.medianBlur(gray,5) cv2.imshow(\"blur\",blur) cv2.moveWindow(\"blur\",0,dispH) # Convert back to color ColorBlur = cv2.cvtColor(blur, cv2.COLOR_GRAY2BGR) cv2.imshow(\"ColorBlur\",ColorBlur) cv2.moveWindow(\"ColorBlur\",dispW,dispH) # Detect cirkels circles = cv2.HoughCircles(blur,cv2.HOUGH_GRADIENT,1,15,param1=100,param2=30,minRadius=3, maxRadius=30) circles = np.uint16(np.around(circles)) N=0 # Count cirkels for i in circles[0, :]: # Outer Circel cv2.circle(diceTray, (i[0],i[1]),i[2],(255,0,0),-1) N = N +1 print(N)",
        "answers": [],
        "votes": []
    },
    {
        "question": "pip3 install --upgrade tensorflowI tried installing tensorflow in my virtual environment and kept on running into this error. I have the necessary prerequisites for pip3(pip 21.0.1) and python3(Python 3.7.9) version and cannot understand why this error is occurring. Any inputs to solve this error would be very much appreciated. pip3 install --upgrade tensorflow ERROR: Could not find a version that satisfies the requirement tensorflow ERROR: No matching distribution found for tensorflow Error message",
        "answers": [
            [
                "pip3 install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.8.0-py3-none-any.whl This line worked for me but then this error popped up while checking for the tensorflow version python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\" Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; File \"/home/nvidia/projects/hand_gesture/venv/lib/python3.7/site-packages/tensorflow/__init__.py\", line 24, in &lt;module&gt; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import File \"/home/nvidia/projects/hand_gesture/venv/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in &lt;module&gt; from tensorflow.python import pywrap_tensorflow File \"/home/nvidia/projects/hand_gesture/venv/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in &lt;module&gt; from tensorflow.python.pywrap_tensorflow_internal import * File \"/home/nvidia/projects/hand_gesture/venv/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 114 def TFE_ContextOptionsSetAsync(arg1, async): ^ SyntaxError: invalid syntax error terminal description"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Im using raspberry pi camera v2 on jetson nano. It works fine using gstreamer with python but trying to run it through javascrip wont work. This is some of my approaches: APPROACH 1: With navigator: &lt;body&gt; &lt;div id=\"container\"&gt; &lt;video autoplay=\"true\" id=\"videoElement\"&gt;&lt;/video&gt; &lt;/div&gt; &lt;!--&lt;script type=\"module\" src=\"./picam.js\"&gt;&lt;/script&gt;--&gt; &lt;script&gt; var video = document.querySelector(\"#videoElement\"); if (navigator.mediaDevices.getUserMedia) { navigator.mediaDevices.getUserMedia({ video: true }) .then(function (stream) { video.srcObject = stream; }) } &lt;/script&gt; &lt;/body&gt; With response: (index):1 Uncaught (in promise) DOMException: Requested device not found APPROACH 2: Using: https://www.npmjs.com/package/pi-camera-connect &lt;body&gt; &lt;div id=\"container\"&gt; &lt;video autoplay=\"true\" id=\"videoElement\"&gt;&lt;/video&gt; &lt;/div&gt; &lt;script&gt; const { StreamCamera, Codec } = require(\"pi-camera-connect\"); const streamCamera = new StreamCamera({ codec: Codec.H264 }); const writeStream = fs.createWriteStream(\"video-stream.h264\"); const videoStream = streamCamera.createStream(); videoStream.pipe(writeStream); streamCamera.startCapture().then(() =&gt; { setTimeout(() =&gt; streamCamera.stopCapture(), 5000); }); &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; Uncaught ReferenceError: require is not defined APPROACH 3: &lt;body&gt; &lt;div id=\"container\"&gt; &lt;video autoplay=\"true\" id=\"videoElement\"&gt;&lt;/video&gt; &lt;/div&gt; &lt;script type=\"module\"&gt; import { StreamCamera, Codec } from \"pi-camera-connect\"; import * as fs from \"fs\"; // Capture 5 seconds of H264 video and save to disk const runApp = async () =&gt; { const streamCamera = new StreamCamera({ codec: Codec.H264 }); const videoStream = streamCamera.createStream(); const writeStream = fs.createWriteStream(\"video-stream.h264\"); videoStream.pipe(writeStream); await streamCamera.startCapture(); await new Promise(resolve =&gt; setTimeout(() =&gt; resolve(), 5000)); await streamCamera.stopCapture(); }; runApp(); &lt;/script&gt; &lt;/body&gt; Error: Uncaught TypeError: Failed to resolve module specifier \"pi-camera-connect\". Anyone successfully run raspberry pi camera on jetson nano with javascript? This python approach works fine though: import cv2 print(cv2.__version__) dispW=640 dispH=480 flip=2 camSet='nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3264, height=2464, format=NV12, framerate=21/1 ! nvvidconv flip-method='+str(flip)+' ! video/x-raw, width='+str(dispW)+', height='+str(dispH)+', format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink' cam= cv2.VideoCapture(camSet) while True: ret, frame = cam.read() cv2.imshow('nanoCam',frame) if cv2.waitKey(1)==ord('q'): break cam.release() cv2.destroyAllWindows()",
        "answers": [],
        "votes": []
    },
    {
        "question": "Okay... So I'm trying to run this repo on my jetson nano 2gb... I followed this official documentation for installation for tensorflow on jetson nano, while I followed https://www.tensorflow.org/install, this official documentation to run tensorflow on windows.. So after setting the dependencies on windows...the started to run smoothly with no problem. But after setting the dependencies on jetson nano... the code showed this error.. python3 object_tracker.py --weights ./checkpoints/yolov4-tiny-416 --model yolov4 --video 0 --tiny --info Traceback (most recent call last): File \"object_tracker.py\", line 238, in &lt;module&gt; app.run(main) File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 303, in run _run_main(main, args) File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main sys.exit(main(argv)) File \"object_tracker.py\", line 73, in main saved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING]) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 603, in load return load_internal(export_dir, tags, options) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 633, in load_internal ckpt_options) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 131, in __init__ self._restore_checkpoint() File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 330, in _restore_checkpoint load_status = saver.restore(variables_path, self._checkpoint_options) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\", line 1320, in restore checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\", line 209, in restore restore_ops = trackable._restore_from_checkpoint_position(self) # pylint: disable=protected-access File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\", line 914, in _restore_from_checkpoint_position tensor_saveables, python_saveables)) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\", line 297, in restore_saveables validated_saveables).restore(self.save_path_tensor, self.options) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/functional_saver.py\", line 340, in restore restore_ops = restore_fn() File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/functional_saver.py\", line 316, in restore_fn restore_ops.update(saver.restore(file_prefix, options)) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/functional_saver.py\", line 111, in restore restored_tensors, restored_shapes=None) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 125, in restore restored_tensor = array_ops.identity(restored_tensor) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper return target(*args, **kwargs) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 287, in identity ret = gen_array_ops.identity(input, name=name) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3992, in identity _ops.raise_from_not_ok_status(e, name) File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status six.raise_from(core._status_to_exception(e.code, message), None) File \"&lt;string&gt;\", line 3, in raise_from tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity] So I thought it could be a memory issue..so I ran tegrastats to check the status of the RAM and the SWAP memory... Here is what tegrastats logged.. RAM 453/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 3MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@28.5C CPU@31C PMIC@100C GPU@30.5C AO@39C thermal@30.75C RAM 453/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 3MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@28.5C CPU@31C PMIC@100C GPU@30.5C AO@39C thermal@30.75C RAM 453/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 3MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@28.5C CPU@31C PMIC@100C GPU@30.5C AO@39C thermal@30.75C RAM 466/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 3MB) CPU [6%@1224,39%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@28.5C CPU@31.5C PMIC@100C GPU@30.5C AO@39.5C thermal@31.25C RAM 476/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 3MB) CPU [10%@1224,33%@1224,1%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@28.5C CPU@31.5C PMIC@100C GPU@30.5C AO@39C thermal@30.75C RAM 480/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 3MB) CPU [26%@1224,11%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@28.5C CPU@31C PMIC@100C GPU@30.5C AO@39C thermal@30.75C RAM 498/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 3MB) CPU [8%@1479,33%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@28.5C CPU@31C PMIC@100C GPU@30.5C AO@39.5C thermal@30.75C RAM 512/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 3MB) CPU [31%@1479,43%@1479,1%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@28.5C CPU@32C PMIC@100C GPU@30.5C AO@39.5C thermal@30.75C RAM 525/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 3MB) CPU [11%@1479,67%@1479,1%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@31.5C PMIC@100C GPU@31C AO@39C thermal@31.5C RAM 539/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 3MB) CPU [8%@1479,47%@1479,11%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@32C PMIC@100C GPU@31C AO@39.5C thermal@31C RAM 552/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [11%@1224,53%@1224,1%@1224,2%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@28.5C CPU@31.5C PMIC@100C GPU@30.5C AO@39.5C thermal@31.25C RAM 570/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [8%@1479,67%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@31.5C PMIC@100C GPU@31C AO@39.5C thermal@31.25C RAM 583/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [4%@1479,70%@1479,7%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@31.5C PMIC@100C GPU@31C AO@39.5C thermal@31.5C RAM 593/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [11%@1479,48%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@32C PMIC@100C GPU@31C AO@39.5C thermal@31.25C RAM 601/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [8%@1224,32%@1224,14%@1224,18%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@31.5C PMIC@100C GPU@31.5C AO@39.5C thermal@31.5C RAM 607/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [10%@1224,5%@1224,5%@1224,1%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@31.5C PMIC@100C GPU@31C AO@39.5C thermal@31.5C RAM 626/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [21%@1224,8%@1224,10%@1224,3%@1224] EMC_FREQ 0% GR3D_FREQ 2% PLL@29C CPU@31.5C PMIC@100C GPU@28.5C AO@39.5C thermal@31C RAM 670/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [12%@1479,5%@1479,21%@1479,6%@1479] EMC_FREQ 0% GR3D_FREQ 27% PLL@29C CPU@31.5C PMIC@100C GPU@29C AO@39.5C thermal@30.25C RAM 741/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [20%@1479,5%@1479,20%@1479,14%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@31.5C PMIC@100C GPU@29C AO@39.5C thermal@30.25C RAM 820/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [23%@1479,27%@1479,20%@1479,15%@1479] EMC_FREQ 0% GR3D_FREQ 52% PLL@29C CPU@32C PMIC@100C GPU@28.5C AO@40C thermal@30.75C RAM 920/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [17%@1479,7%@1479,45%@1479,20%@1479] EMC_FREQ 0% GR3D_FREQ 1% PLL@29C CPU@32C PMIC@100C GPU@29C AO@39.5C thermal@30.5C RAM 1020/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [18%@1224,6%@1224,70%@1224,9%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@32C PMIC@100C GPU@29.5C AO@39.5C thermal@30.75C RAM 1036/1980MB (lfb 22x4MB) SWAP 121/11230MB (cached 7MB) CPU [11%@1479,20%@1479,29%@1479,10%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@31.5C PMIC@100C GPU@31C AO@39.5C thermal@31.5C RAM 1093/1980MB (lfb 12x4MB) SWAP 121/11230MB (cached 7MB) CPU [72%@1479,0%@1479,1%@1479,3%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@31.5C PMIC@100C GPU@31.5C AO@40C thermal@30.25C RAM 1106/1980MB (lfb 8x4MB) SWAP 121/11230MB (cached 7MB) CPU [96%@1479,0%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@31.5C PMIC@100C GPU@31C AO@39.5C thermal@30.25C RAM 1116/1980MB (lfb 6x4MB) SWAP 121/11230MB (cached 7MB) CPU [100%@1479,0%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@32C PMIC@100C GPU@31C AO@40C thermal@31.25C RAM 1122/1980MB (lfb 4x4MB) SWAP 121/11230MB (cached 7MB) CPU [100%@1479,0%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@32C PMIC@100C GPU@31C AO@39.5C thermal@31.5C RAM 1131/1980MB (lfb 1x4MB) SWAP 121/11230MB (cached 7MB) CPU [69%@1479,31%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@39.5C thermal@31.75C RAM 1137/1980MB (lfb 1x2MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@32.5C PMIC@100C GPU@31.5C AO@40C thermal@31.75C RAM 1147/1980MB (lfb 12x1MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@32.5C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1152/1980MB (lfb 9x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@32C PMIC@100C GPU@31.5C AO@40C thermal@31.75C RAM 1161/1980MB (lfb 4x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@39.5C thermal@32.25C RAM 1171/1980MB (lfb 19x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@33C PMIC@100C GPU@31C AO@40C thermal@32.25C RAM 1178/1980MB (lfb 10x512kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@32.5C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1184/1980MB (lfb 3x512kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29C CPU@33C PMIC@100C GPU@31C AO@39.5C thermal@32C RAM 1189/1980MB (lfb 3x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,3%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32C RAM 1204/1980MB (lfb 3x512kB) SWAP 121/11230MB (cached 7MB) CPU [2%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1204/1980MB (lfb 3x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@32C PMIC@100C GPU@31.5C AO@40C thermal@31.75C RAM 1216/1980MB (lfb 19x256kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,1%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@39.5C thermal@32.25C RAM 1223/1980MB (lfb 19x256kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1218/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [56%@1479,41%@1479,3%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32C RAM 1221/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33.5C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1224/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,98%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1229/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33C PMIC@100C GPU@32C AO@40C thermal@32.25C RAM 1223/1980MB (lfb 17x256kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,1%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@32.5C PMIC@100C GPU@31.5C AO@40C thermal@32.5C RAM 1230/1980MB (lfb 17x256kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@31.75C RAM 1230/1980MB (lfb 17x256kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1225/1980MB (lfb 16x256kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1234/1980MB (lfb 13x256kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1230/1980MB (lfb 15x256kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,1%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1225/1980MB (lfb 16x256kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.5C RAM 1226/1980MB (lfb 16x256kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33.5C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1229/1980MB (lfb 16x256kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1232/1980MB (lfb 16x256kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1235/1980MB (lfb 13x256kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40.5C thermal@32.5C RAM 1229/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,1%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33.5C PMIC@100C GPU@31.5C AO@40.5C thermal@32.25C RAM 1230/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@32.5C PMIC@100C GPU@31.5C AO@40C thermal@31.75C RAM 1231/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40.5C thermal@31.75C RAM 1233/1980MB (lfb 6x512kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33C PMIC@100C GPU@32C AO@40.5C thermal@32.5C RAM 1236/1980MB (lfb 3x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40C thermal@32.75C RAM 1239/1980MB (lfb 14x256kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33C PMIC@100C GPU@32C AO@40C thermal@32.25C RAM 1236/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,1%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40.5C thermal@32.5C RAM 1238/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33C PMIC@100C GPU@32C AO@40.5C thermal@32.5C RAM 1246/1980MB (lfb 1x2MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,1%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40.5C thermal@32.5C RAM 1246/1980MB (lfb 1x2MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40.5C thermal@32.75C RAM 1248/1980MB (lfb 1x2MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33.5C PMIC@100C GPU@31.5C AO@40.5C thermal@32.75C RAM 1248/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [2%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40.5C thermal@32.25C RAM 1253/1980MB (lfb 3x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40C thermal@32.5C RAM 1244/1980MB (lfb 1x2MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,2%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@34C PMIC@100C GPU@32C AO@41C thermal@32.75C RAM 1247/1980MB (lfb 3x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40.5C thermal@32.75C RAM 1247/1980MB (lfb 3x1MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@33C PMIC@100C GPU@31.5C AO@40C thermal@32.25C RAM 1249/1980MB (lfb 2x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40.5C thermal@32.25C RAM 1254/1980MB (lfb 4x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40.5C thermal@33C RAM 1246/1980MB (lfb 1x2MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,1%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@41C thermal@32.5C RAM 1251/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40.5C thermal@32.75C RAM 1256/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@41C thermal@33C RAM 1253/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,2%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40.5C thermal@32.75C RAM 1254/1980MB (lfb 2x1MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@33.5C PMIC@100C GPU@32.5C AO@40.5C thermal@33C RAM 1256/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@41C thermal@33C RAM 1261/1980MB (lfb 2x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@33.5C PMIC@100C GPU@32.5C AO@41C thermal@33C RAM 1265/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,2%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@34C PMIC@100C GPU@32C AO@41C thermal@33C RAM 1269/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@34C PMIC@100C GPU@32C AO@40.5C thermal@33C RAM 1264/1980MB (lfb 2x512kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,1%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@40.5C thermal@33C RAM 1272/1980MB (lfb 2x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,1%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@34C PMIC@100C GPU@32.5C AO@41C thermal@33C RAM 1272/1980MB (lfb 2x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@34C PMIC@100C GPU@32C AO@41C thermal@33C RAM 1277/1980MB (lfb 2x512kB) SWAP 121/11230MB (cached 7MB) CPU [2%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@34C PMIC@100C GPU@32.5C AO@41C thermal@32.75C RAM 1279/1980MB (lfb 2x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,1%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@41C thermal@32.5C RAM 1280/1980MB (lfb 2x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@33.5C PMIC@100C GPU@32.5C AO@41C thermal@32.25C RAM 1283/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,1%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@34C PMIC@100C GPU@32.5C AO@41C thermal@32.75C RAM 1283/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@41C thermal@33C RAM 1291/1980MB (lfb 2x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@32C AO@41C thermal@33C RAM 1298/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,100%@1479,0%@1479,1%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@34C PMIC@100C GPU@32C AO@41C thermal@33C RAM 1296/1980MB (lfb 1x2MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,100%@1479,0%@1479,1%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@34C PMIC@100C GPU@32C AO@41C thermal@33.25C RAM 1299/1980MB (lfb 1x2MB) SWAP 121/11230MB (cached 7MB) CPU [3%@1224,84%@1224,1%@1224,1%@1224] EMC_FREQ 0% GR3D_FREQ 2% PLL@30C CPU@33C PMIC@100C GPU@30C AO@41C thermal@33C RAM 1302/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [4%@1479,6%@1479,85%@1479,7%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@34C PMIC@100C GPU@32.5C AO@41C thermal@31.75C RAM 1306/1980MB (lfb 1x1MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,0%@1479,100%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@34C PMIC@100C GPU@32.5C AO@41C thermal@32.75C RAM 1307/1980MB (lfb 5x1MB) SWAP 121/11230MB (cached 7MB) CPU [2%@1479,0%@1479,100%@1479,5%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@34C PMIC@100C GPU@32C AO@41C thermal@33C RAM 1319/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [31%@1224,29%@1224,25%@1224,3%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@33C PMIC@100C GPU@32.5C AO@41C thermal@32.75C RAM 1337/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [7%@1224,5%@1224,0%@1224,5%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@41C thermal@32.25C RAM 1337/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@41C thermal@32.25C RAM 1337/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@41C thermal@32.25C RAM 1337/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@40.5C thermal@32.25C RAM 1337/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@41C thermal@32.25C RAM 1337/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@40.5C thermal@32.25C RAM 1337/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@41C thermal@32.25C RAM 1337/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@32.5C PMIC@100C GPU@31.5C AO@41C thermal@32.25C RAM 1337/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@40.5C thermal@32.25C RAM 1337/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@29.5C CPU@32.5C PMIC@100C GPU@31.5C AO@41C thermal@32C RAM 1338/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [69%@1479,2%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@32.5C PMIC@100C GPU@32.5C AO@41C thermal@32.5C RAM 1338/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [100%@1479,0%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@33C PMIC@100C GPU@32.5C AO@41C thermal@32.5C RAM 1338/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [100%@1479,0%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33C PMIC@100C GPU@32C AO@41C thermal@32.25C RAM 1338/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [100%@1479,0%@1479,0%@1479,0%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33C PMIC@100C GPU@32C AO@41C thermal@32.5C RAM 1338/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [95%@1479,4%@1479,0%@1479,1%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@33C PMIC@100C GPU@32.5C AO@41C thermal@32.5C RAM 1338/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,0%@1479,0%@1479,100%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33C PMIC@100C GPU@32C AO@41C thermal@32.75C RAM 1338/1980MB (lfb 1x512kB) SWAP 121/11230MB (cached 7MB) CPU [0%@1479,0%@1479,0%@1479,100%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33C PMIC@100C GPU@32C AO@41C thermal@32.75C RAM 1320/1980MB (lfb 7x512kB) SWAP 121/11230MB (cached 7MB) CPU [1%@1479,0%@1479,0%@1479,100%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@33C PMIC@100C GPU@32.5C AO@41C thermal@32.75C RAM 1090/1980MB (lfb 7x4MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1224,0%@1224,0%@1224,100%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30.5C CPU@33C PMIC@100C GPU@32.5C AO@41C thermal@32.75C RAM 342/1980MB (lfb 18x4MB) SWAP 121/11230MB (cached 7MB) CPU [6%@1479,9%@1479,17%@1479,1%@1479] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33.5C PMIC@100C GPU@30.5C AO@41C thermal@32.25C RAM 342/1980MB (lfb 18x4MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@40.5C thermal@32.25C RAM 342/1980MB (lfb 18x4MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@40.5C thermal@32.25C RAM 342/1980MB (lfb 18x4MB) SWAP 121/11230MB (cached 7MB) CPU [0%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@32.5C PMIC@100C GPU@32C AO@41C thermal@32.25C RAM 342/1980MB (lfb 18x4MB) SWAP 121/11230MB (cached 7MB) CPU [1%@1224,0%@1224,0%@1224,0%@1224] EMC_FREQ 0% GR3D_FREQ 0% PLL@30C CPU@33C PMIC@100C GPU@32C AO@41C thermal@32.25C I can't understand what could be the possible cause of this.. Things I have tried: Clone a different repo for object detection, same result Changed my tensorflow version to 2.3.0 as requirement for the code..no change Trained the model and ran it through tensorRT...same result Any help would be appreciated... Thanks.. EDIT: Title and tag change..",
        "answers": [
            [
                "Just add this line at the starting of the object_tracker.py file and before importing tensorflow: import os os.environ['CUDA_VISIBLE_DEVICES'] = '1' I hope this solves the error for anyone who faces this error.."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am doing a project where I have to install the jetson nano board to a train and detect the gps location of it. But I cannot find a proper solution to include gps module to the jetson board. Please it would be great if there is anyone who can help me with it. Thanks in advance!",
        "answers": [
            [
                "So from what I know exactly the best way to do so would be to get an actual gps module for the jetson nano (do not know which exact model but I think we can go with the popular 4g model) and an example of one is https://www.waveshare.com/sim7600g-h-4g-for-jetson-nano.htm but you can also do something that isn't so accurate. Little bash script, hope it helps. So first I would ssh into your device, you can run ifconfig to get a little bit of an idea. It by default would be wlan0 for the wirless module and eth0 for the wired connection. Let's make the file touch nanoloc.sh &amp;&amp; sudo chmod +x nanoloc.sh Then just run this little script here: #!/bin/bash #define what shell we want to use above format=\"json\" #xml or json user=\"\" #add your current user nano_ip=\"$(curl ifconfig.co)\" ip_locaddr=https://freegeoip.app/$format/$nano_ip data_file=/mnt/c/Users/$user/output.$format #change this to your expected path touch data_file #make the file echo data_file #make sure we have the right one curl \"$ip_locaddr\" &gt; data_file #write it out cat data_file #let's see what it says Now you should be able to see what it outputs when you run ./nanoloc.sh Hope this helps, also am running this from WSL (you can see from the path where it's mounted) and tested on Linux."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Is it possible to run data science tools like RAPIDS on a JETSON NANO? After some searching, I am still not very clear... also, if it does, will data analysis run faster on it than on a CPU? Any insights will be appreciated. Thanks.",
        "answers": [
            [
                "No, sadly, the Jetson Nano doesn't support RAPIDS. The Nano's GPU uses an older architecture GPU, Maxwell, which RAPIDS does not support (Pascal or better). The TX2, Jetson NX, and the Xaviers, which have a compatible GPU architecture, are the only ones that community members have seen success with. I know that there or some community members attempting a port of one or two of the RAPIDS libraries to the Nano, but that is a personal effort by them and YMMV."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am currently working on my Nvidia Jetson Nano 4GB following this guide. I try to install tensorflow but a few moments later I have THAT 2500lines error: 'python version don't match your environment' . Okay, I want to see my actual version and I have a different answer for each request. What should I do? Python version screen an error screen",
        "answers": [
            [
                "just write python in terminal to see your python version if that is what you are asking?"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Here's my situation: I have an x86 machine that I need to create a Docker image that is compatible with Jetson Nano. I don't have the Jetson nano but I want to test whether or not my Docker image works properly and I need to test a few pieces of code that run a CUDA binary. So I set up my nano docker image nvcr.io/nvidia/l4t-pytorch:r32.4.4-pth1.6-py3 and ran it on my x86 machine. Then I entered the container in interactive mode and started a python interpreter. Then I tried to import PyTorch Python 3.6.9 (default, Oct 8 2020, 12:12:24) [GCC 8.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import torch And it failed. My output was Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/usr/local/lib/python3.6/dist-packages/torch/__init__.py\", line 188, in &lt;module&gt; _load_global_deps() File \"/usr/local/lib/python3.6/dist-packages/torch/__init__.py\", line 141, in _load_global_deps ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL) File \"/usr/lib/python3.6/ctypes/__init__.py\", line 348, in __init__ self._handle = _dlopen(self._name, mode) OSError: libcurand.so.10: cannot open shared object file: No such file or directory My docker image matches the CUDA binaries I should have installed on my host machine. I need to get my code to run but I'm not sure why I continue to have CUDA binary problems even when I'm using an NVIDIA sanctioned docker image. The CUDA binaries should be pre-loaded on the image.",
        "answers": [
            [
                "So the problem here with the current image: nvcr.io/nvidia/l4t-pytorch:r32.4.4-pth1.6-py3 is that accessing the NVIDIA driver on a host x86 machine isn't supported. If you go to the official nvidia-docker github you will see a section for building jetson docker containers on an x86 machine. Then notice right at the bottom it says Known limitation: Unfortunately you won\u2019t be able to run any binary that calls into the NVIDIA driver on the x86 host. Meaning the task of verifying any code that calls into an NVIDIA binary is impossible. Only route is to create the best possible container that you can, and then verify that the code works properly on a physical Jetson device."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Environment :- Raspi v2 camera, Jetson nano board, Ubuntu 18.04 I started with nvarguscamerasrc and it's working :- gst-launch-1.0 nvarguscamerasrc sensor_mode=0 ! 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12'! nvegltransform ! nveglglessink -e and then I tried running these inputs :- gst-launch-1.0 v4l2src device=/dev/video0 ! 'video/x-h264, width=3280, height=2464' ! filesink and also :- gst-launch-1.0 v4l2src device=/dev/video0 ! 'video/x-raw, width=640, height=480' ! filesink and got output as :- Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... ERROR: from element /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: Internal data stream error. Additional debug info: gstbasesrc.c(3055): gst_base_src_loop (): /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: streaming stopped, reason not-negotiated (-4) ERROR: pipeline doesn't want to preroll. Setting pipeline to PAUSED ... Setting pipeline to READY ...",
        "answers": [],
        "votes": []
    },
    {
        "question": "Im working on this project : https://www.hackster.io/jonmendenhall/jetson-nano-search-and-rescue-ai-uav-9ca547 At some point I will need to mount my camera (waveshare ; IMX219-77IR) on top of the drone and I would like to use vlc on Windows or Linux outside of nomachine (because I have installed nomachine server on the nano and the client on windows and because it will run in headless mode),to display what the camera sees when the drone is flying. For this reason I\u2019m trying to configure a gstreamer with RTSP to start a streaming server on the Ubuntu 18.04 that I have installed on the jetson nano. Below u can see what are the commands that I have issued : $ ./test-launch \"videotestsrc ! nvvidconv ! nvv4l2h264enc ! h264parse ! rtph264pay name=pay0 pt=96\" And on the same Jetson Nano, I opened another console where I ran this pipeline to decode the RTSP stream: gst-launch-1.0 uridecodebin uri=rtsp://127.0.0.1:8554/test ! nvoverlaysink I see this picture : The picture is from videotestsrc plugin. I would like to replace videotestsrc with my video source,but I don't know how to do that. I tried these combinations,but none of them worked : ./test-launch \"v4l2src device=/dev/video0 ! nvvidconv ! nvv4l2h264enc ! h264parse ! queue ! rtph264pay name=pay0 pt=96\" ./test-launch \"device=/dev/video0 ! nvvidconv ! nvv4l2h264enc ! h264parse ! queue ! rtph264pay name=pay0 pt=96\" but the error is still the same : Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... Progress: (open) Opening Stream Progress: (connect) Connecting to rtsp://127.0.0.1:8554/test Progress: (open) Retrieving server options Progress: (open) Retrieving media info ERROR: from element /GstPlayBin:playbin0/GstURIDecodeBin:uridecodebin0/GstRTSPSrc:source: Could not read from resource. Additional debug info: gstrtspsrc.c(5917): gst_rtsp_src_receive_response (): /GstPlayBin:playbin0/GstURIDecodeBin:uridecodebin0/GstRTSPSrc:source: Could not receive message. (Timeout while waiting for server response) ERROR: pipeline doesn't want to preroll. Setting pipeline to PAUSED ... Setting pipeline to READY ... Setting pipeline to NULL ... Freeing pipeline ... but why ? I know for sure that my camera (model waveshare ; IMX219-77IR) created a device called /dev/video0 and I know for sure that it works,because this command is able to show my face on the screen : DISPLAY=:0.0 gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM), width=3280, height=2464, format=(string)NV12, framerate=(fraction)20/1' ! nvoverlaysink -e",
        "answers": [],
        "votes": []
    },
    {
        "question": "Sorry if my description is long and boring but I want to give you most important details to solve my problem. Recently I bought a Jetson Nano Developer Kit with 4Gb of RAM, finally!, and in order to get, which I consider, the best configuration for object detection I am following this guide made by Adrian Rosebrock from Pyimagesearch: https://www.pyimagesearch.com/2020/03/25/how-to-configure-your-nvidia-jetson-nano-for-computer-vision-and-deep-learning/ Date:March, 2020. A summary of this guide is the following: 1: Flash Jetson Pack 4.2 .img inside a microSD for Jetson Nano(mine is 32GB 'A' Class) 2: Once inserted on the Nano board, configure Ubuntu 18.04 and get rid of Libreoffice entirely to get more available space 3: Step #5: Install system-level dependencies( Including cmake, python3, and nano editor) 4: Update CMake (without any errors) 5: Install OpenCV system-level dependencies and other development dependencies 6: Set up Python virtual environments on your Jetson Nano( succesfully installed virtualenv and virtualenvwrapper without errors including the bash file edition with nano) 7: Create virtaul env with python 3 and install protobuf and libprotobuf to get an more efficient Tensorflow. Succesfully installed. It took an hour to finish, that's normal 8: Here comes the headbreaker: install numpy and cython inside this env and check it importing numpy library When I try to do this step I get: Illegal instruction(core dumped) as you can see in the image: [Error with Python3.6.9]: https://i.stack.imgur.com/rAZhm.png I said, well let's continue with this tutorial anyway: 9: Install Scipy v1.3.3: everything is ok with first three lines, but when I have to use python to execute the stup.py file, IT shows up again(not the clown). [Can't execute this line either]: https://i.stack.imgur.com/wFmnt.jpg Then I ran an experiment, I have created this \"p2cv4\" env with Python 2, installed numpy and tested it: [With Python 2]: https://i.stack.imgur.com/zCWif.png I can exit() whenever I want and execute other lines that use python So I concluded that is a python version issue. When I want to execute any python code, terminal ends the program with core dumping, apt-get or pip DO NOT show any errors. And I want to use python 3 because someday in the future a package or library will require python 3. For python 3 last version for the Jetson Nano is 3.6.9, and idk which version was currently active in March, 2020, like the one Adrian used at that time In other posts I read that this SIGILL appears when a package or library version like Numpy of TF is not friendly anymore with a specific old or low power CPU, like in this posts: Illegal hardware instruction when trying to import tensorflow, https://github.com/numpy/numpy/issues/9532 So I want to downgrade to a older python version like 3.6.5 or 3.5 but I can't find clear steps to do so in Ubuntu. I thinks this will fix this error and let me continue with configurations on the Jetson Nano. The pyimageseach guide uses Python 3.6 but it do not specifies if is last 3.6.9 or another. If is not python causing this error let me know. HELP please!",
        "answers": [
            [
                "The 'illegal instruction' could be due to an recent change in the getauxval(AT_HWCAP). An attempt to read cpuid information from /sys/devices in the event that getauxval did not succeed. (OpenBLAS PRs 2952 and 3004) export OPENBLAS_CORETYPE=ARMV8 (or whatever the actual hardware is) before launching python should hopefully get around this. For example: OPENBLAS_CORETYPE=ARMV8 python If you would like to make this export permanent, you should open your .bashrc file by typing on the terminal: nano ~/.bashrc Afterwards, just add \"export OPENBLAS_CORETYPE=ARMV8\" to the bottom of your .bashrc file, save/exit and reboot your system: export OPENBLAS_CORETYPE=ARMV8 for more details look at: https://github.com/numpy/numpy/issues/18131 https://www.reddit.com/r/JetsonNano/comments/ktcyoh/illegal_instructioncore_dumped_error_on_jetson/"
            ],
            [
                "Installing the correct numpy version helped me resolve this issue. Whenever i imported torch, cv2 or torchvision, I got the error saying Illegal instruction(core dumped). Use the command pip install numpy==1.19.4 and I hope the error goes."
            ],
            [
                "I had this very same problem following the same guide. BTW, in this scenario, numpy worked just fine in python when NOT in a virtualenv. GDB pointed to a problem in libopenblas. My solution was to start from scratch with a fresh image of jetson-nano-4gb-jp441-sd-card-image.zip and repeat that guide without using virtualenv. More than likely you are the sole developer on that Nano and can live without virtualenv. I have followed these guides with success: https://qengineering.eu/install-opencv-4.5-on-jetson-nano.html Skip the virtualenv portions https://www.pyimagesearch.com/2019/05/06/getting-started-with-the-nvidia-jetson-nano/ I found this to also be required at this point: \"..install the official Jetson Nano TensorFlow by..\" ln -s /usr/include/locale.h /usr/include/xlocale.h Once I made that symbolic link I was able to proceed with the rest of the guide (minus virtualenv) and I managed to not break numpy. This is not ideal, but I hope it helps."
            ],
            [
                "I thinks this a bug with Jetson Nano B01 model. Yesterday I realized that my Nano is a different model than the one Adrian is showing in his guide(A02). Thank you doommonkey to lead me to a different guide that looks pretty much to the pyimagesearch tutorial and more recent one (Dec, 2020), I really appreciate that. I will test it today and if I can build my opencv to functional including Tensorflow. I think this is a half solved problem. It's a shame that virtualenv is giving this awful error for python 3.6.9. In th is particular model. I tried with venv module too and it can't even install numpy so no virtual env for now"
            ],
            [
                "Try this, It worked for me. I tested it with pyenv environment with python 3.7.9 and gcc-8.4 on Jetson nano (the very first version). Just remember because it compiles everything, it's gonna take a lot longer than just downloading the binaries. (because of --no-binary) python3 -m pip install -U numpy --no-cache-dir --no-binary numpy You can update your gcc, because default gcc is 7. I am not sure if it does not work with gcc-7. sudo apt install gcc-8 g++-8 sudo rm /usr/bin/gcc sudo ln -s /usr/bin/gcc-8 /usr/bin/gcc sudo rm /usr/bin/g++ sudo ln -s /usr/bin/g++-8 /usr/bin/g++ Now to confirm, you can check the version with gcc --version g++ --version"
            ],
            [
                "No you can do with the virtualenv: remove all numpy ref in the site packages of the virtual environement. upgrade numpy link numpy folder to venv numpy. Assuming your virtualenv is called ML: cd ~/.virtualenvs/ML/lib/python3.6/site-packages sudo rm -r numpy* #outside VE: deactivate sudo pip install --upgrade numpy #find path to numpy usig pysearchmethod : #use pip uninstall numpy, and answer NO, but note. path sudo pip uninstall numpy #gives me /home/pierre/.local/lib/python3.6/site-packages/numpy cd ~/.virtualenvs/ML/lib/python3.6/site-packages ln -s /home/pierre/.local/lib/python3.6/site-packages/numpy numpy Then you can test it: workon ML python -c 'import numpy' If you get stuff related to permission on /.cache/pip, you can try to change permission of the folder: sudo chown -R $USER ~/.cache/pip If this still doesn't work, try uninstalling numpy on the machine, then re installing it: sudo pip uninstall numpy #Yousof pip install -U numpy --no-cache-dir --no-binary numpy then go back to steps 1-&gt;3."
            ]
        ],
        "votes": [
            61.0000001,
            15.0000001,
            1e-07,
            1e-07,
            1e-07,
            -0.9999999
        ]
    },
    {
        "question": "I'm working with AI-Thermometer project using Nvidia Jeton Nano. The project is using Pi camera v2 for video capturing. Here's the command of showing video streams using Pi camera v2. gst-launch-1.0 nvarguscamerasrc sensor_mode=0 ! 'video/x-raw(memory:NVMM),width=3264, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=2 ! 'video/x-raw,width=960, height=720' ! nvvidconv ! nvegltransform ! nveglglessink -e I want to use the normal USB webcam (such as Logitech c930) instead of Pi camera v2. To do so, I need to stream the USB webcam data using GStreamer in the same way as above pipeline commands. I installed v4l-utils on Ubuntu of Jetson Nano. And tried like this, gst-launch-1.0 v4l2src device=\"/dev/video0\" ! 'video/x-raw(memory:NVMM),width= ... , but it gave a warning and didn't work. How can I show video streams from webcam?",
        "answers": [
            [
                "There should not quotes around the device parameter i.e. device=/dev/video0. If the error persists, then its probably something else."
            ],
            [
                "gst-launch-1.0 v4l2src device=\"/dev/video0\" ! \\ \"video/x-raw, width=640, height=480, format=(string)YUY2\" ! \\ xvimagesink -e"
            ]
        ],
        "votes": [
            1e-07,
            -0.9999999
        ]
    },
    {
        "question": "Im building an application intended to run in producion at a museum on jetson nano. First my idea was to run it on the developer kit but I read that it might be a bad idea since it is not intended for production. But Im still wondering if I can use the developer board? Or will it not survive everyday use? The module comes without carrier board so in the case I am using that one I need to build something myself or buy some addon for connecting usb hdmi etc. So that makes the developer kit more convenient. I also need the GPIO which Im not sure you can access on the module? Has anyone experience of running jetson developer kit on an everydaybasis? Did you experience any problems?",
        "answers": [
            [
                "I have had experience with a burnt-out Nano Developer Kit after daily usage. However, I have had other Nano Developer Kits which still work fine. So I don't think you can conclude anything from that. The reason why the Jetson Module is meant for commercial purposes is that it comes with a warranty and is meant to be integrated with Industry-Grade Components. You decide based on your affordability to replace the Developer Kit in case something happens. Regarding the GPIO, I am not sure if you can conclude that. They are several companies listed on Nivida's website that build customized carrier boards. Decide based on your affordability."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am running YOLO v3 on Jetson Xavier, I am using python and OpenCV DNN module for inferencing, and it is pretty slow and seems as if it is not effective for my application. So I am looking for another detection model that can run in a real-time manner on Xaviar. It has to have reasonable accuracy, so I do not consider running YOLO-tiny. Any suggestion Please? What model is capable of running real-time in a Xaviar device?",
        "answers": [],
        "votes": []
    },
    {
        "question": "when i try pip3 install h5py on the ubuntu 18.04 from nvidia for the nano jetson i get this error ERROR: Command errored out with exit status 1: /home/mimus/virtualenv/venv/bin/python3 /home/mimus/virtualenv/venv/lib/python3.6/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-sr8cr1rz/normal --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'numpy==1.14.5; python_version == \"3.7\"' 'numpy==1.17.5; python_version == \"3.8\"' pkgconfig 'Cython&gt;=0.29.14; python_version &gt;= \"3.8\"' 'numpy==1.12; python_version == \"3.6\"' 'numpy==1.19.3; python_version &gt;= \"3.9\"' 'Cython&gt;=0.29; python_version &lt; \"3.8\"' Check the logs for full command output. i have already installed cython numpy and six but im having the same problem, hope you can help me",
        "answers": [
            [
                "I got h5py to install on the Jetson Nano by downgrading it to version 2.10.0 instead of the default 3.x.x."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Whenever importing torch2trt on a JetRacer with a Jetson Nano, it throws an AttributeError. Is there a way to fix this? I've installed the JetRacer Image from Waveshare: jetcard_v0p0p0 and manually installed torch2trt from here: git clone https://github.com/NVIDIA-AI-IOT/torch2trt cd torch2trt sudo python setup.py install Running from torch2trt import torch2trt gives the following error: --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-4-3f7e7e224fb7&gt; in &lt;module&gt; ----&gt; 1 from torch2trt import torch2trt /usr/lib/python3.6/importlib/_bootstrap.py in _find_and_load(name, import_) /usr/lib/python3.6/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_) /usr/lib/python3.6/importlib/_bootstrap.py in _load_unlocked(spec) /usr/lib/python3.6/importlib/_bootstrap.py in _load_backward_compatible(spec) /usr/local/lib/python3.6/dist-packages/torch2trt-0.1.0-py3.6.egg/torch2trt/__init__.py in &lt;module&gt; 1 from .torch2trt import * ----&gt; 2 from .converters import * 3 import tensorrt as trt 4 5 /usr/lib/python3.6/importlib/_bootstrap.py in _find_and_load(name, import_) /usr/lib/python3.6/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_) /usr/lib/python3.6/importlib/_bootstrap.py in _load_unlocked(spec) /usr/lib/python3.6/importlib/_bootstrap.py in _load_backward_compatible(spec) /usr/local/lib/python3.6/dist-packages/torch2trt-0.1.0-py3.6.egg/torch2trt/converters/__init__.py in &lt;module&gt; 15 from .Linear import * 16 from .LogSoftmax import * ---&gt; 17 from .activation import * 18 from .adaptive_avg_pool2d import * 19 from .adaptive_max_pool2d import * /usr/lib/python3.6/importlib/_bootstrap.py in _find_and_load(name, import_) /usr/lib/python3.6/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_) /usr/lib/python3.6/importlib/_bootstrap.py in _load_unlocked(spec) /usr/lib/python3.6/importlib/_bootstrap.py in _load_backward_compatible(spec) /usr/local/lib/python3.6/dist-packages/torch2trt-0.1.0-py3.6.egg/torch2trt/converters/activation.py in &lt;module&gt; 1 from torch2trt.torch2trt import * 2 from torch2trt.module_test import add_module_test ----&gt; 3 from .unary import UnaryModule 4 5 /usr/lib/python3.6/importlib/_bootstrap.py in _find_and_load(name, import_) /usr/lib/python3.6/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_) /usr/lib/python3.6/importlib/_bootstrap.py in _load_unlocked(spec) /usr/lib/python3.6/importlib/_bootstrap.py in _load_backward_compatible(spec) /usr/local/lib/python3.6/dist-packages/torch2trt-0.1.0-py3.6.egg/torch2trt/converters/unary.py in &lt;module&gt; 71 72 @tensorrt_converter('torch.reciprocal') ---&gt; 73 @tensorrt_converter('torch.reciprocal_') 74 @tensorrt_converter('torch.Tensor.reciprocal') 75 @tensorrt_converter('torch.Tensor.reciprocal_') /usr/local/lib/python3.6/dist-packages/torch2trt-0.1.0-py3.6.egg/torch2trt/torch2trt.py in tensorrt_converter(method, is_real, enabled, imports) 587 module, module_name, qual_name = importlib.import_module(method.__module__), method.__module__, method.__qualname__ 588 --&gt; 589 method_impl = eval('copy.deepcopy(module.%s)' % qual_name) 590 591 def register_converter(converter): /usr/local/lib/python3.6/dist-packages/torch2trt-0.1.0-py3.6.egg/torch2trt/torch2trt.py in &lt;module&gt; AttributeError: module 'torch' has no attribute 'reciprocal_' I have already reinstalled torch2trt to no avail. Kind regards",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm running a pose estimation script on an NVIDIA Jetson Nano. It works fine on a short video I tried, but when I run it on a longer video I get the following error: (python3.6:24822): GStreamer-CRITICAL **: 23:27:53.556: gst_element_make_from_uri: assertion 'gst_uri_is_valid (uri)' failed (python3.6:24822): GStreamer-CRITICAL **: 23:27:53.570: gst_element_make_from_uri: assertion 'gst_uri_is_valid (uri)' failed [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (711) open OpenCV | GStreamer warning: Error opening bin: no source element for URI \"/home/fvpt/Desktop/tech\" [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created [ERROR:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap.cpp (392) open VIDEOIO(GSTREAMER): raised OpenCV exception: OpenCV(4.1.1) /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp:1392: error: (-215:Assertion failed) fps &gt; 0 in function 'open' Process finished with exit code 0 The only place i can find \"open\" in my script is where i load in the json file for the pose estimation. with open(*'path to json'*, 'r') as f: human_pose = json.load(f) I'm not sure what details are relevant to give - I'm running this on Ubuntu 18.04, using PyCharm but the same error happens even when I run the script from terminal. OpenCV version 4.1.1 Python version 3.6.9. It works when I run it on a video from an iphone. I created a video in davinci resolve with multiple videos from multiple sources, but it's a normal mp4, 720x576, 24 fps, nothing weird. Example code: import cv2 import PIL.Image, PIL.ImageFont import numpy as np import argparse import os.path parser = argparse.ArgumentParser(description='TensorRT pose estimation run') parser.add_argument('--model', type=str, default='resnet', help='resnet or densenet') parser.add_argument('--video', type=str, default='/home/fvpt/Desktop/tech project/videos/final.mp4', help='video file name') args = parser.parse_args() cap = cv2.VideoCapture(args.video) ret_val, img = cap.read() H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) fpsvid = cap.get(cv2.CAP_PROP_FPS) fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v') dir, filename = os.path.split(args.video) name, ext = os.path.splitext(filename) out_video = cv2.VideoWriter('/home/fvpt/Desktop/tech project/outputs/mrf%s_%s.mp4' % (args.model, name), fourcc, fpsvid, (W, H)) count = 0 count = 1 while cap.isOpened(): ret_val, dst = cap.read() if ret_val == False: print(\"Frame Read End\") break img = cv2.resize(dst, dsize=(640, 480), interpolation=cv2.INTER_AREA) pilimg = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB) pilimg = PIL.Image.fromarray(pilimg) array = np.asarray(pilimg, dtype=\"uint8\") out_video.write(array) count += 1 cv2.destroyAllWindows() out_video.release() cap.release() This code works fine with this video, and this is the video it gives the error on.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have an object I'd like to track using OpenCV. In my detection algorithm I can create bounded boxes around the objects it sees, and can create a target object to track properly. My detection algorithm works well, but I want to pass this object to a tracking algorithm.I can't quite get this done without having to re write the detection and image display issues. I'm working with an NVIDA Jetson Nanoboard with an Intel Realsense camera if that helps.",
        "answers": [
            [
                "The OpenCV DNN module comes with python samples of state of the art trackers. I've heard good things about the \"siamese\" based ones. Have a look Also the OpenCV contrib repo contains a whole module of various trackers. Give those a try first. They have a simple API."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "suppose i have a jetson nano whit an 4g Module for Jetson Nano, like this https://www.waveshare.com/sim7600g-h-4g-for-jetson-nano.htm now how i can use python to generate the gps data? is there any library or something that can do this job? or do i need to set up something like a flask/django web to read the gps data from an get/post method or so? any help you can give me would be great!",
        "answers": [
            [
                "There's a library called gpsd. Use python as an interface and with thr help of gpsd you'll be able to fetch all NMEA data. Go through the documentation. I used it about 3-4 years ago and it was helpful."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "OpenCV 4.5 - Ubuntu - Jetson Nano 2GB Hello I have a problem with getting video from my webcam (connected by USB to JetsonNano) by OpenCV Here's my code: // cv::VideoCapture cap( ... ); cv::Mat frame; if (!cap.isOpened()) // if not success, exit program { cout &lt;&lt; \"Cannot open the video cam\" &lt;&lt; endl; return -1; } cv::namedWindow(\"test\", cv::WINDOW_AUTOSIZE); while(true) { try { cap &gt;&gt; frame; cv::imshow(\"test\", frame); } catch(const std::exception&amp; e) { std::cerr &lt;&lt; e.what() &lt;&lt; '\\n'; } } 1 | cv::VideoCapture cap(0) gives me: [ WARN:0] global /home/legion/opencv/modules/videoio/src/cap_gstreamer.cpp (935) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1 2 | cv::VideoCapture cap(\"/dev/video0\") gives me: [ WARN:0] global /home/legion/opencv/modules/videoio/src/cap_gstreamer.cpp (1761) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module source reported: Could not read from resource. [ WARN:0] global /home/legion/opencv/modules/videoio/src/cap_gstreamer.cpp (888) open OpenCV | GStreamer warning: unable to start pipeline [ WARN:0] global /home/legion/opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created Cannot open the video cam 3 | When I try to open camera by command, only this one work: $ gst-launch-1.0 v4l2src device=\\\"/dev/video0\\\" ! xvimagesink I gave it a try: cv::VideoCapture cap(\"v4l2src device=\\\"/dev/video0\\\" ! xvimagesink\") it gives me: [ WARN:0] global /home/legion/opencv/modules/videoio/src/cap_gstreamer.cpp (803) open OpenCV | GStreamer warning: cannot find appsink in manual pipeline [ WARN:0] global /home/legion/opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created Cannot open the video cam If I gave other sinks : autosink or appsink, it's also doesn't work. 4 | I tried the solution from that link https://forums.developer.nvidia.com/t/sony-camera-module-cannot-be-opened-with-opencv-on-xavier/84003 cv::VideoCapture cap(\"v4l2src device=/dev/video0 ! video/x-raw,width=1920,height=1080,format=UYVY,framerate=30/1 ! videoconvert ! video/x-raw,format=BGR ! appsink\") and it's gives me : [ WARN:0] global /home/legion/opencv/modules/videoio/src/cap_gstreamer.cpp (1761) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module v4l2src0 reported: Internal data stream error. [ WARN:0] global /home/legion/opencv/modules/videoio/src/cap_gstreamer.cpp (888) open OpenCV | GStreamer warning: unable to start pipeline [ WARN:0] global /home/legion/opencv/modules/videoio/src/cap_gstreamer.cpp (480) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created [ERROR:0] global /home/legion/opencv/modules/videoio/src/cap.cpp (142) open VIDEOIO(CV_IMAGES): raised OpenCV exception: OpenCV(4.5.0) /home/legion/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): v4l2src device=/dev/video0 ! video/x-raw,width=1920,height=1080,format=UYVY,framerate=30/1 ! videoconvert ! video/x-raw,format=BGR ! appsink in function 'icvExtractPattern' Someone knows what's going on and how to fix it? Edit#1: My OpenCV buildinfo : General configuration for OpenCV 4.5.0 ===================================== Version control: unknown Extra modules: Location (extra): /home/legion/opencv_contrib/modules Version control (extra): unknown Platform: Timestamp: 2020-11-24T13:09:24Z Host: Linux 4.9.140-tegra aarch64 CMake: 3.10.2 CMake generator: Unix Makefiles CMake build tool: /usr/bin/make Configuration: RELEASE CPU/HW features: Baseline: NEON FP16 required: NEON disabled: VFPV3 C/C++: Built as dynamic libs?: YES C++ standard: 11 C++ Compiler: /usr/bin/c++ (ver 7.5.0) C++ flags (Release): -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -fvisibility-inlines-hidden -fopenmp -O3 -DNDEBUG -DNDEBUG C++ flags (Debug): -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -fvisibility-inlines-hidden -fopenmp -g -O0 -DDEBUG -D_DEBUG C Compiler: /usr/bin/cc C flags (Release): -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -fopenmp -O3 -DNDEBUG -DNDEBUG C flags (Debug): -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -fopenmp -g -O0 -DDEBUG -D_DEBUG Linker flags (Release): -Wl,--gc-sections -Wl,--as-needed Linker flags (Debug): -Wl,--gc-sections -Wl,--as-needed ccache: NO Precompiled headers: NO Extra dependencies: m pthread cudart_static dl rt nppc nppial nppicc nppicom nppidei nppif nppig nppim nppist nppisu nppitc npps cublas cudnn cufft -L/usr/local/cuda/lib64 -L/usr/lib/aarch64-linux-gnu 3rdparty dependencies: OpenCV modules: To be built: alphamat aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann freetype fuzzy gapi hdf hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python2 python3 quality rapid reg rgbd saliency sfm shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab xfeatures2d ximgproc xobjdetect xphoto Disabled: world Disabled by dependency: - Unavailable: cnn_3dobj cvv java js julia matlab ovis viz Applications: perf_tests examples apps Documentation: NO Non-free algorithms: YES GUI: GTK+: YES (ver 3.22.30) GThread : YES (ver 2.56.4) GtkGlExt: NO OpenGL support: NO VTK support: NO Media I/O: ZLib: /usr/lib/aarch64-linux-gnu/libz.so (ver 1.2.11) JPEG: /usr/lib/aarch64-linux-gnu/libjpeg.so (ver 80) WEBP: build (ver encoder: 0x020f) PNG: /usr/lib/aarch64-linux-gnu/libpng.so (ver 1.6.34) TIFF: build (ver 42 - 4.0.10) JPEG 2000: build (ver 2.3.1) OpenEXR: build (ver 2.3.0) HDR: YES SUNRASTER: YES PXM: YES PFM: YES Video I/O: DC1394: YES (2.2.5) FFMPEG: YES avcodec: YES (57.107.100) avformat: YES (57.83.100) avutil: YES (55.78.100) swscale: YES (4.8.100) avresample: YES (3.7.0) GStreamer: YES (1.14.5) v4l/v4l2: YES (linux/videodev2.h) Parallel framework: TBB (ver 2020.2 interface 11102) Trace: YES (with Intel ITT) Other third-party libraries: Lapack: NO Eigen: YES (ver 3.3.4) Custom HAL: YES (carotene (ver 0.0.1)) Protobuf: build (3.5.1) NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH) NVIDIA GPU arch: 53 NVIDIA PTX archs: cuDNN: YES (ver 8.0.0) OpenCL: YES (no extra features) Include path: /home/legion/opencv/3rdparty/include/opencl/1.2 Link libraries: Dynamic load Python 2: Interpreter: /usr/bin/python2.7 (ver 2.7.17) Libraries: /usr/lib/aarch64-linux-gnu/libpython2.7.so (ver 2.7.17) numpy: /usr/lib/python2.7/dist-packages/numpy/core/include (ver 1.13.3) install path: lib/python2.7/dist-packages/cv2/python-2.7 Python 3: Interpreter: /usr/bin/python3 (ver 3.6.9) Libraries: /usr/lib/aarch64-linux-gnu/libpython3.6m.so (ver 3.6.9) numpy: /home/legion/.local/lib/python3.6/site-packages/numpy/core/include (ver 1.19.4) install path: lib/python3.6/dist-packages/cv2/python-3.6 Python (for build): /usr/bin/python2.7 Java: ant: NO JNI: NO Java wrappers: NO Java tests: NO Install to: /usr ----------------------------------------------------------------- Thanks ~~",
        "answers": [
            [
                "For points 1 and 2, you can test by passing the second argument to VideoCapture(filename[, apiPreference]). Since you have OpenCV built with ffmpeg and v4l2. I would try the following options cv::CAP_FFMPEG, and CAP_V4L2. For point 4, make sure your webcam is 1080p."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm using a NVIDIA Jetson Nano, JetPack 4.4.1, Ubuntu 18.04 and Python 3.6.9. When I try to run this python3 script import jetson.inference import jetson.utils import cv2 net = jetson.inference.detectNet(\"ssd-mobilenet-v2\", threshold=0.5) #camera = jetson.utils.videoSource(\"csi://0\") # '/dev/video0' for V4L2 camera = jetson.utils.videoSource(\"/dev/video0\") display = jetson.utils.videoOutput(\"display://0\") # 'my_video.mp4' for file while display.IsStreaming(): img = camera.Capture() detections = net.Detect(img) display.Render(img) display.SetStatus(\"Object Detection | Network {:.0f} FPS\".format(net.GetNetworkFPS())) the script stucks or stops at the point you can see below: (Maybe one hour ago everything worked fine and after running this script and some others the problem occured.) nvidia@nvidia-desktop:~/jetson-inference/python/examples$ python3 my-detection.py jetson.inference -- detectNet loading build-in network 'ssd-mobilenet-v2' detectNet -- loading detection network model from: -- model networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff -- input_blob 'Input' -- output_blob 'NMS' -- output_count 'NMS_1' -- class_labels networks/SSD-Mobilenet-v2/ssd_coco_labels.txt -- threshold 0.500000 -- batch_size 1 [TRT] TensorRT version 7.1.3 [TRT] loading NVIDIA plugins... [TRT] Registered plugin creator - ::GridAnchor_TRT version 1 [TRT] Registered plugin creator - ::NMS_TRT version 1 [TRT] Registered plugin creator - ::Reorg_TRT version 1 [TRT] Registered plugin creator - ::Region_TRT version 1 [TRT] Registered plugin creator - ::Clip_TRT version 1 [TRT] Registered plugin creator - ::LReLU_TRT version 1 [TRT] Registered plugin creator - ::PriorBox_TRT version 1 [TRT] Registered plugin creator - ::Normalize_TRT version 1 [TRT] Registered plugin creator - ::RPROI_TRT version 1 [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1 [TRT] Could not register plugin creator - ::FlattenConcat_TRT version 1 [TRT] Registered plugin creator - ::CropAndResize version 1 [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1 [TRT] Registered plugin creator - ::Proposal version 1 [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1 [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1 [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1 [TRT] Registered plugin creator - ::Split version 1 [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1 [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1 [TRT] detected model format - UFF (extension '.uff') [TRT] desired precision specified for GPU: FASTEST [TRT] requested fasted precision for device GPU without providing valid calibrator, disabling INT8 [TRT] native precisions detected for GPU: FP32, FP16 [TRT] selecting fastest native precision for GPU: FP16 [TRT] attempting to open engine cache file /usr/local/bin/networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff.1.1.7103.GPU.FP16.engine [TRT] loading network plan from engine cache... /usr/local/bin/networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff.1.1.7103.GPU.FP16.engine [TRT] device GPU, loaded /usr/local/bin/networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff [TRT] Deserialize required 3168303 microseconds. [TRT] [TRT] CUDA engine context initialized on device GPU: [TRT] -- layers 116 [TRT] -- maxBatchSize 1 [TRT] -- workspace 0 [TRT] -- deviceMemory 35449856 [TRT] -- bindings 3 [TRT] binding 0 -- index 0 -- name 'Input' -- type FP32 -- in/out INPUT -- # dims 3 -- dim #0 3 (SPATIAL) -- dim #1 300 (SPATIAL) -- dim #2 300 (SPATIAL) [TRT] binding 1 -- index 1 -- name 'NMS' -- type FP32 -- in/out OUTPUT -- # dims 3 -- dim #0 1 (SPATIAL) -- dim #1 100 (SPATIAL) -- dim #2 7 (SPATIAL) [TRT] binding 2 -- index 2 -- name 'NMS_1' -- type FP32 -- in/out OUTPUT -- # dims 3 -- dim #0 1 (SPATIAL) -- dim #1 1 (SPATIAL) -- dim #2 1 (SPATIAL) [TRT] [TRT] binding to input 0 Input binding index: 0 [TRT] binding to input 0 Input dims (b=1 c=3 h=300 w=300) size=1080000 [TRT] binding to output 0 NMS binding index: 1 [TRT] binding to output 0 NMS dims (b=1 c=1 h=100 w=7) size=2800 [TRT] binding to output 1 NMS_1 binding index: 2 [TRT] binding to output 1 NMS_1 dims (b=1 c=1 h=1 w=1) size=4 [TRT] [TRT] device GPU, /usr/local/bin/networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff initialized. [TRT] W = 7 H = 100 C = 1 [TRT] detectNet -- maximum bounding boxes: 100 [TRT] detectNet -- loaded 91 class info entries [TRT] detectNet -- number of object classes: 91 [gstreamer] initialized gstreamer, version 1.14.5.0 [gstreamer] gstCamera -- attempting to create device v4l2:///dev/video0 [gstreamer] gstCamera -- found v4l2 device: UVC Camera (046d:0825) [gstreamer] v4l2-proplist, device.path=(string)/dev/video0, udev-probed=(boolean)false, device.api=(string)v4l2, v4l2.device.driver=(string)uvcvideo, v4l2.device.card=(string)\"UVC\\ Camera\\ \\(046d:0825\\)\", v4l2.device.bus_info=(string)usb-70090000.xusb-2.1, v4l2.device.version=(uint)264588, v4l2.device.capabilities=(uint)2216689665, v4l2.device.device_caps=(uint)69206017; [gstreamer] gstCamera -- found 38 caps for v4l2 device /dev/video0 [gstreamer] [0] video/x-raw, format=(string)YUY2, width=(int)1280, height=(int)960, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 15/2, 5/1 }; [gstreamer] [1] video/x-raw, format=(string)YUY2, width=(int)1280, height=(int)720, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 15/2, 5/1 }; [gstreamer] [2] video/x-raw, format=(string)YUY2, width=(int)1184, height=(int)656, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 10/1, 5/1 }; [gstreamer] [3] video/x-raw, format=(string)YUY2, width=(int)960, height=(int)720, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 10/1, 5/1 }; [gstreamer] [4] video/x-raw, format=(string)YUY2, width=(int)1024, height=(int)576, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 10/1, 5/1 }; [gstreamer] [5] video/x-raw, format=(string)YUY2, width=(int)960, height=(int)544, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 15/1, 10/1, 5/1 }; [gstreamer] [6] video/x-raw, format=(string)YUY2, width=(int)800, height=(int)600, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [7] video/x-raw, format=(string)YUY2, width=(int)864, height=(int)480, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [8] video/x-raw, format=(string)YUY2, width=(int)800, height=(int)448, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [9] video/x-raw, format=(string)YUY2, width=(int)752, height=(int)416, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [10] video/x-raw, format=(string)YUY2, width=(int)640, height=(int)480, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [11] video/x-raw, format=(string)YUY2, width=(int)640, height=(int)360, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [12] video/x-raw, format=(string)YUY2, width=(int)544, height=(int)288, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [13] video/x-raw, format=(string)YUY2, width=(int)432, height=(int)240, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [14] video/x-raw, format=(string)YUY2, width=(int)352, height=(int)288, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [15] video/x-raw, format=(string)YUY2, width=(int)320, height=(int)240, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [16] video/x-raw, format=(string)YUY2, width=(int)320, height=(int)176, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [17] video/x-raw, format=(string)YUY2, width=(int)176, height=(int)144, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [18] video/x-raw, format=(string)YUY2, width=(int)160, height=(int)120, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [19] image/jpeg, width=(int)1280, height=(int)960, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [20] image/jpeg, width=(int)1280, height=(int)720, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [21] image/jpeg, width=(int)1184, height=(int)656, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [22] image/jpeg, width=(int)960, height=(int)720, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [23] image/jpeg, width=(int)1024, height=(int)576, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [24] image/jpeg, width=(int)960, height=(int)544, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [25] image/jpeg, width=(int)800, height=(int)600, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [26] image/jpeg, width=(int)864, height=(int)480, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [27] image/jpeg, width=(int)800, height=(int)448, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [28] image/jpeg, width=(int)752, height=(int)416, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [29] image/jpeg, width=(int)640, height=(int)480, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [30] image/jpeg, width=(int)640, height=(int)360, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [31] image/jpeg, width=(int)544, height=(int)288, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [32] image/jpeg, width=(int)432, height=(int)240, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [33] image/jpeg, width=(int)352, height=(int)288, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [34] image/jpeg, width=(int)320, height=(int)240, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [35] image/jpeg, width=(int)320, height=(int)176, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [36] image/jpeg, width=(int)176, height=(int)144, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] [37] image/jpeg, width=(int)160, height=(int)120, pixel-aspect-ratio=(fraction)1/1, framerate=(fraction){ 30/1, 25/1, 20/1, 15/1, 10/1, 5/1 }; [gstreamer] gstCamera -- selected device profile: codec=mjpeg format=unknown width=1280 height=720 [gstreamer] gstCamera pipeline string: [gstreamer] v4l2src device=/dev/video0 ! image/jpeg, width=(int)1280, height=(int)720 ! jpegdec ! video/x-raw ! appsink name=mysink [gstreamer] gstCamera successfully created device v4l2:///dev/video0 [video] created gstCamera from v4l2:///dev/video0 ------------------------------------------------ gstCamera video options: ------------------------------------------------ -- URI: v4l2:///dev/video0 - protocol: v4l2 - location: /dev/video0 -- deviceType: v4l2 -- ioType: input -- codec: mjpeg -- width: 1280 -- height: 720 -- frameRate: 30.000000 -- bitRate: 0 -- numBuffers: 4 -- zeroCopy: true -- flipMethod: none -- loop: 0 ------------------------------------------------ [OpenGL] glDisplay -- X screen 0 resolution: 1280x1024 [OpenGL] glDisplay -- X window resolution: 1280x1024 [OpenGL] failed to create X11 Window. jetson.utils -- no output streams, creating fake null output [gstreamer] opening gstCamera for streaming, transitioning pipeline to GST_STATE_PLAYING [gstreamer] gstreamer changed state from NULL to READY ==&gt; mysink [gstreamer] gstreamer changed state from NULL to READY ==&gt; capsfilter1 [gstreamer] gstreamer changed state from NULL to READY ==&gt; jpegdec0 [gstreamer] gstreamer changed state from NULL to READY ==&gt; capsfilter0 [gstreamer] gstreamer changed state from NULL to READY ==&gt; v4l2src0 [gstreamer] gstreamer changed state from NULL to READY ==&gt; pipeline0 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; capsfilter1 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; jpegdec0 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; capsfilter0 [gstreamer] gstreamer stream status CREATE ==&gt; src [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; v4l2src0 [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; pipeline0 [gstreamer] gstreamer message new-clock ==&gt; pipeline0 [gstreamer] gstreamer stream status ENTER ==&gt; src [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; capsfilter1 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; jpegdec0 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; capsfilter0 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; v4l2src0 [gstreamer] gstreamer message stream-start ==&gt; pipeline0 [gstreamer] gstCamera -- onPreroll [gstreamer] gstCamera -- map buffer size was less than max size (1382400 vs 1382407) [gstreamer] gstCamera recieve caps: video/x-raw, format=(string)I420, width=(int)1280, height=(int)720, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(GstVideoMultiviewFlagsSet)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, chroma-site=(string)mpeg2, colorimetry=(string)1:4:0:0, framerate=(fraction)30/1 [gstreamer] gstCamera -- recieved first frame, codec=mjpeg format=i420 width=1280 height=720 size=1382407 RingBuffer -- allocated 4 buffers (1382407 bytes each, 5529628 bytes total) [gstreamer] gstreamer changed state from READY to PAUSED ==&gt; mysink [gstreamer] gstreamer message async-done ==&gt; pipeline0 [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; mysink [gstreamer] gstreamer changed state from PAUSED to PLAYING ==&gt; pipeline0 RingBuffer -- allocated 4 buffers (2764800 bytes each, 11059200 bytes total) Any idea what I can do? If more information are needed please let me know. Best regards chris",
        "answers": [
            [
                "I had the same issue. In my case it was because i was running out of ram space. Try adding swap space. Also switch to a light weight desktop environment like lxde and try again. It worked for me."
            ],
            [
                "Swap the position of the lines with variables camera and display such that display = jetson.utils.videoOutput(\"display://0\") camera = jetson.utils.videoSource(\"/dev/video0\")"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I have a code reading a serialized TensorRT engine: import tensorrt as trt import pycuda.driver as cuda cuda.init() device = cuda.Device(0) context = device.make_context() logger = trt.Logger(trt.Logger.INFO) with trt.Runtime(logger) as runtime: with open('model.trt', 'rb') as in_: engine = runtime.deserialize_cuda_engine(in_.read()) which runs just fine on my Nvidia Jeston Nano, until I compile it with Pyinstaller pyinstaller temp.py In the compiled code runtime.deserialize_cuda_engine returns None and logger says: Cuda Error in loadKernel: 3 (initialization error) [TensorRT] ERROR: INVALID_STATE: std::exception [TensorRT] ERROR: INVALID_CONFIG: Deserialize the cuda engine failed. When I construct the engine from scratch, like cuda.init() device = cuda.Device(0) context = device.make_context() logger = trt.Logger(trt.Logger.INFO) with ExitStack() as stack: builder = stack.enter_context(trt.Builder(logger)) network = stack.enter_context(builder.create_network( 1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH) )) i = network.add_input('input0', trt.float16, (3, 2)) s = network.add_softmax(i) network.mark_output(s.get_output(0)) config = stack.enter_context(builder.create_builder_config()) ...some builder settings like opt profiles and fp16 mode... engine = builder.build_engine(network, config) then everything works fine, even after compilation. The engine was prepared with trtexec on the same computer. Cuda version is V10.2.89, pycuda version is 2019.1.2. I believe it's a standard jetson installation as of August 2020. Any ideas what might be involved here and what workarounds might be?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using gstream to stream with RTSP protocol from my camera connected to a jetson nano to an Android device. At this moment I can see the camera in vlc with this: e./test-launch \"nvarguscamerasrc ! video/x-raw(memory:NVMM),width=(int)320,height=(int)240,format=(string)NV12,framerate=(fraction)15/1 ! omxh264enc name=omxh264enc control-rate=1 ! video/x-h264,profile=baseline,stream-format=(string)byte-stream ! h264parse ! rtph264pay name=pay0\" But I have 2 seconds lag and I think the format is not valid for Android. I found some people who say that the current format I have is h264 MPEG-4 AVC Part 10. I use VideoView to stream in Android. edit: The lag was an vlc problem, I had a configuration with 1000ms.",
        "answers": [],
        "votes": []
    },
    {
        "question": "When I try to set up the jupyter notebook password, I don't get a password hash when I open up the jupyter_notebook_config.json file. This is the output of the json file: { \"NotebookApp\": { \"password\": \"argon2:$argon2id$v=19$m=10240,t=10,p=8$pcTg1mB/X5a3XujQqYq/wQ$/UBQBRlFdzmEmxs6c2IzmQ\" } } I've tried running passwd() from python as well, like in the instructions for Preparing a hashed password instructions found online but it produces the same results as above. No hash. Can someone please let me know what I'm doing wrong? I'm trying to set up a Jetson Nano in similar fashion to the Deep Learing Institute Nano build. With that build you can run Jupyter Lab remotely so the nano can run headless. I'm trying to do the same things with no luck. Thanks!",
        "answers": [
            [
                "This is the default algorithm (argon2): https://github.com/jupyter/notebook/blob/v6.5.2/notebook/auth/security.py#L23 you can provide a different algorithm like sha1 if you like: &gt;&gt;&gt; from notebook.auth import passwd &gt;&gt;&gt; from notebook.auth.security import passwd_check &gt;&gt;&gt; &gt;&gt;&gt; password = 'myPass123' &gt;&gt;&gt; &gt;&gt;&gt; hashed_argon2 = passwd(password) &gt;&gt;&gt; hashed_sha1 = passwd(password, 'sha1') &gt;&gt;&gt; &gt;&gt;&gt; print(hashed_argon2) argon2:$argon2id$v=19$m=10240,t=10,p=8$JRz5GPqjOYJu/cnfXc5MZw$LZ5u6kPKytIv/8B/PLyV/w &gt;&gt;&gt; &gt;&gt;&gt; print(hashed_sha1) sha1:c29c6aeeecef:0b9517160ce938888eb4a6ec9ca44e3a31da9519 &gt;&gt;&gt; &gt;&gt;&gt; passwd_check(hashed_argon2, password) True &gt;&gt;&gt; &gt;&gt;&gt; passwd_check(hashed_sha1, password) True Check whether you don't have a different Jupyter server running on your machine. It happened to me that I was trying over and over a password on port 8888 while my intended server was on port 8889. Another time, Anaconda started a server on localhost:8888, and I was trying to reach a mapped port from a docker container, also on port 8888, and the only way to access was actually on 0.0.0.0:8888."
            ]
        ],
        "votes": [
            11.0000001
        ]
    },
    {
        "question": "I am using Lepton 3.5 with Pure-thermal 2. Compiled C + V4l2 code on Jetson Nano from Groupgets git page: https://github.com/groupgets/purethermal1-uvc-capture. Initially got this error message Libv4l didn't accept RGB24 format. Can't proceed. Then changed image resolution from 80x60 to 160x120 and pixel format YUYV to UYVY then the code works but not generating good images. The image contains multiple strips having different colors as shown below: Changing pixel format to GREY and resolution 640*480, generated the image shown below: Please anyone suggest how to get the perfect images in GREY, UYVY and other supported formats. #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; #include &lt;fcntl.h&gt; #include &lt;errno.h&gt; #include &lt;sys/ioctl.h&gt; #include &lt;sys/types.h&gt; #include &lt;sys/time.h&gt; #include &lt;sys/mman.h&gt; #include &lt;linux/videodev2.h&gt; #include &lt;libv4l2.h&gt; #define CLEAR(x) memset(&amp;(x), 0, sizeof(x)) struct buffer { void *start; size_t length; }; static void xioctl(int fh, int request, void *arg) { int r; do { r = v4l2_ioctl(fh, request, arg); } while (r == -1 &amp;&amp; ((errno == EINTR) || (errno == EAGAIN))); if (r == -1) { fprintf(stderr, \"error %d, %s\\n\", errno, strerror(errno)); exit(EXIT_FAILURE); } } int main(int argc, char **argv) { struct v4l2_format fmt; struct v4l2_buffer buf; struct v4l2_requestbuffers req; enum v4l2_buf_type type; fd_set fds; struct timeval tv; int r, fd = -1; unsigned int i, n_buffers; char *dev_name = \"/dev/video0\"; char out_name[256]; FILE *fout; struct buffer *buffers; fd = v4l2_open(dev_name, O_RDWR | O_NONBLOCK, 0); if (fd &lt; 0) { perror(\"Cannot open device\"); exit(EXIT_FAILURE); } CLEAR(fmt); fmt.type = V4L2_BUF_TYPE_VIDEO_CAPTURE; fmt.fmt.pix.width = 640; fmt.fmt.pix.height = 480; fmt.fmt.pix.pixelformat = V4L2_PIX_FMT_GREY; fmt.fmt.pix.field = V4L2_FIELD_NONE; xioctl(fd, VIDIOC_S_FMT, &amp;fmt); if (fmt.fmt.pix.pixelformat != V4L2_PIX_FMT_GREY) { printf(\"Libv4l didn't accept RGB24 format. Can't proceed.\\n\"); exit(EXIT_FAILURE); } if ((fmt.fmt.pix.width != 80) || (fmt.fmt.pix.height != 60)) printf(\"Warning: driver is sending image at %dx%d\\n\", fmt.fmt.pix.width, fmt.fmt.pix.height); CLEAR(req); req.count = 2; req.type = V4L2_BUF_TYPE_VIDEO_CAPTURE; req.memory = V4L2_MEMORY_MMAP; xioctl(fd, VIDIOC_REQBUFS, &amp;req); buffers = calloc(req.count, sizeof(*buffers)); for (n_buffers = 0; n_buffers &lt; req.count; ++n_buffers) { CLEAR(buf); buf.type = V4L2_BUF_TYPE_VIDEO_CAPTURE; buf.memory = V4L2_MEMORY_MMAP; buf.index = n_buffers; xioctl(fd, VIDIOC_QUERYBUF, &amp;buf); buffers[n_buffers].length = buf.length; buffers[n_buffers].start = v4l2_mmap(NULL, buf.length, PROT_READ | PROT_WRITE, MAP_SHARED, fd, buf.m.offset); if (MAP_FAILED == buffers[n_buffers].start) { perror(\"mmap\"); exit(EXIT_FAILURE); } } for (i = 0; i &lt; n_buffers; ++i) { CLEAR(buf); buf.type = V4L2_BUF_TYPE_VIDEO_CAPTURE; buf.memory = V4L2_MEMORY_MMAP; buf.index = i; xioctl(fd, VIDIOC_QBUF, &amp;buf); } type = V4L2_BUF_TYPE_VIDEO_CAPTURE; xioctl(fd, VIDIOC_STREAMON, &amp;type); for (i = 0; i &lt; 20; i++) { do { FD_ZERO(&amp;fds); FD_SET(fd, &amp;fds); /* Timeout. */ tv.tv_sec = 2; tv.tv_usec = 0; r = select(fd + 1, &amp;fds, NULL, NULL, &amp;tv); } while ((r == -1 &amp;&amp; (errno = EINTR))); if (r == -1) { perror(\"select\"); return errno; } CLEAR(buf); buf.type = V4L2_BUF_TYPE_VIDEO_CAPTURE; buf.memory = V4L2_MEMORY_MMAP; xioctl(fd, VIDIOC_DQBUF, &amp;buf); sprintf(out_name, \"out%03d.ppm\", i); fout = fopen(out_name, \"w\"); if (!fout) { perror(\"Cannot open image\"); exit(EXIT_FAILURE); } fprintf(fout, \"P6\\n%d %d 255\\n\", fmt.fmt.pix.width, fmt.fmt.pix.height); fwrite(buffers[buf.index].start, buf.bytesused, 1, fout); fclose(fout); xioctl(fd, VIDIOC_QBUF, &amp;buf); } type = V4L2_BUF_TYPE_VIDEO_CAPTURE; xioctl(fd, VIDIOC_STREAMOFF, &amp;type); for (i = 0; i &lt; n_buffers; ++i) v4l2_munmap(buffers[i].start, buffers[i].length); v4l2_close(fd); return 0; }",
        "answers": [
            [
                "have you checked the usb descriptor? the purethermal firmware creates an uvc device and with lsusb you can get a list of the supported formats and frames in the usb descriptor."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I trained a U-Net segmentation model with Keras (using TF backend). I am trying to convert its frozen graph (.pb) to TensorRT format on the Jetson Nano but the process is killed (as seen below). I\u2019ve seen on other posts that it could be related to an \u00ab out of memory \u00bb problem. To be known, I already have an SSD MobileNet V2 model running on the Jetson Nano. If I stop the systemctl, I can make inference with the U-Net model without converting it to TensorRT (just using the frozen graph model loaded with Tensorflow). As this way doesn't work when I start the systemctl (so when the other neural network is running), I try to convert my U-Net segmentation model to TensorRT to get an optimized version of it (which failed because of a killed process), but it may not be the right way to do this. Is it possible to run two neural networks on a Jetson Nano ? Is there any other way to do this ? For information, here is the way I try to convert the frozen graph to TensorRT : trt_graph = trt.create_inference_graph( input_graph_def=frozen_graph_gd, # Pass the parsed graph def here outputs=['conv2d_24/Sigmoid'], max_batch_size=1, max_workspace_size_bytes=1 &lt;&lt; 32, # I have tried 25 and 32 here precision_mode='FP16' ) And here is when the process is killed (conversion of the U-Net frozen graph to TensorRT) : 2020-10-05 16:00:58.200269: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2 WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them. 2020-10-05 16:01:11.976893: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer.so.7 2020-10-05 16:01:11.994472: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer_plugin.so.7 WARNING:tensorflow: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see: * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md * https://github.com/tensorflow/addons * https://github.com/tensorflow/io (for I/O related ops) If you depend on functionality not listed there, please file an issue. WARNING:tensorflow:From convert_pb_to_tensorrt.py:14: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead. 2020-10-05 16:01:13.678101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libnvinfer.so.7 2020-10-05 16:01:15.506432: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1 2020-10-05 16:01:15.512224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] ARM64 does not support NUMA - returning NUMA node zero 2020-10-05 16:01:15.512359: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 2020-10-05 16:01:15.512638: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session 2020-10-05 16:01:15.532712: W tensorflow/core/platform/profile_utils/cpu_utils.cc:98] Failed to find bogomips in /proc/cpuinfo; cannot determine CPU frequency 2020-10-05 16:01:15.533264: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x328fd900 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-10-05 16:01:15.533318: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-10-05 16:01:15.632451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] ARM64 does not support NUMA - returning NUMA node zero 2020-10-05 16:01:15.632757: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x30d0edb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-10-05 16:01:15.632808: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): NVIDIA Tegra X1, Compute Capability 5.3 2020-10-05 16:01:15.633163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] ARM64 does not support NUMA - returning NUMA node zero 2020-10-05 16:01:15.633276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1634] Found device 0 with properties: name: NVIDIA Tegra X1 major: 5 minor: 3 memoryClockRate(GHz): 0.9216 pciBusID: 0000:00:00.0 2020-10-05 16:01:15.633348: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2 2020-10-05 16:01:15.633500: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10 2020-10-05 16:01:15.716786: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10 2020-10-05 16:01:15.903326: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10 2020-10-05 16:01:16.060655: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10 2020-10-05 16:01:16.141950: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10 2020-10-05 16:01:16.142219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8 2020-10-05 16:01:16.142553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] ARM64 does not support NUMA - returning NUMA node zero 2020-10-05 16:01:16.142878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] ARM64 does not support NUMA - returning NUMA node zero 2020-10-05 16:01:16.142991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1762] Adding visible gpu devices: 0 2020-10-05 16:01:16.143133: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2 2020-10-05 16:01:27.700226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1175] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-10-05 16:01:27.700377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] 0 2020-10-05 16:01:27.700417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1194] 0: N 2020-10-05 16:01:27.713559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] ARM64 does not support NUMA - returning NUMA node zero 2020-10-05 16:01:27.713897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] ARM64 does not support NUMA - returning NUMA node zero 2020-10-05 16:01:27.714101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1320] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 200 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0, compute capability: 5.3) Killed",
        "answers": [
            [
                "If the model has unsupported layers, converting to tensor RT won't be achieved. If it's the case, using tensorflow's version or TRT can yield results as this version handles well unsupported layers (they will be handled by tensorflow alongside your tens rt converted layers). Hope the answer is close to your problem. Tensor rt is a messy ecosystem"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "How do i solve this problem? The operation system environment is as follows. python 3.7.1 conda 4.5.12",
        "answers": [
            [
                "The FMU binary seem to have some dependencies that cannot be found on your machine. This would indicate that either the FMU is not compatible with the linux version you are running or the FMU is not correctly generated (i.e. it should be self contained and not needing any external binary dependencies). To check which, you can manually unzip your FMU, and in a terminal browse to the binary that is contained in the subfolder (binaries/linux64) and run \"ldd .so\" this will show you the dependencies of the FMU binary."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "As title mentioned, I tried to extract some data from AMG8833 through GPIO on Jetson nano, but always get [Errno 121]Remote I/O error... My AMG8833 SDA connect to GPIO pin 27, and SCL connect to pin 28. I have already tried sudo i2cdetect -y -r 1 Then only shows 29, I have no idea about this, and still cannot work... Is there anyone has any other solutions about this issue? Please share with me, I will be very appreciated for your help. Following is the python code... import busio import time import board import adafruit_amg88xx i2c = busio.I2C(board.SCL, board.SDA) amg = adafruit_amg88xx.AMG88XX(i2c) while True: time.sleep(1) for row in amg.pixels: # Pad to 1 decimal place print([\"{0:.1f}\".format(temp) for temp in row]) print(row) print(\"\") print(\"\\n\") time.sleep(1)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using openCV library with yolov3 and darknet in my project. My app has been written in C++ and it reads rtsp stream and look for a human on the stream. I run it on my Nvidia Jetson Nano and everything is fine but there is one small issue. I have noticeable delay in video analysis. When I run it and I appear in camera view area I can see ~20s lag. I'm analising substream (720p 2fps) but on recognition I would like to capture the right moment of recognition on the main stream (1080p 15fps) which I record using ffmpg. To do so i need to (1) don't have delay on recognition or (2) measure this delay during recognition to define which second of main video I need to capture. I suppose (1) is not possible. Do you know if openCV has such an option to display this delay? How can I measure it? p.s. This delay is not always the same. But I noticed it is from 10 to 20 s, Thank you a lot for any help ;)",
        "answers": [
            [
                "It will be hard to sync the stream, as both fps and stream channels are different. And another problem it's the rtsp stream, openCV can skip a lot of frames caused by bootlenecks and you can't get them back. You may find a answer if you look where is your bootleneck. Probably, as it's a deep learning algorithm, most of the gpu/cpu time will be in the detection algorithm. What i would do is: Ignore the second stream and focus your code on the main stream, add some frames on a buffer and detect, if your counditions are reached, then you iterate over that buffer to save what you need. ps: This can cause problems to, due to the time needed to save the buffer on disk. (Maybe create a Thread for this will help)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "As title mentioned, I tried to extract some data from AMG8833 through GPIO on Jetson nano, but always get [Errno 121]Remote I/O error... My AMG8833 SDA connect to GPIO pin 27, and SCL connect to pin 28. I have already tried sudo i2cdetect -y -r 1 Then only shows 29, I have no idea about this, and still cannot work... Is there anyone has any other solutions about this issue? Please share with me, I will be very appreciated for your help. Following is the python code... import busio import time import board import adafruit_amg88xx i2c = busio.I2C(board.SCL, board.SDA) amg = adafruit_amg88xx.AMG88XX(i2c) while True: time.sleep(1) for row in amg.pixels: # Pad to 1 decimal place print([\"{0:.1f}\".format(temp) for temp in row]) print(row) print(\"\") print(\"\\n\") time.sleep(1)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using openCV library with yolov3 and darknet in my project. My app has been written in C++ and it reads rtsp stream and look for a human on the stream. I run it on my Nvidia Jetson Nano and everything is fine but there is one small issue. I have noticeable delay in video analysis. When I run it and I appear in camera view area I can see ~20s lag. I'm analising substream (720p 2fps) but on recognition I would like to capture the right moment of recognition on the main stream (1080p 15fps) which I record using ffmpg. To do so i need to (1) don't have delay on recognition or (2) measure this delay during recognition to define which second of main video I need to capture. I suppose (1) is not possible. Do you know if openCV has such an option to display this delay? How can I measure it? p.s. This delay is not always the same. But I noticed it is from 10 to 20 s, Thank you a lot for any help ;)",
        "answers": [
            [
                "It will be hard to sync the stream, as both fps and stream channels are different. And another problem it's the rtsp stream, openCV can skip a lot of frames caused by bootlenecks and you can't get them back. You may find a answer if you look where is your bootleneck. Probably, as it's a deep learning algorithm, most of the gpu/cpu time will be in the detection algorithm. What i would do is: Ignore the second stream and focus your code on the main stream, add some frames on a buffer and detect, if your counditions are reached, then you iterate over that buffer to save what you need. ps: This can cause problems to, due to the time needed to save the buffer on disk. (Maybe create a Thread for this will help)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I having trouble importing PyTorch in my jetson nano (jetpack 4.4, Cuda 10.2.89), I have successfully installed it from .whl file and it is in my pip3 lib. But when I import it, it shows this error. Please help. Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/home/parikshit/.local/lib/python3.6/site-packages/torch/__init__.py\", line 188, in &lt;module&gt; _load_global_deps() File \"/home/parikshit/.local/lib/python3.6/site-packages/torch/__init__.py\", line 141, in _load_global_deps ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL) File \"/usr/lib/python3.6/ctypes/__init__.py\", line 348, in __init__ self._handle = _dlopen(self._name, mode) OSError: libmpi_cxx.so.20: cannot open shared object file: No such file or directory```",
        "answers": [
            [
                "cd /home/parikshit/.local/lib/python3.6/site-packages/torch ls # You should find a .so file. Run the following command on that file ldd the_file_you_found.so #In the output you should see \"not found\" #To find out which apt packages provide this .so: sudo apt install apt-file sudo apt-file update #Now search for that \"not found\" library file. Example: apt-file search blah.so.100 #In the first part of each line you will see the name of the file that you need to install. Example: sudo apt install lib-hello #Enjoy! The previous answer resolve my issue about not finding libmpi_cxx.so.20 on Jetson Nano but I have encountered a different error while importing torch. Error is: from torch._C import * ImportError: numpy.core.multiarray failed to import If you have problem importing torch because of failed to import numpy follow the step below. pip install numpy -I You can check the issue on GitHub for details."
            ],
            [
                "Try to do the following: cd /home/parikshit/.local/lib/python3.6/site-packages/torch ls # You should find a .so file. Run the following command on that file ldd the_file_you_found.so #In the output you should see \"not found\" #To find out which apt packages provide this .so: sudo apt install apt-file sudo apt-file update #Now search for that \"not found\" library file. Example: apt-file search blah.so.100 #In the first part of each line you will see the name of the file that you need to install. Example: sudo apt install lib-hello #Enjoy!"
            ],
            [
                "The first solution worked for me! Should be recognized as the answer! I just had to do an extra step before installing the missing package (libopenmpi-dev in my case) $ sudo dpkg --remove --force-all libopenmpi-dev $ sudo apt install libopenmpi-dev"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am using a Jupyter notebook for my project with a Xbox controller to control a robot in real time. import ipywidgets.widgets as widgets is used it works directly assigning them to sliders or motors using traitlets, but I do not how to do calculations with them beforehand. If I assign these values to variables, it gets the current value at the time of running the cell. Here is working code that assigns directly to motor: left_link = traitlets.dlink((controller.axes[1], 'value'), (robot.left_motor, 'value'), transform=lambda x: -x) However, I need to calculate the angle I want to go, therefore calculating arctan2 of two axes of joystick values (the transform: lambda does not suffice I think?). I do not understand how I should take on this problem, as I have never worked with always updating variables, like the Xbox controller variables. Please tell me if you need any more information.",
        "answers": [
            [
                "The thing I was looking for was Observe() like here: import ipywidgets.widgets as widgets int_range = widgets.IntSlider() display(int_range) def on_value_change(change): print(change) print(change['new']) int_range.observe(on_value_change, names='value') This allows reassigning the variable value everytime the controller changes it's value."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I was trying to install OpenCV4 in a docker on jetson nano. It has jetpack 4.4 s os. The docker was successfully created and Tensorflow is running but while installing OpenCV using pip it is showing CMake error. root@5abf405fb92d:~# pip3 install opencv-python Collecting opencv-python Downloading opencv-python-4.4.0.42.tar.gz (88.9 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 88.9 MB 2.5 kB/s Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.4) Building wheels for collected packages: opencv-python Building wheel for opencv-python (PEP 517) ... error ERROR: Command errored out with exit status 1: command: /usr/bin/python3 /usr/local/lib/python3.6/dist-packages/pip/_vendor/pep517/_in_process.py build_wheel /tmp/tmpqpzwrofy cwd: /tmp/pip-install-93nxibky/opencv-python Complete output (9 lines): File \"/tmp/pip-build-env-o_hualnr/overlay/lib/python3.6/site-packages/skbuild/setuptools_wrap.py\", line 560, in setup cmkr = cmaker.CMaker(cmake_executable) File \"/tmp/pip-build-env-o_hualnr/overlay/lib/python3.6/site-packages/skbuild/cmaker.py\", line 95, in __init__ self.cmake_version = get_cmake_version(self.cmake_executable) File \"/tmp/pip-build-env-o_hualnr/overlay/lib/python3.6/site-packages/skbuild/cmaker.py\", line 82, in get_cmake_version \"Problem with the CMake installation, aborting build. CMake executable is %s\" % cmake_executable) Traceback (most recent call last): Problem with the CMake installation, aborting build. CMake executable is cmake ---------------------------------------- ERROR: Failed building wheel for opencv-python Failed to build opencv-python ERROR: Could not build wheels for opencv-python which use PEP 517 and cannot be installed directly",
        "answers": [
            [
                "I had the same problem and i did this, pip install --upgrade pip setuptools wheel then install opencv again, pip install opencv-python this worked for me"
            ],
            [
                "If after pip install --upgrade pip setuptools wheel you still have the same error, You can try to specify the older version of OpenCV to install. Ex. pip3 install opencv-python==3.4.13.47"
            ],
            [
                "Yes .. Finally found a work around. Follow this https://github.com/mdegans/nano_build_opencv and build from source and finally gets installed. PS: It may take a bit long for building, for me it took 10 Hrs :P. Happy Image-Processing.."
            ],
            [
                "I had a similar problem and what solved it for me was not to use python:3-alpine but python:3.8-slim. E.g.: FROM python:3.8-slim WORKDIR /usr/src/app RUN apt update RUN apt -y install build-essential libwrap0-dev libssl-dev libc-ares-dev uuid-dev xsltproc RUN apt-get update -qq \\ &amp;&amp; apt-get install --no-install-recommends --yes \\ build-essential \\ gcc \\ python3-dev \\ mosquitto \\ mosquitto-clients RUN pip3 install --upgrade pip setuptools wheel RUN python3 -m pip install --no-cache-dir \\ numpy scipy matplotlib scikit-build opencv-contrib-python-headless \\ influxdb paho-mqtt configparser Pillow \\ qrcode worked finally for me."
            ],
            [
                "I came across similiar situation I had error Failed to build opencv-python ERROR: Could not build wheels for opencv-python which use PEP 517 and cannot be installed directly WARNING: You are using pip version 19.2.3, however version 22.0.4 is available. You should consider upgrading via the 'python -m pip install --upgrade pip' command. I ran in command prompt with admin privileges following python -m pip install --upgrade pip Now \"pip install opencv-python\" works"
            ],
            [
                "please check your python specifications: - opencv -&gt; python[ version=' &gt;=2.7,&lt;2.8.0a0 &gt;=3.5,&lt;3.6.0a0 &gt;=3.6,&lt;3.7.0a0 &gt;=3.7,&lt;3.8.0a0']"
            ],
            [
                "I handled it by reinstalling python3 from scratch in my MacBook: brew reinstall python@3.9 I also reinstalled numpy and matplotlib packages experimentally. pip3 install numpy pip3 install matplotlib pip3 install opencv-contrib-python The versions: macOS Mojave 10.14.5 Python 3.9.7 OpenCV 4.5.3 OpenCV's version is 4.5.3 by this way: import cv2 print(cv2.__version__) But by \"pip list\", it shows \"opencv-contrib-python 3.4.9.31\"."
            ],
            [
                "try using this pip install opencv-python==4.5.3.56"
            ],
            [
                "I had similar issue in installing OpenCV in Anaconda 3 using Python version 3.6.PIP was not able to build the wheel. The following two commands solved my issue: pip install --upgrade pip setuptools wheel pip3 install opencv-python==4.7.0.68 Thanks"
            ],
            [
                "if you are trying to install opencv in your raspberrypi 3B, Use following steps: sudo raspi-config advanced -- expand filesystem reboot your pi Open your raspi terminal and do following stuff: use command: sudo apt-get update use command: sudo apt-get upgrade check your python version and upgarde it to latest one install pip and upgrade pip use command: mkdir project use command: cd project create virtual environment activate virtual environment install dependencies ,can get dependencies from https://singleboardbytes.com/647/install-opencv-raspberry-pi-4.htm if error in installing libdhf5-dev use command: sudo apt-get install python3-h5py and reinstall libdhf5-dev use command: pip install scikit-build use command: pip install cython before installing opencv ,make sure you are in virtual environment or activate environment use command: pip install --no-cache-dir opencv-contrib-python==4.5.3.56 Remember to use mentioned version... Thank You..."
            ]
        ],
        "votes": [
            84.0000001,
            20.0000001,
            5.0000001,
            2.0000001,
            1.0000001,
            1e-07,
            1e-07,
            1e-07,
            1e-07,
            -0.9999999
        ]
    },
    {
        "question": "I have been using OpenCV for a long time on Jetson Nano. I always started my codes on Jetson Nano terminal with sudo command. For example: sudo python3 process.py When I do: python3 process.py ...I can not import opencv. How can I import opencv without using `sudo command on terminal? Could you please help me?",
        "answers": [
            [
                "You need to check sys.path Run python with no sudo: python3 &gt;&gt;&gt; import sys &gt;&gt;&gt; print(sys.path) Then compare output with python which run with sudo: sudo python3 &gt;&gt;&gt; import sys &gt;&gt;&gt; print(sys.path) I think your outputs will be different. Need to make them the same."
            ],
            [
                "Are you by any chance using conda environment? If so, the python packages installed (system-wide) by the OpenCV build script won't be available in your virtual environment. To solve this, create a symbolic link from your system python site-packages to conda environment site-packages. Usually it will be like this, (for cv2 specifically) - ln -s /usr/local/lib/python3.6/site-packages/cv2/python3.6/cv2.cpython-36m-aarch64-linux-gnu.so /home/mucahid/c4aarch64_installer/lib/python3.7/site-packages/cv2.so"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "First, I want to mention, that this is our first project in a bigger scale and therefore we don't know everything but we learn fast. We developed a code for image recognition. We tried it with a raspberry pi 4b but quickly faced that this is way to slow overall. Currently we are using a NVIDIA Jetson Nano. The first recognition was ok (around 30 sec.) and the second try was even better (around 6-7 sec.). The first took so long because the model will be loaded for the first time. Via an API the image recognition can be triggered and the meta data from the AI model will be the response. We use fast-API for this. But there is a problem right now, where if I load my CNN as a global variable in the beginning of my classification file (loaded on import) and use it within a thread I need to use mp.set_start_method('spawn') because otherwise I will get the following error: \"RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\" Now that is of course an easy fix. Just add the method above before starting my thread. Indeed this works but another challenge occurs at the same time. After setting the start method to 'spawn' the ERROR disappears but the Jetson starts to allocate way to much memory. Because of the overhead and preloaded CNN model, the RAM is around 2.5Gig before the thread starts. After the start it doesn\u2019t stop allocating RAM, it consumes all 4Gig of the RAM and also the whole 6Gig Swap. Right after this, the whole API process kill with this error: \"cannot allocate memory\" which is obvious. I managed to fix that as well just by loading the CNN Model in the classification function. (Not preloading it on the GPU as in the two cases before). However, here I got problem as well. The process of loading the model to the GPU takes around 15s - 20s and this every time the recognition starts. This is not suitable for us and we are wondering why we cannot pre-load the model without killing the whole thing after two image-recognitions. Our goal is to be under 5 sec with this. #clasify import torchvision.transforms as transforms from skimage import io import time from torch.utils.data import Dataset from .loader import * from .ResNet import * #if this part is in the classify() function than no allocation problem occurs net = ResNet152(num_classes=25) net = net.to('cuda') save_file = torch.load(\"./model.pt\", map_location=torch.device('cuda')) net.load_state_dict(save_file) def classify(imgp=\"\"): #do some classification with the net pass if __name__ == '__main__': mp.set_start_method('spawn') #if commented out the first error ocours manager = mp.Manager() return_dict = manager.dict() p = mp.Process(target=classify, args=('./bild.jpg', return_dict)) p.start() p.join() print(return_dict.values()) Any help here will be much appreciated. Thank you.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to run ZoneMinder on Jetson-nano as an IP CAM server, but it seems that Jetson nano can not handle many cams which are using ffmpeg for decoding. If I open three cams on ZoneMinder, CPU usage is always 100%, but GPU usage is 0%, and I can not open more cams. FFMPEG without hardware acceleration? In this discussion, the NV Moderator said that ffmpeg with GPU support is not enabled on Jetson platform ... I know that NVENC/NVDEC can help ffmpeg hardware acceleration, but these hardware blocks are being now executed on CPU, not on the GPU. Custom FFMPEG for jetson-nano don't support GPU acceleration? I found this jetson-ffmpeg repo, can this repo use GPU acceleration? Because I tried but didn't work (still using NVENC/NVDEC acceleration of CPU). If GPU on Jetson nano is sure that can not support ffmpeg acceleration, but I want to use ZoneMinder, is there a better way to solve this problem? e.g. using GStreamer?",
        "answers": [
            [
                "ffmpeg support has been added to the Jetson platform. From nvidia's docs: To install the ffmpeg binary package \u2022Enter these commands: $ sudo apt install ffmpeg To get source files for the ffmpeg package \u2022Enter this command: $ apt source ffmpeg To include the ffmpeg library in L4T builds 1.Add the following lines to /etc/apt/sources.list: $ echo \"deb https://repo.download.nvidia.com/jetson/ffmpeg main main\" | sudo tee -a /etc/apt/sources.list $ echo \"deb-src https://repo.download.nvidia.com/jetson/ffmpeg main main\" | sudo tee -a /etc/apt/sources.list 2.Enter the command: $ sudo apt update To be clear, on the standard Jetson image, append deb https://repo.download.nvidia.com/jetson/ffmpeg main main to your /etc/apt/sources.list file, and then run sudo apt update &amp;&amp; sudo apt install ffmpeg ."
            ],
            [
                "Use (jetson-ffmpeg) a ffmpeg patched for jetson-nano Jetson nano don't need to use GPU for hardware decoding MPEG2, H.264/AVC, HEVC, VP8 and VP9. Those formats are decoded by specific video hardware decoder (NVDEC) that can be accessed by L4T Multimedia API that patches the ffmpeg above. That hardware is separated from GPU that can be used by you for other purposes. You can also use @Simon Labrecque answer if you don't plan using encoding. Although I still prefer the first. Command line hardware decoding jetson-ffmpeg You did something wrong with the above ffmpeg. To use hardware decoding you have to specify the decoder. From their repo README some examples. For decoding a H.264 stream/file ffmpeg -c:v h264_nvmpi -i yourfile\\or\\stream... For decoding a HEVC stream/file ffmpeg -c:v hevc_nvmpi -i yourfile\\or\\stream.... ZoneMinder I don't use Zoneminder but MotionProject that also uses ffmpeg as a backend library. I have been using 4 IP Cameras without problems for some months already. CPU usage around 20% for 8 RTSP streams (high and low resolutions). For ZoneMinder I am not sure but I think that DecoderHWAccelName, DecoderHWAccelDevice parameters should do the trick. Specifying for example h264_nvmpi or nvmpi or whatever needed for your stream. But I can't be precise though because I don't use it anymore. Try their docs look for those parameters on adding-monitors section."
            ]
        ],
        "votes": [
            4.0000001,
            1e-07
        ]
    },
    {
        "question": "In this link, we can access to gstreamer pipeline buffer and convert the frame buffers in numpy array, I want to know, How I can to accesses the frame buffers in GPU mem and then feed into my custom processor without convert frames into numpy array. We have two solutions for using of deepstream decoder(efficient way than opencv+gstreamer): the one way is we need to write custom element of processing and register in gstreamer and then put the custom element in the pipeline and then do processing on frames buffer. this way is good but need to write and knowledge gstreamer programming. this way is same way of deep stream. the second way is we use only decoded of frames from that link, then passed the frames into custom processor units. for this part I have two question: 1- The loop of gstreamer is same as asyncio programming loop? 2- As you know, If we add additional operation into pad prob function, this cause drop performance, but I want to know, Is it possible to put the frames in the pad prob function and do loop.create_task(process(frame)) like async? this cause we here don't wait to perform processing. like this: def tiler_sink_pad_buffer_probe(pad,info,u_data): .... ### capture the frames in GPU buffer without converting into numpy loop.create_task(process(frame)) .... return Gst.PadProbeReturn.OK",
        "answers": [
            [
                "Yeap gstreamer loop in Python is asyncio Well you can do this like me (bad way by creating global variables) ws = None loopIO = None def tiler_sink_pad_buffer_probe(pad,info,u_data): global ws global loopIO .... ### capture the frames in GPU buffer converting into numpy if ws and loopIO: _, jpeg_frame = cv2.imencode('.jpg', frame_image) str_pic = jpeg_frame.tobytes() asyncio.run_coroutine_threadsafe(ws.send(str_pic), loopIO) .... return Gst.PadProbeReturn.OK if __name__ == '__main__': start_server = websockets.serve(consumer_handler, 'localhost', 8765) loopIO = asyncio.get_event_loop() loopIO.run_until_complete(start_server) wst = threading.Thread(target=asyncio.get_event_loop().run_forever) wst.daemon = True wst.start() sys.exit(main(sys.argv))"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Closed. This question needs details or clarity. It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post. Closed 2 years ago. Improve this question I want to use the microphone in my logitech c525 webcamera for speech recognition, on my jetson nano. I can detect the microphone and seems to work in the sound app. But im not able to find it when using python code. I have tried gstreamer, this just made a mp4 file with a high pitch noice that lasted longer than i recorded for. I have also tried speech_recognition module, with pyaudio. Where speech_recognition.Microphone() should connect to the microphone input, here i get no response. Any inputs or tips would be much appreciated",
        "answers": [
            [
                "As recommended in documentation you need to list microphone names first to figure out which microphone is mapped to your USB one: &gt;&gt;&gt; sr.Microphone.list_microphone_names() ['HDA Intel PCH: ALC272 Analog (hw:0,0)', 'HDA Intel PCH: HDMI 0 (hw:0,3)', 'sysdefault', 'front', 'surround40', 'surround51', 'surround71', 'hdmi', 'pulse', 'dmix', 'default'] Once you know the device index you can select it in pyaudio &gt;&gt;&gt; # This is just an example; do not run &gt;&gt;&gt; mic = sr.Microphone(device_index=3) See also here."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm running a CSI camera on Nvidia Jetson Nano board. The camera streaming is done via GStreamer. Everything works fine but I wanted to reduce the time for opening the GStreamer channel. It takes more than 1 sec to open the GStreamer channel (cv2.VideoCapture(cam)). I can reduce the resolution but I wanted to capture images at the highest resolution. Here is the snippet camSet='nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3264, height=2464, format=NV12, framerate=21/1 ! nvvidconv flip-method='+str(flip)+' ! video/x-raw, width='+str(dispW)+', height='+str(dispH)+', format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink' start_time = time.time() cam = cv2.VideoCapture(camSet) print(\"Elpsed Time\", time.time() - start_time) Elapsed Time 1.0194542407989502 Is there any way to reduce this time?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I installed opencv version 4.1 following this guide. Looked like it completed fine, but when i test with: import cv2 #this works print(cv2.__version__) #but not this I get an attributeerror: module 'cv2' has no attribute 'version' This is on the jetson nano with jetpack",
        "answers": [
            [
                "What version of jetpack are you using? I suggest Jetpack 4.4. it comes with opencv 4.1. You don't need to build it from source. Once you install Jetpack 4.4 Make sure you install dependencies for python3 \"cv2\" $ sudo apt-get update $ sudo apt-get install -y build-essential make cmake cmake-curses-gui $ sudo apt-get install -y git g++ pkg-config curl libfreetype6-dev $ sudo apt-get install -y libcanberra-gtk-module libcanberra-gtk3-module $ sudo apt-get install -y python3-dev python3-testresources python3-pip $ sudo pip3 install -U pip $ cd ${HOME}/project/jetson_nano $ ./install_protobuf-3.8.0.sh $ sudo pip3 install numpy matplotlib Test now. Here is a full set up on Jetpack 4.2 including building opencv from source https://github.com/T-DevH/jetson-nano-tfdev (you won't need to do that with Jetpack 4.4)"
            ],
            [
                "Fixed by removing opencv and do another build."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "How many incoming video feeds can Jetson Nano process simultaneously for people counting and object detection. Lets say I have 4 IP Camera, 2 camera feeds i need to do object detection and on other 2 I want to get people count. Do I need 4 nos of Jetson nano or can I use 1 board to process all 4 feeds.",
        "answers": [
            [
                "The jetson nano will do the job. The DeepStream SDK will allow you to optimize your video pipeline. It also provide pre-trained models and the Jetpack 4.4 comes with TensorRT. You should have no issue in running 4 cameras. You can use the same model for the 4 streams achieving detection, tracking (and counting). Here is the installationDS installation guide. You can play with the samples to run multiple streams. It's impressive for a little board."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm using a jetson nano I tried to convert the onnx model https://github.com/onnx/models/tree/master/vision/body_analysis/emotion_ferplus Ran into this error: https://user-images.githubusercontent.com/28679735/86281506-a75e5380-bbab-11ea-8608-9bf8e2f50cc6.png Additional Info: https://user-images.githubusercontent.com/28679735/86281617-d674c500-bbab-11ea-8bbe-16f6d3db7203.png",
        "answers": [
            [
                "After you create the model use this code: TRT_LOGGER = trt.Logger(trt.Logger.WARNING) EXPLICIT_BATCH = 1 &lt;&lt; (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH) with trt.Builder(TRT_LOGGER) as builder, builder.create_network(EXPLICIT_BATCH) as network, trt.OnnxParser(network, TRT_LOGGER) as parser: with open(\"modelfile.onnx\", 'rb') as model: if not parser.parse(model.read()): for error in range(parser.num_errors): print(parser.get_error(error)) engine = builder.build_cuda_engine(network) You can use the engine directly or save and reuse it later. with open(\"output.engine\", \"wb\") as f: f.write(engine.serialize())"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm working on a Jetson Nano, and trying to install pytorch 1.4.0 onto it to run some toy experiments. However, I'm running into a lot of trouble with this. After failing to leverage the prebuilt wheels, I've gone the way of building from scratch, but after a couple hours, it fails with the following error. [3249/3931] Building NVCC (Device) object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/torch_cuda_generated_Unique.cu.o FAILED: caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/torch_cuda_generated_Unique.cu.o cd /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda &amp;&amp; /usr/bin/cmake -E make_directory /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/. &amp;&amp; /usr/bin/cmake -D verbose:BOOL=OFF -D build_configuration:STRING=Release -D generated_file:STRING=/home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/./torch_cuda_generated_Unique.cu.o -D generated_cubin_file:STRING=/home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/./torch_cuda_generated_Unique.cu.o.cubin.txt -P /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/torch_cuda_generated_Unique.cu.o.Release.cmake Killed CMake Error at torch_cuda_generated_Unique.cu.o.Release.cmake:281 (message): Error generating file /home/workingdir/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/./torch_cuda_generated_Unique.cu.o Does anyone know how to interpret this? Did I run out of memory/swap space? Additionally, if anyone knows of an easier way to get pytorch&gt;=1.1.0 on my nano, any tips would be appreciated :) I followed this thread here both for the prebuilt installation and the scratch installation: https://forums.developer.nvidia.com/t/pytorch-for-jetson-nano-version-1-5-0-now-available/72048",
        "answers": [],
        "votes": []
    },
    {
        "question": "My objective is to train a very simple CNN on MNIST using Tensorflow, convert it to TensorRT, and use it to perform inference on the MNIST test set using TensorRT, all on a Jetson Nano, but I am getting several errors and warnings, including \u201cOutOfMemory Error in GpuMemory: 0\u201d. To try and reduce memory footprint, I tried also creating a script where I simply load the TensorRT model (that had already been converted and saved in the previous script) and use it to perform inference on a small subset of the MNIST test set (100 floating point values), but I am still getting the same out of memory error. The entire directory containing the TensorRT model is only 488 KB, and the 100 test points can\u2019t be taking up very much memory, so I am confused about why GPU memory is running out. What could be the reason for this, and how can I solve it? Another thing which seems suspicious is that some of the Tensorflow logging info messages are being printed multiple times, EG \u201cSuccessfully opened dynamic library libcudart\u201d, \u201cSuccessfully opened dynamic library libcublas\u201d, \u201cARM64 does not support NUMA - returning NUMA node zero\u201d. What could be the reason for this (EG dynamic libraries being opened over and over again), and could this have something to do with why the GPU memory keeps running out? Shown below are the 2 Python scripts; the console output from each one is too long to post on Stack Overflow, but they can be seen attached to this Gist: https://gist.github.com/jakelevi1996/8a86f2c2257001afc939343891ee5de7 \"\"\" Example script which trains a simple CNN for 1 epoch on a subset of MNIST, and converts the model to TensorRT format, for enhanced performance which fully utilises the NVIDIA GPU, and then performs inference. Useful resources: - https://stackoverflow.com/questions/58846828/how-to-convert-tensorflow-2-0-savedmodel-to-tensorrt - https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#worflow-with-savedmodel - https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter - https://github.com/tensorflow/tensorflow/issues/34339 - https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/image-classification/image_classification.py Tested on the NVIDIA Jetson Nano, Python 3.6.9, tensorflow 2.1.0+nv20.4, numpy 1.16.1 \"\"\" import os from time import perf_counter import numpy as np t0 = perf_counter() import tensorflow as tf from tensorflow.keras import datasets, layers, models, Input from tensorflow.python.compiler.tensorrt import trt_convert as trt from tensorflow.python.saved_model import signature_constants from tensorflow.python.saved_model import tag_constants from tensorflow.python.framework import convert_to_constants tf.compat.v1.enable_eager_execution() # see github issue above # Get training and test data (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() x_train = np.expand_dims(x_train, -1) / 255.0 x_test = np.expand_dims(x_test, -1) / 255.0 # Create model model = models.Sequential() # model.add(Input(shape=x_train.shape[1:], batch_size=batch_size)) model.add(layers.Conv2D(10, (5, 5), activation='relu', padding=\"same\")) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Flatten()) model.add(layers.Dense(10)) # Compile and train model model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) model.fit( x_train[:10000], y_train[:10000], validation_data=(x_test, y_test), batch_size=100, epochs=1, ) # Save model print(\"Saving model...\") current_dir = os.path.dirname(os.path.abspath(__file__)) model_dir = os.path.join(current_dir, \"CNN_MNIST\") if not os.path.isdir(model_dir): os.makedirs(model_dir) # model.save(model_dir) tf.saved_model.save(model, model_dir) # Convert to TRT format trt_model_dir = os.path.join(current_dir, \"CNN_MNIST_TRT\") converter = trt.TrtGraphConverterV2(input_saved_model_dir=model_dir) converter.convert() converter.save(trt_model_dir) t1 = perf_counter() print(\"Finished TRT conversion; time taken = {:.3f} s\".format(t1 - t0)) # Make predictions using saved model, and print the results (NB using an alias # for tf.saved_model.load, because the normal way of calling this function # throws an error because for some reason it is expecting a sess) saved_model_loaded = tf.compat.v1.saved_model.load_v2( export_dir=trt_model_dir, tags=[tag_constants.SERVING]) graph_func = saved_model_loaded.signatures[ signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] graph_func = convert_to_constants.convert_variables_to_constants_v2(graph_func) x_test_tensor = tf.convert_to_tensor(x_test, dtype=tf.float32) preds = graph_func(x_test_tensor)[0].numpy() print(preds.shape, y_test.shape) accuracy = list(preds.argmax(axis=1) == y_test).count(True) / y_test.size print(\"Accuracy of predictions = {:.2f} %\".format(accuracy * 100)) \"\"\" Example script which trains a simple CNN for 1 epoch on a subset of MNIST, and converts the model to TensorRT format, for enhanced performance which fully utilises the NVIDIA GPU. Useful resources: - https://stackoverflow.com/questions/58846828/how-to-convert-tensorflow-2-0-savedmodel-to-tensorrt - https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#worflow-with-savedmodel - https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter - https://github.com/tensorflow/tensorflow/issues/34339 - https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/image-classification/image_classification.py Tested on the NVIDIA Jetson Nano, Python 3.6.9, tensorflow 2.1.0+nv20.4, numpy 1.16.1 \"\"\" import os from time import perf_counter import numpy as np t0 = perf_counter() import tensorflow as tf from tensorflow.keras import datasets from tensorflow.python.saved_model import signature_constants from tensorflow.python.saved_model import tag_constants from tensorflow.python.framework import convert_to_constants tf.compat.v1.enable_eager_execution() # see github issue above # Get training and test data (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() x_train = np.expand_dims(x_train, -1) / 255.0 x_test = np.expand_dims(x_test, -1) / 255.0 # TEMPORARY: just use 100 test points to minimise GPU memory num_points = 100 x_test, y_test = x_test[:num_points], y_test[:num_points] current_dir = os.path.dirname(os.path.abspath(__file__)) trt_model_dir = os.path.join(current_dir, \"CNN_MNIST_TRT\") # Make predictions using saved model, and print the results (NB using an alias # for tf.saved_model.load, because the normal way of calling this function # throws an error because for some reason it is expecting a sess) saved_model_loaded = tf.compat.v1.saved_model.load_v2( export_dir=trt_model_dir, tags=[tag_constants.SERVING]) graph_func = saved_model_loaded.signatures[ signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] graph_func = convert_to_constants.convert_variables_to_constants_v2(graph_func) x_test_tensor = tf.convert_to_tensor(x_test, dtype=tf.float32) preds = graph_func(x_test_tensor)[0].numpy() print(preds.shape, y_test.shape) accuracy = list(preds.argmax(axis=1) == y_test).count(True) / y_test.size print(\"Accuracy of predictions = {:.2f} %\".format(accuracy * 100)) t1 = perf_counter() print(\"Finished inference; time taken = {:.3f} s\".format(t1 - t0))",
        "answers": [
            [
                "I had the same error on a Jetson Tx2. I think it comes from the shared memory between the GPU and the CPU, tensorflow doesn't allow enough memory or the os limit the allocation. To fix this, you can allow memory growth: gpus = tf.config.experimental.list_physical_devices('GPU') if gpus: try: # Currently, memory growth needs to be the same across GPUs for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices('GPU') print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\") except RuntimeError as e: # Memory growth must be set before GPUs have been initialized print(e) Or you can force tensorflow to allocate enough memory: gpus = tf.config.experimental.list_physical_devices('GPU') if gpus: # Restrict TensorFlow to only allocate 1GB of memory on the first GPU try: tf.config.experimental.set_virtual_device_configuration( gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)]) logical_gpus = tf.config.experimental.list_logical_devices('GPU') print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\") except RuntimeError as e: # Virtual devices must be set before GPUs have been initialized print(e) Those example comes from https://www.tensorflow.org/guide/gpu"
            ],
            [
                "I see in logs that it created GPU device with 600 Mb: Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 638 MB memory) And then it tried to allocate 1Gb: Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.00GiB (rounded to 1073742336). Also it's clear. that GPU device has more memory than 600Mb. It's visible here in the logs: 2020-06-23 23:06:36.463934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:00:00.0 name: NVIDIA Tegra X1 computeCapability: 5.3 coreClock: 0.9216GHz coreCount: 1 deviceMemorySize: 3.87GiB deviceMemoryBandwidth: 194.55MiB/s So maybe your GPU is running some other calculation?"
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001
        ]
    },
    {
        "question": "I have a couple of questions on this piece of code, running on a jetson nano: #include \"stdio.h\" #include \"unistd.h\" #include \"stdlib.h\" float gputemp = 0; float cputemp = 0; int count = 0; int main() { char* cpu; char* gpu; cpu = (char*)malloc(sizeof(char)*6); gpu = (char*)malloc(sizeof(char)*6); while (1) { FILE* fcputemp = fopen(\"/sys/devices/virtual/thermal/thermal_zone1/temp\", \"r\"); FILE* fgputemp = fopen(\"/sys/devices/virtual/thermal/thermal_zone2/temp\",\"r\"); if (!fcputemp || !fgputemp ) { printf(\"Something went wrong\\n\"); exit(EXIT_FAILURE); } cputemp = atoi(fgets(cpu, 6, fcputemp))/1000; gputemp = atoi(fgets(gpu, 6, fgputemp))/1000; printf(\"\\rCpu : %.2f, Gpu : %.2f. Elapsed time : %d\", cputemp, gputemp, count); fflush(stdout); fclose(fcputemp); fclose(fgputemp); count++; sleep(1); } } Here I have to open, get the temperatures, and then close the file each loop iteration in order to get valid data (and dont segfault). My concern here is the number of (expensive) kernel switches needed to do this. I know that premature optimization is evil, but there is another way (or maybe the RIGHT way) to do that, opening the file only once? And why the sensor interface (the file) cant update itself if I have it open? P.S: Yes, I know, I didnt free cpu nor gpu variables, this is only \"demo\" code (just watch how i measure the time passed lol)",
        "answers": [
            [
                "I'm not sure you can do this opening the files once and once only. You could try rewinding, but sysfs isn't a \"real\" filesystem and those aren't real files. If you rewind you might get the same data over and over, especially when using buffered calls like fopen(). The open operation is what prepares that data for reading. Since this is all managed by the kernel it should have very little overhead, and no actual disk activity. Consider that programs like top read thousands of these every second and it's no big deal."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I was following one of the online tutorials but I was getting this error: Traceback (most recent call last): File \u201cssd_object_detection.py\u201d, line 20, in detections = net.forward() cv2.error: OpenCV(4.3.0) /home/blah/opencv/modules/dnn/src/layers/\u2026/cuda4dnn/primitives/\u2026/csl/cudnn/convolution.hpp:461: error: (-217:Gpu API call) CUDNN_STATUS_EXECUTION_FAILED in function \u2018convolve_with_bias_activation\u2019 It's a python script and I use Opencv dnn module with a pre-trained model This is my configuration: Jetson Nano device Ubuntu 18.04 /usr/local/cuda/bin/nvcc --version nvcc: NVIDIA \u00ae Cuda compiler driver Copyright \u00a9 2005-2019 NVIDIA Corporation Built on Wed_Oct_23_21:14:42_PDT_2019 Cuda compilation tools, release 10.2, V10.2.89 \u2013 NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH) \u2013 NVIDIA GPU arch: 53 \u2013 NVIDIA PTX archs: \u2013 cuDNN: YES (ver 8.0) \u2013 NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH) \u2013 NVIDIA GPU arch: 53 \u2013 cuDNN: YES (ver 8.0) opencv 4.3.0 built from source with OPENCV_DNN_CUDA=ON, CUDNN_VERSION=\u20188.0\u2019, WITH_CUDA=ON, WITH_CUDNN=ON, and many other settings enabled Python 3.7.7 This is a snippet of the code I am trying to run (it completes successfully if I don\u2019t use the GPU). It fails at the line detections = net.forward() CLASSES = [\u201cbackground\u201d, \u201caeroplane\u201d] COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3)) net = cv2.dnn.readNetFromCaffe(args[\u201cprototxt\u201d], args[\u201cmodel\u201d]) print(\"[INFO] setting preferable backend and target to CUDA\u2026\") net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA) net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA) print(\"[INFO] accessing video stream\u2026\") vs = cv2.VideoCapture(args[\u201cinput\u201d] if args[\u201cinput\u201d] else 0) writer = None fps = FPS().start() while True: (grabbed, frame) = vs.read() frame = imutils.resize(frame, width=400) (h, w) = frame.shape[:2] blob = cv2.dnn.blobFromImage(frame, 0.007843, (300, 300), 127.5) net.setInput(blob) detections = net.forward() for i in np.arange(0, detections.shape[2]): ....",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to create cross-compiling tools for my c++ application to compile it and push binaries to arm64 device (jetson-nano) during image building. I've added inherit populate_sdk_qt5 line to my own recipe and bitbake -c populate_sdk &lt;my-recipe&gt;. I've got no errors so I run .sh file I've got as an output. It works fine but I cannot find qmake in destination path (tried with find ./ -name 'qmake' without any output). Have you ever had similar situation? Do you have some advice for me? Thanks!",
        "answers": [
            [
                "Try to sync your meta-tegra. There was an update just a few days ago."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I posted about this over on the Isaac forums, but listing it here for visibility as well. I am trying to get the Isaac Realsense examples working on a Jetson Nano with my 435i (firmware downgraded to 5.11.15 per the Isaac documentation), but I've been unable to so far. I've got a Nano flashed with Jetpack4.3 and have installed all dependencies on both the desktop and the Nano. The realsense-viewer works fine, so I know the camera is functioning properly and is being detected by the Nano. However, when I run ./apps/samples/realsense_camera/realsense_camera it throws an error: ERROR engine/alice/components/Codelet.cpp@229: Component 'camera/realsense' of type 'isaac::RealsenseCamera' reported FAILURE: No device connected, please connect a RealSense device ERROR engine/alice/backend/event_manager.cpp@42: Stopping node 'camera' because it reached status 'FAILURE' I've attached the log of this output as well. I get the same error running locally on my desktop, but that's running through WSL so I was willing to write that off. Any suggestions would be greatly appreciated! 0m2020-06-15 17:18:20.620 INFO engine/alice/tools/websight.cpp@166: Loading websight...0m 33m2020-06-15 17:18:20.621 WARN engine/alice/backend/application_json_loader.cpp@174: This application does not have an explicit scheduler configuration. One will be autogenerated to the best of the system's abilities if possible.0m 0m2020-06-15 17:18:20.622 INFO engine/alice/backend/redis_backend.cpp@40: Successfully connected to Redis server. 0m 33m2020-06-15 17:18:20.623 WARN engine/alice/backend/backend.cpp@201: This application does not have an execution group configuration. One will be autogenerated to the best of the systems abilities if possible.0m 33m2020-06-15 17:18:20.623 WARN engine/gems/scheduler/scheduler.cpp@337: No default execution groups specified. Attempting to create scheduler configuration for 4 remaining cores. This may be non optimal for the system and application.0m 0m2020-06-15 17:18:20.623 INFO engine/gems/scheduler/scheduler.cpp@290: Scheduler execution groups are:0m 0m2020-06-15 17:18:20.623 INFO engine/gems/scheduler/scheduler.cpp@299: __BlockerGroup__: Cores = [3], Workers = No0m 0m2020-06-15 17:18:20.623 INFO engine/gems/scheduler/scheduler.cpp@299: __WorkerGroup__: Cores = [0, 1, 2], Workers = Yes0m 0m2020-06-15 17:18:20.660 INFO engine/alice/backend/modules.cpp@226: Loaded module 'packages/realsense/librealsense_module.so': Now has 45 components total0m 0m2020-06-15 17:18:20.679 INFO engine/alice/backend/modules.cpp@226: Loaded module 'packages/rgbd_processing/librgbd_processing_module.so': Now has 51 components total0m 0m2020-06-15 17:18:20.696 INFO engine/alice/backend/modules.cpp@226: Loaded module 'packages/sight/libsight_module.so': Now has 54 components total0m 0m2020-06-15 17:18:20.720 INFO engine/alice/backend/modules.cpp@226: Loaded module 'packages/viewers/libviewers_module.so': Now has 83 components total0m 90m2020-06-15 17:18:20.720 DEBUG engine/alice/application.cpp@348: Loaded 83 components: isaac::RealsenseCamera, isaac::alice::BufferAllocatorReport, isaac::alice::ChannelMonitor, isaac::alice::CheckJetsonPerformanceModel, isaac::alice::CheckOperatingSystem, isaac::alice::Config, isaac::alice::ConfigBridge, isaac::alice::ConfigLoader, isaac::alice::Failsafe, isaac::alice::FailsafeHeartbeat, isaac::alice::InteractiveMarkersBridge, isaac::alice::JsonToProto, isaac::alice::LifecycleReport, isaac::alice::MessageLedger, isaac::alice::MessagePassingReport, isaac::alice::NodeStatistics, isaac::alice::Pose, isaac::alice::Pose2Comparer, isaac::alice::PoseFromFile, isaac::alice::PoseInitializer, isaac::alice::PoseMessageInjector, isaac::alice::PoseToFile, isaac::alice::PoseToMessage, isaac::alice::PoseTree, isaac::alice::PoseTreeJsonBridge, isaac::alice::PoseTreeRelink, isaac::alice::ProtoToJson, isaac::alice::PyCodelet, isaac::alice::Random, isaac::alice::Recorder, isaac::alice::RecorderBridge, isaac::alice::Replay, isaac::alice::ReplayBridge, isaac::alice::Scheduling, isaac::alice::Sight, isaac::alice::SightChannelStatus, isaac::alice::Subgraph, isaac::alice::Subprocess, isaac::alice::TcpPublisher, isaac::alice::TcpSubscriber, isaac::alice::Throttle, isaac::alice::TimeOffset, isaac::alice::TimeSynchronizer, isaac::alice::UdpPublisher, isaac::alice::UdpSubscriber, isaac::map::Map, isaac::map::ObstacleAtlas, isaac::map::OccupancyGridMapLayer, isaac::map::PolygonMapLayer, isaac::map::WaypointMapLayer, isaac::navigation::DistanceMap, isaac::navigation::NavigationMap, isaac::navigation::RangeScanModelClassic, isaac::navigation::RangeScanModelFlatloc, isaac::rgbd_processing::DepthEdges, isaac::rgbd_processing::DepthImageFlattening, isaac::rgbd_processing::DepthImageToPointCloud, isaac::rgbd_processing::DepthNormals, isaac::rgbd_processing::DepthPoints, isaac::rgbd_processing::FreespaceFromDepth, isaac::sight::AliceSight, isaac::sight::SightWidget, isaac::sight::WebsightServer, isaac::viewers::BinaryMapViewer, isaac::viewers::ColorCameraViewer, isaac::viewers::DepthCameraViewer, isaac::viewers::Detections3Viewer, isaac::viewers::DetectionsViewer, isaac::viewers::FiducialsViewer, isaac::viewers::FlatscanViewer, isaac::viewers::GoalViewer, isaac::viewers::ImageKeypointViewer, isaac::viewers::LidarViewer, isaac::viewers::MosaicViewer, isaac::viewers::ObjectViewer, isaac::viewers::OccupancyMapViewer, isaac::viewers::PointCloudViewer, isaac::viewers::PoseTrailViewer, isaac::viewers::SegmentationCameraViewer, isaac::viewers::SegmentationViewer, isaac::viewers::SkeletonViewer, isaac::viewers::TensorViewer, isaac::viewers::TrajectoryListViewer, 0m 33m2020-06-15 17:18:20.723 WARN engine/alice/application.cpp@164: The function Application::findComponentByName is deprecated. Please use `getNodeComponentOrNull` instead. Note that the new method requires a node name instead of a component name. (argument: 'websight/isaac.sight.AliceSight')0m 0m2020-06-15 17:18:20.723 INFO engine/alice/application.cpp@255: Starting application 'realsense_camera' (instance UUID: 'e24992d0-af66-11ea-8bcf-c957460c567e') ...0m 90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@476: Launching 0 pre-start job(s)0m 90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@485: Replaying 0 pre-start event(s)0m 90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@476: Launching 0 pre-start job(s)0m 90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@485: Replaying 0 pre-start event(s)0m 0m2020-06-15 17:18:20.723 INFO engine/alice/backend/asio_backend.cpp@33: Starting ASIO service0m 0m2020-06-15 17:18:20.727 INFO packages/sight/WebsightServer.cpp@216: Sight webserver is loaded0m 0m2020-06-15 17:18:20.727 INFO packages/sight/WebsightServer.cpp@217: Please open Chrome Browser and navigate to http://&lt;ip address&gt;:30000m 33m2020-06-15 17:18:20.727 WARN engine/alice/backend/codelet_canister.cpp@225: Codelet 'websight/isaac.sight.AliceSight' was not added to scheduler because no tick method is specified.0m 33m2020-06-15 17:18:20.728 WARN engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m 33m2020-06-15 17:18:20.728 WARN engine/alice/backend/codelet_canister.cpp@225: Codelet '_check_operating_system/isaac.alice.CheckOperatingSystem' was not added to scheduler because no tick method is specified.0m 33m2020-06-15 17:18:20.728 WARN engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m 33m2020-06-15 17:18:20.730 WARN engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m 1;31m2020-06-15 17:18:20.741 ERROR engine/alice/components/Codelet.cpp@229: Component 'camera/realsense' of type 'isaac::RealsenseCamera' reported FAILURE: No device connected, please connect a RealSense device 0m 1;31m2020-06-15 17:18:20.741 ERROR engine/alice/backend/event_manager.cpp@42: Stopping node 'camera' because it reached status 'FAILURE'0m 33m2020-06-15 17:18:20.743 WARN engine/alice/backend/codelet_canister.cpp@225: Codelet 'camera/realsense' was not added to scheduler because no tick method is specified.0m 0m2020-06-15 17:18:21.278 INFO packages/sight/WebsightServer.cpp@113: Server connected / 10m 0m2020-06-15 17:18:30.723 INFO engine/alice/backend/allocator_backend.cpp@57: Optimized memory CPU allocator.0m 0m2020-06-15 17:18:30.724 INFO engine/alice/backend/allocator_backend.cpp@66: Optimized memory CUDA allocator.0m",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am running a raspberry pi camera on the Jetson Nano with Python 3.6.9, OpenCV version 4.1.0, and the latest version of Ubuntu. I am running the code: import cv2 print(cv2.__version__) dispW=320 dispH=240 flip=2 cam_set='nvarguscamerasrc ! video/x-raw(memory:NVMM), width=3264, height=2464, format=NV12, framerate=21/1 ! nvvidconv flip-method='+str(flip)+' ! video/x-raw, width='+str(dispW)+', height='+str(dispH)+', format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink' cam = cv2.VideoCapture(cam_set) while True: ret, frame = cv2.imread(cam) cv2.imshow('Camera',frame) if cv2.waitKey(1)==ord('q'): break cam.release() cv2.destroyAllWindows() When I run it, it throws an error with a traceback to line 11, saying that \"ret, frame = cv2.imread(cam)\" has a TypeError: bad argument type for built-in operation. I can use opencv to load images, but running a video always seems to throw this error. \"ls -l /dev/video0\" in the command line works fine, so I know I am connected to the camera. Thanks!",
        "answers": [
            [
                "As per documentation: Python: cv2.imread(filename[, flags]) \u2192 retval imread expects a path while you need this snippet: import numpy as np import cv2 cap = cv2.VideoCapture(0) while(True): # Capture frame-by-frame ret, frame = cap.read() # Our operations on the frame come here gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Display the resulting frame cv2.imshow('frame',gray) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break # When everything done, release the capture cap.release() cv2.destroyAllWindows()"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am currently trying to run Nvidia-docker on Jetson Xavier and jetson nano with the Tensorflow framework enabled inside. but the problem I\u2019m facing right now is related to \u201clibcublas.so\u201d. What I had tried the solution mentioned here: https://devtalk.nvidia.com/default/topic/1043951/jetson-agx-xavier/docker-gpu-acceleration-on-jetson-agx-for-ubuntu-18-04-image/post/5296647/#5296647 1 All package installations (pip installs and apt-get installs) completed successfully but when I try to import TensorFlow from both Python 2.7 or 3.6, I get the following error: ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory using Jetson Xavier or jetson nano?",
        "answers": [
            [
                "sudo docker run --net=host --rm --runtime nvidia --ipc=host -v /tmp/.X11-unix/:/tmp/.X11-unix /tmp/argus_socket:/tmp/argus_socket --cap-add SYS_PTRACE -e DISPLAY=$DISPLAY -it [container] I'm assuming it would work on Host first. Source: official forum but up to date. Device is no longer used"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "i am working on jetson nano and utilizing the jetson library provided by hello ai world. i have tested the basic algo which comes with the library all of them are working fine but i did a few changings to make it run on a video got an error here is my code import jetson.inference import jetson.utils import cv2 #import argparse import sys import numpy as np width=720 height=480 vs=cv2.VideoCapture('b.m4v') #video input file net = jetson.inference.detectNet(\"ssd-mobilenet-v2\", threshold=0.5) #loading the model #camera = jetson.utils.gstCamera(1280, 720, \"/dev/video0\") #using V4L2 display = jetson.utils.glDisplay() #initialting a display window while display.IsOpen(): _,frame = vs.read() #reading a frmae img = cv2.cvtColor(frame, cv2.COLOR_BGR2BGRA) #converting it to a bgra format for jetson util img = jetson.utils.cudaFromNumpy(img) #converting image to cuda format from numpy array #img, width, height = camera.CaptureRGBA() detections = net.Detect(img, width, height) #running detections on each image and saving the results in detections display.RenderOnce(img, width, height) #display the output frame with detection display.SetTitle(\"Object Detection | Network {:.0f} FPS\".format(net.GetNetworkFPS())) #display title for the output feed this is the error: [OpenGL] glDisplay -- X screen 0 resolution: 1280x1024 [OpenGL] failed to create X11 Window. jetson.utils -- PyDisplay_Dealloc() Traceback (most recent call last): File \"pool1.py\", line 16, in &lt;module&gt; display = jetson.utils.glDisplay() #initialting a display window Exception: jetson.utils -- failed to create glDisplay device PyTensorNet_Dealloc()",
        "answers": [
            [
                "It's not technically an answer but I think here is the problem. Here is the log of my faulty working code [TRT] binding to input 0 Input binding index: 0 [TRT] binding to input 0 Input dims (b=1 c=3 h=300 w=300) size=1080000 [TRT] binding to output 0 NMS binding index: 1 [TRT] binding to output 0 NMS dims (b=1 c=1 h=100 w=7) size=2800 [TRT] binding to output 1 NMS_1 binding index: 2 [TRT] binding to output 1 NMS_1 dims (b=1 c=1 h=1 w=1) size=4 device GPU, /usr/local/bin/networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff initialized. W = 7 H = 100 C = 1 detectNet \u2013 maximum bounding boxes: 100 detectNet \u2013 loaded 91 class info entries detectNet \u2013 number of object classes: 91 jetson.utils \u2013 PyDisplay_New() jetson.utils \u2013 PyDisplay_Init() [OpenGL] glDisplay \u2013 X screen 0 resolution: 1280x1024 [OpenGL] failed to create X11 Window. jetson.utils \u2013 PyDisplay_Dealloc() Traceback (most recent call last): File \u201cpool1.py\u201d, line 18, in display = jetson.utils.glDisplay() #initialting a display window Exception: jetson.utils \u2013 failed to create glDisplay device PyTensorNet_Dealloc() and here is the log of the builtin alog working [TRT] binding to output 0 NMS dims (b=1 c=1 h=100 w=7) size=2800 [TRT] binding to output 1 NMS_1 binding index: 2 [TRT] binding to output 1 NMS_1 dims (b=1 c=1 h=1 w=1) size=4 device GPU, /usr/local/bin/networks/SSD-Mobilenet-v2/ssd_mobilenet_v2_coco.uff initialized. W = 7 H = 100 C = 1 detectNet \u2013 maximum bounding boxes: 100 detectNet \u2013 loaded 91 class info entries detectNet \u2013 number of object classes: 91 jetson.utils \u2013 PyCamera_New() jetson.utils \u2013 PyCamera_Init() [gstreamer] initialized gstreamer, version 1.14.5.0 [gstreamer] gstCamera attempting to initialize with GST_SOURCE_NVARGUS, camera /dev/video0 [gstreamer] gstCamera pipeline string: v4l2src device=/dev/video0 ! video/x-raw, width=(int)1280, height=(int)720, format=YUY2 ! videoconvert ! video/x-raw, format=RGB ! videoconvert !appsink name=mysink [gstreamer] gstCamera successfully initialized with GST_SOURCE_V4L2, camera /dev/video0 jetson.utils \u2013 PyDisplay_New() jetson.utils \u2013 PyDisplay_Init() [OpenGL] glDisplay \u2013 X screen 0 resolution: 1280x1024 [OpenGL] glDisplay \u2013 display device initialized [gstreamer] opening gstCamera for streaming, transitioning pipeline to GST_STATE_PLAYING [gstreamer] gstreamer changed state from NULL to READY ==&gt; mysink [gstreamer] gstreamer changed state from NULL to READY ==&gt; videoconvert1 [gstreamer] gstreamer changed state from NULL to READY ==&gt; capsfilter1 [gstreamer] gstreamer changed state from NULL to READY ==&gt; videoconvert0 You can clearly see the problem. My main query is is gstramer even supported by opencv 4.1.1 on nano"
            ],
            [
                "try #import cv2 or put it on the top\uff0c in front of import jetson.utils"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "Having an issue on migrating a Qt project from RPi4 to NVIDIA Jetson Nano. We wanted to migrate our project to Jetson Nano to improve image processing performance. The qt dependency of our project is &gt;= 5.11. But Jetson Nano uses Ubuntu 18.04.4 and it has qt5-default package pre-installed in it (in my understanding some system files use it). And this qt5-default packages version is 5.9.5. I tried to downgrade my qt dependency, but every change made lead to harder to fix issue. I tried to upgrade default qt5 version but couldn't find any similar guidance. The guides/questions already exists are about x86 etc. environment. Couldn't find any ARM based solution. The qt downloads doesn't give any buildable for ARM env (or I can't find them). The official documents only talks about cross-compiling. What should I do to overcome this issue? Thanks in advance.",
        "answers": [
            [
                "Okay I finally was able to successfully compile QT 5.12.9 on the Nano itself (no cross compilation). The steps I did: git clone https://code.qt.io/qt/qt5.git cd qt5 git checkout 5.12.9 Then git submodule update --init --recursive cd ~ mkdir qt5-build cd qt5-build Configure and build ../qt5/configure -nomake examples -nomake tests -skip qtwebengine make sudo make install Make took like nearly a whole day to compile all sources. Also I had some compilation errors before. However after skipping webengine and not building the tests and examples in ./configure I was finally able to sucessfully make it."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have NVIDIA Jetson Nano and FullHD Ip camera. Camera streams RTSP/h264. I want to decode frames in python script from this camera for analizies. So, i tried use something like that: # import the necessary packages from imutils.video import VideoStream import imutils import time import cv2 # grab a reference to the webcam print(\"[INFO] starting video stream...\") #vs = VideoStream(src=0).start() vs = VideoStream(src=\"rtsp://login:password@192.168.1.180\").start() time.sleep(2.0) # loop over frames while True: # grab the next frame frame = vs.read() # resize the frame to have a maximum width of 500 pixels frame = imutils.resize(frame, width=500) # show the output frame cv2.imshow(\"Frame\", frame) key = cv2.waitKey(1) &amp; 0xFF # if the `q` key was pressed, break from the loop if key == ord(\"q\"): break # release the video stream and close open windows vs.stop() cv2.destroyAllWindows() That's works, but in that way frame decoded on CPU. How to use GPU decoder?",
        "answers": [
            [
                "The solution: Use cv2.VideoCapture with GStreamer backend: import cv2 pipeline = \"rtspsrc location=\\\"rtsp://login:password@host:port/\\\" ! rtph264depay ! h264parse ! omxh264dec ! nvvidconv ! video/x-raw, format=(string)BGRx! videoconvert ! appsink\" capture = cv2.VideoCaputure(pipeline, cv2.CAP_GSTREAMER) while capture.isOpened(): res, frame = capture.read() cv2.imshow(\"Video\", frame) key = cv2.waitKey(1) &amp; 0xFF if key == ord(\"q\"): break capture.release() cv2.destroyAllWindows()"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Every time I put it in suspend, I have to restart it. I tried clicking keyboard and mouse buttons, but no luck. I also tried pressing reset, power and recovery button on the device. Power just restarts the device instead of recovering from the suspended state. Please help.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm looking to use WebRTC in a project where I need to livestream video from one computer, a robot with a wifi hotspot and USB camera, over a local network to another computer, a controller for the robot. Both computers in this case are single board computers and the video stream is one-way. I'd also like the two computers to communicate data to each other in both directions. I found WebRTC in my research and it seems to have both of these functionalities that I'm looking for. From initial testing with the demo NodeJS programs, WebRTC seems to be almost exactly what I'm looking for. One big bump I've been trying to get over though is that it seems to require by default an online signaling server to be used between the two clients. I researched and found that it's possible to create the signaling server within the local network with a third computer, but such a setup isn't appropriate for my project. I've looked into other solutions and came across OpenVidu which implements WebRTC but allows the signaling server to be built into one of the client computers (at least that's what I've read in another stack overflow post). I have yet to test this tool to see if it resolves my issue, but I'm blocked currently by the docker image used by the project not supporting ARM processors, which are employed by my single board computers, and thus requiring special installation. Before I go through that process which may or may not work, I wanted to ask if there's another simpler solution to running WebRTC without internet access and without needing a third computer as a signaling server or if there were any suggestions for tools other than WebRTC that would be better for my application. I'm very new to the technology and could definitely be missing an easy or existing solution. Any help on this matter would be greatly appreciated. For those curious, the single board computers being used are NVidia Jetson Nanos.",
        "answers": [
            [
                "You can run a local TURN server on your LAN. It is very easy and straightforward. For e.g. turnserver, from coturn project. After install, just run turnserver -p 19302 Now your config file: const config = { iceServers: [ {urls: [\"stun:&lt;&lt;YOUR HOST IP&gt;&gt;:19302\"]} ] }"
            ],
            [
                "Found the solution. Using the Google Codelabs example, the fix involved just removing the default ice server in the config. Essentially, it meant changing const config = { iceServers: [ {urls: [\"stun:stun.l.google.com:19302\"]} ] } to const config = { iceServers: [] }"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am currently working on a Nvidia Jetson Nano. When I installed JetPack 4.4, OpenCV 4.1.1 was included inside and when I was running on my Nano's Python shell, I was able to at least run import cv2 and a few other functions without errors. However, when I was running one of my scripts, I kept encountering this certain error and I thought it was an issue with my OpenCV. This is because the tutorial that I was following, they mentioned that if ran cv2.getBuildInformation() it should include details of CUDA. At the time, my output did not contain CUDA so I thought something was wrong. Then, I've decided to reinstall OpenCV not thinking too much about it by following several tutorials, mainly the one by blogger Piggybank here. I did not uninstall anything as I thought that it would just simply overwrite it. However, the installation did not go through because it mentioned that my disk was running low on memory. I though that the entire operation would be aborted but apparently not. When I try to run import cv2 it gives me: Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; ModuleNotFoundError: No module named 'cv2' I tried to 'completely uninstall' OpenCV via pip and apt-get after but the commands returned saying that OpenCV does not exist. Not too sure what went wrong here and I don't understand I was able to run OpenCV smoothly before but now, I don't have enough memory for it. When I try to reinstall with the blog link above, it only reaches to about 50% of the entire installation process. Please help.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've had issues setting up the DeepstreamSdk IoTedge module on the jetson nano for the last week and I cannot get past it. I've installed IoTedge runtime and all the necessities for IoT edge to run. It runs perfectly including other modules like the simulated Temperature sensor. However when I deploy the DeepstreamSdk v-4.02 on the Jetson nano running Jetpack 4.3, it starts and runs for a couple of minutes but then fails unexpectedly and then after a bit of time starts up again and then fails again. And then Sometimes I restart the IoTedge and it will start up again and then fail. When I use the Developer extension IoTedge in VS code to see what the messages being sent up to the cloud are, I can see the temperature sensor module's messages, however none from the NvidiaDeepstream module. I've had a look in the logs for the NvidiaDeepstream container, and it shows that it is printing out results ( messages to the cloud), but then eventually sends an error code 1. and some sort of message at the end INT8 is not supported, try INT16. All the Azure checks and connectivity and configurations are correct. It is just the deepstreamdk module that doesn't run properly. Does anyone have any suggestions? What info should I provide to make this more clear and understandable? I am following the tutorial on the Github repository for NVIDIA Deepstream + Azure IoT Edge on a NVIDIA Jetson Nano: Link to turorial Link to logs of COntainers",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm currently working on a Nvidia Jetson Nano and I'm not very familiar with Linux. I am trying to run a python file which imports a package called torch. I have installed it alongside with torchvision while following the instructions from NVIDIA here. When I run pip list on my terminal, I am able to see torch listed as one of the packages installed. However, I am unable to run the python file due to the error seen below. When I try to run it on python shell, the same error pops up. FYI: Previously it had issues as the system was using python 2 by default but I have already fixed the path by switching to python 3 by editing the .bashrc file. &gt;&gt;&gt; import torch Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/home/jiayi/.local/lib/python3.6/site-packages/torch/__init__.py\", line 81, in &lt;module&gt; from torch._C import * ImportError: libcudart.so.10.0: cannot open shared object file: No such file or directory I have tried uninstalling and installing via pip but to no avail. When I try to install the pytorch package (following the instructions from a github repo here), an error occurs as seen below and it is due to the same issue. It is able to detect that the torch package is installed but there seems to be an internal issue. Requirement already satisfied: torch==1.4.0 from file:///home/jiayi/jetson-inference/build/torch-1.4.0-cp36-cp36m-linux_aarch64.whl in /home/jiayi/.local/lib/python3.6/site-packages (1.4.0) [jetson-inference] cloning torchvision... [sudo] password for jiayi: Cloning into 'torchvision-36'... remote: Enumerating objects: 71, done. remote: Counting objects: 100% (71/71), done. remote: Compressing objects: 100% (56/56), done. remote: Total 8219 (delta 37), reused 29 (delta 15), pack-reused 8148 Receiving objects: 100% (8219/8219), 10.22 MiB | 3.60 MiB/s, done. Resolving deltas: 100% (5631/5631), done. [jetson-inference] building torchvision for Python 3.6... Traceback (most recent call last): File \"setup.py\", line 14, in &lt;module&gt; import torch File \"/home/jiayi/.local/lib/python3.6/site-packages/torch/__init__.py\", line 81, in &lt;module&gt; from torch._C import * ImportError: libcudart.so.10.0: cannot open shared object file: No such file or directory [jetson-inference] installation complete, exiting with status code 0 [jetson-inference] to run this tool again, use the following commands: $ cd &lt;jetson-inference&gt;/build $ ./install-pytorch.sh",
        "answers": [
            [
                "I meet the exact same problem. The problem seems to be cuda 10.2. Downgrading to 10.0 does not help either. Probably the solution is to manually install everything from Jetpack and making sure that the cuda version to be installed is 10.0."
            ],
            [
                "You can check whether you have installed the correct version of torch that supports you cuda by checking torch.version.cuda and ensuring it is the same cuda version as on your jetson nano. An easier way to install torch would be to download the .whl files from Jetson Zoo. It may also be useful to upgrade your nano to the latest Jetpack versions"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "We would like to periodically retrieve frames from an rtmp stream. For this, we have 2 IP cameras and use two separate threads to continuously refresh the frames. The main program creates the two streams and then calls: getFrame() on each thread, to retrieve the last frame from each camera. The system runs for 3 seconds and collected 3 frames from each stream (1 second in between). In the end, the program writes the images to disk. We notice that there is a lag in the retrieval of the frames. When we request the frame at time x, the frame retrieved is the frame of a few seconds earlier. The lag is different for each camera and increases with the number of cameras added to the below code. We looked into a few things already: - We had the idea that the buffer of OpenCV was filled during the initialization. Therefore, we tried to remove the images from the buffer by calling the grab() method for 50 times before starting the main program. - We used a Python queue to store the frames and then only grab the last frame added to the queue. The camera datetime is synchronized with NTP and is displayed within the image. Comparing this datetime with the datetime of requesting the frame, results in an inconsistent lag. This varies for two cameras between 2 and 10 seconds. (somewhat similar to the OpenCV init time of two cameras) The code is running on an Nvidia JetsonNano. We use the following to store the images locally and give the: write time, camera number and epoch/iteration as file name: def write_local(frames, epoch, names): for camera_number, image in enumerate(frames): name = names[camera_number] path = Path(\".\") if type(image) == np.ndarray: with io.BytesIO() as in_mem_file: path = path / \"test_{0}_{1}_{2}.jpg\".format(epoch, name, datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\").replace(\" \", \"_\").replace('/', '-')) image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) image = Image.fromarray(image) image.save(in_mem_file, format='jpeg') image.seek(0) path.write_bytes(in_mem_file.getbuffer()) We use the following object for streaming multiple rtmp links: class CameraStream: def __init__(self, rtmp_link=None): self.currentFrame = None self.CAMERA_WIDTH = 2560 self.CAMERA_HEIGHT = 1920 self.capture = cv2.VideoCapture(rtmp_link) self.capture.set(cv2.CAP_PROP_FRAME_WIDTH, self.CAMERA_WIDTH) self.capture.set(cv2.CAP_PROP_FRAME_HEIGHT, self.CAMERA_HEIGHT) self.capture.set(cv2.CAP_PROP_BUFFERSIZE, 1) def start(self): self.start_time = time.time() Thread(target=self.updateFrame, args=()).start() def updateFrame(self): while(True): ret, self.current_frame = self.capture.read() def getFrame(self): return self.current_frame Initialize the camera's: wig_0 = webcamImageGetter(rtmp_link= 'rtmp://..') wig_0.start() wig_1 = webcamImageGetter(rtmp_link= 'rtmp://..') wig_1.start() Start the session and collect each second a frame from the camera. s = time.time() time_sinds_last = 0 current_frames = [] while time.time() - s &lt; 100: if time.time() - s &gt; 1: # print the datetime when the frame is requested print(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\").replace(\" \", \"_\").replace('/', '-')) # Collect frames and store in current_frames current_frames += [([wig_0.getFrame(), wig_1.getFrame()], len(current_frames), ['1_1', '1_2'])] s = time.time() # Stop after 3 epochs/iterations if len(current_frames) &gt; 3: break # write current frames to disk for x in current_frames: write_local(frames=x[0], epoch=x[1], names=x[2])",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to connect NVIDIA Jetson Nano through serial communication with Arduino Uno via USB, so when my camera, connected to the jetson nano, detect an object the LED turn on, but it's not working. I think my arduino doesn't receive any data from the jetson. If someone can help me with suggestions, or the answer that would be great. Here is my code for arduino and for jetson nano: Arduino: char data; int LED=13; void setup() { Serial.begin(9600); pinMode(LED, OUTPUT); digitalWrite(LED, LOW); } void loop() { if (Serial.available() ) { data= Serial.read(); } if(data == 'Y' || data == 'y') { digitalWrite(LED, HIGH); delay(5000); } } Jetson Nano: #!/usr/bin/python import jetson.inference import jetson.utils import time import serial import argparse import sys # configure the serial connections (the parameters differs on the device you are connecting to) ser = serial.Serial(port='/dev/ttyUSB0',baudrate=9600) # parse the command line parser = argparse.ArgumentParser(description=\"Locate objects in a live camera stream using an object detection DNN.\", formatter_class=argparse.RawTextHelpFormatter,epilog=jetson.inference.detectNet.Usage()) parser.add_argument(\"--network\", type=str, default=\"ssd-mobilenet-v2\", help=\"pre-trained model to load (see below for options)\") parser.add_argument(\"--overlay\", type=str, default=\"box,labels,conf\", help=\"detection overlay flags (e.g. --overlay=box,labels,conf)\\nvalid combinations are: 'box', 'labels', 'conf', 'none'\") parser.add_argument(\"--threshold\", type=float, default=0.5, help=\"minimum detection threshold to use\") parser.add_argument(\"--camera\", type=str, default=\"0\", help=\"index of the MIPI CSI camera to use (e.g. CSI camera 0)\\nor for VL42 cameras, the /dev/video device to use.\\nby default, MIPI CSI camera 0 will be used.\") parser.add_argument(\"--width\", type=int, default=1280, help=\"desired width of camera stream (default is 1280 pixels)\") parser.add_argument(\"--height\", type=int, default=720, help=\"desired height of camera stream (default is 720 pixels)\") try: opt = parser.parse_known_args()[0] except: print(\"\") parser.print_help() sys.exit(0) # load the object detection network net = jetson.inference.detectNet(opt.network, sys.argv, opt.threshold) # create the camera and display camera = jetson.utils.gstCamera(opt.width, opt.height, opt.camera) display = jetson.utils.glDisplay() # process frames until user exits while display.IsOpen(): # capture the image img, width, height = camera.CaptureRGBA() # detect objects in the image (with overlay) detections = net.Detect(img, width, height, opt.overlay) # print the detections print(\"detected {:d} objects in image\".format(len(detections))) for detection in detections: print(detection) # render the image display.RenderOnce(img, width, height) # update the title bar display.SetTitle(\"{:s} | Network {:.0f} FPS\".format(opt.network, net.GetNetworkFPS())) # print out performance info net.PrintProfilerTimes() if (detections &gt; 0): ser = serial.Serial(port='/dev/ttyUSB0',baudrate=9600) time.sleep(2) print(ser) ser.write('Y')",
        "answers": [
            [
                "as alan.elkin mentioned before. You need to point out the problem. Nevertheless I would debug your problem as follows: check only the serial communication between both devices. I would advice to remove any other logic in your python script. ( Object detection with Camera) review your connection setup. Common problems are : no common GND for both devices, different logic level( most of Arduinos operate at 5V and Jetson Nano at 3.3V) review your serial communication configuration, Baudarte, parity bit.. etc If you have an Oscilloscope, check the Pins sending data . You should see the signal toggling hope this gives you a hint how to identfy your problem"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I used gstreamer + opencv for decoding 12 multi-streams ip camera with 480p5. The predection times of model in nano are 100ms and 130ms for batch_size=1 and batch_size=2. I used threads for H264 HW decoding. and I want to know how to handle this problem for 12 cameras at the same time? I want to know in Nvidia demo shown 8 streams 1080p30 processed at the same time, how do they thandle this challange. I guess they don\u2019t feed inputs of 8 stream to model at the same time, right? I think they feed the first two cameras as batch_size=2 and then feed to model and then second two cameras to model as sequentioal and so on , right? if so, assume they feed each of two cameras to model as same time and also they decode 8 streams at the same time, so if we feed first two camera into model in the time of T1, the frames of rest cameras (i.e 3-8) are rejected in the time of T1 for processing?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I wrote a python script using pyserial to connect NVIDIA Jetson Nano through serial communication with Arduino Uno using Jetson nano J41 pins and Software Serial on Arduino Uno but I'm having an issue with the messages received on arduino uno, sometimes I got bad characters on the message. e.g. I send with pyserial \"hello world\" and when I check arduino serial I got \"he\u2e2elo\"worlf*\" Also when the arduino receive a message it answers with a MESSAGE_OK and the jetson nano always get it right without weird characters. From jetson to uno bad characters are received but from from nano to jetson it's ok. I'm using a logic level converter to connect arduino software serial pins to jetson nano uart pins. I've been trying to figure out was going on but without success, if someone can help me with suggestions, or the answer that would be great. I'm trying with the easiest example, here is my code for arduino and for jetson nano: Arduino: #include &lt;SoftwareSerial.h&gt; String a; // Arduino uno Ext Serial pins int ext_rx_pin = 9; int ext_tx_pin = 8; SoftwareSerial ext(ext_rx_pin, ext_tx_pin); //RX, TX void setup() { // opens serial port Serial.begin(38400); // Setup external serial connection to jetson ext.begin(38400); } void loop() { while (ext.available() &gt; 0) { a = ext.readStringUntil('\\n'); // read the incoming data as string // Print message on ide console Serial.println(a); // Answer to jetson ext.print(\"MESSAGE_OK\"); ext.print(\"\\n\"); } } Jetson nano: #!/usr/bin/python3 import time import serial serial_port = serial.Serial( port=\"/dev/ttyTHS1\", baudrate=38400, timeout=0.5 ) # Wait a second to let the port initialize time.sleep(1) arduino_message = \"\" wait = True try: while True: text = input(\"Input message: \") print(\"Sending:\", text) text = text + \"\\n\" print(text.encode()) for i in text: serial_port.write(i.encode('utf-8')) time.sleep(0.1) wait = True while wait: if serial_port.inWaiting() &gt; 0: data = serial_port.read() arduino_message = arduino_message + data.decode('utf-8') if data == \"\\n\".encode(): wait = False print(arduino_message) arduino_message = \"\" except KeyboardInterrupt: print(\"Exiting Program\") except Exception as exception_error: print(\"Error occurred. Exiting Program\") print(\"Error: \" + str(exception_error)) finally: serial_port.close() pass Also if I try to echo what is send from jetson to uno and then from uno to jetson I got this message because the bad characters: Error: 'utf-8' codec can't decode byte 0xec in position 0: unexpected end of data",
        "answers": [
            [
                "There are a number of single and double bit errors in the expected and received characters. This could be an electrical and/or timing issue. e.g. space (0010 0000) =&gt; \" (0010 0010) d (0010 0010) =&gt; f (0110 0110) \\n (0000 1011 and 0000 1010) =&gt; * (one of which ends up being 0010 1010) Try using a slower baud rate between the boards as timing is especially an issue with software serial. Alternatively, use a hardware serial port on the Arduino Uno to communicate with the other board."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I installed tf_trt_models on Jetson-nano following the instructions here. I am getting the following error Installed /home/tarik-dev/.local/lib/python3.6/site-packages/slim-0.1-py3.6.egg Processing dependencies for slim==0.1 Finished processing dependencies for slim==0.1 ~/tf_trt_models Installing tf_trt_models /home/tarik-dev/tf_trt_models running install Checking .pth file support in /home/tarik-dev/.local/lib/python3.6/site-packages/ /home/tarik-dev/.virtualenvs/nanocv/bin/python -E -c pass TEST FAILED: /home/tarik-dev/.local/lib/python3.6/site-packages/ does NOT support .pth files bad install directory or PYTHONPATH",
        "answers": [
            [
                "Found the solution. In the install script, because I am in virtualenv, I will need to remove --user Here is the install.sh script #!/bin/bash INSTALL_PROTOC=$PWD/scripts/install_protoc.sh MODELS_DIR=$PWD/third_party/models PYTHON=python if [ $# -eq 1 ]; then PYTHON=$1 fi echo $PYTHON # install protoc echo \"Downloading protoc\" source $INSTALL_PROTOC PROTOC=$PWD/data/protoc/bin/protoc # install tensorflow models git submodule update --init pushd $MODELS_DIR/research echo $PWD echo \"Installing object detection library\" echo $PROTOC $PROTOC object_detection/protos/*.proto --python_out=. $PYTHON setup.py install --user popd pushd $MODELS_DIR/research/slim echo $PWD echo \"Installing slim library\" $PYTHON setup.py install --user popd echo \"Installing tf_trt_models\" echo $PWD $PYTHON setup.py install --user"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm working on a Jetson Nano for my MSc thesis. At this time I have to control a camera using PWM signal generated from NVIDIA SBC, so after set PWM capability with their own tool (Jetson-IO), I'm trying to produce the signal. I have been successful in camera control with Raspberry, and because of the similarity of Jetson with Raspberry (in particular about interfaces, pinout libraries and compatible peripherals/devices), I have been making comparisons between them. So, I report here-after the discussion into NVIDIA Forum, and hope someone could help me. https://forums.developer.nvidia.com/t/strange-behaviour-with-jetson-nano-devkit-pwm-signal/113004/18?u=jacopo Thank you Jacopo",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have an object detection tensorflow model running on Jetson Nano. I want to create a flutter App which controls this program on jetson Nano. Is it possible ? How can i do that? I mean, can we interface flutter with jetson nano?",
        "answers": [
            [
                "I don't know much about Jetson Nano but as I understand : Jetson Nano provides the development kit One can write Python program to run on the Jetson Nano. If above is true, you can make these two guys talk by means of REST APIs. The program on Jetson Nano exposes certain features and/or by means of APIs. And the Flutter (read: Dart) programs consumes the APIs. This set-up is just like any other set-up. For example, if the Flutter app wants to write something in the server database then we expose the read/write operations by means of APIs and call those APIs from the Flutter app."
            ],
            [
                "I think the best way is to convert your TensorFlow Model to .tflite format and use the model inside your flutter APP. It would be hard to control your Jetson Nano with Flutter, but you can use some SSH APP on your phone and connect to your Jetson Nano to do your operation."
            ],
            [
                "yes you can. I succeeded in building on jetson."
            ]
        ],
        "votes": [
            1e-07,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I installed OpenCV 4.1.2 from source with CUDA support. Had no issues. and created a symbolic link from OpenCV\u2019s installation directory to my virtualenv ln -s /usr/local/lib/python3.6/site-packages/cv2/python3.6/cv2.cpython-36m-aarch64-linux-gnu.so cv2.so I am having an issue with import cv2 $ python Python 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import cv2 Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; ModuleNotFoundError: No module named 'cv2' &gt;&gt;&gt; I checked site-packages directory and I can see cv2.so. I am obviously missing something. The main issue here in my view I am not able to link to my virtualenv, in fact I am able to check my installation and its working /usr/local/lib/python3.6/site-packages/cv2/python-3.6$ python Python 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import cv2 &gt;&gt;&gt;",
        "answers": [
            [
                "Issue solved a very very little mistake I changed the name from cv2.cpython-36m-aarch64-linux-gnu.so to cv2.so I realized it was an issue with one of the folders, this will do the magic: ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so notice its python-3.6 not python3.6 after cv2"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This is in the continuation to the question Facing issue while running Flask app with TensorRt model on jetson nano Above is resolve but when I am running flask 'app' it keep loading and not showing video. code: def callback(): cuda.init() device = cuda.Device(0) ctx = device.make_context() onnx_model_path = './some.onnx' fp16_mode = False int8_mode = False trt_engine_path = './model_fp16_{}_int8_{}.trt'.format(fp16_mode, int8_mode) max_batch_size = 1 engine = get_engine(max_batch_size, onnx_model_path, trt_engine_path, fp16_mode, int8_mode) context = engine.create_execution_context() inputs, outputs, bindings, stream = allocate_buffers(engine) ctx.pop() ##callback function ends worker_thread = threading.Thread(target=callback()) worker_thread.start() trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream) def do_inference(context, bindings, inputs, outputs, stream, batch_size=1): print(\"start in do_inferece\") # Transfer data from CPU to the GPU. [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs] # Run inference. print(\"before run infernce in do_inferece\") context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle) # Transfer predictions back from the GPU. print(\"before output in do_inferece\") [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs] print(\"before stream synchronize in do_inferece\") # Synchronize the stream stream.synchronize() # Return only the host outputs. print(\"before return in do_inferece\") return [out.host for out in outputs]",
        "answers": [
            [
                "Your worker_thread creates the context required for do_inference. You should call the do_inference method inside the callback() def callback(): cuda.init() device = cuda.Device(0) ctx = device.make_context() onnx_model_path = './some.onnx' fp16_mode = False int8_mode = False trt_engine_path = './model_fp16_{}_int8_{}.trt'.format(fp16_mode, int8_mode) max_batch_size = 1 engine = get_engine(max_batch_size, onnx_model_path, trt_engine_path, fp16_mode, int8_mode) context = engine.create_execution_context() inputs, outputs, bindings, stream = allocate_buffers(engine) trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream) # post-process the trt_outputs ctx.pop()"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have this thread class built to run inference with TensorRT: class GPUThread(threading.Thread): def __init__(self, engine_path): threading.Thread.__init__(self) self.engine_path = engine_path self.engine = self.open_engine(engine_path) def run(self): cuda.init() #self.dev = cuda.Device(0) #self.ctx = self.dev.make_context() self.rt_run() #self.ctx.pop() #del self.ctx return def rt_run(self): with self.engine.create_execution_context() as context: inputs, outputs, bindings, stream = self.allocate_buffers(self.engine) # ... Retrieve image self.load_input(inputs[0].host, image) [output] = self.do_inference( context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream ) return def load_input(self, pagelocked_buffer, image): # ... Image transformations ... # Copy to the pagelocked input buffer np.copyto(pagelocked_buffer, crop_img) return def allocate_buffers(self, engine): inputs = [] outputs = [] bindings = [] stream = cuda.Stream() for binding in engine: size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size dtype = trt.nptype(engine.get_binding_dtype(binding)) # Allocate host and device buffers host_mem = cuda.pagelocked_empty(size, dtype) device_mem = cuda.mem_alloc(host_mem.nbytes) # Append the device buffer to device bindings. bindings.append(int(device_mem)) # Append to the appropriate list. if engine.binding_is_input(binding): inputs.append(HostDeviceMem(host_mem, device_mem)) else: outputs.append(HostDeviceMem(host_mem, device_mem)) return inputs, outputs, bindings, stream def run_inference(self, context, bindings, inputs, outputs, stream, batch_size=1): # Transfer input data to the GPU. [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs] # Run inference. context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle) # Transfer predictions back from the GPU. [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs] # Synchronize the stream stream.synchronize() # Return only the host outputs. return [out.host for out in outputs] When running the code above, I get the error: stream = cuda.Stream() pycuda._driver.LogicError: explicit_context_dependent failed: invalid device context - no currently active context? This function cuda.Stream() is called in allocate_buffers above. So I then try the below in run (note this is the commented out code above): self.dev = cuda.Device(0) self.ctx = self.dev.make_context() self.rt_run() self.ctx.pop() del self.ctx This causes my system to completely freeze when rt_run's create_execution_context is called. I'm guessing there are conflicts between making the PyCuda context and then creating the TensorRT execution context? I'm running this on a Jetson Nano. If I remove the create_execution_context code, I can allocate buffers and it seems that the context is active and found in the worker thread. However, I can't run inference without the TensorRT execution context. execute_async is not a method of self.ctx above. Note that none of these issues arise when running from the main thread. I can just use PyCuda's autoinit and create an execution context as in the above code. So in summary, in a worker thread, I can't allocate buffers unless I call self.dev.make_context but this causes the create_execution_context call to crash the system. If I don't call self.dev.make_context, I can't allocate buffers in the execution context as I get the error invalid device context when calling cuda.Stream() in allocate buffers. What I'm running: TensorRT 6 PyCuda 1.2 Jetson Nano 2019 (A02)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I tried to find out which version of some things i installed on my Jetson Nano. So I wrote a few lines of code and run it in python3 by using a terminal. And by doing the error shown in the picture occured. In another topic I found out that you can avoid some errors by changing the order of your import. That worked for me but obviously something does not work properly. Maybe this could become a big problem if there is a program with many lines of code. I can't image that this error can always be fixed by changing the order of import. Are there ways to prevent this error? ImportError: /usr/lib/aarch64-linux-gnu/libgomp.so.1: cannot allocate memory in static TLS block Someone has a good advice and can help. Thanks. Chris",
        "answers": [
            [
                "Just add the library path in your bashrc file export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1:/$LD_PRELOAD Source bashrc and it should solve the problem on aarch64 devices"
            ],
            [
                "if you preload the library it works correctly - like this LD_PRELOAD=libgomp.so.1 python3 your_python_script this is the link to bugzilla entry - apparently the issue is in glibc https://bugzilla.redhat.com/show_bug.cgi?id=1722181"
            ]
        ],
        "votes": [
            6.0000001,
            5.0000001
        ]
    },
    {
        "question": "How to auto-run python script made by me when the system is booted on jetson nano?",
        "answers": [
            [
                "Step 1: Create a shell file. sudo nano /usr/local/bin/startup.sh: Type this on the terminal. A new sh file is created. This file consists of the python file that is to be executed. I gave the name startup.sh. It can be any name XYZ.sh #! /bin/sh: This is called the shebang. This would execute our script using a Bourne shell. This tells the system that the commands in this file should be fed to the interpreter. sleep 10: This pauses a script for a certain amount of time. He re we pause it for 10 seconds. In the next line, we insert the code that we use to run the program on the terminal. OPENBLAS_CORETYPE=ARMV8 /usr/bin/python3 path/of/the/python/code.py It looks like this: #! /bin/sh sleep 10 OPENBLAS_CORETYPE=ARMV8 /usr/bin/python3 /home/sooraj/Downloads/incabin-monitoring-system-main/netstreamfeb17.py Now we close the file using Ctrl+X. and save it. Step 2: Create a service file sudo nano /etc/systemd/system/startup.service Things to write inside it. [Unit] Description = INTU_IPC start-uo specific script [Service] Type= idle ExecStartPre = /bin/sleep 10 ExecStart = /usr/local/bin/startup.sh User=sooraj# write your user name here [Install] WantedBy = multi-user.target Now we close the file using Ctrl+X. and save it. step 3: Give permission. sudo chmod +x /usr/local/bin/startup.sh step 4: enable, start and stop sudo systemctl enable startup.service sudo systemctl start startup.service sudo systemctl stop.service After starting, to view if it works, we can observe it in the terminal by journalctl -u startup.service -f If we edit the service file for the next time, we need to reload it before enabling and starting. sudo systemctl daemon-reload sudo systemctl enable startup.service sudo systemctl start startup.service Additional information. systemctl is-enabled startup.service #checks if the service file is enabled. systemctl is-failed startup.service #checks if the service file failed to start. systemctl is-active startup.service #checks if the service file is active. sudo systemctl list-unit-files \u2014 type=service #display the list of service files."
            ],
            [
                "Try StartupApplications and add your python execution shell command with proper path. An even better solution will be to use crontab. crontab -e Add @reboot python path/to/script so that the script gets executed every time you reboot. This link might help you."
            ],
            [
                "As an alternative to systemd or crontab, you can try pm2. It's very easy to configure and use. Just follow a quick start guide. Or just test it the following way: pm2 start app.py pm2 save Note that you should initially generate a startup script to make it work on boot."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am stuck with a problem regarding TensorRT and Tensorflow. I am using a NVIDIA jetson nano and I try to convert simple Tensorflow models into TensorRT optimized models. I am using tensorflow 2.1.0 and python 3.6.9. I try to use utilize t.his code sample from the NVIDIA-guide: from tensorflow.python.compiler.tensorrt import trt_convert as trt converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir) converter.convert() converter.save(output_saved_model_dir) To test this, I took a simple example from the tensorflow website . To convert the model into an TensorRT-model, I save the model as a \"savedModel\" and the loaded it into the trt.TrtGraphConverterV2-function: #https://www.tensorflow.org/tutorials/quickstart/beginner import tensorflow as tf from tensorflow.python.compiler.tensorrt import trt_convert as trt import os #mnist = tf.keras.datasets.mnist #(x_train, y_train), (x_test, y_test) = mnist.load_data() #x_train, x_test = x_train / 255.0, x_test / 255.0 model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation='relu'), #tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10) ]) loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy']) # create paths to save models model_name = \"simpleModel\" pb_model = os.path.join(os.path.dirname(os.path.abspath(__file__)),(model_name+\"_pb\")) trt_model = os.path.join(os.path.dirname(os.path.abspath(__file__)),(model_name+\"_trt\")) if not os.path.exists(pb_model): os.mkdir(pb_model) if not os.path.exists(trt_model): os.mkdir(trt_model) tf.saved_model.save(model, pb_model) # https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#usage-example print(\"\\nconverting to trt-model\") converter = trt.TrtGraphConverterV2(input_saved_model_dir=pb_model ) print(\"\\nconverter.convert\") converter.convert() print(\"\\nconverter.save\") converter.save(trt_model) print(\"trt-model saved under: \",trt_model) When I run this code it saves the trt-optimized model,but the model cannot be used. When I load the model and try model.summary() for example it tells me: Traceback (most recent call last): File \"/home/al/Code/Benchmark_70x70/test-load-pb.py\", line 45, in &lt;module&gt; model.summary() AttributeError: '_UserObject' object has no attribute 'summary' This is the complete output of the converter script: 2020-04-01 20:38:07.395780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0 2020-04-01 20:38:11.837436: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6 2020-04-01 20:38:11.879775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6 2020-04-01 20:38:17.015440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1 2020-04-01 20:38:17.054065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:17.061718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:00:00.0 name: NVIDIA Tegra X1 computeCapability: 5.3 coreClock: 0.9216GHz coreCount: 1 deviceMemorySize: 3.87GiB deviceMemoryBandwidth: 23.84GiB/s 2020-04-01 20:38:17.061853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0 2020-04-01 20:38:17.061989: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0 2020-04-01 20:38:17.145546: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0 2020-04-01 20:38:17.252192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0 2020-04-01 20:38:17.368195: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0 2020-04-01 20:38:17.433245: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0 2020-04-01 20:38:17.433451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-04-01 20:38:17.433761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:17.434112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:17.434418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0 2020-04-01 20:38:17.483529: W tensorflow/core/platform/profile_utils/cpu_utils.cc:98] Failed to find bogomips in /proc/cpuinfo; cannot determine CPU frequency 2020-04-01 20:38:17.504302: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13e7b0f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-04-01 20:38:17.504407: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-04-01 20:38:17.713898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:17.714293: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13de1210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-04-01 20:38:17.714758: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): NVIDIA Tegra X1, Compute Capability 5.3 2020-04-01 20:38:17.715405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:17.715650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:00:00.0 name: NVIDIA Tegra X1 computeCapability: 5.3 coreClock: 0.9216GHz coreCount: 1 deviceMemorySize: 3.87GiB deviceMemoryBandwidth: 23.84GiB/s 2020-04-01 20:38:17.715796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0 2020-04-01 20:38:17.715941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0 2020-04-01 20:38:17.716057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0 2020-04-01 20:38:17.716174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0 2020-04-01 20:38:17.716252: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0 2020-04-01 20:38:17.716311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0 2020-04-01 20:38:17.716418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-04-01 20:38:17.716687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:17.716994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:17.717111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0 2020-04-01 20:38:17.736625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0 2020-04-01 20:38:30.190208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-04-01 20:38:30.315240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 2020-04-01 20:38:30.315482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N 2020-04-01 20:38:30.832895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:31.002925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:31.005861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 32 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0, compute capability: 5.3) 2020-04-01 20:38:34.803674: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. converting to trt-model 2020-04-01 20:38:37.808143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6 converter.convert 2020-04-01 20:38:39.618691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:39.618842: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 2020-04-01 20:38:39.619224: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session 2020-04-01 20:38:39.712117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:39.712437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:00:00.0 name: NVIDIA Tegra X1 computeCapability: 5.3 coreClock: 0.9216GHz coreCount: 1 deviceMemorySize: 3.87GiB deviceMemoryBandwidth: 23.84GiB/s 2020-04-01 20:38:39.712594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0 2020-04-01 20:38:39.744930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0 2020-04-01 20:38:40.056630: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0 2020-04-01 20:38:40.153461: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0 2020-04-01 20:38:40.176047: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0 2020-04-01 20:38:40.214052: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0 2020-04-01 20:38:40.231552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-04-01 20:38:40.231927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:40.232253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:40.232388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0 2020-04-01 20:38:40.232538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-04-01 20:38:40.232587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 2020-04-01 20:38:40.232618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N 2020-04-01 20:38:40.232890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:40.233546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:40.233761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 32 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0, compute capability: 5.3) 2020-04-01 20:38:40.579950: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:841] Optimization results for grappler item: graph_to_optimize 2020-04-01 20:38:40.580104: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] function_optimizer: Graph size after: 26 nodes (19), 43 edges (36), time = 179.825ms. 2020-04-01 20:38:40.580157: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] function_optimizer: function_optimizer did nothing. time = 0.152ms. 2020-04-01 20:38:40.941994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:40.942217: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 2020-04-01 20:38:40.942412: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session 2020-04-01 20:38:40.943756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:40.943916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:00:00.0 name: NVIDIA Tegra X1 computeCapability: 5.3 coreClock: 0.9216GHz coreCount: 1 deviceMemorySize: 3.87GiB deviceMemoryBandwidth: 23.84GiB/s 2020-04-01 20:38:40.944010: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0 2020-04-01 20:38:40.944073: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0 2020-04-01 20:38:40.944148: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0 2020-04-01 20:38:40.944209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0 2020-04-01 20:38:40.944266: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0 2020-04-01 20:38:40.944320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0 2020-04-01 20:38:40.944372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-04-01 20:38:40.944572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:40.944816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:40.944911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0 2020-04-01 20:38:40.944993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-04-01 20:38:40.945031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 2020-04-01 20:38:40.945059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N 2020-04-01 20:38:40.945283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:40.945569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:948] ARM64 does not support NUMA - returning NUMA node zero 2020-04-01 20:38:40.945714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 32 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0, compute capability: 5.3) 2020-04-01 20:38:41.037807: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:460] There are 6 ops of 3 different types in the graph that are not converted to TensorRT: Identity, NoOp, Placeholder, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops). 2020-04-01 20:38:41.043736: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:636] Number of TensorRT candidate segments: 1 2020-04-01 20:38:41.046312: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:737] Replaced segment 0 consisting of 12 nodes by TRTEngineOp_0. 2020-04-01 20:38:41.073078: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:841] Optimization results for grappler item: tf_graph 2020-04-01 20:38:41.073159: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] constant_folding: Graph size after: 22 nodes (-4), 35 edges (-8), time = 14.454ms. 2020-04-01 20:38:41.073188: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] layout: Graph size after: 22 nodes (0), 35 edges (0), time = 20.565ms. 2020-04-01 20:38:41.073214: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] constant_folding: Graph size after: 22 nodes (0), 35 edges (0), time = 5.644ms. 2020-04-01 20:38:41.073238: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] TensorRTOptimizer: Graph size after: 11 nodes (-11), 14 edges (-21), time = 28.58ms. 2020-04-01 20:38:41.073265: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] constant_folding: Graph size after: 11 nodes (0), 14 edges (0), time = 2.904ms. 2020-04-01 20:38:41.073289: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:841] Optimization results for grappler item: TRTEngineOp_0_native_segment 2020-04-01 20:38:41.073312: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] constant_folding: Graph size after: 14 nodes (0), 15 edges (0), time = 2.875ms. 2020-04-01 20:38:41.073335: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] layout: Graph size after: 14 nodes (0), 15 edges (0), time = 2.389ms. 2020-04-01 20:38:41.073358: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] constant_folding: Graph size after: 14 nodes (0), 15 edges (0), time = 2.834ms. 2020-04-01 20:38:41.073382: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] TensorRTOptimizer: Graph size after: 14 nodes (0), 15 edges (0), time = 0.218ms. 2020-04-01 20:38:41.073405: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:843] constant_folding: Graph size after: 14 nodes (0), 15 edges (0), time = 5.268ms. converter.save 2020-04-01 20:38:46.730260: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at trt_engine_resource_ops.cc:183 : Not found: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0) trt-model saved under: /home/al/Code/Benchmark_70x70/simpleModel_trt",
        "answers": [
            [
                "Thank you very much for the response. It contains everything I need. To test the converter script, I ran the code in colab and it worked fine, so I guess I need to check my environment for errors. Regarding the model.summary() issue: As you pointed out correctly,it seems like methods from the Keras API are removed when converting the model. I especially needed the model.predict() method to use the new model for prediction. Luckily there are other ways to run inference. Additionaly to the one you posted, I found the one described in this tutorial and used it. I summarized the whole example and explanations in this notebook loaded = tf.saved_model.load('./model_trt') # loading the converted model print(\"The signature keys are: \",list(loaded.signatures.keys())) infer = loaded.signatures[\"serving_default\"] im_select = 0 # choose train-image you want to classify labeling = infer(tf.constant(train_images[im_select],dtype=float))['LastLayer'] ## Here, the Image classification happens; we need the name of the last layer we defined in the beginning #Display result print(\"Image \",im_select,\" is classified as a \",class_names[int(tf.argmax(labeling,axis=1))] ) plt.imshow(train_images[im_select])"
            ],
            [
                "It seems that the conversion has been successful, I have tried using both the .pb files from Keras &amp; TensorRT. Below is the sample code saved_model_loaded = tf.saved_model.load( 'path to trt converted model') # path to keras .pb or TensorRT .pb #for layer in saved_model_loaded.keras_api.layers: graph_func = saved_model_loaded.signatures['serving_default'] frozen_func = convert_variables_to_constants_v2( graph_func) (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 #convert to tensors input_tensors = tf.cast(x_test, dtype=tf.float32) output = frozen_func(input_tensors[:1])[0].numpy() print(output) Note: I have tried both of the model from keras &amp; TensorRT and the result is the same. Regarding the model.summary() Error, It seems that once the model is converted, it removes some of the methods like .summary() But you can use Tensorboard as an alternative if you want to check the graph from tensorRT converted model Below is the sample code import argparse import sys import tensorflow as tf %load_ext tensorboard from tensorflow.python.platform import app from tensorflow.python.summary import summary def import_to_tensorboard(model_dir, log_dir): \"\"\"View an imported protobuf model (`.pb` file) as a graph in Tensorboard. Args: model_dir: The location of the protobuf (`pb`) model to visualize log_dir: The location for the Tensorboard log to begin visualization from. Usage: Call this function with your model location and desired log directory. Launch Tensorboard by pointing it to the log directory. View your imported `.pb` model as a graph. \"\"\" with tf.compat.v1.Session(graph=tf.Graph()) as sess: tf.compat.v1.saved_model.loader.load( sess, [tf.compat.v1.saved_model.tag_constants.SERVING], model_dir) pb_visual_writer = summary.FileWriter(log_dir) pb_visual_writer.add_graph(sess.graph) print(\"Model Imported. Visualize by running: \" \"tensorboard --logdir={}\".format(log_dir)) Call the function import_to_tensorboard('path to trt model', '/logs/') Open the Tensorboard %tensorboard --logdir='path to logs' Let me know if this help."
            ],
            [
                "''' steps to convert tensorflow model to tensor RT model Load the model (. h5 or. hdf5) using model.load_weights(.h5_file_dir) Save the model using tf.saved_model.save(your_model, destn_dir) It will save the model in .pb format with assets and variables folder, keep those as it is. Use the Linux machine to convert .pb model to tensorRT while converting remember just give path for the folder where the pb file and other folders(assets and variables) exists. then start converting. '''"
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001,
            -0.9999999
        ]
    },
    {
        "question": "Closed. This question needs debugging details. It is not currently accepting answers. Edit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question. Closed 1 year ago. Improve this question I am inferencing TensorRt model on jetson nano. It is converted from pytorch through onnx. At the time of inference, the accuracy of TensorRt model has decreased drastically. It is an object detection type model. link for code: https://github.com/NVIDIA/TensorRT/issues/467",
        "answers": [
            [
                "I have figure out the problem. I don't know why but type of input effect the accuracy of model. I have replace img_raw = cv2.imread(image_path, cv2.IMREAD_COLOR) by img_raw = imageio.imread(image_path) I will update the answer if I get to know the reason behind it."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "guys, I have a question. I'm able to run an Object Detection python example on my Jetson Nano. \"ssh-mobilenet-v2\" in this case. It's working fine... I also trained my own dataset &amp; have a .pb file. How can I use it instead of \"ssh-mobilenet-v2\"? import jetson.inference import jetson.utils net = jetson.inference.detectNet(\"ssd-mobilenet-v2\", threshold=0.5) camera = jetson.utils.gstCamera(1280, 720, \"0\") # RPIcam display = jetson.utils.glDisplay() while display.IsOpen(): img, width, height = camera.CaptureRGBA() detections = net.Detect(img, width, height) display.RenderOnce(img, width, height) display.SetTitle(\"Object Detection | Network {:.0f} FPS\".format(net.GetNetworkFPS())) Thanks",
        "answers": [
            [
                "A .pb file is a frozen TensorFlow graph. So you need to install TF and its depending libraries. However, you can't do so to most of the libraries including TF using pip install or sudo apt install, you need to build them from source code or from their wheels .whl file. For a more detailed process follow Maduinos's step by step tutorial in installing TensorFlow, Numpy, Scipy, Pandas, OpenCV, etc..."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to capture an image using the csi camera on jetson nano using the jetcam.csi_camera library. The read method in the library returns an n-dimentional array and uses bgr8 encoding. I want to convert this to an image object in PIL. How can I use PIL.Image.fromarray() to achieve this? I tried but the fromarray() uses rgb mode and my image gets its blue and red channels interchanged. Pardon me and please ask if question is not clear. Thank you in advance:)",
        "answers": [
            [
                "Try reversing the channels: PILimage = Image.fromarray(... bgr8data[:,:,::-1]... )"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using Jetpack 4.3 (an OS image for Nvidia Jetson cards) with a Jetson Nano. In which OpenCV 4.1.1 is pre-installed. I want to work on multiple virtual environments, But I don't want to install (from source) OpenCV in every virtual environment I create.. So since OpenCV is pre-installed out of the virtual environments, is it possible to make use of it in every virtual environment I create? If so, how can I link the OpenCV installation to the virtual environments? Additional info: dpkg -L libopencv returns: /usr /usr/lib /usr/lib/aarch64-linux-gnu /usr/lib/aarch64-linux-gnu/libopencv_calib3d.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_core.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_dnn.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_features2d.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_flann.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_gapi.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_highgui.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_imgcodecs.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_imgproc.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_ml.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_objdetect.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_photo.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_stitching.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_video.so.4.1.1 /usr/lib/aarch64-linux-gnu/libopencv_videoio.so.4.1.1 /usr/share /usr/share/opencv4 /usr/share/opencv4/haarcascades /usr/share/opencv4/haarcascades/haarcascade_eye.xml /usr/share/opencv4/haarcascades/haarcascade_eye_tree_eyeglasses.xml /usr/share/opencv4/haarcascades/haarcascade_frontalcatface.xml /usr/share/opencv4/haarcascades/haarcascade_frontalcatface_extended.xml /usr/share/opencv4/haarcascades/haarcascade_frontalface_alt.xml /usr/share/opencv4/haarcascades/haarcascade_frontalface_alt2.xml /usr/share/opencv4/haarcascades/haarcascade_frontalface_alt_tree.xml /usr/share/opencv4/haarcascades/haarcascade_frontalface_default.xml /usr/share/opencv4/haarcascades/haarcascade_fullbody.xml /usr/share/opencv4/haarcascades/haarcascade_lefteye_2splits.xml /usr/share/opencv4/haarcascades/haarcascade_licence_plate_rus_16stages.xml /usr/share/opencv4/haarcascades/haarcascade_lowerbody.xml /usr/share/opencv4/haarcascades/haarcascade_profileface.xml /usr/share/opencv4/haarcascades/haarcascade_righteye_2splits.xml /usr/share/opencv4/haarcascades/haarcascade_russian_plate_number.xml /usr/share/opencv4/haarcascades/haarcascade_smile.xml /usr/share/opencv4/haarcascades/haarcascade_upperbody.xml /usr/share/opencv4/lbpcascades /usr/share/opencv4/lbpcascades/lbpcascade_frontalcatface.xml /usr/share/opencv4/lbpcascades/lbpcascade_frontalface.xml /usr/share/opencv4/lbpcascades/lbpcascade_frontalface_improved.xml /usr/share/opencv4/lbpcascades/lbpcascade_profileface.xml /usr/share/opencv4/lbpcascades/lbpcascade_silverware.xml /usr/lib/aarch64-linux-gnu/libopencv_calib3d.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_core.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_dnn.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_features2d.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_flann.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_gapi.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_highgui.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_imgcodecs.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_imgproc.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_ml.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_objdetect.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_photo.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_stitching.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_video.so.4.1 /usr/lib/aarch64-linux-gnu/libopencv_videoio.so.4.1",
        "answers": [
            [
                "The correct way to do this is to create the virtual environment with the flag to include system wide packages: python3 -m venv --system-wide-packages .venv The following answer explains the process in detail, including how to handle pip install: https://stackoverflow.com/a/19459977/804840"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to make a program that detects objects then translates the object name into Danish, but when overlaying the text for the translation on the image itself, the letters not found in english, such as \u00c6, \u00d8, and \u00c5, always show up terribly (such as f\u00e6ngsel showing up as f\u00c3|ngsel). In the command line, I am printing out the text that I'm putting into the parameter of the function and it comes out fine on the command prompt but bad when overlaid on the image. I am using python 3 and have the Nvidia Jetson nano. This is the line that is overlaying incorrect text on the image. font.OverlayText(img, width, height, \"{:05.2f}% {:s}\".format(confidence * 100, translateText(class_desc, \"da\")), 5, 5, font.White, font.Gray40) translateText() is a seperate function that works in which you input the text and language to translate to and it returns a string which I have checked is correct and font is defined as font = jetson.utils.cudaFont()",
        "answers": [
            [
                "You are feeding cudafont with UTF-8 encoded text. Looking at the source, there seems to be nothing that handles UTF-8 \u2013 or even, if I am seeing it right, nothing that re-encodes anything from the natural glyph order in the font to a regular encoding: char c = str[n]; if( c &lt; FirstGlyph || c &gt; LastGlyph ) continue; c -= FirstGlyph; ... now c will hold glyph data for a character (from the function int4 cudaFont::TextExtents) That means that this library is severely crippled in its character handling. A glyph, in the following context, only means \"a certain image that can be associated with an existing characters or combination of characters (i.e., a ligature \"\ufb01\" is a single glyph that represents the two characters \"f\" and \"i\"). A font also must define an encoding (built-in, by a certain convention such as PostScript Type 1 fonts, or, in the case of a TrueType/OpenType fonts, defined in their cmap table) to associate character codes with their correct glyphs. CUDA's default fonts DejaVu Sans and DejaVu Sans Mono are TrueType fonts and do contain a cmap table, so their glyphs can physically be in any random order \u2013 it's up to the font designer's discretion. A font may list all of its A glyphs first, then all its B glyphs, and so on, or be in literally any order. If the font contains an encoding table for Latin-1, the table will translate between the character code for A and the font's glyph position for the appropriate glyph. If that same font also contains a wildly different encoding, such as EBCDIC, its own code for A (which is not based on ASCII and therefore not \"the usual\" 0x41, but 0xC1) will still point to the same A glyph in the font. What cudafont does is something entirely else. It assumes that the glyphs for at least basic ASCII appear in exactly that order, starting from the very first glyph in a font. It means that you cannot use any other glyph order than the one that has been imposed by the designer of the font. Also, because cudafont limits character codes to 8 bits only, you cannot access any of the 6,107 (DejaVu Sans) and 3,309 (DejaVu Sans Mono) characters, except those in the range before ASCII code 256. I briefly glanced over the glyph set and it appears to me that they are in Unicode order. Now, in a lucky stroke (for cudafonts programmers), that indeed means that the first 95 characters are in ASCII order. Unfortunately for you, my Western European reader (typically not interested in anything beyond Latin-1's borders) \u2013 the next still follow the common ISO/IEC 8859-1 order but, per convention, the range from 0x7F to 0x9F is missing. In Unicode Latin-1 Supplement, which is based on the original Latin-1, these codes represent control codes and do not have a glyph representation. It still means you cannot feed Python's default UTF-8 encoded strings into it, but apart from plain ASCII you also cannot instruct Python to encode your text to Latin-1. Python can encode it alright: text = 'f\u00e6ngsel' print (text) print (text.encode('latin-1')) f\u00e6ngsel b'f\\xe6ngsel' but due to that missing range, a character is displayed that appears only 33 glyphs further in the list: \u0107. Solution: there are 33 characters missing in between, and so you can adjust your (now) binary string contents to match the physical glyph order again: btext = bytes([c if c &lt;= 0x7f else c-33 for c in text.encode('latin-1')]) print (btext) b'f\\xc5ngsel' and that string should (theoretically) finally render your f\u00e6ngsel. Any hypothetical user that wants to access glyphs beyond the range of this fix are out of luck until cudafont gets updated to (a) properly use a font's encoding, and (b) support the full Unicode range instead of chars only. At least it gets a brief mention in stb_truetype.h: // Todo: // non-MS cmaps"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I followed these topics: https://devtalk.nvidia.com/default/topic/1044958/jetson-agx-xavier/scikit-learn-for-python-3-on-jetson-xavier/ https://devtalk.nvidia.com/default/topic/1049684/jetson-nano/errors-during-install-sklearn-/ https://github.com/scikit-learn/scikit-learn/issues/12707 python version: 3.6.9 Here are all commands I run: sudo apt-get update sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev sudo apt-get install python3-pip sudo pip3 install -U pip testresources setuptools sudo pip3 install -U numpy==1.16.1 future==0.17.1 mock==3.0.5 h5py==2.9.0 keras_preprocessing==1.0.5 keras_applications==1.0.8 gast==0.2.2 enum34 futures protobuf sudo pip3 install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v43 tensorflow-gpu==1.15.0+nv19.12 sudo apt-get install python3-opencv sudo apt-get install python3-pandas sudo apt-get install python3-keras sudo apt-get install gfortran sudo apt-get install python3-scipy sudo apt-get install python3-matplotlib sudo apt-get install python3-imageio pip3 install dlib sudo apt-get install -y build-essential libatlas-base-dev pip3 install --upgrade setuptools sudo pip3 install -U setuptools sudo apt-get install libpcap-dev libpq-dev sudo pip3 install cython sudo pip3 install git+https://github.com/scikit-learn/scikit-learn.git and I got the long error below compile options: '-I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c' aarch64-linux-gnu-gcc: scipy/cluster/_hierarchy.c In file included from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1822:0, from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12, from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4, from scipy/cluster/_hierarchy.c:598: /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp] #warning \"Using deprecated NumPy API, disable it with \" \\ ^~~~~~~ scipy/cluster/_hierarchy.c:19289:18: warning: \u2018__Pyx_CFunc_double____double____double____double____int____int____int___to_py\u2019 defined but not used [-Wunused-function] static PyObject *__Pyx_CFunc_double____double____double____double____int____int____int___to_py(double (*__pyx_v_f)(double, double, double, int, int, int)) { ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ scipy/cluster/_hierarchy.c: In function \u2018__pyx_pw_5scipy_7cluster_10_hierarchy_27nn_chain\u2019: scipy/cluster/_hierarchy.c:13560:10: warning: \u2018__pyx_v_y\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] if (__pyx_t_12) { ^ scipy/cluster/_hierarchy.c:13074:7: note: \u2018__pyx_v_y\u2019 was declared here int __pyx_v_y; ^~~~~~~~~ scipy/cluster/_hierarchy.c: In function \u2018__pyx_pw_5scipy_7cluster_10_hierarchy_23linkage\u2019: scipy/cluster/_hierarchy.c:11431:16: warning: \u2018__pyx_v_y\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] __pyx_t_23 = __pyx_v_y; ~~~~~~~~~~~^~~~~~~~~~~ scipy/cluster/_hierarchy.c:11060:7: note: \u2018__pyx_v_y\u2019 was declared here int __pyx_v_y; ^~~~~~~~~ scipy/cluster/_hierarchy.c:11421:16: warning: \u2018__pyx_v_x\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] __pyx_t_22 = __pyx_v_x; ~~~~~~~~~~~^~~~~~~~~~~ scipy/cluster/_hierarchy.c:11059:7: note: \u2018__pyx_v_x\u2019 was declared here int __pyx_v_x; ^~~~~~~~~ scipy/cluster/_hierarchy.c: In function \u2018__pyx_pw_5scipy_7cluster_10_hierarchy_25fast_linkage\u2019: scipy/cluster/_hierarchy.c:12682:92: warning: \u2018__pyx_v_dist\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] *((double *) ( /* dim=0 */ (__pyx_v_D.data + __pyx_t_44 * __pyx_v_D.strides[0]) )) = __pyx_v_new_dist((*((double *) ( /* dim=0 */ (__pyx_v_D.data + __pyx_t_42 * __pyx_v_D.strides[0]) ))), (*((double *) ( /* dim=0 */ (__pyx_v_D.data + __pyx_t_43 * __pyx_v_D.strides[0]) ))), __pyx_v_dist, __pyx_v_nx, __pyx_v_ny, __pyx_v_nz); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ scipy/cluster/_hierarchy.c:11978:10: note: \u2018__pyx_v_dist\u2019 was declared here double __pyx_v_dist; ^~~~~~~~~~~~ scipy/cluster/_hierarchy.c:11971:7: warning: \u2018__pyx_v_y\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] int __pyx_v_y; ^~~~~~~~~ scipy/cluster/_hierarchy.c: In function \u2018__pyx_pw_5scipy_7cluster_10_hierarchy_29mst_single_linkage\u2019: scipy/cluster/_hierarchy.c:14363:142: warning: \u2018__pyx_v_y\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] *((double *) ( /* dim=1 */ (( /* dim=0 */ (__pyx_v_Z.data + __pyx_t_25 * __pyx_v_Z.strides[0]) ) + __pyx_t_26 * __pyx_v_Z.strides[1]) )) = __pyx_v_y; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~ scipy/cluster/_hierarchy.c:13995:7: note: \u2018__pyx_v_y\u2019 was declared here int __pyx_v_y; ^~~~~~~~~ aarch64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-aarch64-3.6/scipy/cluster/_hierarchy.o -Lbuild/temp.linux-aarch64-3.6 -o build/lib.linux-aarch64-3.6/scipy/cluster/_hierarchy.cpython-36m-aarch64-linux-gnu.so -Wl,--version-script=build/temp.linux-aarch64-3.6/link-version-scipy.cluster._hierarchy.map building 'scipy.cluster._optimal_leaf_ordering' extension compiling C sources C compiler: aarch64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC compile options: '-I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c' aarch64-linux-gnu-gcc: scipy/cluster/_optimal_leaf_ordering.c In file included from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1822:0, from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12, from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4, from scipy/cluster/_optimal_leaf_ordering.c:598: /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp] #warning \"Using deprecated NumPy API, disable it with \" \\ ^~~~~~~ scipy/cluster/_optimal_leaf_ordering.c: In function \u2018__pyx_pf_5scipy_7cluster_22_optimal_leaf_ordering_optimal_leaf_ordering.isra.58\u2019: scipy/cluster/_optimal_leaf_ordering.c:4747:19: warning: \u2018__pyx_v_best_w\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] __pyx_t_117 = __pyx_v_best_w; ~~~~~~~~~~~~^~~~~~~~~~~~~~~~ scipy/cluster/_optimal_leaf_ordering.c:3414:7: note: \u2018__pyx_v_best_w\u2019 was declared here int __pyx_v_best_w; ^~~~~~~~~~~~~~ scipy/cluster/_optimal_leaf_ordering.c:4746:19: warning: \u2018__pyx_v_best_u\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized] __pyx_t_116 = __pyx_v_best_u; ~~~~~~~~~~~~^~~~~~~~~~~~~~~~ scipy/cluster/_optimal_leaf_ordering.c:3413:7: note: \u2018__pyx_v_best_u\u2019 was declared here int __pyx_v_best_u; ^~~~~~~~~~~~~~ aarch64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-aarch64-3.6/scipy/cluster/_optimal_leaf_ordering.o -Lbuild/temp.linux-aarch64-3.6 -o build/lib.linux-aarch64-3.6/scipy/cluster/_optimal_leaf_ordering.cpython-36m-aarch64-linux-gnu.so -Wl,--version-script=build/temp.linux-aarch64-3.6/link-version-scipy.cluster._optimal_leaf_ordering.map C compiler: aarch64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC creating /tmp/tmpvz7d4hd0/tmp creating /tmp/tmpvz7d4hd0/tmp/tmpvz7d4hd0 compile options: '-I/usr/include/python3.6m -c' extra options: '-std=c++14' aarch64-linux-gnu-g++: /tmp/tmpvz7d4hd0/main.c C compiler: aarch64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC creating /tmp/tmp3j3pimiu/tmp creating /tmp/tmp3j3pimiu/tmp/tmp3j3pimiu compile options: '-I/usr/include/python3.6m -c' extra options: '-std=c++14 -fvisibility=hidden' aarch64-linux-gnu-g++: /tmp/tmp3j3pimiu/main.c C compiler: aarch64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC creating /tmp/tmpmxeh9kyu/tmp creating /tmp/tmpmxeh9kyu/tmp/tmpmxeh9kyu compile options: '-I/usr/include/python3.6m -c' aarch64-linux-gnu-g++: /tmp/tmpmxeh9kyu/main.c building 'scipy.fft._pocketfft.pypocketfft' extension compiling C++ sources C compiler: aarch64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC creating build/temp.linux-aarch64-3.6/scipy/fft creating build/temp.linux-aarch64-3.6/scipy/fft/_pocketfft compile options: '-DPOCKETFFT_PTHREADS -I/home/dlinano/.local/include/python3.6m -I/usr/local/include/python3.6 -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c' extra options: '-std=c++14 -fvisibility=hidden' aarch64-linux-gnu-g++: scipy/fft/_pocketfft/pypocketfft.cxx scipy/fft/_pocketfft/pypocketfft.cxx:15:10: fatal error: pybind11/pybind11.h: No such file or directory #include &lt;pybind11/pybind11.h&gt; ^~~~~~~~~~~~~~~~~~~~~ compilation terminated. Running from scipy source directory. /usr/local/lib/python3.6/dist-packages/numpy/distutils/system_info.py:728: UserWarning: Specified path /usr/local/include/python3.6m is invalid. return self.get_paths(self.section, key) error: Command \"aarch64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DPOCKETFFT_PTHREADS -I/home/dlinano/.local/include/python3.6m -I/usr/local/include/python3.6 -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c scipy/fft/_pocketfft/pypocketfft.cxx -o build/temp.linux-aarch64-3.6/scipy/fft/_pocketfft/pypocketfft.o -MMD -MF build/temp.linux-aarch64-3.6/scipy/fft/_pocketfft/pypocketfft.o.d -std=c++14 -fvisibility=hidden\" failed with exit status 1 ---------------------------------------- ERROR: Failed building wheel for scipy Failed to build scipy ERROR: Could not build wheels for scipy which use PEP 517 and cannot be installed directly ---------------------------------------- ERROR: Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.6/dist-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-dfzx1730/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools wheel 'Cython&gt;=0.28.5' 'numpy&gt;=1.13.3' 'scipy&gt;=0.19.1' Check the logs for full command output. Please check this full logs: https://drive.google.com/file/d/1gLcSq86Aic5uFoPr8k6Cp366eRB2tlfw/view?usp=sharing",
        "answers": [
            [
                "This is what worked for me. sudo -H pip3 install scikit-learn Here is my full installation script for the order in which the dependencies are installed. #tensorflow - zoo sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev sudo apt-get install python3-pip sudo -H pip3 install -U pip sudo -H pip3 install -U numpy grpcio absl-py py-cpuinfo psutil portpicker six mock requests gast h5py astor==0.8.0 termcolor protobuf keras-applications keras-preprocessing wrapt google-pasta sudo -H pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu==1.14.0 #Cython sudo -H pip3 install cython #keras-zoo sudo apt-get install -y build-essential libatlas-base-dev gfortran sudo -H pip3 install keras #Pandas sudo -H pip3 install pandas #Scipy sudo -H python3 -m pip install scipy==1.1.0 #Sklearn sudo -H pip3 install scikit-learn"
            ],
            [
                "I had problems with Jupyter (Jetson TX2) I checked which numpy version is used by Jupyter. import numpy as np np.version.version In My case it was 1.13 despite the fact that $ pip3 list showed me version 1.16 for numpy I found solution here https://fooobar.com/questions/15251639/jupyter-notebook-picks-older-version-of-numpy to see the numpy path print (np.__path__) in my case it was /usr/lib/python3/dist-packages went to the directory and removed old version numpy $ cd /usr/lib/python3/dist-packages $ sudo rm -r numpy installed the relevant numpy version $ sudo pip install numpy==1.16.1 that's all."
            ]
        ],
        "votes": [
            7.0000001,
            1.0000001
        ]
    },
    {
        "question": "I have been trying to implement a tracker (CSRT) in OpenCV with C++ but I got an error when the tracker-&gt;init(firstframe, boundingbox) part is executed. Here is the error message below, ..../opencv/modules/core/src/merge.dispatch.cpp:129: error: (-215:Assertion failed) mv[i].size == mv[0].size &amp;&amp; mv[i].depth() == depth in function 'merge' and here is my code. Btw, I am working on Jetson Nano and I had to do some type conversion from float* to cv::Mat. cv::Mat first_frame; float* first_imgRGBA = NULL; camera-&gt;CaptureRGBA(&amp;first_imgRGBA, 1000, 1); first_frame = cv::Mat(camera-&gt;GetHeight(), camera-&gt;GetWidth(), CV_32FC4, first_imgRGBA); first_frame /= 255; cv::cvtColor(first_frame, first_frame, cv::COLOR_RGBA2BGR); Rect2d bbox(287, 23, 86, 320); rectangle(first_frame, bbox, Scalar( 255, 0, 0 ), 2, 1 ); imshow(\"Tracking\", first_frame); tracker-&gt;init(first_frame, bbox); Thank you.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to remotely access a Jetson Nano from my mac. So far I have done the following: I installed microsoft Remote Desktop on my mac I connected the nano with my mac through the Micro-usb Port In network I could see the connection \"linux for Tegra\" where my nano would be 192.168.55.1 With this I still cannot remote access the nano So I open a terminal and typed &gt;ssh username@192.168.55.1 and I could access the nano terminal. So I think now, I have to do &gt;sudo apt install xrdp in order to be able to remotely access the GUI. However, the nano does not have internet connection. Is there a way that it can use the internet connection of my mac to do this? (I am reading a tutorial on Rasppi with windows and apparently this is possible)",
        "answers": [
            [
                "One of the approaches is to install X server for mac and then use ssh -Y username@192.168.55.1. You will get the command line shell first but you can type in commands like gedit - they windows will show up on the host. 192.168.55.1 is the same network as any other. All tools supposed to work over TCP/IP should work with it as well."
            ],
            [
                "USB is a Serial connection by default. On windows use a putty based tool. For example start with MobaXterm &gt; New session &gt; Serial @11500 Login Password It won't forward internet by default. Best simple way is to plug in your phone as USB network sharing and plug in to the laptop via micro-USB. For Linux, Ubuntu, use a serial (\"COM\" related stuff) tool. If you want to do ssh through USB it will require custom editing of networking file. By default Nano is using a bridge called L4TBRO on 192.168.55.1, laptop is client in .100. DNS server is on Nano's side. You would have to create a new interface but using your laptop as DCHP and DNS servers. Note that usb can power the Jetson Nano for continuous execution on a recent laptop. Some recommends not to and it may crash if you run a compilation on all cores. (if it crashes, just disable 2 cores via nvpmodel -m 1)"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am able to process the video frames by saing the frame as an image and then processing it. But was unable to pass frame directly to the object detection. Saving image with imwrite is making program slow... Here is my main method: cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=2), cv2.CAP_GSTREAMER) if cap.isOpened(): window_handle = cv2.namedWindow(\"CSI Camera\", cv2.WINDOW_AUTOSIZE) # Window while cv2.getWindowProperty(\"CSI Camera\", 0) &gt;= 0: ret_val, frame = cap.read() if not ret_val: break frame = imutils.resize(frame, width=600) #cv2.imwrite('box.jpg', frame) #image = Image.open(path) #Error in here!!! predictions = od_model.predict_image(frame) for x in range(len(predictions)): probab = (predictions[x]['probability'])*100 if(probab &gt; 45): print(predictions[x]['tagName'], end=' ') print(probab) #cv2.imshow(\"CSI Camera\", frame) # This also acts as keyCode = cv2.waitKey(30) &amp; 0xFF # Stop the program on the ESC key if keyCode == 27: break cap.release() cv2.destroyAllWindows() else: print(\"Unable to open camera\") Error Message: predictions = od_model.predict_image(frame) File \"/home/bharat/New_IT3/object_detection.py\", line 125, in predict_image inputs = self.preprocess(image) File \"/home/bharat/New_IT3/object_detection.py\", line 130, in preprocess image = image.convert(\"RGB\") if image.mode != \"RGB\" else image AttributeError: 'numpy.ndarray' object has no attribute 'mode'",
        "answers": [
            [
                "open cv reads image in bgr colour spectrum conver it to rgb and send the image for detection, api for the same is - frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I know there's lots of similar questions out there, however, I couldn't find any working solution to my problem. So I've been trying to run a Gstreamer pipeline inside OpenCV's VideoCapture() object. I've tried: cap = cv2.VideoCapture('v4l2src ! xvimagesink') but that results in get an Assertion Failed error. I will actually use rtsp stream from IP camera but for simplicity, I gave the basic USB webcam pipeline as example. My cv2.getBuildInformation() output states YES for Gstreamer. I am working on Nvidia Jetson Nano with Python3 and OpenCV 4.1 installed afterwards. It would be great if I could achieve this so I can carry on with processing the stream etc. Thanks in advance!",
        "answers": [
            [
                "Shouldn't it be something like: cap = cv2.VideoCapture('v4l2src ! videoconvert ! appsink', cv2.CAP_GSTREAMER)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I found that we can optimize the Tensorflow model in several ways. If I am mistaken, please tell me. 1- Using TF-TRT, This API developer by tensorflow and integreted TensoRT to Tensorflow and this API called as : from tensorflow.python.compiler.tensorrt import trt_convert as trt This API can be applied to any tensorflow models (new and old version models) without any converting error, because If this API don't support any new layers, don't consider these layers for TensorRT engines and these layers remain for Tensorflow engine and run on Tensorflow. right? 2- Using TensorRT, This API developed by NVIDA and is independent of Tenorflow library (Not integrated to Tensorflow), and this API called as: import tensorrt as trt If we want to use this api, first, we must converting the tensorflow graph to UFF using uff-convertor and then parse the UFF graph to this API. In this case, If the Tensorflow graph have unsupported layers we must use plugin or custom code for these layers, right? 3- I don't know, when we work with Tensorflow models, Why we use UFF converter then TensorRT, we can use directly TF-TRT API, right? If so, Are you tested the Tensorflow optimization model from these two method to get same performance? what's advantage of this UFF converter method? I have some question about the two cases above: 4- I convert the ssd_mobilenet_v2 using two cases, In the case 1, I achieve slight improvement in speed but in the case 2, I achieve more improvement, why? My opinion is that, In the case 1, The API only consider converting the precision (FP32 to FP16) and merging the possible layers together, But in the case 2, the graph is clean by UFF such as remove any redundant nodes like Asserts and Identity and then converted to tensorrt graph, right? 5- when we convert the trained model files like .ckpt and .meta, ... to frozen inference graph(.pb file), These layers don't remove from graph? only loss states and optimizer states , ... are removed?",
        "answers": [
            [
                "Duplicate post with answers here: https://github.com/NVIDIA/TensorRT/issues/341"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I would like to run the Exported Model from the Google AutoML Vision on NVIDIA Jetson Nano. Since it is easy I wanted to use the pre-built containers to do predictions following the official Edge containers tutorial. The problem is that the pre-built CPU container stored in Google Container Registry (gcr.io/automl-vision-ondevice/gcloud-container-1.12.0:latest) is based on amd64 arch, while NVIDIA Jetson Nano is using arm64 arch (Ubuntu 18.04). That is why 'docker run ...' returns: docker image error: standard_init_linux.go:211: exec user process caused \u201cexec format error\u201d What can I do? Should I build a container similar to the pre-built one compatible with arm64 arch?",
        "answers": [
            [
                "There are two ideas that would help you to achieve your goal: [Idea 1] You could export the model with *.tflite format to do your detection. [Idea 2] Deploy the model as an API service on Google AutoML Vision and call it with python or any other supported language."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to compile openCV but it doesn't seem to work somehow. Is this a problem with cuda? I am running this in a docker-container (nvidia-docker) Hardware is a Jetson Nano with newest JetPack - Version What could that problem be? [ 32%] Building CXX object modules/core/CMakeFiles/opencv_core.dir/src/opengl.cpp.o In file included from /opencv/modules/core/src/opengl.cpp:48:0: /usr/local/cuda/include/cuda_gl_interop.h:63:2: error: #error Please include the appropriate gl headers before including cuda_gl_interop.h #error Please include the appropriate gl headers before including cuda_gl_interop.h ^~~~~ [ 32%] Building CXX object modules/core/CMakeFiles/opencv_core.dir/src/out.cpp.o In file included from /opencv/modules/core/include/opencv2/core/private.cuda.hpp:73:0, from /opencv/modules/core/src/precomp.hpp:56, from /opencv/modules/core/src/opengl.cpp:43: /opencv/modules/core/src/opengl.cpp: In function \u2018void cv::cuda::setGlDevice(int)\u2019: /opencv/modules/core/src/opengl.cpp:118:47: warning: \u2018cudaError_t cudaGLSetGLDevice(int)\u2019 is deprecated [-Wdeprecated-declarations] cudaSafeCall( cudaGLSetGLDevice(device) ); ^ /opencv/modules/core/include/opencv2/core/cuda/common.hpp:74:58: note: in definition of macro \u2018cudaSafeCall\u2019 #define cudaSafeCall(expr) cv::cuda::checkCudaError(expr, __FILE__, __LINE__, CV_Func) ^~~~ In file included from /opencv/modules/core/src/opengl.cpp:48:0: /usr/local/cuda/include/cuda_gl_interop.h:305:57: note: declared here extern __CUDA_DEPRECATED __host__ cudaError_t CUDARTAPI cudaGLSetGLDevice(int device); ^~~~~~~~~~~~~~~~~ modules/core/CMakeFiles/opencv_core.dir/build.make:1368: recipe for target 'modules/core/CMakeFiles/opencv_core.dir/src/opengl.cpp.o' failed make[2]: *** [modules/core/CMakeFiles/opencv_core.dir/src/opengl.cpp.o] Error 1 make[2]: *** Waiting for unfinished jobs.... CMakeFiles/Makefile2:2763: recipe for target 'modules/core/CMakeFiles/opencv_core.dir/all' failed make[1]: *** [modules/core/CMakeFiles/opencv_core.dir/all] Error 2 Makefile:162: recipe for target 'all' failed make: *** [all] Error 2 Some info: General configuration for OpenCV 4.1.1 ===================================== -- Version control: unknown -- -- Extra modules: -- Location (extra): /opencv_contrib/modules -- Version control (extra): unknown -- -- Platform: -- Timestamp: 2020-01-06T00:19:08Z -- Host: Linux 4.9.140-tegra aarch64 -- CMake: 3.10.2 -- CMake generator: Unix Makefiles -- CMake build tool: /usr/bin/make -- Configuration: RELEASE -- -- CPU/HW features: -- Baseline: NEON FP16 -- required: NEON -- disabled: VFPV3 -- -- C/C++: -- Built as dynamic libs?: YES -- C++ Compiler: /usr/bin/c++ (ver 7.4.0) -- C++ flags (Release): -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -DNDEBUG -- C++ flags (Debug): -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -fvisibility-inlines-hidden -g -O0 -DDEBUG -D_DEBUG -- C Compiler: /usr/bin/cc -- C flags (Release): -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -O3 -DNDEBUG -DNDEBUG -- C flags (Debug): -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -g -O0 -DDEBUG -D_DEBUG -- Linker flags (Release): -Wl,--gc-sections -- Linker flags (Debug): -Wl,--gc-sections -- ccache: NO -- Precompiled headers: NO -- Extra dependencies: m pthread /usr/lib/aarch64-linux-gnu/libGL.so /usr/lib/aarch64-linux-gnu/libGLU.so cudart_static -lpthread dl rt /usr/local/cuda/lib64/stubs/libcuda.so nppc nppial nppicc nppicom nppidei nppif nppig nppim nppist nppisu nppitc npps cublas cufft -L/usr/local/cuda/lib64 -L/usr/lib/aarch64-linux-gnu -- 3rdparty dependencies: -- -- OpenCV modules: -- To be built: aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev cvv datasets dnn dnn_objdetect dpm face features2d flann freetype fuzzy gapi hfs highgui img_hash imgcodecs imgproc line_descriptor ml objdetect optflow phase_unwrapping photo plot python3 quality reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking video videoio videostab xfeatures2d ximgproc xobjdetect xphoto -- Disabled: world -- Disabled by dependency: - -- Unavailable: cnn_3dobj hdf java js matlab ovis python2 sfm ts viz -- Applications: apps -- Documentation: NO -- Non-free algorithms: NO -- -- GUI: -- QT: YES (ver 5.9.5) -- QT OpenGL support: YES (Qt5::OpenGL 5.9.5) -- GTK+: NO -- OpenGL support: YES (/usr/lib/aarch64-linux-gnu/libGL.so /usr/lib/aarch64-linux-gnu/libGLU.so) -- VTK support: NO -- -- Media I/O: -- ZLib: /usr/lib/aarch64-linux-gnu/libz.so (ver 1.2.11) -- JPEG: /usr/lib/aarch64-linux-gnu/libjpeg.so (ver 80) -- WEBP: build (ver encoder: 0x020e) -- PNG: /usr/lib/aarch64-linux-gnu/libpng.so (ver 1.6.34) -- TIFF: /usr/lib/aarch64-linux-gnu/libtiff.so (ver 42 / 4.0.9) -- JPEG 2000: build (ver 1.900.1) -- OpenEXR: build (ver 2.3.0) -- HDR: YES -- SUNRASTER: YES -- PXM: YES -- PFM: YES -- -- Video I/O: -- DC1394: YES (2.2.5) -- FFMPEG: YES -- avcodec: YES (57.107.100) -- avformat: YES (57.83.100) -- avutil: YES (55.78.100) -- swscale: YES (4.8.100) -- avresample: NO -- GStreamer: YES (1.14.5) -- v4l/v4l2: YES (linux/videodev2.h) -- -- Parallel framework: TBB (ver 2017.0 interface 9107) -- -- Trace: YES (with Intel ITT) -- -- Other third-party libraries: -- Lapack: NO -- Eigen: YES (ver 3.3.4) -- Custom HAL: YES (carotene (ver 0.0.1)) -- Protobuf: build (3.5.1) -- -- NVIDIA CUDA: YES (ver 10.0, CUFFT CUBLAS FAST_MATH) -- NVIDIA GPU arch: 53 -- NVIDIA PTX archs: -- -- cuDNN: NO -- -- OpenCL: YES (no extra features) -- Include path: /opencv/3rdparty/include/opencl/1.2 -- Link libraries: Dynamic load -- -- Python 3: -- Interpreter: /usr/bin/python3.6 (ver 3.6.9) -- Libraries: /usr/lib/aarch64-linux-gnu/libpython3.6m.so (ver 3.6.9) -- numpy: /usr/local/lib/python3.6/dist-packages/numpy/core/include (ver 1.18.0) -- install path: lib/python3.6/dist-packages/cv2/python-3.6 -- -- Python (for build): /usr/bin/python3.6 -- -- Java: -- ant: NO -- JNI: NO -- Java wrappers: NO -- Java tests: NO -- -- Install to: /usr/local -- ----------------------------------------------------------------- I'd really appreciate some help, i couldn't find anything on the web sadly Big thanks!",
        "answers": [
            [
                "The solution was to use the nvidia-libraries. So i took the lib-files from the JetPack download-folder and used them -worked!"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I recently bought a Jetson Nano and I'm amazed with everything about it. But I don't know what is happening, because I created a very simple neural network with keras and it's taking way to long. I know is taking to long, because I runned the same ANN in my PC's CPU and it was faster than the jetson nano. Here's the code: import numpy as np import matplotlib.pyplot as plt import pandas as pd dataset = pd.read_csv('Churn_Modelling.csv') X = dataset.iloc[:, 3:13].values y = dataset.iloc[:, 13].values from sklearn.preprocessing import LabelEncoder, OneHotEncoder labelencoder_X_1 = LabelEncoder() X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1]) labelencoder_X_2 = LabelEncoder() X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2]) onehotencoder = OneHotEncoder(categorical_features = [1]) X = onehotencoder.fit_transform(X).toarray() X = X[:, 1:] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) from tensorflow import keras from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense classifier = Sequential() classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11)) classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu')) classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) classifier.fit(X_train, y_train, batch_size = 10, epochs = 100) y_pred = classifier.predict(X_test) y_pred = (y_pred &gt; 0.5) I should mention that of course, I did the correct installation of TensorFlow GPU library and not the normal TensorFlow, in fact I used the resources in this link: TensorFlow GPU Jetson Nano",
        "answers": [
            [
                "Jetson Nano is mainly for inferencing. Training is not preferred even though its possible. This link might help. You can try using Nvidia's Transfer Learning Toolkit and Deepstream for ideal and efficient use on Nano."
            ],
            [
                "@Juan Carlos Jchr Hey, just check https://stackexchange.com/sites I think that your question will get better answes here: https://ai.stackexchange.com/"
            ]
        ],
        "votes": [
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "We have installed Jetpack 4.2.3 on jetson nano. This has created a 2 GB built in zram memory. We have additionally added 4 GB swap file using the following https://www.jetsonhacks.com/2019/04/14/jetson-nano-use-more-memory/ The system is taking priority to zram. we want to set the newly created swap to take priority over zram. How to prioritize swap in jetson Nano? How to change Swap partition priority?",
        "answers": [
            [
                "Disabled zram by removing nvzramconfig.sh from /etc/systemd/ and zram is not invoked when the system boots. Now the swap is working perfect"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a HikVision MV-CA050-20UC USB-Camera(USB 3.0). I want to capture video from it on Nvidia Jetson Nano in Ubuntu by using gstreamer. After installing the drivers, /dev/video0 shows up after connecting the camera (it also vanishes once the camera is disconnected). gst-launch-1.0 v4l2src device=\"/dev/video0\" name=e ! 'video/x-raw, width=640, height=480' ! videoconvert ! 'video/x-raw, width=640, height=480, format=(string)YUY2' ! xvimagesink or using this command gst-launch-1.0 -v v4l2src device=/dev/video0 ! video/x-raw,framerate=30/1,width=1280,height=720 ! xvimagesink and it show me this message without any video: Setting pipeline to PAUSED ... ERROR: Pipeline doesnt want to pause. ERROR: from element /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: Error getting capabilities for device /dev/video0: It isnt a v4l2 driver. Check if it is a v4l1 driver. Additional debug info: v4l2_calls.c(94): gst_v4l2_get_capabilities (): /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: system error: No such file or directory Setting pipeline to NULL ... Freeing pipeline ... I am sure that v4l2 is installed correctly. But it could not detect plugged in usb camera. v4l2-ctl --all VIDIOC_QUERYCAP: failed: No such file or directory /dev/video0: not a v4l2 node or this result for 1050Ti system(another system): Failed to query video capabilities: No such file or directory libv4l2: error getting capabilities: No such file or directory VIDIOC_QUERYCAP: failed: No such file or directory /dev/video0: not a v4l2 node also by running v4l2-ctl -d /dev/video0 --list-formats-ext I got same message. Gstreamer version gst-inspect-1.0 --version gst-inspect-1.0 version 1.14.2 GStreamer 1.14.2 Unknown package origin How could I resolve this problem. Thanks in advance.",
        "answers": [
            [
                "Instead of using v4l2src I used Aravis A vision library for genicam based cameras and pylonsrc A gstreamer plugin for image capture from Basler's USB3 cameras. Please read the README before using and the problem was solved(As I used USB3 camera visions)."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I needed to add a time overlay to an rtmp stream, and save to disk. The following pipeline, without the overlay, works fine: gst-launch-1.0 -v \\ rtmpsrc location=rtmp://192.168.x.x/live/0 do-timestamp=true ! queue2 ! flvdemux name=demux \\ flvmux name=mux \\ demux.video ! queue ! decodebin \\ ! nvvidconv \\ ! 'video/x-raw(memory:NVMM),width=1920,height=1080, format=(string)I420, framerate=50/1' \\ ! nvv4l2h264enc ! h264parse \\ ! mux.video \\ demux.audio ! queue name=\"dmx_aud_q\" ! mux.audio \\ mux.src ! queue name=\"mux_q\" ! filesink location=\"rtmp.flv\" Once I add the timeoverlay (or even clockoverlay), the pipeline doesn't run: gst-launch-1.0 -v \\ rtmpsrc location=rtmp://192.168.0.168/x.x do-timestamp=true ! queue2 ! flvdemux name=demux \\ flvmux name=mux \\ demux.video ! queue ! decodebin \\ ! timeoverlay \\ ! nvvidconv \\ ! 'video/x-raw(memory:NVMM),width=1920,height=1080, format=(string)I420, framerate=50/1' \\ ! nvv4l2h264enc ! h264parse \\ ! mux.video \\ demux.audio ! queue name=\"dmx_aud_q\" ! mux.audio \\ mux.src ! queue name=\"mux_q\" ! filesink location=\"rtmp.flv\" The GST_DEBUG=3 logs are as added at the end. The Pipeline graph is as below. From what I understand, at the point where the decodebin hands over to timeoverlay, there is some issue with caps negotiation. I'm not able to figure out how to make timeoverlay accept or output data in a way that the pipeline can continue to mux. Any help to understand what is happening here, and how to find a solution, would be great. This is on an Nvidia Jetson Nano board, running Ubuntu 18.0. (The plugins starting with \"nv\" can usually be replaced with regular ones on other systems, I believe - nvvidconv with videoconvert, nvv4l2h264enc with omxh264enc etc. Setting pipeline to PAUSED ... Opening in BLOCKING MODE 0:00:00.119491546 834 0x55b9d05600 WARN v4l2 gstv4l2object.c:2370:gst_v4l2_object_add_interlace_mode:0x55b9cf2360 Failed to determine interlace mode 0:00:00.119570298 834 0x55b9d05600 WARN v4l2 gstv4l2object.c:2370:gst_v4l2_object_add_interlace_mode:0x55b9cf2360 Failed to determine interlace mode 0:00:00.119623164 834 0x55b9d05600 WARN v4l2 gstv4l2object.c:2370:gst_v4l2_object_add_interlace_mode:0x55b9cf2360 Failed to determine interlace mode 0:00:00.119721552 834 0x55b9d05600 WARN v4l2 gstv4l2object.c:4408:gst_v4l2_object_probe_caps: Failed to probe pixel aspect ratio with VIDIOC_CROPCAP: Unknown error -1 Pipeline is PREROLLING ... 0:00:00.406155973 834 0x55b9a72370 WARN flvdemux gstflvdemux.c:659:gst_flv_demux_parse_tag_script: failed reading a tag, skipping /GstPipeline:pipeline0/GstQueue:dmx_aud_q.GstPad:sink: caps = audio/mpeg, mpegversion=(int)4, framed=(boolean)true, stream-format=(string)raw, rate=(int)44100, channels=(int)2, codec_data=(buffer)1210 /GstPipeline:pipeline0/GstQueue:dmx_aud_q.GstPad:src: caps = audio/mpeg, mpegversion=(int)4, framed=(boolean)true, stream-format=(string)raw, rate=(int)44100, channels=(int)2, codec_data=(buffer)1210 /GstPipeline:pipeline0/GstFlvMux:mux.GstFlvMuxPad:sink_1: caps = audio/mpeg, mpegversion=(int)4, framed=(boolean)true, stream-format=(string)raw, rate=(int)44100, channels=(int)2, codec_data=(buffer)1210 /GstPipeline:pipeline0/GstQueue:queue0.GstPad:sink: caps = video/x-h264, stream-format=(string)avc, width=(int)1920, height=(int)1080, codec_data=(buffer)0142002affe100166742002a95a81e0089f961000003000100000300648401000468ce3c80 /GstPipeline:pipeline0/GstQueue:queue0.GstPad:src: caps = video/x-h264, stream-format=(string)avc, width=(int)1920, height=(int)1080, codec_data=(buffer)0142002affe100166742002a95a81e0089f961000003000100000300648401000468ce3c80 /GstPipeline:pipeline0/GstDecodeBin:decodebin0.GstGhostPad:sink.GstProxyPad:proxypad0: caps = video/x-h264, stream-format=(string)avc, width=(int)1920, height=(int)1080, codec_data=(buffer)0142002affe100166742002a95a81e0089f961000003000100000300648401000468ce3c80 /GstPipeline:pipeline0/GstDecodeBin:decodebin0/GstTypeFindElement:typefind.GstPad:src: caps = video/x-h264, stream-format=(string)avc, width=(int)1920, height=(int)1080, codec_data=(buffer)0142002affe100166742002a95a81e0089f961000003000100000300648401000468ce3c80 /GstPipeline:pipeline0/GstDecodeBin:decodebin0/GstH264Parse:h264parse1.GstPad:sink: caps = video/x-h264, stream-format=(string)avc, width=(int)1920, height=(int)1080, codec_data=(buffer)0142002affe100166742002a95a81e0089f961000003000100000300648401000468ce3c80 /GstPipeline:pipeline0/GstDecodeBin:decodebin0/GstTypeFindElement:typefind.GstPad:sink: caps = video/x-h264, stream-format=(string)avc, width=(int)1920, height=(int)1080, codec_data=(buffer)0142002affe100166742002a95a81e0089f961000003000100000300648401000468ce3c80 /GstPipeline:pipeline0/GstDecodeBin:decodebin0.GstGhostPad:sink: caps = video/x-h264, stream-format=(string)avc, width=(int)1920, height=(int)1080, codec_data=(buffer)0142002affe100166742002a95a81e0089f961000003000100000300648401000468ce3c80 /GstPipeline:pipeline0/GstDecodeBin:decodebin0/GstH264Parse:h264parse1.GstPad:src: caps = video/x-h264, stream-format=(string)byte-stream, width=(int)1920, height=(int)1080, framerate=(fraction)50/1, interlace-mode=(string)progressive, chroma-format=(string)4:2:0, bit-depth-luma=(uint)8, bit-depth-chroma=(uint)8, parsed=(boolean)true, alignment=(string)au, profile=(string)baseline, level=(string)4.2 Opening in BLOCKING MODE 0:00:00.823485062 834 0x55b9ce8a30 WARN v4l2 gstv4l2object.c:4408:gst_v4l2_object_probe_caps: Failed to probe pixel aspect ratio with VIDIOC_CROPCAP: Unknown error -1 0:00:00.823543397 834 0x55b9ce8a30 WARN v4l2 gstv4l2object.c:2370:gst_v4l2_object_add_interlace_mode:0x7f6807a640 Failed to determine interlace mode NvMMLiteOpen : Block : BlockType = 261 NVMEDIA: Reading vendor.tegra.display-size : status: 6 NvMMLiteBlockCreate : Block : BlockType = 261 /GstPipeline:pipeline0/GstDecodeBin:decodebin0/nvv4l2decoder:nvv4l2decoder0.GstPad:sink: caps = video/x-h264, stream-format=(string)byte-stream, width=(int)1920, height=(int)1080, framerate=(fraction)50/1, interlace-mode=(string)progressive, chroma-format=(string)4:2:0, bit-depth-luma=(uint)8, bit-depth-chroma=(uint)8, parsed=(boolean)true, alignment=(string)au, profile=(string)baseline, level=(string)4.2 /GstPipeline:pipeline0/GstDecodeBin:decodebin0/GstCapsFilter:capsfilter1.GstPad:src: caps = video/x-h264, stream-format=(string)byte-stream, width=(int)1920, height=(int)1080, framerate=(fraction)50/1, interlace-mode=(string)progressive, chroma-format=(string)4:2:0, bit-depth-luma=(uint)8, bit-depth-chroma=(uint)8, parsed=(boolean)true, alignment=(string)au, profile=(string)baseline, level=(string)4.2 /GstPipeline:pipeline0/GstDecodeBin:decodebin0/GstCapsFilter:capsfilter1.GstPad:sink: caps = video/x-h264, stream-format=(string)byte-stream, width=(int)1920, height=(int)1080, framerate=(fraction)50/1, interlace-mode=(string)progressive, chroma-format=(string)4:2:0, bit-depth-luma=(uint)8, bit-depth-chroma=(uint)8, parsed=(boolean)true, alignment=(string)au, profile=(string)baseline, level=(string)4.2 0:00:00.932089228 834 0x55b9ce8a30 WARN v4l2 gstv4l2object.c:4408:gst_v4l2_object_probe_caps: Failed to probe pixel aspect ratio with VIDIOC_CROPCAP: Unknown error -1 0:00:00.932560124 834 0x55b9ce8a30 WARN v4l2 gstv4l2object.c:2370:gst_v4l2_object_add_interlace_mode:0x7f6807a640 Failed to determine interlace mode /GstPipeline:pipeline0/GstDecodeBin:decodebin0/nvv4l2decoder:nvv4l2decoder0.GstPad:src: caps = video/x-raw(memory:NVMM), format=(string)NV12, width=(int)1920, height=(int)1080, interlace-mode=(string)progressive, multiview-mode=(string)mono, multiview-flags=(GstVideoMultiviewFlagsSet)0:ffffffff:/right-view-first/left-flipped/left-flopped/right-flipped/right-flopped/half-aspect/mixed-mono, pixel-aspect-ratio=(fraction)1/1, chroma-site=(string)mpeg2, colorimetry=(string)bt709, framerate=(fraction)50/1 (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.065: gst_mini_object_copy: assertion 'mini_object != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.066: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.066: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.066: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.066: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.066: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.067: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.067: gst_mini_object_copy: assertion 'mini_object != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.068: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.068: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.068: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.068: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.068: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.069: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed 0:00:00.942959420 834 0x55b9ce8a30 WARN GST_PADS gstpad.c:4226:gst_pad_peer_query: could not send sticky events 0:00:00.943568965 834 0x55b9ce8a30 WARN v4l2videodec gstv4l2videodec.c:1433:gst_v4l2_video_dec_decide_allocation: Duration invalid, not setting latency 0:00:00.944316482 834 0x55b9ce8a30 WARN v4l2bufferpool gstv4l2bufferpool.c:1054:gst_v4l2_buffer_pool_start: Uncertain or not enough buffers, enabling copy threshold (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.073: gst_mini_object_copy: assertion 'mini_object != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.074: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.074: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.074: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.074: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.074: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.074: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed 0:00:00.948613871 834 0x55b9ce8a30 WARN basetransform gstbasetransform.c:1355:gst_base_transform_setcaps: transform could not transform video/x-h264, stream-format=(string)byte-stream, width=(int)1920, height=(int)1080, framerate=(fraction)50/1, interlace-mode=(string)progressive, chroma-format=(string)4:2:0, bit-depth-luma=(uint)8, bit-depth-chroma=(uint)8, parsed=(boolean)true, alignment=(string)au, profile=(string)baseline, level=(string)4.2 in anything we support 0:00:00.948674601 834 0x55b9ce8a30 WARN basetransform gstbasetransform.c:1415:gst_base_transform_reconfigure: warning: not negotiated 0:00:00.948709446 834 0x55b9ce8a30 WARN basetransform gstbasetransform.c:1415:gst_base_transform_reconfigure: warning: not negotiated WARNING: from element /GstPipeline:pipeline0/GstDecodeBin:decodebin0/GstCapsFilter:capsfilter1: not negotiated Additional debug info: gstbasetransform.c(1415): gst_base_transform_reconfigure (): /GstPipeline:pipeline0/GstDecodeBin:decodebin0/GstCapsFilter:capsfilter1: not negotiated 0:00:00.971426937 834 0x7f70004a80 WARN basesrc gstbasesrc.c:3055:gst_base_src_loop: error: Internal data stream error. 0:00:00.971545793 834 0x7f70004a80 WARN basesrc gstbasesrc.c:3055:gst_base_src_loop: error: streaming stopped, reason not-negotiated (-4) 0:00:00.978535326 834 0x7f6807c8f0 WARN v4l2bufferpool gstv4l2bufferpool.c:1518:gst_v4l2_buffer_pool_dqbuf: Driver should never set v4l2_buffer.field to ANY (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.105: gst_mini_object_copy: assertion 'mini_object != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.105: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.105: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.105: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.105: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.105: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.105: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_mini_object_copy: assertion 'mini_object != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed ERROR: from element /GstPipeline:pipeline0/GstRTMPSrc:rtmpsrc0: Internal data stream error. (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_mini_object_copy: assertion 'mini_object != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.106: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.107: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.107: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.107: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed Additional debug info: gstbasesrc.c(3055): gst_base_src_loop (): /GstPipeline:pipeline0/GstRTMPSrc:rtmpsrc0: streaming stopped, reason not-negotiated (-4) ERROR: pipeline doesn't want to preroll. Setting pipeline to NULL ... (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.107: gst_mini_object_copy: assertion 'mini_object != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_mini_object_copy: assertion 'mini_object != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.108: gst_caps_get_structure: assertion 'GST_IS_CAPS (caps)' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.109: gst_structure_copy: assertion 'structure != NULL' failed (gst-launch-1.0:834): GStreamer-CRITICAL **: 14:19:35.109: gst_caps_append_structure_full: assertion 'GST_IS_CAPS (caps)' failed Freeing pipeline ...",
        "answers": [
            [
                "So as Aswin said, it was solved by adding convert before timeoverlay. Its because timeoverlay cannot work with DMA buffers (thats the (memory:NVMM) means) So the pipeline looks like original except for this change: ... decodebin ! nvvidconv ! 'video/x-raw' ! timeoverlay ! nvvidconv ! 'video/x-raw(memory:NVMM) The errors are basicaly about timeoverlay not being able to link to nvv4l2decoder which outputs the buffers as DMA. We need nvvidconv to copy those buffers so we can work with them in user space. Longer explanation with details: With my faint understanding of DMA/CPU buffers (please correct)- all nv* elements are able to work with DMA buffers (more directly I guess) which speeds up the process. But between nvv4l2decoder and nvv4l2h264enc we put the timeoverlay to add time into the video.. But unfortunately timeoverlay element does not have ability to work with those buffers. So we added the nvviddonv which slowly copies the buffers (it goes through CPU) into user space, slowly we do the changes, and then it slowly goes back to the faster \"DMA buffer space\". But I dont see some way around this. Unless there is some fancy custom element that could work with the buffers directly. There was some work in GStreamer to better work with these buffers in past few years - I saw some presentations by ndufresne about DMA fences (working with DMA buffers asynchronosely) and zero copy pipelines (not sure if it applies): Zero copy: https://youtu.be/kNaa1fPv_uo DMA fences: https://youtu.be/HpmzJGHqObs Some general Linux article: https://01.org/linuxgraphics/gfx-docs/drm/driver-api/dma-buf.html"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have Kodak PIXPRO SP360 4k camera connected to the Jetson Nano or TX2 via USB cable. I want to be able to see that video over browser, either with RTSP stream, Webrtc or something else. It doesn't matter how it works in terms of technology, as long as it works. So if you have any ideas or suggestions be free to share them. I'm currently trying to run the basic setup. ./test-launch \"nvarguscamerasrc ! video/x-raw(memory:NVMM), format=NV12, width=1920, height=1080, framerate=30/1 ! nvvidconv ! video/x-raw, width=640, height=480, format=NV12, framerate=30/1 ! omxh265enc ! rtph265pay name=pay0 pt=96 config-interval=1\" and gst-launch-1.0 rtspsrc location=rtsp://127.0.0.1:8554/test ! queue ! decodebin ! videoconvert ! xvimagesink and I'm getting the error saying Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... Progress: (open) Opening Stream Progress: (connect) Connecting to rtsp://127.0.0.1:8554/test Progress: (open) Retrieving server options Progress: (open) Retrieving media info ERROR: from element /GstPipeline:pipeline0/GstRTSPSrc:rtspsrc0: Could not get/set settings from/on resource. Additional debug info: gstrtspsrc.c(6999): gst_rtspsrc_setup_streams_start (): /GstPipeline:pipeline0/GstRTSPSrc:rtspsrc0: SDP contains no streams ERROR: pipeline doesn't want to preroll. Setting pipeline to PAUSED ... Setting pipeline to READY ... Setting pipeline to NULL ... Freeing pipeline ... (test-launch:22440): GLib-GObject-WARNING **: 11:36:46.018: invalid cast from 'GstRtpH265Pay' to 'GstBin' (test-launch:22440): GStreamer-CRITICAL **: 11:36:46.018: gst_bin_get_by_name: assertion 'GST_IS_BIN (bin)' failed (test-launch:22440): GLib-GObject-WARNING **: 11:36:46.018: invalid cast from 'GstRtpH265Pay' to 'GstBin' (test-launch:22440): GStreamer-CRITICAL **: 11:36:46.018: gst_bin_get_by_name: assertion 'GST_IS_BIN (bin)' failed (test-launch:22440): GLib-GObject-WARNING **: 11:36:46.018: invalid cast from 'GstRtpH265Pay' to 'GstBin' (test-launch:22440): GStreamer-CRITICAL **: 11:36:46.018: gst_bin_get_by_name: assertion 'GST_IS_BIN (bin)' failed I have also tried an option that worked for me on PC but I can't get it to work on Jetson. The setup goes as follows. Download Streameye from https://github.com/ccrisan/streameye and run: netcat -l 8700 | ./streameye -p 1337 To send the webcam stream I run: gst-launch-1.0 v4l2src device=/dev/video0 ! decodebin ! videoconvert ! videoscale ! videorate ! jpegenc quality=30 ! tcpclientsink host=127.0.0.1 port=8700 After this I get: Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... Setting pipeline to PLAYING ... New clock: GstSystemClock ERROR: from element /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: Internal data stream error. Additional debug info: gstbasesrc.c(3064): gst_base_src_loop (): /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: streaming stopped, reason not-negotiated (-4) Execution ended after 0:00:03.944998186 Setting pipeline to PAUSED ... Setting pipeline to READY ... Setting pipeline to NULL ... Freeing pipeline ... Output of this command for my camera is: v4l2-ctl -d /dev/video1 --list-formats-ext ioctl: VIDIOC_ENUM_FMT Index : 0 Type : Video Capture Pixel Format: 'MJPG' (compressed) Name : Motion-JPEG Size: Discrete 3840x2160 Interval: Discrete 0.200s (5.000 fps) Size: Discrete 2880x2880 Interval: Discrete 0.200s (5.000 fps) Size: Discrete 2048x2048 Interval: Discrete 0.200s (5.000 fps) Size: Discrete 1440x1440 Interval: Discrete 0.067s (15.000 fps) Interval: Discrete 0.200s (5.000 fps) Size: Discrete 1920x1080 Interval: Discrete 0.067s (15.000 fps) Interval: Discrete 0.200s (5.000 fps) Size: Discrete 1280x720 Interval: Discrete 0.033s (30.000 fps) Interval: Discrete 0.067s (15.000 fps) Interval: Discrete 0.200s (5.000 fps) Size: Discrete 640x360 Interval: Discrete 0.033s (30.000 fps) Interval: Discrete 0.067s (15.000 fps) Interval: Discrete 0.200s (5.000 fps)",
        "answers": [
            [
                "run your pipe with -v like this and show me result: gst-launch-1.0 v4l2src device=/dev/video0 ! decodebin ! videoconvert ! videoscale ! videorate ! jpegenc quality=30 ! tcpclientsink host=127.0.0.1 port=8700 -v"
            ],
            [
                "If you want to stream it, the simplest way will be to use gst-rtsp-launch which is part of GStreamer prebuild binaries: gst-rtsp-launch '( v4l2src device=/dev/video0 ! videoconvert ! queue ! x264enc tune=\"zerolatency\" byte-stream=true bitrate=10000 ! rtph264pay name=pay0 pt=96 )' Later on you can tune codec, bitrate, but for me this is enough (playable in VLC - rtsp://127.0.0.1:8554/test)"
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am trying to convert a TF 2.0 saved_model to tensorRT on the Jetson Nano. The model was saved in TF 2.0.0. The nano has Jetpack 4.2.2 w/ TensorRT __ and Tensorflow 1.14 (that is the latest Tensorflow release for Jetson). I have been following the instuctions from here which describe how to convert a TF 2.0.0 saved_model into TensorRT. Below is my code: import tensorflow as tf from tensorflow.python.compiler.tensorrt import trt_convert as trt tf.enable_eager_execution() converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir) converter.convert() converter.save(output_saved_model_dir) saved_model_loaded = tf.saved_model.load( output_saved_model_dir, tags=[tag_constants.SERVING]) graph_func = saved_model_loaded.signatures[ signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] frozen_func = convert_to_constants.convert_variables_to_constants_v2( graph_func) def wrap_func(*args, **kwargs): # Assumes frozen_func has one output tensor return frozen_func(*args, **kwargs)[0] output = wrap_func(input_data).numpy() It seems to start converting successfully. However I get an KeyError: 'serving_default' error when it reaches the convert_to_tensor line. My complete printout is below found here (too long for SO), but the python traceback appears below. How can I fix this? Thanks! printout summary (complete printout here): Traceback (most recent call last): File \"tst.py\", line 38, in &lt;module&gt; convert_savedmodel() File \"tst.py\", line 24, in convert_savedmodel converter.convert() File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 956, in convert func = self._saved_model.signatures[self._input_saved_model_signature_key] File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_serialization.py\", line 196, in __getitem__ return self._signatures[key] KeyError: 'serving_default'",
        "answers": [
            [
                "I can see two problems in your experiment: You are using TF-TRT 2.0 API while having TF 1.14 installed. That is not supported. If you have TF 1.14 installed on your system, then you would need to use TF-TRT 1.x API. TF Models saved in TF2.0 are not compatible with TF1.14 according to https://www.tensorflow.org/guide/versions If you only have access to TF1.14, I suggest to re-generate the graph in TF1.14 and save the model there before applying TF-TRT, and then use TF-TRT 1.x API."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "When i used knn for recognition and running following function: def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.49): if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS: raise Exception(\"Invalid image path: {}\".format(X_img_path)) if knn_clf is None and model_path is None: raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\") if knn_clf is None: with open(model_path, 'rb') as f: knn_clf = pickle.load(f) X_img = face_recognition.load_image_file(X_img_path) X_face_locations = face_recognition.face_locations(X_img) if len(X_face_locations) == 0: return [] faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations) closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=4) I got this error: Traceback (most recent call last): File \"Recognition.py\", line 225, in &lt;module&gt; predictions = predict(full_file_path, model_path= \"/home/abc/FS/trained_knn_model.clf\") File \"Recognition.py\", line 75, in predict closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1) File \"/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/base.py\", line 402, in kneighbors X = check_array(X, accept_sparse='csr') File \"/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\", line 542, in check_array allow_nan=force_all_finite == 'allow-nan') File \"/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\", line 56, in _assert_all_finite raise ValueError(msg_err.format(type_err, X.dtype)) ValueError: Input contains NaN, infinity or a value too large for dtype('float64'). When the same code i m running in my cpu it working fine but when i m running in jetson nano it show above error. My jetson nano configuration is Jetpack 4.2.1 r32.2 Ubuntu 18.04 python 3.6.8",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am converting arcface model of onnx version to tensorrt. I am following the official nvidia document for the process. But,I am getting an error of segmentation fault core dumped. Please provide your best possible suggestions. I am running it on jetson nano.",
        "answers": [
            [
                "You could try to convert your model to tensorrt on a more capable machine. Then you can try to run it on jetson nano. The conversion operation can be more demanding than the inference sometimes. I have also encountered a similar issue with SSD_mobilenet_v2 conversion. I tried to convert in on jetson but it failed so I made the conversion on my laptop which has a 1050m."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "So we have are trying to use backward-cpp for printing stacktraces in our c++ application. In particular we are using a signal handler for catching errors and printing them. Note: Using llibdw for detailed stacks Works great on x86_64 but fails to print a complete stacktrace on arm64 nvidia-jetson-nano. Running sample bracktrace_test on x86_64: $ g++ --version g++ (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0 Copyright (C) 2017 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. $ uname --operating-system --kernel-name --kernel-release --hardware-platform --processor Linux 4.15.0-65-generic x86_64 x86_64 GNU/Linux $ g++ -g -rdynamic backward.hpp backtrace_test.cpp -o backtrace_test -ldw $ ./backtrace_test Running same sample bracktrace_test on nano arm64: $ g++ --version g++ (Ubuntu/Linaro 7.4.0-1ubuntu1~18.04.1) 7.4.0 Copyright (C) 2017 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. $ uname --operating-system --kernel-name --kernel-release --hardware-platform --processor Linux 4.9.140-tegra aarch64 aarch64 GNU/Linux $ g++ -g -rdynamic -funwind-tables -fasynchronous-unwind-tables backward.hpp backtrace_test.cpp -o backtrace_test -ldw $ ./backtrace_test Code for the backtrace test: #define BACKWARD_HAS_DW 1 #include &lt;execinfo.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;unistd.h&gt; #include &lt;stdexcept&gt; #include \"backward.hpp\" backward::SignalHandling sh; void myfunc(int ncalls) { throw std::invalid_argument( \"sample invalid exception\"); } int main(int argc, char *argv[]) { myfunc(); exit(EXIT_SUCCESS); } Issue tracking: issue",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am having trouble setting up cellular connection on my Jetson Nano to T-Mobile with a Huawei E220 stick. It used to be able to connect but it does not connect any more. PIN is disabled, and stick works fine under Windows. I think it might be related to the fact that Ubuntu Modem Manager misidentified the stick as E176? Please advise. p:~$ lsusb | grep Huawei Bus 001 Device 005: ID 12d1:1003 Huawei Technologies Co., Ltd. E220 HSDPA Modem / E230/E270/E870 HSDPA/HSUPA Modem p:~$ mmcli -L /org/freedesktop/ModemManager1/Modem/0 [huawei] E176 p:~$ lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 18.04.3 LTS Release: 18.04 Codename: bionic p:~$ nmcli d DEVICE TYPE STATE CONNECTION eth0 ethernet connected Wired connection 1 l4tbr0 bridge connected l4tbr0 ttyUSB1 gsm connecting (prepare) T-Mo dummy0 dummy unmanaged -- rndis0 ethernet unmanaged -- usb0 ethernet unmanaged -- lo loopback unmanaged --",
        "answers": [
            [
                "After a couple of mysterious connect/disconnects I put an USB extension cord between the Nano and the modem et voil\u00e0! Connection is robust now. Never mind how many lines you see in your connection icon, you might have reception issues if activation fails with msgs like these in your logs: $ sudo journalctl -fu NetworkManager -- Logs begin at Fri 2019-10-25 12:49:30 CEST. -- okt 25 12:53:54 jerry-desktop NetworkManager[3947]: &lt;info&gt; [1572000834.4694] device (ttyUSB1): Activation: starting connection 'T-Mobile Default' (94e66a1c-baa3-4d94-b439-80ca55d26735) okt 25 12:53:54 jerry-desktop NetworkManager[3947]: &lt;info&gt; [1572000834.4705] device (ttyUSB1): state change: disconnected -&gt; prepare (reason 'none', sys-iface-state: 'managed') okt 25 12:55:04 jerry-desktop NetworkManager[3947]: &lt;info&gt; [1572000904.4144] connectivity: (l4tbr0) timed out okt 25 12:56:00 jerry-desktop NetworkManager[3947]: &lt;warn&gt; [1572000960.4562] modem-broadband[ttyUSB1]: failed to connect modem: Network timeout okt 25 12:56:00 jerry-desktop NetworkManager[3947]: &lt;info&gt; [1572000960.4566] device (ttyUSB1): state change: prepare -&gt; failed (reason 'gsm-registration-timeout', sys-iface-state: 'managed') okt 25 12:56:00 jerry-desktop NetworkManager[3947]: &lt;warn&gt; [1572000960.4600] device (ttyUSB1): Activation: failed for connection 'T-Mobile Default' okt 25 12:56:00 jerry-desktop NetworkManager[3947]: &lt;info&gt; [1572000960.4627] device (ttyUSB1): state change: failed -&gt; disconnected (reason 'none', sys-iface-state: 'managed') okt 25 12:56:00 jerry-desktop NetworkManager[3947]: &lt;info&gt; [1572000960.4649] policy: auto-activating connection 'T-Mobile Default' okt 25 12:56:00 jerry-desktop NetworkManager[3947]: &lt;info&gt; [1572000960.4679] device (ttyUSB1): Activation: starting connection 'T-Mobile Default' (94e66a1c-baa3-4d94-b439-80ca55d26735) okt 25 12:56:00 jerry-desktop NetworkManager[3947]: &lt;info&gt; [1572000960.4690] device (ttyUSB1): state change: disconnected -&gt; prepare (reason 'none', sys-iface-state: 'managed') okt 25 12:58:06 jerry-desktop NetworkManager[3947]: &lt;warn&gt; [1572001086.4517] modem-broadband[ttyUSB1]: failed to connect modem: Network timeout"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I've build an image for my Jetson Nano with yocto using the meta-tegra layer. This build is using u-boot as bootloader which is set to save the environment on an MMC partition (mmcblk0p14). gdisk -l /dev/mmcblk0 shows the following: Number Start (sector) End (sector) Size Code Name ... 14 20996096 20998143 1024.0 KiB 8300 UBOOTENV ... And the sector size is 512. I've then configured u-boot-tegra/include/configs/p3450-porg.h with: ... /* Env is located in it's own partition */ #define CONFIG_ENV_IS_IN_MMC #define CONFIG_SYS_MMC_ENV_DEV 1 #define CONFIG_ENV_OFFSET (20996096 * 512) ... Where CONFIG_ENV_OFFSET = Start_Sector * Block_Size This works fine (as far as I can see) as the environment is saved successfully to MMC when i use saveenv. However, the environment i get when i print it in u-boot shell is NOT the same as when i print the environment with fw_printenv u-boot tool. I have set the /etc/fw_env.config to: # Device name Device offset Env size /dev/mmcblk0p14 0 0x2000 So what I've gathered is that, either the fw_env.config is set wrong or the u-boot environment is saved somewhere else on the MMC and no the partition 14. Does anyone have suggestions to what i could try? *****************************************************EDIT:***************************************************** Doing dd if=/dev/mmcblk0p14 of=tmp.txt and reading the tmp.txt file shows the environment that the fw_printenv shows and not the environment I'm seeing in u-boot shell. So something must be wrong in the u-boot-tegra/include/configs/p3450-porg.h configuration. I just wonder where it actually writes the environment to when i do a saveenv... Any Idea what I can try to change?",
        "answers": [
            [
                "As stated in the comments to the question, the offset is a 32-bit integer so attempting to give it the value of more than 4,294,967,295 (which 20996096 * 512 is) is not gonna work. To fix it, I've rearranged my partition scheme to have my uboot environment partition as partition 1 instead of 14 and changed the fw_env.config and p3450-porg.h patch accordingly."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I want to minimize my face recognition system from PC to a Jetson Nano board I use this example code: https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py The system works well when running on PC, it can recognize known faces. But when I move the code to Jetson Nano, the system can't recognize known faces. Is there any additional configuration for the ARM processor?",
        "answers": [
            [
                "You have to install all the dependencies first and I am posting the link and commands if you r using TensorFlow. You have to build OpenCV also in Jetson Nano. https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html $ sudo apt-get update $ sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran Install and upgrade pip3. $ sudo apt-get install python3-pip $ sudo pip3 install -U pip testresources setuptools Install the Python package dependencies. $ sudo pip3 install -U numpy==1.16.1 future==0.17.1 mock==3.0.5 h5py==2.9.0 keras_preprocessing==1.0.5 keras_applications==1.0.8 gast==0.2.2 futur"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "i want to minimize my face recognition system from PC to Jetson nano. after the system done encoding some images in dataset, the system freeze for a while and get killed. this my code: import face_recognition import cv2 import numpy as np import os import glob import platform def running_on_jetson_nano(): return platform.machine() == \"aarch64\" def get_jetson_gstreamer_source(capture_width=1280, capture_height=720, display_width=1280, display_height=720, framerate=60, flip_method=0): return (f'nvarguscamerasrc ! video/x-raw(memory:NVMM), ' + f'width=(int){capture_width}, height=(int){capture_height}, ' + f'format=(string)NV12, framerate=(fraction){framerate}/1 ! ' + f'nvvidconv flip-method={flip_method} ! ' + f'video/x-raw, width=(int){display_width}, height=(int){display_height}, format=(string)BGRx ! ' + 'videoconvert ! video/x-raw, format=(string)BGR ! appsink' ) if running_on_jetson_nano(): video_capture = cv2.VideoCapture(get_jetson_gstreamer_source(), cv2.CAP_GSTREAMER) else: video_capture = cv2.VideoCapture(0) path = './dataset' folders = [f for f in glob.glob(path + '**/*', recursive=True)] known_face_encodings = [] known_face_names = [] for f in folders: names = f.split('/')[2] print('encoding file : {}'.format(names)) for images_f in glob.glob(f + '**/*.jpg'): images = face_recognition.load_image_file(images_f) location = face_recognition.face_locations(images) images_encoding = face_recognition.face_encodings(images, known_face_locations = location)[0] known_face_encodings.append(images_encoding) known_face_names.append(names) does anyone know how to solve this problem?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using OpenGL in C++ (technically EGL, on a Jetson Nano.) Let's say I want to draw N Quads. Imagine just a list of colored rectangles. There may be a few thousand such rectangles in the frame. I want to use two vertex buffers: One that defines the geometry of each quad. One that defines the properties common to each quad. The first vertex buffer should define the geometry of each quad. It should have only 4 vertices in it and its data would be just the corners of a quad. Something like: 0, 0, // top left 1, 0, // top right 0, 1, // bottom left 1, 1, // bottom right Then the second vertex buffer should have just the x,y,width,height of all the rectangles. x1, y1, width1, height1, color1, x2, y2, width2, height2, color2, x3, y3, width3, height3, color3, x4, y4, width4, height4, color4, x5, y5, width5, height5, color5, x6, y6, width6, height6, color6, ... etc. The thing is that each one of the items in my rectangle buffer should apply to 4 vertices in the vertex buffer. Is there a way to set this up so that it keeps reusing the same 4 quad vertices over and over for each rectangle and applies the same rectangle properties to 4 vertices at a time? I'm imagining there's something I can do so that I say that the first vertex buffer should use one element per vertex and wraps around, but the second vertex buffer uses one element per every four vertices or something like that. How do I set this up? What I do now: Right now I need one vertex buffer that just has the quad vertices repeated over and over as many times as I have instances. 0, 0, // (1) top left 1, 0, // 0, 1, // 1, 1 // 0, 0, // (2) top left 1, 0, // 0, 1, // 1, 1, // 0, 0, // (3) top left 1, 0, // 0, 1, // 1, 1, // ... etc And my second buffer duplicates its data for each vertex: x1, y1, width1, height1, color1, x1, y1, width1, height1, color1, x1, y1, width1, height1, color1, x1, y1, width1, height1, color1, x2, y2, width2, height2, color2, x2, y2, width2, height2, color2, x2, y2, width2, height2, color2, x2, y2, width2, height2, color2, x3, y3, width3, height3, color3, x3, y3, width3, height3, color3, x3, y3, width3, height3, color3, x3, y3, width3, height3, color3, x4, y4, width4, height4, color4, x4, y4, width4, height4, color4, x4, y4, width4, height4, color4, x4, y4, width4, height4, color4, x5, y5, width5, height5, color5, x5, y5, width5, height5, color5, x5, y5, width5, height5, color5, x5, y5, width5, height5, color5, x6, y6, width6, height6, color6, x6, y6, width6, height6, color6, x6, y6, width6, height6, color6, x6, y6, width6, height6, color6, ... etc. This seems really inefficient and I just want to specify the first 4 vertices once and have it keep reusing them somehow rather than duplicating these 4 vertices N times to have a total of 4*N vertices in my first buffer. And I only want to specify the x,y,width,height,color attributes once for each quad for a total of N vertices, and not once for each overall vertex for a total of 4*N vertices. What do I do?",
        "answers": [
            [
                "Generally speaking, the most efficient way to render a series of quads is to... render a series of quads. You don't send width/height or other per-instance information; you compute the actual positions of the 4 vertices on the CPU and you write them to GPU memory using appropriate buffer object streaming techniques. Specifically, avoid trying to change only a few quads; if your data isn't static, it's probably going to be better to re-upload all of it (to a different/invalidated buffer) rather than modify only a few bytes in-situ. Your hypothetical alternative would only perform better in two scenarios: if the bandwidth of writing data to the GPU is your current bottleneck (whether due to quads or some other transfers you're doing) or if the bandwidth of reading data for rendering is the current bottleneck. You can mitigate this issue by reducing the size of the vertex data. Since we're talking 2D quads, you could very well use shorts for the XY position of each vertex. Or 16-bit floats. Either way, this means that each vertex (position + color) only takes up 8 bytes, which means a quad is just 32-bytes of data. Obviously 12 bytes is less than 32 (12 being the per-instance cost if you use similar compression), but it's still a 33% reduction over the 48 bytes that full float positions would use. If you have done your profiling homework and have determined that 32-bytes-per-quad is too much, vertex instancing is still a bad idea. It is well known that, on some hardware, extremely small instances can kill your VS performance. Therefore, it should be avoided. In this case, it may be best to forgo all vertex attribute usage (your VAO should have all arrays disable and your VS should have no in values defined). Instead, you should fetch instance data directly from SSBOs. The gl_VertexID input value tells you what vertex index is being rendered. Given that you're rendering quads, the current instance would be gl_VertexID / 4. And the current vertex within the quad is gl_VertexID % 4. So your VS would look something like this: struct instance { vec2 position; vec2 size; uint color; //Packed as 4 bytes; unpack with unpackUnorm4x8 uint padding; //Padding needed due to alignment/stride of 8 bytes. }; layout(binding = 0, std430) buffer instance_data { instance instances[]; }; vec2[4] vertex_table = { vec2{0, 0}, vec2{1, 0}, vec2{0, 1}, vec2{1, 1}, }; void main() { instance curr_instance = instances[gl_VertexID / 4]; vec2 vertex = vertex_table[gl_VertexID % 4]; vertex = curr_instance.position + (curr_instance.size * vertex); gl_Position = vec4(vertex.xy, 0.0, 1.0); } How fast this sort of thing will be depends entirely on how well your GPU handles these kinds of global memory reads. Note that it is at least hypothetically possible to reduce the size of the per-instance data back to 12. You can pack the position and size into two 16-bit shorts or half-floats, using unpackUnorm2x16 or unpackHalf2x16 to unpack these values, respectively. If you do this, then your instance struct is just 3 uint values, and there is no need for padding."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "This code cap = cv2.VideoCapture('/dev/video0', cv2.CAP_V4L2) gives VIDEOIO ERROR: V4L2: Pixel format of incoming image is unsupported by OpenCV Here is the output of v4l2-ctl -d /dev/video0 --list-formats: ioctl: VIDIOC_ENUM_FMT Index : 0 Type : Video Capture Pixel Format: 'RG10' Name : 10-bit Bayer RGRG/GBGB Here is my OpenCV: -- OpenCL samples are skipped: OpenCL SDK is required -- -- General configuration for OpenCV 4.1.2-pre ===================================== -- Version control: 4.1.1-365-g9efafc3e3 -- -- Extra modules: -- Location (extra): /home/sovlyn/Downloads/opencv_contrib/modules -- Version control (extra): 4.1.1-62-g83e98d24 -- -- Platform: -- Timestamp: 2019-10-10T09:26:02Z -- Host: Linux 4.9.140-tegra aarch64 -- CMake: 3.10.2 -- CMake generator: Unix Makefiles -- CMake build tool: /usr/bin/make -- Configuration: RELEASE -- -- CPU/HW features: -- Baseline: NEON FP16 -- required: NEON -- disabled: VFPV3 -- -- C/C++: -- Built as dynamic libs?: YES -- C++ Compiler: /usr/bin/c++ (ver 7.4.0) -- C++ flags (Release): -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -DNDEBUG -- C++ flags (Debug): -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -fvisibility-inlines-hidden -g -O0 -DDEBUG -D_DEBUG -- C Compiler: /usr/bin/cc -- C flags (Release): -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -O3 -DNDEBUG -DNDEBUG -- C flags (Debug): -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections -fvisibility=hidden -g -O0 -DDEBUG -D_DEBUG -- Linker flags (Release): -Wl,--gc-sections -- Linker flags (Debug): -Wl,--gc-sections -- ccache: NO -- Precompiled headers: NO -- Extra dependencies: dl m pthread rt -- 3rdparty dependencies: -- -- OpenCV modules: -- To be built: aruco bgsegm bioinspired calib3d ccalib core datasets dnn dnn_objdetect dnn_superres dpm face features2d flann freetype fuzzy gapi hfs highgui img_hash imgcodecs imgproc line_descriptor ml objdetect optflow phase_unwrapping photo plot python3 quality reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab xfeatures2d ximgproc xobjdetect xphoto -- Disabled: world -- Disabled by dependency: - -- Unavailable: cnn_3dobj cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev cvv hdf java js matlab ovis python2 sfm viz -- Applications: tests perf_tests examples apps -- Documentation: NO -- Non-free algorithms: YES -- -- GUI: -- GTK+: YES (ver 2.24.32) -- GThread : YES (ver 2.56.4) -- GtkGlExt: NO -- VTK support: NO -- -- Media I/O: -- ZLib: /usr/lib/aarch64-linux-gnu/libz.so (ver 1.2.11) -- JPEG: /usr/lib/aarch64-linux-gnu/libjpeg.so (ver 80) -- WEBP: build (ver encoder: 0x020e) -- PNG: /usr/lib/aarch64-linux-gnu/libpng.so (ver 1.6.34) -- TIFF: /usr/lib/aarch64-linux-gnu/libtiff.so (ver 42 / 4.0.9) -- JPEG 2000: build (ver 1.900.1) -- OpenEXR: build (ver 2.3.0) -- HDR: YES -- SUNRASTER: YES -- PXM: YES -- PFM: YES -- -- Video I/O: -- DC1394: YES (2.2.5) -- FFMPEG: YES -- avcodec: YES (57.107.100) -- avformat: YES (57.83.100) -- avutil: YES (55.78.100) -- swscale: YES (4.8.100) -- avresample: NO -- GStreamer: YES (1.14.5) -- v4l/v4l2: YES (linux/videodev2.h) -- -- Parallel framework: TBB (ver 2017.0 interface 9107) -- -- Trace: YES (with Intel ITT) -- -- Other third-party libraries: -- Lapack: NO -- Eigen: YES (ver 3.3.4) -- Custom HAL: YES (carotene (ver 0.0.1)) -- Protobuf: build (3.5.1) -- -- OpenCL: YES (no extra features) -- Include path: /home/sovlyn/Downloads/opencv/3rdparty/include/opencl/1.2 -- Link libraries: Dynamic load -- -- Python 3: -- Interpreter: /usr/bin/python3 (ver 3.6.8) -- Libraries: /usr/lib/aarch64-linux-gnu/libpython3.6m.so (ver 3.6.8) -- numpy: /usr/local/lib/python3.6/dist-packages/numpy/core/include (ver 1.17.2) -- install path: lib/python3.6/dist-packages/cv2/python-3.6 -- -- Python (for build): /usr/bin/python3 -- -- Java: -- ant: NO -- JNI: NO -- Java wrappers: NO -- Java tests: NO -- -- Install to: /usr/local",
        "answers": [
            [
                "If you are using pi-cam on jetson, then use below pipe line to get video stream from MIPI CSI camera: cv2.VideoCapture('nvarguscamerasrc ! video/x-raw(memory:NVMM), width=(int)640, height=(int)480, format=(string)NV12, framerate=(fraction)60/1 ! nvvidconv flip-method=0 ! video/x-raw, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink', cv2.CAP_GSTREAMER) Change flip-method to 0 or 2 (for more options, see here). Good luck!"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "This is a continuation of my other post. I've managed to create an image with u-boot and rauce. I've made a simple rauc system.conf: [system] compatible=Jetson Nano bootloader=uboot # [slot.rootfs.0] device=/dev/mmcblk0p1 type=ext4 bootname=system0 # [slot.rootfs.1] device=/dev/mmcblk0p13 type=ext4 bootname=system1 [UPDATED]: Pretty much copy pasted the contrib uboot.sh script. Then I've added a bb file from here into my bsp layer. And added rauc to my IMAGE_INSTALL. When i boot up the nano with my image, rauc isn't working as it should. When i check the status on the service with systemctl status rauc-mark-service-good.service it returns: \u25cf rauc-mark-good.service - Rauc Good-marking Service Loaded: loaded (/lib/systemd/system/rauc-mark-good.service; enabled; vendor preset: enabled) Active: inactive (dead) since Tue 2019-10-01 07:51:22 UTC; 4s ago Process: 4147 ExecStart=/usr/bin/rauc status mark-good (code=exited, status=0/SUCCESS) Main PID: 4147 (code=exited, status=0/SUCCESS) Oct 01 07:51:22 jetson-nano systemd[1]: Started Rauc Good-marking Service. Oct 01 07:51:22 jetson-nano rauc[4147]: Failed getting primary slot: Failed getting primary slot: Unable to find primary boot slot Oct 01 07:51:22 jetson-nano rauc[4147]: rauc mark: marked slot rootfs.0 as good Oct 01 07:51:22 jetson-nano systemd[1]: rauc-mark-good.service: Succeeded. systemctl status rauc returns: \u25cf rauc.service - Rauc Update Service Loaded: loaded (/lib/systemd/system/rauc.service; static; vendor preset: enabled) Active: active (running) since Tue 2019-10-01 07:49:36 UTC; 2min 0s ago Docs: https://rauc.readthedocs.io Main PID: 4092 (rauc) Tasks: 3 (limit: 4178) Memory: 4.4M CGroup: /system.slice/rauc.service \u2514\u25004092 /usr/bin/rauc --mount=/run/rauc service Oct 01 07:49:36 jetson-nano systemd[1]: Starting Rauc Update Service... Oct 01 07:49:36 jetson-nano systemd[1]: Started Rauc Update Service. Oct 01 07:49:48 jetson-nano rauc[4092]: Failed getting primary slot: Failed getting primary slot: Unable to find primary boot slot Oct 01 07:49:48 jetson-nano rauc[4092]: Failed to load status file /slot.raucs: No such file or directory Oct 01 07:49:48 jetson-nano rauc[4092]: mounting slot /dev/mmcblk0p13 Oct 01 07:49:48 jetson-nano rauc[4092]: Failed to load status file /run/rauc/rootfs.1/slot.raucs: No such file or directory Oct 01 07:51:22 jetson-nano rauc[4092]: Failed getting primary slot: Failed getting primary slot: Unable to find primary boot slot Oct 01 07:51:22 jetson-nano rauc[4092]: rauc mark: marked slot rootfs.0 as good And rauc status returns: (rauc:4195): rauc-WARNING **: 07:51:46.126: Failed getting primary slot: Failed getting primary slot: Unable to find primary boot slot Compatible: Jetson Nano Variant: Booted from: rootfs.0 (/dev/mmcblk0p1) Activated: (null) ((null)) slot states: rootfs.0: class=rootfs, device=/dev/mmcblk0p1, type=ext4, bootname=system0 state=booted, description=, parent=(none), mountpoint=/ boot status=bad rootfs.1: class=rootfs, device=/dev/mmcblk0p13, type=ext4, bootname=system1 state=inactive, description=, parent=(none), mountpoint=(none) boot status=bad So there is no /slot.raucs file and it failed to find primary boot slot. After that, systemctl status rauc-mark-good returns that the rootfs.0 slot has been marked as good in the end, but systemctl status rauc shows that the boot status is bad. What am I missing here?",
        "answers": [
            [
                "I edited the uboot script to the following: test -n \"${BOOT_ORDER}\" || setenv BOOT_ORDER \"system0 system1\" test -n \"${BOOT_system0_LEFT}\" || setenv BOOT_system0_LEFT 3 test -n \"${BOOT_system1_LEFT}\" || setenv BOOT_system1_LEFT 3 setenv bootargs for BOOT_SLOT in \"${BOOT_ORDER}\"; do if test \"x${bootargs}\" != \"x\"; then # skip remaining slots elif test \"x${BOOT_SLOT}\" = \"xsystem0\"; then if test ${BOOT_system0_LEFT} -gt 0; then setexpr BOOT_system0_LEFT ${BOOT_system0_LEFT} - 1 echo \"Found valid slot system0, ${BOOT_system0_LEFT} attempts remaining\" setenv distro_bootpart \"1\" setenv boot_line \"mmc 1:1 any ${scriptaddr} /boot/extlinux/extlinux.conf\" fi elif test \"x${BOOT_SLOT}\" = \"xsystem1\"; then if test ${BOOT_system1_LEFT} -gt 0; then setexpr BOOT_system1_LEFT ${BOOT_system1_LEFT} - 1 echo \"Found valid slot system1, ${BOOT_system1_LEFT} attempts remaining\" setenv distro_bootpart \"13\" setenv boot_line \"mmc 1:D any ${scriptaddr} /boot/extlinux/extlinux.conf\" fi fi done if test -n \"${bootargs}\"; then saveenv else echo \"No valid slot found, resetting tries to 3\" setenv BOOT_system0_LEFT 3 setenv BOOT_system1_LEFT 3 saveenv reset fi sysboot ${boot_line} And it ended up working. Apparently there was some issues with the BOOT_ORDER \"system0 system1\" in the the uboot script that was somehow not the same as in the RAUC system.conf. When i re-wrote the script, there was no issues and RAUC was running fine."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Used this to build inside release directory of opencv To build OpenCV inside the virtual environment offersTest cmake -D WITH_CUDA=ON -D CUDA_ARCH_BIN=\"5.3\" -D CUDA_ARCH_PTX=\"\" -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-4.0.0/modules -D WITH_GSTREAMER=ON -D WITH_LIBV4L=ON -D INSTALL_C_EXAMPLES=OFF -D BUILD_opencv_python3=ON -D BUILD_PERF_TESTS=OFF -D BUILD_EXAMPLES=OFF -D CMAKE_BUILD_TYPE=RELEASE -D PYTHON_EXECUTABLE=~/.virtualenvs/offersTest/bin/python3 -D CMAKE_INSTALL_PREFIX=/usr/local .. I wanted the OpenCV to be installed inside the virtual environment offersTest as i have specified the path here -D PYTHON_EXECUTABLE=~/.virtualenvs/offersTest/bin/python3 during make -j2 the interpreter points to the correct location of the python But during sudo make install It prefers the minimum python version 2.7",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've managed to create an image with two rootfs partitions to run on my jetson nano with yocto/poky. I've followed the meta-rauc layer README and rauc user manual, to create the system.conf file and rauc_%.bbappend file and I am able to create bundles successfully. As I understand, I need some sort of u-boot script: In order to enable RAUC to switch the correct slot, its system configuration must specify the name of the respective slot from the bootloader\u2019s perspective. You also have to set up an appropriate boot selection logic in the bootloader itself, either by scripting (as for GRUB, U-Boot) or by using dedicated boot selection infrastructure (such as bootchooser in Barebox). The bootloader must also provide a set of variables the Linux userspace can modify in order to change boot order or priority. Having this interface ready, RAUC will care for setting the boot logic appropriately. It will, for example, deactivate the slot to update before writing to it and reactivate it after having completed the installation successfully. Do I make a script somewhere in the yocto layer or build folder or is it a script i need to put on the jetson nano after making the image? - and what would the contents of this script be? **************************************************EDIT******************************************************** I've made this script: test -n \"${BOOT_ORDER}\" || setenv BOOT_ORDER \"system0 system1\" test -n \"${BOOT_system0_LEFT}\" || setenv BOOT_system0_LEFT 3 test -n \"${BOOT_system1_LEFT}\" || setenv BOOT_system1_LEFT 3 setenv bootargs for BOOT_SLOT in \"${BOOT_ORDER}\"; do if test \"x${bootargs}\" != \"x\"; then # skip remaining slots elif test \"x${BOOT_SLOT}\" = \"xsystem0\"; then if test ${BOOT_system0_LEFT} -gt 0; then setexpr BOOT_system0_LEFT ${BOOT_system0_LEFT} - 1 echo \"Found valid slot system0, ${BOOT_system0_LEFT} attempts remaining\" setenv distro_bootpart \"1\" setenv boot_line \"mmc 1:1 any ${scriptaddr} /boot/extlinux/extlinux.conf\" setenv bootargs \"${default_bootargs} root=/dev/mmcblk0p1 rauc.slot=system0\" fi elif test \"x${BOOT_SLOT}\" = \"xsystem1\"; then if test ${BOOT_system1_LEFT} -gt 0; then setexpr BOOT_system1_LEFT ${BOOT_system1_LEFT} - 1 echo \"Found valid slot system1, ${BOOT_system1_LEFT} attempts remaining\" setenv distro_bootpart \"13\" setenv boot_line \"mmc 1:D any ${scriptaddr} /boot/extlinux/extlinux.conf\" setenv bootargs \"${default_bootargs} root=/dev/mmcblk0p13 rauc.slot=system1\" fi fi done if test -n \"${bootargs}\"; then saveenv else echo \"No valid slot found, resetting tries to 3\" setenv BOOT_system0_LEFT 3 setenv BOOT_system1_LEFT 3 saveenv reset fi sysboot ${boot_line} And I i got this recipe recipes-bsp/u-boot/u-boot-script.bb in my meta-layer: LICENSE = \"GPLv2+\" LIC_FILES_CHKSUM = \"file://Licenses/README;md5=30503fd321432fc713238f582193b78e\" S = \"${WORKDIR}/git\" PACKAGE_ARCH = \"${MACHINE_ARCH}\" DEPENDS = \"u-boot-mkimage-native\" inherit deploy BOOTSCRIPT ??= \"${THISDIR}/uboot.sh\" do_mkimage () { uboot-mkimage -A arm -O linux -T script -C none -a 0 -e 0 \\ -n \"boot script\" -d ${BOOTSCRIPT} ${S}/boot.scr } addtask mkimage after do_compile before do_install do_compile[noexec] = \"1\" do_install () { install -D -m 644 ${S}/boot.scr ${D}/boot.scr } do_deploy () { install -D -m 644 ${D}/boot.scr \\ ${DEPLOYDIR}/boot.scr-${MACHINE}-${PV}-${PR} cd ${DEPLOYDIR} rm -f boot.scr-${MACHINE} ln -sf boot.scr-${MACHINE}-${PV}-${PR} boot.scr-${MACHINE} } addtask deploy after do_install before do_build FILES_${PN} += \"/\" COMPATIBLE_MACHINE = \"jetson-nano\" I can see that the script image is getting into work/jetson_nano_poky-linux/u-boot-tegra/2016.07.../git/ folder. But how do I use it in u-boot? - How do i make sure this script is run automatically every boot?",
        "answers": [
            [
                "U boot part of the default boot sequence tries to find a file named boot.src in the first partition from where it has booted. if this file is found then it will try to run this script. The commands put in the file can be based on RAUC syntax so that when RAUC gets activated in the user space it can update the same environment variables. So RAUC handles the boot sequence via the commands put int the script file. RAUC has no way to directly alter the flow of U Boot boot up sequence"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "If I try to bitbake an image bitbake name-of-image with local.conf containing this: \u2026 WKS_FILE=\"directdisk-multi-rootfs.wks\" IMAGE_FSTYPES = \"wic wic.bmap\" \u2026 Then the build exits with error: ERROR: Couldn't find correct bootimg_dir, exiting If I try to run the wic command in cooked mode, the same error occours. And if I attempt to run wic in raw mode: wic create directdisk-multi-rootfs -e name-of-image --rootfs-dir rootfs1=/home/user/yocto/dev-jetson-nano/build/tmp/work/jetson_nano-poky-linux/name-of-image/1.0-r0/rootfs/ --rootfs-dir rootfs2=/home/user/yocto/dev-jetson-nano/build/tmp/work/jetson_nano-poky-linux/name-of-image/1.0-r0/rootfs/ -b /home/user/yocto/dev-jetson-nano/build/tmp/work/jetson_nano-poky-linux/name-of-image/1.0-r0/recipe-sysroot/usr/share -k /home/user/yocto/dev-jetson-nano/build/tmp/deploy/images/jetson-nano -n /home/user/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/wic-tools/1.0-r0/recipe-sysroot-native I still get the same error. I need to create an image for the jetson-nano that can use RAUC update tool which needs two rootfs to work. Wic tool seems to be able to do that. How to upload it and if it will even work on the jetson nano is another question, but right now I just want to be able to make an image with wic. EDIT: As this is for a SD-card I made my own version of the \"directdisk-multi-rootfs.wks\" file with this: part /boot --source bootimg-partition --ondisk mmcblk0 --fstype=vfat --label boot --active --align 1024 --sourceparams=\"loader=u-boot\" part / --source rootfs --rootfs-dir=rootfs1 --ondisk mmcblk --fstype=ext4 --label platform --align 1024 part /rescue --source rootfs --rootfs-dir=rootfs2 --ondisk mmcblk --fstype=ext4 --label secondary --align 1024 bootloader --timeout=0 --append=\"rootwait rootfstype=ext4 video=vesafb vga=0x318 console=tty0 console=ttyS0,115200n8\" This gives me a new but very similar error: ERROR: No boot files defined, IMAGE_BOOT_FILES unset for entry #1",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm building a yocto image to run on the Jetson Nano. Right now I'm working on a Jetson Nano devkit which boots from the SD-card, and the flashing is described on the meta-tegra GitHub repo wiki. It doesn't say how to flash onto the eMMC on the Jetson Nano, only on the SDcard. Can I copy the yocto build rootfs to the nvidia_sdk L4T tools (replacing the 'rootfs' folder)? But what about the rest of the folders (bootloader, kernel, lib, nv_tegra)? It should be the same binaries, I'm just not so sure the kernel and bootloader is the same, and don't really know about the rest. Anyone dealing with the same issue or, even better, found a way to do this, please let me know.",
        "answers": [
            [
                "I had a conversation with the maintainer of the meta-tegra layer and ended up with creating a new machine configuration: #@TYPE: Machine #@NAME: Nvidia Jetson Nano #@DESCRIPTION: Nvidia Jetson Nano prod board KERNEL_ARGS ?= \"console=ttyS0,115200 console=tty0 fbcon=map:0 net.ifnames=0\" KERNEL_ROOTSPEC ?= \"root=/dev/mmcblk0p${@uboot_var('distro_bootpart')} rw rootwait\" IMAGE_ROOTFS_ALIGNMENT ?= \"1024\" require conf/machine/include/tegra210.inc KERNEL_DEVICETREE ?= \"_ddot_/_ddot_/_ddot_/_ddot_/nvidia/platform/t210/porg/kernel-dts/tegra210-p3448-0002-p3449-0000-b00.dtb\" MACHINE_FEATURES += \"ext2 ext3 vfat\" UBOOT_MACHINE = \"p3450-porg_defconfig\" EMMC_SIZE ?= \"17179869184\" EMMC_DEVSECT_SIZE ?= \"512\" BOOTPART_SIZE ?= \"\" BOOTPART_LIMIT ?= \"10485760\" ROOTFSPART_SIZE ?= \"3221225472\" ODMDATA ?= \"0x94000\" EMMC_BCT ?= \"P3448_A00_4GB_Micron_4GB_lpddr4_204Mhz_P987.cfg\" NVIDIA_BOARD ?= \"t210ref\" NVIDIA_PRODUCT ?= \"p3450-porg\" NVIDIA_BOARD_CFG ?= \"\" TEGRA210_REDUNDANT_BOOT ?= \"0\" PARTITION_LAYOUT_TEMPLATE ?= \"flash_l4t_t210_emmc_p3448.xml\" TEGRA_SPIFLASH_BOOT ?= \"0\" TEGRA_FAB ?= \"300\" TEGRA_BOARDID ?= \"3448\" The machine configuration is almost identical to the devkit's, but some parts had to be changed to match to Jetson Nano Production Module configurations, i.e. change the KERNEL_DEVICETREE the the one matching the newer eMMC Jetson Nano and change TEGRA_FAB accordingly. Then change the PARTITION_LAYOUT_TEMPLATE to match the emmc layout instead of the spi_sd layout (the flash_l4t_t210_emmc_p3448 is the default p3448 emmc layout provided with meta-tegra). After this, Yocto will produce a tegraflash zip that contains the necessary partition files and a rootfs image (along side some flashing tools). Put the Jetson Nano production module into recovery mode (FORCE RECOVERY + RESET), plug in the micro-usb cable and run the doflash.sh script to flash the nano, and voila."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I've created images for the Jetson nano using the meta-tegra layer and flashed it so my jetson nano with a 32gb sd-card. When building an image, the default size of the .sdcard file that is needed to be flashed to the SD-card, is about 16GB . It seems very overkill to have a very VERY basic image, with a root file system .sdcard file with the size of 16GB. After flashing, the SD-card is split into 13 parts, as seen from gparted: When booting the device, I have 1.83gb to use... on a 32gb sd-card with a extremely basic image... I can see there is 13.66gb free on the sdb13 part, but while able to unallocated the space from the sdb13 part, I am unable to allocate it to the sdb1 part due to error. This is however not the point of this post. Why is file size so big?- and is there any way to somehow minimize this size?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've created a minimal xfce image with Yocto/poky on a Jetson Nano using warrior branches (poky warrior, meta-tegra warrior-l4t-r32.2, openembedded warrior) and CUDA 10. Image boots and runs perfectly, and the camera test: $ gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=0 ! 'video/x-raw,width=960, height=616' ! nvvidconv ! nvegltransform ! nveglglessink -e works like a charm. Now I would like to use OpenCV on the camera feed, but I can't get it to work. I've added these packages to IMAGE_INSTALL: ... opencv \\ libopencv-core \\ libopencv-imgproc \\ opencv-samples \\ gstreamer1.0-omx-tegra \\ python3 \\ python3-modules \\ python3-dev \\ python-numpy \\ ... To get the OpenCV installed. When I run /usr/bin/opencv_version, it returns version 3.4.5, python version is 3.7.2 and GCC version is 7.2.1. When I try to run this OpenCV test code it returns [ WARN:0] VIDEOIO(createGStreamerCapture(filename)): trying ... (python3.7:5163): GStreamer-CRITICAL **: ..._: gst_element_get_state: assertion 'GST_IS_ELEMENT (element)' failed [ WARN:0] VIDEOIO(createGStreamerCapture(filename)): result=(nil) isOpened=-1 ... Unable to open camera I've tried looking around online for solutions but they don't seem to work. EDIT: There does appear to be a problem with using CAP_GSTREAMER in the VideoCapture function as running the same program with CAP_FFMPEG instead works just fine on an mp4 video. Using cv2.VideoCapture(\"/dev/video0\", CAP_FFMPEG) just returns with isOpen=-1. How do I get the camera to open in python?",
        "answers": [
            [
                "This is the pipeline that you said works for you: gst-launch-1.0 -v nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=0 ! 'video/x-raw,width=960, height=616' ! nvvidconv ! nvegltransform ! nveglglessink -e This is the pipeline that is mentioned in the script: gst-launch-1.0 -v nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=3280, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=0 ! 'video/x-raw, width=820, height=616, format=BGRx' ! videoconvert ! video/x-raw, format=BGR ! appsink The difference between working and nonworking pipelines is the addition of videoconvert and appsink The error GStreamer-CRITICAL **: ..._: gst_element_get_state: assertion 'GST_IS_ELEMENT (element)' failed indicates there is some GStreamer element missing from your system. You can try adding the missing plugins by adding the following package group to your image: gstreamer1.0-plugins-base Alternatively, you can replace the pipeline in face_detect.py with your working pipeline, but keep in mind that the script probably needs the video converted to BGR before feeding it to appsink for the algorithm to work. You might need to look up documentation for the nvidconv element to see if this is supported. EDIT: Judging by your comment, you may have been missing gstreamer1.0-python as well."
            ],
            [
                "Use the following gstreamer pipeline: stream = 'nvarguscamerasrc ! video/x-raw(memory:NVMM), width=%d, height=%d, format=(string)NV12, framerate=(fraction)%d/1 !nvvidconv flip-method=%d ! nvvidconv ! video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! videoconvert ! appsink' % (1280, 720, 30,0, 640, 480) cap = cv2.VideoCapture(stream,cv2.CAP_GSTREAMER) This will solve the problem"
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "I've created a very minimal image for the jetson nano with the recepe: inherit core-image inherit distro_features_check REQUIRED_DISTRO_FEATURES = \"x11\" IMAGE_FEATURES += \"package-management splash\" CORE_OS = \"packagegroup-core-boot \\ packagegroup-core-x11 \\ packagegroup-xfce-base \\ kernel-modules \\ \" WIFI_SUPPORT = \" \\ ifupdown \\ dropbear\\ crda \\ iw \\ \" DEV_SDK_INSTALL = \" \\ opencv \\ opencv-samples \\ gstreamer1.0-omx-tegra \\ python-numpy \\ binutils \\ binutils-symlinks \\ coreutils \\ cpp \\ cpp-symlinks \\ diffutils \\ elfutils elfutils-binutils \\ file \\ g++ \\ g++-symlinks \\ gcc \\ gcc-symlinks \\ gdb \\ gdbserver \\ gettext \\ git \\ ldd \\ libstdc++ \\ libstdc++-dev \\ libtool \\ ltrace \\ make \\ pkgconfig \\ python3-modules \\ strace \\ \" EXTRA_TOOLS_INSTALL = \" \\ bzip2 \\ ethtool \\ findutils \\ grep \\ i2c-tools \\ iproute2 \\ iptables \\ less \\ lsof \\ nano \\ nmap \\ tcpdump \\ unzip \\ util-linux \\ wget \\ zip \\ curl \\ \" IMAGE_INSTALL += \" \\ ${CORE_OS} \\ ${DEV_SDK_INSTALL} \\ ${EXTRA_TOOLS_INSTALL} \\ ${WIFI_SUPPORT} \\ \" To play around with a raspberry pi v2.1 camera. Everything works so far except ethernet access. When I run ifconfig I get an IPv6 ip-address and everything is looking good (except I would also want a ipv4 address if but haven't looked into that yet). But when I run the command ping google.com Is says \"ping: bad address 'google.com' and if I run ping on 8.8.8.8 it returns \"ping: sendto: network is uncreachable\". It's not the ethernet cable or my router that has a problem, as the same ethernet cable and access works just fine on my PC. When the ethernet is connected to the jetson nano, the green light is constantly on while the orange light is constantly blinking. What could be causing the problem and how do I fix it and get access to the internet again?",
        "answers": [
            [
                "I ran the command: ifup eth0 I got a ipv4 address and then everything worked."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am setting up a nvidia jetson nano to run docker containers. I am using an installation procedure that I already used on my x86_64 Ubuntu 16.04 machine, where docker runs smoothely. Nonetheless, docker seems to be set up correctly, but I can't run it. I flashed the jetson SD card as described in http://nvidia.com/jetsonnano-start, and set up language etc via keyboard and screen. After that I went headless and installed via ssh. The commands I used are shown in the listing below. # in case apt-get is interrupted, e.g. if board freezes, use: $ sudo dpkg --configure -a sudo apt-get update &amp;&amp; apt-get -y upgrade # install Docker, from https://docs.docker.com/install/linux/docker-ce/ubuntu/ sudo apt install -y \\ git \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg2 \\ software-properties-common # Get the Docker signing key for packages curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo \"$ID\")/gpg | sudo apt-key add - # Add the Docker official repos echo \"deb [arch=arm64] https://download.docker.com/linux/$(. /etc/os-release; echo \"$ID\") \\ $(lsb_release -cs) stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list # Install Docker sudo apt update sudo apt-get install -y docker-ce docker-ce-cli containerd.io # starting Docker and enabling it at boot sudo systemctl enable docker sudo systemctl start docker # ad user to docker group, so docker commands can be executed without sudo sudo gpasswd -a $USER docker newgrp docker # install docker-compose sudo apt-get install -y python-pip python-dev libffi6 libffi-dev libssl-dev libxml2-dev libxslt1-dev libjpeg8-dev zlib1g-dev sudo -H pip install docker-compose and after installation, I can see $ docker --version Docker version 19.03.2, build 6a30dfc however, when excuting something, I get exec format errors $ docker run jupyter/minimal-notebook standard_init_linux.go:211: exec user process caused \"exec format error\" Docker does pull the image. And in Dockerfiles, the WORKDIR command works, but RUN commands yield the same 'exec format error'. Any help or hint on how to the this to run without errors is greatly appreciated",
        "answers": [
            [
                "The issue is that the architecture of the jetson nano is aarch64 (64 bit ARM). There is no Anaconda build for this, but a project called Archiconda exists that steps into the gap. See: https://devtalk.nvidia.com/default/topic/1051415/jetson-nano/anaconda-for-jetson-nano/ for a jupyter notebook from Docker on a Jetson Nano, it helped me to follow this Dockerfile (as from the link above): https://github.com/helmuthva/jetson/blob/master/workflow/deploy/ml-base/src/Dockerfile"
            ],
            [
                "Docker is not a complete virtual machine. Programs build for x86_64 will not run on an incompatible processor. You need to build and use aarch64 versions of the Docker, that may require building from the source. Many important Python packages contain the C/C++ core that may be incompatible as well, but Python compiles them during installation."
            ]
        ],
        "votes": [
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "I am attempting to build an image for the jetson-nano using yocto poky-warrior and meta-tegra warrior-l4t-r32.2 layer. I've been following this thread because he had the same problem as me, and the answer on that thread fixed it, but then a new problem occoured.Building with bitbake core-image-minimal Stops with an error stating ERROR: Task (\u2026/jetson-nano/layers/poky-warrior/meta/recipes-core/libxcrypt/libxcrypt.bb:do_configure) failed with exit code '1' I've been told that applying the following patch would fix this problem: diff --git a/meta/recipes-core/busybox/busybox.inc b/meta/recipes- core/busybox/busybox.inc index 174ce5a8c0..e8d651a010 100644 --- a/meta/recipes-core/busybox/busybox.inc +++ b/meta/recipes-core/busybox/busybox.inc @@ -128,7 +128,7 @@ do_prepare_config () { ${S}/.config.oe-tmp &gt; ${S}/.config fi sed -i 's/CONFIG_IFUPDOWN_UDHCPC_CMD_OPTIONS=\"-R -n\"/CONFIG_IFUPDOWN_UDHCPC_CMD_OPTIONS=\"-R -b\"/' ${S}/.config - sed -i 's|${DEBUG_PREFIX_MAP}||g' ${S}/.config + #sed -i 's|${DEBUG_PREFIX_MAP}||g' ${S}/.config } # returns all the elements from the src uri that are .cfg files diff --git a/meta/recipes-core/libxcrypt/libxcrypt.bb b/meta/recipes-core/libxcrypt/libxcrypt.bb index 3b9af6d739..350f7807a7 100644 --- a/meta/recipes-core/libxcrypt/libxcrypt.bb +++ b/meta/recipes-core/libxcrypt/libxcrypt.bb @@ -24,7 +24,7 @@ FILES_${PN} = \"${libdir}/libcrypt*.so.* ${libdir}/libcrypt-*.so ${libdir}/libowc S = \"${WORKDIR}/git\" BUILD_CPPFLAGS = \"-I${STAGING_INCDIR_NATIVE} -std=gnu99\" -TARGET_CPPFLAGS = \"-I${STAGING_DIR_TARGET}${includedir} -Wno-error=missing-attributes\" -CPPFLAGS_append_class-nativesdk = \" -Wno-error=missing-attributes\" +TARGET_CPPFLAGS = \"-I${STAGING_DIR_TARGET}${includedir} \" +CPPFLAGS_append_class-nativesdk = \" \" BBCLASSEXTEND = \"nativesdk\" So I've made a libxcrypt.patch file and copy pasted the patch content and put the file in my poky meta layer. But how do I apply the patch? I can't figure out what to do from here, do I need to make an bbappend file or add to one?- if so which one? or do I need to edit a .bb file?- maybe libxcrypt.bb? And do I need to add these lines: FILESEXTRAPATHS_prepend := \"${THISDIR}/${PN}:\" SRC_URI += \"file://path/to/patch/file\" I've been trying to look at similar stackoverflow posts about this but they don't seem to be precise enough for me to work it out as I am completely new to yocto and the likes. So far I've tried to add the lines FILESEXTRAPATHS_prepend := \"${THISDIR}/${PN}:\" SRC_URI += \"file://path/to/patch/file\" to the libxcrypt.bb file but it says it cannot find the file to patch. Then I found out this could potentially be solved with adding ;striplevel=0 to the SRC_URI line, so I did this: SRC_URI += \"file://path/to/patch/file;striplevel=0\" Which did nothing. Then I tried to put --- a/meta/recipes-core/busybox/busybox.inc +++ b/meta/recipes-core/busybox/busybox.inc In the top of the patch file, but this also did nothing. This is the full error message without attempting to apply the patch: ERROR: libxcrypt-4.4.2-r0 do_configure: configure failed ERROR: libxcrypt-4.4.2-r0 do_configure: Function failed: do_configure (log file is located at /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/temp/log.do_configure.42560) ERROR: Logfile of failure stored in: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/temp/log.do_configure.42560 Log data follows: | DEBUG: SITE files ['endian-little', 'bit-64', 'arm-common', 'arm-64', 'common-linux', 'common-glibc', 'aarch64-linux', 'common'] | DEBUG: Executing shell function autotools_preconfigure | DEBUG: Shell function autotools_preconfigure finished | DEBUG: Executing python function autotools_aclocals | DEBUG: SITE files ['endian-little', 'bit-64', 'arm-common', 'arm-64', 'common-linux', 'common-glibc', 'aarch64-linux', 'common'] | DEBUG: Python function autotools_aclocals finished | DEBUG: Executing shell function do_configure | automake (GNU automake) 1.16.1 | Copyright (C) 2018 Free Software Foundation, Inc. | License GPLv2+: GNU GPL version 2 or later &lt;https://gnu.org/licenses/gpl-2.0.html&gt; | This is free software: you are free to change and redistribute it. | There is NO WARRANTY, to the extent permitted by law. | | Written by Tom Tromey &lt;tromey@redhat.com&gt; | and Alexandre Duret-Lutz &lt;adl@gnu.org&gt;. | AUTOV is 1.16 | NOTE: Executing ACLOCAL=\"aclocal --system-acdir=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot/usr/share/aclocal/ --automake-acdir=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal-1.16\" autoreconf -Wcross --verbose --install --force --exclude=autopoint -I /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/git/m4/ -I /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal/ | autoreconf: Entering directory `.' | autoreconf: configure.ac: not using Gettext | autoreconf: running: aclocal --system-acdir=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot/usr/share/aclocal/ --automake-acdir=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal-1.16 -I /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/git/m4/ -I /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal/ --force -I m4 | autoreconf: configure.ac: tracing | autoreconf: running: libtoolize --copy --force | libtoolize: putting auxiliary files in AC_CONFIG_AUX_DIR, 'm4'. | libtoolize: copying file 'm4/ltmain.sh' | libtoolize: putting macros in AC_CONFIG_MACRO_DIRS, 'm4'. | libtoolize: copying file 'm4/libtool.m4' | libtoolize: copying file 'm4/ltoptions.m4' | libtoolize: copying file 'm4/ltsugar.m4' | libtoolize: copying file 'm4/ltversion.m4' | libtoolize: copying file 'm4/lt~obsolete.m4' | autoreconf: running: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/bin/autoconf --include=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/git/m4/ --include=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal/ --force | autoreconf: running: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/bin/autoheader --include=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/git/m4/ --include=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/usr/share/aclocal/ --force | autoreconf: running: automake --add-missing --copy --force-missing | configure.ac:31: installing 'm4/compile' | configure.ac:30: installing 'm4/config.guess' | configure.ac:30: installing 'm4/config.sub' | configure.ac:17: installing 'm4/install-sh' | configure.ac:17: installing 'm4/missing' | Makefile.am: installing './INSTALL' | Makefile.am: installing 'm4/depcomp' | parallel-tests: installing 'm4/test-driver' | autoreconf: running: gnu-configize | autoreconf: Leaving directory `.' | NOTE: Running ../git/configure --build=x86_64-linux --host=aarch64-poky-linux --target=aarch64-poky-linux --prefix=/usr --exec_prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin --libexecdir=/usr/libexec --datadir=/usr/share --sysconfdir=/etc --sharedstatedir=/com --localstatedir=/var --libdir=/usr/lib --includedir=/usr/include --oldincludedir=/usr/include --infodir=/usr/share/info --mandir=/usr/share/man --disable-silent-rules --disable-dependency-tracking --with-libtool-sysroot=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot --disable-static | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/endian-little | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/arm-common | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/arm-64 | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/common-linux | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/common-glibc | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/site/common | configure: loading site script /home/mci/yocto/dev-jetson-nano/layers/meta-openembedded/meta-networking/site/endian-little | checking for a BSD-compatible install... /home/mci/yocto/dev-jetson-nano/build/tmp/hosttools/install -c | checking whether build environment is sane... yes | checking for aarch64-poky-linux-strip... aarch64-poky-linux-strip | checking for a thread-safe mkdir -p... /home/mci/yocto/dev-jetson-nano/build/tmp/hosttools/mkdir -p | checking for gawk... gawk | checking whether make sets $(MAKE)... yes | checking whether make supports nested variables... yes | checking build system type... x86_64-pc-linux-gnu | checking host system type... aarch64-poky-linux-gnu | checking for aarch64-poky-linux-gcc... aarch64-poky-linux-gcc -march=armv8-a+crc -fstack-protector-strong -D_FORTIFY_SOURCE=2 -Wformat -Wformat-security -Werror=format-security --sysroot=/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot | checking whether the C compiler works... no | configure: error: in `/home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/build': | configure: error: C compiler cannot create executables | See `config.log' for more details | NOTE: The following config.log files may provide further information. | NOTE: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/build/config.log | ERROR: configure failed | WARNING: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/temp/run.do_configure.42560:1 exit 1 from 'exit 1' | ERROR: Function failed: do_configure (log file is located at /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/temp/log.do_configure.42560) ERROR: Task (/home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/recipes-core/libxcrypt/libxcrypt.bb:do_configure) failed with exit code '1' NOTE: Tasks Summary: Attempted 883 tasks of which 848 didn't need to be rerun and 1 failed. This is the full error log when I try to add the lines to the libxcrypt.bb file to apply the patch: ERROR: libxcrypt-4.4.2-r0 do_patch: Command Error: 'quilt --quiltrc /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/recipe-sysroot-native/etc/quiltrc push' exited with 0 Output: Applying patch libxcrypt.patch can't find file to patch at input line 7 Perhaps you used the wrong -p or --strip option? The text leading up to this was: -------------------------- |--- a/meta/recipes-core/busybox/busybox.inc |+++ b/meta/recipes-core/busybox/busybox.inc |diff --git a/meta/recipes-core/busybox/busybox.inc b/meta/recipes-core/busybox/busybox.inc |index 174ce5a8c0..e8d651a010 100644 |--- a/meta/recipes-core/busybox/busybox.inc |+++ b/meta/recipes-core/busybox/busybox.inc -------------------------- No file to patch. Skipping patch. 1 out of 1 hunk ignored can't find file to patch at input line 20 Perhaps you used the wrong -p or --strip option? The text leading up to this was: -------------------------- |diff --git a/meta/recipes-core/libxcrypt/libxcrypt.bb b/meta/recipes-core/libxcrypt/libxcrypt.bb |index 3b9af6d739..350f7807a7 100644 |--- a/meta/recipes-core/libxcrypt/libxcrypt.bb |+++ b/meta/recipes-core/libxcrypt/libxcrypt.bb -------------------------- No file to patch. Skipping patch. 1 out of 1 hunk ignored Patch libxcrypt.patch does not apply (enforce with -f) ERROR: libxcrypt-4.4.2-r0 do_patch: ERROR: libxcrypt-4.4.2-r0 do_patch: Function failed: patch_do_patch ERROR: Logfile of failure stored in: /home/mci/yocto/dev-jetson-nano/build/tmp/work/aarch64-poky-linux/libxcrypt/4.4.2-r0/temp/log.do_patch.34179 ERROR: Task (/home/mci/yocto/dev-jetson-nano/layers/poky-warrior/meta/recipes-core/libxcrypt/libxcrypt.bb:do_patch) failed with exit code '1' NOTE: Tasks Summary: Attempted 811 tasks of which 793 didn't need to be rerun and 1 failed. I know this might be a trivial question for a lot, but as a new developer this is very hard to figure out on my own.",
        "answers": [
            [
                "The concept of patching by adding patch files to meta layers and referencing them in SRC_URI only applies to patching the source code of packages. You can't use it to patch meta data (recipes) itself as you are trying to. Instead you can manually change your local recipes, or add bbappends to your layer to change the existing recipes in poky. The best way to fix it permanently is to look for upstream fixes and update your poky layer if there are fixes, or if not send patches to upstream to fix it. For the bbappend solution for libxcrypt, you would for example create a libxcrypt.bbappend with something like this as content: TARGET_CPPFLAGS = \"-I${STAGING_DIR_TARGET}${includedir} \" CPPFLAGS_reomve_class-nativesdk = \"-Wno-error=missing-attributes\""
            ],
            [
                "The patch you have is for the yocto/poky sources themselves (as opposed to the more usual case of having patches for the actual components that yocto builds and bbappends that modify recipes in other layers somehow). So if you really want to use this patch, there's no need to \"integrate\" it into yocto, just run run git am &lt;patchfile&gt; in your poky root dir or use \"patch\" command directly. This is not very maintainable since your poky now differs from upstream but might work... It should be possible to do the same changes using bbappends that you could then store in your own layer (this way the poky repo would be untouched) but the patch you have does not do that. This would be the most \"proper\" way of paching that you asked about in the comment -- but if you know that you aren't going to ever upgrade poky then it might not be work worth doing."
            ]
        ],
        "votes": [
            4.0000001,
            4.0000001
        ]
    },
    {
        "question": "I'm attempting to create an image for my NVIDIA jetsons-nano (following this guide). When building the very basic image, the build terminates with an error saying it cannot find cuda-repo-l4t-10-0-local-10.0.166... and that is because the NVIDIA SDK downloads cuda-repo-l4t-10-0-local-10.0.326... I can see that the meta-tegra thud branch does in fact contain recipes needing the 10.0.166 CUDA version. Meanwhile the master branch contains recipes needing the updated 10.0.326 CUDA that the NVIDIA SDK provides. So my question is this: can I just copy the cuda recipes folder from master branch (meta-tegra/recipes-devtools/cuda) and replace the cuda recipe folder in the used meta-tegra layer in my build? Or can I download the CUDA 10.0.166 from the SDK instead somehow? [SOLVED]As a side question, the build complains that is cannot find \"cuda-repo-ubuntu1804-10-0-local-10.0.326-410.108_10.0-1_amd64.deb\"... which is because I downloaded from the NVIDIA SDK on a ubuntu 16.04 system and not 18.04.. What can I do about this? I can see that there is recipe for both 18.04 and 16.04, but it runs through both? As another side question, the meta-tegra layer of the thud branch does not have the MACHINE conf for jetson-nano. But I assume these configs are somewhat independent, so I took the jetson-nano config file from the master branch aswell. This is fine right?",
        "answers": [
            [
                "For the cuda SDK, you need to do something like this in local.conf: CUDA_BINARIES_NATIVE = \"cuda-binaries-ubuntu1604-native\" edit: I also want to add that you may have less difficulty gett Warrior to work with the latest SDK rather than backing off to Thud."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm attempting to create an image with bitbake core-image-minimal For my jetson nano (nvidia tegra). I've added the meta-layer for tegra devices from https://github.com/madisongh/meta-tegra and added it to bblayer.conf. I have also added lines IMAGE_CLASSES += \"image_types_tegra\" IMAGE_FSTYPES = \"tegraflash\" to the local.conf file to be able to flash it later. When I attempt to run the bitbake command to create the image, I get the error message: ERROR: No recipes available for: /home/mci/yocto/jetson-nano/meta-tegra/recipes-graphics/vulkan/vulkan-loader_1.1.%.bbappend /home/mci/yocto/jetson-nano/meta-tegra/recipes-graphics/vulkan/vulkan-tools_1.1.%.bbappend /home/mci/yocto/jetson-nano/meta-tegra/recipes-graphics/wayland/weston_7.0.0.bbappend But aren't the files it says there is no recipes for the same recipes it's looking for? Isn't \"vulkan-loader_1.1.%.bbappend\" a recipe? How do I solve this problem? Is it because it can't find the files or is the bbappend not the recipes but something else?",
        "answers": [
            [
                "Michael, I don't have an answer for the vulkan pieces but I do have a few pointers since we seem to be going down a similar path with the nano. Use the warrior branch of yocto You'll need to download the binary pieces of the nvidia sdk through the SDK manager Point to these sdk packages in your local.conf with the NVIDIA_DEVNET_MIRROR variable. ex: \"file:///home/nvidia/yocto/git/poky/devnet/nano-dev\" Because of the binary pieces in step 2, you need to use an older gcc version which isn't really supported in warrior. I used the linaro-7.2 layer. Since gcc7 is not supported in warrior, yocto / openembedded will attempt to pass flags to gcc which will make the build fail. Here's a summary, which I hope is complete, to help you through this. Add DEBUG_PREFIX_MAP=\"\" to local.conf and apply the following patch. diff --git a/meta/recipes-core/busybox/busybox.inc b/meta/recipes-core/busybox/busybox.inc index 174ce5a8c0..e8d651a010 100644 --- a/meta/recipes-core/busybox/busybox.inc +++ b/meta/recipes-core/busybox/busybox.inc @@ -128,7 +128,7 @@ do_prepare_config () { ${S}/.config.oe-tmp &gt; ${S}/.config fi sed -i 's/CONFIG_IFUPDOWN_UDHCPC_CMD_OPTIONS=\"-R -n\"/CONFIG_IFUPDOWN_UDHCPC_CMD_OPTIONS=\"-R -b\"/' ${S}/.config - sed -i 's|${DEBUG_PREFIX_MAP}||g' ${S}/.config + #sed -i 's|${DEBUG_PREFIX_MAP}||g' ${S}/.config } # returns all the elements from the src uri that are .cfg files diff --git a/meta/recipes-core/libxcrypt/libxcrypt.bb b/meta/recipes-core/libxcrypt/libxcrypt.bb index 3b9af6d739..350f7807a7 100644 --- a/meta/recipes-core/libxcrypt/libxcrypt.bb +++ b/meta/recipes-core/libxcrypt/libxcrypt.bb @@ -24,7 +24,7 @@ FILES_${PN} = \"${libdir}/libcrypt*.so.* ${libdir}/libcrypt-*.so ${libdir}/libowc S = \"${WORKDIR}/git\" BUILD_CPPFLAGS = \"-I${STAGING_INCDIR_NATIVE} -std=gnu99\" -TARGET_CPPFLAGS = \"-I${STAGING_DIR_TARGET}${includedir} -Wno-error=missing-attributes\" -CPPFLAGS_append_class-nativesdk = \" -Wno-error=missing-attributes\" +TARGET_CPPFLAGS = \"-I${STAGING_DIR_TARGET}${includedir} \" +CPPFLAGS_append_class-nativesdk = \" \" BBCLASSEXTEND = \"nativesdk\" Best of luck! I apologize if this is a bit rough, but I'm just getting through this myself."
            ],
            [
                "I deleted everything and started with a fresh build, did the EXACT same procedure and added all the same lines to the local.conf and bblayer.conf... But this time, bitbake command is running with no errors at all."
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am creating an app on the nvidia jetson nano (under ubuntu 18.04 LTS) using Qt-creator. To do so I create my own library (named recolib) which uses opencv (version 3.4.6) and then in my Qt app project I include my library. The library compiles fine, but when I tried compiling the app it reaches 100% and then I get the following error: undefined reference to `cv::xfeatures2d::SURF::create(double, int, int, bool, bool)'. Note that the class where this function is called is part of my library \"recolib\". I have build opencv with cmake, I have enabled non-free modules and gave the right path to it and opencv built and installed fine. Moreover, the app works well on mac and on an ubuntu virtual machine. Here are samples of the Cmakelist.txt of the app: cmake_minimum_required(VERSION 3.0) # Add folder where are supportive functions set(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake) set(CMAKE_INCLUDE_CURRENT_DIR ON) set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -std=c++11 -pthread -lm\") SET(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} -std=c++11 -pthread -lm\") ... find_package(OpenCV REQUIRED) if(OPENCV_XFEATURES2D_FOUND) message(\"xfeatures2d found\") endif() ... SET(RECOLIB_DIR \"${PROJECT_SOURCE_DIR}/libs/recotracking-recolib\") include_directories(\"${RECOLIB_DIR}/include\") if(WIN32) link_directories(\"${RECOLIB_DIR}/build/Debug\") else() link_directories(\"${RECOLIB_DIR}/build/\") endif() SET(RECOLIB_LIBRARY recolib) include_directories(${OPENCV_INCLUDE_DIRS}) ... include_directories(${CMAKE_CURRENT_SOURCE_DIR}/include) FILE(GLOB_RECURSE RES_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/*.qrc\") FILE(GLOB_RECURSE SRC_FILES \"${SRC_FOLDER}/*.cpp\") FILE(GLOB_RECURSE UI_FILES \"${SRC_FOLDER}/*.ui\") FILE(GLOB_RECURSE HDR_FILES \"${HDR_FOLDER}/*.h\") link_directories(${OpenCV_LIBRARY_DIRS}) message(\"opencv libs: ${OpenCV_LIBRARIES}\") add_executable(${execName} ${OS_BUNDLE}# Expands to WIN32 or MACOS_BUNDLE depending on OS ${RES_FILES} ${SRC_FILES} ${UI_FILES} ${HDR_FILES} ${META_FILES_TO_INCLUDE} ) ... # Add the Qt5 Widgets for linking if (APPLE) target_link_libraries(${execName} ${OpenCV_LIBRARIES} ${AppKit} ${CoreAudio} ${AudioToolbox} Qt5::Core Qt5::Gui Qt5::QuickControls2 Qt5::MultimediaWidgets Qt5::Multimedia Qt5::MultimediaPrivate Qt5::Widgets Qt5::Sql ${RECOLIB_LIBRARY}) else() target_link_libraries(${execName} ${OpenCV_LIBRARIES} Qt5::Core Qt5::Gui Qt5::QuickControls2 Qt5::MultimediaWidgets Qt5::Multimedia #Qt5::MultimediaPrivate Qt5::Widgets Qt5::Sql ${RECOLIB_LIBRARY}) endif() And the output I get executing it under Qt-creator is the following: Running \"/usr/bin/cmake -E server --pipe=/tmp/cmake-Bg9h0e/socket --experimental\" in /home/bookbeo-novatech/build-recotracking-app-Desktop-Release. Starting to parse CMake project, using: \"-DCMAKE_BUILD_TYPE:STRING=Release\", \"-DCMAKE_CXX_COMPILER:STRING=/usr/bin/g++\", \"-DCMAKE_C_COMPILER:STRING=/usr/bin/gcc\", \"-DCMAKE_PREFIX_PATH:STRING=/usr\", \"-DQT_QMAKE_EXECUTABLE:STRING=/usr/lib/qt5/bin/qmake\". -std=c++11 -pthread -lm The C compiler identification is GNU 7.4.0 The CXX compiler identification is GNU 7.4.0 Check for working C compiler: /usr/bin/gcc Check for working C compiler: /usr/bin/gcc -- works Detecting C compiler ABI info Detecting C compiler ABI info - done Detecting C compile features Detecting C compile features - done Check for working CXX compiler: /usr/bin/g++ Check for working CXX compiler: /usr/bin/g++ -- works Detecting CXX compiler ABI info Detecting CXX compiler ABI info - done Detecting CXX compile features Detecting CXX compile features - done Looking for pthread.h Looking for pthread.h - found Looking for pthread_create Looking for pthread_create - not found Looking for pthread_create in pthreads Looking for pthread_create in pthreads - not found Looking for pthread_create in pthread Looking for pthread_create in pthread - found Found Threads: TRUE Found CUDA: /usr/local/cuda (found suitable exact version \"10.0\") Found OpenCV: /usr/local (found version \"3.4.6\") xfeatures2d found opencv libs: opencv_calib3d;opencv_core;opencv_cudaarithm;opencv_cudabgsegm;opencv_cudacodec;opencv_cudafeatures2d;opencv_cudafilters;opencv_cudaimgproc;opencv_cudalegacy;opencv_cudaobjdetect;opencv_cudaoptflow;opencv_cudastereo;opencv_cudawarping;opencv_cudev;opencv_dnn;opencv_features2d;opencv_flann;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_ml;opencv_objdetect;opencv_photo;opencv_shape;opencv_stitching;opencv_superres;opencv_video;opencv_videoio;opencv_videostab;opencv_aruco;opencv_bgsegm;opencv_bioinspired;opencv_ccalib;opencv_cvv;opencv_datasets;opencv_dnn_objdetect;opencv_dpm;opencv_face;opencv_fuzzy;opencv_hfs;opencv_img_hash;opencv_line_descriptor;opencv_optflow;opencv_phase_unwrapping;opencv_plot;opencv_reg;opencv_rgbd;opencv_saliency;opencv_stereo;opencv_structured_light;opencv_surface_matching;opencv_text;opencv_tracking;opencv_xfeatures2d;opencv_ximgproc;opencv_xobjdetect;opencv_xphoto Configuring done Generating done CMake Warning: Manually-specified variables were not used by the project: QT_QMAKE_EXECUTABLE CMake Project was parsed successfully. So as you can see, except for some issues with pthread (but it seems unrelated to the main problem), the cmake configuration appears to be good. Opencv is found and xfeatures2d too. But when I build the app, at the end of the compilation I get: [ 94%] Building CXX object CMakeFiles/recotracking-app.dir/recotracking-app_autogen/mocs_compilation.cpp.o [ 97%] Building CXX object CMakeFiles/recotracking-app.dir/recotracking-app_autogen/EWIEGA46WW/qrc_resources.cpp.o [100%] Linking CXX executable recotracking-app /home/bookbeo/recotracking-app/libs/recotracking-recolib/build/librecolib.a(PCB.cpp.o): In function `reco::PCB::load(cv::Mat const&amp;, cv::Mat const&amp;, cv::Mat const&amp;, std::vector&lt;reco::RecoPoi, std::allocator&lt;reco::RecoPoi&gt; &gt;)': CMakeFiles/recotracking-app.dir/build.make:1045: recipe for target 'recotracking-app' failed CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/recotracking-app.dir/all' failed Makefile:83: recipe for target 'all' failed /home/bookbeo-novatech/recotracking-app/libs/recotracking-recolib/src/PCB.cpp:47: undefined reference to `cv::xfeatures2d::SURF::create(double, int, int, bool, bool)' collect2: error: ld returned 1 exit status make[2]: *** [recotracking-app] Error 1 make[1]: *** [CMakeFiles/recotracking-app.dir/all] Error 2 make: *** [all] Error 2 10:51:49: Le processus \"/usr/bin/cmake\" s'est termin\u00e9 avec le code 2. Erreur lors de la compilation/d\u00e9ploiement du projet recotracking-app (kit : Desktop) When executing step \"CMake Build\" 10:51:49: Temps \u00e9coul\u00e9 : 06:59. I already tried re-building opencv, it didn't change anything, and the module xfeatures2d seems to be found so I don't know where this error is coming from. If you have any idea, I would really appreciate it. Thanks in advance.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using OpenCV v.4.1.0 on Jetson Nano to capture video with RPi camera and the module cv2.VideoWriter doesn't work well saving video. The file is empty. I've proved cv2.VideoWriter on RPi 3B+ and it worked fine. import numpy as np import cv2 capture_height = 720 capture_width = 1280 frame_rate = 21 display_width = 860 display_height = 640 flip_method = 0 gstr = ('nvarguscamerasrc ! video/x-raw(memory:NVMM),' 'width=%s, height=%s,' 'framerate= %s' 'format=NV12 ! nvvidconv flip-method= %s ! video/x-raw,' 'width=%s, height=%s,' 'format=BGRx ! videoconvert ! appsink' % (capture_width, capture_height, frame_rate, flip_method, display_width, display_height)) filename = 'video.avi' fourcc = cv2.VideoWriter_fourcc(*'XVID') cap = cv2.VideoCapture(gstr, cv2.CAP_GSTREAMER) out = cv2.VideoWriter(filename, fourcc, float(frame_rate), (capture_width,capture_height),True) while True: ret, img = cap.read() out.write(img) cv2.imshow('img',img) if cv2.waitKey(1) &amp; 0xff == ord('q'): break cap.release() out.release() cv2.destroyAllWindows() This code should show an image from RPi camera, and save a video with .avi format.",
        "answers": [
            [
                "A solution to this is typing: out = cv2.VideoWriter(filename, fourcc, float(frame_rate), (display_width, display_height), True)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm working on a Jetson Nano board and I need to connect it with a parrot ar drone 2.0, so in order to do that I would like to use the ardrone_autonomy package, but I can't do that because jetson has Ubuntu 18, which only supports ROS Melodic, which doesn't have this package. Here is what I tried: I built the package from this repository: https://github.com/dsapandora/ardrone_autonomy But I obtained this error: /tmp/ccIoQjBo.s: Assembler messages: /tmp/ccIoQjBo.s:128: Error: unknown mnemonic `bswap' -- `bswap x3' generic.makefile:231: recipe for target '../../Soft/Build/targets_versions/vlib_PROD_MODE_Linux_4.9.140-tegra_GNU_Linux_usrbingcc_5.4.0/video_mem32.o' failed make[8]: *** [../../Soft/Build/targets_versions/vlib_PROD_MODE_Linux_4.9.140-tegra_GNU_Linux_usrbingcc_5.4.0/video_mem32.o] Error 1 vlib.makefile:104: recipe for target 'all' failed make[7]: *** [all] Error 2 Makefile:167: recipe for target 'build_vlib' failed make[6]: *** [build_vlib] Error 2 Makefile:170: recipe for target 'all' failed make[5]: *** [all] Error 2 Makefile:84: recipe for target 'build_libs' failed make[4]: *** [build_libs] Error 2 Makefile:20: recipe for target 'all' failed make[3]: *** [all] Error 2 ardrone_autonomy/CMakeFiles/ardronelib.dir/build.make:110: recipe for target '/catkin_ws/devel/src/ardronelib-stamp/ardronelib-build' failed make[2]: *** [/catkin_ws/devel/src/ardronelib-stamp/ardronelib-build] Error 2 CMakeFiles/Makefile2:578: recipe for target 'ardrone_autonomy/CMakeFiles/ardronelib.dir/all' failed make[1]: *** [ardrone_autonomy/CMakeFiles/ardronelib.dir/all] Error 2 Makefile:138: recipe for target 'all' failed make: *** [all] Error 2 Invoking \"make -j4 -l4\" failed After I resolved that using almost any solution from here: https://github.com/AutonomyLab/ardrone_autonomy/issues/71 I can compile the package without problems but when I try to run the node, it says that the ardrone_driver has died, without any other information. 2.Then I tried to build the package in a docker under ROS Kinetic, but after getting the same bswap error as above, after applying the above solution I get this: /tmp/ccFs5LFZ.s: Assembler messages: /tmp/ccFs5LFZ.s:130: Error: unexpected characters following instruction at operand 2 -- `mov x4,x4,ror#8' generic.makefile:231: recipe for target '../../Soft/Build/targets_versions/vlib_PROD_MODE_Linux_4.9.140- tegra_GNU_Linux_usrbingcc_5.4.0/video_mem32.o' failed make[8]: *** [../../Soft/Build/targets_versions /vlib_PROD_MODE_Linux_4.9.140-tegra_GNU_Linux_usrbingcc_5.4.0 /video_mem32.o] Error 1 vlib.makefile:110: recipe for target 'all' failed make[7]: *** [all] Error 2 Makefile:167: recipe for target 'build_vlib' failed make[6]: *** [build_vlib] Error 2 Makefile:170: recipe for target 'all' failed make[5]: *** [all] Error 2 Makefile:84: recipe for target 'build_libs' failed make[4]: *** [build_libs] Error 2 Makefile:24: recipe for target 'all' failed make[3]: *** [all] Error 2 ardrone_autonomy/CMakeFiles/ardronelib.dir/build.make:110: recipe for target '/ardrone_ws/devel/src/ardronelib-stamp/ardronelib-build' failed make[2]: *** [/ardrone_ws/devel/src/ardronelib-stamp/ardronelib-build] Error 2 CMakeFiles/Makefile2:578: recipe for target 'ardrone_autonomy/CMakeFiles/ardronelib.dir/all' failed make[1]: *** [ardrone_autonomy/CMakeFiles/ardronelib.dir/all] Error 2 make[1]: *** Waiting for unfinished jobs.... [ 2%] Built target _ardrone_autonomy_generate_messages_check_deps_matrix33 Makefile:138: recipe for target 'all' failed make: *** [all] Error 2 Invoking \"make -j2\" failed And here is where I'm stuck, because as much as I know it is a problem of processor architecture (Jetson uses armv8), and all the above solutions are for armv7, but I don't know how to solve this. Please let me know if anyone else found a solution to this problem. Any help would be highly appreciated!",
        "answers": [
            [
                "I finally solved the problem, here are the complete steps, if there is someone else who is struggling with the same problems: I downloaded the docker image using: sudo docker pull ros:kinetic-robot-xenial And then followed this steps to run the image: http://wiki.ros.org/docker/Tutorials/Docker The I cloned the ardrone_autonomy package in a worksapce, from the above link: mkdir -p catkin_ws/src cd catkin_ws/src git clone https://github.com/dsapandora/ardrone_autonomy.git apt-get update cd ../ rosdep install --from-paths src --ignore-src -r -y catkin_make After the error with bswap appears, edit this file: devel/src/ardronelib/ARDroneLib/VP_SDK/VP_Os/linux/intrin.h and replace: static INLINE uint32_t _byteswap_ulong(uint32_t value) { __asm(\"bswap %0\": \"=r\" (value): \"0\" (value)); return value; } with this: static INLINE uint32_t _byteswap_ulong(uint32_t value) { int32_t tmp; __asm __volatile( \"eor %1, %2, %2, ror #16\\n\" \"bic %1, %1, #0x00ff0000\\n\" \"mov %0, %2, ror #8\\n\" \"eor %0, %0, %1, lsr #8\" : \"=r\" (value), \"=r\" (tmp) : \"r\" (value) ); return value; } and comment this line: _BitScanReverse(&amp;index, code); After you do this if you try to give it a catkin_make it should appear the last error which can be resolved \"simply\" (after you understand what is happening there :) ) just by replacing this line (from the same file as above): \"mov %0, %2, ror #8\\n\" with: \"mov %0, %2\\n\" and now should work just fine. PS: Don't forget to source your workspace."
            ],
            [
                "AR Drone 2.0 is still compatible. Just Downgrade the firmware to 2.4.1 (i use this one) from https://www.espaciodrone.com/todos-los-firmware-del-ar-drone-1-y-2/ Power up your AR.Drone Connect your computer to the AR.Drone\u2019s network Open a telnet session to 192.168.1.1 type the following: echo \"1.1.1\" &gt; /firmware/version.txt press enter type the following: echo \"2.4.1\" &gt; /update/version.txt Connect an FTP client to: 192.168.1.1 Port: 5551 (i use nautilius ctrl + l then ftp://192.168.1.1:5551) Upload the downloaded plf file to the FTP server (AR.Drone) Disconnect your computer from the AR.Drone Disconnect the battery from the AR.Drone Reconnect the battery Wait about 5 minutes while the upgrade completes, leave your drone switched on And use this git clone https://github.com/dsapandora/ardrone... It works like a charm..."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am trying to follow this tutorial to read analog input. But instead of using Raspberry Pi, I am trying to use a Jetson Nano to read MCP3008 channel 0 analog input only. I am following the pin layout in the tutorial on Jetson Nano since it shares same layout with Raspberry Pi. import busio import digitalio import board import adafruit_mcp3xxx.mcp3008 as MCP from adafruit_mcp3xxx.analog_in import AnalogIn # create the spi bus spi = busio.SPI(clock=board.SCK, MISO=board.MISO, MOSI=board.MOSI) # create the cs (chip select) cs = digitalio.DigitalInOut(board.D22) # create the mcp object mcp = MCP.MCP3008(spi, cs) # create an analog input channel on pin 0 chan0 = AnalogIn(mcp, MCP.P0) print('Raw ADC Value: ', chan0.value) print('ADC Voltage: ' + str(chan0.voltage) + 'V') The code above throws the error: import board File \"/usr/local/lib/python3.6/dist-packages/board.py\", line 98, in &lt;module&gt; raise NotImplementedError(\"Board not supported\") NotImplementedError: Board not supported I checked the source code, it points to this line. However, I looked up of the board library source, it does support Jetson Nano because of the line board_id == ap_board.JETSON_NANO Any guide can help me further locate the issue will be very helpful. Thanks!",
        "answers": [],
        "votes": []
    }
]