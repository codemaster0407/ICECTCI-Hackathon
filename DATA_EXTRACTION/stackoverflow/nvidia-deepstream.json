[
    {
        "question": "setup: \u2022 Hardware Platform:Jetson Xavier NX \u2022 DeepStream Version: 6.1 \u2022 JetPack Version: 5.0.1 DP \u2022 TensorRT Version: 8.4.0.11 \u2022 Issue Type( questions) Hi folks, I work with deepstream and python bindings I have pipeline that captare UDP H264 stream pass to Yolo object detection and tracking, everything work great. Now I need to handle with something new so before I integrate my network I try build simple pipeline in command line. someone Send to me RTP stream is pipeline (Sender): gst-launch-1.0 v4l2src device=\"/dev/video\u201d ! jpegdec ! omxh264enc ! mpegtsmux ! rtpmp2tpay ! udpsink host=234.0.0.0 port=46002 I build a simple receive pipeline: gst-launch-1.0 udpsrc port=46002 caps=\"application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)MP2T, payload=(int)33\" ! rtpjitterbuffer ! rtpmp2tdepay ! tsdemux ! h264parse ! nvv4l2decoder ! nvvideoconvert ! autovideosink Unfortntaly I don\u2019t see video on screen I thing something worng with the element rtpmp2tdepay or maybe I missing out something I will be happy to any help. I add some graphsView of the pipeline and wireshark: I thing I miss something I would happy to any help.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a Jetson Nano 4GB (B01) SOC board and I am running out of space to install the DeepStream SDK. I want to use DeepStream for video streaming application on my Jetson Nano SOC board, but I need guidance on how to install it . What are the steps to install DeepStream SDK on the Jetson Nano?",
        "answers": [],
        "votes": []
    },
    {
        "question": "setup: \u2022 Hardware Platform:Jetson Xavier NX \u2022 DeepStream Version: 6.1 \u2022 JetPack Version: 5.0.1 DP \u2022 TensorRT Version: 8.4.0.11 \u2022 Issue Type( questions) Hi folks, I run some Python script on my Jetson Xavier that captures the h264 stream and implements YOLO + tracker the display the stream on the screen (this pipeline work fine). Now I add to the pipeline more elements my goal is to encode the new video with the annotation of Yolo and stream rtp UDP multicast stream. I don\u2019t get any error but unfortunately, I don\u2019t see RTP packets in the Wireshark I don\u2019t understand what is the problem, In addition, I would be happy to understand what I need to add to my pipeline after I have the stream with the inference I want to resize to 704X576 and then send the video and not stream video size 640X640. I will be happy to help. Here is my code: #!/usr/bin/env python3 ################################################################################ # SPDX-FileCopyrightText: Copyright (c) 2019-2021 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. # SPDX-License-Identifier: Apache-2.0 # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################ import sys sys.path.append('../') import configparser import gi gi.require_version('Gst', '1.0') from gi.repository import GLib, Gst from common.bus_call import bus_call import pyds fps_streams = {} PGIE_CLASS_ID_PERSON = 4 PGIE_CLASS_ID_GAZEBO = 3 PGIE_CLASS_ID_TRUCK = 2 PGIE_CLASS_ID_PICKUP = 1 PGIE_CLASS_ID_CAR = 0 past_tracking_meta = [0] # UDP Properties PORT = 46002 MULTI_GROUP = \"234.0.0.0\" PORT_SINK = 46003 MULTI_GROUP_SINK = \"244.0.0.0\" def osd_sink_pad_buffer_probe(pad, info, u_data): frame_number = 0 # Intiallizing object counter with 0. obj_counter = { PGIE_CLASS_ID_PERSON:0, PGIE_CLASS_ID_GAZEBO:0, PGIE_CLASS_ID_TRUCK:0, PGIE_CLASS_ID_PICKUP:0, PGIE_CLASS_ID_CAR:0 } num_rects = 0 gst_buffer = info.get_buffer() if not gst_buffer: print(\"Unable to get GstBuffer \") return # Retrieve batch metadata from the gst_buffer # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the # C address of gst_buffer as input, which is obtained with hash(gst_buffer) batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer)) l_frame = batch_meta.frame_meta_list while l_frame is not None: try: # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta # The casting is done by pyds.NvDsFrameMeta.cast() # The casting also keeps ownership of the underlying memory # in the C code, so the Python garbage collector will leave # it alone. frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data) except StopIteration: break frame_number = frame_meta.frame_num num_rects = frame_meta.num_obj_meta l_obj = frame_meta.obj_meta_list while l_obj is not None: try: # Casting l_obj.data to pyds.NvDsObjectMeta obj_meta = pyds.NvDsObjectMeta.cast(l_obj.data) except StopIteration: break obj_counter[obj_meta.class_id] += 1 try: l_obj = l_obj.next except StopIteration: break try: l_frame = l_frame.next except StopIteration: break # past traking meta data if (past_tracking_meta[0] == 1): l_user = batch_meta.batch_user_meta_list while l_user is not None: try: # Note that l_user.data needs a cast to pyds.NvDsUserMeta # The casting is done by pyds.NvDsUserMeta.cast() # The casting also keeps ownership of the underlying memory # in the C code, so the Python garbage collector will leave # it alone user_meta = pyds.NvDsUserMeta.cast(l_user.data) except StopIteration: break if (user_meta and user_meta.base_meta.meta_type == pyds.NvDsMetaType.NVDS_TRACKER_PAST_FRAME_META): try: # Note that user_meta.user_meta_data needs a cast to pyds.NvDsPastFrameObjBatch # The casting is done by pyds.NvDsPastFrameObjBatch.cast() # The casting also keeps ownership of the underlying memory # in the C code, so the Python garbage collector will leave # it alone pPastFrameObjBatch = pyds.NvDsPastFrameObjBatch.cast(user_meta.user_meta_data) except StopIteration: break for trackobj in pyds.NvDsPastFrameObjBatch.list(pPastFrameObjBatch): print(\"streamId=\", trackobj.streamID) print(\"surfaceStreamID=\", trackobj.surfaceStreamID) for pastframeobj in pyds.NvDsPastFrameObjStream.list(trackobj): print(\"numobj=\", pastframeobj.numObj) print(\"uniqueId=\", pastframeobj.uniqueId) print(\"classId=\", pastframeobj.classId) print(\"objLabel=\", pastframeobj.objLabel) for objlist in pyds.NvDsPastFrameObjList.list(pastframeobj): print('frameNum:', objlist.frameNum) print('tBbox.left:', objlist.tBbox.left) print('tBbox.width:', objlist.tBbox.width) print('tBbox.top:', objlist.tBbox.top) print('tBbox.right:', objlist.tBbox.height) print('confidence:', objlist.confidence) print('age:', objlist.age) try: l_user = l_user.next except StopIteration: break return Gst.PadProbeReturn.OK def main(args=1): Gst.init(None) # Create gstreamer elements # Create Pipeline element that will form a connection of other elements print(\"Creating Pipeline \\n \") pipeline = Gst.Pipeline() if not pipeline: sys.stderr.write(\" Unable to create Pipeline \\n\") # Source element for reading from the file print(\"Creating Source \\n \") # source = Gst.ElementFactory.make(\"filesrc\", \"file-source\") source = Gst.ElementFactory.make(\"udpsrc\", \"UDP-source\") source.set_property(\"port\", PORT) source.set_property(\"multicast-group\", MULTI_GROUP) if not source: sys.stderr.write(\" Unable to create Source \\n\") # Since the data format in the input file is elementary h264 stream, # we need a h264parser print(\"Creating H264Parser \\n\") h264parser = Gst.ElementFactory.make(\"h264parse\", \"h264-parser\") # h264parser = Gst.ElementFactory.make(\"mpeg4videoparse\", \"mpeg4-parser\") if not h264parser: sys.stderr.write(\" Unable to create h264 parser \\n\") # Use nvdec_h264 for hardware accelerated decode on GPU print(\"Creating Decoder \\n\") decoder = Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\") decoder.set_property('enable-max-performance', 1) if not decoder: sys.stderr.write(\" Unable to create Nvv4l2 Decoder \\n\") # Create nvstreammux instance to form batches from one or more sources. streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\") if not streammux: sys.stderr.write(\" Unable to create NvStreamMux \\n\") streammux.set_property('width', 640) streammux.set_property('height', 640) streammux.set_property('batch-size', 1) streammux.set_property('batched-push-timeout', 40000) streammux.set_property('live-source', 1) streammux.set_property('compute-hw', 1) # Use nvinfer to run inferencing on decoder's output, # behaviour of inferencing is set through config file pgie = Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\") if not pgie: sys.stderr.write(\" Unable to create pgie \\n\") # Set properties of pgie pgie.set_property('config-file-path', \"config_infer_primary_yoloV5.txt\") tracker = Gst.ElementFactory.make(\"nvtracker\", \"tracker\") if not tracker: sys.stderr.write(\" Unable to create tracker \\n\") # Set properties of tracker config = configparser.ConfigParser() config.read('tracker_config.txt') config.sections() for key in config['tracker']: if key == 'tracker-width': tracker_width = config.getint('tracker', key) tracker.set_property('tracker-width', tracker_width) if key == 'tracker-height': tracker_height = config.getint('tracker', key) tracker.set_property('tracker-height', tracker_height) if key == 'gpu-id': tracker_gpu_id = config.getint('tracker', key) tracker.set_property('gpu_id', tracker_gpu_id) if key == 'll-lib-file': tracker_ll_lib_file = config.get('tracker', key) tracker.set_property('ll-lib-file', tracker_ll_lib_file) if key == 'll-config-file': tracker_ll_config_file = config.get('tracker', key) tracker.set_property('ll-config-file', tracker_ll_config_file) if key == 'enable-batch-process': tracker_enable_batch_process = config.getint('tracker', key) tracker.set_property('enable_batch_process', tracker_enable_batch_process) if key == 'enable-past-frame': tracker_enable_past_frame = config.getint('tracker', key) tracker.set_property('enable_past_frame', tracker_enable_past_frame) if key == 'compute-hw': tracker_enable_compute_hw = config.getint('tracker', key) tracker.set_property('compute-hw', tracker_enable_compute_hw) nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\") if not nvvidconv: sys.stderr.write(\" Unable to create nvvidconv \\n\") # Create OSD to draw on the converted RGBA buffer nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\") if not nvosd: sys.stderr.write(\" Unable to create nvosd \\n\") # Add encoding and transmit udp stream nvvidconv_postosd = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor_postosd\") # Convert to OCU format 704X576 # nvvidconv_postosd.set_property(\"width\", 704) # nvvidconv_postosd.set_property(\"height\", 576) # for rtp stream caps = Gst.ElementFactory.make(\"capsfilter\", \"filter\") caps.set_property(\"caps\", Gst.Caps.from_string( \"application/x-rtp, media=video, clock-rate=90000, encoding-name=(string)H264, payload=96\")) encoder = Gst.ElementFactory.make(\"nvv4l2h264enc\", \"encoder\") encoder.set_property('maxperf-enable', 1) encoder.set_property('preset-level', 1) encoder.set_property('insert-sps-pps', 1) encoder.set_property('insert-vui', 1) rtppay = Gst.ElementFactory.make(\"rtph264pay\", \"rtppay\") sink = Gst.ElementFactory.make(\"udpsink\", \"sink\") sink.set_property('port', PORT_SINK) sink.set_property('auto-multicast', True) sink.set_property('host', MULTI_GROUP_SINK) # sink.set_property('sync', 1) # sink.set_property('async', False) print(\"Adding elements to Pipeline \\n\") pipeline.add(source) pipeline.add(h264parser) pipeline.add(decoder) pipeline.add(streammux) pipeline.add(pgie) pipeline.add(tracker) pipeline.add(nvvidconv) pipeline.add(nvosd) pipeline.add(nvvidconv_postosd) pipeline.add(caps) pipeline.add(encoder) pipeline.add(rtppay) pipeline.add(sink) # we link the elements together # file-source -&gt; h264-parser -&gt; nvh264-decoder -&gt; # nvinfer -&gt; nvvidconv -&gt; nvosd -&gt; video-renderer print(\"Linking elements in the Pipeline \\n\") source.link(h264parser) h264parser.link(decoder) sinkpad = streammux.get_request_pad(\"sink_0\") if not sinkpad: sys.stderr.write(\" Unable to get the sink pad of streammux \\n\") srcpad = decoder.get_static_pad(\"src\") if not srcpad: sys.stderr.write(\" Unable to get source pad of decoder \\n\") srcpad.link(sinkpad) streammux.link(pgie) pgie.link(tracker) tracker.link(nvvidconv) nvvidconv.link(nvosd) nvosd.link(nvvidconv_postosd) nvvidconv_postosd.link(caps) caps.link(encoder) encoder.link(rtppay) rtppay.link(sink) # create and event loop and feed gstreamer bus mesages to it loop = GLib.MainLoop() bus = pipeline.get_bus() bus.add_signal_watch() bus.connect(\"message\", bus_call, loop) # Lets add probe to get informed of the meta data generated, we add probe to # the sink pad of the osd element, since by that time, the buffer would have # had got all the metadata. osdsinkpad = nvosd.get_static_pad(\"sink\") if not osdsinkpad: sys.stderr.write(\" Unable to get sink pad of nvosd \\n\") osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0) print(\"Starting pipeline \\n\") # start play back and listed to event pipeline.set_state(Gst.State.PLAYING) try: loop.run() except: pass # cleanup pipeline.set_state(Gst.State.NULL) if __name__ == '__main__': sys.exit(main(sys.argv)) In addition I try replace caps with the property: caps.set_property( \"caps\", Gst.Caps.from_string(\"video/x-raw(memory:NVMM), format=I420\"))",
        "answers": [],
        "votes": []
    },
    {
        "question": "Hardware: Jetson Nano - Jetpack 4.4 - Python 3.6 - Deepstream 6.0 I'm currently working with deepstream-imagedate-multistream.py 1 example from NVIDIA-AI-IOT repo and I want to use face_recognition library 2 (of adam geitgey) in this sample to recognize faces in multiple video sources. So the question is that how can I directly process frames in this sample pipeline. I have read the deepstream document and have done many tests but still don't have any results. I hope that anybody have encounter the same problem or having similar experience could help me find a solution. I would really appreciate. Thanks.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Device Specification : **\u2022 Hardware Platform (Jetson Xavier NX) **\u2022 Deepstream NGC Container ( DeepStream-l4t container 6.0 samples ) **\u2022 JetPack Version 4.6.1 (L4T 32.7.1) **\u2022 CUDA 10.2.300 **\u2022 TensorRT Version 8.2.1.8 **\u2022 CUDNN : 8.2.1.32 I am trying to run a deepstream application on my device. I get following issue while running my application with my current rtsp. NVPARSER: HEVC: Seeking is not performed on IRAP picture I have tested my rtsp with multiple methods. It is working only with cv2 with ffmpeg as a backend with other methods it is giving errors. The methods I have tried are following. With ffplay and ffmpeg errors are following. enter image description here enter image description here With gst-launch-1.0 / gst-play-1.0 / gst-discoverer-1.0 I am getting same error as one in deepstream Application. enter image description here Can someone give me solution to this issue or just explain me the reason for this issue to occur is it related to my rtsp source or some other issue is happening.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a problem with RTSP streams from some cameras randomly resetting timestamp information in the RTP packets. I am using the Python bindings for GStreamer (and DeepStream) and I when using GST_DEBUG=2 I am able to observe the warning messages when this happens as follows: 0:00:06.294763481 1302530 0x7efb70011a40 WARN videodecoder gstvideodecoder.c:2761:gst_video_decoder_prepare_finish_frame:&lt;nvv4l2decoder2&gt; decreasing timestamp (0:00:03.117683019 &lt; 0:00:15.618723844) 0:00:06.412505164 1302530 0x7efb70011a40 WARN videodecoder gstvideodecoder.c:2761:gst_video_decoder_prepare_finish_frame:&lt;nvv4l2decoder2&gt; decreasing timestamp (0:00:03.157683019 &lt; 0:00:15.618723844) When that happens the pipeline for the source hangs until the reported timestamp catches the previous one. I would like to capture the warning messages, and if they correspond to this problem, reset the RTSP connection. I have followed many examples to capture all warning messages as follows: loop = GLib.MainLoop() bus = pipeline.get_bus() bus.add_signal_watch() connect(\"message\", on_warning, loop) def on_warning(bus, message: Gst.Message, loop): t = message.type if t == Gst.MessageType.WARNING: err, debug = message.parse_warning() sys.stderr.write(f\"Warning: {err}: {debug}\") # CHECK MESSAGE HERE AND RESET CONNECTION IF RECEIVED MULTIPLE TIMES The previous code is able to capture some of the warning messages I observe through the terminal output, but never the ones I would like it to (and many others I observe as decoding warnings).",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've been getting OOM errors after running my Gstreamer pipeline for long periods of time. Running on a Jetson-Xavier Devkit. The smallest reproducible example: gst-launch-1.0 videotestsrc ! omxh265enc ! qtmux ! filesink location=test.mp4 -e Memory before running: sh-5.1# vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 3 0 0 5384300 307304 18665008 0 0 9 59 28 51 6 2 92 0 0 Memory after running 2 min: sh-5.1# vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 10 0 0 5151928 307560 18820992 0 0 9 59 29 1 6 2 92 0 0 Memory after running 10 min: sh-5.1# vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 4 0 0 4794396 308008 19138836 0 0 9 60 31 5 6 2 92 0 0 Memory after stopping: sh-5.1# vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 4836532 308040 19140984 0 0 9 60 31 5 6 2 92 0 0 RAM utilization never stops increasing relatively linearly, and the cache doesn't get freed when I stop the process. This doesn't happen if I replace the filesink with a fakesink: gst-launch-1.0 videotestsrc ! omxh265enc ! qtmux ! fakesink Memory before running: sh-5.1# vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 4836532 308040 19140984 0 0 9 60 31 5 6 2 92 0 0 Memory after running 2 min: sh-5.1# vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 4608692 308192 19141064 0 0 9 60 32 7 6 2 92 0 0 I have noticed the same issue with a splitmuxsink, when using mp4mux, and when switching out the omxh265enc, so I'm assuming there's something going on with the filesink? Does anyone have any insight into this or know how to prevent gstreamer from using all my memory?",
        "answers": [
            [
                "Have you tried using nvv4l2h265enc instead? Edit: I just used nvv4l2h265enc with filesink, and I'm seeing the same issue as you, very intersting. Edit2: I think it's a matter on how commands like free or vmstat calculate virtual memory usage. For example the free command indeed shows less and less free memory overtime, however if you look at the used column, it does not increase overtime. Also using graphical monitor tools on the system like gnome system monitor, show that the memory does not increase in time."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "My goal is to run DeepStream SDK 6.2 (or older version). The official SD Card Image is JP461 (I assume it's JetPack 4.6.1). However, DeepStream SDK 6.2 require JetPack 5.1. The problem is JetPack 5.1 does not mention Nano at all and I can't find older version of DeepStream SDK.",
        "answers": [
            [
                "You will not be able to see the archived versions of Deepstream without signing in. You can find the download links here Note that 6.0.1 is the last release for Jetson Nano. Incase you need docker images, you can directly docker pull on the Jetson (assuming you have Jetpack 4.6.1 installed) docker pull nvcr.io/nvidia/deepstream-l4t:6.0.1-base (You can find all the images here)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Problem: inference results from deepstream and local inference do not match (using same png images). While testing what percentage of predictions match between engine and pth models, only 26% matched out of 180k images. How I reproduce results: I save images after they go through streammux in 416x416 shape and .png format. For each image I also save bounding box coordinates where YoloV4 detected objects. To test predictions I download images and bounding box coordinates for each image, then I crop object based on bounding box coordinates and run resulting image through pth model. Version: Deepstream 5.1 Model training: I train EfficientNetB0 locally with PyTorch and use following transformations for loading data (we are training 128 classes): import Albumentations as A from albumentations.pytorch import ToTensorV2 train_transforms = A.Compose( [ A.Resize(height=224, width=224), A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.RandomGamma(gamma_limit=(75, 90), p=0.8), A.GridDropout(ratio=0.47, p=0.6), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2(), ] ) I run model inference locally with following preprocessing: test_transforms = A.Compose( [ A.Resize(height=224, width=224), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2(), ] ) To export the model: Convert trained model to .onnx: model = efficientnet_b0(pretrained=False) pt_model = torch.load(path_to_torch_model, map_location=torch.device(\"cpu\")) n_features = model.classifier[1].in_features model.classifier[1] = nn.Linear(n_features, classes) model.load_state_dict(pt_model) model = nn.Sequential(model, nn.Softmax(-1)) dummy_input = torch.randn(batch_size, 3, 224, 224) torch.onnx.export( model, dummy_input, path_to_onnx, verbose=False, input_names=[\"input_names\"], output_names=[\"output_names\"], export_params=True, ) I checked that converted onnx model gives same results as pytorch model. Export .onnx to engine file with following command: docker container run \\ --gpus all \\ --rm \\ --volume $(pwd):/workspace/ \\ --volume $(pwd):/data/ \\ --workdir /workspace/ \\ nvcr.io/nvidia/tensorrt:21.02-py3 \\ trtexec --explicitBatch \\ --onnx=best_23.onnx \\ --saveEngine=efficientnet.engine \\ --fp16 \\ --workspace=4096 Deepstream configuration: RTSP stream \u2192 Streammux (reshaping to 416x416) \u2192 YoloV4 (bounding boxes) \u2192 Classification Deepstream classification config: [property] gpu-id=0 offsets=103.53;116.28;123.675 net-scale-factor=0.01735207357279195 labelfile-path=../classifier/labels.txt model-engine-file=\u2026/classifier/efficientnet.engine infer-dims=3;224;224 network-mode=2 network-type=1 num-detected-classes=128 interval=0 classifier-threshold=0 Questions: How can I achieve same preprocessing during training in python as in deepstream inference, because I guess that albumentation package gives different interpolation result than deepstream inference? Are there any other mistakes that I haven't noticed?",
        "answers": [],
        "votes": []
    },
    {
        "question": "We are working on using AWS IVS to work with severeal devices but we want to know two things. Does the url link change anytime wwe use the AWS IVS to stream live video feeds to AWS. How do we associate a device ID in AWS IVS for multiple unit. Note we are currently making use of the Nvidia Jetson nano We have tested AWS IVS using OBS on jetson nano but we are limited on credits hence my ask",
        "answers": [],
        "votes": []
    },
    {
        "question": "Does anyone have a independent evaluation of using Deepstream Gstream pipline instead of a conventional python code? Gstreamer: USB-CAM -&gt; Appsink -&gt; (CPU to GPU) AI analysis (TensorRT) -&gt; CV2.ImShow I think I understand that deepstream uses the GPU only in the gstreamer pipeline but is it faster. Any comparison. Where/what do I gain by using the deepstreamer?",
        "answers": [
            [
                "GStreamer is a pipeline-based multimedia framework that links together a wide variety of media processing systems to complete complex workflows. For instance, GStreamer can be used to build a system that reads files in one format, processes them, and exports them in another. The formats and processes can be changed in a plug and play fashion. Deepstream utilizes gstreamer to do some tasks in a cascade routine. As Deepstream feed input streams to the pipeline and since gstreamer has different plugins, streams pass through this pipeline. Nvidia made some plugin in addition to gstreamer plugin. For example pgie, tracker , tiler, nvvidconv, nvosd, transform, sink and ... are some of them. Deepstream runs main loop on GPU, so primary model and secondary models could do inferences. Accessing to output of each plugins- like pgie or sgie- is possible via metadata. These metadatas includes frame data, object location, time of occurrence and .... It is possible to access these metadata via some function in plugins like gstdsexample. Having these data make it easy to do process on a stream like data. Alongside these benefit it(Deepstream) has some disadvantages: making a complex pipeline and working with it, is hard. When you have a stream data(video, text, speech, image and ...) it's better to utilize this pipeline. Also, python and C++ implementation is possible."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I use deepstream multistreaming in Python. I need to speed up the loop for saving frames from each rtsp source. How can I parallelize this process using Cython? tiler_sink_pad_buffer_probe function in following example shows the case: https://github.com/NVIDIA-AI-IOT/deepstream_python_apps/blob/master/apps/deepstream-imagedata-multistream/deepstream_imagedata-multistream.py This is my function with for loop to process the frames that should be done in parallel: @nvtx.annotate(\"__process_frame_result\") def __process_frame_result(self, frames, _, l_frame_meta: List, ll_obj_meta: List[List]): with nvtx.annotate(\"frame_loop\", color=\"green\"): for frame, frame_meta, l_obj_meta in zip(frames, l_frame_meta, ll_obj_meta): stream_name = self.stream_name[frame_meta.pad_index] required_data = self.extract_info(frame, frame_meta, l_obj_meta, stream_name) self.process.send('process_frames', required_data)",
        "answers": [],
        "votes": []
    },
    {
        "question": "Hi I am new to deepstream and I am trying to save the video path location in the NVDSMetaData but I am not able to find how to do it any help would be appreciated I went through the deepstream_usermeta_test.c file and the nvidia forum but couldn't find what I was looking for",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using Gstreamer (Gst-python) to write a pipeline that samples images out of a video and creates a slideshow out of these images. For that I am using valve element to sample the images and imagefreeze to create the slideshow. My pipeline looks something like the following : source --- tee --- queue1 --- valve --- imagefreeze -- sink --- queue2 --- sink Currently, I am attaching a callback to GObject mainloop with \"timeout_add_seconds\". In this callback I am toggling the drop property of valve to let one image buffer pass through, and it works. The problem is when I insert imagefreeze it does not refresh automatically so I tried to change its state to READY then to PLAYING. This works for the first call of the function but the entire pipeline freezes at the second call and I could not figure out why. here's my callback snippet : def toggle(pipeline): imagefreeze = pipeline.get_by_name(\"freeze\") imagefreeze.set_state(Gst.State.PAUSED) imagefreeze.set_state(Gst.State.READY) valve = pipeline.get_by_name(\"valve\") valve.set_property(\"drop\", 0) time.sleep(0.1) imagefreeze.set_state(Gst.State.PAUSED) imagefreeze.set_state(Gst.State.PLAYING) valve.set_property(\"drop\", 1) return True Is it a correct approach to do so? otherwise how can I force imagefreeze plugin to refresh its output when the input buffer changes?",
        "answers": [],
        "votes": []
    },
    {
        "question": "So my situation is I have a gstreamer pipeline (deepstream more precisely), that runs from C++ program, I have a function that checks user commands and I'm able to pause/resume pipeline that way. My question is there way to get screenshot from pipeline's videostream likewise?",
        "answers": [
            [
                "If you are looking to check if the video is making it through your pipeline, use the \"identity dump=1\" element in your pipeline. This will display frames in a hexdump manner if the video is actually making through at the point at which this identity element is inserted."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am using deepstream pipeline (custom deepstream-test3) for detection. I used my custom yolov4 model, converted using https://github.com/marcoslucianops/DeepStream-Yolo this repo, and got the \u201c.so\u201d file. When I run it on one video there are no missing detections. But while running on two videos, there are missing detections in a few frames. While running on two same videos also see the same issue and also miss on a different frame, there will not be detected in both videos at the same time. It seems the detection just switches from one frame to another in case of multi-streaming and skipping the frame to detect. The python deepstream file https://github.com/NVIDIA-AI-IOT/deepstream_python_apps/blob/master/apps/deepstream-test3/deepstream_test_3.py The config file https://github.com/marcoslucianops/DeepStream-Yolo/blob/master/config_infer_primary.txt",
        "answers": [
            [
                "This was happening due to that repo. The owner of the git repo made the changes, and working properly now."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a TensorFlow Keras model which is stored in .pb format and from .pb format I am converting the model to .onnx format using the tf2onnx model !python -m tf2onnx.convert --saved-model model.pb --output model.onnx now after converting I see that my input layer is in NHWC format and I need to convert the same to NCHW, to achieve that I am using !python -m tf2onnx.convert --saved-model model.pb --output model_3.onnx --inputs-as-nchw input0:0 which is still giving me the same output as NHWC I have to consume the above model in NVIDIA Deepstream which only accepts NCHW format. I found this link which talks about the transpose of the input layer, but unfortunately, that is also not working. Convert between NHWC and NCHW in TensorFlow #import tensorflow as tf images_nhwc = tf.compat.v1.placeholder(tf.float32, [1, 200, 300, 3]) # input batch out = tf.transpose(images_nhwc, [0, 3, 1, 2]) #print(out.get_shape()) model.build(out.get_shape()) It would be really helpful if some experts can share their thoughts on how to convert NHWC to NCHW",
        "answers": [
            [
                "I found the solution. I had to take the latest code of tf2onnx.convert.from_keras. I took the main branch from tf2onnx !pip install --force-reinstall git+https://github.com/onnx/tensorflow-onnx.git@main !pip show tf2onnx !pip freeze | grep tf2onnx once that was done I was able to load the latest functionality and updated code at https://github.com/onnx/tensorflow-onnx/tree/e896723e410a59a600d1a73657f9965a3cbf2c3b . Below is the code I used to convert my model from .pb to .onnx along with NHWC to NCHW. # give the list of *inputs* which should be converted and returned *as nchw* _INPUT = model.input.name model_proto, external_tensor_storage = tf2onnx.convert.from_keras(model, inputs_as_nchw=[_INPUT]) The biggest catch about the above code was [_INPUT] which was suppose to be a list and I was able find this information in the test cases."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Question also posted on their forum here: https://forums.developer.nvidia.com/t/deepstream-back-to-back-detectors-with-dashcamnetand-vehiclemakenet-not-classifying-correctly/220606 Hello, Hardware Platform (Jetson / GPU) dGPU RTX 3080 on Ubuntu 20.04.1 DeepStream Version 6.1 TensorRT Version 8.4.1 NVIDIA GPU Driver Version (valid for GPU only) 515.48.07 I'm trying to use the back-to-back-detectors reference C application with the DashCamNet and VehicleMakeNet models. The DashCamNet detector works, but the VehicleMakeNet classifier is only outputing whatever the first entry in the labels file is (Acura in this case). The changes I've made is from lines 89 to 103, I've replaced that all with an if statement that will print out any classifier metadata if it exists along with instantiating variables, changing the config names and changing the sink to \"fake-renderer\". if (obj_meta-&gt;classifier_meta_list) { class_meta = (NvDsClassifierMeta * )(obj_meta-&gt;classifier_meta_list-&gt;data); if (class_meta-&gt;label_info_list) { label_info = (NvDsLabelInfo * )(class_meta-&gt;label_info_list-&gt;data); g_print(\"Result: %s\\n\", label_info-&gt;result_label); } } DashCamNet Configuration: [property] gpu-id=0 net-scale-factor=0.00392156862745098 offsets=0.0;0.0;0.0 tlt-model-key=tlt_encode tlt-encoded-model=models/tao_pretrained_models/dashcamnet/resnet18_dashcamnet_pruned.etlt labelfile-path=models/tao_pretrained_models/dashcamnet/labels.txt int8-calib-file=models/tao_pretrained_models/dashcamnet/dashcamnet_int8.txt model-engine-file=models/tao_pretrained_models/dashcamnet/resnet18_dashcamnet_pruned.etlt_b1_gpu0_int8.engine infer-dims=3;544;960 uff-input-blob-name=input_1 batch-size=1 process-mode=1 model-color-format=0 ## 0=FP32, 1=INT8, 2=FP16 mode network-mode=1 num-detected-classes=4 interval=0 gie-unique-id=1 output-blob-names=output_cov/Sigmoid;output_bbox/BiasAdd model-color-format=0 maintain-aspect-ratio=0 output-tensor-meta=0 [class-attrs-all] pre-cluster-threshold=0.2 group-threshold=1 ## Set eps=0.7 and minBoxes for cluster-mode=1(DBSCAN) eps=0.2 #minBoxes=3 VehicleMakeNet [property] batch-size=4 classifier-threshold=0.95 gie-unique-id=4 gpu-id=0 input-dims=3;224;224;0 int8-calib-file=models/VehicleMake/vehiclemakenet_int8.txt labelfile-path=models/VehicleMake/labels_vehiclemakenet.txt model-color-format=0 model-engine-file=models/VehicleMake/resnet18_vehiclemakenet_pruned.etlt_b4_gpu0_int8.engine net-scale-factor=1 ## 0=FP32, 1=INT8, 2=FP16 mode network-mode=1 network-type=1 num-detected-classes=4 offsets=124;117;104 operate-on-gie-id=1 output-blob-names=predictions/Softmax process-mode=2 tlt-encoded-model=models/VehicleMake/resnet18_vehiclemakenet_pruned.etlt tlt-model-key=tlt_encode uff-input-blob-name=input_1 Full code /* * Copyright (c) 2020-2021, NVIDIA CORPORATION. All rights reserved. * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ #include &lt;gst/gst.h&gt; #include &lt;glib.h&gt; #include &lt;stdio.h&gt; #include \"gstnvdsmeta.h\" #include &lt;cuda_runtime_api.h&gt; #define MAX_DISPLAY_LEN 64 #define PGIE_CLASS_ID_VEHICLE 0 #define PGIE_CLASS_ID_PERSON 2 #define SGIE_CLASS_ID_LP 1 #define SGIE_CLASS_ID_FACE 0 /* Change this to 0 to make the 2nd detector act as a primary(full-frame) detector. * When set to 1, it will act as secondary(operates on primary detected objects). */ #define SECOND_DETECTOR_IS_SECONDARY 1 /* The muxer output resolution must be set if the input streams will be of * different resolution. The muxer will scale all the input frames to this * resolution. */ #define MUXER_OUTPUT_WIDTH 1280 #define MUXER_OUTPUT_HEIGHT 720 /* Muxer batch formation timeout, for e.g. 40 millisec. Should ideally be set * based on the fastest source's framerate. */ #define MUXER_BATCH_TIMEOUT_USEC 40000 gint frame_number = 0; gchar pgie_classes_str[4][32] = { \"Vehicle\", \"TwoWheeler\", \"Person\", \"Roadsign\" }; #define PRIMARY_DETECTOR_UID 1 #define SECONDARY_DETECTOR_UID 2 /* nvvidconv_sink_pad_buffer_probe will extract metadata received on nvvideoconvert sink pad * and update params for drawing rectangle, object information etc. */ static GstPadProbeReturn nvvidconv_sink_pad_buffer_probe (GstPad * pad, GstPadProbeInfo * info, gpointer u_data) { GstBuffer *buf = (GstBuffer *) info-&gt;data; NvDsObjectMeta *obj_meta = NULL; guint vehicle_count = 0; guint person_count = 0; guint face_count = 0; guint lp_count = 0; NvDsMetaList * l_frame = NULL; NvDsMetaList * l_obj = NULL; NvDsDisplayMeta *display_meta = NULL; NvDsClassifierMeta *class_meta = NULL; NvDsLabelInfo *label_info = NULL; NvDsBatchMeta *batch_meta = gst_buffer_get_nvds_batch_meta (buf); for (l_frame = batch_meta-&gt;frame_meta_list; l_frame != NULL; l_frame = l_frame-&gt;next) { NvDsFrameMeta *frame_meta = (NvDsFrameMeta *) (l_frame-&gt;data); int offset = 0; for (l_obj = frame_meta-&gt;obj_meta_list; l_obj != NULL; l_obj = l_obj-&gt;next) { obj_meta = (NvDsObjectMeta *) (l_obj-&gt;data); /* Check that the object has been detected by the primary detector * and that the class id is that of vehicles/persons. */ if (obj_meta-&gt;unique_component_id == PRIMARY_DETECTOR_UID) { if (obj_meta-&gt;class_id == PGIE_CLASS_ID_VEHICLE) vehicle_count++; if (obj_meta-&gt;class_id == PGIE_CLASS_ID_PERSON) person_count++; } if (obj_meta-&gt;classifier_meta_list) { class_meta = (NvDsClassifierMeta * )(obj_meta-&gt;classifier_meta_list-&gt;data); if (class_meta-&gt;label_info_list) { label_info = (NvDsLabelInfo * )(class_meta-&gt;label_info_list-&gt;data); g_print(\"Result: %s\\n\", label_info-&gt;result_label); } } } display_meta = nvds_acquire_display_meta_from_pool(batch_meta); NvOSD_TextParams *txt_params = &amp;display_meta-&gt;text_params[0]; display_meta-&gt;num_labels = 1; txt_params-&gt;display_text = g_malloc0 (MAX_DISPLAY_LEN); offset = snprintf(txt_params-&gt;display_text, MAX_DISPLAY_LEN, \"Person = %d \", person_count); offset += snprintf(txt_params-&gt;display_text + offset , MAX_DISPLAY_LEN, \"Vehicle = %d \", vehicle_count); offset += snprintf(txt_params-&gt;display_text + offset , MAX_DISPLAY_LEN, \"Face = %d \", face_count); offset += snprintf(txt_params-&gt;display_text + offset , MAX_DISPLAY_LEN, \"License Plate = %d \", lp_count); /* Now set the offsets where the string should appear */ txt_params-&gt;x_offset = 10; txt_params-&gt;y_offset = 12; /* Font , font-color and font-size */ txt_params-&gt;font_params.font_name = \"Serif\"; txt_params-&gt;font_params.font_size = 10; txt_params-&gt;font_params.font_color.red = 1.0; txt_params-&gt;font_params.font_color.green = 1.0; txt_params-&gt;font_params.font_color.blue = 1.0; txt_params-&gt;font_params.font_color.alpha = 1.0; /* Text background color */ txt_params-&gt;set_bg_clr = 1; txt_params-&gt;text_bg_clr.red = 0.0; txt_params-&gt;text_bg_clr.green = 0.0; txt_params-&gt;text_bg_clr.blue = 0.0; txt_params-&gt;text_bg_clr.alpha = 1.0; nvds_add_display_meta_to_frame(frame_meta, display_meta); } g_print (\"Frame Number = %d Vehicle Count = %d Person Count = %d\" \" Face Count = %d License Plate Count = %d\\n\", frame_number, vehicle_count, person_count, face_count, lp_count); frame_number++; return GST_PAD_PROBE_OK; } static gboolean bus_call (GstBus * bus, GstMessage * msg, gpointer data) { GMainLoop *loop = (GMainLoop *) data; switch (GST_MESSAGE_TYPE (msg)) { case GST_MESSAGE_EOS: g_print (\"End of stream\\n\"); g_main_loop_quit (loop); break; case GST_MESSAGE_ERROR:{ gchar *debug; GError *error; gst_message_parse_error (msg, &amp;error, &amp;debug); g_printerr (\"ERROR from element %s: %s\\n\", GST_OBJECT_NAME (msg-&gt;src), error-&gt;message); if (debug) g_printerr (\"Error details: %s\\n\", debug); g_free (debug); g_error_free (error); g_main_loop_quit (loop); break; } default: break; } return TRUE; } int main (int argc, char *argv[]) { GMainLoop *loop = NULL; GstElement *pipeline = NULL, *source = NULL, *h264parser = NULL, *decoder = NULL, *streammux = NULL, *sink = NULL, *primary_detector = NULL, *secondary_detector = NULL, *nvvidconv = NULL, *nvosd = NULL; GstElement *transform = NULL; GstBus *bus = NULL; guint bus_watch_id; GstPad *nvvidconv_sink_pad = NULL; int current_device = -1; cudaGetDevice(&amp;current_device); struct cudaDeviceProp prop; cudaGetDeviceProperties(&amp;prop, current_device); /* Check input arguments */ if (argc != 2) { g_printerr (\"Usage: %s &lt;H264 filename&gt;\\n\", argv[0]); return -1; } /* Standard GStreamer initialization */ gst_init (&amp;argc, &amp;argv); loop = g_main_loop_new (NULL, FALSE); /* Create gstreamer elements */ /* Create Pipeline element that will form a connection of other elements */ pipeline = gst_pipeline_new (\"pipeline\"); /* Source element for reading from the file */ source = gst_element_factory_make (\"filesrc\", \"file-source\"); /* Since the data format in the input file is elementary h264 stream, * we need a h264parser */ h264parser = gst_element_factory_make (\"h264parse\", \"h264-parser\"); /* Use nvdec_h264 for hardware accelerated decode on GPU */ decoder = gst_element_factory_make (\"nvv4l2decoder\", \"nvv4l2-decoder\"); /* Create nvstreammux instance to form batches from one or more sources. */ streammux = gst_element_factory_make (\"nvstreammux\", \"stream-muxer\"); if (!pipeline || !streammux) { g_printerr (\"One element could not be created. Exiting.\\n\"); return -1; } /* Create two nvinfer instances for the two back-to-back detectors */ primary_detector = gst_element_factory_make (\"nvinfer\", \"primary-nvinference-engine1\"); secondary_detector = gst_element_factory_make (\"nvinfer\", \"primary-nvinference-engine2\"); /* Use convertor to convert from NV12 to RGBA as required by nvosd */ nvvidconv = gst_element_factory_make (\"nvvideoconvert\", \"nvvideo-converter\"); /* Create OSD to draw on the converted RGBA buffer */ nvosd = gst_element_factory_make (\"nvdsosd\", \"nv-onscreendisplay\"); /* Finally render the osd output */ if(prop.integrated) { transform = gst_element_factory_make (\"nvegltransform\", \"nvegl-transform\"); } sink = gst_element_factory_make(\"fakesink\", \"fake-renderer\"); if (!source || !h264parser || !decoder || !primary_detector || !secondary_detector || !nvvidconv || !nvosd || !sink) { g_printerr (\"One element could not be created. Exiting.\\n\"); return -1; } if(prop.integrated) { if(!transform) { g_printerr (\"One tegra element could not be created. Exiting.\\n\"); return -1; } } /* we set the input filename to the source element */ g_object_set (G_OBJECT (source), \"location\", argv[1], NULL); g_object_set (G_OBJECT (streammux), \"width\", MUXER_OUTPUT_WIDTH, \"height\", MUXER_OUTPUT_HEIGHT, \"batch-size\", 1, \"batched-push-timeout\", MUXER_BATCH_TIMEOUT_USEC, NULL); /* Set the config files for the two detectors. We demonstrate this by using * the same detector model twice but making them act as vehicle-only and * person-only detectors by adjusting the bbox confidence thresholds in the * two seperate config files. */ g_object_set (G_OBJECT (primary_detector), \"config-file-path\", \"dashcamnet_config.txt\", \"unique-id\", PRIMARY_DETECTOR_UID, NULL); g_object_set (G_OBJECT (secondary_detector), \"config-file-path\", \"vehicletypenet_sgie_config.txt\", \"unique-id\", SECONDARY_DETECTOR_UID, \"process-mode\", 2, NULL); /* we add a message handler */ bus = gst_pipeline_get_bus (GST_PIPELINE (pipeline)); bus_watch_id = gst_bus_add_watch (bus, bus_call, loop); gst_object_unref (bus); /* Set up the pipeline */ /* we add all elements into the pipeline */ if(prop.integrated) { gst_bin_add_many (GST_BIN (pipeline), source, h264parser, decoder, streammux, primary_detector, secondary_detector, nvvidconv, nvosd, transform, sink, NULL); } else { gst_bin_add_many (GST_BIN (pipeline), source, h264parser, decoder, streammux, primary_detector, secondary_detector, nvvidconv, nvosd, sink, NULL); } GstPad *sinkpad, *srcpad; gchar pad_name_sink[16] = \"sink_0\"; gchar pad_name_src[16] = \"src\"; sinkpad = gst_element_get_request_pad (streammux, pad_name_sink); if (!sinkpad) { g_printerr (\"Streammux request sink pad failed. Exiting.\\n\"); return -1; } srcpad = gst_element_get_static_pad (decoder, pad_name_src); if (!srcpad) { g_printerr (\"Decoder request src pad failed. Exiting.\\n\"); return -1; } if (gst_pad_link (srcpad, sinkpad) != GST_PAD_LINK_OK) { g_printerr (\"Failed to link decoder to stream muxer. Exiting.\\n\"); return -1; } gst_object_unref (sinkpad); gst_object_unref (srcpad); /* we link the elements together */ /* file-source -&gt; h264-parser -&gt; nvh264-decoder -&gt; * nvinfer -&gt; nvvidconv -&gt; nvosd -&gt; video-renderer */ if (!gst_element_link_many (source, h264parser, decoder, NULL)) { g_printerr (\"Elements could not be linked: 1. Exiting.\\n\"); return -1; } if(prop.integrated) { if (!gst_element_link_many (streammux, primary_detector, secondary_detector, nvvidconv, nvosd, transform, sink, NULL)) { g_printerr (\"Elements could not be linked: 2. Exiting.\\n\"); return -1; } } else { if (!gst_element_link_many (streammux, primary_detector, secondary_detector, nvvidconv, nvosd, sink, NULL)) { g_printerr (\"Elements could not be linked: 2. Exiting.\\n\"); return -1; } } /* Lets add probe to get informed of the meta data generated, we add probe to * the sink pad of the nvvideoconvert element, since by that time, the buffer would have * had got all the metadata. */ nvvidconv_sink_pad = gst_element_get_static_pad (nvvidconv, \"sink\"); if (!nvvidconv_sink_pad) g_print (\"Unable to get sink pad\\n\"); else gst_pad_add_probe (nvvidconv_sink_pad, GST_PAD_PROBE_TYPE_BUFFER, nvvidconv_sink_pad_buffer_probe, NULL, NULL); /* Set the pipeline to \"playing\" state */ g_print (\"Now playing: %s\\n\", argv[1]); gst_element_set_state (pipeline, GST_STATE_PLAYING); /* Wait till pipeline encounters an error or EOS */ g_print (\"Running...\\n\"); g_main_loop_run (loop); /* Out of the main loop, clean up nicely */ g_print (\"Returned, stopping playback\\n\"); gst_element_set_state (pipeline, GST_STATE_NULL); g_print (\"Deleting pipeline\\n\"); gst_object_unref (GST_OBJECT (pipeline)); g_source_remove (bus_watch_id); g_main_loop_unref (loop); return 0; }",
        "answers": [
            [
                "I think the issue was that the labels.txt file that was provided for the model needed to be comma seperated and doesn\u2019t work if it\u2019s seperated by new lines."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "What is the difference between Frame Meta, Object Meta and User Meta? Please explain in layman terms. I know they are nested and looked up at the docs but was still unable to decipher it.",
        "answers": [
            [
                "Please take a look at documentation, and for further explanation you can take a look at sample apps, providing by deepstream (e.g. test4, written in C - there are frame, object and user metadata presented."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I\u2019m trying to save extracted frames from a Deepstream pipeline to video with OpenCV but all I end up with is a 9KB file. This is my code (executed inside a probe function): batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer)) l_frame = batch_meta.frame_meta_list frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data) frame = pyds.get_nvds_buf_surface(hash(gst_buffer), frame_meta.batch_id) frame_copy = np.array(frame, copy=True, order='C') frame_copy = cv2.cvtColor(frame_copy, cv2.COLOR_RGBA2BGRA) The above code is executed each time the probe function is invoked. Images are saved to a queue: frame_buffer.put(frame_copy) After the required number of frames has been pushed into the queue, I use below code to save the buffered frames to a video file: codec = cv2.VideoWriter_fourcc(*'XVID') out = cv2.VideoWriter('out.avi', codec, fps, (output_width, output_height)) out.write(frame_copy) total_frames = FRAME_RECORDING_THRESH while total_frames &gt; 0: frame = frame_buffer.get() frame = cv2.resize(frame, (output_width, output_height), interpolation = cv2.INTER_LINEAR) out.write(frame) total_frames -= 1 out.release() Unfortunately the file produced is not a valid video file. Is there sth I am doing wrong in the above process? Any help would be greatly appreciated. P.S. Just to test that the frames have been correctly stored inside the queue, if I attempt to save the frames as images inside the while loop: cv2.imwrite(dest_folder + '/' + f'tmp{total_frames}.png', frame) I get properly saved and valid png images. P.S. 2 Frames have a resolution of output_width, output_height at the time they are buffered. Also, trying to do a cv2.resize before they are saved doesn't change anything.",
        "answers": [
            [
                "I can't test it but common mistake is that people think than code out = cv2.VideoWriter('out.avi', codec, fps, (output_width, output_height)) will automatically resize frames to size (output_width, output_height) but it is not true. You have to manually resize frames. If you don't do this then Writer will skip frames and you get broken file - without frames. while total_frames &gt; 0: frame = frame_buffer.get() frame = cv2(frame, (output_width, output_height)) out.write(frame) total_frames -= 1 EDIT: It seems problem can make image with transparent layer A - RGBA - because video don't use A. It needs to remove it. You can convert with cv2.COLOR_RGBA2BGR instead of cv2.COLOR_RGBA2BGRA frame_copy = cv2.cvtColor(frame_copy, cv2.COLOR_RGBA2BGR) Or you can remove last layer from numpy.array frame_copy = frame_copy[ : , : , :3 ]"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to install the Nvidia DeepStream SDK module on my Jetson Nano running JetPack 4.6.1 with Azure IOT Edge. Following this example, but it is based on DeepStream 5.1, and since I have JetPack 4.6.1 I have DeepStream SDK 6.0.1 https://github.com/Azure-Samples/NVIDIA-Deepstream-Azure-IoT-Edge-on-a-NVIDIA-Jetson-Nano I'm getting this error when deploying the DeepStream SDK 5.1 module from the marketplace. 2022-05-21 16:52:36.053 +00:00 [ERR] - Edge agent plan execution failed. System.AggregateException: One or more errors occurred. (Error calling start module NVIDIADeepStreamSDK: Could not start module NVIDIADeepStreamSDK caused by: Could not start module NVIDIADeepStreamSDK caused by: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: error adding seccomp filter rule for syscall clone3: permission denied: unknown) ---&gt; Microsoft.Azure.Devices.Edge.Agent.Edgelet.EdgeletCommunicationException- Message:Error calling start module NVIDIADeepStreamSDK: Could not start module NVIDIADeepStreamSDK caused by: Could not start module NVIDIADeepStreamSDK caused by: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: error adding seccomp filter rule for syscall clone3: permission denied: unknown, StatusCode:500, at: at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2021_12_07.ModuleManagementHttpClient.HandleException(Exception exception, String operation) in /mnt/vss/_work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2021_12_07/ModuleManagementHttpClient.cs:line 231 at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Versioning.ModuleManagementHttpClientVersioned.Execute[T](Func`1 func, String operation) in /mnt/vss/_work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/versioning/ModuleManagementHttpClientVersioned.cs:line 155 at Microsoft.Azure.Devices.Edge.Agent.Edgelet.Version_2021_12_07.ModuleManagementHttpClient.StartModuleAsync(String name) in /mnt/vss/_work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/version_2021_12_07/ModuleManagementHttpClient.cs:line 179 at Microsoft.Azure.Devices.Edge.Agent.Edgelet.ModuleManagementHttpClient.&lt;&gt;c__DisplayClass26_0.&lt;&lt;Throttle&gt;b__0&gt;d.MoveNext() in /mnt/vss/_work/1/s/edge-agent/src/Microsoft.Azure.Devices.Edge.Agent.Edgelet/ModuleManagementHttpClient.cs:line 140 --- End of stack trace from previous location where exception was thrown --- Which DeepStream SDK container should I use instead on my Jetson Nano?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a camera which run at 25fps, Need to access the feed from the same camera with reduced fps (5) using GST. we are using deepstream and hence i was looking for solutions in nvidia forums. The following code is from nvidia deepstream sample apps. I saw a post in nvidia forums they suggested to use videorate to throtle the fps. However i'm confused about placement of videorate property.Should it be placed after uri-decode-bin? Could anyone help? GObject.threads_init() Gst.init(None) pipeline = Gst.Pipeline() source_bin = create_source_bin(cam_url) pipeline.add(source_bin) filter = create_videorate_filter() pipeline.add(filter) create_source_bin (Copied from deepstream python apps sample) def create_source_bin(cam_url): bin_name = \"source-bin-test\" nbin = Gst.Bin.new(bin_name) # Source element for reading from the cam_url. uri_decode_bin = Gst.ElementFactory.make(\"uridecodebin\", \"uri-decode-bin\") uri_decode_bin.set_property(\"uri\", cam_url) uri_decode_bin.connect(\"pad-added\", cb_newpad, nbin) uri_decode_bin.connect(\"child-added\", decodebin_child_added, nbin) Gst.Bin.add(nbin, uri_decode_bin) bin_pad = nbin.add_pad(Gst.GhostPad.new_no_target(\"src\", Gst.PadDirection.SRC)) return nbin My proposed videorate fiter with nvmm memory as seen from nvidia forum posts def create_videorate_filter(): filter = Gst.ElementFactory.make('videorate', 'videorate') caps = Gst.caps_from_string(\"video/x-raw(memory:NVMM),framerate=5/1\") filter.set_property(\"caps\", caps) if not filter: sys.stderr.write(\" Unable to create capsfilter \\n\") return filter Is this the right thing to do? Where should be the videorate filter placed?",
        "answers": [
            [
                "Yes, the videorate element can reduce the framerate as you want. Yes, the videorate must be after the uri-decodebin since you need the uncompressed frames to modify the framerate. No, the create_videorate_filter function is not okay. The videorate element does not have a caps property. However, it does have a max-rate property, so you can change your snippet as: def create_videorate_filter(): filter = Gst.ElementFactory.make('videorate', 'videorate') if not filter: sys.stderr.write(\" Unable to create capsfilter \\n\") filter.set_property(\"max-rate\", 5) return filter"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "This article explains how to do image decoding and preprocessing on server side with Dali while using triton-inference-server. I am trying to find something similar for doing video decoding from h.264 encoded bytes array on server side, before the input \"NTHWC\" array is passed to any of the video recognition models like in mmaction2 or swin-transformer, using ensemble model. All I can find is how to load video from files, but nothing on loading videos from external_source. Also, as a workaround, I guess I can do the desired thing using python-backend by writing the encoded video bytes to a file, and preprocess the video, but that will not inherently support batch processing, and I will either have to handle the batch sequentially or by starting multiprocess pools for processing each batch. highly un-optimal I guess. Any help is highly appreciated.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am really new to GStreamer and DeepStream. I have a pipeline created based on deepstream-test1. This is the order in which the elements appeared: filesrc (an h264) -&gt; h264parse -&gt; nvv4l2decoder -&gt; nvstreammux -&gt; nvinfer -&gt; nvvideoconvert -&gt; nvdsosd -&gt; nvegltransform -&gt; nveglglessink This works fine as intended. Heres the code: import sys sys.path.append('../') import gi gi.require_version('Gst', '1.0') from gi.repository import GObject, Gst from common.is_aarch_64 import is_aarch64 from common.bus_call import bus_call import pyds PGIE_CLASS_ID_VEHICLE = 0 PGIE_CLASS_ID_BICYCLE = 1 PGIE_CLASS_ID_PERSON = 2 PGIE_CLASS_ID_ROADSIGN = 3 def osd_sink_pad_buffer_probe(pad,info,u_data): frame_number=0 #Intiallizing object counter with 0. obj_counter = { PGIE_CLASS_ID_VEHICLE:0, PGIE_CLASS_ID_PERSON:0, PGIE_CLASS_ID_BICYCLE:0, PGIE_CLASS_ID_ROADSIGN:0 } num_rects=0 gst_buffer = info.get_buffer() if not gst_buffer: print(\"Unable to get GstBuffer \") return # Retrieve batch metadata from the gst_buffer # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the # C address of gst_buffer as input, which is obtained with hash(gst_buffer) batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer)) l_frame = batch_meta.frame_meta_list while l_frame is not None: try: # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta # The casting is done by pyds.glist_get_nvds_frame_meta() # The casting also keeps ownership of the underlying memory # in the C code, so the Python garbage collector will leave # it alone. #frame_meta = pyds.glist_get_nvds_frame_meta(l_frame.data) frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data) except StopIteration: break frame_number=frame_meta.frame_num num_rects = frame_meta.num_obj_meta l_obj=frame_meta.obj_meta_list while l_obj is not None: try: # Casting l_obj.data to pyds.NvDsObjectMeta #obj_meta=pyds.glist_get_nvds_object_meta(l_obj.data) obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data) except StopIteration: break obj_counter[obj_meta.class_id] += 1 obj_meta.rect_params.border_color.set(0.0, 0.0, 1.0, 0.0) try: l_obj=l_obj.next except StopIteration: break # Acquiring a display meta object. The memory ownership remains in # the C code so downstream plugins can still access it. Otherwise # the garbage collector will claim it when this probe function exits. display_meta=pyds.nvds_acquire_display_meta_from_pool(batch_meta) display_meta.num_labels = 1 py_nvosd_text_params = display_meta.text_params[0] # Setting display text to be shown on screen # Note that the pyds module allocates a buffer for the string, and the # memory will not be claimed by the garbage collector. # Reading the display_text field here will return the C address of the # allocated string. Use pyds.get_string() to get the string content. py_nvosd_text_params.display_text = \"Frame Number={} Number of Objects={} Vehicle_count={} Person_count={}\".format(frame_number, num_rects, obj_counter[PGIE_CLASS_ID_VEHICLE], obj_counter[PGIE_CLASS_ID_PERSON]) # Now set the offsets where the string should appear py_nvosd_text_params.x_offset = 10 py_nvosd_text_params.y_offset = 12 # Font , font-color and font-size py_nvosd_text_params.font_params.font_name = \"Serif\" py_nvosd_text_params.font_params.font_size = 10 # set(red, green, blue, alpha); set to White py_nvosd_text_params.font_params.font_color.set(1.0, 1.0, 1.0, 1.0) # Text background color py_nvosd_text_params.set_bg_clr = 1 # set(red, green, blue, alpha); set to Black py_nvosd_text_params.text_bg_clr.set(0.0, 0.0, 0.0, 1.0) # Using pyds.get_string() to get display_text as string print(pyds.get_string(py_nvosd_text_params.display_text)) pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta) try: l_frame=l_frame.next except StopIteration: break return Gst.PadProbeReturn.OK def main(args): # Check input arguments if len(args) != 2: sys.stderr.write(\"usage: %s &lt;media file or uri&gt;\\n\" % args[0]) sys.exit(1) # Standard GStreamer initialization GObject.threads_init() Gst.init(None) # Create gstreamer elements # Create Pipeline element that will form a connection of other elements print(\"Creating Pipeline \\n \") pipeline = Gst.Pipeline() if not pipeline: sys.stderr.write(\" Unable to create Pipeline \\n\") # Source element for reading from the file print(\"Creating Source \\n \") source = Gst.ElementFactory.make(\"filesrc\", \"file-source\") if not source: sys.stderr.write(\" Unable to create Source \\n\") # Since the data format in the input file is elementary h264 stream, # we need a h264parser print(\"Creating H264Parser \\n\") h264parser = Gst.ElementFactory.make(\"h264parse\", \"h264-parser\") if not h264parser: sys.stderr.write(\" Unable to create h264 parser \\n\") # Use nvdec_h264 for hardware accelerated decode on GPU print(\"Creating Decoder \\n\") decoder = Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\") if not decoder: sys.stderr.write(\" Unable to create Nvv4l2 Decoder \\n\") # Create nvstreammux instance to form batches from one or more sources. streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\") if not streammux: sys.stderr.write(\" Unable to create NvStreamMux \\n\") # Use nvinfer to run inferencing on decoder's output, # behaviour of inferencing is set through config file pgie = Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\") if not pgie: sys.stderr.write(\" Unable to create pgie \\n\") # Use convertor to convert from NV12 to RGBA as required by nvosd nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\") if not nvvidconv: sys.stderr.write(\" Unable to create nvvidconv \\n\") # Create OSD to draw on the converted RGBA buffer nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\") if not nvosd: sys.stderr.write(\" Unable to create nvosd \\n\") # Finally render the osd output if is_aarch64(): transform = Gst.ElementFactory.make(\"nvegltransform\", \"nvegl-transform\") print(\"Creating EGLSink \\n\") sink = Gst.ElementFactory.make(\"nveglglessink\", \"nvvideo-renderer\") if not sink: sys.stderr.write(\" Unable to create egl sink \\n\") print(\"Playing file %s \" %args[1]) source.set_property('location', args[1]) streammux.set_property('width', 1920) streammux.set_property('height', 1080) streammux.set_property('batch-size', 1) streammux.set_property('batched-push-timeout', 4000000) pgie.set_property('config-file-path', \"dstest1_pgie_config.txt\") print(\"Adding elements to Pipeline \\n\") pipeline.add(source) pipeline.add(h264parser) pipeline.add(decoder) pipeline.add(streammux) pipeline.add(pgie) pipeline.add(nvvidconv) pipeline.add(nvosd) pipeline.add(sink) if is_aarch64(): pipeline.add(transform) # we link the elements together # file-source -&gt; h264-parser -&gt; nvh264-decoder -&gt; # nvinfer -&gt; nvvidconv -&gt; nvosd -&gt; video-renderer print(\"Linking elements in the Pipeline \\n\") source.link(h264parser) h264parser.link(decoder) sinkpad = streammux.get_request_pad(\"sink_0\") if not sinkpad: sys.stderr.write(\" Unable to get the sink pad of streammux \\n\") srcpad = decoder.get_static_pad(\"src\") if not srcpad: sys.stderr.write(\" Unable to get source pad of decoder \\n\") srcpad.link(sinkpad) streammux.link(pgie) pgie.link(nvvidconv) nvvidconv.link(nvosd) if is_aarch64(): nvosd.link(transform) transform.link(sink) else: nvosd.link(sink) # create an event loop and feed gstreamer bus mesages to it loop = GObject.MainLoop() bus = pipeline.get_bus() bus.add_signal_watch() bus.connect (\"message\", bus_call, loop) # Lets add probe to get informed of the meta data generated, we add probe to # the sink pad of the osd element, since by that time, the buffer would have # had got all the metadata. osdsinkpad = nvosd.get_static_pad(\"sink\") if not osdsinkpad: sys.stderr.write(\" Unable to get sink pad of nvosd \\n\") osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0) # start play back and listen to events print(\"Starting pipeline \\n\") pipeline.set_state(Gst.State.PLAYING) try: loop.run() except: pass # cleanup pipeline.set_state(Gst.State.NULL) if __name__ == '__main__': sys.exit(main(sys.argv)) But whenever i try to replace nvegltransform -&gt; nveglglessink of the pipeline with ximagesink i get an error saying: Error: gst-stream-error-quark: Internal data stream error. (1): /dvs/git/dirty/git-master_linux/deepstream/sdk/src/gst-plugins/gst-nvinfer/gstnvinfer.cpp(2288): gst_nvinfer_output_loop (): /GstPipeline:pipeline0/GstNvInfer:primary-inference: streaming stopped, reason not-linked (-1) I needed to remove nveglglessink from the pipeline originally, but as nvegltransform is related to nveglglessink i decided to remove both. And in place of them i used ximagesink. This is the pipeline i am working on (which gives the error mentioned): filesrc (an h264) -&gt; h264parse -&gt; nvv4l2decoder -&gt; nvstreammux -&gt; nvinfer -&gt; nvvideoconvert -&gt; nvdsosd -&gt; ximagesink Heres the code: import sys sys.path.append('../') import gi gi.require_version('Gst', '1.0') from gi.repository import GObject, Gst from common.is_aarch_64 import is_aarch64 from common.bus_call import bus_call import pyds PGIE_CLASS_ID_VEHICLE = 0 PGIE_CLASS_ID_BICYCLE = 1 PGIE_CLASS_ID_PERSON = 2 PGIE_CLASS_ID_ROADSIGN = 3 def osd_sink_pad_buffer_probe(pad,info,u_data): frame_number=0 #Intiallizing object counter with 0. obj_counter = { PGIE_CLASS_ID_VEHICLE:0, PGIE_CLASS_ID_PERSON:0, PGIE_CLASS_ID_BICYCLE:0, PGIE_CLASS_ID_ROADSIGN:0 } num_rects=0 gst_buffer = info.get_buffer() if not gst_buffer: print(\"Unable to get GstBuffer \") return # Retrieve batch metadata from the gst_buffer # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the # C address of gst_buffer as input, which is obtained with hash(gst_buffer) batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer)) l_frame = batch_meta.frame_meta_list while l_frame is not None: try: # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta # The casting is done by pyds.glist_get_nvds_frame_meta() # The casting also keeps ownership of the underlying memory # in the C code, so the Python garbage collector will leave # it alone. #frame_meta = pyds.glist_get_nvds_frame_meta(l_frame.data) frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data) except StopIteration: break frame_number=frame_meta.frame_num num_rects = frame_meta.num_obj_meta l_obj=frame_meta.obj_meta_list while l_obj is not None: try: # Casting l_obj.data to pyds.NvDsObjectMeta #obj_meta=pyds.glist_get_nvds_object_meta(l_obj.data) obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data) except StopIteration: break obj_counter[obj_meta.class_id] += 1 obj_meta.rect_params.border_color.set(0.0, 0.0, 1.0, 0.0) try: l_obj=l_obj.next except StopIteration: break # Acquiring a display meta object. The memory ownership remains in # the C code so downstream plugins can still access it. Otherwise # the garbage collector will claim it when this probe function exits. display_meta=pyds.nvds_acquire_display_meta_from_pool(batch_meta) display_meta.num_labels = 1 py_nvosd_text_params = display_meta.text_params[0] # Setting display text to be shown on screen # Note that the pyds module allocates a buffer for the string, and the # memory will not be claimed by the garbage collector. # Reading the display_text field here will return the C address of the # allocated string. Use pyds.get_string() to get the string content. py_nvosd_text_params.display_text = \"Frame Number={} Number of Objects={} Vehicle_count={} Person_count={}\".format(frame_number, num_rects, obj_counter[PGIE_CLASS_ID_VEHICLE], obj_counter[PGIE_CLASS_ID_PERSON]) # Now set the offsets where the string should appear py_nvosd_text_params.x_offset = 10 py_nvosd_text_params.y_offset = 12 # Font , font-color and font-size py_nvosd_text_params.font_params.font_name = \"Serif\" py_nvosd_text_params.font_params.font_size = 10 # set(red, green, blue, alpha); set to White py_nvosd_text_params.font_params.font_color.set(1.0, 1.0, 1.0, 1.0) # Text background color py_nvosd_text_params.set_bg_clr = 1 # set(red, green, blue, alpha); set to Black py_nvosd_text_params.text_bg_clr.set(0.0, 0.0, 0.0, 1.0) # Using pyds.get_string() to get display_text as string print(pyds.get_string(py_nvosd_text_params.display_text)) pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta) try: l_frame=l_frame.next except StopIteration: break return Gst.PadProbeReturn.OK def main(args): # Check input arguments if len(args) != 2: sys.stderr.write(\"usage: %s &lt;media file or uri&gt;\\n\" % args[0]) sys.exit(1) # Standard GStreamer initialization GObject.threads_init() Gst.init(None) # Create gstreamer elements # Create Pipeline element that will form a connection of other elements print(\"Creating Pipeline \\n \") pipeline = Gst.Pipeline() if not pipeline: sys.stderr.write(\" Unable to create Pipeline \\n\") # Source element for reading from the file print(\"Creating Source \\n \") source = Gst.ElementFactory.make(\"filesrc\", \"file-source\") if not source: sys.stderr.write(\" Unable to create Source \\n\") # Since the data format in the input file is elementary h264 stream, # we need a h264parser print(\"Creating H264Parser \\n\") h264parser = Gst.ElementFactory.make(\"h264parse\", \"h264-parser\") if not h264parser: sys.stderr.write(\" Unable to create h264 parser \\n\") # Use nvdec_h264 for hardware accelerated decode on GPU print(\"Creating Decoder \\n\") decoder = Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\") if not decoder: sys.stderr.write(\" Unable to create Nvv4l2 Decoder \\n\") # Create nvstreammux instance to form batches from one or more sources. streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\") if not streammux: sys.stderr.write(\" Unable to create NvStreamMux \\n\") # Use nvinfer to run inferencing on decoder's output, # behaviour of inferencing is set through config file pgie = Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\") if not pgie: sys.stderr.write(\" Unable to create pgie \\n\") # Use convertor to convert from NV12 to RGBA as required by nvosd nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\") if not nvvidconv: sys.stderr.write(\" Unable to create nvvidconv \\n\") # Create OSD to draw on the converted RGBA buffer nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\") if not nvosd: sys.stderr.write(\" Unable to create nvosd \\n\") # Finally render the osd output # if is_aarch64(): # transform = Gst.ElementFactory.make(\"nvegltransform\", \"nvegl-transform\") # print(\"Creating EGLSink \\n\") # sink = Gst.ElementFactory.make(\"nveglglessink\", \"nvvideo-renderer\") # if not sink: # sys.stderr.write(\" Unable to create egl sink \\n\") print(\"Creating XIMAGESINK \\n\") ximagesink = Gst.ElementFactory.make(\"ximagesink\", \"video-sink\") if not ximagesink: sys.stderr.write(\" Unable to create ximagesink \\n\") print(\"Playing file %s \" %args[1]) source.set_property('location', args[1]) streammux.set_property('width', 1920) streammux.set_property('height', 1080) streammux.set_property('batch-size', 1) streammux.set_property('batched-push-timeout', 4000000) pgie.set_property('config-file-path', \"dstest1_pgie_config.txt\") print(\"Adding elements to Pipeline \\n\") pipeline.add(source) pipeline.add(h264parser) pipeline.add(decoder) pipeline.add(streammux) pipeline.add(pgie) pipeline.add(nvvidconv) pipeline.add(nvosd) pipeline.add(ximagesink) print(\"Linking elements in the Pipeline \\n\") source.link(h264parser) h264parser.link(decoder) # Link elements manually as streammux donot have a static sink pad decoder_srcpad = decoder.get_static_pad(\"src\") if not decoder_srcpad: sys.stderr.write(\" Unable to get source pad of decoder \\n\") streammux_sinkpad = streammux.get_request_pad(\"sink_0\") if not streammux_sinkpad: sys.stderr.write(\" Unable to get the sink pad of streammux \\n\") decoder_srcpad.link(streammux_sinkpad) streammux.link(pgie) pgie.link(nvvidconv) nvvidconv.link(nvosd) nvosd.link(ximagesink) # create an event loop and feed gstreamer bus mesages to it loop = GObject.MainLoop() bus = pipeline.get_bus() bus.add_signal_watch() bus.connect (\"message\", bus_call, loop) # Lets add probe to get informed of the meta data generated, we add probe to # the sink pad of the osd element, since by that time, the buffer would have # had got all the metadata. osdsinkpad = nvosd.get_static_pad(\"sink\") if not osdsinkpad: sys.stderr.write(\" Unable to get sink pad of nvosd \\n\") osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0) # start play back and listen to events print(\"Starting pipeline \\n\") pipeline.set_state(Gst.State.PLAYING) try: loop.run() except: pass # cleanup pipeline.set_state(Gst.State.NULL) if __name__ == '__main__': sys.exit(main(sys.argv)) Could you help me understand what I am doing wrong? Thank you. NOTE: I am using NVIDIA JETSON XAVIER, LINUX, DeepStream 6 and GStreamer 1.0",
        "answers": [
            [
                "You may use nvvideoconvert for copying from NVMM memory into system memory as expected by xvimagesink: ... nvstreammux -&gt; nvinfer -&gt; nvvideoconvert -&gt; nvdsosd -&gt; nvvideoconvert -&gt; xvimagesink"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "unable to use tee and use splitmuxsink to save video ,below is the code and i get linking error while running the below snippet. im using tee to have 2-sinks ( video save and RTSP streaming) nvvidconv_c = \"convertor_%u\" %index print(\"Creating nvvidconv \\n \") nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", nvvidconv_c) if not nvvidconv: sys.stderr.write(\" Unable to create nvvidconv %u\\n\", i) caps_c = \"caps1_%u\" %index caps1 = Gst.ElementFactory.make(\"capsfilter\", caps_c) caps1.set_property(\"caps\", Gst.Caps.from_string(\"video/x-raw(memory:NVMM), format=I420\")) # Make the encoder encoder_c = \"encoder1_%u\" %index print(\"Creating H264 Encoder\") encoder1 = Gst.ElementFactory.make(\"nvv4l2h264enc\", encoder_c) if not encoder1: sys.stderr.write(\" Unable to create encoder\") encoder1.set_property('bitrate', bitrate) caps_c = \"cap2s_%u\" %index caps2 = Gst.ElementFactory.make(\"capsfilter\", caps_c) # caps1.set_property(\"caps\", Gst.Caps.from_string(\"video/x-raw(memory:NVMM), format=I420\")) caps2.set_property(\"caps\", Gst.Caps.from_string(\"video/x-raw, format=I420\")) encoder_c = \"encoder2_%u\" %index print(\"Creating H264 Encoder\") # encoder2 = Gst.ElementFactory.make(\"nvv4l2h264enc\", encoder_c) encoder2 = Gst.ElementFactory.make(\"x264enc\", encoder_c) if not encoder2: sys.stderr.write(\" Unable to create encoder\") encoder2.set_property('bitrate', bitrate) # Make the payload-encode video into RTP packets rtppay_c = \"rtppay_%u\" %index rtppay = Gst.ElementFactory.make(\"rtph264pay\", rtppay_c) print(\"Creating H264 rtppay\") if not rtppay: sys.stderr.write(\" Unable to create rtppay\") # Make the UDP sink updsink_port_num = udp_sink_port_num updsink_port_num += index print(\"\\n updsink_port_num = %u\", updsink_port_num) udpsink_c = \"udpsink_%u\" %index sink = Gst.ElementFactory.make(\"udpsink\", udpsink_c) if not sink: sys.stderr.write(\" Unable to create udpsink\") sink.set_property('host', '224.224.255.255') sink.set_property('port', updsink_port_num) sink.set_property('async', False) sink.set_property('sync', 1) sink.set_property(\"qos\",0) tee=Gst.ElementFactory.make(\"tee\", \"nvsink-tee\") if not tee: sys.stderr.write(\" Unable to create tee \\n\") tee_msg_pad=tee.get_request_pad('src_%u') tee_render_pad=tee.get_request_pad(\"src_%u\") if not tee_msg_pad or not tee_render_pad: sys.stderr.write(\"Unable to get request pads\\n\") queue1 = Gst.ElementFactory.make(\"queue\", \"nvtee-que1\") if not queue1: sys.stderr.write(\" Unable to create queue1 \\n\") queue2 = Gst.ElementFactory.make(\"queue\", \"nvtee-que2\") if not queue2: sys.stderr.write(\" Unable to create queue2 \\n\") splitmuxsink_c = \"splitmuxsink_%u\" %index print(\"Creating splitmuxsink \\n \") splitmuxsink = Gst.ElementFactory.make(\"splitmuxsink\", splitmuxsink_c) # splitmuxsink.set_property('muxer', Gst.ElementFactory.make('matroskamux')) splitmuxsink.set_property('muxer', Gst.ElementFactory.make('qtmux')) splitmuxsink.set_property('location', '/home/ubuntu/sriharsha/videos/testing/segment_%09d.mkv') splitmuxsink.set_property('max-size-time', 10000000000) #10s segments sink_pad = queue1.get_static_pad(\"sink\") tee_msg_pad = tee.get_request_pad('src_%u') tee_render_pad = tee.get_request_pad(\"src_%u\") if not tee_msg_pad or not tee_render_pad: sys.stderr.write(\"Unable to get request pads\\n\") tee_msg_pad.link(sink_pad) sink_pad = queue2.get_static_pad(\"sink\") tee_render_pad.link(sink_pad) try: Gst.Bin.add(nbin, queue, nvvidconv_pre, nvosd, nvvidconv, caps1, encoder1, caps2, encoder2, queue1, queue2 ,tee, splitmuxsink, rtppay, sink) except Exception as e: print(\"error in gst bin add\", e) #link queue.link(nvvidconv_pre) nvvidconv_pre.link(nvosd) nvosd.link(nvvidconv) nvvidconv.link(tee) queue2.link(caps2) caps2.link(encoder2) encoder2.link(encoder2) encoder2.link(splitmuxsink) queue1.link(caps1) caps1.link(encoder1) encoder1.link(rtppay) rtppay.link(sink) #ghostpad pad = queue.get_static_pad(\"sink\") ghost_pad = Gst.GhostPad.new(\"sink\", pad) nbin.add_pad(ghost_pad) and i get the below error : Error: gst-stream-error-quark: Internal data stream error. (1): gstqueue.c(988): gst_queue_handle_sink_event (): /GstPipeline:pipeline0/GstBin:sink-bin-00/GstQueue:queue_sink_0: streaming stopped, reason not-linked (-1)",
        "answers": [
            [
                "Not sure for your case, but after tee splitmuxsink might need to have async-handling property enabled."
            ],
            [
                "I agree with SeB. Setting async-handling=true fixed a similar issue for me. However I also used factories and had async-finalize=true enabled."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I have a python script running real time inference on frames garbed from a ZED2i camera in 1080p@30fps on an nvidia jetson xavier nx. As I'm trying to boost up the performance I was wondering if there is an interface between ZED SDK and DeepStream SDK? More info: object detector: darknet yolov4tiny 416X416 jetpack 4.6 power mode: 20W 6cores",
        "answers": [
            [
                "Here is the gstream cmd. gst-launch-1.0 zedsrc stream-type=0 ! videoconvert ! 'video/x-raw,format=(string)YUY2' ! nvvidconv ! 'video/x-raw(memory:NVMM),format=(string)NV12,width=1280,height=720' ! nvvidconv ! mux.sink_0 nvstreammux live-source=1 name=mux batch-size=1 width=1280 height=720 ! nvinfer config-file-path=/opt/nvidia/deepstream/deepstream-6.0/sources/objectDetector_Yolo/config_infer_primary_yoloV3.txt ! nvvideoconvert ! nvdsosd ! nvegltransform ! nveglglessink sync=0 there is a guy out there tried this already. He provides youtube video github"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Gstreamer is giving me errors reporting that it cannot load plugins because their files do not exists. However, these files do exists. Output: Frames will be saved in frames (gst-plugin-scanner:22): GStreamer-WARNING **: 20:59:31.873: Failed to load plugin '/usr/lib/x86_64-linux-gnu/gstreamer-1.0/deepstream/libnvdsgst_udp.so': librivermax.so.0: cannot open shared object file: No such file or directory (gst-plugin-scanner:22): GStreamer-WARNING **: 20:59:31.938: Failed to load plugin '/usr/lib/x86_64-linux-gnu/gstreamer-1.0/deepstream/libnvdsgst_inferserver.so': libtritonserver.so: cannot open shared object file: No such file or directory Creating Pipeline Creating streamux Creating source_bin 0 Creating source bin source-bin-00 Creating Pgie Creating nvvidconv1 Creating filter1 Creating tiler Creating nvvidconv Creating nvosd Creating EGLSink Atleast one of the sources is live Adding elements to Pipeline Linking elements in the Pipeline Now playing... 1 : rtsp://wowzaec2demo.streamlock.net/vod/mp4:BigBuckBunny_115k.mov Starting pipeline ^CExiting app In order to reproduce the issue you can create the two following files in the same directory. Dockerfile: FROM nvcr.io/nvidia/deepstream:6.0-samples ENV GIT_SSL_NO_VERIFY=1 RUN apt install -y git python-dev python3 python3-pip python3.6-dev python3.8-dev cmake g++ build-essential \\ libglib2.0-dev libglib2.0-dev-bin python-gi-dev libtool m4 autoconf automake wget RUN apt install -y libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev RUN pip3 install --upgrade pip RUN pip3 install numpy opencv-python RUN echo '/usr/local/lib/gstreamer-1.0' &gt;&gt; /etc/ld.so.conf RUN ldconfig RUN cd /opt/nvidia/deepstream/deepstream-6.0/sources &amp;&amp; \\ git clone https://github.com/NVIDIA-AI-IOT/deepstream_python_apps.git RUN cd /opt/nvidia/deepstream/deepstream-6.0/sources/deepstream_python_apps &amp;&amp; \\ git submodule update --init RUN wget https://github.com/NVIDIA-AI-IOT/deepstream_python_apps/releases/download/v1.1.0/pyds-1.1.0-py3-none-linux_x86_64.whl RUN pip3 install pyds-1.1.0-py3-none-linux_x86_64.whl COPY start_example.sh start_example.sh RUN rm -rf ~/.cache/gstreamer-1.0/ start_example.sh : cd /opt/nvidia/deepstream/deepstream-6.0/sources/deepstream_python_apps/apps/deepstream-imagedata-multistream python3 deepstream_imagedata-multistream.py rtsp://wowzaec2demo.streamlock.net/vod/mp4:BigBuckBunny_115k.mov frames Then run: docker build . -t deepstream-custom docker run --gpus '\"'device=0'\"' -it --rm -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY deepstream-custom sh start_example.sh Files librivermax.so.0 and libtritonserver.so can't be find in the system.",
        "answers": [
            [
                "It turns out that this is not an issue. If you wan to to use Triton Server there is a dedicated docker image. As for Rivermax, it is mentioned in the doc if you want to use it, but if you don't, you can safely ignore the warning."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am using jetson NX xavier kit having cuda 10.2.89, open Cv 4.1.1 and tensorRT 7.1.3 . Trying to install pytorch. Tried installing with this line conda install pytorch torchvision cpuonly -c pytorch but when i write this line import torch It throws an error by saying that module not installed. How I can verify if pytorch has been installed correctly.",
        "answers": [
            [
                "Try this one conda install -c pytorch pytorch After executing this command, you need to enter yes(if prompted in the command line) for installing all the related packages. If there is no conflict while installing the libraries, the PyTorch library will be installed. To check if it is properly installed or not, type the command python in your command line and type import torch to check if it is properly installed or not."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I would like to know the CLI command and the script to extract images using FFmpeg of two videos simultaneously and not sequentially. For example - the first frame of the first video the first frame of the second video and the second frame of first video and then the second frame of second video and so on. Videos will be in mp4 or asf and image should be in jpeg",
        "answers": [
            [
                "Use the framepack or interleave filters: ffmpeg -i video0.mp4 -i video1.asf -filter_complex framepack=frameseq output_%04d.jpg Both inputs should have the same width, height, and timebase. Outputs will be named output_0001.jpg, output_0002.jpg, etc. Additional info: FFmpeg image muxer documentation How can I control JPEG image quality from a video file with ffmpeg?"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a working Gstreamer pipeline using RTSP input streams. To handle these given RTSP input streams, the uridecobin element is used. My goal is to reconnect to the RTSP input streams when internet connection is unstable. When the internet connection is down for only few seconds and then it is up, then the pipeline starts to receive the frames again and everything is ok. When the internet connection is down for &gt;20 seconds I get GST_MESSAGE_EOS. I tried to find some timeout variable in every element generated by uridecodebin, but I did not find it. Do you have any hint which element has this timeout variable and how to set it? If it is not possible to set such timeout variable, is there any way to block GST_MESSAGE_EOS? Because when I receive GST_MESSAGE_EOS in bus, I try to remove uridecodebin from the pipeline and create a new one. But it does not work for me when GST_MESSAGE_EOS is received (When I try to remove uridecodebin from the pipeline and create a new one during normal state, it works).",
        "answers": [
            [
                "I found the way how to block GST_MESSAGE_EOS. Create the following function to drop GST_EVENT_EOS: GstPadProbeReturn eos_probe_cb(GstPad *pad, GstPadProbeInfo *info, gpointer u_data) { if (GST_EVENT_TYPE(GST_PAD_PROBE_INFO_DATA(info)) == GST_EVENT_EOS) { return GST_PAD_PROBE_DROP; } return GST_PAD_PROBE_OK; } And then just add this function to some GstPad of your elements: gst_pad_add_probe(src_pad, GST_PAD_PROBE_TYPE_EVENT_DOWNSTREAM, eos_probe_cb, (gpointer) user_data, NULL);"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "If i train a model with the NVidia DeepStream SDK on linux can i use it for inference on Windows? I know that the SDK is not available for windows but is it necessary for inference? I prefer a solution without docker, but also interested in dockered version.",
        "answers": [
            [
                "First of all, as far as I know, DeepStream only offers you the necessary code tools to perform inference in an optimized way on Nvidia hardware (GPU, jetson). This means that it is not a training tool. For this you must use TLT (Transfer Learning Toolkit), which I currently understand is called TAO (Train, Adapt and Optimize). To build DeepStream-based applications, you need the SDK. However, for deployment, the most recommended route is the Docker images offered by Nvidia at https://ngc.nvidia.com/catalog/containers. In the latter case, the SDK is not necessary since the image has everything necessary to run DeepStream applications."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am using Nvidia Jetson Nano to stream and play videos through a python code. But what I have seen is that my CPU usage is going pretty high, and it is not having much impact on its GPU. So this will cause me a problem as I will not be able to utilize my GPU properly. My purpose of using this device is to get better speed and performance, but I found that I am not able to utilize my GPU well. Please someone help me what I can do such that my code will be processed mostly on GPU thus removing burden from CPU. I am using cv2 module of python to play the video.",
        "answers": [
            [
                "You probably need to be more selective on what OpenCV modules you use. You need to use OpenCV modules with CUDA support if you want them to utilize the GPU. Follow for example this guide to get started -&gt; https://learnopencv.com/getting-started-opencv-cuda-module/"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to know if someone can help with a Deepstream model code that takes a video in the source and outputs frames of that particular video in jpg. It would be helpful if you can share the Gstreamer CPP or Python code as well.",
        "answers": [
            [
                "You can use Deepstream's official Python segmentation example and modify it for your case of reading and saving JPEG frames. The following pipeline should work: source -&gt; jpegparser -&gt; decoder -&gt; streammux -&gt; fakesink. You can attach your probe saving function directly to fakesink instead of seg component of the original pipeline. Also, for how to create the fakesink component you can check another Python example on this line"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a deepstream pipeline which goes like this: reading and processing video --&gt; queue --&gt; tee --&gt; queue --&gt; display |----&gt; queue --&gt; appsink The appsink has a callback function attached to the \"new-sample\" bus messages to get a new sample from the buffer and process it. This operation inside the appsink is slow. I want everything running in the appsink to run async and to not slow down the display, is this possible?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to send some object metadata(class_id, confidence value, etc\u2026) to another PC when the object is detected but it causes FPS drops and the stream is frozen. Which parallel programming technique I should use to solve it? Can you give me an example of it? Checking if detected object in the class_dict: if obj_meta.class_id in class_dict: send_one(obj_meta.class_id) I am using this function to send class_id message. from __future__ import print_function import can def send_one(class_id): bus = can.interface.Bus() bus = can.interface.Bus(bustype='socketcan', channel='vcan0', bitrate=250000) msg = can.Message(arbitration_id=0xc0ffee, **data=[class_id]**, is_extended_id=True) try: bus.send(msg) print(\"Message sent on {}\".format(bus.channel_info)) except can.CanError: print(\"Message NOT sent\")",
        "answers": [
            [
                "I am not sure what's your usecase but I would recommend to have a look at msgbroker (DS plugin) for msg passing between the applications"
            ],
            [
                "A little bit more code would help, but I'm assuming you are (were?) doing the check inside a gstreamer buffer probe. Buffer probe blocks buffer downstream so no new buffers keep coming until you've disposed of it. A: using external service: use the msgbroker element to produce messages and inject into alternative service (eg rabbit, kafka). See reference implementation here. Then, use a service-specific consumer to process the data (and call your send_one). B: from python: You should extract metadata as quickly as possible, and then process it from outside. from queue import Queue, Empty from threading import Thread q = Queue() ... #in buffer probe: if obj_meta.class_id in class_dict: q.put(obj_meta.class_id) ... def consume(): while True: try: data = q.get(block=True, timeout=1) except Empty: pass ... consumer = Thread(target=consume) consumer.start() you could improve from this eg by reading in batches, running multiple consumer threads, etc."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "\u2022 Hardware Platform (Jetson / GPU) Jetson Nano 4GB, Ubuntu 18.4 \u2022 DeepStream Version marketplace.azurecr.io/nvidia/deepstream-iot2-l4t:latest \u2022 JetPack Version 4.3 \u2022 Issue Type Output inference class is different from Model class \u2022 How to reproduce the issue ? On DeepStream, deploy a object detection ONNX model. My model is ONNX model exported from Azure Custom Vision. My label file has 2 classes - 'Mask', 'No_Mask'. Deployment works fine and I am able to execute my model using DeepStream. However, output inference class I am getting as 'Vehicle' and 'No_Mask'. Can you please help me understand why I am getting output inference label as \"Vehicle\" when it is not there in my Model. Sample output inference log {\"log\":\" \"1|324|23|380|61|Vehicle|#|||||||0\"\\n\",\"stream\":\"stdout\",\"time\":\"2021-01-05T16:15:15.614591738Z\"} {\"log\":\" \"1|324|23|380|61|Vehicle|#|||||||0\"\\n\",\"stream\":\"stdout\",\"time\":\"2021-01-05T16:15:15.614790179Z\"} {\"log\":\" \"2|141|15|365|161|No Mask\"\\n\",\"stream\":\"stdout\",\"time\":\"2021-01-05T16:15:15.614221209Z\"}",
        "answers": [
            [
                "You've most probably specified wrong labels file or the classes in it are wrong. It's provided in labelfile-path as labelfile-path=labels.txt"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Trying to run nvidia\u2019s deepstream5.0 sdk (sample program) on ubuntu 18.04 by following the document (DeepStream Development Guide \u2014 DeepStream DeepStream Version: 5.0 documentation). Hardware Platform (Jetson / GPU)=GPU NVIDIA GEFORCE RTX 2060 TensorRT Version=7.0 NVIDIA GPU Driver Version (valid for GPU only):450.102 Issue Type( questions, new requirements, bugs)=bugs GCC=7.5 PYTHON 3.7 CUDNN 7.6.5 CUDA 10.2 The application is installed in the path: \u201c/opt/nvidia/deepstream/deepstream-5.0/\u201d. The execution command is \"deepstream-app -c \" Example: deepstream-app -c /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/source30_1080p_dec_infer-resnet_tiled_display_int8.txt However got segmentation fault just after opening a blank screen and closing suddenly ERROR: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:1523 Deserialize engine failed because file path: /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Secondary_CarMake/resnet18.caffemodel_b16_gpu0_int8.engine open error 0:00:01.788894483 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;secondary_gie_2&gt; NvDsInferContext[UID 6]: Warning from NvDsInferContextImpl::deserializeEngineAndBackend() &lt;nvdsinfer_context_impl.cpp:1690&gt; [UID = 6]: deserialize engine from file :/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Secondary_CarMake/resnet18.caffemodel_b16_gpu0_int8.engine failed 0:00:01.788911328 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;secondary_gie_2&gt; NvDsInferContext[UID 6]: Warning from NvDsInferContextImpl::generateBackendContext() &lt;nvdsinfer_context_impl.cpp:1797&gt; [UID = 6]: deserialize backend context from engine from file :/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Secondary_CarMake/resnet18.caffemodel_b16_gpu0_int8.engine failed, try rebuild 0:00:01.788917862 9829 0x5594636fc490 INFO nvinfer gstnvinfer.cpp:619:gst_nvinfer_logger:&lt;secondary_gie_2&gt; NvDsInferContext[UID 6]: Info from NvDsInferContextImpl::buildModel() &lt;nvdsinfer_context_impl.cpp:1715&gt; [UID = 6]: Trying to create engine from model files Warning: Flatten layer ignored. TensorRT implicitly flattens input to FullyConnected layers, but in other circumstances this will result in undefined behavior. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Reading Calibration Cache for calibrator: EntropyCalibration2 INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Generated calibration scales using calibration cache. Make sure that calibration cache has latest scales. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: To regenerate calibration cache, please delete the existing one. TensorRT will generate a new calibration cache. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Detected 1 inputs and 1 output network tensors. ERROR: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:1495 Serialize engine failed because of file path: /opt/nvidia/deepstream/deepstream-5.0/samples/models/Secondary_CarMake/resnet18.caffemodel_b16_gpu0_int8.engine opened error 0:00:11.045161759 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;secondary_gie_2&gt; NvDsInferContext[UID 6]: Warning from NvDsInferContextImpl::buildModel() &lt;nvdsinfer_context_impl.cpp:1743&gt; [UID = 6]: failed to serialize cude engine to file: /opt/nvidia/deepstream/deepstream-5.0/samples/models/Secondary_CarMake/resnet18.caffemodel_b16_gpu0_int8.engine WARNING: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:36 [TRT]: Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles INFO: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:685 [Implicit Engine Info]: layers num: 2 0 INPUT kFLOAT input_1 3x224x224 1 OUTPUT kFLOAT predictions/Softmax 20x1x1 0:00:11.054222978 9829 0x5594636fc490 INFO nvinfer gstnvinfer_impl.cpp:313:notifyLoadModelStatus:&lt;secondary_gie_2&gt; [UID 6]: Load new model:/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/config_infer_secondary_carmake.txt sucessfully ERROR: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:1523 Deserialize engine failed because file path: /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Secondary_CarColor/resnet18.caffemodel_b16_gpu0_int8.engine open error 0:00:11.054352982 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;secondary_gie_1&gt; NvDsInferContext[UID 5]: Warning from NvDsInferContextImpl::deserializeEngineAndBackend() &lt;nvdsinfer_context_impl.cpp:1690&gt; [UID = 5]: deserialize engine from file :/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Secondary_CarColor/resnet18.caffemodel_b16_gpu0_int8.engine failed 0:00:11.054360902 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;secondary_gie_1&gt; NvDsInferContext[UID 5]: Warning from NvDsInferContextImpl::generateBackendContext() &lt;nvdsinfer_context_impl.cpp:1797&gt; [UID = 5]: deserialize backend context from engine from file :/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Secondary_CarColor/resnet18.caffemodel_b16_gpu0_int8.engine failed, try rebuild 0:00:11.054365641 9829 0x5594636fc490 INFO nvinfer gstnvinfer.cpp:619:gst_nvinfer_logger:&lt;secondary_gie_1&gt; NvDsInferContext[UID 5]: Info from NvDsInferContextImpl::buildModel() &lt;nvdsinfer_context_impl.cpp:1715&gt; [UID = 5]: Trying to create engine from model files Warning: Flatten layer ignored. TensorRT implicitly flattens input to FullyConnected layers, but in other circumstances this will result in undefined behavior. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Reading Calibration Cache for calibrator: EntropyCalibration2 INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Generated calibration scales using calibration cache. Make sure that calibration cache has latest scales. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: To regenerate calibration cache, please delete the existing one. TensorRT will generate a new calibration cache. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Detected 1 inputs and 1 output network tensors. ERROR: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:1495 Serialize engine failed because of file path: /opt/nvidia/deepstream/deepstream-5.0/samples/models/Secondary_CarColor/resnet18.caffemodel_b16_gpu0_int8.engine opened error 0:00:19.492522201 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;secondary_gie_1&gt; NvDsInferContext[UID 5]: Warning from NvDsInferContextImpl::buildModel() &lt;nvdsinfer_context_impl.cpp:1743&gt; [UID = 5]: failed to serialize cude engine to file: /opt/nvidia/deepstream/deepstream-5.0/samples/models/Secondary_CarColor/resnet18.caffemodel_b16_gpu0_int8.engine WARNING: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:36 [TRT]: Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles INFO: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:685 [Implicit Engine Info]: layers num: 2 0 INPUT kFLOAT input_1 3x224x224 1 OUTPUT kFLOAT predictions/Softmax 12x1x1 0:00:19.497783953 9829 0x5594636fc490 INFO nvinfer gstnvinfer_impl.cpp:313:notifyLoadModelStatus:&lt;secondary_gie_1&gt; [UID 5]: Load new model:/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/config_infer_secondary_carcolor.txt sucessfully ERROR: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:1523 Deserialize engine failed because file path: /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Secondary_VehicleTypes/resnet18.caffemodel_b16_gpu0_int8.engine open error 0:00:19.497944601 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;secondary_gie_0&gt; NvDsInferContext[UID 4]: Warning from NvDsInferContextImpl::deserializeEngineAndBackend() &lt;nvdsinfer_context_impl.cpp:1690&gt; [UID = 4]: deserialize engine from file :/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Secondary_VehicleTypes/resnet18.caffemodel_b16_gpu0_int8.engine failed 0:00:19.497954066 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;secondary_gie_0&gt; NvDsInferContext[UID 4]: Warning from NvDsInferContextImpl::generateBackendContext() &lt;nvdsinfer_context_impl.cpp:1797&gt; [UID = 4]: deserialize backend context from engine from file :/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Secondary_VehicleTypes/resnet18.caffemodel_b16_gpu0_int8.engine failed, try rebuild 0:00:19.497959157 9829 0x5594636fc490 INFO nvinfer gstnvinfer.cpp:619:gst_nvinfer_logger:&lt;secondary_gie_0&gt; NvDsInferContext[UID 4]: Info from NvDsInferContextImpl::buildModel() &lt;nvdsinfer_context_impl.cpp:1715&gt; [UID = 4]: Trying to create engine from model files Warning: Flatten layer ignored. TensorRT implicitly flattens input to FullyConnected layers, but in other circumstances this will result in undefined behavior. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Reading Calibration Cache for calibrator: EntropyCalibration2 INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Generated calibration scales using calibration cache. Make sure that calibration cache has latest scales. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: To regenerate calibration cache, please delete the existing one. TensorRT will generate a new calibration cache. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Detected 1 inputs and 1 output network tensors. ERROR: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:1495 Serialize engine failed because of file path: /opt/nvidia/deepstream/deepstream-5.0/samples/models/Secondary_VehicleTypes/resnet18.caffemodel_b16_gpu0_int8.engine opened error 0:00:27.394531547 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;secondary_gie_0&gt; NvDsInferContext[UID 4]: Warning from NvDsInferContextImpl::buildModel() &lt;nvdsinfer_context_impl.cpp:1743&gt; [UID = 4]: failed to serialize cude engine to file: /opt/nvidia/deepstream/deepstream-5.0/samples/models/Secondary_VehicleTypes/resnet18.caffemodel_b16_gpu0_int8.engine WARNING: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:36 [TRT]: Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles INFO: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:685 [Implicit Engine Info]: layers num: 2 0 INPUT kFLOAT input_1 3x224x224 1 OUTPUT kFLOAT predictions/Softmax 6x1x1 0:00:27.401846636 9829 0x5594636fc490 INFO nvinfer gstnvinfer_impl.cpp:313:notifyLoadModelStatus:&lt;secondary_gie_0&gt; [UID 4]: Load new model:/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/config_infer_secondary_vehicletypes.txt sucessfully gstnvtracker: Loading low-level lib at /opt/nvidia/deepstream/deepstream-5.0/lib/libnvds_mot_klt.so gstnvtracker: Optional NvMOT_RemoveStreams not implemented gstnvtracker: Batch processing is OFF gstnvtracker: Past frame output is OFF ERROR: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:1523 Deserialize engine failed because file path: /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Primary_Detector/resnet10.caffemodel_b4_gpu0_int8.engine open error 0:00:27.405130601 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;primary_gie&gt; NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::deserializeEngineAndBackend() &lt;nvdsinfer_context_impl.cpp:1690&gt; [UID = 1]: deserialize engine from file :/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Primary_Detector/resnet10.caffemodel_b4_gpu0_int8.engine failed 0:00:27.405139410 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;primary_gie&gt; NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::generateBackendContext() &lt;nvdsinfer_context_impl.cpp:1797&gt; [UID = 1]: deserialize backend context from engine from file :/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Primary_Detector/resnet10.caffemodel_b4_gpu0_int8.engine failed, try rebuild 0:00:27.405144384 9829 0x5594636fc490 INFO nvinfer gstnvinfer.cpp:619:gst_nvinfer_logger:&lt;primary_gie&gt; NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::buildModel() &lt;nvdsinfer_context_impl.cpp:1715&gt; [UID = 1]: Trying to create engine from model files INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Reading Calibration Cache for calibrator: EntropyCalibration2 INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Generated calibration scales using calibration cache. Make sure that calibration cache has latest scales. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: To regenerate calibration cache, please delete the existing one. TensorRT will generate a new calibration cache. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output. INFO: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:39 [TRT]: Detected 1 inputs and 2 output network tensors. ERROR: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:1495 Serialize engine failed because of file path: /opt/nvidia/deepstream/deepstream-5.0/samples/models/Primary_Detector/resnet10.caffemodel_b4_gpu0_int8.engine opened error 0:00:32.442386732 9829 0x5594636fc490 WARN nvinfer gstnvinfer.cpp:616:gst_nvinfer_logger:&lt;primary_gie&gt; NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::buildModel() &lt;nvdsinfer_context_impl.cpp:1743&gt; [UID = 1]: failed to serialize cude engine to file: /opt/nvidia/deepstream/deepstream-5.0/samples/models/Primary_Detector/resnet10.caffemodel_b4_gpu0_int8.engine WARNING: \u2026/nvdsinfer/nvdsinfer_func_utils.cpp:36 [TRT]: Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles INFO: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:685 [Implicit Engine Info]: layers num: 3 0 INPUT kFLOAT input_1 3x368x640 1 OUTPUT kFLOAT conv2d_bbox 16x23x40 2 OUTPUT kFLOAT conv2d_cov/Sigmoid 4x23x40 0:00:32.447113083 9829 0x5594636fc490 INFO nvinfer gstnvinfer_impl.cpp:313:notifyLoadModelStatus:&lt;primary_gie&gt; [UID 1]: Load new model:/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/config_infer_primary.txt sucessfully Runtime commands: h: Print this help q: Quit p: Pause r: Resume NOTE: To expand a source in the 2D tiled display and view object details, left-click on the source. To go back to the tiled display, right-click anywhere on the window. **PERF: FPS 0 (Avg) FPS 1 (Avg) FPS 2 (Avg) FPS 3 (Avg) **PERF: 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) 0.00 (0.00) ** INFO: &lt;bus_callback:181&gt;: Pipeline ready ** INFO: &lt;bus_callback:167&gt;: Pipeline running Segmentation fault (core dumped) My nvidia driver and cuda version shown below: My nvidia driver and cuda version shown below:",
        "answers": [
            [
                "A bit late with the answer. ERROR: \u2026/nvdsinfer/nvdsinfer_model_builder.cpp:1523 Deserialize engine failed because file path: /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/\u2026/\u2026/models/Primary_Detector/resnet10.caffemodel_b4_gpu0_int8.engine open error The error message already give you the clue that is pointing to an engine file that does not exist on the path. Probably provide the full path to the engine file in the config file."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I installed Azure IoTEdge on my Jetson nano and provisioned the IoTEdge runtime to the device using the security daemon file at /etc/iotedge/config.yaml . I see that the status of the IoT Edge Daemon is active (running) and all the 6 IoTEdge runtime modules that I'm using are running: console image -----------------NAME -------------------- STATUS--------- CameraTaggingModule ---------- running DeepStreamAnalytics ------------ running NVIDIADeepStreamSDK ------- running azureblobstorageoniotedge ---- running edgeAgent -------------------------- running edgeHub ---------------------------- running But my NVIDIADeepStreamSDK module doesn't want to recieve messages from my RTSP Camera that is also in the same network as the Jetson, I see it logs of this module and it says: ERROR from src_elem0: Could not read from resource. Debug info: gstrtspsrc.c(5917): gst_rtsp_src_receive_response (): /GstPipeline:pipeline/GstBin:multi_src_bin/GstBin:src_sub_bin0/GstRTSPSrc:src_elem0: Could not receive message. (Timeout while waiting for server response) The NVIDIADeepStreamSDK module can recieve messages succesfully if I use a public RTSP like BigBugBunny rtsp, so I guess the problem might has to do with some configuration of the local network. or should I make my RTSP Camera public? or should I provide some json file with DNS specificacion?, either case, what's the best way to approach this? Any sugestion is appreciated to make this work, thanks.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Please provide complete information as applicable to your setup. \u2022 Hardware Platform (Jetson / GPU) Jetson \u2022 DeepStream Version 5.0 \u2022 JetPack Version (valid for Jetson only) 4.4 \u2022 TensorRT Version 7.0 \u2022 NVIDIA GPU Driver Version (valid for GPU only) \u2022 Issue Type( questions, new requirements, bugs) questions \u2022 How to reproduce the issue ? (This is for bugs. Including which sample app is using, the configuration files content, the command line used and other details for reproducing) print(\"Linking demux to the rtppayload in the Pipeline \\n\") for i in range(number_of_sources): demux_srcpad = streamdemux.get_request_pad(\"src_%u\"%i) if not demux_srcpad: sys.stderr.write(\"Unable to get the src pad of streamdemux \\n\") sinkpad = rtppayload_list[i].get_static_pad(\"sink\") if not sinkpad: sys.stderr.write(\" Unable to get sink pad of rtppayload \\n\") demux_srcpad.link(sinkpad) \u2022 Requirement details( This is for new requirement. Including the module name-for which plugin or for which sample application, the function description) I am trying to create source pads for the nvstreamdemux element at run-time and link to several rtph264pay elements which reside inside the list : rtppayload_list. The above given code results in the following error: gi.overrides.Gst.LinkError: Any help would be appreciated. Thanks !",
        "answers": [
            [
                "you can only link element and pads that are compatible with each other. in you case, nvstreamdemux outputs raw data in NV12 or RGBA format at its source pad whereas rtph264pay takes h264 encoded stream at its input sink pad. So these two are incompatible with each other. You need to link nvstreamdemux to some element that encodes raw data into h264 like nvv4l2h264enc and then in turn link nvv4l2h264enc to rtph264pay. so your pipeline should look like nvstreamdemux-&gt;nvv4l2h264enc-&gt;rtph264pay"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Using the deepstream_app_config_yoloV3.txt and TRT engine file fp16.engine, I got a FPS of 15 on yolov3-spp and a FPS of 30 on yolov3-tiny . My question is this: Is there a way to recreate the fps I got running the deepstream_app_config_yoloV3.txt onto the python deepstream custom apps ? As I want to extract meta-data like detected object name and Bounding box coordinates. If I can do this on the deepstream_app_config_yoloV3.txt app (not the custom-app.py script) I will be more than happy to ditch the python script My setup: Jetson Nano B01 Deep-stream 5.0 Jetpack 4.4 Camera: CSI Pi-camera V2 This is a modified version of deepstream-app-test1 where I changed out the source for the Pi-cam instead of video file. While running the custom App I am getting about 5 fps due to the wired batching issue. Is there anything I should modify to stop this batching and increase fps ? I had added a parameter of 'fps =30/1' to see if it would make a difference but it didn't stop the batching My code: import sys sys.path.append('../') import gi gi.require_version('Gst', '1.0') from gi.repository import GObject, Gst from common.is_aarch_64 import is_aarch64 from common.bus_call import bus_call import pyds PGIE_CLASS_ID_TOOTHBRUSH = 80 PGIE_CLASS_ID_HAIR_DRYER = 79 PGIE_CLASS_ID_TEDDY_BEAR = 78 PGIE_CLASS_ID_SCISSORS = 77 PGIE_CLASS_ID_VASE = 76 PGIE_CLASS_ID_CLOCK = 75 PGIE_CLASS_ID_BOOK = 74 PGIE_CLASS_ID_REFRIGERATOR = 73 PGIE_CLASS_ID_SINK = 72 PGIE_CLASS_ID_TOASTER = 71 PGIE_CLASS_ID_OVEN = 70 PGIE_CLASS_ID_MICROWAVE = 69 PGIE_CLASS_ID_CELL_PHONE = 68 PGIE_CLASS_ID_KEYBOARD = 67 PGIE_CLASS_ID_REMOTE = 66 PGIE_CLASS_ID_MOUSE = 65 PGIE_CLASS_ID_LAPTOP = 64 PGIE_CLASS_ID_TVMONITOR = 63 PGIE_CLASS_ID_TOILET = 62 PGIE_CLASS_ID_DININGTABLE= 61 PGIE_CLASS_ID_BED = 60 PGIE_CLASS_ID_POTTEDPLANT = 59 PGIE_CLASS_ID_SOFA = 58 PGIE_CLASS_ID_CHAIR = 57 PGIE_CLASS_ID_CAKE = 56 PGIE_CLASS_ID_DONUT = 55 PGIE_CLASS_ID_PIZZA = 54 PGIE_CLASS_ID_HOT_DOG = 53 PGIE_CLASS_ID_CARROT = 52 PGIE_CLASS_ID_BROCCOLI = 51 PGIE_CLASS_ID_ORANGE = 50 PGIE_CLASS_ID_SANDWICH = 49 PGIE_CLASS_ID_APPLE = 48 PGIE_CLASS_ID_BANANA = 47 PGIE_CLASS_ID_BOWL = 46 PGIE_CLASS_ID_SPOON = 45 PGIE_CLASS_ID_KNIFE = 44 PGIE_CLASS_ID_FORK = 43 PGIE_CLASS_ID_CUP = 42 PGIE_CLASS_ID_WINE_GLASS = 41 PGIE_CLASS_ID_BOTTLE = 40 PGIE_CLASS_ID_TENNIS_RACKET = 39 PGIE_CLASS_ID_SURFBOARD = 38 PGIE_CLASS_ID_SKATEBOARD = 37 PGIE_CLASS_ID_BASEBALL_GLOVE = 36 PGIE_CLASS_ID_BASEBALL_BAT = 35 PGIE_CLASS_ID_KITE = 34 PGIE_CLASS_ID_SPORTS_BALL = 33 PGIE_CLASS_ID_SNOWBOARD = 32 PGIE_CLASS_ID_SKIS = 31 PGIE_CLASS_ID_FRISBEE = 30 PGIE_CLASS_ID_SUITCASE = 29 PGIE_CLASS_ID_TIE = 28 PGIE_CLASS_ID_HANDBAG = 27 PGIE_CLASS_ID_UMBRELLA = 26 PGIE_CLASS_ID_BACKPACK = 25 PGIE_CLASS_ID_UMBRELLA = 24 PGIE_CLASS_ID_GIRAFFE = 23 PGIE_CLASS_ID_ZEBRA = 22 PGIE_CLASS_ID_BEAR = 21 PGIE_CLASS_ID_ELEPHANT = 20 PGIE_CLASS_ID_COW = 19 PGIE_CLASS_ID_SHEEP = 18 PGIE_CLASS_ID_HORSE = 17 PGIE_CLASS_ID_DOG = 16 PGIE_CLASS_ID_CAT = 15 PGIE_CLASS_ID_BIRD = 14 PGIE_CLASS_ID_BENCH = 13 PGIE_CLASS_ID_PARKING_METER = 12 PGIE_CLASS_ID_STOP_SIGN = 11 PGIE_CLASS_ID_FIRE_HYDRANT = 10 PGIE_CLASS_ID_TRAFFIC_LIGHT = 9 PGIE_CLASS_ID_BOAT = 8 PGIE_CLASS_ID_TRUCK = 7 PGIE_CLASS_ID_TRAIN = 6 PGIE_CLASS_ID_BUS = 5 PGIE_CLASS_ID_AEROPLANE = 4 PGIE_CLASS_ID_MOTORBIKE = 3 PGIE_CLASS_ID_VEHICLE = 2 PGIE_CLASS_ID_BICYCLE = 1 PGIE_CLASS_ID_PERSON = 0 pgie_classes_str= [\"Toothbrush\", \"Hair dryer\", \"Teddy bear\",\"Scissors\",\"Vase\", \"Clock\", \"Book\",\"Refrigerator\", \"Sink\", \"Toaster\",\"Oven\",\"Microwave\", \"Cell phone\", \"Keyboard\",\"Remote\", \"Mouse\", \"Laptop\",\"Tvmonitor\",\"Toilet\", \"Diningtable\", \"Bed\",\"Pottedplant\", \"Sofa\", \"Chair\",\"Cake\",\"Donut\", \"Pizza\", \"Hot dog\",\"Carrot\", \"Broccli\", \"Orange\",\"Sandwich\",\"Apple\", \"Banana\", \"Bowl\",\"Spoon\", \"Knife\", \"Fork\",\"Cup\",\"Wine Glass\", \"Bottle\", \"Tennis racket\",\"Surfboard\", \"Skateboard\", \"Baseball glove\",\"Baseball bat\",\"Kite\", \"Sports ball\", \"Snowboard\",\"Skis\", \"Frisbee\", \"Suitcase\",\"Tie\",\"Handbag\", \"Umbrella\", \"Backpack\",\"Giraffe\", \"Zebra\", \"Bear\",\"Elephant\",\"Cow\", \"Sheep\", \"Horse\",\"Dog\", \"Cat\", \"Bird\",\"Bench\",\"Parking meter\", \"Stop sign\", \"Fire hydrant\",\"Traffic light\", \"Boat\", \"Truck\",\"Train\",\"Bus\", \"Areoplane\", \"Motorbike\",\"Car\", \"Bicycle\", \"Person\"] def osd_sink_pad_buffer_probe(pad,info,u_data): frame_number=0 #Intiallizing object counter with 0. obj_counter = { PGIE_CLASS_ID_TOOTHBRUSH:0, PGIE_CLASS_ID_HAIR_DRYER:0, PGIE_CLASS_ID_TEDDY_BEAR:0, PGIE_CLASS_ID_SCISSORS:0, PGIE_CLASS_ID_VASE:0, PGIE_CLASS_ID_CLOCK:0, PGIE_CLASS_ID_BOOK:0, PGIE_CLASS_ID_REFRIGERATOR:0, PGIE_CLASS_ID_SINK:0, PGIE_CLASS_ID_TOASTER:0, PGIE_CLASS_ID_OVEN:0, PGIE_CLASS_ID_MICROWAVE:0, PGIE_CLASS_ID_CELL_PHONE:0, PGIE_CLASS_ID_KEYBOARD:0, PGIE_CLASS_ID_REMOTE:0, PGIE_CLASS_ID_MOUSE:0, PGIE_CLASS_ID_LAPTOP:0, PGIE_CLASS_ID_TVMONITOR:0, PGIE_CLASS_ID_TOILET:0, PGIE_CLASS_ID_DININGTABLE:0, PGIE_CLASS_ID_BED:0, PGIE_CLASS_ID_POTTEDPLANT:0, PGIE_CLASS_ID_SOFA:0, PGIE_CLASS_ID_CHAIR:0, PGIE_CLASS_ID_CAKE:0, PGIE_CLASS_ID_DONUT:0, PGIE_CLASS_ID_PIZZA:0, PGIE_CLASS_ID_HOT_DOG:0, PGIE_CLASS_ID_CARROT:0, PGIE_CLASS_ID_BROCCOLI:0, PGIE_CLASS_ID_ORANGE:0, PGIE_CLASS_ID_SANDWICH:0, PGIE_CLASS_ID_APPLE:0, PGIE_CLASS_ID_BANANA:0, PGIE_CLASS_ID_BOWL:0, PGIE_CLASS_ID_SPOON:0, PGIE_CLASS_ID_KNIFE:0, PGIE_CLASS_ID_FORK:0, PGIE_CLASS_ID_CUP:0, PGIE_CLASS_ID_WINE_GLASS:0, PGIE_CLASS_ID_BOTTLE:0, PGIE_CLASS_ID_TENNIS_RACKET:0, PGIE_CLASS_ID_SURFBOARD:0, PGIE_CLASS_ID_SKATEBOARD:0, PGIE_CLASS_ID_BASEBALL_GLOVE:0, PGIE_CLASS_ID_BASEBALL_BAT:0, PGIE_CLASS_ID_KITE:0, PGIE_CLASS_ID_SPORTS_BALL:0, PGIE_CLASS_ID_SNOWBOARD:0, PGIE_CLASS_ID_SKIS:0, PGIE_CLASS_ID_FRISBEE:0, PGIE_CLASS_ID_SUITCASE:0, PGIE_CLASS_ID_TIE:0, PGIE_CLASS_ID_HANDBAG:0, PGIE_CLASS_ID_UMBRELLA:0, PGIE_CLASS_ID_BACKPACK:0, PGIE_CLASS_ID_UMBRELLA:0, PGIE_CLASS_ID_GIRAFFE:0, PGIE_CLASS_ID_ZEBRA:0, PGIE_CLASS_ID_BEAR:0, PGIE_CLASS_ID_ELEPHANT:0, PGIE_CLASS_ID_COW:0, PGIE_CLASS_ID_SHEEP:0, PGIE_CLASS_ID_HORSE:0, PGIE_CLASS_ID_DOG:0, PGIE_CLASS_ID_CAT:0, PGIE_CLASS_ID_BIRD:0, PGIE_CLASS_ID_BENCH:0, PGIE_CLASS_ID_PARKING_METER:0, PGIE_CLASS_ID_STOP_SIGN:0, PGIE_CLASS_ID_FIRE_HYDRANT:0, PGIE_CLASS_ID_TRAFFIC_LIGHT:0, PGIE_CLASS_ID_BOAT:0, PGIE_CLASS_ID_TRUCK:0, PGIE_CLASS_ID_TRAIN:0, PGIE_CLASS_ID_BUS:0, PGIE_CLASS_ID_AEROPLANE:0, PGIE_CLASS_ID_MOTORBIKE:0, PGIE_CLASS_ID_VEHICLE:0, PGIE_CLASS_ID_BICYCLE:0, PGIE_CLASS_ID_PERSON:0 } num_rects=0 gst_buffer = info.get_buffer() if not gst_buffer: print(\"Unable to get GstBuffer \") return # Retrieve batch metadata from the gst_buffer # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the # C address of gst_buffer as input, which is obtained with hash(gst_buffer) batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer)) l_frame = batch_meta.frame_meta_list while l_frame is not None: try: # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta # The casting is done by pyds.glist_get_nvds_frame_meta() # The casting also keeps ownership of the underlying memory # in the C code, so the Python garbage collector will leave # it alone. #frame_meta = pyds.glist_get_nvds_frame_meta(l_frame.data) frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data) except StopIteration: break frame_number=frame_meta.frame_num num_rects = frame_meta.num_obj_meta l_obj=frame_meta.obj_meta_list while l_obj is not None: try: # Casting l_obj.data to pyds.NvDsObjectMeta #obj_meta=pyds.glist_get_nvds_object_meta(l_obj.data) obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data) except StopIteration: break obj_counter[obj_meta.class_id] += 1 obj_meta.rect_params.border_color.set(0.0, 0.0, 1.0, 0.0) try: l_obj=l_obj.next except StopIteration: break # Acquiring a display meta object. The memory ownership remains in # the C code so downstream plugins can still access it. Otherwise # the garbage collector will claim it when this probe function exits. display_meta=pyds.nvds_acquire_display_meta_from_pool(batch_meta) display_meta.num_labels = 1 py_nvosd_text_params = display_meta.text_params[0] # Setting display text to be shown on screen # Note that the pyds module allocates a buffer for the string, and the # memory will not be claimed by the garbage collector. # Reading the display_text field here will return the C address of the # allocated string. Use pyds.get_string() to get the string content. py_nvosd_text_params.display_text = \"Frame Number={} Number of Objects={} Vehicle_count={} Person_count={}\".format(frame_number, num_rects, obj_counter[PGIE_CLASS_ID_VEHICLE], obj_counter[PGIE_CLASS_ID_PERSON]) # Now set the offsets where the string should appear py_nvosd_text_params.x_offset = 10 py_nvosd_text_params.y_offset = 12 # Font , font-color and font-size py_nvosd_text_params.font_params.font_name = \"Serif\" py_nvosd_text_params.font_params.font_size = 10 # set(red, green, blue, alpha); set to White py_nvosd_text_params.font_params.font_color.set(1.0, 1.0, 1.0, 1.0) # Text background color py_nvosd_text_params.set_bg_clr = 1 # set(red, green, blue, alpha); set to Black py_nvosd_text_params.text_bg_clr.set(0.0, 0.0, 0.0, 1.0) # Using pyds.get_string() to get display_text as string print(pyds.get_string(py_nvosd_text_params.display_text)) pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta) try: l_frame=l_frame.next except StopIteration: break return Gst.PadProbeReturn.OK def main(args): # Standard GStreamer initialization GObject.threads_init() Gst.init(None) # Create gstreamer elements # Create Pipeline element that will form a connection of other elements print(\"Creating Pipeline \\n \") pipeline = Gst.Pipeline() if not pipeline: sys.stderr.write(\" Unable to create Pipeline \\n\") # Source element for reading from the file print(\"Creating Source \\n \") source = Gst.ElementFactory.make(\"nvarguscamerasrc\", \"src-elem\") if not source: sys.stderr.write(\" Unable to create Source \\n\") # Converter to scale the image nvvidconv_src = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor_src\") if not nvvidconv_src: sys.stderr.write(\" Unable to create nvvidconv_src \\n\") # Caps for NVMM and resolution scaling caps_nvvidconv_src = Gst.ElementFactory.make(\"capsfilter\", \"nvmm_caps\") if not caps_nvvidconv_src: sys.stderr.write(\" Unable to create capsfilter \\n\") # Create nvstreammux instance to form batches from one or more sources. streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\") if not streammux: sys.stderr.write(\" Unable to create NvStreamMux \\n\") # Use nvinfer to run inferencing on decoder's output, # behaviour of inferencing is set through config file pgie = Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\") if not pgie: sys.stderr.write(\" Unable to create pgie \\n\") # Use convertor to convert from NV12 to RGBA as required by nvosd nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\") if not nvvidconv: sys.stderr.write(\" Unable to create nvvidconv \\n\") # Create OSD to draw on the converted RGBA buffer nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\") if not nvosd: sys.stderr.write(\" Unable to create nvosd \\n\") # Finally render the osd output if is_aarch64(): transform = Gst.ElementFactory.make(\"nvegltransform\", \"nvegl-transform\") print(\"Creating EGLSink \\n\") sink = Gst.ElementFactory.make(\"nveglglessink\", \"nvvideo-renderer\") if not sink: sys.stderr.write(\" Unable to create egl sink \\n\") source.set_property('bufapi-version', True) caps_nvvidconv_src.set_property('caps', Gst.Caps.from_string('video/x-raw(memory:NVMM), width=720, height=480, framerate=30/1')) streammux.set_property('width', 720) streammux.set_property('height', 480) streammux.set_property('batch-size', 1) streammux.set_property('batched-push-timeout', 4000000) pgie.set_property('config-file-path', \"config_infer_primary_yoloV3.txt\") print(\"Adding elements to Pipeline \\n\") pipeline.add(source) pipeline.add(nvvidconv_src) pipeline.add(caps_nvvidconv_src) pipeline.add(streammux) pipeline.add(pgie) pipeline.add(nvvidconv) pipeline.add(nvosd) pipeline.add(sink) if is_aarch64(): pipeline.add(transform) # we link the elements together # Csi camera -&gt; -nvvidconv_src -&gt; caps_nvvidconv_src -&gt; # nvinfer (pgie)-&gt; nvvidconv -&gt; nvosd -&gt; video-renderer print(\"Linking elements in the Pipeline \\n\") source.link(nvvidconv_src) nvvidconv_src.link(caps_nvvidconv_src) sinkpad = streammux.get_request_pad(\"sink_0\") if not sinkpad: sys.stderr.write(\" Unable to get the sink pad of streammux \\n\") srcpad = caps_nvvidconv_src.get_static_pad(\"src\") if not srcpad: sys.stderr.write(\" Unable to get source pad of decoder \\n\") srcpad.link(sinkpad) streammux.link(pgie) pgie.link(nvvidconv) nvvidconv.link(nvosd) if is_aarch64(): nvosd.link(transform) transform.link(sink) else: nvosd.link(sink) # create and event loop and feed gstreamer bus mesages loop = GObject.MainLoop() bus = pipeline.get_bus() bus.add_signal_watch() bus.connect (\"message\", bus_call, loop) # Lets add probe to get informed of the meta data generated, we add probe to # the sink pad of the osd element, since by that time, the buffer would have # had got all the metadata. osdsinkpad = nvosd.get_static_pad(\"sink\") if not osdsinkpad: sys.stderr.write(\" Unable to get sink pad of nvosd \\n\") osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0) print(\"Starting pipeline \\n\") # start play back and listed to events pipeline.set_state(Gst.State.PLAYING) try: loop.run() except: pass # cleanup pipeline.set_state(Gst.State.NULL) if __name__ == '__main__': sys.exit(main(sys.argv))",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to install deepstream-5 inside a docker container. I used a container with the following specs: 10.2-cudnn7-tensorrt7-devel-ubuntu18.04 (cuda 10.2.89, nccl 2.7.8, cudnn 7.6.5.32, tensorrt 7.0.0.11) from Here. And python 3.6 After that, I installed deepstream-5 as described in their official site. When I run one of their tests: deepstream-app -c /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txt I get this error. I don't know how to fix it. I tried to remove the cache as suggested in here, now I have warnings beside the errors :D. I checked, and already have the two files that in the warning. I asked on Nvidia-Forum. Here.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have an old geforce GTX (960 M) graphic card , and i wanted to use the Deep-stream SDK , does it support older generation graphic cards ?",
        "answers": [],
        "votes": []
    },
    {
        "question": "In this link, we can access to gstreamer pipeline buffer and convert the frame buffers in numpy array, I want to know, How I can to accesses the frame buffers in GPU mem and then feed into my custom processor without convert frames into numpy array. We have two solutions for using of deepstream decoder(efficient way than opencv+gstreamer): the one way is we need to write custom element of processing and register in gstreamer and then put the custom element in the pipeline and then do processing on frames buffer. this way is good but need to write and knowledge gstreamer programming. this way is same way of deep stream. the second way is we use only decoded of frames from that link, then passed the frames into custom processor units. for this part I have two question: 1- The loop of gstreamer is same as asyncio programming loop? 2- As you know, If we add additional operation into pad prob function, this cause drop performance, but I want to know, Is it possible to put the frames in the pad prob function and do loop.create_task(process(frame)) like async? this cause we here don't wait to perform processing. like this: def tiler_sink_pad_buffer_probe(pad,info,u_data): .... ### capture the frames in GPU buffer without converting into numpy loop.create_task(process(frame)) .... return Gst.PadProbeReturn.OK",
        "answers": [
            [
                "Yeap gstreamer loop in Python is asyncio Well you can do this like me (bad way by creating global variables) ws = None loopIO = None def tiler_sink_pad_buffer_probe(pad,info,u_data): global ws global loopIO .... ### capture the frames in GPU buffer converting into numpy if ws and loopIO: _, jpeg_frame = cv2.imencode('.jpg', frame_image) str_pic = jpeg_frame.tobytes() asyncio.run_coroutine_threadsafe(ws.send(str_pic), loopIO) .... return Gst.PadProbeReturn.OK if __name__ == '__main__': start_server = websockets.serve(consumer_handler, 'localhost', 8765) loopIO = asyncio.get_event_loop() loopIO.run_until_complete(start_server) wst = threading.Thread(target=asyncio.get_event_loop().run_forever) wst.daemon = True wst.start() sys.exit(main(sys.argv))"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I would like to open a video stream by OpenCv and push frame by frame inside a DeepStream pipeline to use tesornRT to make an inference on Yolov3 model, but i do not know how to make it works. I'm trying to follow the directives that I found here, but still nothing... This is my code : #include &lt;gst/gst.h&gt; #include &lt;gst/app/gstappsrc.h&gt; #include &lt;gst/app/gstappsink.h&gt; #include &lt;opencv2/core/core.hpp&gt; #include &lt;opencv2/core/types_c.h&gt; #include &lt;opencv2/imgproc/imgproc.hpp&gt; #include &lt;opencv2/highgui/highgui.hpp&gt; static GMainLoop *loop; static void cb_need_data (GstElement *appsrc, guint unused_size, gpointer user_data) { static gboolean white = FALSE; static GstClockTime timestamp = 0; guint size,depth,height,width,step,channels; GstFlowReturn ret ; IplImage* img; guchar *data1; GstMapInfo map; cv::Mat imgMat = imread(\"cat.jpg\",cv::IMREAD_COLOR); cvtColor(imgMat,imgMat,cv::COLOR_BGR2YUV); IplImage imgIpl = imgMat; img = &amp;imgIpl; height = img-&gt;height; width = img-&gt;width; step = img-&gt;widthStep; channels = img-&gt;nChannels; depth = img-&gt;depth; data1 = (guchar *)img-&gt;imageData; size = height*width*channels; GstBuffer *buffer = NULL;//gst_buffer_new_allocate (NULL, size, NULL); g_print(\"frame_height: %d \\n\",img-&gt;height); g_print(\"frame_width: %d \\n\",img-&gt;width); g_print(\"frame_channels: %d \\n\",img-&gt;nChannels); g_print(\"frame_size: %d \\n\",height*width*channels); buffer = gst_buffer_new_allocate (NULL, size, NULL); gst_buffer_map (buffer, &amp;map, GST_MAP_WRITE); memcpy( (guchar *)map.data, data1, gst_buffer_get_size( buffer ) ); /* this makes the image black/white */ //gst_buffer_memset (buffer, 0, white ? 0xff : 0x0, size); white = !white; GST_BUFFER_PTS (buffer) = timestamp; GST_BUFFER_DURATION (buffer) = gst_util_uint64_scale_int (1, GST_SECOND, 1); timestamp += GST_BUFFER_DURATION (buffer); //gst_app_src_push_buffer ((GstAppSrc *)appsrc, buffer); g_signal_emit_by_name (appsrc, \"push-buffer\", buffer, &amp;ret); if (ret != GST_FLOW_OK) { g_print(\"quit\"); /* something wrong, stop pushing */ g_main_loop_quit (loop); } //g_print(\"return\"); } gint main (gint argc, gchar *argv[]) { GstElement *pipeline, *appsrc, *conv, *videosink, *sink,*nvosd,*streammux; /* init GStreamer */ gst_init (&amp;argc, &amp;argv); loop = g_main_loop_new (NULL, FALSE); /* setup pipeline */ pipeline = gst_pipeline_new (\"pipeline\"); appsrc = gst_element_factory_make (\"appsrc\", \"source\"); conv = gst_element_factory_make (\"videoconvert\", \"conv\"); streammux = gst_element_factory_make (\"nvstreammux\", \"stream-muxer\"); sink = gst_element_factory_make (\"nveglglessink\", \"nvvideo-renderer\"); //videosink = gst_element_factory_make(\"appsink\",\"app-sink\"); /* setup */ g_object_set (G_OBJECT (appsrc), \"caps\", gst_caps_new_simple (\"video/x-raw\", \"format\", G_TYPE_STRING, \"RGB\", \"width\", G_TYPE_INT, 640, \"height\", G_TYPE_INT, 360, \"framerate\", GST_TYPE_FRACTION, 1, 1, NULL), NULL); gst_bin_add_many (GST_BIN (pipeline), appsrc, conv,streammux,sink,NULL); gst_element_link_many (appsrc,conv,streammux,sink ,NULL); //g_object_set (videosink, \"device\", \"/dev/video0\", NULL); /* setup appsrc */ g_object_set (G_OBJECT (appsrc), \"stream-type\", 0, \"format\", GST_FORMAT_TIME, NULL); g_signal_connect (appsrc, \"need-data\", G_CALLBACK (cb_need_data), NULL); /* play */ gst_element_set_state (pipeline, GST_STATE_PLAYING); g_main_loop_run (loop); /* clean up */ gst_element_set_state (pipeline, GST_STATE_NULL); gst_object_unref (GST_OBJECT (pipeline)); g_main_loop_unref (loop); return 0; } I am an absolutely beginner, if someone can show some code is going to be much better. Thanks.",
        "answers": [
            [
                "you need to create a pipeline as follows appsrc ! nvvideoconvert ! nvstreammux ! nvinfer ! nvvideoconvert ! nvdsosd ! nveglglessink \"appsrc\" takes your frame as input \"nvvideoconvert\" does format conversion \"nvstreammux\" multiplexes streams in case of multiple sources \"nvinfer\" does inferencing on the input stream \"nvvideoconvert\" converts frame to RGBA now \"nvdsosd\" draws bounding boxes on the frame \"nveglglessink\" displays the frame #include &lt;gst/gst.h&gt; #include &lt;gst/app/gstappsrc.h&gt; #include &lt;gst/app/gstappsink.h&gt; #include &lt;opencv2/core/core.hpp&gt; #include &lt;opencv2/core/types_c.h&gt; #include &lt;opencv2/imgproc/imgproc.hpp&gt; #include &lt;opencv2/highgui/highgui.hpp&gt; static GMainLoop *loop; #define APPSRC_WIDTH 320 #define APPSRC_HEIGHT 240 #define RUN_VIDEO 0 static void cb_need_data (GstElement *appsrc, guint unused_size, gpointer user_data) { static gboolean white = FALSE; static GstClockTime timestamp = 0; guint size,depth,height,width,step,channels; GstFlowReturn ret ; IplImage* img; guchar *data1; GstMapInfo map; cv::Mat imgMat = imread(\"/opt/nvidia/deepstream/deepstream-4.0/samples/streams/sample_720p.jpg\",cv::IMREAD_COLOR); cv::resize(imgMat, imgMat, cv::Size(APPSRC_WIDTH, APPSRC_HEIGHT)); cvtColor(imgMat,imgMat,cv::COLOR_BGR2RGBA); IplImage imgIpl = imgMat; img = &amp;imgIpl; height = img-&gt;height; width = img-&gt;width; step = img-&gt;widthStep; channels = img-&gt;nChannels; depth = img-&gt;depth; data1 = (guchar *)img-&gt;imageData; size = height*width*channels; GstBuffer *buffer = NULL;//gst_buffer_new_allocate (NULL, size, NULL); g_print(\"frame_height: %d \\n\",img-&gt;height); g_print(\"frame_width: %d \\n\",img-&gt;width); g_print(\"frame_channels: %d \\n\",img-&gt;nChannels); g_print(\"frame_size: %d \\n\",height*width*channels); buffer = gst_buffer_new_allocate (NULL, size, NULL); gst_buffer_map (buffer, &amp;map, GST_MAP_WRITE); memcpy( (guchar *)map.data, data1, gst_buffer_get_size( buffer ) ); /* this makes the image black/white */ //gst_buffer_memset (buffer, 0, white ? 0xff : 0x0, size); white = !white; GST_BUFFER_PTS (buffer) = timestamp; GST_BUFFER_DURATION (buffer) = gst_util_uint64_scale_int (1, GST_SECOND, 1); timestamp += GST_BUFFER_DURATION (buffer); //gst_app_src_push_buffer ((GstAppSrc *)appsrc, buffer); g_signal_emit_by_name (appsrc, \"push-buffer\", buffer, &amp;ret); if (ret != GST_FLOW_OK) { g_print(\"quit\"); /* something wrong, stop pushing */ g_main_loop_quit (loop); } //g_print(\"return\"); } gint main (gint argc, gchar *argv[]) { GstElement *pipeline, *appsrc, *conv, *capsfilter_converter, *videosink,*streammux, *nvinfer, *nvconv, *nvosd,*sink; GstElement *filesrc, *parser, *decoder; GstCaps * scaler_caps = NULL, *convertCaps = NULL, *nvconvert_caps; /* init GStreamer */ gst_init (&amp;argc, &amp;argv); loop = g_main_loop_new (NULL, FALSE); /* setup pipeline */ pipeline = gst_pipeline_new (\"pipeline\"); appsrc = gst_element_factory_make (\"appsrc\", \"source\"); filesrc = gst_element_factory_make (\"filesrc\", \"file-source\"); parser = gst_element_factory_make (\"h264parse\", \"parser\"); decoder = gst_element_factory_make (\"nvv4l2decoder\", \"decoder\"); conv = gst_element_factory_make (\"nvvideoconvert\", \"nv-conv-1\"); capsfilter_converter = gst_element_factory_make (\"capsfilter\", \"converter-caps\"); streammux = gst_element_factory_make (\"nvstreammux\", \"stream-muxer\"); nvinfer = gst_element_factory_make (\"nvinfer\", \"nv-infer\"); nvconv = gst_element_factory_make (\"nvvideoconvert\", \"nv-conv-2\"); nvosd = gst_element_factory_make (\"nvdsosd\", \"nv-onscreendisplay\"); sink = gst_element_factory_make (\"nveglglessink\", \"nvvideo-renderer\"); /* setup */ g_object_set (G_OBJECT (appsrc), \"caps\", gst_caps_new_simple (\"video/x-raw\", \"format\", G_TYPE_STRING, \"RGBA\", \"width\", G_TYPE_INT, APPSRC_WIDTH, \"height\", G_TYPE_INT, APPSRC_HEIGHT, \"framerate\", GST_TYPE_FRACTION, 1, 1, NULL), NULL); capsfilter_converter = gst_element_factory_make (\"capsfilter\", \"converter-caps\"); nvconvert_caps = gst_caps_new_simple (\"video/x-raw\", \"format\", G_TYPE_STRING, \"RGBA\", NULL); GstCapsFeatures *feature = NULL; feature = gst_caps_features_new (\"memory:NVMM\", NULL); gst_caps_set_features (nvconvert_caps, 0, feature); g_object_set (G_OBJECT (capsfilter_converter), \"caps\", nvconvert_caps, NULL); g_object_set (G_OBJECT (streammux), \"width\", APPSRC_WIDTH, \"height\", APPSRC_HEIGHT, \"batch-size\", 1, \"batched-push-timeout\", 5000, NULL); g_object_set (G_OBJECT (conv), \"nvbuf-memory-type\", 0, \"num-surfaces-per-frame\", 1, NULL); g_object_set (G_OBJECT (streammux), \"nvbuf-memory-type\", 0, \"num-surfaces-per-frame\", 1, NULL); g_object_set (G_OBJECT (filesrc), \"location\", \"/opt/nvidia/deepstream/deepstream-4.0/samples/streams/sample_720p.h264\", NULL); std::string config_file_path_FR = \"/opt/nvidia/deepstream/deepstream-4.0/samples/configs/deepstream-app/config_infer_primary.txt\"; g_object_set (G_OBJECT (nvinfer), \"config-file-path\", config_file_path_FR.c_str(), NULL); #if RUN_VIDEO gst_bin_add_many (GST_BIN (pipeline), filesrc, parser, decoder, conv,streammux, nvinfer, nvosd, nvconv, sink,NULL); #else gst_bin_add_many (GST_BIN (pipeline), appsrc, conv, capsfilter_converter, streammux, nvinfer, nvosd, nvconv, sink,NULL); #endif GstPad *sinkpad, *srcpad; gchar pad_name[16] = { }; g_snprintf (pad_name, 15, \"sink_%u\", 0); sinkpad = gst_element_get_request_pad (streammux, pad_name); if (!sinkpad) { g_printerr (\"Streammux request sink pad failed. Exiting.\\n\"); return -1; } #if RUN_VIDEO srcpad = gst_element_get_static_pad (decoder, \"src\"); #else srcpad = gst_element_get_static_pad (capsfilter_converter, \"src\"); #endif if (!srcpad) { g_printerr (\"Failed to get src pad of source bin. Exiting.\\n\"); return -1; } if (gst_pad_link (srcpad, sinkpad) != GST_PAD_LINK_OK) { g_printerr (\"Failed to link source bin to stream muxer. Exiting.\\n\"); return -1; } #if RUN_VIDEO gst_element_link_many (filesrc, parser, decoder, NULL); #else gst_element_link_many (appsrc,conv, capsfilter_converter, NULL); #endif gst_element_link_many (streammux, nvinfer, nvconv, nvosd, sink ,NULL); g_signal_connect (appsrc, \"need-data\", G_CALLBACK (cb_need_data), NULL); /* play */ gst_element_set_state (pipeline, GST_STATE_PLAYING); g_main_loop_run (loop); /* clean up */ gst_element_set_state (pipeline, GST_STATE_NULL); gst_object_unref (GST_OBJECT (pipeline)); g_main_loop_unref (loop); return 0; } to run inferencing for your model you need to set path to config-file for your model and set the path of image/video you want to run inferencing on. to run this on video h264 encoded video, just change #define RUN_VIDEO 0to #define RUN_VIDEO 1"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "System: I've a Windows Server 2019 OS installed with a NVIDIA Tesla T4 Tensor Core GPU. Goal: Planning to read real time streaming videos from an IP camera and to further process frame by frame. Goal is to leverage NVIDIA DeepStream SDK, but issue is, it isn't available for Windows OS. So, I'm thinking on the docker lines, but since am very new to docker containers, would like to know if I can install a docker on Windows and can run this deepstream docker image on that. If not, is there any way I can run this Linux based DeepStream docker image on Windows? Any help shall be greatly acknowledged.",
        "answers": [
            [
                "I have never worked with the windows server before it should be the same as a docker in Linux VM. First, you need to pull docker images for deepstream docker pull nvcr.io/nvidia/deepstream:5.0-dp-20.04-triton and then try to run sample apps provided in the docker image. Refer this for the procedure. if you are interested in python apps you can check sample apps here. Note:- make sure you are able to access display from inside the container cause deepstream use eglsink in their samples app which will try to open a display window on your screen or you can change the sink type to filesink if you want to save it is a file. Refer this for available plugins and their attributes."
            ],
            [
                "According to the post in Nivida forum, Windows not supported. As alternative, I wonder if anyone used the Nvidia Graph Composer in Windows."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "Working on nvidia deep stream - inference engine, unable to get the classifier class. Always it shows index 0. Any help is appreciated. l_classifier = obj_meta.classifier_meta_list print('First Classifier at: ', l_classifier) classifier_cnt = 0 while l_classifier is not None: classifier_cnt += 1 print('Parsing Classifier at: ', l_classifier) try: classifier_meta = pyds.glist_get_nvds_classifier_meta( l_classifier.data) print('Classifier Component ID:' + str(classifier_meta.unique_component_id)) # nxt_classifier = classifier_meta.next # print(nxt_classifier) # print(dir(classifier_meta)) except Exception as ex: print('Could not parse MetaData: ', ex) l_label = classifier_meta.label_info_list uid=classifier_meta.unique_component_id numLabel=classifier_meta.num_labels classId = classifier_meta.class_id label_info=pyds.glist_get_nvds_label_info(l_label.data) classifier_class = label_info.result_class_id num_classes = label_info.num_classes label_id = label_info.label_id result_prob = label_info.result_prob print(\"1 l_label :\",l_label) print(\"1 u id ------------ :\",uid) print(\"1 numLabel :\",numLabel) print(\"1 label_info :\",label_info) print(\"1 classifier_class:\",classifier_class) print(\"1 num_classes :\",num_classes) print(\"1 label_id :\",label_id) print(\"classId ==&gt;\", classId) l_classifier = l_classifier.next print('Next Classifier: ', l_classifier) Sample output is given as 1 l_label : &lt;pyds.GList object at 0x7fa740cfcf80&gt; 1 u id ------------ : 4 1 numLabel : 1 1 label_info : &lt;pyds.NvDsLabelInfo object at 0x7fa740cfcf48&gt; 1 classifier_class: 0 1 num_classes : 0 1 label_id : 0 Classifier class always appears as 0",
        "answers": [
            [
                "A bit too late but label_info has the classifier data. You need to cast l_label to get label_info. Then you need to add \"l_label=l_label.next\" so that it moves to the next item. Following link has the obtainable data. https://docs.nvidia.com/metropolis/deepstream/python-api/PYTHON_API/NvDsMeta/NvDsLabelInfo.html l_label = class_meta.label_info_list while l_label is not None: try: label_info = pyds.NvDsLabelInfo.cast(l_label.data) except StopIteration: break print(label_info.label_id) try: l_label=l_label.next except StopIteration: break try: l_class=l_class.next except StopIteration: break"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I\u2019m trying to consume HLS feed generated by AWS Kinesis as input to DeepStream pipeline but I\u2019m getting following error: Error: gst-stream-error-quark: GStreamer encountered a general stream error.(1): qtdemux.c(7067):gst_qtdemux_process_adapter (): /GstPipeline:pipeline0/GstBin:source-bin-00/GstURIDecodeBin:uri-decode-bin/GstDecodeBin:decodebin0/GstQTDemux: qtdemux1: no \u2018moov\u2019 atom within the first 10 MB I\u2019ve built the pipeline using python bindings and I\u2019m using uridecodebin to resolve input URI. My GStreamer pipeline sequence is as follows: uridecodebin --&gt; nvstreammux --&gt; nvinfer --&gt; nvvideoconvert --&gt; nvdsosd --&gt; nveglglessink Furthermore, I am also able to capture frames using a simple OpenCV python script. Following is my hardware configuration and software version: Hardware Platform (GPU) = NVIDIA T4 DeepStream Version= 5.0 TensorRT Version=7.0.0 NVIDIA GPU Driver Version (valid for GPU only)=440.64.00 What should I do to get it working?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Hello everyone my question is related with Nvidia Deepstream-5.0 SDK, I am trying to run a sample test deepstream-nvdsanalytics-test which is in \"/source/apps/sample_apps\" in NVIDIA Deepstream container. I want to save the video file by using Filesink. I got the suggestion to look into create_encode_file_bin function which is in\"/source/apps/apps-common/src/deepstream_sink_bin.c\". I tried changing the code deepstream_nvdsanalytics_test.cpp taking create_encode_file_bin as a reference but got some errors. I am posting my pipeline, edited code and error please have a look. pipeline used- pgie-&gt;nvtracker-&gt;nvdsanalytics-&gt;tiler-&gt;nvvidconv-&gt;nvosd-&gt;nvideoconvvert-&gt;caps filter(x/raw)-&gt;encoder-&gt;codecparse-&gt;mux-&gt;filesink Error- (deepstream-nvdsanalytics-test:203): GStreamer-WARNING **: 16:08:13.115: Name \u2018nvvideo-converter\u2019 is not unique in bin \u2018nvdsanalytics-test-pipeline\u2019, not adding (deepstream-nvdsanalytics-test:203): GStreamer-CRITICAL **: 16:08:13.116: gst_element_link_pads_full: assertion \u2018GST_IS_ELEMENT (dest)\u2019 failed Elements could not be linked. Exiting. code- #include &lt;gst/gst.h&gt; #include &lt;glib.h&gt; #include &lt;stdio.h&gt; #include &lt;math.h&gt; #include &lt;string.h&gt; #include &lt;sys/time.h&gt; #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;unordered_map&gt; #include \"gstnvdsmeta.h\" #include \"nvds_analytics_meta.h\" #include \"deepstream_config.h\" #ifndef PLATFORM_TEGRA #include \"gst-nvmessage.h\" #endif [....] int main (int argc, char *argv[]) { GMainLoop *loop = NULL; GstElement *pipeline = NULL, *streammux = NULL, *sink = NULL, *pgie = NULL, *nvtracker = NULL, *nvdsanalytics = NULL, *nvvidconv = NULL, *nvosd = NULL, *nvvidconv1 = NULL, *transform1 = NULL, *cap_filter = NULL, *encoder = NULL, *codecparse = NULL, *mux = NULL, *tiler = NULL; GstCaps *caps = NULL; #ifdef PLATFORM_TEGRA GstElement *transform = NULL; #endif GstBus *bus = NULL; guint bus_watch_id; GstPad *nvdsanalytics_src_pad = NULL; guint i, num_sources; guint tiler_rows, tiler_columns; guint pgie_batch_size; gulong bitrate = 2000000; guint profile = 0; /* Check input arguments */ if (argc &lt; 2) { g_printerr (\"Usage: %s &lt;uri1&gt; [uri2] ... [uriN] \\n\", argv[0]); return -1; } num_sources = argc - 1; /* Standard GStreamer initialization */ gst_init (&amp;argc, &amp;argv); loop = g_main_loop_new (NULL, FALSE); /* Create gstreamer elements */ /* Create Pipeline element that will form a connection of other elements */ pipeline = gst_pipeline_new (\"nvdsanalytics-test-pipeline\"); /* Create nvstreammux instance to form batches from one or more sources. */ streammux = gst_element_factory_make (\"nvstreammux\", \"stream-muxer\"); if (!pipeline || !streammux) { g_printerr (\"One element could not be created. Exiting.\\n\"); return -1; } gst_bin_add (GST_BIN (pipeline), streammux); for (i = 0; i &lt; num_sources; i++) { GstPad *sinkpad, *srcpad; gchar pad_name[16] = { }; GstElement *source_bin = create_source_bin (i, argv[i + 1]); if (!source_bin) { g_printerr (\"Failed to create source bin. Exiting.\\n\"); return -1; } gst_bin_add (GST_BIN (pipeline), source_bin); g_snprintf (pad_name, 15, \"sink_%u\", i); sinkpad = gst_element_get_request_pad (streammux, pad_name); if (!sinkpad) { g_printerr (\"Streammux request sink pad failed. Exiting.\\n\"); return -1; } srcpad = gst_element_get_static_pad (source_bin, \"src\"); if (!srcpad) { g_printerr (\"Failed to get src pad of source bin. Exiting.\\n\"); return -1; } if (gst_pad_link (srcpad, sinkpad) != GST_PAD_LINK_OK) { g_printerr (\"Failed to link source bin to stream muxer. Exiting.\\n\"); return -1; } gst_object_unref (srcpad); gst_object_unref (sinkpad); } /* Use nvinfer to infer on batched frame. */ pgie = gst_element_factory_make (\"nvinfer\", \"primary-nvinference-engine\"); /* Use nvtracker to track detections on batched frame. */ nvtracker = gst_element_factory_make (\"nvtracker\", \"nvtracker\"); /* Use nvdsanalytics to perform analytics on object */ nvdsanalytics = gst_element_factory_make (\"nvdsanalytics\", \"nvdsanalytics\"); /* Use nvtiler to composite the batched frames into a 2D tiled array based * on the source of the frames. */ tiler = gst_element_factory_make (\"nvmultistreamtiler\", \"nvtiler\"); /* Use convertor to convert from NV12 to RGBA as required by nvosd */ nvvidconv = gst_element_factory_make (\"nvvideoconvert\", \"nvvideo-converter\"); if (!nvvidconv) { g_printerr (\"nvvdiconv element could not be created. Exiting.\\n\"); } /* Create OSD to draw on the converted RGBA buffer */ nvosd = gst_element_factory_make (\"nvdsosd\", \"nv-onscreendisplay\"); if (!nvosd) { g_printerr (\"nvosd element could not be created. Exiting.\\n\"); } /* converter to convert RGBA to NV12 */ nvvidconv1 = gst_element_factory_make (\"nvvideoconvert\", \"nvvideo-converter1\"); if (!nvvidconv1) { g_printerr (\"nvvidconv1 element could not be created. Exiting.\\n\"); } /*create cap_filter */ cap_filter = gst_element_factory_make (NVDS_ELEM_CAPS_FILTER, \"cap_filter\"); if (!cap_filter) { g_printerr (\"cap_filter element could not be created. Exiting.\\n\"); } /* create cap for filter */ caps = gst_caps_from_string (\"video/x-raw, format=I420\"); g_object_set (G_OBJECT (cap_filter), \"caps\", caps, NULL); /* creatge encoder*/ encoder = gst_element_factory_make (NVDS_ELEM_ENC_H264_HW, \"encoder\"); if (!encoder) { g_printerr (\"encoder element could not be created. Exiting.\\n\"); } /* create transform1 */ transform1 = gst_element_factory_make (NVDS_ELEM_VIDEO_CONV, \"transform1\"); g_object_set (G_OBJECT (transform1), \"gpu-id\", 0, NULL); if (!transform1) { g_printerr (\"transform1 element could not be created. Exiting.\\n\"); } #ifdef IS_TEGRA g_object_set (G_OBJECT (encoder), \"bufapi-version\", 1, NULL); #endif g_object_set (G_OBJECT (encoder), \"profile\", profile, NULL); g_object_set (G_OBJECT (encoder), \"bitrate\", bitrate, NULL); /* create codecparse */ codecparse = gst_element_factory_make (\"h264parse\", \"h264-parser\"); if (!codecparse) { g_printerr (\"codecparse element could not be created. Exiting.\\n\"); } /* create mux */ mux = gst_element_factory_make (NVDS_ELEM_MUX_MP4, \"mux\"); if (!mux) { g_printerr (\"mux element could not be created. Exiting.\\n\"); } /* create sink */ sink = gst_element_factory_make (NVDS_ELEM_SINK_FILE, \"filesink\"); if (!sink) { g_printerr (\"sink element could not be created. Exiting.\\n\"); } g_object_set (G_OBJECT (sink), \"location\", \"capture.mp4\", \"sync\", 0, \"async\" , FALSE, NULL); // /* Finally render the osd output */ #ifdef PLATFORM_TEGRA transform = gst_element_factory_make (\"nvegltransform\", \"nvegl-transform\"); #endif // sink = gst_element_factory_make (NVDS_ELEM_SINK_FILE, \"filesink\"); // g_object_set (G_OBJECT (sink), \"location\", \"capture.mp4\", \"sync\", 0, \"async\" , FALSE, NULL); if (!pgie || !nvtracker || !nvdsanalytics || !nvvidconv || !nvosd || !nvvidconv1 || !cap_filter || !encoder || !codecparse || !mux || !sink) { g_printerr (\"One element could not be created. Exiting.\\n\"); return -1; } #ifdef PLATFORM_TEGRA if(!transform) { g_printerr (\"One tegra element could not be created. Exiting.\\n\"); return -1; } #endif g_object_set (G_OBJECT (streammux), \"width\", MUXER_OUTPUT_WIDTH, \"height\", MUXER_OUTPUT_HEIGHT, \"batch-size\", num_sources, \"batched-push-timeout\", MUXER_BATCH_TIMEOUT_USEC, NULL); /* Configure the nvinfer element using the nvinfer config file. */ g_object_set (G_OBJECT (pgie), \"config-file-path\", \"nvdsanalytics_pgie_config.txt\", NULL); /* Configure the nvtracker element for using the particular tracker algorithm. */ g_object_set (G_OBJECT (nvtracker), \"ll-lib-file\", \"/opt/nvidia/deepstream/deepstream-5.0/lib/libnvds_nvdcf.so\", \"ll-config-file\", \"tracker_config.yml\", \"tracker-width\", 640, \"tracker-height\", 480, NULL); /* Configure the nvdsanalytics element for using the particular analytics config file*/ g_object_set (G_OBJECT (nvdsanalytics), \"config-file\", \"config_nvdsanalytics.txt\", NULL); /* Override the batch-size set in the config file with the number of sources. */ g_object_get (G_OBJECT (pgie), \"batch-size\", &amp;pgie_batch_size, NULL); if (pgie_batch_size != num_sources) { g_printerr (\"WARNING: Overriding infer-config batch-size (%d) with number of sources (%d)\\n\", pgie_batch_size, num_sources); g_object_set (G_OBJECT (pgie), \"batch-size\", num_sources, NULL); } tiler_rows = (guint) sqrt (num_sources); tiler_columns = (guint) ceil (1.0 * num_sources / tiler_rows); /* we set the tiler properties here */ g_object_set (G_OBJECT (tiler), \"rows\", tiler_rows, \"columns\", tiler_columns, \"width\", TILED_OUTPUT_WIDTH, \"height\", TILED_OUTPUT_HEIGHT, NULL); /* we add a message handler */ bus = gst_pipeline_get_bus (GST_PIPELINE (pipeline)); bus_watch_id = gst_bus_add_watch (bus, bus_call, loop); gst_object_unref (bus); /* Set up the pipeline */ /* we add all elements into the pipeline */ #ifdef PLATFORM_TEGRA gst_bin_add_many (GST_BIN (pipeline), pgie, nvtracker, nvdsanalytics , nvvidconv, nvosd, nvvidconv1, cap_filter, encoder, codecparse, mux, sink, NULL); /* we link the elements together * nvstreammux -&gt; nvinfer -&gt; nvtracker -&gt; nvdsanalytics -&gt; nvtiler -&gt; * nvvideoconvert -&gt; nvosd -&gt; transform -&gt; sink */ if (!gst_element_link_many (streammux, pgie, nvtracker, nvdsanalytics, nvvidconv, nvosd, nvvidconv1, cap_filter, encoder, codecparse, mux, sink, NULL)) { g_printerr (\"Elements could not be linked. Exiting.\\n\"); return -1; } #else gst_bin_add_many (GST_BIN (pipeline), pgie, nvtracker, nvdsanalytics, nvvidconv, nvosd, nvvidconv1, cap_filter, encoder, codecparse, mux, sink, NULL); /* we link the elements together * nvstreammux -&gt; nvinfer -&gt; nvtracker -&gt; nvdsanalytics -&gt; nvtiler -&gt; * nvvideoconvert -&gt; nvosd -&gt; sink */ if (!gst_element_link_many (streammux, pgie, nvtracker, nvdsanalytics, nvvidconv, nvosd, nvvidconv1, cap_filter, encoder, codecparse, mux, sink, NULL)) { g_printerr (\"Elements could not be linked. Exiting.\\n\"); return -1; } #endif /* Lets add probe to get informed of the meta data generated, we add probe to * the sink pad of the nvdsanalytics element, since by that time, the buffer * would have had got all the metadata. */ nvdsanalytics_src_pad = gst_element_get_static_pad (nvdsanalytics, \"src\"); if (!nvdsanalytics_src_pad) g_print (\"Unable to get src pad\\n\"); else gst_pad_add_probe (nvdsanalytics_src_pad, GST_PAD_PROBE_TYPE_BUFFER, nvdsanalytics_src_pad_buffer_probe, NULL, NULL); /* Set the pipeline to \"playing\" state */ g_print (\"Now playing:\"); for (i = 0; i &lt; num_sources; i++) { g_print (\" %s,\", argv[i + 1]); } g_print (\"\\n\"); gst_element_set_state (pipeline, GST_STATE_PLAYING); /* Wait till pipeline encounters an error or EOS */ g_print (\"Running...\\n\"); g_main_loop_run (loop); /* Out of the main loop, clean up nicely */ g_print (\"Returned, stopping playback\\n\"); gst_element_set_state (pipeline, GST_STATE_NULL); g_print (\"Deleting pipeline\\n\"); gst_object_unref (GST_OBJECT (pipeline)); g_source_remove (bus_watch_id); g_main_loop_unref (loop); return 0; } Please let me know if any other information is required from my side. Thank you in advance.",
        "answers": [
            [
                "Modify the pipeline as per the below git diff (note: the below diff is for deepstream-test2 app, it'll work deepstream-nvdsanalytics-test as well) source: diff --git a/deepstream-test2/deepstream_test2_app.c b/deepstream-test2/deepstream_test2_app.c index 2b1ff34..c31441e 100644 --- a/deepstream-test2/deepstream_test2_app.c +++ b/deepstream-test2/deepstream_test2_app.c @@ -318,6 +318,16 @@ main (int argc, char *argv[]) GstBus *bus = NULL; guint bus_watch_id = 0; GstPad *osd_sink_pad = NULL; + + /* Added to save output to file */ + GstElement *nvvidconv1 = NULL, + *filter1 = NULL, *filter2 = NULL, + *filter3 = NULL, + *videoconvert = NULL, + *filter4 = NULL, + *x264enc = NULL, + *qtmux = NULL; + GstCaps *caps1 = NULL, *caps2 = NULL, *caps3 = NULL, *caps4 = NULL; /* Check input arguments */ if (argc != 2) { @@ -373,17 +383,35 @@ main (int argc, char *argv[]) /* Create OSD to draw on the converted RGBA buffer */ nvosd = gst_element_factory_make (\"nvdsosd\", \"nv-onscreendisplay\"); + /* Added to save output to file */ + nvvidconv1 = gst_element_factory_make (\"nvvideoconvert\", \"nvvideo-converter1\"); + videoconvert = gst_element_factory_make (\"videoconvert\", \"converter\"); + x264enc = gst_element_factory_make (\"x264enc\", \"h264 encoder\"); + qtmux = gst_element_factory_make (\"qtmux\", \"muxer\"); + /* Finally render the osd output */ #ifdef PLATFORM_TEGRA transform = gst_element_factory_make (\"nvegltransform\", \"nvegl-transform\"); #endif - sink = gst_element_factory_make (\"nveglglessink\", \"nvvideo-renderer\"); + sink = gst_element_factory_make (\"filesink\", \"nvvideo-renderer\"); + + /* caps filter for nvvidconv to convert NV12 to RGBA as nvosd expects input + * in RGBA format */ + filter1 = gst_element_factory_make (\"capsfilter\", \"filter1\"); + filter2 = gst_element_factory_make (\"capsfilter\", \"filter2\"); + filter3 = gst_element_factory_make (\"capsfilter\", \"filter3\"); + filter4 = gst_element_factory_make (\"capsfilter\", \"filter4\"); if (!source || !h264parser || !decoder || !pgie || !nvtracker || !sgie1 || !sgie2 || !sgie3 || !nvvidconv || !nvosd || !sink) { g_printerr (\"One element could not be created. Exiting.\\n\"); return -1; } + /* Added to test saving output to file */ + if (!nvvidconv1 || !x264enc || !qtmux || !filter3 || !filter4) { + g_printerr (\"One element could not be created. Exiting.\\n\"); + return -1; + } #ifdef PLATFORM_TEGRA if(!transform) { @@ -395,6 +423,9 @@ main (int argc, char *argv[]) /* Set the input filename to the source element */ g_object_set (G_OBJECT (source), \"location\", argv[1], NULL); + /* Added to save output to file */ + g_object_set (G_OBJECT (sink), \"location\", \"out.mp4\", NULL); + g_object_set (G_OBJECT (streammux), \"batch-size\", 1, NULL); g_object_set (G_OBJECT (streammux), \"width\", MUXER_OUTPUT_WIDTH, \"height\", @@ -429,9 +460,24 @@ main (int argc, char *argv[]) #else gst_bin_add_many (GST_BIN (pipeline), source, h264parser, decoder, streammux, pgie, nvtracker, sgie1, sgie2, sgie3, - nvvidconv, nvosd, sink, NULL); + filter1, nvvidconv, filter2, nvosd, nvvidconv1, filter3, videoconvert, filter4, + x264enc, qtmux, sink, NULL); #endif + /* Added to save output to file */ + caps1 = gst_caps_from_string (\"video/x-raw(memory:NVMM), format=NV12\"); + g_object_set (G_OBJECT (filter1), \"caps\", caps1, NULL); + gst_caps_unref (caps1); + caps2 = gst_caps_from_string (\"video/x-raw(memory:NVMM), format=RGBA\"); + g_object_set (G_OBJECT (filter2), \"caps\", caps2, NULL); + gst_caps_unref (caps2); + caps3 = gst_caps_from_string (\"video/x-raw, format=RGBA\"); + g_object_set (G_OBJECT (filter3), \"caps\", caps3, NULL); + gst_caps_unref (caps3); + caps4 = gst_caps_from_string (\"video/x-raw, format=NV12\"); + g_object_set (G_OBJECT (filter4), \"caps\", caps4, NULL); + gst_caps_unref (caps4); + GstPad *sinkpad, *srcpad; gchar pad_name_sink[16] = \"sink_0\"; gchar pad_name_src[16] = \"src\"; @@ -470,7 +516,8 @@ main (int argc, char *argv[]) } #else if (!gst_element_link_many (streammux, pgie, nvtracker, sgie1, - sgie2, sgie3, nvvidconv, nvosd, sink, NULL)) { + sgie2, sgie3, filter1, nvvidconv, filter2, nvosd, nvvidconv1, filter3, + videoconvert, filter4, x264enc, qtmux, sink, NULL)) { g_printerr (\"Elements could not be linked. Exiting.\\n\"); return -1; } For deepstream-nvdsanalytics-test, pipeline would be: gst_bin_add_many (GST_BIN (pipeline), pgie, nvtracker, nvdsanalytics, tiler, filter1, nvvidconv, filter2, nvosd, nvvidconv1, filter3, videoconvert, filter4, x264enc, qtmux, sink, NULL);"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Trying to run nvidia's deepstream5.0 sdk (sample program) on ubuntu 18.04 by following the document (https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html). The application is installed in the path: \"/opt/nvidia/deepstream/deepstream-5.0/\". The execution command is \"deepstream-app -c &lt;config file&gt;\" Example: \"deepstream-app -c /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/source30_1080p_dec_infer-resnet_tiled_display_int8.txt\" However,got the run-time error. The error report is given below. amarnath@amarnath-Precision-T3610:/opt$ deepstream-app -c /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/source30_1080p_dec_infer-resnet_tiled_display_int8.txt ERROR: ../nvdsinfer/nvdsinfer_model_builder.cpp:1408 Deserialize engine failed because file path: /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/../../models/Primary_Detector/resnet10.caffemodel_b30_gpu0_int8.engine open error 0:00:00.324481231 31390 0x564e46ff5ea0 WARN nvinfer gstnvinfer.cpp:599:gst_nvinfer_logger:&lt;primary_gie&gt; NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::deserializeEngineAndBackend() &lt;nvdsinfer_context_impl.cpp:1566&gt; [UID = 1]: deserialize engine from file :/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/../../models/Primary_Detector/resnet10.caffemodel_b30_gpu0_int8.engine failed 0:00:00.324517060 31390 0x564e46ff5ea0 WARN nvinfer gstnvinfer.cpp:599:gst_nvinfer_logger:&lt;primary_gie&gt; NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::generateBackendContext() &lt;nvdsinfer_context_impl.cpp:1673&gt; [UID = 1]: deserialize backend context from engine from file :/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/../../models/Primary_Detector/resnet10.caffemodel_b30_gpu0_int8.engine failed, try rebuild 0:00:00.324530469 31390 0x564e46ff5ea0 INFO nvinfer gstnvinfer.cpp:602:gst_nvinfer_logger:&lt;primary_gie&gt; NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::buildModel() &lt;nvdsinfer_context_impl.cpp:1591&gt; [UID = 1]: Trying to create engine from model files WARNING: ../nvdsinfer/nvdsinfer_model_builder.cpp:1163 INT8 not supported by platform. Trying FP16 mode. WARNING: ../nvdsinfer/nvdsinfer_model_builder.cpp:1177 FP16 not supported by platform. Using FP32 mode. WARNING: ../nvdsinfer/nvdsinfer_func_utils.cpp:34 [TRT]: TensorRT was linked against cuDNN 7.6.4 but loaded cuDNN 7.6.3 INFO: ../nvdsinfer/nvdsinfer_func_utils.cpp:37 [TRT]: Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output. INFO: ../nvdsinfer/nvdsinfer_func_utils.cpp:37 [TRT]: Detected 1 inputs and 2 output network tensors. WARNING: ../nvdsinfer/nvdsinfer_func_utils.cpp:34 [TRT]: TensorRT was linked against cuDNN 7.6.4 but loaded cuDNN 7.6.3 0:00:04.170021642 31390 0x564e46ff5ea0 INFO nvinfer gstnvinfer.cpp:602:gst_nvinfer_logger:&lt;primary_gie&gt; NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::buildModel() &lt;nvdsinfer_context_impl.cpp:1624&gt; [UID = 1]: serialize cuda engine to file: /opt/nvidia/deepstream/deepstream-5.0/samples/models/Primary_Detector/resnet10.caffemodel_b30_gpu0_fp32.engine successfully WARNING: ../nvdsinfer/nvdsinfer_func_utils.cpp:34 [TRT]: TensorRT was linked against cuDNN 7.6.4 but loaded cuDNN 7.6.3 WARNING: ../nvdsinfer/nvdsinfer_func_utils.cpp:34 [TRT]: Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles INFO: ../nvdsinfer/nvdsinfer_model_builder.cpp:685 [Implicit Engine Info]: layers num: 3 0 INPUT kFLOAT input_1 3x368x640 1 OUTPUT kFLOAT conv2d_bbox 16x23x40 2 OUTPUT kFLOAT conv2d_cov/Sigmoid 4x23x40 0:00:04.175528889 31390 0x564e46ff5ea0 INFO nvinfer gstnvinfer_impl.cpp:311:notifyLoadModelStatus:&lt;primary_gie&gt; [UID 1]: Load new model:/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/config_infer_primary.txt sucessfully Runtime commands: h: Print this help q: Quit p: Pause r: Resume NOTE: To expand a source in the 2D tiled display and view object details, left-click on the source. To go back to the tiled display, right-click anywhere on the window. ** INFO: &lt;bus_callback:181&gt;: Pipeline ready ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: engine.cpp (418) - Cuda Error in enqueueInternal: 209 (no kernel image is available for execution on the device) ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: FAILED_EXECUTION: std::exception ERROR: nvdsinfer_backend.cpp:290 Failed to enqueue inference batch ERROR: nvdsinfer_context_impl.cpp:1408 Infer context enqueue buffer failed, nvinfer error:NVDSINFER_TENSORRT_ERROR 0:00:04.432182851 31390 0x564e400e0b20 WARN nvinfer gstnvinfer.cpp:1188:gst_nvinfer_input_queue_loop:&lt;primary_gie&gt; error: Failed to queue input batch for inferencing ERROR from primary_gie: Failed to queue input batch for inferencing Debug info: gstnvinfer.cpp(1188): gst_nvinfer_input_queue_loop (): /GstPipeline:pipeline/GstBin:primary_gie_bin/GstNvInfer:primary_gie Quitting ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: engine.cpp (418) - Cuda Error in enqueueInternal: 209 (no kernel image is available for execution on the device) ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: FAILED_EXECUTION: std::exception ERROR: nvdsinfer_backend.cpp:290 Failed to enqueue inference batch ERROR: nvdsinfer_context_impl.cpp:1408 Infer context enqueue buffer failed, nvinfer error:NVDSINFER_TENSORRT_ERROR 0:00:04.476620553 31390 0x564e400e0b20 WARN nvinfer gstnvinfer.cpp:1188:gst_nvinfer_input_queue_loop:&lt;primary_gie&gt; error: Failed to queue input batch for inferencing ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: engine.cpp (418) - Cuda Error in enqueueInternal: 209 (no kernel image is available for execution on the device) ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: FAILED_EXECUTION: std::exception ERROR: nvdsinfer_backend.cpp:290 Failed to enqueue inference batch ERROR: nvdsinfer_context_impl.cpp:1408 Infer context enqueue buffer failed, nvinfer error:NVDSINFER_TENSORRT_ERROR 0:00:04.541993813 31390 0x564e400e0b20 WARN nvinfer gstnvinfer.cpp:1188:gst_nvinfer_input_queue_loop:&lt;primary_gie&gt; error: Failed to queue input batch for inferencing ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: engine.cpp (418) - Cuda Error in enqueueInternal: 209 (no kernel image is available for execution on the device) ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: FAILED_EXECUTION: std::exception ERROR: nvdsinfer_backend.cpp:290 Failed to enqueue inference batch ERROR: nvdsinfer_context_impl.cpp:1408 Infer context enqueue buffer failed, nvinfer error:NVDSINFER_TENSORRT_ERROR 0:00:04.608814180 31390 0x564e400e0b20 WARN nvinfer gstnvinfer.cpp:1188:gst_nvinfer_input_queue_loop:&lt;primary_gie&gt; error: Failed to queue input batch for inferencing ERROR from primary_gie: Failed to queue input batch for inferencing Debug info: gstnvinfer.cpp(1188): gst_nvinfer_input_queue_loop (): /GstPipeline:pipeline/GstBin:primary_gie_bin/GstNvInfer:primary_gie ERROR from primary_gie: Failed to queue input batch for inferencing Debug info: gstnvinfer.cpp(1188): gst_nvinfer_input_queue_loop (): /GstPipeline:pipeline/GstBin:primary_gie_bin/GstNvInfer:primary_gie ERROR from primary_gie: Failed to queue input batch for inferencing Debug info: gstnvinfer.cpp(1188): gst_nvinfer_input_queue_loop (): /GstPipeline:pipeline/GstBin:primary_gie_bin/GstNvInfer:primary_gie ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: engine.cpp (418) - Cuda Error in enqueueInternal: 209 (no kernel image is available for execution on the device) ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: FAILED_EXECUTION: std::exception ERROR: nvdsinfer_backend.cpp:290 Failed to enqueue inference batch ERROR: nvdsinfer_context_impl.cpp:1408 Infer context enqueue buffer failed, nvinfer error:NVDSINFER_TENSORRT_ERROR 0:00:04.774548068 31390 0x564e400e0b20 WARN nvinfer gstnvinfer.cpp:1188:gst_nvinfer_input_queue_loop:&lt;primary_gie&gt; error: Failed to queue input batch for inferencing ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: engine.cpp (418) - Cuda Error in enqueueInternal: 209 (no kernel image is available for execution on the device) ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: FAILED_EXECUTION: std::exception ERROR: nvdsinfer_backend.cpp:290 Failed to enqueue inference batch ERROR: nvdsinfer_context_impl.cpp:1408 Infer context enqueue buffer failed, nvinfer error:NVDSINFER_TENSORRT_ERROR 0:00:04.778180781 31390 0x564e400e0b20 WARN nvinfer gstnvinfer.cpp:1188:gst_nvinfer_input_queue_loop:&lt;primary_gie&gt; error: Failed to queue input batch for inferencing ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: engine.cpp (418) - Cuda Error in enqueueInternal: 209 (no kernel image is available for execution on the device) ERROR: ../nvdsinfer/nvdsinfer_func_utils.cpp:31 [TRT]: FAILED_EXECUTION: std::exception ERROR: nvdsinfer_backend.cpp:290 Failed to enqueue inference batch ERROR: nvdsinfer_context_impl.cpp:1408 Infer context enqueue buffer failed, nvinfer error:NVDSINFER_TENSORRT_ERROR 0:00:04.848116827 31390 0x564e400e0b20 WARN nvinfer gstnvinfer.cpp:1188:gst_nvinfer_input_queue_loop:&lt;primary_gie&gt; error: Failed to queue input batch for inferencing App run failed The output screen: My nvidia driver and cuda version shown below: Any help is appreciated.",
        "answers": [
            [
                "For the original question, the config files have relative file paths so you need to change your shell working directory to the location of the config file. For the second question by Krunal, review the /opt/nvidia/deepstream/deepstream/samples/configs/tlt_pretrained_models/README which contains commands to run to download the model files. Also be sure to execute deepstream-app from that tlt_pretrained_models directory since the config files contain relative paths."
            ],
            [
                "I also had that blank deepstream 5.0 screen . In my case the problem was solved by updating the driver NVIDIA driver 440.64 to NVIDIA driver 450.51 . Anyway, to evade burning up in dependency hell, just simply pull the DeepStream SDK docker container image and run it with nvidia-docker installed in your host machine : https://ngc.nvidia.com/catalog/containers/nvidia:deepstream UPDATE the nvcr.io/nvidia/deepstream:5.0-20.07-devel container works even with older versions of nvidia driver : i successfully tested the samples with NVIDIA driver 440.95.01 in my ubuntu - GTX 1050 laptop ."
            ],
            [
                "OKAY!!! try: sudo apt-get install nvidia-prime -y echo \"installed prime\" sudo prime-select nvidia reboot run deepstream and enjoy \ud83d\ude0e .. output: tested on both GTX,RTX dgpus try easy deepstream on https://github.com/bharath5673/Deepstream"
            ],
            [
                "Deepstream 5.0 GA is meant to run with JetPack 4.4, which includes CUDA 10.2, TensorRT 7.1 and cuDNN 8.0. You're getting cuDNN version mismatch warning as well. Try using NVIDIA SDK Manager (as horrible as it is) to install the correct environment, after removing your current CUDA setup. You can skip the Target Components if you want it to be installed on your machine only. It is also worth updating your driver to the latest available, as mentioned by @Fizmath."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07,
            1e-07,
            -0.9999999
        ]
    },
    {
        "question": "I was running a Nvidia deepstream container on one of the GPU worker nodes in kubernetes cluster which is deployed as a job and below is my YAML file, apiVersion: batch/v1 kind: Job metadata: name: deepstream-test spec: backoffLimit: 1 template: spec: containers: - name: nvidia-deepstream image: lkkrishna945/deepstream-5.0:test command: [\"deepstream-app\",\"-c\",\"/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/source30_1080p_dec_infer-resnet_tiled_display_int8_edited.txt\"] ports: - containerPort: 8554 resources: limits: nvidia.com/gpu: 1 restartPolicy: Never After I deployed this job it is running fine but the output of this is running on one of the worker nodes of on-perm opensource kubernetes cluster but I wanted to stream that running output which is a video. Here's my Dockerfile which is built using base Nvidia deepstream container, FROM nvcr.io/nvidia/deepstream:5.0-dp-20.04-triton ADD source30_1080p_dec_infer-resnet_tiled_display_int8_edited.txt /opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/ CMD [\"deepstream-app -c /samples/configs/deepstream-app/source30_1080p_dec_infer-resnet_tiled_display_int8_edited.txt\"] Can anyone help with any suggestions/solution on this?",
        "answers": [
            [
                "If you are streaming the output via RTSP, then you need to expose the port to which you are streaming RTSP and then map the docker port with host port. Then you can tap to the RTSP stream."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I was trying to install tensorRT 7.0 in ubuntu 18.4 (nv-tensorrt-repo-ubuntu1804-cuda10.2-trt7.0.0.11-ga-20191216_1-1_amd64.deb) debian. Followed the documentation https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-debian. I am getting the below error with libnvinfer7. Searching for this around the planet, unable to find, lost my time and sleep. Kindly help me out with this: amarnath@amarnath-Precision-T3610:/opt/pixuate$ sudo apt install tensorrt Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: tensorrt : Depends: libnvinfer7 (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvinfer-plugin7 (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvparsers7 (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvonnxparsers7 (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvinfer-bin (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvinfer-dev (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvinfer-plugin-dev (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvparsers-dev (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvonnxparsers-dev (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvinfer-samples (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvinfer-doc (= 7.0.0-1+cuda10.2) but it is not going to be installed E: Unable to correct problems, you have held broken packages. Well, tried \"sudo apt-get install python3-libnvinfer-dev\" amarnath@amarnath-Precision-T3610:/opt/pixuate$ sudo apt-get install python3-libnvinfer-dev Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: python3-libnvinfer-dev : Depends: python3-libnvinfer (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvinfer-dev (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvinfer-plugin-dev (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvparsers-dev (= 7.0.0-1+cuda10.2) but it is not going to be installed Depends: libnvonnxparsers-dev (= 7.0.0-1+cuda10.2) but it is not going to be installed E: Unable to correct problems, you have held broken packages.",
        "answers": [
            [
                "In the TensorRT installation section from https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html there is this sentence: requires that the CUDA Toolkit and cuDNN have also been installed using Debian or RPM packages If you install the CUDA toolkit and cuDNN using deb files the unmet dependencies error should be resolved. NOTE: Before installing check the versions of Ubuntu, CUDA and cuDNN you want to install. In the installation tips below CUDA 10.2 and cuDNN 7.6.5 were used. This is tested for TensorRT 7.0.0. CUDA .deb install wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804x86_64cuda-ubuntu1804.pin sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb sudo apt-key add /var/cuda-repo-10-2-local-10.2.89-440.33.01/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda CUDNN .deb install First download the .deb files: cuDNN Developer Library for Ubuntu18.04 (Deb) cuDNN Runtime Library for Ubuntu18.04 (Deb) After that install the downloaded packages: sudo dpkg -i libcudnn7_7.6.5.32-1+cuda10.2_amd64.deb sudo dpkg -i libcudnn7-dev_7.6.5.32-1+cuda10.2_amd64.deb NOTE: These installation instructions are from the official nvidia web sites"
            ],
            [
                "By default, the system will try to upgrade the libnvinfer versions to the most latest ones (including upgrading CUDA to version 11.x). So we first need to install the necessary versions and then place a hold on them to restrict any automatic upgrade attempts that may cause unmet dependencies. sudo apt-get install libnvinfer7=7.0.0-1+cuda10.2 libnvonnxparsers7=7.0.0-1+cuda10.2 libnvparsers7=7.0.0-1+cuda10.2 libnvinfer-plugin7=7.0.0-1+cuda10.2 libnvinfer-dev=7.0.0-1+cuda10.2 libnvonnxparsers-dev=7.0.0-1+cuda10.2 libnvparsers-dev=7.0.0-1+cuda10.2 libnvinfer-plugin-dev=7.0.0-1+cuda10.2 python3-libnvinfer=7.0.0-1+cuda10.2 sudo apt-mark hold libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 libnvinfer-dev libnvonnxparsers-dev libnvparsers-dev libnvinfer-plugin-dev python3-libnvinfer python3-libnvinfer-dev After that you can install tensorrt without any problems: sudo apt-get install tensorrt"
            ],
            [
                "I have this problem for tensorrt 8.2.5-1 and cuda11.6 : tensorrt : Depends: libnvinfer-samples (= 8.2.5-1+cuda11.4) but it is not going to be installed It was solved by installing cuda 11.7 and tensorrt 8.4.1.5. TensorRT installation fails, missing libnvinfer-samples"
            ]
        ],
        "votes": [
            7.0000001,
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "I've had issues setting up the DeepstreamSdk IoTedge module on the jetson nano for the last week and I cannot get past it. I've installed IoTedge runtime and all the necessities for IoT edge to run. It runs perfectly including other modules like the simulated Temperature sensor. However when I deploy the DeepstreamSdk v-4.02 on the Jetson nano running Jetpack 4.3, it starts and runs for a couple of minutes but then fails unexpectedly and then after a bit of time starts up again and then fails again. And then Sometimes I restart the IoTedge and it will start up again and then fail. When I use the Developer extension IoTedge in VS code to see what the messages being sent up to the cloud are, I can see the temperature sensor module's messages, however none from the NvidiaDeepstream module. I've had a look in the logs for the NvidiaDeepstream container, and it shows that it is printing out results ( messages to the cloud), but then eventually sends an error code 1. and some sort of message at the end INT8 is not supported, try INT16. All the Azure checks and connectivity and configurations are correct. It is just the deepstreamdk module that doesn't run properly. Does anyone have any suggestions? What info should I provide to make this more clear and understandable? I am following the tutorial on the Github repository for NVIDIA Deepstream + Azure IoT Edge on a NVIDIA Jetson Nano: Link to turorial Link to logs of COntainers",
        "answers": [],
        "votes": []
    }
]