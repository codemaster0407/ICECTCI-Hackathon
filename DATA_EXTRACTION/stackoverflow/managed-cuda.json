[
    {
        "question": "Specifically, my issue is that I have CUDA code that needs &lt;curand_kernel.h&gt; to run. This isn't included by default in NVRTC. Presumably then when creating the program context (i.e. the call to nvrtcCreateProgram), I have to send in the name of the file (curand_kernel.h) and also the source code of curand_kernel.h? I feel like I shouldn't have to do that. It's hard to tell; I haven't managed to find an example from NVIDIA of someone needing standard CUDA files like this as a source, so I really don't understand what the syntax is. Some issues: curand_kernel.h also has includes... Do I have to do the same for each of these? I am not even sure the NVRTC compiler will even run correctly on curand_kernel.h, because there are some language features it doesn't support, aren't there? Next: if you've sent in the source code of a header file to nvrtcCreateProgram, do I still have to #include it in the code to be executed / will it cause an error if I do so? A link to example code that does this or something like it would be appreciated much more than a straightforward answer; I really haven't managed to find any.",
        "answers": [
            [
                "You have to send the \"filename\" and the source of each header separately. When the preprocessor does its thing, it'll use any #include filenames as a key to find the source for the header, based on the collection that you provide. I suspect that, in this case, the compiler (driver) doesn't have file system access, so you have to give it the source in much the same way that you would for shader includes in OpenGL. So: Include your header's name when calling nvrtcCreateProgram. The compiler will, internally, generate the equivalent of a std::map&lt;string,string&gt; containing the source of each header indexed by the given name. In your kernel source, use #include \"foo.cuh\" as usual. The compiler will use foo.cuh as an index or key into its internal map (created when you called nvrtcCreateProgram), and will retrieve the header source from that collection Compilation proceeds as normal. One of the reasons that nvrtc provides only a \"subset\" of features is that the compiler plays in a somewhat sandboxed environment, without necessarily having all of the supporting tools and utilities lying around that you have with offline compilation. So, you have to manually handle a lot of the stuff that the normal nvcc + (gcc | MSVC| clang) combination provides. A possible, but non-ideal, solution would be to preprocess the file that you need in your IDE, save the result and then #include that. However, I bet there is a better way to do that. if you just want curand, consider diving into the library and extracting the part you need (blech) or using another GPU-friendly rand implementation. On older CUDA versions, I just generated a big array of random floats on the host, uploaded it to the GPU, and sampled it in the kernels. This related link may be helpful."
            ],
            [
                "You do not need to load curand_kernel.h yourself and add it to the include \"aliases\" mechanism. Instead, you can simply add the CUDA include directory to your (set of) include paths, e.g. by adding --include-path=/usr/local/cuda/include to your NVRTC compiler options. (I do this in my GPU-kernel-runner test harness, by default, to be on the safe side.)"
            ]
        ],
        "votes": [
            5.0000001,
            1e-07
        ]
    },
    {
        "question": "I want to use a string matching in managedCuda. But how can I initialized it? i've tried using C# version, here's the examples: stringAr = new List&lt;string&gt;(); stringAr.Add(\"you\"); stringAr.Add(\"your\"); stringAr.Add(\"he\"); stringAr.Add(\"she\"); stringAr.Add(\"shes\"); for the string matching, i've used this code: bool found = false; for (int i = 0; i &lt; stringAr.Count; i++) { found = (stringAr[i]).IndexOf(textBox2.Text) &gt; -1; if (found) break; } if (found &amp;&amp; textBox2.Text != \"\") { label1.Text = \"Found!\"; } else { label1.Text = \"Not Found!\"; } I also allocate input h_A in host memory string[] h_B = new string[N]; When i want to allocate in device memory and copy vectors from host memory to device memory CudaDeviceVariable&lt;string[]&gt; d_B = h_B; It gave me this error The type 'string[]' must be a non-nullable value type in order to use it as parameter 'T' in the generic type or method 'CudaDeviceVariable&lt;T&gt;' any help?",
        "answers": [
            [
                "Based on the documentation and your error message, only non-nullable value type can be used with CudaDeviceVariable. Change stringAr list to a char[] (or byte[]) array, and then allocate it on device by using CudaDeviceVariable with generic parameter char (or byte). EDIT1 Here is the code that change stringAr to byte[] array: byte[] stringArAsBytes = stringAr .SelectMany(s =&gt; System.Text.Encoding.ASCII.GetBytes(s)) .ToArray(); then try something like this: CudaDeviceVariable&lt;byte&gt; d_data = stringArAsBytes;"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I've a library in C++ to run a string matching (PFAC library)PFAC-lib. How do i run this library from WinForm C#? I also used managedCuda to run cuda code from my C#. Any idea?",
        "answers": [
            [
                "\"PFAC library provides C-style API. Programmers can link the library to C/C++ code or other language.\" You should be able to dllimport the parts you need for the code to work. but you might have a hard time as i just looked at the struct create needs. good luck"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "issue: As I increase the amount of data that is being processed inside of loop that is inside of CUDA kernel - it causes the app to abort! exception: ManagedCuda.CudaException: 'ErrorLaunchFailed: An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointer and accessing out of bounds shared memory. question: I would appreciate if somebody could shed a light on limitations that I am hitting with my current implementation and what exactly causes the app to crash.. Alternatively, I am attaching a full kernel code, for the sake if somebody could say how it can be re-modelled in such a way, when no exceptions are thrown. The idea is that kernel is accepting combinations and then performing calculations on the same set of data (in a loop). Therefore, loop calculations that are inside shall be sequential. The sequence in which kernel itself is executed is irrelevant. It's combinatorics problem. Any bit of advice is welcomed. code (Short version, which is enough to abort the app): extern \"C\" { __device__ __constant__ int arraySize; __global__ void myKernel( unsigned char* __restrict__ output, const int* __restrict__ in1, const int* __restrict__ in2, const double* __restrict__ in3, const unsigned char* __restrict__ in4) { for (int row = 0; row &lt; arraySize; row++) { // looping over sequential data. } } } In the example above if the arraySize is somewhere close to 50_000 then the app starts to abort. With the same kind of input parameters, if we override or hardcore the arraySize to 10_000 then the code finishes successfully. code - kernel (full version) #iclude &lt;cuda.h&gt; #include \"cuda_runtime.h\" #include &lt;device_launch_parameters.h&gt; #include &lt;texture_fetch_functions.h&gt; #include &lt;builtin_types.h&gt; #define _SIZE_T_DEFINED #ifndef __CUDACC__ #define __CUDACC__ #endif #ifndef __cplusplus #define __cplusplus #endif texture&lt;float2, 2&gt; texref; extern \"C\" { __device__ __constant__ int width; __device__ __constant__ int limit; __device__ __constant__ int arraySize; __global__ void myKernel( unsigned char* __restrict__ output, const int* __restrict__ in1, const int* __restrict__ in2, const double* __restrict__ in3, const unsigned char* __restrict__ in4) { int index = blockIdx.x * blockDim.x + threadIdx.x; if (index &gt;= limit) return; bool isTrue = false; int varA = in1[index]; int varB = in2[index]; double calculatable = 0; for (int row = 0; row &lt; arraySize; row++) { if (isTrue) { int idx = width * row + varA; if (!in4[idx]) continue; calculatable = calculatable + in3[row]; isTrue = false; } else { int idx = width * row + varB; if (!in4[idx]) continue; calculatable = calculatable - in3[row]; isTrue = true; } } if (calculatable &gt;= 0) { output[index] = 1; } } } code - host (full version) public static void test() { int N = 10_245_456; // size of an output CudaContext cntxt = new CudaContext(); CUmodule cumodule = cntxt.LoadModule(@\"kernel.ptx\"); CudaKernel myKernel = new CudaKernel(\"myKernel\", cumodule, cntxt); myKernel.GridDimensions = (N + 255) / 256; myKernel.BlockDimensions = Math.Min(N, 256); // output byte[] out_host = new byte[N]; // i.e. bool var out_dev = new CudaDeviceVariable&lt;byte&gt;(out_host.Length); // input int[] in1_host = new int[N]; int[] in2_host = new int[N]; double[] in3_host = new double[50_000]; // change it to 10k and it's OK byte[] in4_host = new byte[10_000_000]; // i.e. bool var in1_dev = new CudaDeviceVariable&lt;int&gt;(in1_host.Length); var in2_dev = new CudaDeviceVariable&lt;int&gt;(in2_host.Length); var in3_dev = new CudaDeviceVariable&lt;double&gt;(in3_host.Length); var in4_dev = new CudaDeviceVariable&lt;byte&gt;(in4_host.Length); // copy input parameters in1_dev.CopyToDevice(in1_host); in2_dev.CopyToDevice(in2_host); in3_dev.CopyToDevice(in3_host); in4_dev.CopyToDevice(in4_host); myKernel.SetConstantVariable(\"width\", 2); myKernel.SetConstantVariable(\"limit\", N); myKernel.SetConstantVariable(\"arraySize\", in3_host.Length); // exception is thrown here myKernel.Run(out_dev.DevicePointer, in1_dev.DevicePointer, in2_dev.DevicePointer,in3_dev.DevicePointer, in4_dev.DevicePointer); out_dev.CopyToHost(out_host); } analysis My initial assumption was that I am having memory issues, however, according to VS debugger I am hitting a little above 500mb of data on a host environment. So I imagine that no matter how much data I copy to GPU - it shouldn't exceed 1Gb or even maximum 11Gb. Later on I have noticed that the crashing only is happening when the loop that is inside a kernel is having many records of data to process. It makes me to believe that I am hitting some kind of thread time-out limitations or something of that sort. Without a solid proof. system My system specs are 16Gb of Ram, and GeForce 1080 Ti 11Gb. Using Cuda 9.1., and managedCuda version 8.0.22 (also tried with 9.x version from master branch) edit 1: 26.04.2018 Just tested the same logic, but only on OpenCL. The code not only finished successfully, but also performs 1.5-5x time better than the CUDA, depending on the input parameter sizes: kernel void Test (global bool* output, global const int* in1, global const int* in2, global const double* in3, global const bool* in4, const int width, const int arraySize) { int index = get_global_id(0); bool isTrue = false; int varA = in1[index]; int varB = in2[index]; double calculatable = 0; for (int row = 0; row &lt; arraySize; row++) { if (isTrue) { int idx = width * row + varA; if (!in4[idx]) { continue; } calculatable = calculatable + in3[row]; isTrue = false; } else { int idx = width * row + varB; if (!in4[idx]) { continue; } calculatable = calculatable - in3[row]; isTrue = true; } } if (calculatable &gt;= 0) { output[index] = true; } } I don't really want to start OpenCL/CUDA war here. If there is anything I should be concerned about in my original CUDA implementation - please let me know. edit: 26.04.2018. After following suggestions from the comment section I was able to increase the amount of data processed, before an exception is thrown, by 3x. I was able to achieve that by switching to .ptx generated in Release mode, rather than Debug mode. This improvement could be related to the fact that in Debug settings we also have Generate GPU Debug information set to Yes and other unnecessary settings that could affect performance.. I will now try to search info about how timings can be increased for kernel.. I am still not reaching the results of OpenCL, but getting close. For CUDA file generation I am using VS2017 Community, CUDA 9.1 project, v140 toolset, build for x64 platform, post build events disabled, configuration type: utility. Code generation set to: compute_30,sm_30. I am not sure why it's not sm_70, for example. I don't have other options.",
        "answers": [
            [
                "I have managed to improve the CUDA performance over OpenCL. And what's more important - the code can now finish executing without exceptions. The credits go to Robert Crovella. Thank You! Before showing the results here are some specs: CPU Intel i7 8700k 12 cores (6+6) GPU GeForce 1080 Ti 11Gb Here are my results (library/technology): CPU parallel for loop: 607907 ms (default) GPU (Alea, CUDA): 9905 ms (x61) GPU (managedCuda, CUDA): 6272 ms (x97) GPU (Coo, OpenCL): 8277 ms (x73) THE solution 1: The solution was to increase the WDDM TDR Delay from default 2 seconds to 10 seconds. As easy as that. The solution 2: I was able to squeeze out a bit more of performance by: updating the compute_30,sm_30 settings to compute_61,sm_61 in CUDA project properties using the Release settings instead of Debug using .cubin file instead of .ptx If anyone still wants to suggesst some ideas on how to improve the performance any further - please share them! I am opened to ideas. This question has been resolved, though! p.s. if your display blinks in the same fashion as described here, then try increasing the delay as well."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Problem Description I try to get a kernel summing up all elements of an array to work. The kernel is intended to be launched with 256 threads per block and an arbitary number of blocks. The length of the array passsed in as a is always a multiple of 512, in fact it is #blocks * 512. One block of the kernel should sum up 'its' 512 elements (256 threads can sum up 512 elements using this algorithm), storing the result in out[blockIdx.x]. The final summation over the values in out ,and therefore the results of the blocks, will be done on the host. This kernel works fine for up to 6 blocks, meaning up to 3072 elements. But launching it with more than 6 blocks result in the first block calculating a strictly greater, wrong result than the other blocks (i. e. out = {572, 512, 512, 512, 512, 512, 512}), this wrong result is reproducable, the wrong value is the same for multiple executions. I guess this means there is a structural error somewhere in my code, which has something to do with blockIdx.x, but the only use this is to calculate blockStart, and this seams to be a correct calculation, also for the first block. I verified if my host code computes the correct number of blocks for the kernel and passes in an array of correct size. That's not the problem. Of course I read a lot of similar questions here on stackoverflow, but none seems to describe my problem (See i. e. here or here) The kernel is called via managedCuda (C#), I don't know if this might be a problem. Hardware I use a MX150 with the follwing specifications: Revision Number: 6.1 Total global memory: 2147483648 Total shared memory per block: 49152 Total registers per block: 65536 Warp size: 32 Max Threads per block: 1024 Max Blocks: 2147483648 Number of multiprocessors: 3 Code Kernel __global__ void Vector_Reduce_As_Sum_Kernel(float* out, float* a) { int tid = threadIdx.x; int blockStart = blockDim.x * blockIdx.x * 2; int i = tid + blockStart; int leftSumElementIdx = blockStart + tid * 2; a[i] = a[leftSumElementIdx] + a[leftSumElementIdx + 1]; __syncthreads(); if (tid &lt; 128) { a[i] = a[leftSumElementIdx] + a[leftSumElementIdx + 1]; } __syncthreads(); if(tid &lt; 64) { a[i] = a[leftSumElementIdx] + a[leftSumElementIdx + 1]; } __syncthreads(); if (tid &lt; 32) { a[i] = a[leftSumElementIdx] + a[leftSumElementIdx + 1]; } __syncthreads(); if (tid &lt; 16) { a[i] = a[leftSumElementIdx] + a[leftSumElementIdx + 1]; } __syncthreads(); if (tid &lt; 8) { a[i] = a[leftSumElementIdx] + a[leftSumElementIdx + 1]; } __syncthreads(); if (tid &lt; 4) { a[i] = a[leftSumElementIdx] + a[leftSumElementIdx + 1]; } __syncthreads(); if (tid &lt; 2) { a[i] = a[leftSumElementIdx] + a[leftSumElementIdx + 1]; } __syncthreads(); if (tid == 0) { out[blockIdx.x] = a[blockStart] + a[blockStart + 1]; } } Kernel Invocation //Get the cuda kernel //PathToPtx and MangledKernelName must be replaced CudaContext cntxt = new CudaContext(); CUmodule module = cntxt.LoadModule(\"pathToPtx\"); CudaKernel vectorReduceAsSumKernel = new CudaKernel(\"MangledKernelName\", module, cntxt); //Get an array to reduce float[] array = new float[4096]; for(int i = 0; i &lt; array.Length; i++) { array[i] = 1; } //Calculate execution info for the kernel int threadsPerBlock = 256; int numOfBlocks = array.Length / (threadsPerBlock * 2); //Memory on the device CudaDeviceVariable&lt;float&gt; m_d = array; CudaDeviceVariable&lt;float&gt; out_d = new CudaDeviceVariable&lt;float&gt;(numOfBlocks); //Give the kernel necessary execution info vectorReduceAsSumKernel.BlockDimensions = threadsPerBlock; vectorReduceAsSumKernel.GridDimensions = numOfBlocks; //Run the kernel on the device vectorReduceAsSumKernel.Run(out_d.DevicePointer, m_d.DevicePointer); //Fetch the result float[] out_h = out_d; //Sum up the partial sums on the cpu float sum = 0; for(int i = 0; i &lt; out_h.Length; i++) { sum += out_h[i]; } //Verify the correctness if(sum != 4096) { throw new Exception(\"Thats the wrong result!\"); } Update: The very helpfull and only answer did address all my problems. Thank you! The problem was an unforeseen race condition. Important Hint: In the comments the author of managedCuda pointed out all NPPs methods are indeed already implmented in managedCuda (using ManagedCuda.NPP.NPPsExtensions;). I wasn't aware of that, and i guess so are many people reading ths question.",
        "answers": [
            [
                "You are not correctly incorporating into your code the idea that each block will process 512 elements out of your total array. According to my testing, you need to make at least 2 changes to fix this: In the kernel, you have incorrectly calculated the starting point for each block: int blockStart = blockDim.x * blockIdx.x; since blockDim.x is 256, but each block processes 512 elements, you must multiply this by 2. (the multiplication by 2 in your calculation of leftSumElementIdx doesn't take care of this -- since it is only multiplying tid). In your host code, your number of blocks calculation is incorrect: vectorReduceAsSumKernel.GridDimensions = array.Length / threadsPerBlock; for a value of 2048 for array.Length and a value of 256 for threadsPerBlock, this creates 8 blocks. But as you already indicate, your intention is to launch for blocks (2048/512). So you need to multiply the denominator by 2: vectorReduceAsSumKernel.GridDimensions = array.Length / (2*threadsPerBlock); In addition, your reduction sweep pattern is broken. It is warp-execution-order dependent, to give the proper result, and CUDA does not specify a warp execution order. To see why, let's take a simple example. Let's consider just a single threadblock, with a starting point of the array being all 1, just as you have initialized it. Now, warp 0 consists of threads 0-31. Your reduction sweep operation is like this: a[i] = a[leftSumElementIdx] + a[leftSumElementIdx + 1]; So each thread in warp 0 will collect two other values and add them, and store them. Thread 31 will take the values a[62] and a[63] and add them together. If the values of a[62] and a[63] are still 1, as initialized, then this will work as expected. But the values of a[62] and a[63] are written to by warp 1, consisting of threads 32-63. So if warp 1 executes before warp 0 (perfectly legal), then you will get a different result. This is a global memory race condition. It is arising due to the fact that your input array is both the source and destination of your intermediate results, and __syncthreads() will not sort this out for you. It doesn't force warps to execute in any particular order. One possible solution is to fix your sweep pattern. On any given reduction cycle, let's have a sweep pattern where each thread writes and reads values that are not touched by any other thread during that cycle. The following adaptation of your kernel code accomplishes that: __global__ void Vector_Reduce_As_Sum_Kernel(float* out, float* a) { int tid = threadIdx.x; int blockStart = blockDim.x * blockIdx.x * 2; int i = tid + blockStart; for (int j = blockDim.x; j &gt; 0; j&gt;&gt;=1){ if (tid &lt; j) a[i] += a[i+j]; __syncthreads();} if (tid == 0) { out[blockIdx.x] = a[i]; } } For general purpose reductions, this is still a very slow method. This tutorial covers how to write faster reductions. And, as already pointed out, managedCuda may have methods to avoid writing a kernel at all."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Hello I need to retrieve the cuda version whether its cuda8 or cuda9. anyone know a way I could tell? I heard cuda managed would tell me but all it did was tell me the driver version \"361.xx\"",
        "answers": [
            [
                "Given you are using Managed CUDA, you can't. Managed CUDA (and most other frameworks for using CUDA APIs in other languages like JCUDA and PyCUDA) use the CUDA driver API. There is no concept of \"CUDA version\" in the driver API, that is strictly a feature of the runtime API. As a result, you can't get the runtime API version with the driver API. The only relevant version in the driver API is the driver version, which you already have found out how to access."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am porting some calculation from C# to CUDA. There many classes in C# which I want to port, for each c# class I create .cu and .cuh file in my CUDA project. All classes related, and all they used in calculations. I need to save structure of my C# code, because it will be very easy to made error in other case. P.S. In case I put all code in one file - everything works as expected but read or fix some issues becomes real pain. I want to compile CUDA project and use it in my C# via ManagedCuda library. I can compile test CUDA project with one .cu file to .ptx file, load it in C# via ManagedCuda and call function from it. But when I want to compile my real projects with multiple cu files, in result I got multiple .ptx files for each .cu file in project, even more I am not able to load this .ptx file via ManagedCuda, I got next error: ErrorInvalidPtx: This indicates that a PTX JIT compilation failed. But this error expected, because there cross reference in ptx files, and they have sense only if the loaded together. My goal is to compile my CUDA project to one file, but in same time I do not want to be limited to only specific video card which I have. For this I need to use PTX(or cubin with ptx included) this PTX file will be compiled for specific device in moment you load it. I tried to set Generate Relocatable Device Code to Yes (-rdc=true) and compile to PTX and Cubin - result same I get few independent files for each .cu file.",
        "answers": [
            [
                "The very short answer is no, you can't do that. The toolchain cannot merged PTX code at the compilation phase. If you produce multiple PTX files, you will need to use the JIT linker facilities of the CUDA runtime to produce a module which can be loaded into your context. I have no idea whether Managed CUDA supports that or not. Edit to add that is appears that Managed CUDA does support runtime linking (see here)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "The latest changes to ManagedCuda were made about 5 months ago. Is the project still supported? Will there be updates? If not, what is a good alternative?",
        "answers": [
            [
                "The project is still alive, but the Cuda 9 update needs some more time: A lot of changes and not enough time :)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I've declared a static array in kernel.cu file __device__ int myStaticArray[5]; I can modify this array from host using myKernel.SetConstantVariable(\"myStaticArray\", new int[]{1,2,3,4,5}); After a few processing, I want to copy this array to host, how can I do so? EDIT1: I noticed that the array will be reset every time when I run a new kernel. I cannot use that array to keep intermediate values to be used in the next kernel. Is it possible to keep those values in static ways? EDIT2: The problem in EDIT1 occur because I load multiple kernels using LoadKernelPTX. The correct way is to load module once, then construct multiple kernels from that module. (as suggested in https://github.com/kunzmi/managedCuda/wiki/CudaKernel) This way, I can have shared static array/variable across multiple kernels.",
        "answers": [
            [
                "Either you declare the array as __device__ __constant__ int myStaticArray[5]; and treat it as a constant array (i.e. you don't write to it). Then you can initialize the values from host using myKernel.SetConstantVariable(...). Or you you keep it as an __device__ int myStaticArray[5];, then you can declare a CudaDeviceVariable using the constructor CudaDeviceVariable(CUmodule module, string name) that gathers the pointer to that static variable. You can then do any copy-to-host or copy-to-device as usual. Note that names in Cuda get mangled if the variable is not declared as extern \"C\", so you might have to look up the full mangled name in the PTX-file."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to understand where a Stream might help me with processing multiple Regions of Interest on a video frame. If using NPP functions that support a stream, is this a case where one would launch as many streams as there are ROIs? Possibly even creating a CPU thread for each Stream? Or is the benefit in using one stream to process all the ROIs and possibly using this single stream from multiple threads in the CPU?",
        "answers": [
            [
                "In CUDA, usage of streams generally helps to better utilize GPU in two ways. Firstly, memory copies between host and device can be overlapped by kernel execution if copying and execution occur in different streams. Secondly, individual kernels running in different streams can overlap if there are enough resources on the GPU. Further, whether creating a thread for each ROI would help depends on comparison of GPU vs CPU (if any) utilization. If there is a lot of processing on CPU and CPU holds off GPU computation, creating more threads helps. There are further details (see the documentation for actual version of CUDA) which constrain overlapping of operations in the streams. A memory copy overlaps with a kernel execution only if memory source or destination in RAM is page-locked. Or, synchronization between streams occurs when host thread issues command(s) in the default stream. (Since CUDA 7 each thread has its own default stream, so processing ROIs in different threads would help again.) Hence, satisfying certain conditions, it should improve performance of your algorithm if the processing of ROIs occurs in different streams up to certain limit (depending on resource consumption of the kernels, ratio of memory copies and computation, etc...)"
            ]
        ],
        "votes": [
            9.0000001
        ]
    },
    {
        "question": "Here I come with some trouble using managedCUDA, I got an application written in CUDA C/C++ and I wanted to launch it using managedCUDA. To begin with my problem : I got this error : Ungandled Exception: System.ArgumentException: Object contains non-primitive or non-blittable data. It occured on a line where I made a variable.CopyToDevice(otherVariable) I search for what is non-primitive and what is non-blittable For non-primitive I got : non-primitive types (or) User Defined Ex: class , struct , enum , interface, delegate, array. For non-blittableI got : The following table lists* non-blittable types from the System namespace. &gt;Delegates, which are data structures that refer to a static method or to a &gt;class instance, are also non-blittable. *table list : System.Array, System.Boolean, System.Char, System.Class, &gt;System.Object, System.Mdarray, System.String, Systeme.Valuetype, Systeme.Szarray So here is a sample of my code : using ManagedCuda; using ManagedCuda.BasicTypes; using System; using System.Linq; using System.Runtime.InteropServices; using System.Diagnostics; using System.Text.RegularExpressions; namespace Code { class Program { [StructLayout(LayoutKind.Sequential)] struct Cartesian { // all in is public /* some double and Global variables */ /* some function (Cartesian, void, double, LatLonAlt type) */ } [StructLayout(LayoutKind.Sequential)] struct LatLonAlt { // all in is public /* some double, Cartesian and Global variables */ /* some function (LatLonAlt, void, Cartesian type) */ } [StructLayout(LayoutKind.Sequential)] struct Global { /* some function (double, Cartesian, int, void type) */ } [StructLayout(LayoutKind.Sequential)] struct Propagator { // all in is public /* some double, int and Global variables */ /* some function (void, Cartesian, double type) */ } [StructLayout(LayoutKind.Sequential)] struct EarthCoordinates { // all in is publis /* some Cartesian, double, LatLonAlt, bool and Global variables */ /* one EarthCoordinates \"constructor\" and one Cartesian function */ } static void Main(string[] args) { Propagator[] host_prop = new Propagator[180]; initPropagator(ref host_prop); CudaDeviceVariable&lt;Propagator&gt; dev_prop = new CudaDeviceVariable&lt;Propagator&gt;(180); dev_prop.CopyToDevice(host_prop); EarthCoordinates[] earthStation = new EarthCoordinates[1]; initEarthCoordinates(ref earthStation); CudaDeviceVariable&lt;EarthCoordinates&gt; dev_station = new CudaDeviceVariable&lt;EarthCoordinates&gt;(1); dev_station.CopyToDevice(earthStation); } } } The error did not shows up on the line : dev_prop.CopyToDevice(host_prop); Seems that Global isn't the problem, neither the fact that Propagator is a struct But on the line : dev_station.CopyToDevice(earthStation); As you can see I did the \"same\" thing for both variables so, that's not how I proceed that cause the error. I'm guessing it come from EarthCoordinates struct that contain other struct object, and it may be this, that is the problem.. So, knowing I'm using managedCUDA and I can't really do as in CUDA C/C++, I don't have any idea how to solve this error.. So is there any way to make this work ? Thanks to all !!",
        "answers": [
            [
                "Ok so, after some digging, I figured out that the problem simply come from the boolean value... So I changed it into an int (0/false - 1/true) and the copy work again !"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Today, I'm trying to launch my CUDA C/C++ program from a C# application. So, I did some research on the web, but I didn't found that much information. I only saw the \"GitHub\" but, no... So I've got a Kernel defined like follow : (that's an example) __global__ void kernel(Cartesian a, Cartesian b, Cartesian *c) With \"Cartesian\" : class Cartesian { public: double x; double y; double z; }; With what I understand from managedCUDA. It's like replacing the main function of a CUDA C/C++ program. Using a lib that \"do the work for us\" So i followed an example from this page : https://algoslaves.wordpress.com/2013/08/25/nvidia-cuda-hello-world-in-managed-c-and-f-with-use-of-managedcuda/ And write my C# program like this : The part that create the context : (dont really get this \"notion\") static void InitKernels() { CudaContext cntxt = new CudaContext(); CUmodule cumodule = cntxt.LoadModule(@\"C:\\Users\\stage\\Documents\\Visual Studio 2013\\Projects\\Cs_link_test\\Cs_link_test\\x64\\Release\\kernel.ptx\"); addWithCuda = new CudaKernel(\"kernel\", cumodule, cntxt); } The part that launch (I guess) the function and get back the modification made by the kernel : static Func&lt;Cartesian, Cartesian, Cartesian&gt; cudaAdd = (a, b) =&gt; { CudaDeviceVariable&lt;Cartesian&gt; result_dev; Cartesian result_host; addWithCuda.Run(a, b, result_dev.DevicePointer); result_dev.CopyToHost(ref result_host); return result_host; }; And from this part I don't understand anything from the line : static Func&lt;Cartesian, Cartesian, Cartesian&gt; cudaAdd = (a, b) =&gt; I'm not familiar with C# (just saying) So my problem come from the error caused by result_dev and result_host; The error says : Use of unassigned local variable 'result_*' So, is it because they arren't initialized ? If so, why result_host cause an error ? It must get the data from result_dev which must be modified by the kernel.. If not, how to fix this ? And I also wanted to know, is it possible to passe Class parameter through a kernel function ? If so, how to set a CudaDeviceVariable, cause it says that the type must be non-nullable. It's why I change class with struct.",
        "answers": [
            [
                "Ok so.. I just figured out how to solve my problem. Read the \"discussion\" section on https://managedcuda.codeplex.com/discussions/659183 help me to do it. So how to proceed to pass struct parameter to a kernel using managedCUDA ? First thing I did wrong (I guess) is to use the Func&lt;T, T, T&gt;part. You must declare your class in your .cu file like follow : class Cartesian { public: double x; double y; double z; } And the same in your .cs file like follow : [StructLayout(LayoutKind.Sequential)] struct Cartesian { public double x; public double y; public double z; public Cartesian(double x, double y, double z) { this.x = x; this.y = y; this.z = z; } }; Then you can initialize your kernel as you want, I do it like this : static void InitKernels() { CudaContext ctx = new CudaContext(); CUmodule cumodule = ctx.LoadModule(@\"C:\\Users\\stage\\Documents\\Visual Studio 2013\\Projects\\Cs_link_test\\Cs_link_test\\x64\\Release\\kernel.ptx\"); kernel = new CudaKernel(\"kernelPosGeo\", cumodule, ctx); kernel.BlockDimensions = 1024; kernel.GridDimensions = 614; } And what you need to do is simply call your kernel with the parameters you want. Cartesian a = new Cartesian(1, 2, 3); kernel.Run(a); I guess I had a problem because I used Func&lt;T, T,T&gt; but till I don't use it anymore, it seems easier. And the declaration of Func had at maximum 2 parameter in and 1 out. So I've got a Kernel that have 4 or 5 parameters and I was limited here... But right now, do not have any problem."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm currently attempting to connect some form of output from a CUDA program to a GL_TEXTURE_2D for use in rendering. I'm not that worried about the output type from CUDA (whether it'd be an array or surface, I can adapt the program to that). So the question is, how would I do that? (my current code copies the output array to system memory, and uploads it to the GPU again with GL.TexImage2D, which is obviously highly inefficient - when I disable those two pieces of code, it goes from approximately 300 kernel executions per second to a whopping 400) I already have a little bit of test code, to at least bind a GL texture to CUDA, but I'm not even able to get the device pointer from it... ctx = CudaContext.CreateOpenGLContext(CudaContext.GetMaxGflopsDeviceId(), CUCtxFlags.SchedAuto); uint textureID = (uint)GL.GenTexture(); //create a texture in GL GL.TexParameter(TextureTarget.Texture2D, TextureParameterName.TextureMinFilter, (int)TextureMinFilter.Linear); GL.TexParameter(TextureTarget.Texture2D, TextureParameterName.TextureMagFilter, (int)TextureMagFilter.Linear); GL.TexImage2D(TextureTarget.Texture2D, 0, PixelInternalFormat.Rgba, width, height, 0, OpenTK.Graphics.OpenGL.PixelFormat.Rgba, PixelType.UnsignedByte, null); //allocate memory for the texture in GL CudaOpenGLImageInteropResource resultImage = new CudaOpenGLImageInteropResource(textureID, CUGraphicsRegisterFlags.WriteDiscard, CudaOpenGLImageInteropResource.OpenGLImageTarget.GL_TEXTURE_2D, CUGraphicsMapResourceFlags.WriteDiscard); //using writediscard because the CUDA kernel will only write to this texture //then, as far as I understood the ManagedCuda example, I have to do the following when I call my kernel //(done without a CudaGraphicsInteropResourceCollection because I only have one item) resultImage.Map(); var ptr = resultImage.GetMappedPointer(); //this crashes kernelSample.Run(ptr); //pass the pointer to the kernel so it knows where to write resultImage.UnMap(); The following exception is thrown when attempting to get the pointer: ErrorNotMappedAsPointer: This indicates that a mapped resource is not available for access as a pointer. What do I need to do to fix this? And even if this exception can be resolved, how would I solve the other part of my question; that is, how do I work with the acquired pointer in my kernel? Can I use a surface for that? Access it as an arbitrary array (pointer arithmetic)? Edit: Looking at this example, apparently I don't even need to map the resource every time I call the kernel, and call the render function. But how would this translate to ManangedCUDA?",
        "answers": [
            [
                "Thanks to the example I found, I was able to translate that to ManagedCUDA (after browsing the source code and fiddling around), and I'm happy to announce that this does really improve my samples per second from about 300 to 400 :) Apparently it is needed to use a 3D array (I haven't seen any overloads in ManagedCUDA using 2D arrays) but that doesn't really matter - I just use a 3D array/texture which is exactly 1 deep. id = GL.GenTexture(); GL.BindTexture(TextureTarget.Texture3D, id); GL.TexParameter(TextureTarget.Texture3D, TextureParameterName.TextureMinFilter, (int)TextureMinFilter.Linear); GL.TexParameter(TextureTarget.Texture3D, TextureParameterName.TextureMagFilter, (int)TextureMagFilter.Linear); GL.TexImage3D(TextureTarget.Texture3D, 0, PixelInternalFormat.Rgba, width, height, 1, 0, OpenTK.Graphics.OpenGL.PixelFormat.Bgra, PixelType.UnsignedByte, IntPtr.Zero); //allocate memory for the texture but do not upload anything CudaOpenGLImageInteropResource resultImage = new CudaOpenGLImageInteropResource((uint)id, CUGraphicsRegisterFlags.SurfaceLDST, CudaOpenGLImageInteropResource.OpenGLImageTarget.GL_TEXTURE_3D, CUGraphicsMapResourceFlags.WriteDiscard); resultImage.Map(); CudaArray3D mappedArray = resultImage.GetMappedArray3D(0, 0); resultImage.UnMap(); CudaSurface surfaceResult = new CudaSurface(kernelSample, \"outputSurface\", CUSurfRefSetFlags.None, mappedArray); //nothing needs to be done anymore - this call connects the 3D array from the GL texture to a surface reference in the kernel Kernel code: surface outputSurface; __global__ void Sample() { ... surf3Dwrite(output, outputSurface, pixelX, pixelY, 0); }"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm currently working with ManagedCuda, and want to generate random numbers on the device. However I can't seem to find a simple example how to do this (browsing through objects in the ManagedCuda.CudaRand namespace and comparing with the C++ equivalent doesn't get me any further). Actual question: How can I generate random numbers in a kernel when using managedCuda instead of the regular C++ API?",
        "answers": [
            [
                "As it seems, you only want to use the device side API of CURAND, you will be then entirely independent of managedCuda: All you need to do in managedCuda is to allocate a large enough chunk of memory to save the current curandStates. You don\u2019t even need a reference to managedCuda's CudaRand.dll. Then you create an init kernel that calls for each thread curand_init() and then in your actual kernel you use curand_normal() or any of the other rand-functions. A step-by-step example is given in the curand manual in chapter 3.6."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "In this thread x64 allows less threads per block than Win32? there was a questions about running out of registers. I was under the impression the Nvidia has dropped support for x86 in CUDA 7.5 and beyond. This may be a foolish question but does that mean that all pointers are going to require two registers going forward? And that potentially less threads/block will be the way things work going forward?",
        "answers": [
            [
                "This may be a foolish question but does that mean that all pointers are going to require two registers going forward? Yes. All pointers in x64 mode will require 2 (32-bit) registers for storage. And that potentially less threads/block will be the way things work going forward? Certainly there should be no impact on the number of blocks that can be launched. Regarding threads, yes, there is potentially an impact on threads per block (since the product of threads per block launched times registers per thread must be lower than the machine limit), but as I stated in my answer to the question you linked, the limitation on threads can usually be worked around using one of several methods as mentioned there. Many kernels will not be impacted, because they are not \"up against the limit\". For those kernels that are \"up against the limit\", there are well established techniques to mitigate the effect and allow you to run the desired number of threads per block, up to 1024. Ultimately this means the issue presented is not one of capability so much as it is one of performance optimization, which issue will always be present."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I am trying to make FFT plus kernel calculation. FFT : managedCUDA library kernel calc : own kernel C# code public void cuFFTreconstruct() { CudaContext ctx = new CudaContext(0); CudaKernel cuKernel = ctx.LoadKernel(\"kernel_Array.ptx\", \"cu_ArrayInversion\"); float[] fData = new float[Resolution * Resolution * 2]; float[] result = new float[Resolution * Resolution * 2]; CudaDeviceVariable&lt;float&gt; devData = new CudaDeviceVariable&lt;float&gt;(Resolution * Resolution * 2); CudaDeviceVariable&lt;float&gt; copy_devData = new CudaDeviceVariable&lt;float&gt;(Resolution * Resolution * 2); int i, j; Random rnd = new Random(); double avrg = 0.0; for (i = 0; i &lt; Resolution; i++) { for (j = 0; j &lt; Resolution; j++) { fData[(i * Resolution + j) * 2] = i + j * 2; fData[(i * Resolution + j) * 2 + 1] = 0.0f; } } devData.CopyToDevice(fData); CudaFFTPlan1D plan1D = new CudaFFTPlan1D(Resolution * 2, cufftType.C2C, Resolution * 2); plan1D.Exec(devData.DevicePointer, TransformDirection.Forward); cuKernel.GridDimensions = new ManagedCuda.VectorTypes.dim3(Resolution / 256, Resolution, 1); cuKernel.BlockDimensions = new ManagedCuda.VectorTypes.dim3(256, 1, 1); cuKernel.Run(devData.DevicePointer, copy_devData.DevicePointer, Resolution); devData.CopyToHost(result); for (i = 0; i &lt; Resolution; i++) { for (j = 0; j &lt; Resolution; j++) { ResultData[i, j, 0] = result[(i * Resolution + j) * 2]; ResultData[i, j, 1] = result[(i * Resolution + j) * 2 + 1]; } } ctx.FreeMemory(devData.DevicePointer); ctx.FreeMemory(copy_devData.DevicePointer); } kernel code //Includes for IntelliSense #define _SIZE_T_DEFINED #ifndef __CUDACC__ #define __CUDACC__ #endif #ifndef __cplusplus #define __cplusplus #endif #include &lt;cuda.h&gt; #include &lt;device_launch_parameters.h&gt; #include &lt;texture_fetch_functions.h&gt; #include \"float.h\" #include &lt;builtin_types.h&gt; #include &lt;vector_functions.h&gt; // Texture reference texture&lt;float2, 2&gt; texref; extern \"C\" { __global__ void cu_ArrayInversion(float* data_A, float* data_B, int Resolution) { int image_x = blockIdx.x * blockDim.x + threadIdx.x; int image_y = blockIdx.y; data_B[(Resolution * image_x + image_y) * 2] = data_A[(Resolution * image_y + image_x) * 2]; data_B[(Resolution * image_x + image_y) * 2 + 1] = data_A[(Resolution * image_y + image_x) * 2 + 1]; } } However this program does not work well. Following error was occurred: ErrorLaunchFailed: An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointer and accessing out of bounds shared memory. The context cannot be used, so it must be destroyed (and a new one should be created). All existing device memory allocations from this context are invalid and must be reconstructed if the program is to continue using CUDA.",
        "answers": [
            [
                "The FFT-plan takes the number of elements, i.e. number of complex numbers, as argument. So remove the * 2 in the first argument of the plan's constructor. And the times two for the number of batches also doesn't make sense... Further I'd use the float2 or cuFloatComplex type (in ManagedCuda.VectorTypes) to represent the complex numbers and not two raw floats. And to free memory, use the Dispose methods of CudaDeviceVariable. Otherwise it will be called internally by the GC somewhat later. The host code would then look something like this: int Resolution = 512; CudaContext ctx = new CudaContext(0); CudaKernel cuKernel = ctx.LoadKernel(\"kernel.ptx\", \"cu_ArrayInversion\"); //float2 or cuFloatComplex float2[] fData = new float2[Resolution * Resolution]; float2[] result = new float2[Resolution * Resolution]; CudaDeviceVariable&lt;float2&gt; devData = new CudaDeviceVariable&lt;float2&gt;(Resolution * Resolution); CudaDeviceVariable&lt;float2&gt; copy_devData = new CudaDeviceVariable&lt;float2&gt;(Resolution * Resolution); int i, j; Random rnd = new Random(); double avrg = 0.0; for (i = 0; i &lt; Resolution; i++) { for (j = 0; j &lt; Resolution; j++) { fData[(i * Resolution + j)].x = i + j * 2; fData[(i * Resolution + j)].y = 0.0f; } } devData.CopyToDevice(fData); //Only Resolution times in X and Resolution batches CudaFFTPlan1D plan1D = new CudaFFTPlan1D(Resolution, cufftType.C2C, Resolution); plan1D.Exec(devData.DevicePointer, TransformDirection.Forward); cuKernel.GridDimensions = new ManagedCuda.VectorTypes.dim3(Resolution / 256, Resolution, 1); cuKernel.BlockDimensions = new ManagedCuda.VectorTypes.dim3(256, 1, 1); cuKernel.Run(devData.DevicePointer, copy_devData.DevicePointer, Resolution); devData.CopyToHost(result); for (i = 0; i &lt; Resolution; i++) { for (j = 0; j &lt; Resolution; j++) { //ResultData[i, j, 0] = result[(i * Resolution + j)].x; //ResultData[i, j, 1] = result[(i * Resolution + j)].y; } } //And better free memory using Dispose() //ctx.FreeMemory is only meant for raw device pointers obtained from somewhere else... devData.Dispose(); copy_devData.Dispose(); plan1D.Dispose(); //For Cuda Memory checker and profiler: CudaContext.ProfilerStop(); ctx.Dispose();"
            ],
            [
                "Thank you for this suggestion. I tried suggested code. However, the error was remain. (error : ErrorLaunchFailed: An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointer and accessing out of bounds shared memory. The context cannot be used, so it must be destroyed (and a new one should be created). All existing device memory allocations from this context are invalid and must be reconstructed if the program is to continue using CUDA.) To use the float2, I changed the cu code as follows extern \"C\" { __global__ void cu_ArrayInversion(float2* data_A, float2* data_B, int Resolution) { int image_x = blockIdx.x * blockDim.x + threadIdx.x; int image_y = blockIdx.y; data_B[(Resolution * image_x + image_y)].x = data_A[(Resolution * image_y + image_x)].x; data_B[(Resolution * image_x + image_y)].y = data_A[(Resolution * image_y + image_x)].y; } When program executes the \"cuKernel.Run\", the process stoped. ptx file .version 4.3 .target sm_20 .address_size 32 // .globl cu_ArrayInversion .global .texref texref; .visible .entry cu_ArrayInversion( .param .u32 cu_ArrayInversion_param_0, .param .u32 cu_ArrayInversion_param_1, .param .u32 cu_ArrayInversion_param_2 ) { .reg .f32 %f&lt;5&gt;; .reg .b32 %r&lt;17&gt;; ld.param.u32 %r1, [cu_ArrayInversion_param_0]; ld.param.u32 %r2, [cu_ArrayInversion_param_1]; ld.param.u32 %r3, [cu_ArrayInversion_param_2]; cvta.to.global.u32 %r4, %r2; cvta.to.global.u32 %r5, %r1; mov.u32 %r6, %ctaid.x; mov.u32 %r7, %ntid.x; mov.u32 %r8, %tid.x; mad.lo.s32 %r9, %r7, %r6, %r8; mov.u32 %r10, %ctaid.y; mad.lo.s32 %r11, %r10, %r3, %r9; shl.b32 %r12, %r11, 3; add.s32 %r13, %r5, %r12; mad.lo.s32 %r14, %r9, %r3, %r10; shl.b32 %r15, %r14, 3; add.s32 %r16, %r4, %r15; ld.global.v2.f32 {%f1, %f2}, [%r13]; st.global.v2.f32 [%r16], {%f1, %f2}; ret; }"
            ],
            [
                "Thank you for the message. host code using System; using System.Collections.Generic; using System.ComponentModel; using System.Data; using System.Drawing; using System.Linq; using System.Text; using System.Threading.Tasks; using System.Windows.Forms; using System.Drawing.Imaging; using ManagedCuda; using ManagedCuda.CudaFFT; using ManagedCuda.VectorTypes; namespace WFA_CUDA_FFT { public partial class CuFFTMain : Form { float[, ,] FFTData2D; int Resolution; const int cuda_blockNum = 256; public CuFFTMain() { InitializeComponent(); Resolution = 1024; } private void button1_Click(object sender, EventArgs e) { cuFFTreconstruct(); } public void cuFFTreconstruct() { CudaContext ctx = new CudaContext(0); ManagedCuda.BasicTypes.CUmodule cumodule = ctx.LoadModule(\"kernel.ptx\"); CudaKernel cuKernel = new CudaKernel(\"cu_ArrayInversion\", cumodule, ctx); float2[] fData = new float2[Resolution * Resolution]; float2[] result = new float2[Resolution * Resolution]; FFTData2D = new float[Resolution, Resolution, 2]; CudaDeviceVariable&lt;float2&gt; devData = new CudaDeviceVariable&lt;float2&gt;(Resolution * Resolution); CudaDeviceVariable&lt;float2&gt; copy_devData = new CudaDeviceVariable&lt;float2&gt;(Resolution * Resolution); int i, j; Random rnd = new Random(); double avrg = 0.0; for (i = 0; i &lt; Resolution; i++) { for (j = 0; j &lt; Resolution; j++) { fData[i * Resolution + j].x = i + j * 2; avrg += fData[i * Resolution + j].x; fData[i * Resolution + j].y = 0.0f; } } avrg = avrg / (double)(Resolution * Resolution); for (i = 0; i &lt; Resolution; i++) { for (j = 0; j &lt; Resolution; j++) { fData[(i * Resolution + j)].x = fData[(i * Resolution + j)].x - (float)avrg; } } devData.CopyToDevice(fData); CudaFFTPlan1D plan1D = new CudaFFTPlan1D(Resolution, cufftType.C2C, Resolution); plan1D.Exec(devData.DevicePointer, TransformDirection.Forward); cuKernel.GridDimensions = new ManagedCuda.VectorTypes.dim3(Resolution / cuda_blockNum, Resolution, 1); cuKernel.BlockDimensions = new ManagedCuda.VectorTypes.dim3(cuda_blockNum, 1, 1); cuKernel.Run(devData.DevicePointer, copy_devData.DevicePointer, Resolution); copy_devData.CopyToHost(result); for (i = 0; i &lt; Resolution; i++) { for (j = 0; j &lt; Resolution; j++) { FFTData2D[i, j, 0] = result[i * Resolution + j].x; FFTData2D[i, j, 1] = result[i * Resolution + j].y; } } //Clean up devData.Dispose(); copy_devData.Dispose(); plan1D.Dispose(); CudaContext.ProfilerStop(); ctx.Dispose(); } } } kernel code //Includes for IntelliSense #define _SIZE_T_DEFINED #ifndef __CUDACC__ #define __CUDACC__ #endif #ifndef __cplusplus #define __cplusplus #endif #include &lt;cuda.h&gt; #include &lt;device_launch_parameters.h&gt; #include &lt;texture_fetch_functions.h&gt; #include \"float.h\" #include &lt;builtin_types.h&gt; #include &lt;vector_functions.h&gt; #include &lt;vector&gt; // Texture reference texture&lt;float2, 2&gt; texref; extern \"C\" { // Device code __global__ void cu_ArrayInversion(float2* data_A, float2* data_B, int Resolution) { int image_x = blockIdx.x * blockDim.x + threadIdx.x; int image_y = blockIdx.y; data_B[(Resolution * image_x + image_y)].y = data_A[(Resolution * image_y + image_x)].x; data_B[(Resolution * image_x + image_y)].x = data_A[(Resolution * image_y + image_x)].y; } } First I compiled with .Net4.5. This program did not work, and error (System.BadImageFormatException) was showed. However when the FFT function is comment out, the kernel program run. Second I chaneged from .Net 4.5 to .Net 4.0. The FFT function works, but kernel does not run and shows errors. My PC is windows 8.1 pro and I use visual studio 2013."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I'm trying to profile an CUDA-application written in C# with managedCuda using either Nsight Visual Studio Edition or Visual Profiler. Both profilers work well with a plain C++ CUDA app. To test the profilers with managedCuda I want to profile the project \"vectorAdd\" in ManagedCudaSamples. First I tried to use Nvidia Nsight Visual Studio Edition 5.0 integrated in VS 2013. I use the x64 Debug configuration. If I try to launch the app in \"Application Control\" in Nsight Performance Analysis I get an error message: Analysis Session - Start Application Unable to launch 64-bit managed application '...\\ManagedCudaSamples\\vectorAdd\\bin\\x64\\Debug\\vectorAdd.exe'. Additionally I tried to use Nvidia Visual Profiler 7.5 for profiling the same application. On running vectorAdd.exe nvprof console shows the following output: ==2944== NVPROF is profiling process 2944, command: ...\\ManagedCudaSamples\\vectorAdd\\bin\\x64\\Debug\\vectorAdd.exe ==2944== Warning: Some profiling data are not recorded. Make sure cudaProfilerStop() or cuProfilerStop() is called before application exit to flush profile data. ==2944== Generated result file: ...\\nvvp_workspace\\.metadata\\.plugins\\com.nvidia.viper\\launch\\7\\api_2944.log I'm new to CUDA and would be thankful for any advice how to profile managedCuda applications.",
        "answers": [
            [
                "You need to call CudaContext.ProfilerStop() just before you exit the application (or destroy the context) in order to flush the collected data to the profiler. The managedCuda samples don't include this call why the profiler doesn't see the collected info. This explains the second error you get. And regarding the first error: In the release notes of Nsight 5.0 you can find a known issue: Managed applications built with the AnyCpu configuration are not supported. The target application must be built using either the Win32 or x64 configurations. The VS-project for vectorAdd is always set to AnyCPU, regardless what the solution platform is, see the configuration manager of the managedCuda samples-solution to change that."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I\u2019m working on a prototype that integrates WPF, Direct3D9 (using Microsoft\u2019s D3DImage WPF class), and CUDA (I need to be able to generate a texture for the D3DImage on the GPU). The problem is, CUDA doesn\u2019t update my texture. No error codes are returned, the texture just stays unchanged. Even if I read after my own write, I don't see any changes. How to update my D3D9 texture? I'm not even running any CUDA kernels, for debug purposes I only using cuMemcpy2D API to write the CUDA memory by copying some fake data from the CPU. Here\u2019s the code, it\u2019s C# but I\u2019ve placed native APIs in the comments: static void updateTexture( Texture tx ) { var size = tx.getSize(); using( CudaDirectXInteropResource res = new CudaDirectXInteropResource( tx.NativePointer, CUGraphicsRegisterFlags.None, CudaContext.DirectXVersion.D3D9 ) ) // cuGraphicsD3D9RegisterResource { res.Map(); // = cuGraphicsMapResources using( CudaArray2D arr = res.GetMappedArray2D( 0, 0 ) ) // cuGraphicsSubResourceGetMappedArray, cuArrayGetDescriptor. The size is correct here, BTW { // Debug code below - don't run any kernels for now, just call cuMemcpy2D to write the GPU memory uint[] arrWhite = new uint[ size.Width * size.Height ]; for( int i = 0; i &lt; arrWhite.Length; i++ ) arrWhite[ i ] = 0xFF0000FF; arr.CopyFromHostToThis( arrWhite ); // cuMemcpy2D uint[] test = new uint[ size.Width * size.Height ]; arr.CopyFromThisToHost( test ); // The values here are correct } res.UnMap(); // cuGraphicsUnmapResources } tx.AddDirtyRectangle(); // Map again and check what's in the resource using( CudaDirectXInteropResource res = new CudaDirectXInteropResource( tx.NativePointer, CUGraphicsRegisterFlags.None, CudaContext.DirectXVersion.D3D9 ) ) { res.Map(); using( CudaArray2D arr = res.GetMappedArray2D( 0, 0 ) ) { uint[] test = new uint[ size.Width * size.Height ]; arr.CopyFromThisToHost( test ); // All zeros :-( Debug.WriteLine( \"First pixel: {0:X}\", test[ 0 ] ); } res.UnMap(); } }",
        "answers": [
            [
                "As hinted by the commenter, I\u2019ve tried creating a single instance of CudaDirectXInteropResource along with the D3D texture. It worked. It\u2019s counter-intuitive and undocumented, but it looks like cuGraphicsUnregisterResource destroys the newly written data. At least on my machine with GeForce GTX 960, Cuda 7.0 and Windows 8.1 x64. So, the solution \u2014 call cuGraphicsD3D9RegisterResource once per texture, and use cuGraphicsMapResources / cuGraphicsUnmapResources API to allow CUDA to access the texture data."
            ]
        ],
        "votes": [
            5.0000001
        ]
    }
]