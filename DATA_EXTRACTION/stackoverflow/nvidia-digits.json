[
    {
        "question": "I just tried to install Nvidia Digits 6.1.1 on my Ubuntu 16.04 Pc. I did every steps carefully but at the end of the installation i get this error on startup. Traceback (most recent call last): File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main \"__main__\", fname, loader, pkg_name) File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals File \"/home/kenan/digits/digits/__main__.py\", line 70, in &lt;module&gt; main() File \"/home/kenan/digits/digits/__main__.py\", line 55, in main import digits.webapp File \"digits/webapp.py\", line 7, in &lt;module&gt; from flask_socketio import SocketIO File \"/usr/local/lib/python2.7/dist-packages/flask_socketio/__init__.py\", line 16, in &lt;module&gt; import socketio File \"/home/kenan/.local/lib/python2.7/site-packages/socketio/__init__.py\", line 3, in &lt;module&gt; from .client import Client File \"/home/kenan/.local/lib/python2.7/site-packages/socketio/client.py\", line 7, in &lt;module&gt; import engineio File \"/home/kenan/.local/lib/python2.7/site-packages/engineio/__init__.py\", line 3, in &lt;module&gt; from .client import Client File \"/home/kenan/.local/lib/python2.7/site-packages/engineio/client.py\", line 2, in &lt;module&gt; from json import JSONDecodeError ImportError: cannot import name JSONDecodeError i appreciate any suggestions. thanks.",
        "answers": [
            [
                "I solved the problem. If anyone needs, here is the way i did, first i deleted from json import JSONDecodeError line in file client.py which is located at ./local/lib/python2.7/site-packages/engineio/ after that modification i got an error on lib werkzeug and the solution is that; pip install werkzeug==0.16.0 After these steps Digits worked."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have used standard code to download MNIST_Fashion dataset and run a CNN, using Tensorflow 2 (2.3.1) and Keras (2.4.0). The code works fine on a normal laptop without GPU. However, on a laptop with NVIDIA RTX 2080 Max-Q I get error message: 'No algorithm worked!'. Duo you have any suggestions how to run the code on laptop with GPU? The code I have used: from __future__ import absolute_import, division, print_function, unicode_literals from tensorflow import keras as ks fashion_mnist = ks.datasets.fashion_mnist (training_images, training_labels), (test_images, test_labels) = fashion_mnist.load_data() class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] training_images = training_images / 255.0 test_images = test_images / 255.0 training_images = training_images.reshape(60000, 28, 28, 1) test_images = test_images.reshape(10000, 28, 28, 1) cnn_model = ks.models.Sequential() cnn_model.add(ks.layers.Conv2D(50, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1), name='Conv2D_l')) cnn_model.add(ks.layers.MaxPooling2D((2, 2), padding='same', name='MaxPooling_2D')) cnn_model.add(ks.layers.Flatten(name='Flatten')) cnn_model.add(ks.layers.Dense(50, activation='relu', name='Hidden_layer')) cnn_model.add(ks.layers.Dense(10, activation='softmax', name='Output_layer')) cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) cnn_model.fit(training_images, training_labels, epochs=100)",
        "answers": [
            [
                "Providing the full error message might be more useful next time. I assume, adding these lines might solve your issue: from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config)"
            ],
            [
                "I am running on Ubuntu, apart from what Frightera said above which I would always add something similar: gpu_devices = tf.config.experimental.list_physical_devices('GPU') for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True) I would usually free my GPU memory buy killing the python processes I ran previously. Ctrl + Alt + T to open the terminal: sudo fuser -v /dev/nvidia* A table will emerge, then do sudo kill -9 &lt;PID number&gt; where &lt; PID number &gt; is the number corresponding to the python process seen in the table. After this, go and rerun your code and be happy."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am currently trying to use Nvidia DIGITS to train a CNN on a custom dataset for object detection, and eventually I want to run that network on an Nvidia Jetson TX2. I followed the recommended instructions to download the DIGITS image from Docker, and I am able to successfully train a network with reasonable accuracy. But when I try to run my network in python using OpenCv, I get this error, \"error: (-215) pbBlob.raw_data_type() == caffe::FLOAT16 in function blobFromProto\" I have read in a few other threads that this is due to the fact that DIGITS stores its networks in a form that is incompatible with OpenCv's DNN functionality. Before training my network, I have tried selecting the option in DIGITS that is supposed to make the network compatible with other software, however that doesn't seem to change the network at all, and I get the same error when running my python script. This is the script I run that creates the error (it comes from this tutorial https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/) # import the necessary packages import numpy as np import argparse import cv2 # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\"-i\", \"--image\", required=True, help=\"path to input image\") ap.add_argument(\"-p\", \"--prototxt\", required=True, help=\"path to Caffe 'deploy' prototxt file\") ap.add_argument(\"-m\", \"--model\", required=True, help=\"path to Caffe pre-trained model\") ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2, help=\"minimum probability to filter weak detections\") args = vars(ap.parse_args()) # initialize the list of class labels MobileNet SSD was trained to # detect, then generate a set of bounding box colors for each class CLASSES = [\"dontcare\", \"HatchPanel\"] COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3)) # load our serialized model from disk print(\"[INFO] loading model...\") net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"]) # load the input image and construct an input blob for the image # by resizing to a fixed 300x300 pixels and then normalizing it # (note: normalization is done via the authors of the MobileNet SSD # implementation) image = cv2.imread(args[\"image\"]) (h, w) = image.shape[:2] blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 0.007843, (300, 300), 127.5) # pass the blob through the network and obtain the detections and # predictions print(\"[INFO] computing object detections...\") net.setInput(blob) detections = net.forward() # loop over the detections for i in np.arange(0, detections.shape[2]): # extract the confidence (i.e., probability) associated with the # prediction confidence = detections[0, 0, i, 2] # filter out weak detections by ensuring the `confidence` is # greater than the minimum confidence if confidence &gt; args[\"confidence\"]: # extract the index of the class label from the `detections`, # then compute the (x, y)-coordinates of the bounding box for # the object idx = int(detections[0, 0, i, 1]) box = detections[0, 0, i, 3:7] * np.array([w, h, w, h]) (startX, startY, endX, endY) = box.astype(\"int\") # display the prediction label = \"{}: {:.2f}%\".format(CLASSES[idx], confidence * 100) print(\"[INFO] {}\".format(label)) cv2.rectangle(image, (startX, startY), (endX, endY), COLORS[idx], 2) y = startY - 15 if startY - 15 &gt; 15 else startY + 15 cv2.putText(image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2) # show the output image cv2.imshow(\"Output\", image) cv2.waitKey(0) This should output the image specified in the call to the script, with the output of the neural network drawn over top of the image. But instead, the script crashes with the before mentioned error. I have seen other threads with people that have this same error, but as of yet, none of them have arrived at a solution that works with the current version of DIGITS. My full setup is as follows: OS: Ubuntu 16.04 Nvidia DIGITS Docker Image Version: 19.01-caffe DIGITS Version: 6.1.1 Caffe Version: 0.17.2 Caffe Flavor: Nvidia OpenCV Version: 4.0.0 Python Version: 3.5 Any help is much appreciated.",
        "answers": [
            [
                "Harrison McIntyre, Thank you! This PR fixes it: https://github.com/opencv/opencv/pull/13800. Please note that there is a layer with type \"ClusterDetections\". It's not supported by OpenCV but you can implement it using custom layers mechanic (see a tutorial)"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I've built a model using DIGITS by Nvidia, but when I try to run it using caffe, I don't know where the Weights are. Any idea how I'd find this. I have the architecture because that is provided right on the output model screen.",
        "answers": [
            [
                "The weights are not accessible from any of the output models given on the Digits UI, however they are accessible! I use NVIDIAs DGX, which can take python code. To pull weights on that platform (where I route the models to save I use this bit of code: net = caffe.Net('../models/bvlc_reference_caffenet/deploy.prototxt', '../models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel', caffe.TEST) params = ['fc6', 'fc7', 'fc8'] fc_params = {pr: (net.params[pr][0].data, net.params[pr][1].data) for pr in params} for fc in params: print '{} weights are {} dimensional and biases are {} dimensional'.format(fc, fc_params[fc][0].shape, fc_params[fc][1].shape) Good Luck!"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I used NVIDIA DIGITS to train an image classifier. The training images are in JPG format. DIGITS uses Pillow to load images. I noticed that, at inference time, there are relevant differences in accuracy if the JPG images are loaded with OpenCV instead of Pillow. I think that this is related to the different JPEG decoding. Now, since my classifier is going to be used on a live app where images are captured with openCV from a webcam, I would like to know if this difference could be a problem and, if so, how can I overcome it. EDIT: I know that Pillow uses the RGB format and OpenCV uses the BGR format but the difference is not related to this because I convert the images before making the comparison. The network is trained on BGR images.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am new to Digits and TX2. I am trying to create object detection model using the tutorial from: https://github.com/dusty-nv/jetson-inference I created dataset sucessfully. The issue is with the model While creating a model, I am getting the following error. Memory required for data: 3268934784 creating layer bbox_loss Creating Layer bbox_loss bbox_loss &lt;- bboxes-obj-masked-norm bbox_loss &lt;- bbox-obj-label-norm bbox_loss -&gt; loss_bbox Setting up bbox_loss Top shape: (1) with loss weight 2 Memory required for data: 3268934788 Creating layer coverage_loss Creating Layer coverage_loss coverage_loss &lt;- coverage_coverage/sig_0_split_0 coverage_loss &lt;- coverage-label_slice-label_4_split_0 coverage_loss -&gt; loss_coverage Setting up coverage_loss Top shape: (1) with loss weight 1 Memory required for data: 3268934792 Creating layer cluster The job directory information on the left is: Job Directory /home/nvidia/DIGITS/digits/jobs/20180816-161051-e67a Disk Size 0 B Network (train/val) train_val.prototxt Network (deploy) deploy.prototxt Network (original) original.prototxt Solver solver.prototxt Raw caffe output caffe_output.log Pretrained Model /home/nvidia/bvlc_googlenet.caffemodel.4 Visualizations Tensorboard The error on the server is 2018-08-16 16:10:53 [20180816-161051-e67a] [INFO ] Task subprocess args: \"/home/nvidia/Caffe/caffe/build/tools/caffe train --solver=/home/nvidia/DIGITS/digits/jobs/20180816-161051-e67a/solver.prototxt --gpu=0 --weights=/home/nvidia/bvlc_googlenet.caffemodel.4\" 2018-08-16 16:11:00 [20180816-161051-e67a] [ERROR] Train Caffe Model task failed with error code 1 I have no idea on how to free up memory as I have more than 2 gb available in the job directory. Please help me. Thanks in advance.",
        "answers": [
            [
                "Had the same issue for the last few days, maybe it will help someone in the future. Firstly, make sure that you have the right version of protobuf. You can check it with: protoc --version If it's 2.* you have to update to 3.*, for example to build it as listed here https://github.com/NVIDIA/DIGITS/blob/digits-6.0/docs/BuildProtobuf.md, and then rebuild the Caffe. Also, make sure that you have the compatible version of pip package of protobuf. For me the following version is working well right now for Digits and Caffe from the tutorial https://github.com/dusty-nv/jetson-inference : pip install --user --upgrade protobuf==3.1.0.post1"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I tried to install NVIDIA DIGITS on ubuntu 14.04 I followed this website to install all the pre-requisites: https://github.com/NVIDIA/DIGITS/blob/master/docs/BuildDigits.md I couldn't set up the server The command: ./digits-devserver returned this: The first part The second part Please let me know how I can tackle this attribute error(AttributeError: 'module' object has no attribute 'nanprod')",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have an image which has 48 bands. I used PCA then band number is reduced to 12 for representing %99.9 data. But png, jpg hold only 3 bands. How can I use hyperspectral images with Nvidia digits ( Caffe )? I tried to combine 4 pictures with 3 bands. But the result is very bad. I do not have a problem when I make the Arff file and give it to WEKA. But I could not figure out how to use hyperspectral images with neural networks. What is the best way of using hyperspectral images with neural networks (Nvidia digits ( Caffe ))?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to use DIGITS from nVidia on my Windows machine without building it myself. I thought it's easier to use Docker (starting to change my mind). My understanding is that Docker on Windows is basically using a Linux VM to actually run Docker.. so, the Docker interface on host Windows is simply interacting with the Docker installed on the Linux VM which will then host a guest Linux container to run DIGITS. When I try to run the DIGITS container, I get the following error: ============ == DIGITS == ============ NVIDIA Release 18.05 (build 425957) Container image Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved. DIGITS Copyright (c) 2014-2017, NVIDIA CORPORATION. All rights reserved. Caffe Copyright (c) 2014, 2015, The Regents of the University of California (Regents). All rights reserved. Torch Copyright (c) 2016, Soumith Chintala, Ronan Collobert, Koray Kavukcuoglu, Clement Farabet. All rights reserved. TensorFlow Copyright (c) 2017, The TensorFlow Authors. All rights reserved. Various files include modifications (c) NVIDIA CORPORATION. All rights reserved. NVIDIA modifications are covered by the license terms that apply to the underlying project or file. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available. Use 'nvidia-docker run' to start this container; see https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker . ___ ___ ___ ___ _____ ___ | \\_ _/ __|_ _|_ _/ __| | |) | | (_ || | | | \\__ \\ |___/___\\___|___| |_| |___/ 6.1.1 Did you forget to \"make pycaffe\"? A valid Caffe installation was not found on your system. Use the envvar CAFFE_ROOT to indicate a valid installation. Traceback (most recent call last): File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main \"__main__\", fname, loader, pkg_name) File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals File \"/opt/digits/digits/__main__.py\", line 70, in &lt;module&gt; main() File \"/opt/digits/digits/__main__.py\", line 53, in main import digits.config File \"/opt/digits/digits/config/__init__.py\", line 7, in &lt;module&gt; from . import ( # noqa File \"/opt/digits/digits/config/caffe.py\", line 230, in &lt;module&gt; executable, version, flavor = load_from_path() File \"/opt/digits/digits/config/caffe.py\", line 57, in load_from_path import_pycaffe() File \"/opt/digits/digits/config/caffe.py\", line 126, in import_pycaffe import caffe File \"/usr/local/python/caffe/__init__.py\", line 1, in &lt;module&gt; from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, \\ File \"/usr/local/python/caffe/pycaffe.py\", line 13, in &lt;module&gt; from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\ ImportError: libnvidia-ml.so.1: cannot open shared object file: No such file or directory DIGITS docker image needs nVidia drivers to be installed on the host AFAIK. I think the error here means the drivers are not installed. However, I don't know how to install nVidia drivers on the Linux host since that's simply a VM used by Docker interface on Windows. Is there a way to access that Linux VM and install nVidia drivers on it?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I trained pretrained GoogleNet(caffe) on Digits. The training went well and testing in Digits UI show no problem whatsoever. But when I export model and try to use example.py that digits provided. This error came up. Cannot copy param 0 weights from layer 'conv1/7x7_s2/bn'; shape mismatc h. Source param shape is 1 64 1 1 (64); target param shape is 64 (64). To learn this layer's parameters from sc ratch rather than copying from a saved net, rename the layer. I have checked the deploy.prototxt and train_val.prototxt. The dimension is correct. Any suggestion?",
        "answers": [
            [
                "Try editing your 'deploy.prototxt' for layer 'conv1/7x7_s2/bn'. Add param { share_mode: PERMISSIVE } For each of the trainable parameters of the layer. See caffe.proto for more information."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to train a neural network to detect steganographic images using Tensorflow and Nvidia Digits. I loaded a data set which has two sub directories - Cover Images and Steg Images. I think the network has to process the cover/stegano image pairs together to learn which are the covers and which are steganographic images. Am I correct? How does batch size work? If I give 1 does it take one image from both sub directories and process them? or do I have to input batch number as 2 for that? How does shuffling data on each epoch work? does it shuffle both sub directories equally? as an example will 1.jpg be the third photo on both folders or will it be different on them both?",
        "answers": [
            [
                "I think the network has to process the cover/stegano image pairs together to learn which are the covers and which are steganographic images. Am I correct? I am not familiar with object detection (right?) in Nvidia Digits, so please check out their tutorials for more information. You need to think about the kind of labeling the training data first. Usually in the examples I see only use one training folder and one validation folder (each: images and labels) - Digits divides your dataset, e.g. into 90 % training and 10 % validation images. How does batch size work? If I give 1 does it take one image from both sub directories and process them? or do I have to input batch number as 2 for that? With batch number you tell Digits how many images you use per iteration. It's used for dataset division (memory for calculations is limited; you can't fit the whole dataset into one iteration). In one epoch the whole dataset is processed. As written above, one image at a time, as far as I know. How does shuffling data on each epoch work? does it shuffle both sub directories equally? as an example will 1.jpg be the third photo on both folders or will it be different on them both? The data should be shuffled automatically."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to train a neural network to detect steganographic images. I used Nvidia Digits with Tensorflow. My problem is the loss starts to gradually decrease and then starts to jump around. My Neural Network is - from model import Tower from utils import model_property import tensorflow as tf import tensorflow.contrib.slim as slim import utils as digits class UserModel(Tower): @model_property def inference(self): x = tf.reshape(self.x, shape=[-1, self.input_shape[0], self.input_shape[1], self.input_shape[2]]) with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_initializer=tf.contrib.layers.xavier_initializer(), weights_regularizer=slim.l2_regularizer(0.00001)): conv1 = tf.layers.conv2d(inputs=x, filters=64, kernel_size=7, padding='Valid', strides=2, activation=tf.nn.relu) rnorm1 = tf.nn.local_response_normalization(input=conv1) conv2 = tf.layers.conv2d(inputs=rnorm1, filters=16, kernel_size=5, padding='Valid', strides=1, activation=tf.nn.relu) rnorm2 = tf.nn.local_response_normalization(input=conv2) flatten = tf.contrib.layers.flatten(rnorm2) fc1 = tf.contrib.layers.fully_connected(inputs=flatten, num_outputs=1000, activation_fn=tf.nn.relu) fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1000, activation_fn=tf.nn.relu) fc3 = tf.contrib.layers.fully_connected(inputs=fc2, num_outputs=2, activation_fn=None) return fc3 @model_property def loss(self): model = self.inference loss = digits.classification_loss(model, self.y) accuracy = digits.classification_accuracy(model, self.y) self.summaries.append(tf.summary.scalar(accuracy.op.name, accuracy)) return loss I am using SGD with 0.0005 base learning rate. I changed the step size to 5% with the gamma of 0.95. (I used these settings as I researched and learnt loss starts to jump around after a while when the learning rate isn't reducing fast enough - earlier I used 0.0005 with base rate and nvidia digits default step size). Do you know how to make the loss gradually reduce? Any advice or guidance to make the network will be appreciated. Thanks!",
        "answers": [
            [
                "So if anyone is having the same issue, what I did was adjust the initial loss to 0.0001, step size to 5% and gamma to 0.9. It gave me a mostly gradual reduction of loss. But I am thinking the learning rate is too low as the loss doesn't go down as much as I would like it to."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I've built digits from this tutorial recently, everything is ok and I finally trained my AlexNet model (also trained a SqueezNet so that I can upload the model here) ! the problem is when I download my model from Digits, I can not load it into my program for testing!I have tested my program with GoogleNet downloaded from this link and it's working fine! I'm using OpenCV readNetFromCaffe in this function to load Caffe model void deepNetwork::loadModel( cv::String model ,cv::String weight ,string lablesPath,int ps){ patchSize=ps; labeslPath=lablesPath; try { net = dnn::readNetFromCaffe(weight,model); cerr&lt;&lt;\"loaded succ\"&lt;&lt;endl; } catch (cv::Exception&amp; e) { std::cerr &lt;&lt; \"Exception: \" &lt;&lt; e.what() &lt;&lt; std::endl; }} I get the following error loading my model OpenCV Error: Assertion failed (pbBlob.raw_data_type() == caffe::FLOAT16) in blo bFromProto, file /home/nvidia/build-opencv/opencv/modules/dnn/src/caffe/caffe_im porter.cpp, line 242 Exception: /home/nvidia/build-opencv/opencv/modules/dnn/src/caffe/caffe_importer .cpp:242: error: (-215) pbBlob.raw_data_type() == caffe::FLOAT16 in function blo bFromProto OpenCV Error: Requested object was not found (Requested blob \"data\" not found) i n setInput, file /home/nvidia/build-opencv/opencv/modules/dnn/src/dnn.cpp, line 1606 terminate called after throwing an instance of 'cv::Exception' what(): /home/nvidia/build-opencv/opencv/modules/dnn/src/dnn.cpp:1606: error: (-204) Requested blob \"data\" not found in function setInput Aborted (core dumped) any help would be appreciated &lt;3 opencv version 3.3.1 also tested on (3.3.0 ,3.4.1) same error! testing on a system without Cuda, Cudnn or Caffe just pure c++ and OpenCv... but i've trained my model on a aws ec2 instance (p3.2xlarge ) with Cuda,Cudnn and caffe ! you can download the trained squeezNet model (.prototxt and .caffemodel) here",
        "answers": [
            [
                "finally, I found the problem! it's a version problem I have digits 6.1.1 working with nvcaffe 0.17.0 for training which is not compatible with previous Caffe and OpenCv libraries ! you have to downgrade NvCaffe to version 0.15.14 and it will open with OpenCv easily!"
            ],
            [
                "OpenCV DNN model expect caffemodel in BVLC format. But, NVCaffe stores the caffe model in more efficient format which different than BVLC Caffe. If you want model compatible with both BVLC/Caffe as well as NVcaffe. Add this flag in solver.prototxt store_blobs_in_old_format = true Please read the DIGITS NVCaffe Documentation. NVCaffe Documenation - store_blobs_in_old_format"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I Trained my model using DIGITS ( NVCaffe) and I use it in opencv similar to opencv example that use .prototxt &amp; .caffemodel and i tested it and work good. But when I use Models that trained by digits i got this error: OpenCV Error: Assertion failed (pbBlob.raw_data_type() == caffe::FLOAT16) in blobFromProto, file /opt/opencv/modules/dnn/src/caffe/caffe_importer.cpp, line 242 Exception: /opt/opencv/modules/dnn/src/caffe/caffe_importer.cpp:242: error: (-215) pbBlob.raw_data_type() == caffe::FLOAT16 in function blobFromProto help me to solve it! this problem mentioned many times ago without answer! http://answers.opencv.org/question/177086/unable-to-use-caffe-model-trained-in-nvidia-digits-in-opencv-dnn-code/",
        "answers": [
            [
                "OpenCV suppports models trained in BVLC format and not NVCaffe. Try training your model by changing the solver parameter from NVCaffe to Compatible in DIGITS."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am new to this Deep Learning. I have learnt the basics through reading and trying to implement a real network to see how/if it'll really work. I chose Tensorflow in digits and the following network because they give out the exact architecture with training materiel. Steganalysis with DL I wrote the following code for the architecture in Steganalysis with DL by looking at networks existing networks in digits and Tensorflow documentation. from model import Tower from utils import model_property import tensorflow as tf import tensorflow.contrib.slim as slim import utils as digits class UserModel(Tower): @model_property def inference(self): x = tf.reshape(self.x, shape=[-1, self.input_shape[0], self.input_shape[1], self.input_shape[2]]) with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_initializer=tf.contrib.layers.xavier_initializer(), weights_regularizer=slim.l2_regularizer(0.0001)): conv1 = tf.layers.conv2d(inputs=x, filters=64, kernel_size=7, padding='same', strides=2, activation=tf.nn.relu) rnorm1 = tf.nn.local_response_normalization(input=conv1) conv2 = tf.layers.conv2d(inputs=rnorm1, filters=16, kernel_size=5, padding='same', strides=1, activation=tf.nn.relu) rnorm2 = tf.nn.local_response_normalization(input=conv2) flatten = tf.contrib.layers.flatten(rnorm2) fc1 = tf.contrib.layers.fully_connected(inputs=flatten, num_outputs=1000, activation_fn=tf.nn.relu) fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1000, activation_fn=tf.nn.relu) fc3 = tf.contrib.layers.fully_connected(inputs=fc2, num_outputs=2) sm = tf.nn.softmax(fc3) return fc3 @model_property def loss(self): model = self.inference loss = digits.classification_loss(model, self.y) accuracy = digits.classification_accuracy(model, self.y) self.summaries.append(tf.summary.scalar(accuracy.op.name, accuracy)) return loss I tried running it but the accuracy is pretty low. Could someone tell me if I've done it completely wrong or what's wrong with it and tell me how to properly code it? UPDATE: Thank you Nessuno! With the fix you mentioned I came up with this code: from model import Tower from utils import model_property import tensorflow as tf import tensorflow.contrib.slim as slim import utils as digits class UserModel(Tower): @model_property def inference(self): x = tf.reshape(self.x, shape=[-1, self.input_shape[0], self.input_shape[1], self.input_shape[2]]) with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_initializer=tf.contrib.layers.xavier_initializer(), weights_regularizer=slim.l2_regularizer(0.00001)): conv1 = tf.layers.conv2d(inputs=x, filters=64, kernel_size=7, padding='Valid', strides=2, activation=tf.nn.relu) rnorm1 = tf.nn.local_response_normalization(input=conv1) conv2 = tf.layers.conv2d(inputs=rnorm1, filters=16, kernel_size=5, padding='Valid', strides=1, activation=tf.nn.relu) rnorm2 = tf.nn.local_response_normalization(input=conv2) flatten = tf.contrib.layers.flatten(rnorm2) fc1 = tf.contrib.layers.fully_connected(inputs=flatten, num_outputs=1000, activation_fn=tf.nn.relu) fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1000, activation_fn=tf.nn.relu) fc3 = tf.contrib.layers.fully_connected(inputs=fc2, num_outputs=2, activation_fn=None) return fc3 @model_property def loss(self): model = self.inference loss = digits.classification_loss(model, self.y) accuracy = digits.classification_accuracy(model, self.y) self.summaries.append(tf.summary.scalar(accuracy.op.name, accuracy)) return loss Solver type is SGD. Learning rate is 0.001. I am shuffling training data.I have increased training data to 6000 (3000 per category, 20% from that is reserved for validation). I downloaded the training data from this link. But I am only getting the following graph. I think this is overfitting. Do you have any suggestions to improve the validation accuracy?",
        "answers": [
            [
                "In NVIDIA digits, classification_loss, exactly as in tensorflow tf.nn.softmax_cross_entropy_with_logits expects as input a linear layer of neuron. Instead, you're passing as input sm = tf.nn.softmax(fc3), hence you're applying the softmax operation 2 times and this is the reasong of your low accuracy. In order to solve this issue, just change the model output layer to fc3 = slim.fully_connected(fc2, 2, activation_fn=None, scope='fc3') return fc3"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I installed Caffe and DIGITs on python 3 and it seems to work partially, however I still get this Protobuf error about version 2.6.1 versus 3.5. So I reinstalled Protobuf and was then following through with reinstalling Caffe. I have an Anaconda python3.6 environment and I was recompiling Caffe in that environment. During the cmake .. step, I saw that the version of python that is detected is Python2.7 instead of Python3--even though I am in the conda python 3.6 environment. Even the version of numpy detected is for 2.7.: Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython2.7.so (found suitable version \"2.7.12\", minimum required is \"2.7\") -- Found NumPy: /home/krishnab/.local/lib/python2.7/site-packages/numpy/core/include (found suitable version \"1.11.0\", minimum required is \"1.7.1\") -- NumPy ver. 1.11.0 found (include: /home/krishnab/.local/lib/python2.7/site-packages/numpy/core/include) So my real question is whether DIGITS works with Caffe in python 3? I could not find any clear indication in the documentation except for an opaque reference to an issue: https://github.com/NVIDIA/DIGITS/issues/511 Can anyone clarify?",
        "answers": [],
        "votes": []
    },
    {
        "question": "What is the difference between Base Learning rate in Nvidia Digits UI during training and Learning rate param(lr_pm) for each layer defined in the caffe .prototxt file. Finally which learning rate is used?",
        "answers": [
            [
                "The learning rate for a particular layer is the global learning rate multiplied with the lr_mult of that specific layer."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Has anyone had success in training a semantic segmentation model (FCN8s) using DIGITS (v5) in multi-GPU mode? I can successfully train on a single GPU, but it's very slow. When I select multiple GPUs (my workstation has 4 Titan Xp), the entire system shuts down and restarts. This seems to be a problem that others have reported in the nVidia DIGITS Google group. Any insight is greatly appreciated. Ubuntu 14.04 Caffe 0.15.13 DIGITS 5.0",
        "answers": [],
        "votes": []
    },
    {
        "question": "The package Digits requires an environment variable to be set with the location of Caffe install directory. The simple way to install Caffe is apt-get install caffe-cuda. However, I can't figure out where it is installed. It is not installed in the home directory, and dpkg -L caffe-cuda shows only a few files in /usr/share/doc. Any ideas?",
        "answers": [
            [
                "The following command may help you: find / -name \"*caffe*\" The result on colaboratory is not allowed by stackoverflow to paste here, because stackoverflow mistake it by code."
            ],
            [
                "Since you are installing caffe through sudo apt-get install all the binaries and libraries will be put to default locations and the caffe path will be added. Now, u can use caffe through cmd directly from any directory. Hope it helps!!"
            ]
        ],
        "votes": [
            1.0000001,
            -0.9999999
        ]
    },
    {
        "question": "I want to use mnist dataset to train a simple autoencoder in caffe and with nvidia-digits. I have: caffe: 0.16.4 DIGITS: 5.1 python 2.7 I use the structure provided here: https://github.com/BVLC/caffe/blob/master/examples/mnist/mnist_autoencoder.prototxt Then I face 2 problems: When I use the provided structure I get this error: Traceback (most recent call last): File \"digits/scheduler.py\", line 512, in run_task task.run(resources) File \"digits/task.py\", line 189, in run self.before_run() File \"digits/model/tasks/caffe_train.py\", line 220, in before_run self.save_files_generic() File \"digits/model/tasks/caffe_train.py\", line 665, in save_files_generic 'cannot specify two val image data layers' AssertionError: cannot specify two val image data layers when I remove the layer for ''test-on-test'', I get a bad result like this: https://screenshots.firefox.com/8hwLmSmEP2CeiyQP/localhost What is the problem??",
        "answers": [
            [
                "The first problem occurs because the .prototxt has two layers with name data and TEST phase. The first layer that uses data, i.e. flatdata, does not know which data to use (the test-to-train or test-to-test). That's why when you remove one of the data layers with TEST phase, the error does not happen. Edit: I've checked the solver file and it has a test_stage parameter that should switch between the test files, but it's clearly not working in your case. The second problem is a little more difficult to solve. My knowledge in autoencoders is limited. It seems your euclidean loss changes very little during your iterations; I would check the base learning rate in your solver.prototxt and decrease it. Check how the losses fluctuate. Besides that, for the epochs/iterations that achieved a low error, have you checked the output data/images? Do they make sense?"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm running a DIGITS in a container and I'm starting that container with -p 5001 parameter. How can I connect that DIGITS site using my host pc? Here is how my container looks like when i run DIGITS app.",
        "answers": [
            [
                "Try running docker inspect &lt;container ID&gt; (your container id starts with 2f1a6, which will provide you with network configuration of your container. You can then try to connect to container_ip:5001 to access your service."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to implement autoencoder on mnist_generic with caffe in NVIDIA digits. I made the dataset based on the approach explained here: https://github.com/NVIDIA/DIGITS/tree/master/examples/autoencoder But I do not want to use Torch or Tensorflow to build an autoencoder. I wanna make it with Caffe. I used the mnist_autoencoder provided in caffe and ran that in DIGITS. But I get error like: ERROR: cannot specify two val image data layers when I removed a layer for test (the third layer in the architecture) I got this error: ERROR: Check failed: bottom[0]-&gt;count() == bottom[1]-&gt;count() (78400 vs. 235200) SIGMOID_CROSS_ENTROPY_LOSS layer inputs must have the same count. here is the autoencoder architecture: name: \"MNISTAutoencoder\" layer { name: \"data\" type: \"Data\" top: \"data\" include { phase: TRAIN } transform_param { scale: 0.0039215684 } data_param { batch_size: 100 backend: LMDB } } layer { name: \"data\" type: \"Data\" top: \"data\" include { phase: TEST stage: \"test-on-train\" } transform_param { scale: 0.0039215684 } data_param { batch_size: 100 backend: LMDB } } layer { name: \"data\" type: \"Data\" top: \"data\" include { phase: TEST stage: \"test-on-test\" } transform_param { scale: 0.0039215684 } data_param { batch_size: 100 backend: LMDB } } layer { name: \"flatdata\" type: \"Flatten\" bottom: \"data\" top: \"flatdata\" } layer { name: \"encode1\" type: \"InnerProduct\" bottom: \"data\" top: \"encode1\" param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 1 decay_mult: 0 } inner_product_param { num_output: 1000 weight_filler { type: \"gaussian\" std: 1 sparse: 15 } bias_filler { type: \"constant\" value: 0 } } } layer { name: \"encode1neuron\" type: \"Sigmoid\" bottom: \"encode1\" top: \"encode1neuron\" } layer { name: \"encode2\" type: \"InnerProduct\" bottom: \"encode1neuron\" top: \"encode2\" param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 1 decay_mult: 0 } inner_product_param { num_output: 500 weight_filler { type: \"gaussian\" std: 1 sparse: 15 } bias_filler { type: \"constant\" value: 0 } } } layer { name: \"encode2neuron\" type: \"Sigmoid\" bottom: \"encode2\" top: \"encode2neuron\" } layer { name: \"encode3\" type: \"InnerProduct\" bottom: \"encode2neuron\" top: \"encode3\" param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 1 decay_mult: 0 } inner_product_param { num_output: 250 weight_filler { type: \"gaussian\" std: 1 sparse: 15 } bias_filler { type: \"constant\" value: 0 } } } layer { name: \"encode3neuron\" type: \"Sigmoid\" bottom: \"encode3\" top: \"encode3neuron\" } layer { name: \"encode4\" type: \"InnerProduct\" bottom: \"encode3neuron\" top: \"encode4\" param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 1 decay_mult: 0 } inner_product_param { num_output: 30 weight_filler { type: \"gaussian\" std: 1 sparse: 15 } bias_filler { type: \"constant\" value: 0 } } } layer { name: \"decode4\" type: \"InnerProduct\" bottom: \"encode4\" top: \"decode4\" param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 1 decay_mult: 0 } inner_product_param { num_output: 250 weight_filler { type: \"gaussian\" std: 1 sparse: 15 } bias_filler { type: \"constant\" value: 0 } } } layer { name: \"decode4neuron\" type: \"Sigmoid\" bottom: \"decode4\" top: \"decode4neuron\" } layer { name: \"decode3\" type: \"InnerProduct\" bottom: \"decode4neuron\" top: \"decode3\" param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 1 decay_mult: 0 } inner_product_param { num_output: 500 weight_filler { type: \"gaussian\" std: 1 sparse: 15 } bias_filler { type: \"constant\" value: 0 } } } layer { name: \"decode3neuron\" type: \"Sigmoid\" bottom: \"decode3\" top: \"decode3neuron\" } layer { name: \"decode2\" type: \"InnerProduct\" bottom: \"decode3neuron\" top: \"decode2\" param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 1 decay_mult: 0 } inner_product_param { num_output: 1000 weight_filler { type: \"gaussian\" std: 1 sparse: 15 } bias_filler { type: \"constant\" value: 0 } } } layer { name: \"decode2neuron\" type: \"Sigmoid\" bottom: \"decode2\" top: \"decode2neuron\" } layer { name: \"decode1\" type: \"InnerProduct\" bottom: \"decode2neuron\" top: \"decode1\" param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 1 decay_mult: 0 } inner_product_param { num_output: 784 weight_filler { type: \"gaussian\" std: 1 sparse: 15 } bias_filler { type: \"constant\" value: 0 } } } layer { name: \"loss\" type: \"SigmoidCrossEntropyLoss\" bottom: \"decode1\" bottom: \"flatdata\" top: \"cross_entropy_loss\" loss_weight: 1 include { phase: TRAIN } } layer { name: \"decode1neuron\" type: \"Sigmoid\" bottom: \"decode1\" top: \"decode1neuron\" } layer { name: \"loss\" type: \"EuclideanLoss\" bottom: \"decode1neuron\" bottom: \"flatdata\" top: \"l2_error\" loss_weight: 0 include { phase: TRAIN } } Do you know how I can train this mnist autoencoder using caffe in nvidia-digits?",
        "answers": [],
        "votes": []
    },
    {
        "question": "the code in pytorch is here, I only used the vgg19 arch. In order to make the cifar10 dataset preprocessed same in caffe and pytorch, I remove all the transforms in main.py but toTensor(). I found the cifar10 dataset's range is [0,1] in pytorch, but caffe is [0,255], so I scale 1/255 below. Any extra preprocessing is canceled to make things easier. here is my caffe net definition prototxt: layer { name: \"data\" type: \"Data\" top: \"data\" top: \"label\" include { phase: TRAIN } transform_param { scale: 0.00392156862745 } data_param { source: \"/home/snk/Documents/digits-jobs/20180106-135745-4cac/train_db\" batch_size: 128 backend: LMDB } } layer { name: \"data\" type: \"Data\" top: \"data\" top: \"label\" include { phase: TEST } transform_param { scale: 0.00392156862745 } data_param { source: \"/home/snk/Documents/digits-jobs/20180106-135745-4cac/val_db\" batch_size: 128 backend: LMDB } } layer { name: \"conv1_1\" type: \"Convolution\" bottom: \"data\" top: \"conv1_1\" convolution_param { num_output: 64 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu1_1\" type: \"ReLU\" bottom: \"conv1_1\" top: \"conv1_1\" } layer { name: \"conv1_2\" type: \"Convolution\" bottom: \"conv1_1\" top: \"conv1_2\" convolution_param { num_output: 64 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu1_2\" type: \"ReLU\" bottom: \"conv1_2\" top: \"conv1_2\" } layer { name: \"pool1\" type: \"Pooling\" bottom: \"conv1_2\" top: \"pool1\" pooling_param { pool: MAX kernel_size: 2 stride: 2 } } layer { name: \"conv2_1\" type: \"Convolution\" bottom: \"pool1\" top: \"conv2_1\" convolution_param { num_output: 128 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu2_1\" type: \"ReLU\" bottom: \"conv2_1\" top: \"conv2_1\" } layer { name: \"conv2_2\" type: \"Convolution\" bottom: \"conv2_1\" top: \"conv2_2\" convolution_param { num_output: 128 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu2_2\" type: \"ReLU\" bottom: \"conv2_2\" top: \"conv2_2\" } layer { name: \"pool2\" type: \"Pooling\" bottom: \"conv2_2\" top: \"pool2\" pooling_param { pool: MAX kernel_size: 2 stride: 2 } } layer { name: \"conv3_1\" type: \"Convolution\" bottom: \"pool2\" top: \"conv3_1\" convolution_param { num_output: 256 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu3_1\" type: \"ReLU\" bottom: \"conv3_1\" top: \"conv3_1\" } layer { name: \"conv3_2\" type: \"Convolution\" bottom: \"conv3_1\" top: \"conv3_2\" convolution_param { num_output: 256 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu3_2\" type: \"ReLU\" bottom: \"conv3_2\" top: \"conv3_2\" } layer { name: \"conv3_3\" type: \"Convolution\" bottom: \"conv3_2\" top: \"conv3_3\" convolution_param { num_output: 256 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu3_3\" type: \"ReLU\" bottom: \"conv3_3\" top: \"conv3_3\" } layer { name: \"conv3_4\" type: \"Convolution\" bottom: \"conv3_3\" top: \"conv3_4\" convolution_param { num_output: 256 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu3_4\" type: \"ReLU\" bottom: \"conv3_4\" top: \"conv3_4\" } layer { name: \"pool3\" type: \"Pooling\" bottom: \"conv3_4\" top: \"pool3\" pooling_param { pool: MAX kernel_size: 2 stride: 2 } } layer { name: \"conv4_1\" type: \"Convolution\" bottom: \"pool3\" top: \"conv4_1\" convolution_param { num_output: 512 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu4_1\" type: \"ReLU\" bottom: \"conv4_1\" top: \"conv4_1\" } layer { name: \"conv4_2\" type: \"Convolution\" bottom: \"conv4_1\" top: \"conv4_2\" convolution_param { num_output: 512 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu4_2\" type: \"ReLU\" bottom: \"conv4_2\" top: \"conv4_2\" } layer { name: \"conv4_3\" type: \"Convolution\" bottom: \"conv4_2\" top: \"conv4_3\" convolution_param { num_output: 512 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu4_3\" type: \"ReLU\" bottom: \"conv4_3\" top: \"conv4_3\" } layer { name: \"conv4_4\" type: \"Convolution\" bottom: \"conv4_3\" top: \"conv4_4\" convolution_param { num_output: 512 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu4_4\" type: \"ReLU\" bottom: \"conv4_4\" top: \"conv4_4\" } layer { name: \"pool4\" type: \"Pooling\" bottom: \"conv4_4\" top: \"pool4\" pooling_param { pool: MAX kernel_size: 2 stride: 2 } } layer { name: \"conv5_1\" type: \"Convolution\" bottom: \"pool4\" top: \"conv5_1\" convolution_param { num_output: 512 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu5_1\" type: \"ReLU\" bottom: \"conv5_1\" top: \"conv5_1\" } layer { name: \"conv5_2\" type: \"Convolution\" bottom: \"conv5_1\" top: \"conv5_2\" convolution_param { num_output: 512 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu5_2\" type: \"ReLU\" bottom: \"conv5_2\" top: \"conv5_2\" } layer { name: \"conv5_3\" type: \"Convolution\" bottom: \"conv5_2\" top: \"conv5_3\" convolution_param { num_output: 512 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu5_3\" type: \"ReLU\" bottom: \"conv5_3\" top: \"conv5_3\" } layer { name: \"conv5_4\" type: \"Convolution\" bottom: \"conv5_3\" top: \"conv5_4\" convolution_param { num_output: 512 pad: 1 kernel_size: 3 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu5_4\" type: \"ReLU\" bottom: \"conv5_4\" top: \"conv5_4\" } layer { name: \"pool5\" type: \"Pooling\" bottom: \"conv5_4\" top: \"pool5\" pooling_param { pool: MAX kernel_size: 2 stride: 2 } } layer { name: \"drop6\" type: \"Dropout\" bottom: \"pool5\" top: \"drop_pool5\" dropout_param { dropout_ratio: 0.5 } } layer { name: \"fc6\" type: \"InnerProduct\" bottom: \"drop_pool5\" top: \"fc6\" inner_product_param { num_output: 512 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu6\" type: \"ReLU\" bottom: \"fc6\" top: \"fc6\" } layer { name: \"drop7\" type: \"Dropout\" bottom: \"fc6\" top: \"fc6\" dropout_param { dropout_ratio: 0.5 } } layer { name: \"fc7\" type: \"InnerProduct\" bottom: \"fc6\" top: \"fc7\" inner_product_param { num_output: 512 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"relu7\" type: \"ReLU\" bottom: \"fc7\" top: \"fc7\" } layer { name: \"fc8\" type: \"InnerProduct\" bottom: \"fc7\" top: \"fc8\" inner_product_param { num_output: 10 weight_filler { type: \"xavier\" } bias_filler { type: \"xavier\" } } } layer { name: \"acc\" type: \"Accuracy\" bottom: \"fc8\" bottom: \"label\" top: \"accuracy\" include { phase: TEST } } layer { name: \"loss\" type: \"SoftmaxWithLoss\" bottom: \"fc8\" bottom: \"label\" top: \"loss\" } Here is my solver.prototxt test_iter: 79 test_interval: 391 base_lr: 0.05 display: 40 max_iter: 117300 lr_policy: \"step\" gamma: 0.5 momentum: 0.9 weight_decay: 0.0005 stepsize: 11730 snapshot: 3910 snapshot_prefix: \"snapshot\" solver_mode: GPU net: \"train_val.prototxt\" solver_type: SGD The fact is pytorch can train the model at a very quick speed, the acc after one epoch is about 20%, but in caffe the loss is always around 2.3XXX (about -log(0.1), random guess loss), I suspect it was because the different weight initialization, so I changed caffe's filler.hpp's xavier to 'efficient backprop'(i.e U(-std, std), std=1/(sqrt(fan_in))), but it didn't work. Now, the only difference is the bias initialization method, in pytorch it uses the weight's fan_in, but in caffe I think it uses the output_num as fan_in (because its shape is [1,N], N is the number of output neurons, and in filler.hpp it use blob.count() / blob.num() as fan_in for xavier ). Who can help me ? I think if all the configuration is the same, the training process is almost the same ,but it breaked my opinion.",
        "answers": [],
        "votes": []
    },
    {
        "question": "My Linux terminal screenshot here # the error entry in linux terminal screenshot if FLAGS.validation_db: val_model.start_queue_runners(sess) Validation(sess, val_model, 0) #the call func above val_model def start_queue_runners(self, sess): logging.info('Starting queue runners (%s)', self.stage) # Distinguish the queue runner collection (for easily obtaining them by collection key) queue_runners = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS, scope=self.stage+'.*') for qr in queue_runners: if self.stage in qr.name: tf.add_to_collection(digits.GraphKeys.QUEUE_RUNNERS, qr) self.queue_coord = tf.train.Coordinator() self.queue_threads = tf.train.start_queue_runners(sess=sess, coord=self.queue_coord, collection=digits.GraphKeys.QUEUE_RUNNERS) logging.info('Queue runners started (%s)', self.stage) what happend in my linux-terminal\uff0cwhere is need to modify? i find some answer about it in stackoverflow,but its not sut me the specific error under: 2017-12-19 05:49:34 [INFO] Starting queue runners (val) Traceback (most recent call last): File \"/root/digits/digits/tools/tensorflow/main.py\", line 627, in main val_model.start_queue_runners(sess) File \"/root/digits/digits/tools/tensorflow/model.py\", line 208, in start_queue_runners tf.add_to_collection(digits.GraphKeys.QUEUE_RUNNERS, qr) File \"/usr/local/lib/python2.7/dist- packages/tensorflow/python/framework/ops.py\", line 4 248, in add_to_collection get_default_graph().add_to_collection(name, value) File \"/usr/local/lib/python2.7/dist- packages/tensorflow/python/framework/ops.py\", line 2 792, in add_to_collection self._check_not_finalized() File \"/usr/local/lib/python2.7/dist- packages/tensorflow/python/framework/ops.py\", line 2 181, in _check_not_finalized raise RuntimeError(\"Graph is finalized and cannot be modified.\") RuntimeError: Graph is finalized and cannot be modified.",
        "answers": [
            [
                "You do not need to modify the graph to start queue runners. It's not immediately clear what you are trying to do here, but the following code is equivalent and does not modify the graph: def start_queue_runners(self, sess): logging.info('Starting queue runners (%s)', self.stage) queue_runners = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS, scope=self.stage+'.*') self.queue_coord = tf.train.Coordinator() # Start the queue runner threads directly without adding them to a collection. for qr in queue_runners: if self.stage in qr.name: qr.create_threads(sess, coord=self.queue_coord) logging.info('Queue runners started (%s)', self.stage)"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I trained a LeNet-gray-28x28 image detection Tensorflow model using Nvidia DIGITS, giving me the results I expect. Now, I have to classify some images outside of DIGITS and I want to use the model I trained to. So I get the LeNet model used by DIGITS and I create a class to use it : import tensorflow as tf import tensorflow.contrib.slim as slim import tflearn from tflearn.layers.core import input_data class LeNetModel(): def gray28(self, nclasses): x = input_data(shape=[None, 28, 28, 1]) # scale (divide by MNIST std) # x = x * 0.0125 with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_initializer=tf.contrib.layers.xavier_initializer(), weights_regularizer=slim.l2_regularizer(0.0005)): model = slim.conv2d(x, 20, [5, 5], padding='VALID', scope='conv1') model = slim.max_pool2d(model, [2, 2], padding='VALID', scope='pool1') model = slim.conv2d(model, 50, [5, 5], padding='VALID', scope='conv2') model = slim.max_pool2d(model, [2, 2], padding='VALID', scope='pool2') model = slim.flatten(model) model = slim.fully_connected(model, 500, scope='fc1') model = slim.dropout(model, 0.5, is_training=False, scope='do1') model = slim.fully_connected(model, nclasses, activation_fn=None, scope='fc2') return tflearn.DNN(model) I downloaded my model from DIGITS and I instantiate it using (in another file) : self.ballmodel = LeNetModel().gray28(2) self.ballmodel.load(\"src/perftrack/prototype/models/ball/snapshot_5.ckpt\") But, when I launch my script, I get these exceptions : 2017-11-26 14:55:50.330524: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key conv1/biases not found in checkpoint 2017-11-26 14:55:50.330948: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key Global_Step not found in checkpoint 2017-11-26 14:55:50.331270: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key is_training not found in checkpoint 2017-11-26 14:55:50.331564: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key conv2/weights not found in checkpoint 2017-11-26 14:55:50.332823: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key conv1/weights not found in checkpoint 2017-11-26 14:55:50.332891: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key conv2/biases not found in checkpoint 2017-11-26 14:55:50.333620: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key fc2/weights not found in checkpoint 2017-11-26 14:55:50.334021: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key fc1/weights not found in checkpoint 2017-11-26 14:55:50.334173: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key fc1/biases not found in checkpoint 2017-11-26 14:55:50.334431: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key fc2/biases not found in checkpoint ... raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.NotFoundError: Key conv1/biases not found in checkpoint [[Node: save_1/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_1/tensor_names, save_1/RestoreV2_1/shape_and_slices)]] [[Node: save_1/RestoreV2_1/_19 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_38_save_1/RestoreV2_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]] So I use https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/inspect_checkpoint.py script to inspect the key names my checkpoint contains, and I get things like : model/conv1/biases model/conv2/weights ... So I rewrite my network, adding the model/ prefix manually : model = slim.conv2d(x, 20, [5, 5], padding='VALID', scope='model/conv1') model = slim.max_pool2d(model, [2, 2], padding='VALID', scope='model/pool1') model = slim.conv2d(model, 50, [5, 5], padding='VALID', scope='model/conv2') model = slim.max_pool2d(model, [2, 2], padding='VALID', scope='model/pool2') model = slim.flatten(model) model = slim.fully_connected(model, 500, scope='model/fc1') model = slim.dropout(model, 0.5, is_training=False, scope='model/do1') model = slim.fully_connected(model, nclasses, It fixes some of the missing keys warning but : I sense that this is not the right way to fix it I can't fix two keys : Global_Step (I have a global_step key in my checkpoint) is_training (I don't know what it is) So my question is : how can I redefine these key names in my network to match the ones I find in my checkpoint ?",
        "answers": [
            [
                "Because my question is mostly due to my bad understanding of TensorFlow, I do a trip on the official documentation, and I've found some answers. Firstly, I combine the use of contrib/slim and contrib/tflearn and even if it is possible, it is not really relevant. So I rewrite the network using only slim : import tensorflow as tf import tensorflow.contrib.slim as slim class LeNetModel(): def gray28(self, nclasses): # x = input_data(shape=[None, 28, 28, 1]) x = tf.placeholder(tf.float32, shape=[1, 28, 28], name=\"x\") rs = tf.reshape(x, shape=[-1, 28, 28, 1]) # scale (divide by MNIST std) # x = x * 0.0125 with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_initializer=tf.contrib.layers.xavier_initializer(), weights_regularizer=slim.l2_regularizer(0.0005)): model = slim.conv2d(rs, 20, [5, 5], padding='VALID', scope='conv1') model = slim.max_pool2d(model, [2, 2], padding='VALID', scope='pool1') model = slim.conv2d(model, 50, [5, 5], padding='VALID', scope='conv2') model = slim.max_pool2d(model, [2, 2], padding='VALID', scope='pool2') model = slim.flatten(model) model = slim.fully_connected(model, 500, scope='fc1') model = slim.dropout(model, 0.5, is_training=True, scope='do1') model = slim.fully_connected(model, nclasses, activation_fn=None, scope='fc2') return x, model I return the x placeholder and the model, and I use it to load the DIGITS pre-trained model (checkpoint) : import tensorflow as tf import tensorflow.contrib.slim as slim import cv2 from models.lenet import LeNetModel # Helper function to load/resize images def image(path): img = cv2.imread(path, 0) return cv2.resize(img, dsize=(28,28)) # Define a function that adds the model/ prefix to all variables : def name_in_checkpoint(var): return 'model/' + var.op.name #Instantiate the model x, model = LeNetModel().gray28(2) # Define the variables to restore : # Exclude the \"is_training\" that I don't care about variables_to_restore = slim.get_variables_to_restore(exclude=[\"is_training\"]) # Rename the other variables with the function name_in_checkpoint variables_to_restore = {name_in_checkpoint(var):var for var in variables_to_restore} # Create a Saver to restore the checkpoint, given the variables restorer = tf.train.Saver(variables_to_restore) #Launch a session to restore the checkpoint and try to infer some images : with tf.Session() as sess: # Restore variables from disk. restorer.restore(sess, \"src/prototype/models/snapshot_5.ckpt\") print(\"Model restored.\") print(sess.run(model, feed_dict={x:[image(\"/home/damien/Vid\u00e9os/1/positives/img/1-img143.jpg\")]})) print(sess.run(model, feed_dict={x:[image(\"/home/damien/Vid\u00e9os/0/positives/img/1-img1.jpg\")]})) And it works !"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am using NVIDIA DIGITS for image classification. After I train my network and I test the model on the test data, I want to save the visualization and statistics that DIGITS can generate in my defined folder. How can I do it? For example how can I save each image square of the following image separately in a folder I specify in the program written for this part??",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to use Torch for training. I have my image data in LMDB format and I have done the conversion using NVIDIA DIGITS (caffe). How can I convert this .lmdb format to csv or .t7 so that I can use it in Torch?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am looking for detection/localization CNNs to run within the NVidia DIGITS training platform. So far it seems they only support their homebrew DetectNet for this purpose. Looking around it seems that other SOTA networks such as faster-RCNN, SSD, and YOLO might compete with DetectNet in terms of performance and accuracy, but it does not look like they currently have any support in DIGITS. (Faster-RCNN has a fairly popular implementation, but it is run out of a version of Caffe not supported by DIGITS.) If anyone has had any success obtaining and using SOTA detection networks with NVidia DIGITS, would you mind supplying links/documentation regarding?",
        "answers": [
            [
                "Currently, NVIDIA provides NVIDIA Transfer Learning Toolkit which is useful for training CNNs. NVIDIA provides SSD, Faster RCNN, DetectNet, etc. You can find more here: https://developer.nvidia.com/transfer-learning-toolkit"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "EDIT: FOUND THE PROBLEM. I forgot to add a custom class when creating the dataset. Amateur mistake, but leaving this up for others who make a similar error. After successfully following the KITTI tutorial provided by NVIDIA, I began trying to run DIGITS on my own dataset. The problem is that almost immediately after starting, the loss coverage for both the train and validation goes straight to almost zero, on the scale of 10e-4 loss. This is a picture of the loss as a function of the epoch (account is too new to embed image). Extra details: ~900 training images and ~300 validation images (Note: I have been told by a friend that he got it to work with this data set, so I don't believe the number of images is the problem) The images are very simple and contain a model of a car on a mostly plain background. I had to create the validation data by taking images/labels from the training data The model I used is the DetectNet model mentioned in the DIGITS object detection tutorial. I used the GoogLeNet pretrained model also mentioned in the tutorial. What I tried: Changing the hyperparameters appears to do nothing As mentioned earlier, running the model for longer than 1 epoch does not change anything (loss coverage stays close to zero)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I made following python layer and added it to the LeNet architecture. But when building model it gives an error. I am to apply my Python layer using Numpy but when I am using OpenCV it gives an error. Following I am adding my code and corresponding error from a log file. import cv2 import caffe import random def doEqualizeHist(img): img = img.astype(np.uint8) img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) return cv2.equalizeHist(img) class EqualizeLayer(caffe.Layer): def setup(self, bottom, top): assert len(bottom) == 1, 'requires a single layer.bottom' assert bottom[0].data.ndim &gt;= 3, 'requires image data' assert len(top) == 1, 'requires a single layer.top' def reshape(self, bottom, top): # Copy shape from bottom top[0].reshape(*bottom[0].data.shape) def forward(self, bottom, top): # Copy all of the data top[0].data[...] = bottom[0].data[...] for ii in xrange(0, top[0].data.shape[0]): imin = top[0].data[ii, :, :, :].transpose(1, 2, 0) top[0].data[ii, :, :, :] = doEqualizeHist(imin).transpose(2, 0, 1) def backward(self, top, propagate_down, bottom): pass Error Message: 0812 06:41:53.452097 14355 net.cpp:723] Ignoring source layer train-data OpenCV Error: Assertion failed (scn == 3 || scn == 4) in cvtColor, file /build/opencv-SviWsf/opencv-2.4.9.1+dfsg/modules/imgproc/src/color.cpp, line 3737 Traceback (most recent call last): File \"/var/lib/digits/jobs/20170812-064148-f44d/digits_python_layers.py\", line 27, in forward top[0].data[ii, :, :, :] = doEqualizeHist(imin).transpose(2, 0, 1) File \"/var/lib/digits/jobs/20170812-064148-f44d/digits_python_layers.py\", line 8, in doEqualizeHist img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) cv2.error: /build/opencv-SviWsf/opencv-2.4.9.1+dfsg/modules/imgproc/src/color.cpp:3737: error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "answers": [
            [
                "For future reference, an \"Assertion failed\" error message in OpenCV means you passed invalid data to a function. In this case, the assertion that failed is scn == 3 || scn == 4. To know exactly what that means, you can look at the source file where the assertion failed: modules/impgproc/src/color.cpp and examine the function where it happened: cvtColor at line 3737. Look to see what the variable scn represents. In your case, the problem is that you're converting img to a single-channel format and then attempting to convert it from RGB to grayscale. That conversion is first asserting the input is a 3- or 4-channel format. It isn't so the assertion fails."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I started to use AWS EC2 for deep learning and I'm going to use NVIDIA DIGITS. I already got a g3.4xlarge instance with 50G storage with I defined during creating instance process. Now my question is that if I upload my image's folder which is about 6GB and configure all DIGITS installation. Am I going to lose all the data and configuration after stopping my instance?",
        "answers": [
            [
                "No you won't, EBS backed data will persist even after stopping, g3.4xlarge comes with EBS backed. Data will be lost only when store it in instance storage."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm currently training an image classifier using Nvidia DIGITS. I'm downloading 1,000,000 images as part of the ILSVRC12 dataset. As you may know, this dataset consists of 1,000 classes, with 1,000 images per class. The problem is that a lot of the images are downloaded from dead Flickr URLs, thus populating a decent portion of my dataset (about 5-10%) with the generic \"unavailable\" image shown below. I plan on going through and deleting each copy of this \"generic\" image, thus leaving my dataset with only images relevant to each class. This action would make the size of the classes uneven. They would no longer contain 1,000 images each. They would contain between 900-1,000 images each. Does the size of each class have to be equal? In other words, can I delete these generic images without affecting the accuracy of my classifier? Thanks in advance for you feedback.",
        "answers": [
            [
                "The number of training data per class does not have to be exactly equal. 10% difference one way or another won't affect the training process significantly. If you are still concern about the label imbalance, you may consider using \"InfogainLoss\" layer to compensate for the missing examples. PS, You make take advantage of the fact that all invalid flickr photos are in fact identical and remove them automatically based on their md5sum. See this answer for example on how to filter out these images when downloading imagenet photos."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "According to information on the Nvidia website Digits uses datatasets in Kitti format. Is there possibilty in Digits or in external application to prepare such dataset or I will have to write it on my own? I would like to simply draw bounding boxes on the displayed image and then have it converted to txt appropiate txt file. Thanks in advance!",
        "answers": [
            [
                "Yep, you can use one of the available solutions for bounding box annotations eg. RectLabel, save the annotations in Pascal VOC format and then transform it to Kitti using one of the freely available converters, eg: VOD Converter"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have trained a model using Caffe and NVIDIA's DIGITS. Testing it on DIGITS for the following images results in the following: When I download the model from DIGITS I get a snapshot_iter_24240.caffemodel along with deploy.prototxt, mean.binaryproto and labels.txt. (and solver.prototxt and train_val.prototxt which I think is not relevant) I use coremltools to convert the caffemodel to mlmodel running the following: import coremltools # Convert a caffe model to a classifier in Core ML coreml_model = coremltools.converters.caffe.convert(('snapshot_iter_24240.caffemodel', 'deploy.prototxt', 'mean.binaryproto'), image_input_names = 'data', class_labels = 'labels.txt') # Now save the model coreml_model.save('food.mlmodel') The code outputs the following: (/anaconda/envs/coreml) bash-3.2$ python run.py ================= Starting Conversion from Caffe to CoreML ====================== Layer 0: Type: 'Input', Name: 'input'. Output(s): 'data'. Ignoring batch size and retaining only the trailing 3 dimensions for conversion. Layer 1: Type: 'Convolution', Name: 'conv1'. Input(s): 'data'. Output(s): 'conv1'. Layer 2: Type: 'ReLU', Name: 'relu1'. Input(s): 'conv1'. Output(s): 'conv1'. Layer 3: Type: 'LRN', Name: 'norm1'. Input(s): 'conv1'. Output(s): 'norm1'. Layer 4: Type: 'Pooling', Name: 'pool1'. Input(s): 'norm1'. Output(s): 'pool1'. Layer 5: Type: 'Convolution', Name: 'conv2'. Input(s): 'pool1'. Output(s): 'conv2'. Layer 6: Type: 'ReLU', Name: 'relu2'. Input(s): 'conv2'. Output(s): 'conv2'. Layer 7: Type: 'LRN', Name: 'norm2'. Input(s): 'conv2'. Output(s): 'norm2'. Layer 8: Type: 'Pooling', Name: 'pool2'. Input(s): 'norm2'. Output(s): 'pool2'. Layer 9: Type: 'Convolution', Name: 'conv3'. Input(s): 'pool2'. Output(s): 'conv3'. Layer 10: Type: 'ReLU', Name: 'relu3'. Input(s): 'conv3'. Output(s): 'conv3'. Layer 11: Type: 'Convolution', Name: 'conv4'. Input(s): 'conv3'. Output(s): 'conv4'. Layer 12: Type: 'ReLU', Name: 'relu4'. Input(s): 'conv4'. Output(s): 'conv4'. Layer 13: Type: 'Convolution', Name: 'conv5'. Input(s): 'conv4'. Output(s): 'conv5'. Layer 14: Type: 'ReLU', Name: 'relu5'. Input(s): 'conv5'. Output(s): 'conv5'. Layer 15: Type: 'Pooling', Name: 'pool5'. Input(s): 'conv5'. Output(s): 'pool5'. Layer 16: Type: 'InnerProduct', Name: 'fc6'. Input(s): 'pool5'. Output(s): 'fc6'. Layer 17: Type: 'ReLU', Name: 'relu6'. Input(s): 'fc6'. Output(s): 'fc6'. Layer 18: Type: 'Dropout', Name: 'drop6'. Input(s): 'fc6'. Output(s): 'fc6'. WARNING: Skipping training related layer 'drop6' of type 'Dropout'. Layer 19: Type: 'InnerProduct', Name: 'fc7'. Input(s): 'fc6'. Output(s): 'fc7'. Layer 20: Type: 'ReLU', Name: 'relu7'. Input(s): 'fc7'. Output(s): 'fc7'. Layer 21: Type: 'Dropout', Name: 'drop7'. Input(s): 'fc7'. Output(s): 'fc7'. WARNING: Skipping training related layer 'drop7' of type 'Dropout'. Layer 22: Type: 'InnerProduct', Name: 'fc8_food'. Input(s): 'fc7'. Output(s): 'fc8_food'. Layer 23: Type: 'Softmax', Name: 'prob'. Input(s): 'fc8_food'. Output(s): 'prob'. ================= Summary of the conversion: =================================== Detected input(s) and shape(s) (ignoring batch size): 'data' : 3, 227, 227 Size of mean image: (H,W) = (256, 256) is greater than input image size: (H,W) = (227, 227). Mean image will be center cropped to match the input image dimensions. Network Input name(s): 'data'. Network Output name(s): 'prob'. (/anaconda/envs/coreml) bash-3.2$ After about 45 seconds food.mlmodel is generated. I import it into an iOS project using Xcode Version 9.0 beta 3 (9M174d) and run the following the code in a single view iOS project. // // ViewController.swift // SeeFood // // Created by Reza Shirazian on 7/23/17. // Copyright \u00a9 2017 Reza Shirazian. All rights reserved. // import UIKit import CoreML import Vision class ViewController: UIViewController { override func viewDidLoad() { super.viewDidLoad() var images = [CIImage]() // guard let ciImage = CIImage(image: #imageLiteral(resourceName: \"pizza\")) else { // fatalError(\"couldn't convert UIImage to CIImage\") // } images.append(CIImage(image: #imageLiteral(resourceName: \"pizza\"))!) images.append(CIImage(image: #imageLiteral(resourceName: \"spaghetti\"))!) images.append(CIImage(image: #imageLiteral(resourceName: \"burger\"))!) images.append(CIImage(image: #imageLiteral(resourceName: \"sushi\"))!) images.forEach{detectScene(image: $0)} // Do any additional setup after loading the view, typically from a nib. } override func didReceiveMemoryWarning() { super.didReceiveMemoryWarning() // Dispose of any resources that can be recreated. } func detectScene(image: CIImage) { guard let model = try? VNCoreMLModel(for: food().model) else { fatalError() } // Create a Vision request with completion handler let request = VNCoreMLRequest(model: model) { [weak self] request, error in guard let results = request.results as? [VNClassificationObservation], let topResult = results.first else { fatalError(\"unexpected result type from VNCoreMLRequest\") } // Update UI on main queue //let article = (self?.vowels.contains(topResult.identifier.first!))! ? \"an\" : \"a\" DispatchQueue.main.async { [weak self] in results.forEach({ (result) in if Int(result.confidence * 100) &gt; 1 { print(\"\\(Int(result.confidence * 100))% it's \\(result.identifier)\") } }) print(\"********************************\") } } let handler = VNImageRequestHandler(ciImage: image) DispatchQueue.global(qos: .userInteractive).async { do { try handler.perform([request]) } catch { print(error) } } } } which outputs the following: 22% it's cup cakes 8% it's ice cream 5% it's falafel 5% it's macarons 3% it's churros 3% it's gyoza 3% it's donuts 2% it's tacos 2% it's cannoli ******************************** 35% it's cup cakes 22% it's frozen yogurt 8% it's chocolate cake 7% it's chocolate mousse 6% it's ice cream 2% it's donuts ******************************** 38% it's gyoza 7% it's falafel 6% it's tacos 4% it's hamburger 3% it's oysters 2% it's peking duck 2% it's hot dog 2% it's baby back ribs 2% it's cannoli ******************************** 7% it's hamburger 6% it's pork chop 6% it's steak 6% it's peking duck 5% it's pho 5% it's prime rib 5% it's baby back ribs 4% it's mussels 4% it's grilled salmon 2% it's filet mignon 2% it's foie gras 2% it's pulled pork sandwich ******************************** this is completely off and inconsistent with how the model was performing on DIGITS. I'm not sure what I'm doing wrong or if I've missed a step. I tried creating the model without mean.binaryproto but that made no difference. If it helps here is the deploy.prototxt input: \"data\" input_shape { dim: 1 dim: 3 dim: 227 dim: 227 } layer { name: \"conv1\" type: \"Convolution\" bottom: \"data\" top: \"conv1\" param { lr_mult: 1.0 decay_mult: 1.0 } param { lr_mult: 2.0 decay_mult: 0.0 } convolution_param { num_output: 96 kernel_size: 11 stride: 4 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0.0 } } } layer { name: \"relu1\" type: \"ReLU\" bottom: \"conv1\" top: \"conv1\" } layer { name: \"norm1\" type: \"LRN\" bottom: \"conv1\" top: \"norm1\" lrn_param { local_size: 5 alpha: 0.0001 beta: 0.75 } } layer { name: \"pool1\" type: \"Pooling\" bottom: \"norm1\" top: \"pool1\" pooling_param { pool: MAX kernel_size: 3 stride: 2 } } layer { name: \"conv2\" type: \"Convolution\" bottom: \"pool1\" top: \"conv2\" param { lr_mult: 1.0 decay_mult: 1.0 } param { lr_mult: 2.0 decay_mult: 0.0 } convolution_param { num_output: 256 pad: 2 kernel_size: 5 group: 2 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0.1 } } } layer { name: \"relu2\" type: \"ReLU\" bottom: \"conv2\" top: \"conv2\" } layer { name: \"norm2\" type: \"LRN\" bottom: \"conv2\" top: \"norm2\" lrn_param { local_size: 5 alpha: 0.0001 beta: 0.75 } } layer { name: \"pool2\" type: \"Pooling\" bottom: \"norm2\" top: \"pool2\" pooling_param { pool: MAX kernel_size: 3 stride: 2 } } layer { name: \"conv3\" type: \"Convolution\" bottom: \"pool2\" top: \"conv3\" param { lr_mult: 1.0 decay_mult: 1.0 } param { lr_mult: 2.0 decay_mult: 0.0 } convolution_param { num_output: 384 pad: 1 kernel_size: 3 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0.0 } } } layer { name: \"relu3\" type: \"ReLU\" bottom: \"conv3\" top: \"conv3\" } layer { name: \"conv4\" type: \"Convolution\" bottom: \"conv3\" top: \"conv4\" param { lr_mult: 1.0 decay_mult: 1.0 } param { lr_mult: 2.0 decay_mult: 0.0 } convolution_param { num_output: 384 pad: 1 kernel_size: 3 group: 2 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0.1 } } } layer { name: \"relu4\" type: \"ReLU\" bottom: \"conv4\" top: \"conv4\" } layer { name: \"conv5\" type: \"Convolution\" bottom: \"conv4\" top: \"conv5\" param { lr_mult: 1.0 decay_mult: 1.0 } param { lr_mult: 2.0 decay_mult: 0.0 } convolution_param { num_output: 256 pad: 1 kernel_size: 3 group: 2 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0.1 } } } layer { name: \"relu5\" type: \"ReLU\" bottom: \"conv5\" top: \"conv5\" } layer { name: \"pool5\" type: \"Pooling\" bottom: \"conv5\" top: \"pool5\" pooling_param { pool: MAX kernel_size: 3 stride: 2 } } layer { name: \"fc6\" type: \"InnerProduct\" bottom: \"pool5\" top: \"fc6\" param { lr_mult: 1.0 decay_mult: 1.0 } param { lr_mult: 2.0 decay_mult: 0.0 } inner_product_param { num_output: 4096 weight_filler { type: \"gaussian\" std: 0.005 } bias_filler { type: \"constant\" value: 0.1 } } } layer { name: \"relu6\" type: \"ReLU\" bottom: \"fc6\" top: \"fc6\" } layer { name: \"drop6\" type: \"Dropout\" bottom: \"fc6\" top: \"fc6\" dropout_param { dropout_ratio: 0.5 } } layer { name: \"fc7\" type: \"InnerProduct\" bottom: \"fc6\" top: \"fc7\" param { lr_mult: 1.0 decay_mult: 1.0 } param { lr_mult: 2.0 decay_mult: 0.0 } inner_product_param { num_output: 4096 weight_filler { type: \"gaussian\" std: 0.005 } bias_filler { type: \"constant\" value: 0.1 } } } layer { name: \"relu7\" type: \"ReLU\" bottom: \"fc7\" top: \"fc7\" } layer { name: \"drop7\" type: \"Dropout\" bottom: \"fc7\" top: \"fc7\" dropout_param { dropout_ratio: 0.5 } } layer { name: \"fc8_food\" type: \"InnerProduct\" bottom: \"fc7\" top: \"fc8_food\" param { lr_mult: 1.0 decay_mult: 1.0 } param { lr_mult: 2.0 decay_mult: 0.0 } inner_product_param { num_output: 101 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0.0 } } } layer { name: \"prob\" type: \"Softmax\" bottom: \"fc8_food\" top: \"prob\" }",
        "answers": [
            [
                "The discrepancy between the predictions on DIGITS using CaffeModel and CoreML was due to the fact that CoreML was interpreting the input data differently that DIGITS. Changing the call to convert with the following parameters resolved the issue coreml_model = coremltools.converters.caffe.convert(('snapshot_iter_24240.caffemodel', 'deploy.prototxt', 'mean.binaryproto'), image_input_names = 'data', class_labels = 'labels.txt', is_bgr=True, image_scale=255.) http://pythonhosted.org/coremltools/generated/coremltools.converters.caffe.convert.html#coremltools.converters.caffe.convert 99% it's spaghetti bolognese ******************************** 73% it's pizza 10% it's lasagna 7% it's spaghetti bolognese 2% it's spaghetti carbonara ******************************** 97% it's sushi ******************************** 97% it's hamburger ********************************"
            ],
            [
                "In its current form, coremltools has a tendency to change the input/output types and value ranges to suite its own internal optimizations. I highly recommend reimporting your newly-created .mlmodel file into your Python code and verify what data types it expects. For example: it will convert Int values into Float (uses Double type in Swift) and Bool values to Int (True:1, False:0)"
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "I wanted to use NVIDIA DIGITS for image segmentation. According to this tutorial, https://github.com/NVIDIA/DIGITS/tree/master/examples/semantic-segmentation , I have created caffmodel named as 'fcn_alexnet.caffemodel'. But, it is not showing on my DIGITS page. What shall I do to add this to DIGITS?",
        "answers": [
            [
                "I hope I am not too late to help. To use the \"pretrained FCN-AlexNet\", you need to follow exactly the instructions. select the Custom Network tab make sure the Caffe sub-tab is selected copy/paste this prototxt (please refer to the link you had attached) in Pretrained model(s) specify the path to the pre-trained FCN-Alexnet (*this need to be done)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I trained my model in Nvidia Digits 5 and I would now like to extract the accuracy and loss plots that were generated during training for a report. Is this data saved somewhere so that it would possible to extract the data for these plots so that I could plot it in Python and perhaps ultimately modify the plots to compare different models etc?",
        "answers": [
            [
                "The best solution I have found is to either look at the HTML file or to scan the text file caffe_output.log that is produced by Caffe. The text file is usually stored in /var/digits/jobs/insert_your_job_id/ but you can also just run on linux systems: locate caffe_output.log"
            ],
            [
                "Go to your DIGITS job folder and locate your job's subfolder. Inside you'll find a file status.pickle, which is a pickled object containing all your job's information. You can load it in python like so: import digits import pickle data = pickle.load(open('status.pickle','rb')) This object is somewhat generic and may contain multiple tasks. For a typical classification task it will likely be just one, but you will still need to access it via data.tasks[0]. From there you can grab the plots: data.tasks[0].combined_graph_data() which returns a somewhat convoluted dict (unfortunately - since your network can produce many accuracy/loss outputs, as well as even custom ones). It contains everything you need though - I managed to plot accuracy with: plt.plot( data.tasks[0].combined_graph_data()['columns'][2][1:] ) but it's likely that you'll have to write a bit of custom code. As always, dir() is your friend."
            ]
        ],
        "votes": [
            3.0000001,
            2.0000001
        ]
    },
    {
        "question": "I am trying to use NVIDIA DIGITS to perform image segmentation (by following this article). After training the model, I was able to perform the segmentation on individual images. Now I would like to give a list of test images and labeled images to DIGITS, and I want it to calculate accuracy for the input images. Is it possible to do it using DIGITS, if yes, then what are the steps? (options to select on framework while training/testing model, format of input file, folder structure, etc.) Also, if DIGITS can't do it, then are there any other frameworks? (I read this post, but it would be better if I can directly use some plug-and-play framework, to begin with) Thanks!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have several databases and I need to do classification on them on NVIDIA DIGITS. But importing my big data into DIGITS takes a lot of time ( 2-4 days)!!! Imagine I have converted 2 image sets into .lmdb forms like: data1 data2 --&gt; folder train1_db: data.mdb, lock.mdb --&gt; folder train2_db: data.mdb, lock.mdb --&gt; folder val1_db: data.mdb, lock.mdb --&gt; folder val2_db: data.mdb, lock.mdb --&gt; mean.binaryproto --&gt; mean.binaryproto --&gt; some other txt files... --&gt; some other txt files... Now I need to concatenate these two .lmdb databases and save time. So I have done that separately in python from Merge two LMDB databases for feeding to the network (caffe) and I have the third dataset containing: train_db and val_db folders each containing data.mdb and lock.mdb files like above. data3 --&gt; folder train3_db: train1_db + train2_db --&gt; folder va3_db: val_db + va2_db I need to import these into DIGITS so that I train a network on them. My questions are: 1- should I import the folders train_db and val_db in image LMDB part? 2- I searched for label LMDB but I did not understand what I should do in this part. Could you please clearly explain what I should do? Many thanks for your help.",
        "answers": [
            [
                "You have to create them in the same way that they did. I read them first then created what they did. This works if you changing an existing Classification DataSet with the same class structure. You do have to edit the pickle file to update total number of images for both train and val in 2 places. You have to generate the lmdb files just like they have them. By the way\u2026 Of course they don\u2019t recommend this: Check out: https://github.com/NVIDIA/DIGITS/issues/1035 Here is my code: https://github.com/GemHunt/lmdb-testing/blob/master/create_lmdb_rotate_whole_image.py"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Is it still possible to run trainning in some kind of multi gpu setting if I have Peer access not supported between device ordinals?(as I understand GPUs are 'not connected') for example by calculating each batch separately on GPU and then merge on CPU as I understand this is the way 'batch accumulation' work in DIGITS with Caffe backend. Raw output: 2017-05-10 15:27:54.360688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 1 2017-05-10 15:27:54.360949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 2 2017-05-10 15:27:54.361504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 3 2017-05-10 15:27:54.361738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 0 2017-05-10 15:27:54.361892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 2 2017-05-10 15:27:54.362065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 3 2017-05-10 15:27:54.362263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 2 and 0 2017-05-10 15:27:54.362485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 2 and 1 2017-05-10 15:27:54.362693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 2 and 3 2017-05-10 15:27:54.362885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 3 and 0 2017-05-10 15:27:54.362927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 3 and 1 2017-05-10 15:27:54.362967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 3 and 2 2017-05-10 15:27:54.364638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3 2017-05-10 15:27:54.364668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0: Y N N N 2017-05-10 15:27:54.364687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1: N Y N N 2017-05-10 15:27:54.364702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2: N N Y N 2017-05-10 15:27:54.364717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3: N N N Y",
        "answers": [
            [
                "This message is benign (it is an \"INFO\" message, not an error). Everything in Tensorflow will work, but perhaps more slowly than it could on different hardware that did support peer-to-peer access. The message means the NVIDIA driver is reporting that peer-to-peer access is not possible between your GPUs. See: https://developer.nvidia.com/gpudirect for more information. You can use the command nvidia-smi topo -m to display the bus topology."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I had already installed Cuda 7.5 and now I have installed Cuda 8.0. The Nvidia card is TITAN X (Pascal). I also only activated the following compute abilitiesCUDA_ARCHfield inmakefile.config` (the rest are commented): -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 I successfully could run make all and make test for Caffe installation, however, when I tried to run make runtest after some time it is showing error: F0509 11:19:15.078367 29181 math_functions.cu:416] Check failed: status == CURAND_STATUS_SUCCESS (201 vs. 0) CURAND_STATUS_LAUNCH_FAILURE When I type ldd ./build/tools/caffe | grep cuda, I get libcudart.so.7.5 =&gt; /usr/lib/x86_64-linux-gnu/libcudart.so.7.5 (0x00007fd3ea210000) libicudata.so.55 =&gt; /usr/lib/x86_64-linux-gnu/libicudata.so.55 (0x00007fd3c9f3c000) How to remove *.so of cuda7? or what is possible solution?Thanks",
        "answers": [
            [
                "I could solve this issue by uninstalling Nvidia cuda toolkit. I followed the command here . I reinstalled cuda 8.0 and added the following paths to .bashrc: export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have already installed flask-socketio, when I try to run digits, it is showing me the following error digits/webapp.py:7: ExtDeprecationWarning: Importing flask.ext.socketio is deprecated, use flask_socketio instead. from flask.ext.socketio import SocketIO Traceback (most recent call last): File \"/home/sam/anaconda2/envs/testcaffe/lib/python2.7/runpy.py\", line 174, in _run_module_as_main \"__main__\", fname, loader, pkg_name) File \"/home/sam/anaconda2/envs/testcaffe/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals File \"/home/sam/DIGITS/digits/__main__.py\", line 70, in &lt;module&gt; main() File \"/home/sam/DIGITS/digits/__main__.py\", line 55, in main import digits.webapp File \"digits/webapp.py\", line 7, in &lt;module&gt; from flask.ext.socketio import SocketIO File \"/home/sam/anaconda2/envs/testcaffe/lib/python2.7/site-packages/flask/exthook.py\", line 110, in load_module raise ImportError('No module named %s' % fullname) ImportError: No module named flask.ext.socketio How to solve this? thanks",
        "answers": [
            [
                "You can do: sudo apt-get install python-flaskext.socketio After that error you would probably have this other error: ImportError: No module named flask.ext.wtf That you can fix by: sudo apt-get install python-flaskext.wtf Probably the best you can do is install all dependencies by using the requirements file proper from digits. pip install -r requirements.txt"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have to analyse some images of drops, taken using a microscope, which may contain some cell. What would be the best thing to do in order to do it? Every acquisition of images returns around a thousand pictures: every picture contains a drop and I have to determine whether the drop has a cell inside or not. Every acquisition dataset presents with a very different contrast and brightness, and the shape of the cells is slightly different on every setup due to micro variations on the focus of the microscope. I have tried to create a classification model following the guide \"TensorFlow for poets\", defining two classes: empty drops and drops containing a cell. Unfortunately the result wasn't successful. I have also tried to label the cells and giving to an object detection algorithm using DIGITS 5, but it does not detect anything. I was wondering if these algorithms are designed to recognise more complex object or if I have done something wrong during the setup. Any solution or hint would be helpful! Thank you! This is a collage of drops from different samples: the cells are a bit different from every acquisition, due to the different setup and ambient lights",
        "answers": [
            [
                "This kind of problem should definitely be possible. I would suggest starting with a cifar 10 convolutional neural network tutorial and customizing it for your problem. In future posts you should tell us how your training is progressing. Make sure you're outputting the following information every few steps (maybe every 10-100 steps): Loss/cost function output, you should see your loss decreasing over time. Classification accuracy on the current batch of your training data Classification accuracy on a held out test set (if you've implemented test set evaluation, you might implement this second) There are many, many, many things that can go wrong, from bad learning rates, to preprocessing steps that go awry. Neural networks are very hard to debug, they are very resilient to bugs, making it hard to even know if you have a bug in your code. For that reason make sure you're visualizing everything. Another very important step to follow is to save the images exactly as you are passing them to tensorflow. You will have them in a matrix form, you can save that matrix form as an image. Do that immediately before you pass the data to tensorflow. Make sure you are giving the network what you expect it to receive. I can't tell you how many times I and others I know have passed garbage into the network unknowingly, assume the worst and prove yourself wrong! Your next post should look something like this: I'm training a convolutional neural network in tensorflow My loss function (sigmoid cross entropy) is decreasing consistently (show us a picture!) My input images look like this (show us a picture of what you ACTUALLY FEED to the network) My learning rate and other parameters are A, B, and C I preprocessed the data by doing M and N The accuracy the network achieves on training data (and/or test data) is Y In answering those questions you're likely to solve 10 problems along the way, and we'll help you find the 11th and, with some luck, last one. :) Good luck!"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to install the text classification plugins for Nvidia Digits but am getting confused by the installation instructions I installed Digits without a problem, but on looking for the Digits root am hitting a blank when trying add the plugins via Pip. I used 'whereis' to find Digits but on using that location Pip gives me an error, saying that it looks like a folder. What I should do to 'point to the to the top of the Digits installation'? I'm on Ubuntu 16.04. From the instructions: install the top-level DIGITS package. Point the $DIGITS_ROOT envvar to the top of your DIGITS installation then do: $ pip install -e $DIGITS_ROOT Install the text classifications plug-ins: $ pip install $DIGITS_ROOT/plugins/data/textClassification $ pip install $DIGITS_ROOT/plugins/view/textClassification",
        "answers": [
            [
                "I downloaded the plugins directory from GitHub then in the pip install commands pointing to those directories (eg ~Downloads/Digits/plugins/data/textClassification)"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a dataset of 18000+ images, they are tagged as 0,1 and 2. I need to train a model on Nvidiea digits. While making a dataset, it asks to provide subdirectories per category (i.e. it demands 3 folder : one for 0, one for 1 and one for 2) but the folder I have contains mixed images (they are tagged as o, 1 and 2). Manually it is difficult to sort them. Is there nay other way with which I can proceed.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Training using DetectNET model provided by Kitti on Tesla-P40(22.4 GB) GPU. F0412 12:27:08.371733 26664 detectnet_transform_layer.cpp:212] Check failed: bboxLen == sizeof(BboxLabel) / sizeof(Dtype) (237 vs. 16) *** Check failure stack trace: *** @ 0x7fc9f8d9884d google::LogMessage::Fail() @ 0x7fc9f8d9a61c google::LogMessage::SendToLog() @ 0x7fc9f8d9843c google::LogMessage::Flush() @ 0x7fc9f8d9af2e google::LogMessageFatal::~LogMessageFatal() @ 0x7fc9f9a0758f caffe::DetectNetTransformationLayer&lt;&gt;::blobToLabels() @ 0x7fc9f9b3e393 caffe::DetectNetTransformationLayer&lt;&gt;::Forward_gpu() @ 0x7fc9f9aced98 caffe::Net&lt;&gt;::ForwardFromTo() @ 0x7fc9f9acf117 caffe::Net&lt;&gt;::Forward() @ 0x7fc9f9ac27ca caffe::Solver&lt;&gt;::Test() @ 0x7fc9f9ac304e caffe::Solver&lt;&gt;::TestAll() @ 0x7fc9f9ac561d caffe::Solver&lt;&gt;::Step() @ 0x7fc9f9ac5fde caffe::Solver&lt;&gt;::Solve() @ 0x40b59b train() @ 0x408c0c main @ 0x7fc9e7dd4b35 __libc_start_main @ 0x4094eb (unknown) Aborted (core dumped) I followed this to create dataset in lmdb. what does bboxLen == sizeof(BboxLabel) means?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have six channel image (2 RGB image concatenated). I want to train the model with these images using AlexNet. I packed the images into lmdb. Then I used the OTHER option for dataset and model, however I am getting the following error when I am creating the model . ERROR: Top blob 'data' produced by multiple sources. Creating layer train-data Creating Layer train-data Top blob 'data' produced by multiple sources. Opened lmdb /lmdb_database/train_labels I am using standard AlexNet architecture. Data Preparation in lmdb I have two images in RGB coming from two different modalities. I modified createdb.py script from siamese example to concatenate two images. Once I have the dataset then I am using the standard AlexNet Prototext to train the model",
        "answers": [
            [
                "Check your input data layers, is it possible you forgot to specify stage/phase for them? it seems like caffe is trying to use both at the same time"
            ],
            [
                "Today I have encountered this problem too. And I found a solution. My data layer's name is 'data' and then top is 'data' too, so it has a wrong. And I change my data layer, it works."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "DIGITS was working very well on my device, however, it is giving an error ERROR: Check failed: error == cudaSuccess (77 vs. 0) an illegal memory access was encountered I do not know how to tackle this.Thanks",
        "answers": [],
        "votes": []
    },
    {
        "question": "I tried to used a pre-trained model that already was trained on three-channel color images, however, I am getting an error because of shape difference. Could someone let me know how can I tackle this issue? One user had suggested using Tile layer, but I could not find any relevant document/help for using this layer or any other solution. I really appreciate your help.",
        "answers": [
            [
                "There is not much information in caffe.proto about tile layer. If you look at the code it just copies data tiles times for each outer dimension. For your case it should be: layer{ name: \"tile\" type: \"Tile\" bottom: \"bottom-blob\" top: \"top-blob\" tiling_param { // axis is 1 by default tiles: 3 } }"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am running fcn-alexnet for semantic segmentation, I downloaded the pretrained model. Since my data is single channel,it is showing an error: ERROR: Cannot copy param 0 weights from layer 'conv1'; shape mismatch. Source param shape is 96 3 11 11 (34848); target param shape is 96 1 11 11 (11616). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer. Could someone please guide me? The shape is as follows: Feature shape (1, 256, 256) Label shape (1, 256, 256)",
        "answers": [
            [
                "If you only needed to retrain the last layer, you could just rename it (see this answer: https://stackoverflow.com/a/39837047/2404152). In your case, the problem is that 96x3x11x11 != 96x1x11x11. You're trying to apply a pretrained model that was intended for color images to a grayscale dataset (as you already discovered). The simplest fix is to just train on color images as well. One way to do this would be to add a Tile layer to copy the input three times. Obviously, that wouldn't be very efficient. But that's the only way I can think of to use the pretrained model on grayscale data."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to install \"DIGITS\" in a virtual environment, I am receiving the following error: digits/webapp.py:7: ExtDeprecationWarning: Importing flask.ext.socketio is deprecated, use flask_socketio instead. from flask.ext.socketio import SocketIO /home/ss/anaconda2/envs/caffeenv/lib/python2.7/site-packages/gevent/builtins.py:93: ExtDeprecationWarning: Importing flask.ext.wtf is deprecated, use flask_wtf instead. result = _import(*args, **kwargs) Traceback (most recent call last): import skfmm File \"/home/ss/anaconda2/envs/caffeenv/lib/python2.7/site-packages/gevent/builtins.py\", line 93, in __import__ result = _import(*args, **kwargs) ImportError: No module named skfmm Edited: After installing pip install scikit-fmm. The following error appeared: digits/webapp.py:7: ExtDeprecationWarning: Importing flask.ext.socketio is deprecated, use flask_socketio instead. from flask.ext.socketio import SocketIO /home/ss/anaconda2/envs/caffeenv/lib/python2.7/site-packages/gevent/builtins.py:93: ExtDeprecationWarning: Importing flask.ext.wtf is deprecated, use flask_wtf instead. result = _import(*args, **kwargs) Traceback (most recent call last): File \"/home/ss/anaconda2/envs/caffeenv/lib/python2.7/runpy.py\", line 174, in _run_module_as_main \"__main__\", fname, loader, pkg_name) ... exc_class, code = self._get_exc_class_and_code(code_or_exception) File \"/home/ss/anaconda2/envs/caffeenv/lib/python2.7/site-packages/flask/app.py\", line 1104, in _get_exc_class_and_code exc_class = default_exceptions[exc_class_or_code] KeyError: 300",
        "answers": [
            [
                "As Ankur Jyoti Phukan said you should install the package with the command pip install scikit-fmm You may need to add the --user flag at the end depending on the permissions of your system."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I just had a weird experience while building caffe.. I executed make --jobs=4 and it ran all well till it reached 92%. Then while it was linking the CXX executables compute_image_mean, convert_image set, extract_features, and caffe it threw the error ../lib/libcaffe.so.1.0.0-rc3: undefined reference to 'cudnnConvolutionBackwardData_v3'. What worries me here is that I have successfully built caffe before, with the same installation, same paths, the same device..basically the same everything. What exactly is going on? I haven't touched cudnn and installed DIGITS yesterday. I had some installation errors because of the caffe path not being recognized (that's why I'm here and building it again), but no issue in its requirement for cudnn, which proves nothing is wrong with cudnn. Aside from advice on how to fix it, I would like to know if caffe automatically resets it's paths ovver time or something similar. Operating system: Ubuntu 14.04 LTS CUDA version: 7.5 Full output: Linking CXX shared library ../../lib/libcaffe.so [ 90%] Built target caffe Scanning dependencies of target caffe.bin [ 91%] Building CXX object tools/CMakeFiles/caffe.bin.dir/caffe.cpp.o Linking CXX executable caffe ../lib/libcaffe.so.1.0.0-rc3: undefined reference to `cudnnConvolutionBackwardData_v3' ../lib/libcaffe.so.1.0.0-rc3: undefined reference to `cudnnConvolutionBackwardFilter_v3' collect2: error: ld returned 1 exit status make[2]: *** [tools/caffe] Error 1 make[1]: *** [tools/CMakeFiles/caffe.bin.dir/all] Error 2 make: *** [all] Error 2 Thanks",
        "answers": [
            [
                "I came across the same problem on Ubuntu 16.04, CUDA 8.0. I just edited the file src/caffe/layers/cudnn_conv_layer.cu by removing the _v3 for the two functions, and the error went away. Not the most sound solution, but worth a try."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "When I run ./digits-devserver on Ubuntu 12.04 I get following error: ./digits-devserver ___ ___ ___ ___ _____ ___ | \\_ _/ __|_ _|_ _/ __| | |) | | (_ || | | | \\__ \\ |___/___\\___|___| |_| |___/ 5.1-dev Traceback (most recent call last): File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main \"__main__\", fname, loader, pkg_name) File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals File \"/home/myuser/Desktop/DIGITS/digits/__main__.py\", line 70, in &lt;module&gt; main() File \"/home/myuser/Desktop/DIGITS/digits/__main__.py\", line 55, in main import digits.webapp File \"digits/webapp.py\", line 47, in &lt;module&gt; import digits.views # noqa File \"digits/views.py\", line 18, in &lt;module&gt; from digits import dataset, extensions, model, utils, pretrained_model File \"digits/extensions/__init__.py\", line 4, in &lt;module&gt; from .data import * # noqa File \"digits/extensions/data/__init__.py\", line 7, in &lt;module&gt; from . import imageProcessing File \"digits/extensions/data/imageProcessing/__init__.py\", line 4, in &lt;module&gt; from .data import DataIngestion File \"digits/extensions/data/imageProcessing/data.py\", line 12, in &lt;module&gt; from .forms import DatasetForm File \"digits/extensions/data/imageProcessing/forms.py\", line 6, in &lt;module&gt; from flask.ext.wtf import Form File \"/usr/lib/python2.7/dist-packages/flask/exthook.py\", line 62, in load_module __import__(realname) File \"/usr/lib/python2.7/dist-packages/flaskext/wtf/__init__.py\", line 72, in &lt;module&gt; __all__ += fields.__all__ AttributeError: 'module' object has no attribute '__all__' Exception KeyError: KeyError(28050128,) in &lt;module 'threading' from '/usr/lib/python2.7/threading.pyc'&gt; ignored What can be the source of this error and how can it be fixed?",
        "answers": [
            [
                "DIGITS only officially supports Ubuntu 14.04 and 16.04: https://github.com/NVIDIA/DIGITS/blob/digits-5.0/docs/BuildDigits.md#building-digits So the first thing I'd try would be upgrading your system. Other than that, go through the list of requirements and make sure you've installed the correct versions of everything: https://github.com/NVIDIA/DIGITS/blob/digits-5.0/requirements.txt"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "After i finished Installing DIGITS When i Running DIGITS,use python digits-devserver An error showed \"Invalid syntax\" in line6.enter image description here",
        "answers": [
            [
                "digits-devserver is a bash script, not a Python script. https://github.com/NVIDIA/DIGITS/issues/1484#issuecomment-283416267"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "New with Deep learning Frame work. trying on with Nvidia Digits and encounter problems when running Digit-denserver. Installation of digits i follow the link: https://github.com/NVIDIA/DIGITS/blob/master/docs/BuildDigits.md Installation was success how ever meet a problem when running digits. error code fail to find module compt Any suggestion ? system info OS: Ubuntu 14.04 Cuda 8.0 Cudnn 5.0",
        "answers": [
            [
                "Solve the problem by installing the missing module with pip install. it quite a pain to install one by one. hope others can provide a better answer for it."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm using Torch 7 and lua 5.1. I will need to do object recognition from an input video stream. I've installed DIGITS from NVidia. I've heard there are existing models that are already pre-trained provided by Google (or someone other source). Where I can find them ?",
        "answers": [
            [
                "I would recommend you to check https://github.com/BVLC/caffe/wiki/Model-Zoo it is a common GitHub repo with a bunch of trained models. Even though it is made for caffe framework there is a library (really easy to use) in torch that lets you use them without problems: https://github.com/szagoruyko/loadcaffe"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm very new to deep learning and i'm trying to obtain a classification with lua. I've installed digits with torch and lua 5.1 and i've train the following model : After that, i've made a classification with the digits server to test the exemple and here is the result : I've exported the model and now i'm trying to do a classification with the following lua code : local image_url = '/home/delpech/mnist/test/5/04131.png' local network_url = '/home/delpech/models/snapshot_30_Model.t7' local network_name = paths.basename(network_url) print '==&gt; Loading network' local net = torch.load(network_name) --local net = torch.load(network_name):unpack():float() net:evaluate() print(net) print '==&gt; Loading synsets' print 'Loads mapping from net outputs to human readable labels' local synset_words = {} --for line in io.lines'/home/delpech/models/labels.txt' do table.insert(synset_words, line:sub(11)) end for line in io.lines'/home/delpech/models/labels.txt' do table.insert(synset_words, line) end print 'synset words' for line in io.lines'/home/delpech/models/labels.txt' do print(line) end print '==&gt; Loading image and imagenet mean' local im = image.load(image_url) print '==&gt; Preprocessing' local I = image.scale(im,28,28,'bilinear'):float() print 'Propagate through the network, sort outputs in decreasing order and show 10 best classes' local _,classes = net:forward(I):view(-1):sort(true) for i=1,10 do print('predicted class '..tostring(i)..': ', synset_words[classes[i]]) end But here is the output : delpech@delpech-K55VD:~/models$ lua classify.lua ==&gt; Downloading image and network ==&gt; Loading network nn.Sequential { [input -&gt; (1) -&gt; (2) -&gt; (3) -&gt; (4) -&gt; (5) -&gt; (6) -&gt; (7) -&gt; (8) -&gt; (9) -&gt; (10) -&gt; output] (1): nn.MulConstant (2): nn.SpatialConvolution(1 -&gt; 20, 5x5) (3): nn.SpatialMaxPooling(2x2, 2,2) (4): nn.SpatialConvolution(20 -&gt; 50, 5x5) (5): nn.SpatialMaxPooling(2x2, 2,2) (6): nn.View(-1) (7): nn.Linear(800 -&gt; 500) (8): nn.ReLU (9): nn.Linear(500 -&gt; 10) (10): nn.LogSoftMax } ==&gt; Loading synsets Loads mapping from net outputs to human readable labels synset words 0 1 2 3 4 5 6 7 8 9 ==&gt; Loading image and imagenet mean ==&gt; Preprocessing Propagate through the network, sort outputs in decreasing order and show 5 best classes predicted class 1: 4 predicted class 2: 8 predicted class 3: 0 predicted class 4: 1 predicted class 5: 9 predicted class 6: 6 predicted class 7: 7 predicted class 8: 2 predicted class 9: 5 predicted class 10: 3 And this is actually not the classification provided by digits...",
        "answers": [
            [
                "OK, after searching in the digits code source, it looked like i've missed two things : you have to get the mean image in the job folder and make the following pre-process : print '==&gt; Preprocessing' for i=1,im_mean:size(1) do im[i]:csub(im_mean[i]) end and the fact that i had to load my images in this way and multiply every pixel to 255. local im = image.load(image_url):type('torch.FloatTensor'):contiguous(); im:mul(255) Here is the total anwser : require 'image' require 'nn' require 'torch' require 'paths' local function main() print '==&gt; Downloading image and network' local image_url = '/home/delpech/mnist/test/7/03079.png' local network_url = '/home/delpech/models/snapshot_30_Model.t7' local mean_url = '/home/delpech/models/mean.jpg' print '==&gt; Loading network' local net = torch.load(network_url) net:evaluate(); print '==&gt; Loading synsets' print 'Loads mapping from net outputs to human readable labels' local synset_words = {} for line in io.lines'/home/delpech/models/labels.txt' do table.insert(synset_words, line) end print '==&gt; Loading image and imagenet mean' local im = image.load(image_url):type('torch.FloatTensor'):contiguous();--:contiguous() im:mul(255) local I = image.scale(im,28,28,'bilinear'):float() local im_mean = image.load(mean_url):type('torch.FloatTensor'):contiguous(); im_mean:mul(255) local Imean = image.scale(im,28,28,'bilinear'):float() print '==&gt; Preprocessing' for i=1,im_mean:size(1) do im[i]:csub(im_mean[i]) end local _,classes = net:forward(im):sort(true); for i=1,10 do print('predicted class '..tostring(i)..': ', synset_words[classes[i]]) end end main()"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "The Caffe from NVidia is older than BVLC/Caffe. How can I use the latest version of Caffe in NVidia DIGITS?",
        "answers": [
            [
                "Either set the CAFFE_ROOT environment variable, or install your build of Caffe to PATH and PYTHONPATH. https://github.com/NVIDIA/DIGITS/blob/digits-5.0/docs/BuildCaffe.md#download-source https://github.com/NVIDIA/DIGITS/blob/digits-5.0/docs/Configuration.md#environment-variables"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "i have a question regarding NVIDIA DIGITS Framework. So i have been using caffe without DIGITS and have used HDF5 Layers so far. There i could use multiple \"top\" (data_0, data_1, data_2) inputs (see code below). So i could feed the net with more then one input image. But in DIGITS only lmdb input layer works. So is it possible to create a lmdb input layer with multiple input images?? layer { name: \"data\" type: \"HDF5Data\" top: \"data_0\" top: \"data_1\" top: \"data_2\" top: \"label\" hdf5_data_param { source: \"train.txt\" batch_size: 64 shuffle: true } }",
        "answers": [
            [
                "Sorry, that isn't supported in DIGITS. Since DIGITS manages your datasets for you, it also sets up the data layers in your network for you. This way you don't need to copy+paste the LMDB paths into your network when you want to run a previous network on a new dataset or when you move the location of your jobs on disk. It's a decision which sacrifices flexibility for the sake of making the common case easy. For classification, one LMDB should have two tops: \"data\" and \"label\". For other dataset types, there should be one LMDB with a single \"data\" top, and another LMDB with a single \"label\" top. If you need a more complicated data layer setup, then you'll need to either use Caffe directly or make some changes to the DIGITS source code. DIGITS's HDF5 support is not great because Caffe's HDF5 support is not great."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to use pre-trained model for the face identification. I try to use Siamese architecture which requires a few number of images. Could you give me any trained model which I can change for the Siamese architecture? How can I change the network model which I can put two images to find their similarities (I do not want to create image based on the tutorial here)? I only want to use the system for real time application. Do you have any recommendations?",
        "answers": [
            [
                "I suppose you can use this model, described in Xiang Wu, Ran He, Zhenan Sun, Tieniu Tan A Light CNN for Deep Face Representation with Noisy Labels (arXiv 2015) as a a strating point for your experiments. As for the Siamese network, what you are trying to earn is a mapping from a face image into some high dimensional vector space, in which distances between points reflects (dis)similarity between faces. To do so, you only need one network that gets a face as an input and produce a high-dim vector as an output. However, to train this single network using the Siamese approach, you are going to duplicate it: creating two instances of the same net (you need to explicitly link the weights of the two copies). During training you are going to provide pairs of faces to the nets: one to each copy, then the single loss layer on top of the two copies can compare the high-dimensional vectors representing the two faces and compute a loss according to a \"same/not same\" label associated with this pair. Hence, you only need the duplication for the training. In test time ('deploy') you are going to have a single net providing you with a semantically meaningful high dimensional representation of faces. For a more advance Siamese architecture and loss see this thread. On the other hand, you might want to consider the approach described in Oren Tadmor, Yonatan Wexler, Tal Rosenwein, Shai Shalev-Shwartz, Amnon Shashua Learning a Metric Embedding for Face Recognition using the Multibatch Method (arXiv 2016). This approach is more efficient and easy to implement than pair-wise losses over image pairs."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I am using a python layer in Digits to crop each training image with random sizes crops, and then using cv2.resize method to resize to expected top dimensions. The resizing works as expected when used offline on a saved batch of images, the numpy array gets correctly resized to shape (128x3x227x227). When using the python layer in digits however i get code -11 Error. Caffe output log shows this: I1212 12:11:41.999608 14949 solver.cpp:291] Solving I1212 12:11:41.999610 14949 solver.cpp:292] Learning Rate Policy: fixed I1212 12:11:42.001058 14949 solver.cpp:349] Iteration 0, Testing net (#0) I1212 12:11:42.001065 14949 net.cpp:693] Ignoring source layer train-data *** Aborted at 1481541102 (unix time) try \"date -d @1481541102\" if you are using GNU date *** PC: @ 0x7f7c46b63acf cv::resize() *** SIGSEGV (@0x0) received by PID 14949 (TID 0x7f7c564d0ac0) from PID 0; stack trace: *** @ 0x7f7c53b734b0 (unknown) @ 0x7f7c46b63acf cv::resize() @ 0x7f67e132d736 pyopencv_cv_resize() @ 0x7f7c547a8c55 PyEval_EvalFrameEx @ 0x7f7c548d301c PyEval_EvalCodeEx @ 0x7f7c548292e0 (unknown) @ 0x7f7c547fc1e3 PyObject_Call @ 0x7f7c5487031c (unknown) @ 0x7f7c547fc1e3 PyObject_Call @ 0x7f7c548d2447 PyEval_CallObjectWithKeywords @ 0x7f7c54830f17 PyEval_CallFunction @ 0x7f67f34add85 caffe::PythonLayer&lt;&gt;::Forward_cpu() @ 0x7f7c55bc6207 caffe::Net&lt;&gt;::ForwardFromTo() @ 0x7f7c55bc6577 caffe::Net&lt;&gt;::Forward() @ 0x7f7c55be8dda caffe::Solver&lt;&gt;::Test() @ 0x7f7c55be992e caffe::Solver&lt;&gt;::TestAll() @ 0x7f7c55be9a4c caffe::Solver&lt;&gt;::Step() @ 0x7f7c55bea5e9 caffe::Solver&lt;&gt;::Solve() @ 0x40cf6f train() @ 0x4088e8 main @ 0x7f7c53b5e830 __libc_start_main @ 0x4091b9 _start @ 0x0 (unknown) Additional information: the environment in which I test the resize method offline (when it works) is the same environment I use for Digits/caffe I can use the scipy misc.imresize method instead and it works as expected, however the scipy method doesn't preserve the original numpy values, converting them back to uint8 in 0-256 range, so I can't use that method. It's also much slower than cv2 (tested on offline batches) If someone can suggest an alternative interpolation method to resize the numpy nd-array that would be very useful too Many thanks",
        "answers": [
            [
                "Solved, it was a problem with caffe installation due to cmake flags and virtualenv. I was running digits from a virtualenv where the opencv-python version was 3.1.0 When installing caffe, however, cmake checked the system opencv-python version, which was 2.4.9, and built caffe accordingly. This was the reason for the error, uncorrect python bindings. To solve it, a new virtualenv was created, with opencv 2.4.9, and digits has been started from that environment."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have installed caffe on ubuntu 16.04, python 2.7, cuda 8, and have tested mnist on that. it worked fine. Now I want to install digits. I have installed it but when I do ./digits-devserver, it gives me the following error: @pc2user:~/digits$ ./digits-devserver ___ ___ ___ ___ _____ ___ | \\_ _/ __|_ _|_ _/ __| | |) | | (_ || | | | \\__ \\ |___/___\\___|___| |_| |___/ 5.1-dev Did you forget to \"make pycaffe\"? \"/home/user/caffe-master\" from CAFFE_ROOT does not point to a valid installation of Caffe. Use the envvar CAFFE_ROOT to indicate a valid installation. Traceback (most recent call last): File \"/home/user/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main \"__main__\", fname, loader, pkg_name) File \"/home/user/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals File \"/home/user/digits/digits/__main__.py\", line 70, in &lt;module&gt; main() File \"/home/user/digits/digits/__main__.py\", line 53, in main import digits.config File \"digits/config/__init__.py\", line 7, in &lt;module&gt; from . import ( # noqa File \"digits/config/caffe.py\", line 226, in &lt;module&gt; executable, version, flavor = load_from_envvar('CAFFE_ROOT') File \"digits/config/caffe.py\", line 37, in load_from_envvar import_pycaffe(python_dir) File \"digits/config/caffe.py\", line 126, in import_pycaffe import caffe File \"/home/user/caffe-master/python/caffe/__init__.py\", line 1, in &lt;module&gt; from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver File \"/home/user/caffe-master/python/caffe/pycaffe.py\", line 13, in &lt;module&gt; from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\ ImportError: /home/user/anaconda2/lib/python2.7/site-packages/scipy/special/../../../../libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/user/caffe-master/python/caffe/_caffe.so) user@pc2user:~/digits$ Moreover, thsi is my ~/.profile: Please help me to find out how to define the root of caffe to digits.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I installed DIGITS 4 on ubuntu 14.04, everything seems running well. I used to install CUDA first in my previous machines but I didn't do it this time since I thought DIGITS should get all things set. But when I try to check the CUDA version with nvcc, it says no such a command. Looks like the CUDA toolkit was not there. Even I do found those library files exits in /usr/local/cuda-7.5 Now I'm confused and not certain what DIGITS installed. If I have to install some other stuff need to run with CUDA. Is it necessary to install the CUDA toolkit or not? Is the library good enough?",
        "answers": [
            [
                "It will install the runtime libraries, but not the compiler. You don't need nvcc to run compiled CUDA code."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "My network contains some specific layers which are not supported by current tensorRT. so I want to run the conv layers and pooling layers on tensorRT and then use the output from tensorRT as the input of my caffe model which contains some specific layers. Is there some API or example code thar I can refer to? Thanks",
        "answers": [
            [
                "See the source code in the samples directory of your TensorRT installation."
            ],
            [
                "For those stumbling now on this issue I got this to work by making the input and output of TensorRT inference the mutable_gpu_data from caffe blobs: auto* gpuImagePtr = inputBlob-&gt;mutable_gpu_data(); cudaMemcpy(gpuImagePtr, inputData, mNetInputMemory, cudaMemcpyHostToDevice); std::vector&lt;void*&gt; buffers(2); buffers[0] = gpuImagePtr; buffers[1] = outputBlob-&gt;mutable_gpu_data(); cudaContext-&gt;enqueue(batchSize, &amp;buffers[0], stream, nullptr);"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I try to use caffe fp16 on TX1, but many specific layers are not supported by the nvidia current version for fp16 caffe. I tried google the internet about how to make the official caffe support half precision, but found nothing about that.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I`ve been dealing with a problem with a neural net to classify text. I have my own datasets of news in spanish and it is labeled as: positive, neutral, negative opinion. I'm comparing with amazon polarity dataset which is only positive and negative. My model is based on the character level convolutional neural network paper (Xiang Zhang et al. 2015) The model works on the amazon database but not in mine. It firs look like underfitting, as it doesn't learn anything. Then it start learning but after a few hours of training, it starts overfitting. How should the database text be for the neural net to understand it? I'm using torch7 with NVIDIA DIGITS for training with GPU.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am new with using NVIDIA DIGITS. My train dataset has following structure and its format is .hdf5 . crops Dataset {27482, 3, 128, 192} labels Dataset {27482, 12} mean Dataset {3, 128, 192} pids Dataset {27482} I know how to feed model with simpler formats like .txt or .jpg. My question is how can i feed my model with .hdf5 format in NVIDIA DIGITS",
        "answers": [
            [
                "HDF5 datasets are only used for image classification datasets in DIGITS, and even then the support isn't very full-featured. Why? Caffe doesn't support HDF5 nearly as well as it supports LMDBs: For large datasets, you have to break them up into separate files (see here) Data is not prefetched - the whole dataset is read into memory at once (see here) Data transformations are not supported with the HDF5Data layer (see here) Since DIGITS is primarily Caffe-based for now, our main dataset format is LMDB. If/when we support more backend frameworks, we may decide to standardize on a more generic format like HDF5 or zipfiles."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I've a lot of values from all days over one entire year. I'm wanna verify if they have a kind of similarity for each month (verify if these days values correspond to the correct month and/or predict for future same months from another future year). From the https://github.com/NVIDIA/DIGITS/tree/master/examples/regression and the alternative method: manually creating LMDB files, what can I do? I've to put all these values from all days over one entire year into the val_db and train_db? After this, generate one test image for each month with all days from these months? Or I've to make a val_db and train_db for each month separately? Thank you.",
        "answers": [
            [
                "Honestly DIGITS might be a little overkill for you. You don't need much computation power to train a little logistic regression problem and the results should be pretty easy to interpret. But, if you want to forge ahead with DIGITS, you'll have to Format your data as a CxHxW data blob (maybe just 1x1xN where N is the number of features you have) Format your label as a CxHxW data blob (maybe just 1x1x1) Create a Caffe model that works for simple logistic regression (maybe just a single inner-product layer?) To get help with this, please post on our user group instead of here on SO (following the README)."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to make a Training/Validation LMDB set for use with NVIDIA Digits, but I can't find any good examples/tutorials. I understand how to create an LMDB database, but I'm uncertain on how to correctly format the data. I get how to create an image using the caffe_pb2 Datum by setting channels/width/height/data and save them. But, how do I create the Labels LMDB? Do I still use a Caffe Datum? If so, what do I set the channels/width/height to? Will it work if I have a single value label? Thanks",
        "answers": [
            [
                "DIGITS only really supports data in LMDBs for now. Each value in the LMDB key/val store must be a Caffe Datum, which limits the number of dimensions to 3. Even though Caffe Datums allow for a single numeric label (datum.label), when uploading a prebuilt LMDB to DIGITS you need to specify a separate database for the labels. That's inefficient if you only have a single numeric label (since you could have done it all in one DB), but it's more generic and scalable to other label types. Sorry, you're right that this isn't documented very well right now. Here are some source files you can browse for inspiration if you're so inclined: Data are images, labels are (gradientX, gradientY) Data are image pairs, labels are 1 or 0 Data are text snippets, labels are numeric classes Generic script for creating LMDBs from any data blobs coming from extensions/plugins"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Based on a, b, c, d, Action Recognition with Deep Learning, Long-term Recurrent Convolutional Networks, e, Generic Features for Video Analysis,... there are several methods for analyses video by caffe but what is exactly the input for caffe. Can we put video in different folders like image for training?",
        "answers": [
            [
                "DIGITS doesn't support video data yet. When we do we'll add some sort of video example here: https://github.com/NVIDIA/DIGITS/tree/master/examples"
            ],
            [
                "As far as my experience, you can't directly do it with digits, because in digits no default settings for sequences of frame analysis. A very famous project in Caffe known as C3D for action recognition can be used for training a new model or fine-tune existing moded for action or activity recognition. C3D"
            ]
        ],
        "votes": [
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "I try to use pre-trained model (VGG 16) to DIGITS for my dataset which has two classes. but I got wrongly training. (I change the name of last layers to fc8_new) I uploaded deploy.prototxt and VGG_ILSVRC_16_layers.caffemodel and synset_words.txt successfully into DIGITS and tested with my data-set which has two classes.",
        "answers": [
            [
                "A deploy.prototxt won't include any loss or accuracy layers. I'm not really sure what Caffe will do when you try to train a network without any loss layers. As I answered here, you're going to need to create an \"all-in-one\" network which includes all the layers needed for training, validation and deployment in a single network. @Shai linked to this helpful answer as well. As explained on our GitHub README, we prefer to answer these questions on our user group instead of here on SO."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I try to use pretrained model (VGG 16) to DIGITS but I got this error. ERROR: Check failed: error == cudaSuccess (2 vs. 0) out of memory and conv2_2 does not need backward computation. relu2_1 does not need backward computation. conv2_1 does not need backward computation. pool1 does not need backward computation. relu1_2 does not need backward computation. conv1_2 does not need backward computation. relu1_1 does not need backward computation. conv1_1 does not need backward computation. data does not need backward computation. This network produces output label This network produces output softmax Network initialization done. Solver scaffolding done. Finetuning from /home/digits/digits/jobs/20161020-095911-9d01/model.caffemodel Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/digits/digits/jobs/20161020-095911-9d01/model.caffemodel Successfully upgraded file specified using deprecated V1LayerParameter Attempting to upgrade input file specified using deprecated input fields: /home/digits/digits/jobs/20161020-095911-9d01/model.caffemodel Successfully upgraded file specified using deprecated input fields. Note that future Caffe releases will only support input layers and not input fields. Check failed: error == cudaSuccess (2 vs. 0) out of memory I uploaded deploy.prototxt and VGG_ILSVRC_16_layers.caffemodel and synset_words.txt successfully into DIGITS and tested with my data-set which has two classes.",
        "answers": [
            [
                "Sometimes digits-server can not clear the memory. Try with this command if you are using ubuntu: sudo restart nvidia-digits-server If this do not work and you face again again the same, you need to reduce the batch_size"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I try to use pretrained model (VGG 19) to DIGITS but I got this error. ERROR: Your deploy network is missing a Softmax layer! Read the documentation for custom networks and/or look at the standard networks for examples I try to test with my dataset which has only two classes. I read this and this try to modify last layer but also I got error. How can I modify layers based on new dataset? I try to modify the last layer and I got error ERROR: Layer 'softmax' references bottom 'fc8' at the TRAIN stage however this blob is not included at that stage. Please consider using an include directive to limit the scope of this layer.",
        "answers": [
            [
                "You're having a problem because you're trying to upload a \"train/val\" network when you really need to be uploading an \"all-in-one\" network. Unfortunately, we don't document this very well. I've created an RFE to remind us to improve the documentation. Try to adjust the last layers in your network to look something like this: https://github.com/NVIDIA/DIGITS/blob/v4.0.0/digits/standard-networks/caffe/lenet.prototxt#L162-L184 For more information, here is how I've proposed updating Caffe's example networks to all-in-one nets, and here is how I updated the default DIGITS networks to be all-in-one nets."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I want to upload pretrained model to DIGITS. If I use zip file does not accept file. What should inside the zip file? How can I upload pretrained model live VGG-19 into DIGITS and use it for different dataset? How can I fine tune the new model for new dataset which is different in image size and original images?",
        "answers": [
            [
                "Try downloading a model from another training job on your server. Look at the contents of that zipfile, and then create a new zipfile with your new content in the same format. Also, you can just upload the individual files using the form rather than creating a zipfile."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I want to use VGG_ILSVRC_19_layers as a pretrained model in digits but with different dataset. Do I need different label files? How can I upload this model and use it for my dataset? for the VGG 16 layers I got ERROR: Cannot copy param 0 weights from layer 'fc6'; shape mismatch. Source param shape is 1 1 4096 25088 (102760448); target param shape is 4096 32768 (134217728). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer. how can modify layers?",
        "answers": [
            [
                "Your labels are associated with your dataset in DIGITS - not your model. When you upload a pretrained VGG model, you'll probably need to rename your last inner product layer (see this answer) so that your model will work on N classes instead of 1000."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I can't find any module called 'sgunicorn'. This is my /var/log/digits/digits.log of my NVIDIA DIGITS 4 instance: Error: class uri 'socketio.sgunicorn.GeventSocketIOWorker' invalid or not found: [Traceback (most recent call last): File \"/usr/lib/python2.7/dist-packages/gunicorn/util.py\", line 133, in load_class mod = __import__('.'.join(components)) ImportError: No module named sgunicorn These are my pip modules: Cheetah==2.4.4 Cython==0.20.1post0 Flask==0.10.1 Flask-SocketIO==0.6.0 Flask-WTF==0.11 Jinja2==2.7.2 Landscape-Client==14.12 M2Crypto==0.21.1 MarkupSafe==0.18 MySQL-python==1.2.3 PAM==0.4.2 Pillow==2.3.0 PyYAML==3.10 Twisted-Core==13.2.0 Twisted-Names==13.2.0 Twisted-Web==13.2.0 WTForms==2.0.1 Werkzeug==0.9.4 apt-xapian-index==0.45 argparse==1.2.1 aws-cfn-bootstrap==1.4 awscli==1.10.66 beautifulsoup4==4.2.1 blinker==1.3 boto3==1.1.1 botocore==1.4.56 certifi==2015.04.28 chardet==2.0.1 circus==0.14.0 cloud-init==0.7.5 colorama==0.2.5 configobj==4.7.2 cvxopt==1.1.4 decorator==3.4.0 docutils==0.12 filelock==2.0.6 futures==2.2.0 gevent==1.0 gevent-socketio==0.3.6 gevent-websocket==0.9.3 greenlet==0.4.2 gunicorn==17.5 gyp==0.1 h5py==2.2.1 html5lib==0.999 iowait==0.2 ipython==1.2.1 itsdangerous==0.22 jmespath==0.9.0 joblib==0.7.1 jsonpatch==1.3 jsonpointer==1.0 leveldb==0.1 lmdb==0.87 lockfile==0.12.2 matplotlib==1.3.1 msgpack-python==0.4.6 networkx==1.8.1 newrelic-api==1.0.4 nose==1.3.1 numexpr==2.2.2 numpy==1.8.2 oauth==1.0.1 openpyxl==1.7.0 pandas==0.13.1 patsy==0.2.1 pexpect==3.1 prettytable==0.7.2 protobuf==2.5.0 psutil==3.4.2 pyOpenSSL==0.13 pyasn1==0.1.9 pycrypto==2.6.1 pycups==1.9.66 pycurl==7.19.3 pydot==1.0.28 pygobject==3.12.0 pyinotify==0.9.4 pyparsing==2.0.1 pyserial==2.6 pysmbc==1.0.14.1 pystache==0.5.4 python-apt==0.9.3.5ubuntu2 python-daemon==1.6.1 python-dateutil==2.5.3 python-debian==0.1.21-nmu2ubuntu2 python-engineio==1.0.3 python-gflags==1.5.1 python-socketio==1.6.0 pytz==2012c pyzmq==14.0.1 requests==2.2.1 rsa==3.4.2 s3transfer==0.1.4 salt==2016.3.1 scikit-image==0.9.3 scikit-learn==0.14.1 scipy==0.13.3 simplegeneric==0.8.1 simplejson==3.3.1 six==1.10.0 ssh-import-id==3.21 statsmodels==0.5.0 system-service==0.1.6 tables==3.1.1 tornado==4.2.1 urllib3==1.7.1 virtualenv==15.0.3 wheel==0.24.0 wsgiref==0.1.2 xlrd==0.9.2 xlwt==0.7.5 zope.interface==4.0.5 I've already tried to downgrade some modules but the problem still persists. Any suggestions? Thank you",
        "answers": [],
        "votes": []
    },
    {
        "question": "I use DIGIT to classify (I test GoogLeNet with Adaptive Gradient, Stochastic gradient descent, and Nesterov's accelerated gradient). The images are color and 256*256. After training I use \"Test a single image\" option and test one image. The result is show prefect match and classify image correctly. Then I use downloaded model for applying in OpenCV 3.1 (windows 64bit, visual studio 2013, Nvidia GPU) based on \"http://docs.opencv.org/trunk/d5/de7/tutorial_dnn_googlenet.html\". However, always I got different class and wrong answer. Edit: I try cvtColor(img, img, COLOR_BGR2RGB) and the problem not solve. Still I got wrong result. I try different data transformations like none, image, and pixel. Also different solver type.",
        "answers": [
            [
                "I would be surprised if OpenCV 3 vs 2 is causing this issue. Instead, I expect that the discrepancy is due to a difference in data pre-processing. Here's an example of how to do data pre-processing for a Caffe model that was trained in DIGITS: https://github.com/NVIDIA/DIGITS/blob/v4.0.0/examples/classification/example.py#L40-L85 Also make sure you read these \"gotchas\": https://github.com/NVIDIA/DIGITS/blob/v4.0.0/examples/classification/README.md#limitations"
            ],
            [
                "OpenCV uses by default the now very uncommon BGR (blue, green, red) ordering of the color channels. Normal is RGB. Why OpenCV Using BGR Colour Space Instead of RGB This could explain the bad performance of the model."
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "digits 4.0 0.14.0-rc.3 /Ubuntu (aws) training a 5 class GoogLenet model with about 800 training samples in each class. I was trying to use the bvlc_imagent as pre-trained model. These are the steps I took: downloaded imagenet from http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel and placed it in /home/ubuntu/models 2. a. Pasted the \"train_val.prototxt\" from here https://github.com/BVLC/caffe/blob/master/models/bvlc_reference_caffenet/train_val.prototxt into the custom network tab and b. '#' commented out the \"source\" and \"backend\" lines (since it was complaning about them) In the pre-trained models text box pasted the path to the '.caffemodel'. in my case: \"/home/ubuntu/models/bvlc_googlenet.caffemodel\" I get this error: ERROR: Cannot copy param 0 weights from layer 'loss1/classifier'; shape mismatch. Source param shape is 1 1 1000 1024 (1024000); target param shape is 6 1024 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer. I have pasted various train_val.prototext from github issues etc and no luck unfortunately, I am not sure why this is getting so complicated, in older versions of digits, we could just enter the path to the folder and it was working great for transfer learning. Could someone help?",
        "answers": [
            [
                "Rename the layer from \"loss1/classifier\" to \"loss1/classifier_retrain\". When fine-tuning a model, here's what Caffe does: # pseudo-code for layer in new_model: if layer.name in old_model: new_model.layer.weights = old_model.layer.weights You're getting an error because the weights for \"loss1/classifier\" were for a 1000-class classification problem (1000x1024), and you're trying to copy them into a layer for a 6-class classification problem (6x1024). When you rename the layer, Caffe doesn't try to copy the weights for that layer and you get randomly initialized weights - which is what you want. Also, I suggest you use this network description which is already set up as an all-in-one network description for GoogLeNet. It will save you some trouble. https://github.com/NVIDIA/DIGITS/blob/digits-4.0/digits/standard-networks/caffe/googlenet.prototxt"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have seen caffe installation for Mac. But I have a question. If my Mac does not have GPU, then I have no chances to use GPU?? and I have to use CPU-only? or I have the chance of using (virtual!) GPU by NVIDIA web driver? Moreover, can I have digits on my Mac? as I try to download it, it does not have any options for Mac download and it is just for Ubuntu! I am very confused about these questions! Can you please make me clear about these?",
        "answers": [
            [
                "The difference in architectures between CPU and GPU does not allow simple transformation of the code written for one architecture to the other. The GPU drivers are specifically written for the GPU architecture and cannot be easily virtualized. On the other hand, some software supports both. This includes OpenGL instructions and caffe (http://caffe.berkeleyvision.org/). NVidia DIGITS is based on caffe and therefore can work without a dedicated GPU (Here the thread how to install on Macs: https://github.com/NVIDIA/DIGITS/issues/88) According to https://www.github.com/NVIDIA/DIGITS/issues/251 CUDA cannot be run on computers that do not have a dedicated NVidia GPU, but according to How to run my CUDA application on ATI or Intel card in software mode? there is a program gpuocelot that receives CUDA instructions and can work on NVidia GPU, AMD GPU and x86. In scientific shared computing they wrote separate programs for different devices, e.g. Einstein at Home has four separate programs to find gravitational waves: CPU, NVidia GPU (CUDA), AMD GPU and ARM. To make DIGITS work you need to build Caffe with CPU_ONLY and tell DIGITS not to use any GPUs by running digits-devserver with the --config flag (https://github.com/NVIDIA/caffe/blob/v0.13.2/Makefile.config.example#L9-L10, https://github.com/NVIDIA/DIGITS/issues/251). Other possibility: you can still use the --config flag with the web installer. Try this: ./runme.sh --config. Choose \"N\" to select none. Also a possibility: I am trying to answer how you can choose CPU or GPUs.. Within the caffe folder, there is a Makefile.config.example file.. Copy the contents of this file into a new file and rename it as \"Makefile.config\". If you want to use CPU, then 1. comment out the \"USE_CUDNN :=1 Within \"Makefile.config\" file, 2. uncomment CPU_ONLY := 1 3. issue the make all command again within the caffe folder.. And if nothing helps you can do the procedure two times because it helped someone at the end of the thread."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am using nvidia-digits to train a CNN and get predictions. However the .../classify_one.json returns top 5 predictions by default. I need a larger n, how can I modify my request param to get the top n for example? I couldn't find anything in DIGITS documentation.",
        "answers": [
            [
                "Try modifying the accuracy_param of your top \"Accuracy\" layer of the model's prototxt file. You should have something like accuracy_param { top_k: 5 } Change the top_k: 5 to the number you wish to get, e.g., top_k: 10 for getting top 10 classes."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using digits for image classification task. I wanted to score the test db and get the predicted prob in csv file. Can anyone tell me where should i make changes in the digits files for that?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I created a GoogleNet Model via Nvidia DIGITS with two classes (called positive and negative). If I classify an image with DIGITS, it shows me a nice result like positive: 85.56% and negative: 14.44%. If it pass that model it into pycaffe's classify.py with the same image, I get a result like array([[ 0.38978559, -0.06033826]], dtype=float32) So, how do I read/interpret this result? How do I calculate the confidence levels (not sure if this is the right term) shown by DIGITS from the results shown by classify.py?",
        "answers": [
            [
                "This issue led me to the solution. As the log shows, the network produces three outputs. Classifier#classify only returns the first output. So e.g. by changing predictions = out[self.outputs[0]] to predictions = out[self.outputs[2]], I get the desired values."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am using Nvidia Digits Box with GPU (Nvidia GeForce GTX Titan X) and Tensorflow 0.6 to train the Neural Network, and everything works. However, when I check the Volatile GPU Util using nvidia-smi -l 1, I notice that it's only 6%, and I think most of the computation is on CPU, since I notice that the process which runs Tensorflow has about 90% CPU usage. The result is the training process is very slow. I wonder if there are ways to make full usage of GPU instead of CPU to speed up the training process. Thanks!",
        "answers": [
            [
                "I suspect you have a bottleneck somewhere (like in this github issue) -- you have some operation which doesn't have GPU implementation, so it's placed on CPU, and the GPU is idling because of data transfers. For instance, until recently reduce_mean was not implemented on GPU, and before that Rank was not implemented on GPU, and it was implicitly being used by many ops. At one point, I saw a network from fully_connected_preloaded.py being slow because there was a Rank op that got placed on CPU, and hence triggering the transfer of entire dataset from GPU to CPU at each step. To solve this I would first recommend upgrading to 0.8 since it had a few more ops implemented for GPU (reduce_prod for integer inputs, reduce_mean and others). Then you can create your session with log_device_placement=True and see if there are any ops placed on CPU or GPU that would cause excessive transfers per step. There are often ops in the input pipeline (such as parse_example) which don't have GPU implementations, I find it helpful sometimes to pin the whole input pipeline to CPU using with tf.device(\"/cpu:0\"): block"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "In a recent discussion, I found out that some parts of the deploy.prototxt exist only because they have been directly copied from the train_test.prototxt and are ignored during testing. For example: layer { name: \"conv1\" type: \"Convolution\" bottom: \"data\" top: \"conv1\" param { #Starting here lr_mult: 1 } param { lr_mult: 2 } #To here convolution_param { #is this section useful? num_output: 20 kernel_size: 5 stride: 1 weight_filler { type: \"xavier\" } bias_filler { type: \"constant\" } } } I was told that the section containing LR for weights as biases was useless in deploy files and could be deleted. This got me thinking, is the convolution_param portion absolutely required? If yes, do we still have to define the weight and bias fillers as we will only do testing using this file and fillers are initialized only when we need to train a network. Is there any other detail that is unnecessary?",
        "answers": [
            [
                "The convolution_param portion is required but you can remove weight_filler and bias_filler if you want."
            ],
            [
                "layer { name: \"conv1\" type: \"Convolution\" bottom: \"data\" top: \"conv1\" convolution_param { num_output: 20 kernel_size: 5 stride: 1 } } The above layer will run well during Test."
            ]
        ],
        "votes": [
            2.0000001,
            2.0000001
        ]
    },
    {
        "question": "I am new to machine learning. I've been messing around with NVIDIA Digits to train a new dataset. My dataset however is too inaccurate and I think it is because there is too much background in the image that it is getting confused as to what the actual object is. My question: Is there a way (possibly using RCNN) to crop out the background and then proceed to train using the cropped image? The object is consistent (ex only one object like a singular person but there may be people in the background) and always by itself.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to call the running instance of DIGITS from a front-end Android mobile application. DIGITS provides a REST API so the curl command would be: curl http://192.168.0.10:5000/models/images/classification/classify_one.json -XPOST -F job_id=20150604-034131-da2c -F image_file=@/path/to/image/desert_0.jpg It is important to mention that the curl tool is using the Content-Type multipart/form-data according to RFC 2388. This enables uploading of binary files etc. Many people use Android Volley for HTTP POST transfer however for multipart/form-data a custom request should be written which turns to be complex. I am looking to send parameter name/value to the server where value can be either an image or a String. I have tried so far to adapt the solutions suggested [here] and [here] but didn't figure out how to send name/value to the server. Adapting these answers or suggesting some more up-to-date libraries would be great.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Is there a simple way (e.g. without modifying caffe code) to load wights from multiple pretrained networks into one network? The network contains some layers with same dimensions and names as both pretrained networks. I am trying to achieve this using NVidia DIGITS and Caffe. EDIT: I thought it wouldn't be possible to do it directly from DIGITS, as confirmed by answers. Can anyone suggest a simple way to modify the DIGITS code to be able to select multiple pretrained networks? I checked the code a bit, and thought the training script would be a good place to start, but I don't have in-depth knowledge of Caffe, so I'm not sure what the best/quickest way to achieve this would be.",
        "answers": [
            [
                "As Shai suggested, there was no way of doing this, so I decided to clone the official repository and make the appropriate changes. I changed the code so that multiple pretrained networks can be loaded by using a colon as separator. I created a pull request on the official repository and my changes were then merged with the main branch of DIGITS, meaning it is now possible to use this functionality in DIGITS."
            ],
            [
                "AFAIK there is no straight forward way of doing so. However, you can use net surgery to load the pretrained models and manually assign their weights to the target net. Once you have a single net with all the weights initialized according to the various pretrained models, you can save it and use it as a single pretrained model for the rest of your work."
            ]
        ],
        "votes": [
            6.0000001,
            2.0000001
        ]
    },
    {
        "question": "Background: I wish to use both Caffe and Digits such that I can use Caffe within the Digits framework or external to it. However, for a particular project, I require that Caffe uses OpenCV 3 and not OpenCV 2.4, which Digits installs by default. This project uses Caffe external to Digits, and does not make any use of the Digits framework. It appears that by installing Digits, my OpenCV 3 installation was \"clobbered\" with OpenCV 2.4, which is now causing problems within my original Caffe installation. To make things clearer, below is a listing of the steps that I have taken. From a fresh Ubuntu 14.04 installation: Installed Caffe dependencies (except OpenCV) as per the Ubuntu install guide Installed OpenCV 3 from source to /usr/local Tested OpenCV installation Demos worked fine including OpenCV 3-specific code Compiled Caffe, setting the Makefile.config to use OpenCV 3 Tested Caffe installation All tests passed, demos worked fine Installed Digits as per the install guide Caffe and OpenCV 2.4 were installed by default by the installer script OpenCV 3 clobbered by OpenCV 2.4 (?) Carried out the steps from the Digits Getting Started guide All steps were successful Suspected OpenCV conflict, so tried to compile a Caffe demo Error occurred relating to OpenCV 3.0 and 2.4 conflict -- details below. Compilation command: g++ classification.cpp -o classification -I/home/josh/software/caffe/include/ -L/home/josh/software/caffe/build/lib/ -lcaffe -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand -I/home/josh/software/cudnn/include/ -L/home/josh/software/cudnn/lib64/ -lcudnn -L/usr/lib/x86_64-linux-gnu/ -lglog -L/usr/local/lib -lboost_system -lopencv_core -lopencv_highgui -lopencv_imgproc -lopencv_imgcodecs -DUSE_OPENCV Error message: /usr/bin/ld: warning: libopencv_core.so.3.0, needed by /home/josh/software/caffe/build/lib//libcaffe.so, may conflict with libopencv_core.so.2.4 /usr/bin/ld: /tmp/ccHaWcOl.o: undefined reference to symbol '_ZN2cv6String10deallocateEv' //usr/local/lib/libopencv_core.so.3.0: error adding symbols: DSO missing from command line collect2: error: ld returned 1 exit status Questions: How do I best resolve the OpenCV version conflict without breaking either Caffe/Digits installations? Do I need to remove OpenCV and reinstall Caffe and Digits? If so, what do I need to do differently to prevent the OpenCV version conflicts while still allowing Caffe (with OpenCV 3) and Digits to work side-by-side?",
        "answers": [
            [
                "When OpenCV library is installed, the shared libraries are stored like libopencv_core.so.2.4.8, where 2.4.8 is the version number of OpenCV. libopencv_core.so file usually links to the last version of OpenCV installed in the system. Thus, though libopencv_core.so was initially linked to libopencv_core.so.3.0, when you installed the older version, the link got updated to link to the older version. The workaround will be to specifically call the latest OpenCV library in caffe. To do so, you will have to modify the Makefile present in caffe. Replace all occurrences of opencv_* to :libopencv_*.so.3.0"
            ],
            [
                "if you install Caffe, using opencv3.0 is not a good ideal. Caffe is not very compatible with it. And if you persist in using it , please remove 2.4 version, using command sudo apt-get autoremove libopencv-dev. The reason for this version conflict, is you install libopencv-dev (default version is 2.4) and install opencv with 3.0. So remove it, and then compile caffe."
            ]
        ],
        "votes": [
            1.0000001,
            -0.9999999
        ]
    },
    {
        "question": "I need to classify images into one of 2000 classes. I am using the Nvidia DIGITS + caffe (GoogLeNet) and provided 10K samples per class (so a whopping 20 million images, ~1Tb data!). But the data prep (\"create db\") task itself is being estimated to be 102 days and I shudder to think what the actual training time will be if that estimate is correct. What is the best way to approach this challenge? should I break up dataset into 3-4 models? and use them separately? Use a smaller dataset and risk less accuracy? something else? Thanks for helping out a newbie.",
        "answers": [
            [
                "You got the training data!! This is the most challenging part of all the process, don't give up on it! Try breaking the task of data creation into steps, and parrallelize them. Training will take long (depending on your GPU) but you should be patient."
            ],
            [
                "I think you can use ImageDataLayer rather than LMDB or levelDB."
            ],
            [
                "First, you need to know what scenario you should use LMDB/LevelDB and what benefit do you get(especially, training in parallel.) But what things you will encounter is that creating LMDB is large and slow, and you also need to use SSD instead of HDD to decrease time-consuming problem. Second, when you only need to train CNN with small datasets (e.g &lt; 2M images), you can just use ImageDataLayer same as @kli_nlpr said. Finally, just like @Shai said, the most challenge of all process always is preparing data, and check training whether the outcome is what you expected, if it is not you should check data, preparing data again or check training configuration. If you tried all possible solutions, however you still feel slow, and you can tried to change your hardware as GPU cluster."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am trying to build a model in DIGITS. I am using CPU only to do the learning.. However, I get a CUDA driver version error although I am not using GPU. What could this problem be? I have attached my solver.prototxt below.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Closed. This question needs debugging details. It is not currently accepting answers. Edit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question. Closed 7 years ago. Improve this question Not sure if this is an issue or not - but I have been searching for days and cannot seem to figure it out! Any image I try to classify individually using digits seems to run okay. However, when using the \"classify many images\" button, the network crashes because of the aforementioned title/bug/I don't even know what the hell it is. I'm entirely new to caffe and DIGITS, and as I said I've spent days googling this problem - and cant seem to figure it out. What is the 5th dimension on the image, and if I do actually have 5D images, how do I convert them to 4D?",
        "answers": [
            [
                "Turns out it was a bug - Posted on git and they flagged it officially and patched - to anyone else experiencing this problem, update your DIGITS files"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I trained a neural network model on Digits and it seemed to run fine there. Then i exported the trained model files and copied them into a different system running the standard caffe web demo. I hoped to just be able to plug those files in and have them run in Caffe but i am getting an error. Specifically I copied my model into bvlc_reference_caffenet.caffemodel, the deploy.prototxt into deploy.prototxt, and the mean.binaryproto into the ilsvrc_2012_mean.npy file. However when I try to run it , it appears to not like the format of the mean.binaryproto file as indicated by the error message: IOError: Failed to interpret file '/home/vagrant/caffe/python/caffe/imagenet/ilsvrc_2012_mean.npy' as a pickle what am I doing wrong here? Do I need to process the mean.binaryproto file from Digits somehow before I use it with caffe?",
        "answers": [
            [
                "You need to convert the .binaryproto file to a numpy file. There is a nice example here using caffe.io and caffe.proto."
            ]
        ],
        "votes": [
            2.0000001
        ]
    }
]