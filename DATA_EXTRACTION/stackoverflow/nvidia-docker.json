[
    {
        "question": "This is a common problem but I still cannot resolve the issue. I'm trying to run a docker app called FakeFinder and I'm using the command sudo docker-compose up -d --build as directed by the setup instructions to run it, but run into the above error when I do. I'm using the Deep learning AMI (ubuntu 18.04) Version 52.0 which comes with nvidia-docker and all the drivers set up. Running sudo docker info | grep -i runtime doesn't show nvidia under the runtimes regardless of what I try. My /etc/docker/daemon.json is reasonable and looks as follows: { \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } } } Changing path to \"usr/bin/nvidia-container-runtime\" does not help. I've tried to restart dockerd with sudo pkill -SIGHUP dockerd I've tried restarting daemon with sudo systemctl daemon-reload sudo systemctl restart docker I've tried all three options seen here to add the runtime: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/user-guide.html",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to replicate this on my local machine. https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam#quickstart. I followed the steps and reached an error after running the following script. cd ${ISAAC_ROS_WS}/src/isaac_ros_common &amp;&amp; \\ ./scripts/run_dev.sh ${ISAAC_ROS_WS} Running isaac_ros_dev-x86_64-container docker: Error response from daemon: unknown or invalid runtime name: nvidia. When I execute the script, it builds the container fine but is unable to run it. I have tried all the suggestions that are posted elsewhere including sudo systemctl daemon-reload sudo systemctl restart docker sudo pkill -SIGHUP dockerd I am unsure where the issue is. I am running Ubuntu 22.04 using WSL2 and everything is up to date. I have nvidia-container-runtime and nvidia-docker2 installed. My daemon.json file is located under Linux/Ubuntu/etc/docker and this is what it contains. { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"args\": [], \"path\": \"nvidia-container-runtime\" } } } If this is useful, this is the information I receive when I run nvidia-smi. +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.54.06 Driver Version: 536.40 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA GeForce RTX 2060 ... On | 00000000:09:00.0 On | N/A | | 0% 49C P8 24W / 175W | 824MiB / 8192MiB | 6% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | 0 N/A N/A 22 G /Xwayland N/A | | 0 N/A N/A 29 G /Xwayland N/A | | 0 N/A N/A 31 G /Xwayland N/A | +---------------------------------------------------------------------------------------+ When I run docker info|grep -i runtime it returns WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support Runtimes: runc io.containerd.runc.v2 Default Runtime: runc instead of the expected Runtimes: nvidia runc Default Runtime: runc Thank you in advance.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a Jetson Nano 4GB (B01) SOC board and I am running out of space to install the DeepStream SDK. I want to use DeepStream for video streaming application on my Jetson Nano SOC board, but I need guidance on how to install it . What are the steps to install DeepStream SDK on the Jetson Nano?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have built a GPU device plugin for kubernetes. GPU devices are getting allocated by the plugin, but the GPU drivers are not getting detected inside the container. As much I know, I need to mount several directories in order for the container to detect Nvidia drivers. I am using the nvidia/cuda:12.0.0-devel-ubuntu22.04 docker image due to which cuda is being detected, but for nvidia drivers I am not sure what all directories needs to be mounted by the device plugin. I have tried mounting /usr/local/nvidia, but it gives me CreateContainerError. Any suggestions ?",
        "answers": [
            [
                "You should install nvidia-container-toolkit in all nodes. distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sudo tee /etc/apt/sources.list.d/libnvidia-container.list sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit Next step edit /etc/docker/daemon.json { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } } Install plugin kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta4/nvidia-device-plugin.yml You can read more https://github.com/NVIDIA/k8s-device-plugin"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have installed jenkins using the following documentaion to run with Dind : https://www.jenkins.io/doc/book/installing/docker/ on a machine with a GPU Both the containers here are running with --gpus all parameter. My requirement in Jenkins is that I need to handle gpu workloads in the pipelines and jobs and hence I wanted to try out a simple example just so as to check if the agent as docker image gets access to the gpu. Here's the pipeline : pipeline { agent none options { skipStagesAfterUnstable() } stages { stage('Build') { agent { docker { image 'nvidia/cuda:11.6.2-base-ubuntu20.04' } } steps { sh 'nvidia-smi' } } } } when I run this pipeline I receive this error in the pipeline logs : [Pipeline] { [Pipeline] sh + nvidia-smi /var/jenkins_home/workspace/demo-bit@tmp/durable-95bc9af6/script.sh: 1: nvidia-smi: not found it probably is because, we need to provide the gpu parameter for the agent, but how do I provide it within the pipeline?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am pretty new to using docker and docker with nvidia. I have a docker image built called 'pixel2mesh_service' which builds from the latest pytorch image running cuda-11.7. I am attempting to run the docker container using my built image docker run --rm --name pixel2mesh_service -it --gpus=all pixel2mesh_service, though without success. I run into this error: nvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown Others have posted about this issue though there doesn't seem to be a unique solution. Can the libnvidia-ml.so.1 library be manually installed in the Dockerfile? Are there required nvidia or cuda dependencies that need to be included in the Dockerfile in order to run the container? Dockerfile: FROM pytorch/pytorch:latest WORKDIR /usr/app/ RUN apt-get update &amp;&amp; apt-get install -y git libgl1 libegl1 libgomp1 python3-pip wget COPY requirements.txt . RUN pip install -r requirements.txt",
        "answers": [],
        "votes": []
    },
    {
        "question": "I made a fresh install of docker desktop from official docs docker desktop. Then I installed NVIDIA Container Toolkit following official docs nvidia container toolkit. When I run docker with non-root permission: docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi It gave the following error: docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy' nvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown. The error was not happened if I use sudo permission. Also, it is fine if I run a non-gpu container (without --gpu all option). Some information about my PC: Ubuntu 22.04 Nvida driver: 525, CUDA version: 12.0 Docker desktop: 23.0.5 I have tried several suggestions: Add user to docker group, and video group Install nvidia-docker2 Try changing ldconfig path: change ldconfig Change GPU permissions: sudo chmod 666 /dev/nvidia* sudo chmod 666 /dev/nvidia-uvm* Set permission for libnvidia-ml.so.1 sudo chown root:video /usr/local/nvidia/lib64/libnvidia-ml.so.1 sudo chmod 664 /usr/local/nvidia/lib64/libnvidia-ml.so.1 And some other methods which I will continue listing when I remember. I am really desperate now. Does anyone have any suggestions? Any help will be highly appreciated.",
        "answers": [
            [
                "I'm using Ubuntu/Linux and Docker Desktop. Have the same situation. Workaround is to use a docker default context docker context use default in your terminal."
            ],
            [
                "After trying countless methods, I came across this post: docker desktop problem. As pointed out in the post, this is an unsolved problem of Docker Desktop. So I switched to Docker Engine only, and I could use GPU normally without any difficulties. While waiting for the fix of Docker Desktop, I think Docker Engine is more than enough for me."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am trying to build a docker image that has dcn_v2 installed and built for CUDA support. I have installed nvidia-drivers (450), nvidia-cuda-runtime, nvidia-docker, nvidia-cuda-toolkit on the machine. my dockerfile starts FROM pytorch/pytorch:1.7.0-cuda11.0-cudnn8-devel And at some point after installing other requirements has RUN git clone -b pytorch_1.7 https://github.com/ifzhang/DCNv2.git WORKDIR DCNv2 RUN python3 setup.py build develop I have followed the instructions found here and made nvidia-runtime default for my docker, and restarted the system. my /etc/docker/daemon.json looks like this { \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } }, \"default-runtime\": \"nvidia\" } I also tried following the suggestion here And build with DOCKER_BUILDKIT=0 docker build &lt;blah&gt; However once i reach the installation I get an error RuntimeError: Found no NVIDIA driver on your system.",
        "answers": [
            [
                "I could not manage to make drivers available during the build. However if someone finds themselves in this situation: Create a base image with just your \"FROM\" statement Run your image with --gpus all Build whatever you needed gpus and drivers for inside the container. From a different terminal do docker ps Get the id of the running container Do docker commit container_id Tag Use the newly created image as base and save that image somewhere Looks super hacky, but hey - it works."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "This question already has answers here: Running docker desktop containers with --gpus tag hangs without any response in wsl (2 answers) Closed 4 months ago. I am trying to run a docker container on Windows with GPU support. The container is stuck at run with no output/logs. It is not even responding to kill or rm. Hardware OS: Windows x86 amd64 GPU: RTX 3060 Ampere architecture Following https://docs.nvidia.com/cuda/wsl-user-guide/index.html CUDA Version: 12.1 CUDA Toolkit in WSL2: https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=WSL-Ubuntu&amp;target_version=2.0&amp;target_type=deb_local Docker latest version with WSL2 backend docker run output",
        "answers": [
            [
                "Solved with Docker Desktop 4.17.0"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a Nvidia A30 linux instance having a single 24 GB GPU, and I am planning to host 3 similar APIs on the same instance. These APIs are containers from the same docker image. I have exposed these 3 containers to access the GPU using Nvidia Container Toolkit, and as expected I am able to get the desired outputs from these container APIs. The problem lies here: When only one container is up, and it receives a request, the GPU performs at its maximum capacity. But if I up the remaining 2 containers and they too start receiving simultaneous requests, the GPU performance literally gets divided by 3. I have tried to set --shm-size and --memory in docker run command to different settings but to no avail. Can someone help out with this?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I used to use docker desktop with wsl2 integration and there was no problem running containers with gpu support. However, after a recent update to docker desktop v4.17.1 ( march 2023 ), any containers that I run specifically using the --gpus all tag on wsl hangs forever without any response. The same containers run without any issue unless if specified with the --gpus tag. running cuda container with nvidia-smi on wsl hangs without any response Note: nvidia-smi works fine in wsl. System: windows 11. Tried fresh installing docker desktop. Tried fresh installation of all wsl distros. wsl distros have access to the gpu and nvidia cuda drivers. able to use the docker desktop within wsl without any issues unless running any container using the --gpus tag hangs without any errors or a response.",
        "answers": [
            [
                "April 2023 Update Docker Desktop 4.18.0 has been released which resolves this issue. Original answer This seems to be a known issue with Docker Desktop 4.17.1. Older versions can be found here: https://docs.docker.com/desktop/release-notes/"
            ],
            [
                "Docker containers with --gpus tag runs fine on wsl without any issues after downgrading docker desktop version to v4.17.0 ( Feb 2023 update ) from v4.17.1 ( march 2023 update )."
            ]
        ],
        "votes": [
            11.0000001,
            4.0000001
        ]
    },
    {
        "question": "I'm trying to run a docker container created from the image nvidia/cuda:12.0.1-cudnn8-runtime-ubuntu22.04, using Ubuntu 22.04 under WSL 2 version 1.1.3.0 in Windows 11 and Docker Desktop 4.17.1. Running lsb_release -a confirms the version of Ubuntu: user@desktop:~$ lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 22.04.1 LTS Release: 22.04 Codename: jammy In Docker Desktop, the option \"Use the WSL 2 based engine\" is checked in Settings -&gt; General, as is \"Enable integration with my default WSL distro\" in Settings -&gt; Resources -&gt; WSL integration. On the same page, \"Enable integration with additional distros:\" is switched on for Ubuntu-22.04. Running nvidia-smi from an Ubuntu terminal produces user@desktop:~$ nvidia-smi Tue Mar 21 22:43:15 2023 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 525.89.02 Driver Version: 528.49 CUDA Version: 12.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA RTX A200... On | 00000000:F3:00.0 Off | N/A | | N/A 61C P8 4W / 17W | 40MiB / 4096MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 32 G /Xwayland N/A | | 0 N/A N/A 34 G /Xwayland N/A | | 0 N/A N/A 13020 G /Xwayland N/A | +-----------------------------------------------------------------------------+ For what's worth, running nvidia-smi.exe from a PowerShell terminal produces a similar but not identical result; the version of NVIDIA-SMI shows as 528.49 in Windows instead of 525.89.02 as seen above in Ubuntu. Running the container without --gpus produces the expected result right away, i.e., a working container without GPU functionality: user@desktop:~$ docker run -it nvidia/cuda:12.0.1-cudnn8-runtime-ubuntu22.04 ========== == CUDA == ========== CUDA Version 12.0.1 Container image Copyright (c) 2016-2022, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available. Use the NVIDIA Container Toolkit to start this container with GPU support; see https://docs.nvidia.com/datacenter/cloud-native/ . root@80a1fd519f3a:/# Multiple attempts to run the container with --gpus 0, --gpus 1, or --gpus all produced no output within one hour, after which I closed the terminal window - CTRL+C did not stop execution. The outcomes above were observed also with Ubuntu 20.04 and variants of the CUDA image, such as nvidia/cuda:11.6.0-deve-ubuntu20.04 and nvidia/cuda:12.1.0-ubuntu22.04. I also tried breaking down the running of the container into separate create and start steps; the issues described still occur at the start step. I have benefited from answers to this question, in particular the last answer, from August 2, 2020. Related questions such as pytorch cannot detect gpu in nvidia/cuda:11.3.1-cudnn8-runtime-ubuntu20.04 base image refer to issues after the container starts, but I do not get to that point.",
        "answers": [
            [
                "It seems version 4.17.1 of docker-desktop is broken. CUDA containers worked fine in &lt;=4.17.0 for me but, after upgrading to 4.17.1, the container start-up process just hangs."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to build a docker image that can use GPU for DeepLearning models. The problem is that I am getting this issue when trying to build the image RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx process \"python serializer.py config.yml\" did not complete successfully: exit code: 1 This is the content of my Dockerfile FROM nvidia/cuda:11.3.0-base-ubuntu18.04 ENV FORCE_CUDA=\"0\" RUN apt-get update &amp;&amp; apt-get install -y RUN pip install torch==1.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html WORKDIR /home/server COPY . /home/server/ RUN [\"python\",\"serializer.py\",\"config.yml\"] Outside the docker builder, all works fine. It seems that when doing the docker builder the GPUs on the system are not beeing recognized. Any help will be useful, thank you! My main goal is creating a pickle version of a DeepLearning model, but I need to have access to a GPU when doing the docker builder in order to perform that.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Hi I am trying to run rastervision pipeline on a GPU NVIDIA GEOFORCE 3050 RTX. Ubuntu 22.04 Pytorch: Version: 1.12.0+cu116 CUDA: 12 But when I run the Docker container like that: sudo docker run --rm --runtime=nvidia --gpus all -it -v ${RV_QUICKSTART_CODE_DIR}:/opt/src/code -v ${RV_QUICKSTART_OUT_DIR}:/opt/data/output quay.io/azavea/raster-vision:pytorch-0.20 /bin/bash The model does not train and outputs this error: RuntimeError: CUDA error: no kernel image is available for execution on the device CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. PD: running nvidia-smi outputs the characteristics of the GPU, meaning it is recognized. I would very much appreciate some help in this issue. Thanks! This is the output I get: `Skipping 'analyze' command... python -m rastervision.pipeline.cli run_command /opt/data/output/pipeline-config.json train Running train command... 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Building datasets ... 2023-03-09 08:53:29:rastervision.core.data.raster_source.rasterio_source: WARNING - Raster block size (2, 650) is too non-square. This can slow down reading. Consider re-tiling using GDAL. 2023-03-09 08:53:29:rastervision.core.data.raster_source.rasterio_source: WARNING - Raster block size (2, 650) is too non-square. This can slow down reading. Consider re-tiling using GDAL. 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Physical CPUs: 12 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Logical CPUs: 16 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Total memory: 15.30 GB 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Size of /opt/data volume: 445.44 GB 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Size of / volume: 445.44 GB 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Python version: 3.9.16 (main, Jan 11 2023, 16:05:54) [GCC 11.2.0] /bin/sh: 1: nvcc: not found 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Thu Mar 9 08:53:29 2023 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 525.89.02 Driver Version: 525.89.02 CUDA Version: 12.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:01:00.0 Off | N/A | | N/A 37C P3 14W / 30W | 262MiB / 4096MiB | 7% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Devices: 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB] 0, NVIDIA GeForce RTX 3050 Ti Laptop GPU, 525.89.02, 4096 MiB, 262 MiB, 3639 MiB 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - PyTorch version: 1.12.1+cu102 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - CUDA available: True 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - CUDA version: 10.2 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - CUDNN version: 7605 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Number of CUDA devices: 1 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Active CUDA Device: GPU 0 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - model=SemanticSegmentationModelConfig(backbone=&lt;Backbone.resnet50: 'resnet50'&gt;, pretrained=True, init_weights=None, load_strict=True, external_def=None) solver=SolverConfig(lr=0.0001, num_epochs=1, test_num_epochs=2, test_batch_sz=4, overfit_num_steps=1, sync_interval=1, batch_sz=2, one_cycle=True, multi_stage=[], class_loss_weights=None, ignore_class_index=None, external_loss_def=None) data=SemanticSegmentationGeoDataConfig(scene_dataset='&lt;1 train_scenes, 1 validation_scenes, 0 test_scenes&gt;', window_opts=\"method=&lt;GeoDataWindowMethod.random: 'random'&gt; size=300 stride=None padding=None pad_direction='end' size_lims=(300, 301) h_lims=None w_lims=None max_windows=10 max_sample_attempts=100 efficient_aoi_sampling=True\") predict_mode=False test_mode=False overfit_mode=False eval_train=False save_model_bundle=True log_tensorboard=True run_tensorboard=False output_uri='/opt/data/output/train' 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Using device: cuda 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - train_ds: 10 items 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - valid_ds: 10 items 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - test_ds: 0 items 2023-03-09 08:53:29:rastervision.pytorch_learner.learner: INFO - Plotting sample training batch. 2023-03-09 08:53:30:rastervision.pytorch_learner.learner: INFO - Plotting sample validation batch. 2023-03-09 08:53:31:rastervision.pytorch_learner.learner: INFO - epoch: 0 Training: 0%| | 0/5 [00:00&lt;?, ?it/s] Traceback (most recent call last): File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main return _run_code(code, main_globals, None, File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code exec(code, run_globals) File \"/opt/src/rastervision_pipeline/rastervision/pipeline/cli.py\", line 251, in &lt;module&gt; _main() File \"/opt/src/rastervision_pipeline/rastervision/pipeline/cli.py\", line 247, in _main main() File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__ return self.main(*args, **kwargs) File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1055, in main rv = self.invoke(ctx) File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1657, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke return ctx.invoke(self.callback, **ctx.params) File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in invoke return __callback(*args, **kwargs) File \"/opt/src/rastervision_pipeline/rastervision/pipeline/cli.py\", line 236, in run_command _run_command( File \"/opt/src/rastervision_pipeline/rastervision/pipeline/cli.py\", line 218, in _run_command command_fn() File \"/opt/src/rastervision_core/rastervision/core/rv_pipeline/rv_pipeline.py\", line 154, in train backend.train(source_bundle_uri=self.config.source_bundle_uri) File \"/opt/src/rastervision_pytorch_backend/rastervision/pytorch_backend/pytorch_learner_backend.py\", line 120, in train learner.main() File \"/opt/src/rastervision_pytorch_learner/rastervision/pytorch_learner/learner.py\", line 267, in main self.train() File \"/opt/src/rastervision_pytorch_learner/rastervision/pytorch_learner/learner.py\", line 1265, in train train_metrics = self.train_epoch( File \"/opt/src/rastervision_pytorch_learner/rastervision/pytorch_learner/learner.py\", line 1188, in train_epoch output = self.train_step(batch, batch_ind) File \"/opt/src/rastervision_pytorch_learner/rastervision/pytorch_learner/semantic_segmentation_learner.py\", line 26, in train_step out = self.post_forward(self.model(x)) File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl return forward_call(*input, **kwargs) File \"/opt/conda/lib/python3.9/site-packages/torchvision/models/segmentation/_utils.py\", line 23, in forward features = self.backbone(x) File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl return forward_call(*input, **kwargs) File \"/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py\", line 69, in forward x = module(x) File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl return forward_call(*input, **kwargs) File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\", line 148, in forward self.num_batches_tracked.add_(1) # type: ignore[has-type] RuntimeError: CUDA error: no kernel image is available for execution on the device CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. make: *** [/opt/data/output/Makefile:6: 0] Error 1`",
        "answers": [
            [
                "This error arises when CUDA code was not compiled to target your GPU architecture. Here, the version of PyTorch the Rastervision Docker image is using does not include CUDA code compiled for sm_86 (Ampere GeForce). As a workaround, you can force the reinstallation of a version of PyTorch that contains code for sm_86. Once you start your container using docker run, run the following command: pip install --force-reinstall torch==1.12.1+cu113 --extra-index-url https://download.pytorch.org/whl/"
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I'm building a DeepStream Docker image for NVIDIA GPUs as mentioned in this link. I have the NVIDIA Container Toolkit installed and The original Dockerfile works and after building it I can start a container with GPU support using this command: sudo docker run --runtime=nvidia --gpus all --name Test -it deepstream:dgpu The problem is that I want to install PyTorch during the docker build sequence and use it. As soon as PyTorch is imported in the build sequence, the Found no NVIDIA driver on your system error arises: #0 0.895 Traceback (most recent call last): #0 0.895 File \"./X.py\", line 15, in &lt;module&gt; #0 0.895 dummy_input = torch.randn([1, 3, 224, 224], device='cuda') #0 0.895 File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\", line 229, in _lazy_init #0 0.895 torch._C._cuda_init() #0 0.895 RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx I have the proper driver for the docker: NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 And I can properly use PyTorch After docker build is done and I've started the container with GPU support. So it seems that the docker build process does not take the NVIDIA driver or GPUs into account and I can only use the GPUs AFTER the build has been completed. It also seems that there is no --runtime=nvidia --gpus all flags to pass to the docker build command. How can I fix this problem so that I can use PyTorch &amp; CUDA during the build process? UPDATE: The problem seems to be because of the BuildKit version as discussed here, here and here. But I haven't still found the exact way to properly fix it (instead of setting DOCKER_BUILDKIT=0).",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm a newbie in the NVIDIA GPU domain. I'm trying to use NVIDIA's time-slicing feature with Kubernetes using gpu-operator. As suggested here, I followed the instructions and created a configmap with replicas as 2. Here is the configmap definition: apiVersion: v1 kind: ConfigMap metadata: name: time-slicing-config namespace: gpu-operator data: rtx-3070: |- version: v1 sharing: timeSlicing: resources: - name: nvidia.com/gpu replicas: 2 Following are the commands that I executed: kubectl create -f time-slicing-config.yaml helm install --wait gpu-operator -n gpu-operator --create-namespace nvidia/gpu-operator --set mig.strategy=none --set migManager.enabled=false --set devicePlugin.config.name=time-slicing-config Post this, the following PODs get stuck in Init:CrashLoopBackoff : gpu-feature-discovery-8klpg nvidia-device-plugin-daemonset-4mcwc nvidia-operator-validator-86j6v Logs for the container config-manager-init inside POD gpu-feature-discovery-8klpg shows: W0302 17:58:24.821579 139 client_config.go:608] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. time=\"2023-03-02T17:58:24Z\" level=info msg=\"Waiting for change to 'nvidia.com/device-plugin.config' label\" time=\"2023-03-02T17:58:24Z\" level=info msg=\"Label change detected: nvidia.com/device-plugin.config=nodename-1677691729\" time=\"2023-03-02T17:58:24Z\" level=info msg=\"Error: specified config nodename-1677691729 does not exist\" I'm not sure why it looks for the config nodename-1677691729 which potentially seems to be the cause of helm installation failing. I would really appreciate it if anyone could tell me if I'm doing something wrong or something is missing. Expected After helm install command, all the PODs are expected to be in Running state. I should be able to deploy two application PODs sharing the GPU.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to run a Docker container that requires access to my host NVIDIA GPU, using the --gpus all flag to enable GPU access. When I run the container with the nvidia-smi command, I can see an active GPU, indicating that the container has access to the GPU. However, when I simply try to run TensorFlow, PyTorch, or ONNX Runtime inside the container, these libraries do not seem to be able to detect or use the GPU. Specifically, when I run the container with the following command, I see only the CPUExecutionProvider, but not the CUDAExecutionProvider in ONNX Runtime: sudo docker run --gpus all mycontainer:latest However, when I run the same container with the nvidia-smi command, I get the active GPU prompt: sudo docker run --gpus all mycontainer:latest nvidia-smi This is the active GPU prompt: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.29.05 Driver Version: 495.29.05 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:01:00.0 Off | N/A | | N/A 44C P0 27W / N/A | 10MiB / 7982MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ And this is the Dockerfile, I built mycontainer with: FROM nvidia/cuda:11.5.0-base-ubuntu20.04 WORKDIR /home COPY requirements.txt /home/requirements.txt # Add the deadsnakes PPA for Python 3.10 RUN apt-get update &amp;&amp; \\ apt-get install -y software-properties-common libgl1-mesa-glx cmake protobuf-compiler &amp;&amp; \\ add-apt-repository ppa:deadsnakes/ppa &amp;&amp; \\ apt-get update # Install Python 3.10 and dev packages RUN apt-get update &amp;&amp; \\ apt-get install -y python3.10 python3.10-dev python3-pip &amp;&amp; \\ rm -rf /var/lib/apt/lists/* # Install virtualenv RUN pip3 install virtualenv # Create a virtual environment with Python 3.10 RUN virtualenv -p python3.10 venv # Activate the virtual environment ENV PATH=\"/home/venv/bin:$PATH\" # Install Python dependencies RUN pip3 install --upgrade pip \\ &amp;&amp; pip3 install --default-timeout=10000000 torch torchvision --extra-index-url https://download.pytorch.org/whl/cu116 \\ &amp;&amp; pip3 install --default-timeout=10000000 -r requirements.txt # Copy files COPY /src /home/src # Set the PYTHONPATH and LD_LIBRARY_PATH environment variable to include the CUDA libraries ENV PYTHONPATH=/usr/local/cuda-11.5/lib64 ENV LD_LIBRARY_PATH=/usr/local/cuda-11.5/lib64 # Set the CUDA_PATH and CUDA_HOME environment variable to point to the CUDA installation directory ENV CUDA_PATH=/usr/local/cuda-11.5 ENV CUDA_HOME=/usr/local/cuda-11.5 # Set the default command CMD [\"sh\", \"-c\", \". /home/venv/bin/activate &amp;&amp; python main.py $@\"] I have checked that the version of TensorFlow, PyTorch, and ONNX Runtime that I am using is compatible with the version of CUDA installed on my system. I have also made sure to set the LD_LIBRARY_PATH environment variable correctly to include the path to the CUDA libraries. Finally, I have made sure to include the --gpus all flag when starting the container, and to properly configure the NVIDIA Docker runtime and device plugin. Despite these steps, I am still unable to access the GPU inside the container when using TensorFlow, PyTorch, or ONNX Runtime. What could be causing this issue, and how can I resolve it? Please let me know, if you need further information.",
        "answers": [
            [
                "You should install onnxruntime-gpu to get CUDAExecutionProvider. docker run --gpus all -it nvcr.io/nvidia/pytorch:22.12-py3 bash pip install onnxruntime-gpu python3 -c \"import onnxruntime as rt; print(rt.get_device())\" GPU"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to install the nvidia docker support on my ubuntu 22.04. I ran the following commands: git clone https://github.com/NVIDIA/nvidia-docker.git cd nvidia-docker make the above commands have executed fully. But I'm getting an error when I run the following command: COMMAND: sudo make install ERROR: make: *** No rule to make target 'install'. Stop. Do I really have to run 'sudo make install' command to successfully install the nvidia docker?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm new to Stackoverflow and the NVIDIA runtime, and I'm trying to run a Docker container with the NVIDIA runtime using Docker Compose. However, I'm getting an error that I don't get when running the container directly with docker run. Here's the relevant section of my docker-compose.yml file: services: nvidia-test: image: nvidia/cuda:11.5.2-base-ubuntu20.04 command: nvidia-smi deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] When I run docker-compose up, I get the following error: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy' nvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown However, when I run the container directly with the docker run command, as follows, I don't get this/any error: sudo docker run --rm --runtime=nvidia --gpus all nvidia/cuda:11.5.2-base-ubuntu20.04 nvidia-smi I'm not sure what could be causing this error. Can someone help me understand the issue and how to resolve it so that I can run the container with the NVIDIA runtime using Docker Compose? Currently I am using docker-compose version v2.16.0, and I installed NVIDIA-Container-Toolkit following this link. Here are the NVIDIA Driver and CUDA version installed on my machine: GPU Specs Please let me know if you need additional information from me to better understand the issue. I already sudo systemctl status nvidia-persistenced to check the Persistence Daemon. But it is active (running).",
        "answers": [
            [
                "Adding sudo in front of the docker-compose up solved the problem. I assume that elevated privileges are required to allow Docker to properly access the necessary NVIDIA tools and libraries. Same as for the sudo docker run .... Also, note that the --runtime=nvidia in sudo docker run ... is not needed anymore for newer nvidia-container-toolkit versions."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "When i run my project in my system it was running fine, but when i made it as nvidia-docker2 container and run it i am getting the following error : I ensured my pytorch version, cuda version are almost same in both the environments, whereas python version differs , 3.10 in my system, 3.8 in the docker container Docker container Details : \u279c Face-Recognition-From-Crowd \u26a1 3 hours ago (\ue725 master)\u25b6 sudo docker run --gpus all --device /dev/video0 --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -it face-crowd bash ============= == PyTorch == ============= NVIDIA Release 22.11 (build 48503342) PyTorch Version 1.13.0a0+936e930 Container image Copyright (c) 2022, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. Copyright (c) 2014-2022 Facebook Inc. Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright (c) 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright (c) 2011-2013 NYU (Clement Farabet) Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright (c) 2006 Idiap Research Institute (Samy Bengio) Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) Copyright (c) 2015 Google Inc. Copyright (c) 2015 Yangqing Jia Copyright (c) 2013-2016 The Caffe contributors All rights reserved. Various files include modifications (c) NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license root@a08973389041:/app# python3 run.py --source live Traceback (most recent call last): File \"/usr/local/lib/python3.8/dist-packages/flask/app.py\", line 2548, in __call__ return self.wsgi_app(environ, start_response) File \"/usr/local/lib/python3.8/dist-packages/flask/app.py\", line 2528, in wsgi_app response = self.handle_exception(e) File \"/usr/local/lib/python3.8/dist-packages/flask/app.py\", line 2525, in wsgi_app response = self.full_dispatch_request() File \"/usr/local/lib/python3.8/dist-packages/flask/app.py\", line 1822, in full_dispatch_request rv = self.handle_user_exception(e) File \"/usr/local/lib/python3.8/dist-packages/flask/app.py\", line 1820, in full_dispatch_request rv = self.dispatch_request() File \"/usr/local/lib/python3.8/dist-packages/flask/app.py\", line 1796, in dispatch_request return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args) File \"/app/app.py\", line 57, in video_feed video = Process(os.path.abspath('./temp'), File \"/app/process.py\", line 45, in __init__ self.recognizer = Predictor(file=False, label=True) File \"/app/scripts/FaceRecognition.py\", line 31, in __init__ self.model = SingleShotLearningFR(pretrained=True) File \"/app/scripts/FRMethods/SingleShotLearningFR.py\", line 29, in __init__ super(SingleShotLearningFR, self).__init__() File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py\", line 124, in __init__ self._register_sharded_tensor_state_dict_hooks_if_available() File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/module.py\", line 2022, in _register_sharded_tensor_state_dict_hooks_if_available self.__class__._register_load_state_dict_pre_hook( File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1467, in _register_load_state_dict_pre_hook self._load_state_dict_pre_hooks[handle.id] = _WrappedHook(hook, self if with_module else None) File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 54, in __init__ self.module: weakref.ReferenceType[\"Module\"] = weakref.ref(module) TypeError: cannot create weak reference to 'weakcallableproxy' object Note : Ask me the details u need, i will add it as edit. Also mention the possibility of mistakes that causes this error",
        "answers": [
            [
                "I had similar problem, getting the same error message. The workaround which worked for me is to downgrade pytorch-lightning pip package from 1.9.0 to 1.8.6. Bug is caused by using pytorch package that was compiled from a specific commit, which breaks version comparison code (1.13.0a0+d0d6b1f &gt;= 1.13 does not hold in my case) as described in PL issue. The issue points out another (preferred) workaround - to use stable version of pytorch or wait till fix is released."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm not a Docker expert and I'm trying to create a container for machine learning projects. This is mostly for academic purpose, as I'm studying machine learning. I wrote a dockerfile (and a devcontainer.json file to open the container in vscode) that runs fine until I add the lines to build tensorflow. I found three problems but I don't know what I'm missing: I got a warning about the fact that Bazel installation isn't a release version When the building phase arrives at ./configure I can't configure through prompt question I got an error at the very building phase with ERROR: The project you're trying to build requires Bazel 5.3.0 (specified in /docklearning/tensorflow/.bazelversion), but it wasn't found in /usr/bin. This is the Dockerfile: FROM nvidia/cuda:12.0.0-runtime-ubuntu22.04 as base ARG USER_UID=1000 #switch to non-interactive frontend ENV DEBIAN_FRONTEND=noninteractive WORKDIR /docklearning ADD . /docklearning # Install packages RUN apt-get update -q &amp;&amp; apt-get install -q -y --no-install-recommends \\ apt-transport-https curl gnupg apt-utils wget gcc g++ npm unzip build-essential ca-certificates curl git gh \\ make nano iproute2 nano openssh-client openssl procps \\ software-properties-common bzip2 subversion neofetch \\ fontconfig &amp;&amp; \\ curl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor &gt;bazel-archive-keyring.gpg &amp;&amp; \\ mv bazel-archive-keyring.gpg /usr/share/keyrings &amp;&amp; \\ echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/bazel-archive-keyring.gpg] https://storage.googleapis.com/bazel-apt stable jdk1.8\" \\ | tee /etc/apt/sources.list.d/bazel.list &amp;&amp; \\ apt update &amp;&amp; apt install -q -y bazel &amp;&amp; \\ apt-get full-upgrade -q -y &amp;&amp; \\ cd ~ &amp;&amp; \\ wget https://github.com/ryanoasis/nerd-fonts/releases/download/v2.1.0/Meslo.zip &amp;&amp; \\ mkdir -p .local/share/fonts &amp;&amp; \\ unzip Meslo.zip -d .local/share/fonts &amp;&amp; \\ cd .local/share/fonts &amp;&amp; rm *Windows* &amp;&amp; \\ cd ~ &amp;&amp; \\ rm Meslo.zip &amp;&amp; \\ fc-cache -fv &amp;&amp; \\ apt-get install -y zsh zsh-doc chroma # Install anaconda RUN wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh -O Anaconda.sh &amp;&amp; \\ /bin/bash Anaconda.sh -b -p /opt/conda &amp;&amp; \\ rm Anaconda.sh # Install LSD for ls substitute and clean up RUN wget https://github.com/Peltoche/lsd/releases/download/0.23.1/lsd_0.23.1_amd64.deb -P /tmp &amp;&amp; \\ dpkg -i /tmp/lsd_0.23.1_amd64.deb &amp;&amp; \\ rm /tmp/lsd_0.23.1_amd64.deb &amp;&amp; \\ apt-get autoremove -y &amp;&amp; \\ apt-get autoclean &amp;&amp; \\ rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*/apt/lists/* &amp;&amp; \\ useradd -r -m -s /bin/bash -u ${USER_UID} docklearning # Add conda to PATH ENV PATH=/opt/conda/bin:$PATH ENV HOME=/home/docklearning # Make zsh default shell RUN chsh -s /usr/bin/zsh docklearning # link conda to /etc/profile.d RUN ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh # Update Conda Base env RUN conda update -n base --all -y &amp;&amp; \\ conda install -c conda-forge bandit opt_einsum keras-preprocessing &amp;&amp; \\ conda clean -a -q -y # Download Tensorflow from github and build from source RUN git clone https://github.com/tensorflow/tensorflow.git &amp;&amp; \\ cd tensorflow &amp;&amp; \\ ./configure &amp;&amp; \\ bazel build --config=opt --config=cuda --cxxopt=\"-mavx2\" //tensorflow/tools/pip_package:build_pip_package &amp;&amp; \\ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg &amp;&amp; \\ pip install /tmp/tensorflow_pkg/tensorflow-*.whl &amp;&amp; \\ cd .. &amp;&amp; \\ rm -rf tensorflow # Create shell history file RUN mkdir ${HOME}/zsh_history &amp;&amp; \\ chown docklearning ${HOME}/zsh_history &amp;&amp; \\ mkdir ${HOME}/.ssh # Switch to internal user USER docklearning WORKDIR ${HOME} # Copy user configuration files COPY --chown=docklearning ./config/.aliases.sh ./ COPY --chown=docklearning ./config/.bashrc ./ COPY --chown=docklearning ./config/.nanorc ./ # Configure Zsh for internal user ENV ZSH=${HOME}/.oh-my-zsh ENV ZSH_CUSTOM=${ZSH}/custom ENV ZSH_PLUGINS=${ZSH_CUSTOM}/plugins ENV ZSH_THEMES=${ZSH_CUSTOM}/themes RUN wget -qO- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh | zsh || true RUN git clone --single-branch --branch 'master' --depth 1 https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_PLUGINS}/zsh-syntax-highlighting \\ &amp;&amp; git clone --single-branch --branch 'master' --depth 1 https://github.com/zsh-users/zsh-autosuggestions ${ZSH_PLUGINS}/zsh-autosuggestions \\ &amp;&amp; git clone --single-branch --depth 1 https://github.com/romkatv/powerlevel10k.git ${ZSH_THEMES}/powerlevel10k COPY --chown=docklearning ./config/.p10k.zsh ./ COPY --chown=docklearning ./config/.zshrc ./ CMD [ \"/bin/zsh\" ]",
        "answers": [
            [
                "I wouldn't say this question about docker or TensorFlow. It is about package installation and OS configuration. In this case, you need to fix the bazel version first by specifying it explicitly: apt install -q -y bazel-5.3.0 But that package is quite wired and doesn't create a symlink, so you need to create it: ln -s /bin/bazel-5.3.0 /bin/bazel You can validate if bazel installed by running it: `bazel --version bazel 5.3.0`"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Description Trying to deploy the triton docker image as container on kubernetes cluster Triton Information What version of Triton are you using? -&gt; 22.10 Are you using the Triton container or did you build it yourself? I used the server repo with following command: python3 compose.py --backend onnxruntime --backend python --backend tensorflow2 --repoagent checksum --container-version 22.10 then again created new triton image with following dockerfile: FROM tritonserver:latest RUN apt install python3-pip -y RUN pip install tensorflow==2.7.0 RUN pip install transformers==2.11.0 RUN pip install tritonclient RUN pip install tritonclient[all] and dockerfile is being with following command: docker build -t customtritonimage -f ./DockerFiles/DockerFile . To Reproduce directory structure: parent directory -&gt; tritonnludeployment files in it -&gt; DockerFiles (folder containing docker files), k8_trial.yaml, model_repo_triton (all the models here in triton-supported directory shape and has required files) I am using this 'k8_trial.yaml' file for starting kubectl deployment apiVersion: apps/v1 kind: Deployment metadata: name: flower labels: app: flower spec: replicas: 3 selector: matchLabels: app: flower template: metadata: labels: app: flower spec: volumes: - name: models hostPath: # server: 216.48.183.17 path: /root/Documents/tritonnludeployment # readOnly: false type: Directory containers: - name: flower ports: - containerPort: 8000 name: http-triton - containerPort: 8001 name: grpc-triton - containerPort: 8002 name: metrics-triton image: \"customtritonimage:latest\" imagePullPolicy: Never volumeMounts: - mountPath: /root/Documents/tritonnludeployment name: models command: [\"/bin/sh\", \"-c\"] args: [\"cd /models /opt/tritonserver/bin/tritonserver --model-repository=/models/model_repo_triton --allow-gpu-metrics=false --strict-model-config=false\"] # resources: # requests: # memory: \"500Mi\" # cpu: \"500Mi\" # limits: # memory: \"900Mi\" # cpu: \"900Mi\" # nvidia.com/gpu: 1 Describe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well). Expected behavior kubectl deployment should start, with triton container as one of the pods Which step i am doing wrong!",
        "answers": [
            [
                "And what is the error message you are getting? Some of the issues I noticed: use the expected file name know to docker, i.e. Dockerfile not DockerFile make sure base image exists (tritonserver:latest does not, you probably want one of these) first update the sources (RUN apt install ... -&gt; RUN apt update &amp;&amp; apt install ...) reduce layers number by installing multiple python packages at once tritonclient[all] already includes tritonclient don't run containers as root (tritonserver does not require it anyway) make sure you pull the image first time (imagePullPolicy: Never -&gt; IfNotPresent) remove multiple and unnecessary commands from args (such as cd /models) tritonserver can import all subfolders, so --model-repository=/models is probably better"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I get the latest version from the https://github.com/Skycoder42/QHotkey I try to build with the command: cmake -B build -S . -DQT_DEFAULT_MAJOR_VERSION=6 I get those errors during the compilation CMake Error at CMakeLists.txt:16 (find_package): Found package configuration file: /opt/qt6/lib/cmake/Qt6/Qt6Config.cmake but it set Qt6_FOUND to FALSE so package \"Qt6\" is considered to be NOT FOUND. Reason given by package: Failed to find Qt component \"Core\". Expected Config file at \"/opt/qt6/lib/cmake/Qt6Core/Qt6CoreConfig.cmake\" exists Failed to find Qt component \"Gui\". Expected Config file at \"/opt/qt6/lib/cmake/Qt6Gui/Qt6GuiConfig.cmake\" exists I have QT installed on my machine. It is taken from this location: wget https://download.qt.io/official_releases/qt/6.3/6.3.1/single/qt-everywhere-src-6.3.1.tar.xz QHotKey is part of a larger project, I am trying to build it in the nvidia docker I am building.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to build an image in my docker image. It will contain nvidia/gadugl and ROS melodic. Here: FROM nvidia/cudagl:11.4.2-base-ubuntu18.04 # Minimal setup RUN apt-get update \\ &amp;&amp; apt-get install -y locales lsb-release ARG DEBIAN_FRONTEND=noninteractive RUN dpkg-reconfigure locales # Install ROS melodic RUN sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" &gt; /etc/apt/sources.list.d/ros-latest.list' RUN apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 RUN apt-get update \\ &amp;&amp; apt-get install -y --no-install-recommends ros-melodic-desktop-full RUN apt-get install -y --no-install-recommends python3-rosdep RUN rosdep init \\ &amp;&amp; rosdep fix-permissions \\ &amp;&amp; rosdep update RUN echo \"source /opt/ros/melodic/setup.bash\" &gt;&gt; ~/.bashrc I don't know how long it's suppsoed to take and how much GB, but this doesn't seem correct to me: Sending build context to Docker daemon 94.48GB Maybe I'm wrong?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I followed the instructions to install the nvidia-docker2 from the official documentation https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html Whenever I run their test example: sudo docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi I still get the error: docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]]. 3 I rebooted but still no effect. I am on Ubuntu 22.04 with my nvidia drivers updated. Nvidia-smi works on the machine but not working using docker EDIT (SOLVED): Finally I found out what was going on. When reinstalling, it was working, however if rebooting, it was going again to a previous state where it was not working. This was due to the installation of another docker service installed using \"snapd\" so I had to purge completely docker: sudo snap remove docker and after I could \"Reinstall everything\" and it finally is stable, even after rebooting",
        "answers": [
            [
                "Unfortunately I was not able to \"Fix\" properly the issue so I purge all docker package and all nvidia container packages and reinstalled everything and now it works!! Good old methods work fine :)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "My goal is to be able to run Vulkan application in a docker container using the Nvidia Container Toolkit. Ideally running Ubuntu 22.04 on the host and in the container. I've created a git repo to allow others to better reproduce this issue: https://github.com/rickyjames35/vulkan_docker_test The README explains my findings but I will reiterate them here. For this test I'm running Ubuntu 22.04 on my host as well as in the container FROM ubuntu:22.04. For this test I'm seeing that the only device vulkaninfo is finding is llvmpipe which is a CPU based graphics driver. I'm also seeing that llvmpipe can't render when running vkcube both in the container and on the host for Ubuntu 22.04. Here is the container output for vkcube: Selected GPU 0: llvmpipe (LLVM 13.0.1, 256 bits), type: 4 Could not find both graphics and present queues On my host I can tell it to use llvmpipe: vkcube --gpu_number 1 Selected GPU 1: llvmpipe (LLVM 13.0.1, 256 bits), type: Cpu Could not find both graphics and present queues As you can see they have the same error. What's interesting is if I swap the container to FROM ubuntu:20.04 then llvmpipe can render but this is moot since I do not wish to do CPU rendering. The main issue here is that Vulkan is unable to detect my Nvidia GPU from within the container when using the Nvidia Container Toolkit with NVIDIA_DRIVER_CAPABILITIES=all and NVIDIA_VISIBLE_DEVICES=all. I've also tried using nvidia/vulkan. When running vulkaninfo in this container I get: vulkaninfo ERROR: [Loader Message] Code 0 : vkCreateInstance: Found no drivers! Cannot create Vulkan instance. This problem is often caused by a faulty installation of the Vulkan driver or attempting to use a GPU that does not support Vulkan. ERROR at /vulkan-sdk/1.3.236.0/source/Vulkan-Tools/vulkaninfo/vulkaninfo.h:674:vkCreateInstance failed with ERROR_INCOMPATIBLE_DRIVER I'm suspecting this has to to with me running Ubuntu 22.04 on the host although the whole point of docker is the host OS generally should not affect the container. In the test above I was using nvidia-driver-525 I've tried using different versions of the driver with the same results. At this point I'm not sure if I'm doing something wrong or if Vulkan is not supported in the Nvidia Container Toolkit for Ubuntu 22.04 even though it claims to be.",
        "answers": [
            [
                "The vulkan drivers on 22.04 were at version 1.1 if you use ubuntu:23_04 you get vulkan 1.3 drivers and I was able to get this to work with the following FROM ubuntu:23.04 ARG DEBIAN_FRONTEND=noninteractive ENV NVIDIA_DRIVER_CAPABILITIES compute,graphics,utility # Install Vulkan RUN apt-get update \\ &amp;&amp; apt-get install -y \\ libxext6 \\ libvulkan1 \\ libvulkan-dev \\ vulkan-tools COPY nvidia_icd.json /etc/vulkan/icd.d You'll also need this nvidia_icd.json { \"file_format_version\" : \"1.0.0\", \"ICD\": { \"library_path\": \"libGLX_nvidia.so.0\", \"api_version\" : \"1.3\" } } Then inside you new container you can run vulkaninfo --summary and see your nvidia card."
            ],
            [
                "I had a similar problem when trying to set up a docker container using the nvidia/cuda:12.0.0-devel-ubuntu22.04 image. I was able to get it to work using the unityci/editor image. This is the docker command I used. docker run -dit -e DISPLAY=$DISPLAY -v /tmp/.X11-unix/:/tmp/.X11-unix/ -v /dev:/dev --gpus='all,\"capabilities=compute,utility,graphics,display\"' unityci/editor:ubuntu-2022.2.1f1-base-1.0.1 After setting up the container, I had to apt install vulkan-utils and libnvidia-gl-525 then everything works. Hope this helps!"
            ],
            [
                "I got it working all the way without needing to copy over nvidia_icd.json. Ian Purton your answer was very helpful in tracking down the answer. It led me here https://github.com/NVIDIA/nvidia-container-toolkit/issues/16 which shows that what Ian Purton posted was required until v1.12.0 of the NVIDIA Container Toolkit which fixes this problem. I've updated my repo with the fix. https://github.com/rickyjames35/vulkan_docker_test What I have in my repo is the most bare bones way you can run Vulkan in a docker container using docker compose."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I'm generating tvm and istalling and using it in another docker container TVM wheel generation code: # install cmake RUN yum install -y wget RUN wget https://github.com/Kitware/CMake/releases/download/v3.20.2/cmake-3.20.2.tar.gz -O cmake.tar.gz &amp;&amp; \\ mkdir cmake &amp;&amp; tar -zxvf cmake.tar.gz -C cmake --strip-components 1 &amp;&amp; \\ cd cmake &amp;&amp; ./bootstrap -- -DCMAKE_USE_OPENSSL=OFF &amp;&amp; make -j8 &amp;&amp; make install &amp;&amp; cd .. &amp;&amp; \\ rm cmake.tar.gz &amp;&amp; rm -rf cmake RUN echo \"CUDA $(ls /lib64/ | grep cu)\" &gt; /tmp/build_2.output # install tvm RUN git clone --recursive https://github.com/apache/incubator-tvm tvm &amp;&amp; \\ cd tvm &amp;&amp; \\ git reset --hard 338940dc5044885412f9a6045cb8dcdf9fb639a4 &amp;&amp; \\ git submodule init &amp;&amp; \\ git submodule update &amp;&amp; \\ mkdir ./build &amp;&amp; \\ cp cmake/config.cmake build &amp;&amp; \\ cd build &amp;&amp; \\ cmake --debug-output -DUSE_CUDA=ON -DUSE_CUDNN=ON -DUSE_CUBLAS=ON -DUSE_THRUST=ON -DUSE_LLVM=ON .. &amp;&gt;&gt; /tmp/build_2.output &amp;&amp; \\ make -j$(nproc) &amp;&gt;&gt; /tmp/build_2.output &amp;&amp; \\ cd ../python &amp;&amp; python3.8 setup.py bdist_wheel Verified that cuda .so files were created in /tmp/build_2.output While installing and running the tvm in another place getting below error: tvm.runtime.load_module *TVMError: Binary was created using cuda but a loader of that name is not registered. Available loaders are GraphRuntimeFactory, metadata, GraphExecutorFactory, VMExecutable. Perhaps you need to recompile with this runtime enabled.* How to register cuda loader?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to use a CUDA container in Docker as a non root user, but am running into permission problems. Here's an example Dockerfile: FROM nvidia/cudagl:11.2.2-runtime-ubuntu18.04 RUN useradd -ms /bin/bash testuser -G video,sudo USER testuser ENTRYPOINT \"/bin/bash\" Running nvidia-smi gives the following error: Failed to initialize NVML: Insufficient Permissions My application uses VirtualGL and Xvfb to render Chrome with a GPU if that's relevant. Works perfectly fine with the root user.",
        "answers": [
            [
                "TL;DR - Check the gid of vglusers group on the host. Add this group with the gid in the container, and add the user to this group. So investigating this a bit, I looked at the nvidia devices in the container: root@56cef279b83f:/# cd /dev root@56cef279b83f:/dev# ls -l | grep nvidia crw-rw---- 1 root 1005 195, 0 Nov 23 23:13 nvidia0 crw-rw---- 1 root 1005 195, 255 Nov 23 23:13 nvidiactl crw-rw---- 1 root 1005 195, 254 Nov 23 23:13 nvidia-modeset crw-rw-rw- 1 root root 506, 0 Nov 23 23:13 nvidia-uvm crw-rw-rw- 1 root root 506, 1 Nov 23 23:13 nvidia-uvm-tools The nvidia devices belonged to a group with gid 1005. This was odd as there was no group in the container with that ID. I went to look into the devices on the host, and as per my VGL setup, they belong to root, or the vglusers group. (venv) jsim@goliath:/var/log$ cd /dev/ (venv) jsim@goliath:/dev$ ls -l | grep nvidia crw-rw---- 1 root vglusers 195, 0 Nov 24 10:13 nvidia0 drwxr-xr-x 2 root root 80 Nov 24 10:31 nvidia-caps crw-rw---- 1 root vglusers 195, 255 Nov 24 10:13 nvidiactl crw-rw---- 1 root vglusers 195, 254 Nov 24 10:13 nvidia-modeset crw-rw-rw- 1 root root 506, 0 Nov 24 10:13 nvidia-uvm crw-rw-rw- 1 root root 506, 1 Nov 24 10:13 nvidia-uvm-tools As it turns out, vglusers has a gid of 1005! jsim@goliath:/dev$ cat /etc/group | grep vglusers vglusers:x:1005:jsim So in my Dockerfile, all I had to do is add the group vglusers with gid 1005, and add my user to this group. Problem solved. RUN groupadd -g 1005 vglusers &amp;&amp; \\ useradd -ms /bin/bash testuser -u 1000 -g 1005 &amp;&amp; \\ usermod -a -G video,sudo testuser"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm installing packages in several envs which depends on cudatoolkits in docker. My docker image is pull from nvidia/cuda which already have cuda libs, but conda doesn't know, then conda will install cudatoolkits again. I wonder how to let conda know that I have cuda libs in /usr/local/cuda. My solution is to install all packages with --no-deps, but it is so complicated, I have to fetch all dependencies, then install them except cudatoolkit ONE BY ONE. I wonder how to cheat conda, let conda know cudatoolkit has already installed.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Our current Docker definition of production environment starts like this ARG CUDA=\"10.0\" ARG CUDNN=\"7\" FROM nvidia/cuda:${CUDA}-cudnn${CUDNN}-devel-ubuntu18.04 I need to update the environment to use CUDA 11.7 and I tried to update the definition above to have ARG CUDA=\"11.7\" ARG CUDNN=\"8\" But I get the error: manifest for nvidia/cuda:11.7-cudnn8-devel-ubuntu18.04 not found: manifest unknown: manifest unknown Any suggestions on how to build a Docker image with CUDA 11.7?",
        "answers": [
            [
                "FROM nvidia/cuda:11.7.1-cudnn8-devel-ubuntu20.04"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I've installed nvidia-container-runtime on my machine (Ubuntu 22.04), and can access the GPU through docker run. docker run -it --rm --gpus all selenium/node-chrome:3.141.59 nvidia-smi Mon Oct 24 00:32:32 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 520.61.05 Driver Version: 520.61.05 CUDA Version: 11.8 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:0A:00.0 Off | N/A | | 0% 41C P8 44W / 370W | 68MiB / 10240MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ However, when running with the following docker-compose.yml, nvidia-smi can't be found. Applications inside the container don't seem to be using the GPU either. version: \"3.8\" services: nvidia: image: selenium/node-chrome:3.141.59 runtime: nvidia deploy: resources: reservations: devices: - capabilities: [gpu] command: [\"nvidia-smi\"] Running docker-compose up [+] Running 1/0 \u283f Container docker-compose-gpu-nvidia-1 Recreated 0.0s Attaching to docker-compose-gpu-nvidia-1 Error response from daemon: failed to create shim: OCI runtime create failed: runc create failed: unable to start container process: exec: \"nvidia-smi\": executable file not found in $PATH: unknown If I swap the selenium image to nvidia/cuda, docker-compose can see the GPU. Why is the GPU accessible in docker run but not docker-compose?",
        "answers": [
            [
                "Specifying the driver &amp; count fixed this. version: \"3.8\" services: nvidia: image: selenium/node-chrome:3.141.59 runtime: nvidia deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] command: [\"nvidia-smi\"] I'm not sure why this worked - the docs seem to indicate that omitting these will just use all available GPUs."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm running nvidia/cuda:11.8.0-base-ubuntu20.04 on Google Kubernetes Engine using GPU Timesharing on T4 gpus Checking the driver capabilities I get compute and utility. I was hoping to also get graphics and video. Is this a limitation of Timesharing on GKE?",
        "answers": [
            [
                "It should let you use the resources for graphics and video, however time-sharing GPU are ideal for workloads that are not using a high amount of the resources all the time. Limitations for using Time-Sharing GPU's on GKE's GKE enforces memory (address space) isolation, performance isolation, and fault isolation between containers that share a physical GPU. However, memory limits aren't enforced on time-shared GPUs. To avoid running into out-of-memory (OOM) issues, set GPU memory limits in your applications. To avoid security issues, only deploy workloads that are in the same trust boundary to time-shared GPUs. GKE might reject certain time-shared GPU requests to prevent unexpected behavior during capacity allocation The maximum number of containers that can share a single physical GPU is 48. When planning your time-sharing configuration, consider the resource needs of your workloads and the capacity of the underlying physical GPUs to optimize your performance and responsiveness."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Outside of the container, my user (UID=1000) is able to use nvidia-smi. However, inside the docker container, the non-root user (same UID of 1000) is unable to use nvidia-smi, running into Failed to initialize NVML: Insufficient Permissions. However, sudo nvidia-smi inside the container is able to use nvidia-smi. As far as I can search, this issue hasn't occurred before, any clues where to start chasing it down? Inside the container: On my host:",
        "answers": [],
        "votes": []
    },
    {
        "question": "I need to cuda develop a python program through docker but it seems that docker does not recognize nvidia drivers. The dockerfile builds and runs normally but when i run the python program numba.cuda.is_available() returns false even though numba.cuda.gpus returns my gpu. I have installed nvidia-docker2 and nvidia images run with nvidia-smi have the same output with nvidia-smi locally | NVIDIA-SMI 515.65.01 Driver Version: 515.65.01 CUDA Version: 11.7 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:29:00.0 On | N/A | | 0% 38C P8 9W / 125W | 358MiB / 6144MiB | 1% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | Dockerfile: FROM pure/python:3.6-cuda10.2-base # Set the working directory to /attack/toy-rnn/dash WORKDIR /app # Copy the current directory contents into the container at /app COPY . /app #RUN nvcc --version COPY requirements.txt /app/requirements.txt ENV PATH=\"/usr/local/cuda-10.2/bin:$PATH\" ENV LD_LIBRARY_PATH=\"/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\" ENV NVIDIA_VISIBLE_DEVICES=all ENV NVIDIA_DRIVER_CAPABILITIES=all RUN rm /etc/apt/sources.list.d/cuda.list RUN rm /etc/apt/sources.list.d/nvidia-ml.list RUN apt-get update \\ &amp;&amp; apt-get install -y --no-install-recommends graphviz less \\ &amp;&amp; rm -rf /var/lib/apt/lists/* \\ &amp;&amp; pip install --no-cache-dir pyparsing pydot RUN pip install --trusted-host pypi.python.org -r /app/requirements.txt RUN apt-get update &amp;&amp; apt-get install -y lsof RUN pip install -e iirs RUN pip install -e simple-websocket-server-master RUN pip install conda #COPY /NVIDIA-Linux/libnvidia-nvvm.so.515.65.01 /usr/local/lib/python3.6/site-packages/numba/cuda/cudadrv #RUN conda install cudatoolkit RUN pip install numba ENV PATH=\"/usr/local/cuda-10.2/bin:$PATH\" ENV LD_LIBRARY_PATH=\"/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\" ENV NVIDIA_VISIBLE_DEVICES=all ENV NVIDIA_DRIVER_CAPABILITIES=all RUN numba -s ENV CYBER_PORT \"5000\" ENV CYBER_API_CALL \"/ag-engine-server/rest/json/v2/attack-graph/remediations/block-nodes\" ENV CYBER_GET_ACTIONS \"/remediations\" ENV TMP_GRAPH_FILE \"tmp/attack_graph_received.json\" ENV IIRS_PORT \"17891\" ENV IDS_IP \"0.0.0.0\" ENV IDS_PORT \"36010\" ENV IDS_API_CALL \"/test\" ENV IGNORE_ALERT_TIMESTAMPS \"True\" ENV IGNORE_TMS_ALERTS \"True\" ENV ANTI_CONGESTION \"True\" # In seconds, float. ENV ALERT_CONGESTION_THRESHOLD \"0.5\" # This will ignore \"flow\" alerts with the field \"alerted\" set to false. ENV IGNORE_ALERTED_FALSE \"True\" ENV COMPROMISED_THRESHOLD \"0.5\" ENV TMS_THRESHOLD \"0.4\" # Whether to enable the POMCP model. ENV ENABLE_GT_MODEL \"True\" # Do we consider all not blocked exploits to be available? # \"Strict\" or \"Typical\" ENV GENERATOR_POLICY \"Strict\" ENV GENERATOR_OVERRIDE_BUS_MSG \"False\" # In milliseconds ENV UI_ACTION_PENDING_TIMEOUT \"60000\" # - POMCP Settings ------------------------------------------------------------ # Originally set to 1500 ENV NO_PARTICLES \"2\" ENV EMPTY_OBSERVATION_UPDATES \"True\" ENV FORCE_GENERAL_RULES_ONLY \"False\" # 0.14 sec * number of loops per particle (timeout) = ~2 min # Originally set to 80 ENV POSTERIOR_SAMPLE_TIMEOUT \"80\" # MAX_EMPTY_ACTION_ROUNDS will be considered only if ENABLE_RESTARTS is True ENV ENABLE_RESTARTS \"True\" ENV MAX_EMPTY_ACTION_ROUNDS \"3\" # - LOCAL DEBUG --------------------------------------------------------------- ENV CYBER_IP \"172.17.0.1\" ENV BUS_PREFIX \"5002.\" ENV BUS_USER \"\" ENV BUS_PASSWORD \"\" ENV BUS_IP \"172.17.0.1\" ENV BUS_PORT \"61613\" WORKDIR /app/iirs EXPOSE 17891 CMD [\"python\", \"/app/iirs/api.py\"] I run dockerfile with this command -&gt; sudo docker run -d -e NVIDIA_DRIVER_CAPABILITIES=compute,utility -e NVIDIA_VISIBLE_DEVICES=all --name $CONTAINER_NAME --network host $CONTAINER_NAME Any help would be greatly appreciated",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have been running a nvidia docker image since 13 days and it used to restart without any problems using docker start -i &lt;containerid&gt; command. But, today while I was downloading pytorch inside the container, download got stuck at 5% and gave no response for a while. I couldn't exit the container either by ctrl+d or ctrl+c. So, I exited the terminal and in new terminal I ran this docker start -i &lt;containerid&gt; again. But ever since this particular container is not responding to any command. Be it start/restart/exec/commit ...nothing! any command with this container ID or name is just non-responsive and had to exit out of it only after ctrl+c I cannot restart the docker service since it will kill all running docker containers. Cannot even stop the container using this docker container stop &lt;containerid&gt; Please help.",
        "answers": [
            [
                "I had to restart docker process to revive my container. There was nothing else I could do to solve it. used sudo service docker restart and then revived my container using docker run. I will try to build the dockerfile out of it in order to avoid future mishaps."
            ],
            [
                "You can make use of docker RestartPolicy: docker update --restart=always &lt;container&gt; while mindful of caveats on the docker version you running. or explore an answer by @Yale Huang from a similar question: How to add a restart policy to a container that was already created"
            ],
            [
                "Have look at this... https://github.com/docker/for-win/issues/13488 As mentioned in my comment: I see significant performance differences between Hyper-V and WSL. Hyper-V seems to have much faster IO access than WSL (both WSL vers. 1 and WSL vers. 2) which speeds up builds among others."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am trying to build service from docker file when start docker compose. The docker-compose.yml looks like: version: '3' services: app: build: context: . shm_size: 4gb # ... The build process need to enable GPU. I know how to enable GPU in deploy according to this blog. But it seems docker compose file build do not support GPU.",
        "answers": [
            [
                "I think, I have read two different method. The simple, try that: https://docs.docker.com/compose/gpu-support/ https://www.tremplin-numerique.org/en/how-to-run-compose-docker-containers-with-gpu-access services: test: image: nvidia/cuda:10.2-base command: nvidia-smi runtime: nvidia The complex, It run a docker via a run. Then via docker compose, it connect to its docker. In this method, it describes all steps. https://dinhanhthi.com/docker-gpu/ docker run \\ --gpus all\\ --name docker_thi_test\\ --rm\\ -v abc:abc\\ -p 8888:8888 Yaml version: '2' services: jupyter: container_name: 'docker_thi_test' build: . volumes: - ./notebooks:/tf/notebooks ports: - 8888:8888 environment: - NVIDIA_VISIBLE_DEVICES=0 - PASSWORD=12345"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Im working on Ubuntu 20. I've installed docker, nvidia-docker2. On Pycharm, I've followed jetbrain guide, but in the advanced steps it isn't consistent with what I see in my setup. I use PyCharm Proffesional 2022.2. In this step: in the run options I put additionally --runtime=nvidia and --gpus=all. Step 4 finishes as same as in the guide (almost, but it seems that it doesn't bother anything so on that later) and on step 5 I put manually the path to the interpreter in the virtual environment I've created using the Dockerfile. In that way I am able to run the command of nvidia-smi and see correctly the GPU, but I don't see any packages I've installed during the Dockerfile build. There is another option to connect the interpreter a little bit differently in which I do see the packages, but I can't run the nvidia-smi command and the torch.cuda.is_availble return False. The way is instead of doing this as in the guide: I press on the little down arrow in left of the Add Interpreter button and then click on Show all: After which I can press the + button : works, so it might be PyCharm \"Python Console\" issue. and then I can choose Docker: which will result in the difference mentioned above in functionality and also in the path dispalyed (the first one is the first remote interpreter top to bottom direction and the second is the second correspondingly): Here of course the effect of the first and the second correspondingly: Here is the results of the interpreter run with the first method connected interpreter: and here is the second: Of the following code: Here is the Dockerfile file if you want to take a look: Anyone configured it correctly and can help ? Thank you in advance. P.S: if I run the docker from services and enter the terminal the command nvidia-smi works fine and also the import of torch and the command torch.cuda.is_available return True. P.S.2: The thing that has worked for me for now is to change the Dockerfile to install directly torch with pip without create conda environement. Then I set the path to the python2.7 and I can run the code, but not debug it. for run the result is as expected (the packages list as was shown before is still empty, but it works, I guess somehow my IDE cannot access the packages list of the remote interpreter in that case, I dont know why): But the debugger outputs the following error: Any suggestions for the debugger issue also will be welcome, although it is a different issue.",
        "answers": [
            [
                "Please update to 2022.2.1 as it looks like a known regression that has been fixed. Let me know if it still does not work well."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "so using a regular docker I came to a conclusion that 2 different CUDA versions aren't compatible for the following run concept: use the local GPU with CUDA 11 for example with the docker environment with lower OS version and lower CUDA version, because the container must approach the local GPU thorough its CUDA, and because they aren't compatible, the whole thing is impossible. Is this exactly the issue nvidia-docker2 is addressing ? Suppose my OS is ubuntu 20+, CUDA 11+ and I need to run code that must run with CUDA 8 which is only compatible with UBUNTU 16 and I have another code that compatible with CUDA 10 on Ubuntu 18. As much as I saw and correct me if I'm wrong, nvidia-docker2 would make me being able to run nvidia-smi command on the container itself, thus the container simulates (\"thinks\") that the gpu is local to it, thus I can create one container with ubuntu 16, another one with 18, and my GPU will happily participate with any CUDA, cudatoolkit and cudnn versions as I want (install on the containers) ? I think it was also written that those components can be only in the containers, thus it doesn't matter what CUDA version I have on my computer, am I wrong ? And if that is the case, another question will be, would I be able with docker and cuda-container-toolkit to run the interpreter from the container as I can do in the moment using docker and PyCharm, i.e does it support this functionality additionally for being able to run different CUDAs on different containers ? Or am I wrong and hoped to optimistically that it is possible to debug different docer environments with incompatible cuda versions with the same local GPU without installing diffenet UBUNTU versions on the hard drive ? Or does the last suggestion is the only one possible (few Ubuntus on the same computer)? Sounds as the most confident and easy solution anyway, but correct me where I am wrong.",
        "answers": [
            [
                "Is this exactly the issue nvidia-docker2 is addressing ? The primary issue has to do with the GPU driver. The GPU driver has components that run in kernel space and other components that run in user space. The implication of this is that for successful usage in docker, these components (user-space: inside the container, kernel space: outside the container) must match. That is a key function for the NVIDIA container toolkit/container runtime that augments docker: To make whatever is inside the container pertaining to the GPU driver match whatever is outside the container. Other aspects of the CUDA toolkit (runtime libraries, nvcc, etc.) are separate, and regardless of whether you use the NVIDIA container toolkit or not, the code inside the container will need whatever it uses of that (e.g. runtime libraries, nvcc, etc.) to be present inside the container. The stuff outside the container for these items is irrelevant (unless, of course, you are providing it via a mount from outside to inside). Apart from all that, CUDA itself has a dependency between the CUDA version of the toolkit, and the driver. In a nutshell, in ordinary usage, the CUDA version that is in the container must be a version that can be supported by the driver. Newer drivers support older toolkits. Older drivers do not support newer toolkits, unless you take special measures. Related resources: 1 2 3 4 5 6 7 To have the most flexibility in your setup, make sure you have the latest GPU driver installed in your base machine. And use the NVIDIA container toolkit. \"Older\" CUDA toolkits/docker containers should run fine in that setup."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have problem understanding cuda and docker ecosystem. On a host (ubuntu 22.04) server I want to spawn multiple Machiene Learning Jupyter notebooks. Is it enough if I install in the host ubuntu ONLY Nvidia drivers like this: sudo apt-get install linux-headers-$(uname -r) DISTRIBUTION=$(. /etc/os-release;echo $ID$VERSION_ID | sed -e 's/\\.//g') echo $DISTRIBUTION wget https://developer.download.nvidia.com/compute/cuda/repos/$DISTRIBUTION/x86_64/cuda-keyring_1.0-1_all.deb sudo dpkg -i cuda-keyring_1.0-1_all.deb sudo apt-get update sudo apt-get -y install cuda-drivers sudo reboot #After reboot verify if the CUDA driver is installed: nvidia-smi and then install cuda in containers like this: wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600 sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub sudo add-apt-repository \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /\" sudo apt-get update sudo apt-get -y install cuda I think the purest way is to install only required packages on host systems and enrich the containers with all the necessary packages. That is why I wonder if this approach is reasonable. Do I understand correctly that the container will then use the drivers from host system? Is installing cuda enough to be installed in the conatiners, or I shall install cuda-toolkit as it contains more additional packages?",
        "answers": [
            [
                "Do I understand correctly that the container will then use the drivers from host system? Yes, the container will use the drivers from the host system. If you are building your own container, do not install the drivers in the container. Is installing cuda enough to be installed in the conatiners, or I shall install cuda-toolkit as it contains more additional packages? You might not ever want to install cuda in this scenario. You can install cuda (which will also install the drivers mentioned in your question 1) in the host machine. That is acceptable. In the container, if you are building it yourself, you don't want to install cuda, at most the cuda-toolkit. Can I use only nvidia-drivers for host machine of docker based system? There are generally 3 items needed in the host machine, beyond the linux OS install, to make it ready for CUDA-enabled container usage: The GPU drivers A recent docker version The NVIDIA container toolkit (see above link for install instructions) It is not necessary to install the CUDA toolkit (i.e. the items beyond the GPU driver install) in the host machine. These will usually be installed in the container if they are needed in the container."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I installed the nvidia-docker2 following the instructions here. When running the following command I will get the expected output as shown. sudo docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.29.05 Driver Version: 495.29.05 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:0B:00.0 On | N/A | | 24% 31C P8 13W / 250W | 222MiB / 11011MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ However, running the above command without \"sudo\" results in the following error for me: $ docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown. Can anyone please help me with how I can solve this problem?",
        "answers": [
            [
                "Add docker group to your user: sudo usermod -aG docker your_user Update: Check here https://github.com/NVIDIA/nvidia-docker/issues/539 Maybe something from the comments will help you."
            ],
            [
                "try adding \"sudo\" to you docker command. e.g sudo docker-compose ..."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am having interesting and weird issue. When I start docker container with gpu it works fine and I see all the gpus in docker. However, few hours or few days later, I can't use gpus in docker. When I do nvidia-smi in docker machine. I see this msg \"Failed to initialize NVML: Unknown Error\" However, in the host machine, I see all the gpus with nvidia-smi. Also, when I restart the docker machine. It totally works fine and showing all gpus. My Inference Docker machine should be turned on all the time and do the inference depends on server requests. Does any one have same issue or the solution for this problem?",
        "answers": [
            [
                "I had the same Error. I tried the health check of docker as a temporary solution. When nvidia-smi failed, the container will be marked unhealth, and restart by willfarrell/autoheal. Docker-compose Version: services: gpu_container: ... healthcheck: test: [\"CMD-SHELL\", \"test -s `which nvidia-smi` &amp;&amp; nvidia-smi || exit 1\"] start_period: 1s interval: 20s timeout: 5s retries: 2 labels: - autoheal=true - autoheal.stop.timeout=1 restart: always autoheal: image: willfarrell/autoheal environment: - AUTOHEAL_CONTAINER_LABEL=all volumes: - /var/run/docker.sock:/var/run/docker.sock restart: always Dockerfile Version: HEALTHCHECK \\ --label autoheal=true \\ --label autoheal.stop.timeout=1 \\ --start-period=60s \\ --interval=20s \\ --timeout=10s \\ --retries=2 \\ CMD nvidia-smi || exit 1 with autoheal daemon: docker run -d \\ --name autoheal \\ --restart=always \\ -e AUTOHEAL_CONTAINER_LABEL=all \\ -v /var/run/docker.sock:/var/run/docker.sock \\ willfarrell/autoheal"
            ],
            [
                "I had the same weird issue. According to your description, it's most likely relevant to this issue on nvidia-docker official repo: https://github.com/NVIDIA/nvidia-docker/issues/1618 I plan to try the solution mentioned in related thread which suggest to upgrade the kernel cgroup version on host machine from v1 to v2. ps: We have verified this solution on our production environment and it really works! But unfortunately, this solution need at least linux kernel 4.5. If it is not possible to upgrade kernel, the method mentioned by sih4sing5hog5 could also be a workaround solution."
            ],
            [
                "There is a workaround that I tried and found it work. Please check this link: https://github.com/NVIDIA/nvidia-docker/issues/1730 I summarize the cause of the problem and elaborate on a solution here for your convenience. Cause: The host performs daemon-reload (or a similar activity). If the container uses systemd to manage cgroups, daemon-reload \"triggers reloading any Unit files that have references to NVIDIA GPUs.\" Then, your container loses access the reloaded GPU references. How to check if your problem is caused by the issue: When your container still has GPU access, open a \"host\" terminal and run sudo systemctl daemon-reload Then, go back to your container. If nvidia-smi in the container has the problem right away, you may continue to use the workarounds. Workarounds: Although I saw in one discussion that NVIDIA planned to release a formal fix in mid Jun, as of July 8, 2023, I did not see it yet. So, this should be still useful for you, especially when you just can't update your container stack. The easiest way is to disable cgroups in your containers through docker daemon.json. If disabling cgroups does not hurt you, here is the steps. All is done in the host system. sudo nano /etc/docker/daemon.json Then, within the file, add this parameter setting. \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"] Do not forget to add a comma before this parameter setting. It is a well-known JSON syntax, but I think some may not be familiar with it. This is an example edited file from my machine. { \"runtimes\": { \"nvidia\": { \"args\": [], \"path\": \"nvidia-container-runtime\" } }, \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"] } As for the last step, restart the docker service in the host. sudo service docker restart Note: if your container runs its own NVIDIA driver, the above steps will not work, but the reference link has more detail for dealing with it. I elaborate only on a simple solution that I expect many people will find it useful."
            ],
            [
                "Slightly different, but for other people that might stumble upon this. For me the GPUs were not available already after start of the docker container with nvidia-docker, but only showed Failed to initialize NVML: Unknown Error on nivida-smi. After some hours of looking for a solution I stumbled upon the similar error Failed to initialise NVML: Driver/library version mismatch. And one suggestion was to simply reboot the host machine. I did that and it now works. This happened after I upgraded both Ubuntu 20-&gt;22 and Docker 19-&gt;20 along with the nvidia drivers 525.116.04."
            ],
            [
                "I had the same issue, I just ran screen watch -n 1 nvidia-smi in the container and now it works continuously."
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001,
            1.0000001,
            1e-07,
            -2.9999999
        ]
    },
    {
        "question": "I am on a silicon chip Macbook Pro running macOS Monterey version 12.0.1 and the following error occurs on attempting to connect to the NGC service using sudo docker login ncvr.io. Error saving credentials: error storing credentials - err: exit status 1, out: `Post \"http://ipc/registry/credstore-updated\": dial unix /var/root/Library/Containers/com.docker.docker/Data/backend.sock: connect: no such file or directory`",
        "answers": [
            [
                "You will need to have docker running. Try opening Docker Desktop and attempt your login again."
            ],
            [
                "i had the same issue and could resolve it by deleting the ~/.docker/config.json file. originally it contained only one entry being \"credStore=desktop\". after deletion, and trying to login again, it changed to \"osxkeychain\". source: https://github.com/docker/docker-credential-helpers/issues/213"
            ]
        ],
        "votes": [
            55.0000001,
            26.0000001
        ]
    },
    {
        "question": "I was training models last night on my Ubuntu workstation, and then woke up this morning and saw this message: Failed to initialize NVML: Driver/library version mismatch Apparently the NVIDIA system driver automatically updated itself, and now I need to reboot the machine to use my GPUs... How do I prevent automatic updates from NVIDIA?",
        "answers": [
            [
                "If this is because of unattended upgrades (likely), then you should add the NVIDIA drivers to it's blacklist. Add nvidia- and libnvidia- to /etc/apt/apt.conf.d/50unattended-upgrades like so: Unattended-Upgrade::Package-Blacklist { \"nvidia-\"; \"libnvidia-\"; ... } Consider adding additional lines if you see any other NVIDIA driver names in the output of apt list --installed | grep nv"
            ],
            [
                "I think I have had the same issue. It is because of so-called unattended upgrades on Ubuntu. Solution 1: check the changed packages and revert the updates Check the apt history logs less /var/log/apt/history.log Then you can see what packages have changed. Use apt or aptitude to revert the changes. Solution 2: disable unattended upgrades Use this guide to disable unattended upgrades. Please consider if this solution works for you as you have to install security updates manually after this change. Solution 3: hold specific packages Use this guide on how to hold certain packages. Read the apt history as mentioned above to determine which packages you have to put on hold. Probably CUDA related packages such as nvidia-cuda-toolkit. Hard to say since some information is missing from your post. You can see all nvidia related packages like this dpkg -l *nvidia* I hope at least one of my solutions works for you :) P.S. you have to change the title. NVIDIA isn't upgrading anything on your system by itself. Ubuntu is the one causing your trouble ;)"
            ]
        ],
        "votes": [
            9.0000001,
            2.0000001
        ]
    },
    {
        "question": "I am new to docker. I am trying to run one github repository in docker container. I have few questions regarding the same. The docker image was created using CUDA version 9.0. I have Tesla T4 GPU and Driver version 470.129.06 in my amazon EC2 instance. I want to update CUDA version from 9 to 10.0. Is is possible to update cuda version inside the docker container? If yes, then How do we update it? Can you share some useful links? Whenever I run nvidia-docker run command it enters into fish interactive shell. I want it to start bash interactive shell. I don\u2019t know how to enter in bash shell after running docker The command I am running is: nvidia-docker run -it --rm -v /Path_to_dataset:/root/data -v $Path_to_model:/root/model --ipc=host scrin/second-pytorch I tried adding bash in command but does not work (I get error: open: No such file or directory). nvidia-docker run -it --rm -v /Path_to_dataset:/root/data -v $Path_to_model:/root/model --ipc=host scrin/second-pytorch bash The git repository I am using is: https://github.com/traveller59/second.pytorch If anyone can help me. Thank you in advance.",
        "answers": [
            [
                "I found the solution. 1. wget https://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run sh cuda_10.2.89_440.33.01_linux.run If you enter in fish shell run command bash then it will enter in bash interactive shell."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Sometimes I can't communicate with my Nvidia GPUs inside a docker container when I came back to my workplace from home, even though the previously launched process that utilizes GPUs is running well. The running process (training a neural network via Pytorch) is not affected by the disconnection but I cannot launch a new process. nvidia-smi gives Failed to initialize NVML: Unknown Error and torch.cuda.is_available() returns False likewise. I met two different cases: nvidia-smi works fine when it is done at the host machine. In this case, the situation can be solved by restarting the docker container via docker stop $MYCONTAINER followed by docker start $MYCONTAINER at the host machine. nvidia-smi doesn't work at the host machine nor nvcc --version, throwing Failed to initialize NVML: Driver/library version mismatch and Command 'nvcc' not found, but can be installed with: sudo apt install nvidia-cuda-toolkit error. Strange point is that the current process still runs well. In this case, installing the driver again or rebooting the machine solves the problem. However, these solutions require stopping all current processes. It would be unavailable when I should not stop the current process. Does somebody has suggestion for solving this situation? Many thanks. (sofwares) Docker version: 20.10.14, build a224086 OS: Ubuntu 22.04 Nvidia driver version: 510.73.05 CUDA version: 11.6 (hardwares) Supermicro server Nvidia A5000 * 8 (pic1) nvidia-smi not working inside of a docker container, but worked well on the host machine. (pic2) nvidia-smi works after restarting a docker container, which is the case 1 I mentioned above Additionally, Failed to initialize NVML: Unknown Error is reproducible by calling systemctl daemon-reload at the host machine after starting a container.",
        "answers": [
            [
                "For the problem of Failed to initialize NVML: Unknown Error and having to restart the container, please see this ticket and post your system/package information there as well: https://github.com/NVIDIA/nvidia-docker/issues/1671 There's a workaround on the ticket, but it would be good to have others post their configuration to help fix the issue. Downgrading containerd.io to 1.6.6 works as long as you specify no-cgroups = true in /etc/nvidia-container-runtime/config.toml and specify the devices to docker run like docker run --gpus all --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidia-modeset:/dev/nvidia-modeset --device /dev/nvidia-uvm:/dev/nvidia-uvm --device /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools --device /dev/nvidiactl:/dev/nvinvidiactl --rm -it nvidia/cuda:11.4.2-base-ubuntu18.04 bash so sudo apt-get install -y --allow-downgrades containerd.io=1.6.6-1 and sudo apt-mark hold containerd.io to prevent the package from being updated. So do that, edit the config file, and pass all of the /dev/nvidia* devices in to docker run. For the Failed to initialize NVML: Driver/library version mismatch issue, that is caused by the drivers updating but you haven't rebooted yet. If this is a production machine, I would also hold the driver package to stop that from auto-updating as well. You should be able to figure out the package name from something like sudo dpkg --get-selections \"*nvidia*\""
            ],
            [
                "Need to install appropriate version of NVIDIA drivers, recommend drivers could be found through following command. ubuntu-drivers devices Inappropriate versions of drivers might cause multiple issues as mentioned below, even if we might able to forward gpu instance to container, cuda projects might not work. After installing docker, we followed this guide to forward GPU instance from host to container."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm running a docker with gpu enabled (nvidia-docker), and CUDA 11.3 installed, as well as cupy. I can use torch models with no issue. I can see that my gpu is used as it should be. In python, spacy loads fine, the gpu seems activated, and I can load models with no problem: &gt;&gt;&gt; import spacy &gt;&gt;&gt; spacy.require_gpu() True &gt;&gt;&gt; spacy.load(\"en_core_web_trf\") &lt;spacy.lang.en.English object at 0x7efb7ddd6fd0&gt; But when I try to actually use these models to analyze some text, I get this error: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` From what I read in other posts this error is supposed to appear when you run out of memory, but in this case it happens immediatly, the gpu stays at 0% usage as far as I can tell. Here is the full exception I get: https://pastebin.com/x8kECw7p.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I installed nvidia-docker on Ubuntu 22.04 in order to use my GPU in my containers. It works when I use the command sudo docker run --gpus all test/image:latest nvidia-smi, but if I try without sudo, I get this: docker: Error response from daemon: Unknown runtime specified nvidia. I can't use sudo in all environments, so this won't do. Plus now docker ps and docker image ls only show containers and images created without sudo, to display the other dockers I have to do sudo docker ps and so on for all docker commands. Moreover docker-desktop completely ignores any container launched with a sudo command, which makes it useless.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to run the YoloV4 (Demo 5) in TensorRt demos repo on AWS ec2. I created ec2 VM with nvidia-gpu (with AMI - Amazon Linux 2 AMI with NVIDIA TESLA GPU Driver), which has: NVIDIA-SMI 450.119.01 Driver Version: 450.119.01 CUDA Version: 11.0. On this EC2 I pulled and entered into the tensorrt official container, with: sudo docker run --gpus all -it -v /home/ec2-user/player-detection:/home nvcr.io/nvidia/tensorrt:20.02-py3 bash I did the following steps: Ran python3 -m pip install --upgrade setuptools pip &amp;&amp; python3 -m pip install nvidia-pyindex &amp;&amp; pip install nvidia-tensorrt. Inside the yolo/ folder, I ran: pip3 install -r requirements.txt. pip3 install onnx==1.9.0. Inside the plugins/ folder, I ran make. Inside the yolo/ folder, I ran ./download_yolo.sh &amp;&amp; python3 yolo_to_onnx.py -m yolov4 &amp;&amp; python3 onnx_to_tensorrt.py -m yolov4. I got the following error for the python3 onnx_to_tensorrt.py -m yolov4 command: \"RuntimeError: cannot get YoloLayer_TRT plugin creator\" From reading https://github.com/jkjung-avt/tensorrt_demos/issues/476 it seems that the problem is related to dynamic libaries. I tried to view the libaries that I have, and got: $ ldd libyolo_layer.so linux-vdso.so.1 (0x00007fff142a4000) libnvinfer.so.7 =&gt; /usr/lib/x86_64-linux-gnu/libnvinfer.so.7 (0x00007f9673734000) libcudart.so.11.0 =&gt; /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.0 (0x00007f96734af000) libstdc++.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f9673126000) libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f9672f0e000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f9672b1d000) libcudnn.so.8 =&gt; /usr/lib/x86_64-linux-gnu/libcudnn.so.8 (0x00007f96728f4000) libmyelin.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libmyelin.so.1 (0x00007f9672074000) libnvrtc.so.11.1 =&gt; /usr/local/cuda-11.1/targets/x86_64-linux/lib/libnvrtc.so.11.1 (0x00007f966feac000) librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f966fca4000) libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f966faa0000) libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f966f702000) /lib64/ld-linux-x86-64.so.2 (0x00007f9699135000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f966f4e3000) libcublas.so.11 =&gt; /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcublas.so.11 (0x00007f9668008000) libcublasLt.so.11 =&gt; /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcublasLt.so.11 (0x00007f965a23e000) It seems that I miss some, and also when I tried to print all the plugins, I didn't see the YoloLayer_TRT. Any idea how to solve it?",
        "answers": [
            [
                "Did you load your libyolo_layer.so using ctypes module? It seems that YoloLayer_TRT is custom plugin built by yolo team, and you have to load library file to use it in python. https://github.com/jkjung-avt/tensorrt_demos/issues/476#issuecomment-935225260"
            ],
            [
                "The solution was: change the image tag to :21.10-py3 (thx to Hyunwoo Kim!). change the TENSORRT_INCS to /usr/include/x86_64-linux-gnu/NvInfer* and TENSORRT_LIBS to /usr/lib/x86_64-linux-gnu/libnvinfer*. changed computes to 70 (in my environment) - you can check your version here."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I'm using NVIDIA Docker in a Linux machine (Ubuntu 20.04). I've created a container named user1 using nvidia/cuda:11.0-base image as follows: docker run --gpus all --name user1 -dit nvidia/cuda:11.0-base /bin/bash And, here is what I see if I run docker ps -a: admin@my_desktop:~$ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a365362840de nvidia/cuda:11.0-base \"/bin/bash\" 3 seconds ago Up 2 seconds user1 I want to access to that container via ssh using its unique IP address from a totally different machine (other than my_desktop, which is the host). First of all, is it possible to grant each container a unique IP address? If so, how can I do it? Thanks in advance.",
        "answers": [
            [
                "In case you want to access to your container with ssh from an external VM, you need to do the following Install the ssh daemon for your container Run the container and expose its ssh port I would propose the following Dockerfile, which builds from nvidia/cuda:11.0-base and creates an image with the ssh daemon inside Dockerfile # Instruction for Dockerfile to create a new image on top of the base image (nvidia/cuda:11.0-base) FROM nvidia/cuda:11.0-base ARG root_password RUN apt-get update || echo \"OK\" &amp;&amp; apt-get install -y openssh-server RUN mkdir /var/run/sshd RUN echo \"root:${root_password}\" | chpasswd RUN sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config RUN sed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config EXPOSE 22 CMD [\"/usr/sbin/sshd\", \"-D\"] Build the image from the Dockerfile docker image build --build-arg root_password=password --tag nvidia/cuda:11.0-base-ssh . Create the container docker container run -d -P --name ssh nvidia/cuda:11.0-base-ssh Run docker ps to see the container port Finally, access the container ssh -p 49157 root@&lt;VM_IP&gt; EDIT: As David Maze correctly pointed out. You should be aware that the root password will be visible in the image history. Also this way overwrites the original container process. This process, if it is to be adopted it needs to be modified in case you need it for production use. This serves as a starting point for someone who wishes to add ssh to his container."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am working on Nvidia Jetson AGX Xavier within Dockerized container\u2026I want to take input from RTSP stream\u2026it\u2019s encoding type is H264 and .avi video input.The input stream frame size is 1920x1080 (in code I am resizing that into 1280x720) I have used this GStreamer pipeline cap = cv2.VideoCapture(\u2018rtspsrc location=\u201crtsp_link\u201d latency=200 ! queue ! rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink\u2019, cv2.CAP_GSTREAMER) It is able to read the frames but after reading frames, frames got completely blurred , that\u2019s why object detection is not happening.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to dockerize a Django Cuda application that runs on Nginx and Gunicorn.Problem is when I go to do prediction .. I get an error cuda drivers not found My DockerFile: FROM nvidia/cuda FROM python:3.6.8 ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 WORKDIR /app COPY ./requirements.txt /app/requirements.txt RUN python -m pip install --upgrade pip RUN pip install cmake RUN pip install opencv-python==4.2.0.32 # RUN pip install pywin32==227 RUN pip install -r requirements.txt COPY . /app RUN python manage.py collectstatic --noinput RUN pip install gunicorn RUN mkdir -p /home/app/staticfiles/ Ngnix DockerFile FROM nginx:1.21-alpine RUN rm /etc/nginx/conf.d/default.conf COPY nginx.conf /etc/nginx/conf.d Ngnix config file upstream project_settings { server web:8000; } server { listen 80; client_max_body_size 0; location / { proxy_pass http://project_settings; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $host; proxy_redirect off; } location /static/ { alias /home/app/staticfiles/; } } Main Docker compose file services: nginx: build: ./nginx ports: - 1300:80 volumes: - static_volume:/home/app/staticfiles/ depends_on: - web web: build: . command: gunicorn project_settings.wsgi:application --bind 0.0.0.0:8000 volumes: - static_volume:/home/app/staticfiles/ image: sampleapp1121asa expose: - 8000 deploy: resources: reservations: devices: - capabilities: [ gpu ] volumes: static_volume: Things are not working with docker compose, when I try to build the dockerfile seperately and then run using docker run --rm --gpus all -p 8000:8000 deefakedetectiondockerimage python3 manage.py runserver 0.0.0.0:8000 it works but the problem with this approach is I can not serve static file in docker. Ngnix is required to serve the static file, it means I need to run this through docker compose only",
        "answers": [
            [
                "I found the solution for the same. Actually, things become difficult to run from docker-compose when you are trying to run multiple images in single container. So I build the image using DockerFile for application and separate image for Ngnix and enable communication of both the container with unix socket connections. My updated dockerfile for application: #pull the nvidia cuda GPU docker image FROM nvidia/cuda #pull python 3.6.8 docker image FROM python:3.6.8 ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 #create a directory to serve static files RUN mkdir -p /home/app/staticfiles/app/uploaded_videos/ WORKDIR /app COPY ./requirements.txt /app/requirements.txt RUN python -m pip install --upgrade pip RUN pip install cmake RUN pip install opencv-python==4.2.0.32 RUN pip install -r requirements.txt COPY . /app RUN python manage.py collectstatic --noinput RUN pip install gunicorn RUN mkdir -p /app/uploaded_videos/app/uploaded_videos/ VOLUME /app/run/ ENTRYPOINT [\"/app/bin/gunicorn_start.sh\"] gunicorn_start.sh script #!/bin/bash NAME=\"project_settings\" # Name of the application DJANGODIR=/app # Django project directory SOCKFILE=/app/run/gunicorn.sock # we will communicte using this unix socket NUM_WORKERS=3 # how many worker processes should Gunicorn spawn DJANGO_SETTINGS_MODULE=project_settings.settings # which settings file should Django use DJANGO_WSGI_MODULE=project_settings.wsgi # WSGI module name echo \"Starting $NAME as `whoami`\" # Create the run directory if it doesn't exist RUNDIR=$(dirname $SOCKFILE) test -d $RUNDIR || mkdir -p $RUNDIR # Start your Django Gunicorn gunicorn project_settings.wsgi:application --bind=unix:$SOCKFILE --workers $NUM_WORKERS --timeout 600 My updated docker file for Nginx FROM nginx WORKDIR /etc/nginx/ RUN rm /etc/nginx/conf.d/default.conf COPY nginx.conf /etc/nginx/conf.d EXPOSE 80 For step by step process you can follow this blog"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "What I'm trying to do is to run a lot of Alphafold runs. I followed some articles and able to successfully finish my first trial run but I realized it couldn't recognized the GPU I attached to the host from the Docker container. The commands that I used are as follow, # dsub command dsub --provider google-cls-v2 \\ --project ${PROJECT_ID} \\ --logging gs://$BUCKET/logs \\ --image=$IMAGE \\ --script=alphafold.sh \\ --mount DB=\"${IMAGE_URL} 3000\" \\ --machine-type n1-standard-8 \\ --boot-disk-size 100 \\ --subnetwork ${SUBNET_NAME} \\ --accelerator-type nvidia-tesla-k80 \\ --accelerator-count 1 \\ --preemptible \\ --zones ${ZONE_NAMES} \\ --tasks batch_tasks.tsv 1-2 # alphafold.sh cd /app/alphafold mkdir -p output python run_alphafold.py \\ --fasta_paths=${FASTA} \\ --data_dir=${DB} \\ --output_dir=output \\ --use_gpu_relax=True \\ --uniref90_database_path=${DB}/uniref90/uniref90.fasta \\ --mgnify_database_path=${DB}/mgnify/mgy_clusters_2018_12.fa \\ --template_mmcif_dir=${DB}/pdb_mmcif/mmcif_files \\ --max_template_date=2020-05-14 \\ --obsolete_pdbs_path=${DB}/pdb_mmcif/obsolete.dat \\ --pdb70_database_path=${DB}/pdb70/pdb70 \\ --uniclust30_database_path=${DB}/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\ --bfd_database_path=${DB}/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\ --db_preset=full_dbs \\ --benchmark=False tar zcvf ${OUT_PATH} output And inside the stdout, I saw these few lines, I0416 02:16:30.765444 140264211408704 tpu_client.py:54] Starting the local TPU driver. I0416 02:16:30.766041 140264211408704 xla_bridge.py:212] Unable to initialize backend 'tpu_driver': Not found: Unable to find driver in registry given worker: local:// I0416 02:16:31.607698 140264211408704 xla_bridge.py:212] Unable to initialize backend 'tpu': Invalid argument: TpuPlatform is not available. I saw the log that it's trying to install GPU driver and put it inside the kernel with a image called cos-gpu-installer from a Google registry before starting the user-command container. But when I tried the nvidia-smi command when the user-command is still running, docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi I got could not select device driver \"\" with capabilities: [[gpu]].. The Dockerfile that I used for building the docker image is from the Alphafold docker folder, git clone https://github.com/deepmind/alphafold.git cd alphafold docker build -f docker/Dockerfile -t alphafold . docker tag alphafold us.gcr.io/${PROJECT_ID}/alphafold docker push us.gcr.io/${PROJECT_ID}/alphafold Any suggestions on how to troubleshoot the issue would be appreciated.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I got a K8S+DinD issue: launch Kubernetes cluster start a main docker image and a DinD image inside this cluster when running a job requesting GPU, got error could not select device driver \"nvidia\" with capabilities: [[gpu]] Full error http://localhost:2375/v1.40/containers/long-hash-string/start: Internal Server Error (\"could not select device driver \"nvidia\" with capabilities: [[gpu]]\") exec to the DinD image inside of K8S pod, nvidia-smi is not available. Some debugging and it seems it's due to the DinD is missing the Nvidia-docker-toolkit, I had the same error when I ran the same job directly on my local laptop docker, I fixed the same error by installing nvidia-docker2 sudo apt-get install -y nvidia-docker2. I'm thinking maybe I can try to install nvidia-docker2 to the DinD 19.03 (docker:19.03-dind), but not sure how to do it? By multiple stage docker build? Thank you very much! update: pod spec: spec: containers: - name: dind-daemon image: docker:19.03-dind",
        "answers": [
            [
                "I got it working myself. Referring to https://github.com/NVIDIA/nvidia-docker/issues/375 https://github.com/Henderake/dind-nvidia-docker First, I modified the ubuntu-dind image (https://github.com/billyteves/ubuntu-dind) to install nvidia-docker (i.e. added the instructions in the nvidia-docker site to the Dockerfile) and changed it to be based on nvidia/cuda:9.2-runtime-ubuntu16.04. Then I created a pod with two containers, a frontend ubuntu container and the a privileged docker daemon container as a sidecar. The sidecar's image is the modified one I mentioned above. But since this post is 3 year ago from now, I did spent quite some time to match up the dependencies versions, repo migration over 3 years, etc. My modified version of Dockerfile to build it ARG CUDA_IMAGE=nvidia/cuda:11.0.3-runtime-ubuntu20.04 FROM ${CUDA_IMAGE} ARG DOCKER_CE_VERSION=5:18.09.1~3-0~ubuntu-xenial RUN apt-get update -q &amp;&amp; \\ apt-get install -yq \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common &amp;&amp; \\ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - &amp;&amp; \\ add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" &amp;&amp; \\ apt-get update -q &amp;&amp; apt-get install -yq docker-ce docker-ce-cli containerd.io # https://github.com/docker/docker/blob/master/project/PACKAGERS.md#runtime-dependencies RUN set -eux; \\ apt-get update -q &amp;&amp; \\ apt-get install -yq \\ btrfs-progs \\ e2fsprogs \\ iptables \\ xfsprogs \\ xz-utils \\ # pigz: https://github.com/moby/moby/pull/35697 (faster gzip implementation) pigz \\ # zfs \\ wget # set up subuid/subgid so that \"--userns-remap=default\" works out-of-the-box RUN set -x \\ &amp;&amp; addgroup --system dockremap \\ &amp;&amp; adduser --system -ingroup dockremap dockremap \\ &amp;&amp; echo 'dockremap:165536:65536' &gt;&gt; /etc/subuid \\ &amp;&amp; echo 'dockremap:165536:65536' &gt;&gt; /etc/subgid # https://github.com/docker/docker/tree/master/hack/dind ENV DIND_COMMIT 37498f009d8bf25fbb6199e8ccd34bed84f2874b RUN set -eux; \\ wget -O /usr/local/bin/dind \"https://raw.githubusercontent.com/docker/docker/${DIND_COMMIT}/hack/dind\"; \\ chmod +x /usr/local/bin/dind ##### Install nvidia docker ##### # Add the package repositories RUN curl -fsSL https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add --no-tty - RUN distribution=$(. /etc/os-release;echo $ID$VERSION_ID) &amp;&amp; \\ echo $distribution &amp;&amp; \\ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\ tee /etc/apt/sources.list.d/nvidia-docker.list RUN apt-get update -qq --fix-missing RUN apt-get install -yq nvidia-docker2 RUN sed -i '2i \\ \\ \\ \\ \"default-runtime\": \"nvidia\",' /etc/docker/daemon.json RUN mkdir -p /usr/local/bin/ COPY dockerd-entrypoint.sh /usr/local/bin/ RUN chmod 777 /usr/local/bin/dockerd-entrypoint.sh RUN ln -s /usr/local/bin/dockerd-entrypoint.sh / VOLUME /var/lib/docker EXPOSE 2375 ENTRYPOINT [\"dockerd-entrypoint.sh\"] #ENTRYPOINT [\"/bin/sh\", \"/shared/dockerd-entrypoint.sh\"] CMD [] When I use exec to login into the Docker-in-Docker container, I can successfully run nvidia-smi (which previously return not found error then cannot run any GPU resource related docker run) Welcome to pull my image at brandsight/dind:nvidia-docker"
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "In the command line I am used to run/create containers with specific GPUs using the --gpus argument: docker run -it --gpus '\"device=0,2\"' ubuntu nvidia-smi The Docker SDK for Python documentation was not very helpful and I could not find a good explanation on how to do the same with the python SDK. Is there a way to do it?",
        "answers": [
            [
                "This is how you can run/create docker containers with specific GPUs using the Docker SDK for Python: client.containers.run('ubuntu', \"nvidia-smi\", device_requests=[ docker.types.DeviceRequest(device_ids=[\"0,2\"], capabilities=[['gpu']])]) This way you can also use other GPU resource options specified here: https://docs.docker.com/config/containers/resource_constraints/"
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "I am trying to use docker (Docker Desktop for Windows 10 Pro) with the WSL2 Backend (WINDOWS SUBSHELL LINUX (WSL) (Ubuntu 20.04.4 LTS)). That part seems to be working fine, except I would like to pass my GPU (Nvidia RTX A5000) through to my docker container. Before I even get that far, I am still trying to set things up. I found a very good tutorial aimed at 18.04, but found all the steps are the same for 20.04, just with some version numbers bumpede. At the end, I can see that my Cuda versions do not match. You can see that here, . The real issue is when I try to run the test command as shown on the docker website: docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark I get this error: --&gt; docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: Running hook #0:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: requirement error: unsatisfied condition: cuda&gt;=11.6, please update your driver to a newer version, or use an earlier cuda container: unknown. ... and I just don't know what to do, or how I can fix this. Can someone explain how to get the GPU to pass through to a docker container successfully.",
        "answers": [
            [
                "I had the same issue on Ubuntu when I tried to run the container: s.evloev@some-pc:~$ docker run --gpus all --rm nvidia/cuda:11.7.0-base-ubuntu18.04 docker: Error response from daemon: failed to create shim: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy' nvidia-container-cli: requirement error: unsatisfied condition: cuda&gt;=11.7, please update your driver to a newer version, or use an earlier cuda container: unknown. In my case it occurred when I tried to launch docker image that have nvidia cuda version which is higher than what was installed on my host. When I checked my cuda version that was installed on my host I have found that it is version 11.3. s.evloev@some-pc:~$ nvidia-smi Thu Jul 21 15:06:33 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 465.19.01 Driver Version: 465.19.01 CUDA Version: 11.3 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | |... | +-----------------------------------------------------------------------------+ So when I try to run the same cuda version (11.3) it works well: s.evloev@some-pc:~$ docker run -it --gpus all --rm nvidia/cuda:11.3.0-base-ubuntu18.04 nvidia-smi Thu Jul 21 12:13:46 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 465.19.01 Driver Version: 465.19.01 CUDA Version: 11.3 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:65:00.0 Off | N/A | | 0% 44C P8 7W / 180W | 1404MiB / 8110MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+"
            ],
            [
                "The comment from @RobertCrovella resolved this: please update your driver to a newer version when using WSL, the driver in your WSL setup is not something you install in WSL, it is provided by the driver on the windows side. Your WSL driver is 472.84 and this is too old to work with CUDA 11.6 (it only supports up to CUDA 11.4). So you would need to update your windows side driver to the latest one possible for your GPU, if you want to run a CUDA 11.6 test case. Regarding the \"mismatch\" of CUDA versions, this provides general background material for interpretation. Downloading the most current Nvidia driver: Version: R510 U3 (511.79) WHQL Release Date: 2022.2.14 Operating System: Windows 10 64-bit, Windows 11 Language: English (US) File Size: 640.19 MB Now I am able to support CUDA 11.6, and the test from the docker documentation now works: --&gt; docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark Run \"nbody -benchmark [-numbodies=&lt;numBodies&gt;]\" to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=&lt;N&gt; (number of bodies (&gt;= 1) to run in simulation) -device=&lt;d&gt; (where d=0,1,2.... for the CUDA device to use) -numdevices=&lt;i&gt; (where i=(number of CUDA devices &gt; 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation) NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. &gt; Windowed mode &gt; Simulation data stored in video memory &gt; Single precision floating point simulation &gt; 1 Devices used for simulation GPU Device 0: \"Ampere\" with compute capability 8.6 &gt; Compute 8.6 CUDA device: [NVIDIA RTX A5000] 65536 bodies, total time for 10 iterations: 58.655 ms = 732.246 billion interactions per second = 14644.916 single-precision GFLOP/s at 20 flops per interaction Thank you for the quick response!"
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "google-chrome --disable-gpu --remote-debugging-port=9222 --remote-debugging-address=0.0.0.0 --no-first-run --no-default-browser-check --autoplay-policy=no-user-gesture-required --disable-infobars --window-size=$window_size_x,$window_size_y --window-position=0,0 --disable-dev-shm-usage --no-sandbox --no-sandbox-and-elevated --no-zygote --enable-webgl --use-gl=desktop --use-skia-renderer --enable-gpu-rasterization --enable-logging=stderr --kiosk wwww.google.com Details: Tried removing --disable-gpu flag in headless chrome In my scenario, I want to shift all load to GPU as it's free and other things require a GPU based machine and these types of machines have fewer CPU cores which means we can not run more Kubernetes pods on a single machine because of a CPU bottleneck OS: Ubuntu Using Kubernetes pod which is ubuntu based pod. Using Nvidia based machine in the AWS environment.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to use nvidia-docker in WSL2 - Ubuntu 18.04 LTS. When I type: nvidia-smi It returns: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.41.01 Driver Version: 496.49 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:01:00.0 Off | N/A | | N/A 53C P8 12W / N/A | 370MiB / 6144MiB | N/A Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Everything seems well, HOWEVER, when I type: nvidia-docker run -it --name=\"medical\" -v /home/tim:/root -p 8892:8892 -p 6010:6010 timliu98/us_backup:compatible /bin/bash It returns an error: docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: mount error: file creation failed: /var/lib/docker/overlay2/6168a51b56d40ba448bd429ed67550daf7c6da438b04e3dbb3499401af4f3007/merged/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1: file exists: unknown. ERRO[0001] error waiting for container: context canceled Kernel: Linux LAPTOP-ITV4QSFR 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux Docker version: Client: Docker Engine - Community Version: 20.10.12 API version: 1.41 Go version: go1.16.12 Git commit: e91ed57 Built: Mon Dec 13 11:45:27 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 20.10.12 API version: 1.41 (minimum version 1.12) Go version: go1.16.12 Git commit: 459d0df Built: Mon Dec 13 11:43:36 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.12 GitCommit: 7b11cfaabd73bb80907dd23182b9347b4245eb5d runc: Version: 1.0.2 GitCommit: v1.0.2-0-g52b36a2 docker-init: Version: 0.19.0 GitCommit: de40ad0 Nvidia packages version: Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-=======================================-========================-========================-=================================================================================== ii libnvidia-container-tools 1.8.0-1 amd64 NVIDIA container runtime library (command-line tools) ii libnvidia-container1:amd64 1.8.0-1 amd64 NVIDIA container runtime library un nvidia-container-runtime &lt;none&gt; &lt;none&gt; (no description available) un nvidia-container-runtime-hook &lt;none&gt; &lt;none&gt; (no description available) ii nvidia-container-toolkit 1.8.0-1 amd64 NVIDIA container runtime hook un nvidia-docker &lt;none&gt; &lt;none&gt; (no description available) ii nvidia-docker2 2.9.0-1 all nvidia-docker CLI wrapper Nvidia container library version: cli-version: 1.8.0 lib-version: 1.8.0 build date: 2022-02-04T09:17+00:00 build revision: 05959222fe4ce312c121f30c9334157ecaaee260 build compiler: x86_64-linux-gnu-gcc-7 7.5.0 build platform: x86_64 build flags: -D_GNU_SOURCE -D_FORTIFY_SOURCE=2 -DNDEBUG -std=gnu11 -O2 -g -fdata-sections -ffunction-sections -fplan9-extensions -fstack-protector -fno-strict-aliasing -fvisibility=hidden -Wall -Wextra -Wcast-align -Wpointer-arith -Wmissing-prototypes -Wnonnull -Wwrite-strings -Wlogical-op -Wformat=2 -Wmissing-format-attribute -Winit-self -Wshadow -Wstrict-prototypes -Wunreachable-code -Wconversion -Wsign-conversion -Wno-unknown-warning-option -Wno-format-extra-args -Wno-gnu-alignof-expression -Wl,-zrelro -Wl,-znow -Wl,-zdefs -Wl,--gc-sections May I know the potential solution? I had tried so many tutorials unluckily all failed. Thanks all in advance and should you need more information, please let me know.",
        "answers": [
            [
                "After running nvidia-smi -a the error that the GPU access blocked by the operating system\" in WSL2 As explained, the fix is to make sure that [Windows 21H2](What's new in Windows 10, version 21H2) is installed via Windows Update in order to enable GPU support for WSL. After installing and rebooting I was able to continue PyTorch development after verifying GPU Access as suggested by Docker: PS C:\\Users\\alexh&gt; wsl alexh@DESKTOP-U21F0MC:/mnt/c/Users/alexh$ docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark Output: Run \"nbody -benchmark [-numbodies=&lt;numBodies&gt;]\" to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=&lt;N&gt; (number of bodies (&gt;= 1) to run in simulation) -device=&lt;d&gt; (where d=0,1,2.... for the CUDA device to use) -numdevices=&lt;i&gt; (where i=(number of CUDA devices &gt; 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation) NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. &gt; Windowed mode &gt; Simulation data stored in video memory &gt; Single precision floating point simulation &gt; 1 Devices used for simulation GPU Device 0: \"Ampere\" with compute capability 8.6 &gt; Compute 8.6 CUDA device: [NVIDIA GeForce RTX 3080] 69632 bodies, total time for 10 iterations: 59.047 ms = 821.146 billion interactions per second = 16422.926 single-precision GFLOP/s at 20 flops per interaction"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm working on a project for university. We have to containerize a Gazebo simulation using Docker. While researching on how to do this I came across this link, and tried running the simulation shown in it. If it works I think we can just change some stuff and make it work for our project. This is the link detailing the simulation I'm trying to run: https://docs.aws.amazon.com/robomaker/latest/dg/run-hello-world-ros-2.html All the code needed to reproduce what we have tried to do is found in that Amazon AWS guide so I won't paste it in here for the most part. Some info on the system and the technologies we're working with: OS: Ubuntu 20.04 focal fossa Gazebo 11 ROS2 Foxy NVIDIA GeForce rtx 2070 with version 510 drivers The simulation is split in 3 images, Base, RobotApp and SimulationApp. We then run the 2 app images. When using the command \"docker run\" given in the example the first one creates a spinning object and seems to work just fine. Docker command for the first one: docker run -it -e DISPLAY -v /tmp/.X11-unix/:/tmp/.X11-unix/ --name robot_app \\ -u robomaker -e ROBOMAKER_GAZEBO_MASTER_URI=http://localhost:5555 \\ -e ROBOMAKER_ROS_MASTER_URI=http://localhost:11311 \\ helloworldsampleappros2foxygazebo11robotapp:latest Docker command for the second one: docker run -it -e DISPLAY -v /tmp/.X11-unix/:/tmp/.X11-unix/ --name sim_app \\ -u robomaker -e ROBOMAKER_GAZEBO_MASTER_URI=http://localhost:5555 \\ -e ROBOMAKER_ROS_MASTER_URI=http://localhost:11311 \\ helloworldsampleappros2foxygazebo11simapp:latest The second one gives me this error message along some warnings: [gzserver-1] libGL error: No matching fbConfigs or visuals found [gzserver-1] libGL error: failed to load driver: swrast Here's all the messages printed to console when running the second Docker command: ROS_VERSION=2 NVIDIA_VISIBLE_DEVICES=all GAZEBO_MASTER_URI=http://localhost:11345 ROS_PYTHON_VERSION=3 HOSTNAME=0ff33fe3f61a ROBOMAKER_GAZEBO_MASTER_URI=http://localhost:5555 PWD=/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws NVIDIA_DRIVER_CAPABILITIES=graphics, display ROBOMAKER_ROS_MASTER_URI=http://localhost:11311 HOME=/home/robomaker GAZEBO_PLUGIN_PATH=/usr/lib/x86_64-linux-gnu/gazebo-11/plugins: LANG=C.UTF-8 GAZEBO_MODEL_DATABASE_URI=http://models.gazebosim.org AMENT_PREFIX_PATH=/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install/turtlebot3_description_reduced_mesh:/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install/hello_world_simulation:/opt/ros/foxy CMAKE_PREFIX_PATH=/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install/turtlebot3_description_reduced_mesh:/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install/hello_world_simulation GAZEBO_RESOURCE_PATH=/usr/share/gazebo-11: COLCON_PREFIX_PATH=/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install PYTHONPATH=/opt/ros/foxy/lib/python3.8/site-packages TERM=xterm DISPLAY=:1 SHLVL=1 LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins: ROS_LOCALHOST_ONLY=0 LC_ALL=C.UTF-8 GAZEBO_MODEL_PATH=/usr/share/gazebo-11/models: PATH=/opt/ros/foxy/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBIAN_FRONTEND=noninteractive ROS_DISTRO=foxy QT_X11_NO_MITSHM=1 OGRE_RESOURCE_PATH=/usr/lib/x86_64-linux-gnu/OGRE-1.9.0 OLDPWD=/ _=/usr/bin/printenv [INFO] [launch]: All log files can be found below /home/robomaker/.ros/log/2022-02-04-12-17-26-791225-0ff33fe3f61a-39 [INFO] [launch]: Default logging verbosity is set to INFO /home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install/turtlebot3_description_reduced_mesh/share/turtlebot3_description_reduced_mesh/launch/spawn_turtlebot.launch.py:61: UserWarning: The parameter 'node_executable' is deprecated, use 'executable' instead Node( [INFO] [gzserver-1]: process started with pid [41] [INFO] [spawn_entity.py-2]: process started with pid [43] [spawn_entity.py-2] [INFO] [1643977047.395921509] [spawn_entity]: Spawn Entity started [spawn_entity.py-2] [INFO] [1643977047.396308383] [spawn_entity]: Loading entity XML from file /home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install/turtlebot3_description_reduced_mesh/share/turtlebot3_description_reduced_mesh/urdf/turtlebot3_waffle_pi.urdf [spawn_entity.py-2] [INFO] [1643977047.398652203] [spawn_entity]: Waiting for service /spawn_entity, timeout = 30 [spawn_entity.py-2] [INFO] [1643977047.399013641] [spawn_entity]: Waiting for service /spawn_entity [gzserver-1] libGL error: No matching fbConfigs or visuals found [gzserver-1] libGL error: failed to load driver: swrast [spawn_entity.py-2] [INFO] [1643977047.904187249] [spawn_entity]: Calling service /spawn_entity [gzserver-1] [WARN] [1643977048.247457510] [rcl]: Found remap rule '~/out:=imu'. This syntax is deprecated. Use '--ros-args --remap ~/out:=imu' instead. [gzserver-1] [INFO] [1643977048.250959996] [turtlebot3_imu]: &lt;initial_orientation_as_reference&gt; is unset, using default value of false to comply with REP 145 (world as orientation reference) [gzserver-1] [WARN] [1643977048.251600689] [rcl]: Found remap rule '~/out:=imu'. This syntax is deprecated. Use '--ros-args --remap ~/out:=imu' instead. [spawn_entity.py-2] [INFO] [1643977048.335266898] [spawn_entity]: Spawn status: SpawnEntity: Successfully spawned entity [robot] [gzserver-1] [WARN] [1643977048.341230833] [rcl]: Found remap rule '~/out:=scan'. This syntax is deprecated. Use '--ros-args --remap ~/out:=scan' instead. [gzserver-1] [WARN] [1643977048.344790289] [rcl]: Found remap rule '~/out:=scan'. This syntax is deprecated. Use '--ros-args --remap ~/out:=scan' instead. [INFO] [spawn_entity.py-2]: process has finished cleanly [pid 43] [gzserver-1] [INFO] [1643977048.476454488] [turtlebot3_diff_drive]: Wheel pair 1 separation set to [0.287000m] [gzserver-1] [INFO] [1643977048.476502091] [turtlebot3_diff_drive]: Wheel pair 1 diameter set to [0.066000m] [gzserver-1] [INFO] [1643977048.477502575] [turtlebot3_diff_drive]: Subscribed to [/cmd_vel] [gzserver-1] [INFO] [1643977048.478593142] [turtlebot3_diff_drive]: Advertise odometry on [/odom] [gzserver-1] [INFO] [1643977048.479685530] [turtlebot3_diff_drive]: Publishing odom transforms between [odom] and [base_footprint] [gzserver-1] [WARN] [1643977048.487264720] [rcl]: Found remap rule '~/out:=joint_states'. This syntax is deprecated. Use '--ros-args --remap ~/out:=joint_states' instead. [gzserver-1] [INFO] [1643977048.490953326] [turtlebot3_joint_state]: Going to publish joint [wheel_left_joint] [gzserver-1] [INFO] [1643977048.490991604] [turtlebot3_joint_state]: Going to publish joint [wheel_right_joint] [gzserver-1] [WARN] [1643977048.491042853] [rcl]: Found remap rule '~/out:=joint_states'. This syntax is deprecated. Use '--ros-args --remap ~/out:=joint_states' instead. After this I follow the last set of instructions: # Enable access to X server to launch Gazebo from docker container $ xhost + # Check that the robot_app and sim_app containers are running. The command should list both containers $ docker container ls # Connect to the sim app container $ docker exec -it sim_app bash # Launch Gazebo from within the container $ /home/robomaker/simulation-entrypoint.sh ros2 launch gazebo_ros gzclient.launch.py And get the following error: [gzclient -1] libGL error: No matching fbConfigs or visuals found [gzclient -1] libGL error: failed to load driver: swrast [gzclient -1] libGL error: No matching fbConfigs or visuals found [gzclient -1] libGL error: failed to load driver: swrast [gzclient -1] Segmentation fault (core dumped) [ERROR] [gzclient -1]: process has died [pid 156, exit code 139, cmd 'gzclient ']. Here's all the messages printed to console when running the commands above: ROS_VERSION=2 NVIDIA_VISIBLE_DEVICES=all GAZEBO_MASTER_URI=http://localhost:11345 ROS_PYTHON_VERSION=3 HOSTNAME=0ff33fe3f61a ROBOMAKER_GAZEBO_MASTER_URI=http://localhost:5555 PWD=/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws NVIDIA_DRIVER_CAPABILITIES=graphics, display ROBOMAKER_ROS_MASTER_URI=http://localhost:11311 HOME=/home/robomaker GAZEBO_PLUGIN_PATH=/usr/lib/x86_64-linux-gnu/gazebo-11/plugins: LANG=C.UTF-8 GAZEBO_MODEL_DATABASE_URI=http://models.gazebosim.org LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36: AMENT_PREFIX_PATH=/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install/turtlebot3_description_reduced_mesh:/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install/hello_world_simulation:/opt/ros/foxy CMAKE_PREFIX_PATH=/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install/turtlebot3_description_reduced_mesh:/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install/hello_world_simulation GAZEBO_RESOURCE_PATH=/usr/share/gazebo-11: COLCON_PREFIX_PATH=/home/robomaker/workspace/aws-robomaker-sample-application-helloworld-ros2/simulation_ws/install PYTHONPATH=/opt/ros/foxy/lib/python3.8/site-packages TERM=xterm DISPLAY=:1 SHLVL=2 LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins: ROS_LOCALHOST_ONLY=0 LC_ALL=C.UTF-8 GAZEBO_MODEL_PATH=/usr/share/gazebo-11/models: PATH=/opt/ros/foxy/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBIAN_FRONTEND=noninteractive ROS_DISTRO=foxy QT_X11_NO_MITSHM=1 OGRE_RESOURCE_PATH=/usr/lib/x86_64-linux-gnu/OGRE-1.9.0 OLDPWD=/ _=/usr/bin/printenv [INFO] [launch]: All log files can be found below /home/robomaker/.ros/log/2022-02-04-12-17-58-244883-0ff33fe3f61a-124 [INFO] [launch]: Default logging verbosity is set to INFO [INFO] [gzclient -1]: process started with pid [156] [gzclient -1] libGL error: No matching fbConfigs or visuals found [gzclient -1] libGL error: failed to load driver: swrast [gzclient -1] libGL error: No matching fbConfigs or visuals found [gzclient -1] libGL error: failed to load driver: swrast [gzclient -1] Segmentation fault (core dumped) [ERROR] [gzclient -1]: process has died [pid 156, exit code 139, cmd 'gzclient ']. That said we checked every post for hours to try to fix this: [gzserver-1] libGL error: No matching fbConfigs or visuals found [gzserver-1] libGL error: failed to load driver: swrast but it still does not work. We tried reinstalling NVIDIA drivers, found a bunch of posts about some Mesa drivers that give this error, but i checked and i don't have Mesa under my driver files. This is the forum: https://askubuntu.com/questions/834254/steam-libgl-error-no-matching-fbconfigs-or-visuals-found-libgl-error-failed-t And this is what i get after running sudo ldconfig -p | grep -i gl.so : libwayland-egl.so.1 (libc6,x86-64) =&gt; /lib/x86_64-linux-gnu/libwayland-egl.so.1 libcogl.so.20 (libc6,x86-64) =&gt; /lib/x86_64-linux-gnu/libcogl.so.20 libQt5OpenGL.so.5 (libc6,x86-64) =&gt; /lib/x86_64-linux-gnu/libQt5OpenGL.so.5 libQt5OpenGL.so (libc6,x86-64) =&gt; /lib/x86_64-linux-gnu/libQt5OpenGL.so libOpenGL.so.0 (libc6,x86-64) =&gt; /lib/x86_64-linux-gnu/libOpenGL.so.0 libOpenGL.so.0 (libc6) =&gt; /lib/i386-linux-gnu/libOpenGL.so.0 libOpenGL.so (libc6,x86-64) =&gt; /lib/x86_64-linux-gnu/libOpenGL.so libGL.so.1 (libc6,x86-64) =&gt; /lib/x86_64-linux-gnu/libGL.so.1 libGL.so.1 (libc6) =&gt; /lib/i386-linux-gnu/libGL.so.1 libGL.so (libc6,x86-64) =&gt; /lib/x86_64-linux-gnu/libGL.so libEGL.so.1 (libc6,x86-64) =&gt; /lib/x86_64-linux-gnu/libEGL.so.1 libEGL.so.1 (libc6) =&gt; /lib/i386-linux-gnu/libEGL.so.1 libEGL.so (libc6,x86-64) =&gt; /lib/x86_64-linux-gnu/libEGL.so We also tought maybe we're missing openGL or something similar inside the Dockerfile so we tried to get Hardware acceleration to work, following instructions here: http://wiki.ros.org/docker/Tutorials/Hardware%20Acceleration and so we have tried installing nvidia-docker2. If we run nvidia-docker version we get: NVIDIA Docker: 2.8.0 Client: Docker Engine - Community Version: 20.10.12 API version: 1.41 Go version: go1.16.12 Git commit: e91ed57 Built: Mon Dec 13 11:45:33 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 20.10.12 API version: 1.41 (minimum version 1.12) Go version: go1.16.12 Git commit: 459d0df Built: Mon Dec 13 11:43:42 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.12 GitCommit: 7b11cfaabd73bb80907dd23182b9347b4245eb5d runc: Version: 1.0.2 GitCommit: v1.0.2-0-g52b36a2 docker-init: Version: 0.19.0 GitCommit: de40ad0 We followed the instructions provided here: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html to set the environmental variables we need to control which libraries will be mounted in the container. Since our error seems to indicate that OpenGL is missing I added ENV NVIDIA_VISIBLE_DEVICES all ENV NVIDIA_DRIVER_CAPABILITIES graphics, display to the first Dockerfile. graphics is required for openGL and display for x11 display (which I think we use since something about it is written in the original Dockerfile, but I'm not sure about it). After all this and some more, we still get those errors.",
        "answers": [],
        "votes": []
    },
    {
        "question": "When building a container (through docker build, docker run or docker-compose) using NVIDIA containers, I get the following error, somewhat randomly. When I usually start building the container, it works the first time. But next few times usually don't work. ------ &gt; [internal] load metadata for nvcr.io/nvidia/l4t-base:r32.5.0: ------ failed to solve: rpc error: code = Unknown desc = failed to solve with frontend dockerfile.v0: failed to create LLB definition: failed to authorize: rpc error: code = Unknown desc = failed to fetch anonymous token: unexpected status: 401 Unauthorized My current workaround is to use a slightly different image temporarily whilst I\u2019m adjusting other parts of the Dockerfile.",
        "answers": [
            [
                "You have to docker login nvcr.io Most of this is documented in the setup docs, unfortunately nvcr.io doesn't have very good SEO or UX, so search results and most attempts don't show this page. This is, confusingly, different to ngc catalog / docker login ngc.nvidia.com. Install NGC CLI: (for macOS) Run curl -O https://ngc.nvidia.com/downloads/ngccli_mac.zip &amp;&amp; unzip ngccli_mac.zip &amp;&amp; chmod u+x ngc mv ngc /usr/local/bin/ngc Setup your environment: Create an account and get your API key from https://ngc.nvidia.com/ Add the API key to the CLI: ngc config set Login to registry: docker login nvcr.io Observe docker CLI output containing correct auth details: [auth] nvidia/tensorrt:pull,push token for nvcr.io Tips Make sure you login to the right container registry. It's in the name of the docker image you are using. e.g. nvcr.io/nvidia/tensorrt:22.01-py3 uses nvcr.io. Explanation With the benefit of hindsight, I understood failed to fetch anonymous token: unexpected status: 401 Unauthorized. I was not authenticated (or more correctly, I was authenticated to Docker Hub and NGC Catalog instead of nvcr.io). It was giving me an anonymous token, which hit the rate limit. I needed to create an account to get higher limits."
            ],
            [
                "Restarting docker server solved the issue sudo systemctl restart docker"
            ],
            [
                "If you happen to get this error and it has worked before, you probably have run into a download limit from nvcr. To circumvent it, it suffices to just restart docker. For Docker desktop click on the docker icon in the taskbar and click restart. Or on Mac your can use cmd + r."
            ],
            [
                "Updating Docker from my version (4.5.0) to the latest version (4.9.0) helped me."
            ]
        ],
        "votes": [
            27.0000001,
            6.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm trying to deploy on google cloud. So, my Dockerfile : FROM ubuntu:20.04 RUN apt update RUN apt -y install sudo RUN apt -y install curl RUN sudo apt install -y build-essential RUN curl -O https://storage.googleapis.com/nvidia-drivers-us-public/GRID/GRID13.0/NVIDIA-Linux-x86_64-470.63.01-grid.run RUN chmod +x NVIDIA-Linux-x86_64-470.63.01-grid.run RUN sudo /bin/bash NVIDIA-Linux-x86_64-470.63.01-grid.run FROM python:3.8 RUN adduser meat RUN passwd -d meat USER meat WORKDIR /home/meat RUN python3 -m venv meat-env RUN /bin/bash -c \"source meat-env/bin/activate\" RUN /usr/local/bin/python -m pip install --upgrade pip COPY requirements.txt . RUN pip3 install -r requirements.txt COPY . . CMD [\"python\", \"main.py\"] and I got this error: Step 8/20 : RUN sudo /bin/bash NVIDIA-Linux-x86_64-470.63.01-grid.run ---&gt; Running in 811998f9cea8 Verifying archive integrity... OK Uncompressing NVIDIA Accelerated Graphics Driver for Linux-x86_64 470.63.01 ............... and several dots(.) later [91mError opening terminal: unknown. [0m unable to stream build output: The command '/bin/sh -c sudo /bin/bash NVIDIA-Linux-x86_64-470.63.01-grid.run' returned a non-zero code: 1 Failed to build the app. Error: unable to stream build output: The command '/bin/sh -c sudo /bin/bash NVIDIA-Linux-x86_64-470.63.01-grid.run' returned a non-zero code: 1 I made something wrong with my Dockerfile? Or is there a differente way to do the command?",
        "answers": [
            [
                "@DazWilkin's comment is correct: It's not possible to install graphics driver to Cloud Run fully managed because it doesn't expose any GPU. You should deploy it in Cloud Run Anthos(GKE). But you'll need to configure your GKE Cluster to install your GPU, According to the documentation, you should follow the steps: Add a GPU-enabled node pool to your GKE cluster. In this step, you can enable Enable Virtual Workstation (NVIDIA GRID). Please choose a GPU such as NVIDIA Tesla T4, P4 or P100 to enable NVIDIA GRID. Install NVIDIA's device drivers to the nodes. You can now create a service that will consume GPUs and deploy the image to Cloud Run for Anthos: Setting up your service to consume GPUs"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to build a docker image where I want to compile custom kernels with pytorch. Therefore I need access to the available gpus in order to compile the custom kernels during docker build process. On the host machine everything is setted up including nvidia-container-runtime, nvidia-docker, Nvidia-Drivers, Cuda etc. The following command shows docker runtime information on the host system: $ docker info|grep -i runtime Runtimes: nvidia runc Default Runtime: runc As you can see the default runtime of docker in my case is runc. I think changing the default runtime from runc to nvidia would solve this problem, as noted here. The proposed solution doesn't work in my case because: I have no permissions to change the default runtime on system I use I have no permissions to make changes to the daemon.json file Is there a way to get access to the gpus during the build process in the Dockerfile in order to compile custom pytorch kernels for CPU and GPU (in my case DCNv2)? Here is the minimal example of my Dockerfile to reproduce this problem. In this image, DCNv2 is only compiled for CPU and not for GPU. FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04 RUN apt-get update &amp;&amp; \\ DEBIAN_FRONTEND=noninteractive apt-get install -y tzdata &amp;&amp; \\ apt-get install -y --no-install-recommends software-properties-common &amp;&amp; \\ add-apt-repository ppa:deadsnakes/ppa &amp;&amp; \\ apt update &amp;&amp; \\ apt install -y --no-install-recommends python3.6 &amp;&amp; \\ apt-get install -y --no-install-recommends \\ build-essential \\ python3.6-dev \\ python3-pip \\ python3.6-tk \\ pkg-config \\ software-properties-common \\ git RUN ln -s /usr/bin/python3 /usr/bin/python &amp; \\ ln -s /usr/bin/pip3 /usr/bin/pip RUN python -m pip install --no-cache-dir --upgrade pip setuptools &amp;&amp; \\ python -m pip install --no-cache-dir torch==1.4.0 torchvision==0.5.0 RUN git clone https://github.com/CharlesShang/DCNv2/ #Compile DCNv2 WORKDIR /DCNv2 RUN bash ./make.sh # clean up RUN apt-get clean &amp;&amp; \\ rm -rf /var/lib/apt/lists/* #Build: docker build -t my_image . #Run: docker run -it my_image An not opitmal solution which worked would be be the following: Comment out line RUN bash ./make.sh in Dockerfile Build image: docker build -t my_image . Run image in interactive mode: docker run --gpus all -it my_image Compile DCNv2 manually: root@1cd02fd62461:/DCNv2# ./make.sh Here DCNv2 is compiled for CPU and GPU, but that seems to me not an ideal solution, because I must compile DCNv2 every time when i start the container.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to run tensorflow with gpu support in a docker on a virtual machine. I have tried lots of online solutions including: tried different docker images of versions of tensorflow: 2.6, 2.4, 1.15, 1.14 built tensorflow from source inside the container based on this guide several times with different bazel flags https://www.tensorflow.org/install/source for 2.6 and 1.14 tried to make the GPU visible by these kinds of commands:TensorFlow : failed call to cuInit: CUDA_ERROR_NO_DEVICE used nvidia tensorflow docker none of the solutions work for me, here some steps: I verified that drivers and cuda and cudnn toolkit are installed inside the container using nvidia-smi and nvcc -V: Python version is : Python 3.8.10 and tensorflow version is: import tensorflow as tf tf.__version__ '2.6.0' The error appears with: tf.config.list_physical_devices() So the GPU is somehow not visible to the tensorflow. All tensorflow builds return the same error: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error but for example for 1.14 there is an additional comment regarding the CPU type: Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA The GPU is a A100 and the CPU is Intel(R) Xeon(R) Gold 6226R. What is going on here? How do I fix this?",
        "answers": [
            [
                "I realized that the GPU has a multi-instance feature: Therefore, the GPU instances should be configured: sudo nvidia-smi mig -cgi 0 -C and afterwards when calling nvidia-smi you get: And the problem is solved!"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "While running a distributed training on 4 A6000 GPUs, I get the following error: [E ProcessGroupNCCL.cpp:630] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(OpType=BROADCAST, Timeout(ms)=1800000) ran for 1803710 milliseconds before timing out. [E ProcessGroupNCCL.cpp:390] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down. terminate called after throwing an instance of 'std::runtime_error' what(): [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(OpType=BROADCAST, Timeout(ms)=1800000) ran for 1804406 milliseconds before timing out. [E ProcessGroupNCCL.cpp:390] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down. I use standard NVidia PyTorch docker. Interesting thing is that training works fine for small datasets but for the bigger datasets, I get this error. So I can confirm that the training code is correct and does work. There is no actual runtime error or any other information to get an actual error messages anywhere.",
        "answers": [
            [
                "Following two have solved the issue: Increase default SHM (shared memory) for CUDA to 10g (I think 1g would have worked as well). You can do this in docker run command by passing --shm-size=10g. I also pass --ulimit memlock=-1. export NCCL_P2P_LEVEL=NVL. Debugging Tips To check current SHM, df -h # see the row for shm To see NCCL debug messages: export NCCL_DEBUG=INFO Run p2p bandwidth test for GPU to GPU communication link: cd /usr/local/cuda/samples/1_Utilities/p2pBandwidthLatencyTest sudo make ./p2pBandwidthLatencyTest For A6000 4 GPU box this prints: The matrix shows bandwith betweeb each pair of GPU and with P2P, it should be high."
            ],
            [
                "https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group set timeout param in torch.distributed.init_process_group(), default is 30 mins torch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=- 1, rank=- 1, store=None, group_name='', pg_options=None)"
            ],
            [
                "When I set NCCL_DEBUG=INFO, the output said, \"Could not enable P2P between dev 2(=d5000) and dev 1(=d1000) ...\". So, I set the environment variable NCCL_P2P_DISABLE=1, this solved my problem."
            ],
            [
                "For me, the problem turned out to be the torchrun command for PyTorch 1.10.1. I only needed to switch to the python -m torch.distributed.launch command and everything worked. I spent many hours on the StackOverflow and the PyTorch Forum but no one mentioned this solution, so I'm sharing it to save people time. torchrun seems to be working fine for PyTorch 1.11 and above."
            ]
        ],
        "votes": [
            8.0000001,
            7.0000001,
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "I have to bundle up my system using Docker. But my system uses Java (JAR file to run) and python with PyTorch. I initially tried to use openjdk:buster base Docker image and then installed python3 on top of it. So both JAR and PyTorch worked, but PyTorch is only CPU supportive. But now I have to speed-up my PyTorch code using GPU, and for that I need NVIDIA-Cuda. In a separate Docker, I found nvidia/cuda:10.2-base-ubuntu18.04 works for my PyTorch. But this Docker can't run JAR file. So I am stuck in combining these 2. I either want to install NVIDIA-Cuda dependencies to openjdk Docker base image install openjdk (openjdk-14) dependencies to NVIDIA-Cuda Docker base image Anyone has any suggestions on how I can do that or any alternative hacks ?",
        "answers": [
            [
                "You can have a single image, instead of two by creating your own docker image that uses the nvidia image, and install java on it. I.e. have a Dockerfile as below FROM nvidia/cuda:10.2-base-ubuntu18.04 RUN apt-get update RUN apt-get install openjdk-14-jdk COPY &lt;your jar file&gt; &lt;a path&gt; CMD [ \"java\" \"other java flags/args&gt;\" \"-jar\" \"&lt;path to your jar file&gt;\"] run docker build on that Dockerfile, and docker run as you normally would, and your java code should have access to NVIDIA-Cuda. (Also note, some prefer ENTRYPOINT to CMD)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I installed nvidia-docker and to test my installation, I ran docker run --rm --gpus all nvidia/cuda:10.0-base nvidia-smi. I get this +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Quadro T2000 wi... On | 00000000:01:00.0 On | N/A | | N/A 46C P0 10W / N/A | 2294MiB / 3911MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ The driver version and CUDA version are exactly the same as what I get when I run nvidia-smi outside the container in my regular terminal. My understanding of why the driver version is the same is that device drivers are hardware specific, and thus aren't installed inside the container, and the reason why nvidia-docker exists is to allow software running inside the container to talk to the device drivers. Is this correct? My main point of confusion is why the CUDA version is reported as 11.4 from inside the container. When I launch a bash terminal inside this container and look at the CUDA installation in /usr/local, I only see version 10.0, so why is nvidia-smi inside the container giving me CUDA version installed on my host system? I believe these questions display a fundamental misunderstanding either of how nvidia-smi works, or how nvidia-docker works, so could someone point me towards resources that might help me resolve this misunderstanding?",
        "answers": [
            [
                "You can't have more than 1 GPU driver operational in this setting. Period. That driver is installed in the base machine. If you do something not recommended, like install it or attempt to install it in the container, it is still the one in the base machine that is in effect for the base machine as well as the container. Note that anything reported by nvidia-smi pertains to the GPU driver only, and therefore is using the driver installed in the base machine, whether you run it inside or outside of the container. There may be detailed reporting differences like visible GPUs, but this doesn't impact versions reported. The CUDA runtime version will be the one that is installed in the container. Period. It has no ability to inspect what is outside the container. If it happens to match what you see outside the container, then it is simply the case that you have the same configuration outside the container as well as inside. Probably most of your confusion would be resolved with this answer and perhaps your question is a duplicate of that one."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "Hi I'm having an issue with setting up docker for a deep learning project. Im new to deep learning actually and setting it up. I realise that it works in Linux so I had to download a WSL so I downloaded ubuntu 20.04. So I want to restart the service with systemd with the systemctl command. I'm setting up the NVIDIA Container ToolKit btw. So I type the following command thinking it can work like that. $ sudo systemctl restart docker So that's the first problem I had, the 2nd is I want to try and test whether this WSL can use my GPU and I typed the following command: $ docker run --rm --gpus all nvidia/cuda:10.4-base nvidia-smi But this was what I get back; been searching the whole internet cant find an answer that's specific to the one I had. docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: Running hook #0:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown. Much help would be appreciated, I'm actually trying to access Alphafold2 with my own PC so this is my first time using this deep learning software and I'm having trouble with the initial installation of the requirements, been trying to solve for hours past 12am. This is important for my Final year project so I hope I can find some answers here. :) thanks for reply in advance.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I set up CUDA on WSL2 Ubuntu 20.04 and am able to successfully run commands like: docker run --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark and docker run -it --gpus all -p 8888:8888 tensorflow/tensorflow:latest-gpu-py3-jupyter but docker run --gpus all --rm nvidia/cuda:10.0-runtime nvidia-smi gives me this error and I do not have a good mental model of how docker works: docker: Error response from daemon: OCI runtime create failed: container_linux.go:367: starting container process caused: exec: \"nvidia-smi\": executable file not found in $PATH: unknown. This command works outside of docker. nvidia-smi",
        "answers": [
            [
                "Whether nvidia-smi works outside of Docker is irrelevant. The error message is telling you that the image nvidia/cuda:10.0-runtime does not have nvidia-smi on the $PATH, which probably means it doesn't have it installed at all. If the nvidia-smi executable is on the image but not on the $PATH, then you just need to provide the absolute path to the executable. If the executable is not on the image, then you need to use a different image which does have nvidia-smi on the $PATH, either by extending nvidia/cuda:10.0-runtime via a Dockerfile or by using a different image. (Since nvidia-smi is really for development and sysadmin purposes, it doesn't surprise me that something labelled runtime is missing it.)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I found that when I pull NVIDIA Docker from here with CUDA&gt;=11.0 and Ubuntu&gt;=18.04, it does not comes with Nsight Compute (ncu) for kernel profiling. It only comes with nvprof would not work if I am profiling on an A100 or RTX3090. Is there any way to solve this problem? For example, install ncu separately?",
        "answers": [
            [
                "For example, install ncu separately? You can install Nsight Compute by itself. See here."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to deploy GPU workload into my swarm cluster. My service manifest looks like this. nvidia: image: \"tensorflow/tensorflow:devel-gpu\" volumes: - type: bind source: ./ubuntu_gpu target: /app consistency: cached build: context: ./ubuntu_gpu ports: - 6186:80 working_dir: \"/app\" runtime: nvidia environment: - LD_LIBRARY_PATH=/usr/local/cuda-11.3/targets/x86_64-linux/lib deploy: resources: reservations: devices: - driver: nvidia capabilities: [gpu] The approach described above was taken from docker official documentation on GPU support https://docs.docker.com/compose/gpu-support/ When I deploy the stack, I get this error below. Additional property devices is not allowed It seems like docker swarm does not have visibility of the GPU device attached to the worker node. Does anyone know if this feature works only docker-compose but not docker-swarm or if there is a work around this ?",
        "answers": [
            [
                "At the time of writing, docker swarmkit does not seem to support devices, see this issue on the docker github, there is quite the longstanding discussion around it."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to run a container based on python:3.8.8-slim-buster that needs access to the GPU. When I build it from this Dockerfile: FROM python:3.8.8-slim-buster CMD [\"sleep\", \"infinity\"] and then run it with \"--gpus all\" flag and exec nvidia-smi i get a proper response: Sat Jun 19 12:26:57 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 465.27 Driver Version: 465.27 CUDA Version: 11.3 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:01:00.0 Off | N/A | | N/A 45C P8 N/A / N/A | 301MiB / 1878MiB | 14% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ and when I use this docker-compose: services: test: image: tensorflow/tensorflow:2.5.0-gpu command: sleep infinity deploy: resources: reservations: devices: - capabilities: [gpu] and exec nvidia-smi after running it i get the same response. But when i replace the image in the docker-compose to python:3.8.8-slim-buster like in the Dockerfile, i get this response: OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \"nvidia-smi\": executable file not found in $PATH: unknown I appreciate any help figuring this out.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to use the base images provided by NVIDIA that let us use their GPUs via Docker containers. Because I am using docker, there is no need for me to have CUDA Toolkit or CuDNN on my system. All I need to have is the right driver - which I have. I can run the official pytorch docker containers and the containers utilize my GPU. However when I run anything using the base images from NVIDIA then I get the following Warning - $ docker run --gpus all -it --rm -p 8000:8000 ubuntu-cuda-gpu:latest /usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.) return torch._C._cuda_getDeviceCount() &gt; 0 The application executes, it just uses CPU. But I want to be able to use my GPU like I can when I run the same code(it is a simple pytorch example) using official pytorch docker images. The base image used is - FROM nvidia/cuda:11.3.1-cudnn8-runtime-ubuntu20.04 # Setup RUN apt update &amp;&amp; \\ apt install -y bash \\ build-essential \\ git \\ curl \\ ca-certificates \\ python3 \\ python3-pip &amp;&amp; \\ rm -rf /var/lib/apt/lists # Your stuff RUN python3 -m pip install --no-cache-dir --upgrade pip &amp;&amp; \\ python3 -m pip install --no-cache-dir \\ torch \\ transformers \\ ... If I just run the image without any machine learning code and try to execute nvidia-smi then I get the output as - $ docker run --gpus all -it --rm -p 8000:8000 ubuntu-cuda-gpu:latest nvidia-smi Sat Jun 12 19:15:21 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.80 Driver Version: 460.80 CUDA Version: 11.3 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce RTX 3060 Off | 00000000:01:00.0 Off | N/A | | 0% 31C P8 9W / 170W | 14MiB / 12053MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ This leads me to believe that at least something is right. But why is it that I am not able to use my GPU and how to make sure that I can? I am on Ubuntu 20.04.",
        "answers": [],
        "votes": []
    },
    {
        "question": "So my workflow is gonna be a bit wonky, but I'm not permitted to use Docker, so I have Singularity instead. I'm running some code that is giving me this error: RuntimeError: nvrtc: error: failed to open libnvrtc-builtins.so.11.1. Make sure that libnvrtc-builtins.so.11.1 is installed correctly. nvrtc compilation failed: If more details are needed, I can provide. I'm operating in a Singularity container built from a docker_hub image I created here: https://hub.docker.com/repository/docker/elavizadeh/imaginaire_cuda_10.2/builds (Ignore the fact that the title says cuda_10.2, I'll fix this all up if and when this works.) Which is based on this dockerfile I created here: https://github.com/EvanLavizadeh/imaginaire-1/blob/master/Dockerfile In that Dockerfile, I specify a base image \"FROM nvcr.io/nvidia/pytorch:20.12-py3\" https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_20-12.html#rel_20-12 I specifically chose that version of that base image because it claims it has Cuda toolkit 11.1.1 which I am hoping will contain libnvrtc-builtins.so.11.1. But when I spin up my container, cuda reports version 11.0.221: Singularity&gt; nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2020 NVIDIA Corporation Built on Wed_Jul_22_19:09:09_PDT_2020 Cuda compilation tools, release 11.0, V11.0.221 Build cuda_11.0_bu.TC445_37.28845127_0 Additionally, when I look at the files in /usr/local/cuda/lib64, I see libnvrtc-builtins.so.11.0 but not libnvrtc-builtins.so.11.1 I believe this is the source of my error but I don't know how to correct it. I either need my code to use libnvrtc-builtins.so.11.0 instead of libnvrtc-builtins.so.11.1 or I need my container to build Cuda 11.1 and hopefully generate libnvrtc-builtins.so.11.0.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm building a custom rapidsai docker image based on its devel image. Here is the docker file. FROM rapidsai/rapidsai-dev:0.19-cuda11.0-devel-ubuntu18.04-py3.7 # Defining working directory and adding source code WORKDIR /usr/src/app RUN echo \"Make sure cuml is installed:\" RUN python -c \"import cuml\" But when I built it with this command, nvidia-docker build . -t test it returns an error saying: Step 4/4 : RUN python -c \"import cuml\" ---&gt; Running in 553d12bf7e68 Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; ModuleNotFoundError: No module named 'cuml' It seems that it can't recognize cuml library which is already a part of the libraries of the base image. Why it can't import it?",
        "answers": [
            [
                "Just fixed it. This will work without any issue FROM rapidsai/rapidsai-dev:0.19-cuda11.0-devel-ubuntu18.04-py3.7 # Defining working directory and adding source code WORKDIR /usr/src/app RUN echo \"conda activate rapids\" &gt;&gt; ~/.bashrc SHELL [\"/bin/bash\", \"--login\", \"-c\"] then install some libraries..and RUN export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/libcuda.so.1:${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} RUN export LD_LIBRARY_PATH=/usr/local/cuda-11.0/compat/libcuda.so.1:${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} COPY test_cuml.py . RUN echo \"conda activate rapids\" &gt;&gt; ~/.bashrc SHELL [\"/bin/bash\", \"--login\", \"-c\"] ENTRYPOINT [\"/opt/conda/envs/rapids/bin/python\", \"test_cuml.py\"]"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to run the clara train example, but when I execute the startClaraTrainNoteBooks.sh, the container cannot find the nvidia driver. I already know that the script executes docker-compose.yml. So I tested whether docker-compose can found the nvidia driver: services: test: image: nvidia/cuda:10.2-base command: nvidia-smi deploy: resources: reservations: devices: - driver: nvidia capabilities: [gpu] device_ids: ['0'] Output: USER@test:~$ docker-compose up WARNING: Found orphan containers (hp_nvsmi_1) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up. Starting hp_test_1 ... done Attaching to hp_test_1 test_1 | Mon Jun 7 09:01:44 2021 test_1 | +-----------------------------------------------------------------------------+ test_1 | | NVIDIA-SMI 460.27.04 Driver Version: 460.27.04 CUDA Version: 11.2 | test_1 | |-------------------------------+----------------------+----------------------+ test_1 | | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | test_1 | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | test_1 | | | | MIG M. | test_1 | |===============================+======================+======================| test_1 | | 0 GeForce RTX 206... Off | 00000000:01:00.0 Off | N/A | test_1 | | 0% 34C P8 17W / 215W | 100MiB / 7979MiB | 0% Default | test_1 | | | | N/A | test_1 | +-------------------------------+----------------------+----------------------+ test_1 | test_1 | +-----------------------------------------------------------------------------+ test_1 | | Processes: | test_1 | | GPU GI CI PID Type Process name GPU Memory | test_1 | | ID ID Usage | test_1 | |=============================================================================| test_1 | +-----------------------------------------------------------------------------+ hp_test_1 exited with code 0 But the startClaraTrainNoteBooks.sh cna not find it. root@claratrain:/claraDevDay# nvidia-smi root@claratrain:/claraDevDay# Actually, startDocker.sh can find the driver. root@c7c2d5597eb8:/claraDevDay# nvidia-smi Mon Jun 7 09:11:43 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.27.04 Driver Version: 460.27.04 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce RTX 206... Off | 00000000:01:00.0 Off | N/A | | 0% 35C P8 17W / 215W | 100MiB / 7979MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ root@c7c2d5597eb8:/claraDevDay# What should I do?",
        "answers": [
            [
                "The docker-compose.yml script need to rewrite like this and working: # SPDX-License-Identifier: Apache-2.0 version: \"3.8\" services: claratrain: container_name: claradevday-pt hostname: claratrain ##### use vanilla clara train docker #image: nvcr.io/nvidia/clara-train-sdk:v4.0 ##### to build image with GPU dashboard inside jupyter lab build: context: ./dockerWGPUDashboardPlugin/ # Project root dockerfile: ./Dockerfile # Relative to context image: clara-train-nvdashboard:v4.0 depends_on: - tritonserver ports: - \"3030:8888\" # Jupyter lab port - \"3031:5000\" # AIAA port ipc: host volumes: - ${TRAIN_DEV_DAY_ROOT}:/claraDevDay/ - /raid/users/aharouni/data:/data/ command: \"jupyter lab /claraDevDay --ip 0.0.0.0 --allow-root --no-browser --config /claraDevDay/scripts/jupyter_notebook_config.py\" # command: tail -f /dev/null # tty: true deploy: resources: reservations: devices: - driver: nvidia capabilities: [ gpu ] # To specify certain GPU uncomment line below #device_ids: ['0,3'] ############################################################# tritonserver: image: nvcr.io/nvidia/tritonserver:21.02-py3 container_name: aiaa-triton hostname: tritonserver restart: unless-stopped command: &gt; sh -c \"chmod 777 /triton_models &amp;&amp; /opt/tritonserver/bin/tritonserver \\ --model-store /triton_models \\ --model-control-mode=\"poll\" \\ --repository-poll-secs=5 \\ --log-verbose ${TRITON_VERBOSE}\" volumes: - ${TRAIN_DEV_DAY_ROOT}/AIAA/workspace/triton_models:/triton_models # shm_size: 1gb # ulimits: # memlock: -1 # stack: 67108864 # logging: # driver: json-file"
            ]
        ],
        "votes": [
            -0.9999999
        ]
    },
    {
        "question": "I am trying to use the docker image with new drivers &gt;460 for CUDA 11 but my host machine has cuda-10. According to the image below (https://github.com/NVIDIA/nvidia-docker) docker uses the drivers present in the host hence I am getting error as kernel mismatch required 450+ kernel version 418. I have installed the drivers on my container. I there any way to use the bare metal gpus from the container itself and bypass the host drivers??",
        "answers": [],
        "votes": []
    },
    {
        "question": "Initial: I'm using Windows 10, running docker command on the powershell. My Local disk space is low but I'm running this on my hard disk (D:). Command line: PS D:\\Biop\\CLARA_method&gt; docker pull nvcr.io/nvidia/clara-train-sdk:v3.1.01 v3.1.01: Pulling from nvidia/clara-train-sdk 7595c8c21622: Pulling fs layer d13af8ca898f: Pulling fs layer **{... ... ... ... more of the same codes and either \"Pulling fs layer\" or \"Waiting\"}** c18477ba8094: Waiting b97dab84d0d6: Waiting 6bb9b0853045: Pulling fs layer 2c75b980ef51: Pulling fs layer f41e15600a89: Pulling fs layer f7d0f7f7c600: Waiting 44057ff905c4: Pulling fs layer ef24ea424719: Waiting 26f07601bfd2: Waiting open /var/lib/docker/tmp/GetImageBlob439951377: read-only file system Effect: There's no file at the file location mentioned, I don't even know if anything downloaded at all. Please let me know if more information is needed this is my first time on StackOverflow",
        "answers": [],
        "votes": []
    },
    {
        "question": "Docker daemon.json change the default runtime to nvidia and add the following. { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } }, } This will include gpu information in the docker that is started by default. Will this approach interfere with dockers that don't need to run with gpu? Will gpu information be saved when a docker that does not require gpu runs?",
        "answers": [
            [
                "What nvidia-container-runtime does to a container depends on environment variables set in that container. The list of affecting environment variables and their values can be found here but I want to mention this one specifically: NVIDIA_VISIBLE_DEVICES Possible values void or empty or unset: nvidia-container-runtime will have the same behavior as runc. So if your container has no NVIDIA_VISIBLE_DEVICES environment variable, nvidia-container-runtime should work as runc (the standard way for Docker to launch containers)."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to install Jarvis using this instructions: https://ngc.nvidia.com/catalog/collections/nvidia:jarvis . Unfortunately when I run jarvis_init.sh I get following error: docker: Error response from daemon: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #0:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown. Error in downloading models. I have tried restarting docker and PC, but it didn't help. I am using Docker Desktop with WSL2 Ubuntu 20.04. Any help or advice will be much appreciated. Thank you in advance.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am running a nvidia-docker container with the initial flag --gpus=\"device=0\" and now I would like to add the second device to the running container. Is there a way to do that?",
        "answers": [
            [
                "According to this, it's not possible. A possible workaround: I ran into a similar situation, where I had set up a development environment, but I forgot to add the --gpus all option. Since I didn't want to lose my work, my workaround was to commit the container to an image using docker commit &lt;running_container&gt; &lt;image_name&gt; and then run the new image with the --gpus all option docker run -it [...] --gpus all --name &lt;new_container_name&gt; &lt;image_name&gt;"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have an Nvidia driver v:396, and I can't update it as another application running inside docker depends on it. So, I used this repo https://github.com/SmileTM/Tensorflow2.X-GPU-CUDA9.0 to install tf2 inside docker container nvidia/cuda:9.0-cudnn7-devel But when I install the tensorflow and try to run tf.test.is_gpu_available() I get the following output: &gt;&gt;&gt; tf.test.is_gpu_available() WARNING:tensorflow:From &lt;stdin&gt;:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.config.list_physical_devices('GPU')` instead. 2021-05-02 17:55:39.369149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1 2021-05-02 17:55:40.073345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:05:00.0 name: Quadro P4000 computeCapability: 6.1 coreClock: 1.48GHz coreCount: 14 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 226.62GiB/s 2021-05-02 17:55:40.073783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.9.0 2021-05-02 17:55:40.075887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.9.0 2021-05-02 17:55:40.077755: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.9.0 2021-05-02 17:55:40.107478: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.9.0 2021-05-02 17:55:40.108647: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.9.0'; dlerror: /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusolver.so.9.0: undefined symbol: GOMP_critical_end; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 2021-05-02 17:55:40.110743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.9.0 2021-05-02 17:55:40.116016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2021-05-02 17:55:40.116046: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... 2021-05-02 17:55:40.384507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix: 2021-05-02 17:55:40.384578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 2021-05-02 17:55:40.384595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N False",
        "answers": [
            [
                "import ctypes ctypes.CDLL(\"libgomp.so.1\", mode=ctypes.RTLD_GLOBAL) Now, tf.test.is_gpu_available() returns True."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am running a Tensorflow model on an AKS cluster with GPU nodes. The model currently runs in a single TF Serving container (https://hub.docker.com/r/tensorflow/serving) in a single pod on a single GPU node. By default the TF serving container will claim all available RAM in the pod, but I can downscale the memory request by the container in my deployment.yaml file and still get the same results in acceptable processing time. I was wondering if there is any possibility to run two TF models in parallel on the same GPU node. Memory-wise it should work, but when I try to adapt the replicaset of my deployment to two, it tries to deploy two pods but the second one is hanging on the status pending. $ kubectl get po -n myproject -w NAME READY STATUS RESTARTS AGE myproject-deployment-cb7769df4-ljcfc 1/1 Running 0 2m myproject-deployment-cb7769df4-np9qd 0/1 Pending 0 26s If I describe the pod I get the following error $ kubectl describe po -n myproject myproject-deployment-cb7769df4-np9qd Name: myproject-deployment-cb7769df4-np9qd Namespace: myproject &lt;...&gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 105s default-scheduler 0/1 nodes are available: 1 Insufficient nvidia.com/gpu. Since the first pod 'claims' the GPU, the second one cannot use it anymore and remains in status pending. I see two different possiblities: Run two TF serving containers in one pod on one GPU node Run two pods, each with one TF serving container on one GPU node Is any of the options above feasible? My deployment can be found below. apiVersion: apps/v1 kind: Deployment metadata: name: myproject-deployment labels: app: myproject-server namespace: myproject spec: replicas: 1 selector: matchLabels: app: myproject-server template: metadata: labels: app: myproject-server spec: containers: - name: server image: tensorflow/serving:2.3.0-gpu ports: - containerPort: 8500 volumeMounts: - name: azurestorage mountPath: /models resources: requests: memory: \"10Gi\" cpu: \"1\" limits: memory: \"12Gi\" cpu: \"2\" nvidia.com/gpu: 1 args: [\"--model_config_file=/models/models.config\", \"--monitoring_config_file=/models/monitoring.config\"] volumes: - name: azurestorage persistentVolumeClaim: claimName: pvcmodels",
        "answers": [
            [
                "Interesting question - as far as I know, this is not possible, also not for two containers running as the same pod (resources are configured on container level), at least not out of the box (see https://github.com/kubernetes/kubernetes/issues/52757) I found this while searching for an answer: https://blog.ml6.eu/a-guide-to-gpu-sharing-on-top-of-kubernetes-6097935ababf, but that involves tinkering with kubernetes itself. You could run multiple processes in the same container to achieve sharing, however this goes a bit against the idea of kubernetes/containers and of course won't work for 2 completely different workloads/services."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to create a GPU microservice using Nvidia cuda Base image, but during the docker build, I am facing Driver not found issue, can someone point out what is missing here? DockerFile: FROM nvidia/cuda:10.1-devel # Install some basic utilities RUN apt-get update &amp;&amp; apt-get install -y \\ curl \\ ca-certificates \\ sudo \\ git \\ bzip2 \\ libx11-6 \\ &amp;&amp; rm -rf /var/lib/apt/lists/* ENV CONDA_AUTO_UPDATE_CONDA=false ENV PATH=/home/user/miniconda/bin:$PATH RUN curl -sLo ~/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh \\ &amp;&amp; chmod +x ~/miniconda.sh \\ &amp;&amp; ~/miniconda.sh -b -p ~/miniconda \\ &amp;&amp; rm ~/miniconda.sh \\ &amp;&amp; conda install -y python==3.7 \\ &amp;&amp; conda clean -ya ENV PATH=\"/usr/local/cuda-10.1/bin:$PATH\" ENV LD_LIBRARY_PATH=\"/usr/local/cuda-10.1/lib64:$LD_LIBRARY_PATH\" ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility ENV NVIDIA_VISIBLE_DEVICES=all ENV FORCE_CUDA=\"1\" RUN conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=10.1 -c pytorch RUN pip install -v -e . Error: \"/home/user/miniconda/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1013, in _get_cuda_arch_flags capability = torch.cuda.get_device_capability() File \"/home/user/miniconda/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 320, in get_device_capability prop = get_device_properties(device) File \"/home/user/miniconda/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 325, in get_device_properties _lazy_init() # will define _get_device_properties and _CudaDeviceProperties File \"/home/user/miniconda/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 196, in _lazy_init _check_driver() File \"/home/user/miniconda/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 101, in _check_driver http://www.nvidia.com/Download/index.aspx\"\"\") AssertionError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx The issues happens during execution of last step in docker file. I tried using multiple Nvidia base docker images, but didn't help much. (cuda:10.1-base-ubuntu18.04, cuda:10.1-runtime-ubuntu18.04) Any pointers appreciated.",
        "answers": [
            [
                "After lot of trial and errors and going through a lot of documentation, this is what worked fine. ARG PYTORCH=1.3 ARG CUDA=10.1 ARG CUDNN=7 FROM pytorch/pytorch:1.3-cuda10.1-cudnn7-devel RUN mkdir /app WORKDIR /app ENV TORCH_CUDA_ARCH_LIST=\"5.2 6.0 6.1 7.0+PTX\" ENV TORCH_NVCC_FLAGS=\"-Xfatbin -compress-all\" ENV CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" RUN apt-get update &amp;&amp; apt-get install -y libglib2.0-0 libsm6 libxrender-dev libxext6 \\ &amp;&amp; apt-get clean \\ &amp;&amp; rm -rf /var/lib/apt/lists/* # Install some basic utilities RUN apt-get update &amp;&amp; apt-get install -y \\ curl \\ ca-certificates \\ sudo \\ git \\ bzip2 \\ libx11-6 \\ &amp;&amp; rm -rf /var/lib/apt/lists/* RUN apt-get update &amp;&amp; \\ apt-get install -y --no-install-recommends \\ build-essential g++ \\ libglib2.0-0 libsm6 libxrender-dev libxext6 wget # Create a non-root user and switch to it RUN adduser --disabled-password --gecos '' --shell /bin/bash user \\ &amp;&amp; chown -R user:user /app RUN echo \"user ALL=(ALL) NOPASSWD:ALL\" &gt; /etc/sudoers.d/90-user USER user # All users can use /home/user as their home directory ENV HOME=/home/user RUN chmod 777 /home/user # Install Miniconda and Python 3.7 ENV CONDA_AUTO_UPDATE_CONDA=false ENV PATH=/home/user/miniconda/bin:$PATH RUN curl -sLo ~/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh \\ &amp;&amp; chmod +x ~/miniconda.sh \\ &amp;&amp; ~/miniconda.sh -b -p ~/miniconda \\ &amp;&amp; rm ~/miniconda.sh \\ &amp;&amp; conda install -y python==3.7 \\ &amp;&amp; conda clean -ya RUN conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=10.1 -c pytorch RUN pip install -v -e . Hope this helps! Good luck!"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "We use pytorch in a nvidia/cuda:xx.x-base-ubuntu18.04 (base) image with a final size of ~6 GB. I know that nvcc is available in nvidia/cuda:xx.x-devel (devel) tag, but it increases the image size to &gt; 10 GB. Can I install nvcc only in the base image?",
        "answers": [
            [
                "nividia provided installers have command line options. this will allow to install selected components. (such as with or without graphics drivers, samples, docs, ...)"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a docker-compose file that looks like the following: version: \"3.9\" services: api: build: . ports: - \"5000\" deploy: resources: reservations: devices: - capabilities: [gpu] count: 1 When I run docker-compose up, this runs as intended, using the first GPU on the machine. However, if I run docker-compose up --scale api=2, I would expect each docker container to reserve one GPU on the host. The actual behaviour is that both containers receive the same GPU, meaning that they compete for resources. Additionally, I also get this behaviour if I have two containers specified in the docker-compose.yml, both with count: 1. If I manually specify device_ids for each container, it works. How can I make it so that each docker container reserves exclusive access to 1 GPU? Is this a bug or intended behaviour?",
        "answers": [
            [
                "The behavior of docker-compose when a scale is requested is to create additional containers as per the exact specification provided by the service. There are very few specification parameters that will vary during the creation of the additional containers and the devices which are part of the host_config set of parameters are copied without modifications. docker-compose is python project, so if this is important feature for you, you can try to implement it. The logic that drives the lifecycle of the services (creation, scaling, etc.) reside in compose/services.py."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "My docker run is failing because git complains that I didnt set a user config which I never needed for my older images. From git.mysite.com:user/project * branch dev -&gt; FETCH_HEAD *** Please tell me who you are. Run git config --global user.email \"you@example.com\" git config --global user.name \"Your Name\" to set your account's default identity. Omit --global to set the identity only in this repository. fatal: unable to auto-detect email address (got 'root@V_2_compute1.(none)') I have two Dockerfiles which are quite similar but behave differently. The first one with nvcr.io/nvidia/tensorflow:20.02-tf1-py3 as its source and the second one with nvcr.io/nvidia/tensorflow:20.12-tf2-py3. The first is a Ubuntu 18.04 and the second one Ubuntu 20.04. When inspecting the containers, they have different git versions (2.17.1 vs 2.25.1) but I'm not sure if this is the problem. Here are the two Dockerfile (simplified): # syntax=docker/dockerfile:experimental FROM nvcr.io/nvidia/tensorflow:20.02-tf1-py3 as intermediate ARG SSH_PRIVATE_KEY RUN apt-get update \\ &amp;&amp; export DEBIAN_FRONTEND=noninteractive \\ &amp;&amp; apt-get install -y git \\ &amp;&amp; mkdir /srv/username \\ &amp;&amp; cd /srv/username RUN mkdir -p -m 0600 ~/.ssh &amp;&amp; ssh-keyscan git.mysite.com &gt;&gt; ~/.ssh/known_hosts ADD \"https://www.random.org/cgi-bin/randbyte?nbytes=10&amp;format=h\" skipcache RUN --mount=type=ssh git clone -b master git@git.mysite.com:user/project.git /srv/username/project ENV PYTHONPATH \"${PYTHONPATH}:/srv/username/project\" CMD /usr/sbin/service ssh restart &amp;&amp; cd /srv/username/project &amp;&amp; git pull origin feature-branch &amp;&amp; git checkout feature-branch \\ &amp;&amp; /srv/username/project/script.py The problematic one: # syntax=docker/dockerfile:experimental FROM nvcr.io/nvidia/tensorflow:20.12-tf2-py3 as intermediate ARG SSH_PRIVATE_KEY RUN apt-get update \\ &amp;&amp; export DEBIAN_FRONTEND=noninteractive \\ &amp;&amp; apt-get install -y git \\ &amp;&amp; mkdir /srv/username \\ &amp;&amp; cd /srv/username RUN mkdir -p -m 0600 ~/.ssh &amp;&amp; ssh-keyscan git.mysite.com &gt;&gt; ~/.ssh/known_hosts ADD \"https://www.random.org/cgi-bin/randbyte?nbytes=10&amp;format=h\" skipcache RUN --mount=type=ssh git clone -b master git@git.mysite.com:user/project.git /srv/username/project ENV PYTHONPATH \"${PYTHONPATH}:/srv/username/project\" CMD /usr/sbin/service ssh restart &amp;&amp; cd /srv/username/project &amp;&amp; git pull origin dev &amp;&amp; git checkout dev \\ &amp;&amp; /srv/username/project/script.py Why do I now have to config a user in the latest ?",
        "answers": [
            [
                "I didn't find why the error occured but I found a solution to remove it. Instead of cloning master then pulling the branch, I directly clone the branch I want to use. The cloning line is now : RUN --mount=type=ssh git clone --single-branch -b dev git@git.mysite.com:user/project.git /srv/username/project Notice the use of -b dev with --single-branch to clone only the specified branch. I am still not sure why the error occured in one case but not in the other. It might be because --mount=type=ssh can't be used in last CMD but then it should have failed in both case."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to run this deep learn algorithm https://github.com/albarji/neural-style-docker that's containerized in docker pushing the image on minikube, I understand that Docker isolate the container from the machine so you can't use your GPU, that's why you use nvidia-docker but I'm not sure if you can run nvdia-docker in minikube and I don't know how to make it work. What I'm trying to do is to run the algorithm through the minikube node to prove that the algorithm can work with a kubernetes cluster. I managed to make the algorithm work locally with docker following the documentation and I have installed NVIDIA drivers and working on Ubuntu 18.04",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using nvidia prebuilt docker container NVIDIA Release 20.12-tf2 to run my experiment. I am using TensorFlow Version 2.3.1. Currently, I am running my model on one of GPU, I still have 3 more idle GPUs so I intend to use my alternative experiment on any idle GPUs. Here is the output of nvidia-smi: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:6A:00.0 Off | 0 | | N/A 70C P0 71W / 70W | 14586MiB / 15109MiB | 100% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 Tesla T4 Off | 00000000:6B:00.0 Off | 0 | | N/A 39C P0 27W / 70W | 212MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 Tesla T4 Off | 00000000:6C:00.0 Off | 0 | | N/A 41C P0 28W / 70W | 212MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 Tesla T4 Off | 00000000:6D:00.0 Off | 0 | | N/A 41C P0 28W / 70W | 212MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ update: prebuilt -container: I'm using nvidia-prebuilt container as follow: docker run -ti --rm --gpus all --shm-size=1024m -v /home/hamilton/data:/data nvcr.io/nvidia/tensorflow:20.12-tf2-py3 To utilize idle GPU for my other experiments, I tried to add those in my python script: attempt-1 import tensorflow as tf devices = tf.config.experimental.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(devices[0], True) but this attempt gave me following error: raise ValueError(\"Memory growth cannot differ between GPU devices\") ValueError: Memory growth cannot differ between GPU devices I googled this error but none of them discussed on GitHub is not working for me. attempt-2 I also tried this: gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) but this attempt also gave me error like this: Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated. people discussed this error on github but still not able to get rid of error on my side. latest attempt: I also tried parallel training with TensorFlow and added those to my python script: device_type = \"GPU\" devices = tf.config.experimental.list_physical_devices(device_type) devices_names = [d.name.split(\"e:\")[1] for d in devices] strategy = tf.distribute.MirroredStrategy(devices=devices_names[:3]) with strategy.scope(): opt = Adam(learning_rate=0.1) model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) but this gave me also error and the program stopped. Can anyone help me how to automatically select idle GPUs for the training model in tensorflow? Does anyone know any workable approach? What's wrong with my above attempt? Any possible ideas to utilize idle GPUs while running the program on one of the GPUs? any thoughts?",
        "answers": [
            [
                "Thanks to @Hern\u00e1nAlarc\u00f3n suggestion, I tried like this and it worked like charm: docker run -ti --rm --gpus device=1,3 --shm-size=1024m -v /home/hamilton/data:/data nvcr.io/nvidia/tensorflow:20.12-tf2-py3 this may not be an elegant solution but it worked like charm. I am open to other possible remedies to fix this sort of problem."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Description of the issue Context information (for bug reports) Output of docker-compose version docker-compose version 1.17.1, build unknown docker-py version: 2.5.1 CPython version: 2.7.17 OpenSSL version: OpenSSL 1.1.1 11 Sep 2018 Output of docker version Client: Version: 19.03.6 API version: 1.40 Go version: go1.12.17 Git commit: 369ce74a3c Built: Fri Dec 18 12:21:44 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 19.03.6 API version: 1.40 (minimum version 1.12) Go version: go1.12.17 Git commit: 369ce74a3c Built: Thu Dec 10 13:23:49 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.3-0ubuntu1~18.04.4 GitCommit: runc: Version: spec: 1.0.1-dev GitCommit: docker-init: Version: 0.18.0 GitCommit: Output of docker-compose config (Make sure to add the relevant -f and other flags) ERROR: The Compose file './docker-compose.yml' is invalid because: services.testserver.deploy.resources.reservations value Additional properties are not allowed ('devices' was unexpected) Steps to reproduce the issue Creating a Dockerfile with a simple pull of nvidia cuda image and a command to check the nvidia-gpu FROM nvidia/cuda:10.2-base CMD nvidia-smi 2.Works like a charm when we build the image and run it without docker compose docker image build testserver/ -t testserverimage docker run --gpus all -exec -it testserverimage Shows the nvidia-gpu devices Sat Feb 20 13:10:46 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.80.02 Driver Version: 450.80.02 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00001918:00:00.0 Off | 0 | | N/A 52C P0 71W / 149W | 7897MiB / 11441MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ Now creating the docker-compose.yml version: \"3.5\" services: testserver: image: nvidia/cuda:10.2-base build: './modelserver' deploy: resources: reservations: devices: - capabilities: [gpu] driver: nvidia Observed result ERROR: The Compose file './docker-compose.yml' is invalid because: services.testserver.deploy.resources.reservations value Additional properties are not allowed ('devices' was unexpected) Expected result Sat Feb 20 13:10:46 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.80.02 Driver Version: 450.80.02 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00001918:00:00.0 Off | 0 | | N/A 52C P0 71W / 149W | 7897MiB / 11441MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ Stacktrace / full error message ERROR: The Compose file './docker-compose.yml' is invalid because: services.testserver.deploy.resources.reservations value Additional properties are not allowed ('devices' was unexpected) Additional information OS version / distribution, docker-compose install method, etc. OS Information: NAME=\"Ubuntu\" VERSION=\"18.04.5 LTS (Bionic Beaver)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 18.04.5 LTS\" VERSION_ID=\"18.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=bionic UBUNTU_CODENAME=bionic Docker compose installation: sudo apt install docker-compose",
        "answers": [
            [
                "In the documentation https://docs.docker.com/compose/gpu-support/#enabling-gpu-access-to-service-containers : Docker Compose v1.28.0+ allows to define GPU reservations using the device structure defined in the Compose Specification. Your docker-compose version is 1.17.1, so you need to upgrade your docker-compose to, at least, 1.28.0."
            ]
        ],
        "votes": [
            12.0000001
        ]
    },
    {
        "question": "So long story short: I had a working docker image where i could show graphical things through the Xserver. I tested it multiple times and everything seemed to work flawlessly. However after installing Nvidia and CUDA i started getting this error: Error setting socket option (IP_ADD_MEMBERSHIP). Error setting socket option (IP_ADD_MEMBERSHIP). libGL error: No matching fbConfigs or visuals found libGL error: failed to load driver: swrast X Error of failed request: GLXBadContext Major opcode of failed request: 151 (GLX) Minor opcode of failed request: 6 (X_GLXIsDirect) Serial number of failed request: 38 Current serial number in output stream: 37 libGL error: No matching fbConfigs or visuals found libGL error: failed to load driver: swrast X Error of failed request: GLXBadContext Major opcode of failed request: 151 (GLX) Minor opcode of failed request: 6 (X_GLXIsDirect) Serial number of failed request: 38 Current serial number in output stream: 37 Since i need CUDA for another project i can't just uninstall it. Currently i'm running these commands: docker build -t $container_name $repo_DIR/. docker run -it -d --net=host --env=\"DISPLAY\" --env=\"QT_X11_NO_MITSHM=1\" --name=$container_name -v $HOME/repos/$container_name/src/:\"/home/docker/catkin_ws/src_extern/\" --gpus all $container_name bash xhost +local:\"docker inspect --format='{{ .Config.Hostname }}' $container_name\" docker start $container_name docker exec --user docker -it $container_name bash I've tried different approaches from forums, but i couldn't find anything that worked or any post with my exact problem. Any suggestions? If possible it should also be able to run without cuda/nvidia since i need the image for my laptop too. Also it would be fine if it worked around cuda/nvidia since i don't need it for the docker image anyways. Some other information I'm Using Arch and CUDA-10.2 with nvidia-460 driver. The docker image runs on ubuntu:bionic and these are all the files including libGL: /usr/lib/x86_64-linux-gnu/libGLESv1_CM.so.1 /usr/lib/x86_64-linux-gnu/libGLESv2.so /usr/lib/x86_64-linux-gnu/libGLX_indirect.so.0 /usr/lib/x86_64-linux-gnu/libGLU.so.1.3.1 /usr/lib/x86_64-linux-gnu/libGLESv2.so.2 /usr/lib/x86_64-linux-gnu/libGLU.so.1 /usr/lib/x86_64-linux-gnu/libGLU.so /usr/lib/x86_64-linux-gnu/libGL.so /usr/lib/x86_64-linux-gnu/libGLU.a /usr/lib/x86_64-linux-gnu/libGLX_mesa.so.0 /usr/lib/x86_64-linux-gnu/libGLX.so /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/x86_64-linux-gnu/libGL.so.1.0.0 /usr/lib/x86_64-linux-gnu/libGLdispatch.so /usr/lib/x86_64-linux-gnu/libGLX.so.0.0.0 /usr/lib/x86_64-linux-gnu/libGLX.so.0 /usr/lib/x86_64-linux-gnu/libGLdispatch.so.0.0.0 /usr/lib/x86_64-linux-gnu/libGLX_mesa.so.0.0.0 /usr/lib/x86_64-linux-gnu/libGLESv1_CM.so.1.0.0 /usr/lib/x86_64-linux-gnu/libGLESv1_CM.so /usr/lib/x86_64-linux-gnu/libGLdispatch.so.0 /usr/lib/x86_64-linux-gnu/libGLESv2.so.2.0.0 EDIT 1: Solution, apparently it was only rviz and gazebo that was broken, and it was fixed by following this Edit 2: Changed to more relevant title.",
        "answers": [
            [
                "It is only gazebo and rviz that was broken after installing nvidia. I fiexed it by following this. The part i missed in my dockerfile was: # nvidia-container-runtime ENV NVIDIA_VISIBLE_DEVICES \\ ${NVIDIA_VISIBLE_DEVICES:-all} ENV NVIDIA_DRIVER_CAPABILITIES \\ ${NVIDIA_DRIVER_CAPABILITIES:+$NVIDIA_DRIVER_CAPABILITIES,}graphics"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to merge two docker images. Here is my Dockerfile FROM nvidia/cuda:10.0-devel-ubuntu18.04 AS cuda10 FROM osrf/ros:foxy-desktop COPY --from=cuda10 /usr/local/cuda-10.0 /usr/local/cuda-10.0 RUN cd /usr/local &amp;&amp; ln -s cuda-10.0 cuda COPY --from=cuda10 \\ /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.460.32.03 \\ /usr/lib/x86_64-linux-gnu/libnvidia-fatbinaryloader.so.410.129 \\ /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.410.129 \\ /usr/lib/x86_64-linux-gnu/libnvidia-compiler.so.460.32.03 \\ /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.460.32.03 \\ /usr/lib/x86_64-linux-gnu/libnvidia-allocator.so.460.32.03 \\ /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.460.32.03 \\ /usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.460.32.03 \\ /usr/lib/x86_64-linux-gnu/libcuda.so.410.129 \\ /usr/lib/x86_64-linux-gnu/libcuda.so.460.32.03 \\ /usr/lib/x86_64-linux-gnu/ Build fails: $ docker build . -t nvidia-ros:osrf Step 5/7 : COPY --from=cuda10 /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.460.32.03 /usr/lib/x86_64-linux-gnu/libnvidia-fatbinaryloader.so.410.129 /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.410.129 /usr/lib/x86_64-linux-gnu/libnvidia-compiler.so.460.32.03 /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.460.32.03 /usr/lib/x86_64-linux-gnu/libnvidia-allocator.so.460.32.03 /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.460.32.03 /usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.460.32.03 /usr/lib/x86_64-linux-gnu/libcuda.so.410.129 /usr/lib/x86_64-linux-gnu/libcuda.so.460.32.03 /usr/lib/x86_64-linux-gnu/ COPY failed: stat usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.460.32.03: file does not exist However these files do exist: $ docker run -it --rm --gpus all nvidia/cuda:10.0-devel-ubuntu18.04 root@fc9c1d8ccdc2:/# ls -la /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.* lrwxrwxrwx 1 root root 37 Jan 30 14:13 /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1 -&gt; libnvidia-ptxjitcompiler.so.460.32.03 -rw-r--r-- 1 root root 12129448 Aug 20 2019 /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.410.129 -rw-r--r-- 1 root root 10516984 Dec 27 18:55 /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.460.32.03",
        "answers": [
            [
                "TL;DR: This file is mounted by the runtime (docs), so it will not be present at the build time. You need to have a couple environment variables in your image or at the container start for the NVIDIA runtime to mount driver libraries inside. Check out the Dockerfile at the end for an example. To investigate this I ran this command first: docker run --rm --entrypoint=\"\" -it nvidia/cuda:10.0-devel-ubuntu18.04 \\ stat /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.460.32.03 And got the same error: stat: cannot stat '/usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.460.32.03': No such file or directory So I went into the directory and looked with ls: root@8c34c353bcbb:/usr/lib/x86_64-linux-gnu# ls libnvidia-ptxjitcompiler.so ls: cannot access 'libnvidia-ptxjitcompiler.so': No such file or directory root@8c34c353bcbb:/usr/lib/x86_64-linux-gnu# ls libn libnccl.so libnccl_static.a libnpth.so.0 libnsl.so libnss_files.so libnss_nisplus.so libnccl.so.2 libnettle.so.6 libnpth.so.0.1.1 libnss_compat.so libnss_hesiod.so libnccl.so.2.6.4 libnettle.so.6.4 libnsl.a libnss_dns.so libnss_nis.so There file was missing. Then I used the command you have shared: docker run -it --rm --runtime nvidia nvidia/cuda:10.0-devel-ubuntu18.04 root@4a1602f3d5c0:/# ls -la /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.* lrwxrwxrwx 1 root root 34 Jan 30 14:48 /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1 -&gt; libnvidia-ptxjitcompiler.so.450.66 -rw-r--r-- 1 root root 12129448 Aug 20 2019 /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.410.129 -rwxr-xr-x 1 root root 9947144 Sep 28 10:57 /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.450.66 The files were there, but the version was different and it matched my NVIDIA driver version: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.66 Driver Version: 450.66 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ So it appeared to me that this file only exists when you use NVIDIA runtime to start the container. I googled this and found a confirmation here. Documentation states that you need to run a container with several environment variables for driver libs to be mounted. So I've run env command in an official NVIDIA container and copied every variable with NVIDIA_ prefix into the Dockerfile: FROM nvidia/cuda:10.0-devel-ubuntu18.04 AS cuda10 FROM osrf/ros:foxy-desktop COPY --from=cuda10 /usr/local/cuda-10.0 /usr/local/cuda-10.0 RUN cd /usr/local &amp;&amp; ln -s cuda-10.0 cuda ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility ENV NVIDIA_REQUIRE_CUDA=cuda&gt;=10.0 brand=tesla,driver&gt;=384,driver&lt;385 brand=tesla,driver&gt;=410,driver&lt;411 ENV NVIDIA_VISIBLE_DEVICES=all Running the new image with NVIDIA runtime I found the files mounted: docker run --runtime nvidia --rm -it afae756457a9 root@7ebdef701231:/# stat /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.450.66 File: /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.450.66 Size: 9947144 Blocks: 19432 IO Block: 4096 regular file Device: 801h/2049d Inode: 131438 Links: 1 Access: (0755/-rwxr-xr-x) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2021-01-30 14:48:05.765015216 +0000 Modify: 2020-09-28 10:57:18.067125173 +0000 Change: 2020-09-28 10:57:18.067125173 +0000 Birth: -"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to run the code from this repository: https://github.com/danielgordon10/thor-iqa-cvpr-2018 It has the following requirements Python 3.5 CUDA 8 or 9 cuDNN Tensorflow 1.4 or 1.5 Ubuntu 16.04, 18.04 an installation of darknet My system satisfies neither of these. I don't want to reinstall tf/cuda/cudnn on my machine (especially if have to do that everytime I try to run deep learning code with different tensorflow requirements everytime). I'm looking for a way to install the requirements and run the code regardless of the host. To my knowledge that is exactly what Docker is for. Looking into this there exist docker images from nvidia. For example one called \"nvidia/cuda:9.1-cudnn7-runtime\". Based on the name I assumed that any image build with this as the base comes with cuda installed. This does not seem to be the case as if I try to install darknet it will fail with the error that \"cuda_runtime.h\" is missing. So what my question basicaly boils down to is: How do I keep multiple different versions of cuda and tensorflow on the same machine ? Ideally with docker (or similar) so I won't have to do the process to many times. It feels like I'm missing and/or don't understand something obvious, because I can't imagine that it can be so hard to run tensorflow code with different versions without reinstalling things from scratch all the time.",
        "answers": [],
        "votes": []
    },
    {
        "question": "How to create a fake nvidia runtime in my installation of docker, so I can run docker-compose files that specify runtime: nvidia on my laptop ? Something that makes docker --runtime=nvidia equivalent to docker --runtime=runc ? This would make me able to run containers designed for use with a GPU on my laptop. Currently, docker-compose files that specify runtime: nvidia cause: ERROR: for toto Cannot create container for service toto: Unknown runtime specified nvidia. PS: I know this is janky but I need to test the containers, without rewriting docker-compose.yaml or having multiple docker-compose files if possible.",
        "answers": [
            [
                "For this, I needed to add a runtime called nvidia, just like if I had an nvidia GPU. But since I don't have docker-nvidia-runtime, I need to redirect to runc: $ find / -name runc 2&gt;/dev/null /usr/bin/runc Final command: $ sudo tee /etc/docker/daemon.json &lt;&lt;EOF { \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/runc\", \"runtimeArgs\": [] } } } EOF sudo pkill -SIGHUP dockerd"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to build a custom docker image to server our image classification model. Using Ubuntu 18.04 on Google cloud. GPU model Nvidia-t4. On the same machine, using Tensorflow - GPU 1.9.0 and its working as expected. When I build the docker file with the command: sudo nvidia-docker build -t name . Seeing the following error message. Model is loaded on CPU instead of GPU and inference in run on CPU. 2021-01-05 20:46:59.617414: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2021-01-05 20:46:59.618426: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUresult(-1) 2021-01-05 20:46:59.618499: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:152] no NVIDIA GPU device is present: /dev/nvidia0 does not exist Docker File: FROM tensorflow/tensorflow:1.9.0-gpu-py3 as base ENV CUDA_HOME /usr/local/cuda ENV PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 \\ &amp;&amp; echo \"/usr/local/cuda/lib64/stubs\" &gt; /etc/ld.so.conf.d/z-cuda-stubs.conf \\ &amp;&amp; ldconfig ENV NVIDIA_VISIBLE_DEVICES all ADD . /app WORKDIR /app RUN apt-get -yqq update RUN apt-get install -yqq libsm6 libxext6 libxrender-dev RUN pip install -r requirements.txt RUN python3 run_model.py Do I need to add anything more in my docker file?",
        "answers": [
            [
                "Nothing to worry about. Just burn the system."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I saw several Q&amp;As on this topic and tried both approaches. Any advice on how to proceed with either route are appreciated: Running nvidia-docker from within WSL2 I followed NVIDIA docs and this tutorial. Everything installs and docker command runs from within Ubuntu 20.04. However, sudo service docker start returns: docker: unrecognized service Update: this turns to be a known issue. I was able to install and run docker on Ubuntu following these instructions. However, next I am running into an issue like this $docker run --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark Status: Downloaded newer image for nvcr.io/nvidia/k8s/cuda-sample:nbody docker: Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:459: container init caused: Running hook #0:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: nvml error: driver not loaded: unknown. ERRO[0065] error waiting for container: context canceled there is a similar issue for WSL1 here, where advice is well to install WSL2. I am running into the same thing under WSL2. Running NVIDIA docker from Windows: Another school of thought suggest removing docker from WSL Ubuntu and running Windows docker instead. Then one can connect to it from WSL. Well, I am not able to run nvidia-docker from Windows at all: $ docker run --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark Unable to find image 'nvcr.io/nvidia/k8s/cuda-sample:nbody' locally nbody: Pulling from nvidia/k8s/cuda-sample ... docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]]. there are tips on how to fix it here, but it is all about docker running from within Linux. Which does not work (See above). Additionally, after removing docker from ubuntu I can still run docker from Ubuntu20.04 or when I run wsl from Powershell: $ which docker /mnt/c/ProgramData/DockerDesktop/version-bin/docker $ docker The command 'docker' could not be found in this WSL 2 distro. We recommend to activate the WSL integration in Docker Desktop settings. See https://docs.docker.com/docker-for-windows/wsl/ for details. I have definitely enabled WSL2-based engine and integration for Ubuntu 20.04 enabled in two different tabs in Docker settings. System Windows 10 WSL 2 Ubuntu 20.04 within WSL2 Windows Docker with WSL2-based engine and integration enabled Any help how to diagnose it further is much appreciated",
        "answers": [
            [
                "As of June 2022, with Windows 10, I didn't have to do anything to enable this. I have a pre-existing Ubuntu 20.04 install on WSL 2, and Docker running on Windows with WSL 2 integration enabled. I ran docker run -it --rm --gpus all ubuntu nvidia-smi and it showed my GPU."
            ],
            [
                "I think the best way to get nVidia GPU (CUDA) running on WSL2 is to follow the tutorial from nVidia documentation CUDA on WSL. There are three thing you need do them correctly and in order: First, check your windows if there is an updates, update if there is any. Then you need to subscribe in Microsoft Windows Insider Program, specifically subscribe in Dev Channel (Fast ring). Make sure that is set to Dev channel not Beta Channel nor Release Preview Channel. Here was the tricky part for me, after you select Dev channel check your windows version by running (winver) program (search for it in the search bar of windows) if it's below 20145 go re-check your windows for an update (you will see in the update discription version above 20145 is availble). You nedd to install it, here you need to be patient because it take while to download and install the update, it will ask you for windows restart. After the restart you're good to go (you will also notice some design diffrence between the two version of windows). Check again your windows version with (winver) to find it, it's indeed above 20145. Next, install the NVIDIA preview driver for WSL 2, it's pretty straight forward process. Finally, install WSL2 from tutorial, this is also pretty straight forward process. Personaly, i follewed this youtube tutorial from David Bombal channel. After these three steps preformed correctly, you can follow along the nVidia tutorial from Setting up CUDA Toolkit. Here is another nVidia document stating the same issue you're facing in Chapter 8. They claim that This error usually indicates that the right Microsoft Windows Insider Preview Builds, WSL 2, NVIDIA drivers and NVIDIA Container Toolkit may not be installed correctly. For me, it was my Windows Insider Preview Builds version not properly set to version 20145 or higher."
            ],
            [
                "by https://docs.docker.com/docker-for-windows/wsl/ uncheck \"Enable integration with my default WSL distro\" and apply, then uncheck \"Use the WSL 2 based engine\" and apply. check all above and enable: \"Enable integration with additional distros:\" Ubuntu-20.04 open terminal: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES it works on my win10 docker 3.6..."
            ]
        ],
        "votes": [
            6.0000001,
            4.0000001,
            -0.9999999
        ]
    },
    {
        "question": "Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers. This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered. Closed 2 years ago. Improve this question When I try to run a downgraded cuda version on the container this happens. Can I use a downgraded Cuda version on the Container and another cuda version of the host machine?",
        "answers": [
            [
                "Yes, you can. When you specify to docker --gpus all (or some variant of that switch) you are using the GPU-enabled version of docker (which is recommended, if you are using GPUs). When properly set up, that docker GPU enablement should allow you to run a version of CUDA that is less than or equal to the CUDA (driver API) version that is reported by nvidia-smi. In your case, nvidia-smi is reporting support for up to CUDA version 11.1, and that means you can pull/run/use a docker container that is based on CUDA 10.0, for example (as is the case in your example.) There don't appear to be any problems in the output you have shown. The CUDA toolkit version you have installed on the base machine is irrelevant for what is being used in the container, but the CUDA driver version (which is what is reported by nvidia-smi will dictate the latest CUDA toolkit version you can use, whether in the container, or on the base machine. But the CUDA toolkit in the base machine and the CUDA toolkit in the container do not interact, and have no relevance to each other. Only the GPU driver version (i.e. the CUDA version supported by the GPU driver installed on the base machine) matters for compatibility."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "when trying to run systemctl along with GPU, systemctl is not working without --privileged and when trying to limit GPUS by providing --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=0 --privileged the container shows all the GPUS available inside the container. if --privileged is not specified --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=0 the container shows the desired amount of GPUs",
        "answers": [
            [
                "NV_GPU='GPU number' nvidia-docker run --runtime=nvidia -it -v path:path nvcr.io/nvidia/tensorflow:xx.xx Example: NV_GPU='0,1' nvidia-docker run --runtime=nvidia -it -v /storage/research/:/storage/research/ nvcr.io/nvidia/tensorflow:17.11-tf2-p3"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to build a new docker image by adding some python packages to my base docker image. The sudo docker build -t myimage1:cuda10.2 . runs without any issues but I cannot import any of the packages in the new image. I am using sudo docker run --gpus all -it myimage1:cuda10.2 to run the image. Can somebody help me understand what am I missing here? Dockerfile FROM nvcr.io/nvidia/rapidsai/rapidsai:cuda10.2-runtime-ubuntu18.04 WORKDIR /rapids/notebooks/ COPY requirements.txt ./ RUN pip install --upgrade pip &amp;&amp; \\ pip install --no-cache-dir -r requirements.txt requirements.txt ujson This is the trace for docker build Sending build context to Docker daemon 4.096kB Step 1/5 : FROM nvcr.io/nvidia/rapidsai/rapidsai:cuda10.2-runtime-ubuntu18.04 ---&gt; 98901cabda0a Step 2/5 : WORKDIR /rapids/notebooks/ ---&gt; Using cache ---&gt; 162a9bb732c7 Step 3/5 : COPY requirements.txt ./ ---&gt; 7bb48a384987 Step 4/5 : RUN pip install --upgrade pip &amp;&amp; pip install --no-cache-dir -r requirements.txt ---&gt; Running in 5f3ca4ed3f93 Collecting pip Downloading pip-20.2.4-py2.py3-none-any.whl (1.5 MB) Installing collected packages: pip Attempting uninstall: pip Found existing installation: pip 20.0.2 Uninstalling pip-20.0.2: Successfully uninstalled pip-20.0.2 Successfully installed pip-20.2.4 Collecting ujson Downloading ujson-4.0.1-cp38-cp38-manylinux1_x86_64.whl (181 kB) Installing collected packages: ujson Successfully installed ujson-4.0.1 Removing intermediate container 5f3ca4ed3f93 ---&gt; 58e94a2c4c98 Step 5/5 : COPY . . ---&gt; d5897f4da4ff Successfully built d5897f4da4ff Successfully tagged myimage1:cuda10.2 If I run the image and do pip install ujson This is what I get Collecting ujson Downloading ujson-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (179 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 179 kB 947 kB/s Installing collected packages: ujson Successfully installed ujson-4.0.1 The only difference is that the ujson package is cp37 while that installed by Dockerfile is cp38? Can somebody explain why the packages installed are for different Python versions?",
        "answers": [
            [
                "In the rapids containers there's a virtual environment named rapids where all of the packages are installed and is activated in the default entrypoint for the container. You should change your RUN command to: RUN source activate rapids &amp;&amp; \\ pip install --upgrade pip &amp;&amp; \\ pip install --no-cache-dir -r requirements.txt"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to query GPU usage metrics of GKE pods. Here is what I've done for test: Created GKE cluster with two node pools, one of them has two cpu-only nodes and the other has one node with NVIDIA Tesla T4 GPU. All nodes are running Container-Optimized OS. As written in https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#installing_drivers, I ran kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml. kubectl create -f dcgm-exporter.yaml # dcgm-exporter.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: \"dcgm-exporter\" labels: app.kubernetes.io/name: \"dcgm-exporter\" app.kubernetes.io/version: \"2.1.1\" spec: updateStrategy: type: RollingUpdate selector: matchLabels: app.kubernetes.io/name: \"dcgm-exporter\" app.kubernetes.io/version: \"2.1.1\" template: metadata: labels: app.kubernetes.io/name: \"dcgm-exporter\" app.kubernetes.io/version: \"2.1.1\" name: \"dcgm-exporter\" spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: cloud.google.com/gke-accelerator operator: Exists containers: - image: \"nvidia/dcgm-exporter:2.0.13-2.1.1-ubuntu18.04\" # resources: # limits: # nvidia.com/gpu: \"1\" env: - name: \"DCGM_EXPORTER_LISTEN\" value: \":9400\" - name: \"DCGM_EXPORTER_KUBERNETES\" value: \"true\" name: \"dcgm-exporter\" ports: - name: \"metrics\" containerPort: 9400 securityContext: runAsNonRoot: false runAsUser: 0 capabilities: add: [\"SYS_ADMIN\"] volumeMounts: - name: \"pod-gpu-resources\" readOnly: true mountPath: \"/var/lib/kubelet/pod-resources\" tolerations: - effect: \"NoExecute\" operator: \"Exists\" - effect: \"NoSchedule\" operator: \"Exists\" volumes: - name: \"pod-gpu-resources\" hostPath: path: \"/var/lib/kubelet/pod-resources\" --- kind: Service apiVersion: v1 metadata: name: \"dcgm-exporter\" labels: app.kubernetes.io/name: \"dcgm-exporter\" app.kubernetes.io/version: \"2.1.1\" annotations: prometheus.io/scrape: 'true' prometheus.io/port: '9400' spec: selector: app.kubernetes.io/name: \"dcgm-exporter\" app.kubernetes.io/version: \"2.1.1\" ports: - name: \"metrics\" port: 9400 The pod runs only on the gpu node but crashes with the following error: time=\"2020-11-21T04:27:21Z\" level=info msg=\"Starting dcgm-exporter\" Error: Failed to initialize NVML time=\"2020-11-21T04:27:21Z\" level=fatal msg=\"Error starting nv-hostengine: DCGM initialization error\" With uncommenting the resources: limits: nvidia.com/gpu: \"1\", it successfully runs. However, I don't want this pod to occupy any GPU but just watch them. How can I run the dcgm-exporter without allocating GPU to it? I tried with Ubuntu nodes but failed, too.",
        "answers": [
            [
                "It worked with these: Set privileged: true to securityContext. Add volume mount \"nvidia-install-dir-host\". apiVersion: apps/v1 kind: DaemonSet metadata: name: \"dcgm-exporter\" labels: app.kubernetes.io/name: \"dcgm-exporter\" app.kubernetes.io/version: \"2.1.1\" spec: updateStrategy: type: RollingUpdate selector: matchLabels: app.kubernetes.io/name: \"dcgm-exporter\" app.kubernetes.io/version: \"2.1.1\" template: metadata: labels: app.kubernetes.io/name: \"dcgm-exporter\" app.kubernetes.io/version: \"2.1.1\" name: \"dcgm-exporter\" spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: cloud.google.com/gke-accelerator operator: Exists containers: - image: \"nvidia/dcgm-exporter:2.0.13-2.1.1-ubuntu18.04\" env: - name: \"DCGM_EXPORTER_LISTEN\" value: \":9400\" - name: \"DCGM_EXPORTER_KUBERNETES\" value: \"true\" name: \"dcgm-exporter\" ports: - name: \"metrics\" containerPort: 9400 securityContext: privileged: true volumeMounts: - name: \"pod-gpu-resources\" readOnly: true mountPath: \"/var/lib/kubelet/pod-resources\" - name: \"nvidia-install-dir-host\" mountPath: \"/usr/local/nvidia\" tolerations: - effect: \"NoExecute\" operator: \"Exists\" - effect: \"NoSchedule\" operator: \"Exists\" volumes: - name: \"pod-gpu-resources\" hostPath: path: \"/var/lib/kubelet/pod-resources\" - name: \"nvidia-install-dir-host\" hostPath: path: \"/home/kubernetes/bin/nvidia\" --- kind: Service apiVersion: v1 metadata: name: \"dcgm-exporter\" labels: app.kubernetes.io/name: \"dcgm-exporter\" app.kubernetes.io/version: \"2.1.1\" annotations: prometheus.io/scrape: 'true' prometheus.io/port: '9400' spec: selector: app.kubernetes.io/name: \"dcgm-exporter\" app.kubernetes.io/version: \"2.1.1\" ports: - name: \"metrics\" port: 9400"
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I have retrieved the following Docker image from NVIDIA NGC, using Singularity: https://ngc.nvidia.com/catalog/containers/nvidia:cuda. I have pulled the tag '11.1-cudnn8-devel-ubuntu18.04' as follows: singularity pull docker://nvcr.io/nvidia/cuda:11.1-cudnn8-devel-ubuntu18.04 This image is then available locally as 'cuda_11.1-cudnn8-devel-ubuntu18.04.sif'. I then attempt to augment this image with some build tools and some Python according to the following definition file: Bootstrap: localimage From: cuda_11.1-cudnn8-devel-ubuntu18.04.sif %post apt-get update &amp;&amp; apt-get install -y libopenmpi-dev make gcc pkg-config g++ python3 python3-sklearn python3-requests python3-bs4 python3-urllib3 As follows: singularity build --force --fakeroot cuda_11.1-cudnn8-python-devel-ubuntu18.04.sif cuda-python.def During installation of tzdata in the image, it asks for user input to determine my time zone: Setting up tzdata (2020a-0ubuntu0.18.04) ... debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.) debconf: falling back to frontend: Readline Configuring tzdata ------------------ Please select the geographic area in which you live. Subsequent configuration questions will narrow this down by presenting a list of cities, representing the time zones in which they are located. 1. Africa 4. Australia 7. Atlantic 10. Pacific 13. Etc 2. America 5. Arctic 8. Europe 11. SystemV 3. Antarctica 6. Asia 9. Indian 12. US Geographic area: 8 This halts the build process and even though I type in 8 and press ENTER, nothing happens. It seems as if singularity is not providing a proper \"hole through\" for ordinary command line input. How can I fix this?",
        "answers": [
            [
                "Apparently, this a well-known problem among more experienced Docker/Singularity-users. It turns out I can work around it by setting, at the beginning of my %post script: export DEBIAN_FRONTEND=noninteractive and ending with: unset DEBIAN_FRONTEND I am not sure how tzdata ends up being configured then, but that is not really so important to me in this case."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a cloudformation template that I have created in hopes to spin up an ec2 instance with the necessary dependencies (where these dependencies are installed as bash in UserData) to leverage GPU hardware within a docker container. The main dependencies are: 1) nvidia drivers, 2) docker, and 3) nvidia-docker2. The first two dependencies install as expected and after several moments of running can be verified by 1) nvidia-smi, and docker --version. The third dependency however consistently does not install. For reference here are the relevant parts of my UserData bash: # install gpu stuff apt-get install linux-headers-$(uname -r) distribution=$(. /etc/os-release;echo $ID$VERSION_ID | sed -e 's/\\.//g') wget https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/cuda-$distribution.pin mv cuda-$distribution.pin /etc/apt/preferences.d/cuda-repository-pin-600 apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/7fa2af80.pub echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64 /\" | tee /etc/apt/sources.list.d/cuda.list apt-get update apt-get -y install cuda-drivers # install docker on system curl https://get.docker.com | sh systemctl start docker &amp;&amp; systemctl enable docker distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | tee /etc/apt/sources.list.d/nvidia-docker.list apt-get -y install nvidia-docker2 &gt; /var/log/mason # add nvidia runtime stuff # echo \"{ \\\"runtimes\\\": { \\\"nvidia\\\": { \\\"path\\\": \\\"/usr/bin/nvidia-container-runtime\\\", \\\"runtimeArgs\\\": [] } } }\" &gt;&gt; /etc/docker/daemon.json systemctl restart docker I have tried to pipe the stdout from apt-get -y install nvidia-docker2 to a log file but the logs only show: Reading package lists... Building dependency tree... Reading state information... and seems to be stuck there. Other potential helpful bits: AMI: ubuntu 18.04 image I will also note that I am able to SSH into the instance and install the apt-get -y install nvidia-docker2 in the command terminal without a hitch (or any user prompt or anything). Can anyone help me figure out how to trouble shoot this issue or does anyone see any potential problems in what I have shared above? The stdout pipe to file is about the only trick I know to debug such an issue as this. Please let me know if I can update/edit this post to make this issue easier to debug.",
        "answers": [
            [
                "Based on the comments. The issue was caused by not updating ubuntu's repositories after adding nvidia-docker2 repo. The solution was to run apt-get update after the addition of the repo."
            ],
            [
                "replace: distribution=$(. /etc/os-release;echo $ID$VERSION_ID | sed -e 's/\\.//g') with: distribution = ubuntu18.04"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I have docker swarm of two nodes - manager node (aws instance) and worker node (multi-gpu rig on a desk next to me), both on Ubuntu 18.04 and Docker.io 19.03.6, build 369ce74a3c. On a worker node I set up nvidia-docker runtime and tested it (it works). On a manager node I set up an overlay network and now I'm trying to start service with gpu access and join it to my overlay network, but no luck - service isn't starting with assigned node no longer meets constraints. How I start service: docker service create --name=hw --constraint=node.id==xyriecy63n8995enp2mro0nvx --network=d9gqsljvmpy7 --generic-resource \"gpu=1\" busybox:latest sh -c \"while true; do echo Hello; sleep 2; done\" And what status it has: ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ur0uut7xq8qyjafejwt3xlbv4 hw.1 busybox:latest@sha256:d366a4665ab44f0648d7a00ae3fae139d55e32f9712c67accd604bb55df9d05a node-4 Ready Rejected less than a second ago \"assigned node no longer meets constraints\" w83690e7dzcc56ahysp8s5xi9 \\_ hw.1 busybox:latest@sha256:d366a4665ab44f0648d7a00ae3fae139d55e32f9712c67accd604bb55df9d05a node-4 Shutdown Rejected less than a second ago \"node is missing network attachments, ip addresses may be exhausted\" Task details: docker inspect ur0uut7xq8qyjafejwt3xlbv4 [ { \"ID\": \"ur0uut7xq8qyjafejwt3xlbv4\", \"Version\": { \"Index\": 156466 }, \"CreatedAt\": \"2020-10-13T06:53:54.822993602Z\", \"UpdatedAt\": \"2020-10-13T06:54:00.063967596Z\", \"Labels\": {}, \"Spec\": { \"ContainerSpec\": { \"Image\": \"busybox:latest@sha256:d366a4665ab44f0648d7a00ae3fae139d55e32f9712c67accd604bb55df9d05a\", \"Args\": [ \"sh\", \"-c\", \"while true; do echo Hello; sleep 2; done\" ], \"Init\": false, \"DNSConfig\": {}, \"Isolation\": \"default\" }, \"Resources\": { \"Limits\": {}, \"Reservations\": { \"GenericResources\": [ { \"DiscreteResourceSpec\": { \"Kind\": \"gpu\", \"Value\": 1 } } ] } }, \"Placement\": { \"Constraints\": [ \"node.id==xyriecy63n8995enp2mro0nvx\" ], \"Platforms\": [ { \"Architecture\": \"amd64\", \"OS\": \"linux\" }, { \"OS\": \"linux\" }, { \"OS\": \"linux\" }, { \"OS\": \"linux\" }, { \"Architecture\": \"arm64\", \"OS\": \"linux\" }, { \"Architecture\": \"386\", \"OS\": \"linux\" }, { \"Architecture\": \"mips64le\", \"OS\": \"linux\" }, { \"Architecture\": \"ppc64le\", \"OS\": \"linux\" }, { \"Architecture\": \"s390x\", \"OS\": \"linux\" } ] }, \"Networks\": [ { \"Target\": \"d9gqsljvmpy7wjrxa5q09bgtb\" } ], \"ForceUpdate\": 0 }, \"ServiceID\": \"mef68axo6ztmu7ojkiwcxxj0a\", \"Slot\": 1, \"NodeID\": \"xyriecy63n8995enp2mro0nvx\", \"Status\": { \"Timestamp\": \"2020-10-13T06:53:59.979035656Z\", \"State\": \"rejected\", \"Message\": \"preparing\", \"Err\": \"node is missing network attachments, ip addresses may be exhausted\", \"ContainerStatus\": { \"ContainerID\": \"\", \"PID\": 0, \"ExitCode\": 0 }, \"PortStatus\": {} }, \"DesiredState\": \"shutdown\", \"NetworksAttachments\": [ { \"Network\": { \"ID\": \"d9gqsljvmpy7wjrxa5q09bgtb\", \"Version\": { \"Index\": 32157 }, \"CreatedAt\": \"2020-10-12T13:39:55.061260869Z\", \"UpdatedAt\": \"2020-10-12T13:39:55.062498427Z\", \"Spec\": { \"Name\": \"testnet\", \"Labels\": {}, \"DriverConfiguration\": { \"Name\": \"overlay\" }, \"Attachable\": true, \"IPAMOptions\": { \"Driver\": { \"Name\": \"default\" }, \"Configs\": [ { \"Subnet\": \"172.25.0.0/16\", \"Gateway\": \"172.25.0.1\" } ] }, \"Scope\": \"swarm\" }, \"DriverState\": { \"Name\": \"overlay\", \"Options\": { \"com.docker.network.driver.overlay.vxlanid_list\": \"4097\" } }, \"IPAMOptions\": { \"Driver\": { \"Name\": \"default\" }, \"Configs\": [ { \"Subnet\": \"172.25.0.0/16\", \"Gateway\": \"172.25.0.1\" } ] } }, \"Addresses\": [ \"172.25.96.221/16\" ] } ], \"GenericResources\": [ { \"NamedResourceSpec\": { \"Kind\": \"gpu\", \"Value\": \"GPU-50fd60c4\" } } ] } ] My overlay network: docker inspect d9gqsljvmpy7 [ { \"Name\": \"testnet\", \"Id\": \"d9gqsljvmpy7wjrxa5q09bgtb\", \"Created\": \"2020-10-12T13:39:55.061260869Z\", \"Scope\": \"swarm\", \"Driver\": \"overlay\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.25.0.0/16\", \"Gateway\": \"172.25.0.1\" } ] }, \"Internal\": false, \"Attachable\": true, \"Ingress\": false, \"ConfigFrom\": { \"Network\": \"\" }, \"ConfigOnly\": false, \"Containers\": null, \"Options\": { \"com.docker.network.driver.overlay.vxlanid_list\": \"4097\" }, \"Labels\": null } ] Service starts normally without ether --network or --generic-resource. Starting without --network and attaching after start also doesn't work. I enabled debug logs on both nodes but didn't see anything suspicious other than same error message: Oct 12 13:40:45 node-4 dockerd[1166]: time=\"2020-10-12T13:40:45.975574449Z\" level=error msg=\"fatal task error\" error=\"node is missing network attachments, ip addresses may be exhausted\" module=node/agent/taskmanager node.id=xyriecy63n8995enp2mro0nvx service.id=mef68axo6ztmu7ojkiwcxxj0a task.id=twcbj9emeopm2qfq0i7lwftbe Also I tested network exhaustion with docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock docker/ip-util-check and obviously it finds nothing: Overlay IP Utilization Report ---- Network testnet/d9gqsljvmpy7 has an IP address capacity of 65533 and uses 0 addresses spanning over 0 nodes Network OK: network will have 49149 available IPs before passing the 75% subnet use So, how can one start gpu-tied service and attach it to overlay network?",
        "answers": [
            [
                "Apparently, there is no need to specify --generic-resource in my case. Without it service has access to all gpus, listed to docker via --node-generic-resource gpu=xxx. Downside is you can't control gpu count per service, but I can live with it."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm a complete newcomer to Docker, so the following questions might be a bit naive, but I'm stuck and I need help. I'm trying to reproduce some results in research. The authors just released code along with a specification of how to build a Docker image to reproduce their results. The relevant bit is copied below: I believe I installed Docker correctly: $ docker --version Docker version 19.03.13, build 4484c46d9d $ sudo docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ However, when I try checking that my nvidia-docker installation was successful, I get the following error: $ sudo docker run --gpus all --rm nvidia/cuda:10.1-base nvidia-smi docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:432: running prestart hook 0 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: nvml error: driver not loaded\\\\\\\\n\\\\\\\"\\\"\": unknown. It looks like the key error is: nvidia-container-cli: initialization error: nvml error: driver not loaded I don't have a GPU locally and I'm finding conflicting information on whether CUDA needs to be installed before NVIDIA Docker. For instance, this NVIDIA moderator says \"A proper nvidia docker plugin installation starts with a proper CUDA install on the base machine.\" My questions are the following: Can I install NVIDIA Docker without having CUDA installed? If so, what is the source of this error and how do I fix it? If not, how do I create this Docker image to reproduce the results?",
        "answers": [
            [
                "Can I install NVIDIA Docker without having CUDA installed? Yes, you can. The readme states that nvidia-docker only requires NVIDIA GPU driver and Docker engine installed: Note that you do not need to install the CUDA Toolkit on the host system, but the NVIDIA driver needs to be installed If so, what is the source of this error and how do I fix it? That's either because you don't have a GPU locally or it's not NVIDIA, or you messed up somewhere when installed drivers. If you have a CUDA-capable GPU I recommend using NVIDIA guide to install drivers. If you don't have a GPU locally, you can still build an image with CUDA, then you can move it somewhere where there is a GPU. If not, how do I create this Docker image to reproduce the results? The problem is that even if you manage to get rid of CUDA in Docker image, there is software that requires it. In this case fixing the Dockerfile seems to me unnecessary - you can just ignore Docker and start fixing the code to run it on CPU."
            ],
            [
                "I think you need ENV NVIDIA_VISIBLE_DEVICES=void then RUN your work finally ENV NVIDIA_VISIBLE_DEVICES=all"
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "Closed. This question is opinion-based. It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post. Closed 2 years ago. Improve this question I want to access my NVIDIA GPUs from inside containers. Can I do this without nvidia-container-runtime? Requiring a custom Docker runtime just to talk to one device seems very strange. There is a whole universe of PCI devices out there. Why does this one need its own runtime? For example, suppose I had both NVIDIA and AMD GPUs. Would I be unable to access both from inside one container? I understand that nvidia-container-runtime lets me control which GPUs are visible via NVIDIA_VISIBLE_DEVICES. But I do not care about this. I am not using containers to isolate devices; I am using containers to manage CUDA/CUDNN/TensorFlow version h*ll. And if I did want to isolate devices, I would use the same mechanism as forever: By controlling access to nodes in /dev. In short, the whole \"custom runtime\" design looks flawed to me. So, questions: What am I missing? Can I obtain access to my NVIDIA GPUs using the stock Docker (or podman) runtime? If not, why not?",
        "answers": [
            [
                "I certainly won't be able to answer every conceivable question related to this. I will try to give a summary. Some of what I write here is based on what's documented here and here. My discussion here will also be focused on linux, and docker (not windows, not singularity, not podman, etc.). I'm also not likely to be able to address in detail questions like \"why don't other PCI devices have to do this?\". I'm also not trying to make my descriptions of how docker works perfectly accurate to an expert in the field. The NVIDIA GPU driver has components that run in user space and also other components that run in kernel space. These components work together and must be in harmony. This means the kernel mode component(s) for driver XYZ.AB must be used only with user-space components from driver XYZ.AB (not any other version), and vice-versa. Roughly speaking, docker is a mechanism to provide an isolated user-space linux presence that runs on top of, and interfaces to, the linux kernel (where all the kernel space stuff lives). The linux kernel is in the base machine (outside the container) and much/most of linux user space code is inside the container. This is one of the architectural factors that allow you to do neato things like run an ubuntu container on a RHEL kernel. From the NVIDIA driver perspective, some of its components need to be installed inside the container and some need to be installed outside the container. Can I obtain access to my NVIDIA GPUs using the stock Docker (or podman) runtime? Yes, you can, and this is what people did before nvidia-docker or the nvidia-container-toolkit existed. You need to install the exact same driver in the base machine as well as in the container. Last time I checked, this works (although I don't intend to provide instructions here.) If you do this, the driver components inside the container match those outside the container, and it works. What am I missing? NVIDIA (and presumably others) would like a more flexible scenario. The above description means that if a container was built with any other driver version (than the one installed on your base machine) it cannot work. This is inconvenient. The original purpose of nvidia-docker was to do the following: At container load time, install the runtime components of the driver, which are present in the base machine, into the container. This harmonizes things, and although it does not resolve every compatibility scenario, it resolves a bunch of them. With a simple rule \"keep your driver on the base machine updated to the latest\" it effectively resolves every compatibility scenario that might arise from a mismatched driver/CUDA runtime. (The CUDA toolkit, and anything that depends on it, like CUDNN, need only be installed in the container.) As you point out, the nvidia-container-toolkit has picked up a variety of other, presumably useful, functionality over time. I'm not spending a lot of time here talking about the compatibility strategy (\"forward\") that exists for compiled CUDA code, and the compatibility strategy (\"backward\") that exists when talking about a specific driver and the CUDA versions supported by that driver. I'm also not intending to provide instructions for use of the nvidia-container-toolkit, that is already documented, and many questions/answers about it already exist also. I won't be able to respond to follow up questions like \"why was it architected that way?\" or \"that shouldn't be necessary, why don't you do this?\""
            ],
            [
                "To answer my own question: No, we do not need nvidia-container-runtime. The NVIDIA shared libraries are tightly coupled to each point release of the driver. NVIDIA likes to say \"the driver has components that run in user space\", but of course that is a contradiction in terms. So for any version of the driver, you need to make the corresponding release of these shared libraries accessible inside the container. A brief word on why this is a bad design: Apart from the extra complexity, the NVIDIA shared libraries have dependencies on other shared libraries in the system, in particular C and X11. If a newer release of the NVIDIA libraries ever required features from newer C or X11 libraries, a system running those newer libraries could never host an older container. (Because the container would not be able to run the newer injected libraries.) The ability to run old containers on new systems is one of the most important features of containers, at least in some applications. I guess we have to hope that never happens. The HPC community figured this out and made it work some time ago. Here are some old instructions for creating a portable Singularity GPU container which injects the required NVIDIA shared libraries when the container runs. You could easily follow a similar procedure to create a portable OCI or Docker GPU container. These days, Singularity supports a --nv flag to inject the necessary shared libraries automatically. It also supports a --rocm flag for AMD GPUs. (Yes, AMD chose the same bad design.) Presumably you could combine these flags if you needed both. All of these details are pretty well-documented in the Singularity manual. Bottom line: If you are asking the same question I was, try Singularity."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I got the same Dockerfile and the same code as below but the result different One gives correct printouts while another crashes without any message!! code import dlib import cv2 print(dlib.__version__) # 19.17.99, the same cnn_face_detector = dlib.cnn_face_detection_model_v1('mmod_human_face_detector.dat') img = cv2.imread(\"1.jpg\") rects = cnn_face_detector(img, 1) # one crashes every time on this line print(\"Number of faces detected: {}\".format(len(rects))) for i, d in enumerate(rects): face = d.rect print(\"Detection {}: Left: {} Top: {} Right: {} Bottom: {} Confidence: {}\".format(i, face.left(), face.top(), face.right(), d.rect.bottom(), d.confidence)) cv2.rectangle(img, (face.left(),face.top()), (face.right(),face.bottom()), (0,0,255),2) command to run nvidia-docker run -it --rm -w=\"/usr/src/app\" -v $(pwd):/usr/src/app myDocEnv test.py correct ouput 19.17.99 Number of faces detected: 1 Detection 0: Left: 245 Top: 100 Right: 415 Bottom: 269 Confidence: 1.079284906387329 incorrect output 19.17.99 the only difference The only difference between them is the GPU hardware device The one who crashes use GeForce GTX 1650 while it runs without trouble on P620\u30011050Ti",
        "answers": [
            [
                "RUN mkdir -p /usr/local/cuda/nccl/lib &amp;&amp; \\ ln -sf /usr/lib/x86_64-linux-gnu/libnccl.so.2 /usr/local/cuda/nccl/lib/ &amp;&amp;\\ ln -sf /usr/lib/x86_64-linux-gnu/libcudnn.so.7 /usr/local/cuda/lib64/ RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ libcublas10 \\ libcublas-dev RUN apt install -y libprotobuf-dev protobuf-compiler RUN export CUDA_cublas_LIBRARY=/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublas.so adding these few things fixed my problem"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have cuda-10.1 installed on my PC. Currently, the latest version of cuda is cuda11.0. I am considering using docker cuda version 11.0 without changing the cuda version on my PC. In this case, will the cuda used in the container be 11.0?",
        "answers": [
            [
                "The CUDA version used in the container will be whatever CUDA version is installed in the container. It doesn't have any connection to the CUDA version installed on the PC (i.e. the base machine). However, the GPU driver version installed on the PC must support the CUDA version you intend to use in the container. (If you also have CUDA installed on the PC, and want to use it there also, outside of any container use, then the GPU driver version installed on the PC must also be sufficient to support that version of CUDA as well.) You can see the minimum driver versions required to support specific CUDA versions in table 2 here. If you're going to use CUDA in a container you are strongly encouraged to use the NVIDIA container toolkit."
            ]
        ],
        "votes": [
            12.0000001
        ]
    },
    {
        "question": "I wanted to run some code on CPU only on a GPU machine. Currently I have CUDA_VISIBLE_DEVICES= nvidia-docker ... but the model still will be copied to the GPU. Is there something I am missing?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a project that I maintain using Dockerfile. It is a data science project so and the docker can be used in many environments, local, in the cloud, with or without GPU. The difference between the GPU and the non-GPU version is very small, just this: FROM ubuntu:18.04 -&gt; non-GPU FROM nvidia/cuda -&gt; GPU What is the best way to handle this scenario? P.S: I am using docker-compose to build the docker",
        "answers": [
            [
                "You can use multiple docker files inside your docker-compose yml file as below, \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 GPU project \u2502 \u2514\u2500\u2500 GPU.Dockerfile \u2514\u2500\u2500 NON-GPU project \u2514\u2500\u2500 NONGPU.Dockerfile Also it's possible to get syntax highlighting in VS code by giving files .Dockerfile extension(instead of name) e.g. GPU.Dockerfile, NONGPU.Dockerfile etc."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to install nvidia-docker on my azure virtual machine with: sudo apt-get install -y nvidia-docker2 I get this error: Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: nvidia-docker2 : Depends: docker-ce (&gt;= 18.06.0~ce~3-0~ubuntu) but it is not installable or docker-ee (&gt;= 18.06.0~ce~3-0~ubuntu) but it is not installable or docker.io (&gt;= 18.06.0) but it is not going to be installed E: Unable to correct problems, you have held broken packages. As I understand after some research, this is because I have an older docker version. Trying to follow this instructions: https://github.com/NVIDIA/nvidia-docker/issues/857#issuecomment-439586831 But this gives me the following error: E: Version '18.06.1~ce~3-0~ubuntu' for 'docker-ce' was not found Next, I was trying to follow these instructions to uninstall old version of docker: https://docs.docker.com/engine/install/ubuntu/ I then got the result: Package 'docker-engine' is not installed, so not removed sudo docker version gives the following output: Client: Version: 3.0.11+azure API version: 1.40 Go version: go1.12.17 Git commit: eb310fca49568dccd87c6136f774ef6fff2a1b51 Built: Tue Mar 3 21:59:52 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 3.0.11+azure API version: 1.40 (minimum version 1.12) Go version: go1.12.17 Git commit: aa6a9891b0 Built: Tue Mar 10 18:53:36 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.2.13 GitCommit: 7ad184331fa3e55e52b890ea95e65ba581ae3429 runc: Version: 1.0.0-rc10 GitCommit: dc9208a3303feef5b3839f4323d9beb36df0a9dd docker-init: Version: 0.18.0 GitCommit: fec3683 Please help.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to use GPU from inside my docker container. I'm using docker with version 19.03 on Ubuntu 18.04. Outside the docker container if I run nvidia-smi I get the below output: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.05 Driver Version: 450.51.05 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 30C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ If I run the same thing inside the container created from nvidia/cuda docker image, I get the same output as above and everything is running smoothly. torch.cuda.is_available() returns True. But if I run the same nvidia-smi command inside any other docker container, it gives the following output where you can see that the CUDA Version is coming as N/A. Inside the containers torch.cuda.is_available() also returns False. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.05 Driver Version: 450.51.05 CUDA Version: N/A | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 30C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ I have installed nvidia-container-toolkit using the following commands. curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu18.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update sudo apt-get install nvidia-container-toolkit sudo systemctl restart docker I started my containers using the following commands sudo docker run --rm --gpus all nvidia/cuda nvidia-smi sudo docker run -it --rm --gpus all ubuntu nvidia-smi",
        "answers": [
            [
                "For anybody arriving here looking how to do it with docker compose, add to your service: deploy: resources: reservations: devices: - driver: nvidia capabilities: - gpu - utility # nvidia-smi - compute # CUDA. Required to avoid \"CUDA version: N/A\" - video # NVDEC/NVENC. For instance to use a hardware accelerated ffmpeg. Skip it if you don't need it Doc: https://docs.docker.com/compose/gpu-support"
            ],
            [
                "docker run --rm --gpus all nvidia/cuda nvidia-smi should NOT return CUDA Version: N/A if everything (aka nvidia driver, CUDA toolkit, and nvidia-container-toolkit) is installed correctly on the host machine. Given that docker run --rm --gpus all nvidia/cuda nvidia-smi returns correctly. I also had problem with CUDA Version: N/A inside of the container, which I had luck in solving: Please see my answer https://stackoverflow.com/a/64422438/2202107 (obviously you need to adjust and install the matching/correct versions of everything)"
            ]
        ],
        "votes": [
            6.0000001,
            5.0000001
        ]
    },
    {
        "question": "I am creating a nvidia-docker image with the following included in the Dockerfile: RUN curl -so /miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; chmod +x /miniconda.sh &amp;&amp; /miniconda.sh -b -p /miniconda &amp;&amp; rm /miniconda.sh ENV PATH=/miniconda/bin:$PATH #this is stored in cache ---&gt; fa383a2e1344 # check path RUN /miniconda/bin/conda I get the following error: /bin/sh: 1: /miniconda/bin/conda: not found The command '/bin/sh -c /miniconda/bin/conda' returned a non-zero code: 127 When I test the path using: nvidia-docker run --rm fa383a2e1344 ls then /miniconda does not exist hence the error. I then altered the Dockerfile to replace /miniconda with a env var path ie: ENV CONDA_DIR $HOME/miniconda # Install Miniconda RUN curl -so /miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh \\ &amp;&amp; chmod +x /miniconda.sh \\ &amp;&amp; /miniconda.sh -b -p CONDA_DIR \\ &amp;&amp; rm /miniconda.sh ENV PATH=$CONDA_DIR:$PATH # check path RUN $CONDA_DIR/conda And get the error: /bin/sh: 1: /miniconda/conda: not found The command '/bin/sh -c $CONDA_DIR/conda' returned a non-zero code: 127",
        "answers": [
            [
                "I was able to get it working by setting the path to current dir rather than hitting / WORKDIR /miniconda RUN curl -so ./miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh \\ &amp;&amp; chmod +x ./miniconda.sh \\ &amp;&amp; ./miniconda.sh -b -p CONDA_DIR Here is the build result for reference docker build - &lt; Dockerfile Sending build context to Docker daemon 3.072kB Step 1/5 : FROM node:12.16.0-alpine ---&gt; 466593119d17 Step 2/5 : RUN apk update &amp;&amp; apk add --no-cache curl ---&gt; Using cache ---&gt; 1d6830c38dfa Step 3/5 : WORKDIR /miniconda ---&gt; Using cache ---&gt; 8ee9890a7109 Step 4/5 : WORKDIR /miniconda ---&gt; Running in 63238c179aea Removing intermediate container 63238c179aea ---&gt; 52f571393bf6 Step 5/5 : RUN curl -so ./miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; chmod +x ./miniconda.sh &amp;&amp; ./miniconda.sh -b -p CONDA_DIR ---&gt; Running in b59e945ad7a9 Removing intermediate container b59e945ad7a9 ---&gt; 74ce06c9af66 Successfully built 74ce06c9af66"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "We have been using CUDA 10.2 for our machine learning tasks and AWS EKS AMIs don't have Nvidia drivers that support CUDA 10.2 (driver version &gt;=440). Where should I get the required images?",
        "answers": [],
        "votes": []
    },
    {
        "question": "This link provided the original make bug i was facing. I patched my version of opencv exactly as was said and now i have new bugs. https://github.com/opencv/opencv/issues/17542 I am very new to C++. My main goal is to compile from source my custom opencv-python api into nvidia jetson nvidia docker image adapted for ARM64 processing architecture. The make -j6 compiled till 100 and gave new errors. from what i understand, i needed to change the naming of the cv2 Ptr functions and parameters from /app/opencv-stitch/modules/core/include/opencv2/core/cvstd_wrapper.hpp:122:97cv after having fix xfeatures2d namespace conflict from pyopencv_generated_funcs.h and pyopencv_generated_types_content.h . this is all foreign to me and i don't know where to start navigating the c++ api structure to know where to fix these or if im having the correct intuition at all. help would be much appreciated!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a Dockerfile based on nvidia/cuda like so: FROM nvidia/cuda:11.0-base ... I want to be able to build this Dockerfile on our CI server that does not have a Nvidia GPU. When I try to do that, I get this error: ------ &gt; [1/6] FROM docker.io/nvidia/cuda:11.0-base: ------ failed to solve with frontend dockerfile.v0: failed to solve with frontend gateway.v0: rpc error: code = Unknown desc = failed to build LLB: failed to load cache key: docker.io/nvidia/cuda:11.0-base not found The error says that the image is not found, but I think this is a bit misleading. I've been able to isolate the problem to whether or not a GPU is present. When building this Dockerfile on a server with a Nvidia GPU, I don't get this error. Is it possible to build a Dockerfile based on an nvidia/cuda image on a server without a GPU? This would save costs on our CI server. I plan to deploy the resulting docker container on a server that does have a GPU so, in other words, is it possible to defer the presence of a GPU to run time instead of build time?",
        "answers": [
            [
                "It sounds like you may need to load the nvidia components possibly including any proprietary blobs and kernel modules. If the modules are not present, this could be why the compile error (missing dependencies). But from this website https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html it looks like the drivers are looking for the hardware when they load, which is probably why they are not available when you attempt to compile."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I come across this error when running an image on nvidia-docker. This image used to run well, but now it fails. The change in the device I'm made: cloned docker/compose, then removed docker/compose. I manage to run nvidia-docker hello-world, but this does not use Nvidia drivers. I replaced XXX instead of the full path. running command: nvidia-docker run --name my_test arg1 bash Blockquote docker: Error response from daemon: create nvidia_driver_410.48: found reference to volume 'nvidia_driver_410.48' in driver 'nvidia-docker', but got an error while checking the driver: error while checking if volume \"nvidia_driver_410.48\" exists in driver \"nvidia-docker\": Post http://XXXdocker%2Fplugins%2Fnvidia-docker.sock/VolumeDriver.Get: dial unix XXX/docker/plugins/nvidia-docker.sock: connect: connection refused: volume name must be unique. Blockquote",
        "answers": [
            [
                "Try to delete the old containers(including stopped containers), list and delete the nvidia docker volumes. Restart docker daemon if this does not solve the problem. Else: try to purge/reinstall nvidia-container-toolkit and restart the docker daemon on top of the aforementioned step."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Hi I have several times compiled opencv in my host machine or several arm based system before. As all you now on arm based system compiling opencv takes longer time so I used Quemu to virtualized x86 processor to arm64 and I pulled nvidia jetpack from nvidia dochub page. I am using \"nvcr.io/nvidia/l4t-base\" so I have aarch64 based docker env. When I follow my basic cmake configuration in this docker env. cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D WITH_CUDA=ON \\ -D CUDA_ARCH_BIN=6.2 \\ -D CUDA_ARCH_PTX=\"\" \\ -D ENABLE_FAST_MATH=ON \\ -D CUDA_FAST_MATH=ON \\ -D WITH_CUBLAS=ON \\ -D WITH_LIBV4L=ON \\ -D WITH_GSTREAMER=ON \\ -D WITH_GSTREAMER_0_10=OFF \\ -D WITH_QT=ON \\ -D WITH_OPENGL=ON \\ -D OPENCV_EXTRA_MODULES_PATH=/../opencv3/opencv_contrib-3.4.9/modules \\ -D CPACK_BINARY_DEB=ON \\ ../ I got attached following error log. But basically it says \"fatal error: sys/videoio.h: No such file or directory\" so I previously got videdev.h error but I solved installing v4l package. Interesting point is when I give only cmake .. it works and install opencv but default conf. havent cuda and extra lib option. When I inspect after \"cmake ..\" command there is still same fatal error in log file even though I saw configuration done. I cant figure this error out. Even though it says with default cmake command same error but configuration goes done. When I apply cuda config it says same error but configuration cannnot be done. I am totaly sure cmake have any wrong parameter because of I have used on my jetson tx2 it works like perfect. Do you have any idea ? Btw nvidia docker has 10.2 cuda support I checked with nvcc -V command Maybe in docker env it is impposible to compile opencv with cuda support or Should I start cuda supported docker container different way ? Error log : https://paste.ubuntu.com/p/w9hjBxqJ6D/ Output log : https://paste.ubuntu.com/p/rqsvq356dR/ Here is another output linked by target \"opencv_annotation\" in directory /opencv3/opencv-3.4.9/apps/annotation linked by target \"opencv_visualisation\" in directory /opencv3/opencv-3.4.9/apps/visualisation linked by target \"opencv_interactive-calibration\" in directory /opencv3/opencv-3.4.9/apps/interactive-calibration linked by target \"opencv_version\" in directory /opencv3/opencv-3.4.9/apps/version CUDA_nppist_LIBRARY (ADVANCED) linked by target \"opencv_cudev\" in directory /opencv3/opencv-3.4.9/modules/cudev linked by target \"opencv_cudev\" in directory /opencv3/opencv-3.4.9/modules/cudev linked by target \"opencv_test_cudev\" in directory /opencv3/opencv-3.4.9/modules/cudev/test linked by target \"opencv_test_core\" in directory /opencv3/opencv-3.4.9/modules/core linked by target \"opencv_perf_core\" in directory /opencv3/opencv-3.4.9/modules/core linked by target \"opencv_core\" in directory /opencv3/opencv-3.4.9/modules/core linked by target \"opencv_core\" in directory /opencv3/opencv-3.4.9/modules/core linked by target \"opencv_test_cudaarithm\" in directory /opencv3/opencv-3.4.9/modules/cudaarithm linked by target \"opencv_cudaarithm\" in directory /opencv3/opencv-3.4.9/modules/cudaarithm linked by target \"opencv_cudaarithm\" in directory /opencv3/opencv-3.4.9/modules/cudaarithm linked by target \"opencv_perf_cudaarithm\" in directory /opencv3/opencv-3.4.9/modules/cudaarithm linked by target \"opencv_flann\" in directory /opencv3/opencv-3.4.9/modules/flann linked by target \"opencv_flann\" in directory /opencv3/opencv-3.4.9/modules/flann linked by target \"opencv_test_flann\" in directory /opencv3/opencv-3.4.9/modules/flann linked by target \"opencv_perf_imgproc\" in directory /opencv3/opencv-3.4.9/modules/imgproc linked by target \"opencv_test_imgproc\" in directory /opencv3/opencv-3.4.9/modules/imgproc linked by target \"opencv_imgproc\" in directory /opencv3/opencv-3.4.9/modules/imgproc linked by target \"opencv_imgproc\" in directory /opencv3/opencv-3.4.9/modules/imgproc linked by target \"opencv_test_ml\" in directory /opencv3/opencv-3.4.9/modules/ml linked by target \"opencv_ml\" in directory /opencv3/opencv-3.4.9/modules/ml linked by target \"opencv_ml\" in directory /opencv3/opencv-3.4.9/modules/ml linked by target \"opencv_video\" in directory /opencv3/opencv-3.4.9/modules/video linked by target \"opencv_video\" in directory /opencv3/opencv-3.4.9/modules/video linked by target \"opencv_perf_video\" in directory /opencv3/opencv-3.4.9/modules/video linked by target \"opencv_test_video\" in directory /opencv3/opencv-3.4.9/modules/video linked by target \"opencv_test_cudabgsegm\" in directory /opencv3/opencv-3.4.9/modules/cudabgsegm linked by target \"opencv_cudabgsegm\" in directory /opencv3/opencv-3.4.9/modules/cudabgsegm linked by target \"opencv_cudabgsegm\" in directory /opencv3/opencv-3.4.9/modules/cudabgsegm linked by target \"opencv_perf_cudabgsegm\" in directory /opencv3/opencv-3.4.9/modules/cudabgsegm linked by target \"opencv_test_cudafilters\" in directory /opencv3/opencv-3.4.9/modules/cudafilters linked by target \"opencv_perf_cudafilters\" in directory /opencv3/opencv-3.4.9/modules/cudafilters linked by target \"opencv_cudafilters\" in directory /opencv3/opencv-3.4.9/modules/cudafilters linked by target \"opencv_cudafilters\" in directory /opencv3/opencv-3.4.9/modules/cudafilters linked by target \"opencv_test_cudaimgproc\" in directory /opencv3/opencv-3.4.9/modules/cudaimgproc linked by target \"opencv_cudaimgproc\" in directory /opencv3/opencv-3.4.9/modules/cudaimgproc linked by target \"opencv_cudaimgproc\" in directory /opencv3/opencv-3.4.9/modules/cudaimgproc linked by target \"opencv_perf_cudaimgproc\" in directory /opencv3/opencv-3.4.9/modules/cudaimgproc linked by target \"opencv_test_cudawarping\" in directory /opencv3/opencv-3.4.9/modules/cudawarping linked by target \"opencv_cudawarping\" in directory /opencv3/opencv-3.4.9/modules/cudawarping linked by target \"opencv_cudawarping\" in directory /opencv3/opencv-3.4.9/modules/cudawarping linked by target \"opencv_perf_cudawarping\" in directory /opencv3/opencv-3.4.9/modules/cudawarping linked by target \"opencv_dnn\" in directory /opencv3/opencv-3.4.9/modules/dnn linked by target \"opencv_dnn\" in directory /opencv3/opencv-3.4.9/modules/dnn linked by target \"opencv_perf_dnn\" in directory /opencv3/opencv-3.4.9/modules/dnn linked by target \"opencv_test_dnn\" in directory /opencv3/opencv-3.4.9/modules/dnn linked by target \"opencv_features2d\" in directory /opencv3/opencv-3.4.9/modules/features2d linked by target \"opencv_features2d\" in directory /opencv3/opencv-3.4.9/modules/features2d linked by target \"opencv_perf_features2d\" in directory /opencv3/opencv-3.4.9/modules/features2d linked by target \"opencv_test_features2d\" in directory /opencv3/opencv-3.4.9/modules/features2d linked by target \"opencv_imgcodecs\" in directory /opencv3/opencv-3.4.9/modules/imgcodecs linked by target \"opencv_imgcodecs\" in directory /opencv3/opencv-3.4.9/modules/imgcodecs linked by target \"opencv_perf_imgcodecs\" in directory /opencv3/opencv-3.4.9/modules/imgcodecs linked by target \"opencv_test_imgcodecs\" in directory /opencv3/opencv-3.4.9/modules/imgcodecs linked by target \"opencv_test_photo\" in directory /opencv3/opencv-3.4.9/modules/photo linked by target \"opencv_photo\" in directory /opencv3/opencv-3.4.9/modules/photo linked by target \"opencv_photo\" in directory /opencv3/opencv-3.4.9/modules/photo linked by target \"opencv_perf_photo\" in directory /opencv3/opencv-3.4.9/modules/photo linked by target \"opencv_test_shape\" in directory /opencv3/opencv-3.4.9/modules/shape linked by target \"opencv_shape\" in directory /opencv3/opencv-3.4.9/modules/shape linked by target \"opencv_shape\" in directory /opencv3/opencv-3.4.9/modules/shape linked by target \"opencv_test_videoio\" in directory /opencv3/opencv-3.4.9/modules/videoio linked by target \"opencv_videoio\" in directory /opencv3/opencv-3.4.9/modules/videoio linked by target \"opencv_videoio\" in directory /opencv3/opencv-3.4.9/modules/videoio linked by target \"opencv_perf_videoio\" in directory /opencv3/opencv-3.4.9/modules/videoio linked by target \"opencv_calib3d\" in directory /opencv3/opencv-3.4.9/modules/calib3d linked by target \"opencv_calib3d\" in directory /opencv3/opencv-3.4.9/modules/calib3d linked by target \"opencv_perf_stitching\" in directory /opencv3/opencv-3.4.9/modules/stitching linked by target \"opencv_test_superres\" in directory /opencv3/opencv-3.4.9/modules/superres linked by target \"opencv_superres\" in directory /opencv3/opencv-3.4.9/modules/superres linked by target \"opencv_superres\" in directory /opencv3/opencv-3.4.9/modules/superres linked by target \"opencv_perf_superres\" in directory /opencv3/opencv-3.4.9/modules/superres linked by target \"opencv_test_videostab\" in directory /opencv3/opencv-3.4.9/modules/videostab linked by target \"opencv_videostab\" in directory /opencv3/opencv-3.4.9/modules/videostab linked by target \"opencv_videostab\" in directory /opencv3/opencv-3.4.9/modules/videostab linked by target \"opencv_traincascade\" in directory /opencv3/opencv-3.4.9/apps/traincascade linked by target \"opencv_createsamples\" in directory /opencv3/opencv-3.4.9/apps/createsamples linked by target \"opencv_annotation\" in directory /opencv3/opencv-3.4.9/apps/annotation linked by target \"opencv_visualisation\" in directory /opencv3/opencv-3.4.9/apps/visualisation linked by target \"opencv_interactive-calibration\" in directory /opencv3/opencv-3.4.9/apps/interactive-calibration linked by target \"opencv_version\" in directory /opencv3/opencv-3.4.9/apps/version Lats Progress: When I removed -DOPENCV_EXTRA_MODULES_PATH=$HOME/opencv_contrib-3.4.9/modules \\ -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-10.2 \\ -DCUDA_ARCH_BIN=6.2 \\ -DCUDA_ARCH_PTX=\"\" \\ and Configuring done but I still need cuda and extra lib",
        "answers": [
            [
                "Solved with starting docker image host Cuda components and giving cmake conf. the directory of the toolkit. The important point is that if container comes with different versions of Cuda you shouldn't use it. Use host cuda and give host cuda component directory or copy them into docker -D CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-10.0 \\"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Today, we updated the GPU driver for our host machine, and the new containers that we created are all working fine. However, all of our existing docker containers give the following error when running the nvidia-smi command inside: Failed to initialize NVML: Driver/library version mismatch How to rescue these old containers? Our previous GPU driver version in the host machine was 384.125 and it is now 430.64. Host Configuration nvidia-smi gives +-----------------------------------------------------------------------------+ | NVIDIA-SMI 430.64 Driver Version: 430.64 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla V100-DGXS... Off | 00000000:07:00.0 On | 0 | | N/A 40C P0 39W / 300W | 182MiB / 32505MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 Tesla V100-DGXS... Off | 00000000:08:00.0 Off | 0 | | N/A 40C P0 39W / 300W | 12MiB / 32508MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 2 Tesla V100-DGXS... Off | 00000000:0E:00.0 Off | 0 | | N/A 39C P0 40W / 300W | 12MiB / 32508MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 3 Tesla V100-DGXS... Off | 00000000:0F:00.0 Off | 0 | | N/A 40C P0 38W / 300W | 12MiB / 32508MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1583 G /usr/lib/xorg/Xorg 169MiB | +-----------------------------------------------------------------------------+ nvcc --version gives nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2017 NVIDIA Corporation Built on Fri_Sep__1_21:08:03_CDT_2017 Cuda compilation tools, release 9.0, V9.0.176 dpkg -l | grep -i docker gives ii dgx-docker-cleanup 1.0-1 amd64 DGX Docker cleanup script rc dgx-docker-options 1.0-7 amd64 DGX docker daemon options ii dgx-docker-repo 1.0-1 amd64 docker repository configuration file ii docker-ce 5:18.09.2~3-0~ubuntu-xenial amd64 Docker: the open-source application container engine ii docker-ce-cli 5:18.09.2~3-0~ubuntu-xenial amd64 Docker CLI: the open-source application container engine ii nvidia-container-runtime 2.0.0+docker18.09.2-1 amd64 NVIDIA container runtime ii nvidia-docker 1.0.1-1 amd64 NVIDIA Docker container tools rc nvidia-docker2 2.0.3+docker18.09.2-1 all nvidia-docker CLI wrapper docker version gives Client: Version: 18.09.2 API version: 1.39 Go version: go1.10.6 Git commit: 6247962 Built: Sun Feb 10 04:13:50 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 18.09.2 API version: 1.39 (minimum version 1.12) Go version: go1.10.6 Git commit: 6247962 Built: Sun Feb 10 03:42:13 2019 OS/Arch: linux/amd64 Experimental: false",
        "answers": [
            [
                "I ran into this issue as well. In my case, I had the line: apt install -y nvidia-cuda-toolkit in my Dockerfile. Removing this line resolved the issue. In general, I would recommend using an nvidia provided container compatible with the drivers on your local machine."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "System: I've a Windows Server 2019 OS installed with a NVIDIA Tesla T4 Tensor Core GPU. Goal: Planning to read real time streaming videos from an IP camera and to further process frame by frame. Goal is to leverage NVIDIA DeepStream SDK, but issue is, it isn't available for Windows OS. So, I'm thinking on the docker lines, but since am very new to docker containers, would like to know if I can install a docker on Windows and can run this deepstream docker image on that. If not, is there any way I can run this Linux based DeepStream docker image on Windows? Any help shall be greatly acknowledged.",
        "answers": [
            [
                "I have never worked with the windows server before it should be the same as a docker in Linux VM. First, you need to pull docker images for deepstream docker pull nvcr.io/nvidia/deepstream:5.0-dp-20.04-triton and then try to run sample apps provided in the docker image. Refer this for the procedure. if you are interested in python apps you can check sample apps here. Note:- make sure you are able to access display from inside the container cause deepstream use eglsink in their samples app which will try to open a display window on your screen or you can change the sink type to filesink if you want to save it is a file. Refer this for available plugins and their attributes."
            ],
            [
                "According to the post in Nivida forum, Windows not supported. As alternative, I wonder if anyone used the Nvidia Graph Composer in Windows."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I'm having trouble understanding how to deploy a PyTorch docker image on a remote machine that has an Nvidia GPU. I'm relatively unfamiliar with Docker in general, and this is my first time using Docker for the purposes of deep learning. I am able to train my models on my local machine without problems via a trivial command-line call to python main.py, which is what I'm familiar with. But now that I have to SSH into this machine I'm required to use Docker. So far I have pulled a few images without really knowing what I'm doing. So, here's what got me the furthest so far: First, I transferred my code to the machine via a GitLab repository -- nothing fancy. I also transferred some large datasets via SSH into the parent directory of my GitLab repository. I.e., it looks something like this: DeepLearningProject/ \u251c\u2500\u2500 Data_folder/ \u2502 \u251c\u2500\u2500 datasets/ \u2502 \u251c\u2500\u2500 results/ \u2502 \u2514\u2500\u2500 etc... \u2514\u2500\u2500 MyGitFolder/ \u251c\u2500\u2500 lots_of_python_files.py \u2514\u2500\u2500 main.py Next, I pulled the following image via docker pull anibali/pytorch:1.4.0-cuda10.1 as the version numbers satisfy our requirements. Here's where the guessing game got out of hand. So, I have an image with Pytorch and all the Nvidia stuff, but I'm still missing my own python package dependencies. I'm not sure where the installation hereof takes place, for example in a requirements.txt file. Thus, python reports missing packages when I run the following, as per a tutorial I found online: docker run --rm -it --init \\ --gpus=all \\ --ipc=host \\ --user=\"$(id -u):$(id -g)\" \\ --volume=\"$PWD:/app\" \\ anibali/pytorch:1.4.0-cuda10.1 python3 main.py So, I thought I could perhaps enter a bash terminal by excluding the python3 main.py part of the above command. This just opens the python3 interpreter interface. I assume this is due to the image's Dockerfile(?) ending on CMD [\"python3\"], and I'm unsure as to how I can install my requirements through this. As requested, please find an example of the error message for the missing python packages below: Traceback (most recent call last): File \"main.py\", line 16, in &lt;module&gt; from model import (generate_model, load_pretrained_model, make_data_parallel, File \"/app/model.py\", line 4, in &lt;module&gt; from models import resnet, resnet2p1d, pre_act_resnet, wide_resnet, resnext, densenet File \"/app/models/resnext.py\", line 9, in &lt;module&gt; from utils import partialclass File \"/app/utils.py\", line 7, in &lt;module&gt; from sklearn.metrics import precision_recall_fscore_support ModuleNotFoundError: No module named 'sklearn' I assume this image does not have sklearn in its requirements.txt. Is there a way I could 'pipe' in my own requirements.txt when executing docker run? I believe this to be my main issue here. Any advice would be appreciated.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to use TensorRt using the python API. I am trying to use it in multiple threads where the Cuda context is used with all the threads (everything works fine in a single thread). I am using docker with tensorrt:20.06-py3 image, and an onnx model, and Nvidia 1070 GPU. The multiple thread approach should be allowed, as mentioned here TensorRT Best Practices. I created the context in the main thread: cuda.init() device = cuda.Device(0) ctx = device.make_context() I tried two methods, first to build the engine in the main thread and use it in the execution thread. This case gives this error. [TensorRT] ERROR: ../rtSafe/cuda/caskConvolutionRunner.cpp (373) - Cask Error in checkCaskExecError&lt;false&gt;: 10 (Cask Convolution execution) [TensorRT] ERROR: FAILED_EXECUTION: std::exception Second, I tried to build the model in the thread it gives me this error: pycuda._driver.LogicError: explicit_context_dependent failed: invalid device context - no currently active context? The error appears when I call 'cuda.Stream()' I am sure that I can run multiple Cuda streams in parallel under the same Cuda context, but I don't know how to do it.",
        "answers": [
            [
                "I found a solution. The idea is to create a normal global ctx = device.make_context() Then in each execution thread do a: ctx.push() --- Execute Inference Code --- ctx.pop() The link for the source and full sample is here"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am using Tensorflow docker. Inside docker matplotlib is installed using apt-get install python-matplotlib. After installation tensorflow can't be imported. Before installation was ok. &gt;&gt;&gt; import tensorflow Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; ImportError: No module named tensorflow &gt;&gt;&gt;",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am currently trying to run Nvidia-docker on Jetson Xavier and jetson nano with the Tensorflow framework enabled inside. but the problem I\u2019m facing right now is related to \u201clibcublas.so\u201d. What I had tried the solution mentioned here: https://devtalk.nvidia.com/default/topic/1043951/jetson-agx-xavier/docker-gpu-acceleration-on-jetson-agx-for-ubuntu-18-04-image/post/5296647/#5296647 1 All package installations (pip installs and apt-get installs) completed successfully but when I try to import TensorFlow from both Python 2.7 or 3.6, I get the following error: ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory using Jetson Xavier or jetson nano?",
        "answers": [
            [
                "sudo docker run --net=host --rm --runtime nvidia --ipc=host -v /tmp/.X11-unix/:/tmp/.X11-unix /tmp/argus_socket:/tmp/argus_socket --cap-add SYS_PTRACE -e DISPLAY=$DISPLAY -it [container] I'm assuming it would work on Host first. Source: official forum but up to date. Device is no longer used"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Goal: I'm trying to use Nvidia GPU capabilities on a Minikube cluster that uses the default Docker driver. Problem: I'm able to use nvidia-docker with the default docker context, but when switching to minikube docker-env I get the following error: $ docker run --gpus all nvidia/cuda:10.0-base nvidia-smi docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]]. ERRO[0000] error waiting for container: context canceled Environment: Ubuntu 18.04 Minikube v1.10.0 Docker version: $ docker version Client: Docker Engine - Community Version: 19.03.10 API version: 1.40 Go version: go1.13.10 Git commit: 9424aeaee9 Built: Thu May 28 22:16:49 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 19.03.2 API version: 1.40 (minimum version 1.12) Go version: go1.12.9 Git commit: 6a30dfca03 Built: Wed Sep 11 22:45:55 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.3.3-14-g449e9269 GitCommit: 449e926990f8539fd00844b26c07e2f1e306c760 runc: Version: 1.0.0-rc10 GitCommit: docker-init: Version: 0.18.0 GitCommit: Nvidia Container Runtime version: $ nvidia-container-runtime --version runc version 1.0.0-rc10 commit: dc9208a3303feef5b3839f4323d9beb36df0a9dd spec: 1.0.1-dev Additional Info: The cluster was created with: minikube start --cpus 3 --memory 8G The following minikube addons are currently enabled: $ minikube addons list |-----------------------------|----------|--------------| | ADDON NAME | PROFILE | STATUS | |-----------------------------|----------|--------------| | dashboard | minikube | disabled | | default-storageclass | minikube | enabled \u2705 | | efk | minikube | disabled | | freshpod | minikube | disabled | | gvisor | minikube | disabled | | helm-tiller | minikube | disabled | | ingress | minikube | disabled | | ingress-dns | minikube | disabled | | istio | minikube | disabled | | istio-provisioner | minikube | disabled | | logviewer | minikube | disabled | | metallb | minikube | disabled | | metrics-server | minikube | disabled | | nvidia-driver-installer | minikube | enabled \u2705 | | nvidia-gpu-device-plugin | minikube | enabled \u2705 | | registry | minikube | disabled | | registry-aliases | minikube | disabled | | registry-creds | minikube | disabled | | storage-provisioner | minikube | enabled \u2705 | | storage-provisioner-gluster | minikube | disabled | |-----------------------------|----------|--------------| And this is a working example outside the minikube context: $ docker run --gpus all nvidia/cuda:10.0-base nvidia-smi Fri Jun 5 09:23:49 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 106... Off | 00000000:01:00.0 On | N/A | | 0% 51C P8 6W / 120W | 1293MiB / 6077MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| +-----------------------------------------------------------------------------+",
        "answers": [
            [
                "This is a community wiki answer. Feel free to edit and expand it if needed. Nvidia GPU is not officially supported with the docker driver for Minikube. This leaves you with two possible options: Try to use NVIDIA Container Toolkit and NVIDIA device plugin. This is a workaround way and might not be the best solution in your use case. Use the KVM2 driver or None driver. These two are officially supported and documented. I hope it helps."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I need to run a cuda binary on kubernetes. I've set up the nodes to use the kubernetes nvidia device plugin with nvidia-docker2. Here is my Dockerfile: FROM ubuntu:18.04 COPY addarrays /addarrays ENTRYPOINT [ \"/addarrays\" ] When I run the docker image through nvidia-docker2 or kubernetes it gives this error: Cuda failure addarrays.cu:9: 'CUDA driver version is insufficient for CUDA runtime version' Cuda failure addarrays.cu:9: 'CUDA driver version is insufficient for CUDA runtime version' Cuda failure addarrays.cu:9: 'CUDA driver version is insufficient for CUDA runtime version' Cuda failure addarrays.cu:9: 'CUDA driver version is insufficient for CUDA runtime version' Cuda failure addarrays.cu:9: 'CUDA driver version is insufficient for CUDA runtime version' Cuda failure addarrays.cu:9: 'CUDA driver version is insufficient for CUDA runtime version' addarrays: addarrays.cu:62: int main(): Assertion `hostArrayTmp[i] == hostArrayDest[i]' failed. It looks like my docker image needs an nvidia driver. I've modified the Dockerfile like this: FROM ubuntu:18.04 RUN apt install software-properties-common RUN add-apt-repository ppa:graphics-drivers RUN apt update RUN apt install nvidia-driver-440 COPY addarrays /addarrays ENTRYPOINT [ \"/addarrays\" ] The software-properties-common is needed to install add-apt-repository, but it fails with this message: E: Unable to locate package software-properties-common What do I need to do to get an nvidia driver installed in my docker image?",
        "answers": [
            [
                "You have to do the apt update first. On install commands you should use the -y flag. Also you should concatenate the commands into a single run command. FROM ubuntu:18.04 RUN apt update &amp;&amp; \\ apt install software-properties-common -y &amp;&amp; \\ add-apt-repository ppa:graphics-drivers &amp;&amp; \\ apt install nvidia-driver-440 -y COPY addarrays /addarrays ENTRYPOINT [ \"/addarrays\" ]"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Docker newbie question here, so please be nice. I know this might be asked before but I could not find anything related to nvidia-docker. I completed the installation instructions on the official guide. When I wanted to test Nvidia-docker: docker run --gpus all nvidia/cuda:10.0-base nvidia-smi I got this error: (base) user@adminme:~$ docker run --gpus all --rm nvidia/cuda nvidia-smi docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'. I found this answer here, but it felt a bit different for my case. I am very new to docker and still learning. let me know what you think? here is some information about my remote Linux machine: (base) user@adminme:~$ lspci | grep -i nvidia 02:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1080] (rev a1) 02:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1) nvidia-smi command: (base) user@adminme:~$ nvidia-smi Sun May 31 01:12:25 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.64.00 Driver Version: 440.64.00 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:02:00.0 Off | N/A | | 0% 33C P8 9W / 215W | 17MiB / 8116MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 2545 G /usr/lib/xorg/Xorg 15MiB | +-----------------------------------------------------------------------------+ docker-version : (base) user@adminme:~$ docker --version Docker version 19.03.10, build 9424aeaee9",
        "answers": [
            [
                "The quick fix would be to run the container using sudo: sudo docker run --gpus all nvidia/cuda:10.0-base nvidia-smi If you want to run docker as non-root user then you need to add it to the docker group. Create the docker group if it does not exist sudo groupadd docker Add your user to the docker group. sudo usermod -aG docker $USER Run the following command or Logout and login again and run (that doesn't work you may need to reboot your machine first) newgrp docker Check if docker can be run without root docker run --gpus all nvidia/cuda:10.0-base nvidia-smi Ref:- https://docs.docker.com/engine/install/linux-postinstall/"
            ],
            [
                "In addition to what nischay goyal answered, sometimes after adding the user to the docker group you have to do su - ${USER} in order to log out and log back."
            ]
        ],
        "votes": [
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "Can someone please tell me how to install NVIDIA docker in google colab? I searched for an installation method, but I could not get any..",
        "answers": [],
        "votes": []
    },
    {
        "question": "Running the snippet below takes twice as long when run inside a docker-container (keeping all python versions the same) using the Nvidia-docker runtime, then locally executed. I tried various different base images, but they have a similar (slower) runtime. I would have expected that running code inside a Docker container does not impact runtime performance? So I feel that I'm missing something. import numpy as np from time import time import torch def run(): print(\"doing run\") random_data = [] for _ in range(320): random_data.append(np.random.randint(256, size=(1, 84, 84))) tensor = torch.tensor(random_data, device=\"cuda\") print(tensor.shape) n_runs = int(1e3) runtimes = [] for i in range(n_runs): start = time() run() end = time() took = end-start runtimes.append(took) print(f\"It took {took} second\") print(\"====\") print(f\"avg_runtime: {np.average(runtimes)}\")",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to create a docker build in Xavier. When I run my piece of code without docker it works smooth and I got The CUDA compiler identification. But when I am trying to make a build with dockerfile it gave me an error of CUDA compiler identification is unknown. Below is my dockerfile steps: FROM nvcr.io/nvidia/l4t-base:r32.3.1 RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends make g++ &amp;&amp; apt-get install -y cmake gcc libopenblas-dev build-essential WORKDIR /home/username/docker_fc/tensorrt_l2norm_helper CMD [\"python3\", \"./step01_pb_to_uff.py\"] COPY . /home/username/docker_fc/ RUN cmake --version RUN nvcc --version RUN mkdir build &amp;&amp; cd build &amp;&amp; pwd &amp;&amp; cmake .. &amp;&amp; make I got error in the last step with cmake. my mvcc version is release 10.0, V10.0.326. my cmake version is 3.10.2 Can anyone tell me what is missing in Dockerfile?",
        "answers": [
            [
                "The base image of l4t does not load the runtime components of nvidia by default. They only have the stubs. If you want to do this, you will need to enable the default-runtime nvidia in the /etc/docker/daemon.json file. This will load all the runtime components such as nvcc. { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } }, } Just take note that if you do this, the size of your built docker will be larger"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'd like to train my model with tensorflow-gpu docker image can be pulled from the official. https://www.tensorflow.org/install/docker?hl=ja I pulled tensorflow/tensorflow:latest-gpu-py3 and try to run it. nvidis-smi shows as below and looks fine. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.33.01 Driver Version: 440.33.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1080 On | 00000000:01:00.0 Off | N/A | | 0% 36C P8 12W / 240W | 449MiB / 8118MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| +-----------------------------------------------------------------------------+ However once I run my training program, an error occurs and killed. Seems like it successfully detects gpu but it switches to see cpu for training. I don't know why and would like to fix it very much. Any advice will help. Thanks. 2020-04-24 04:40:09.584129: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1 2020-04-24 04:40:09.614730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 04:40:09.615432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1 coreClock: 1.8225GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s 2020-04-24 04:40:09.615467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-04-24 04:40:09.615499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-04-24 04:40:09.633870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-04-24 04:40:09.638759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-04-24 04:40:09.673825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-04-24 04:40:09.678372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-04-24 04:40:09.678419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-04-24 04:40:09.678636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 04:40:09.679369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 04:40:09.679907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0 2020-04-24 04:40:09.680346: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2020-04-24 04:40:09.709091: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299130000 Hz 2020-04-24 04:40:09.709813: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5ee7c50 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-04-24 04:40:09.709844: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-04-24 04:40:09.808073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 04:40:09.808621: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5ee9ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-04-24 04:40:09.808639: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1 2020-04-24 04:40:09.808809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 04:40:09.811968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1 coreClock: 1.8225GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s 2020-04-24 04:40:09.812004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-04-24 04:40:09.812017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-04-24 04:40:09.812034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-04-24 04:40:09.812048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-04-24 04:40:09.812060: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-04-24 04:40:09.812074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-04-24 04:40:09.812084: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-04-24 04:40:09.812157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 04:40:09.812612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 04:40:09.813015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0 2020-04-24 04:40:09.813534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-04-24 04:40:10.318993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-04-24 04:40:10.319040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 2020-04-24 04:40:10.319050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N 2020-04-24 04:40:10.319956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 04:40:10.320671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 04:40:10.321302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1) training starts Epoch 1/1 2020-04-24 04:40:12.664294: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2763676800 exceeds 10% of system memory. 2020-04-24 04:40:14.068163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 Killed and after I did pip install tensorflow-gpu==1.15.0, I get this error instead. 2020-04-24 07:33:12.894652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1 2020-04-24 07:33:12.908383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 07:33:12.909008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225 pciBusID: 0000:01:00.0 2020-04-24 07:33:12.909092: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory 2020-04-24 07:33:12.909139: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory 2020-04-24 07:33:12.909184: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory 2020-04-24 07:33:12.909229: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory 2020-04-24 07:33:12.909274: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory 2020-04-24 07:33:12.909317: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory 2020-04-24 07:33:12.912485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-04-24 07:33:12.912518: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... 2020-04-24 07:33:12.912823: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2020-04-24 07:33:12.937080: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299130000 Hz 2020-04-24 07:33:12.937348: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5231640 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-04-24 07:33:12.937374: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-04-24 07:33:13.027806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-24 07:33:13.028359: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4e45350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-04-24 07:33:13.028377: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1 2020-04-24 07:33:13.028453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-04-24 07:33:13.028462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165] WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead. Epoch 1/10 2020-04-24 07:33:13.930867: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2763676800 exceeds 10% of system memory. 1/69600 [..............................] - ETA: 62:22:56 - loss: 5.8635e-042020-04-24 07:33:16.725973: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2763676800 exceeds 10% of system memory. 2/69600 [..............................] - ETA: 56:18:29 - loss: 3.3783e-042020-04-24 07:33:19.324047: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2763676800 exceeds 10% of system memory. 3/69600 [..............................] - ETA: 54:16:40 - loss: 0.0038 2020-04-24 07:33:21.922656: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2763676800 exceeds 10% of system memory. 4/69600 [..............................] - ETA: 53:18:49 - loss: 0.01262020-04-24 07:33:24.531029: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2763676800 exceeds 10% of system memory. 46/69600 [..............................] - ETA: 50:50:19 - loss: 0.0270",
        "answers": [
            [
                "What version of tensorflow are you using? The current version installs both CPU and GPU support. Earlier versions require installing a separate package for gpu support: pip install tensorflow-gpu==1.15 It may also be a memory issue. See this thread: How can I solve 'ran out of gpu memory' in TensorFlow"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I encounter a problem with OpenCV that I have for several days now : it segfaults when calling the cv2.VideoCapture() function. When launching my script (with GDB) : extract-all_1 | Thread 1 \"python3\" received signal SIGSEGV, Segmentation fault. extract-all_1 | 0x00007f83857fe33b in bool pyopencv_to&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;(_object*, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;&amp;, ArgInfo const&amp;) [clone .isra.1286] () extract-all_1 | from /usr/lib/python3/dist-packages/cv2/python-3.6/cv2.cpython-36m-x86_64-linux-gnu.so extract-all_1 | (gdb) quit When running my script without GDB, the container exits with code 139 I identified the problem occures when calling the \"cv2.VideoCapture()\" function : def perform_video_extraction(video_path): input_movie = cv2.VideoCapture(video_path) nb_total_frames = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT)) [...] Hints : I process MP4 video files I've tried compressing my videos that are &gt;30fps to 25fps I've tried with OpenCV 3.4.9, 4.1.0, 4.1.1, 4.1.2, 4.2.0 and 4.3.0 (pip install) I've tried compiling OpenCV 4.2.0 and 4.3.0 from source I've tried each version above successively with CUDA 10.0, 10.1 and 10.2 : each version for each case produces the same error This segfault is not reproduced when using the CPU (non-cuda) version of OpenCV Here is my Dockerfile (CUDA 10.2 with OpenCV 4.2.0 built from source) : https://pastebin.com/raw/a42wtcRG Here is what the cmake summary build returns : https://pastebin.com/raw/SFPUakyL My config : Ubuntu 18.04 Nvidia Docker (CUDA 10.2, CUDNN 7, Ubuntu 18.04, devel) Python 3.6 Have you any recommendation for debugging this problem ? Thank you",
        "answers": [
            [
                "I managed to debug the problem. Due to a stupid encoding issue. Adding : ENV LANG C.UTF-8 to my Dockerfile managed to make the container run (my original pastebin mentioned this line but after doublecheck, I didn't have it). I was able to find out this idea because of this more accurate backtrace from GDB : root@f42846d26d89:/opencv-4.2.0/build# gdb --args python3 -u /usr/app/scripts/extract.py GNU gdb (Ubuntu 8.1-0ubuntu3.2) 8.1.0.20180409-git Copyright (C) 2018 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"x86_64-linux-gnu\". Type \"show configuration\" for configuration details. For bug reporting instructions, please see: &lt;http://www.gnu.org/software/gdb/bugs/&gt;. Find the GDB manual and other documentation resources online at: &lt;http://www.gnu.org/software/gdb/documentation/&gt;. For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\"... Reading symbols from python3...(no debugging symbols found)...done. (gdb) run Starting program: /usr/bin/python3 -u /usr/app/scripts/extract.py warning: Error disabling address space randomization: Operation not permitted [Thread debugging using libthread_db enabled] Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\". [...] Thread 1 \"python3\" received signal SIGSEGV, Segmentation fault. getUnicodeString (str=\"\", obj=&lt;optimized out&gt;) at /opencv-4.2.0/modules/python/src2/pycompat.hpp:69 69 if (PyBytes_Check(bytes)) (gdb) backtrace #0 0x00007f2959a1433b in getUnicodeString (str=\"\", obj=&lt;optimized out&gt;) at /opencv-4.2.0/modules/python/src2/pycompat.hpp:69 #1 0x00007f2959a1433b in pyopencv_to&lt;std::__cxx11::basic_string&lt;char&gt; &gt;(PyObject*, cv::String&amp;, ArgInfo const&amp;) (obj=&lt;optimized out&gt;, value=\"\", info=...) at /opencv-4.2.0/modules/python/src2/cv2.cpp:731 #2 0x00007f2959dd6a2d in pyopencv_cv_VideoCapture_VideoCapture(pyopencv_VideoCapture_t*, PyObject*, PyObject*) (self=0x7f2965344190, args=0x7f296307c3c8, kw=0x0) at /opencv-4.2.0/build/modules/python_bindings_generator/pyopencv_generated_types_content.h:21272 #3 0x0000000000551b81 in () #4 0x00000000005aa6ec in _PyObject_FastCallKeywords () #5 0x000000000050abb3 in () #6 0x000000000050c5b9 in _PyEval_EvalFrameDefault () #7 0x0000000000509d48 in () #8 0x000000000050aa7d in () #9 0x000000000050c5b9 in _PyEval_EvalFrameDefault () #10 0x0000000000508245 in () #11 0x000000000050b403 in PyEval_EvalCode () #12 0x0000000000635222 in () #13 0x00000000006352d7 in PyRun_FileExFlags () #14 0x0000000000638a8f in PyRun_SimpleFileExFlags () #15 0x0000000000639631 in Py_Main () #16 0x00000000004b0f40 in main () (gdb) list 64 { 65 bool res = false; 66 if (PyUnicode_Check(obj)) 67 { 68 PyObject * bytes = PyUnicode_AsUTF8String(obj); 69 if (PyBytes_Check(bytes)) 70 { 71 const char * raw = PyBytes_AsString(bytes); 72 if (raw) 73 { (gdb) /opencv-4.2.0 being my install path It seems like my filenames were not in a right encoding format. Finally, I specify that pip installing the python binding directly works perfectly fine now this modification has been brought."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I was trying to re-implement this code from Github and it requires me to install nvidia-docker and run it. The installation of nvidia-docker seemed successful. However, when I run the command nvidia-docker run -it --ipc=host deep-colorization, it throws the following error:: docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:432: running prestart hook 1 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\\\\\n\\\\\\\"\\\"\": unknown. ERRO[0002] error waiting for container: context canceled I am not sure what the error means as I don't have any previous experience with the docker ecosystem. Any kind of assistance is appreciated. I am running Ubuntu 18 by the way. Thanking you in advance.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm running nvprof to profile GPU usage of a TensorRT server-client model. Here's what I'm doing: Run nvprof on terminal 1 within a docker container with TensorRT enabled, nvprof --profile-all-processes -o results%p.nvvp Run TensorRT server on terminal 2 within the same docker container as the first step Request a service on terminal 3 within a different docker container as the first two steps. When the third step finishes, the client exists normally but the server and nvprof are kept running. So naturally, I closed the TensorRT server with ctrl-c. When I do this, on terminal 1 (running nvprof) it tells me that the application has had an internal profiling error, and the resulting output file does not have any timeline information on it. (It is only a 380KB big, whereas other files run about the same duration, 2-3 minutes, are about a few MB big at least) It seemed like ending TensorRT server with ctrl-C is the problem, so I tried to give nvprof a timeout option, namely nvprof --profile-all-processes -o results%p.nvvp --timeout 200 in the first step (200 seconds is more than enough for the whole process to finish) But while this does make nvprof raise this message: Execution timeout, stopping the application..., it does not actually stop the TensorRT server. Basically, I'd like to know if there's any way to stop a running TensorRT server exit normally without using ctrl-C, or if there is a workaround with this issue using nvprof and TensorRT together. Any help or push in the right direction would be greatly appreciated. Thanks! P.S. Original question was posted here about 3 hours ago.",
        "answers": [
            [
                "So it turns out, TensorRT was not the problem. When creating and first running the docker container for the server, I have not added the privileged option. Running docker container with docker run --rm -it -d --gpus all --privileged ... helps nvprof profile the server behavior even when the server program is killed with Ctrl-C."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have two machines with different OS. One is ubuntu 18.04 and the other is debian buster. Both of them have docker installations ( from here ) and and the latest nvidia-docker installed. Both systems have a gpu which I want to use inside the containers. So I created a dockerfile which will install everything I require. FROM ubuntu:18.04 RUN apt update -y RUN apt upgrade -y RUN apt install \\ nvidia-cuda-toolkit \\ -y In the first machine which has as host ubuntu and a gpu that require .435 driver to work it works fine (docker run --rm --gpus all my-image:1 nvidia-smi) But when I try the same dockerfile with 0 changes on my Debian machine it will return missmatched version of drivers. The second matching which has as a host debian and a gpu that require .418 dirver to work it complains about missmatched versions. Isn't supposed to be host independent installation ? What am I missing and I cannot build the same Dockerfile in both systems? I can see toolkit bringing drivers on its own but as it is apt installation I have no control over it. Thank you in advance",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've been attempting to deploy a machine learning solution with Tensorflow Serving on an embedded device (Jetson Xavier [ARMv8]) One model used by the solution is a stock Xception network, generated by: xception = tf.keras.applications.Xception(include_top=False, input_shape=(299, 299, 3), pooling=None xception.save(\"./saved_xception_model/1\", save_format=\"tf\") Running the Xception model on the device generates reasonable performance - about 0.1s to predict, ignoring all processioning: xception = tf.keras.models.load_model(\"saved_xception_model/1\", save_format=\"tf\") image = get_some_image() # image is numpy.ndarray image.astype(\"float32\") image /= 255 image = cv2.resize(image, (299, 299)) # Tensorflow predict takes ~0.1s xception.predict([image]) However, once the model is running in a Tensorflow Serving GPU container, via Nvidia-Docker, the model is much slower - about 3s to predict. I've been trying to isolate the cause of the poor performance, and I've run out of ideas. So far I've tested: Tweaking TF Serving's batching parameters to go all out on latency (batch_timeout_micros: 0, max_batch_size: 1, and noticed a modest 0.5s gain in performance. Optimizing the model with TensorRT via saved_model_cli. Running the Xception model in isolation, as the only model being served by TF Serving. Experimenting with doubling the memory allocated per TF process. Experimenting with enabling and disabling batching altogether. Experimenting with enabling and disabling model warmup. I would expect TF Serving to provide the same (more or less, allowing for GRPC encoding and decoding) prediction time as TF, and they do for other models I'm running. None of my efforts have got Xception upto the ~0.1s performance I would expect. My install of Tensorflow is built by Nvidia, from TF version 2.0. My TF Serving container is self-built from the TF-Serving 2.0 source, with GPU support. I start a Tensorflow Serving container as follows: tf_serving_cmd = \"docker run --runtime=nvidia -d\" tf_serving_cmd += \" --name my-container\" tf_serving_cmd += \" -p=8500:8500 -p=8501:8501\" tf_serving_cmd += \" --mount=type=bind,source=/home/xception_model,target=/models/xception_model\" tf_serving_cmd += \" --mount=type=bind,source=/home/model_config.pb,target=/models/model_config.pb\" tf_serving_cmd += \" --mount=type=bind,source=/home/batching_config.pb,target=/models/batching_config.pb\" # Self built TF serving image for Jetson Xavier, ARMv8. tf_serving_cmd += \" ${MY_ORG}/serving\" # I have tried 0.5 with no performance difference. # TF-Serving does not complain it wants more memory in either case. tf_serving_cmd += \" --per_process_gpu_memory_fraction:0.25\" tf_serving_cmd += \" --model_config_file=/models/model_config.pb\" tf_serving_cmd += \" --flush_filesystem_caches=true\" tf_serving_cmd += \" --enable_model_warmup=true\" tf_serving_cmd += \" --enable_batching=true\" tf_serving_cmd += \" --batching_parameters_file=/models/batching_config.pb\" I'm at the point where I'm starting to wonder if this is a bug in TF-Serving, although I have no idea where (Yeah, I know it's never a bug, it's always the user...) Can anyone suggest why TF-Serving might underperform compared to TF?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to install Kubernetes and docker 19.03 with NVIDIA GPU supporting. Before docker 19.03, the default rumtime of docker needs to be assigned to nvidia. Now the method is not supported, the recommend method is to insert \"--gpus all\" in command line. Is there any way to make \"--gpus all\" as the default setting of docker? It is also acceptable to change the command of Kubernetes for invoking docker, but I have not found the solution. BTW, I don't want to use NVIDIA's k8s-device-plugin because I want to control GPUs by myself. I just need all GPUs are exposed to PODs.",
        "answers": [
            [
                "According to NVIDIA's documents, we need to install Nvidia docker 2.0 even if it is not a recommended method. After the installing, you can set the Nvidia runtime as the default. Kubernetes cannot support the new command \"--gpus all\" currently."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a deep learning workstation where there are 4 GPUs with 6 GB of memory each. Would it be possible to make a docker container see the 4 GPUs as one but with 24 GB? Thank you.",
        "answers": [
            [
                "Short answer: No. Alternate answer: Yes, but requires additional hardware, expensive, and probably incompatible with your existing hardware. Explanation: It is possible if your GPUs are connected using NVIDIA NVLink (take a look at the details here https://www.nvidia.com/en-us/design-visualization/nvlink-bridges/). Usually NVLink used for pairs of GPUs, like GPU0 connected with GPU1 and GPU2 connected with GPU3, in this case best option you can obtain is 2 GPUs with doubled memory each. Another option is special InfiniBand module, installed to modern GPU servers by some vendors."
            ],
            [
                "I haven't work with docker before but work a lot with CUDA with Multiple GPU. Since multiple GPUs is physically are separated, hence working with multiple GPUs required a lot of memory synchronization in code level. I don't think that docker can virtually merge all the GPU memory as that will make the computation very complicated on the GPU side. working with Multiple GPU required custom kernel to synchronize to each other. The best analogy I relate is, \"Can you get two bare-metal computers to merge the RAM and run Microsoft Word as if it were a single machine?\"."
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am trying to setup one small kubenertes cluster on my ubuntu 18.04 LTS server. Now every step is done, but checking the GPU status fails. The container keeps reporting errors: 1. Issue Description I have done steps by Quick-Start, but when I run the test case, it reports error. 2. Steps to reproduce the issue exec shell cmd docker run --security-opt=no-new-privileges --cap-drop=ALL --network=none -it -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins nvidia/k8s-device-plugin:1.9 check the erros 2020/02/09 00:20:15 Starting to serve on /var/lib/kubelet/device-plugins/nvidia.sock 2020/02/09 00:20:15 Could not register device plugin: rpc error: code = Unimplemented desc = unknown service deviceplugin.Registration 2020/02/09 00:20:15 Could not contact Kubelet, retrying. Did you enable the device plugin feature gate? 2020/02/09 00:20:15 You can check the prerequisites at: https://github.com/NVIDIA/k8s-device-plugin#prerequisites 2020/02/09 00:20:15 You can learn how to set the runtime at: https://github.com/NVIDIA/k8s-device-plugin#quick-start 3. Environment Information - outputs of nvidia-docker run --rm dlws/cuda nvidia-smi NVIDIA-SMI 440.48.02 Driver Version: 440.48.02 CUDA Version: 10.2 outputs of nvidia-docker run --rm dlws/cuda nvidia-smi NVIDIA-SMI 440.48.02 Driver Version: 440.48.02 CUDA Version: 10.2 contents of /etc/docker/daemon.json contents: { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } } } docker version: 19.03.2 kubernetes version: 1.15.2",
        "answers": [
            [
                "Finally I found the answer, hope this post would be helpful for others who encounter the same issue: For kubernetes 1.15, use k8s-device-plugin:1.11 instead. The version 1.9 is not able to communicate with kubelet."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Note: This is on a headless AWS box on VNC, the current desktop I'm running on is DISPLAY=:1.0 I am trying to build a container that can hold an opengl application but I'm having trouble getting vglrun to work correctly. I am currently running it with --gpus all on the docker run line as well # xhost +si:localuser:root # docker run --rm -it \\ -e DISPLAY=unix$DISPLAY \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ --gpus all centos:7 \\ sh -c \"yum install epel-release -y &amp;&amp; \\ yum install -y VirtualGL glx-utils &amp;&amp; \\ vglrun glxgears\" No protocol specified [VGL] ERROR: Could not open display :0 On the host: $ nvidia-smi Tue Jan 28 22:32:24 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.44 Driver Version: 440.44 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla M60 Off | 00000000:00:1E.0 Off | 0 | | N/A 30C P8 16W / 150W | 56MiB / 7618MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 2387 G /usr/bin/X 55MiB | +-----------------------------------------------------------------------------+ I can confirm running glxgears without vglrun works fine but my application I'm trying to build into docker inherently uses vglrun. I have also tried using the nvidia container nvidia/opengl:1.1-glvnd-runtime-centos7 with no success. running it with vglrun -d :1.0 glxgears or vglrun -d unix:1.0 glxgears gives me this error: Error: couldn't get an RGB, Double-buffered visual What am I doing wrong here? does vglrun not work in a container? EDIT: It seems I was approaching this problem the wrong way, it seems to work when I'm on the primary :0 display but when using VNC to view display :1, the Mesa drivers get used instead of the Nvidia ones. Is there a way I can use the GPU on spawned VNC displays?",
        "answers": [
            [
                "I met the same problem and I solved by setting the variable \"VGL_DISPLAY\". docker run ... -e VGL_DISPLAY=$DISPLAY ... Then it worked! Please try."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm running the nvcr.io/nvidia/tensorflow:19.12-tf2-py3 docker image with the following runtime information: Tensorflow 2.0.0 (tf.__version__) Python 3.6 (!python --version) TensorRT 6.0.1 (!dpkg -l | grep nvinfer) cuda 10.2 I have built a model in TensorFlow 2.0 and converted+saved it to a dir: 1/ \u251c\u2500\u2500 assets/ | \u2514\u2500\u2500 trt-serialized-engine.TRTEngineOp_0 \u251c\u2500\u2500 variables/ | \u251c\u2500\u2500 variables.data-00000-of-00002 | \u251c\u2500\u2500 variables.data-00001-of-00002 | \u2514\u2500\u2500 variables.index \u2514\u2500\u2500 saved_model.pb Now when I try deserializing the cuda engine with the TensorRT python API: import tensorrt as trt TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE) serialized_engine = './tmp/unet-FP32/1/assets/trt-serialized-engine.TRTEngineOp_0' # serialized_engine = './tmp/unet-FP32/1/saved_model.pb' trt_runtime = trt.Runtime(TRT_LOGGER) with open(serialized_engine, 'rb') as f: engine = trt_runtime.deserialize_cuda_engine(f.read()) I receive the following error messages: [TensorRT] ERROR: ../rtSafe/coreReadArchive.cpp (31) - Serialization Error in verifyHeader: 0 (Magic tag does not match) [TensorRT] ERROR: INVALID_STATE: std::exception [TensorRT] ERROR: INVALID_CONFIG: Deserialize the cuda engine failed. I do the saving and loading on the exact same machine, inside the exact same docker container. Am I wrong to assume that 'trt-serialized-engine.TRTEngineOp_0' contains the actual serialized model? I have also tried doing it with the uff-parserm, but the uff shipped in the NVidia container is incompatible with tensorflow 2.0. Any ideas how to deserialize my trt engine?",
        "answers": [
            [
                "If the engine was created and ran on different versions, this may happen. TensorRT engines are not compatible across different TensorRT versions. Go to this reference for more info."
            ],
            [
                "For future readers, if you get this error building and running on the same machine or even the same container, your tensorrt python package might have a different version than your system package try the following commands to check In python: import tensorrt as trt trt.__version__ In terminal: dpkg -l | grep TensorRT"
            ]
        ],
        "votes": [
            4.0000001,
            1.0000001
        ]
    },
    {
        "question": "I'm trying to run a docker container with GPU access and one that does not remove itself when it exits. I'm trying nvidia-docker run -it -v ~/dir/to/my/data:/data nvidia-smi but it tells me: Unable to find image 'nvidia-smi:latest' locally docker: Error response from daemon: pull access denied for nvidia-smi, repository does not exist or may require 'docker login'. See 'docker run --help'. Now I have checked zaproxy: unable to find image 'in:latest' locally this question and (even though I do think my problem is a little different) tried nvidia-docker run -it -v '~/dir/to/my/data':/data nvidia-smi only to get the same error. I have also created a docker account and logged in with docker login but it doesn't seem to do me any good. How can I solve this problem? Any help or push in the right direction would be greatly appreciated. Thanks!",
        "answers": [
            [
                "You are missing the Docker image name in your command. nvidia-smi is the command name, not the image name. Add nvidia/cuda:9.0-base right before it. That's a Docker image that exists. nvidia-docker run -it -v ~/dir/to/my/data:/data nvidia/cuda:9.0-base nvidia-smi"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a GPU application that does unit-testing during the image building stage. With Docker 19.03, one can specify nvidia runtime with docker run --gpus all but I also need access to the gpus for docker build because I do unit-testing. How can I achieve this goal? For older version of docker that use nvidia-docker2 it was not possible to specifiy runtime during build stage, BUT you can set the default runtime to be nvidia, and docker build works fine that way. Can I do that in Docker 19.03 that doesn't need nvidia-docker anymore? If so, how?",
        "answers": [
            [
                "You need use nvidia-container-runtime as explained in docs: \"It is also the only way to have GPU access during docker build\". Steps for Ubuntu: Install nvidia-container-runtime: sudo apt-get install nvidia-container-runtime Edit/create the /etc/docker/daemon.json with content: { \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } }, \"default-runtime\": \"nvidia\" } Restart docker daemon: sudo systemctl restart docker Build your image (now GPU available during build): docker build -t my_image_name:latest ."
            ],
            [
                "A \"solution\" I found is to first run a base image with the host nvidia drivers mounted on it docker run -it --rm --gpus ubuntu And then build my app within the container manually and commit the resulting image. This is not ideal and it would be best to have access to nvidia-smi during the build phase."
            ],
            [
                "IMPORTANT NOTICE (in addition to the existing answer) Currently (march 2023), if you have docker compose installed, just configuring the default runtime may still not be enough. In addition to configuring the default runtime, you have to disable the default docker build kit, with: DOCKER_BUILDKIT=0 docker build &lt;blah&gt; This applies even if you're not using docker compose, but it applies to docker compose as well of course. See also: https://github.com/docker/compose/issues/9681 https://forums.developer.nvidia.com/t/docker-build-starts-ignoring-default-runtime-setting-after-initially-working/111155/11"
            ]
        ],
        "votes": [
            60.0000001,
            7.0000001,
            7.0000001
        ]
    },
    {
        "question": "I am trying to call Laia - a deeplearning toolkit for HRW : https://github.com/jpuigcerver/Laia This is the code I have: INPUT_DIR=`pwd`/RecognitionHand/dir_input OUTPUT_DIR=`pwd`/RecognitionHand/dir_output CHAR_TRANSCRIBE_FILE=char.txt WORD_TRANSCRIBE_FILE=word.txt rm $INPUT_DIR/filelist/filenames.lst ls -d -1 $INPUT_DIR/images/* &gt; $INPUT_DIR/filelist/filenames.lst COMMAND=\"decode --batch_size 20 --log_level info --symbols_table \\ $INPUT_DIR/symbtable/symbs.txt \\ $INPUT_DIR/model/model_htr.t7 \\ $INPUT_DIR/filelist/filenames.lst&gt; $OUTPUT_DIR/$CHAR_TRANSCRIBE_FILE\"; # local volumes mapped to the docker volumes OPTS=( -u $(id -u):$(id -g) ); [ -d \"/home\" ] &amp;&amp; OPTS+=( -v /home:/home ); [ -d \"/mnt\" ] &amp;&amp; OPTS+=( -v /mnt:/mnt ); [ -d \"/media\" ] &amp;&amp; OPTS+=( -v /media:/media ); [ -d \"/tmp\" ] &amp;&amp; OPTS+=( -v /tmp:/tmp ); # call the GPU docker for transcribing docker run --rm -t \"${OPTS[@]}\" laia:active \\ bash -c \"cd $(pwd) &amp;&amp; PATH=\\\" .:$PATH:\\$PATH\\\" laia-$COMMAND\"; The last docker command refers to nvidia-docker and I am getting this strange error: THCudaCheck FAIL file=/tmp/luarocks_cutorch-scm-1-918/cutorch/lib/THC/THCGeneral.c line=66 error=35 : CUDA driver version is insufficient for CUDA runtime version [2020-01-02 14:43:45 WARN] /opt/torch/share/lua/5.1/laia/util/base.lua:39: Optional lua module \"cutorch\" was not found! [2020-01-02 14:43:45 WARN] /opt/torch/share/lua/5.1/laia/util/base.lua:39: Optional lua module \"cunn\" was not found! [2020-01-02 14:43:45 WARN] /opt/torch/share/lua/5.1/laia/util/base.lua:39: Optional lua module \"laia.util.cudnn\" was not found! [2020-01-02 14:43:45 WARN] /opt/torch/share/lua/5.1/laia/util/base.lua:39: Optional lua module \"laia.ImageDistorter\" was not found! /opt/torch/bin/luajit: /opt/torch/lib/luarocks/rocks/laia/scm-1/bin/laia-decode:16: attempt to call field 'registerOptions' (a nil value) stack traceback: /opt/torch/lib/luarocks/rocks/laia/scm-1/bin/laia-decode:16: in main chunk [C]: at 0x00405d50 Why is this happening? Has someone met a similar error when running nvidia-docker please?",
        "answers": [
            [
                "CUDA driver version is insufficient for CUDA runtime version means that the nvidia driver for your system is not compatible to the runtime inside the docker image you download. You have to (at least) match those versions. Another important thing is to check what is the necessary cuda runtime for the tool that you want to use. Assuming that you followed the necessary version and downloaded the correct docker image, you will need to update your system nvidia driver to match the docker image then."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to create a Windows Docker Container with access to GPUs. To start I just wanted to try check if I can access GPU on Docker containers. Dockerfile FROM mcr.microsoft.com/windows:1903 CMD [ \"ping\", \"-t\", \"localhost\" ] Build and run docker build -t debug_image . docker run -d --gpus all --mount src=\"C:\\Program Files\\NVIDIA Corporation\\NVSMI\",target=\"C:\\Program Files\\NVIDIA Corporation\\NVSMI\",type=bind debug_image docker exec -it CONTAINER_ID powershell Problem and question Now that I'm inside, I try to execute my shared NVIDIA SMI executable. However, I got an error and it's not capable of running. The obvious question is why, if the host is capable. PS C:\\Program Files\\NVIDIA Corporation\\NVSMI&gt; .\\nvidia-smi.exe NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. This can also be happening if non-NVIDIA GPU is running as primary display, and NVIDIA GPU is in WDDM mode. About NVIDIA Driver, AFAIK it should not return any problem, since it works on the HOST, where NVIDIA Driver is installed. My host has 2 NVIDIA GPUs, and it has no \"primary\" display as it's a server with no screen connected. AFAIK, it's CPU doesn't have an integrated GPU, so I would assume one of the connected NVIDIA GPUs is the primary display (if it does exist when no display is connected to the server)(also, I think one should be it, because one renders the screen when I connect through TeamViewer if needed, and dxdiag returns one of them as Display 1). About WDDM mode, I've found ways to change it, but didn't found ways to check the current mode. So basically the question, is why is it not working? Any insight or help in the previous points would be helpful. Update. About: 1) I've updated my drivers from 431 to 441, latest version available for GTX 1080 Ti, and the error message remains the same. 2-3) I've confirmed that GTX (Except some Titan models) cannot run in TCC mode. Therefore they're running in WDDM mode.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to optimize YoloV3 using tensorRT I came this post called Have you Optimized your Deep Learning Model Before Deployment? Docker is used in the post. Used Enabling GPUs in the Container Runtime Ecosystem to install nvidia-docker2 Pulled the latest version of the docker image docker pull aminehy/tensorrt-opencv-python3:version-1.3 from aminehy/tensorrt-opencv-python3 Here are the images $ sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZE nvcr.io/nvidia/cuda 10.1-cudnn7-devel-ubuntu18.04 b4879c167fc1 2 weeks ago 3.67GB aminehy/tensorrt-opencv-python3 version-1.3 0302e477816d 4 months ago 5.36GB aminehy/tensorrt-opencv-python3 latest 604502819d12 4 months ago 4.94GB aminehy/tensorrt-opencv-python3 version-1.1 d693210c500c 4 months ago 4.94GB I ran $sudo docker run -it --rm -v $(pwd):/workspace --runtime=nvidia -w /workspace -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=unix$DISPLAY aminehy/tensorrt-opencv-python3:version-1.3``` ===================== == NVIDIA TensorRT == ===================== NVIDIA Release 19.05 (build 6392482) NVIDIA TensorRT 5.1.5 (c) 2016-2019, NVIDIA CORPORATION. All rights reserved. Container image (c) 2019, NVIDIA CORPORATION. All rights reserved. https://developer.nvidia.com/tensorrt To install Python sample dependencies, run /opt/tensorrt/python/python_setup.sh root@a38b20eeb740:/workspace# cd /opt/tensorrt/python/ root@a38b20eeb740:/opt/tensorrt/python# chmod +x python_setup.sh root@a38b20eeb740:/opt/tensorrt/python# ./python_setup.sh Requirement already satisfied: Pillow in /usr/local/lib/python3.5/dist-packages (from -r /opt/tensorrt/samples/sampleSSD/requirements.txt (line 1)) (6.0.0) WARNING: You are using pip version 19.2.1, however version 19.3.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. Ignoring torch: markers 'python_version == \"3.7\"' don't match your environment ...... ...... ...... Setting up graphsurgeon-tf (5.1.5-1+cuda10.1) ... Setting up uff-converter-tf (5.1.5-1+cuda10.1) ... Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; File \"/usr/lib/python2.7/dist-packages/uff/__init__.py\", line 1, in &lt;module&gt; from uff import converters, model # noqa File \"/usr/lib/python2.7/dist-packages/uff/model/__init__.py\", line 1, in &lt;module&gt; from . import uff_pb2 as uff_pb # noqa File \"/usr/lib/python2.7/dist-packages/uff/model/uff_pb2.py\", line 6, in &lt;module&gt; from google.protobuf.internal import enum_type_wrapper ImportError: No module named google.protobuf.internal chmod: cannot access '/bin/convert_to_uff.py': No such file or directory Can't seem to find any file called convert_to_uff.py inside bin Whats is going? Where did I go wrong?",
        "answers": [
            [
                "Try reinstalling protobuf to be sure: pip install protobuf"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to create an nvidia-docker image with installed TensorRT for my specific application. I can't use any of the provided TensortRT base images, as they are using CUDA version not compatible with the application, but I have a custom TensorRT debian package which is used in my organization. The problem is, when I install it from the Dockerfile, it also installs nvidia drivers. As a result, the container is successfully created, but can't be started - the result is: svc_moma_usr@PL1LXD-529389:~/gutkowsp/Docker_projects/test_cuda$ nvidia-docker run tensorrt-test docker: Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:432: running prestart hook 1 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: mount error: file creation failed: /var/lib/docker/overlay2/97f449ff2535b1ad304520dae75c613931888658a66b89235b0d040a872a625c/merged/usr/bin/nvidia-smi: file exists\\\\\\\\n\\\\\\\"\\\"\": unknown. ERRO[0001] error waiting for container: context canceled The dockerfile is: FROM nvidia/cuda:9.1-devel-ubuntu16.04 ENV DEBIAN_FRONTEND noninteractive ENV CUDNN_VERSION 7.0.5.15 LABEL com.nvidia.cudnn.version=\"${CUDNN_VERSION}\" RUN apt update -y &amp;&amp; \\ apt install software-properties-common -y &amp;&amp; \\ apt-add-repository --yes --update ppa:ansible/ansible &amp;&amp; \\ apt install ansible -y RUN apt update -y &amp;&amp; \\ apt install -y --no-install-recommends \\ libcudnn7=$CUDNN_VERSION-1+cuda9.1 \\ libcudnn7-dev=$CUDNN_VERSION-1+cuda9.1 RUN apt update -y &amp;&amp; \\ apt install tensorrt -y How this problem of unnecessary drivers is solved? This seems to me like a common issue, as in general nvidia docker images typically have installed nvidia software, which usually comes with drivers. Maybe someone can share the dockerfiles for the TensorRT images for reference?",
        "answers": [
            [
                "For anyone who facing the same issue: If necessary use CUDNN enabled docker image, like 11.7.1-cudnn8-runtime-ubuntu18.04 to avoid the necessity to install it using apt Run apt update Run apt install &lt;your package&gt; -y --dry-run | grep nvidia Add all listed nvidia packages to apt ignore list - add a dash after the package name with an asterisk in place of version number apt install &lt;your package&gt; libnvidia-compute-*-server- \\ libnvidia-compute-*- --dry-run | grep nvidia Make sure that none of nvidia packages will be installed. If necessary add newly discovered packages to ignore list. If everything is OK then remove --dry-run flag and install your package apt install &lt;your package&gt; libnvidia-compute-*-server- libnvidia-compute-*-"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have set up a bunch of ros nodes that each run inside a docker container and are started via docker-compose. I had no problems running it on my laptop, besides rviz being slow since it was running on the cpu only. Now I am moving the project onto a machine that has an nVidia RTX2080 on ubuntu18.04LTS and the same setup produces these errors. I have already installed nvidia-docker2 and the daemon.json is setting the default runtime as nvidia. I don't really know where to start looking at errors. Posts I found were closed without a solution to my problem. How to get rViz running in nVidia-docker2? rviz_1 | libGL error: No matching fbConfigs or visuals found rviz_1 | libGL error: failed to load driver: swrast rviz_1 | libGL error: No matching fbConfigs or visuals found rviz_1 | libGL error: failed to load driver: swrast rviz_1 | libGL error: No matching fbConfigs or visuals found rviz_1 | libGL error: failed to load driver: swrast rviz_1 | [ INFO] [1576658065.533954900]: rviz version 1.13.6 rviz_1 | [ INFO] [1576658065.534009692]: compiled against Qt version 5.9.5 rviz_1 | [ INFO] [1576658065.534021481]: compiled against OGRE version 1.9.0 (Ghadamon) rviz_1 | [ INFO] [1576658065.548489531]: Forcing OpenGl version 0. rviz_1 | [ WARN] [1576658065.859692866]: OGRE EXCEPTION(3:RenderingAPIException): Unable to create a suitable GLXContext in GLXContext::GLXContext at /build/ogre-1.9-B6QkmW/ogre-1.9-1.9.0+dfsg1/RenderSystems/GL/src/GLX/OgreGLXContext.cpp (line 61) rviz_1 | rviz::RenderSystem: error creating render window: OGRE EXCEPTION(3:RenderingAPIException): Unable to create a suitable GLXContext in GLXContext::GLXContext at /build/ogre-1.9-B6QkmW/ogre-1.9-1.9.0+dfsg1/RenderSystems/GL/src/GLX/OgreGLXContext.cpp (line 61) rviz_1 | rviz::RenderSystem: error creating render window: OGRE EXCEPTION(3:RenderingAPIException): Unable to create a suitable GLXContext in GLXContext::GLXContext at /build/ogre-1.9-B6QkmW/ogre-1.9-1.9.0+dfsg1/RenderSystems/GL/src/GLX/OgreGLXContext.cpp (line 61)",
        "answers": [
            [
                "See http://wiki.ros.org/docker/Tutorials/Hardware%20Acceleration. Nvidia docker images need to be built by your own. $ cd path/to/dir $ cat Dockerfile FROM your-repo/your-image:your-tag # nvidia-container-runtime ENV NVIDIA_VISIBLE_DEVICES \\ ${NVIDIA_VISIBLE_DEVICES:-all} ENV NVIDIA_DRIVER_CAPABILITIES \\ ${NVIDIA_DRIVER_CAPABILITIES:+$NVIDIA_DRIVER_CAPABILITIES,}graphics $ docker build -t your-nvidia-image . $ xhost +local: $ docker run --gpus all -it -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY your-nvidia-image /bin/bash"
            ],
            [
                "Had the same issue in docker compose that I recently solved. Added the following into my dockerfile and rebuild: ENV NVIDIA_VISIBLE_DEVICES \\ ${NVIDIA_VISIBLE_DEVICES:-all} ENV NVIDIA_DRIVER_CAPABILITIES \\ ${NVIDIA_DRIVER_CAPABILITIES:+$NVIDIA_DRIVER_CAPABILITIES,}graphics And also make sure in compose.yaml, you have set the following: volume: -\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" -\"/var/run/dbus:/var/run/dbus\" enivironment: -\"/usr/local/nvidia/bin:${PATH}\" -\"NVIDIA_VISIBLE_DEVICES: all\" -\"NVIDIA_DRIVER_CAPABILITIES:compute,compat32,utility,graphics,video,display\" - \"DISPLAY=unix$DISPLAY\" - \"QT_X11_NO_MITSHM=1\" - \"XDG_RUNTIME_DIR= /run/user/1000\""
            ],
            [
                "I really faced the same problem today. I tested rviz in docker on my CPU laptop and everything worked fine until I tested the setup on a GPU accelerated hardware. I have been able to solve it by using the solution proposed in http://wiki.ros.org/docker/Tutorials/Hardware%20Acceleration under \"nvidia-docker2\". Obviously, you need to build your image with the environment variables \"NVIDIA_VISIBLE_DEVICES\" and \"NVIDIA_DRIVER_CAPABILITIES\" and run your container with runtime=nvidia. Hope this helps."
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I have a service which loads docker daemon docker and I want to utilize nvidia gpus inside this service, I have created an image which has a dockerd as entrypoint and installed nvidia cuda driver but when I am trying to deploy my application for test purposes I am facing errors related with nvidia driver inside the service\u2019s image docker: Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused \u201cprocess_linux.go:449: container init caused \u201cprocess_linux.go:432: running prestart hook 1 caused \\\u201cerror running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\n\\\u201d\u201d\u201d: unknown. #image: chatzich/dinvidia stages: - test - package before_script: - echo \"Before script section\" after_script: - echo \"After script section\" test: image: chatzich/dinvidia services: - name: chatzich/dinvidia alias: dinvidia stage: build variables: DOCKER_HOST: tcp://dinvidia:2375 DOCKER_DRIVER: overlay2 DOCKER_TLS_CERTDIR: \"\" tags: - gpu script: # Build the production image - echo \"Build stage script\" - ldconfig -p | grep nvidia - docker --version - docker run --rm --runtime=nvidia nvidia/cuda ldconfig -p | grep nvidia stage: test package: stage: package script: # Build the production image - echo \"Package stage script\"",
        "answers": [],
        "votes": []
    },
    {
        "question": "Docker swarm has 3 nodes, of which 1 has gpu. I'm trying to run the gpu service on gpu node. when I run command: docker service create --generic-resource \"gpu=1\" --replicas 1 --name nvidia-cuda nvidia/cuda has error: no suitable node (insufficient resources on 3 nodes) Anyone can help me please? Error image Solved: https://github.com/NVIDIA/nvidia-docker/issues/141#issuecomment-356458450",
        "answers": [],
        "votes": []
    },
    {
        "question": "Deploying a container with docker-compose by using as runtime: nvidia to yml file, gitlab-ci prompts with an error message ERROR: for container_name Cannot create container for service worker: Unknown runtime specified nvidia",
        "answers": [],
        "votes": []
    },
    {
        "question": "this is my docker-compose.yaml: version: '2.3' services: nvidia-smi-test: runtime: nvidia image: nvidia/cuda:9.0-base environment: - NVIDIA_VISIBLE_DEVICES=all when I run docker-compose up it will appear: ERROR: The Compose file './docker-compose.yaml' is invalid because: Unsupported config option for services.nvidia-smi-test: 'runtime' but if I don't use docker-compose just use command docker run --runtime=nvidia nvidia/cuda:9.0-base nvidia-smi there is no error \uff0c I don't know exactly what went wrong\u3002Below is version of docker and docker-compose: Docker version 19.03.5, build 633a0ea838 docker-compose version 1.17.0, build ac53b73 I would be very grateful if everyone could make some comments\uff01",
        "answers": [
            [
                "I found the problem, this is because the docker-compose version is not right, Docker Compose must be version 1.19.0 or higher. I just uninstall the docker-compose (version 1.17.0) and install docker-compose(version 1.21.2) everything goes well!"
            ]
        ],
        "votes": [
            9.0000001
        ]
    },
    {
        "question": "I have made changes to the docker for aws cloudformation template to change the ami to https://aws.amazon.com/marketplace/pp/Amazon-Web-Services-Deep-Learning-AMI-Ubuntu-1604/B077GCH38C for the availability of nvidia docker and changed the instance type to g3.4xlarge. I made a bunch of other tweaks as well. When I create the stack, I can ssh into an instance, and docker swarm is initialized and has access to all the nodes. There are no error logs. But, periodically, the EC2 instances get shut down without any informative logs in the system log of the terminated instances. I was wondering if anyone has any idea why this may be happening Here is my cloudformation template: pastebin.com/5465RgSN Updated clarification: The stack is supposed to create 3 nodes (3 manager, 0 workers). A few minutes after the creation of the stack, the EC2 instances begin to shut-down and in their place, new instances get created and join the swarm. When I ssh into an EC2 instance, I usually have 2-3 minutes until it gets shut down.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm working on a container that requires the nvidia runtime. I can specify this runtime in a v2.3 docker-compose file like so: version: \"2.3\" services: my-service: image: \"my-image\" runtime: \"nvidia\" ... Running docker-compose up my-service works just fine. I get the nvidia runtime and everything works fine. I've tried this just by changing the \"2.3\" to \"3\" and I get the following error when I do docker-compose up my-service: ERROR: The Compose file './docker-compose.yml' is invalid because: Unsupported config option for services.my-service: 'runtime' If I take out the runtime: \"nvidia\" line, this comes up without problems\u2014except of course it's not using nvidia and I need access to the GPU on the host to get the performance I want. Is there an equivalent for runtime in docker-compose v3? If not, why was this option dropped? Thanks in advance. :)",
        "answers": [
            [
                "I realize this question is rather old but I ran into it yesterday. TL;DR : Upgrade you docker-compose to 1.27.0+ Details There has been quite a discussion about the removal of the runtime keyword in the dedicated Docker bug thread : https://github.com/docker/compose/issues/6691 Finally, in the 1.27.0, Docker has decided to allow it back. So you just need to have the correct version of docker-compose. I would recommend the pip install path as their versions are more up to date (current docker-compose version in Debian buster is 1.21). And it seems there are other good reasons to do so, see here."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I\u2019m running a virtual vachine on GCP with a tesla GPU. And try to deploy a PyTorch-based app to accelerate it with GPU. I want to make docker use this GPU, have access to it from containers. I managed to install all drivers on host machine, and the app runs fine there, but when I try to run it in docker (based on nvidia/cuda container) pytorch fails: File \"/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\", line 82, in _check_driver http://www.nvidia.com/Download/index.aspx\"\"\") AssertionError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from To get some info about nvidia drivers visible to the container, I run this: docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi But it complains: docker: Error response from daemon: Unknown runtime specified nvidia. On the host machine nvidia-smi output looks like this: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.33.01 Driver Version: 440.33.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... On | 00000000:00:04.0 Off | 0 | | N/A 39C P0 35W / 250W | 873MiB / 16280MiB | 0% Default | +-------------------------------+----------------------+----------------------+ If I check my runtimes in docker, I get only runc runtime, no nvidia as in examples around the internet. $ docker info|grep -i runtime Runtimes: runc Default Runtime: runc How can I add this nvidia runtime environment to my docker? Most posts and questions I found so far say something like \"I just forgot to restart my docker daemon, it worked\", but it does not help me. Whot should I do? I checked many issues on github, and #1, #2 and #3 StackOverflow questions - didn't help.",
        "answers": [
            [
                "The nvidia runtime you need, is nvidia-container-runtime. Follow the installation instructions here: https://github.com/NVIDIA/nvidia-container-runtime#installation Basically, you install it with your package manager first, if it's not present: sudo apt-get install nvidia-container-runtime Then you add it to docker runtimes: https://github.com/nvidia/nvidia-container-runtime#daemon-configuration-file This option worked for me: $ sudo tee /etc/docker/daemon.json &lt;&lt;EOF { \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } } EOF sudo pkill -SIGHUP dockerd Check that it's added: $ docker info|grep -i runtime Runtimes: nvidia runc Default Runtime: runc"
            ],
            [
                "As an update to @Viacheslav Shalamov's answer, the nvidia-container-runtime package is now part of the nvidia-container-toolkit which can also be installed with: sudo apt install nvidia-cuda-toolkit and then follow the same instruction above to set nvidia as default runtime."
            ]
        ],
        "votes": [
            23.0000001,
            1.0000001
        ]
    },
    {
        "question": "I have a question about the TF_FORCE_GPU_ALLOW_GROWTH flag option with tensorflow serving. I am able to run multiple models in a docker container with TF serving. In order to limit the GPU usage, I passed the TF_FORCE_GPU_ALLOW_GROWTH=true flag as an environment variable. So as the model server loads initially, it takes up some 250 MiB , and after my first inference request, it expands to occupy around 6.4 GB of GPU memory (out of 10GB total GPU mem). This is irrespective of whether one model was loaded or multiple were loaded. My question is that how do we check the individual memory consumption of each model? Does the server load all models to GPU memory which is why the usage does not cross 6.4 GB limit? Does tf serving have any functionality to unload models from GPU after the inferencing has been done?",
        "answers": [],
        "votes": []
    },
    {
        "question": "My requirement: Make the inference task run on GPU for object detection using tensorflow. Current status: I am using AWS GPU instance (p2.xlarge) for training as well as for inference. The training part runs well on GPU. No problem here. (Graphics card: Tesla M60) For getting predictions, I have created a flask server encapsulating the tensorflow detection with some additional logic to it. I am going to deploy this service (Flask + tensorflow) as a docker container. The base image that I am using is tensorflow/tensorflow:1.12.0-gpu-py3. My dockerfile looks something like this: FROM tensorflow/tensorflow:1.12.0-gpu-py3 COPY ./app /app COPY ./requirements.txt /app RUN pip3 install -r /app/requirements.txt RUN mkdir /app/venv WORKDIR /app RUN export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim ENTRYPOINT [\"python3\", \"/app/main.py\"] ENV LISTEN_PORT 8080 EXPOSE 8080 I am able to deploy this by: docker run --runtime=nvidia --gpus all --name &lt;my-long-img-name&gt; -v &lt;somepath&gt;:&lt;anotherpath&gt; -p 8080:8080 -d &lt;my-long-img-name&gt; and successfully make calls to the endpoints on port 8080 from postman. Basically, what I mean is all the drivers are setup properly. One of the endpoint in flask is like: (For testing if GPU is being used or not) @app.route(\"/testgpu\", methods=[\"GET\"]) def testgpu(): import tensorflow as tf with tf.device('/gpu:0'): a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a') b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b') c = tf.matmul(a, b) with tf.Session() as sess: print (sess.run(c)) When I call this endpoint I get no errors (If there was no gpu detected it would throw error). This means gpu is detected for this snippet. YAY !! I also added these 2 lines to my main code execution flow: from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) and it outputs: Local devices : [name: \"/device:CPU:0\" device_type: \"CPU\" memory_limit: 268435456 locality { } incarnation: 17661279486087266140 , name: \"/device:XLA_GPU:0\" device_type: \"XLA_GPU\" memory_limit: 17179869184 locality { } incarnation: 9205152708262911170 physical_device_desc: \"device: XLA_GPU device\" , name: \"/device:XLA_CPU:0\" device_type: \"XLA_CPU\" memory_limit: 17179869184 locality { } incarnation: 3134142118233627849 physical_device_desc: \"device: XLA_CPU device\" , name: \"/device:GPU:0\" device_type: \"GPU\" memory_limit: 7447009690 locality { bus_id: 1 links { } } incarnation: 6613138223738633761 physical_device_desc: \"device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2\" ] YAY again, the GPU is detected. Even the logs from tensorflow is taking GPU. 2019-11-18 08:45:29.944580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-11-18 08:45:29.944603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 2019-11-18 08:45:29.944611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2019-11-18 08:45:29.944721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7101 MB memory) -&gt; physical GPU (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2) Everything seems smooth here, but the main part where GPU should be running is not taking it. It is using CPU. There is this another endpoint (let's say, /getpredictions) along with /testgpu that is mentioned above which runs the detection and returns the output. The problem: Whenever I call /getpredictions from postman on port 8080 instead of using GPU it takes CPU and returns the output in around ~30+ seconds. Is there anything missing here? Any workarounds? Let me know if I need to add some more information to the question.",
        "answers": [
            [
                "from the docs you should add gpu option when running container like this: docker run -it --gpus all -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have an Ubuntu 18.04 installation on a computer with the following CPU and GPU properties ..$cat /proc/cpuinfo/ ... flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust smep erms invpcid mpx rdseed smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d ..$ nvidia-smi Sat Nov 16 13:41:35 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 430.26 Driver Version: 430.26 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 106... Off | 00000000:01:00.0 On | N/A | | 8% 56C P0 37W / 150W | 216MiB / 3016MiB | 0% Default | +-------------------------------+----------------------+----------------------+ ... +-------------------------------+----------------------+----------------------+ | 7 GeForce GTX 106... Off | 00000000:07:00.0 Off | N/A | | 0% 26C P8 5W / 150W | 2MiB / 3019MiB | 0% Default | +-------------------------------+----------------------+----------------------+ Important to notice is that my CPU does not support the AVX or AVX2 instruction. Also that I have both CUDA and the Nvidia driver installed (along with nvidia-docker). On my bare metal system I have tensorflow:1.14.0 installed. I specifically have this version as a result of the issue I am facing, better explained in this SO QA; after tensorflow 1.15.0 the AVX instruction was used by default. With this version of tensorflow installed on my bare metal machine I can import the library and train a model successfully. (base) :~$ conda list # packages in environment at /home/kevin/anaconda3: # # Name Version Build Channel ... keras 2.2.4 0 keras-applications 1.0.8 py_0 keras-base 2.2.4 py37_0 keras-preprocessing 1.1.0 py_1 ... python 3.7.3 h0371630_0 ... tensorboard 1.14.0 py37hf484d3e_0 tensorflow 1.14.0 mkl_py37h45c423b_0 tensorflow-base 1.14.0 mkl_py37h7ce6ba3_0 tensorflow-estimator 1.14.0 py_0 ... (base) :~$ python -c \"import tensorflow\" (base) :~$ Though my issue comes in when trying to use a tensorflow docker image which is generated with the following Dockerfile FROM tensorflow/tensorflow:1.14.0-gpu-py3 LABEL description=\"SRCNN-Nvidia-Docker-Keras\" WORKDIR /app # Install the libraries required for opencv-python RUN apt-get update RUN apt-get install -y libsm6 libxext6 libxrender-dev # Install the required python libraries ADD library-requirements.txt . RUN pip install -r library-requirements.txt # Create a mount point in the container to link file systems VOLUME /app/SRCNN # library-requiremnets: keras, numpy, matplotlib, h5py, pillow, opencv-python, scipy I can successfully build the image and run the container, though whilst in the container I cannot import tensorflow root@8a221a7eca5f:/app# pip list Package Version -------------------- -------- ... Keras 2.3.1 Keras-Applications 1.0.8 Keras-Preprocessing 1.1.0 ... tensorboard 1.14.0 tensorflow-estimator 1.14.0 tensorflow-gpu 1.14.0 ... root@8a221a7eca5f:/app# python -c \"import tensorflow\" Illegal instruction (core dumped) root@8a221a7eca5f:/app# As far as I can tell the only reason I should be seeing the Illegal instruction error is when trying to load tensorflow &gt; 1.15.0 as a result of not having the AVX instruction. Though, when using version 1.14 I can import on my bare metal machine, but not in the 1.14 version docker container. What else could be the cause of this? Is my only real solution to simply compile tensorflow from source in the docker image?",
        "answers": [
            [
                "If culprit is AVX support (And I think this is the case), instead of compiling yourself you can use community wheels - there are few compiled without AVX."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I use Pytorch image for GPUs: gcr.io/deeplearning-platform-release/pytorch-gpu.1-2:latest. I deploy it to GCE with K80 and V100 GPUs. import torch torch.cuda.device_count() #returns 0 Cuda is installed. When I ssh into docker container and run following command on terminal, I can see it. cat /usr/local/cuda/version.txt CUDA Version 10.0.130 FYI, nvidia-smi command from terminal does not work. What am I doing wrong? Or is there a problem with docker images?",
        "answers": [
            [
                "It seems that the NVIDIA driver has not been installed correctly. Please note that \u201ceach version of CUDA requires a minimum GPU driver version or a later version.\u201d To check the minimum driver required for your version of CUDA, see this link: Toolkit and Compatible Driver Versions. You can follow this link to install the driver manually. Also you can find some libraries that needs to be installed in this link."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm running a BERT finetuning using the official script run_pretraining.py This makes use of a TPUEstimator estimator = tf.contrib.tpu.TPUEstimator( use_tpu=FLAGS.use_tpu, model_fn=model_fn, config=run_config, train_batch_size=FLAGS.train_batch_size, eval_batch_size=FLAGS.eval_batch_size) estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps) where the model_fn creates model = modeling.BertModel( config=bert_config, is_training=is_training, input_ids=input_ids, input_mask=input_mask, token_type_ids=segment_ids, use_one_hot_embeddings=use_one_hot_embeddings) with the optimizer TPUEstimatorSpec: train_op = optimization.create_optimizer( total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu) output_spec = tf.contrib.tpu.TPUEstimatorSpec( mode=mode, loss=total_loss, train_op=train_op, scaffold_fn=scaffold_fn) According to the docs, the TPUEstimator should be backed by GPU as fallback of the TPU. In the script there is any other setup of Tensorflow session. What happens running the script within a docker container with docker run --runtime=nvidia, the training starts in CPU: |===============================+======================+======================| | 0 GeForce GTX 1080 On | 00000000:01:00.0 On | N/A | | 0% 47C P8 7W / 200W | 593MiB / 8111MiB | 1% Default | +-------------------------------+----------------------+----------------------+ To be sure that we had the GPU device visible I have added os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\", and started a session: def load_tensorflow_shared_session(self): \"\"\" Load a Tensorflow/Keras shared session \"\"\" N_CPU = multiprocessing.cpu_count() # OMP_NUM_THREADS controls MKL's intra-op parallelization # Default to available physical cores os.environ['OMP_NUM_THREADS'] = str( max(1, N_CPU) ) # LP: set Tensorflow logging level MXM_DIST = os.getenv('MXM_DIST', 'prod') if MXM_DIST == 'prod': tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) else: tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG) # LP: create a config by gpu cpu backend config = tf.ConfigProto( device_count={ 'GPU' : 1, 'CPU': N_CPU }, intra_op_parallelism_threads = 0, inter_op_parallelism_threads = N_CPU, allow_soft_placement=True ) config.gpu_options.allow_growth = True config.gpu_options.per_process_gpu_memory_fraction = 0.6 # LP: create session by config self.tf_session = tf.Session(config=config) return self.tf_session therefore wrapping everything in it: train_input_fn = input_fn_builder( input_files=input_files, max_seq_length=FLAGS.max_seq_length, max_predictions_per_seq=FLAGS.max_predictions_per_seq, is_training=True) with session.as_default(): with session.graph.as_default(): estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps) I can see GPU:0 in the device list, but processing occurs on the GPU anyways: PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 15225 root 20 0 37.956g 0.017t 271720 S 951.8 28.5 904:59.30 run_pretraining",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using tensorflow-serving to run models on my local nvidia-docker image tensorflow/serving:latest-gpu with below configuration gpu:GeForce GTX 1080 with 12 GB Driver Version: 418.88 CUDA Version: 10.1 But my inference is taking almost \"double\" time when I am running in aws-ec2 with below configuration. gpu: Tesla K80 with 12 GB Driver Version: 418.40.04 CUDA Version: 10.1 Why is this happening?",
        "answers": [],
        "votes": []
    },
    {
        "question": "My docker image size is 17 GB. I ran the image and deleted a folder of 4GB inside docker container. After committing the container the new image created was also 17 GB . How is it possible?",
        "answers": [
            [
                "As you add another layer by committing this will result in an additional layer which deletes the files of of a previous layers. If you run an image all layers will be executed and interpreted. Docker: About images, containers, and storage drivers docker build --squash can help you to reduce the image size as it merges all layers into one, but it is experimental."
            ],
            [
                "Found the logic: The problem is once i commit a CONTAINER of size say 17 GB and exit it a new image with size 17 GB is created. Even if run the image again and delete files inside them and recommit it, the size wont change. The point is before committing a container we must delete all the unnecessary files and commit the container"
            ]
        ],
        "votes": [
            4.0000001,
            1e-07
        ]
    },
    {
        "question": "i was doing some CUFFT routine in docker and faced some problem. I use the following Dockerfile. FROM nvidia/cuda:9.1-runtime-ubuntu16.04 ENV NVIDIA_VISIBLE_DEVICES all ENV LD_LIBRARY_PATH /usr/local/cuda-9.1/lib64/ FROM python:3.7 COPY --from=0 /usr/local/cuda-9.1 /usr/local/cuda-9.1 ENV VIRTUAL_ENV=/opt/venv ENV PATH=\"/opt/venv:$PATH\" RUN pip install numpy RUN apt update &amp;&amp; \\ apt-get -y install gcc &amp;&amp; \\ apt-get -y install apt-utils &amp;&amp; \\ apt-get -y install g++ &amp;&amp; \\ apt-get -y install pciutils &amp;&amp; \\ apt-get -y install libc6 ADD helmsolver /helmsolver CMD ls /usr &amp;&amp; ls /usr/local CMD dpkg -l | grep -i cuda CMD cd helmsolver &amp;&amp; bash tests.sh To build and run docker i use such commands. docker build -t helm . docker run --gpus all helm I'm able to run my code on host, but after running in docker the error 35 (cudaErrorInsufficientDriver) appears in this type of code cudaMalloc((void**)&amp;d_array, memsize). What's wrong with my code or is it that just some .so files are missing? Here are my CUDA, docker, nvidia-smi versions nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2017 NVIDIA Corporation Built on Fri_Nov__3_21:07:56_CDT_2017 Cuda compilation tools, release 9.1, V9.1.85 Docker version 19.03.4 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418.67 Driver Version: 418.67 CUDA Version: N/A | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GT 640 On | 00000000:01:00.0 N/A | N/A | | 40% 36C P8 N/A / N/A | 48MiB / 4035MiB | N/A Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 760 On | 00000000:02:00.0 N/A | N/A | | 17% 36C P8 N/A / N/A | 1MiB / 4037MiB | N/A Default | +-------------------------------+----------------------+----------------------+",
        "answers": [
            [
                "Adding NVIDIA_DRIVER_CAPABILITIES compute, utility as ENV solves the issue."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am new to docker and learning about it recently. When I was trying to use it for one of my assignments. I got the following error of which I did not find any good solution anywhere. I have properly installed nvidia-docker and build the image before trying out the following command as given in the link. The nvidia-docker version is: ii nvidia-docker 1.0.1-1 amd64 NVIDIA Docker container tools This is the error I got after running the above command. Unable to find image 'retinanet:latest' locallydocker: Error response from daemon: pull access denied for retinanet, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. The following is what I got after building the image. $ sudo docker build -t retinanet:latest retinanet/ Sending build context to Docker daemon 208.9kB Step 1/3 : FROM nvcr.io/nvidia/pytorch:19.05-py3 ---&gt; 7e98758d4777 Step 2/3 : COPY . retinanet/ ---&gt; Using cache ---&gt; a32277843b1f Step 3/3 : RUN pip install --no-cache-dir -e retinanet/ ---&gt; Using cache ---&gt; a0195cf77814 Successfully built a0195cf77814 Successfully tagged retinanet:latest The following is the result I got after docker images. $ sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZE &lt;none&gt; &lt;none&gt; 7cd7d8e7cedc 2 days ago 7.59GB nvcr.io/nvidia/pytorch 19.09-py3 9d6f9ccfbe31 6 weeks ago 9.15GB nvidia/cuda 9.0-base 1443caa429f9 7 weeks ago 137MB retinanet latest a0195cf77814 7 weeks ago 7.59GB nvcr.io/nvidia/pytorch 19.07-py3 71df86c191f8 3 months ago 8.32GB nvcr.io/nvidia/pytorch 19.05-py3 7e98758d4777 5 months ago 7.55GB When I use the command docker run -ti retinanet:latest The workspace is created but I get the following warnings which indicates that I am not utilizing the GPU facility. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available. Use 'nvidia-docker run' to start this container; see https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker . NOTE: MOFED driver for multi-node communication was not detected. Multi-node communication performance may be reduced. NOTE: The SHMEM allocation limit is set to the default of 64MB. This may be insufficient for PyTorch. NVIDIA recommends the use of the following flags: nvidia-docker run --ipc=host ... Note: I followed the solution to this link but it is not helpful in my case. Any ideas and suggestions would be very helpful. Thanks in advance.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to copy a local Python file to a running container on Kubernetes and it fails: $ kubectl cp /path/to/file.py namespace/pod:/path/in/container/file.py tar: This does not look like a tar archive tar: Exiting with failure status due to previous errors command terminated with exit code 2 I know the tar binary must be available in the container, and it is. Does anyone know what's going on here and how I can solve this? UPDATE: After some more testing I can confirm that this only happens on nodes that run nvidia-docker rather than the normal docker. When piping things into kubectl exec on these nodes the stream is always empty. So the following command yields an empty file in the pod running on a GPU-enabled node while the file is non-empty on other nodes without GPU support: cat nonempty_file.txt | kubectl exec -i pod -- tee /home/jovyan/empty_file.txt This has been tested using the exact same image/container on both nodes.",
        "answers": [
            [
                "Problem is solved by updating EKS AMI version. Please install new release: eks-ami-releases. See: eks-ami-kubectl."
            ],
            [
                "Please take a look for the docs: Supported releases and component skew, Nodes may lag masters components by up to two minor versions but should be at a version no newer than the master; a client should be skewed no more than one minor version from the master, but may lead the master by up to one minor version. For example, a v1.3 master should work with v1.1, v1.2, and v1.3 nodes, and should work with v1.2, v1.3, and v1.4 clients. other github issues, k8s docs, kubectl is supported within one minor version (older or newer) of kube-apiserver. v1.16.0 release kubectl cp no longer supports copying symbolic links from containers; to support this use case, see kubectl cp --help for examples using tar directly opy files and directories to and from containers. Examples: # !!!Important Note!!! # Requires that the 'tar' binary is present in your container # image. If 'tar' is not present, 'kubectl cp' will fail. Please try and install appropriate kubectl version. Hope this help"
            ]
        ],
        "votes": [
            1.0000001,
            -0.9999999
        ]
    },
    {
        "question": "I'm using Google's AI Platform to train machine learning models using a custom Docker image. To run existing code without modifications, I would like to mount a GCS bucket inside the container. I think one way to achieve this is to install gcloud to authentication and gcsfuse for mounting in the container. My Dockerfile looks like this: FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 WORKDIR /root # Install system packages. RUN apt-get update RUN apt-get install -y curl # ... # Install gcsfuse. RUN echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" | tee /etc/apt/sources.list.d/gcsfuse.list RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - RUN apt-get update RUN apt-get install -y gcsfuse # Install gcloud. RUN apt-get install -y apt-transport-https RUN apt-get install -y ca-certificates RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - RUN apt-get update RUN apt-get install -y google-cloud-sdk # ... ENTRYPOINT [\"entrypoint.sh\"] Inside the entry point script, I then try to authenticate with Google cloud and mount the bucket. My entrypoint.sh looks like this: #!/bin/sh set -e gcloud auth login gcsfuse my-bucket-name /root/output python3 script.py --logdir /root/output/experiment I then build the container and run it either locally for testing or remotely on the AI Platform for the full training run: # Run locally for testing. nvidia-docker build -t my-image-name . nvidia-docker run -it --rm my-image-name # Run on AI Platform for full training run. nvidia-docker build -t my-image-name . gcloud auth configure-docker nvidia-docker push my-image-name gcloud beta ai-platform jobs submit training --region us-west1 --scale-tier custom --master-machine-type standard_p100 --master-image-uri my-image-name Both locally and on the AI Platform, the entrypoint.sh script hangs at the line gcloud auth login, probably because it waits for user input. Is there a better way of authenticating with Google Cloud from within the container? If not, how can I automate the line that currently hangs?",
        "answers": [
            [
                "Instead of using gcloud auth login which is primarily meant for human/user authentication, consider using gcloud auth activate-service-account and supplying a key file. See here for details: https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account I would recommend not placing the keys file inside the image but instead provide it externally. Another alternative is to realize that the authentication can implicit via environment variables. So following cloud native practices, have the environment provide the credentials needed and don't try and authenticate inside your environment at all. If you plan to run your container inside GCP Compute Engine or GKE you can implicitly provide the service account to the container from outside the container."
            ],
            [
                "If the default service account meets your needs, you can configure your container to use it like this. You may also be able to give it what it needs by granting it extra permissions. If you want to use your own service account, you'll need to authenticate as a service account via: gcloud auth activate-service-account --key-file=somekey.json That way the container won't hang while asking you to authenticate via a browser. So the obvious next question is: How do I insert my service account's key into the container? The Strategy First, you'll want to generate a key file for whatever service account you do want to use. It's not a good idea to store credentials in docker images, so I put the key in a script which I then put in a storage bucket. So the container downloads and runs the script, which switches the configured identity to a service account of my choosing. Entrypoint # runs as the default service account gsutil cp \"$1\" /run/cmd chmod +x /run/cmd /run/cmd Run Script (in bucket) cat &lt;&lt; EOF!! &gt; /dev/shm/sa_key THE KEY FILE CONTENTS GO HERE EOF!! gcloud auth activate-service-account --key-file=/dev/shm/sa_key # commands below this line are performed with the specified identity The default service account has access to the storage buckets in its project, so the script above will have to go in such a bucket. Be sure that that bucket is appropriately protected, anyone with access to it can assume the identity of the service account whose keys it contains. Testing Locally docker run -v \"/home/me/.config/gcloud:/root/.config/gcloud\" \\ theimagename gs://my-project_job1/run_script This will use your user's active gcloud creds to pull down the script and then it will switch to the service account. When it finishes, your host's gcloud will be configured to use the service account--so you may need to switch it back to yourself vi gcloud auth login. To avoid this, you can instead mount a copy of that directory, that way the original remains untouched. Running in GCP gcloud ai-platform jobs submit training job1 \\ --region us-west2 \\ --master-image-uri us.gcr.io/my-project/theimagename:latest \\ -- gs://my-project_job1/run_script I hacked this up a bit to remove references to parts of my project that are irrelevant here, so this probably won't run as is, but I think this shows the gist of how I've been using it: https://gist.github.com/MatrixManAtYrService/737cb408e5a27c2aaa19576b0f6ec18a"
            ]
        ],
        "votes": [
            5.0000001,
            -0.9999999
        ]
    },
    {
        "question": "i'm new to Docker, especially to Nvidia-Docker. I'm trying to wrap my code into docker container and run it on some hosts. But apparently something goes wrong and i'm not able to run my code in docker. I have installed Nvidia-docker and Dockerfile is taken from here. Here is my full docker code FROM nvidia/cuda:9.1-runtime-ubuntu16.04 RUN apt-get update &amp;&amp; apt-get install -y \\ cuda-command-line-tools-$CUDA_PKG_VERSION \\ cuda-libraries-dev-$CUDA_PKG_VERSION \\ cuda-minimal-build-$CUDA_PKG_VERSION \\ &amp;&amp; \\ rm -rf /var/lib/apt/lists/* ENV LIBRARY_PATH /usr/local/cuda/lib64/stubs FROM python:3.7-slim RUN pip install numpy RUN apt update &amp;&amp; \\ apt-get -y install gcc &amp;&amp; \\ apt-get -y install g++ ENV NVIDIA_VISIBLE_DEVICES all ENV NVIDIA_DRIVER_CAPABILITIES compute,utility ADD helmsolver /helmsolver CMD dpkg -l | grep -i cuda CMD cd helmsolver &amp;&amp; bash tests.sh And a bash script code where cudahelmf and cudahelmd are previously compiled by nvcc helm3dcudafnd.cu -o cudahelm -I/usr/local/cuda/samples/common/inc/ -lcufft -lcufftw -D DOUBLE #!/bin/sh mkdir helmholtz cd helmholtz mkdir build mkdir workdir mkdir src mkdir scripts ls cp ../cudahelmf ./build cp ../cudahelmd ./build cp ../tmp.py ./scripts/ cd workdir python3 ../scripts/script1.py 21 21 1 ../build/cudahelmd config.cfg &gt;&gt; results_double.txt ../build/cudahelmf config.cfg &gt;&gt; results_float.txt To build and run docker i use nvidia-docker build -t helm . nvidia-docker run --rm -ti helm And after running i have error ../build/cudahelmd: error while loading shared libraries: libcufft.so.9.1: cannot open shared object file: No such file or directory What am i doing wrong? Does it happens because of the -lcufft compile option and docker doesn't know where to get it? And docker doesn't have /usr/local/cuda/ directory after installation. It seems strange due to cuda-libraries-dev include cufft library and installations ends successfully. Here is nvcc version on my computer where code was compiled and previously tested. nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2017 NVIDIA Corporation Built on Fri_Nov__3_21:07:56_CDT_2017 Cuda compilation tools, release 9.1, V9.1.85 And nvidia-docker version Docker version 19.03.3, build a872fc2f86 P.S. Maybe there is an option to compile code in docker?",
        "answers": [
            [
                "the problem is you are running a multistage dockerfile without COPY from one to another , therefore you will be end only with the standalone python3 container which has nothing from nvidia container, so you need to copy the required files like this in python container: COPY --from=0 SOURCE DEST"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to run a simple tensorflow example using the tensorflow provided gpu-enabled docker image, from within the Pycharm IDE. Everything works except that when I run it, tensorflow does not detect GPU from the container and defaults back to CPU : tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: Running: Ubuntu 18.04.3 Docker 19.03.3 latest version of NVIDIA docker support as per : https://github.com/NVIDIA/nvidia-docker I have set up my Pycharm project with the remote interpreter feature to run the image:tensorflow:latest-gpu If I run the container from the command line with: docker run --gpus all --rm tensorflow/tensorflow:latest-gpu nvidia-smi I get this: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 430.26 Driver Version: 430.26 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:41:00.0 On | N/A | | 28% 26C P8 9W / 250W | 443MiB / 11177MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| +-----------------------------------------------------------------------------+ ..which tells me that the docker install, the image, and the nvidia docker support are all ok. Now when Pycharm runs the container, it does NOT include \"--gpus all\" command line option. If I run the same command above without the --gpus all parameter: docker run --rm tensorflow/tensorflow:latest-gpu nvidia-smi I get: docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"exec: \\\"nvidia-smi\\\": executable file not found in $PATH\": unknown. So this would point to Pycharm not adding a flag as the culprit. However, according to documentation at : https://docs.docker.com/config/containers/resource_constraints/ (bottom section on GPU) the environment variable NVIDIA_VISIBLE_DEVICES=all should accomplish the same thing. I confirmed this environment variable does get set even without the --gpus all param. Additionally, there does not seem to be a way to add additional command line params from within Pycharm. So I am stuck. I feel that this setup is not too exotic and hope that I am missing something basic.",
        "answers": [
            [
                "I've had a similar issue with pycharm and pytorch. Not sure why it happens, but I found a workaround for now, by making the following call when the script starts: import os os.system('nvidia-smi') PS: the error I encountered was RuntimeError: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I just started learning about docker so this question might be trivial for some of you. I installed the latest version of docker which is 19.03.2 in my windows 10 Enterprise(64 bit) and switched on the Linux Containers. My guide suggested me to use docker for my deep learning project(PyTorch framework based). I read several articles telling about as to why using docker makes life easy and I could follow most of them. For me, the main reason for using docker was due to the unavailability of GPU resource. So I successfully installed the docker 19.03.2 in my windows system. But for using GPU support from NVIDIA I had to install nvidia-docker. This is when I got confused. There are few places which explain installing procedure for Linux environment but I could not find any for a windows system. Also few says that in the future, nvidia-docker2 packages will no longer be supported. So I am very confused at this point. Some of my friends suggested me to use google colab instead of docker. I agree with them it's very useful but what if I still want to use docker only. I believe there must be some solution. Any help and suggestion will be highly appreciated.",
        "answers": [
            [
                "I found the answer to the above question. The answer is simple and clear. nvidia-docker is not available for windows. You guys can check here Is Microsoft Windows supported?: Is Microsoft Windows supported? No, we do not support Microsoft Windows (regardless of the version), however you can use the native Microsoft Windows Docker client to deploy your containers remotely (refer to the dockerd documentation). -- https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#user-content-is-macos-supported Thanks"
            ],
            [
                "nvidia-docker is now available for Windows WSL2, please check https://docs.nvidia.com/cuda/wsl-user-guide/index.html but you have to be a member in Windows Insider Program. Depending on private testing, at this moment performance on CUDA applications on native Linux is much better than WSL2, maybe we get performance improvements soon."
            ]
        ],
        "votes": [
            4.0000001,
            1.0000001
        ]
    },
    {
        "question": "I'm trying to build a docker image that uses nvidia hardware decoding in gstreamer and have encountered a strange problem with making the image. The build process does not find the nvidia cuda related stuff while running docker build (or nvidia-docker build), but when I spin up the failed image as a container and do those very same steps from within the container everything works. I even saved the container as image which gave me a persistent image that works as intended. Has anyone experienced similar problem and can shed some light on it? Dockerfile: FROM nvcr.io/nvidia/deepstream:3.0-18.11 AS base ENV DEBIAN_FRONTEND noninteractive #install some dependencies. NOTE - not removing apt cache for the MWE RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ libdc1394-22 \\ tmux \\ vim \\ libjpeg-dev \\ libpng-dev \\ libpng12-dev \\ cuda-toolkit-10-0 \\ python3-setuptools \\ python3-pip ninja-build pkg-config gobject-introspection gnome-devel bison flex libgirepository1.0-dev liborc-0.4-dev RUN pip3 install meson &amp;&amp; ldconfig FROM base #pull and make gstreamer: RUN cd /tmp &amp;&amp; mkdir gstreamer RUN git clone https://github.com/GStreamer/gst-build.git /tmp/gstreamer \\ &amp;&amp; cd /tmp/gstreamer \\ &amp;&amp; git checkout tags/1.16.0 \\ &amp;&amp; ./setup.py -Dgtk_doc=disabled -Dgst-plugins-bad:nvdec=enabled -Dgst-plugins-bad:nvenc=enabled -Dgst-plugins-bad:iqa=disabled -Dgst-plugins-bad:bluez=disabled --reconfigure \\ &amp;&amp; ninja -C build \\ &amp;&amp; ninja install -C build Testing: build and run the container. Inside the container: $ gst-inspect-1.0 nvdec No such element or plugin 'nvdec' $ cd /tmp/gstreamer $ ./setup.py -Dgtk_doc=disabled -Dgst-plugins-bad:nvdec=enabled -Dgst-plugins-bad:nvenc=enabled -Dgst-plugins-bad:iqa=disabled -Dgst-plugins-bad:bluez=disabled --reconfigure $ ninja -C build $ ninja install -C build $ gst-inspect-1.0 nvdec Factory Details: Rank primary (256) [... all plugin parameters show up] GObject +----GInitiallyUnowned +----GstObject +----GstElement +----GstVideoDecoder +----GstNvDec EDIT1 The image builds with no errors, only when I try to call gstreamer it is built with no acceleration. I noticed that in the build process the major difference is meson.build:109:2: Exception: Problem encountered: The nvdec plugin was enabled explicitly, but required CUDA dependencies were not found. which does not happen when building from within the container. Lack of error is related, most likely, to the ninja+meson build system which looks for compatible packages, reports the exception, but doesn't throw it and continues as if nothing wrong happened EDIT2 Answering comment: To build it and get the error, just build the attached docker image: sudo docker build -t gst16:latest . &gt; build.log This will dump all the output into the build.log file. I don't have a docker registry that I could use for this and the docker image gets quite big by docker standards (~8 Gigs), but to produce successfully, it's fairly simple: sudo docker run --runtime=\"nvidia\" -ti gst16:latest /bin/bash or sudo nvidia-docker run -ti gst16:latest /bin/bash which seems to work the same for me. Notice no --rm flag! From within the container: #check if nvidia decoder plugin is there: gst-inspect-1.0 nvdec #fail! #now build it from within: cd /opt/gstreamer ./setup.py -Dgtk_doc=disabled -Dgst-plugins-bad:nvdec=enabled -Dgst-plugins-bad:nvenc=enabled -Dgst-plugins-bad:iqa=disabled -Dgst-plugins-bad:bluez=disabled --reconfigure ninja -C build ninja install -C build gst-inspect-1.0 nvdec #success reported Now to get the image, exit the container (ctrl+d) and in the host shell: sudo docker container ls -a to view all containers including stopped ones from gst16:latest get the CONTAINER_ID and copy it sudo docker commit &lt;CONTAINER_ID&gt; gst16:manual and after a few seconds you should have the container saved as an image. Verify with sudo docker images run the new image with sudo docker run --runtime=`nvidia` --rm -ti gst16:manual /bin/bash from within the container try again the gst-inspect-1.0 nvdec to verify it's working EDIT3 $ nvidia-docker --version Docker version 18.09.0, build 4d60db4",
        "answers": [
            [
                "I think I found the solution/reason Writing it here in case someone finds themselves in similar situation, plus I hate finding old threads with similar problem and no answer or \"nevermind, I solved it\" as the only follow up Docker build does not have any ties to nvidia runtime and gstreamer requires access to the full nvidia toolchain in order to build the plugins that need it. This is to be resolved with gstreamer 1.18 but until then, there is no way to build gstreamer with nvidia codecs in docker build. The workaround: Build image with all dependencies. Run a container of said image using runtime=\"nvidia\" but don't use --rm flag In the container, build gstreamer and install it as normally. Verify with gst-inspect-1.0 Commit the container as new image: docker commit &lt;container_name&gt; &lt;temporary_image_name&gt; Tag the temporary image properly."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I've looked up older answers on this which did not help (#1, #2) I'm getting this error when trying to launch a docker-compose projcet with a container that has the runtime: nvidia flag. Following the latest instructions, I installed docker (version 19.03) and the latest nvidia-docker per the repository for Ubuntu. I did not register the runtime anywhere, as the documentation clearly states that it is not necessary now. Running a single container works, e.g. docker run --gpus all nvidia/cuda:10.0-base nvidia-smi this works perfectly - but when I try to launch the docker-compose project, it fails with the following error ERROR: for MY_SERVICE Cannot create container for service MY_SERVICE: Unknown runtime specified nvidia I don't know if it has anything to do with this, but I'm running on GCP Compute Engine, Ubuntu 18.04",
        "answers": [
            [
                "The newest version of nvidia-docker does not have a docker-compose support yet. From a discussion with a contributor, it has made clear to me that in order to work with docker-compose one must change a specific step in the installation process to install the previous version of nvidia-docker, as said per the contributor: One the README (about installation) -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - $ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list $ sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit $ sudo systemctl restart docker Replace the line: nvidia-container-toolkit By: $ sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I've got a multi-service application configured in a v3.5 docker compose file. One of the services is to have access to the (one) GPU on the (one) node in the swarm. However, if I start the service using the docker compose file, I don't seem to have access to the GPU, as reported by keras: import keras from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) prints Using TensorFlow backend. [name: \"/device:CPU:0\" device_type: \"CPU\" memory_limit: 268435456 locality { } incarnation: 10790773049987428954, name: \"/device:XLA_CPU:0\" device_type: \"XLA_CPU\" memory_limit: 17179869184 locality { } incarnation: 239154712796449863 physical_device_desc: \"device: XLA_CPU device\"] If I run the same image from the command line like this: docker run -it --rm $(ls /dev/nvidia* | xargs -I{} echo '--device={}') $(ls /usr/lib/*-linux-gnu/{libcuda,libnvidia}* | xargs -I{} echo '-v {}:{}:ro') -v $(pwd):/srv --entrypoint /bin/bash ${MY_IMG} The output is [name: \"/device:CPU:0\" device_type: \"CPU\" memory_limit: 268435456 locality { } incarnation: 3178082198631681841, name: \"/device:XLA_CPU:0\" device_type: \"XLA_CPU\" memory_limit: 17179869184 locality { } incarnation: 15685155444461741733 physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\" device_type: \"XLA_GPU\" memory_limit: 17179869184 locality { } incarnation: 4056441191345727860 physical_device_desc: \"device: XLA_GPU device\"] Config: I've installed nvidia-docker and configured the node according to this guide: /etc/systemd/system/docker.service.d/override.conf: [Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --default-runtime=nvidia --node-generic-resource gpu=GPU-b7ad85d5 and /etc/nvidia-container-runtime/config.toml: disable-require = false swarm-resource = \"DOCKER_RESOURCE_GPU\" [nvidia-container-cli] #root = \"/run/nvidia/driver\" #path = \"/usr/bin/nvidia-container-cli\" environment = [] #debug = \"/var/log/nvidia-container-toolkit.log\" #ldcache = \"/etc/ld.so.cache\" load-kmods = true #no-cgroups = false #user = \"root:video\" ldconfig = \"@/sbin/ldconfig.real\" [nvidia-container-runtime] #debug = \"/var/log/nvidia-container-runtime.log\" The relevant part of the docker compose file: docker-compose.yaml: version: '3.5' ... services: ... my-service: ... deploy: resources: reservations: generic_resources: - discrete_resource_spec: kind: 'gpu' value: 1 Question: What else is needed to get access to the GPU in that docker service?",
        "answers": [
            [
                "NVIDIA-Docker is only working on Docker Compose 2.3 Change version to version: '2.3' https://github.com/NVIDIA/nvidia-docker/wiki#do-you-support-docker-compose."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have installed my Nvidia-drivers on my remote-server and $nvidia-smi also returns the list of devices +-----------------------------------------------------------------------------+ | NVIDIA-SMI 410.48 Driver Version: 410.48 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce RTX 208... Off | 00000000:01:00.0 Off | N/A | | 36% 64C P2 78W / 300W | 9813MiB / 10989MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce RTX 208... Off | 00000000:02:00.0 Off | N/A | | 0% 45C P8 8W / 300W | 774MiB / 10989MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 333 C python 9039MiB | | 0 11773 C python 191MiB | | 0 12111 C python 191MiB | | 0 12591 C python 191MiB | | 0 12999 C python 191MiB | | 1 11773 C python 191MiB | | 1 12111 C python 191MiB | | 1 12591 C python 191MiB | | 1 12999 C python 191MiB | +-----------------------------------------------------------------------------+ But when I try to use. &gt;&gt;&gt; from tensorflow.python.client import device_lib &gt;&gt;&gt; device_lib.list_local_devices() It is taking too long time to give the list of devices Python 3.7.3 (default, Mar 27 2019, 22:11:17) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; from tensorflow.python.client import device_lib &gt;&gt;&gt; device_lib.list_local_devices() 2019-09-05 15:56:27.568310: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2019-09-05 15:56:27.596133: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz 2019-09-05 15:56:27.597408: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55da9a2fe370 executing computations on platform Host. Devices: 2019-09-05 15:56:27.597421: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt; 2019-09-05 15:56:27.598606: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1 2019-09-05 15:59:27.755714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.757588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.758287: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55da9c6d9140 executing computations on platform CUDA. Devices: 2019-09-05 15:59:27.758299: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5 2019-09-05 15:59:27.758303: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5 2019-09-05 15:59:27.758508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.758883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755 pciBusID: 0000:01:00.0 2019-09-05 15:59:27.758919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.759313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755 pciBusID: 0000:02:00.0 2019-09-05 15:59:27.759441: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2019-09-05 15:59:27.760153: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 2019-09-05 15:59:27.760803: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0 2019-09-05 15:59:27.760941: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0 2019-09-05 15:59:27.761755: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0 2019-09-05 15:59:27.762395: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0 2019-09-05 15:59:27.764370: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7 2019-09-05 15:59:27.764435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.764846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.765262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.765638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.766026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1 2019-09-05 15:59:27.766046: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2019-09-05 15:59:27.767185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-09-05 15:59:27.767194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187] 0 1 2019-09-05 15:59:27.767197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0: N Y 2019-09-05 15:59:27.767200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1: Y N 2019-09-05 15:59:27.767370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.767755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.768175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.768546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 950 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5) 2019-09-05 15:59:27.768811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2019-09-05 15:59:27.769219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 9703 MB memory) -&gt; physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5) [name: \"/device:CPU:0\" device_type: \"CPU\" memory_limit: 268435456 locality { } incarnation: 14451063918691325445 , name: \"/device:XLA_CPU:0\" device_type: \"XLA_CPU\" memory_limit: 17179869184 locality { } incarnation: 3472161188084064797 physical_device_desc: \"device: XLA_CPU device\" , name: \"/device:XLA_GPU:0\" device_type: \"XLA_GPU\" memory_limit: 17179869184 locality { } incarnation: 1975372846861552523 physical_device_desc: \"device: XLA_GPU device\" , name: \"/device:XLA_GPU:1\" device_type: \"XLA_GPU\" memory_limit: 17179869184 locality { } incarnation: 8192574289833793917 physical_device_desc: \"device: XLA_GPU device\" , name: \"/device:GPU:0\" device_type: \"GPU\" memory_limit: 996802560 locality { bus_id: 1 links { link { device_id: 1 type: \"StreamExecutor\" strength: 1 } } } incarnation: 9560317274260358344 physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\" , name: \"/device:GPU:1\" device_type: \"GPU\" memory_limit: 10175270093 locality { bus_id: 1 links { link { type: \"StreamExecutor\" strength: 1 } } } incarnation: 16877706551631715197 physical_device_desc: \"device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5\" and when I try to check the info to use nvidia-docker sudo nvidia-container-cli -k -d /dev/tty info it gives the following output: I0905 09:00:43.600569 9993 driver.c:233] driver service terminated with signal 15 nvidia-container-cli: initialization error: driver error: timed out",
        "answers": [
            [
                "Seems like there is an issue with my nvlink, so if Incase if your training is taking too much Time to start. Please check if the links are up and not down. The following command is as follows. $ nvidia-smi nvlink --status GPU 0: GeForce RTX 2080 Ti (UUID: GPU-797d7153-ea28-d678-dc38-859b914d6dd7) Link 0: 25.781 GB/s Link 1: 25.781 GB/s GPU 1: GeForce RTX 2080 Ti (UUID: GPU-8807c553-7571-582d-c2ee-02993527b0a6) Link 0: 25.781 GB/s Link 1: 25.781 GB/s Thanks"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I would like to restart a docker container, after exiting it and rebooting, with the same runtime with which it was initially created. Here's what I did so far. Create the container: sudo docker run --runtime=nvidia [...] Restart Docker after exiting the container and rebooting: service docker restart Restart the container previously created: sudo docker start my_container Reopen the container. docker exec -it my_container [...] The program which is then launched in the container doesn't use the Nvidia GPU as expected. It instead uses the system CPU. Any help would be greatly appreciated.",
        "answers": [
            [
                "I got the expected result by creating a new container with the --restart=unless-stopped policy, which lets Docker restart the container by itself when the Docker service is restarted. There seems to be more that's being done in this process than the start/exec sequence that I was using."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Hello Stackoverflow experts! I have Telsa K40m on the private cluster, and my goal is to check how much performance I can gain by partitioning the GPU into multiple vGPUs and then run them concurrently with Docker containers. My concern is that I am not sure whether vGPUs support Docker Container or only limited to VMs. Would someone help me answer this question? Thanks!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm building a image which requires testing GPU usability in the meantime. GPU containers runs well: $ docker run --rm --runtime=nvidia nvidia/cuda:9.2-devel-ubuntu18.04 nvidia-smi Wed Aug 7 07:53:25 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 396.54 Driver Version: 396.54 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 TITAN X (Pascal) Off | 00000000:04:00.0 Off | N/A | | 24% 43C P8 17W / 250W | 2607MiB / 12196MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| +-----------------------------------------------------------------------------+ but failed when building with GPU: $ cat Dockerfile FROM nvidia/cuda:9.2-devel-ubuntu18.04 RUN nvidia-smi # RUN build something # RUN tests require GPU $ docker build . Sending build context to Docker daemon 2.048kB Step 1/2 : FROM nvidia/cuda:9.2-devel-ubuntu18.04 ---&gt; cdf6d16df818 Step 2/2 : RUN nvidia-smi ---&gt; Running in 88f12f9dd7a5 /bin/sh: 1: nvidia-smi: not found The command '/bin/sh -c nvidia-smi' returned a non-zero code: 127 I'm new to docker but I think we need sanity checks when building an image. So how could I build docker image with cuda runtime?",
        "answers": [
            [
                "Configuring docker daemon with --default-runtime=nvidia solved the problem. Please refer to this wiki for more info."
            ],
            [
                "Today (27.03.2023) I faced another issue with building a docker image with cuda runtime. Despite configuring proper nvidia runtime my docker build . and docker-compose commands couldn't access CUDA. I solved it by disabling the new docker build kit with: DOCKER_BUILDKIT=0 docker build . or DOCKER_BUILDKIT=0 docker-compose build You can also permanently disable the new docker build kit by modifying /etc/docker/daemon.json by adding: { \"features\": { \"buildkit\" : true } } It appears that the new docker build kit has some problems with handling CUDA and GPUs so if you are using it check my solution."
            ],
            [
                "Maybe it's because you are using \"RUN\" command on the Dockerfile. I'd try \"CMD\" (see documentation for this command) or \"ENTRYPOINT\" due to call 'docker run' with arguments. I think that \"RUN\" commands are for previous jobs that you need to execute before the container get available, instead of a process with output and stuff. Good luck with that,"
            ]
        ],
        "votes": [
            6.0000001,
            1e-07,
            -0.9999999
        ]
    },
    {
        "question": "I'm currently using Docker 19.03 and Kubernetes 1.13.5 and Rancher 2.2.4. Since 19.03, Docker has officially support natively NVIDIA GPUs just by passing --gpus option. Example (from NVIDIA/nvidia-docker github): docker run --gpus all nvidia/cuda nvidia-smi But in Kubernetes, there's no option to pass Docker CLI options. So if I need to run a GPU instance, I have to install nvidia-docker2, which is not convenient to use. Is there anyway to pass the Docker CLI options or passing NVIDIA runtime without installing nvidia-docker2",
        "answers": [
            [
                "GPU's are scheduled via device plugins in Kubernetes. The official NVIDIA GPU device plugin has the following requirements: Kubernetes nodes have to be pre-installed with NVIDIA drivers. Kubernetes nodes have to be pre-installed with nvidia-docker 2.0 nvidia-container-runtime must be configured as the default runtime for docker instead of runc. NVIDIA drivers ~= 361.93 Once the nodes are setup GPU's become another resource in your spec like cpu or memory. spec: containers: - name: gpu-thing image: whatever resources: limits: nvidia.com/gpu: 1"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have been unsuccessful in creating a Docker image using a Dockerfile for an image which contains: Python3 and pip so I can use pip to install my Python application's package requirements and then have access to a Python3 interpreter to run the app which primarily involves Keras, TensorFlow, and OpenCV NVIDIA driver and CUDA support sufficient to allow for TensorFlow to utilize the GPU when running the application I have tried building an image with a Dockerfile starting with a Python base image and adding the NVIDIA driver like so: # minimal Python-enabled base image FROM python:3.7 # add the NVIDIA driver RUN apt-get update RUN apt-get -y install software-properties-common RUN add-apt-repository ppa:graphics-drivers/ppa RUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys FCAE110B1118213C RUN apt-get update RUN apt-get --yes install nvidia-driver-418 I get a lot of output from running a docker build on the above Dockerfile but in the end, it gives messages that indicate that it's trying to install a later driver version that what I specified (430 instead of 418) and then it prompts for user input to set up the keyboard: Building for architecture x86_64 Building initial module for 4.19.0-5-amd64 Error! Bad return status for module build on kernel: 4.19.0-5-amd64 (x86_64) Consult /var/lib/dkms/nvidia/430.40/build/make.log for more information. dpkg: error processing package nvidia-dkms-430 (--configure): installed nvidia-dkms-430 package post-installation script subprocess returned error exit status 10 Setting up xfonts-base (1:1.0.5) ... Setting up libdrm2:amd64 (2.4.97-1) ... dpkg: dependency problems prevent configuration of nvidia-driver-430: nvidia-driver-430 depends on nvidia-dkms-430 (= 430.40-0ubuntu0~gpu19.10.1); however: Package nvidia-dkms-430 is not configured yet. dpkg: error processing package nvidia-driver-430 (--configure): dependency problems - leaving unconfigured Setting up xauth (1:1.0.10-1) ... Setting up xserver-common (2:1.20.4-1) ... Setting up keyboard-configuration (1.191) ... debconf: unable to initialize frontend: Dialog debconf: (TERM is not set, so the dialog frontend is not usable.) debconf: falling back to frontend: Readline Configuring keyboard-configuration ---------------------------------- Please select the layout matching the keyboard for this machine. 1. English (US) 2. English (US) - Cherokee 3. English (US) - English (Colemak) 4. English (US) - English (Dvorak) 5. English (US) - English (Dvorak, alt. intl.) 6. English (US) - English (Dvorak, intl., with dead keys) 7. English (US) - English (Dvorak, left-handed) 8. English (US) - English (Dvorak, right-handed) 9. English (US) - English (Macintosh) 10. English (US) - English (US, alt. intl.) 11. English (US) - English (US, euro on 5) 12. English (US) - English (US, intl., with dead keys) 13. English (US) - English (Workman) 14. English (US) - English (Workman, intl., with dead keys) 15. English (US) - English (classic Dvorak) 16. English (US) - English (intl., with AltGr dead keys) 17. English (US) - English (programmer Dvorak) 18. English (US) - English (the divide/multiply keys toggle the layout) 19. English (US) - Russian (US, phonetic) 20. English (US) - Serbo-Croatian (US) 21. Other Keyboard layout: When I enter 1 everything appears to hang, so this isn't working yet. I have also tried a Dockerfile beginning with an NVIDIA image and then adding Python and pip on top, like so: FROM nvidia/driver:418.40.04-ubuntu18.04 RUN apt-get update RUN apt-get -y install python3 RUN apt-get -y install python3-pip Running docker build with the above gives this error: Step 4/8 : RUN apt-get -y install python3-pip ---&gt; Running in eaa9a2ec71a9 Reading package lists... Building dependency tree... Reading state information... E: Unable to locate package python3-pip The command '/bin/sh -c apt-get -y install python3-pip' returned a non-zero code: 100 What other approaches or fixes for one of the above attempts could I try?",
        "answers": [
            [
                "you can use this : FROM nvidia/driver:418.40.04-ubuntu18.04 RUN apt-get -y update \\ &amp;&amp; apt-get install -y software-properties-common \\ &amp;&amp; apt-get -y update \\ &amp;&amp; add-apt-repository universe RUN apt-get -y update RUN apt-get -y install python3 RUN apt-get -y install python3-pip"
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers. This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered. Closed 4 years ago. Improve this question I can't see the Jupyter files in /home/easton/notebooks, how can I fix this? nvidia-docker run -it --rm --name tf -p 8888:8888 -p 6006:6006 -v /home/easton/notebooks:/notebooks tensorflow/tensorflow:1.14.0-gpu-py3-jupyter",
        "answers": [
            [
                "You are mounting the wrong directory, according to documentation the correct path for TensorFlow is ~/notebooks/ or /tf/notebooks in the container. Docker run command will be nvidia-docker run -it --rm --name tf -p 8888:8888 -p 6006:6006 -v /home/easton/notebooks:/tf/notebooks tensorflow/tensorflow:latest-py3-jupyter You can check offical documentation and here Optional Features jupyter tags include Jupyter and some TensorFlow tutorial notebooks.. They start a Jupyter notebook server on boot. Mount a volume to /tf/notebooks to work on your own notebooks. docker run -it --rm -v $(realpath ~/notebooks):/tf/notebooks -p 8888:8888 tensorflow/tensorflow:latest-py3-jupyter Run a Jupyter notebook server with your own notebook directory (assumed here to be ~/notebooks). To use it, navigate to localhost:8888 in your browser."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to transfer-learn a pretrained MobileNet Model on a c5.large instance (AWS). I am first training (burn-in) the last dense layer for a couple of epochs (tried between 5-20, does not seem to matter a whole lot). After the burn-in period, I want to train the full model. However, this stops after a couple of epochs without an error. Earlier I was trying without the burn-in period and that worked \"fine-ish\". Would typically crash the server after ~50 epochs (which is why I added the clipnorm, which did help a bit). Any ideas on how to debug this are welcome. Console Output: Total params: 3,239,114 Trainable params: 3,217,226 Non-trainable params: 21,888 _________________________________________________________________ Epoch 6/25 1/46 [..............................] - ETA: 9:22 - loss: 0.2123 2/46 [&gt;.............................] - ETA: 7:46 - loss: 0.2028ubuntu@ip-XXX:~$ ls Training Code: base_model = _mobilenet.MobileNet( input_shape=(224, 224, 3), include_top=False, pooling=\"avg\" ) if not options.mobile_net_weights: pretrained_weights = os.path.join( os.path.dirname(pretrained.__file__), \"weights_mobilenet_aesthetic_0.07.hdf5\" ) base_model.load_weights(pretrained_weights, by_name=True) # add dropout and dense layer x = Dropout(0.6)(base_model.output) x = Dense(units=classes, activation=last_activation)(x) pretrained_model = Model(base_model.inputs, x) # start training only dense layers for layer in base_model.layers: layer.trainable = False pretrained_model.compile(loss=loss, optimizer=Adam(lr=0.001, decay=0, clipnorm=1.0)) pretrained_model.summary() # add path equal to image_id labels = [dict(item, **{\"path\": item[\"image_id\"]}) for item in load_json(labels_path)] training, validation = train_test_split(labels, test_size=0.05, shuffle=True) train_data_gen = _DataGenerator( training, batch_size=options.batch_size, base_dir=options.image_path, n_classes=classes, basenet_preprocess=_mobilenet.preprocess_input, ) validation_data_gen = _DataGenerator( validation, batch_size=options.batch_size, base_dir=options.image_path, n_classes=classes, basenet_preprocess=_mobilenet.preprocess_input, training=False, ) train_job_dir = f\"train_jobs/{datetime.datetime.now().isoformat()}\" train_job_dir = os.path.join(options.results_path, train_job_dir) tensorboard = TensorBoardBatch(log_dir=os.path.join(train_job_dir, \"logs\")) model_save_name = \"weights_{epoch:02d}_{val_loss:.3f}.hdf5\" model_file_path = os.path.join(train_job_dir, \"weights\", model_save_name) if not os.path.exists(os.path.join(train_job_dir, \"weights\")): os.makedirs(os.path.join(train_job_dir, \"weights\")) model_checkpointer = ModelCheckpoint( filepath=model_file_path, monitor=\"val_loss\", verbose=1, save_best_only=True, save_weights_only=True, ) pretrained_model.fit_generator( train_data_gen, steps_per_epoch=len(training) / options.batch_size / 10, epochs=5, verbose=1, callbacks=[tensorboard, model_checkpointer], validation_data=validation_data_gen, validation_steps=len(validation) / options.batch_size, ) # start training all layers for layer in base_model.layers: layer.trainable = True pretrained_model.compile( loss=loss, optimizer=Adam(lr=0.0001, decay=0.000023, clipnorm=1.0) ) pretrained_model.summary() pretrained_model.fit_generator( train_data_gen, steps_per_epoch=len(training) / options.batch_size / 10, epochs=25, initial_epoch=5, verbose=1, callbacks=[tensorboard, model_checkpointer], validation_data=validation_data_gen, validation_steps=len(validation) / options.batch_size, ) Update and followup The original problem seemed to have been caused by too little available memory on the machine. I do have a somehow unrelated, yet related question though. When trying to use GPU acceleration I have been banging my head against the wall, as I can't seem to get it working. Is there any good (logically structured and easy to follow) information out there how one would use: Docker on a local machine (to build a GPU-accelerated enabled image) Install all the relevant (nvidia-)drivers on the GPU instance (what an insane version chaos) Run the Docker container (nvidia-docker2, nvidia-docker or --runtime==nvidia ?? ) What the hell is Cuda and why do I need it? Some sources that I found suggested to run Cuda in Docker, why? When I seemed like I got some of it working (i.e. set up drivers, some version) and had managed to build a GPU-enabled (i.e. tensorflow-gpu) Docker image I got this error: docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:430: container init caused \\\"process_linux.go:413: running prestart hook 1 caused \\\\\"error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --compute --utility --require=cuda&gt;=10.0 brand=tesla,driver&gt;=384,driver&lt;385 brand=tesla,driver&gt;=410,driver&lt;411 --pid=2113 /var/lib/docker/overlay2/4bf49d2555c40278b3249f73bf3d33484181f51b374b77b69a474fc39e37441b/merged]\\\\nnvidia-container-cli: requirement error: unsatisfied condition: driver &gt;= 410\\\\n\\\\\"\\\"\": unknown.",
        "answers": [
            [
                "nvidia-container-cli: requirement error: unsatisfied condition: driver&gt;= 410 The CUDA/Driver/GPU compatibility matrix is available here: https://github.com/NVIDIA/nvidia-docker/wiki/CUDA#requirements Regarding your questions: Docker on a local machine (to build a GPU-accelerated enabled image) I usually install docker-ce (Community edition) on my Ubuntu machine. The instructions here are straightforward: https://docs.docker.com/install/linux/docker-ce/ubuntu/ Install all the relevant (nvidia-)drivers on the GPU instance (what an insane version chaos) It's better to install nvidia-drivers and CUDA in one go by downloading the installer for your OS from here. This way you wouldn't encounter CUDA-driver mismatch issues. https://developer.nvidia.com/cuda-downloads (e.g. on Ubuntu, sudo apt-get install cuda) Run the Docker container (nvidia-docker2, nvidia-docker or --runtime==nvidia ?? ) nvidia-docker2 supersedes nvidia-docker. It maps the GPU device(/dev/nvidiaX) into the Docker container and also sets the runtime to nvidia. Usage of --runtime=nvidia is required only if docker command is used. If you're using Docker version 19.03 or later then nvidia-docker2 is not required as mentioned in the Quickstart section here: https://github.com/NVIDIA/nvidia-docker/. What the hell is Cuda and why do I need it? CUDA Toolkit is the parallel programming toolkit created by NVIDIA for GPU programming. Deep Learning frameworks like Tensorflow and Pytorch internally use this for running your code(model training) on the GPU. https://devblogs.nvidia.com/even-easier-introduction-cuda/ Some sources that I found suggested to run Cuda in Docker, why? As mentioned in the previous answer, as the DL frameworks execute within the Docker container the CUDA toolkit as well needs to run within the container so that the frameworks can use the toolkit functionality. The block diagram available at this link is very helpful to visualize this: https://github.com/NVIDIA/nvidia-docker/ https://cloud.githubusercontent.com/assets/3028125/12213714/5b208976-b632-11e5-8406-38d379ec46aa.png . The GPU driver sits on top of the host OS, the Docker container hosts the CUDA toolkit and the Applications(Model training/inference code written in a DL framework like Tensorflow or PyTorch)"
            ],
            [
                "Let me give you a simple solution to your HUGE problem(now that you have solved memory issue, even though I think training with only 3 million params on a large instance should not give you problems): Install Conda. So, whats happening here is your cuda docker is not compatible with your nvidia drivers or vice versa. Installing cuda is a pain process(I think many people can relate with me here). But you can install cuda compatible versions of tensorflow and pytorch easily using conda. Here is a personal set of commands that I use every time when setting up a cloud instance: For python 2.x: wget https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86.sh bash Miniconda2-latest-Linux-x86_64.sh (if conda not found use the command 'bash' then type in 'conda --version' to check) conda install numpy conda install tensorflow-gpu For Python3: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh conda create -n TFGPU -c defaults tensorflow-gpu conda activate TFGPU conda install pytorch torchvision cudatoolkit=9.0 -c pytorch conda install jupyter conda install keras You can check the console output by verifying : $python3 &gt;&gt;&gt;import tensorflow as tf &gt;&gt;&gt;sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) This should probably fix all your errors. Else if there are some nvidia driver problems, you can install nvidia-smi manually: #!/bin/bash echo \"Checking for CUDA and installing.\" # Check for CUDA and try to install. if ! dpkg-query -W cuda-9-0; then # The 16.04 installer works with 16.10. curl -O http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb dpkg -i ./cuda-repo-ubuntu1604_9.0.176-1_amd64.deb apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub apt-get update apt-get install cuda-9-0 -y fi # Enable persistence mode nvidia-smi -pm 1"
            ],
            [
                "what is the size of data set you are retraining using transfer learning. I had the same problem in that instance, reducing the batch size solved my problem."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I have a container that loads a Pytorch model. Every time I try to start it up, I get this error: Traceback (most recent call last): File \"server/start.py\", line 166, in &lt;module&gt; start() File \"server/start.py\", line 94, in start app.register_blueprint(create_api(), url_prefix=\"/api/1\") File \"/usr/local/src/skiff/app/server/server/api.py\", line 30, in create_api atomic_demo_model = DemoModel(model_filepath, comet_dir) File \"/usr/local/src/comet/comet/comet/interactive/atomic_demo.py\", line 69, in __init__ model = interactive.make_model(opt, n_vocab, n_ctx, state_dict) File \"/usr/local/src/comet/comet/comet/interactive/functions.py\", line 98, in make_model model.to(cfg.device) File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 381, in to return self._apply(convert) File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 187, in _apply module._apply(fn) File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 187, in _apply module._apply(fn) File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 193, in _apply param.data = fn(param.data) File \"/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 379, in convert return t.to(device, dtype if t.is_floating_point() else None, non_blocking) File \"/usr/local/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 161, in _lazy_init _check_driver() File \"/usr/local/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 82, in _check_driver http://www.nvidia.com/Download/index.aspx\"\"\") AssertionError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx I know that nvidia-docker2 is working. $ docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi Tue Jul 16 22:09:40 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418.39 Driver Version: 418.39 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce RTX 208... Off | 00000000:1A:00.0 Off | N/A | | 0% 44C P0 72W / 260W | 0MiB / 10989MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce RTX 208... Off | 00000000:1B:00.0 Off | N/A | | 0% 44C P0 66W / 260W | 0MiB / 10989MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 2 GeForce RTX 208... Off | 00000000:1E:00.0 Off | N/A | | 0% 44C P0 48W / 260W | 0MiB / 10989MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 3 GeForce RTX 208... Off | 00000000:3E:00.0 Off | N/A | | 0% 41C P0 54W / 260W | 0MiB / 10989MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 4 GeForce RTX 208... Off | 00000000:3F:00.0 Off | N/A | | 0% 42C P0 48W / 260W | 0MiB / 10989MiB | 1% Default | +-------------------------------+----------------------+----------------------+ | 5 GeForce RTX 208... Off | 00000000:41:00.0 Off | N/A | | 0% 42C P0 1W / 260W | 0MiB / 10989MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ However, I keep getting the error above. I've tried the following: Setting \"default-runtime\": nvidia in /etc/docker/daemon.json Using docker run --runtime=nvidia &lt;IMAGE_ID&gt; Adding the variables below to my Dockerfile: ENV NVIDIA_VISIBLE_DEVICES all ENV NVIDIA_DRIVER_CAPABILITIES compute,utility LABEL com.nvidia.volumes.needed=\"nvidia_driver\" I expect this container to run - we have a working version in production without these issues. And I know that Docker can find the drivers, as the output above shows. Any ideas?",
        "answers": [
            [
                "I got the same error. After trying number of solutions I found the below docker run -ti --runtime=nvidia -e NVIDIA_DRIVER_CAPABILITIES=compute,utility -e NVIDIA_VISIBLE_DEVICES=all &lt;image_name&gt;"
            ],
            [
                "In order for docker to use the host GPU drivers and GPUs, some steps are necessary. Make sure an nvidia driver is installed on the host system Follow the steps here to setup the nvidia container toolkit Make sure cuda, cudnn is installed in the image Run a container with the --gpus flag (as explained in the link above) I guess you have done the first 3 points because nvidia-docker2 is working. So since you don't have a --gpus flag in your run command this could be the issue. I usually run my containers with the following command docker run --name &lt;container_name&gt; --gpus all -it &lt;image_name&gt; -it is just that the container is interactive and starts a bash environment."
            ],
            [
                "For me, I was running from a vanilla ubuntu base docker image, i.e. FROM ubuntu Changing to an Nvidia-provided Docker base image solved the issue for me: FROM nvidia/cuda:11.2.1-runtime-ubuntu20.04"
            ],
            [
                "just use\"docker run --gpus all\",add \"--gpus all\" or \"--gpus 0\" !"
            ],
            [
                "If you are running your solution on a GPU powered AWS EC2 machine and are using an EKS optimized accelerated AMI, as was the case with us, then you are not required to set the runtime to nvidia by yourself, as that is the default runtime of the accelarated AMIs. The same can be verified by checking the /etc/systemd/system/docker.service.d/nvidia-docker-dropin.conf ssh into the AWS machine cat /etc/systemd/system/docker.service.d/nvidia-docker-dropin.conf All that was required was to set these 2 environment variables, as suggested by Chirag in the above answer and here(Nvidia container-toolkit user guide) -e NVIDIA_DRIVER_CAPABILITIES=compute,utility or -e NVIDIA_DRIVER_CAPABILITIES=all -e NVIDIA_VISIBLE_DEVICES=all Before reaching to the final solution, I also tried setting the runtime in daemon.json. To start with, the AMIs we were using did not have a daemon.json file, they instead contain a key.json file. Tried setting the runtime in both the files, but restarting the docker always overwrote the changes in key.json or simply deleted the daemon.json file."
            ]
        ],
        "votes": [
            9.0000001,
            8.0000001,
            1.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "Im deploying an application in a docker container that requires CUDA 10. This is necessary to run some of the underlying pytorch functionality that the application uses. However, the host server is running docker ce 17, Nvidia-docker v 1.0 with CUDA version 9, and I will not be able to upgrade the host. I\u2019m under the impression that I\u2019m handcuffed to the v1 nvidia docker runtime and CUDA version available on the host. Is there a way to run CUDA 10 on the container so I can leverage the functionality of this toolkit?",
        "answers": [
            [
                "In the general case, any specific CUDA version will require a minimum GPU driver version. That is covered in places like here and here (table 1). So to use CUDA 9.0 you would need at least a GPU driver version that supports CUDA 9.0, such as a R384 driver. To use CUDA 10.0 you would need at least a GPU driver version that supports CUDA 10.0, such as a R410 driver. The usage of containers doesn't fundamentally change this. If you want to use a container that has CUDA 10 code in it, your base machine needs a driver that supports CUDA 10. NVIDIA did start publishing compatibility libraries that allow modifications to the above statements. These compatibility libraries are available but not installed by default with a CUDA toolkit install. These compatibility libraries only work in certain cases, and they have certain requirements to be usable. The compatibility libraries are documented here. One of the specific requirements for use of these compatibility libraries is that the GPU(s) in use must be Tesla-brand GPUs. GeForce, Quadro, Jetson, and Titan family GPUs are not supported by these compatibility libraries. Furthermore, the libraries only work with certain combination of CUDA toolkit versions, and GPU driver versions installed on the base machine. This \"compatibility matrix\" is documented here (Table 3). Only the specific combinations of CUDA toolkit versions with installed driver versions will be usable for compatibility. To pick one example, if you wish to use CUDA 10.0, and your base machine has a Tesla GPU with a R396 driver installed, there is no compatibility support. In the same setup, however, if you wish to use CUDA 10.1, there is compatibility support for that. If you have satisfied the requirements for compatibility usage, then the remaining step would be to install the compatibility libraries (or build your container from a base container that has the compatibility libraries already installed). For a package manager CUDA install method, the method to install the compatibility libraries is simple (example on Ubuntu, installing the CUDA 10.1 compatibility to match CUDA 10.1 toolkit install): sudo apt-get install cuda-compat-10.1 Make sure to match the version to the CUDA toolkit version that you are using (that you installed with the package manager method, or that was already installed in your container). This compatibility \"path\" only began in the CUDA 9.0 timeframe. Systems that are equipped with drivers that predate CUDA 9.0 will not be usable in any way for this compatibility path. There are also various functional limitations and restrictions, which are covered in the documentation. When this \"compatibility path\" is correctly installed and in use, the overall system configuration can \"appear\" to be violating the rules indicated at the top of this answer. For example a CUDA 10.1 application could possibly be running on a machine that had only a R396 driver installed. For the specific question in view here, OP eventually indicated that the base machine had a Quadro GPU, so this \"compatibility path\" does not apply, and the only way to run e.g. a CUDA 10.0 container would be if a CUDA 10.0-capable driver is installed in the base machine, e.g. R410 or later driver."
            ]
        ],
        "votes": [
            10.0000001
        ]
    },
    {
        "question": "As in the subject, how can I check who created a certain docker image? When I check docker images i don't get this information. Thanks.",
        "answers": [
            [
                "The recommended way in Docker documentation is to set LABEL maintainer \"someone@example.org\" in your Dockerfile, you'll then be able to find it by using docker inspect myimage:tag | grep \"maintainer\" The solution proposed by \u03a6Xoc\u0119 \uc6c3 \u041fepe\u00fapa \u30c4 works but it requires the deprecated MAINTAINER instruction in your Dockerfile"
            ],
            [
                "You can do \"docker inspect\" and find in the json the author information ex: $ docker inspect hello-world:latest | grep \"Author\""
            ]
        ],
        "votes": [
            4.0000001,
            3.0000001
        ]
    },
    {
        "question": "I have docker version 18.09.5 in Ubuntu16.05. Client: Version: 18.09.5 API version: 1.39 Go version: go1.10.4 Git commit: e8ff056 Built: Thu May 9 23:18:36 2019 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 18.09.5 API version: 1.39 (minimum version 1.12) Go version: go1.10.4 Git commit: e8ff056 Built: Thu May 9 22:53:57 2019 OS/Arch: linux/amd64 Experimental: false I need to install nvidia-docker2 and the error is The following packages have unmet dependencies: nvidia-docker2 : Depends: docker-ce (= 5:18.09.7~3-0~ubuntu-xenial) but it is not installable or docker-ee (= 5:18.09.7~3-0~ubuntu-xenial) but it is not installable E: Unable to correct problems, you have held broken packages. So need to upgrade to 18.09.7. I followed the link for update. But it looked, it is not doing right. So I aborted as follow. sudo aptitude install lxc-docker The following NEW packages will be installed: aufs-tools{a} lxc-docker lxc-docker-1.9.1{ab} 0 packages upgraded, 3 newly installed, 0 to remove and 11 not upgraded. Need to get 8,590 kB of archives. After unpacking 30.6 MB will be used. The following packages have unmet dependencies: lxc-docker-1.9.1 : Conflicts: docker.io but 18.09.5-0ubuntu1~16.04.2 is installed and it is kept back. The following actions will resolve these dependencies: Remove the following packages: 1) docker.io Accept this solution? [Y/n/q/?] n The following actions will resolve these dependencies: Keep the following packages at their current version: 1) lxc-docker [Not Installed] 2) lxc-docker-1.9.1 [Not Installed] Accept this solution? [Y/n/q/?] q Abandoning all efforts to resolve these dependencies. Abort. So for my case, what is the right way to install nvidia-docker2? Do I need to upgrade docker or how to upgrade?",
        "answers": [
            [
                "The problem was solved. Followed this link. The trick was we can see multiple repositories using apt-cache madison docker-ce Then 18.09.7 version was chosen to install. Now nvidia-docker-2 is installed."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I wanted to use following command to open jupyter: docker run --runtime=nvidia --name tensorflow1 -it -p 8888:8888 -p 6006:6006 tensorflow/tensorflow:latest-gpu-py3-jupyter I can't open it with browser. The systerm looks good because it says: To access the notebook, open this file in a browser: file:///root/.local/share/jupyter/runtime/nbserver-8-open.html Or copy and paste one of these URLs: http://(568ebbf84a86 or 127.0.0.1):8888/?token=17fc57d57c89f56c460748f464b488c59f8ddccf5793e7 But when I open it with external ip address, I can't connect and system says: [W 06:15:52.336 NotebookApp] 404 GET http://110.249.212.46/testget?q=23333&amp;port=8888 (110.249.212.46) 38.11ms referer=None I have built external ip address and built firewall. There is no problem if I use the following command: docker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu and following test has pasted: python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\" how to solve this problem?",
        "answers": [
            [
                "After one month debug, finally,finally,finally,finally,finally,finally,finally,finally,finally,finally,finally,finally I fond the answer: google cloud platform only open one port for docker deployment which is 8080 so if you want to open docker jupyter notebook, you should use the following method: docker run --runtime=nvidia --name tensorflow1 -it -p 8080:8888 tensorflow/tensorflow:latest-gpu-py3-jupyter pay attention to 8080:8888"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "1. Issue or feature description Can not install nvidia-docker2 under Ubuntu18.04 2. Steps to reproduce the issue $ cat /etc/issue Ubuntu 18.04.1 LTS \\n \\l $ docker --version Docker version 18.06.1-ce, build e68fc7a $ sudo apt install nvidia-docker2 Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: nvidia-docker2 : Depends: docker-ce (= 5:18.09.0~3-0~ubuntu-bionic) but it is not installable or docker-ee (= 5:18.09.0~3-0~ubuntu-bionic) but it is not installable E: Unable to correct problems, you have held broken packages. $ uname -a Linux zixia-desktop 4.15.0-42-generic #45-Ubuntu SMP Thu Nov 15 19:32:57 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux",
        "answers": [
            [
                "The Solution Remove the default docker.io and install Docker from docker official repository. The script as the following: sudo apt-get remove docker docker-engine docker.io sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" sudo apt-get update sudo apt-get install docker-ce sudo apt-get install nvidia-docker2 Related to https://github.com/NVIDIA/nvidia-docker/issues/887"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Steps to recreate: nvidia-docker run --rm nvcr.io/nvidia/pytorch:19.05-py3 python3 -c \"import tensorrt\" Traceback (most recent call last): File \"\", line 1, in ModuleNotFoundError: No module named 'tensorrt' Other potentially useful info: nvidia-docker run --rm nvcr.io/nvidia/pytorch:19.05-py3 nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Thu_Apr_18_19:10:59_PDT_2019 Cuda compilation tools, release 10.1, V10.1.163 nvidia-docker run --rm nvcr.io/nvidia/pytorch:19.05-py3 nvidia-smi Sun Jun 9 06:05:01 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418.67 Driver Version: 418.67 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce RTX 208... On | 00000000:01:00.0 Off | N/A | | 0% 37C P0 65W / 260W | 105MiB / 10989MiB | 2% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce RTX 208... On | 00000000:02:00.0 Off | N/A | | 0% 35C P8 19W / 260W | 1MiB / 10989MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| +-----------------------------------------------------------------------------+ I can confirm the TensorRT samples (only checked sampleMNIST) work. I installed samples via nv-tensorrt-repo-ubuntu1804-cuda10.1-trt5.1.5.0-ga-20190427_1-1_amd64.deb. Also: dpkg -l | grep libnvinfer ii libnvinfer-dev 5.1.5-1+cuda10.1 amd64 TensorRT development libraries and headers ii libnvinfer-samples 5.1.5-1+cuda10.1 all TensorRT samples and documentation ii libnvinfer5 5.1.5-1+cuda10.1 amd64 TensorRT runtime libraries",
        "answers": [
            [
                "There is a separate TensorRT image that comes with the python bindings. The nvcr.io/nvidia/pytorch images don't come with TensorRT by default. You can try the TensorRT image instead: nvidia-docker run --rm nvcr.io/nvidia/tensorrt:19.09-py3 \\ python3 -c \"import tensorrt as trt; print(trt.__version__)\""
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I would like to start the orthanc server based on the below docker command. However when I execute the command, I get the error as shown below. Please note that both the orthanc.json and orthanc-db are present in the respective folders /orthanc/orthanc.json - orthanc.json is present under orthanc folder /orthanc/orthanc-db - orthanc-db is present under orthanc folder /etc/orthanc/orthanc.json - orthanc.json is present under /etc/orthanc folder /var/lib/orthanc/orthanc-db - orthanc-db is present under /var/lib/orthanc folder All the paths listed above are valid. I am able to navigate to them Docker command to start orthanc server docker run -p 4242:4242 -p 8042:8042 --rm --name orthanc -v /orthanc/orthanc.json:/etc/orthanc/orthanc.json -v /orthanc/orthanc- db:/var/lib/orthanc/orthanc-db jodogne/orthanc-plugins /etc/orthanc -- verbose Error message after executing the command Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_lin ux.go:424: container init caused \\\"rootfs_linux.go:58: mounting \\\\\\\"/orthanc/orthanc.json\\\\\\\" to rootfs \\\\\\\"/var/lib/docker/overlay2/ 48131fde47610cf1bac93d0316e2c1d6dfbfdb90a0e6cc24344cc6a1308eaccd/merged\\ \\\\\"at \\\\\\\"/var/lib/docker/overlay2/48131fde47610cf1bac93d031 6e2c1d6dfbfdb90a0e6cc24344cc6a1308eaccd/merged/etc/orthanc/orthanc.json\\ \\\\\"caused \\\\\\\"not a directory\\\\\\\"\\\"\": unknown: Are you tryin g to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type. Can you please help me fix this issue? I am trying to start the orthanc server through this docker command. not sure why it's throwing an error when the files are present.",
        "answers": [
            [
                "You are running the container from the same directory where your folders are (the ones you are mounting). This means that the path should be prefixed with the current working directory: docker run -p 4242:4242 -p 8042:8042 --rm --name orthanc -v $(pwd)/orthanc/orthanc.json:/etc/orthanc/orthanc.json -v $(pwd)/orthanc/orthanc- db:/var/lib/orthanc/orthanc-db jodogne/orthanc-plugins /etc/orthanc -- verbose"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "The image at https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow uses python 3.5. Is a python 3.6 or 3.7 image available so I don't have to change all f-strings to Python 3.5 format?",
        "answers": [
            [
                "Not at the moment. All NVIDIA TensorFlow images are released to that location, and they only support python 2.7 and 3.5 at the moment. New releases are documented here: https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/index.html"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a jupyter notebook running in docker container. I would like to port forward it to my local desktop browser. For that I do the below steps 1) Publish the container port to the remote host port 2) Port forward the remote host port to the localhost (desktop) Though I tried doing this, I get an error message that \"Page cannot be displayed\". Can you please let me know if I am making any error with the docker commands Publish ports (container to remote host) docker run --runtime=nvidia -it --rm -v /home/selva/aiaa_demo:/mnt/aiaa_spleen -p 8787:8888 $DOCKER_IMAGE jupyter notebook /opt/nvidia/medical/annotation/examples/MSD_Task09_Spleen --ip 0.0.0.0 --allow-root --no-browser Port forwarding in ubuntu bash screen ssh -L 8343:127.0.0.1:8787 onegpu The execution of above two commands doesn't help me in opening the browser in local desktop However, when I use --network-host , it works docker run --runtime=nvidia --network=host -it --rm -v /home/selva/demo:/mnt/disease -p 8787:8888 $DOCKER_IMAGE jupyter notebook /opt/nvidia/med/ann/examples/MSD --ip 0.0.0.0 --allow-root --no-browser Can you please tell me what is the mistake with my docker command or what can be the reason as to why it isn't opening? I am expecting to be able to open the jupyter notebook locally in my desktop without --network=host option and your help in fixing my docker command / port related issue",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to run the below docker command. This command is supposed to produce an output as shown below in screenshot but it throws an error as \"Unknown flag - ip\". Once this error is resolved, I would also like to port forward it to view the browser in my local environment as we don't any UI for remote server. However, I am encountering problem in getting this done successfully. Please note my docker version is 18.09.4, build d14af54 There are certain jupyter notebook examples that are stored in the below folder /opt/nvidia/data/image/examples/brats I would like to be able to view this jupyter notebook examples in my local environment. So, based on my understanding I felt that I had to do two steps step 1) Execute the docker run command as shown below in code section step 2) Port forward to local environment However, I am getting an error after execution of step 1. Actual output (error) of step 1 Expected output (success) of step 1 If step one had been successful, this below shown screenshot content should have been the actual output which will provide me details to port forward and login to the jupyter notebook Once, I am able to execute the step 1 successfully, I should be able to port forward Step 1 - Code docker run --runtime=nvidia -it --rm -v /home/selva/demo:/mnt/demo -p 8888:8888 -w /opt/nvidia/data/image/examples/brats $DOCKER_IMAGE jupyter notebook --ip 0.0.0.0 --allow-root --no-browser Step 2 - Code ssh -L 8234:127.0.0.1:8888 localhost As our docker is running in a remote gpu, we don't have UI to view it. In order to access it locally in my desktop(http://localhost:8234), I do port forwarding. Currently, neither am able to access the Jupyter notebook using 127.0.0.1:8888 or 127.0.0.1:8234. In both the scenarios, it fails I would like to be able to fix the issue of step 1 and be able to execute step 2 (port forward) and see the Jupyter notebook in my local browser. Can you please help?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to get gpu support on my container without the nvidia-docker I know with the nvidia docker, I just have to use --runtime=nvidia but my current circumstances does not allow using nvidia-docker I tried installing the nvidia driver, cuda, cudnn on my container but it fails. How can I use tensorflow gpu without nvidia docker on my container?",
        "answers": [
            [
                "You can use x11docker Running a docker image on X with gpu is as simple as x11docker --gpu imagename"
            ],
            [
                "You'll be happy to know that the latest Docker version now comes with support for nvidia GPU's. You'll need to use --device flag to specify your Nvidia driver. See - How to use GPU a docker container Earlier, you had to install nvidia-docker which was plain docker with a thin layer of abstraction for nvidia GPU's. See - Nvidia Docker"
            ],
            [
                "You cannot simply install nvidia drivers in a docker container. The container must have access to the hardware. Though I'm not certain, but mounts might help you with that issue. See- https://docs.docker.com/storage/ You can use anaconda to install and use Tensorflow-gpu. Make sure you have the latest nvidia drivers installed. Install Anaconda 2 or 3 from the official site. https://www.anaconda.com/distribution/ Create a new environment and install tensorflow-gpu and cudatoolkit. $conda create -n tf-gpu tensorflow-gpu python cudnn cudatoolkit You can also specify the version of application. E.g $conda create -n tf-gpu tensorflow-gpu python=3.5 cudnn cudatoolkit=8 Please do check if your hardware has the minimum compute capability to support the version of CUDA that you are/will be using."
            ],
            [
                "If you can't pass --runtime=nvidia as a command-line option (eg docker-compose), you can set the default runtime in the Docker daemon config file /etc/docker/daemon.json: { \"default-runtime\": \"nvidia\" }"
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I need to run a Python script within an NVIDA GPU Cloud (NGC) container on Docker in Ubuntu and I want to use Visual Studio Code to edit, run and debug it. I have installed the VS Code Docker Extension and read the documentation but none of it seems to fit my purpose. I have followed the NGC docs, installed the NVIDIA Container Runtime for Docker (nvidia-docker2) and am now at the point where on the command line I would launch an NGC container tarball docker load -i foo.tar sudo docker run {...} How do I configure VS Code so that I can run and debug Python scripts within this container?",
        "answers": [
            [
                "Download the NVIDA GPU Cloud (NGC) container. Create /home/bob/foobar.py in Visual Studio Code with the VS Code Docker Extension import ptvsd import time ptvsd.enable_attach(address = ('0.0.0.0', 5678)) ptvsd.wait_for_attach() time.sleep(2) print(\"all righty then\") Set a breakpoint on the last line. Debug|Add Configuration Docker: Attach to Node In launch.json add to \"configurations\" { \"name\": \"Python Attach (Remote Debug ptsvd default)\", \"type\": \"python\", \"request\": \"attach\", \"pathMappings\": [ { \"localRoot\": \"/home/bob\", // You may also manually specify the directory containing your source code. \"remoteRoot\": \"/home/bob\" // Linux example; adjust as necessary for your OS and situation. } ], \"port\": 5678, // Set to the remote port. \"host\": \"0.0.0.0\" // Set to your remote host's public IP address. }, Open a Terminal window: $ docker load -i foo.tar $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE nvidia/cuda 9.0-base 9dcd7cd95db6 2 weeks ago 135MB nvcr.io/nvidia/cuda latest 506c995952d1 7 weeks ago 2.74GB $ docker run -p 5678:5678 latest root@deadbeef: python -m pip install --user --upgrade ptvsd root@deadbeef: python foobar.py Start the debugger with configuration \"Python Attach (Remote Debug ptsvd default)\". It stops at the breakpoint."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to replicate work/experiments which require me to follow this particular tutorial on setting up Jupyter + Tensorflow + Nvidia GPU + Docker + Google Compute Engine. ' I'm able to successfully install nvidia-docker. However, in the tutorial, under the section Verify the GPU is Visible from a Docker Container, when I try to run sudo nvidia-docker-plugin I get the following error (see last line): nvidia-docker-plugin | 2019/04/23 15:17:47 Loading NVIDIA unified memory nvidia-docker-plugin | 2019/04/23 15:17:47 Loading NVIDIA management library nvidia-docker-plugin | 2019/04/23 15:17:47 Discovering GPU devices nvidia-docker-plugin | 2019/04/23 15:17:47 Provisioning volumes at /var/lib/nvidia-docker/volumes nvidia-docker-plugin | 2019/04/23 15:17:47 Serving plugin API at /run/docker/plugins nvidia-docker-plugin | 2019/04/23 15:17:47 Serving remote API at localhost:3476 nvidia-docker-plugin | 2019/04/23 15:17:47 Error: listen tcp 127.0.0.1:3476: bind: address already in use And when I run sudo nvidia-docker run --rm nvidia/cuda nvidia-smi I happen to get the following executable file not found in $PATH\": unknown error: docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"exec: \\\"nvidia-smi\\\": executable file not found in $PATH\": unknown. ERRO[0000] error waiting for container: context canceled I'm pretty new to docker; thus it would be nice if someone could help walk me through the solution. I've tried searching for answers, but the actual process for solving the problem evades me. Any help would be highly appreciated. EDIT: I set the GCE instance as specified in the tutorial (i.e. Ubuntu 16.04 LTS, 50GB boot disk, 1 GPU, with jupyter and tensorboard)",
        "answers": [
            [
                "To solve the first, looks like nvidia-docker-plugin is already running. To find this service, use : sudo netstat -tlpn | grep 3476 And kill it with : sudo pkill nvidia-docker The second, install nvidia-docker2 and reload the Docker daemon configuration with : curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \\ sudo apt-key add - distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\ sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update # Install nvidia-docker2 and reload the Docker daemon configuration sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd Links for more details : https://github.com/NVIDIA/nvidia-docker/issues/301 https://medium.com/@sh.tsang/docker-tutorial-5-nvidia-docker-2-0-installation-in-ubuntu-18-04-cb80f17cac65"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm trying to run this simple line of code in a docker container that comes with Pytorch. import torch torch.cuda.set_device(0) I get this error: RuntimeError: cuda runtime error (35) : CUDA driver version is insufficient for CUDA runtime version at torch/csrc/cuda/Module.cpp:32 Running torch.cuda.is_available() returns False. The host machine has the most up-to-date Nvidia drivers. Pytorch ships with Cuda, so there should be no incompatibility issues. What could cause this problem? Edit: @Patel Sunil's answer to this question answers my question, but I didn't come across this question in my search because their question is broad, while my question is specific to the cuda runtime/driver error. I posted this as a separate question for those who come across this error but don't know what it is a symptom of (namely, forgetting to use nvidia-docker).",
        "answers": [
            [
                "The problem was that I was running the container with docker, not nvidia-docker. Running the docker container with nvidia-docker fixed the problem."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a docker container that completes successfully and is in proper working order. I can see that jupyter, python3 install successfully. Other than the user-friendly interface Jupyter provides, the docker is working as expected. I have already tried running this code within the image build and receive the same result. Other Google searches for this have only shown people using Cloudera/Anaconda. I am using neither. me@gpu:which python3 /usr/bin/python3 me@gpu:which jupyter /usr/local/bin/jupyter me@gpu:jupyter notebook --ip 0.0.0.0 --port 8888 --allow-root Traceback (most recent call last): File \"/usr/local/bin/jupyter\", line 6, in &lt;module&gt; from jupyter_core.command import main I expect to be able to access a jupyter notebook via localhost:8888.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to mount my file(/home/ubuntu/grace/new_project on ubuntu) into Jenkins Docker container and run 'nvidia-docker'. The directory that I want to copy it is in /var/jenkins_home/new_folder on Docker container. This is the command to run Jenkins server using docker. sudo docker run --rm \\ -u root -p 8080:8080 \\ -v jenkins-data:/var/jenkins_home \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v \"$HOME\":/home jenkinsci/blueocean And I want to run this command in Jenkins. sudo nvidia-docker run -d \\ -v /home/ubuntu/grace:/notebooks/grace \\ -it --name test classification:ver1.2 sudo nvidia-docker exec -it test python main.py I expect my project (/home/ubuntu/grace/new_project) is in /var/jenkins_home/new_folder/ on docker container so that I can run 'nvidia-docker' to run 'main.py'. My ultimate goal is to make Jenkins pipeline to run my project using 'sudo nvidia-docker exec -it auto python main.py' from shell script. Anybody can help me?",
        "answers": [],
        "votes": []
    },
    {
        "question": "We have currently setup a GPU computing cluster with SLURM as a resource manager. As this is a cluster for deep-learning, we manage dependencies by using nvidia-docker images to facilitate different frameworks and CUDA versions. Our typical use case is to allocate resources with srun and give a command to run nvidia-docker which runs the experiment scripts as per the following: srun --gres=gpu:[num gpus required] nvidia-docker run --rm -u $(id -u):$(id -g) /bin/bash -c [python scripts etc..] &amp; We have discovered an issue where if a slurm job is cancelled using the scancel command, the docker process on the node is cancelled, but whatever experiment scripts that were started in the docker still continue. As far as we understand, this is not a fault in SLURM, but rather it is the case that killing a docker process does not kill its spawned processes, they will only be killed with the docker kill command. While there might be some way to execute the docker kill command in a SLURM prologue script, we were wondering if anyone else has had this problem and if they have solved it somehow. To summerize, we would like to know: How can we ensure that processes started in a nvidia-docker container, which in turn was started by a SLURM SRUN, are killed with SCANCEL?",
        "answers": [
            [
                "Configuring Slurm to use cgroups might help here. With cgroups enabled, any process belonging to a job is attached to a cgroup and destroyed when the job ends. Destruction is take care of by the kernel so there is no way a regular process can escape that."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to train a Yolo computer vision model using a container I built which includes an installation of Darknet. The container is using the Nvidia supplied base image: nvcr.io/nvidia/cuda:9.0-devel-ubuntu16.04 Using Nvidia-Docker on my local machine with a gtx 1080 ti, training runs very fast, however that same container running as an Azure Container Instance with a P100 gpu trains very slowly. It's almost as if it's not utilizing the gpu. I also noticed that the \"nvidia-smi\" command does not work in the container running in Azure, but it does work when I ssh into the container running locally on my machine. Here is the Dockerfile I am using FROM nvcr.io/nvidia/cuda:9.0-devel-ubuntu16.04 LABEL maintainer=\"alex.c.schultz@gmail.com\" \\ description=\"Pre-Configured Darknet Machine Learning Environment\" \\ version=1.0 # Container Dependency Setup RUN apt-get update RUN apt-get upgrade -y RUN apt-get install software-properties-common -y RUN apt-get install vim -y RUN apt-get install dos2unix -y RUN apt-get install git -y RUN apt-get install wget -y RUN apt-get install python3-pip -y RUN apt-get install libopencv-dev -y # setup virtual environment WORKDIR / RUN pip3 install virtualenv RUN virtualenv venv WORKDIR venv RUN mkdir notebooks RUN mkdir data RUN mkdir output # Install Darknet WORKDIR /venv RUN git clone https://github.com/AlexeyAB/darknet RUN sed -i 's/GPU=0/GPU=1/g' darknet/Makefile RUN sed -i 's/OPENCV=0/OPENCV=1/g' darknet/Makefile WORKDIR /venv/darknet RUN make # Install common pip packages WORKDIR /venv COPY requirements.txt ./ RUN . /venv/bin/activate &amp;&amp; pip install -r requirements.txt # Setup Environment EXPOSE 8888 VOLUME [\"/venv/notebooks\", \"/venv/data\", \"/venv/output\"] CMD . /venv/bin/activate &amp;&amp; jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root The requirements.txt file is as shown below: jupyter matplotlib numpy opencv-python scipy pandas sklearn",
        "answers": [
            [
                "The issue was that my training data was on an Azure File Share volume and the network latency was causing the training to be slow. I copied the data from the share into my container and then pointed the training to it and everything ran much faster."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to use the tensorflow-GPU serving in the windows 10 system. But I haven't found any solution for installing the Nvidia docker for windows. Please give me a suggestion, how to use the Windows machine GPU in deployment.",
        "answers": [
            [
                "Neither of nvidia-docker and nvidia-container-runtime have support for Windows, and there are no plans so far to support the OS either. Only linux host is supported. Check the FAQ here: Is macOS supported? No, we do not support macOS (regardless of the version), however you can use the native macOS Docker client to deploy your containers remotely (refer to the dockerd documentation). Is Microsoft Windows supported? No, we do not support Microsoft Windows (regardless of the version), however you can use the native Microsoft Windows Docker client to deploy your containers remotely (refer to the dockerd documentation)."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have been using Singularity 2.6 for a while and recently upgraded my Singularity to 3.0 (I cannot upgrade to 3.0.3 for technical reasons). Using Singularity 3.0 I build my new container with the same exact definition file that I used to build my container using Singularity 2.6. Since then, I cannot reliably run NVIDIA FleX anymore and get the following errors every now and then. I have absolutely not changed anything in my NVIDIA FleX code in the past 10 months or so and I have been running FleX almost every day and never had any issue like this. I wonder, what could be the issue and what is the error trying to say? What could I be doing wrong or need to change to prevent this from happening? Based on the print statements that I put in the code, all I know is this happens when I call the UnmapBuffers() function in my FleX code; but I have been doing this since I wrote this code and never had any issues. Although this might not be very helpful, but here's what the function does: /* Copyright (c) 2013-2017 NVIDIA Corporation. All rights reserved. This code contains NVIDIA Confidential Information and is disclosed to you under a form of NVIDIA software license agreement provided separately to you. NVIDIA Corporation and its licensors retain all intellectual property and proprietary rights in and to this software and related documentation and any modifications thereto. Any use, reproduction, disclosure, or distribution of this software and related documentation without an express license agreement from NVIDIA Corporation is strictly prohibited. ALL NVIDIA DESIGN SPECIFICATIONS, CODE ARE PROVIDED \"AS IS.\". NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. Information and code furnished is believed to be accurate and reliable. However, NVIDIA Corporation assumes no responsibility for the consequences of use of such information or for any infringement of patents or other rights of third parties that may result from its use. No license is granted by implication or otherwise under any patent or patent rights of NVIDIA Corporation. Details are subject to change without notice. This code supersedes and replaces all information previously supplied. NVIDIA Corporation products are not authorized for use as critical components in life support devices or systems without express written approval of NVIDIA Corporation. */ void UnmapBuffers(SimBuffers* buffers) { // particles buffers-&gt;positions.unmap(); buffers-&gt;restPositions.unmap(); buffers-&gt;velocities.unmap(); buffers-&gt;phases.unmap(); buffers-&gt;densities.unmap(); buffers-&gt;anisotropy1.unmap(); buffers-&gt;anisotropy2.unmap(); buffers-&gt;anisotropy3.unmap(); buffers-&gt;normals.unmap(); buffers-&gt;diffusePositions.unmap(); buffers-&gt;diffuseVelocities.unmap(); buffers-&gt;diffuseCount.unmap(); buffers-&gt;smoothPositions.unmap(); buffers-&gt;activeIndices.unmap(); // convexes buffers-&gt;shapeGeometry.unmap(); buffers-&gt;shapePositions.unmap(); buffers-&gt;shapeRotations.unmap(); buffers-&gt;shapePrevPositions.unmap(); buffers-&gt;shapePrevRotations.unmap(); buffers-&gt;shapeFlags.unmap(); // rigids buffers-&gt;rigidOffsets.unmap(); buffers-&gt;rigidIndices.unmap(); buffers-&gt;rigidMeshSize.unmap(); buffers-&gt;rigidCoefficients.unmap(); buffers-&gt;rigidPlasticThresholds.unmap(); buffers-&gt;rigidPlasticCreeps.unmap(); buffers-&gt;rigidRotations.unmap(); buffers-&gt;rigidTranslations.unmap(); buffers-&gt;rigidLocalPositions.unmap(); buffers-&gt;rigidLocalNormals.unmap(); // springs buffers-&gt;springIndices.unmap(); buffers-&gt;springLengths.unmap(); buffers-&gt;springStiffness.unmap(); // inflatables buffers-&gt;inflatableTriOffsets.unmap(); buffers-&gt;inflatableTriCounts.unmap(); buffers-&gt;inflatableVolumes.unmap(); buffers-&gt;inflatableCoefficients.unmap(); buffers-&gt;inflatablePressures.unmap(); // triangles buffers-&gt;triangles.unmap(); buffers-&gt;triangleNormals.unmap(); buffers-&gt;uvs.unmap(); } And finally, here's the errors I'm getting: *** buffer overflow detected ***: /om/user/arsalans/Programs/FleX/bin/linux64/NvFlexDemoReleaseCUDA_x64 terminated ======= Backtrace: ========= /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x2aff01f497e5] /lib/x86_64-linux-gnu/libc.so.6(__fortify_fail+0x5c)[0x2aff01feb15c] /lib/x86_64-linux-gnu/libc.so.6(+0x117160)[0x2aff01fe9160] /lib/x86_64-linux-gnu/libc.so.6(+0x1166c9)[0x2aff01fe86c9] /lib/x86_64-linux-gnu/libc.so.6(_IO_default_xsputn+0x80)[0x2aff01f4d6b0] /lib/x86_64-linux-gnu/libc.so.6(_IO_vfprintf+0x7bd)[0x2aff01f1f92d] /lib/x86_64-linux-gnu/libc.so.6(__vsprintf_chk+0x84)[0x2aff01fe8754] /lib/x86_64-linux-gnu/libc.so.6(__sprintf_chk+0x7d)[0x2aff01fe86ad] /om/user/arsalans/Programs/FleX/bin/linux64/NvFlexDemoReleaseCUDA_x64[0x42ca74] /om/user/arsalans/Programs/FleX/bin/linux64/NvFlexDemoReleaseCUDA_x64[0x42696f] /om/user/arsalans/Programs/FleX/bin/linux64/NvFlexDemoReleaseCUDA_x64[0x42785c] /om/user/arsalans/Programs/FleX/bin/linux64/NvFlexDemoReleaseCUDA_x64[0x40b314] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x2aff01ef2830] /om/user/arsalans/Programs/FleX/bin/linux64/NvFlexDemoReleaseCUDA_x64[0x40ddd9] ======= Memory map: ======== 00400000-006a7000 r-xp 00000000 f96:de686 198159349837841020 /om/user/arsalans/Programs/FleX/bin/linux64/NvFlexDemoReleaseCUDA_x64 008a7000-008aa000 r--p 002a7000 f96:de686 198159349837841020 /om/user/arsalans/Programs/FleX/bin/linux64/NvFlexDemoReleaseCUDA_x64 008aa000-008af000 rw-p 002aa000 f96:de686 198159349837841020 /om/user/arsalans/Programs/FleX/bin/linux64/NvFlexDemoReleaseCUDA_x64 008af000-008e0000 rw-p 00000000 00:00 0 01bdf000-0336c000 rw-p 00000000 00:00 0 [heap] 200000000-200200000 rw-s 00000000 00:05 59206 /dev/nvidiactl 200200000-200400000 ---p 00000000 00:00 0 200400000-200404000 rw-s 00000000 00:05 59206 /dev/nvidiactl 200404000-200600000 ---p 00000000 00:00 0 200600000-200a00000 rw-s 00000000 00:05 59206 /dev/nvidiactl 200a00000-201600000 ---p 00000000 00:00 0 201600000-201604000 rw-s 00000000 00:05 59206 /dev/nvidiactl 201604000-201800000 ---p 00000000 00:00 0 201800000-201c00000 rw-s 00000000 00:05 59206 /dev/nvidiactl 201c00000-202800000 ---p 00000000 00:00 0 202800000-202804000 rw-s 00000000 00:05 59206 /dev/nvidiactl 202804000-202a00000 ---p 00000000 00:00 0 202a00000-202e00000 rw-s 00000000 00:05 59206 /dev/nvidiactl 202e00000-203a00000 ---p 00000000 00:00 0 203a00000-203a04000 rw-s 00000000 00:05 59206 /dev/nvidiactl 203a04000-203c00000 ---p 00000000 00:00 0 203c00000-204000000 rw-s 00000000 00:05 59206 /dev/nvidiactl 204000000-204c00000 ---p 00000000 00:00 0 204c00000-204c04000 rw-s 00000000 00:05 59206 /dev/nvidiactl 204c04000-204e00000 ---p 00000000 00:00 0 204e00000-205200000 rw-s 00000000 00:05 59206 /dev/nvidiactl 205200000-205e00000 ---p 00000000 00:00 0 205e00000-205e04000 rw-s 00000000 00:05 59206 /dev/nvidiactl 205e04000-206000000 ---p 00000000 00:00 0 206000000-206400000 rw-s 00000000 00:05 59206 /dev/nvidiactl 206400000-207000000 ---p 00000000 00:00 0 207000000-207004000 rw-s 00000000 00:05 59206 /dev/nvidiactl 207004000-207200000 ---p 00000000 00:00 0 207200000-207600000 rw-s 00000000 00:05 59206 /dev/nvidiactl 207600000-208200000 ---p 00000000 00:00 0 208200000-208204000 rw-s 00000000 00:05 59206 /dev/nvidiactl 208204000-208400000 ---p 00000000 00:00 0 208400000-208800000 rw-s 00000000 00:05 59206 /dev/nvidiactl 208800000-209400000 ---p 00000000 00:00 0 209400000-209404000 rw-s 00000000 00:05 59206 /dev/nvidiactl 209404000-209600000 ---p 00000000 00:00 0 209600000-209a00000 rw-s 00000000 00:05 59206 /dev/nvidiactl 209a00000-209a04000 rw-s 00000000 00:05 59206 /dev/nvidiactl 209a04000-209c00000 ---p 00000000 00:00 0 209c00000-20a000000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20a000000-20a004000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20a004000-20a200000 ---p 00000000 00:00 0 20a200000-20a600000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20a600000-20a604000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20a604000-20a800000 ---p 00000000 00:00 0 20a800000-20ac00000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20ac00000-20ac04000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20ac04000-20ae00000 ---p 00000000 00:00 0 20ae00000-20b200000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20b200000-20b204000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20b204000-20b400000 ---p 00000000 00:00 0 20b400000-20b800000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20b800000-20b804000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20b804000-20ba00000 ---p 00000000 00:00 0 20ba00000-20be00000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20be00000-20be04000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20be04000-20c000000 ---p 00000000 00:00 0 20c000000-20c400000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20c400000-20c600000 ---p 00000000 00:00 0 20c600000-20c800000 rw-s 00000000 00:05 59206 /dev/nvidiactl 20c800000-300200000 ---p 00000000 00:00 0 10000000000-10204000000 ---p 00000000 00:00 0 2aff00413000-2aff00439000 r-xp 00000000 07:01 43453 /lib/x86_64-linux-gnu/ld-2.23.so 2aff00439000-2aff0043a000 rw-p 00000000 00:00 0 2aff0043a000-2aff0043b000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff0043b000-2aff0043c000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff0043c000-2aff0043d000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff0043d000-2aff0043e000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff0043e000-2aff0043f000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff0043f000-2aff00440000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff00440000-2aff00441000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff00441000-2aff00442000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff00442000-2aff00443000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff00443000-2aff00444000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff00444000-2aff00445000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff00445000-2aff00446000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff00446000-2aff00447000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff00447000-2aff00448000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff00448000-2aff00449000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff00449000-2aff0044a000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff0044a000-2aff0044b000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff0044b000-2aff0044c000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff0044c000-2aff0044d000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff0044d000-2aff0044e000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff0044e000-2aff0044f000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff0044f000-2aff00450000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff00450000-2aff00451000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff00451000-2aff00452000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff00452000-2aff00453000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff00453000-2aff005e9000 rw-p 00000000 00:00 0 2aff005e9000-2aff005ea000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff005ea000-2aff005eb000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff005eb000-2aff005ec000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff005ec000-2aff005ed000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff005ed000-2aff005ee000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff005ee000-2aff005ef000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff005ef000-2aff005f0000 rw-s 00000000 00:05 59207 /dev/nvidia0 2aff005f0000-2aff005f1000 rw-p 00000000 00:00 0 2aff00638000-2aff00639000 r--p 00025000 07:01 43453 /lib/x86_64-linux-gnu/ld-2.23.so 2aff00639000-2aff0063a000 rw-p 00026000 07:01 43453 /lib/x86_64-linux-gnu/ld-2.23.so 2aff0063a000-2aff0063b000 rw-p 00000000 00:00 0 2aff0063b000-2aff006c4000 r-xp 00000000 08:05 5669428 /.singularity.d/libs/libGL.so.1 2aff006c4000-2aff008c4000 ---p 00089000 08:05 5669428 /.singularity.d/libs/libGL.so.1 2aff008c4000-2aff008de000 rw-p 00089000 08:05 5669428 /.singularity.d/libs/libGL.so.1 2aff008de000-2aff008df000 rw-p 00000000 00:00 0 2aff008df000-2aff0094c000 r-xp 00000000 07:01 80764 /usr/lib/x86_64-linux-gnu/libGLU.so.1.3.1 2aff0094c000-2aff00b4c000 ---p 0006d000 07:01 80764 /usr/lib/x86_64-linux-gnu/libGLU.so.1.3.1 2aff00b4c000-2aff00b4d000 r--p 0006d000 07:01 80764 /usr/lib/x86_64-linux-gnu/libGLU.so.1.3.1 2aff00b4d000-2aff00b4e000 rw-p 0006e000 07:01 80764 /usr/lib/x86_64-linux-gnu/libGLU.so.1.3.1 2aff00b4e000-2aff00bd8000 r-xp 00000000 f96:de686 198159325678590248 /om/user/arsalans/Programs/FleX/external/libGLEW.so.1.10 2aff00bd8000-2aff00dd7000 ---p 0008a000 f96:de686 198159325678590248 /om/user/arsalans/Programs/FleX/external/libGLEW.so.1.10 2aff00dd7000-2aff00dde000 r--p 00089000 f96:de686 198159325678590248 /om/user/arsalans/Programs/FleX/external/libGLEW.so.1.10 2aff00dde000-2aff00ddf000 rw-p 00090000 f96:de686 198159325678590248 /om/user/arsalans/Programs/FleX/external/libGLEW.so.1.10 2aff00ddf000-2aff00de3000 rw-p 00000000 00:00 0 2aff00de3000-2aff00e07000 r-xp 00000000 07:01 43555 /lib/x86_64-linux-gnu/libpng12.so.0.54.0 2aff00e07000-2aff01006000 ---p 00024000 07:01 43555 /lib/x86_64-linux-gnu/libpng12.so.0.54.0 2aff01006000-2aff01007000 r--p 00023000 07:01 43555 /lib/x86_64-linux-gnu/libpng12.so.0.54.0 2aff01007000-2aff01008000 rw-p 00024000 07:01 43555 /lib/x86_64-linux-gnu/libpng12.so.0.54.0 2aff01008000-2aff0100b000 r-xp 00000000 07:01 43491 /lib/x86_64-linux-gnu/libdl-2.23.so 2aff0100b000-2aff0120a000 ---p 00003000 07:01 43491 /lib/x86_64-linux-gnu/libdl-2.23.so 2aff0120a000-2aff0120b000 r--p 00002000 07:01 43491 /lib/x86_64-linux-gnu/libdl-2.23.so 2aff0120b000-2aff0120c000 rw-p 00003000 07:01 43491 /lib/x86_64-linux-gnu/libdl-2.23.so 2aff0120c000-2aff01213000 r-xp 00000000 07:01 43566 /lib/x86_64-linux-gnu/librt-2.23.so 2aff01213000-2aff01412000 ---p 00007000 07:01 43566 /lib/x86_64-linux-gnu/librt-2.23.so 2aff01412000-2aff01413000 r--p 00006000 07:01 43566 /lib/x86_64-linux-gnu/librt-2.23.so 2aff01413000-2aff01414000 rw-p 00007000 07:01 43566 /lib/x86_64-linux-gnu/librt-2.23.so 2aff01414000-2aff01586000 r-xp 00000000 07:01 81948 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21 2aff01586000-2aff01786000 ---p 00172000 07:01 81948 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21 2aff01786000-2aff01790000 r--p 00172000 07:01 81948 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21 2aff01790000-2aff01792000 rw-p 0017c000 07:01 81948 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21 2aff01792000-2aff01796000 rw-p 00000000 00:00 0 2aff01796000-2aff0189e000 r-xp 00000000 07:01 43520 /lib/x86_64-linux-gnu/libm-2.23.so 2aff0189e000-2aff01a9d000 ---p 00108000 07:01 43520 /lib/x86_64-linux-gnu/libm-2.23.so 2aff01a9d000-2aff01a9e000 r--p 00107000 07:01 43520 /lib/x86_64-linux-gnu/libm-2.23.so 2aff01a9e000-2aff01a9f000 rw-p 00108000 07:01 43520 /lib/x86_64-linux-gnu/libm-2.23.so 2aff01a9f000-2aff01ab5000 r-xp 00000000 07:01 43501 /lib/x86_64-linux-gnu/libgcc_s.so.1 2aff01ab5000-2aff01cb4000 ---p 00016000 07:01 43501 /lib/x86_64-linux-gnu/libgcc_s.so.1 2aff01cb4000-2aff01cb5000 rw-p 00015000 07:01 43501 /lib/x86_64-linux-gnu/libgcc_s.so.1 2aff01cb5000-2aff01ccd000 r-xp 00000000 07:01 43560 /lib/x86_64-linux-gnu/libpthread-2.23.so 2aff01ccd000-2aff01ecc000 ---p 00018000 07:01 43560 /lib/x86_64-linux-gnu/libpthread-2.23.so 2aff01ecc000-2aff01ecd000 r--p 00017000 07:01 43560 /lib/x86_64-linux-gnu/libpthread-2.23.so 2aff01ecd000-2aff01ece000 rw-p 00018000 07:01 43560 /lib/x86_64-linux-gnu/libpthread-2.23.so 2aff01ece000-2aff01ed2000 rw-p 00000000 00:00 0 2aff01ed2000-2aff02092000 r-xp 00000000 07:01 43475 /lib/x86_64-linux-gnu/libc-2.23.so 2aff02092000-2aff02292000 ---p 001c0000 07:01 43475 /lib/x86_64-linux-gnu/libc-2.23.so 2aff02292000-2aff02296000 r--p 001c0000 07:01 43475 /lib/x86_64-linux-gnu/libc-2.23.so 2aff02296000-2aff02298000 rw-p 001c4000 07:01 43475 /lib/x86_64-linux-gnu/libc-2.23.so 2aff02298000-2aff0229c000 rw-p 00000000 00:00 0 2aff0229c000-2aff022ac000 r-xp 00000000 08:05 5669427 /.singularity.d/libs/libGLX.so.0 2aff022ac000-2aff024ab000 ---p 00010000 08:05 5669427 /.singularity.d/libs/libGLX.so.0 2aff024ab000-2aff024ac000 rw-p 0000f000 08:05 5669427 /.singularity.d/libs/libGLX.so.0 2aff024ac000-2aff024cc000 rw-p 00000000 00:00 0 2aff024cc000-2aff02552000 r-xp 00000000 08:05 5669426 /.singularity.d/libs/libGLdispatch.so.0 2aff02552000-2aff02752000 ---p 00086000 08:05 5669426 /.singularity.d/libs/libGLdispatch.so.0 2aff02752000-2aff0277a000 rw-p 00086000 08:05 5669426 /.singularity.d/libs/libGLdispatch.so.0 2aff0277a000-2aff0279a000 rw-p 00000000 00:00 0 2aff0279a000-2aff027b3000 r-xp 00000000 07:01 43598 /lib/x86_64-linux-gnu/libz.so.1.2.8 2aff027b3000-2aff029b2000 ---p 00019000 07:01 43598 /lib/x86_64-linux-gnu/libz.so.1.2.8 2aff029b2000-2aff029b3000 r--p 00018000 07:01 43598 /lib/x86_64-linux-gnu/libz.so.1.2.8 2aff029b3000-2aff029b4000 rw-p 00019000 07:01 43598 /lib/x86_64-linux-gnu/libz.so.1.2.8 2aff029b4000-2aff02ae9000 r-xp 00000000 07:01 80884 /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0 2aff02ae9000-2aff02ce9000 ---p 00135000 07:01 80884 /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0 2aff02ce9000-2aff02cea000 r--p 00135000 07:01 80884 /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0 2aff02cea000-2aff02cee000 rw-p 00136000 07:01 80884 /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0 2aff02cee000-2aff02cff000 r-xp 00000000 07:01 80909 /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0 2aff02cff000-2aff02efe000 ---p 00011000 07:01 80909 /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0 2aff02efe000-2aff02eff000 r--p 00010000 07:01 80909 /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0 2aff02eff000-2aff02f00000 rw-p 00011000 07:01 80909 /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0 2aff02f00000-2aff02f21000 r-xp 00000000 07:01 82152 /usr/lib/x86_64-linux-gnu/libxcb.so.1.1.0 2aff02f21000-2aff03120000 ---p 00021000 07:01 82152 /usr/lib/x86_64-linux-gnu/libxcb.so.1.1.0 2aff03120000-2aff03121000 r--p 00020000 07:01 82152 /usr/lib/x86_64-linux-gnu/libxcb.so.1.1.0 2aff03121000-2aff03122000 rw-p 00021000 07:01 82152 /usr/lib/x86_64-linux-gnu/libxcb.so.1.1.0 2aff03122000-2aff03124000 r-xp 00000000 07:01 80888 /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0 2aff03124000-2aff03324000 ---p 00002000 07:01 80888 /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0 2aff03324000-2aff03325000 r--p 00002000 07:01 80888 /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0 2aff03325000-2aff03326000 rw-p 00003000 07:01 80888 /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0 2aff03326000-2aff0332b000 r-xp 00000000 07:01 80905 /usr/lib/x86_64-linux-gnu/libXdmcp.so.6.0.0 2aff0332b000-2aff0352a000 ---p 00005000 07:01 80905 /usr/lib/x86_64-linux-gnu/libXdmcp.so.6.0.0 2aff0352a000-2aff0352b000 r--p 00004000 07:01 80905 /usr/lib/x86_64-linux-gnu/libXdmcp.so.6.0.0 2aff0352b000-2aff0352c000 rw-p 00005000 07:01 80905 /usr/lib/x86_64-linux-gnu/libXdmcp.so.6.0.0 2aff0352c000-2aff03d6e000 r-xp 00000000 08:05 5669446 /.singularity.d/libs/libcuda.so 2aff03d6e000-2aff03f6d000 ---p 00842000 08:05 5669446 /.singularity.d/libs/libcuda.so 2aff03f6d000-2aff040be000 rw-p 00841000 08:05 5669446 /.singularity.d/libs/libcuda.so 2aff040be000-2aff040cc000 rw-p 00000000 00:00 0 2aff040cc000-2aff04109000 r-xp 00000000 08:05 5669452 /.singularity.d/libs/libnvidia-fatbinaryloader.so.390.77 2aff04109000-2aff04308000 ---p 0003d000 08:05 5669452 /.singularity.d/libs/libnvidia-fatbinaryloader.so.390.77 2aff04308000-2aff04313000 rw-p 0003c000 08:05 5669452 /.singularity.d/libs/libnvidia-fatbinaryloader.so.390.77 2aff04313000-2aff04318000 rw-p 00000000 00:00 0 2aff04318000-2aff0a318000 ---p 00000000 00:00 0 2aff0a318000-2aff0a319000 ---p 00000000 00:00 0 2aff0a319000-2aff0a519000 rw-p 00000000 00:00 0 [stack:32784] 2aff0a519000-2aff0a51a000 ---p 00000000 00:00 0 2aff0a51a000-2aff0a79b000 rw-p 00000000 00:00 0 [stack:32785] 2aff0a800000-2aff0aa00000 ---p 00000000 00:00 0 2aff0aa00000-2aff0aa01000 ---p 00000000 00:00 0 2aff0aa01000-2aff0ac72000 rw-p 00000000 00:00 0 [stack:32786] 2aff0ae00000-2aff0b000000 ---p 00000000 00:00 0 2aff0b000000-2aff0b200000 rw-s 00000000 00:04 1306015074 /dev/zero (deleted) 2aff0b200000-2aff0b400000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff0b400000-2aff0b600000 rw-s 00000000 00:04 1306015075 /dev/zero (deleted) 2aff0b600000-2aff0b800000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff0b800000-2aff0bad6000 rw-s 00000000 00:05 59206 /dev/nvidiactl 2aff0bad6000-2aff0be00000 ---p 00000000 00:00 0 2aff0c000000-2aff0c021000 rw-p 00000000 00:00 0 2aff0c021000-2aff10000000 ---p 00000000 00:00 0 2aff10000000-2aff10021000 rw-p 00000000 00:00 0 2aff10021000-2aff14000000 ---p 00000000 00:00 0 2aff14000000-2aff1e400000 ---p 00000000 00:00 0 2aff1e400000-2aff1e600000 rw-s 00000000 00:04 1306015079 /dev/zero (deleted) 2aff1e800000-2aff1ea00000 rw-s 00000000 00:04 1306015092 /dev/zero (deleted) 2aff1ec00000-2aff2c400000 ---p 00000000 00:00 0 2aff2c600000-2aff2c800000 rw-s 00000000 00:04 1306015083 /dev/zero (deleted) 2aff2c800000-2aff2ca00000 rw-s 00000000 00:04 1306015086 /dev/zero (deleted) 2aff2ca00000-2aff2cc00000 rw-s 00000000 00:04 1306015087 /dev/zero (deleted) 2aff2cc00000-2aff2ce00000 rw-s 00000000 00:04 1306015093 /dev/zero (deleted) 2aff2d000000-2aff2d2ac000 rw-s 00000000 00:04 1306015095 /dev/zero (deleted) 2aff2d2ac000-2aff2f200000 ---p 00000000 00:00 0 2aff2f200000-2aff2f400000 rw-s 00000000 00:04 1306015096 /dev/zero (deleted) 2aff2f400000-2aff2f600000 rw-s 00000000 00:04 1306015097 /dev/zero (deleted) 2aff2f600000-2aff2f800000 rw-s 00000000 00:04 1306015098 /dev/zero (deleted) 2aff2f800000-2aff2fa00000 rw-s 00000000 00:04 1306015099 /dev/zero (deleted) 2aff2fa00000-2aff2fc00000 rw-s 00000000 00:04 1306015100 /dev/zero (deleted) 2aff2fc00000-2aff2fe00000 rw-s 00000000 00:04 1306015101 /dev/zero (deleted) 2aff2fe00000-2aff30400000 ---p 00000000 00:00 0 7ffc22c4e000-7ffc22c70000 rw-p 00000000 00:00 0 [stack] 7ffc22d86000-7ffc22d88000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall] For those who are Singularity users: unfortunately, I cannot try using Singularity 2.6 anymore to see if the issue goes away because I am using a cluster and cannot install things myself. Note that I am pulling NVIDIA's Docker image in my container when building it. Here's the main part of my Singularity definition file when building my container: BootStrap: docker From: nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04 %post # Set up some required environment defaults apt-get -y update &amp;&amp; apt-get -y install software-properties-common &amp;&amp; yes '' | add-apt-repository ppa:deadsnakes/ppa apt-get -y update &amp;&amp; apt-get -y install locales locale-gen en_US.UTF-8 apt-get -y update &amp;&amp; apt-get -y install make \\ dpkg \\ wget \\ bzip2 \\ libglib2.0-0 \\ libxext6 \\ libsm6 \\ libxrender1 \\ g++ \\ gcc \\ xvfb \\ libyaml-cpp-dev \\ git \\ cmake \\ vim \\ curl \\ ca-certificates \\ python3.6 \\ python3.6-dev",
        "answers": [
            [
                "It turns out my code was trying to create a file on disk during the simulations and the file wouldn't get created for some reason, which could be related to this. The code would always work in the past but I don't know what has changed in my container that leads to that part of the code not being able to create that file anymore."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I need to check if the kubernetes node is configured correctly. Need to use nvidia-docker for one of the worker nodes. Using: https://github.com/NVIDIA/k8s-device-plugin How can I confirm that the configuration is correct for the device plugin? $ kubectl describe node mynode Roles: worker Capacity: cpu: 4 ephemeral-storage: 15716368Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 62710736Ki nvidia.com/gpu: 1 pods: 110 Allocatable: cpu: 3800m ephemeral-storage: 14484204725 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 60511184Ki nvidia.com/gpu: 1 pods: 110 System Info: Machine ID: f32e0af35637b5dfcbedcb0a1de8dca1 System UUID: EC2A40D3-76A8-C574-0C9E-B9D571AA59E2 Boot ID: 9f2fa456-0214-4f7c-ac2a-2c62c2ef25a4 Kernel Version: 3.10.0-957.1.3.el7.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://18.9.1 Kubelet Version: v1.11.2 Kube-Proxy Version: v1.11.2 However, I can see the nvidia.com/gpu under node resources, the question is: is the Container Runtime Version supposed to say nvidia-docker if the node is configured correctly? Currently, it shows docker which seems fishy, I guess!",
        "answers": [
            [
                "Not sure if you did it already, but it seems to be clearly described: After installing NVIDIA drivers and NVIDIA docker, you need to enable nvidia runtime on your node, by editing /etc/docker/daemon.json as specified here. So as the instruction says, if you can see that runtimes is correct, you just need to edit that config. Then deploy a DeamonSet (which is a way of ensuring that a pod runs on each node, with access to host network and devices): kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.11/nvidia-device-plugin.yml Now your containers are ready to consume the GPU - as described here."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am using https://hub.docker.com/r/jarkt/docker-remote-api/ to get Docker stats remotely using CURL as follows: curl http://&lt;IP&gt;:&lt;PORT&gt;/containers/&lt;container_name&gt;/stats . Looking for a similar one for getting GPU stats for a specific docker container.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using https://hub.docker.com/r/jarkt/docker-remote-api/ to get Docker stats remotely using CURL as follows: curl http://&lt;IP&gt;:&lt;PORT&gt;/containers/&lt;container_name&gt;/stats . Looking for a similar one for getting GPU stats for a specific docker container.",
        "answers": [],
        "votes": []
    },
    {
        "question": "The aim is to run an OpenCL/OpenGL (interop) app inside a docker container. But I have not been successful yet. Intro I have laptop with an NVidia graphics card so I thought leveraging on NVidia Dockerfiles [1,2] would be a good starting point. The following Dockerfile: # Dockerfile to run OpenGL app FROM nvidia/opengl:1.0-glvnd-runtime-ubuntu16.04 ENV NVIDIA_DRIVER_CAPABILITIES ${NVIDIA_DRIVER_CAPABILITIES},display RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ mesa-utils &amp;&amp; \\ rm -rf /var/lib/apt/lists/* works quite well, and I was able to run glxgears. Running OpenCL on its own container was no big deal either: # Dockerfile to run OpenCL app FROM nvidia/opengl:1.0-glvnd-runtime-ubuntu16.04 RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ ocl-icd-libopencl1 \\ clinfo &amp;&amp; \\ rm -rf /var/lib/apt/lists/* RUN mkdir -p /etc/OpenCL/vendors &amp;&amp; \\ echo \"libnvidia-opencl.so.1\" &gt; /etc/OpenCL/vendors/nvidia.icd ENV NVIDIA_VISIBLE_DEVICES all ENV NVIDIA_DRIVER_CAPABILITIES compute,utility and clinfo successfully shows information about my device. Attempt Finally here's my attempt at creating a container with both OpenGL and OpenCL drivers: # Dockerfile mixing OpenGL and OpenCL FROM nvidia/opengl:1.0-glvnd-runtime-ubuntu16.04 ENV NVIDIA_DRIVER_CAPABILITIES ${NVIDIA_DRIVER_CAPABILITIES},display RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ mesa-utils \\ ocl-icd-libopencl1 \\ clinfo &amp;&amp; \\ rm -rf /var/lib/apt/lists/* RUN mkdir -p /etc/OpenCL/vendors &amp;&amp; \\ echo \"libnvidia-opencl.so.1\" &gt; /etc/OpenCL/vendors/nvidia.icd ENV NVIDIA_VISIBLE_DEVICES all ENV NVIDIA_DRIVER_CAPABILITIES compute,utility And now, although clinfo still prints OpenCL device information, glxgears on the other hand fails with the following error: Error: couldn't get an RGB, Double-buffered visual Any idea how to make this work? Thanks in advance. References [1] https://gitlab.com/nvidia/opencl/blob/ubuntu16.04/devel/Dockerfile [2] https://gitlab.com/nvidia/opencl/blob/ubuntu16.04/runtime/Dockerfile",
        "answers": [
            [
                "ENV NVIDIA_DRIVER_CAPABILITIES compute,utility You forgot the capability display."
            ],
            [
                "What did work for me is the following STEP 1: added at the end of the Dockerfile the following two lines ENV NVIDIA_VISIBLE_DEVICES all ENV NVIDIA_DRIVER_CAPABILITIES compute,utility,display STEP 2: to run the container $ sudo xhost +local:root $ docker run --gpus all -it --rm --name container_name \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY=$DISPLAY \\ -e QT_X11_NO_MITSHM=1 \\ --net=host \\ image_name bash"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am working on an IoTEdge Module with image detection capabilities. For the image processing/analysing I am using Detectron which needs to run in an docker nvidia runtime. Is it possible to enable an nvidia runtime for IoTEdge Modules and Docker Moby and how? I am not able to figure out on how to make it work. There is an entry about the topic here, but I am still not able to get it work: https://github.com/moby/moby/issues/23917 https://github.com/NVIDIA/nvidia-docker/wiki/Internals I figured out, how to get it work with Docker CE, unfortunatly, the documentation says, Moby is not supported by IoT Edge. I havn't found any sideeffects yet, but for production it would be nice to understand the impact.",
        "answers": [
            [
                "You can try setting the runtime as nvidia in the create options in your deployment.json in addition to any other create options you specify. \"createOptions\": { \"HostConfig\": { \"runtime\": \"nvidia\" }}"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am planning to containerize legacy gpu intensive windows application using Nvidia Docker and kubernetis. But nvidia docker doesn't support windows as of now. please suggest some ideas. can I use RancherVM or kubevirt?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Is it possible to run two Nvidia Docker containers, each with its own Nvidia driver version? On my cloud instance, I have an older application running for which newer Nvidia drivers are causing issues. I'd like the ability to keep running it with the older driver, while allowing newer applications on the same instance to use newer drivers. I was thinking I could accomplish this with containers but I'm worried that they only allow you to containerize things in user space.",
        "answers": [
            [
                "The answer is no. The driver is installed on the host. These articles: NVIDIA Docker: GPU Server Application Deployment Made Easy and the newer Enabling GPUs in the Container Runtime Ecosystem discuss how the stack is set up. The key takeaway is that Nvidia brings their own version of runc (the part of Docker that actually runs the container process). This modified version of runc communicates with the host OS to make driver-level details available to the container process. EDIT (Oct 2022): I believe this answer is outdated. Nvidia/Docker have since released Driver Containers which allow provisioning of the driver as a container. The only thing that's needed is the Nvidia Container Toolkit and some light configuration. See this link for details."
            ],
            [
                "Answering for an update, actually now its possible to different driver version inside different containers. All you have to do is change the NVIDIA Container Toolkit config file (/etc/nvidia-container-runtime/config.toml) so that the root directive points to the driver container as shown below: disable-require = false swarm-resource = \"DOCKER_RESOURCE_GPU\" [nvidia-container-cli] root = \"/run/nvidia/driver\" path = \"/usr/bin/nvidia-container-cli\" environment = [] debug = \"/var/log/nvidia-container-toolkit.log\" ldcache = \"/etc/ld.so.cache\" load-kmods = true no-cgroups = false user = \"root:video\" ldconfig = \"@/sbin/ldconfig.real\" [nvidia-container-runtime] debug = \"/var/log/nvidia-container-runtime.log\" This command can be directly used to make this modification : $sudo sed -i 's/^#root/root/' /etc/nvidia-container-runtime/config.toml After that you can run a container with a required version of driver in background using : $sudo docker run --name nvidia-driver -d --privileged --pid=host \\ -v /run/nvidia:/run/nvidia:shared \\ -v /var/log:/var/log \\ --restart=unless-stopped \\ nvidia/driver:450.80.02-ubuntu18.04 and then run the GPU conatiner with required version of CUDA for your work: $sudo docker run --gpus all nvidia/cuda:11.0-base nvidia-smi"
            ],
            [
                "Containers are for isolating the processes. Only the host's kernel is same for all containers, which is not the case for virtual machine. So you can create one container for old application with older driver and another container for new application with new nvidia driver. Containers are meant for this. But for nvidia docker you may require 1 gpu per pod, but this can be bypassed using some simple methods, which is not a good solution"
            ]
        ],
        "votes": [
            3.0000001,
            1e-07,
            -0.9999999
        ]
    },
    {
        "question": "While attempting to compile darknet in the build command of a docker container I constantly run into the exception include/darknet.h:11:30: fatal error: cuda_runtime.h: No such file or directory. I am building the container from the instructions here: https://github.com/NVIDIA/nvidia-docker/wiki/Deploy-on-Amazon-EC2. I have a simple Dockerfile I am testing with - the relevant parts: FROM nvidia/cuda:9.2-runtime-ubuntu16.04 ... WORKDIR / RUN apt-get install -y git RUN git clone https://github.com/pjreddie/darknet.git WORKDIR /darknet # Set OpenCV makefile flag RUN sed -i '/OPENCV=0/c\\OPENCV=1' Makefile RUN sed -i '/GPU=0/c\\GPU=1' Makefile #RUN ln -s /usr/local/cuda-9.2 /usr/local/cuda # HERE I have been playing with commands to show me the state of the docker image to try to troubleshoot the problem RUN find / -name \"cuda_runtime.h\" RUN ls /usr/local/cuda/lib64/ RUN less /usr/local/cuda/README RUN make Most of the documentation I see references using the nvidia libraries when running a container, but the darknet compiles differently when built with gpu support so I need cuda_runtime.h available at build time. Perhaps I misunderstand what nvidia-docker is doing - I'm assuming that nvidia-docker exists because the Nvidia code must be installed on the actual host machine and not inside the container &amp; they use some mechanism to share the \"native\" code with the containers so the GPU can be managed - is that correct? Should I even be trying to build darknet when building my container or should I be installing it on the host machine, then making it available somehow to the container? This seems to go against the portability of the containers but I can live with some constraints to get access to the GPU.",
        "answers": [
            [
                "FROM nvidia/cuda:9.2-runtime-ubuntu16.04 Your image only has bits and pieces of CUDA-9.2 needed to run a CUDA app, but does not have the bits needed to build one. You need to use -devel variant."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "Am on ubuntu 18.04 running in interactive env like this: docker run --runtime=nvidia -it --rm -v $PWD:/root/stuff -w /root tensorflow/tensorflow:latest-gpu-py3 bash Curiously, I don't get segfaults when I run non-interactively i.e. docker run ... python stuff/mnist.py nvidia details: $ nvidia-smi Thu Nov 29 22:09:25 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 415.18 Driver Version: 415.18 CUDA Version: 10.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce RTX 2070 Off | 00000000:01:00.0 On | N/A | | 30% 32C P8 11W / 175W | 358MiB / 7949MiB | 3% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1471 G /usr/lib/xorg/Xorg 18MiB | | 0 1523 G /usr/bin/gnome-shell 50MiB | | 0 1919 G /usr/lib/xorg/Xorg 129MiB | | 0 2063 G /usr/bin/gnome-shell 114MiB | | 0 3762 G ...quest-channel-token=2440404091774701506 43MiB | +-----------------------------------------------------------------------------+ root@4a46cc9acb73:~# python -X faulthandler -vv stuff/mnist.py Train on 60000 samples, validate on 10000 samples Epoch 1/15 2018-11-29 22:06:26.371579: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2018-11-29 22:06:26.500120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2018-11-29 22:06:26.500670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62 pciBusID: 0000:01:00.0 totalMemory: 7.76GiB freeMemory: 7.29GiB 2018-11-29 22:06:26.500686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0 2018-11-29 22:06:26.723360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix: 2018-11-29 22:06:26.723400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 2018-11-29 22:06:26.723407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2018-11-29 22:06:26.723859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7015 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5) Fatal Python error: Segmentation fault Thread 0x00007f82a1277700 (most recent call first): File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1439 in __call__ File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 2986 in __call__ File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 215 in fit_loop File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 1639 in fit File \"stuff/mnist.py\", line 36 in &lt;module&gt; Segmentation fault (core dumped)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I wrote the following Dockerfile to build an image with OpenPose (https://github.com/CMU-Perceptual-Computing-Lab/openpose) # Download base image FROM nvidia/cuda:9.0-cudnn7-devel # Set keyboard configuration in advance of installing CUDA #RUN apt-get update &amp;&amp; apt-get install -yq keyboard-configuration #==========Caffe dependencies================= RUN apt-get --assume-yes update &amp;&amp; \\ apt-get --assume-yes install build-essential \\ libatlas-base-dev \\ libprotobuf-dev \\ libleveldb-dev \\ libsnappy-dev \\ libhdf5-serial-dev \\ protobuf-compiler RUN apt-get --assume-yes install --no-install-recommends libboost-all-dev # Remaining dependencies RUN apt-get --assume-yes install libgflags-dev \\ libgoogle-glog-dev \\ liblmdb-dev #=========OpenCV============ RUN apt-get update &amp;&amp; apt-get --assume-yes install libopencv-dev \\ python-opencv \\ python-pip \\ python-dev \\ cmake \\ libeigen3-dev \\ doxygen #=========Python Libs======= RUN pip install --upgrade numpy \\ protobuf \\ opencv-python #=========OpenPose========== RUN apt-get -y install git RUN git clone --depth 10 https://github.com/CMU-Perceptual-Computing-Lab/openpose.git /opt/openpose RUN apt-get install -y cmake # Set environment variables ENV PATH \"/usr/local/cuda-9.0/bin:$PATH\" ENV LD_LIBRARY_PATH \"/usr/local/cuda-9.0/lib64:$LD_LIBRARY_PATH\" ENV OPENPOSE_ROOT /opt/openpose RUN cd /opt/openpose &amp;&amp; \\ mkdir -p build &amp;&amp; cd build &amp;&amp; \\ cmake \\ -DCMAKE_BUILD_TYPE=\"Release\" \\ -DBUILD_EXAMPLES=ON \\ -DBUILD_DOCS=ON \\ -DBUILD_SHARED_LIBS=ON \\ -DDOWNLOAD_BODY_25_MODEL=ON \\ -DDOWNLOAD_BODY_COCO_MODEL=ON \\ -DDOWNLOAD_BODY_MPI_MODEL=ON \\ -DDOWNLOAD_HAND_MODEL=ON \\ -DWITH_3D_RENDERER:BOOL=OFF \\ -DBUILD_PYTHON=ON ../ RUN printenv &amp;&amp; cd opt/openpose/build &amp;&amp; make all -n -j\"$(nproc)\" When building the Dockerfile with nvidia-docker build -t openpose_image . I get the following error in the last step: Step 14/14 : RUN printenv &amp;&amp; cd opt/openpose/build &amp;&amp; make all -n -j\"$(nproc)\" ---&gt; Running in 8cbc7a731e6d LIBRARY_PATH=/usr/local/cuda/lib64/stubs HOSTNAME=8cbc7a731e6d LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 HOME=/root CUDA_VERSION=9.0.176 NVIDIA_REQUIRE_CUDA=cuda&gt;=9.0 NVIDIA_DRIVER_CAPABILITIES=compute,utility PATH=/usr/local/cuda-9.0/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin CUDA_PKG_VERSION=9-0=9.0.176-1 CUDNN_VERSION=7.4.1.5 PWD=/ NVIDIA_VISIBLE_DEVICES=all NCCL_VERSION=2.3.7 OPENPOSE_ROOT=/opt/openpose /usr/bin/cmake -H/opt/openpose -B/opt/openpose/build --check-build-system CMakeFiles/Makefile.cmake 0 /usr/bin/cmake -E cmake_progress_start /opt/openpose/build/CMakeFiles /opt/openpose/build/CMakeFiles/progress.marks make -f CMakeFiles/Makefile2 all make -f CMakeFiles/doc_doxygen.dir/build.make CMakeFiles/doc_doxygen.dir/depend make -f CMakeFiles/openpose_caffe.dir/build.make CMakeFiles/openpose_caffe.dir/depend cd /opt/openpose/build &amp;&amp; /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /opt/openpose /opt/openpose /opt/openpose/build /opt/openpose/build /opt/openpose/build/CMakeFiles/doc_doxygen.dir/DependInfo.cmake --color= make -f CMakeFiles/doc_doxygen.dir/build.make CMakeFiles/doc_doxygen.dir/build cd /opt/openpose/build &amp;&amp; /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /opt/openpose /opt/openpose /opt/openpose/build /opt/openpose/build /opt/openpose/build/CMakeFiles/openpose_caffe.dir/DependInfo.cmake --color= make -f CMakeFiles/openpose_caffe.dir/build.make CMakeFiles/openpose_caffe.dir/build /usr/bin/cmake -E cmake_echo_color --switch= --blue --bold --progress-dir=/opt/openpose/build/CMakeFiles --progress-num=1 \"Generating API documentation with Doxygen\" cd /opt/openpose/doc &amp;&amp; /usr/bin/doxygen /opt/openpose/doc/doc_autogeneration.doxygen /usr/bin/cmake -E cmake_echo_color --switch= --blue --bold --progress-dir=/opt/openpose/build/CMakeFiles --progress-num=4 \"Creating directories for 'openpose_caffe'\" /usr/bin/cmake -E make_directory /opt/openpose/3rdparty/caffe /usr/bin/cmake -E make_directory /opt/openpose/build/caffe/src/openpose_caffe-build /usr/bin/cmake -E make_directory /opt/openpose/build/caffe /usr/bin/cmake -E make_directory /opt/openpose/build/caffe/tmp /usr/bin/cmake -E make_directory /opt/openpose/build/caffe/src/openpose_caffe-stamp /usr/bin/cmake -E make_directory /opt/openpose/build/caffe/src /usr/bin/cmake -E touch /opt/openpose/build/caffe/src/openpose_caffe-stamp/openpose_caffe-mkdir /usr/bin/cmake -E cmake_echo_color --switch= --blue --bold --progress-dir=/opt/openpose/build/CMakeFiles --progress-num=5 \"No download step for 'openpose_caffe'\" /usr/bin/cmake -E echo_append /usr/bin/cmake -E touch /opt/openpose/build/caffe/src/openpose_caffe-stamp/openpose_caffe-download /usr/bin/cmake -E cmake_echo_color --switch= --blue --bold --progress-dir=/opt/openpose/build/CMakeFiles --progress-num=7 \"No patch step for 'openpose_caffe'\" /usr/bin/cmake -E echo_append /usr/bin/cmake -E touch /opt/openpose/build/caffe/src/openpose_caffe-stamp/openpose_caffe-patch /usr/bin/cmake -E cmake_echo_color --switch= --blue --bold --progress-dir=/opt/openpose/build/CMakeFiles --progress-num=6 \"No update step for 'openpose_caffe'\" /usr/bin/cmake -E echo_append /usr/bin/cmake -E touch /opt/openpose/build/caffe/src/openpose_caffe-stamp/openpose_caffe-update /usr/bin/cmake -E cmake_echo_color --switch= --blue --bold --progress-dir=/opt/openpose/build/CMakeFiles --progress-num=8 \"Performing configure step for 'openpose_caffe'\" cd /opt/openpose/build/caffe/src/openpose_caffe-build &amp;&amp; /usr/bin/cmake -DCMAKE_INSTALL_PREFIX:PATH=/opt/openpose/build/caffe -DUSE_CUDNN=ON -DCUDA_ARCH_NAME=Auto -DCPU_ONLY=OFF -DCMAKE_BUILD_TYPE=Release -DBUILD_docs=OFF -DBUILD_python=OFF -DBUILD_python_layer=OFF -DUSE_LEVELDB=OFF -DUSE_LMDB=OFF -DUSE_OPENCV=OFF \"-GUnix Makefiles\" /opt/openpose/3rdparty/caffe cd /opt/openpose/build/caffe/src/openpose_caffe-build &amp;&amp; /usr/bin/cmake -E touch /opt/openpose/build/caffe/src/openpose_caffe-stamp/openpose_caffe-configure /usr/bin/cmake -E cmake_echo_color --switch= --blue --bold --progress-dir=/opt/openpose/build/CMakeFiles --progress-num=9 \"Performing build step for 'openpose_caffe'\" cd /opt/openpose/build/caffe/src/openpose_caffe-build &amp;&amp; make /usr/bin/cmake -E cmake_echo_color --switch= --progress-dir=/opt/openpose/build/CMakeFiles --progress-num=1 \"Built target doc_doxygen\" make[3]: *** No targets specified and no makefile found. Stop. CMakeFiles/openpose_caffe.dir/build.make:110: recipe for target 'caffe/src/openpose_caffe-stamp/openpose_caffe-build' failed make[2]: *** [caffe/src/openpose_caffe-stamp/openpose_caffe-build] Error 2 CMakeFiles/Makefile2:104: recipe for target 'CMakeFiles/openpose_caffe.dir/all' failed make[1]: *** [CMakeFiles/openpose_caffe.dir/all] Error 2 Makefile:83: recipe for target 'all' failed make: *** [all] Error 2 The command '/bin/sh -c printenv &amp;&amp; cd opt/openpose/build &amp;&amp; make all -n -j\"$(nproc)\"' returned a non-zero code: 2 Two weird things also happen here. 1) If I run the image built so far with docker run -it openpose_image and then cd opt/openpose/build &amp;&amp; make all -j\"$(nproc)\" I get the same exact error. 2) If I do instead nvidia-docker run -it openpose_image and then cd opt/openpose/build &amp;&amp; make all -j\"$(nproc)\" the building runs smoothly, ending successfully without raising errors. Can somebody help me to figure out what is going on? Thanks",
        "answers": [
            [
                "I assume that's because shell expansion doesn't happen with the RUN statements. I just tried using backticks instead (i.e. make -j`nproc`) and it appeared to work"
            ],
            [
                "Shell expansion does work in RUN statements. Just remove your double quotes. RUN printenv &amp;&amp; cd opt/openpose/build &amp;&amp; make all -n -j $(nproc)"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "A DGX-1 has quite a lot of power. However, when using it I only utilize 34% of one of eight cards. Are there some points and tweaks I might missed? I know I can't parallelize everything, there is an upper limit. But to be honest, I expected a better performance. I'm running a nvidia-docker with tensorflow preinstalled. The script running is from dennybritz which is quite well used. I ran the docker one one card. Since it didn't utilized the whole card I didn't gave him second one. Would this have any benefit? Of course I could ran multiple instances and pick the best one. But I'd rather have results sooner when having 170TFLOPS accessible.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to install NVIDIA docker. I used these lines: curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \\ sudo apt-key add - distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\ sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update and then: $ sudo apt-get install nvidia-docker Now trying to check if it installed correctly by typing: nvidia-docker run --rm nvidia/cuda nvidia-smi This error appears: nvidia-docker | 2018/11/06 13:09:24 Error: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/version: dial unix /var/run/docker.sock: connect: permission denied Later I tried: sudo nvidia-docker run --rm nvidia/cuda nvidia-smi This error appears: Using default tag: latest latest: Pulling from nvidia/cuda 473ede7ed136: Pull complete c46b5fa4d940: Pull complete 93ae3df89c92: Pull complete 6b1eed27cade: Pull complete d31e9163d0a5: Pull complete 8668af631f88: Pull complete 0d99f8ab6ae2: Pull complete 74440c29d798: Pull complete Digest: sha256:a6b5fd418d1cd0bc6d8a60c1c4ba33670508487039b828904f8494ec29e6b450 Status: Downloaded newer image for nvidia/cuda:latest docker: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused \"exec: \\\"nvidia-smi\\\": executable file not found in $PATH\": unknown. I install Linux with Ubuntu. Can someone help me, please?",
        "answers": [
            [
                "You have two errors which are quite self explicites. First of all, it seems your login user is not allowed to connect to docker daemon. This is quite a standard issue, you just need to add your user login to docker group, It should solve this issue. You will need to logout/login again for this change to become active. Second, this is also quite a standard linux issue, your shell has a environment variable called PATH , containing all folders where it will be looking for a command binary, when this command doesn't contain full path to the binary. For exemple, when you typed curl to download docker-nvidia, your shell find it in /usr/bin/ folder, because this folder is declared into the PATH variable. Same applies for containers you download and different users on your local system. You can investigate this specific error message and find this issue on github: https://github.com/NVIDIA/nvidia-docker/issues/388"
            ],
            [
                "Just for reference adding this note. The latest tag is deprecated, and this error is expects: Look under Deprecated: \"latest\" tag on https://hub.docker.com/r/nvidia/cuda"
            ],
            [
                "Start by setting the GPG and remote repo for the package $ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \\ sudo apt-key add - distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\ sudo tee /etc/apt/sources.list.d/nvidia-docker.list Then update the apt lists $ sudo apt-get update Now you install nvidia-docker (2) and reload the Docker daemon configurations $ sudo apt-get install -y nvidia-docker2 $ sudo pkill -SIGHUP dockerd Nvidia GPUs first require drivers to be installed. Here is how you make sure they are installed $ sudo apt-get remove nvidia -384 ; sudo apt-get install nvidia-384 Now, the only thing left to do is test your environment and to make sure everything is installed correctly. Just simply launch the nvidia-smi (system management interface) application. $ docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi The output will be similar to this: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.77 Driver Version: 390.77 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:1E.0 Off | 0 | | N/A 39C P0 83W / 149W | 0MiB / 11441MiB | 98% Default | +-------------------------------+----------------------+----------------------+ Reference: https://cnvrg.io/how-to-setup-docker-and-nvidia-docker-2-0-on-ubuntu-18-04/"
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "When I ran the following command: sudo apt-get install -y nvidia-docker2 I got the following error response: The following packages have unmet dependencies: nvidia-docker2 : Depends: docker-ce (= 18.06.1~ce~3-0~ubuntu) but it is not installable or docker-ee (= 18.06.1~ee~3-0~ubuntu) but it is not installable E: Unable to correct problems, you have held broken packages.",
        "answers": [
            [
                "In /etc/apt/sources.list make sure the last word is stable, not edge deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable In my case the above line was missing in the /etc/apt/sources.list . I added it at the bottom &amp; executed the following commands: Then the usual: sudo apt-get update sudo apt-get install -y docker-ce Verify your docker version with: sudo docker version"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am new to Kubernetes. Now I need to add 2 nodes with GPU. The origin environment is 5 cpu node. Should I install nvidia driver and nvidia docker in the 5 cpu node too?",
        "answers": [
            [
                "As @Shashank Pai mentioned, take a look at NVIDIA documentation Here in order to check all the advantages of using NVIDIA GPUs in Kubernetes cluster."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to figure out how the docker containers are scheduled. Does anyone know which files I should read or what factors are considered when the containers are scheduled? I am also interested in nvidia-docker and experimenting with Rodinia benchmark. Would the size of GPU memory use be one of the factors considered when scheduling? Opened to any suggestions about analyzing or improving docker!",
        "answers": [],
        "votes": []
    },
    {
        "question": "I tried to install the nvidia-docker after installing docker-ce. I followed this : https://github.com/NVIDIA/nvidia-docker to install nvidia-docker. It seems to have installed correctly. I tried to run: $ sudo docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi docker: Error response from daemon: Unknown runtime specified nvidia. See 'docker run --help'. Although, this works (without --runtime=nvidia): $ docker container run -ti ubuntu bash Some additional info on my system: It is an ubuntu server 16.04 with 8 GPUs (Titan Xp) and nvidia driver version 387.26. I can run nvidia-smi -l 1 on the host system and it works as expected. $ dpkg -l | grep -E '(nvidia|docker)' ii docker-ce 18.06.1~ce~3-0~ubuntu amd64 Docker: the open-source application container engine ii libnvidia-container-tools 1.0.0-1 amd64 NVIDIA container runtime library (command-line tools) ii libnvidia-container1:amd64 1.0.0-1 amd64 NVIDIA container runtime library ii nvidia-container-runtime 2.0.0+docker18.06.1-1 amd64 NVIDIA container runtime ii nvidia-container-runtime-hook 1.4.0-1 amd64 NVIDIA container runtime hook ii nvidia-docker2 2.0.3+docker18.06.1-1 all nvidia-docker CLI wrapper $ cat /etc/docker/daemon.json { \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } } } I have come across: https://github.com/NVIDIA/nvidia-docker/issues/501, but I am not sure how I should go about it.",
        "answers": [
            [
                "From nvidia-docker github repo: curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd"
            ],
            [
                "Actually, you can try to restart docker daemon by following command. sudo systemctl daemon-reload sudo systemctl restart docker Or you can try to reboot your system. to make nvidia-docker work"
            ],
            [
                "This is how I resolve the above problem for CentOS 7; hopefully it can help anyone who has similar problems. Add necessary repos to get nvidia-container-runtime: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo (Optional) In my case, I disabled the experimental repos: sudo yum-config-manager --disable libnvidia-container-experimental sudo yum-config-manager --disable nvidia-container-runtime-experimental Install nvidia-container-runtime package: sudo yum install nvidia-container-runtime Update docker daemon: sudo vim /etc/docker/daemon.json with the path to nvidia-container-runtime: { \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } } Finally, you need to make docker update the path: sudo pkill -SIGHUP dockerd"
            ],
            [
                "It seems you may need to purge docker and reinstall it as in the post: github issues sudo apt remove docker-ce sudo apt autoremove sudo apt-get install docker-ce=5:18.09.0~3-0~ubuntu-bionic sudo apt install nvidia-docker2"
            ],
            [
                "From nvidia-docker Frequently Asked Questions: Why do I get the error Unknown runtime specified nvidia? Make sure the runtime was registered to dockerd. You also need to reload the configuration of the Docker daemon."
            ],
            [
                "If you're having trouble installing nvidia-docker then try running this shell script. It worked for me even when nvidia-docker crashed."
            ],
            [
                "Change the --runtime=nvidia tag to --runtine=gpus all hopefully it will run"
            ]
        ],
        "votes": [
            6.0000001,
            5.0000001,
            4.0000001,
            2.0000001,
            1.0000001,
            1e-07,
            -2.9999999
        ]
    },
    {
        "question": "I am trying to execute a codebase (detect and track by facebook) inside the docker container that requires GPU access. My docker image is linux based with CUDA toolkit installed however I cannot see any GPU devices in proc filesystem. The host machine is windows with CUDA 9 toolkit and drivers installed. When I try to execute the code it says: CUDA driver version is insufficient for CUDA runtime version On searching, I realized, I can leverage nvidia-docker plugin to map the host nvidia drivers to container OS but by the looks of it, I couldn't find any support for windows. Is there any other way around to execute the codebase inside docker container on my windows OS?",
        "answers": [
            [
                "It seems that this is not possible on any Windows OS. https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#is-microsoft-windows-supported"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "As stated in the question, \"Does sagemaker use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?\" Some common error message showed as \"CannotStartContainerError. Please ensure the model container for variant variant-name-1 starts correctly when invoked with 'docker run serve\u2019.\" and it didn't show as running with nividia driver. So, do we need manually set up?",
        "answers": [
            [
                "I'm using tensorflow-gpu image as base images for my containers and I can use the gpu without specifying anything gpu related. When building docker containers for sagemaker you have to beware of folder structure and that your container is able to start with the command serve(which the error suggest). If you have problem setting this up I find this example the most useful one to get the hang of it."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "When running a GPU-enabled docker container on an EC2 p2.xlarge instance I experience a delay of between 30 and 90 seconds before the container starts running. Subsequent containers start fast (1 second delay). The EC2 is running ubuntu 18.04 with NVIDIA driver version 396.54 and nvidia-docker2 (following the official installation guide: https://github.com/NVIDIA/nvidia-docker) I am testing using the latest official CUDA image: docker run --rm nvidia/cuda nvidia-smi Persinstence mode is enabled on my machine. As stated in https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#how-do-i-install-the-nvidia-driver under \"Why is my container slow to start with 2.0?\" it should be the solution, but doesn't work for me. Any ideas what might be causing the delay and how to fix it are appreciated.",
        "answers": [
            [
                "I see in the comments that you've already pulled the Docker image from the internet, but are you sure that the image wasn't saved to an EBS snapshot? For example, during the creation of the AMI with NVIDIA Docker, you might have pulled that image and saved it to the root AMI volume. If that's the case, then you have this delay because of how EBS volumes are being restored from snapshots. From AWS documentation (Initializing Amazon EBS Volumes): ... storage blocks on volumes that were restored from snapshots must be initialized (pulled down from Amazon S3 and written to the volume) before you can access the block. This preliminary action takes time and can cause a significant increase in the latency of an I/O operation the first time each block is accessed. So when you're running your Docker container the first time, AWS is downloading the data from S3 to your EBS volume, it takes some time. The second time your container starts fast because the data is already on the volume."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Question: Can somebody explain to me why selinux rules are applied in case of running container in daemon mode but not in interactive mode ? Use case: I am running a docker container with nvidia-gpu support. When I am trying to run it interactive mode, everythong works fine: docker run -ti --runtime=nvidia --user jovyan -p 81:8888 hub-nbk-gpu:stable nvidia-smi Thu Aug 30 14:07:53 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 396.26 Driver Version: 396.26 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:1F.0 Off | 0 | | N/A 32C P0 28W / 250W | 0MiB / 16280MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ But when I want to run it in daemon mode, selinux seems to block it: docker run -d --runtime=nvidia --user jovyan -p 81:8888 hub-nbk-gpu:stable 4ad334909bb963aa29d63c0929f79a3beb0ce015685d1a5835dda4137cbff367 docker: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused \"permission denied\": unknown. Of course, if I disable selinux everything works fine: getenforce Enforcing sudo setenforce 0 getenforce Permissive docker run -d --runtime=nvidia --user jovyan -p 81:8888 hub-nbk-gpu:stable 83fd5ca737523c8757005ce80999c52081c23360b3deb9603e8d86eb357aa64a",
        "answers": [],
        "votes": []
    },
    {
        "question": "I installed Nvidia Video Codec SDK 8.2 + ffmpeg inside a docker container by using nvidia-docker but when I run this ffmpeg -f rawvideo -s:v 1920x1080 -r 30 -pix_fmt yuv420p -i HeavyHand_1080p.yuv -c:v h264_nvenc -preset slow -cq 10 -bf 2 -g 150 output.mp4 I got this error Cannot load libnvidia-encode.so.1 The minimum required Nvidia driver for nvenc is 390.25 or newer Error initializing output stream 0:0 -- Error while opening encoder for output stream #0:0 - maybe incorrect parameters such as bit_rate, rate, width or height Otherwise nvidia-smi displays this this gpu used is GeForce 1050 Ti and the cuda version is 9.0",
        "answers": [
            [
                "When running inside a Docker container, by default not all of Nvidia libraries are mounted inside the container. Read more here and here. You can see all libraries available in your Docker container by running this: ldconfig -p | grep -E 'libcuda|libnvidia' To fix this, you should modify the NVIDIA_DRIVER_CAPABILITIES environment variable in your Docker image. For example, you can append ENV NVIDIA_DRIVER_CAPABILITIES=all to your Dockerfile to mount all libraries/binaries in your docker container. By doing so, the libnvidia-encode library will also be mounted, and so FFMPEG will be able to use NVENC codec."
            ],
            [
                "If you are using nvidia-docker for running the docker image then everything should be taken from the host. You should have cuda installed on you host system first. \"sudo apt-get install cuda\" and then run the docker image. You should also use the nvidia-cuda image instead of linux (in the dockerfile)"
            ]
        ],
        "votes": [
            11.0000001,
            1.0000001
        ]
    },
    {
        "question": "The official PyTorch Docker image is based on nvidia/cuda, which is able to run on Docker CE, without any GPU. It can also run on nvidia-docker, I presume with CUDA support enabled. Is it possible to run nvidia-docker itself on an x86 CPU, without any GPU? Is there a way to build a single Docker image that takes advantage of CUDA support when it is available (e.g. when running inside nvidia-docker) and uses the CPU otherwise? What happens when you use torch.cuda from inside Docker CE? What exactly is the difference between Docker CE and why can't nvidia-docker be merged into Docker CE?",
        "answers": [
            [
                "nvidia-docker is a shortcut for docker --runtime nvidia. I do hope they merge it one day, but for now it's a 3rd party runtime. They explain what it is and what it does on their GitHub page. A modified version of runc adding a custom pre-start hook to all containers. If environment variable NVIDIA_VISIBLE_DEVICES is set in the OCI spec, the hook will configure GPU access for the container by leveraging nvidia-container-cli from project libnvidia-container. Nothing stops you from running images meant for nvidia-docker with normal docker. They work just fine but if you run something in them that requires the GPU, that will fail. I don't think you can run nvidia-docker on a machine without a GPU. It won't be able to find the CUDA files it's looking for and will error out. To create an image that can run on both docker and nvidia-docker, your program inside it needs to be able to know where it's running. I am not sure if there's an official way, but you can try one of the following: Check if nvidia-smi is available Check if the directory specified in $CUDA_LIB_PATH exists Check if your program can load the CUDA libraries successfully, and if it can't just fallback"
            ]
        ],
        "votes": [
            17.0000001
        ]
    },
    {
        "question": "I'm pulling my hair out at this point, I've spent a ton of time trying different things to get my card working to use Tensorflow. My latest attempt (which has similar problems to before) was that I tried installing the tensorflow docker https://hub.docker.com/r/tensorflow/tensorflow/ I installed the nvidia-docker and ran the SMI and it seemed to report back that my GPU exists. Then I ran this command nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu After downloading and starting up, I try running the notebooks, (first the hello tensorflow notebook). As soon as I try to \"import\" tensorflow (just using the default unmodified notebook) I get a KernelRestart. KernelRestarter: restarting kernel (1/5), keep random ports I'm not really sure what the next best step is, I don't know how to troubleshoot docker containers, and then inside that the jupyter notebook. I've had similar issues previously while attempting to run locally without a docker container. Any suggestions on what a good next step is? I spent more than I cared to on this card and am out of ideas on how to get it to work. (I believe I could import locally on my machine using tensorflow-gpu installed, however when I got to a conv2d section I would get could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED if I'm recalling, but it's been a few hectic days) Edit: Yes to cuda and cudnn and i apt get installed nvidia-390, it seemed like a good test was nvidia-smi which works. I just finished compiling tf from scratch and still failling (in this case, importing tf doesnt fail, but same not initalized error, and perhaps not the right nvidia version it mentioned, and called out nvidia-390.77 i think) Im considering a fresh 18.04 install and an earlier nvidia-3xx version install, attempting to 'downgrade' resulted in broken apt, and multiple days of trying to fix EDIT2 : I also came to the realization I installed CUDA 9.0, but the cudnn7.1 with 9.1 CUDA (you can download from nvidia that combo whatever that means). I'm attempting to revert, but I am having plenty of trouble backing out, I'm extremely close to just wiping and re-installing ubuntu and going from there. I have all the commands and think it might be easier, but I'm not sure if that will solve it. (eg cudnn-9.0-linux-x64-v7.1) EDIT3 : Came back to respond to this. I wrote up a gist of what I had to do to get my GPU working in ubuntu 16.04 for my main machine, however I didn't test it in docker, here is the gist of it. https://gist.github.com/onaclov2000/c22fe1456ffa7da6cebd67600003dffb Copy pasting here: # 1070 Ti Fresh Install 16.04 (download updates, and include 3rd party) sudo apt-get update sudo apt-get upgrade sudo apt-get install nvidia-384 # Contents sudo bash -c 'cat &gt;&gt; /etc/modprobe.d/blacklist-nouveau.conf &lt;&lt; 'EOF' blacklist nouveau options nouveau modeset=0 EOF' sudo update-initramfs -u sudo reboot # Takes about 30-40 minutes 1.5GB approx wget https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda_9.0.176_384.81_linux.run sudo sh cuda_9.0.176_384.81_linux.run No to install nvidia accelerated Graphics Driver for Linux yes to Cuda 9.0 toolkit default yes to symbolic link yes to samples default location is fine #Alternately (need to test) #sudo sh cuda_9.0.176_384.81_linux.run --silent --toolkit --samples cat &gt;&gt; ~/.bashrc &lt;&lt; 'EOF' export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} EOF cd ~/NVIDIA_CUDA-9.0_Samples/1_Utilities/deviceQuery make ./deviceQuery # Assuming make was successful cd ~/NVIDIA_CUDA-9.0_Samples/1_Utilities/bandwidthTest make ./bandwidthTest # Assuming make was successful # Look for Result = PASS sudo apt-get install nvidia-cuda-toolkit # Couldn't find on 16.04 maybe this is a 18.04 upgrade? #sudo apt-get install cuda-toolkit-9.0 cuda-command-line-tools-9-0 # At this point the driver and CUDA are installed, now it's time to install the CUDNN driver/piece. #This is the link that I have, be sure to use v7 not v7.1 as I haven't had luck in the past with that (though it might work). https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.5/prod/9.0_20171129/cudnn-9.0-linux-x64-v7 # 333 MB so will take a bit cd ~/Downloads tar -xvf cudnn-9.0-linux-x64-v7.tgz cd cuda sudo cp lib64/* /usr/local/cuda/lib64/ sudo cp include/* /usr/local/cuda/include/ sudo apt-get install git tmux cd ~/Downloads # At this point I'm going to install Anaconda wget https://repo.continuum.io/archive/Anaconda3-4.3.1-Linux-x86_64.sh -O anaconda-install.sh bash anaconda-install.sh # Follow Prompts adding path to bash source ~/.bashrc conda create --name ml source activate ml pip install tensorflow-gpu==1.5 # test the install cd ~ mkdir projects cd projects git clone https://github.com/tensorflow/models # Addional notes Run a sample from the cuda samples folder /NVIDIA_CUDA-9.0_Samples/1_Utilities/deviceQuery make ./deviceQuery Output: Plenty but ends with the following deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.0, CUDA Runtime Version = 9.0, NumDevs = 2 Result = PASS This tells you which cudnn is installed cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 Outputs: #define CUDNN_MAJOR 7 #define CUDNN_MINOR 1 #define CUDNN_PATCHLEVEL 4 -- #define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL) # This tells you what nvcc --version Outputs: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2017 NVIDIA Corporation Built on Fri_Sep__1_21:08:03_CDT_2017 Cuda compilation tools, release 9.0, V9.0.176 Finally, I updated to 18.04 but haven't chased all this down again, so I will update with a 18.04 version at the gist above as I move forward.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to setup Kubernetes with Nvidia GPU nodes/slaves. I followed the guide at https://docs.nvidia.com/datacenter/kubernetes-install-guide/index.html and I was able to get the node join the cluster. I tried the below kubeadm example pod: apiVersion: v1 kind: Pod metadata: name: gpu-pod spec: containers: - name: cuda-container image: nvidia/cuda:9.0-base command: [\"sleep\"] args: [\"100000\"] extendedResourceRequests: [\"nvidia-gpu\"] extendedResources: - name: \"nvidia-gpu\" resources: limits: nvidia.com/gpu: 1 affinity: required: - key: \"nvidia.com/gpu-memory\" operator: \"Gt\" values: [\"8000\"] The pod fails scheduling &amp; the kubectl events shows: 4s 2m 14 gpu-pod.15487ec0ea0a1882 Pod Warning FailedScheduling default-scheduler 0/2 nodes are available: 1 Insufficient nvidia.com/gpu, 1 PodToleratesNodeTaints. I'm using AWS EC2 instances. m5.large for the master node &amp; g2.8xlarge for the slave node. Describing the node also gives \"nvidia.com/gpu: 4\". Can anybody help me out if I'm missing any steps/configurations?",
        "answers": [
            [
                "According to the AWS G2 documentation, g2.8xlarge servers have the following resources: Four NVIDIA GRID GPUs, each with 1,536 CUDA cores and 4 GB of video memory and the ability to encode either four real-time HD video streams at 1080p or eight real-time HD video streams at 720P. 32 vCPUs. 60 GiB of memory. 240 GB (2 x 120) of SSD storage. Looking at the comments, 60 GB is standard RAM, and it is used for regular calculations. g2.8xlarge servers have 4 GPUs with 4 GB of GPU memory each, and this memory is used for calculations in nvidia/cuda containers. In your case, it is requested 8 GB of GPU memory per GPU, but your server has only 4 GB. Therefore, the cluster experiences a lack of resources for scheduling the POD. So, try to reduce the memory usage in the Pod settings or try to use a server with a larger amount of GPU memory."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "In Nvidia's developer page (https://devblogs.nvidia.com/nvidia-docker-gpu-server-application-deployment-made-easy/) It states that nvidia-docker provides \"driver-agnostic CUDA images\". I would just like to inquire/clarify if this is only driver version specific or does this also apply to OS? For example: Host = CentOS Docker Image/Container = Ubuntu Does using nvidia-docker provide a way to utilize the CentOS's nvidia driver in the Ubuntu Docker Container? Currently what I do is I always have 2 Docker files for supporting Ubuntu Host and CentOS Host and manually mount /dev/nvidia0 and copy the library files (or install the driver) inside the docker image. I've asked this already to the Nvidia, but still waiting for them to answer. I'll be trying it my self too to find out but I just thought to try my luck if anyone from SO already knows the answer. Thank you in advance guys.",
        "answers": [
            [
                "I've tested this and it does work. \"driver-agnostic CUDA images\" is not only limitted to different versions of the driver but also across different OS (binary) Thank you."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am currently trying to train Faster RCNN Inception V2 model (pre-trained with COCO) with GTSDB dataset. I have the FullIJCNN dataset and I divided the dataset into three part as training, validation and test. Lastly I've created 3 different csv files respectively and then created TFRecord files for train and validation. On the other hand, I have a code block that reads ground truth box coordinates with respect to each image and draws boxes around traffic signs on the image. Also it writes class labels correctly. Here is a few examples. Again, these boxes are not predicted by a network. They drawn manually by a function. Drawn Boxes 1 Drawn Boxes 2 Then I have created a label file using the README file included in the dataset folder and added 0 background line to first line of the labels.txt to make it work with my code (I think this was some stupid thing to do) because it was throwing out of index error. However there is no key for \"background\" in my .pbtxt file to make it start from 1. Lastly I've configured the faster_rcnn_inception_v2_coco.config file, changed num_classes: 90 to num_classes: 43 since dataset has 43 classes, num_examples: 5000 to num_examples: 186 since I've divided the dataset to have 186 test examples. Used num_steps: 200000 as it is. Lastly I've started the training job by running python object_detection/model_main.py \\ --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\ --model_dir=${MODEL_DIR} \\ --num_train_steps=50000 \\ --num_eval_steps=2000 \\ --alsologtostderr command and this is the traceback (sorry for code block, I don't know how to add logs specifically): import matplotlib; matplotlib.use('Agg') # pylint: disable=multiple-statements WARNING:tensorflow:Estimator's model_fn (&lt;function model_fn at 0x7fc4cd6a4938&gt;) includes params argument, but params are not passed to Estimator. WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards. WARNING:tensorflow:From /home/models/research/object_detection/core/box_predictor.py:407: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead WARNING:tensorflow:From /home/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2037: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version. Instructions for updating: Please switch to tf.train.get_or_create_global_step WARNING:tensorflow:From /home/models/research/object_detection/core/losses.py:317: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version. Instructions for updating: Future major versions of TensorFlow will allow gradients to flow into the labels input on backprop by default. See @{tf.nn.softmax_cross_entropy_with_logits_v2}. /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \" 2018-07-26 09:48:21.785041: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA 2018-07-26 09:48:21.923329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235 pciBusID: 9b2f:00:00.0 totalMemory: 11.17GiB freeMemory: 11.10GiB 2018-07-26 09:48:21.923382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0 2018-07-26 09:48:22.153991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix: 2018-07-26 09:48:22.154053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929] 0 2018-07-26 09:48:22.154075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0: N 2018-07-26 09:48:22.154333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10763 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 9b2f:00:00.0, compute capability: 3.7) 2018-07-26 09:58:31.794649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0 2018-07-26 09:58:31.794723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix: 2018-07-26 09:58:31.794747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929] 0 2018-07-26 09:58:31.794765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0: N 2018-07-26 09:58:31.794884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10763 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 9b2f:00:00.0, compute capability: 3.7) WARNING:tensorflow:Ignoring ground truth with image id 2066941970 since it was previously added WARNING:tensorflow:Ignoring detection with image id 2066941970 since it was previously added WARNING:tensorflow:Ignoring ground truth with image id 2013299735 since it was previously added WARNING:tensorflow:Ignoring detection with image id 2013299735 since it was previously added WARNING:tensorflow:Ignoring ground truth with image id 1416415107 since it was previously added It created lots of warnings like this: WARNING:tensorflow:Ignoring ground truth with image id 2013299735 since it was previously added WARNING:tensorflow:Ignoring detection with image id 2013299735 since it was previously added The reason of these messages is num_examples has been set to 2000 despite my original config file has the line num_examples: 186. I don't understand why it is creating a new config file with different parameter. However after the whole log full of those messages, it gives a report but I can't be sure what this is exactly trying to tell me. Here is the report: creating index... index created! creating index... index created! Running per image evaluation... Evaluate annotation type *bbox* DONE (t=0.07s). Accumulating evaluation results... DONE (t=0.02s). Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000 Lastly I've checked Tensorboard to make sure it is training correctly but what I see is frustrating. Here is a screenshot of Tensorboard graphs of my model (loss): Loss General Loss I feel like I am doing something wrong. I don't know if this is a specific question or not but I tried to give detail as possible as much. My questions are: What changes should I make in these steps? Why my function draws true boxes but my model can't figure out what's going on? Thanks in advance!",
        "answers": [
            [
                "The reason you are receiving the warnings is because items from your dataset are being evaluated multiple times. The values you specify for num_train_steps and num_eval_steps should correlate to your train_config batch_size and the size of your dataset. For example if your batch size is 24 and you have 24000 training records the num_train_steps should be set to 1000 and likewise the same calculation method for num_eval_steps but with the number of evaluation records. The model_main.py script does not seem to be leveraging the values you specify in your pipeline.config file if you execute the script with those values specified."
            ],
            [
                "I came across the same problem and after a while, I thought out this solution which worked for me but must not be the global solution; if you are using a dataset spread across multiple folders and you are using your own made tf_record converter it might be a problem in a collision of each frame naming across the whole dataset. Since I used full path as filename (avoided collision) I haven't seen the WARNING anymore. I hope it will help somebody. tf_example = tf.train.Example(features=tf.train.Features(feature={ 'image/height': dataset_util.int64_feature(im_height), 'image/width': dataset_util.int64_feature(im_width), 'image/filename': dataset_util.bytes_feature(filename), 'image/source_id': dataset_util.bytes_feature(filename), 'image/encoded': dataset_util.bytes_feature(encoded_image_data), 'image/format': dataset_util.bytes_feature(image_format), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmins), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymins), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), }))"
            ],
            [
                "If the warnings are still there, then again check your generate_tfrecord.py file. I had changed the tfrecord file for my convenience and there was some bug in it. I would suggest just check whether you tfrecord file is correctly showing the ground truth boxes. And then only proceed for training."
            ],
            [
                "I had exactly the sample issue and yes, after I change the num_example to match my validation cases, the error has gone."
            ]
        ],
        "votes": [
            10.0000001,
            5.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I use custom images (AMIs) configured for machine learning on GPU-enabled EC2 instances. This means cuda, libcudnn6, nvidia-docker etc are all properly setup on them. However when Kops starts new nodes from these AMIs (I use cluster-autoscaler) it overrides my properly setup docker. How can I prevent that? For now I run a custom script on startup that re-installs nvidia-docker properly, but that's obviously not ideal.",
        "answers": [
            [
                "Kops will only install docker if there's a difference between the version it expects to use and the version that is already installed on the node. Note that Kops will downgrade docker if the installed version is higher than what it expects! So the solution to my problem was to have a pre-installed version that matches spec.docker.version. For this we had to downgrade docker to 17.03.2 and nvidia-docker to 2.0.3+docker17.03.2-1."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "If intend to run a python script using the following command nvidia-docker run -v /home/$USER/:/home/$USER doc/deep_rl python script.py This throws an error No module named chester.run_exp However if I first run the docker interactively by adding -it flag nvidia-docker run -it -v /home/$USER/:/home/$USER doc/deep_rl and once inside the docker, i do python script.py everything runs perfectly.",
        "answers": [
            [
                "Try this command. nvidia-docker run -it --rm -v /home/harjatin/:/home/harjatin harjatin/deep_rl python script.py It will work."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Getting the above error while running any docker command. When i run the following command $ sudo docker info Output Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? None of the docker commands are working. All of them throws same error. $ sudo systemctl status docker Output. \u25cf docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/docker.service.d \u2514\u2500docker-override.conf Active: inactive (dead) (Result: exit-code) since Mon 2018-07-02 09:27:04 EDT; 1min 2s ago Docs: https://docs.docker.com Process: 23866 ExecStart=/usr/bin/dockerd -H fd:// -s overlay2 --disable-legacy-registry=false &lt;br&gt;(code=exited, status=1/FAILURE) Main PID: 23866 (code=exited, status=1/FAILURE) Jul 02 09:27:04 cse-bennettuniv systemd[1]: Failed to start Docker Application Container Engine. Jul 02 09:27:04 cse-bennettuniv systemd[1]: docker.service: Unit entered failed state. Jul 02 09:27:04 cse-bennettuniv systemd[1]: docker.service: Failed with result 'exit-code'. Jul 02 09:27:04 cse-bennettuniv systemd[1]: docker.service: Service hold-off time over, scheduling restart. Jul 02 09:27:04 cse-bennettuniv systemd[1]: Stopped Docker Application Container Engine. Jul 02 09:27:04 cse-bennettuniv systemd[1]: docker.service: Start request repeated too quickly. Jul 02 09:27:04 cse-bennettuniv systemd[1]: Failed to start Docker Application Container Engine. $ sudo systemctl status nvidia-docker Output \u25cf nvidia-docker.service - NVIDIA Docker plugin Loaded: loaded (/lib/systemd/system/nvidia-docker.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2018-07-02 08:54:55 EDT; 37min ago Docs: https://github.com/NVIDIA/nvidia-docker/wiki Main PID: 3173 (nvidia-docker-p) Tasks: 10 Memory: 61.8M CPU: 3.739s CGroup: /system.slice/nvidia-docker.service \u2514\u25003173 /usr/bin/nvidia-docker-plugin -s /var/lib/nvidia-docker Jul 02 08:54:55 cse-bennettuniv systemd[1]: Starting NVIDIA Docker plugin... Jul 02 08:54:55 cse-bennettuniv systemd[1]: Started NVIDIA Docker plugin. Jul 02 08:54:55 cse-bennettuniv nvidia-docker-plugin[3173]: /usr/bin/nvidia-docker-plugin | 2018/07/02 08:54:55 Loading NVIDIA unified memory Jul 02 08:54:55 cse-bennettuniv nvidia-docker-plugin[3173]: /usr/bin/nvidia-docker-plugin | 2018/07/02 08:54:55 Loading NVIDIA management library Jul 02 08:54:55 cse-bennettuniv nvidia-docker-plugin[3173]: /usr/bin/nvidia-docker-plugin | 2018/07/02 08:54:55 Discovering GPU devices Jul 02 08:54:59 cse-bennettuniv nvidia-docker-plugin[3173]: /usr/bin/nvidia-docker-plugin | 2018/07/02 08:54:59 Provisioning volumes at /var/lib/nvidia-docker/volumes Jul 02 08:54:59 cse-bennettuniv nvidia-docker-plugin[3173]: /usr/bin/nvidia-docker-plugin | 2018/07/02 08:54:59 Serving plugin API at /var/lib/nvidia-docker Jul 02 08:54:59 cse-bennettuniv nvidia-docker-plugin[3173]: /usr/bin/nvidia-docker-plugin | 2018/07/02 08:54:59 Serving remote API at localhost:3476 When I run the following comman... $ sudo service docker restart Output Job for docker.service failed because the control process exited with error code. See _\"systemctl status docker.service\"_ and _\"journalctl -xe\"_ for details. I have already run the \"systemctl status docker.service\" and I also run the \"journalctl -xe\" command to understand whats the problem. The journalctl output contains Jul 03 13:23:06 cse-bennettuniv systemd[1]: Starting Docker Application Container Engine... Jul 03 13:23:06 cse-bennettuniv dockerd[9390]: ERROR: The '--disable-legacy-registry' flag has been removed. Interacting with legacy (v1) registries is no longer supported Jul 03 13:23:06 cse-bennettuniv systemd[1]: docker.service: Main process exited, code=exited, status=1/FAILURE Jul 03 13:23:06 cse-bennettuniv systemd[1]: Failed to start Docker Application Container Engine. Jul 03 13:23:06 cse-bennettuniv systemd[1]: docker.service: Unit entered failed state. Jul 03 13:23:06 cse-bennettuniv systemd[1]: docker.service: Failed with result 'exit-code'. Jul 03 13:23:06 cse-bennettuniv systemd[1]: docker.service: Service hold-off time over, scheduling restart. Jul 03 13:23:06 cse-bennettuniv systemd[1]: Stopped Docker Application Container Engine. A more complete txt file of the outpot thrown by \"journalctl -xe\" command",
        "answers": [
            [
                "Thank you @DavidMaze and @Exa for refining the issue. I googled this particular error ERROR: The '--disable-legacy-registry' flag has been removed. Interacting with legacy (v1) registries is no longer supported . And then I removed it from the file and saved it. After that, it was working fine but there was a new error in nvidia-docker. So my Superior suggested me to reinstall docker and nvidia-docker. I followed this LINK to reinstall things. Everything got installed without any error. But when i tried to start the nvidia-docker, it stared throwing this perticular error: Failed to start nvidia-docker.service: Unit nvidia-docker.service not found. Following are the details. sudo systemctl status docker \u25cf docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/docker.service.d \u2514\u2500docker-override.conf Active: active (running) since Thu 2018-07-05 14:49:50 EDT; 23s ago Docs: https://docs.docker.com Main PID: 60189 (dockerd) Tasks: 51 Memory: 239.4M CPU: 723ms CGroup: /system.slice/docker.service \u251c\u250060189 /usr/bin/dockerd -H fd:// -s overlay2 \u2514\u250060200 docker-containerd --config /var/run/docker/containerd/containerd.toml Jul 05 14:49:50 cse-bennettuniv dockerd[60189]: time=\"2018-07-05T14:49:50.795448032-04:00\" level=warning msg=\"Your kernel does not support swap memory limit\" Jul 05 14:49:50 cse-bennettuniv dockerd[60189]: time=\"2018-07-05T14:49:50.795489588-04:00\" level=warning msg=\"Your kernel does not support cgroup rt period\" Jul 05 14:49:50 cse-bennettuniv dockerd[60189]: time=\"2018-07-05T14:49:50.795499101-04:00\" level=warning msg=\"Your kernel does not support cgroup rt runtime\" Jul 05 14:49:50 cse-bennettuniv dockerd[60189]: time=\"2018-07-05T14:49:50.796089811-04:00\" level=info msg=\"Loading containers: start.\" Jul 05 14:49:50 cse-bennettuniv dockerd[60189]: time=\"2018-07-05T14:49:50.886351523-04:00\" level=info msg=\"Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon opti Jul 05 14:49:50 cse-bennettuniv dockerd[60189]: time=\"2018-07-05T14:49:50.913124323-04:00\" level=info msg=\"Loading containers: done.\" Jul 05 14:49:50 cse-bennettuniv dockerd[60189]: time=\"2018-07-05T14:49:50.932021936-04:00\" level=info msg=\"Docker daemon\" commit=9ee9f40 graphdriver(s)=overlay2 version=18.03.1-ce Jul 05 14:49:50 cse-bennettuniv dockerd[60189]: time=\"2018-07-05T14:49:50.932079832-04:00\" level=info msg=\"Daemon has completed initialization\" Jul 05 14:49:50 cse-bennettuniv dockerd[60189]: time=\"2018-07-05T14:49:50.938512369-04:00\" level=info msg=\"API listen on /var/run/docker.sock\" Jul 05 14:49:50 cse-bennettuniv systemd[1]: Started Docker Application Container Engine. sudo systemctl status nvidia-docker \u25cf nvidia-docker.service Loaded: not-found (Reason: No such file or directory) Active: inactive (dead) Jul 05 14:40:51 cse-bennettuniv systemd[1]: Stopped NVIDIA Docker plugin. Jul 05 14:40:51 cse-bennettuniv systemd[1]: Stopped NVIDIA Docker plugin. Jul 05 14:44:07 cse-bennettuniv systemd[1]: Stopped NVIDIA Docker plugin. Jul 05 14:44:07 cse-bennettuniv systemd[1]: Stopped NVIDIA Docker plugin. Here it gives the reason that there is no such file or directory. sudo service nvidia-docker start Failed to start nvidia-docker.service: Unit nvidia-docker.service not found. This I was doing to to veryfy the issue. But it got confirmed when I ran the following code in /lib/systemd/system dgxuser@cse-bennettuniv:/lib/systemd/system$ ls | grep nvidia nvidia-persistenced.service This means there is no file named nvidia-docker.service. Now I dont understand even after install it properly, why I can't start the nvidia-docker?"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am getting the following error while trying to install nvidia-docker using the command sudo yum install nvidia-docker2 (referring here &amp; here) inside an Amazon deep learning AMI running on a p2.xlarge instance: Error: Package: nvidia-docker2-2.0.3-1.docker18.03.1.ce.amzn1.noarch (nvidia-docker) Requires: docker = 18.03.1ce Installed: docker-17.12.1ce-1.135.amzn1.x86_64 (@amzn-updates) docker = 17.12.1ce-1.135.amzn1 Available: docker-17.03.2ce-1.59.amzn1.x86_64 (amzn-main) docker = 17.03.2ce-1.59.amzn1 Available: docker-17.06.2ce-1.93.amzn1.x86_64 (amzn-updates) docker = 17.06.2ce-1.93.amzn1 Available: docker-17.06.2ce-1.94.amzn1.x86_64 (amzn-updates) docker = 17.06.2ce-1.94.amzn1 Available: docker-17.09.1ce-1.111.amzn1.x86_64 (amzn-updates) docker = 17.09.1ce-1.111.amzn1 Available: docker-17.12.0ce-1.129.amzn1.x86_64 (amzn-updates) docker = 17.12.0ce-1.129.amzn1 Has anyone faced this error before?",
        "answers": [
            [
                "We have to install nvidia-docker2 with sudo yum install -y nvidia-docker2-2.0.3-1.docker17.12.1.ce.amzn1 instead of just sudo yum install -y nvidia-docker2 Maybe currently there's some version mismatch."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I would like to know how to increase the timeout limit of nvidia-docker at initialization. When 2 or more of my 4-GPU server are busy, I always get a timeout error: nvidia-container-cli: initialization error: driver error: timed out when launching docker: docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi Thank you very much in advance for your help!",
        "answers": [
            [
                "I don't know how to change the timeout, though you can work around this problem by starting nvidia-persistenced beforehand, which will initialize the GPU devices and keep them open, so the driver doesn't have to go through that process during docker startup."
            ],
            [
                "This is not an exact answer to the question, but only a workaround to overcome the timed out error. Before launching docker, run nvidia-smi to see which processes are running on GPUs. Disable these processes using: kill -TSTP [pid] Then launch docker. When done continue the previously disabled processes using: kill -CONT [pid]"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I have a custom container (derived from nvidia/cuda:9.0-runtime) to run trainings on sagemaker. But on startup i'm getting the error CUDA driver version is insufficient for CUDA runtime version at torch/csrc/cuda/Module.cpp:32 which apparently wants to tell that my cuda version doesnt support the graphics driver (...how nice would it be to expose both version numbers along with the error message...), but i cannot figure out how to find out what display driver is mounted in the container. All i can find is that it says that sagemaker has nvidia-docker buildin. I tried to fire nvidia-smi before the error occures, but that command isnt known in the container. There is a mysterious sentence \"If you plan to use GPU devices for model inferences (by specifying GPU-based ML compute instances in your CreateEndpointConfig request), make sure that your containers are nvidia-docker compatible.\" I'm pretty sure that this is the case, but there is no checkbox or whatever to toggle \"run this container with host GPU access\". Any ideas how i can proceed?",
        "answers": [
            [
                "Doh! Found the problem, i had choosen ml.m4.xlarge as instance type, apparently those dont have a GPU / are not sharing it. Switching to ml.p2.xlarge solved the problem."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm running Jenkins on a machine with 4 GPUs and run Jenkins jobs using nvidia-docker to use the GPUs. There is a NVIDIA_VISIBLE_DEVICES property that I can pass to nvidia-docker that let's me specify which GPUs are accessible in the container. What I'd like to do is to use Jenkins for managing GPU resources and queueing jobs accordingly, i.e. make parameterised jobs that ask user how many GPUs they need for the job and it can queue and start the job when they become available. Ideally I'd like to do this over multiple slave nodes where for instance each node has 4 GPUs. Does this seem possible?",
        "answers": [
            [
                "The lock step should bring you a bit further. It provides couple of options to create and use/lock access to resources shared among couple of builds. Please note that the job will be stated and then blocked during execution (so don't place the lock inside a node allocation)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Is there a difference between: nvidia-docker run and docker run --runtime=nvidia ? In the official docs they use the latter but I've seen the former in other tutorials online.",
        "answers": [
            [
                "docker run --runtime=nvidia is only available since nvidia-docker v2. Both commands are equivalent with nvidia-docker v2, the former is a script provided for backward compatibility with nvidia-docker v1."
            ]
        ],
        "votes": [
            9.0000001
        ]
    },
    {
        "question": "i'm trying to get nvidia-docker to run on my centos7 system: $ cat /etc/systemd/system/docker.service.d/override.conf [Service] ExecStart= ExecStart=/usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --add-runtime=nvidia=/usr/bin/nvidia-container-runtime --default-runtime=docker-runc --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --seccomp-profile=/etc/docker/seccomp.json $OPTIONS $DOCKER_STORAGE_OPTIONS $DOCKER_NETWORK_OPTIONS $ADD_REGISTRY $BLOCK_REGISTRY $INSECURE_REGISTRY $REGISTRIES $ cat /etc/docker/daemon.json { } $ docker --version Docker version 1.13.1, build 774336d/1.13.1 $ sudo systemctl daemon-reload $ sudo systemctl restart docker so far so good: $ sudo docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ now, lets try with the nvidia runtime: $ sudo docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi standard_init_linux.go:178: exec user process caused \"permission denied\" but strangely... $ sudo docker run --runtime=nvidia --rm nvidia/cuda sh -c nvidia-smi Wed May 16 06:41:17 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.42 Driver Version: 390.42 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:06:00.0 Off | 0 | | N/A 28C P8 26W / 149W | 0MiB / 11441MiB | 0% Default | +-------------------------------+----------------------+----------------------+",
        "answers": [
            [
                "so... in the end i completely disabled selinux and rebooted and that fixed it."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am currently running a lot of similar Docker containers which are created and run by a Python script via the official API. Since Docker natively doesn't support GPU mapping, I tested Nvidia-Docker, which fulfills my requirements, but I'm not sure how to integrate it seamlessly in my script. I tried to find the proper API calls for Nvidia-Docker using Google and the docs, but I didn't manage to find anything useful. My current code looks something like this: # assemble a new container using the params obtained earlier container_id = client.create_container(img_id, command=commands, stdin_open=True, tty=True, volumes=[folder], host_config=client.create_host_config(binds=[mountpoint,]),detach=False) # run it client.start(container_id) The documentation for the API can be found here. From Nvidia-Dockers Github page: The default runtime used by the Docker\u00ae Engine is runc, our runtime can become the default one by configuring the docker daemon with --default-runtime=nvidia. Doing so will remove the need to add the --runtime=nvidia argument to docker run. It is also the only way to have GPU access during docker build. Basically, I want to add the --runtime=nvidia-docker argument to my create_container call, but there is no support for that as it seems. But since I need to switch between runtimes multiple times during the script execution (mixing Nvidia-Docker and native Docker containers) the quick and dirty way would be to run a bash command using subprocess but I feel like there has to be a better way. TL;DR: I am looking for a way to run Nvidia-Docker containers from a Python script.",
        "answers": [
            [
                "run() and create() methods have runtime parameter according to https://docker-py.readthedocs.io/en/stable/containers.html Which has sense because docker cli tool is pretty simple and every command translate in a call to the docker engine service REST API"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to build a docker container with cuda using the following piece in the docker build part RUN wget -O $MRCNN_DIR/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.deb &amp;&amp; \\ dpkg -i $MRCNN_DIR/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb &amp;&amp; \\ apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub &amp;&amp; \\ apt-get update &amp;&amp; \\ apt-get install -y cuda-9.0 This gives me a prompt for keyboard-config at build time. debconf: unable to initialize frontend: How can I suppress this? Or am I doing the wrong thing?",
        "answers": [
            [
                "have to set ENV in your dockerfile ENV DEBIAN_FRONTEND noninteractive"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm teaching myself what is docker and how to use it. I'm really new to docker, so hope to learn some basics here. I installed nvidia-docker (following installation guide) and tensorflow/tensorflow:nightly-gpu-py3 (nightly-gpu, start GPU (CUDA) container) on my computer. Docker: NVIDIA Docker 2.0.3, Version: 17.12.1-ce Host OS: Ubuntu 16.04 Desktop Host Arch: amd64 My Probelm Both cifar10_multi_gpu_train (written in python with tensorflow) and simple monte-carlo simulation (written in pure cuda) fail to run (fatal error: no curand.h) while fdm (written in in pure cuda) or simple matrix multiplication (written in python with tensorflow) work in the container (tensorflow/tensorflow:nightly-gpu-py3). Codes that only use CPUs (like a3c) work fine with tensorflow. Some codes that use GPUs returns an error message. (when code use &lt;curand.h&gt;) Details In the container (tensorflow/tensorflow:nightly-gpu-py3), when i run the monte-carlo simulation, i get the following error: fatal error: curand.h: No such file or directory locate curand.h returns nothing, but when I try locate curand, I get: /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so.9.0 /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so.9.0.176 /usr/share/doc/cuda-curand-9-0 /usr/share/doc/cuda-curand-9-0/changelog.Debian.gz /usr/share/doc/cuda-curand-9-0/copyright /var/lib/dpkg/info/cuda-curand-9-0.list /var/lib/dpkg/info/cuda-curand-9-0.md5sums /var/lib/dpkg/info/cuda-curand-9-0.postinst /var/lib/dpkg/info/cuda-curand-9-0.postrm /var/lib/dpkg/info/cuda-curand-9-0.shlibs and for locate cudnn.h: /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/util/use_cudnn.h for locate cuda.h: /usr/include/linux/cuda.h /usr/local/cuda-9.0/targets/x86_64-linux/include/cuda.h /usr/local/cuda-9.0/targets/x86_64-linux/include/dynlink_cuda.h /usr/local/cuda-9.0/targets/x86_64-linux/include/dynlink_cuda_cuda.h /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/platform/cuda.h /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/platform/stream_executor_no_cuda.h nvcc --version returns: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2017 NVIDIA Corporation Built on Fri_Sep__1_21:08:03_CDT_2017 Cuda compilation tools, release 9.0, V9.0.176 In the host (outside the container), when I try nvidia-docker run nvidia/cuda nvidia-smi, I get +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.30 Driver Version: 390.30 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:03:00.0 On | N/A | | 0% 48C P8 22W / 250W | 301MiB / 11177MiB | 1% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 108... Off | 00000000:81:00.0 Off | N/A | | 0% 51C P8 22W / 250W | 2MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| +-----------------------------------------------------------------------------+ What I've Done Reinstall nvidia-docker, nightly-gpu-py3 and #include &lt;curand.h&gt;--&gt; failed In nightly-gpu-py3 container, reinstall cuda/cuda toolki and #include &lt;curand.h&gt;--&gt; failed Tried to run all codes in other machine that does not use docker and cuda/tensorflow-gpu are already installed. They work fine. I guess I totally misunderstand the concept of nvidia-docker and what images/containers do. Question After I installed nvidia-docker, I can run a container using nvidia-docker run &lt;myImage&gt;. Isn't docker image means it can save dependencies (PATH, packages, ...) to run a certain code (in my case, code that uses &lt;curand.h&gt;)? (and container do actual work?) Does tensorflow/tensorflow:nightly-gpu-py3 image has CUDA Toolkit/cuDNN? Does no &lt;curand.h&gt; in nightly-gpu-py3 means I installed/donwloaded nvidia-docker/nightly-gpu-py3 improperly? Installing CUDA Toolkit or reinstalling cuda inside the container (nightly-gpu-py3) has failed (I followed process here). Is there any way that I can use &lt;curand.h&gt; inside the container (nightly-gpu-py3)? sudo nvidia-docker run -it --rm -p 8888:8888 -p 6006:6006 &lt;image&gt; /bin/bash is my command to start a new container with given image. Could it be a problem?",
        "answers": [],
        "votes": []
    },
    {
        "question": "i followed the instructions to install nvidia-docker 2 and then installed kubernetes 1.10 via kubeadm (on rhel7): i did the following: curl -s -L https://nvidia.github.io/nvidia-docker/rhel7.4/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo yum update yum install docker yum install -y nvidia-container-runtime-hook yum install --downloadonly --downloaddir=/tmp/ nvidia-docker2-2.0.3-1.docker1.13.1.noarch nvidia-container-runtime-2.0.0-1.docker1.13.1.x86_64 rpm -Uhv --replacefiles /tmp/nvidia-container-runtime-2.0.0-1.docker1.13.1.x86_64.rpm /tmp/nvidia-docker2-2.0.3-1.docker1.13.1.noarch.rpm mkdir -p /etc/systemd/system/docker.service.d/ cat &lt;&lt;EOF &gt; /etc/systemd/system/docker.service.d/override.conf [Service] ExecStart= ExecStart=/usr/bin/dockerd-current --authorization-plugin=rhel-push-plugin --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --seccomp-profile=/etc/docker/seccomp.json $OPTIONS $DOCKER_STORAGE_OPTIONS $DOCKER_NETWORK_OPTIONS $ADD_REGISTRY $BLOCK_REGISTRY $INSECURE_REGISTRY $REGISTRIES EOF cat &lt;&lt;EOF &gt; /etc/docker/daemon.json { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } } EOF systemctl restart docker docker run --rm nvidia/cuda nvidia-smi # success! i can even schedule gpu'd containers and see all gpus from within the container. however, when i deploy a container with: resources: limits: nvidia.com/gpu: 1 the pods remain as: jupyter jupyterlab-gpu 0/1 Pending 0 1m &lt;none&gt; &lt;none&gt; describe shows: Name: jupyterlab-gpu Namespace: jupyter Node: &lt;none&gt; Labels: app=jupyterhub component=singleuser-server heritage=jupyterhub hub.jupyter.org/username=me Annotations: &lt;none&gt; Status: Pending IP: Containers: notebook: Image: slaclab/slac-jupyterlab-gpu Port: 8888/TCP Host Port: 0/TCP Limits: cpu: 2 memory: 2147483648 nvidia.com/gpu: 1 Requests: cpu: 500m memory: 536870912 nvidia.com/gpu: 1 Environment: JUPYTERHUB_USER: me JUPYTERLAB_IDLE_TIMEOUT: 43200 JPY_API_TOKEN: 1fca7b3d716e4d54a98d8054d17b16fb CPU_LIMIT: 2.0 JUPYTERHUB_SERVICE_PREFIX: /user/me/ MEM_GUARANTEE: 536870912 JUPYTERHUB_API_URL: http://10.103.19.59:8081/hub/api JUPYTERHUB_OAUTH_CALLBACK_URL: /user/me/oauth_callback JUPYTERHUB_BASE_URL: / JUPYTERHUB_API_TOKEN: 1fca7b3d716e4d54a98d8054d17b16fb CPU_GUARANTEE: 0.5 JUPYTERHUB_CLIENT_ID: user-me MEM_LIMIT: 2147483648 JUPYTERHUB_HOST: Mounts: /home/ from generic-user-home (rw) /var/run/secrets/kubernetes.io/serviceaccount from no-api-access-please (ro) Conditions: Type Status PodScheduled False Volumes: generic-user-home: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: generic-user-home ReadOnly: false no-api-access-please: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: QoS Class: Burstable Node-Selectors: group=gpu Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 14s (x13 over 2m) default-scheduler 0/8 nodes are available: 1 node(s) were not ready, 6 node(s) didn't match node selector, 7 Insufficient nvidia.com/gpu. i am able to schedule containers to the node without the gpu resource limit without issues. is there a way i can validate that kubectl (?) can 'see' the gpus?",
        "answers": [
            [
                "You can view nodes detail via kubectl get nodes -oyaml, nvidia.com/gpu resources will be listed under status.allocatable and status.capacity alongside with cpu and memory"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Im trying to run docker with tensorflow using Nvidia GPUs, however when I run my container I get the following error: pgp_1 | Traceback (most recent call last): pgp_1 | File \"/opt/app-root/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in &lt;module&gt; pgp_1 | from tensorflow.python.pywrap_tensorflow_internal import * pgp_1 | File \"/opt/app-root/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in &lt;module&gt; pgp_1 | _pywrap_tensorflow_internal = swig_import_helper() pgp_1 | File \"/opt/app-root/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper pgp_1 | _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) pgp_1 | File \"/opt/app-root/lib64/python3.6/imp.py\", line 243, in load_module pgp_1 | return load_dynamic(name, filename, file) pgp_1 | File \"/opt/app-root/lib64/python3.6/imp.py\", line 343, in load_dynamic pgp_1 | return _load(spec) pgp_1 | ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory Docker-compose My docker compose file looks like: version: '3' services: pgp: devices: - /dev/nvidia0 - /dev/nvidia1 - /dev/nvidia2 - /dev/nvidia3 - /dev/nvidia4 - /dev/nvidiactl - /dev/nvidia-uvm image: \"myimg/pgp\" ports: - \"5000:5000\" environment: - LD_LIBRARY_PATH=/opt/local/cuda/lib64/ - GPU_DEVICE=4 - NVIDIA_VISIBLE_DEVICES all - NVIDIA_DRIVER_CAPABILITIES compute,utility volumes: - ./train_package:/opt/app-root/src/train_package - /usr/local/cuda/lib64/:/opt/local/cuda/lib64/ As you can see, I have tried having a volume to map host cuda to the docker container but this didnt help. I am able to successfully run nvidia-docker run --rm nvidia/cuda nvidia-smi Versions Cuda cat /usr/local/cuda/version.txt shows CUDA Version 9.0.176 nvcc -V nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2017 NVIDIA Corporation Built on Fri_Sep__1_21:08:03_CDT_2017 Cuda compilation tools, release 9.0, V9.0.176 nvidia-docker version NVIDIA Docker: 2.0.3 Client: Version: 17.12.1-ce API version: 1.35 Go version: go1.9.4 Git commit: 7390fc6 Built: Tue Feb 27 22:17:40 2018 OS/Arch: linux/amd64 Server: Engine: Version: 17.12.1-ce API version: 1.35 (minimum version 1.12) Go version: go1.9.4 Git commit: 7390fc6 Built: Tue Feb 27 22:16:13 2018 OS/Arch: linux/amd64 Experimental: false Tensorflow 1.5 with gpu support, via pip ldconfig -p | grep cuda libnvrtc.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc.so.9.0 libnvrtc.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc.so libnvrtc-builtins.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc-builtins.so.9.0 libnvrtc-builtins.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc-builtins.so libnvgraph.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvgraph.so.9.0 libnvgraph.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvgraph.so libnvblas.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvblas.so.9.0 libnvblas.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvblas.so libnvToolsExt.so.1 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvToolsExt.so.1 libnvToolsExt.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvToolsExt.so libnpps.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnpps.so.9.0 libnpps.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnpps.so libnppitc.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppitc.so.9.0 libnppitc.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppitc.so libnppisu.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppisu.so.9.0 libnppisu.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppisu.so libnppist.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppist.so.9.0 libnppist.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppist.so libnppim.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppim.so.9.0 libnppim.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppim.so libnppig.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppig.so.9.0 libnppig.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppig.so libnppif.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppif.so.9.0 libnppif.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppif.so libnppidei.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppidei.so.9.0 libnppidei.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppidei.so libnppicom.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppicom.so.9.0 libnppicom.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppicom.so libnppicc.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppicc.so.9.0 libnppicc.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppicc.so libnppial.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppial.so.9.0 libnppial.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppial.so libnppc.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppc.so.9.0 libnppc.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnppc.so libicudata.so.55 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libicudata.so.55 libcusparse.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusparse.so.9.0 libcusparse.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusparse.so libcusolver.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusolver.so.9.0 libcusolver.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusolver.so libcurand.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so.9.0 libcurand.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so libcuinj64.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcuinj64.so.9.0 libcuinj64.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcuinj64.so libcufftw.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufftw.so.9.0 libcufftw.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufftw.so libcufft.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufft.so.9.0 libcufft.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufft.so libcudart.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0 libcudart.so.7.5 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libcudart.so.7.5 libcudart.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so libcudart.so (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libcudart.so libcuda.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libcuda.so.1 libcuda.so (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libcuda.so libcublas.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcublas.so.9.0 libcublas.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcublas.so libaccinj64.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libaccinj64.so.9.0 libaccinj64.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libaccinj64.so libOpenCL.so.1 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libOpenCL.so.1 libOpenCL.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libOpenCL.so Tests with Tensorflow on Docker vs host The following works, when running on the host: python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" v1.5.0-0-g37aa430d84 1.5.0 Run container nvidia-docker run -d --name testtfgpu -p 8888:8888 -p 6006:6006 gcr.io/tensorflow/tensorflow:latest-gpu Log in nvidia-docker exec -it testtfgpu bash Test Tensorflow version pip show tensorflow-gpu shows: pip show tensorflow-gpu Name: tensorflow-gpu Version: 1.6.0 Summary: TensorFlow helps the tensors flow Home-page: https://www.tensorflow.org/ Author: Google Inc. Author-email: opensource@google.com License: Apache 2.0 Location: /usr/local/lib/python2.7/dist-packages Requires: astor, protobuf, gast, tensorboard, six, wheel, absl-py, backports.weakref, termcolor, enum34, numpy, grpcio, mock Python 2 python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" Results in: Illegal instruction (core dumped) Python 3 python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" Results in: python3 -c \"import tensorflow as tf; print(tf.GIT_ Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; ImportError: No module named 'tensorflow'",
        "answers": [
            [
                "The problem because of your cuDNN version. Tensorflow-GPU 1.5 version will support cuDNN 7.0._ version. You can download that from here. Make sure that your CUDA version 9.0._ and cuDNN version 7.0._ . Please refer link in here for more details."
            ],
            [
                "It looks like a conflict between CUDA's version and TensorFlow's First, try to check your CUDA version with one of the commands such as nvcc --version or cat /usr/local/cuda/version.txt If that's 8.x, you may need to reinstall CUDA or simpler, downgrade TensorFlow to 1.4. Otherwise, if your CUDA is 9.x, you need TensorFlow 1.5 or newer. Hope that helps."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I have a queue of models, which I allow only 2 to be executed in parallel, since I have 2 GPUs. For that, in the beginning of my code I try to determine which GPU is available by using GPUtil. Maybe its relevant, this code in run inside a docker container that was launched using the --runtime=nvidia flag. The code that determines which GPU to run on, looks like this: import os import GPUtil gpu1, gpu2 = GPUtil.getGPUs() available_gpu = gpu1 if gpu1.memoryFree &gt; gpu2.memoryFree else gpu2 os.environ['CUDA_VISIBLE_DEVICES'] = str(available_gpu.id) import tensorflow as tf Now, I launched two scripts this way (with a slight delay until the first one occupied a GPU) but both of them tried to use the same GPU! I went further to examine the problem - I manually set the os.environ['CUDA_VISIBLE_DEVICES'] = '1' and let the model run. As it was training, I checked the output of nvidia-smi and saw the following user@server:~$ docker exec awesome_gpu_container nvidia-smi Mon Mar 12 06:59:27 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 384.111 Driver Version: 384.111 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 980 Ti Off | 00000000:01:00.0 On | N/A | | 0% 50C P2 131W / 280W | 5846MiB / 6075MiB | 81% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 1080 Off | 00000000:03:00.0 Off | N/A | | 0% 39C P8 14W / 200W | 2MiB / 8114MiB | 0% Default | +-------------------------------+----------------------+----------------------+ And I notice that while I've set the visible device to be 1 it is actually running on 0 I stress again, that my mission is while queuing multiple models that each one that start running will decide for itself which GPU to use. I explored allow_soft_placement=True, but that allocated the memory on both GPUs so I stopped the process. Bottom line, how can I make sure my training scripts only use one GPU, and make them choose the free one?",
        "answers": [
            [
                "As described in the CUDA programming guide, the default device enumeration used by CUDA is \"fastest first\": CUDA_\u200bDEVICE_\u200bORDER FASTEST_FIRST, PCI_BUS_ID, (default is FASTEST_FIRST) FASTEST_FIRST causes CUDA to guess which device is fastest using a simple heuristic, and make that device 0, leaving the order of the rest of the devices unspecified. PCI_BUS_ID orders devices by PCI bus ID in ascending order. If you set CUDA_\u200bDEVICE_\u200bORDER=PCI_BUS_ID the CUDA ordering will match the device ordering shown by nvidia-smi. Since you are using docker, you can also enforce a stronger isolation with our runtime: docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=0 ... But that's at container startup time."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Im trying to repurpose a Cryptocoin mining rig for AI research. The operating system is Ubuntu 16.04 LTS. In order to utilize the graphics cards I need to install Nvidia drivers and Cuda. However, whenever I restart the box I always get the following error: The system is running in low-graphics mode My question is, how can I tell Ubuntu to not use these drivers? I need them installed, but they are only used by nvidia-docker.",
        "answers": [
            [
                "Here is a link that should help you on ask Ubuntu. https://askubuntu.com/questions/335285/how-to-change-proprietary-video-driver-using-the-command-line. Ubuntu 12.04 You can use jockey-text to disable and enable Nvidia drivers in Ubuntu. For example: \u2022 Run jockey-text --list to get the list of available options. \u2022 Run sudo jockey-text -d xorg:nvidia_304 to remove the 304 driver. \u2022 Run sudo jockey-text -e xorg:nvidia_304 to enable the same driver. \u2022 Run jockey-text --help to see the help manual."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I followed this article to setup my GPU server, install CUDA and pull the nvidia-docker image that runs a gpu-enabled tensorflow instance with a jupyter notebook installed. however, I noticed I do not have anaconda3 inside that in order to update to python 3.6 and update all the packages. I wasnt able to attach the nividia-docker image and install anaconda3, since all the commands like wget to download are missing. How would I do it or is there an alternative way?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to run ffmpeg with cuvid hw-accelerated decoding in the container based on official nvidia/cuda image. Ffmpeg is not able to find libnvcuvid.so, although there are all required cuda libs. The output of ldconfig -p | grep libnv from the container: libnvrtc.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc.so libnvrtc-builtins.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc-builtins.so libnvidia-ptxjitcompiler.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1 libnvidia-opencl.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1 libnvidia-ml.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1 libnvidia-fatbinaryloader.so.390.12 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-fatbinaryloader.so.390.12 libnvidia-compiler.so.390.12 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-compiler.so.390.12 libnvidia-cfg.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.1 libnvgraph.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvgraph.so libnvblas.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvblas.so libnvToolsExt.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvToolsExt.so Should I just copy libnvcuvid.so from the host? Wouldn't it break if underlying driver version changes?",
        "answers": [
            [
                "I found the answer here. Just need to pass env variable ENV NVIDIA_DRIVER_CAPABILITIES video,compute,utility or -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video. Now I have all required libs ldconfig -p | grep libnv: libnvrtc.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc.so.9.0 libnvrtc.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc.so libnvrtc-builtins.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc-builtins.so.9.0 libnvrtc-builtins.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc-builtins.so libnvidia-ptxjitcompiler.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1 libnvidia-opencl.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1 libnvidia-ml.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1 libnvidia-fatbinaryloader.so.390.30 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-fatbinaryloader.so.390.30 libnvidia-encode.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-encode.so.1 libnvidia-compiler.so.390.30 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-compiler.so.390.30 libnvidia-cfg.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.1 libnvgraph.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvgraph.so.9.0 libnvgraph.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvgraph.so libnvcuvid.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libnvcuvid.so.1 libnvblas.so.9.0 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvblas.so.9.0 libnvblas.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvblas.so libnvToolsExt.so.1 (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvToolsExt.so.1 libnvToolsExt.so (libc6,x86-64) =&gt; /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvToolsExt.so"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have been trying to train a 3DCNN network with a specific architecture. I wanted to create a dockerfile with all the steps necessary to have the network working. The issue is that If I run the neural network network in the host I have no problem, everything works fine. But doing almost the same on a docker container I always get the \"segmentation fault (core dumped)\" error. Both installations are not exactly the same but the variations (maybe some extra package installed) shouldn't be a problem, right? Besides I don't have any error until it starts iterating, so it seems like is a memory problem. The GPU works on the docker container and is the same GPU as the host. the python code is the same. The Docker container neural network network start training with the data but on the epoch 1 it gets the \"segmentation fault (core dumped)\". So my question is the following: Is it possible to have critical differences between the host and a docker container even if they have exactly the same packages installed? Especially with relation to tensorflow and GPU. Because the error must be from outside the code, given that the code works in a similar environment. Hope I explained myself enough to give the idea of my question, thank you.",
        "answers": [
            [
                "A docker image will resolve, at runtime, will resolve its system calls by the host kernel. See \"How can Docker run distros with different kernels?\". In your case, your Error is Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1, SSE4.2 See \"How to compile Tensorflow with SSE4.2 and AVX instructions?\" (referenced by tensorflow/tensorflow issue 8037) You could try and build an image from a Tensorflow built from source, using a docker multi-stage build."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I proceeded trouble shooting for tensorflow can't use GPU in docker. I find nvidia driver DSO files is 375.66, incompatible with my current version 375.26. So I deleted the dir /var/lib/nvidia-docker/volumes/nvidia_driver/367.66. But when I closed the container, I can't rerun it again. I reinstall the nvidia driver, cuda, nvidia-docker for many times. When I start the container, it always echoes the error: Error response from daemon: get nvidia_driver_375.66: no such volume: nvidia_driver_375.66 What should I do?",
        "answers": [
            [
                "I have solved this problem, make dir-tree under 'var/lib/',like this: 'nvidia-docker/volumes/nvidia_driver/'. uninstall nvidia-docker properly, and re-install it again, the 375.66 dir back again."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm trying to figure out how to use nvidia-docker (https://github.com/NVIDIA/nvidia-docker) using https://docs.ansible.com/ansible/latest/docker_container_module.html#docker-container. Problem My current Ansible playbook execute my container using \"docker\" command instead of \"nvidia-docker\". What I have done According to some readings, I have tried adding my devices, without success docker_container: name: testgpu image: \"{{ image }}\" devices: ['/dev/nvidiactl', '/dev/nvidia-uvm', '/dev/nvidia0', '/dev/nvidia-uvm-tools] state: started note I tried different syntax for devices (inline ..), but still getting the same problem This command does not throws any error. As expected it creates a Docker container with my image and try to start it. Looking at my container logs: terminate called after throwing an instance of 'std::runtime_error' what(): No CUDA driver found which is the exact same error I'm getting when running docker run -it &lt;image&gt; instead of nvidia-docker run -it &lt;image&gt; Any ideas how to override docker command when using docker_container with Ansible? I can confirm my CUDA drivers are installed, and all the path /dev/nvidia* are valid. Thanks",
        "answers": [
            [
                "docker_container module doesn't use docker executable, it uses Docker daemon API through docker-py Python library. Looking at nvidia-docker wrapper script, it sets --runtime=nvidia and -e NVIDIA_VISIBLE_DEVICES. To set NVIDIA_VISIBLE_DEVICES you can use env argument of docker_container. But I see no ways to set runtime via docker_container module as of current Ansible 2.4. You can try to overcome this by setting \"default-runtime\": \"nvidia\" in your daemon.json configuration file, so Docker daemon will use nvidia runtime by default."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am using nvidia-docker to train few deep learning models. Every time I attach to my running container I realised, vim settings are not adapting into container's vim. So I installed vundle, and copied host's .vimrc options to containers root/.vimrc. I don't have specific error. But :NERDTree or other commands are not running please help. Here is my .vimrc settings set shell=/bin/bash set nocompatible \" be iMproved, required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'VundleVim/Vundle.vim' \" The following are examples of different formats supported. \" Keep Plugin commands between vundle#begin/end. \" plugin on GitHub repo Plugin 'tpope/vim-fugitive' \" plugin from http://vim-scripts.org/vim/scripts.html \" Plugin 'L9' \" Git plugin not hosted on GitHub Plugin 'git://git.wincent.com/command-t.git' \" git repos on your local machine (i.e. when working on your own plugin) \" Plugin 'file:///home/gmarik/path/to/plugin' \" The sparkup vim script is in a subdirectory of this repo called vim. \" Pass the path to set the runtimepath properly. Plugin 'rstacruz/sparkup', {'rtp': 'vim/'} \" Install L9 and avoid a Naming conflict if you've already installed a \" different version somewhere else. \" Plugin 'ascenator/L9', {'name': 'newL9'} \" All of your Plugins must be added before the following line call vundle#end() \" required filetype plugin indent on \" required \" To ignore plugin indent changes, instead use: \"filetype plugin on \" \" Brief help \" :PluginList - lists configured plugins \" :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate \" :PluginSearch foo - searches for foo; append `!` to refresh local cache \" :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal \" \" see :h vundle for more details or wiki for FAQ \" Put your non-Plugin stuff after this line \" An example for a vimrc file. \" \" Maintainer: Bram Moolenaar &lt;Bram@vim.org&gt; \" Last change: 2016 Mar 25 \" \" To use it, copy it to \" for Unix and OS/2: ~/.vimrc \" for Amiga: s:.vimrc \" for MS-DOS and Win32: $VIM\\_vimrc \" for OpenVMS: sys$login:.vimrc \" When started as \"evim\", evim.vim will already have done these settings. if v:progname =~? \"evim\" finish endif \" Use Vim settings, rather than Vi settings (much better!). \" This must be first, because it changes other options as a side effect. set nocompatible \" allow backspacing over everything in insert mode set backspace=indent,eol,start if has(\"vms\") set nobackup \" do not keep a backup file, use versions instead else set backup \" keep a backup file (restore to previous version) set undofile \" keep an undo file (undo changes after closing) endif set history=50 \" keep 50 lines of command line history set ruler \" show the cursor position all the time set showcmd \" display incomplete commands set incsearch \" do incremental searching \" For Win32 GUI: remove 't' flag from 'guioptions': no tearoff menu entries \" let &amp;guioptions = substitute(&amp;guioptions, \"t\", \"\", \"g\") \" Don't use Ex mode, use Q for formatting map Q gq \" CTRL-U in insert mode deletes a lot. Use CTRL-G u to first break undo, \" so that you can undo CTRL-U after inserting a line break. inoremap &lt;C-U&gt; &lt;C-G&gt;u&lt;C-U&gt; \" In many terminal emulators the mouse works just fine, thus enable it. if has('mouse') set mouse=a endif \" Switch syntax highlighting on, when the terminal has colors \" Also switch on highlighting the last used search pattern. if &amp;t_Co &gt; 2 || has(\"gui_running\") syntax on set hlsearch endif \" Only do this part when compiled with support for autocommands. if has(\"autocmd\") \" Enable file type detection. \" Use the default filetype settings, so that mail gets 'tw' set to 72, \" 'cindent' is on in C files, etc. \" Also load indent files, to automatically do language-dependent indenting. filetype plugin indent on \" Put these in an autocmd group, so that we can delete them easily. augroup vimrcEx au! \" For all text files set 'textwidth' to 78 characters. autocmd FileType text setlocal textwidth=78 \" When editing a file, always jump to the last known cursor position. \" Don't do it when the position is invalid or when inside an event handler \" (happens when dropping a file on gvim). autocmd BufReadPost * \\ if line(\"'\\\"\") &gt;= 1 &amp;&amp; line(\"'\\\"\") &lt;= line(\"$\") | \\ exe \"normal! g`\\\"\" | \\ endif augroup END else set autoindent \" always set autoindenting on endif \" has(\"autocmd\") \" Convenient command to see the difference between the current buffer and the \" file it was loaded from, thus the changes you made. \" Only define it when not defined already. if !exists(\":DiffOrig\") command DiffOrig vert new | set bt=nofile | r ++edit # | 0d_ | diffthis \\ | wincmd p | diffthis endif if has('langmap') &amp;&amp; exists('+langnoremap') \" Prevent that the langmap option applies to characters that result from a \" mapping. If unset (default), this may break plugins (but it's backward \" compatible). set langnoremap endif \" Add optional packages. \" \" The matchit plugin makes the % command work better, but it is not backwards \" compatible. packadd matchit \" execute pathogen#infect() filetype plugin indent on set number Plugin 'scrooloose/nerdcommenter' Result of :PluginInstall \"Done!\" screenshot inside container Additional detail: Installed NERDTree inside docker using git clone cd .vim/bundle/ git clone https://github.com/scrooloose/nerdtree.git",
        "answers": [
            [
                "used apt-vim to install nerdtree. Worked well for me apt-vim Installed apt-vim manually"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm part of a team developing a machine learning application. currently we're using Vagrant with a Docker provider as a uniform dev environment. We want to utilize the GPUs on our computers when we play around during development, and I found that Nvidia released nvidia-docker to enable that for a simple docker container. How can I use nvidia-docker as a provider for Vagrant? If it's not possible, is there any equivalent solution? It is important for us to develop on top of the same docker image that we deploy since we depend on multiple interacting opensource libraries, and we want to manage them in one place (no dependencies breaking when deploying)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using nvidia/cuda:8.0-devel image and tried to run it. But I get the following error. sudaraka@RnDCompute:~$ docker run -it --runtime=nvidia nvidia/cuda:8.0-devel docker: Error response from daemon: oci runtime error: container_linux.go:265: starting container process caused \"process_linux.go:368: container init caused \\\"process_linux.go:351: running prestart hook 1 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --compute --utility --require=cuda&gt;=8.0 --pid=12053 /var/lib/docker/overlay2/a72cba41e94578ff91c71ab56b07d8e9153386e43383482ac649419ae0a77220/merged]\\\\\\\\nnvidia-container-cli: initialization error: cuda error: no cuda-capable device is detected\\\\\\\\n\\\\\\\"\\\"\". It says that no cuda-capable device is detected. I have cuda 8.0 installed. sudaraka@RnDCompute:~$ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Mon_Jan_23_12:24:11_CST_2017 Cuda compilation tools, release 8.0, V8.0.62 As well as the driver. sudaraka@RnDCompute:~$ cat /proc/driver/nvidia/version NVRM version: NVIDIA UNIX x86_64 Kernel Module 384.98 Thu Oct 26 15:16:01 PDT 2017 GCC version: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) My docker version is 17.09.0-ce. What can be the problem for the error I am getting as no cuda-capable device is detected? Thank You",
        "answers": [
            [
                "It is possible that the installation was not done successfully, could you show the result of nvidia-smi? , you should try to install cuda and nvidia-driver in a separate mode, like this: cuda 9.2 and nvidia-driver 396 CUDA wget https://developer.nvidia.com/compute/cuda/9.2/Prod2/local_installers/cuda_9.2.148_396.37_linux sudo chmod 777 cuda_9.2.148_396.37_linux sudo sh cuda_9.2.148_396.37_linux Several questions here, do not in the following: Install NVIDIA Accelerated Graphics Driver (N) Do you want to install the OpenGL libraries? (N) Do you want to run nvidia-xconfig? (N) NVIDIA sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update sudo apt install nvidia-396 After installation cuda it will be in /usr/local/cuda, and test nvidia driver with nvidia-smi Install, docker and nvidia-docker too, and test test: docker run -it --runtime=nvidia nvidia/cuda:8.0-devel or nvidia-docker run -it --runtime=nvidia nvidia/cuda:8.0-devel I hope it's useful, let me know!"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am able to run a tensorflow container w/ access to the GPU from the command line w/ the following command $ sudo docker run --runtime=nvidia --rm gcr.io/tensorflow/tensorflow:latest-gpu I would like to be able to run this container from docker-compose. Is it possible to specify the --runtime flag from docker-compose.yml?",
        "answers": [
            [
                "Currently (Aug 2018), NVIDIA container runtime for Docker (nvidia-docker2) supports Docker Compose. Yes, use Compose format 2.3 and add runtime: nvidia to your GPU service. Docker Compose must be version 1.19.0 or higher. Example docker-compose.yml: version: '2.3' services: nvsmi: image: ubuntu:16.04 runtime: nvidia environment: - NVIDIA_VISIBLE_DEVICES=all command: nvidia-smi More example from NVIDIA blog uses Docker Compose to show how to launch multiple GPU containers with the NVIDIA Container Runtime."
            ],
            [
                "You should edit /etc/docker/daemon.json, adding the first level key \"default-runtime\": \"nvidia\", restart docker daemon (ex. \"sudo service docker restart\") and then all containers on that host will run with nvidia runtime. More info on daemon.json here"
            ],
            [
                "Or better: using systemd and assuming the path is /usr/libexec/oci/hooks.d/nvidia Configure mkdir -p /etc/systemd/system/docker.service.d/ cat &gt; /etc/systemd/system/docker.service.d/nvidia-containers.conf &lt;&lt;EOF [Service] ExecStart= ExecStart=/usr/bin/dockerd -D --add-runtime nvidia=/usr/libexec/oci/hooks.d/nvidia --default-runtime=nvidia EOF Restart systemctl daemon-reload systemctl restart docker Demo Don't need to specify --runtime=nvidia since we set default-runtime=nvidia in the configuration step. docker run --rm gcr.io/tensorflow/tensorflow:latest-gpu Solution Inspired from my tutorial about KATA runtime."
            ]
        ],
        "votes": [
            44.0000001,
            33.0000001,
            5.0000001
        ]
    },
    {
        "question": "I train a DNN via NVidia docker image nvcr.io/nvidia/torch. Everything works fine except that it is far slower (+41%) than the training time when executed on my machine. One batch execution takes around 410ms instead of 290ms on bare metal. My nvidia-docker run command: nvidia-docker run -it --network=host --ipc=host -v /mnt/data1:/mnt/data1 my-custom-image bash my-custom-image is based on nvcr.io/nvidia/torch. I only add my training scripts (.lua) and install luajit. All results are written in /mnt/data1 and not inside the container itself. Is it normal or am I doing something wrong? How can I investigate where wasted time comes from? Update: I double checked and nothing is written inside the container during the training. All data are written on /mnt/data1. Update2: I tried the inference routine inside the container, and it doesn't take more time than bare metal setup.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have went through 3 different issues in the nvidia-docker repo about this exact problem but actually couldn't figure out what's wrong. I'm a heavy docker user but I don't understand much of the terminology and solution used in those issues. When I run nvidia-smi as sudo or not, everything works great and I get the standard output. My nvidia-docker-plugin is up and running, and I get these messages when I run nvidia-docker run --rm nvidia/cuda nvidia-smi: nvidia-docker-plugin | 2017/11/04 09:14:18 Received mount request for volume 'nvidia_driver_387.22' Blockquote nvidia-docker-plugin | 2017/11/04 09:14:18 Received unmount request for volume 'nvidia_driver_387.22' I also tried to run the deepo repository, can't get it to work as all my containers exit upon starting, and the nvidia-docker run --rm nvidia/cuda nvidia-smi outputs the error: container_linux.go:247: starting container process caused \"exec: \\\"nvidia-smi\\\": executable file not found in $PATH\" /usr/bin/docker-current: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused \"exec: \\\"nvidia-smi\\\": executable file not found in $PATH\". What am I doing wrong? I run Fedora 26, if it makes any difference",
        "answers": [
            [
                "On Ubuntu, you should install nvidia-modprobe package. I understand that also exists in Fedora. For some reason, this dependency isn't required either documented."
            ],
            [
                "I've just solved this. Removing the volume related to nvidia-docker-plugin solved the issue. For future readers, just read out the log messages on your nvidia-docker-plugin, look for the mount/unmount logged lines, and use the following command to remove the volume docker volume rm -f &lt;volume_to_remove&gt; where volume_to_remove should be something like nvidia_driver_387.22 (which matched my case) Seems like the issue is that the mapping to the nvidia-smi call is made upon the volume creation and removing and reattaching the volume fixes this"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "How do use nvidia-docker to create service in swarm mode of docker operation. I am trying to train a tensorflow model in this swarm network to undergo distributed learning. I found that one way could be to run a swarm network of different containers in different machines and use GPU on each machine to undergo distributed training. If its not possible in swarm mode, Is there any possible way to accomplish the above task? docker service create --name tensorflow --network overnet saikishor/tfm:test azt0tczwkxaqpkh9yaea4laq1 Since --detach=false was not specified, tasks will be created in the background. In a future release, --detach=false will become the default but under docker service ls, I have this ID NAME MODE REPLICAS IMAGE PORTS uf6jgp3tm6dp tensorflow replicated 0/1 saikishor/tfm:test",
        "answers": [
            [
                "It is impossible when the question is asked, but not now. Since nvidia-docker2 released, a new docker container runtime, usually named as nvidia, is supported. This enables docker run --runtime nvidia ... to access GPU like nvidia-docker run .... Besides, after the dockerd option --default-runtime nvidia is configured, tools like docker-compose, Docker Swarm and Kubernetes can use GPU too. Install Debian-based distributions curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \\ sudo apt-key add - distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\ sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update RHEL-based distributions distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \\ sudo tee /etc/yum.repos.d/nvidia-docker.repo Config { \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } }, \"default-runtime\": \"nvidia\", ... }"
            ],
            [
                "As of now, nvidia-docker is not supporting docker swarm. So, there is no possibility now. We need to create an external network to plug them together."
            ]
        ],
        "votes": [
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm trying to get the nvidia hardware acceleration running inside of a Docker container. So far I hat no success. When running glxgears I get the following error. root@fea7a51ac757:/# glxgears libGL error: No matching fbConfigs or visuals found libGL error: failed to load driver: swrast X Error of failed request: BadValue (integer parameter out of range for operation) Major opcode of failed request: 154 (GLX) Minor opcode of failed request: 3 (X_GLXCreateContext) Value in failed request: 0x0 Serial number of failed request: 35 Current serial number in output stream: 37 My docker file looks like this FROM osrf/ros:lunar-desktop-full # nvidia-docker hooks LABEL com.nvidia.volumes.needed=\"nvidia_driver\" ENV PATH /usr/local/nvidia/bin:${PATH} ENV LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64:${LD_LIBRARY_PATH} EXPOSE 11311 EXPOSE 11345 And I started the container with nvidia-docker run -it --volume=/tmp/.X11-unix:/tmp/.X11-unix --device=/dev/dri:/dev/dri --env=\"DISPLAY\" my-custom-image I currently I don't know that the nvidia-driver inside of the container needs to have the same version. but I don't know how to check this of if this is even the problem. nvidia-smi says +-----------------------------------------------------------------------------+ | NVIDIA-SMI 384.90 Driver Version: 384.90 | |-------------------------------+----------------------+----------------------+ I used this as an guide to solve the problem without any success",
        "answers": [
            [
                "Which NVIDIA docker version are you using? I ask because up until a month ago, there was no OpenGL support in the newer 2.0 version. Last month they made a dockerfile that has OpenGL on it as a base image. https://hub.docker.com/r/nvidia/opengl/ GLX gears should work out of the ubuntu repository (mesa utils) in the container assuming you have the NVIDIA drivers installed on your host and you you pass the x11 display arguments/bindmounts when your run the image. This is what I had in my dockerfile. https://github.com/coreyryanhanson/dockerfiles/blob/master/glxgears/ubuntu16%20opengl/Dockerfile And the x11 arguments that you can add to the docker run command when starting the container are: -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY=unix$DISPLAY"
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "When trying to install driverless-ai on AWS EC2, I execute all the steps until the step 6 in the guide: \"6. Start the Driverless AI docker image:\" I try the command described there: nvidia-docker run --rm -u \u2018id -u\u2018:\u2018id -g\u2018 -p 12345:12345 -p 9090:9090 -v \u2018pwd\u2018/data:/data -v \u2018pwd\u2018/log:/log -v \u2018pwd\u2018/license:/license opsh2oai/h2oai-runtime and get back: unknown shorthand flag: 'g' in -g\u2018",
        "answers": [
            [
                "You really to need to pass the current linux user uid/pid with the -p parameter of \"nvidia-docker run\" command. This is how u can do it. If you run id command on a Ubuntu machine you will see the following udi/gid for the logged user name \"ubuntu\": $ id uid=1000(ubuntu) gid=1000(ubuntu) You will be using this info with the -p parameter with \"nvidia-docker run\" command as below: nvidia-docker run -u 1000:1000 What you can do it run the following command to get help on nvidia-docker: nvidia-docker run --help"
            ],
            [
                "It looks like you are using the wrong quote. Your example has an apostrophe (') and it should be a backquote (`). Backquote (correct): $ echo `id -g` 20 Apostrophe (incorrect): $ echo 'id -g' id -g Also note that the instructions now refer to an AMI-based startup (so you shouldn't have to type this stuff in yourself anymore)."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I want to overlap the use of the GPU by many docker containers. Nvidia provides an utility to do this: the Multi Process Service which is documented here. Specifically it says: When CUDA is first initialized in a program, the CUDA driver attempts to connect to the MPS control daemon. If the connection attempt fails, the program continues to run as it normally would without MPS. If however, the connection attempt succeeds, the MPS control daemon proceeds to ensure that an MPS server, launched with same user id as that of the connecting client, is active before returning to the client. The MPS client then proceeds to connect to the server. All communication between the MPS client, the MPS control daemon, and the MPS server is done using named pipes. By default, the named pipes are placed in /tmp/nvidia-mps/, so I share that directory with the containers using a volume. But this is not enough for the cuda driver on the container to \"see\" the MPS server. Which resources should I share between the host and the container so it can connect to the MPS server?",
        "answers": [
            [
                "To start a container with access to mps, it must have a bind mount to /tmp/nvidia-mps and the same interprocess-communication group as the host. For example: docker run -v /tmp/nvidia-mps:/tmp/nvidia-mps --ipc=host nvidia/cuda"
            ],
            [
                "I do not believe that mapping /tmp/nvidia-mps into the container is required. As long as the IPC namespace is the same, it should work. If you are running the MPS control daemon on the host then you would need to use the docker run flag --ipc=host, as previously mentioned, because MPS will be using /dev/shm (this is where the IPC namespace maps to) on the host. Using the --ipc=host flag will tell docker to map the host's /dev/shm into the container, rather than creating a private /dev/shm inside the container. E.g. docker run --ipc=host nvidia/cuda Note it is also possible to host MPS inside a container and share that container's IPC namespace (/dev/shm) between containers."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "So I'm trying set run OpenAI gym in a docker container, but it looks like this: Notice the pong window has a weird render issue where it's repeating things and the colors are off. Here is space invaders: NOTE FOR \"NOT A PROGRAMMING ISSUE\" PEOPLE: The solution involves the correct bash script code to call the right API methods to render the arrays of pixels correctly. Also only a graphics programmer is likely to \"recognize the render glitch\". My setup is very simple. - I'm on a local ubuntu 16.04 install with an Nvidia gtx1060 and corei7 - I installed nvida runfile driver with --no-opengl-files (as per instructions from Nvidia and many place). - Specifically, I'm running floydhub/pytorch docker image. Does anyone recognize the particular render glitch and what it could mean? It almost looks like a StackOverflow of a frame buffer! What can I do to track down the bug? EDIT: I have eliminated all the extra dependencies I had been installing and am just doing simple x-forwarding according to the ROS GUI guide. You can easily reproduce this as follows: docker run -it --user=$(id -u) --env=\"DISPLAY\" --workdir=\"/home/$USER\" --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" floydhub/pytorch:0.1.11-gpu-py3.6 bash Now in the image, type python and then the following: import gym gym.make('Pong-v0').render() That should open up an x-forwarded window on your machine, but the display is corrupt (at least for me) Above I actually used SpaceInvaders-v0",
        "answers": [],
        "votes": []
    },
    {
        "question": "(Posting here before submitting an issue to tensorflow as their issue template suggests) I'm trying to build a tensorflow docker image with python 3.6, I have the following Dockerfile FROM nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 RUN apt-get update \\ &amp;&amp; apt-get install -y --no-install-recommends \\ build-essential \\ curl \\ libfreetype6-dev \\ libpng12-dev \\ libzmq3-dev \\ pkg-config \\ rsync \\ software-properties-common \\ unzip \\ libcupti-dev \\ &amp;&amp; add-apt-repository -y ppa:jonathonf/python-3.6 \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y python3.6 python3.6-dev \\ &amp;&amp; apt-get clean \\ &amp;&amp; rm -rf /var/lib/apt/lists/* RUN curl -O https://bootstrap.pypa.io/get-pip.py \\ &amp;&amp; python3.6 get-pip.py \\ &amp;&amp; rm get-pip.py RUN python3.6 -m pip install --no-cache-dir -U ipython pip setuptools RUN python3.6 -m pip install --no-cache-dir tensorflow RUN ln -s /usr/bin/python3.6 /usr/bin/python ENV LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH ENV CUDA_HOME /usr/local/cuda-8.0 CMD [\"ipython\"] I build the image and run a script which forces gpu:0: nvidia-docker build -t tensorflow . ... (builds successfully) nvidia-docker run --rm -v $PWD/test.py:/test.py tensorflow python /test.py ... InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'b': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device. [[Node: b = Const[dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [3,2] values: [1 2][3]...&gt;, _device=\"/device:GPU:0\"]()]] I've tried the same script with the offical gpu image tensorflow/tensorflow:latest-gpu and it works fine. So nvidia-docker and the GPU itself definitely works with tensorflow. With the image i built nvidia cuda and cudnn appear to be installed correctly: nvidia-docker run --rm tensorflow bash -c \"nvidia-smi; nvcc --version; cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2\" Sun Jul 23 22:50:11 2017 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375.66 Driver Version: 375.66 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 750 Off | 0000:01:00.0 On | N/A | | 21% 35C P8 1W / 38W | 795MiB / 976MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| +-----------------------------------------------------------------------------+ nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Tue_Jan_10_13:22:03_CST_2017 Cuda compilation tools, release 8.0, V8.0.61 #define CUDNN_MAJOR 5 #define CUDNN_MINOR 1 #define CUDNN_PATCHLEVEL 10 -- #define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL) #include \"driver_types.h\" What am I doing wrong? (test.py is just): import tensorflow as tf with tf.device('/gpu:0'): a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a') b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b') c = tf.matmul(a, b) sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) print(sess.run(c)) (I've tried with a base image nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04 which is used by tensorflow/tensorflow:latest-gpu but to no avail)",
        "answers": [
            [
                "Turns out it was as simple as installing tensorflow-gpu not tensorflow, odd that the tensorflow docs don't explain that but basically this was me being dumb."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "nvidia-docker can't talk to http://localhost:3476/docker/cli/json Traceback (most recent call last): File \"/usr/local/bin/nvidia-docker-compose\", line 43, in &lt;module&gt; resp = request.urlopen('http://{0}/docker/cli/json'.format(args.nvidia_docker_host)).read().decode() File \"/usr/lib/python2.7/urllib2.py\", line 154, in urlopen return opener.open(url, data, timeout) File \"/usr/lib/python2.7/urllib2.py\", line 429, in open response = self._open(req, data) File \"/usr/lib/python2.7/urllib2.py\", line 447, in _open '_open', req) File \"/usr/lib/python2.7/urllib2.py\", line 407, in _call_chain result = func(*args) File \"/usr/lib/python2.7/urllib2.py\", line 1228, in http_open return self.do_open(httplib.HTTPConnection, req) File \"/usr/lib/python2.7/urllib2.py\", line 1198, in do_open raise URLError(err) urllib2.URLError: &lt;urlopen error [Errno 111] Connection refused&gt;",
        "answers": [
            [
                "A fresh install of nvidia-docker-compose fixed this: wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb sudo dpkg -i /tmp/nvidia-docker*.deb &amp;&amp; rm /tmp/nvidia-docker*.deb Then to test it: Test nvidia-smi nvidia-docker run --rm nvidia/cuda nvidia-smi"
            ],
            [
                "Encountered this also, a customer wasn't managing to run nvidia-docker-compose. Turns out even after reinstallations of docker and nvidia-docker, the query made by nvidia-docker to docker on localhost:3476 was not getting any response (see nvidia-docker-compose code here) I managed to solve this by generating a hand-made docker-compose file as they turn out to be quite simple, follow this example, replace 375.66 with your nvidia driver version and put as many /dev/nvidia[n] lines as you have graphic cards (did not try to put services on separate GPUs but go for it !): services: exampleservice0: devices: - /dev/nvidia0 - /dev/nvidia1 - /dev/nvidiactl - /dev/nvidia-uvm - /dev/nvidia-uvm-tools environment: - EXAMPLE_ENV_VARIABLE=example image: company/image volumes: - ./disk:/disk - nvidia_driver_375.66:/usr/local/nvidia:ro version: '2' volumes: media: null nvidia_driver_375.66: external: true Then just run this hand-made docker-compose file with a classic docker-compose command."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I have a Docker image with Caffe compiled with cudnn support. CUDA and all other dependencies are installed correctly in the image and it works correctly when using nvidia-docker to provide drivers from the host machine. I would like to run this same image without using nvidia-docker and just set Caffe to CPU mode. However, when I do this, I still see errors that the correct Nvidia drivers can't be found. It's as if building Caffe with cudnn support causes Caffe to require the GPU drivers. This is problematic for my use case: providing one single Docker image that can be used interchangeably as either a CPU-based image or a GPU-based image. How can I install Caffe with cudnn / GPU support enabled, but still run it with no runtime dependency on any CUDA / GPU dependencies? Note: this is not a question about disabling GPU support with a CPU_ONLY flag during Caffe build time. Rather, it's about taking Caffe built to be able to use the GPU, but then run it in a manner that doesn't require any drivers, CUDA libraries, etc., in a CPU-only runtime mode. The type of error I see at runtime is below: I0523 21:09:42.175459 14 layer_factory.hpp:77] Creating layer conv1 I0523 21:09:42.175561 14 net.cpp:84] Creating Layer conv1 I0523 21:09:42.175606 14 net.cpp:406] conv1 &lt;- data I0523 21:09:42.175660 14 net.cpp:380] conv1 -&gt; conv1 F0523 21:09:42.177079 14 cudnn_conv_layer.cpp:52] Check failed: error == cudaSuccess (35 vs. 0) CUDA driver version is insufficient for CUDA runtime version *** Check failure stack trace: *** Aborted (core dumped) Here I am just loading a prototxt model after calling caffe.set_mode_cpu() using Caffe's Python binding. If I compile in CPU_ONLY mode, everything works, or if I actually run with nvidia-docker on a machine hosting the right drivers. But I'm specifically looking for a single Docker image that would be portable between host machines that both do and do not have GPUs or the necessary driver dependencies.",
        "answers": [
            [
                "After more digging, this reference suggests that it is the LayerSetup function for particular convolution layers causing the problem. In particular, in cudnn_conv_layer.cpp, the LayerSetup implementation makes calls to CUDA_CHECK and CUDNN_CHECK for e.g. CUDA stream handling. It looks like when Caffe is compiled with cudnn support, these checks will fail if you attempt to execute in CPU mode, even after set_mode_cpu(). I am not sure whether a change to the CUDNN_CHECK macro would be sufficient, or if it requires writing a totally new Caffe layer that checks the runtime mode before determining which resource checks to perform. Either case is not workable for my current situation, so I'll split out CPU and GPU functionality into two separate Dockerfiles."
            ],
            [
                "I agree that you should be able to make that choice by simply setting solver_mode in solver.prototxt or some similar setting. However, that's not how BVLC designed Caffe; there are various other links in the code that assume it can avail itself of the GPUs implied by the CUDA code's presence. The solution would be to dig through the code, find the references to the CUDA presence flags, and alter the code to dynamically use the \"CPU_ONLY\" branches when your CPU flag was set."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I wrote a Dockerfile that builds my QT application and I'm having some problems on the build. If the build command is on the Dockerfile, it trhows this error: ninja: error: '/usr/lib/x86_64-linux-gnu/libnvcuvid.so', needed by 'bin/x64/release/*****/librtmpPlugin.so', missing and no known rule to make it i added a symbolic link that solve this error on a temporary container: ln -s /usr/local/nvidia/lib64/libnvcuvid.so.1 /usr/lib/x86_64-linux-gnu/libnvcuvid.so But when i added that line and build-ed again, i still got the same error. First i thought it was because some cache from dangling images but cleaning everything problem persist. This is some of my ENV keys: ENV LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64 ENV LIBRARY_PATH /usr/local/cuda/lib64/stubs:${LIBRARY_PATH} ENV PATH /usr/local/nvidia/bin:/usr/local/cuda/bin:${PATH} ENV OPENCL_HEADERS /usr/local/cuda/include ENV LIBOPENCL /usr/local/cuda/lib64 ENV CUDA_TOOLKIT_ROOT_DIR /usr/local/cuda ENV CUDA_NVCC_EXECUTABLE /usr/bin/nvcc This are some of the 'hacks' that i did to remove other errors: RUN mv /usr/lib/x86_64-linux-gnu/libOpenCL.so.1 /usr/lib/x86_64-inux-gnu/libOpenCL.so.1_old RUN ln -s /usr/local/cuda/lib64/libOpenCL.so.1 /usr/lib/x86_64-linux-gnu/libOpenCL.so.1 RUN ln -s /usr/local/cuda-8.0/targets/x86_64-linux/lib/stubs/libcuda.so /usr/lib/x86_64-linux-gnu/libcuda.so RUN ln -s /usr/local/cuda-8.0/targets/x86_64-linux/lib/stubs/libnvidia-ml.so /usr/lib/x86_64-linux-gnu/libnvidia-ml.so I'm using nvidia cuda 8 image + nvidia docker on the last version, Docker version 17.03.1-ce. I thought that there wasn't any difference between a Dockerfile compile process and the container runtime.",
        "answers": [
            [
                "The image nvidia/cuda doesn't come with the nvidia drivers the way my software build scheme requires, so i had to install the drive equivalent of what nividia-smi output showed. apt-get install -y nvidia-381 I know i might have some problems on the future, but this is the way yo go now. i also had to rename the libcuda.so because nvidia-381 package have a lot of dependencies and removed the libcuda.so from the CUDA packages and i started to get a lot of warnings. RUN mv /usr/lib/x86_64-linux-gnu/libcuda.so /usr/lib/x86_64-linux-gnu/libcuda.so_old RUN ln -s /usr/local/cuda-8.0/targets/x86_64-linux/lib/stubs/libcuda.so /usr/lib/x86_64-linux-gnu/libcuda.so"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Is there any way to use nvidia-docker with Nomad? The program for computing on Nvidia works locally but it doesn't work with nvidia-docker (it uses CPU instead of GPU). What is the preferred way to do that? Use nvidia-docker driver for Nomad Use raw docker exec to run nvidia-docker Somehow connect Nomad to nvidia-docker engine Has anyone experience with that?",
        "answers": [
            [
                "This is something I've spent a lot of time implementing, at this time (nomad 0.7.0 though running 5.6 myself) there is no 'nomadic' way to implement a nvidia-docker job without using raw fork/exec which doesn't provide container orchestration, service discovery, log shipping, resource management (i.e. bin packing). I was surprised that the nvidia-docker command doesn't actually act on behalf of docker, alternatively it forwards commands to docker. The only time it is really ever useful is when calling the run/exec commands (ie nvidia-docker run --yar blar) as it calls a helper program that returns a json response with the appropriate device and volume mounts in json format. When the container data is sent to the actual docker socket it includes the correct device and volume for the version of cuda that is installed on the host (inspect your container ). The other part of implementing this solution using an exec driver is to create a task that acts on behalf of the deployment if you wish to have rolling deploys. I am using a simple script to orchestrate a rolling deploy inside the same task group as the nvidia-docker task. So long as you are using stagger, max parallel (set to 1) in your task group and ensuring you have a dynamic argument like random or date in the orchestration task (nomad will not update the task if there are 0 differences) you should be set. Once nomad has the ability to compute gpu (need custom fingerprint here: https://github.com/hashicorp/nomad/tree/master/client/fingerprint ) resource type and has the ability to mount non block type devices (ie something not a disk) it should be possible to circumvent using nvidia-docker. I hope this helps, be sure to bump the feature request here: https://github.com/hashicorp/nomad/issues/2938 To also expand on running this operation using conventional docker, you must also mount the volume created by nvidia-docker. docker volume ls will show named volumes, you must mount the cuda volume for your container to have access to the drivers (unless you are already stuffing into your container, not recommended)."
            ],
            [
                "This has native support as of Nomad 0.9: https://www.hashicorp.com/blog/using-hashicorp-nomad-to-schedule-gpu-workloads"
            ],
            [
                "The idea was to create a proper Docker image for that: FROM debian:wheezy # Run Ubuntu in non-interactive mode ENV DEBIAN_FRONTEND noninteractive # Provide CUDA environmental variables that match the installed version on host machine ENV CUDA_DRIVER 375.39 ENV CUDA_INSTALL http://us.download.nvidia.com/XFree86/Linux-x86_64/${CUDA_DRIVER}/NVIDIA-Linux-x86_64-${CUDA_DRIVER}.run # Configure dependencies RUN \\ # Update available packages apt-get update \\ --quiet \\ # Install all requirements &amp;&amp; apt-get install \\ --yes \\ --no-install-recommends \\ --no-install-suggests \\ build-essential \\ module-init-tools \\ wget \\ # Clean up leftovers &amp;&amp; apt-get clean \\ &amp;&amp; rm -rf /var/lib/apt/lists/* # Install CUDA drivers RUN wget \\ $CUDA_INSTALL \\ -P /tmp \\ --no-verbose \\ &amp;&amp; chmod +x /tmp/NVIDIA-Linux-x86_64-${CUDA_DRIVER}.run \\ &amp;&amp; /tmp/NVIDIA-Linux-x86_64-${CUDA_DRIVER}.run \\ -s \\ -N \\ --no-kernel-module \\ &amp;&amp; rm -rf /tmp/* ENTRYPOINT [\"/bin/bash\"] and then: build the base Docker image: docker build . -t cuda start a container with cuda base image execute: docker run \\ --device=/dev/nvidia0:/dev/nvidia0 \\ --device=/dev/nvidiactl:/dev/nvidiactl \\ --device=/dev/nvidia-uvm:/dev/nvidia-uvm \\ -it \\ --rm cuda A message like: Failed to initialize NVML: Unknown Error Could be due to a mismatch between the host and container driver versions of missing host /dev entries."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07,
            -1.9999999
        ]
    },
    {
        "question": "I would like to run 2 docker images with docker-compose. one image should run with nvidia-docker and the other with docker. I've seen this post use nvidia-docker-compose launch a container, but exited soon but this is not working for me(not even running only one image)... any idea would be great.",
        "answers": [
            [
                "UPDATE : please check nvidia-docker 2 and its support of docker-compose first https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#do-you-support-docker-compose (I'd first suggest adding the nvidia-docker tag). If you look at the nvidia-docker-compose code here it only generates a specific docker-file for docker-compose after a query of the nvidia configuration on localhost:3476. You can also make by hand this docker-compose file as they turn out to be quite simple, follow this example, replace 375.66 with your nvidia driver version and put as many /dev/nvidia[n] lines as you have graphic cards (did not try to put services on separate GPUs but go for it !): services: exampleservice0: devices: - /dev/nvidia0 - /dev/nvidia1 - /dev/nvidiactl - /dev/nvidia-uvm - /dev/nvidia-uvm-tools environment: - EXAMPLE_ENV_VARIABLE=example image: company/image volumes: - ./disk:/disk - nvidia_driver_375.66:/usr/local/nvidia:ro version: '2' volumes: media: null nvidia_driver_375.66: external: true Then just run this hand-made docker-compose file with a classic docker-compose command. Maybe you can then compose with non nvidia dockers by skipping the nvidia specific stuff in the other services."
            ],
            [
                "Additionally to the accepted answer, here's my approach, a bit shorter. I needed to use the old version of docker-compose (2.3) because of the required runtime: nvidia (won't necessarily work with version: 3 - see this). Setting NVIDIA_VISIBLE_DEVICES=all will make all the GPUs visible. version: '2.3' services: your-service-name: runtime: nvidia environment: - NVIDIA_VISIBLE_DEVICES=all # ...your stuff My example is available here. Tested on NVIDIA Docker 2.5.0, Docker CE 19.03.13 and NVIDIA-SMI 418.152.00 and CUDA 10.1 on Debian 10."
            ]
        ],
        "votes": [
            11.0000001,
            3.0000001
        ]
    },
    {
        "question": "I am setting up an internal Jupyterhub on a multi GPU server. Jupyter access is provided through a docker instance. I'd like to limit access for each user to no more than a single GPU. I'd appreciate any suggestion or comment. Thanks.",
        "answers": [
            [
                "You can try it with nvidia-docker-compose version: \"2\" services process1: image: nvidia/cuda devices: - /dev/nvidia0"
            ],
            [
                "The problem can be solved in this way, just add the environment variable \u201cNV_GPU\u201d before \u201cnvidia-docker\u201d as follow: [root@bogon ~]# NV_GPU='4,5' nvidia-docker run -dit --name tf_07 tensorflow/tensorflow:latest-gpu /bin/bash e04645c2d7ea658089435d64e72603f69859a3e7b6af64af005fb852473d6b56 [root@bogon ~]# docker attach tf_07 root@e04645c2d7ea:/notebooks# root@e04645c2d7ea:/notebooks# ll /dev total 4 drwxr-xr-x 5 root root 460 Dec 29 03:52 ./ drwxr-xr-x 22 root root 4096 Dec 29 03:52 ../ crw--w---- 1 root tty 136, 0 Dec 29 03:53 console lrwxrwxrwx 1 root root 11 Dec 29 03:52 core -&gt; /proc/kcore lrwxrwxrwx 1 root root 13 Dec 29 03:52 fd -&gt; /proc/self/fd/ crw-rw-rw- 1 root root 1, 7 Dec 29 03:52 full drwxrwxrwt 2 root root 40 Dec 29 03:52 mqueue/ crw-rw-rw- 1 root root 1, 3 Dec 29 03:52 null crw-rw-rw- 1 root root 245, 0 Dec 29 03:52 nvidia-uvm crw-rw-rw- 1 root root 245, 1 Dec 29 03:52 nvidia-uvm-tools crw-rw-rw- 1 root root 195, 4 Dec 29 03:52 nvidia4 crw-rw-rw- 1 root root 195, 5 Dec 29 03:52 nvidia5 crw-rw-rw- 1 root root 195, 255 Dec 29 03:52 nvidiactl lrwxrwxrwx 1 root root 8 Dec 29 03:52 ptmx -&gt; pts/ptmx drwxr-xr-x 2 root root 0 Dec 29 03:52 pts/ crw-rw-rw- 1 root root 1, 8 Dec 29 03:52 random drwxrwxrwt 2 root root 40 Dec 29 03:52 shm/ lrwxrwxrwx 1 root root 15 Dec 29 03:52 stderr -&gt; /proc/self/fd/2 lrwxrwxrwx 1 root root 15 Dec 29 03:52 stdin -&gt; /proc/self/fd/0 lrwxrwxrwx 1 root root 15 Dec 29 03:52 stdout -&gt; /proc/self/fd/1 crw-rw-rw- 1 root root 5, 0 Dec 29 03:52 tty crw-rw-rw- 1 root root 1, 9 Dec 29 03:52 urandom crw-rw-rw- 1 root root 1, 5 Dec 29 03:52 zero root@e04645c2d7ea:/notebooks# or\uff0cread nvidia-docker of github's wiki"
            ],
            [
                "There are 3 options. Docker with NVIDIA RUNTIME (version 2.0.x) According to official documentation docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=2,3 nvidia-docker (version 1.0.x) based on a popular post nvidia-docker run .... -e CUDA_VISIBLE_DEVICES=0,1,2 (it works with tensorflow) programmatically import os os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\""
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001,
            1.0000001
        ]
    }
]