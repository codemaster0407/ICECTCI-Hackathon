[
    {
        "question": "Investigating possible solutions for this problem, I thought about using CUDA graphs' host execution nodes (cudaGraphAddHostNode). I was hoping to have the option to block and unblock streams on the host side instead of the device side with the wait kernel, while still using graphs. I made a test program with two graphs. One graph does a host-to-device copy, calls a host function (with a host execution node) that waits in a loop for the \"event\" variable to stop being equal to 0 (i.e. waits for the \"event\"), then does a device-to-host copy. Another graph does a memset on the device memory, then calls a host function that sets the \"event\" variable to 1 (i.e. signals the \"event\"). I launch the first graph on one stream, the second on another, then synchronize on the first stream. The result was that both graphs were launched as expected, the \"wait\" host function was executed, and the first stream was blocked successfully. However, even though the second graph was launched, the \"signal\" host function was never executed. I realized that CUDA's implementation is likely serializing all host execution nodes in the context, so the \"signal\" node is forever waiting for the \"wait\" node to finish before executing. The documentation is even saying that \"host functions without a mandated order (such as in independent streams) execute in undefined order and may be serialized\". I also tried launching the graphs from separate host threads, but that didn't work. Is there some kind of way to make host execution nodes on different streams concurrent that I'm missing?",
        "answers": [
            [
                "No, this isn't a reliable method. It is evident that additional thread(s) are spun up by the CUDA runtime to handle host callbacks, but there is no detail or specification. In order for such a thing to work, you would need the two synchronizing agents to each have their own thread, running concurrently. That way, if the waiting agent spun up first, the signaling agent would still be able to execute and deliver a signal. But for cudaLaunchHostFunc (and we can surmise the same thing with graph host nodes) it is explicitly stated: Host functions without a mandated order (such as in independent streams) execute in undefined order and may be serialized. (emphasis added) Serialization of host functions would make such a scheme not workable. Is there some kind of way to make host execution nodes on different streams concurrent that I'm missing? There aren't any additional controls or specification for this, that I am aware of."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I want to check for an error flag living in managed memory that might have been written by a kernel running on a certain stream. Depending on the error flag I need to throw an exception. I would simply sync this stream and check the flag from the host, but I need to do so from inside a CUDA graph. AFAIK I need to somehow encode this host-side error checking inside a cudaLaunchHostFunc callback. I am trying to understand how the cudaLaunchHostFunc function deals with exceptions. The documentation does not mention anything about it. Is there any way to catch of an exception thrown from inside the function provided to cudaLaunchHostFunc? Consider the following MWE: #include&lt;iostream&gt; #include &lt;stdexcept&gt; __global__ void kern(){ int id = blockIdx.x*blockDim.x + threadIdx.x; printf(\"Kernel\\n\"); return; } void foo(void* data){ std::cerr&lt;&lt;\"Callback\"&lt;&lt;std::endl; throw std::runtime_error(\"Error in callback\"); } void launch(){ cudaStream_t st = 0; kern&lt;&lt;&lt;1,1,0,st&gt;&gt;&gt;(); cudaHostFn_t fn = foo; cudaLaunchHostFunc(st, fn, nullptr); cudaDeviceSynchronize(); } int main(){ try{ launch(); } catch(...){ std::cerr&lt;&lt;\"Catched exception\"&lt;&lt;std::endl; } return 0; } The output of this code is: Kernel Callback terminate called after throwing an instance of 'std::runtime_error' what(): Error in callback Aborted (core dumped) The exception is thrown but it appears that it is not propagated to the launch function. I would have expected the above launch() function to be equivalent (exception-wise) to the following: void launch(){ cudaStream_t st = 0; kern&lt;&lt;&lt;1,1,0,st&gt;&gt;&gt;(); cudaStreamSynchronize(st); foo(nullptr); // cudaHostFn_t fn = foo; // cudaLaunchHostFunc(st, fn, nullptr); cudaDeviceSynchronize(); } which does outputs the expected: Kernel Callback Catched exception Additionally, in the first case, all cuda calls return cudaSuccess.",
        "answers": [
            [
                "Thanks to the comments I understand now that my question is essentially the same as, for instance, this one: How can I propagate exceptions between threads? The techniques used to take exceptions from a worker thread to the main thread also apply here. For completion, the foo and launch functions in my dummy example could be rewritten as follows void foo(void* data){ auto e = static_cast&lt;std::exception_ptr*&gt;(data); std::cerr&lt;&lt;\"Callback\"&lt;&lt;std::endl; try{ throw std::runtime_error(\"Error in callback\"); } catch(...){ *e = std::current_exception(); } } void launch(){ cudaStream_t st = 0; dataD = 0; kern&lt;&lt;&lt;1,1,0,st&gt;&gt;&gt;(); cudaStreamSynchronize(st); cudaHostFn_t fn = foo; std::exception_ptr e; cudaLaunchHostFunc(st, fn, (void*)&amp;e); cudaDeviceSynchronize(); if(e) std::rethrow_exception(e); } Which prints the expected: Kernel Callback Catched exception"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "The CUDA graph API exposes a function call for adding a \"batch memory operations\" node to a graph: CUresult cuGraphAddBatchMemOpNode ( CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_BATCH_MEM_OP_NODE_PARAMS* nodeParams ); but the documentation for this API call does not explain what the flags field of ... is used for, and what one should set the flags to. So what value should I be passing?",
        "answers": [
            [
                "A related API function is cuStreamBatchMemOp CUresult cuStreamBatchMemOp ( CUstream stream, unsigned int count, CUstreamBatchMemOpParams* paramArray, unsigned int flags ); it essentially takes the fields of CUDA_BATCH_MEM_OP_NODE_PARAMS as its separate parameters. Its documentation says that flags is \"reserved for future expansion; must be 0\"."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "cuDeviceGetGraphMemAttribute() takes a void pointer to a result variable. But - what type does it expect the pointed-to value to be? The documentation (for CUDA v12.0) doesn't say. I'm guessing it's an unsigned 64-bit type, but I want to make sure.",
        "answers": [
            [
                "For all current attributes you can get with this function, the void * must point to a cuuint64_t. Thanks goes to @AbatorAbeter for pointing out where this is stated."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Consider the CUDA graphs API function cuFindNodeInClone(). The documentation says, that it: Returns: CUDA_SUCCESS, CUDA_ERROR_INVALID_VALUE This seems problematic to me. How can I tell whether the search failed (e.g. because there is no copy of the passed node in the graph), or whether the node or graph are simply invalid (e.g. nullptr)? Does the second error value signify both? Can I get a third error value which is just not mentioned?",
        "answers": [
            [
                "When using the runtime API, the returned node is nullptr if the original node does not exist in the cloned graph. For nullptr original node or nullptr cloned graph, the output node is left unmodified. #include &lt;iostream&gt; #include &lt;cassert&gt; int main(){ cudaError_t status; cudaGraph_t graph; status = cudaGraphCreate(&amp;graph, 0); assert(status == cudaSuccess); cudaGraphNode_t originalNode; status = cudaGraphAddEmptyNode(&amp;originalNode, graph, nullptr, 0); assert(status == cudaSuccess); cudaGraph_t graphclone; status = cudaGraphClone(&amp;graphclone, graph); assert(status == cudaSuccess); cudaGraphNode_t anotherNode; status = cudaGraphAddEmptyNode(&amp;anotherNode, graph, nullptr, 0); assert(status == cudaSuccess); cudaGraphNode_t nodeInClone = (cudaGraphNode_t)7; status = cudaGraphNodeFindInClone(&amp;nodeInClone, originalNode, graphclone); std::cout &lt;&lt; cudaGetErrorString(status) &lt;&lt; \" \" &lt;&lt; (void*)nodeInClone &lt;&lt; \"\\n\"; nodeInClone = (cudaGraphNode_t)7; status = cudaGraphNodeFindInClone(&amp;nodeInClone, nullptr, graphclone); std::cout &lt;&lt; cudaGetErrorString(status) &lt;&lt; \" \" &lt;&lt; (void*)nodeInClone &lt;&lt; \"\\n\"; nodeInClone = (cudaGraphNode_t)7; status = cudaGraphNodeFindInClone(&amp;nodeInClone, originalNode, nullptr); std::cout &lt;&lt; cudaGetErrorString(status) &lt;&lt; \" \" &lt;&lt; (void*)nodeInClone &lt;&lt; \"\\n\"; nodeInClone = (cudaGraphNode_t)7; status = cudaGraphNodeFindInClone(&amp;nodeInClone, anotherNode, graphclone); std::cout &lt;&lt; cudaGetErrorString(status) &lt;&lt; \" \" &lt;&lt; (void*)nodeInClone &lt;&lt; \"\\n\"; } On my machine with CUDA 11.8, this prints no error 0x555e3cf287c0 invalid argument 0x7 invalid argument 0x7 invalid argument 0"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm using the following the code to learn about how to use \"CUDA graphs\". The parameter NSTEP is set as 1000, and the parameter NKERNEL is set as 20. The kernel function shortKernel has three parameters, it will perform a simple calculation. #include &lt;cuda_runtime.h&gt; #include &lt;iostream&gt; #define N 131072 // tuned such that kernel takes a few microseconds #define NSTEP 1000 #define NKERNEL 20 #define BLOCKS 256 #define THREADS 512 #define CHECK(call) \\ do { \\ const cudaError_t error_code = call; \\ if (error_code != cudaSuccess) { \\ printf(\"CUDA Error\\n\"); \\ printf(\" File: %s\\n\", __FILE__); \\ printf(\" Line: %d\\n\", __LINE__); \\ printf(\" Error code: %d\\n\", error_code); \\ printf(\" Error text: %s\\n\", cudaGetErrorString(error_code)); \\ exit(1); \\ } \\ } while (0) __global__ void shortKernel(float * out_d, float * in_d, int i){ int idx=blockIdx.x*blockDim.x+threadIdx.x; if(idx&lt;N) out_d[idx]=1.23*in_d[idx] + i; } void test2() { cudaStream_t stream; cudaStreamCreate(&amp;stream); cudaSetDevice(0); float x_host[N], y_host[N]; // initialize x and y arrays on the host for (int i = 0; i &lt; N; i++) { x_host[i] = 2.0f; y_host[i] = 2.0f; } float *x, *y, *z; CHECK(cudaMalloc((void**)&amp;x, N*sizeof(float))); CHECK(cudaMalloc((void**)&amp;y, N*sizeof(float))); CHECK(cudaMalloc((void**)&amp;z, N*sizeof(float))); cudaMemcpy(x, x_host, sizeof(float) * N, cudaMemcpyHostToDevice); cudaEvent_t begin, end; CHECK(cudaEventCreate(&amp;begin)); CHECK(cudaEventCreate(&amp;end)); // start recording cudaEventRecord(begin, stream); bool graphCreated=false; cudaGraph_t graph; cudaGraphExec_t instance; // Run graphs for(int istep=0; istep&lt;NSTEP; istep++){ if(!graphCreated){ cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal); for(int ikrnl=0; ikrnl&lt;NKERNEL; ikrnl++){ shortKernel&lt;&lt;&lt;BLOCKS, THREADS, 0, stream&gt;&gt;&gt;(y, x, ikrnl); } cudaStreamEndCapture(stream, &amp;graph); cudaGraphNode_t* nodes = NULL; size_t num_nodes = 0; CHECK(cudaGraphGetNodes(graph, nodes, &amp;num_nodes)); std::cout &lt;&lt; \"Num of nodes in the graph: \" &lt;&lt; num_nodes &lt;&lt; std::endl; CHECK(cudaGraphInstantiate(&amp;instance, graph, NULL, NULL, 0)); graphCreated=true; } CHECK(cudaGraphLaunch(instance, stream)); cudaStreamSynchronize(stream); } // End run graphs cudaEventRecord(end, stream); cudaEventSynchronize(end); float time_ms = 0; cudaEventElapsedTime(&amp;time_ms, begin, end); std::cout &lt;&lt; \"CUDA Graph - CUDA Kernel overall time: \" &lt;&lt; time_ms &lt;&lt; \" ms\" &lt;&lt; std::endl; cudaMemcpy(y_host, y, sizeof(float) * N, cudaMemcpyDeviceToHost); for(int i = 0; i &lt; N; i++) { std::cout &lt;&lt; \"res \" &lt;&lt; y_host[i] &lt;&lt; std::endl; } // Free memory cudaFree(x); cudaFree(y); } int main() { test2(); std::cout &lt;&lt; \"end\" &lt;&lt; std::endl; return 0; } My expected results are shown as the following: res 2.46 res 3.46 res 4.46 res 5.46 res 6.46 ... However, the actual results are shown like this: res 21.46 res 21.46 res 21.46 res 21.46 res 21.46 res 21.46 ... It seems that the all kernels' parameter i is set as NKERNEL-1. I am very confused about it, could someone give any explanations? Thanks! I had changed the for loop as follows: // Run graphs for(int istep=0; istep&lt;NSTEP; istep++){ if(!graphCreated){ cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal); for(int ikrnl=0; ikrnl&lt;NKERNEL; ikrnl++){ if(ikrnl == 0) shortKernel&lt;&lt;&lt;BLOCKS, THREADS, 0, stream&gt;&gt;&gt;(y, x, 0); else if(ikrnl == 1) shortKernel&lt;&lt;&lt;BLOCKS, THREADS, 0, stream&gt;&gt;&gt;(y, x, 1); else if(ikrnl == 2) shortKernel&lt;&lt;&lt;BLOCKS, THREADS, 0, stream&gt;&gt;&gt;(y, x, 2); else shortKernel&lt;&lt;&lt;BLOCKS, THREADS, 0, stream&gt;&gt;&gt;(y, x, ikrnl); } cudaStreamEndCapture(stream, &amp;graph); cudaGraphNode_t* nodes = NULL; size_t num_nodes = 0; CHECK(cudaGraphGetNodes(graph, nodes, &amp;num_nodes)); std::cout &lt;&lt; \"Num of nodes in the graph: \" &lt;&lt; num_nodes &lt;&lt; std::endl; CHECK(cudaGraphInstantiate(&amp;instance, graph, NULL, NULL, 0)); graphCreated=true; } CHECK(cudaGraphLaunch(instance, stream)); cudaStreamSynchronize(stream); } // End run graphs However, the results are still the same: res 21.46 res 21.46 res 21.46 res 21.46 res 21.46 res 21.46 ...",
        "answers": [
            [
                "The results are expected and correct. Every time you run the graph, this entire for-loop gets executed: for(int ikrnl=0; ikrnl&lt;NKERNEL; ikrnl++){ shortKernel&lt;&lt;&lt;BLOCKS, THREADS, 0, stream&gt;&gt;&gt;(y, x, ikrnl); } After the first iteration of that for-loop, the results will all be 2.46, after the second iteration the results will all be 3.46, and after the 20th iteration (ikrnl = 19) the results will all be 21.46. Every time you run the graph, you will get that same result. Expecting any kind of variation in the result such as this: res 2.46 res 3.46 res 4.46 res 5.46 res 6.46 Is completely illogical, because every thread is doing precisely the same thing. Every thread starts with the same value in x, and does the same calculation on it. There is no reason to expect any difference between y[0] and y[1], for example. Rather than trying to wade through CUDA graphs, its clear you don't have a good grasp of what the kernel is doing. My suggestion would be that you write an ordinary CUDA code that calls that kernel just once, without any CUDA graph usage, and study the output. After that, you can put a for-loop around the kernel, and watch the result behavior after every iteration of the for-loop. You don't need CUDA graphs to understand what is going on here."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am testing out cuda graphs. My graph is as follows. the code for this is as follows #include &lt;cstdio&gt; #include &lt;cstdlib&gt; #include &lt;fstream&gt; #include &lt;iostream&gt; #include &lt;vector&gt; #define NumThreads 20 #define NumBlocks 1 template &lt;typename PtrType&gt; __global__ void kernel1(PtrType *buffer, unsigned int numElems) { int tid = threadIdx.x + blockIdx.x * blockDim.x; buffer[tid] = (PtrType)tid; } template &lt;typename PtrType&gt; __global__ void kernel2(PtrType *buffer, unsigned int numElems) { int tid = threadIdx.x + blockIdx.x * blockDim.x; if(tid &lt; numElems/2) buffer[tid] += 5; } template &lt;typename PtrType&gt; __global__ void kernel3(PtrType *buffer, unsigned int numElems) { int tid = threadIdx.x + blockIdx.x * blockDim.x; if(tid&gt;=numElems/2) buffer[tid] *= 5; } template &lt;typename PtrType&gt; void print(void *data) { PtrType *buffer = (PtrType *)data; std::cout &lt;&lt; \"[\"; for (unsigned int i = 0; i &lt; NumThreads; ++i) { std::cout &lt;&lt; buffer[i] &lt;&lt; \",\"; } std::cout &lt;&lt; \"]\\n\"; } void runCudaGraph(cudaGraph_t &amp;Graph, cudaGraphExec_t &amp;graphExec, cudaStream_t &amp;graphStream) { cudaGraphInstantiate(&amp;graphExec, Graph, nullptr, nullptr, 0); cudaStreamCreateWithFlags(&amp;graphStream, cudaStreamNonBlocking); cudaGraphLaunch(graphExec, graphStream); cudaStreamSynchronize(graphStream); } void destroyCudaGraph(cudaGraph_t &amp;Graph, cudaGraphExec_t &amp;graphExec, cudaStream_t &amp;graphStream) { cudaCtxResetPersistingL2Cache(); cudaGraphExecDestroy(graphExec); cudaGraphDestroy(Graph); cudaStreamDestroy(graphStream); cudaDeviceReset(); } template &lt;typename PtrType&gt; void createCudaGraph(cudaGraph_t &amp;Graph, cudaGraphExec_t &amp;graphExec, cudaStream_t &amp;graphStream, PtrType *buffer, unsigned int numElems, PtrType *hostBuffer) { cudaGraphCreate(&amp;Graph, 0); cudaGraphNode_t Kernel1; cudaKernelNodeParams nodeParams = {0}; memset(&amp;nodeParams, 0, sizeof(nodeParams)); nodeParams.func = (void *)kernel1&lt;PtrType&gt;; nodeParams.gridDim = dim3(NumBlocks, 1, 1); nodeParams.blockDim = dim3(NumThreads/NumBlocks, 1, 1); nodeParams.sharedMemBytes = 0; void *inputs[2]; inputs[0] = (void *)&amp;buffer; inputs[1] = (void *)&amp;numElems; nodeParams.kernelParams = inputs; nodeParams.extra = nullptr; cudaGraphAddKernelNode(&amp;Kernel1, Graph, nullptr, 0, &amp;nodeParams); cudaGraphNode_t Kernel2; memset(&amp;nodeParams, 0, sizeof(nodeParams)); nodeParams.func = (void *)kernel2&lt;PtrType&gt;; nodeParams.gridDim = dim3(NumBlocks, 1, 1); nodeParams.blockDim = dim3(NumThreads/NumBlocks, 1, 1); nodeParams.sharedMemBytes = 0; inputs[0] = (void *)&amp;buffer; inputs[1] = (void *)&amp;numElems; nodeParams.kernelParams = inputs; nodeParams.extra = NULL; cudaGraphAddKernelNode(&amp;Kernel2, Graph, &amp;Kernel1, 1, &amp;nodeParams); cudaGraphNode_t Kernel3; memset(&amp;nodeParams, 0, sizeof(nodeParams)); nodeParams.func = (void *)kernel3&lt;PtrType&gt;; nodeParams.gridDim = dim3(NumBlocks, 1, 1); nodeParams.blockDim = dim3(NumThreads/NumBlocks, 1, 1); nodeParams.sharedMemBytes = 0; inputs[0] = (void *)&amp;buffer; inputs[1] = (void *)&amp;numElems; nodeParams.kernelParams = inputs; nodeParams.extra = NULL; cudaGraphAddKernelNode(&amp;Kernel3, Graph, &amp;Kernel1, 1, &amp;nodeParams); cudaGraphNode_t copyBuffer; std::vector&lt;cudaGraphNode_t&gt; dependencies = {Kernel2, Kernel3}; cudaGraphAddMemcpyNode1D(&amp;copyBuffer, Graph,dependencies.data(),dependencies.size(),hostBuffer, buffer, numElems*sizeof(PtrType), cudaMemcpyDeviceToHost); cudaGraphNode_t Host1; cudaHostNodeParams hostNodeParams; memset(&amp;hostNodeParams, 0, sizeof(hostNodeParams)); hostNodeParams.fn = print&lt;PtrType&gt;; hostNodeParams.userData = (void *)&amp;hostBuffer; cudaGraphAddHostNode(&amp;Host1, Graph, &amp;copyBuffer, 1, &amp;hostNodeParams); } int main() { cudaGraph_t graph; cudaGraphExec_t graphExec; cudaStream_t graphStream; unsigned int numElems = NumThreads; unsigned int bufferSizeBytes = numElems * sizeof(unsigned int); unsigned int hostBuffer[numElems]; memset(hostBuffer, 0, bufferSizeBytes); unsigned int *deviceBuffer; cudaMalloc(&amp;deviceBuffer, bufferSizeBytes); createCudaGraph(graph, graphExec, graphStream, deviceBuffer,numElems, hostBuffer); runCudaGraph(graph, graphExec, graphStream); destroyCudaGraph(graph, graphExec, graphStream); std::cout &lt;&lt; \"graph example done!\" &lt;&lt; std::endl; } When I run this example I get a result of [3593293488,22096,3561843129,22096,3561385808,22096,3593293488,22096,3598681264,22096,3561792984,22096,2687342880,0,0,0,3598597376,22096,3598599312,0,] However I expect: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95] I can't figure out where I went wrong. I used cuda-gdb and it seems right on the GPU. However, somewhere in the memCpy and sending to host function it goes wrong. Any ideas?",
        "answers": [
            [
                "You are not passing the correct pointer to the host callback. void createCudaGraph(cudaGraph_t &amp;Graph, cudaGraphExec_t &amp;graphExec, cudaStream_t &amp;graphStream, PtrType *buffer, unsigned int numElems, PtrType *hostBuffer) { ... hostNodeParams.userData = (void *)&amp;hostBuffer; } This takes the address of a stack variable which is no longer valid when the host function is called. Since hostBuffer already points to the array you want to print, you can just pass this pointer directly. hostNodeParams.userData = (void *)hostBuffer; That aside, I would like to mention that there is a handy function cudaGraphDebugDotPrint which can output a graph to file that can be converted to png to help with debugging. With your original code, it will show that the pointer used as memcpy destination and the pointer passed to the host function are different."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Closed. This question needs debugging details. It is not currently accepting answers. Edit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question. Closed last year. Improve this question I am trying to implement a cuda graph experiment. There are three kernels, kernel_0, kernel_1, and kernel_2. They will be executed sequentially and have dependencies. Right now I am going to only capture kernel_1. These are my code: #include &lt;stdio.h&gt; #include &lt;chrono&gt; #include &lt;cuda.h&gt; #include &lt;cuda_runtime.h&gt; #include &lt;iostream&gt; #define N 50000 #define NSTEP 1000 #define NKERNEL 20 using namespace std::chrono; static const char *_cudaGetErrorEnum(cudaError_t error) { return cudaGetErrorName(error); } template &lt;typename T&gt; void check(T result, char const *const func, const char *const file, int const line) { if (result) { fprintf(stderr, \"CUDA error at %s:%d code=%d(%s) \\\"%s\\\" \\n\", file, line, static_cast&lt;unsigned int&gt;(result), _cudaGetErrorEnum(result), func); exit(EXIT_FAILURE); } } #define checkCudaErrors(val) check((val), #val, __FILE__, __LINE__) __global__ void shortKernel_0(float * out_d, float * in_d){ int idx=blockIdx.x*blockDim.x+threadIdx.x; if(idx&lt;N) { in_d[idx] = 1.0; out_d[idx]=1 + in_d[idx]; } } __global__ void shortKernel_1(float * out_d, float * in_d){ int idx=blockIdx.x*blockDim.x+threadIdx.x; if(idx&lt;N) out_d[idx]=2*in_d[idx]; } __global__ void shortKernel_2(float * out_d, float * in_d){ int idx=blockIdx.x*blockDim.x+threadIdx.x; if(idx&lt;N) { out_d[idx]=3*in_d[idx]; } } void test(){ size_t size_bytes = N * sizeof(float); void * in_d_0; void * out_d_0; void * out_d_1; void * out_d_2; int threads = 128; int blocks = (N+threads)/threads; int iter = 10; cudaStream_t stream; cudaStreamCreate(&amp;stream); CUmemoryPool pool_; cuDeviceGetDefaultMemPool(&amp;pool_, 0); uint64_t threshold = UINT64_MAX; cuMemPoolSetAttribute(pool_, CU_MEMPOOL_ATTR_RELEASE_THRESHOLD, &amp;threshold); cudaGraph_t graph; cudaGraphExec_t instance; bool graphCreated=false; for (int i =0; i &lt; iter; i++){ cuMemAllocFromPoolAsync(reinterpret_cast&lt;CUdeviceptr*&gt;(&amp;in_d_0), size_bytes, pool_, stream); cuMemAllocFromPoolAsync(reinterpret_cast&lt;CUdeviceptr*&gt;(&amp;out_d_0), size_bytes, pool_, stream); shortKernel_0&lt;&lt;&lt;blocks, threads,0, stream&gt;&gt;&gt;(reinterpret_cast&lt;float *&gt;(out_d_0), reinterpret_cast&lt;float *&gt;(in_d_0)); if (!graphCreated){ cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal); cuMemAllocFromPoolAsync(reinterpret_cast&lt;CUdeviceptr*&gt;(&amp;out_d_1), size_bytes, pool_, stream); cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(in_d_0), stream); shortKernel_1&lt;&lt;&lt;blocks, threads,0, stream&gt;&gt;&gt;(reinterpret_cast&lt;float *&gt;(out_d_1), reinterpret_cast&lt;float *&gt;(out_d_0)); cudaStreamEndCapture(stream, &amp;graph); checkCudaErrors(cudaGraphInstantiate(&amp;instance, graph, NULL, NULL, 0)); checkCudaErrors(cudaGraphUpload(instance, stream)); graphCreated = true; }else{ checkCudaErrors(cudaGraphLaunch(instance, stream)); } cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(out_d_0), stream); cuMemAllocFromPoolAsync(reinterpret_cast&lt;CUdeviceptr*&gt;(&amp;out_d_2), size_bytes, pool_, stream); shortKernel_2&lt;&lt;&lt;blocks, threads,0, stream&gt;&gt;&gt;(reinterpret_cast&lt;float *&gt;(out_d_2), reinterpret_cast&lt;float *&gt;(out_d_1)); cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(out_d_1), stream); cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(out_d_2), stream); } cudaDeviceSynchronize(); printf(\"With async malloc done!\"); cudaStreamDestroy(stream); cudaGraphDestroy(graph); cudaGraphExecDestroy(instance); } int main() { test(); return 0; } The output from kernel_0 is consumed by kernel_1. and The output from kernel_1 is consumed by kernel_2. However, when I ran with compute-sanitizer, I got some errors. Any idea on this error? Part of error is attached: ========= Program hit CUDA_ERROR_INVALID_VALUE (error 1) due to \"invalid argument\" on CUDA API call to cuMemFreeAsync. ========= Saved host backtrace up to driver entry point at error ========= Host Frame: [0x2ef045] ========= in /usr/local/cuda/compat/lib.real/libcuda.so.1 ========= Host Frame:test() [0xb221] ========= in /opt/test-cudagraph/./a.out ========= Host Frame:main [0xb4b3] ========= in /opt/test-cudagraph/./a.out ========= Host Frame:__libc_start_main [0x24083] ========= in /usr/lib/x86_64-linux-gnu/libc.so.6 ========= Host Frame:_start [0xaf6e] ========= in /opt/test-cudagraph/./a.out",
        "answers": [
            [
                "1. Figuring out where the error occurs, exactly To get the \"idea\", you need to wrap all of your API calls with error checks. Doing so properly is a bit tricky, since the cudaError_t runtime-API status type and the CUresult driver-API status type don't agree on all values, so you would need to overload the error-check function: void check(cudaError_t result, char const *const func, const char *const file, int const line) { if (result) { fprintf(stderr, \"CUDA runtime error at %s:%d code=%d(%s) \\\"%s\\\" \\n\", file, line, static_cast&lt;unsigned int&gt;(result), _cudaGetErrorEnum(result), func); exit(EXIT_FAILURE); } } void check(CUresult result, char const *const func, const char *const file, int const line) { if (result) { const char* error_name = \"(UNKNOWN)\"; cuGetErrorName(result, &amp;error_name); fprintf(stderr, \"CUDA driver error at %s:%d code=%d(%s) \\\"%s\\\" \\n\", file, line, static_cast&lt;unsigned int&gt;(result), error_name, func); exit(EXIT_FAILURE); } } when you then wrap all your calls with an error check, running the program gets you: CUDA driver error at a.cu:102 code=1(CUDA_ERROR_INVALID_VALUE) \"cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(out_d_1), stream)\" and the line triggering the error is: checkCudaErrors(cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(out_d_1), stream)); i.e. the CUDA driver believes out_d_1 is not a valid device pointer for (asynchronous) freeing. This was the easy part which isn't even that specific to your program. 2. The errors There are two problems in your code: On the first pass of your for loop, you capture the graph using stream capture. When capturing a graph this way, no actual work is done during the graph capture process. This means that on the first iteration of the for loop, this line cuMemAllocFromPoolAsync(reinterpret_cast&lt;CUdeviceptr*&gt;(&amp;out_d_1), size_bytes, pool_, stream); does nothing. No allocation is performed. out_d_1 is not modified. However during that same for loop iteration, you attempt to free that pointer here: cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(out_d_1), stream);, but on that particular for loop iteration it was never allocated. So the free fails. This explains the cuMemFreeAsync problem related to the usage here: cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(out_d_1), stream); There is also a problem with the usage of cuMemFreeAsync during the capture process, specifically this line: cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(in_d_0), stream); We can see that the allocation for that item (in_d_0) that you are attempting to free during graph capture (i.e. during the graph execution) is allocated outside the graph. But this is a no-no. See the documentation for cuMemFreeAsync: During stream capture, this function results in the creation of a free node and must therefore be passed the address of a graph allocation 3. What can you do about it? Combining those two items, one possible way to fix your posted code is as follows: $ cat t2068.cu #include &lt;stdio.h&gt; #include &lt;chrono&gt; #include &lt;cuda.h&gt; #include &lt;cuda_runtime.h&gt; #include &lt;iostream&gt; #define N 50000 #define NSTEP 1000 #define NKERNEL 20 using namespace std::chrono; static const char *_cudaGetErrorEnum(cudaError_t error) { return cudaGetErrorName(error); } template &lt;typename T&gt; void check(T result, char const *const func, const char *const file, int const line) { if (result) { fprintf(stderr, \"CUDA error at %s:%d code=%d(%s) \\\"%s\\\" \\n\", file, line, static_cast&lt;unsigned int&gt;(result), _cudaGetErrorEnum(result), func); exit(EXIT_FAILURE); } } #define checkCudaErrors(val) check((val), #val, __FILE__, __LINE__) __global__ void shortKernel_0(float * out_d, float * in_d){ int idx=blockIdx.x*blockDim.x+threadIdx.x; if(idx&lt;N) { in_d[idx] = 1.0; out_d[idx]=1 + in_d[idx]; } } __global__ void shortKernel_1(float * out_d, float * in_d){ int idx=blockIdx.x*blockDim.x+threadIdx.x; if(idx&lt;N) out_d[idx]=2*in_d[idx]; } __global__ void shortKernel_2(float * out_d, float * in_d){ int idx=blockIdx.x*blockDim.x+threadIdx.x; if(idx&lt;N) { out_d[idx]=3*in_d[idx]; } } void test(){ size_t size_bytes = N * sizeof(float); void * in_d_0; void * out_d_0; void * out_d_1; void * out_d_2; int threads = 128; int blocks = (N+threads)/threads; int iter = 10; cudaStream_t stream; cudaStreamCreate(&amp;stream); CUmemoryPool pool_; cuDeviceGetDefaultMemPool(&amp;pool_, 0); uint64_t threshold = UINT64_MAX; cuMemPoolSetAttribute(pool_, CU_MEMPOOL_ATTR_RELEASE_THRESHOLD, &amp;threshold); cudaGraph_t graph; cudaGraphExec_t instance; bool graphCreated=false; for (int i =0; i &lt; iter; i++){ cuMemAllocFromPoolAsync(reinterpret_cast&lt;CUdeviceptr*&gt;(&amp;in_d_0), size_bytes, pool_, stream); cuMemAllocFromPoolAsync(reinterpret_cast&lt;CUdeviceptr*&gt;(&amp;out_d_0), size_bytes, pool_, stream); shortKernel_0&lt;&lt;&lt;blocks, threads,0, stream&gt;&gt;&gt;(reinterpret_cast&lt;float *&gt;(out_d_0), reinterpret_cast&lt;float *&gt;(in_d_0)); // moved the next line outside of the graph region cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(in_d_0), stream); if (!graphCreated){ cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal); cuMemAllocFromPoolAsync(reinterpret_cast&lt;CUdeviceptr*&gt;(&amp;out_d_1), size_bytes, pool_, stream); //cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(in_d_0), stream); shortKernel_1&lt;&lt;&lt;blocks, threads,0, stream&gt;&gt;&gt;(reinterpret_cast&lt;float *&gt;(out_d_1), reinterpret_cast&lt;float *&gt;(out_d_0)); cudaStreamEndCapture(stream, &amp;graph); checkCudaErrors(cudaGraphInstantiate(&amp;instance, graph, NULL, NULL, 0)); checkCudaErrors(cudaGraphUpload(instance, stream)); graphCreated = true; } // modified so that we run the instantiated graph on every iteration checkCudaErrors(cudaGraphLaunch(instance, stream)); cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(out_d_0), stream); cuMemAllocFromPoolAsync(reinterpret_cast&lt;CUdeviceptr*&gt;(&amp;out_d_2), size_bytes, pool_, stream); shortKernel_2&lt;&lt;&lt;blocks, threads,0, stream&gt;&gt;&gt;(reinterpret_cast&lt;float *&gt;(out_d_2), reinterpret_cast&lt;float *&gt;(out_d_1)); cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(out_d_1), stream); cuMemFreeAsync(reinterpret_cast&lt;const CUdeviceptr&amp;&gt;(out_d_2), stream); } cudaDeviceSynchronize(); printf(\"With async malloc done!\\n\"); cudaStreamDestroy(stream); cudaGraphDestroy(graph); cudaGraphExecDestroy(instance); } int main() { test(); return 0; } $ nvcc -o t2068 t2068.cu -lcuda $ compute-sanitizer ./t2068 ========= COMPUTE-SANITIZER With async malloc done! ========= ERROR SUMMARY: 0 errors $ A reasonable question might be \"If freeing a non-graph allocation is not allowed in a graph, why didn't graph capture fail?\" I suspect the answer to that is that the graph capture mechanism is not able to determine at the point of graph capture whether your CUdeviceptr will contain an entity that was allocated during graph execution, or not. You might also want to consider avoiding the de-allocation and re-allocation of other buffers. After all, the buffer sizes are constant over all iterations. Some observations about this stream ordered memory allocation in graphs: an item allocated outside the graph cannot be freed in the graph an item allocated in the graph can be freed in the graph an item allocated in the graph need not be freed immediately at the end of graph execution, it can be freed later (in non-graph code, as is demonstrated here) an item allocated in a graph should be freed before the graph attempts to allocate it again, but also specifically, before the graph is launched again. Hopefully the reasons for this are obvious; it would be a typical memory leak. However you may get a graph runtime error if you forget this. You can use a control at graph instantiation to auto-free such allocations at the graph launch point: If any allocations created by [the graph being launched] remain unfreed ... and hGraphExec was not instantiated with CUDA_GRAPH_INSTANTIATE_FLAG_AUTO_FREE_ON_LAUNCH, the launch will fail with CUDA_ERROR_INVALID_VALUE."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using cuda graph stream capture API to implement a small demo with multi streams. Referenced by the CUDA Programming Guide here, I wrote the complete code. In my knowledge, kernelB should execute on stream1, but with nsys I found kernelB is executed on a complete new stream. It is under-control. The scheduling graph is showed below: Here is my code: #include &lt;iostream&gt; __global__ void kernelA() {} __global__ void kernelB() {} __global__ void kernelC() {} int main() { cudaStream_t stream1, stream2; cudaStreamCreate(&amp;stream1); cudaStreamCreate(&amp;stream2); cudaGraphExec_t graphExec = NULL; cudaEvent_t event1, event2; cudaEventCreate(&amp;event1); cudaEventCreate(&amp;event2); for (int i = 0; i &lt; 10; i++) { cudaGraph_t graph; cudaGraphExecUpdateResult updateResult; cudaGraphNode_t errorNode; cudaStreamBeginCapture(stream1, cudaStreamCaptureModeGlobal); kernelA&lt;&lt;&lt;512, 512, 0, stream1&gt;&gt;&gt;(); cudaEventRecord(event1, stream1); cudaStreamWaitEvent(stream2, event1, 0); kernelB&lt;&lt;&lt;256, 512, 0, stream1&gt;&gt;&gt;(); kernelC&lt;&lt;&lt;16, 512, 0, stream2&gt;&gt;&gt;(); cudaEventRecord(event2, stream2); cudaStreamWaitEvent(stream1, event2, 0); cudaStreamEndCapture(stream1, &amp;graph); if (graphExec != NULL) { cudaGraphExecUpdate(graphExec, graph, &amp;errorNode, &amp;updateResult); } if (graphExec == NULL || updateResult != cudaGraphExecUpdateSuccess) { if (graphExec != NULL) { cudaGraphExecDestroy(graphExec); } cudaGraphInstantiate(&amp;graphExec, graph, NULL, NULL, 0); } cudaGraphDestroy(graph); cudaGraphLaunch(graphExec, stream1); cudaStreamSynchronize(stream1); } }",
        "answers": [
            [
                "\"An operation may be scheduled at any time once the nodes on which it depends are complete. Scheduling is left up to the CUDA system.\" Here."
            ],
            [
                "I also ask in Nvidia Forums, Robert answered this question which help me a lot. Someone who are interested in the scheduling of cuda graph can also reference to this answer here."
            ]
        ],
        "votes": [
            2.0000001,
            2.0000001
        ]
    },
    {
        "question": "I am trying to utilize CUDA Graphs for the computation of Fast Fourier Transform (FFT) using CUDA's cuFFT APIs. I modified the sample FFT code present on Github into the following FFT code using CUDA Graphs: #include &lt;cuda.h&gt; #include \"cuda_runtime.h\" #include \"device_launch_parameters.h\" #include \"device_functions.h\" #include &lt;iostream&gt; #include &lt;cufft.h&gt; // Complex data type typedef float2 Complex; static __device__ inline Complex ComplexScale(Complex, float); static __device__ inline Complex ComplexMul(Complex, Complex); static __global__ void ComplexPointwiseMulAndScale(Complex*, const Complex*, int, float); #define CUDA_CALL( call ) \\ { \\ cudaError_t result = call; \\ if ( cudaSuccess != result ) \\ std::cerr &lt;&lt; \"CUDA error \" &lt;&lt; result &lt;&lt; \" in \" &lt;&lt; __FILE__ &lt;&lt; \":\" &lt;&lt; __LINE__ &lt;&lt; \": \" &lt;&lt; cudaGetErrorString( result ) &lt;&lt; \" (\" &lt;&lt; #call &lt;&lt; \")\" &lt;&lt; std::endl; \\ } #define CUDA_FFT_CALL( call ) \\ { \\ cufftResult result = call; \\ if ( CUFFT_SUCCESS != result ) \\ std::cerr &lt;&lt; \"FFT error \" &lt;&lt; result &lt;&lt; \" in \" &lt;&lt; __FILE__ &lt;&lt; \":\" &lt;&lt; __LINE__ &lt;&lt; \": \" &lt;&lt; result &lt;&lt; std::endl; \\ } // The filter size is assumed to be a number smaller than the signal size #define SIGNAL_SIZE 10 #define FILTER_KERNEL_SIZE 4 static __device__ inline Complex ComplexScale(Complex a, float s) { Complex c; c.x = s * a.x; c.y = s * a.y; return c; } // Complex multiplication static __device__ inline Complex ComplexMul(Complex a, Complex b) { Complex c; c.x = a.x * b.x - a.y * b.y; c.y = a.x * b.y + a.y * b.x; return c; } // Complex pointwise multiplication static __global__ void ComplexPointwiseMulAndScale(Complex* a, const Complex* b, int size, float scale) { const int numThreads = blockDim.x * gridDim.x; const int threadID = blockIdx.x * blockDim.x + threadIdx.x; for (int i = threadID; i &lt; size; i += numThreads) { a[i] = ComplexScale(ComplexMul(a[i], b[i]), scale); } } int main() { printf(\"[simpleCUFFT] is starting...\\n\"); int minRadius = FILTER_KERNEL_SIZE / 2; int maxRadius = FILTER_KERNEL_SIZE - minRadius; int padded_data_size = SIGNAL_SIZE + maxRadius; // Allocate HOST Memories Complex* h_signal = (Complex*)malloc(sizeof(Complex) * SIGNAL_SIZE); //host signal Complex* h_filter_kernel = (Complex*)malloc(sizeof(Complex) * FILTER_KERNEL_SIZE); //host filter Complex* h_padded_signal= (Complex*)malloc(sizeof(Complex) * padded_data_size); // host Padded signal Complex* h_padded_filter_kernel = (Complex*)malloc(sizeof(Complex) * padded_data_size); // host Padded filter kernel Complex* h_convolved_signal = (Complex*)malloc(sizeof(Complex) * padded_data_size); // to store convolution RESULTS memset(h_convolved_signal, 0, padded_data_size * sizeof(Complex)); //Allocate DEVICE Memories Complex* d_signal; //device signal cudaMalloc((void**)&amp;d_signal, sizeof(Complex) * padded_data_size); Complex* d_filter_kernel; cudaMalloc((void**)&amp;d_filter_kernel, sizeof(Complex) * padded_data_size); //device kernel //CUDA GRAPH bool graphCreated = false; cudaGraph_t graph; cudaGraphExec_t instance; cudaStream_t stream; cudaStreamCreate(&amp;stream); // CUFFT plan cufftHandle plan; CUDA_FFT_CALL(cufftPlan1d(&amp;plan, padded_data_size, CUFFT_C2C, 1)); cufftSetStream(plan, stream); // bind plan to the stream // Initalize the memory for the signal for (unsigned int i = 0; i &lt; SIGNAL_SIZE; ++i) { h_signal[i].x = rand() / (float)RAND_MAX; h_signal[i].y = 0; } // Initalize the memory for the filter for (unsigned int i = 0; i &lt; FILTER_KERNEL_SIZE; ++i) { h_filter_kernel[i].x = rand() / (float)RAND_MAX; h_filter_kernel[i].y = 0; } //REPEAT 3 times int nRepeatationsNeeded = 3; for (int repeatations = 0; repeatations &lt; nRepeatationsNeeded; repeatations++) { std::cout &lt;&lt; \"\\n\\n\" &lt;&lt; \"Repeatation ------ \" &lt;&lt; repeatations &lt;&lt; std::endl; if (!graphCreated) { //Start Graph Recording --------------!!!!!!!! CUDA_CALL(cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal)); //Pad Data CUDA_CALL(cudaMemcpyAsync(h_padded_signal + 0, h_signal, SIGNAL_SIZE * sizeof(Complex), cudaMemcpyHostToHost, stream)); memset(h_padded_signal + SIGNAL_SIZE, 0, (padded_data_size - SIGNAL_SIZE) * sizeof(Complex)); //CUDA_CALL(cudaMemsetAsync(h_padded_signal + SIGNAL_SIZE, 0, (padded_data_size - SIGNAL_SIZE) * sizeof(Complex), stream)); CUDA_CALL(cudaMemcpyAsync(h_padded_filter_kernel + 0, h_filter_kernel + minRadius, maxRadius * sizeof(Complex), cudaMemcpyHostToHost, stream)); /*CUDA_CALL(cudaMemsetAsync(h_padded_filter_kernel + maxRadius, 0, (padded_data_size - FILTER_KERNEL_SIZE) * sizeof(Complex), stream));*/ memset(h_padded_filter_kernel + maxRadius, 0, (padded_data_size - FILTER_KERNEL_SIZE) * sizeof(Complex)); CUDA_CALL(cudaMemcpyAsync(h_padded_filter_kernel + padded_data_size - minRadius, h_filter_kernel, minRadius * sizeof(Complex), cudaMemcpyHostToHost, stream)); // MemCpy H to D CUDA_CALL(cudaMemcpyAsync(d_signal, h_padded_signal, sizeof(Complex) * padded_data_size, cudaMemcpyHostToDevice, stream)); //Signal CUDA_CALL(cudaMemcpyAsync(d_filter_kernel, h_padded_filter_kernel, sizeof(Complex) * padded_data_size, cudaMemcpyHostToDevice, stream)); //Kernel //COMPUTE FFT CUDA_FFT_CALL(cufftExecC2C(plan, (cufftComplex*)d_signal, (cufftComplex*)d_signal, CUFFT_FORWARD)); // Transform signal CUDA_FFT_CALL(cufftExecC2C(plan, (cufftComplex*)d_filter_kernel, (cufftComplex*)d_filter_kernel, CUFFT_FORWARD)); // Transform kernel ComplexPointwiseMulAndScale &lt;&lt; &lt;64, 1, 0, stream &gt;&gt; &gt; (d_signal, d_filter_kernel, padded_data_size, 1.0f / padded_data_size); // Multiply and normalize CUDA_CALL(cudaGetLastError()); CUDA_FFT_CALL(cufftExecC2C(plan, (cufftComplex*)d_signal, (cufftComplex*)d_signal, CUFFT_INVERSE)); // Transform signal back // Copy device memory to host CUDA_CALL(cudaMemcpyAsync(h_convolved_signal, d_signal, sizeof(Complex) * padded_data_size, cudaMemcpyDeviceToHost, stream)); //END Graph Recording CUDA_CALL(cudaStreamEndCapture(stream, &amp;graph)); CUDA_CALL(cudaGraphInstantiate(&amp;instance, graph, NULL, NULL, 0)); graphCreated = true; } else { CUDA_CALL(cudaGraphLaunch(instance, stream)); CUDA_CALL(cudaStreamSynchronize(stream)); } //verify results for (int i = 0; i &lt; SIGNAL_SIZE; i++) std::cout &lt;&lt; \"index: \" &lt;&lt; i &lt;&lt; \", fft: \" &lt;&lt; h_convolved_signal[i].x &lt;&lt; std::endl; } //Destroy CUFFT context cufftDestroy(plan); // cleanup memory cudaStreamDestroy(stream); free(h_signal); free(h_filter_kernel); free(h_padded_signal); free(h_padded_filter_kernel); cudaFree(d_signal); cudaFree(d_filter_kernel); return 0; } PROBLEM: The Output of the above program is below, in which it can be seen that the values of the result are also ZEROS for the first iteration. How can I resolve this?",
        "answers": [
            [
                "The results are zero for the first iteration, because for the first iteration, the work is all issued in capture mode. In capture mode, no CUDA work actually gets done. From here: When a stream is being captured, work launched into the stream is not enqueued for execution. I pointed you to this same area of the documentation in a comment to your last question. You might wish to read the entire programming guide section on graphs, and there are also blogs available."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have kernel A, B, and C which need to be executed sequentially. A-&gt;B-&gt;C They are executed in a while loop until some condition will be met. while(predicate) { A-&gt;B-&gt;C } The while loop may be executed from 3 to 2000 times - information about a fact that a loop should stopped is produced by kernel C. As the execution is related to multiple invocations of relatively small kernels CUDA Graph sounds like a good idea. However, CUDA graph implementation I have seen are all linear or tree-like without loops. Generally, if the loop is not possible, the long chain of kernels of the length 2000 with possibility of early stop invoked from kernel C would be also OK. However, is it possible to stop the graph execution in some position by the call from inside of the kernel?",
        "answers": [
            [
                "CUDA graphs have no conditionals. A vertex of the graph is visited/executed when its predecessors are complete, and that's that. So, fundamentally, you cannot do this with a CUDA graph. What can you do? Have a smaller graph for the loop iteration, and repeatedly schedule it. Have A, B and C start their execution by checking the loop predicate - and skip all work if it holds. With that being the case, you can schedule many instances of A-&gt;B-&gt;C-&gt;A-&gt;B-&gt;C etc - which, starting at some point, will do nothing. Don't rely on the CUDA graphs API. It's not a general-purpose parallel execution mechanism. :-("
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have tried to change the current device in CUDA graphs by creating this host node: cudaGraph_t graph; // Node #1: Create the 1st setDevice cudaHostNodeParams hostNodeParams = {0}; memset(&amp;hostNodeParams, 0, sizeof(hostNodeParams)); hostNodeParams.fn = [](void *data) { int passed_device_ordinal = *(int *)(data); cout &lt;&lt; \"CUDA-Graph: in the host node: changing the device to: \" &lt;&lt; passed_device_ordinal &lt;&lt; endl; CUDA_CHECK(cudaSetDevice(passed_device_ordinal)); }; hostNodeParams.userData = (void *)&amp;device_1; // Node #1: Add the 1st setDevice CUDA_CHECK(cudaGraphAddHostNode(&amp;setDevice_1, graph, &amp;copy_0to1, 1, &amp;hostNodeParams)); When running the code, I get this output: CUDA-Graph: in the host node: changing the device to: 1 Error operation not permitted at line 68 in file src/MultiGPU.cu Is it possible to change the device within a CUDA graph?",
        "answers": [
            [
                "During the execution of a graph, the current device cannot be changed via a host callback, since callbacks are not allowed to make cuda api calls. There are two ways to specify the device on which a kernel within the graph will execute. Use stream-capture to create a multi-gpu graph. When manually constructing the graph, nodes will be assigned to the currently active device. Use cudaSetDevice before adding your kernel. The following code demonstrates both with a simple pipeline which executes (kernel, memcpy to host, host callback) on each gpu. #include &lt;thread&gt; #include &lt;future&gt; #include &lt;chrono&gt; #include &lt;array&gt; #include &lt;vector&gt; #include &lt;cassert&gt; __global__ void kernel(int* data){ *data = 42; } struct CallbackData{ int* pinnedBuffer; std::vector&lt;int&gt;* vec; }; void callback(void* args){ CallbackData* data = static_cast&lt;CallbackData*&gt;(args); data-&gt;vec-&gt;push_back(*data-&gt;pinnedBuffer); } int main(){ constexpr int numDevices = 2; std::array&lt;int, numDevices&gt; deviceIds{0,1}; constexpr int numIterations = 100; std::array&lt;cudaStream_t, numDevices&gt; streams{}; std::array&lt;cudaEvent_t, numDevices&gt; events{}; std::array&lt;int*, numDevices&gt; deviceBuffers{}; std::array&lt;int*, numDevices&gt; pinnedBuffers{}; std::array&lt;std::vector&lt;int&gt;, numDevices&gt; vectors{}; std::array&lt;CallbackData, numDevices&gt; callbackArgs{}; for(int i = 0; i &lt; numDevices; i++){ cudaSetDevice(deviceIds[i]); cudaStreamCreate(&amp;streams[i]); cudaEventCreate(&amp;events[i], cudaEventDisableTiming); cudaMalloc(&amp;deviceBuffers[i], sizeof(int)); cudaMallocHost(&amp;pinnedBuffers[i], sizeof(int)); vectors[i].reserve(numIterations); callbackArgs[i].pinnedBuffer = pinnedBuffers[i]; callbackArgs[i].vec = &amp;vectors[i]; } cudaSetDevice(deviceIds[0]); cudaStream_t mainstream; cudaStreamCreate(&amp;mainstream); cudaEvent_t mainevent; cudaEventCreate(&amp;mainevent, cudaEventDisableTiming); auto launch = [&amp;](){ cudaEventRecord(mainevent, mainstream); for(int i = 0; i &lt; numDevices; i++){ cudaSetDevice(deviceIds[i]); auto&amp; stream = streams[i]; cudaStreamWaitEvent(stream, mainevent); for(int k = 0; k &lt; numIterations; k++){ kernel&lt;&lt;&lt;1,1,0,stream&gt;&gt;&gt;(deviceBuffers[i]); cudaMemcpyAsync(pinnedBuffers[i], deviceBuffers[i], sizeof(int), cudaMemcpyDeviceToHost, stream); cudaLaunchHostFunc(stream, callback, (void*)&amp;callbackArgs[i]); } cudaEventRecord(events[i], stream); cudaStreamWaitEvent(mainstream, events[i]); } cudaSetDevice(deviceIds[0]); }; // no graph launch(); cudaStreamSynchronize(mainstream); for(int i = 0; i &lt; numDevices; i++){ assert(vectors[i].size() == numIterations); for(auto x : vectors[i]){ assert(x == 42); } vectors[i].clear(); } //stream capture graph { cudaStreamBeginCapture(mainstream, cudaStreamCaptureModeRelaxed); launch(); cudaGraph_t graph; cudaStreamEndCapture(mainstream, &amp;graph); cudaGraphExec_t execGraph; cudaGraphNode_t errorNode; cudaError_t status = cudaGraphInstantiate(&amp;execGraph, graph, &amp;errorNode, nullptr, 0); assert(status == cudaSuccess) ; cudaGraphDestroy(graph); cudaGraphLaunch(execGraph, mainstream); cudaStreamSynchronize(mainstream); for(int i = 0; i &lt; numDevices; i++){ assert(vectors[i].size() == numIterations); for(auto x : vectors[i]){ assert(x == 42); } vectors[i].clear(); } cudaGraphExecDestroy(execGraph); } //construct graph manually { cudaGraph_t graph; cudaGraphCreate(&amp;graph, 0); for(int i = 0; i &lt; numDevices; i++){ cudaSetDevice(deviceIds[i]); cudaGraphNode_t* prev = nullptr; cudaGraphNode_t kernelNode; cudaGraphNode_t memcpyNode; cudaGraphNode_t hostNode; cudaKernelNodeParams kernelNodeParams{}; kernelNodeParams.func = (void *)kernel; kernelNodeParams.gridDim = dim3(1, 1, 1); kernelNodeParams.blockDim = dim3(1, 1, 1); kernelNodeParams.sharedMemBytes = 0; void *kernelArgs[1] = {(void *)&amp;deviceBuffers[i]}; kernelNodeParams.kernelParams = kernelArgs; kernelNodeParams.extra = NULL; cudaHostNodeParams hostNodeParams{}; hostNodeParams.fn = callback; hostNodeParams.userData = &amp;callbackArgs[i]; for(int k = 0; k &lt; numIterations; k++){ cudaGraphAddKernelNode(&amp;kernelNode, graph, prev, (prev == nullptr ? 0 : 1), &amp;kernelNodeParams); cudaGraphAddMemcpyNode1D(&amp;memcpyNode, graph, &amp;kernelNode, 1, pinnedBuffers[i], deviceBuffers[i], sizeof(int), cudaMemcpyDeviceToHost); cudaGraphAddHostNode(&amp;hostNode, graph, &amp;memcpyNode, 1, &amp;hostNodeParams); prev = &amp;hostNode; } cudaSetDevice(deviceIds[0]); } cudaGraphExec_t execGraph; cudaGraphNode_t errorNode; cudaError_t status = cudaGraphInstantiate(&amp;execGraph, graph, &amp;errorNode, nullptr, 0); assert(status == cudaSuccess) ; cudaGraphDestroy(graph); cudaGraphLaunch(execGraph, mainstream); cudaStreamSynchronize(mainstream); for(int i = 0; i &lt; numDevices; i++){ assert(vectors[i].size() == numIterations); for(auto x : vectors[i]){ assert(x == 42); } vectors[i].clear(); } cudaGraphExecDestroy(execGraph); } cudaEventDestroy(mainevent); cudaStreamDestroy(mainstream); for(int i = 0; i &lt; numDevices; i++){ cudaSetDevice(deviceIds[i]); cudaStreamDestroy(streams[i]); cudaEventDestroy(events[i]); cudaFree(deviceBuffers[i]); cudaFreeHost(pinnedBuffers[i]); } }"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have a program where multiple host threads try to capture a cuda graph and execute it. It produces the correct results, but it cannot be run with cuda-memcheck. When run with cuda-memcheck, the following error appears. Program hit cudaErrorStreamCaptureInvalidated (error 901) due to \"operation failed due to a previous error during capture\" on CUDA API call to cudaLaunchKernel. When only one host thread is used cuda-memcheck shows no error. Here is example code which can be compiled with nvcc 10.2 : nvcc -arch=sm_61 -O3 main.cu -o main #include &lt;iostream&gt; #include &lt;memory&gt; #include &lt;algorithm&gt; #include &lt;cassert&gt; #include &lt;vector&gt; #include &lt;thread&gt; #include &lt;iterator&gt; #ifndef CUERR #define CUERR { \\ cudaError_t err; \\ if ((err = cudaGetLastError()) != cudaSuccess) { \\ std::cout &lt;&lt; \"CUDA error: \" &lt;&lt; cudaGetErrorString(err) &lt;&lt; \" : \" \\ &lt;&lt; __FILE__ &lt;&lt; \", line \" &lt;&lt; __LINE__ &lt;&lt; std::endl; \\ exit(1); \\ } \\ } #endif __global__ void kernel(int id, int num){ printf(\"kernel %d, id %d\\n\", num, id); } struct Data{ bool isValidGraph = false; int id = 0; int deviceId = 0; cudaGraphExec_t execGraph = nullptr; cudaStream_t stream = nullptr; }; void buildGraphViaCapture(Data&amp; data){ cudaSetDevice(data.deviceId); CUERR; if(!data.isValidGraph){ std::cerr &lt;&lt; \"rebuild graph\\n\"; if(data.execGraph != nullptr){ cudaGraphExecDestroy(data.execGraph); CUERR; } assert(data.stream != cudaStreamLegacy); cudaStreamCaptureStatus captureStatus; cudaStreamIsCapturing(data.stream, &amp;captureStatus); CUERR; assert(captureStatus == cudaStreamCaptureStatusNone); cudaStreamBeginCapture(data.stream, cudaStreamCaptureModeRelaxed); CUERR; for(int i = 0; i &lt; 64; i++){ kernel&lt;&lt;&lt;1,1,0,data.stream&gt;&gt;&gt;(data.id, i); } cudaGraph_t graph; cudaStreamEndCapture(data.stream, &amp;graph); CUERR; cudaGraphExec_t execGraph; cudaGraphNode_t errorNode; auto logBuffer = std::make_unique&lt;char[]&gt;(1025); std::fill_n(logBuffer.get(), 1025, 0); cudaError_t status = cudaGraphInstantiate(&amp;execGraph, graph, &amp;errorNode, logBuffer.get(), 1025); if(status != cudaSuccess){ if(logBuffer[1024] != '\\0'){ std::cerr &lt;&lt; \"cudaGraphInstantiate: truncated error message: \"; std::copy_n(logBuffer.get(), 1025, std::ostream_iterator&lt;char&gt;(std::cerr, \"\")); std::cerr &lt;&lt; \"\\n\"; }else{ std::cerr &lt;&lt; \"cudaGraphInstantiate: error message: \"; std::cerr &lt;&lt; logBuffer.get(); std::cerr &lt;&lt; \"\\n\"; } CUERR; } cudaGraphDestroy(graph); CUERR; data.execGraph = execGraph; data.isValidGraph = true; } } void execute(Data&amp; data){ buildGraphViaCapture(data); assert(data.isValidGraph); cudaGraphLaunch(data.execGraph, data.stream); CUERR; } void initData(Data&amp; data, int id, int deviceId){ data.id = id; data.deviceId = deviceId; cudaStreamCreate(&amp;data.stream); CUERR; } void destroyData(Data&amp; data){ if(data.execGraph != nullptr){ cudaGraphExecDestroy(data.execGraph); CUERR; } cudaStreamDestroy(data.stream); CUERR; } int main(){ std::vector&lt;int&gt; deviceIds{0}; std::vector&lt;std::thread&gt; threads; for(int deviceId : deviceIds){ for(int k = 0; k &lt; 4; k++){ threads.emplace_back([&amp;,deviceId](){ std::vector&lt;Data&gt; vec(3); initData(vec[0], deviceId * 10 + 4*k + 0, deviceId); initData(vec[1], deviceId * 10 + 4*k + 1, deviceId); int cur = 0; for(int iter = 0; iter &lt; 10; iter++){ cudaStreamSynchronize(vec[cur].stream); CUERR; execute(vec[cur]); CUERR; cur = 1 - cur; } cudaStreamSynchronize(vec[0].stream); CUERR; cudaStreamSynchronize(vec[1].stream); CUERR; destroyData(vec[0]); destroyData(vec[1]); }); } } for(auto&amp; t : threads){ t.join(); } cudaDeviceReset(); return 0; } Why does the error only appear when multiple threads are used, and why exactly is the capture invalidated? Edit 20th march 2022: The error still exists with CUDA-MEMCHECK version 11.5.114. However, cuda-memcheck is now deprecated in favor of compute-sanitizer. The latter does no longer report cudaErrorStreamCaptureInvalidated",
        "answers": [
            [
                "Cuda graphs are not thread safe. If you read the documentation, it says that: Graph objects (cudaGraph_t, CUgraph) are not internally synchronized and must not be accessed concurrently from multiple threads. API calls accessing the same graph object must be serialized externally. You need to access the graph object in a critical section."
            ],
            [
                "We ran into this problem aswell - where even though we are working on different CUDA graph objects, we still get errors. Our (ugly) solution is to wrap the cudaStreamBeginCapture and cudaStreamEndCapture in a RAII struct with a static mutex. It solves the problem for now, but I'm going to inquire further on the CUDA developer forums."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "When I am trying to capture stream execution to build CUDA graph, call to thrust::reduce causes a runtime error cudaErrorStreamCaptureUnsupported: operation not permitted when stream is capturing. I have tried returning the reduction result to both host and device variables, and I am calling reduction in a proper stream by the means of thrust::cuda::par.on(stream). Is there any way I can add thrust functions execution to CUDA graphs?",
        "answers": [
            [
                "Thrust's reduction operation is a blocking operation on the host side. I am assuming that you are using the result of reduction as a parameter to one of your following kernels. So that when you are capturing a CUDA graph, it cannot instantiate the graph executable because you are dependent on a variable that is on the host side but not available until the reduction kernel finishes execution. As a solution, you can try adding a host node to your graph that returns the result of the reduction."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "CUDA 10 added runtime API calls for putting streams (= queues) in \"capture mode\", so that instead of executing, they are returned in a \"graph\". These graphs can then be made to actually execute, or they can be cloned. But what is the rationale behind this feature? Isn't it unlikely to execute the same \"graph\" twice? After all, even if you do run the \"same code\", at least the data is different, i.e. the parameters the kernels take likely change. Or - am I missing something? PS - I skimmed this slide deck, but still didn't get it.",
        "answers": [
            [
                "My experience with graphs is indeed that they are not so mutable. You can change the parameters with 'cudaGraphHostNodeSetParams', but in order for the change of parameters to take effect, I had to rebuild the graph executable with 'cudaGraphInstantiate'. This call takes so long that any gain of using graphs is lost (in my case). Setting the parameters only worked for me when I build the graph manually. When getting the graph through stream capture, I was not able to set the parameters of the nodes as you do not have the node pointers. You would think the call 'cudaGraphGetNodes' on a stream captured graph would return you the nodes. But the node pointer returned was NULL for me even though the 'numNodes' variable had the correct number. The documentation explicitly mentions this as a possibility but fails to explain why."
            ],
            [
                "Task graphs are quite mutable. There are API calls for changing/setting the parameters of task graph nodes of various kinds, so one can use a task graph as a template, so that instead of enqueueing the individual nodes before every execution, one changes the parameters of every node before every execution (and perhaps not all nodes actually need their parameters changed). For example, See the documentation for cudaGraphHostNodeGetParams and cudaGraphHostNodeSetParams."
            ],
            [
                "Another useful feature is the concurrent kernel executions. Under manual mode, one can add nodes in the graph with dependencies. It will explore the concurrency automatically using multiple streams. The feature itself is not new but make it automatic becomes useful for certain applications."
            ],
            [
                "When training a deep learning model it happens often to re-run the same set of kernels in the same order but with updated data. Also, I would expect Cuda to do optimizations by knowing statically what will be the next kernels. We can imagine that Cuda can fetch more instructions or adapt its scheduling strategy when knowing the whole graph."
            ],
            [
                "CUDA Graphs is trying to solve the problem that in the presence of too many small kernel invocations, you see quite some time spent on the CPU dispatching work for the GPU (overhead). It allows you to trade resources (time, memory, etc.) to construct a graph of kernels that you can use a single invocation from the CPU instead of doing multiple invocations. If you don't have enough invocations, or your algorithm is different each time, then it won't worth it to build a graph. This works really well for anything iterative that uses the same computation underneath (e.g., algorithms that need to converge to something) and it's pretty prominent in a lot of applications that are great for GPUs (e.g., think of the Jacobi method). You are not going to see great results if you have an algorithm that you invoke once or if your kernels are big; in that case the CPU invocation overhead is not your bottleneck. A succinct explanation of when you need it exists in the Getting Started with CUDA Graphs. Where task graph based paradigms shine though is when you define your program as tasks with dependencies between them. You give a lot of flexibility to the driver / scheduler / hardware to do scheduling itself without much fine-tuning from the developer's part. There's a reason why we have been spending years exploring the ideas of dataflow programming in HPC."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001,
            1.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "CUDA graphs are a new way to synthesize complex operations from multiple operations. With \"stream capture\", it appears that you can run a mix of operations, including CuBlas and similar library operations and capture them as a singe \"meta-kernel\". What's unclear to me is how the data flow works for these graphs. In the capture phase, I allocate memory A for the input, memory B for the temporary values, and memory C for the output. But when I capture this in a graph, I don't capture the memory allocations. So when I then instantiate multiple copies of these graphs, they cannot share the input memory A, temporary workspace B or output memory C. How then does this work? I.e. when I call cudaGraphLaunch, I don't see a way to provide input parameters. My captured graph basically starts with a cudaMemcpyHostToDevice, how does the graph know which host memory to copy and where to put it? Background: I found that CUDA is heavily bottlenecked on kernel launches; my AVX2 code was 13x times slower when ported to CUDA. The kernels themselves seem fine (according to NSight), it's just the overhead of scheduling several hundred thousand kernel launches.",
        "answers": [
            [
                "A memory allocation would typically be done outside of a graph definition/instantiation or \"capture\". However, graphs provide for \"memory copy\" nodes, where you would typically do cudaMemcpy type operations. At the time of graph definition, you pass a set of arguments for each graph node (which will depend on the node type, e.g. arguments for the cudaMemcpy operation, if it is a memory copy node, or kernel arguments if it is a kernel node). These arguments determine the actual memory allocations that will be used when that graph is executed. If you wanted to use a different set of allocations, one method would be to instantiate another graph with different arguments for the nodes where there are changes. This could be done by repeating the entire process, or by starting with an existing graph, making changes to node arguments, and then instantiating a graph with those changes. Currently, in cuda graphs, it is not possible to perform runtime binding (i.e. at the point of graph \"launch\") of node arguments to a particular graph/node. It's possible that new features may be introduced in future releases, of course. Note that there is a CUDA sample code called simpleCudaGraphs available in CUDA 10 which demonstrates the use of both memory copy nodes, and kernel nodes, and also how to create dependencies (effectively execution dependencies) between nodes."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to generate call graphs for a code that I have in CUDA with egypt but the usual way doesn't seem to work (since nvcc doesn't have any flag that can do the same thing as -fdump-rtl-expand). More details : I have a really large code (of which I am not the author) that spans over multiple .cu files and it would be easier for me to understand what it's doing if I had a call graph. I bet that an answer to this question would be of use to other people as well. Any ideas on how this can be done with cuda (.cu) files?",
        "answers": [
            [
                "You can do this with the CUDA support of clang 3.8. First, compile your CUDA code to emit llvm (example on Windows with CUDA 7.5 installed): clang++ -c main.cu --cuda-gpu-arch=sm_35 -o main.ll -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v7.5\\include\" Then, use the generated ll to build the callgraph with opt: opt.exe main.ll -analyze -dot-callgraph Note that opt is not part of the default binary distribution, you may need to build it yourself (I had a 3.7.1 build and it has been able to manage the ll from 3.8). Example main.cu file: #include &lt;cuda_runtime.h&gt; __device__ int f() { return 1; } __device__ float g(float* a) { return a[f()] ; } __device__ float h() { return 42.0f ; } __global__ void kernel (int a, float* b) { int c = a + f(); g(b); b[c] = h(); } Generated dot file: digraph \"Call graph\" { label=\"Call graph\"; Node0x1e3d438 [shape=record,label=\"{external node}\"]; Node0x1e3d438 -&gt; Node0x1e3cfb0; Node0x1e3d438 -&gt; Node0x1e3ce48; Node0x1e3d438 -&gt; Node0x1e3d0a0; Node0x1e3d438 -&gt; Node0x1e3d258; Node0x1e3d438 -&gt; Node0x1e3cfd8; Node0x1e3d438 -&gt; Node0x1e3ce98; Node0x1e3d438 -&gt; Node0x1e3d000; Node0x1e3d438 -&gt; Node0x1e3cee8; Node0x1e3d438 -&gt; Node0x1e3d078; Node0x1e3d000 [shape=record,label=\"{__cuda_module_ctor}\"]; Node0x1e3d000 -&gt; Node0x1e3ce98; Node0x1e3d000 -&gt; Node0x1e3d168; Node0x1e3d078 [shape=record,label=\"{__cuda_module_dtor}\"]; Node0x1e3d078 -&gt; Node0x1e3cee8; Node0x1e3cfb0 [shape=record,label=\"{^A?f@@YAHXZ}\"]; Node0x1e3d0a0 [shape=record,label=\"{^A?h@@YAMXZ}\"]; Node0x1e3ce48 [shape=record,label=\"{^A?g@@YAMPEAM@Z}\"]; Node0x1e3ce48 -&gt; Node0x1e3cfb0; Node0x1e3d258 [shape=record,label=\"{^A?kernel@@YAXHPEAM@Z}\"]; Node0x1e3d258 -&gt; Node0x1e3cfb0; Node0x1e3d258 -&gt; Node0x1e3ce48; Node0x1e3d258 -&gt; Node0x1e3d0a0; Node0x1e3d168 [shape=record,label=\"{__cuda_register_kernels}\"]; Node0x1e3cee8 [shape=record,label=\"{__cudaUnregisterFatBinary}\"]; Node0x1e3cee8 -&gt; Node0x1e3d528; Node0x1e3cfd8 [shape=record,label=\"{__cudaRegisterFunction}\"]; Node0x1e3cfd8 -&gt; Node0x1e3d528; Node0x1e3ce98 [shape=record,label=\"{__cudaRegisterFatBinary}\"]; Node0x1e3ce98 -&gt; Node0x1e3d528; }"
            ]
        ],
        "votes": [
            1.0000001
        ]
    }
]