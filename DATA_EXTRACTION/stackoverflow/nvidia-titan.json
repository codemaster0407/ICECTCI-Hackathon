[
    {
        "question": "I'm trying to run a CuDNNLSTM layer on Tesla V100-SXM2 GPU, but error appears due to TensorFlow-gpu 2.0.0 installed (can not downgrade because is a shared server). ConfigProto options are deprecated at tf 2.0.0, so previous threads like this does not help. os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" # Or 2, 3, etc. other than 0 tf.config.gpu.set_per_process_memory_growth(True) tf.config.set_soft_device_placement(True) If I use this code lines, another error shows up: module notfoundError: no module named 'tensorflow.contrib'",
        "answers": [
            [
                "it was that the first GPU's memory was already allocated by another workmate. I mannage to select another free GPU just by using the following code and ie. input = 'gpu:3' def config_device(computing_device): if 'gpu' in computing_device: device_number = computing_device.rsplit(':', 1)[1] os.environ[\"CUDA_VISIBLE_DEVICES\"] = device_number # with tf.device(computing_device): gpus = tf.config.experimental.list_physical_devices('GPU') if gpus: try: # Currently, memory growth needs to be the same across GPUs for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices('GPU') print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\") except RuntimeError as e: # Memory growth must be set before GPUs have been initialized print(e)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a ubuntu 16.04 installation with 2 nvidia GPUs: GPU 0: GeForce GT 610 (UUID: GPU-710e856e-358f-7b7d-95b7-e4eae7037c1f) GPU 1: GeForce GTX TITAN X (UUID: GPU-5eacd6f3-f9e4-5795-c75c-26e34ced55ce) nvidia-smi outputs: Sun Jun 10 17:21:47 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 384.130 Driver Version: 384.130 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GT 610 Off | 00000000:02:00.0 N/A | N/A | | 40% 49C P8 N/A / N/A | 133MiB / 1985MiB | N/A Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX TIT... Off | 00000000:03:00.0 Off | N/A | | 22% 50C P8 15W / 250W | 2MiB / 12207MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 Not Supported | +-----------------------------------------------------------------------------+ I have followed the steps in https://www.tensorflow.org/install/install_linux#InstallingAnaconda to install anaconda based tensoflow for GPU. However, if I start a TF Session, I get the following error: Python 2.7.15 |Anaconda, Inc.| (default, May 1 2018, 23:32:55) [GCC 7.2.0] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import tensorflow as tf &gt;&gt;&gt; x = tf.Variable( \"Hello..!\" ) &gt;&gt;&gt; sess = tf.Session() 2018-06-10 17:16:07.662527: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2018-06-10 17:16:07.843402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076 pciBusID: 0000:03:00.0 totalMemory: 11.92GiB freeMemory: 11.80GiB 2018-06-10 17:16:07.880682: E tensorflow/core/common_runtime/direct_session.cc:154] Internal: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/opt/miniconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1560, in __init__ super(Session, self).__init__(target, graph, config=config) File \"/opt/miniconda2/envs/tf-gpu/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 633, in __init__ self._session = tf_session.TF_NewSession(self._graph._c_graph, opts) tensorflow.python.framework.errors_impl.InternalError: Failed to create session. What am I missing? How to get rid of this error?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I was wondering if someone might be able to help me figure out if the new Titan V from nVidia support GPUDirect. As far as I can tell it seems limited to Tesla and Quadro cards. Thank you for taking the time to read this.",
        "answers": [
            [
                "GPUDirect Peer-to-Peer (P2P) is supported between any 2 \"like\" CUDA GPUs (of compute capability 2.0 or higher), if the system topology supports it, and subject to other requirements and restrictions. In a nutshell, the system topology requirement is that both GPUs participating must be enumerated under the same PCIE root complex. If in doubt, \"like\" means identical. Other combinations may be supported (e.g. 2 GPUs of the same compute capability) but this is not specified, or advertised as supported. If in doubt, try it out. Finally, these things must be \"discoverable\" by the GPU driver. If the GPU driver cannot ascertain these facts, and/or the system is not part of a whitelist maintained in the driver, then P2P support will not be possible. Note that in general, P2P support may vary by GPU or GPU family. The ability to run P2P on one GPU type or GPU family does not necessarily indicate it will work on another GPU type or family, even in the same system/setup. The final determinant of GPU P2P support are the tools provided that query the runtime via cudaDeviceCanAccessPeer. So the statement here \"is supported\" should not be construed to refer to a particular GPU type. P2P support can vary by system and other factors as well. No statements made here are a guarantee of P2P support for any particular GPU in any particular setup. GPUDirect RDMA is only supported on Tesla and possibly some Quadro GPUs. So, if you had a system that had 2 Titan V GPUs plugged into PCIE slots that were connected to the same root complex (usually, except in Skylake CPUs, it should be sufficient to say \"connected to the same CPU socket\"), and the system (i.e. core logic) was recognized by the GPU driver, I would expect P2P to work between those 2 GPUs. I would not expect GPUDirect RDMA to work to a Titan V, under any circumstances. YMMV. If in doubt, try it out, before making any large purchasing decisions."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I run ASR experiment using Kaldi on SGE cluster consisting of two workstation with TITAN XP. And randomly I meet the following problem: ERROR (nnet3-train[5.2.62~4-a2342]:FinalizeActiveGpu():cu-device.cc:217) cudaError_t 1 : \"__global__ function call is not configured\" returned from 'cublasCreate(&amp;handle_)' I guess something is wrong with GPU driver or hardware. Could you please offer some help? And here is the complete log",
        "answers": [
            [
                "I had similar issue in running darknet in one of the TX2 with reference to https://blog.csdn.net/JIEJINQUANIL/article/details/103091537 enter the root by sudo su Then source the catkin_ws Then launch the darkent. Then can run. Here is my result Hope you can solve it by similar method"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "My code is pasted below: #-------NETWORK 1--------------- network1 = Sequential() #Dense layers - 1st param is output network1.add(Dense(2048, input_shape=(8500,),name=\"dense_one\")) network1.add(Dense(2048,activation='sigmoid',name = \"dense_two\")) network1.add(Dense(1000,activation='sigmoid',name = \"dense_three\")) network1.add(Dense(100,activation = 'relu',name = \"dense_four\")) for l in network1.layers: print l.name, l.input_shape , \"=======&gt;\", l.output_shape print network1.summary() #-------- NETWORK 2----------- network2 = Sequential() network2.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(224,224,3))) network2.add(Conv2D(64, kernel_size = (3,3))) network2.add(MaxPooling2D(pool_size=(2,2))) network2.add(Dropout(0.5)) network2.add(Dense(100,activation='sigmoid',name =\"network2_three\")) network2.add(Flatten()) #-------------------MERGED NETWORK------------------# model = Sequential() model.add(Merge([network1,network2],mode = 'concat')) Above code runs at 46secs per epoch, which I feel is pretty slow for given sample of 600 data points. My input is in a hdf5 file of size 180 MB. I ran CNN Bechmark tests and it seems to work fine (as pasted below) and it looks like something is worng with my input or the way I am passing it. 2017-09-07 12:58:15.380999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0) 2017-09-07 12:58:17.686796: step 0, duration = 0.059 2017-09-07 12:58:18.244654: step 10, duration = 0.056 2017-09-07 12:58:18.802592: step 20, duration = 0.056 2017-09-07 12:58:19.364020: step 30, duration = 0.059 2017-09-07 12:58:19.983245: step 40, duration = 0.058 2017-09-07 12:58:20.541404: step 50, duration = 0.056 2017-09-07 12:58:21.098754: step 60, duration = 0.055 2017-09-07 12:58:21.656521: step 70, duration = 0.056 2017-09-07 12:58:22.216097: step 80, duration = 0.056 2017-09-07 12:58:22.773647: step 90, duration = 0.056 2017-09-07 12:58:23.275301: Forward across 100 steps, 0.056 +/- 0.002 sec / batch 2017-09-07 12:58:24.844090: step 0, duration = 0.126 2017-09-07 12:58:26.091721: step 10, duration = 0.124 2017-09-07 12:58:27.340821: step 20, duration = 0.125 2017-09-07 12:58:28.589284: step 30, duration = 0.125 2017-09-07 12:58:29.842128: step 40, duration = 0.125 2017-09-07 12:58:31.094425: step 50, duration = 0.125 2017-09-07 12:58:32.348420: step 60, duration = 0.125 2017-09-07 12:58:33.600602: step 70, duration = 0.125 2017-09-07 12:58:34.853246: step 80, duration = 0.125 2017-09-07 12:58:36.105065: step 90, duration = 0.125 2017-09-07 12:58:37.232945: Forward-backward across 100 steps, 0.125 +/- 0.001 sec / batch How do I debug the way my input is being processed?",
        "answers": [],
        "votes": []
    },
    {
        "question": "My Operating System is Windows 10 and I am using Keras with Tensorflow backend on CPU. I want to buy the \"Nvidia Titan x (Pascal)\" GPU as it is recommended for tensorflow on Nvidia website: http://www.nvidia.com/object/gpu-accelerated-applications-tensorflow-configurations.html They recommend Ubuntu 14.04 as the OS. Does anybody know if I can use Tensorflow on Nvidia Titan x (Pascal) GPU, on my Windows 10 machine? Thanks a lot.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am training an Inception-like model using TensorFlow r1.0 with GPU Nvidia Titan X. I added some summary operations to visualize the training procedure, using code as follows: def variable_summaries(var): \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\" with tf.name_scope('summaries'): mean = tf.reduce_mean(var) tf.summary.scalar('mean', mean) with tf.name_scope('stddev'): stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean))) tf.summary.scalar('stddev', stddev) tf.summary.scalar('max', tf.reduce_max(var)) tf.summary.scalar('min', tf.reduce_min(var)) tf.summary.histogram('histogram', var) When I run these operations, the time cost of training one epoch is about 400 seconds. But when I turn off these operations, the time cost of training one epoch is just 90 seconds. How to optimize the graph to minimize the summary operations time cost?",
        "answers": [
            [
                "Summaries of course slow down the training process, because you do more operations and you need to write them to disc. Also, histogram summaries slow the training even more, because for histograms you need more data to be copied from GPU to CPU than for scalar values. So I would try to use histogram logging less often than the rest, that could make some difference. The usual solution is to compute summaries only every X batches. Since you compute summaries only one per epoch and not every batch, it might be worth trying even less summaries logging. Depends on how many batches you have in your dataset, but usually you don't lose much information by gathering a bit less logs."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "When training deep learning model, I found that GPU is not fully utilise if I set the train and validate(test) batch size to be same, say 32, 64, ..., 512. Then I check NVIDIA Titan X specifications: NVIDIA CUDA\u00ae Cores: 3584 Memory: 12 GB GDDR5X In order to reduce test time for CNN model, I want to increase the number of samples in a batch as large as possible. I tried: set number of samples per batch to 3584, cuda out of memrory error. set number of samples per batch to 2048, cuda out of memrory error. set number of samples per batch to 1024, works. but I am not sure whether GPU is fully utilised or not. Question: How to easily pick the number of samples per batch to fully utilize GPU on deep model forward operation?",
        "answers": [
            [
                "Use watch nvidia-smi to check how much GPU memory your processes are using. FYI: Configuring Theano so that it doesn't directly crash when a GPU memory allocation fails Tradeoff batch size vs. number of iterations to train a neural network: From Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. https://arxiv.org/abs/1609.04836 : The stochastic gradient descent method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, usually 32--512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. There have been some attempts to investigate the cause for this generalization drop in the large-batch regime, however the precise answer for this phenomenon is, hitherto unknown. In this paper, we present ample numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions -- and that sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We also discuss several empirical strategies that help large-batch methods eliminate the generalization gap and conclude with a set of future research ideas and open questions. [\u2026] The lack of generalization ability is due to the fact that large-batch methods tend to converge to sharp minimizers of the training function. These minimizers are characterized by large positive eigenvalues in $\\nabla^2 f(x)$ and tend to generalize less well. In contrast, small-batch methods converge to flat minimizers characterized by small positive eigenvalues of $\\nabla^2 f(x)$. We have observed that the loss function landscape of deep neural networks is such that large-batch methods are almost invariably attracted to regions with sharp minima and that, unlike small batch methods, are unable to escape basins of these minimizers. [\u2026]"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I work in an environment in which computational resources are shared, i.e., we have a few server machines equipped with a few Nvidia Titan X GPUs each. For small to moderate size models, the 12 GB of the Titan X is usually enough for 2\u20133 people to run training concurrently on the same GPU. If the models are small enough that a single model does not take full advantage of all the computational units of the GPU, this can actually result in a speedup compared with running one training process after the other. Even in cases where the concurrent access to the GPU does slow down the individual training time, it is still nice to have the flexibility of having multiple users simultaneously train on the GPU. The problem with TensorFlow is that, by default, it allocates the full amount of available GPU memory when it is launched. Even for a small two-layer neural network, I see that all 12 GB of the GPU memory is used up. Is there a way to make TensorFlow only allocate, say, 4 GB of GPU memory, if one knows that this is enough for a given model?",
        "answers": [
            [
                "You can set the fraction of GPU memory to be allocated when you construct a tf.Session by passing a tf.GPUOptions as part of the optional config argument: # Assume that you have 12GB of GPU memory and want to allocate ~4GB: gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333) sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) The per_process_gpu_memory_fraction acts as a hard upper bound on the amount of GPU memory that will be used by the process on each GPU on the same machine. Currently, this fraction is applied uniformly to all of the GPUs on the same machine; there is no way to set this on a per-GPU basis."
            ],
            [
                "config = tf.ConfigProto() config.gpu_options.allow_growth=True sess = tf.Session(config=config) https://github.com/tensorflow/tensorflow/issues/1578"
            ],
            [
                "For TensorFlow 2.0 and 2.1 (docs): import tensorflow as tf tf.config.gpu.set_per_process_memory_growth(True) For TensorFlow 2.2+ (docs): import tensorflow as tf gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) The docs also list some more methods: Set environment variable TF_FORCE_GPU_ALLOW_GROWTH to true. Use tf.config.experimental.set_virtual_device_configuration to set a hard limit on a Virtual GPU device."
            ],
            [
                "Here is an excerpt from the Book Deep Learning with TensorFlow In some cases it is desirable for the process to only allocate a subset of the available memory, or to only grow the memory usage as it is needed by the process. TensorFlow provides two configuration options on the session to control this. The first is the allow_growth option, which attempts to allocate only as much GPU memory based on runtime allocations, it starts out allocating very little memory, and as sessions get run and more GPU memory is needed, we extend the GPU memory region needed by the TensorFlow process. 1) Allow growth: (more flexible) config = tf.ConfigProto() config.gpu_options.allow_growth = True session = tf.Session(config=config, ...) The second method is per_process_gpu_memory_fraction option, which determines the fraction of the overall amount of memory that each visible GPU should be allocated. Note: No release of memory needed, it can even worsen memory fragmentation when done. 2) Allocate fixed memory: To only allocate 40% of the total memory of each GPU by: config = tf.ConfigProto() config.gpu_options.per_process_gpu_memory_fraction = 0.4 session = tf.Session(config=config, ...) Note: That's only useful though if you truly want to bind the amount of GPU memory available on the TensorFlow process."
            ],
            [
                "For Tensorflow version 2.0 and 2.1 use the following snippet: import tensorflow as tf gpu_devices = tf.config.experimental.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(gpu_devices[0], True) For prior versions , following snippet used to work for me: import tensorflow as tf tf_config=tf.ConfigProto() tf_config.gpu_options.allow_growth=True sess = tf.Session(config=tf_config)"
            ],
            [
                "All the answers above assume execution with a sess.run() call, which is becoming the exception rather than the rule in recent versions of TensorFlow. When using the tf.Estimator framework (TensorFlow 1.4 and above) the way to pass the fraction along to the implicitly created MonitoredTrainingSession is, opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.333) conf = tf.ConfigProto(gpu_options=opts) trainingConfig = tf.estimator.RunConfig(session_config=conf, ...) tf.estimator.Estimator(model_fn=..., config=trainingConfig) Similarly in Eager mode (TensorFlow 1.5 and above), opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.333) conf = tf.ConfigProto(gpu_options=opts) tfe.enable_eager_execution(config=conf) Edit: 11-04-2018 As an example, if you are to use tf.contrib.gan.train, then you can use something similar to bellow: tf.contrib.gan.gan_train(........, config=conf)"
            ],
            [
                "You can use TF_FORCE_GPU_ALLOW_GROWTH=true in your environment variables. In tensorflow code: bool GPUBFCAllocator::GetAllowGrowthValue(const GPUOptions&amp; gpu_options) { const char* force_allow_growth_string = std::getenv(\"TF_FORCE_GPU_ALLOW_GROWTH\"); if (force_allow_growth_string == nullptr) { return gpu_options.allow_growth(); }"
            ],
            [
                "Tensorflow 2.0 Beta and (probably) beyond The API changed again. It can be now found in: tf.config.experimental.set_memory_growth( device, enable ) Aliases: tf.compat.v1.config.experimental.set_memory_growth tf.compat.v2.config.experimental.set_memory_growth References: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/config/experimental/set_memory_growth https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth See also: Tensorflow - Use a GPU: https://www.tensorflow.org/guide/gpu for Tensorflow 2.0 Alpha see: this answer"
            ],
            [
                "All the answers above refer to either setting the memory to a certain extent in TensorFlow 1.X versions or to allow memory growth in TensorFlow 2.X. The method tf.config.experimental.set_memory_growth indeed works for allowing dynamic growth during the allocation/preprocessing. Nevertheless one may like to allocate from the start a specific-upper limit GPU memory. The logic behind allocating a specific GPU memory would also be to prevent OOM memory during training sessions. For example, if one trains while opening video-memory consuming Chrome tabs/any other video consumption process, the tf.config.experimental.set_memory_growth(gpu, True) could result in OOM errors thrown, hence the necessity of allocating from the start more memory in certain cases. The recommended and correct way in which to allot memory per GPU in TensorFlow 2.X is done in the following manner: gpus = tf.config.experimental.list_physical_devices('GPU') if gpus: # Restrict TensorFlow to only allocate 1GB of memory on the first GPU try: tf.config.experimental.set_virtual_device_configuration( gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]"
            ],
            [
                "Shameless plug: If you install the GPU supported Tensorflow, the session will first allocate all GPUs whether you set it to use only CPU or GPU. I may add my tip that even you set the graph to use CPU only you should set the same configuration(as answered above:) ) to prevent the unwanted GPU occupation. And in an interactive interface like IPython and Jupyter, you should also set that configure, otherwise, it will allocate all memory and leave almost none for others. This is sometimes hard to notice."
            ],
            [
                "If you're using Tensorflow 2 try the following: config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config)"
            ],
            [
                "For Tensorflow 2.0 this this solution worked for me. (TF-GPU 2.0, Windows 10, GeForce RTX 2070) physical_devices = tf.config.experimental.list_physical_devices('GPU') assert len(physical_devices) &gt; 0, \"Not enough GPU hardware devices available\" tf.config.experimental.set_memory_growth(physical_devices[0], True)"
            ],
            [
                "# allocate 60% of GPU memory from keras.backend.tensorflow_backend import set_session import tensorflow as tf config = tf.ConfigProto() config.gpu_options.per_process_gpu_memory_fraction = 0.6 set_session(tf.Session(config=config))"
            ],
            [
                "this code has worked for me: import tensorflow as tf config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config)"
            ],
            [
                "Well I am new to tensorflow, I have Geforce 740m or something GPU with 2GB ram, I was running mnist handwritten kind of example for a native language with training data containing of 38700 images and 4300 testing images and was trying to get precision , recall , F1 using following code as sklearn was not giving me precise reults. once i added this to my existing code i started getting GPU errors. TP = tf.count_nonzero(predicted * actual) TN = tf.count_nonzero((predicted - 1) * (actual - 1)) FP = tf.count_nonzero(predicted * (actual - 1)) FN = tf.count_nonzero((predicted - 1) * actual) prec = TP / (TP + FP) recall = TP / (TP + FN) f1 = 2 * prec * recall / (prec + recall) plus my model was heavy i guess, i was getting memory error after 147, 148 epochs, and then I thought why not create functions for the tasks so I dont know if it works this way in tensrorflow, but I thought if a local variable is used and when out of scope it may release memory and i defined the above elements for training and testing in modules, I was able to achieve 10000 epochs without any issues, I hope this will help.."
            ],
            [
                "i tried to train unet on voc data set but because of huge image size, memory finishes. i tried all the above tips, even tried with batch size==1, yet to no improvement. sometimes TensorFlow version also causes the memory issues. try by using pip install tensorflow-gpu==1.8.0"
            ]
        ],
        "votes": [
            335.0000001,
            218.0000001,
            106.0000001,
            62.0000001,
            28.0000001,
            22.0000001,
            14.0000001,
            14.0000001,
            11.0000001,
            9.0000001,
            7.0000001,
            5.0000001,
            3.0000001,
            3.0000001,
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "All, I have the following lines of code for setting up a 3D image in OpenCL: const size_t NPOLYORDERS = 16; const size_t NPOLYBINS = 1024; cl::Image3D my3DImage; cl::ImageFormat imFormat(CL_R, CL_FLOAT); my3Dimage = cl::Image3D(clContext, CL_MEM_READ_ONLY, imFormat, NPOLYORDERS, NPOLYORDERS, NPOLYBINS); The code runs fine when I use the Intel OpenCL CPU driver (by creating a context with CL_DEVICE_TYPE_CPU), but fails with a segfault when I use the nVidia driver with a TITAN black (by creating a context with CL_DEVICE_TYPE_GPU). All of this is on RHEL6.4 with a 2.6.32-358 kernel using the latest nVidia driver available, using the Intel OpenCL runtime 14.1_x64_4.4.0.118 and 2014_4.4.0.134_x64 Intel OpenCL SDK. All of the other code appears to be working on the nVidia device. I can compile the kernel, create contexts, buffers, etc, but this one constructor seems to fail. I checked what the max sizes allowed for an Image3D were using cl::Device::getInfo, and it reports that HxWxD limits are 4096x4096x4096, so I'm well below the limit with my 16x16x1024 image size. I also checked to make sure the CL_R and CL_FLOAT types were supported formats, which they appear to be. At first I thought it was failing because of trying to copy the host memory, but the segfault is occurring before I even enqueue the image read. The best I've been able to determine from my gdb back trace is that the problem appears to be in line 4074 of CL/cl.hpp: #0 0x000000000000 in ?? () #1 0x00000000004274fe in cl::Image3D::Image3D (this=0x7fffffffffdcb0, context=..., flags=140737488345384, format=..., width=0, height=140737488345392, depth=1024, row_pitch=0, slice_pitch=0, host_ptr=0x0, err=0x0) at /usr/include/CL/cl.hpp:4074 #2 0x0000000000421986 in clCorrelationMatrixGenerator::initializeOpenCL ( this=0x7fffffffffdfa8) at ./libs/matrix_generator/OpenCLMatrixGenerator.cc:194 As you can see, the width and height arguments to Image3D's constructor look wonky, but I'm not sure those are the real values and not optimized out values due to the compiler. My questions are thus: Is there something I'm doing wrong with regards to nVidia cards, that doesn't apply on the Intel CPU OpenCL driver? Is there a known binary incompatibility between the Intel SDK and the nVidia OpenCL ICD?",
        "answers": [
            [
                "As some of the commenters have pointed out, the nVidia OpenCL implementation doesn't support clCreateImage, which is used by the underlying cl::Image constructor. This is because nVidia only supports up to OpenCL 1.1, and the functions in question are part of OpenCL 1.2. There is, however, a way around this without major refactoring of the code. The cl.hpp in the Intel SDK supports using OpenCL 1.1 for the wrapped functionality of the C++ openCL implementation. This can be enabled by defining CL_USE_DEPRECATED_OPENCL_1_1_APIS."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm using GPUs for scientific computing. Recently Nvidia released its flagship product GeForce Titan Z. I would like to know, how this processor fairs against Tesla K40 (another NVIDIA product). I have already checked the specs but keen to know of any benchmarks between these two processors, or on the ability of Titan Z for scientific computing applications. I also would like to know if the Titan Z should be treated as single GPU or two GPU from the programming perspective. Thanks in Advance, Regards, Sakthi K",
        "answers": [
            [
                "Its got two chips; undoubtly it will act as two separate cards, from a compute perspective, like all other cards of this kind before it. I have worked with titans and other NVidia gaming cards for scientific computing extensively over the last years, and they work just fine for my purposes, but as always, 'it depends'. First of all, if you absolutely do need double precision, then they are a bad deal. Of course most applictions, including scientific simulations, are not actually constrained by the limits of single precision floats; but for some applications it does matter. So the K40 has more memory per chip, and more double precision performance. But if you are sure you dont need either of those (like I do for my next build), a pair of Titan Z's is a pretty good way to cram an insane amount of single precision performance into a manageable form factor. (edit: I see titan z unlike previous gaming cards has full double precision too; so if you do need double precision, that adds to its value. personally, I find memory more often limiting than fp precision though)"
            ],
            [
                "Titan Z is in essence two Titan Black chips on same card with individual 6GB VRAM. Main advantage of Titan Z is that two chips are synchronized so load will be distributed evenly and also that it costs ~$2100 vs cost of $1700 for single Titan Black. The Titan Black itself is designed for mainly single precision units, limited double precision units and relatively low end memory. I would suggest Titan X is probably better choice than either Titan Z or Titan Black due to two reasons: (1) it costs only $1100 (2) it has twice the memory. So you can actually build much powerful system using two Titan X instead of one Titan Z with approximately the same cost. This is especially true for deep learning related work. In fact, NVidia Digits Devbox features Titan X as well. If your computation requires double precision (FP64) then you need to take a look at Tesla chips like K40 which are almost twice as expensive and with same memory as Titan X. Most deep learning and machine learning related work involves single precision so K40 or K80 is not suitable compared Titan X."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    }
]