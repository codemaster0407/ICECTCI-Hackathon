[
    {
        "question": "I was fiddling with some SASS while looking into the way %laneid is used. After a gaffe which wasted a minute of someone's life (sorry about that - you know who you are), I now have the following: CUDA code: __forceinline__ __device__ unsigned lane_id() { unsigned ret; asm volatile (\"mov.u32 %0, %laneid;\" : \"=r\"(ret)); return ret; } __global__ void dummy(unsigned *C) { C[0] = lane_id(); } SASS (for SM 6.1): /*0008*/ MOV R1, c[0x0][0x20]; /* 0x4c98078000870001 */ /*0010*/ { MOV R2, c[0x0][0x140]; /* 0x4c98078005070002 */ /*0018*/ S2R R0, SR_LANEID; } /* 0xf0c8000000070000 */ /* 0x001ffc011e2007ff */ /*0028*/ MOV R3, c[0x0][0x144]; /* 0x4c98078005170003 */ /*0030*/ STG.E [R2], R0; /* 0xeedc200000070200 */ /*0038*/ EXIT; /* 0xe30000000007000f */ /* 0x001f8000fc0007ff */ /*0048*/ BRA 0x40; /* 0xe2400fffff07000f */ /*0050*/ NOP; /* 0x50b0000000070f00 */ /*0058*/ NOP; /* 0x50b0000000070f00 */ /* 0x001f8000fc0007e0 */ /*0068*/ NOP; /* 0x50b0000000070f00 */ /*0070*/ NOP; /* 0x50b0000000070f00 */ /*0078*/ NOP; /* 0x50b0000000070f00 */ So, the STG instruction - store to global memory I would guess - doesn't take SR_LANEID immediately, but rather the register into which it was placed by the inline PTX. Is this because (Pascal) GPUs can't store from a special register, or is it a missed optimization opportunity?",
        "answers": [
            [
                "You can't store from a special register directly; it takes a special operation (S2R) to read a special register's value. Rationale: Giving all instructions addressing modes to access special registers would counter the RISC philosophy and (in my humble opinion) would not be instruction bits well spent, given how likely such an operation is going to occur in practice."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am attempting to build and run the example code from the NVlabs project SASSI which can be used to instrument CUDA code. However, I am struggling to get even the included sample Makefile and matrixMul.cu to build and run properly. I have tried adding the --maxrregcount=16 and -rcd=true nvcc flags, but I think my real issue is with linking properly to the SASSI libraries. All of the library paths listed in the nvcc output below resolve to actual directories with .so files. Any tips or debugging steps are much appreciated. $ make clean rm -f -f matrixMul *.o $ make branch /usr/local/sassi7//bin/nvcc -I./inc -c -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -Xlinker \"--wrap=main\" -Xlinker \"--wrap=exit\" -lineinfo -Xptxas --sassi-inst-before=\"cond-branches\" -Xptxas --sassi-before-args=\"cond-branch-info\" -g -O3 -dc -o matrixMul.o matrixMul.cu * * SASSI Instrumentation Details * * For the settings you passed in, you'll need to make sure that you have * an instrumentation library with the following properties: * - It MUST BE compiled using only 16 registers!! To accomplish this * simply compile your library with the nvcc flag, --maxrregcount=16 * - It must define the following functions: * device void sassi_before_handler(SASSIBeforeParams*,SASSICondBranchParams*) * * * SASSI Instrumentation Details * * For the settings you passed in, you'll need to make sure that you have * an instrumentation library with the following properties: * - It MUST BE compiled using only 16 registers!! To accomplish this * simply compile your library with the nvcc flag, --maxrregcount=16 * - It must define the following functions: * device void sassi_before_handler(SASSIBeforeParams*,SASSICondBranchParams*) * ****************************************************************************** /usr/local/sassi7//bin/nvcc -o matrixMul matrixMul.o -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -Xlinker \"--wrap=main\" -Xlinker \"--wrap=exit\" -L../instlibs/lib -lbranch -L/usr/local/sassi7//extras/CUPTI/lib64 -lcupti -lcudadevrt -Xlinker -rpath,/usr/local/sassi7//extras/CUPTI/lib64 -L/lib -lboost_regex -lcrypto -Xlinker -rpath,/lib nvlink error : Undefined reference to '_Z20sassi_before_handlerP17SASSIBeforeParamsP21SASSICondBranchParams' in 'matrixMul.o' make: * [matrixMul] Error 255**",
        "answers": [
            [
                "The solution was to install Boost, set the appropriate paths in instlibs/env.mk and run make all from instlibs/ before trying to build the example code. The paths were set correctly, but not all of the required SASSI libraries had been generated."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Here's a snippet of some SASS code for a kernel I'm working on (for an sm52 target, compiled in debugging mode): /*0028*/ ISETP.GE.U32.AND P0, PT, R1, R0, PT; /* 0x5b6c038000070107 */ /*0030*/ @P0 BRA 0x40; /* 0xe24000000080000f */ /*0038*/ BPT.TRAP 0x1; /* 0xe3a00000001000c0 */ /* 0x007fbc0321e01fef */ /*0048*/ IADD R2, R1, RZ; /* 0x5c1000000ff70102 */ /*0050*/ I2I.U32.U32 R2, R2; /* 0x5ce0000000270a02 */ /*0058*/ MOV R2, R2; /* 0x5c98078000270002 */ /* 0x007fbc03fde01fef */ /*0068*/ MOV R3, RZ; /* 0x5c9807800ff70003 */ /*0070*/ MOV R2, R2; /* 0x5c98078000270002 */ /*0078*/ MOV R3, R3; /* 0x5c98078000370003 */ /* 0x007fbc03fde01fef */ /*0088*/ MOV R4, R2; /* 0x5c98078000270004 */ /*0090*/ MOV R5, R3; /* 0x5c98078000370005 */ /*0098*/ MOV R2, c[0x0][0x4]; /* 0x4c98078000170002 */ /* 0x007fbc03fde01fef */ /*00a8*/ MOV R3, RZ; /* 0x5c9807800ff70003 */ /*00b0*/ LOP.OR R2, R4, R2; /* 0x5c47020000270402 */ /*00b8*/ LOP.OR R3, R5, R3; /* 0x5c47020000370503 */ I'm noticing more than a couple of instructions of the form \"Move the contents of register Rn to register Rn\" - and that doesn't seen to make sense. I know that when compiling without debugging info enabled, and with optimizations, I don't get these instructions. But, even in debugging mode - why are they there? What's their purpose? AFAIK, when compiling CPU code for debugging you don't get these kind of instructions.",
        "answers": [
            [
                "The simple answer you get that get strange code because you've turned on debugging which turns off optimization. This is normal with modern optimizing compilers because of how they work. They break down operations into a primitive static single-assignment (SSA) form which makes it easier to optimize but when not optimizing generates worse code that more simpler non-optimizing compiler would. There's also a possibility, though I don't think it's the case here, that the instructions are deliberately inserted NOPs in order delay execution. GPUs have instruction sets that are much much different than the general purpose CPUs that you may familiar with. For example most CPUs work as if instructions are executed one at a time and strictly in the order they're given. This is true despite the fact that modern CPUs will try to execute instructions in parallel and even out of order, for improved performance. GPUs typically don't work this way. If you try to use the result that a previous instruction stores in some register before that instruction is finished, you'll get the old value of the register. Unlike a CPU, a GPU won't automatically wait for the instruction to finish before executing the next instruction that depends on it. If you look at the dissembled code you'll notice that instructions are grouped into bundles of three instructions. You might also see that there's hidden instructions between the bundles. The machine code for the instruction is shown on the right (eg. /* 0x007fbc0321e01fef */), but its not disassembled on the left and its address isn't shown despite taking up an 8-byte slot like any other instruction. This actually a scheduling block control code. It's not a real instruction, but instead it instructs the GPU how it should schedule the instructions in the bundle before it. It tells the GPU things like which instructions need to wait for previous instructions to complete and how long they should wait. Finally there's one more possibility, though extremely unlikely, that the redundant MOVs aren't actually NOPs at all. They could be acting on yet to overwritten register values and in parallel with other instructions in some weird manner that gives them a useful effect other than a delay. However this would be a very advanced optimization technique that I would only expect in hand-tuned assembly code, not in a compiler that isn't even generating optimized code."
            ],
            [
                "Based on general compiler knowledge, I have no knowledge about CUDA. Most of the programming languages have mostly context/state-less commands. Each such command can be compiled separately on it's own, into the target machine code/opcode output (making this compilation step sort of simple to implement, dealing only with single actually parsed command). Some exceptions are various prefix/suffix/with modifiers, or things like continue/break to control loops. For example variable = variable + 2; can be compiled into \"add two to variable\" independently from previous and next command in the source (simple and fast), which turns into: \"load variable from memory into register, add two to register, store value from register back to variable memory\". Which register will be used is difficult to decide. If you would think about it for a while, a random register allocation is just as good as any other naive allocation rule. That is often the way how registers are allocated at the early stage of compilation (using any register with smallest penalty for being clobbered). But then you need some \"bridge\" code to connect commands between themselves, either using strictly variables in memory (having then no bridge code at all), or reusing/sharing some values between commands, just moving them into proper register (your \"non sense\" mov rN,rN instructions, saving some fetch instructions from memory). Compilation stage(s) optimizing register allocations (trying to increase sharing/reusing of registers, reassigning registers for some commands and compiling them again, sometimes even reordering blocks of commands to make the register sharing more optimal) is non-trivial task and an time consuming compilation step, which is not required for the code to work. The debug compilation skips this step to produce binary faster. Also in debug build it's desirable to store variable values into it's memory after each source command, to make results visible in debugger, although in optimized release build the compiler may recognize the \"intermediary\" nature of some results, and keeps them temporarily in registers only."
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    }
]