[
    {
        "question": "I cannot find the answer anywhere and I may have overlooked it but it seems that one cannot use __constant__ memory (along with cudaMemcpyToSymbol) and peer-to-peer access with UVA. I've tried the simpleP2P nvidia sample code which works fine on the 4 NV100 with nvlink I have, but as long as I declare the factor 2 in the kernel as: __constant__ float M_; // in global space float M = 2.0; cudaMemcpyToSymbol(M_, &amp;M, sizeof(float), 0, cudaMemcpyDefault); the results are basically zero. If I define it using C preprocessor (e.g. #define M_ 2.0), it works fine. So I'm wondering, is that true or am I doing something wrong? and are there any other kind of memory that also cannot be accessed this way (texture memory for example)?",
        "answers": [
            [
                "The relation between your question of why \"the results are basically zero\" and P2P access with UVA is not immediately clear to me. is that true or am I doing something wrong? It's hard to say as your question is a bit vague and no complete example is shown. __constant__ float M_ allocates a variable M_ on the constant memory of all CUDA visible devices. In order to set the value on multiple devices you should do something like: __constant__ float M_; // &lt;= This declares M_ on the constant memory of all CUDA visible devices __global__ void showMKernel() { printf(\"****** M_ = %f\\n\", M_); } int main() { float M = 2.0; // Make sure that the return values are properly checked for cudaSuccess ... int deviceCount = -1; cudaGetDeviceCount(&amp;deviceCount); // Set M_ on the constant memory of each device: for (int i = 0; i &lt; deviceCount; i++) { cudaSetDevice(i); cudaMemcpyToSymbol(M_, &amp;M, sizeof(float), 0, cudaMemcpyDefault); } // Now, run a kernel to show M_: for (int i = 0; i &lt; deviceCount; i++) { cudaSetDevice(i); printf(\"Device %g :\\n\", i); showMKernel&lt;&lt;&lt;1,1&gt;&gt;&gt;(); cudaDeviceSynchronize(); } } which returns: Device 0 : ****** M = 2.000000 Device 1 : ****** M = 2.000000 // so on for other devices Now, if I replace // Set M_ on the constant memory of each device: for (int i = 0; i &lt; deviceCount; i++) { cudaSetDevice(i); cudaMemcpyToSymbol(M_, &amp;M, sizeof(float), 0, cudaMemcpyDefault); } with cudaMemcpyToSymbol(M_, &amp;M, sizeof(float), 0, cudaMemcpyDefault); this will only set the value of M_ on the active device and therefore returns Device 0 : ****** M = 2.000000 Device 1 : ****** M = 0.000000 // &lt;= I assume this is what you meant by 'the results are basically zero' // M = 0 for other devices too are there any other kind of memory that also cannot be accessed this way (texture memory for example)? Again I'm not entirely sure what this way is. I think generally you cannot access the constant memory or the texture memory of one device from any other devices, though I am not 100% certain. UVA assigns one address space for CPU and GPU memories such that memory copying between host and the global memory of multiple devices become easily accessible through the use of cudaMemcpy with kind cudaMemcpyDefault. Also, P2P communication between devices allows for direct accesses and transfers of data between the global memory of multiple devices. Similar to the __constant__ example above, when you declare a texture like texture &lt;float&gt; some_texture, some_texture will be defined for each visible device, however you would need to explicitly bind some_texture to your texture reference on each device when working with multiple devices."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm considering possibilities to process data on a GPU, that is too big for the GPU memory, and I have a few questions. If I understand that correctly, with mapped memory the data resides in the main memory and is transferred to the GPU only when accessed, so it shouldn't be a problem to allocate more than fits into the GPU memory. UVA is similar to the mapped memory, but the data can be stored in both the CPU and the GPU memory. But is it possible for the GPU then to access the main memory (as with mapped memory) while being full with its own data? Can a memory overflow happen in this case? I've read that with mapped memory the data goes directly to the local memory without being transferred to the global one first, and in this case there shouldn't be any overflow. Is that true and, if so, is that also true for UVA? In CUDA 6.0, UM doesn't allow to oversubscribe the GPU memory (and generally doesn't allow to allocate more memory than the GPU has, even in the main memory), but with CUDA 8.0 it becomes possible (https://devblogs.nvidia.com/parallelforall/beyond-gpu-memory-limits-unified-memory-pascal/). Did I get it right?",
        "answers": [
            [
                "Yes, with mapped (i.e. pinned, \"zero-copy\") method, the data stays in host memory and is transferred to the GPU on-demand, but never becomes resident in GPU memory (unless GPU code stores it there). If you access it multiple times, you may need to transfer it multiple times from the host. UVA (Unified Virtual Addressing see here) is not the same thing as UM (Unified Memory, see here) or managed memory (== UM), so I shall refer to this case as UM, not UVA. With UM on a pre-pascal device, UM \"managed\" allocations will be moved automatically between CPU and GPU subject to some restrictions, but you cannot oversubscribe GPU memory. The maximum amount of all ordinary GPU allocations plus UM allocations cannot exceed GPU physical memory. With UM plus CUDA 8.0 or later plus a Pascal or newer GPU, you can oversubscribe GPU memory with UM (\"managed\") allocations. These allocations are then nominally limited to the size of your system memory (minus whatever other demands there are on system memory). In this case, data is moved back and forth automatically between host and device memory, by the CUDA runtime, using a demand-paging method. UVA is not an actual data management technique in CUDA. It is an underlying technology that enables some features, like certain aspects of mapped memory and generally enables UM features."
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "I'm familiarizing myself with a new cluster equipped with Pascal P100 GPUs+Nvlink. I wrote a ping-pong program to test gpu&lt;-&gt;gpu and gpu&lt;-&gt;cpu bandwidths and peer-to-peer access. (I'm aware the cuda samples contain such a program, but I wanted to do it myself for better understanding.) Nvlink bandwidths appear reasonable (~35 GB/s bidirectional, with the theoretical maximum being 40). However, while debugging the ping-pong I discovered some odd behavior. First of all, cudaMemcpyAsync succeeds no matter what cudaMemcpyKind I specify, for example, if cudaMemcpyAsync is copying memory from host to device, it will succeed even if I pass cudaMemcpyDeviceToHost as the kind. Secondly, when host memory is not page locked, cudaMemcpyAsync does the following: Copying memory from the host to the device appears to succeed (no segfaults or cuda runtime errors, and the data appears to transfer properly). Copying memory from the device to the host fails silently: no segfault occurs, and cudaDeviceSynchronize after the memcpy returns cudaSuccess, but checking the data reveals that data on the gpu did not transfer properly to the host. Is this behavior to be expected? I have included a minimal working sample code that demonstrates it on my system (the sample is not the ping-pong app, all it does is test cudaMemcpyAsync with various parameters). The P100s have UVA enabled, so it is plausible to me that cudaMemcpyAsync is simply inferring the locations of the src and dst pointers and ignoring the cudaMemcpyKind argument. However, I'm not sure why cudaMemcpyAsync fails to throw an error for non-page-locked host memory. I was under the impression that was a strict no-no. #include &lt;stdio.h&gt; #include &lt;cuda_runtime.h&gt; #include &lt;stdlib.h&gt; #define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); } inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true) { if (code != cudaSuccess) { fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line); if (abort) exit(code); } } __global__ void checkDataDevice( int* current, int* next, int expected_current_val, int n ) { int tid = threadIdx.x + blockIdx.x*blockDim.x; for( int i = tid; i &lt; n; i += blockDim.x*gridDim.x ) { if( current[i] != expected_current_val ) printf( \"Error on device: expected = %d, current[%d] = %d\\n\" , expected_current_val , i , current[i] ); // Increment the data so the next copy is properly tested next[i] = current[i] + 1; } } void checkDataHost( int* current, int* next, int expected_current_val, int n ) { for( int i = 0; i &lt; n; i++ ) { if( current[i] != expected_current_val ) printf( \"Error on host: expected = %d, current[%d] = %d\\n\" , expected_current_val , i , current[i] ); // Increment the data so the next copy is properly tested next[i] = current[i] + 1; } } int main( int argc, char** argv ) { bool pagelocked = true; // invoking the executable with any additional argument(s) will turn off page locked memory, i.e., // Run with pagelocked memory: ./a.out // Run with ordinary malloc'd memory: ./a.out jkfdlsja if( argc &gt; 1 ) pagelocked = false; int copybytes = 1e8; // Ok to use int instead of size_t for 1e8. cudaStream_t* stream = (cudaStream_t*)malloc( sizeof(cudaStream_t) ); cudaStreamCreate( stream ); int* srcHost; int* dstHost; int* srcDevice; int* dstDevice; cudaMalloc( (void**)&amp;srcDevice, copybytes ); cudaMalloc( (void**)&amp;dstDevice, copybytes ); if( pagelocked ) { printf( \"Using page locked memory\\n\" ); cudaMallocHost( (void**)&amp;srcHost, copybytes ); cudaMallocHost( (void**)&amp;dstHost, copybytes ); } else { printf( \"Using non page locked memory\\n\" ); srcHost = (int*)malloc( copybytes ); dstHost = (int*)malloc( copybytes ); } for( int i = 0; i &lt; copybytes/sizeof(int); i++ ) srcHost[i] = 1; cudaMemcpyKind kinds[4]; kinds[0] = cudaMemcpyHostToDevice; kinds[1] = cudaMemcpyDeviceToHost; kinds[2] = cudaMemcpyHostToHost; kinds[3] = cudaMemcpyDeviceToDevice; // Test cudaMemcpyAsync in both directions, // iterating through all \"cudaMemcpyKinds\" to verify // that they don't matter. int expected_current_val = 1; for( int kind = 0; kind&lt;4; kind++ ) { // Host to device copy cudaMemcpyAsync( dstDevice , srcHost , copybytes , kinds[kind] , *stream ); gpuErrchk( cudaDeviceSynchronize() ); checkDataDevice&lt;&lt;&lt;56*8,256&gt;&gt;&gt;( dstDevice , srcDevice , expected_current_val , copybytes/sizeof(int) ); expected_current_val++; // Device to host copy cudaMemcpyAsync( dstHost , srcDevice , copybytes , kinds[kind] , *stream ); gpuErrchk( cudaDeviceSynchronize() ); checkDataHost( dstHost , srcHost , expected_current_val , copybytes/sizeof(int) ); expected_current_val++; } free( stream ); cudaFree( srcDevice ); cudaFree( dstDevice ); if( pagelocked ) { cudaFreeHost( srcHost ); cudaFreeHost( dstHost ); } else { free( srcHost ); free( dstHost ); } return 0; }",
        "answers": [
            [
                "When having trouble with a CUDA code, I strongly recommend using rigorous (== every single call return code is checked) proper CUDA error checking. Your error checking is flawed, and the flaws are leading to some of your confusion. First of all, in the page-locked case, a given (mapped) pointer is accessible/valid on both the host and the device. Therefore every possible enumeration of the direction (H2D, D2H, D2D, H2H) is legal and valid. As a result, no errors will be returned and the copy operation is successful. In the non-page-locked case, the above is not true, so generally speaking the indicated transfer direction had better match the implied transfer direction, as inspected from the pointers. If it does not, the cudaMemcpyAsync will return an error code (cudaErrorInvalidValue == 11). In your case, you are ignoring this error result. You can prove this to yourself, if you have enough patience (it would be better if you just flagged the first error, rather than printing out every mismatch in 10M+ elements), by running your code with cuda-memcheck (another good thing to do whenever you are having trouble with a CUDA code) or else just do proper, rigorous error checking. When the cudaMemcpyAsync operation indicates a failure, the operation does not complete successfully, so the data is not copied, and your data checking indicates mismatches. Hopefully this is now not surprising, since the expected copy operation actually did not happen (nor did it fail \"silently\"). Perhaps you are confused thinking that the way to catch an error on any sort of Async operation is to do a cudaDeviceSynchronize and then check for errors on that. This is not correct for cudaMemcpyAsync. An error which can be detected at invocation of the cudaMemcpyAsync operation will be returned immediately by the call itself, and will not be returned as a result of subsequent CUDA calls (clearly) since this type of error is non-sticky. The moral of the story: DO proper CUDA error checking. Rigorously. Run your code with cuda-memcheck. Here's a fully worked example, with a trivial modification to your code to make the output \"sane\" in the failing case, demonstrating that there is an error indicated in the failing case: $ cat t153.cu #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); } inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true) { if (code != cudaSuccess) { fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line); if (abort) exit(code); } } __global__ void checkDataDevice( int* current, int* next, int expected_current_val, int n ) { int tid = threadIdx.x + blockIdx.x*blockDim.x; for( int i = tid; i &lt; n; i += blockDim.x*gridDim.x ) { if( current[i] != expected_current_val ) printf( \"Error on device: expected = %d, current[%d] = %d\\n\" , expected_current_val , i , current[i] ); // Increment the data so the next copy is properly tested next[i] = current[i] + 1; } } void checkDataHost( int* current, int* next, int expected_current_val, int n ) { for( int i = 0; i &lt; n; i++ ) { if( current[i] != expected_current_val ){ printf( \"Error on host: expected = %d, current[%d] = %d\\n\" , expected_current_val , i , current[i] ); exit(0);} // Increment the data so the next copy is properly tested next[i] = current[i] + 1; } } int main( int argc, char** argv ) { bool pagelocked = true; // invoking the executable with any additional argument(s) will turn off page locked memory, i.e., // Run with pagelocked memory: ./a.out // Run with ordinary malloc'd memory: ./a.out jkfdlsja if( argc &gt; 1 ) pagelocked = false; int copybytes = 1e8; // Ok to use int instead of size_t for 1e8. cudaStream_t* stream = (cudaStream_t*)malloc( sizeof(cudaStream_t) ); cudaStreamCreate( stream ); int* srcHost; int* dstHost; int* srcDevice; int* dstDevice; cudaMalloc( (void**)&amp;srcDevice, copybytes ); cudaMalloc( (void**)&amp;dstDevice, copybytes ); if( pagelocked ) { printf( \"Using page locked memory\\n\" ); cudaMallocHost( (void**)&amp;srcHost, copybytes ); cudaMallocHost( (void**)&amp;dstHost, copybytes ); } else { printf( \"Using non page locked memory\\n\" ); srcHost = (int*)malloc( copybytes ); dstHost = (int*)malloc( copybytes ); } for( int i = 0; i &lt; copybytes/sizeof(int); i++ ) srcHost[i] = 1; cudaMemcpyKind kinds[4]; kinds[0] = cudaMemcpyHostToDevice; kinds[1] = cudaMemcpyDeviceToHost; kinds[2] = cudaMemcpyHostToHost; kinds[3] = cudaMemcpyDeviceToDevice; // Test cudaMemcpyAsync in both directions, // iterating through all \"cudaMemcpyKinds\" to verify // that they don't matter. int expected_current_val = 1; for( int kind = 0; kind&lt;4; kind++ ) { // Host to device copy cudaMemcpyAsync( dstDevice , srcHost , copybytes , kinds[kind] , *stream ); gpuErrchk( cudaDeviceSynchronize() ); checkDataDevice&lt;&lt;&lt;56*8,256&gt;&gt;&gt;( dstDevice , srcDevice , expected_current_val , copybytes/sizeof(int) ); expected_current_val++; // Device to host copy cudaMemcpyAsync( dstHost , srcDevice , copybytes , kinds[kind] , *stream ); gpuErrchk( cudaDeviceSynchronize() ); checkDataHost( dstHost , srcHost , expected_current_val , copybytes/sizeof(int) ); expected_current_val++; } free( stream ); cudaFree( srcDevice ); cudaFree( dstDevice ); if( pagelocked ) { cudaFreeHost( srcHost ); cudaFreeHost( dstHost ); } else { free( srcHost ); free( dstHost ); } return 0; } $ nvcc -arch=sm_61 -o t153 t153.cu $ cuda-memcheck ./t153 a ========= CUDA-MEMCHECK Using non page locked memory ========= Program hit cudaErrorInvalidValue (error 11) due to \"invalid argument\" on CUDA API call to cudaMemcpyAsync. ========= Saved host backtrace up to driver entry point at error ========= Host Frame:/usr/lib/x86_64-linux-gnu/libcuda.so.1 [0x2ef423] ========= Host Frame:./t153 [0x489a3] ========= Host Frame:./t153 [0x2e11] ========= Host Frame:/lib/x86_64-linux-gnu/libc.so.6 (__libc_start_main + 0xf5) [0x21ec5] ========= Host Frame:./t153 [0x2a49] ========= Error on host: expected = 2, current[0] = 0 ========= ERROR SUMMARY: 1 error $"
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I know CUDA only got UVA (Unified Virtual Addressing) with version 4.0. But - is that only a software feature? Or does it require some kind of hardware support (on the GPU side I mean)? Notes: In this GTC 2011 presentation it says a Fermi-class GPU is necessary for P2P copies, but it doesn't say that's necessary for UVA itself. Note: I know UVA is not a good idea on a 32-bit-CPU system, I don't mean that kind of hardware support.",
        "answers": [
            [
                "The UVA which was introduced back in May 2011 with CUDA 4.0 requires for hardware support some Fermi-class GPUs. So, this implies compute capability 2.0 onwards. But apparently, that's not enough since, according to slide #17 of this presentation of the new features of CUDA 4.0, it seems to be only supported in 64-bit (which makes sense since otherwise you would run out of address space pretty quick), and with TCC (Tesla Compute Cluster) when on Windows. I'm not sure if this later limitation still exists since I never ever developed on Windows."
            ]
        ],
        "votes": [
            3.0000001
        ]
    }
]