[
    {
        "question": "Closed. This question is not reproducible or was caused by typos. It is not currently accepting answers. This question was caused by a typo or a problem that can no longer be reproduced. While similar questions may be on-topic here, this one was resolved in a way less likely to help future readers. Closed 15 days ago. Improve this question It is been hours writing scripts and I think I am tired overlooking something simple. I have the following pycuda script import cv2 import numpy as np import time import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.gpuarray as gpuarray def apply_threshold(img_src,img_width, img_height, img_dest, mythreshold): mod = SourceModule(\"\"\" __global__ void ThresholdKernel( const int src_sizeX, //&lt; source image size. x: width, const unsigned char* src, //&lt; source image pointer const int dst_sizeX, //&lt; destination image size. x: width, y: height const int dst_sizeY, unsigned char* dst, //&lt; destination image pointer const int mythreshold) { int col = blockIdx.x * blockDim.x + threadIdx.x; int row = blockIdx.y * blockDim.y + threadIdx.y; if (dst_sizeX &lt;= col || dst_sizeY &lt;= row) return; auto src_val = src[row * src_sizeX + col]; unsigned char dst_val = src_val &gt; mythreshold ? 255 : 0; dst[row * dst_sizeX + col] = dst_val; } \"\"\") block_dim =(32,8,1) grid_dim_x = (img_width + block_dim[0] -1) // block_dim[0] grid_dim_y = (img_width + block_dim[1] -1) // block_dim[1] print(grid_dim_x,grid_dim_y) thresholdkernel = mod.get_function(\"ThresholdKernel\") thresholdkernel(np.int32(img_width), img_src, np.int32(img_width),np.int32(img_height), img_dest,np.int32(mythreshold), block = block_dim , grid = (grid_dim_x,grid_dim_y)) mythreshold = 128 img_path = \"../images/lena_gray.png\" img = cv2.imread(img_path) if img is None: print(\"Image not found\") exit() else: height,width,channels = img.shape print(\"Hegiht, width and channels\",height,width,channels) print(type(width)) img_gpu = cuda.mem_alloc(img.nbytes) cuda.memcpy_htod(img_gpu,img) dtype=img.dtype # dest_img=gpuarray.empty_like(img.shape,dtype=dtype) dest_img = cuda.mem_alloc(img.nbytes) apply_threshold(img_gpu,width,height,dest_img ,mythreshold ) image_result= np.empty_like(img) cuda.memcpy_dtoh(image_result,dest_img ) cv2.imshow(\"Original image\",img) cv2.imshow(\"Thresholded\",image_result) cv2.waitKey(0) cv2.destroyAllWindows() When I run it I get a binarized picture but this one What am I overlooking that makes the kernel only process part of the image? It must be something really simple EDIT: I found the problem. The way I am reading the image. It should be img = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE) Now it works, although for some reason it takes 10 times the time of a similar script I have that does the same... well...",
        "answers": [
            [
                "I assume it's because you are using img_width for both grid_dim_x and grid_dim_y. But you probably meant to use img_height for grid_dim_y. Give this a shot: grid_dim_x = (img_width + block_dim[0] -1) // block_dim[0] grid_dim_y = (img_height + block_dim[1] -1) // block_dim[1]"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have read the argument that sometimes implementing things with CUDA on the GPU takes more time than doing it with the CPU because of: The time to allocate device memory The time to transfer to and back to that memory alright, so I have written a script (two actually) in which I do not include the above considerations when I measure the time. It is not ideal but I measure only the time consumed by the kernel. Not transfer, not allocation. Also, I use a kernel that does nothing. So no complex operator to delay us. However, even there the kernel takes 10 times more than a opencv operation done in the CPU. Here the pycuda script import cv2 import numpy as np import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule import argparse parser = argparse.ArgumentParser() parser.add_argument('--show', action='store_true',help=\"show the video while making\") parser.add_argument('--resize',type=int,default=800,help=\"if resize is needed\") parser.add_argument('--noconvert', action='store_true',help=\"avoid rgb conversion\") # Parse and print the results args = parser.parse_args() print(args) # Path to the input H.264 file input_video_path = '70secsmovie.h264' # Replace with the path to your input H.264 file # Path to the output MP4 file output_video_path = 'output_video_cuda.mp4' # Replace with the desired output MP4 file name # Open the input video file video_capture = cv2.VideoCapture(input_video_path) # Check if the video file was opened successfully if not video_capture.isOpened(): print(\"Failed to open the video file.\") exit() # Set the desired width for the output frames desired_width = args.resize #800 # Get the video properties frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH)) frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT)) fps = int(video_capture.get(cv2.CAP_PROP_FPS)) aspect_ratio = frame_width / frame_height desired_height = int(desired_width / aspect_ratio) # Create a VideoWriter object to save the output video codec = cv2.VideoWriter_fourcc(*'mp4v') # output_video = cv2.VideoWriter(output_video_path, codec, fps, (frame_width, frame_height)) output_video = cv2.VideoWriter(output_video_path, codec, fps, (desired_width, desired_height)) # Load the CUDA kernel for drawing the rectangle mod = SourceModule(\"\"\" __global__ void draw_rectangle_kernel(unsigned char *image, int image_width, int x, int y, int width, int height, unsigned char *color) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; if (row &gt;= y &amp;&amp; row &lt; y + height &amp;&amp; col &gt;= x &amp;&amp; col &lt; x + width) { // Perform no operation } } \"\"\") draw_rectangle_kernel = mod.get_function(\"draw_rectangle_kernel\") # Set the block dimensions block_dim_x, block_dim_y = 16, 16 # Calculate the grid dimensions grid_dim_x = (frame_width + block_dim_x - 1) // block_dim_x grid_dim_y = (frame_height + block_dim_y - 1) // block_dim_y # Define the rectangle properties (you can modify these as desired) x, y, width, height = 100, 100, 200, 150 color = np.array([0, 255, 0], dtype=np.uint8) # Initialize the frame count frame_count = 0 average = 0.0 start = cuda.Event() end = cuda.Event() # Read, process, and write each frame from the input video while True: # Read a frame from the video file ret, frame = video_capture.read() # If the frame was not read successfully, the end of the video file is reached if not ret: break # Increment the frame count frame_count += 1 if not args.noconvert: # Convert the frame to the RGB format frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) else: frame_rgb = frame # start.record() # start.synchronize() # Upload the frame to the GPU frame_gpu = cuda.mem_alloc(frame_rgb.nbytes) cuda.memcpy_htod(frame_gpu, frame_rgb) start.record() start.synchronize() # Invoke the CUDA kernel to draw the rectangle # grid_dim_x = (frame_width + block_dim_x - 1) // block_dim_x # grid_dim_y = (frame_height + block_dim_y - 1) // block_dim_y draw_rectangle_kernel(frame_gpu, np.int32(frame_width), np.int32(x), np.int32(y), np.int32(width), np.int32(height), cuda.In(color), block=(block_dim_x, block_dim_y, 1), grid=(grid_dim_x, grid_dim_y)) end.record() end.synchronize() # Download the modified frame from the GPU frame_with_rectangle_rgb = np.empty_like(frame_rgb) cuda.memcpy_dtoh(frame_with_rectangle_rgb, frame_gpu) # end.record() # end.synchronize() secs = start.time_till(end)*1e-3 # print(\"Time of Squaring on GPU with inout\") # print(\"%fs\" % (secs)) average = average + secs if not args.noconvert: # Convert the modified frame back to the BGR format frame_with_rectangle_bgr = cv2.cvtColor(frame_with_rectangle_rgb, cv2.COLOR_RGB2BGR) else: frame_with_rectangle_bgr = frame_with_rectangle_rgb # Resize the frame to the desired width and height while maintaining the aspect ratio resized_frame = cv2.resize(frame_with_rectangle_bgr, (desired_width, desired_height)) # Write the modified frame to the output video output_video.write(resized_frame) # Write the modified frame to the output video # output_video.write(frame_with_rectangle_bgr) if args.show: # Display the modified frame (optional) # cv2.imshow('Modified Frame', frame_with_rectangle_bgr) cv2.imshow('Modified Frame', resized_frame) # Wait for the 'q' key to be pressed to stop (optional) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break # Release the video capture and writer objects and close any open windows video_capture.release() output_video.release() if args.show: cv2.destroyAllWindows() # Print the total frame count print(\"Total frames processed:\", frame_count) print(\"Operation took \", (average/frame_count)) and here to compare the opencv script import cv2 import argparse parser = argparse.ArgumentParser() parser.add_argument('--show', action='store_true',help=\"show the video while making\") # Parse and print the results args = parser.parse_args() print(args) # Path to the input H.264 file input_video_path = '70secsmovie.h264' # Replace with the path to your input H.264 file # Path to the output MP4 file output_video_path = 'output_video.mp4' # Replace with the desired output MP4 file name # Open the input video file video_capture = cv2.VideoCapture(input_video_path) # Check if the video file was opened successfully if not video_capture.isOpened(): print(\"Failed to open the video file.\") exit() # Set the desired width for the output frames desired_width = 800 # Get the video properties frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH)) frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT)) fps = int(video_capture.get(cv2.CAP_PROP_FPS)) codec = cv2.VideoWriter_fourcc(*'mp4v') aspect_ratio = frame_width / frame_height desired_height = int(desired_width / aspect_ratio) # Create a VideoWriter object to save the output video # output_video = cv2.VideoWriter(output_video_path, codec, fps, (frame_width, frame_height)) output_video = cv2.VideoWriter(output_video_path, codec, fps, (desired_width, desired_height)) # Initialize the frame count frame_count = 0 average = 0.0 # Read, process, and write each frame from the input video while True: # Read a frame from the video file ret, frame = video_capture.read() # If the frame was not read successfully, the end of the video file is reached if not ret: break # Increment the frame count frame_count += 1 # Draw a rectangle on the frame (you can modify the rectangle's properties here) x, y, width, height = 100, 100, 200, 150 start = cv2.getTickCount() cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 255, 0), 2) end = cv2.getTickCount() time = (end - start)/ cv2.getTickFrequency() # print(\"Time for Drawing Rectangle using OpenCV\") # print(\"%fs\" % (time)) average = average + time # Resize the frame to the desired width and height while maintaining the aspect ratio resized_frame = cv2.resize(frame, (desired_width, desired_height)) # Write the modified frame to the output video output_video.write(resized_frame) if args.show: # Display the modified frame (optional) cv2.imshow('Modified Frame', resized_frame) # Wait for the 'q' key to be pressed to stop (optional) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break # Release the video capture and writer objects and close any open windows video_capture.release() output_video.release() if args.show: cv2.destroyAllWindows() # Print the total frame count print(\"Total frames processed:\", frame_count) print(\"Operation took \", (average/frame_count)) With that the opencv script takes Total frames processed: 704 Operation took 3.119159232954547e-05 while the pycuda took Total frames processed: 704 Operation took 0.0003763223639266063 How can pycuda (or cuda for that matter) be useful for image processing?",
        "answers": [
            [
                "OpenCV on CPU is equivalent to a few racing cars. They can carry few independent algorithms to target fast. CUDA is a 7-kilometer-long train. It carries thousands at once. So it takes some time for everyone to go into train. Also, you take a bus to reach it first. pcie GPU home ---&gt; bus ---&gt; train ======&gt; NewYork 1200 passengers ======&gt; Detroit 5000 passengers ======&gt; Canada 10000 passengers CPU home ---&gt; 3x cars ~~~~~~&gt; 4 passengers to shopping mall ~~~~~~&gt; 2 passengers to office nearby ~~~~~~&gt; 1 passenger to airport You can always use train(+bus to reach the train) to go shopping but it is slower than using a car. Especially it is not efficient if you are the only passenger in the bus &amp; the train. When you tell the train to move, actually you talk to the driver. It is up to the driver's mercy to respond quick. Some drivers are slow, some are fast. But the hardware, the train, has always a slowness to gain momentum. Once it enables all of its modules, it can process tens of thousands of CUDA threads at once. It is not fair to compare a throughput-optimized device against a latency-optimized device in a latency benchmark. Let's compare the cpu against GPU in a throughput benchmark. RTX4070 can do 17 TB/s bandwidth from its combined L1 caches in a real-world algorithm and at least 15 trillion multiplications/additions per second in another. Unless you give the GPU enough computational/throughput of work, CPU will win everytime.Under fairly optimized algorithms, GPUs tend to be more power-efficient &amp; higher throughput. GPU starts winning when the workload takes more than hundreds of microseconds. Scaling is important. When you double the work, GPU time does not increase much but CPU does increase. Using GPU has these overheads: driver overhead: microsecond(s) pcie overhead: microsecond(s) thread launcher/scheduler overhead: depends on number of threads launched, can be nanoseconds to miliseconds, or more vram overhead: maybe close to a microsecond RAM overhead: nanoseconds CPU has this: cache overhead maybe RAM overhead if cache miss occurs If latency is high priority, CPUs have AVX instructions to complete operations on cache/RAM quick. OpenCV has parallelism support so it likely uses AVX already. You can process an image that is already in CPU cache, at terabytes per second (or megabytes per microsecond) throughput. It is not worthy enough to send it through PCIE to GPU if there is only a single bitshift operation per pixel.There has to be some work at least equal to the overhead of drivers, gpu, etc before deciding to send to GPU."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have the following script that tries to paint a rectangle on a image import cv2 import numpy as np import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule def draw_square(image_gpu, image_width,image_height, x, y, width, height, color): block_dim = (16, 16) # CUDA block dimensions grid_dim_x = (image_width + block_dim[0] - 1) // block_dim[0] # CUDA grid dimensions (x-axis) grid_dim_y = (image_height + block_dim[1] - 1) // block_dim[1] # CUDA grid dimensions (y-axis) mod = SourceModule(\"\"\" __global__ void draw_square_kernel(unsigned char *image, int image_width, int x, int y, int width, int height, unsigned char *color) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; if (row &gt;= y &amp;&amp; row &lt; y + height &amp;&amp; col &gt;= x &amp;&amp; col &lt; x + width) { int pixel_idx = row * image_width * 3 + col * 3; image[pixel_idx] = color[0]; image[pixel_idx + 1] = color[1]; image[pixel_idx + 2] = color[2]; } } \"\"\", no_extern_c=True) draw_square_kernel = mod.get_function(\"draw_square_kernel\") draw_square_kernel(image_gpu, np.int32(image_width), np.int32(x), np.int32(y), np.int32(width), np.int32(height), cuda.In(color, block=block_dim, grid=(grid_dim_x, grid_dim_y))) # Load the image image_path = 'Lena.png' # Replace with the path to your image image = cv2.imread(image_path) # Convert the image to the RGB format image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Upload the image to the GPU image_gpu = cuda.to_device(image_rgb) # Define the square coordinates x, y = 100, 100 # Top-left corner coordinates width, height = 200, 200 # Width and height of the square # Define the color of the square (Green in this example) color = np.array([0, 255, 0], dtype=np.uint8) # Draw a square on the GPU image draw_square(image_gpu, image_rgb.shape[1], image_rgb.shape[0],x, y, width, height, color) # Download the modified image from the GPU image_with_square = np.empty_like(image_rgb) cuda.memcpy_dtoh(image_with_square, image_gpu) # Convert the image back to the BGR format for display image_with_square_bgr = cv2.cvtColor(image_with_square, cv2.COLOR_RGB2BGR) # Display the image with the square cv2.imshow('Image with Square', image_with_square_bgr) cv2.waitKey(0) cv2.destroyAllWindows() However when I try to run it I get python 3_rectangle6.py Traceback (most recent call last): File \"/cbe421fe-1303-4821-9392-a849bfdd00e2/MyStudy/PyCuda/practice/3_rectangle_pycuda3.py\", line 52, in &lt;module&gt; draw_square(image_gpu, image_rgb.shape[1], image_rgb.shape[0],x, y, width, height, color) File \"/cbe421fe-1303-4821-9392-a849bfdd00e2/MyStudy/PyCuda/practice/3_rectangle_pycuda3.py\", line 29, in draw_square draw_square_kernel = mod.get_function(\"draw_square_kernel\") File \"/miniconda3/envs/py39Cuda2/lib/python3.9/site-packages/pycuda/compiler.py\", line 332, in get_function return self.module.get_function(name) pycuda._driver.LogicError: cuModuleGetFunction failed: named symbol not found As you can see this is my sixth attempt, and still draw_square_kernel is not recognized...",
        "answers": [
            [
                "Because you have added the no_extern_c=True option to your SourceModule instance, the code you pass to the compiler is not automagically bracketed with extern \"C\" {} and the resulting compilation is made using C++ rather than C linkage. CUDA uses the Itanium C++ ABI, so the resulting mangled symbol name will be something like: _Z18draw_square_kernelPhiiiiiS_. To get your code to work, you can either remove the no_extern_c=True option (but take the time to understand what the implications of that are), or use the mangled symbol name in the get_function call. You can nvcc or an object analysis tool to get the exact mangled symbol name for your kernel."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have the following start of a code import numpy as np from pycuda import driver, gpuarray from pycuda.compiler import SourceModule import pycuda.autoinit MATRIX_SIZE = 3 matrix_mul_kernel = \"\"\" __global__ void Matrix_Mul_Kernel(float *d_a, float *d_b, float *d_c) { int tx = threadIdx.x; int ty = threadIdx.y; float value = 0; int s=5; printf(\"X %d Y \\\\n\",s); for (int i = 0; i &lt; %(MATRIX_SIZE)s; ++i) { float d_a_element = d_a[ty * %(MATRIX_SIZE)s + i]; float d_b_element = d_b[i * %(MATRIX_SIZE)s + tx]; value += d_a_element * d_b_element; } d_c[ty * %(MATRIX_SIZE)s + tx] = value; } \"\"\" matrix_mul = matrix_mul_kernel % {'MATRIX_SIZE': MATRIX_SIZE} mod = SourceModule(matrix_mul) The part inside the kernel with printf, if I do printf(\"hello\"); it goes fine but when trying to print an integer (I was trying to print tx and ty but never mind, any would be fine) an error appears Traceback (most recent call last): File \"/media/cbe421fe-1303-4821-9392-a849bfdd00e2/MyStudy/PyCuda/9_matrix_mul.py\", line 26, in &lt;module&gt; matrix_mul = matrix_mul_kernel % {'MATRIX_SIZE': MATRIX_SIZE} TypeError: %d format: a number is required, not dict Why is this code failing? Previously when no constant was used, I could print the thread x and y EDIT: Even stranger when I do this printf(\"X %s Y \\\\n\",5); It does not fail but prints this X {'MATRIX_SIZE': 3} Y X {'MATRIX_SIZE': 3} Y X {'MATRIX_SIZE': 3} Y X {'MATRIX_SIZE': 3} Y X {'MATRIX_SIZE': 3} Y X {'MATRIX_SIZE': 3} Y X {'MATRIX_SIZE': 3} Y X {'MATRIX_SIZE': 3} Y X {'MATRIX_SIZE': 3} Y So apparently no matter the variable it is always interpreted as the dictionary {'MATRIX_SIZE': 3} therefore the error. The question is why? what is happening here?",
        "answers": [
            [
                "The issue is that your printf call uses the same string interpolation specifier (%d) used by python's string interpolation. From Python's documentation: When the right argument is a dictionary (or other mapping type), then the formats in the string must include a parenthesised mapping key into that dictionary inserted immediately after the '%' character. To avoid mixing python and cuda's string interpolation, you can use python's newer string formatting. matrix_mul = matrix_mul_kernel.format(MATRIX_SIZE=3) Wherever you need MATRIX_SIZE, use {MATRIX_SIZE}"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have the following pycuda program import pycuda.gpuarray as gpuarray import pycuda.driver as drv import pycuda.autoinit import numpy import time n=100 h_a=numpy.float32(numpy.random.randint(1,5,(1,n))) h_b=numpy.float32(numpy.random.randint(1,5,(1,n))) # n=10 # a=numpy.float32(numpy.random.randint(1,5,(n,n))) # b=numpy.float32(numpy.random.randint(1,5,(n,n))) tic=time.time() # axb=a*b h_result=numpy.sum(h_a*h_b) #print(numpy.dot(a,b)) toc=time.time()-tic print(\"Answer of dot product using numpy\") print(h_result) print(\"Dot Product on CPU\") print(toc,\"s\") start = drv.Event() end=drv.Event() start.record() a_gpu = gpuarray.to_gpu(h_a) b_gpu = gpuarray.to_gpu(h_b) axbGPU = gpuarray.dot(a_gpu,b_gpu) end.record() end.synchronize() secs = start.time_till(end)*1e-3 print(\"Answer of dot product using GPU\") print(axbGPU.get()) print(\"Dot Product on GPU\") print(\"%fs\" % (secs)) if(h_result==axbGPU.get()): print(\"The computed dot product is correct\") WHen I run this program I get python 8_gpu_dot.py Answer of dot product using numpy 611.0 Dot Product on CPU 2.4318695068359375e-05 s Answer of dot product using GPU 611.0 Dot Product on GPU 0.114620s The computed dot product is correct How come the CPU time is so fast while the GPU takes so much time?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have the following program import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void myfirst_kernel() { printf(\"I am in block no: %d thread no: %d \\\\n\", blockIdx.x, threadIdx.x); } \"\"\") function = mod.get_function(\"myfirst_kernel\") function(grid=(10,2),block=(1,1,1)) As you can see I am running 10 blocks and 2 threads per block. However the output is python thread_execution.py I am in block no: 1 thread no: 0 I am in block no: 7 thread no: 0 I am in block no: 1 thread no: 0 I am in block no: 7 thread no: 0 I am in block no: 3 thread no: 0 I am in block no: 0 thread no: 0 I am in block no: 3 thread no: 0 I am in block no: 6 thread no: 0 I am in block no: 9 thread no: 0 I am in block no: 0 thread no: 0 I am in block no: 9 thread no: 0 I am in block no: 6 thread no: 0 I am in block no: 5 thread no: 0 I am in block no: 2 thread no: 0 I am in block no: 5 thread no: 0 I am in block no: 8 thread no: 0 I am in block no: 4 thread no: 0 I am in block no: 2 thread no: 0 I am in block no: 8 thread no: 0 I am in block no: 4 thread no: 0 I was expecting threadIdx.x would give me 1 too. Why is always 0?",
        "answers": [
            [
                "You are not running multiple threads per block. This: function(grid=(10,2),block=(1,1,1)) launches a grid of 10 x 2 blocks, each of one thread each. threadIdx.x will be zero in each case, with blockIdx.x varying between 0 and 9 (as shown in your output), and blockIdx.y varying between 0 and 1 (not shown in your output but the reason there are two outputs per value of blockIdx.x)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm running a test under python to calculate sha256 for a range between 0x763ecdbd1409000 through 0x763ecdbf0000000 This can be done easily with hashlib, but as I'm testing against speed, I would like to try pycuda for huge performance boost. The problem is that every search results I've came with, lack of what I mentioned before. Any advise/help will be appreciated.",
        "answers": [],
        "votes": []
    },
    {
        "question": "How can i solve the problem : Segmentation fault (core dumped) when importing pycuda: \"import pycuda.autoinit\" I tried to debug the problem with gdb and i get the following output: `(gdb) r -c \"import pycuda.autoinit\" Starting program: /usr/bin/python3 -c \"import pycuda.autoinit\" [Thread debugging using libthread_db enabled] Using host libthread_db library \"/lib/aarch64-linux-gnu/libthread_db.so.1\". [New Thread 0x7fb5d691f0 (LWP 8796)] [New Thread 0x7fb55681f0 (LWP 8797)] [New Thread 0x7fb2d671f0 (LWP 8798)] [New Thread 0x7fb15661f0 (LWP 8799)] [New Thread 0x7fafd651f0 (LWP 8800)] [New Thread 0x7fae5641f0 (LWP 8801)] [New Thread 0x7facd631f0 (LWP 8802)] Thread 1 \"python3\" received signal SIGSEGV, Segmentation fault. 0x0000007faa6eca48 in ?? () (gdb) backtrace #0 0x0000007faa6eca48 in ?? () #1 0x0000007faa4f907c in ?? () from /usr/lib/aarch64-linux-gnu/libapt-pkg.so.5.0 #2 0x0000007faa4f9dec in pkgInitSystem(Configuration&amp;, pkgSystem*&amp;) () from /usr/lib/aarch64-linux-gnu/libapt-pkg.so.5.0 #3 0x0000007faa748364 in ?? () from /usr/lib/python3/dist-packages/apt_pkg.cpython-36m-aarch64-linux-gnu.so #4 0x00000000005bcd24 in _PyCFunction_FastCallDict () #5 0x000000000052ca48 in ?? () #6 0x0000000000531698 in _PyEval_EvalFrameDefault () #7 0x000000000052c0e0 in ?? () #8 0x000000000053a7a4 in ?? () #9 0x00000000005bd008 in PyCFunction_Call () #10 0x00000000005340b4 in _PyEval_EvalFrameDefault () #11 0x000000000052c0e0 in ?? () #12 0x000000000052c674 in ?? () #13 0x000000000052c8cc in ?? () #14 0x0000000000531698 in _PyEval_EvalFrameDefault () #15 0x000000000052a950 in ?? () #16 0x000000000052c8cc in ?? () #17 0x0000000000531698 in _PyEval_EvalFrameDefault () #18 0x000000000052a950 in ?? () #19 0x000000000052c8cc in ?? () ---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---` I tried also with faulthandler and i got the following output: $ python3 Python 3.6.9 (default, Mar 10 2023, 16:46:00) [GCC 8.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import faulthandler &gt;&gt;&gt; faulthandler.enable() &gt;&gt;&gt; import pycuda.autoinit Fatal Python error: Segmentation fault Current thread 0x0000007f7f450010 (most recent call first): File \"/usr/lib/python3/dist-packages/apt/__init__.py\", line 35 in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678 in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 665 in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 955 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"/usr/lib/python3/dist-packages/apport/packaging_impl.py\", line 24 in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678 in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 665 in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 955 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"/usr/lib/python3/dist-packages/apport/fileutils.py\", line 26 in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678 in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 665 in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 955 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"/usr/lib/python3/dist-packages/apport/report.py\", line 30 in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678 in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 665 in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 955 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"/usr/lib/python3/dist-packages/apport/__init__.py\", line 5 in &lt;module&gt; File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678 in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 665 in _load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 955 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"&lt;frozen importlib._bootstrap&gt;\", line 219 in _call_with_frames_removed File \"&lt;frozen importlib._bootstrap&gt;\", line 941 in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 971 in _find_and_load File \"/usr/lib/python3/dist-packages/apport_python_hook.py\", line 72 in apport_excepthook Segmentation fault (core dumped) Environment Nvidia card:Jetson AGX Xavier TensorRT Version : 8.0.1.6 CUDA Version : 10.2.300 CUDNN Version : 8.2.1.32 Operating System + Version : Ubuntu 18.04 Any help please",
        "answers": [],
        "votes": []
    },
    {
        "question": "Code: from ultralytics import YOLO # Load a model model = YOLO(\"yolov8m.yaml\") # build a new model from scratch # Use the model results = model.predict(source=\"0\", show=True, stream=True, classes=0, device='0') model.train(data=\"conf.yaml\", epochs=20, imgsz=600, device=' 0') # train the model Error Eror:NotImplementedError: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher]. File \"C:\\Users\\pogti\\Downloads\\dataset_rust_ai\\main.py\", line 9, in &lt;module&gt; model.train(data=\"conf.yaml\", epochs=20, imgsz=600, device=' 0') # train the model If I do below, I get: True import torch print(torch.cuda.is_available())",
        "answers": [
            [
                "All good i delete cuda and install. And all work if soem need my code: from ultralytics import YOLO if name == 'main': # Load a model model = YOLO(\"yolov8m.yaml\") # build a new model from scratch # Use the model results = model.predict(source=\"0\", show=True, stream=True, classes=0, device='0') model.train(data=\"conf.yaml\", epochs=150, imgsz=600, device='0') # train the model"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to do a LU decomposition of a square matrix A by using scikit-cuda and pycuda. I tried a few demo code from the scikit-cuda github website and all seemed fine. However, when I tried to call the low-level interface cublas.cublasDgetrfBatched in my little example, I failed with the error code below: PyCUDA WARNING: a clean-up operation failed (dead context maybe?) cuMemFree failed: an illegal memory access was encountered Here is my little python code import numpy as np import pycuda.autoinit import skcuda.cublas as cublas import pycuda.gpuarray as gpuarray N = 10 N_BATCH = 1 # only 1 matrix to be decomposed A_SHAPE = (N, N) a = np.random.rand(*A_SHAPE).astype(np.float64) a_batch = np.expand_dims(a, axis=0) a_gpu = gpuarray.to_gpu(a_batch.T.copy()) # transpose a to follow \"F\" order p_gpu = gpuarray.zeros(N * N_BATCH, np.int32) info_gpu = gpuarray.zeros(N_BATCH, np.int32) cublas_handle = cublas.cublasCreate() cublas.cublasDgetrfBatched( cublas_handle, N, a_gpu.gpudata, N, p_gpu.gpudata, info_gpu.gpudata, N_BATCH, ) cublas.cublasDestroy(cublas_handle) print(a_gpu) I am a novice user of scikit-cuda. So, could someone give me a hand?",
        "answers": [
            [
                "Like @talonmies commented, a pointer to an array of address of matrix on the device should be used. import numpy as np import pycuda.autoinit import skcuda.cublas as cublas import pycuda.gpuarray as gpuarray N = 10 N_BATCH = 1 # only 1 matrix to be decomposed A_SHAPE = (N, N) a = np.random.rand(*A_SHAPE).astype(np.float64) a_batch = np.expand_dims(a, axis=0) a_gpu = gpuarray.to_gpu(a_batch.T.copy())) # transpose a to follow \"F\" order # use np.array won't work instead of gpuarray.to_gpu. # .ptr is pointer to matrix a on the device # can be further revised to programmatically create an array of pointers a_gpu_batch = gpuarray.to_gpu(np.asarray([a_gpu.ptr]) p_gpu = gpuarray.zeros(N * N_BATCH, np.int32) info_gpu = gpuarray.zeros(N_BATCH, np.int32) cublas_handle = cublas.cublasCreate() cublas.cublasDgetrfBatched( cublas_handle, N, a_gpu_batch.gpudata, N, p_gpu.gpudata, info_gpu.gpudata, N_BATCH, ) cublas.cublasDestroy(cublas_handle) print(a_gpu)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to use cuda to make a basic fragment shader, and I have found that actually executing the kernel takes over a second, which is unacceptable for a shader that I'm trying to run in real time. I found using the synchronize method and by commenting some of the kernel that it is the memory accesses to the output array that are what's causing it to be so slow. I haven't really tried anything to solve the problem because I can't even fathom where to start. This is in PyCUDA, which I don't think really matters, but here's the kernel code: __global__ void fragment_shader(int palette_lim,float *palette, float *input, float *output) { int fragment_idx = (3*gridDim.y*blockIdx.x)+(3*blockIdx.y); float min_dist = sqrtf(3); float color_dist; int best_c = 0; for (int c=0;c&lt;palette_lim;c++) { color_dist = sqrtf(pow(input[fragment_idx]-palette[c*3],2)+pow(input[fragment_idx+1]-palette[c*3+1],2)+pow(input[fragment_idx+2]-palette[c*3+2],2)); if (color_dist &lt; min_dist) { min_dist = color_dist; best_c = c; } } //These are the lines that make it slow. If these lines get commented out, it runs in a time that would be acceptable output[fragment_idx] = palette[best_c*3]; output[fragment_idx+1] = palette[best_c*3+1]; output[fragment_idx+2] = palette[best_c*3+2]; } EDIT: After playing around with it a bit more, I found that it also has to do with what's being assigned to the output array, because when I had it write some constants rather than something from the palette it also worked just fine, it just didn't do anything useful then.",
        "answers": [
            [
                "First some remarks on your actual computation: You compare sqrtf(x) &lt; sqrtf(3). Roots are expensive. Just compare x &lt; 3.f Even if you want to keep the square root to avoid overflowing the float range (probably not a concern), don't use sqrt(pow(x, 2)+...), for that matter don't use pow just for squaring. Use hypotf for 2D or norm3df for 3D vectors You save the last value in your palette that is below the limit. It very much looks like you want to fit the best color Now let's analyze your memory accesses: fragment index Let's look at fragment_idx = 3*gridDim.y*blockIdx.x+3*blockIdx.y: You're not taking threadIdx.x and threadIdx.y into account. This is your main problem: Many threads act on the same input and output data. You likely want this: fragment_idx = 3 * (threadIdx.y * blockDim.x + threadIdx.x) input So you load 3 floats. For starters, why do you reload it inside the loop when it isn't dependent on the loop iteration? I assume the compiler saves you from that access but don't get in the habit of doing that. Second, your access pattern isn't properly coalesced since a) these are 3 independent accesses and b) CUDA cannot coalesce accesses to float3 vectors even if you did it properly. Please read section 9.2.1 Coalesced Access to Global Memory of the Best Practices Guide. For better performance you have two options: You add 1 float per fragment_idx so you can load the whole thing as a float4 You transpose the your input array from a Nx3 matrix into an 3xN matrix palette Same problem with the access of 3 floats. Plus, now every thread reads the same values since c doesn't depend on the thread index. At the very least, the access should go through the __ldg function to use the L1 cache. Preferably you prefetch the palette into shared memory output The write access has the same issue with uncoalesced access. Plus, since best_c varies among threads, the read accesses to palette are random. You had to load the palette values before in your loop. Just save the best palette value in a local variable and reuse it to store the output in the end. Methodology Two remarks: Try to make your code valid before making it fast. That would have caught the fragment_idx If you simplify the code such as removing the output, the compiler will happily optimize most of your code away. That's not how you do proper performance assessment. Use a profiler. CUDA comes with very good ones Minimal fix This is the simplest code to rectify the issues. It doesn't solve the issues with loading vector3 variables and it doesn't use shared memory. That requires more involved changes __device__ float sqr_norm(float3 a, float3 b) { a.x -= b.x, a.y -= b.y, a.z -= b.z; return a.x * a.x + a.y * a.y + a.z * a.z; } __global__ void fragment_shader(int palette_lim, const float *palette, const float *input, float *output) { int fragment_idx = 3 * (threadIdx.y * blockDim.x + threadIdx.x); /* TODO: Switch to float4 for better memory access patterns */ float3 inputcolor = make_float3( input[fragment_idx], input[fragment_idx + 1], input[fragment_idx + 2]); float min_dist_sqr = 3.f; /* The old code always used color index 0 if there was no fit */ float3 best_color = make_float3( __ldg(palette), __ldg(palette + 1), __ldg(palette + 2)); float best_dist = sqr_norm(best_color, inputcolor); for(int c = 1; c &lt; palette_lim; c++) { /* TODO: Prefetch into shared memory */ float3 color = make_float3( __ldg(palette + c), __ldg(palette + c + 1), __ldg(palette + c + 2)); float dist = sqr_norm(color, inputcolor); /* Since we always used color 0 in the old code, * the min_dist is somewhat pointless */ if(dist &lt; min_dist_sqr &amp;&amp; dist &lt; best_dist) { best_color = color; best_dist = dist; } } output[fragment_idx] = best_color.x; output[fragment_idx + 1] = best_color.y; output[fragment_idx + 2] = best_color.z; } Extensive fix Here is a more extensive rewrite: All arrays are changed to float4 (RGBA instead of RGB). The extra channel is ignored for distance computation but it is propagated. Typically one tries to use the value for something, e.g. you could store the distance value in there Shared memory is used to buffer the color palette. The dynamic shared memory requirement is outlined in the code comments __device__ float sqr_dist_rgb(float4 a, float4 b) { a.x -= b.x, a.y -= b.y, a.z -= b.z; return a.x * a.x + a.y * a.y + a.z * a.z; } __global__ void fragment_shader(int palette_lim, const float4 *palette, const float4 *input, float4 *output) { /* Call with dynamic shared memory: * 2 * sizeof(float4) * blockDim.x * blockDim.y */ extern __shared__ float4 colorbuf[]; const int buf_size = blockDim.x * blockDim.y; const int buf_idx = threadIdx.y * blockDim.x + threadIdx.x; const int fragment_idx = threadIdx.y * blockDim.x + threadIdx.x; const float4 inputcolor = input[fragment_idx]; float4 best_color = __ldg(palette); const float min_dist_sqr = 3.f; float best_dist = sqr_dist_rgb(best_color, inputcolor); for(int cb = 0, b = 0; cb &lt; palette_lim; b ^= 1, cb += buf_size) { /* We use a double buffer scheme to reduce the __syncthreads calls */ float4* cur_buf = b ? colorbuf + buf_size : colorbuf; if(cb + buf_idx &lt; palette_lim) cur_buf[buf_idx] = __ldg(palette + cb + buf_idx); __syncthreads(); const int n = min(buf_size, palette_lim - cb); for(int c = 0; c &lt; n; c++) { float4 color = cur_buf[c]; float dist = sqr_dist_rgb(color, inputcolor); if(dist &lt; min_dist_sqr &amp;&amp; dist &lt; best_dist) { best_color = color; best_dist = dist; } } } output[fragment_idx] = best_color; } Algorithmic improvements For larger palettes, a brute force search like this is suboptimal. Spatial index algorithms can do the same thing but faster. The classic structure for this would be a KD tree. If you search for this, you will find some CUDA implementations that might be worth checking out."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I have the following script to split video into chunks and get certain number of chunks. It works good but uses cpu instead of my RTX 3090 GPU How can I make it use GPU in the final render? Thank you so much for answers import math import moviepy.editor as mp import subprocess import sys # Define the input video file path and desired output duration input_file = \"install_second_part_2x.mp4\" output_duration = 158 # in seconds # Load the input video file into a moviepy VideoFileClip object video = mp.VideoFileClip(input_file) # Calculate the number of 5-second chunks in the video chunk_duration = 5 num_chunks = math.ceil(video.duration / chunk_duration) # Create a list to hold the selected chunks selected_chunks = [] # Calculate the number of chunks to select output_num_chunks = math.ceil(output_duration / chunk_duration) # Calculate the stride between adjacent chunks to select stride = num_chunks // output_num_chunks # Loop through the chunks and select every stride-th chunk for i in range(num_chunks): if i % stride == 0: chunk = video.subclip(i*chunk_duration, (i+1)*chunk_duration) selected_chunks.append(chunk) progress = i / num_chunks * 100 sys.stdout.write(f\"\\rProcessing: {progress:.2f}%\") sys.stdout.flush() # Keep adding chunks until the output duration is reached output = selected_chunks[0] for chunk in selected_chunks[1:]: if output.duration &lt; output_duration: output = mp.concatenate_videoclips([output, chunk]) else: break # Write the output video to a file output_file = \"install_part_cut.mp4\" output.write_videofile(output_file)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am a new Rapids learner. I installed a Rapids-23.02 framework with 6G GPU, and 32G RAM on ubantu . When I run a program that only uses rapids for acceleration, the GPU memories only used 3G (nvidia-smi), and RAM memory only used 7.5G. I have tried the torch.mutiprossoer, But there always be memory overload and shut down. Is there a valuable reference case for pycuda multithreading. So, what other approach can provide memory utilization to obtain twice or three times the speed (considering GPU memory). I also want to know, whether API \uff08the shortest path between the SOURCE and TARGET\uff09 has been published in curgraph 23.02 for python. sorry, I just can't find it in the document.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have been trying to parallelize a code of mine using pycuda. I need to initialize 10^5 threads with each thread running around 4000 iterations. This should be well withing the block and grid limits of my GPU (grid = (98,1,1), block = (1024,1,1)). However executing the program gives me the following error: \"cuLaunchKernel failed: too many resources requested for launch\" Here's the code (please don't worry about the cuda kernel functions, I have tested them separately in a .cu file and they work completely fine): import numpy as np import matplotlib.pyplot as plt import pycuda.driver as cuda import pycuda.gpuarray as gpuarray from pycuda.compiler import SourceModule import pycuda.autoinit mod = SourceModule(\"\"\" #include&lt;math.h&gt; __device__ void iterate(double r,double *x,double *y,int n){ for(int i=0;i&lt;n;i++){ *x = r * (3 * *y + 1) * *x * (1 - *x); *y = r * (3 * *x + 1) * *y * (1 - *y); } } __global__ void calc_lyap(double* arr,double* lyap,int n){ int blocknum = blockIdx.z * (gridDim.x * gridDim.y) + blockIdx.y * (gridDim.x) + blockIdx.x; int threadnum = threadIdx.z * (blockDim.x * blockDim.y) + threadIdx.y * (blockDim.x) + threadIdx.x; int index = blocknum * (blockDim.x * blockDim.y * blockDim.z) + threadnum; double d0 = pow(10,-12); double r = arr[index]; double x1=0.1,y1=0.1; iterate(r,&amp;x1,&amp;y1,1000); double x2 = x1, y2 = x1 + d0; double sum=0; for(int i=0;i&lt;n;i++){ iterate(r,&amp;x1,&amp;y1,1); iterate(r,&amp;x2,&amp;y2,1); double d1 = sqrt(pow((x1-x2),2) + pow((y1-y2),2)); if(d1!=0){ sum+=log2(d1/d0); } x2 = x1 + d0 * (x2 - x1) / d1; y2 = y1 + d0 * (y2 - y1) / d1; } sum = sum/n; lyap[index] = sum; } \"\"\") lyap = mod.get_function(\"calc_lyap\") arr_d = gpuarray.to_gpu(np.linspace(0.4,1.2,10**5)) lyap_d = gpuarray.to_gpu(np.zeros(10**5)) n = gpuarray.to_gpu(np.array([3000])) lyap(arr_d,lyap_d,n[0],grid=(10**5//1024+1,1,1),block=(1024,1,1)) lyap_ = lyap_d.get() print(lyap_) I tried reducing the size of the problem to just a sample i.e I changed 10^5 to just 10 and the block and grid dimensions to grid=(1,1,1) and block=(10,1,1) but it still yields the same error. Python version: 3.10.8 Pycuda version: 2022.2.2 Compiler version: nvcc 11.8.89 OS: Windows GPU: Nvidia RTX 3050 Mobile Laptop GPU",
        "answers": [
            [
                "Going through the CUDA Documentation I found the docs for \"CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES = 701\", which mentions that this error not only occurs when you have too many arguments but also when your arguments are of the wrong type i.e passing int64 values when you have used \"int\" in c which is typically 32 bytes. My mistake here lied in the initialization of the variable n I pass as an parameter. n = gpuarray.to_gpu(np.array([3000])) The first mistake was that np.array automatically intializes your array to float64, moreover the error still persisted when I changed the code to: n = gpuarray.to_gpu(np.array([3000]).astype(np.int32)) However it finally worked when I initialized n as, n = np.int32(3000) lyap(arr_d,lyap_d,n,grid=(10**5//1024+1,1,1),block=(1024,1,1)) So my mistake was that I passed a parameter of the wrong type, although I dont understand why it would work when I initialized it as an int32 array and passed the index 0 value as the parameter. I'm assuming it has something to do with how pycuda and numpy store their array elements."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "So I try python -m pip install pycuda and it fails (Here's some of the output from the failed install): Building wheel for pycuda (pyproject.toml) ... error error: subprocess-exited-with-error WARNING: nvcc not in path. ERROR: Failed building wheel for pycuda Failed to build pycuda ERROR: Could not build wheels for pycuda, which is required to install pyproject.toml-based projects I have installed the Visual Studio 2022 build tools but I have not added anything to my path, which is all I figure I have todo but I don't know what I have to add. The pycuda wiki's installation page specifies C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\Common7\\IDE; but I don't have those directories. Here's the tree of my visual studio install D:\\VisualStudioThings\\install\\VC. (I cannot include the tree for the whole install because it's too much) Common7 CoreCon DIA SDK ImportProjects Licenses Microsoft Azure Tools MSBuild SDK Team Tools VB VC VC# VSSDK Xml VC \u251c\u2500\u2500\u2500Auxiliary \u2502 \u251c\u2500\u2500\u2500Build \u2502 \u2514\u2500\u2500\u2500VS \u2502 \u251c\u2500\u2500\u2500include \u2502 \u2502 \u2514\u2500\u2500\u2500CppCoreCheck \u2502 \u251c\u2500\u2500\u2500lib \u2502 \u2502 \u251c\u2500\u2500\u2500arm \u2502 \u2502 \u251c\u2500\u2500\u2500onecore \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500arm \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500x64 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500x86 \u2502 \u2502 \u251c\u2500\u2500\u2500x64 \u2502 \u2502 \u2514\u2500\u2500\u2500x86 \u2502 \u2514\u2500\u2500\u2500UnitTest \u2502 \u251c\u2500\u2500\u2500include \u2502 \u2502 \u251c\u2500\u2500\u2500UWP \u2502 \u2502 \u2514\u2500\u2500\u2500v150 \u2502 \u2514\u2500\u2500\u2500lib \u2502 \u251c\u2500\u2500\u2500ARM \u2502 \u251c\u2500\u2500\u2500ARM64 \u2502 \u251c\u2500\u2500\u2500UWP \u2502 \u2502 \u251c\u2500\u2500\u2500arm \u2502 \u2502 \u251c\u2500\u2500\u2500arm64 \u2502 \u2502 \u2514\u2500\u2500\u2500x64 \u2502 \u251c\u2500\u2500\u2500x64 \u2502 \u2514\u2500\u2500\u2500x86 \u251c\u2500\u2500\u2500Redist \u2502 \u2514\u2500\u2500\u2500MSVC \u2502 \u251c\u2500\u2500\u250014.34.31931 \u2502 \u2502 \u251c\u2500\u2500\u2500Auxiliary \u2502 \u2502 \u251c\u2500\u2500\u2500debug_nonredist \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500x64 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.DebugCRT \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.DebugCXXAMP \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.DebugOpenMP \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Microsoft.VC143.OpenMP.LLVM \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500x86 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.DebugCRT \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.DebugCXXAMP \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.DebugOPENMP \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Microsoft.VC143.OpenMP.LLVM \u2502 \u2502 \u251c\u2500\u2500\u2500onecore \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500debug_nonredist \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500x64 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.DebugCRT \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Microsoft.VC143.DebugOpenMP \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500x86 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.DebugCRT \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Microsoft.VC143.DebugOPENMP \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500x64 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.CRT \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Microsoft.VC143.OpenMP \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500x86 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.CRT \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Microsoft.VC143.OPENMP \u2502 \u2502 \u251c\u2500\u2500\u2500x64 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.CRT \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.CXXAMP \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Microsoft.VC143.OpenMP \u2502 \u2502 \u2514\u2500\u2500\u2500x86 \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.CRT \u2502 \u2502 \u251c\u2500\u2500\u2500Microsoft.VC143.CXXAMP \u2502 \u2502 \u2514\u2500\u2500\u2500Microsoft.VC143.OPENMP \u2502 \u2514\u2500\u2500\u2500v143 \u2514\u2500\u2500\u2500Tools \u251c\u2500\u2500\u2500Llvm \u2502 \u251c\u2500\u2500\u2500bin \u2502 \u2514\u2500\u2500\u2500x64 \u2502 \u2514\u2500\u2500\u2500bin \u2514\u2500\u2500\u2500MSVC \u2514\u2500\u2500\u250014.34.31933 \u251c\u2500\u2500\u2500Auxiliary \u251c\u2500\u2500\u2500bin \u2502 \u251c\u2500\u2500\u2500Hostx64 \u2502 \u2502 \u251c\u2500\u2500\u2500x64 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u25001033 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500onecore \u2502 \u2502 \u2514\u2500\u2500\u2500x86 \u2502 \u2502 \u2514\u2500\u2500\u25001033 \u2502 \u2514\u2500\u2500\u2500Hostx86 \u2502 \u251c\u2500\u2500\u2500x64 \u2502 \u2502 \u2514\u2500\u2500\u25001033 \u2502 \u2514\u2500\u2500\u2500x86 \u2502 \u2514\u2500\u2500\u25001033 \u251c\u2500\u2500\u2500crt \u2502 \u2514\u2500\u2500\u2500src \u2502 \u251c\u2500\u2500\u2500concrt \u2502 \u251c\u2500\u2500\u2500i386 \u2502 \u251c\u2500\u2500\u2500linkopts \u2502 \u251c\u2500\u2500\u2500stl \u2502 \u251c\u2500\u2500\u2500vccorlib \u2502 \u251c\u2500\u2500\u2500vcruntime \u2502 \u2514\u2500\u2500\u2500x64 \u251c\u2500\u2500\u2500include \u2502 \u251c\u2500\u2500\u2500cliext \u2502 \u251c\u2500\u2500\u2500CodeAnalysis \u2502 \u251c\u2500\u2500\u2500cvt \u2502 \u251c\u2500\u2500\u2500experimental \u2502 \u251c\u2500\u2500\u2500fuzzer \u2502 \u251c\u2500\u2500\u2500Manifest \u2502 \u251c\u2500\u2500\u2500msclr \u2502 \u2502 \u2514\u2500\u2500\u2500com \u2502 \u2514\u2500\u2500\u2500sanitizer \u2514\u2500\u2500\u2500lib \u251c\u2500\u2500\u2500onecore \u2502 \u251c\u2500\u2500\u2500arm \u2502 \u251c\u2500\u2500\u2500x64 \u2502 \u2514\u2500\u2500\u2500x86 \u251c\u2500\u2500\u2500x64 \u2502 \u251c\u2500\u2500\u2500onecore \u2502 \u251c\u2500\u2500\u2500store \u2502 \u2514\u2500\u2500\u2500uwp \u2514\u2500\u2500\u2500x86 \u251c\u2500\u2500\u2500onecore \u251c\u2500\u2500\u2500store \u2502 \u2514\u2500\u2500\u2500references \u2514\u2500\u2500\u2500uwp So my question is what do I add to my path?",
        "answers": [
            [
                "I can reproduce your issue: Since you are based on windows, you can try the below steps: pip install --upgrade pip pip install --upgrade setuptools wheel If the above steps still don't make it work on your side, I think the issue should comes from the source doesn't have whl file. Because pip will search package from https://pypi.org/, but in this place, I don't see the package pycuda has whl there. Based on this, I found whl file of pycuda from other place, download it and install the package via this: python -m pip install packagefilename.whl It works well on my side, I can install it at the end:"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I tried using std::tuple in my kernel code, but received many error: this declaration may not have extern \"C\" linkage errors that pointed to utility and tuple It complains on the include. The following repros for me. from pycuda.compiler import SourceModule mod = SourceModule(\"\"\"#include &lt;tuple&gt;\"\"\") Do I need to do something special in my kernel code or in my Python code to specify I want to use the C++ compiler? Cuda version: 11.8 PyCuda version: 2022.2.1",
        "answers": [
            [
                "Do I need to do something special in my kernel code or in my Python code to specify I want to use the C++ compiler? To be clear, you are using the C++ compiler. But PyCUDA automagically wraps the code you pass into a SourceModule instance in extern \u201cC\u201d unless you explicitly tell it not to: Unless no_extern_c is True, the given source code is wrapped in extern \u201cC\u201d { \u2026 } to prevent C++ name mangling. The underlying reason from a C++ perspective is that templated instances of types and functions can\u2019t resolve with C linkage, thus the error. However, even after you fix that problem, prepared to be disappointed. CUDA supports a lot of C++ language features, but it doesn\u2019t support the standard library and you can\u2019t use std::tuple within kernel code. NVIDIA does provide their own (very limited) reimplementation of the C++ standard library, and it does have a basic tuple type. That might work for you."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I want to run simple code in pycuda import numpy as np import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\",arch='sm_61') I have chosen arch='sm_61' because if I remove it this error appears: nvcc fatal : Value 'sm_86' is not defined for option 'gpu-architecture' I have cuda 10.1 and cuda 11.2 and the default nvcc version is V10.1. I don't want to remove any of them. (someone suggested this as a solution) now I get this error: pycuda._driver.LogicError: cuModuleLoadDataEx failed: device kernel image is invalid - how can I solve this issue? The OS is windows 10. pycuda 2022.2 Python 3.7.4 nvcc version: V10.1.105 I will change the pycuda version or python version if needed.",
        "answers": [],
        "votes": []
    },
    {
        "question": "when I install pcyuda this error shows up; error: command 'x86_64-linux-gnu-gcc' failed with exit status 1 I am following according to these steps : https://wiki.tiker.net/PyCuda/Installation/Linux/Ubuntu/",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to install pycuda with pip install pycuda in virtual environment. I installed python3.10-distutils, but the error remains. My system Ubuntu22.04, python3.10. I also have tried manual installation with instructions from https://wiki.tiker.net/PyCuda/Installation/Linux/. It also failed with ModuleNotFoundError: No module named 'setuptools.command.build_ext'. Any suggestions? Collecting pycuda Using cached pycuda-2022.1.tar.gz (1.7 MB) Installing build dependencies ... done Getting requirements to build wheel ... done Preparing metadata (pyproject.toml) ... done Collecting pytools&gt;=2011.2 Using cached pytools-2022.1.12.tar.gz (70 kB) Preparing metadata (setup.py) ... error error: subprocess-exited-with-error \u00d7 python setup.py egg_info did not run successfully. \u2502 exit code: 1 \u2570\u2500&gt; [32 lines of output] Traceback (most recent call last): File \"&lt;string&gt;\", line 2, in &lt;module&gt; File \"&lt;pip-setuptools-caller&gt;\", line 34, in &lt;module&gt; File \"/tmp/pip-install-s4dqljda/pytools_9b2a0e1beabb402e9d87af0960b817ca/setup.py\", line 14, in &lt;module&gt; setup(name=\"pytools\", File \"/home/konstantin/Yandex.Disk/code/env/lib/python3.10/site-packages/setuptools/__init__.py\", line 87, in setup return distutils.core.setup(**attrs) File \"/home/konstantin/Yandex.Disk/code/env/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 172, in setup ok = dist.parse_command_line() File \"/home/konstantin/Yandex.Disk/code/env/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 474, in parse_command_line args = self._parse_command_opts(parser, args) File \"/home/konstantin/Yandex.Disk/code/env/lib/python3.10/site-packages/setuptools/dist.py\", line 1107, in _parse_command_opts nargs = _Distribution._parse_command_opts(self, parser, args) File \"/home/konstantin/Yandex.Disk/code/env/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 533, in _parse_command_opts cmd_class = self.get_command_class(command) File \"/home/konstantin/Yandex.Disk/code/env/lib/python3.10/site-packages/setuptools/dist.py\", line 954, in get_command_class self.cmdclass[command] = cmdclass = ep.load() File \"/usr/lib/python3.10/importlib/metadata/__init__.py\", line 171, in load module = import_module(match.group('module')) File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"&lt;frozen importlib._bootstrap&gt;\", line 1050, in _gcd_import File \"&lt;frozen importlib._bootstrap&gt;\", line 1027, in _find_and_load File \"&lt;frozen importlib._bootstrap&gt;\", line 1006, in _find_and_load_unlocked File \"&lt;frozen importlib._bootstrap&gt;\", line 688, in _load_unlocked File \"&lt;frozen importlib._bootstrap_external&gt;\", line 883, in exec_module File \"&lt;frozen importlib._bootstrap&gt;\", line 241, in _call_with_frames_removed File \"/home/konstantin/Yandex.Disk/code/env/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 24, in &lt;module&gt; from setuptools.command.sdist import sdist File \"/home/konstantin/Yandex.Disk/code/env/lib/python3.10/site-packages/setuptools/command/sdist.py\", line 2, in &lt;module&gt; import distutils.command.sdist as orig ModuleNotFoundError: No module named 'distutils.command.sdist' [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed \u00d7 Encountered error while generating package metadata. \u2570\u2500&gt; See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I recently updated CUDA to 11.6 and now when I try to use pyCuda I get pycuda.driver.CompileError: nvcc compilation of C:\\Users\\imsog\\AppData\\Local\\Temp\\tmpkgtu92cq\\kernel.cu failed [command: nvcc --cubin -arch sm_75 -m64 -Ic:\\users\\imsog\\anaconda3\\lib\\site-packages\\pycuda\\cuda kernel.cu] [stdout: nvcc fatal : nvcc cannot find a supported version of Microsoft Visual Studio. Only the versions between 2017 and 2019 (inclusive) are supported! The nvcc flag '-allow-unsupported-compiler' I already have VS2019 installed though. The newest version of tensorflow runs fine. I also removed all Cuda 11.6 files and reinstalled it. Still the same error. In my path variables related to VS I have - C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64 C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.29.30133\\bin\\Hostx64\\x64 C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE I have verified that pyCuda is compiling with Cuda 11.6. I just can't figure out why it can't find what it needs in VS. I have verified that removing Cuda 11.6 and going back to the old version allows everything to run fine. So it's finding the visual studio files it needs for older versions but fails to for VS2019",
        "answers": [
            [
                "I still can't figure this out but I figured out how to use the unsupported compiler flag with pycuda and it actually works. In the PyCuda folder open compiler.py and go to line 129. Change cmdline = [nvcc, \"--\" + target] + options + [cu_file_name] To cmdline = [nvcc, \"--\" + target] + [\"-allow-unsupported-compiler\"] + options + [cu_file_name]"
            ],
            [
                "I just resolved this. I was following the last recommend in this link: https://forums.developer.nvidia.com/t/cuda-10-1-vs2019-environment-problem/73453/3 and install VC 2019 into a short path like this C:\\VS19, then open a prompt in C:\\VS19\\VC\\Auxiliary\\Build and run vcvarsall.bat amd64 now you can test nvcc in this prompt to see if it works. but through this way, you would have to work on that prompt."
            ]
        ],
        "votes": [
            3.0000001,
            2.0000001
        ]
    },
    {
        "question": "I have retrained a model with tensorflow v2 and I want it to run on a Jetson Nano GPU. For that I had to save the model from .h5 as .pb then to .onnx and then to .trt (for which I also had to make the conversion to onnx with opset 12). Now when I can finally run the model, I am reusing an old code that used to work with the old .trt model but at locking a page: import pycuda.driver as cuda .... for binding in engine: size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size dtype = trt.nptype(engine.get_binding_dtype(binding)) host_mem = cuda.pagelocked_empty(size, dtype) device_mem = cuda.mem_alloc(host_mem.nbytes) This results in an error: pycuda_driver.LogicError: cuMemAlloc failed: invalid argument After further debugging it turns out cuda.pagelocked_empty(size, dtype) retuns [] at the output binding separable_conv2d_29 with size=0 and dtype=numpy.float32. With the running code, the size is &gt;0 for both input and output bindings.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have previously import pycuda.autoinit module but when I try to run the code I still get an error 4 # Initialize CUDA ----&gt; 5 cuda.init() 6 7 from pycuda.tools import make_default_context # noqa: E402 RuntimeError: cuInit failed: no CUDA-capable device is detected",
        "answers": [
            [
                "Be sure to have the proper CUDA version installed in your PC, different hardware need different version of CUDA installed. You can check from: https://developer.nvidia.com/cuda-gpus"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Receiving an error during pip pycuda installation on ubuntu as follows: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1 note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for pycuda Failed to build pycuda ERROR: Could not build wheels for pycuda, which is required to install pyproject.toml-based projects",
        "answers": [
            [
                "I solved the problem by doing: For Python 2.x use: sudo apt-get install python-dev For Python 2.7 use: sudo apt-get install libffi-dev For Python 3.x use: sudo apt-get install python3-dev or for a specific version of Python 3, replace x with the minor version in sudo apt-get install python3.x-dev"
            ],
            [
                "For me this worked: export CPATH=$CPATH:/usr/local/cuda-11.8/targets/x86_64-linux/include export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/cuda-11.8/targets/x86_64-linux/lib pip install pycuda"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am moving my first steps into PyCuda to perform some parallel computation and I came across a behavior I do not understand. I started from the very basic tutorial that can be found on PyCuda official website (a simple script to double all elements of an array https://documen.tician.de/pycuda/tutorial.html). The code is the following: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.random.randn(4,4) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") func = mod.get_function(\"doublify\") func(a_gpu, block=(4,4,1)) a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print(a_doubled) print(a) Is quite clear and it works. An example result is [[-1.9951048 -1.7537887 -1.3228793 -1.1585734 ] [-0.96863186 -1.7235669 -0.3331826 -1.1527038 ] [ 2.4142797 -0.35531005 1.8844942 3.996446 ] [ 1.400629 -2.7957075 -0.78042877 0.13829945]] [[-0.9975524 -0.87689435 -0.66143966 -0.5792867 ] [-0.48431593 -0.86178344 -0.1665913 -0.5763519 ] [ 1.2071398 -0.17765503 0.9422471 1.998223 ] [ 0.7003145 -1.3978537 -0.39021438 0.06914973]] But then I tried to modify slightly the code to deal with integer numbers: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.array([[1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4]]) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) mod = SourceModule(\"\"\" __global__ void doublify(int *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") func = mod.get_function(\"doublify\") func(a_gpu, block=(4,4,1)) a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print(a_doubled) print(a) ... and this does not work. Only a part of the 2d array is multiplied by 2, the rest is unchanged. Example result: [[2 4 6 8] [2 4 6 8] [1 2 3 4] [1 2 3 4]] [[1 2 3 4] [1 2 3 4] [1 2 3 4] [1 2 3 4]] Why is this happening? What is the difference between the tutorial and the modified code? Thanks to all!",
        "answers": [
            [
                "OK so I kinda solved staying with float type, even though I need to work with integers. Apparently there are some behind-the-scene mechanism when allocating memory for integers and this does not fit with PyCuda."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I would like to use pycuda and the FFT functions from scikit-cuda together. The code below creates a skcuda.fft.Plan, deletes that plan and then tries to allocate a pycuda.gpuarray.GPUArray. import pycuda.autoinit import numpy as np import pycuda import skcuda import skcuda.fft as cufft plan = cufft.Plan((2,2), np.complex64, np.complex64) del plan # equivalent to `skcuda.cufft.cufftDestroy(plan.handle)` #skcuda.cufft.cufftDestroy(plan.handle) # equivalent to `del plan` pycuda.gpuarray.empty((2,2), np.float32) The last line throws pycuda._driver.LogicError: cuMemAlloc failed: context is destroyed. Somehow, skcuda.cufft.cufftDestroy(plan.handle) also destroys the pycuda context (which is of type pycuda._driver.Context). Can somebody see a good fix?",
        "answers": [
            [
                "The master replied (https://github.com/inducer/pycuda/discussions/356): By using pycuda.autoinit, you're putting pycuda in charge of context management. That's not typically a good recipe for interacting with libraries that use the CUDA runtime API (like cuFFT, to my understanding). You might be better off retaining the \"primary context\" made by/for the runtime API and using that instead. Essentially, CUDA has a thing called primary context that is \"unique per device\" and that can be retained and released. This is different from ordinary contexts that can be created and destroyed. See also this forum discussion on the difference and why it exists. The relevant section of the CUDA programming guide makes context management clear. The sections Initialization and Interoperability between Runtime and Driver APIs complete the picture of how device contexts are created and used. The solution to my specific problem above is retaining the primary context instead of letting pyCUDA create a new context. The easiest way to do this is via: import pycuda.autoprimaryctx instead of import pycuda.autoinit Voila, everything works now. See also the documentation and the code for pycuda.autoprimaryctx."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Since nobody answered this question I'm trying again. Is it possible to exchange data between Pycuda and OpenCV Cuda module? Pycuda has its own class Pycuda GPUArray and OpenCV has its own Gpu_Mat. The plan is to perform some kind of action on the image (for example now only to invert it) on Pycuda, keep it on GPU, and then perform Canny with OpenCV. import numpy import time import numpy as np import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.gpuarray as gpuarray import cv2 mod = SourceModule(\"\"\" __global__ void multiply_them(uchar3 *dest, uchar3* img, int row, int col) { int i = blockDim.x * blockIdx.x + threadIdx.x; int j = blockIdx.y * blockDim.y + threadIdx.y; if (j &gt;= row || i &gt;= col) { return; } dest[j * col + i].x = 255 - img[j * col + i].x; dest[j * col + i].y = 255 - img[j * col + i].y; dest[j * col + i].z = 255 - img[j * col + i].z; } \"\"\") img = cv2.imread(\"./colorful_image.jpg\", 0) dest = numpy.zeros_like(img) col = np.int32(img.shape[1]) row = np.int32(img.shape[0]) start = time.perf_counter() multiply_them = mod.get_function(\"multiply_them\") img_gpu = gpuarray.to_gpu(img.astype(numpy.uint8)) dest_gpu = gpuarray.to_gpu(dest.astype(numpy.uint8)) block_size = (32,32,1) grid_size = (int(col/block_size[0] + 1), int(row/block_size[1] + 1),1) multiply_them(dest_gpu, img_gpu, row, col, block=block_size, grid=grid_size) # dest_gpu = dest_gpu.get() # If we download to CPU it work fine but we dont want that. # dest_gpu = cv2.cuda_GpuMat(dest_gpu) cannyFilter = cv2.cuda.createCannyEdgeDetector(50, 120) gpu_img_canny = cannyFilter.detect(dest_gpu) b = gpu_img_canny.download() cv2.imwrite(\"./slika_canny.jpg\", b) stop = time.perf_counter() print(\"Time: \", stop-start)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to import pycuda-2021.1 in a python script. My OS is Ubuntu 18.04. I have cuda toolkit 11.2 installed and my nvidia driver version is 460.27.04. My python interpretor is Python 3.8. When I execute import pycuda.driver as cuda I seem to be able to do so without executing the python script under sudo. However, when using sudo, this leads to the following error: ImportError: libcurand.so.10: cannot open shared object file: No such file or directory How can I resolve this issue?",
        "answers": [
            [
                "I've found a solution myself, by refering to this post: \u5b8c\u7f8e\u89e3\u51b3ImportError: libcudart.so.10.0: cannot open shared object file: No such file or directory (this post is in Chinese). Basically, the solution is to execute the following command: sudo ldconfig /path/to/your/cuda/lib64 which in my case should be sudo ldconfig /usr/local/cuda-11.2/lib64"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to use PyCuda right now. I followed the tutorial on the official page and this code is working perfectly in my environment. import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.random.randn(4,4) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") func = mod.get_function(\"doublify\") func(a_gpu, block=(4,4,1)) a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print(a_doubled) print(a) Output: [[ 1.098445 0.8321258 1.2041918 2.530517 ] [-2.3432384 2.3117933 2.7848036 4.025257 ] [ 1.3831481 -0.436876 -2.0395236 0.7690674 ] [-1.2118376 1.2235037 -0.89722884 2.9550834 ]] [[ 0.5492225 0.4160629 0.6020959 1.2652586 ] [-1.1716192 1.1558967 1.3924018 2.0126286 ] [ 0.69157404 -0.218438 -1.0197618 0.3845337 ] [-0.6059188 0.61175185 -0.44861442 1.4775417 ]] The problem is I am getting this error: File \"C:\\Users\\arasu\\AppData\\Local\\Temp/ipykernel_22084/2324443706.py\", line 12, in &lt;module&gt; \"\"\") File \"C:\\Users\\arasu\\anaconda3\\envs\\hda\\lib\\site-packages\\pycuda\\compiler.py\", line 358, in __init__ include_dirs, File \"C:\\Users\\arasu\\anaconda3\\envs\\hda\\lib\\site-packages\\pycuda\\compiler.py\", line 298, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"C:\\Users\\arasu\\anaconda3\\envs\\hda\\lib\\site-packages\\pycuda\\compiler.py\", line 87, in compile_plain checksum.update(preprocess_source(source, options, nvcc).encode(\"utf-8\")) File \"C:\\Users\\arasu\\anaconda3\\envs\\hda\\lib\\site-packages\\pycuda\\compiler.py\", line 59, in preprocess_source \"nvcc preprocessing of %s failed\" % source_path, cmdline, stderr=stderr CompileError: nvcc preprocessing of C:\\Users\\arasu\\AppData\\Local\\Temp\\tmpsvfxijmf.cu failed While trying to run this code. What am I missing here? import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void myfirst_kernel() { printf(\"Hello,PyCUDA!!!\"); } \"\"\") function = mod.get_function(\"myfirst_kernel\") function(block=(1,1,1)) I checked my cuda versions and checked the visual studio environment variable and they all seemed normal to me. The thing I do not understand is, how is the first code is working if there is a problem here.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I had code in OpenCL where I use clCreateProgramWithBinary() to create the program from binary. I am porting this application to CUDA and I don't find any similar function. Can someone help me with how I can create the program from binary or equivalent of clCreateProgramWithBinary in CUDA?",
        "answers": [
            [
                "AFAIK, the CUDA equivalent of clCreateProgramWithBinary() will be cuModuleLoad (). Please check cuModuleLoad () for precompiled binary. It loads a compute module. For further reference please check module management in CUDA Driver API API Reference Manual."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I would like to construct a Cupy GPU array view of the array that already exists on the GPU and I'm handed the following: Pointer to the array. I know the data type and the size of the data. I'm also given a pitch. How one would construct an array view (avoiding copies preferably)? I tried the following: import cupy as cp import numpy as np shape = (w, h, c, b) # example s = np.product(shape)*4 # this is 1D mem = cp.cuda.UnownedMemory(ptr=image_batch_ptr, owner=None, size=s) memptr = cp.cuda.MemoryPointer(mem, 0) d = cp.ndarray(shape=shape, dtype=np.float32, memptr=memptr) But this does not seem to produce the correct alignment. Specifically, I'm having trouble with integrating pitch into the picture -- is it even possible?",
        "answers": [
            [
                "I found a way to solve it. This is indeed possible with cupy but requires first moving (on device) 2D allocation to 1D allocation with copy.cuda.runtime.memcpy2D We initialise an empty cp.empty We copy the data from 2D allocation to that array using cupy.cuda.runtime.memcpy2D, there we can set the pitch and width. We use MemoryKind kind = 3 which is the device to device copy. This seems to be the optimal way to create a proper cp.ndarray without moving to host."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Windows 10 Python 3.8 CUDA 11.5 I've installed what I believe to be a matching pycuda from this file: pycuda-2021.1+cuda115-cp38-cp38-win_amd64.whl This simple example fails import pycuda.driver as drv drv.init() print(\"Detected {} CUDA devices\".format(drv.Device.count())) With this error: Traceback (most recent call last): File \"C:/University of Arizona/weeds/tests/cuda-summary.py\", line 5, in &lt;module&gt; import pycuda.driver as drv File \"C:\\Users\\evan\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pycuda\\driver.py\", line 65, in &lt;module&gt; from pycuda._driver import * # noqa ImportError: DLL load failed while importing _driver: The specified procedure could not be found. NVCC is in my path Adding os.add_dll_directory(os.path.join(os.environ['CUDA_PATH'], 'bin')) has no effect The script works just fine on my Jetson Nano Any ideas on how to get past this? I've searched and tried several solutions.",
        "answers": [
            [
                "It is quite strange, but updating the Nvidia display driver fixed the issue for me. But in my case there was no issue on my PC with the same versions of Windows, Python, CUDA and PyCuda. The issue appeared on the different Windows 10 machine, when launching the exe, packaged by PyInstaller. So, updating the display driver was the fix for this machine."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "The title says it all, but here is my problem in more detail: I'm implementing a finite elements solver in python + pycuda that should run on distributed systems. To hide the communication latency, I'm trying to overlap computation and communication (with 2 separate streams). My problem is that the kernels used for the communication (on one stream) are executed at the end of the main computation kernel (see pic below). My question is: how can I tell my GPU to first execute the communication kernels? I'm using a RTX2060M, so stream priority is supported, and the presence of the attribute STREAM_PRIORITIES_SUPPORTED in pycuda makes me think that it's possible to set stream priorities from pycuda.",
        "answers": [
            [
                "It appears that at the date of writing (February 2022), PyCUDA has not implemented stream creation with priorities. So while what you want to do can be done with the CUDA driver API (which PyCUDA uses), that feature is not presently exposed in PyCUDA."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I need to compile a cuda .cu file using nvcc from command line. The file is \"vectorAdd_kernel.cu\" and contains the following piece of code: extern \"C\" __global__ void VecAdd_kernel(const float* A, const float* B, float* C, int N) { int i = blockDim.x * blockIdx.x + threadIdx.x; if (i &lt; N) C[i] = A[i] + B[i]; } I used the following command (I need to get a .cubin file): nvcc --cubin --use-local-env --cl-version 2010 -keep -I \"C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\include\" vectorAdd_kernel.cu The compiler creates files vectorAdd_kernel.cpp4.ii and vectorAdd_kernel.cpp1.ii then it stops with the following output: C:\\Users\\Massimo\\Desktop\\Pluto&gt;nvcc --cubin --use-local-env --cl-version 2010 vectorAdd_kernel.cu -keep -I \"C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\include\" vectorAdd_kernel.cu vectorAdd_kernel.cu c:\\program files (x86)\\microsoft visual studio 10.0\\vc\\include\\codeanalysis\\sourceannotations.h(29): error: invalid redeclaration of type name \"size_t\" C:/Program Files (x86)/Microsoft Visual Studio 10.0/VC/include\\new(51): error: first parameter of allocation function must be of type## Heading ## \"size_t\" C:/Program Files (x86)/Microsoft Visual Studio 10.0/VC/include\\new(55): error: first parameter of allocation function must be of type \"size_t\" Can you please help me in solving this issue?",
        "answers": [
            [
                "I just encountered this in Visual Studio 2017 and Cuda v9.0 trying to compile from the command line with nvcc. After a lengthy session I realised my Visual Studio Command line tools were setup to use cl.exe from the x86 director instead of the x64. There are a number of ways to resolve it, one way is to override the directory it looks for its compiler tools with - for example as in : nvcc -ccbin \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX86\\x64\" -o add_cuda add_cuda.cu It then worked fine. I'll also mention that I used the which.exe utility from the git tools to figure out what version of cl.exe it was accessing, but the where command - native to windows - works as well. Update: Another way - probably a better way - to handle this is to just set the Visual Studio environment variables correctly to 64 bits like this for the Enterprise edition: \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 For the Community edition substitute \"Community\" for \"Enterprise\" in the path. You can also select the toolset with (for example) --vcvars_ver=14.0 which selects the 14.0 toolset, necessary to compile CUDA 9.1 with the 15.5 version of Visual Studio. Then you can build simply with this: nvcc -o add_cuda add_cuda.cu"
            ],
            [
                "VS Community 2019: Open a x64 Native Tools Command Prompt for VS 2019 ********************************************************************** ** Visual Studio 2019 Developer Command Prompt v16.3.6 ** Copyright (c) 2019 Microsoft Corporation ********************************************************************** [vcvarsall.bat] Environment initialized for: 'x64' C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community&gt;cd c:\\Users\\AFP\\Downloads\\cuda_by_example\\chapter03 c:\\Users\\AFP\\Downloads\\cuda_by_example\\chapter03&gt;nvcc hello_world.cu hello_world.cu Creating library a.lib and object a.exp c:\\Users\\AFP\\Downloads\\cuda_by_example\\chapter03&gt; If environment is not initialized for x64 and you have opened x86 Native Tools Command Prompt for VS 2019, run: ********************************************************************** ** Visual Studio 2019 Developer Command Prompt v16.3.6 ** Copyright (c) 2019 Microsoft Corporation ********************************************************************** [vcvarsall.bat] Environment initialized for: 'x86' C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community&gt;cd c:\\Users\\AFP\\Downloads\\cuda_by_example\\chapter03 c:\\Users\\AFP\\Downloads\\cuda_by_example\\chapter03&gt;nvcc hello_world.cu hello_world.cu YOU GET LOT OF ERRORS .... 75 errors detected in the compilation of \"C:/Users/AFP/AppData/Local/Temp/tmpxft_00004504_00000000-12_hello_world.cpp1.ii\". c:\\Users\\AFP\\Downloads\\cuda_by_example\\chapter03&gt;\"c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 ********************************************************************** ** Visual Studio 2019 Developer Command Prompt v16.3.6 ** Copyright (c) 2019 Microsoft Corporation ********************************************************************** [vcvarsall.bat] Environment initialized for: 'x64' c:\\Users\\AFP\\Downloads\\cuda_by_example\\chapter03&gt;nvcc hello_world.cu hello_world.cu Creating library a.lib and object a.exp c:\\Users\\AFP\\Downloads\\cuda_by_example\\chapter03&gt;"
            ],
            [
                "I've had similar problem. The code where build breaks in SourceAnnotations.h : #ifdef _WIN64 typedef unsigned __int64 size_t; #else typedef _W64 unsigned int size_t; #endif I've added _WIN64 compiler symbol with this --compiler-options \"-D _WIN64\". My nvcc build string looked like this: nvcc kernel.cu --cubin --compiler-options \"-D _WIN64\""
            ]
        ],
        "votes": [
            21.0000001,
            3.0000001,
            1.0000001
        ]
    },
    {
        "question": "import math # all the libraries i import import numpy as np !pip install pycuda import pycuda.gpuarray as gpu import pycuda.cumath as cm import pycuda.autoinit import pycuda.driver as drv from pycuda.compiler import SourceModule I have an error that gets thrown after using PyCUDA GPUarrays with a for loop. I defined a function PropagatorS that uses a for loop and it ran fine when I had just been using numpy, but not after switching to cuda. def PropagatorS(N, L, area, z0): p = gpu.zeros((N,N), dtype = 'complex_') for ii in gpu.arange(0, N, 1): for jj in gpu.arange(0, N, 1): u = (ii - N/2 - 1)/area v = (jj - N/2 - 1)/area p[ii, jj] = cm.exp(1j*np.pi*L*z0*(u**2 + v**2)) return p Trying with some values: p = PropagatorS(200, 700*10**-9, 0.002, 0.08)) returns \"IndexError: invalid subindex in axis 0\". The error happens in this line: ---&gt; p[ii, jj] = cm.exp(1j*np.pi*L*z0*(u**2 + v**2)) I am using Colab to run this code. I can't find any troubleshooting threads on this, hopefully someone can help. :)",
        "answers": [
            [
                "This is not how you use cumath. cumath functions like exp take an array argument, and perform the work on that array. There is no need for the doubly-nested for-loops. so: math.exp takes an argument and raises e to the power of that argument. cumath.exp takes an input array, and returns an array of the same shape, where each element of the returned array is e raised to the power of the corresponding element in the input array. Here is a trivial example: $ cat t31.py import math # all the libraries i import import numpy as np import pycuda.gpuarray as gpu import pycuda.cumath as cm import pycuda.autoinit import pycuda.driver as drv from pycuda.compiler import SourceModule def PropagatorS(N): q = gpu.zeros((N,N), dtype = np.float32) p = gpu.ones_like(q) cm.exp(p, out=q) return q p = PropagatorS(4) print(p) $ LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64 python t31.py [[ 2.71828175 2.71828175 2.71828175 2.71828175] [ 2.71828175 2.71828175 2.71828175 2.71828175] [ 2.71828175 2.71828175 2.71828175 2.71828175] [ 2.71828175 2.71828175 2.71828175 2.71828175]] $ I think to do what you want you have at least a couple options: create an array with your desired exponents in numpy. Transfer that numpy array to a GPU array. Then call cumath.exp on that GPU array. write a pycuda kernel to do it. Here is one possible example of how to do it using method 1 i.e. cumath.exp: $ cat t32.py import math # all the libraries i import import numpy as np import pycuda.gpuarray as gpu import pycuda.cumath as cm import pycuda.autoinit import pycuda.driver as drv from pycuda.compiler import SourceModule def PropagatorS(N, L, area, z0): p = np.zeros((N,N), dtype = np.complex64) for ii in range(0, N, 1): for jj in range(0, N, 1): u = (ii - N/2 - 1)/area v = (jj - N/2 - 1)/area p[ii, jj] = 1j*np.pi*L*z0*(u**2 + v**2) q = gpu.to_gpu(p) r = cm.exp(q) return r p = PropagatorS(4, 700*10**-9, 0.002, 0.08) print(p) $ LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64 python t32.py [[ 0.70264995+0.71153569j 0.84094459+0.54112124j 0.90482706+0.42577928j 0.92267275+0.385584j ] [ 0.84094459+0.54112124j 0.93873388+0.34464294j 0.97591674+0.21814324j 0.98456436+0.17502306j] [ 0.90482706+0.42577928j 0.97591674+0.21814324j 0.99613363+0.0878512j 0.99903291+0.04396812j] [ 0.92267275+0.385584j 0.98456436+0.17502306j 0.99903291+0.04396812j 1.00000000+0.j ]]"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "What is causing this error? raise DistributionNotFound(req, requirers) pkg_resources.DistributionNotFound: The 'pycuda' distribution was not found and is required by the application ------------------------------------------------------------------- PyCUDA ERROR: The context stack was not empty upon module cleanup. ------------------------------------------------------------------- A context was still active when the context stack was being cleaned up. At this point in our execution, CUDA may already have been deinitialized, so there is no way we can finish cleanly. The program will be aborted now. Use Context.pop() to avoid this problem.",
        "answers": [
            [
                "The error is pretty self-explanatory. The code you're trying to run wants to use the pycuda package, but the package is not installed. You can try running pip install pycuda, preferably inside a virtuaenv."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "How do I release memory after a Pycuda function call? For example in below, how do I release memory used by a_gpu so then I will have enough memory to be assigned to b_gpu instead of having the error as below? I tried importing from pycuda.tools import PooledDeviceAllocation or import pycuda.tools.PooledDeviceAllocation hoping to use the free() function but they both result in error when importing ImportError: cannot import name 'PooledDeviceAllocation' from 'pycuda.tools' (D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycuda\\tools.py) and ModuleNotFoundError: No module named 'pycuda.tools.PooledDeviceAllocation'; 'pycuda.tools' is not a package. If it should work on newer version of Pycuda, but just my version of Pycuda is too old, is there any other way to release memory in my version or older version of Pycuda? I hope the upgrade of Pycuda to be the last resort as my NVidia card is as old as 2060 series and in case the new version of Pycuda does not support my old card. Thanks a lot in advance. import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import os _path = r\"D:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\bin\\Hostx64\\x64\" if os.system(\"cl.exe\"): os.environ['PATH'] += ';' + _path if os.system(\"cl.exe\"): raise RuntimeError(\"cl.exe still not found, path probably incorrect\") import numpy as np a = np.zeros(1000000000).astype(np.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) mod = SourceModule(\"\"\" __global__ void func1(float *a) { a[0] = 1; } \"\"\") func = mod.get_function(\"func1\") func(a_gpu, block=(1,1,1)) a_out = np.empty_like(a) cuda.memcpy_dtoh(a_out, a_gpu) print (a_out) # Memory release code wanted here b = np.zeros(1000000000).astype(np.float32) b_gpu = cuda.mem_alloc(b.nbytes) cuda.memcpy_htod(b_gpu, b) mod = SourceModule(\"\"\" __global__ void func2(float *b) { b[1] = 1; } \"\"\") func = mod.get_function(\"func2\") func(b_gpu, block=(1,1,1)) b_out = np.empty_like(b) cuda.memcpy_dtoh(b_out, b_gpu) print (b_out) [1. 0. 0. ... 0. 0. 0.] Traceback (most recent call last): File \"D:\\PythonProjects\\Test\\CUDA\\Test_PyCUDA_MemoryRelease.py\", line 47, in &lt;module&gt; b_gpu = cuda.mem_alloc(b.nbytes) MemoryError: cuMemAlloc failed: out of memory",
        "answers": [
            [
                "Try with free() applied to the DeviceAllocation object (in this case a_gpu) import pycuda.driver as cuda a = np.zeros(1000000000).astype(np.float32) a_gpu = cuda.mem_alloc(a.nbytes) a_gpu.free() From the documentation: free() Release the held device memory now instead of when this object becomes unreachable. Any further use of the object is an error and will lead to undefined behavior. Check: cuda.mem_get_info()"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "What does version name 'cp27' or 'cp35' mean in Python? Like the files in https://pypi.python.org/pypi/gensim#downloads I am using Python 2.7 on a 64-bit Window 7 PC, and don't know which version of python package I should install. There are three questions: Which of \"gensim-0.12.4-cp27-none-win_amd64.whl\" or \"gensim-0.12.4.win-amd64-py2.7.exe\" should I install? I have installed 'WinPython-64bit-2.7.10.3' on 64-bit Window 7 PC which I am using. What does 'cp27' mean in Python or Python version name? I searched online with keywords 'Python cp27' but failed to find any answers. Are there differences between these two versions of python packages? ('0.12.4-cp27-none-win_amd64' and 'win-amd64-py2.7') If there are, what are the differences?",
        "answers": [
            [
                "If you check out the Python Enhancement Proposal (more commonly known as a PEP), you'll see that the cpN refers to the particular version of Python in gensim-0.12.4-cp27-none-win_amd64.whl you can break it apart: 0.12.4 - package version, they may be using semantic versioning cp27 - this package is for CPython. IronPython, Jython, or PyPy will probably be unhappy. none - no feature of this package depends on the python Application Binary Interface, or ABI win_amd64 - this has been compiled for 64-bit Windows. That means that it probably has some code written in C/C++ .whl - that means this is a wheel distribution. Which is handy, because it means if you're running CPython 2.7 64-bit on Windows, and assuming you have pip installed, all you have to do to get this package is run: py -2.7 -m pip install --use-wheel gensim (assuming that it's available on pypi, of course). You may need to py -2.7 -m pip install wheel first. But other than that, that should be all it takes."
            ],
            [
                "These stand for the version of CPython (i.e. the Python official distribution you get from python.org) which the wheel files are built for. For example cp27 is meant to be used on a CPython version 2.7. Warning: cp32 is meant to be used in a CPython version 3.2. The difference between the 32 bits version and the 64 bits version is stated in another suffix, e.g. win32 or amd64 in the filename."
            ]
        ],
        "votes": [
            46.0000001,
            11.0000001
        ]
    },
    {
        "question": "When trying the below code with int d[1];, it works fine, but with int d[in_integer]; or int c[in_matrix[0]]; it results in nvcc compilation failed. May I see if anyone can suggest why? Is it possible to declare array in pycuda with size determined by function parameter value? import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import os import numpy as np _path = r\"D:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\bin\\Hostx64\\x64\" if os.system(\"cl.exe\"): os.environ['PATH'] += ';' + _path if os.system(\"cl.exe\"): raise RuntimeError(\"cl.exe still not found, path probably incorrect\") a = np.asarray([1, 2]) a = a.astype(np.int32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) b = np.zeros(1).astype(np.int32) b_gpu = cuda.mem_alloc(b.nbytes) cuda.memcpy_htod(b_gpu, b) mod = SourceModule(\"\"\" __global__ void array_declaration(int *in_matrix, int *out_matrix, int in_integer) { int d[1]; //int d[in_integer]; //int d[in_matrix[0]]; out_matrix[0] = in_matrix[0]; } \"\"\") func = mod.get_function(\"array_declaration\") func(a_gpu, b_gpu, np.int32(1), block=(1,1,1)) b_out = np.empty_like(b) cuda.memcpy_dtoh(b_out, b_gpu) print(b_out) the error is as below Traceback (most recent call last): File \"D:\\PythonProjects\\TradeAnalysis\\Test\\TestCUDAArrayDeclaration.py\", line 37, in &lt;module&gt; \"\"\") File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 291, in __init__ arch, code, cache_dir, include_dirs) File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 254, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 137, in compile_plain stderr=stderr.decode(\"utf-8\", \"replace\")) CompileError: nvcc compilation of C:\\Users\\HENRYC~1\\AppData\\Local\\Temp\\tmpy0kcq_5i\\kernel.cu failed",
        "answers": [
            [
                "Is it possible to declare array in pycuda with size determined by function parameter value? Instead of this: int d[in_integer]; do this: int *d = new int[in_integer]; or, equivalently: int *d = (int *)malloc(in_integer*sizeof(d[0])); Technically this is a pointer with an allocation, not an array, but it will function similarly for most use cases in C++ or CUDA C++. This sort of in-kernel device memory allocation has a variety of caveats: by default the total allocated space (the number of threads that currently have an allocation open times the allocation size) used by your kernel cannot exceed 8MB. This is adjustable. you should typically have a corresponding C++ delete[] operation after you are finished using the pointer (or free() if you used malloc()), in your kernel code (which also may help with the item above) the CUDA device runtime indicates an allocation error here by returning a NULL pointer. When having trouble, its good practice to check for NULL in the kernel code, before using the pointer. the device allocation step itself can noticeably impact kernel performance (execution duration). If your kernel is doing a lot of work/many other things, it may not be noticeable at all. For kernels doing little work, it may be very noticeable. Once the allocation is completed, using the allocated pointer shouldn't have any significant performance difference from using an array definition where the size is known as a compile-time constant."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "When trying the below code with int d[1];, it works fine, but with int d[in_integer]; or int c[in_matrix[0]]; it results in nvcc compilation failed. May I see if anyone can suggest why? Is it possible to declare array in pycuda with size determined by function parameter value? import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import os import numpy as np _path = r\"D:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\bin\\Hostx64\\x64\" if os.system(\"cl.exe\"): os.environ['PATH'] += ';' + _path if os.system(\"cl.exe\"): raise RuntimeError(\"cl.exe still not found, path probably incorrect\") a = np.asarray([1, 2]) a = a.astype(np.int32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) b = np.zeros(1).astype(np.int32) b_gpu = cuda.mem_alloc(b.nbytes) cuda.memcpy_htod(b_gpu, b) mod = SourceModule(\"\"\" __global__ void array_declaration(int *in_matrix, int *out_matrix, int in_integer) { int d[1]; //int d[in_integer]; //int d[in_matrix[0]]; out_matrix[0] = in_matrix[0]; } \"\"\") func = mod.get_function(\"array_declaration\") func(a_gpu, b_gpu, np.int32(1), block=(1,1,1)) b_out = np.empty_like(b) cuda.memcpy_dtoh(b_out, b_gpu) print(b_out) the error is as below Traceback (most recent call last): File \"D:\\PythonProjects\\TradeAnalysis\\Test\\TestCUDAArrayDeclaration.py\", line 37, in &lt;module&gt; \"\"\") File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 291, in __init__ arch, code, cache_dir, include_dirs) File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 254, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 137, in compile_plain stderr=stderr.decode(\"utf-8\", \"replace\")) CompileError: nvcc compilation of C:\\Users\\HENRYC~1\\AppData\\Local\\Temp\\tmpy0kcq_5i\\kernel.cu failed",
        "answers": [
            [
                "Is it possible to declare array in pycuda with size determined by function parameter value? Instead of this: int d[in_integer]; do this: int *d = new int[in_integer]; or, equivalently: int *d = (int *)malloc(in_integer*sizeof(d[0])); Technically this is a pointer with an allocation, not an array, but it will function similarly for most use cases in C++ or CUDA C++. This sort of in-kernel device memory allocation has a variety of caveats: by default the total allocated space (the number of threads that currently have an allocation open times the allocation size) used by your kernel cannot exceed 8MB. This is adjustable. you should typically have a corresponding C++ delete[] operation after you are finished using the pointer (or free() if you used malloc()), in your kernel code (which also may help with the item above) the CUDA device runtime indicates an allocation error here by returning a NULL pointer. When having trouble, its good practice to check for NULL in the kernel code, before using the pointer. the device allocation step itself can noticeably impact kernel performance (execution duration). If your kernel is doing a lot of work/many other things, it may not be noticeable at all. For kernels doing little work, it may be very noticeable. Once the allocation is completed, using the allocated pointer shouldn't have any significant performance difference from using an array definition where the size is known as a compile-time constant."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to generate random number following a Gaussian law with a mean and a standard deviation. For now I write this code. import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np import matplotlib.pyplot as plt import time class GN: def __init__(self, ): self.NbCells = int(1024 * 100) self.init_vectors() self.Create_GPU_SourceModule() BLOCK_SIZE = 1024 self.grid = (int(self.NbCells / BLOCK_SIZE), 1, 1) self.block = (BLOCK_SIZE, 1, 1) def put_vect_on_GPU(self, Variable): Variable_gpu = cuda.mem_alloc(Variable.nbytes) cuda.memcpy_htod(Variable_gpu, Variable) return Variable_gpu def init_vectors(self): self.V = self.put_vect_on_GPU(np.zeros((self.NbCells), dtype=np.float32)) self.m = self.put_vect_on_GPU(np.ones((self.NbCells), dtype=np.float32) * 120) self.s = self.put_vect_on_GPU(np.ones((self.NbCells), dtype=np.float32) * 60) def Create_GPU_SourceModule(self): # self.mod = SourceModule(\"\"\" #include &lt;math.h&gt; #include &lt;curand.h&gt; #include &lt;cuda.h&gt; __global__ void randgauss( float *m, float *s, float *res) { int idx = threadIdx.x + blockDim.x * blockIdx.x; int n=1; curandGenerator_t gen ; float d_normals; curandCreateGenerator(&amp;gen, CURAND_RNG_PSEUDO_MTGP32) ; curandGenerateNormal(gen, &amp;d_normals, n, m[idx], s[idx]); res[idx] = d_normals; } \"\"\") def updateParameters(self): func = self.mod.get_function(\"sinus\") func(self.m, self.s, self.V, block=self.block, grid=self.grid) def gen(self, N): V = np.zeros((N, self.NbCells), dtype=np.float32) for k in range(N): self.updateParameters() cuda.memcpy_dtoh(V[k, :], self.V) return V GN = GN() t0 = time.time() Vm = GN.gen(10000) print('GPU', time.time() - t0) plt.figure() plt.subplot(111) plt.plot(Vm[:, 0] ) # plt.plot(t,Vm[:,0::1000]) # plt.show() When I run it, I have this message: kernel.cu(13): error: calling a __host__ function(\"curandCreateGenerator\") from a __global__ function(\"randgauss\") is not allowed kernel.cu(13): error: identifier \"curandCreateGenerator\" is undefined in device code I don't understand how should I used the curandGenerateNormal function properly.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have tried the below code and the results are different if I create a numpy 2D array myself and using dataframe.to_numpy() to create 2D array. Could anyone help to explain why? The result if I use a = input_matrix.to_numpy() or a = np.array([[1, 100, 200, 300], [1, 100, 200, 300], [1, 100, 200, 300]]) are different a = input_matrix.to_numpy() returns the below. I even tried transposing a (by a = a.T) after to_numpy() but the output is still the same. Could anyone suggest a way which can transpose that matrix from to_numpy successfully? input array is [[ 1. 100. 200. 300.] [ 1. 100. 200. 300.] [ 1. 100. 200. 300.]] returned array is [ 1. 1. 1. 100. 200. 300.] while a = np.array([[1, 100, 200, 300], [1, 100, 200, 300], [1, 100, 200, 300]]) returns the below input array is [[ 1. 100. 200. 300.] [ 1. 100. 200. 300.] [ 1. 100. 200. 300.]] returned array is [ 1. 100. 200. 300. 200. 100.] import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import pandas as pd import os import numpy as np _path = r\"D:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\bin\\Hostx64\\x64\" if os.system(\"cl.exe\"): os.environ['PATH'] += ';' + _path if os.system(\"cl.exe\"): raise RuntimeError(\"cl.exe still not found, path probably incorrect\") input_matrix = pd.DataFrame(data={'a': [1, 1, 1], 'b': [100, 100, 100], 'c': [200, 200, 200], 'd': [300, 300, 300]}) a = input_matrix.to_numpy() # a = np.array([[1, 100, 200, 300], [1, 100, 200, 300], [1, 100, 200, 300]]) a = a.astype(np.float32) print('input array is') print(a) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) a_out = np.zeros(6) a_out = a_out.astype(np.float32) a_out_gpu = cuda.mem_alloc(a_out.nbytes) cuda.memcpy_htod(a_out_gpu, a_out) mod = SourceModule(\"\"\" __global__ void matrix_location_trial(float *in_matrix, float *out_matrix) { out_matrix[0] = in_matrix[0]; out_matrix[1] = in_matrix[1]; out_matrix[2] = in_matrix[2]; out_matrix[3] = in_matrix[3]; out_matrix[4] = in_matrix[6]; out_matrix[5] = in_matrix[9]; } \"\"\") func = mod.get_function(\"matrix_location_trial\") func(a_gpu, a_out_gpu, block=(1,1,1)) returned_array = np.empty_like(a_out) cuda.memcpy_dtoh(returned_array, a_out_gpu) print('returned array is') print(returned_array)",
        "answers": [
            [
                "The storage order in both cases is different: Dataframe input_matrix = pd.DataFrame( data={ 'a': [1, 1, 1], 'b': [100, 100, 100], 'c': [200, 200, 200], 'd': [300, 300, 300] } ) a = input_matrix.to_numpy().astype(np.float32) print(a.flags) Output: C_CONTIGUOUS : False F_CONTIGUOUS : True OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False NumPy array a = np.array( [[1, 100, 200, 300], [1, 100, 200, 300], [1, 100, 200, 300]], dtype=np.float32 ) print(a.flags) Output: C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False The difference in both cases here is in the values of flags C_CONTIGUOUS and F_CONTIGUOUS. Arrays created using np.array() are C_CONTIGUOUS by default whereas the same cannot be guaranteed for numpy arrays created using other ways, like from input_matrix.to_numpy() in this case. To resolve this issue, you just have to make the array C_CONTIGUOUS again before you copy the data to GPU memory like this: a = input_matrix.to_numpy() a = a.astype(np.float32) # Change order to C_CONTIGUOUS a = a.copy(order=\"C\") print('input array is') print(a) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) After adding that line, I was able to get the following output in both cases: [[ 1. 100. 200.] [300. 200. 100.]] The difference between which of the flags C_CONTIGUOUS and F_CONTIGUOUS is True for an array is related to how the array is stored in memory. C language stores data in Row-major order while Fortran stores it in Column-major order. NumPy supports storing your data in both ways. You can read more about storage here."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm trying to implement a neuron model with Hodgkin and Huxley formalism on my RTX 2080 Ti with PyCuda. The code is quite large so I wont put all of it here. the first part of my class is to set the number of neurons, create all variables in the GPU and get the block and grid size according on the number of neurons (1 neuron by thread) class Inter_PC: def __init__(self, ): self.NbODEs = 25 self.NbCells = int(1024 * 1) self.init_vector() self.init_vector_param() self.Create_GPU_SourceModule() BLOCK_SIZE = 1024 self.grid = (int(self.NbCells / BLOCK_SIZE), 1) self.block = (BLOCK_SIZE, 1, 1) In the function init_vector and init_vector_param, I put vectors to compute ODE results in the GPU def init_vector(self): self.Vs_PC_dydx1 = self.put_vect_on_GPU(np.zeros((self.NbCells), dtype=np.float32)) self.Vs_PC_dydx2 = self.put_vect_on_GPU(np.zeros((self.NbCells), dtype=np.float32)) self.Vs_PC_dydx3 = self.put_vect_on_GPU(np.zeros((self.NbCells), dtype=np.float32)) self.Vs_PC_dydx4 = self.put_vect_on_GPU(np.zeros((self.NbCells), dtype=np.float32)) self.Vs_PC_y = self.put_vect_on_GPU(np.zeros((self.NbCells), dtype=np.float32)) self.Vs_PC_yt = self.put_vect_on_GPU(np.zeros((self.NbCells), dtype=np.float32)) ... def init_vector_param(self): self.E_leak = self.put_vect_on_GPU(np.ones((self.NbCells), dtype=np.float32) * -65) self.E_Na = self.put_vect_on_GPU(np.ones((self.NbCells), dtype=np.float32) * 55) ... def put_vect_on_GPU(self, Variable): Variable_gpu = cuda.mem_alloc(Variable.nbytes) cuda.memcpy_htod(Variable_gpu, Variable) return Variable_gpu In the function Create_GPU_SourceModule, I create kernels to use on the GPU. def Create_GPU_SourceModule(self): self.mod = SourceModule(\"\"\" #include &lt;math.h&gt; __global__ void m_inf_PC(float *V_PC, float *res) { int idx = threadIdx.x + blockDim.x * blockIdx.x; res[idx] = 1.0 / ( 1. * exp(-(V_PC[idx] + 40.) / 3.)); } __global__ void h_inf_PC(float *V_PC, float *res) { int idx = threadIdx.x + blockDim.x * blockIdx.x; res[idx] = 1.0 / ( 1. * exp((V_PC[idx] + 45.) / 3.)); } ... I have the a function to update all my variables in a RK4 solver updateParameters def setParameters(self): func = self.mod.get_function(\"set_vect_val\") func(self.Vs_PC_y, self.E_leak, block=self.block, grid=self.grid) func = self.mod.get_function(\"set_vect_val\") func(self.Vd_PC_y, self.E_leak, block=self.block, grid=self.grid) func = self.mod.get_function(\"h_inf_PC\") func(self.Vs_PC_y, self.h_s_PC_y, block=self.block, grid=self.grid) func = self.mod.get_function(\"m_KDR_inf_PC\") func(self.Vs_PC_y, self.m_KDR_s_PC_y, block=self.block, grid=self.grid) func = self.mod.get_function(\"m_m_inf_PC\") func(self.Vs_PC_y, self.m_s_PC_y, block=self.block, grid=self.grid) func = self.mod.get_function(\"m_m_inf_PC\") func(self.Vd_PC_y, self.m_d_PC_y, block=self.block, grid=self.grid) func = self.mod.get_function(\"h_inf_PC\") func(self.Vd_PC_y, self.h_d_PC_y, block=self.block, grid=self.grid) When I run the code I get this error: Traceback (most recent call last): File \"C:/Users/maxime/Desktop/SESAME/PycharmProjects/Modele_Micro3/Class_PyrCell_GPU.py\", line 1668, in &lt;module&gt; Vm = PC.rk4_Time(30000) File \"C:/Users/maxime/Desktop/SESAME/PycharmProjects/Modele_Micro3/Class_PyrCell_GPU.py\", line 1637, in rk4_Time self.updateParameters() File \"C:/Users/maxime/Desktop/SESAME/PycharmProjects/Modele_Micro3/Class_PyrCell_GPU.py\", line 998, in updateParameters func = self.mod.get_function(\"h_inf_PC\") File \"C:\\Python389\\lib\\site-packages\\pycuda\\compiler.py\", line 326, in get_function return self.module.get_function(name) pycuda._driver.LogicError: cuModuleGetFunction failed: an illegal memory access was encountered PyCUDA WARNING: a clean-up operation failed (dead context maybe?) cuMemFree failed: an illegal memory access was encountered What I'm not understanding is that the error does not occur the first time I use the kernel h_inf_PC, it happens on the 13th line of the function setParameters but I already calling the same kernel in line 5 of the same function. If I comment out the calling to the kernel (h_inf_PC) that causes the issue, the error switched on another calling to a kernel but not necessarily the next one.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am new to PyCUDA and trying to implement the Odd-even sort using PyCUDA. I managed to run it successfully on arrays whose size is limited by 2048 (using one thread block), but as soon as I tried to use multiple thread blocks, the result was no longer correct. I suspected this might be a synchronization problem but had no idea how to fix it. bricksort_src = \"\"\" __global__ void bricksort(int *in, int *out, int n){ int tid = threadIdx.x + (blockIdx.x * blockDim.x); if((tid * 2) &lt; n) out[tid * 2] = in[tid *2]; if((tid * 2 + 1) &lt; n) out[tid * 2 + 1] = in[tid * 2 + 1]; __syncthreads(); // odd and even are used for adjusting the index // to avoid out-of-index exception int odd, even, alter; odd = ((n + 2) % 2) != 0; even = ((n + 2) % 2) == 0; // alter is used for alternating between the odd and even phases alter = 0; for(int i = 0; i &lt; n; i++){ int idx = tid * 2 + alter; int adjust = alter == 0 ? odd : even; if(idx &lt; (n - adjust)){ int f, s; f = out[idx]; s = out[idx + 1]; if (f &gt; s){ out[idx] = s; out[idx + 1] = f; } } __syncthreads(); alter = 1 - alter; } } \"\"\" bricksort_ker = SourceModule(source=bricksort_src) bricksort = bricksort_ker.get_function(\"bricksort\") np.random.seed(0) arr = np.random.randint(0,10,2**11).astype('int32') iar = gpuarray.to_gpu(arr) oar = gpuarray.empty_like(iar) n = iar.size num_threads = np.ceil(n/2) if (num_threads &lt; 1024): blocksize = int(num_threads) gridsize = 1 else: blocksize = 1024 gridsize = int(np.ceil(num_threads / blocksize)) bricksort(iar, oar, np.int32(n), block=(blocksize,1,1), grid=(gridsize,1,1))",
        "answers": [
            [
                "Assembling comments into an answer: odd-even sort can't be easily/readily extended beyond a single threadblock (because it requires synchronization) CUDA __syncthreads() only synchronizes at the block level. Without synchronization, CUDA specifies no particular order to thread execution. for serious sorting work, I recommend a library implementation such as cub. If you want to do this from python I recommend cupy. CUDA has a sample code that demonstrates odd-even sorting at the block level, but because of the sync issue it chooses a merge method to combine results it should be possible to write an odd-even sort kernel that only does a single swap, then call this kernel in a loop. The kernel call itself acts as a device-wide synchronization point. alternatively, it should be possible to do the work in a single kernel launch using cooperative groups grid sync. none of these methods are likely to be faster than a good library implementation (which won't depend on odd-even sorting to begin with)."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have created a Streamlit App to as a demo of a project on Multilingual Text Classification using mBERT in PyTorch. When I run the app with the command python app.py it works fine but when I try to use Streamlit with the command streamlit run app.py it throws a PyCUDA Error. Following is the code present in app.py: import torch from typing import Text import streamlit as st import pandas as pd from textblob import TextBlob from inference.inference_onnx import run_onnx_inference from inference.inference_tensorRT import run_trt_inference from googletrans import Translator st.title(\"LinClass: Multilingual Text Classifier\") input_text = st.text_input('Text:') #################### # Google Translate API #################### translator = Translator() input_text = translator.translate( input_text, dest= \"en\" ) input_text = input_text.text #################### #Select Precision and Inference Method #################### df = pd.DataFrame() df[\"lang\"] = [\"en\"] precision = st.sidebar.selectbox(\"Select Precision:\", (\"16 Bit\", \"32 Bit\") ) inference = st.sidebar.selectbox(\"Inference Method:\", (\"ONNX\", \"TensorRT\") ) if st.button('Show Selected Configuration'): st.subheader(\"Selected Configuration:\") st.write(\"Precision: \", precision) st.write(\"Inference: \", inference) st.subheader(\"Results\") def result(x): \"\"\" Function to classify the comment toxicity based on the probability and given threshold params: x(float) - Probability of Toxicity \"\"\" if x &gt;= 0.4: st.write(\"Toxic\") else: st.write(\"Non Toxic\") #################### # Implement Selected Configuration #################### if precision==\"16 Bit\": if inference==\"ONNX\": df[\"comment_text\"] = [input_text] predictions = run_onnx_inference( onnx_model_path = \"/workspace/data/multilingual-text-classifier/output models/mBERT_lightning_fp16_2GPU.onnx\", stage=\"inference\", df_test = df ) predictions = torch.sigmoid(torch.tensor(predictions)) st.write(input_text) st.write(predictions) result(predictions) if inference==\"TensorRT\": df[\"content\"] = [input_text] predictions = run_trt_inference( trt_model_path = \"/workspace/data/multilingual-text-classifier/output models/mBERT_lightning_fp16_bs16.engine\", stage=\"inference\", df_test = df ) predictions = predictions.astype(\"float32\") predictions = torch.sigmoid(torch.tensor(predictions)) st.write(input_text) st.write(predictions) result(predictions) if precision==\"32 Bit\": if inference==\"ONNX\": df[\"comment_text\"] = [input_text] predictions = run_onnx_inference( onnx_model_path = \"/workspace/data/multilingual-text-classifier/output models/mBERT_fp32.onnx\", stage=\"inference\", df_test = df ) predictions = torch.sigmoid(torch.tensor(predictions)) st.write(input_text) st.write(predictions) result(predictions) if inference==\"TensorRT\": df[\"content\"] = [input_text] predictions = run_trt_inference( trt_model_path = \"/workspace/data/multilingual-text-classifier/output models/mBERT_fp32.engine\", stage=\"inference\", df_test = df ) predictions = predictions.astype(\"float32\") predictions = torch.sigmoid(torch.tensor(predictions)) st.write(input_text) st.write(predictions) result(predictions) #################### # Take Feedback #################### st.subheader(\"Feedback:\") feedback = st.radio( \"Are you satisfied with the results?\", ('Yes', 'No')) st.write(\"Thanks for the Feedback!\") Error ------------------------------------------------------------------- PyCUDA ERROR: The context stack was not empty upon module cleanup. ------------------------------------------------------------------- A context was still active when the context stack was being cleaned up. At this point in our execution, CUDA may already have been deinitialized, so there is no way we can finish cleanly. The program will be aborted now. Use Context.pop() to avoid this problem. ------------------------------------------------------------------- Aborted (core dumped)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have seen many ways to generate an array of random numbers. but I want to generate a single random number. Is there any function as rand() in c++. I don't want a series of random numbers. I just need to generate a random number inside the kernel. is there any builtin function to generate random numbers? I have tried the given code below, but it not working. import numpy as np import pycuda.autoinit from pycuda.compiler import SourceModule from pycuda import gpuarray code = \"\"\" #include &lt;curand_kernel.h&gt; __device__ float getRand() { curandState_t s; curand_init(clock64(), 123456, 0, &amp;s); return curand_uniform(&amp;s); } __global__ void myRand(float *values) { values[0] = getRand(); } \"\"\" mod = SourceModule(code) myRand = mod.get_function(\"myRand\") gdata = gpuarray.zeros(2, dtype=np.float32) myRand(gdata, block=(1,1,1), grid=(1,1,1)) print(gdata) Errors are like: /usr/local/cuda/bin/../targets/x86_64-linux/include/curand_poisson.h(548): error: this declaration may not have extern \"C\" linkage /usr/local/cuda/bin/../targets/x86_64-linux/include/curand_discrete2.h(69): error: this declaration may not have extern \"C\" linkage /usr/local/cuda/bin/../targets/x86_64-linux/include/curand_discrete2.h(78): error: this declaration may not have extern \"C\" linkage /usr/local/cuda/bin/../targets/x86_64-linux/include/curand_discrete2.h(86): error: this declaration may not have extern \"C\" linkage 30 errors detected in the compilation of \"kernel.cu\".",
        "answers": [
            [
                "The basic problem is that, by default, PyCUDA silently applies C linkage to all code compiled in a SourceModule. As the error is showing, cuRand requires C++ linkage, so getRand can't have C linkage. You can fix this either by changing these two lines: mod = SourceModule(code) myRand = mod.get_function(\"myRand\") to mod = SourceModule(code, no_extern_c=True) myRand = mod.get_function(\"_Z6myRandPf\") This disables C linkage, but does mean you need to supply the C++ mangled name to the get_function call. You will need to look at the verbose compiler output or compile the code outside of PyCUDA to get that name (for example Godbolt). Alternatively you can modify the code like this: import numpy as np import pycuda.autoinit from pycuda.compiler import SourceModule from pycuda import gpuarray code = \"\"\" #include &lt;curand_kernel.h&gt; __device__ float getRand() { curandState_t s; curand_init(clock64(), 123456, 0, &amp;s); return curand_uniform(&amp;s); } extern \"C\" { __global__ void myRand(float *values) { values[0] = getRand(); } } \"\"\" mod = SourceModule(code, no_extern_c=True) myRand = mod.get_function(\"myRand\") gdata = gpuarray.zeros(2, dtype=np.float32) myRand(gdata, block=(1,1,1), grid=(1,1,1)) print(gdata) This leaves the kernel with C linkage, but doesn't touch the device function which is using cuRand."
            ],
            [
                "you can import random in python . and use random.randint(). to generate random number in specified range by defining range in function. exrandom.randint(0,50)"
            ]
        ],
        "votes": [
            5.0000001,
            -3.9999999
        ]
    },
    {
        "question": "Trying to find a quick way to compute fft on GPU. For other calculations, i use pycuda. It would work pyfft for me, but it is already outdated, and i cannot install it via pip. I use reikna, but using it, i cannot use the data that is in the GPU. Because of this, I re-upload them. fnc_coor(data_gpu, ta_gpu, fnci, Nri, Nai, s_rf_gpu, block=(BLOCK_SIZE,BLOCK_SIZE,1),grid=grid) api = any_api() thr = api.Thread.create() fft = FFT(s_rf, axes=(1,)) fftc = fft.compile(thr) s_rf_gpu = thr.to_device(s_rf_gpu.get()) fftc(s_rf_gpu, s_rf_gpu)",
        "answers": [],
        "votes": []
    },
    {
        "question": "i've recently been trying out PyCuda. I currently want to do somthing very simple, allocate some memory. Im assuming i have some fundamental misunderstanding because this is quite a simple task. My understanding is that with the code below i am create a 2d Cuda array 512 wide, 160 high and an elementsize of 1 byte. Heres some test code below. import pycuda.driver as cuda import pycuda.autoinit # Alloc some gpu memory test_pitch = cuda.mem_alloc_pitch(512,160,1) When i try running this code i get the following error Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; pycuda._driver.LogicError: cuMemAllocPitch failed: invalid argument If anyone has any insights as to what im doing wrong that would be greatly appreciated.",
        "answers": [
            [
                "Quoting from the CUDA driver API documentation cuMemAllocPitch ( CUdeviceptr* dptr, size_t* pPitch, size_t WidthInBytes, size_t Height, unsigned int ElementSizeBytes ) The function may pad the allocation to ensure that corresponding pointers in any given row will continue to meet the alignment requirements for coalescing as the address is updated from row to row. ElementSizeBytes specifies the size of the largest reads and writes that will be performed on the memory range. ElementSizeBytes may be 4, 8 or 16 (since coalesced memory transactions are not possible on other data sizes) In this case the first two arguments are the return values of mem_alloc_pitch, and ElementSizeBytes is access_size in the PyCUDA call. You have: cuda.mem_alloc_pitch(512,160,1) i.e. your access_size is 1, which is illegal. Only 4, 8, or 16 are legal. Thus the error."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "when I try to run an example of Matrix multiplication by pycuda. kernel_code_template = \"\"\" __global__ void MatrixMulKernel(float *a,float *b,float *c){ int tx = threadIdx.x; int ty = threadIdx.y; float Pvalue = 0; for(int i=0; i&lt;%(N)s; ++i){ float Aelement = a[ty * %(N)s + i]; float Belement = b[i * %(M)s + tx]; Pvalue += Aelement * Belement; } c[ty * %[M]s + tx] = Pvalue; } \"\"\" M, N = 2, 3 kernel_code = kernel_code_template % {'M': M, 'N': N} it reported error like: kernel_code = kernel_code_template % {'M': M, 'N': N} TypeError: not enough arguments for format string I've tried to check if there is anything wrong with \"%\" mark but got nothing yet.",
        "answers": [
            [
                "I think you are mixing syntaxes, % with .format string substituions. Check here for a nice summary: https://pyformat.info/ Now I spot the error (line 11): %[M]s --&gt; %(M)s"
            ],
            [
                "To directly answer the question, you need to change %[M]s to %(M)s in your code, like so: kernel_code_template = \"\"\" __global__ void MatrixMulKernel(float *a,float *b,float *c){ int tx = threadIdx.x; int ty = threadIdx.y; float Pvalue = 0; for(int i=0; i&lt;%(N)s; ++i){ float Aelement = a[ty * %(N)s + i]; float Belement = b[i * %(M)s + tx]; Pvalue += Aelement * Belement; } c[ty * %(M)s + tx] = Pvalue; } \"\"\" Then it will work as expected. However, I'd strongly recommend you start using f-strings as you indicated you were working with Python 3.7: kernel_code_template = f\"\"\" __global__ void MatrixMulKernel(float *a,float *b,float *c){{ int tx = threadIdx.x; int ty = threadIdx.y; float Pvalue = 0; for(int i=0; i&lt;{N}; ++i){{ float Aelement = a[ty * {N} + i]; float Belement = b[i * {M} + tx]; Pvalue += Aelement * Belement; }} c[ty * {M} + tx] = Pvalue; }} \"\"\" I get there are trade-offs, though, namely those {{ and }} escapes. There is also the .format() method as @Brandt pointed out. Choose what feels best to you."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "when I run this example of pycuda , a kernel part of which can run in C interpreter correctly: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" #include&lt;stdio.h&gt; __global__ void say_hi(){ printf(\"I am %dth thread in : \\ threadIdx.x:%d, threadIdx.y:%d,\\ blockIdx.x:%d, blockIdx.y:%d,\\ blockDim.x:%d, blockDim.y:%d.\\n\", (threadIdx.x+threadIdx.y*blockDim.x+ \\ (blockIdx.x*blockDim.x*blockDim.y)+ \\ (blockIdx.y*blockDim.x*blockDim.y)), threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y, blockDim.x, blockDim.y); } \"\"\") func = mod.get_function(\"say_hi\") func(block=(4, 4, 4), grid=(2, 2, 1)) but errors show up: pycuda.driver.CompileError: nvcc compilation of C:\\xxx\\Local\\Temp\\tmpqfqdmr3f\\kernel.cu failed [command: nvcc --cubin -arch sm_75 -m64 -Id:\\xxx\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pycuda\\cuda kernel.cu] [stdout: kernel.cu(6): error: missing closing quote kernel.cu(7): error: missing closing quote 2 errors detected in the compilation of \"kernel.cu\". kernel.cu ] is it something wrong with my compiler\uff1f plz help if anyone have ideas(Sorry for the long question)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I tried to run the Nvidia TensoRT's python samples, but got an error importing pycuda: ImportError: .../pycuda-2020.1-py3.6-linux-x86_64.egg/pycuda/_driver.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN5boost6detail12set_tss_dataEPKvPFvPFvPvES3_ES5_S3_b My environment: Nvidia Driver Version: 460.73.01 CUDA Version: 10.0 CUDNN Version: 7.6.4 Operating System + Version: Ubuntu 16.04 Python Version: 3.6.13 How I installed pycuda: pip install numpy==1.16 sudo apt-get install build-essential python-dev python-setuptools libboost-python-dev libboost-thread-dev -y install boost: ./bootstrap.sh --with-libraries=python ./b2 --with-python include=\".../include/python3.6m/\" sudo ./b2 install install pycuda: I first run ./configure.py --cuda-root=/usr/local/cuda-10.0, then I changed the settings in siteconf.py: USE_SHIPPED_BOOST --&gt; False; BOOST_PYTHON_LIBNAME --&gt; ['boost_python36']. Then make sudo make install After installing PyCuda, I installed TensorFlow 1.15 and TensorRT 7. The error happened on the line from pycuda._driver import * # noqa. Please teach me how to address this problem. Thanks",
        "answers": [
            [
                "I solved this problem by downloading the files in this repo, placing them into the pycuda folder. Then I set USE_SHIPPED_BOOST = True and wrapped it up."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I need to multiply a matrix with its transpose and I am running out of memory on my GPU with eror message numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY I am expecting the size of my matrix to be around 10k rows and 100k columns so multiplying it with its trnspose will give a result of a square matrix of 10k rows and 10k columns. The matrix only contains 0 and 1. This is the script that I am running. from numba import cuda, uint16 import numba import numpy import math import time TPB = 16 @cuda.jit() def matmul_shared_mem(A, B, C): sA = cuda.shared.array((TPB, TPB), dtype=uint16) sB = cuda.shared.array((TPB, TPB), dtype=uint16) x, y = cuda.grid(2) tx = cuda.threadIdx.x ty = cuda.threadIdx.y if x &gt;= C.shape[0] and y &gt;= C.shape[1]: return tmp = 0. for i in range(int(A.shape[1] / TPB)): sA[tx, ty] = A[x, ty + i * TPB] sB[tx, ty] = B[tx + i * TPB, y] cuda.syncthreads() for j in range(TPB): tmp += sA[tx, j] * sB[j, ty] cuda.syncthreads() C[x, y] = tmp A = numpy.random.randint(2, size=(TPB * 625, 50000)) B = A.transpose() C_shared_mem = cuda.device_array((A.shape[0], B.shape[1])) threads_per_block = (TPB, TPB) blocks_per_grid_x = int(math.ceil(A.shape[0] / threads_per_block[0])) blocks_per_grid_y = int(math.ceil(B.shape[1] / threads_per_block[1])) blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y) start_gpu_shared_memory = time.time() matmul_shared_mem[blocks_per_grid, threads_per_block](A, B, C_shared_mem) cuda.synchronize() end_gpu_shared_memory = time.time() time_gpu_shared = end_gpu_shared_memory - start_gpu_shared_memory print(\"GPU time(shared memory):\" + str(time_gpu_shared)) Update 1: Based on your suggestions, I made certain changes but well I am still running out of memory. import numpy as np import numba as nb colm = int(200000/8) rows = 100000 cols = int(colm*8) AU = np.random.randint(2,size=(rows, cols),dtype=np.int8) A = np.empty((rows,colm), dtype=np.uint8) @nb.njit('void(uint8[:,:],int8[:,:])', parallel=True) def compute(A, AU): for i in nb.prange(A.shape[0]): for j in range(A.shape[1]): offset = j * 8 res = AU[i,offset] &lt;&lt; 7 res |= AU[i,offset+1] &lt;&lt; 6 res |= AU[i,offset+2] &lt;&lt; 5 res |= AU[i,offset+3] &lt;&lt; 4 res |= AU[i,offset+4] &lt;&lt; 3 res |= AU[i,offset+5] &lt;&lt; 2 res |= AU[i,offset+6] &lt;&lt; 1 res |= AU[i,offset+7] A[i,j] = res compute(A, AU) from numba import cuda, uint8, int32 import numba import numpy as np import math import time TPB = 8 TPB1 = 9 @cuda.jit() def bit_A_AT(A, C): sA = cuda.shared.array((TPB, TPB), dtype=uint8) sB = cuda.shared.array((TPB, TPB1), dtype=uint8) x, y = cuda.grid(2) tx = cuda.threadIdx.x ty = cuda.threadIdx.y bx = cuda.blockIdx.x by = cuda.blockIdx.y if bx &gt;= by: tmp = int32(0) for i in range((A.shape[1]+TPB-1) // TPB): if y &lt; A.shape[0] and (i*TPB+tx) &lt; A.shape[1]: sA[ty, tx] = A[y, i*TPB+tx] else: sA[ty, tx] = 0 if (TPB*bx+ty) &lt; A.shape[0] and (i*TPB+tx) &lt; A.shape[1]: sB[ty, tx] = A[TPB*bx+ty, i*TPB+tx] else: sB[ty, tx] = 0 cuda.syncthreads() for j in range(TPB): tmp1 = sA[ty,j] &amp; sB[tx, j] test = uint8(1) for k in range(8): if (tmp1 &amp; test) &gt; 0: tmp += 1 test &lt;&lt;= 1 cuda.syncthreads() if y &lt; C.shape[0] and x &lt; C.shape[1]: C[y, x] = tmp C = np.empty((A.shape[0], A.shape[0]), dtype=np.int32) threads_per_block = (TPB, TPB) blocks_per_grid_x = int(math.ceil(A.shape[0] / threads_per_block[0])) blocks_per_grid_y = int(math.ceil(A.shape[0] / threads_per_block[1])) blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y) start_gpu_shared_memory = time.time() bit_A_AT[blocks_per_grid, threads_per_block](A, C) cuda.synchronize() end_gpu_shared_memory = time.time() time_gpu_shared = end_gpu_shared_memory - start_gpu_shared_memory print(\"GPU time(shared memory):\" + str(time_gpu_shared)) Any idea how I can fiix this?",
        "answers": [
            [
                "The following method should reduce the amount of device memory required for the calculation of A x AT. We'll use the following ideas: since the input array (A) only takes on values of 0,1, we'll reduce the storage for that array down to the minimum convenient size, int8, i.e. one byte per element since the B array is just the transpose of the A array, there is no need to handle it explicitly. We can derive it from the A array, somehwhat similar to here, although that is performing AT x A the matrix multiplication of A x AT involves taking the dot-product of the rows of matrix A, as indicated here we will provide the A transposed version in the sB array using adjusted indexing there are a range of other changes to your code, to address various errors and also to improve load/store efficiency, such as a general reversal of your usage of x,y indices I've also fixed your usage of syncthreads and modified the code to allow arbitrary values for row and column dimensions Here is a worked example: $ cat t62.py from numba import cuda, int32, int8 import numba import numpy as np import math import time TPB = 32 TPB1 = TPB+1 @cuda.jit() def byte_A_AT(A, C): sA = cuda.shared.array((TPB, TPB), dtype=int8) sB = cuda.shared.array((TPB, TPB1), dtype=int8) x, y = cuda.grid(2) tx = cuda.threadIdx.x ty = cuda.threadIdx.y bx = cuda.blockIdx.x by = cuda.blockIdx.y # uncomment and indent remainder of kernel to only do the \"symmetric half\" of calculation # if bx &gt;= by: tmp = int32(0) for i in range((A.shape[1]+TPB-1)// TPB): if y &lt; A.shape[0] and (i*TPB+tx) &lt; A.shape[1]: sA[ty, tx] = A[y, i*TPB+tx] else: sA[ty, tx] = 0 if (TPB*bx+ty) &lt; A.shape[0] and (i*TPB+tx) &lt; A.shape[1]: sB[ty, tx] = A[TPB*bx+ty, i*TPB+tx] else: sB[ty, tx] = 0 cuda.syncthreads() for j in range(TPB): tmp += int32(sA[ty,j]) * int32(sB[tx, j]) cuda.syncthreads() if y &lt; C.shape[0] and x &lt; C.shape[1]: C[y, x] = tmp rows = 1041 cols = 1043 print('host mem: ', (rows*cols*2+rows*rows*4*2)//1048576, 'MB device mem: ', (rows*cols+rows*rows*4)//1048576, 'MB') A = np.random.randint(2,size=(rows, cols),dtype=np.int8) AT = A.transpose() CU = np.matmul(A,AT, dtype = np.int32) C = np.empty((A.shape[0], A.shape[0]), dtype=np.int32) threads_per_block = (TPB, TPB) blocks_per_grid_x = int(math.ceil(A.shape[0] / threads_per_block[0])) blocks_per_grid_y = int(math.ceil(A.shape[0] / threads_per_block[1])) blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y) byte_A_AT[blocks_per_grid, threads_per_block](A, C) cuda.synchronize() start_gpu_shared_memory = time.time() byte_A_AT[blocks_per_grid, threads_per_block](A, C) cuda.synchronize() end_gpu_shared_memory = time.time() time_gpu_shared = end_gpu_shared_memory - start_gpu_shared_memory print(\"GPU time(shared memory):\" + str(time_gpu_shared)) test = np.array_equal(C, CU) print(test) if test == False: for i in range(C.shape[0]): for j in range(C.shape[1]): if C[i,j] != CU[i,j]: print(i, ' ' , j ,' ' , C[i,j] , ' ' , CU[i,j]) $ python t62.py host mem: 10 MB device mem: 5 MB GPU time(shared memory):0.019593000411987305 True $ Notes: most of the runtime of the above code will be spent in python (in the np.matmul() operation, which is really only used to verify results, it should not be necessary for an actual implementation), not in the GPU portion. As the matrices are made larger, the code will run much more slowly. as mentioned in the comments, the result of A x AT is a symmetric matrix. My code does not take advantage of this, however we could crudely take advantage of it by uncommenting the if test at the beginning of the kernel and then indenting the remainder of the kernel. However this will cause the host code np.array_equal test to fail, of course. the device memory consumption for this is calculated in the code. for your largest value in the comments (rows = 30k, cols = 200k) this would amount to about 10GB, so it will still not run in your 8GB GPU. I have created a version of this code which packs 8 elements per byte for the A matrix, which would further reduce memory demand, however writing this code to handle the case of arbitrary column dimensions (vs. multiples of 8) proves to be rather messy. However that code could get the total device memory consumption down to about 5GB for 30k rows and 200k columns case. Here is the one bit per element version, it has a requirement that the number of columns be whole-number divisible by 8: $ cat t61.py from numba import cuda, uint8, int32 import numba import numpy as np import math import time TPB = 32 TPB1 = 33 @cuda.jit() def bit_A_AT(A, C): sA = cuda.shared.array((TPB, TPB), dtype=uint8) sB = cuda.shared.array((TPB, TPB1), dtype=uint8) x, y = cuda.grid(2) tx = cuda.threadIdx.x ty = cuda.threadIdx.y bx = cuda.blockIdx.x by = cuda.blockIdx.y tmp = int32(0) for i in range((A.shape[1]+TPB-1) // TPB): if y &lt; A.shape[0] and (i*TPB+tx) &lt; A.shape[1]: sA[ty, tx] = A[y, i*TPB+tx] else: sA[ty, tx] = 0 if (TPB*bx+ty) &lt; A.shape[0] and (i*TPB+tx) &lt; A.shape[1]: sB[ty, tx] = A[TPB*bx+ty, i*TPB+tx] else: sB[ty, tx] = 0 cuda.syncthreads() for j in range(TPB): tmp1 = sA[ty,j] &amp; sB[tx, j] test = uint8(1) for k in range(8): if (tmp1 &amp; test) &gt; 0: tmp += 1 test &lt;&lt;= 1 cuda.syncthreads() if y &lt; C.shape[0] and x &lt; C.shape[1]: C[y, x] = tmp colm = 131 rows = 1041 cols = int(colm*8) print('host mem: ', (rows*cols*2+rows*rows*4*2)//1048576, 'MB device mem: ', (((rows*cols)//8)+rows*rows*4)//1048576, 'MB') AU = np.random.randint(2,size=(rows, cols),dtype=np.int8) AUT = AU.transpose() CU = np.matmul(AU,AUT,dtype=np.int32) A = np.empty((rows,colm), dtype=np.uint8) for i in range(A.shape[0]): for j in range(A.shape[1]): A[i,j] = 0 for k in range(8): if AU[i,(j*8)+k] == 1: A[i,j] = A[i,j] | (1&lt;&lt;(7-k)) C = np.empty((A.shape[0], A.shape[0]), dtype=np.int32) threads_per_block = (TPB, TPB) blocks_per_grid_x = int(math.ceil(A.shape[0] / threads_per_block[0])) blocks_per_grid_y = int(math.ceil(A.shape[0] / threads_per_block[1])) blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y) bit_A_AT[blocks_per_grid, threads_per_block](A, C) cuda.synchronize() start_gpu_shared_memory = time.time() bit_A_AT[blocks_per_grid, threads_per_block](A, C) cuda.synchronize() end_gpu_shared_memory = time.time() time_gpu_shared = end_gpu_shared_memory - start_gpu_shared_memory print(\"GPU time(shared memory):\" + str(time_gpu_shared)) test = np.array_equal(C, CU) print(test) if test == False: for i in range(C.shape[0]): for j in range(C.shape[1]): if C[i,j] != CU[i,j]: print(i, ' ' , j ,' ' , C[i,j] , ' ' , CU[i,j]) break $ python t61.py host mem: 10 MB device mem: 4 MB GPU time(shared memory):0.009343624114990234 True $ EDIT: Responding to some questions in the comments, updates, and now taking into account that the A matrix may have significantly more than 30k rows, this will cause the C matrix to increase as well. If the A matrix can be fit in GPU memory, we can reduce the memory demand of the C matrix by computing it in pieces. These pieces will be a group of rows computed together, which I refer to as a row_slice of the C matrix. The following code demonstrates that this can be achieved with relatively minor changes to the code above: $ cat t63.py from numba import cuda, uint8, int32 import numba as nb import numpy as np import math import time TPB = 32 TPB1 = 33 @cuda.jit() def bit_slice_A_AT(A, C, row_offset): sA = cuda.shared.array((TPB, TPB), dtype=uint8) sB = cuda.shared.array((TPB, TPB1), dtype=uint8) x, y = cuda.grid(2) tx = cuda.threadIdx.x ty = cuda.threadIdx.y bx = cuda.blockIdx.x by = cuda.blockIdx.y tmp = int32(0) for i in range((A.shape[1]+TPB-1) // TPB): if y+row_offset &lt; A.shape[0] and (i*TPB+tx) &lt; A.shape[1]: sA[ty, tx] = A[y+row_offset, i*TPB+tx] else: sA[ty, tx] = 0 if (TPB*bx+ty) &lt; A.shape[0] and (i*TPB+tx) &lt; A.shape[1]: sB[ty, tx] = A[TPB*bx+ty, i*TPB+tx] else: sB[ty, tx] = 0 cuda.syncthreads() for j in range(TPB): tmp1 = sA[ty,j] &amp; sB[tx, j] test = uint8(1) for k in range(8): if (tmp1 &amp; test) &gt; 0: tmp += 1 test &lt;&lt;= 1 cuda.syncthreads() if y &lt; C.shape[0] and x &lt; C.shape[1]: C[y, x] = tmp @nb.njit('void(uint8[:,:],int8[:,:])', parallel=True) def bitpack(A, AU): for i in nb.prange(A.shape[0]): for j in range(A.shape[1]): offset = j * 8 res = AU[i,offset] &lt;&lt; 7 res |= AU[i,offset+1] &lt;&lt; 6 res |= AU[i,offset+2] &lt;&lt; 5 res |= AU[i,offset+3] &lt;&lt; 4 res |= AU[i,offset+4] &lt;&lt; 3 res |= AU[i,offset+5] &lt;&lt; 2 res |= AU[i,offset+6] &lt;&lt; 1 res |= AU[i,offset+7] A[i,j] = res colm = 131 rows = 1535 cols = int(colm*8) row_slice = 512 print('host mem: ', (rows*cols*2+rows*rows*4*2)//1048576, 'MB device mem: ', (((rows*cols)//8)+row_slice*rows*4)//1048576, 'MB') AU = np.random.randint(2,size=(rows, cols),dtype=np.int8) CU = np.matmul(AU,AU.T,dtype=np.int32) A = np.empty((rows,colm), dtype=np.uint8) bitpack(A, AU) A_dev = cuda.to_device(A) threads_per_block = (TPB, TPB) C = np.empty((row_slice, A.shape[0]), dtype=np.int32) blocks_per_grid_x = int(math.ceil(A.shape[0] / threads_per_block[0])) blocks_per_grid_y = int(row_slice / threads_per_block[1]) blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y) for i in range((A.shape[0]+row_slice-1)//row_slice): bit_slice_A_AT[blocks_per_grid, threads_per_block](A_dev, C, i*row_slice) lower = i*row_slice upper = min(lower+row_slice, CU.shape[0]) width = upper-lower test = np.array_equal(C[:width,:], CU[i*row_slice:i*row_slice+width,:]) print(test) cuda.synchronize() C_dev = cuda.device_array_like(C) start_gpu_shared_memory = time.time() for i in range((A.shape[0]+row_slice-1)//row_slice): bit_slice_A_AT[blocks_per_grid, threads_per_block](A_dev, C_dev, i*row_slice) cuda.synchronize() end_gpu_shared_memory = time.time() time_gpu_shared = end_gpu_shared_memory - start_gpu_shared_memory print(\"GPU time(shared memory):\" + str(time_gpu_shared)) $ python t63.py host mem: 21 MB device mem: 3 MB True True True GPU time(shared memory):0.010116815567016602 $ This means, as suggested, for the case of rows = 100k and columns = 200k as given in the latest update to the question, we should be able to divide the C matrix into chunks of say 5k rows. The memory usage for the A matrix would be 2.5GB, but for the C matrix, since we are only computing a 5k row slice at a time, the device memory storage required would be 100k*5k*4 bytes, so 2GB for this example. After some further study, we can speed up the host code matmul operation by switching from int8 datatype to float32 datatype. This makes that op quite a bit faster, but the GPU code still seems to ~4x faster than that: $ cat t64.py from numba import cuda, uint8, int32 import numba as nb import numpy as np import math import time TPB = 32 TPB1 = 33 @cuda.jit() def bit_slice_A_AT(A, C, row_offset): sA = cuda.shared.array((TPB, TPB), dtype=uint8) sB = cuda.shared.array((TPB, TPB1), dtype=uint8) x, y = cuda.grid(2) tx = cuda.threadIdx.x ty = cuda.threadIdx.y bx = cuda.blockIdx.x by = cuda.blockIdx.y tmp = int32(0) for i in range((A.shape[1]+TPB-1) // TPB): if y+row_offset &lt; A.shape[0] and (i*TPB+tx) &lt; A.shape[1]: sA[ty, tx] = A[y+row_offset, i*TPB+tx] else: sA[ty, tx] = 0 if (TPB*bx+ty) &lt; A.shape[0] and (i*TPB+tx) &lt; A.shape[1]: sB[ty, tx] = A[TPB*bx+ty, i*TPB+tx] else: sB[ty, tx] = 0 cuda.syncthreads() for j in range(TPB): tmp1 = sA[ty,j] &amp; sB[tx, j] test = uint8(1) for k in range(8): if (tmp1 &amp; test) &gt; 0: tmp += 1 test &lt;&lt;= 1 cuda.syncthreads() if y &lt; C.shape[0] and x &lt; C.shape[1]: C[y, x] = tmp @nb.njit('void(uint8[:,:],float32[:,:])', parallel=True) def bitpack(A, AU): for i in nb.prange(A.shape[0]): for j in range(A.shape[1]): offset = j * 8 res = int(AU[i,offset]) &lt;&lt; 7 res |= int(AU[i,offset+1]) &lt;&lt; 6 res |= int(AU[i,offset+2]) &lt;&lt; 5 res |= int(AU[i,offset+3]) &lt;&lt; 4 res |= int(AU[i,offset+4]) &lt;&lt; 3 res |= int(AU[i,offset+5]) &lt;&lt; 2 res |= int(AU[i,offset+6]) &lt;&lt; 1 res |= int(AU[i,offset+7]) A[i,j] = res colm = 1000 rows = 6000 cols = int(colm*8) row_slice = 512 print('host mem: ', (rows*cols*4+rows*colm+rows*rows*4+rows*row_slice*4)//1048576, 'MB device mem: ', (((rows*cols)//8)+row_slice*rows*4)//1048576, 'MB') t1 = time.time() AU = np.random.randint(2,size=(rows, cols),dtype=np.int8) AU = AU.astype(np.float32) print(\"randint:\" + str(time.time()-t1)) t1 = time.time() #CU = np.empty((rows, rows), dtype=np.int32) CU = np.matmul(AU,AU.T,dtype=np.float32) print(\"matmul:\" + str(time.time()-t1)) t1 = time.time() A = np.empty((rows,colm), dtype=np.uint8) print(\"np.empty:\" + str(time.time()-t1)) t1 = time.time() bitpack(A, AU) print(\"bitpack:\" + str(time.time()-t1)) t1 = time.time() A_dev = cuda.to_device(A) threads_per_block = (TPB, TPB) C = np.empty((row_slice, A.shape[0]), dtype=np.int32) blocks_per_grid_x = int(math.ceil(A.shape[0] / threads_per_block[0])) blocks_per_grid_y = int(row_slice / threads_per_block[1]) blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y) for i in range((A.shape[0]+row_slice-1)//row_slice): bit_slice_A_AT[blocks_per_grid, threads_per_block](A_dev, C, i*row_slice) lower = i*row_slice upper = min(lower+row_slice, CU.shape[0]) width = upper-lower test = np.array_equal(C[:width,:], CU[i*row_slice:i*row_slice+width,:]) print(test) cuda.synchronize() C_dev = cuda.device_array_like(C) start_gpu_shared_memory = time.time() for i in range((A.shape[0]+row_slice-1)//row_slice): bit_slice_A_AT[blocks_per_grid, threads_per_block](A_dev, C_dev, i*row_slice) cuda.synchronize() end_gpu_shared_memory = time.time() time_gpu_shared = end_gpu_shared_memory - start_gpu_shared_memory print(\"GPU time(shared memory):\" + str(time_gpu_shared)) $ python t64.py host mem: 337 MB device mem: 17 MB randint:0.1817936897277832 matmul:3.498671531677246 np.empty:7.62939453125e-05 bitpack:0.03707313537597656 True True True True True True True True True True True True GPU time(shared memory):0.8318064212799072 $ I haven't thoroughly tested these codes. Bugs may exist. Use at your own risk. For attribution, the numba bit-packing code seems to have come from here."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I am trying to use a TensorRT engine for inference in a python class that inherits from multiprocessing. The engine works in a standalone python script on my system, but now while integrating it into the codebase, the multiprocessing used in the class seems to be causing problems. I am not getting any errors. It just skips everything after the line self.runtime = trt.Runtime(self.trt_logger). My debugger from vscode does not go into the function either. In the docs the following is mentioned, that I do not fully understand: The TensorRT builder may only be used by one thread at a time. If you need to run multiple builds simultaneously, you will need to create multiple builders. The TensorRT runtime can be used by multiple threads simultaneously, so long as each object uses a different execution context. The following parts of my code are started, joined and terminated from another file: # more imports import logging import multiprocessing import tensorrt as trt import pycuda.driver as cuda import pycuda.autoinit class MyClass(multiprocessing.Process): def __init__(self, messages): multiprocessing.Process.__init__(self) # other stuff self.exit = multiprocessing.Event() def load_tensorrt_model(self, config): '''Load tensorrt model with engine''' logging.debug('Start') # Reading the config parameters related to the engine engine_file = config['trt_engine']['trt_folder'] + os.path.sep + config['trt_engine']['engine_file'] class_names_file = config['trt_engine']['trt_folder'] + os.path.sep + config['trt_engine']['class_names_file'] # Verify if all the necessary files are present, if so load the detection network if os.path.exists(engine_file) and os.path.exists(class_names_file): try: logging.debug('In try statement') self.trt_logger = trt.Logger() f = open(engine_file, 'rb') logging.debug('I can get here, but no further') self.runtime = trt.Runtime(self.trt_logger) logging.debug('Cannot get here') self.engine = self.runtime.deserialize_cuda_engine(f.read()) # More stuff I have found someone with a multithreading problem, but as of now I was unable to use this to solve my problem. Any help is appreciated. System specs: Python 3.6.9 Jetson NX Jetpack 4.4.1 L4T 32.4.4 Tensorrt 7.1.3.0-1 Cuda10.2 Ubuntu 18.04",
        "answers": [
            [
                "same problem. It seems pycuda autoinit not working well under a multi process scenario. try to replace import pycuda.autoinit with cuda.init() self.cuda_context = cuda.Device(0).make_context()"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am exploring to move from OpenCL to CUDA, and did a few tests to benchmark the speed of CUDA in various implementations. To my surprise, in the examples below, the PyCUDA implementation is about 20% faster than the C CUDA example. I read many posts talking about \"release build\" of C CUDA code. I did try having -Xptxas -O3 in the makefile and that really did not make a difference. I also tried to adjust the block size, with which the kernel was executed. Unfortunately, it did not help improve the speed, either. My questions here are: What could be the reasons leading to the speed difference between C CUDA and PYCUDA? If the \"advanced\" (lack of a better word) compiling in PYCUDA is one of reasons, how can I optimize the compiling of my C CUDA code? Are there any other ways to improve the speed of C CUDA in this case? While I appreciate general comments, I am looking for actionable suggestions that I can validate on my machine. Thanks! import pycuda.autoinit import pycuda.driver as drv import numpy as np from pycuda.compiler import SourceModule import time mod = SourceModule( \"\"\" __global__ void saxpy(int n, const float a, float *x, float *y) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i &lt; n){ y[i] = a * x[i] + y[i]; } } \"\"\" ) saxpy = mod.get_function(\"saxpy\") N = 1 &lt;&lt; 25 time_elapse = 0.0 for i in range(100): # print(i) # print(N) x = np.ones(N).astype(np.float32) y = 2 * np.ones(N).astype(np.float32) start = time.time() saxpy( np.int32(N), np.float32(2.0), drv.In(x), drv.InOut(y), block=(512, 1, 1), grid=(int(N / 512) + 1, 1), ) time_elapse += (time.time() - start) print(time_elapse ) print(y[-100:-1]) print(y.sum()) print(N * 4.0) #include &lt;stdio.h&gt; #include &lt;time.h&gt; #define DIM 512 __global__ void saxpy(int n, float a, float *x, float *y) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i &lt; n) y[i] = a * x[i] + y[i]; } int main(int num_iterations) { double start; double cputime; int N = 1 &lt;&lt; 25; float *x, *y, *d_x, *d_y; int i, j; for (j = 0; j &lt; num_iterations; j++) { x = (float *)malloc(N * sizeof(float)); y = (float *)malloc(N * sizeof(float)); cudaMalloc(&amp;d_x, N * sizeof(float)); cudaMalloc(&amp;d_y, N * sizeof(float)); for (i = 0; i &lt; N; i++) { x[i] = 1.0f; y[i] = 2.0f; } cudaMemcpy(d_x, x, N * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(d_y, y, N * sizeof(float), cudaMemcpyHostToDevice); // Perform SAXPY on 1M elements start = clock(); saxpy&lt;&lt;&lt;(N + DIM) / DIM, DIM&gt;&gt;&gt;(N, 2.0f, d_x, d_y); cputime += ((double)(clock() - start) / CLOCKS_PER_SEC); cudaMemcpy(y, d_y, N * sizeof(float), cudaMemcpyDeviceToHost); // float maxError = 0.0f; // for (int i = 0; i &lt; N; i++){ // maxError = max(maxError, abs(y[i] - 4.0f)); // //printf(\"y[%d]: %f\\n\", i,y[i]); // } // printf(\"Max error: %f\\n\", maxError); cudaFree(d_x); cudaFree(d_y); free(x); free(y); } printf(\"cpu time is %f\\n\", cputime); return 0; } I saved the above file as cuda_example.cu and compile it with the following commands in a makefile: nvcc -arch=sm_61 -Xptxas -O3,-v -o main cuda_example.cu",
        "answers": [
            [
                "If I execute your CUDA-C code as is, and set num_iterations to 300 like this: int num_iterations =300; then the execution of your program takes about 60s on a Geforce GTX 1650. Your code is extremely inefficient, as you copy data back and forth between GPU and device at every iteration. So, lets restrict the loop to just the kernel execution: #include &lt;stdio.h&gt; #include &lt;time.h&gt; #define DIM 512 __global__ void saxpy(int n, float a, float *x, float *y) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i &lt; n) y[i] = a * x[i] + y[i]; } int main() { double start = clock(); int N = 1 &lt;&lt; 25; float *x, *y, *d_x, *d_y; int i, j; int num_iterations = 300; x = (float *)malloc(N * sizeof(float)); y = (float *)malloc(N * sizeof(float)); cudaMalloc(&amp;d_x, N * sizeof(float)); cudaMalloc(&amp;d_y, N * sizeof(float)); for (i = 0; i &lt; N; i++) { x[i] = 1.0f; y[i] = 2.0f; } cudaMemcpy(d_x, x, N * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(d_y, y, N * sizeof(float), cudaMemcpyHostToDevice); for (j = 0; j &lt; num_iterations; j++){ saxpy&lt;&lt;&lt;(N + DIM) / DIM, DIM&gt;&gt;&gt;(N, 2.0f, d_x, d_y); cudaDeviceSynchronize(); } cudaMemcpy(y, d_y, N * sizeof(float), cudaMemcpyDeviceToHost); cudaFree(d_x); cudaFree(d_y); free(x); free(y); double cputime = ((double)(clock() - start) / CLOCKS_PER_SEC); printf(\"cpu time is %f\\n\", cputime); return 0; } If I do that, then the execution time becomes 1.36 seconds. Doing sth similar to the PyCUDA code I got about 19s of execution time."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a very simple function where I'm passing in a char array and doing a simple character match. I want to return an array of 1/0 depending on which characters are matched. Problem: although I can see the value has been set in the data structure (as I print it in the function after it's assigned) when the int array is copied back from the device the values aren't as expected. I'm sure it's something silly. import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np mod = SourceModule(\"\"\" __global__ void test(const char *q, const int chrSize, int *d, const int intSize) { int v = 0; if( q[threadIdx.x * chrSize] == 'a' || q[threadIdx.x * chrSize] == 'c' ) { v = 1; } d[threadIdx.x * intSize] = v; printf(\"x=%d, y=%d, val=%c ret=%d\\\\n\", threadIdx.x, threadIdx.y, q[threadIdx.x * chrSize], d[threadIdx.x * intSize]); } \"\"\") func = mod.get_function(\"test\") # input data a = np.asarray(['a','b','c','d'], dtype=np.str_) # allocate/copy to device a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) # destination array d = np.zeros((4), dtype=np.int16) # allocate/copy to device d_gpu = cuda.mem_alloc(d.nbytes) cuda.memcpy_htod(d_gpu, d) # run the function func(a_gpu, np.int8(a.dtype.itemsize), d_gpu, np.int8(d.dtype.itemsize), block=(4,1,1)) # copy data back and priint cuda.memcpy_dtoh(d, d_gpu) print(d) Output: x=0, y=0, val=a ret=1 x=1, y=0, val=b ret=0 x=2, y=0, val=c ret=1 x=3, y=0, val=d ret=0 [1 0 0 0] Expected output: x=0, y=0, val=a ret=1 x=1, y=0, val=b ret=0 x=2, y=0, val=c ret=1 x=3, y=0, val=d ret=0 [1 0 1 0]",
        "answers": [
            [
                "You have two main problems, neither of which have anything to do with memcpy_dtoh: You have declared d and d_gpu as dtype np.int16, but the kernel is expecting C++ int, leading to a type mistmatch. You should use the np.int32 type to define the arrays. The indexing of d within the kernel is incorrect. If you have declared the array to the compiler as a 32 bit type, indexing the array as d[threadIdx.x] will automatically include the correct alignment for the type. Passing and using intSize to the kernel for indexing d is not required and it is incorrect to do so. If you fix those two issues, I suspect the code will work as intended."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Closed. This question needs details or clarity. It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post. Closed 2 years ago. Improve this question I am trying to apply a matrix inverse to a given matrix, but the kernel only works for matrices up to 5x5. If I use any matrix bigger in dimension, The results are incorrect. mod1 = SourceModule(\"\"\" __global__ void invert(float* A, float* I, int n) { int tx = blockIdx.x * blockDim.x + threadIdx.x; int ty = blockIdx.y * blockDim.y + threadIdx.y; for(int i = 0; i &lt; n; i++) { int col = i * n + ty; int row = tx * n + i; if (tx == i) { I[tx * n + ty] /= A[i * n + i]; A[tx * n + ty] /= A[i * n + i]; } if (tx != i) { I[tx * n + ty] -= I[col] * A[row]; A[tx * n + ty] -= A[col] * A[row]; } } }\"\"\" )",
        "answers": [
            [
                "The code is not correct to make inverse of matrix, you can't calculate all elements of I in parallel. I am even amazed that it worked for matrices up to 5x5. The correct solution is to consider the rows of the matrix one by one from first to last in series, divide the row in both matrices by the diagonal element in A, then subtract it from all following rows after multiplying it with the element of each row that under the diagonal element, you can do this step in parallel. Finally, after finishing this for all rows, do the same thing backward from last row to first row, this code can clarify this to you: void inverse(float* A, float* I, int n) { int i, j; size_t size; float v; float * d_A, * d_I, * d_v; size = (unsigned __int64)n * n * sizeof(float); cudaMalloc(&amp;d_A, size); cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice); cudaMalloc(&amp;d_I, size); cudaMalloc(&amp;d_v, sizeof(float)); for (i = 0; i &lt; n; i++) I[i * n + i] = 1; for (i = 0; i &lt; n; i++) { GetVal&lt;&lt;&lt;1, 1&gt;&gt;&gt;(d_A, i * (n + 1), d_v); \\\\ Get value of diagonal element cudaMemcpy(&amp;v, d_v, sizeof(float), cudaMemcpyDeviceToHost); if (i != n - 1) \\\\ Divide row in A matrix starting from element after diagonal DivideRow&lt;&lt;&lt;1, n - i - 1&gt;&gt;&gt;(d_A, i * (n + 1) + 1, n - i - 1, v); DivideRow&lt;&lt;&lt;1, n&gt;&gt;&gt;(d_I, i * n, n, v); \\\\ Divide row in I matrix cudaDeviceSynchronize(); if (i != n - 1) \\\\ Subtracting rows { dim3 GridA(1, 1); dim3 BlockA(n - i - 1, n - i - 1); dim3 GridI(1, 1); dim3 BlockI(n - i - 1, n); ModifyRow&lt;&lt;&lt;GridA, BlockA&gt;&gt;&gt;(d_A, i, i, i + 1, n - i - 1, n - i - 1); ModifyRow&lt;&lt;&lt;GridI, BlockI&gt;&gt;&gt;(d_A, n, i, i, d_I, i + 1, 0, n - i - 1, n); cudaDeviceSynchronize(); } } cudaFree(d_v); for (i = n - 1; i &gt; 0; i--) \\\\ Backward subtraction { dim3 GridI(1, 1); dim3 BlockI(i, n); ModifyRow&lt;&lt;&lt;GridI, BlockI&gt;&gt;&gt;(d_A, n, i, i, d_I, 0, 0, i, n); cudaDeviceSynchronize(); } cudaMemcpy(I, d_I, size, cudaMemcpyDeviceToHost); cudaFree(d_A); cudaFree(d_I); } __global__ void GetVal(float* A, int p, float* v) { v[0] = A[p]; } __global__ void DivideRow(float* A, int s, int n, floatd) { int c; c = blockIdx.x * blockDim.x + threadIdx.x; if (c &lt; n) A[s + c] /= d; } __global__ void ModifyRow(float* MM, int n, int fr, int fc, float* A, int sr, int sc, int nr, int nc) { int r, c, nA; r = blockIdx.x * blockDim.x + threadIdx.x; if (r &gt;= nr) return; c = blockIdx.y * blockDim.y + threadIdx.y; if (c &gt;= nc) return; nA = sc + nc; A[(sr + r) * nA + sc + c] -= MM[(sr + r) * n + fc] * A[fr * nA + sc + c]; } Just be careful that the maximum block size is 1024, so if your matrix is larger than 32x32, you will have to modify grid and block sizes."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using tensorRT to perform inference with CUDA. I'd like to use CuPy to preprocess some images that I'll feed to the tensorRT engine. The preprocessing function, called my_function, works fine as long as tensorRT is not run between different calls of the my_function method (see code below). Specifically, the issue is not strictly related by tensorRT but by the fact that tensorRT inference requires to be wrapped by push and pop operations of the pycuda context. With respect to the following code, the last execution of my_function will raise the following error: File \"/home/ubuntu/myfile.py\", line 188, in _pre_process_cuda img = ndimage.zoom(img, scaling_factor) File \"/home/ubuntu/.local/lib/python3.6/site-packages/cupyx/scipy/ndimage/interpolation.py\", line 482, in zoom kern(input, zoom, output) File \"cupy/core/_kernel.pyx\", line 822, in cupy.core._kernel.ElementwiseKernel.__call__ File \"cupy/cuda/function.pyx\", line 196, in cupy.cuda.function.Function.linear_launch File \"cupy/cuda/function.pyx\", line 164, in cupy.cuda.function._launch File \"cupy_backends/cuda/api/driver.pyx\", line 299, in cupy_backends.cuda.api.driver.launchKernel File \"cupy_backends/cuda/api/driver.pyx\", line 124, in cupy_backends.cuda.api.driver.check_status cupy_backends.cuda.api.driver.CUDADriverError: CUDA_ERROR_INVALID_HANDLE: invalid resource handle Note: in the following code I haven't reported the entire tensorRT inference code. In fact, simply pushing and popping a pycuda context generates the error Code: import numpy as np import cv2 import time from PIL import Image import requests from io import BytesIO from matplotlib import pyplot as plt import cupy as cp from cupyx.scipy import ndimage import tensorrt as trt import pycuda.driver as cuda import pycuda.autoinit def my_function(numpy_frame): dtype = 'float32' img = cp.array(numpy_frame, dtype='float32') # print(img) img = ndimage.zoom(img, (0.5, 0.5, 3)) img = (cp.array(2, dtype=dtype) / cp.array(255, dtype=dtype)) * img - cp.array(1, dtype=dtype) img = img.transpose((2, 0, 1)) img = img.ravel() return img # load image url = \"https://www.pexels.com/photo/109919/download/?search_query=&amp;tracking_id=411xe21veam\" response = requests.get(url) img = Image.open(BytesIO(response.content)) img = np.array(img) # initialize tensorrt TRT_LOGGER = trt.Logger(trt.Logger.WARNING) trt_runtime = trt.Runtime(TRT_LOGGER) cfx = cuda.Device(0).make_context() my_function(img) # ok my_function(img) # ok # ----- TENSORRT --------- cfx.push() # .... tensorrt inference.... cfx.pop() # ----- TENSORRT --------- my_function(img) # &lt;---- error I even tried to do it other ways, but unfortunately with the same result: cfx.push() my_function(img) # ok cfx.pop() cfx.push() my_function(img) # error cfx.pop() @admin: if you can think of a better name for this question feel free to edit it :)",
        "answers": [
            [
                "There were multiple contexts open. For instance, it seems that all of the following open a context: import pycuda.autoinit cfx.cuda.Device(0).make_context() cfx.push() So if you run the three command above, then simply running one cfx.pop() won't be enough. You will need to run cfx.pop() three times to pop all the contexts."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to sum up an array in the GPU and then obtain it back on the host. For this, I am using the pycuda.gpuarray.sum() function. import pycuda.gpuarray a = np.array([1,2,3,4,5]) b = gpuarray.to_gpu(a) c = gpuarray.sum(b) c = c.get() print(c) #Prints array(15) print(type(c)) #Prints numpy.ndarray print(c[0]) #Error, Index error - too many indices for array print(c.shape) #Prints (), empty tuple How to I obtain the solution of the sum() function back as a normal integer element?",
        "answers": [
            [
                "the function gpuarray.sum() just returns a scalar after summing up all the element, as @Robert Crovella said. So your variable c will always get initialized as a 0-dimensional numpy array (in other words, a scalar), which is why you get the empty tuple as an output, and an error, when you try to access an element within. If you want this to be a 1-dimensional array you can do this: import pycuda.autoinit import pycuda.gpuarray as gpuarray import numpy as np a = np.array([1,2,3,4,5]) d_a = gpuarray.to_gpu(a) d_c = gpuarray.sum(d_a) h_d = np.zeros((1,)) h_d[0] = d_c.get() print(\"np.array h_d: \", h_d) print(\"h_d[0] = \", h_d[0]) (PyCuda 2020.1)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have recently started to work with Cuda, I have multithread, multiprocess coding experience on C++, Java and Python. With PyCuda I see example codes like this, ker = SourceModule(\"\"\" __global__ void scalar_multiply_kernel(float *outvec, float scalar, float *vec) { int i = threadIdx.x; outvec[i] = scalar*vec[i]; } \"\"\") It seems the thread id itself partakes in the logic of the code. Then the question is will there be enough thread ids covering my entire array (whose indexing I apparently need to reach all elements there), and what happens if I change the size of the array. Will the indexing always be between 0 and N?",
        "answers": [
            [
                "In CUDA the thread id is only unique per so-called thread block, meaning, that your example kernel only does the right thing with only one block doing work. This is probably done in early examples to ease you into the ideas, but it is generally a very bad thing to do in terms of performance: With one block, you can only utilize one of many streaming multiprocessors (SMs) in a GPU and even that SM will only be able to hide memory access latencies when it has enough parallel work to do while waiting. A single thread-block also limits you in the number of threads and therefore in the problem-size, if your kernel doesn't contain a loop so every thread can compute more than one element. Kernel execution is seen strongly hierarchically: Restricting ourselves to one dimensional indexing for simplicity, a kernel is executed on a so-called grid of gridDim.x thread blocks, each containing blockDim.x threads numbered per block by threadIdx.x, while each block is numbered via blockIdx.x. To get a unique ID for a thread (in a fashion that ideally uses the hardware to load elements from an array), you have to take blockIdx.x * blockDim.x + threadIdx.x. If more than one element shall be computed by every thread, you use a loop of the form for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; InputSize; i += gridDim.x * blockDim.x) { /* ... */ } This is called a grid-stride loop, because gridDim.x * blockDim.x is the number of all threads working on the kernel. Different strides (especially having a thread working on consecutive elements: stride = 1) might work, but will be much slower due to the non-ideal memory access pattern."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Is there a way to use a memory pointer that has already been allocated to create a GpuMat in OpenCV python? There is the C++ definition for GpuMat() which takes void* data as input. cv::cuda::GpuMat::GpuMat ( Size size, int type, void * data, size_t step = Mat::AUTO_STEP ) cv::cuda::GpuMat::GpuMat ( int rows, int cols, int type, void * data, size_t step = Mat::AUTO_STEP ) There are no equivalent python bindings for these definitions in OpenCV, so this option does not work in Python. I have a PyCuda array which is already created. So, I can access the Cuda pointer of that array. Could we use the existing pointer in Python to create a GpuMat without having to download the data back to the host memory, and upload it again? Looking at the GpuMat class in Python, I do not see a way to access the cudaPtr or data variables of the GpuMat class to allocate something using the defaultAllocator interface.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Sometimes when I change my environment variables in Windows, and then use software the depends on those variables, they are not properly updated. And good example is to change a variable, then open up Windows Command Line and echo the variable and see that it hasn't been changed, even though you properly changed it in the Environment Variables window. Another example I'm dealing with right now: I've been using Python 2.4.x for a while for a project, which uses the env var PYTHONPATH who's value has been: C:\\Python24;C:\\Python24\\lib Today I installed Python 2.5.x for the project. I changed my PYTHONPATH to be: C:\\Python25;C:\\Python25\\lib When I use Python 2.5 to run a script and do this: import sys print sys.path It prints: 'C:\\\\PYTHON24', 'C:\\\\PYTHON24\\\\lib' (and some other Python 2.5 related default installation paths) So clearly, the old PYTHONPATH environment variable changes aren't really sticking.... Does anyone know why this happens and how to fix it?",
        "answers": [
            [
                "When you change an environment variable in the System Properties tab, the new value will propagate to the Windows Explorer, and any apps (such as cmd.exe) opened from the Windows Explorer (or the Run box, Start Menu, etc.) should see the new value. However, if you're running a program such as an editor or python or some non-Microsoft program launcher, then change an environment variable, and then launch cmd.exe from that program (instead of Windows Explorer) you are likely to see the old value of the environment variable. The reason is that the running program ignored the notification from Windows saying that the environment has changed (not at all unusual), and since the launched process inherits the environment variables, the child process won't see the changes. The workaround is to make sure you start your app from Windows Explorer or the Run box. Rebooting your machine will work also (if rebooting doesn't solve the problem, then something else is going on)."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am trying to parallelize the bitonic sort with pycuda. For this I use SourceModule and the C code of the parallel bitonic sort. For the memory copies management I use InOut of the pycuda.driver that simplify some of the memory transfers import pycuda.autoinit import pycuda.driver as drv from pycuda.compiler import SourceModule from pycuda import gpuarray import numpy as np from time import time ker = SourceModule( \"\"\" __device__ void swap(int &amp; a, int &amp; b){ int tmp = a; a = b; b = tmp; } __global__ void bitonicSort(int * values, int N){ extern __shared__ int shared[]; int tid = threadIdx.x + blockDim.x * blockIdx.x; // Copy input to shared mem. shared[tid] = values[tid]; __syncthreads(); // Parallel bitonic sort. for (int k = 2; k &lt;= N; k *= 2){ // Bitonic merge: for (int j = k / 2; j&gt;0; j /= 2){ int ixj = tid ^ j; if (ixj &gt; tid){ if ((tid &amp; k) == 0){ //Sort ascending if (shared[tid] &gt; shared[ixj]){ swap(shared[tid], shared[ixj]); } } else{ //Sort descending if (shared[tid] &lt; shared[ixj]){ swap(shared[tid], shared[ixj]); } } } __syncthreads(); } } values[tid] = shared[tid]; } \"\"\" ) N = 8 #lenght of A A = np.int32(np.random.randint(1, 20, N)) #random numbers in A BLOCK_SIZE = 256 NUM_BLOCKS = (N + BLOCK_SIZE-1)//BLOCK_SIZE bitonicSort = ker.get_function(\"bitonicSort\") t1 = time() bitonicSort(drv.InOut(A), np.int32(N), block=(BLOCK_SIZE,1,1), grid=(NUM_BLOCKS,1), shared=4*N) t2 = time() print(\"Execution Time {0}\".format(t2 - t1)) print(A) As in the kernel I use extern __shared__, in pycuda I use the shared parameter with the respective 4*N. Also try using __shared__ int shared[N] in the kernel but it doesn't work either (check here: Getting started with shared memory on PyCUDA) Running in Google Collab I get the following error: /usr/local/lib/python3.6/dist-packages/pycuda/compiler.py in __init__(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs) 292 293 from pycuda.driver import module_from_buffer --&gt; 294 self.module = module_from_buffer(cubin) 295 296 self._bind_module() LogicError: cuModuleLoadDataEx failed: an illegal memory access was encountered Does anyone know what could be generating this error?",
        "answers": [
            [
                "Your device code isn't accounting for the sizes of your arrays correctly. You are launching 256 threads in a single block. That means that you will have 256 threads, with tid numbered 0..255, trying to execute each line of code. For example, in this case: shared[tid] = values[tid]; You will have, for example, one thread trying to do shared[255] = values[255]; Neither your shared nor values array are that large. That is the reason for the illegal memory access error. The simplest solution for this kind of trivial problem is to make your array sizes match your block size. BLOCK_SIZE = N According to my testing, that change clears up any errors and results in a properly sorted array. It won't work for N greater than 1024, or multi-block usage, but your code would have to be modified for a multi-block sort, anyway. If you still have trouble after making that change, I suggest restarting your python session or your colab session."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I would like to implement a variant of convolution in pycuda. For simplicity, I'll show rectangle kernel of the interpolation. The standard convolution can be applied as following: import pycuda.autoinit import pycuda.driver as drv import numpy as np from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void func(float *dest, float *a) { const int img_size = 64; const int kernel_size = 3; const int kernel_size_half = kernel_size/2; const int tx = blockIdx.x * blockDim.x + threadIdx.x; const int ty = blockIdx.y * blockDim.y + threadIdx.y; int tx_kernel; tx_kernel = tx - kernel_size_half; for (int idx=-kernel_size_half; idx &lt;= kernel_size_half; idx++) { tx_kernel = tx + idx ; if ((tx_kernel &lt; 0) || (tx_kernel &gt; img_size-1)) continue; dest[ty * img_size + tx] += a[ty * img_size + tx_kernel] / ((float) kernel_size); } } \"\"\") Instead of calculating the current position wrt neighbours, I would like to do the opposite, to add the value of the current pixel to the neighbours. I.e: to change the line: dest[ty * img_size + tx] += a[ty * img_size + tx_kernel] / ((float) kernel_size); to: dest[ty * img_size + tx_kernel] += a[ty * img_size + tx] / ((float) kernel_size); However, The first works fine but the second is not, it fails by updating the neighbours. Is there a way to bypass it? Note: I simplified the question to focus on what I need, the general problem is to use a different convolution kernel for each pixel instead of same one as I asked in the question.",
        "answers": [
            [
                "to change the line: dest[ty * img_size + tx] += a[ty * img_size + tx_kernel] / ((float) kernel_size); to: dest[ty * img_size + tx_kernel] += a[ty * img_size + tx] / ((float) kernel_size); However, The first works fine but the second is not, it fails by updating the neighbours. Is there a way to bypass it? The first method is preferred from a performance perspective. However if you wish to \"update the neighbors\" then it should be possible to recast the second operation as: atomicAdd(&amp;(dest[ty * img_size + tx_kernel]), a[ty * img_size + tx] / ((float) kernel_size));"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a code reading a serialized TensorRT engine: import tensorrt as trt import pycuda.driver as cuda cuda.init() device = cuda.Device(0) context = device.make_context() logger = trt.Logger(trt.Logger.INFO) with trt.Runtime(logger) as runtime: with open('model.trt', 'rb') as in_: engine = runtime.deserialize_cuda_engine(in_.read()) which runs just fine on my Nvidia Jeston Nano, until I compile it with Pyinstaller pyinstaller temp.py In the compiled code runtime.deserialize_cuda_engine returns None and logger says: Cuda Error in loadKernel: 3 (initialization error) [TensorRT] ERROR: INVALID_STATE: std::exception [TensorRT] ERROR: INVALID_CONFIG: Deserialize the cuda engine failed. When I construct the engine from scratch, like cuda.init() device = cuda.Device(0) context = device.make_context() logger = trt.Logger(trt.Logger.INFO) with ExitStack() as stack: builder = stack.enter_context(trt.Builder(logger)) network = stack.enter_context(builder.create_network( 1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH) )) i = network.add_input('input0', trt.float16, (3, 2)) s = network.add_softmax(i) network.mark_output(s.get_output(0)) config = stack.enter_context(builder.create_builder_config()) ...some builder settings like opt profiles and fp16 mode... engine = builder.build_engine(network, config) then everything works fine, even after compilation. The engine was prepared with trtexec on the same computer. Cuda version is V10.2.89, pycuda version is 2019.1.2. I believe it's a standard jetson installation as of August 2020. Any ideas what might be involved here and what workarounds might be?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to measure ONLY the inference time in the Jetson TX2. How can I improve my function to do that? As right now I am measuring: the transfer of the image from CPU to GPU transfer of results from GPU to CPU the inference Or is that not possible because of the way GPUs work? I mean, how many times will I have to use stream.synchronize() if I divide/segment the function into 3 parts: transfer from CPU to GPU Inference transfer from GPU to CPU Thank you CODE IN INFERENCE.PY def do_inference(engine, pics_1, h_input, d_input, h_output, d_output, stream, batch_size): \"\"\" This is the function to run the inference Args: engine : Path to the TensorRT engine. pics_1 : Input images to the model. h_input: Input in the host (CPU). d_input: Input in the device (GPU). h_output: Output in the host (CPU). d_output: Output in the device (GPU). stream: CUDA stream. batch_size : Batch size for execution time. height: Height of the output image. width: Width of the output image. Output: The list of output images. \"\"\" # Context for executing inference using ICudaEngine with engine.create_execution_context() as context: # Transfer input data from CPU to GPU. cuda.memcpy_htod_async(d_input, h_input, stream) # Run inference. #context.profiler = trt.Profiler() ##shows execution time(ms) of each layer context.execute(batch_size=1, bindings=[int(d_input), int(d_output)]) # Transfer predictions back from the GPU to the CPU. cuda.memcpy_dtoh_async(h_output, d_output, stream) # Synchronize the stream. stream.synchronize() # Return the host output. out = h_output return out CODE IN TIMER.PY for i in range (count): start = time.perf_counter() # Classification - calling TX2_classify.py out = eng.do_inference(engine, image, h_input, d_input, h_output, d_output, stream, 1) inference_time = time.perf_counter() - start print(\"TIME\") print(inference_time * 1000) print(\"\\n\") pred = postprocess_inception(out) print(pred) print(\"\\n\")",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to install the PyCUDA module to run some python script I downloaded, but trying to install it with pip doesn't work. I run pip install pycuda on the command line At first, I get this: Collecting pycuda Using cached pycuda-2020.1.tar.gz (1.6 MB) Requirement already satisfied: pytools&gt;=2011.2 in c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages (from pycuda) (2020.4) Requirement already satisfied: decorator&gt;=3.2.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages (from pycuda) (4.4.2) Requirement already satisfied: appdirs&gt;=1.4.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages (from pycuda) (1.4.4) Requirement already satisfied: mako in c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages (from pycuda) (1.1.3) Requirement already satisfied: six&gt;=1.8.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages (from pytools&gt;=2011.2-&gt;pycuda) (1.11.0) Requirement already satisfied: numpy&gt;=1.6.0 in c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages (from pytools&gt;=2011.2-&gt;pycuda) (1.14.1) Requirement already satisfied: MarkupSafe&gt;=0.9.2 in c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages (from mako-&gt;pycuda) (1.1.1) Using legacy 'setup.py install' for pycuda, since package 'wheel' is not installed. Installing collected packages: pycuda Running setup.py install for pycuda ... error Then, this appears (all in red): ERROR: Command errored out with exit status 1: command: 'c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Jules\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rehu_ea2\\\\pycuda\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Jules\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rehu_ea2\\\\pycuda\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Jules\\AppData\\Local\\Temp\\pip-record-vlpoymu1\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\Include\\pycuda' cwd: C:\\Users\\Jules\\AppData\\Local\\Temp\\pip-install-rehu_ea2\\pycuda\\ Complete output (82 lines): *************************************************************** *** WARNING: nvcc not in path. *** May need to set CUDA_INC_DIR for installation to succeed. *************************************************************** ************************************************************* *** I have detected that you have not run configure.py. ************************************************************* *** Additionally, no global config files were found. *** I will go ahead with the default configuration. *** In all likelihood, this will not work out. *** *** See README_SETUP.txt for more information. *** *** If the build does fail, just re-run configure.py with the *** correct arguments, and then retry. Good luck! ************************************************************* *** HIT Ctrl-C NOW IF THIS IS NOT WHAT YOU WANT ************************************************************* Continuing in 1 seconds... c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\lib\\distutils\\dist.py:261: UserWarning: Unknown distribution option: 'test_requires' warnings.warn(msg) running install running build running build_py creating build creating build\\lib.win32-3.6 creating build\\lib.win32-3.6\\pycuda copying pycuda\\autoinit.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\characterize.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\compiler.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\cumath.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\curandom.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\debug.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\driver.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\elementwise.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\gpuarray.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\reduction.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\scan.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\tools.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\_cluda.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\_mymako.py -&gt; build\\lib.win32-3.6\\pycuda copying pycuda\\__init__.py -&gt; build\\lib.win32-3.6\\pycuda creating build\\lib.win32-3.6\\pycuda\\gl copying pycuda\\gl\\autoinit.py -&gt; build\\lib.win32-3.6\\pycuda\\gl copying pycuda\\gl\\__init__.py -&gt; build\\lib.win32-3.6\\pycuda\\gl creating build\\lib.win32-3.6\\pycuda\\sparse copying pycuda\\sparse\\cg.py -&gt; build\\lib.win32-3.6\\pycuda\\sparse copying pycuda\\sparse\\coordinate.py -&gt; build\\lib.win32-3.6\\pycuda\\sparse copying pycuda\\sparse\\inner.py -&gt; build\\lib.win32-3.6\\pycuda\\sparse copying pycuda\\sparse\\operator.py -&gt; build\\lib.win32-3.6\\pycuda\\sparse copying pycuda\\sparse\\packeted.py -&gt; build\\lib.win32-3.6\\pycuda\\sparse copying pycuda\\sparse\\pkt_build.py -&gt; build\\lib.win32-3.6\\pycuda\\sparse copying pycuda\\sparse\\__init__.py -&gt; build\\lib.win32-3.6\\pycuda\\sparse creating build\\lib.win32-3.6\\pycuda\\compyte copying pycuda\\compyte\\array.py -&gt; build\\lib.win32-3.6\\pycuda\\compyte copying pycuda\\compyte\\dtypes.py -&gt; build\\lib.win32-3.6\\pycuda\\compyte copying pycuda\\compyte\\__init__.py -&gt; build\\lib.win32-3.6\\pycuda\\compyte running egg_info writing pycuda.egg-info\\PKG-INFO writing dependency_links to pycuda.egg-info\\dependency_links.txt writing requirements to pycuda.egg-info\\requires.txt writing top-level names to pycuda.egg-info\\top_level.txt reading manifest file 'pycuda.egg-info\\SOURCES.txt' reading manifest template 'MANIFEST.in' warning: no files found matching 'doc\\source\\_static\\*.css' warning: no files found matching 'doc\\source\\_templates\\*.html' warning: no files found matching '*.cpp' under directory 'bpl-subset\\bpl_subset\\boost' warning: no files found matching '*.html' under directory 'bpl-subset\\bpl_subset\\boost' warning: no files found matching '*.inl' under directory 'bpl-subset\\bpl_subset\\boost' warning: no files found matching '*.txt' under directory 'bpl-subset\\bpl_subset\\boost' warning: no files found matching '*.h' under directory 'bpl-subset\\bpl_subset\\libs' warning: no files found matching '*.ipp' under directory 'bpl-subset\\bpl_subset\\libs' warning: no files found matching '*.pl' under directory 'bpl-subset\\bpl_subset\\libs' writing manifest file 'pycuda.egg-info\\SOURCES.txt' creating build\\lib.win32-3.6\\pycuda\\cuda copying pycuda\\cuda\\pycuda-complex-impl.hpp -&gt; build\\lib.win32-3.6\\pycuda\\cuda copying pycuda\\cuda\\pycuda-complex.hpp -&gt; build\\lib.win32-3.6\\pycuda\\cuda copying pycuda\\cuda\\pycuda-helpers.hpp -&gt; build\\lib.win32-3.6\\pycuda\\cuda copying pycuda\\sparse\\pkt_build_cython.pyx -&gt; build\\lib.win32-3.6\\pycuda\\sparse running build_ext building '_driver' extension error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": http://landinghub.visualstudio.com/visual-cpp-build-tools ---------------------------------------- ERROR: Command errored out with exit status 1: 'c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Jules\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rehu_ea2\\\\pycuda\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Jules\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rehu_ea2\\\\pycuda\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Jules\\AppData\\Local\\Temp\\pip-record-vlpoymu1\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\jules\\appdata\\local\\programs\\python\\python36-32\\Include\\pycuda' Check the logs for full command output. I have no idea what this error is about, or how to fix it (and I don't want to get into complicated installations that will take me hours and might not even work). I'm not even sure what this module does (yet). Do you have an idea to help me? (edit: I'm on Windows 10, and I have not installed anything about PyCUDA before)",
        "answers": [
            [
                "Try the following. pip install pipwin pipwin install pycuda"
            ]
        ],
        "votes": [
            11.0000001
        ]
    },
    {
        "question": "I have a pycuda program here that reads in an image from the command line and saves a version back with the colors inverted: import pycuda.autoinit import pycuda.driver as device from pycuda.compiler import SourceModule as cpp import numpy as np import sys import cv2 modify_image = cpp(\"\"\" __global__ void modify_image(int pixelcount, unsigned char* inputimage, unsigned char* outputimage) { int id = threadIdx.x + blockIdx.x * blockDim.x; if (id &gt;= pixelcount) return; outputimage[id] = 255 - inputimage[id]; } \"\"\").get_function(\"modify_image\") print(\"Loading image\") image = cv2.imread(sys.argv[1], cv2.IMREAD_UNCHANGED).astype(np.uint8) print(\"Processing image\") pixels = image.shape[0] * image.shape[1] newchannels = [] for channel in cv2.split(image): output = np.zeros_like(channel) modify_image( device.In(np.int32(pixels)), device.In(channel), device.Out(output), block=(1024,1,1), grid=(pixels // 1024 + 1, 1)) newchannels.append(output) finalimage = cv2.merge(newchannels) print(\"Saving image\") cv2.imwrite(\"processed.png\", finalimage) print(\"Done\") It works perfectly fine, even on larger images. However, in trying to expand the functionality of the program, I came across a really strange issue wherein adding a second variable argument to the kernel causes the program to completely fail, simply saving a completely black image. The following code does not work; import pycuda.autoinit import pycuda.driver as device from pycuda.compiler import SourceModule as cpp import numpy as np import sys import cv2 modify_image = cpp(\"\"\" __global__ void modify_image(int pixelcount, int width, unsigned char* inputimage, unsigned char* outputimage) { int id = threadIdx.x + blockIdx.x * blockDim.x; if (id &gt;= pixelcount) return; outputimage[id] = 255 - inputimage[id]; } \"\"\").get_function(\"modify_image\") print(\"Loading image\") image = cv2.imread(sys.argv[1], cv2.IMREAD_UNCHANGED).astype(np.uint8) print(\"Processing image\") pixels = image.shape[0] * image.shape[1] newchannels = [] for channel in cv2.split(image): output = np.zeros_like(channel) modify_image( device.In(np.int32(pixels)), device.In(np.int32(image.shape[0])), device.In(channel), device.Out(output), block=(1024,1,1), grid=(pixels // 1024 + 1, 1)) newchannels.append(output) finalimage = cv2.merge(newchannels) print(\"Saving image\") cv2.imwrite(\"processed.png\", finalimage) print(\"Done\") where the only difference is on two lines, the kernel header and it's call. The actual code of the kernel itself is unchanged, and yet this small addition completely breaks the program. Neither the compiler nor interpreter throw any errors. I have no idea how to begin to debug it, and am thoroughly confused.",
        "answers": [
            [
                "The device.In and relatives are designed for use with objects which support the Python buffer protocols (like numpy arrays). The source of your problem is using them to transfer non-buffer objects. Just pass your scalars with the correct numpy dtype directly to your kernel call. Don't use device.In. The fact this worked in the original case was a complete accident"
            ],
            [
                "Okay, so by changing the variable arguments to pointers in the kernel it fixed the code, i'm not sure how or why. Here is the modified version of the kernel; __global__ void modify_image(int* pixelcount, int* width, unsigned char* inputimage, unsigned char* outputimage) { int id = threadIdx.x + blockIdx.x * blockDim.x; if (id &gt;= *pixelcount) return; outputimage[id] = 255 - inputimage[id]; } The remainder of the code is unchanged. If anybody wants to explain why this is a successful fix, I would greatly appreciate it."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "I converted the trained model to onnx format, and then created the TensorRT engine file from onnx model. I used the below snnipet code for doing this? import pycuda.driver as cuda import pycuda.autoinit import numpy as np import tensorrt as trt # logger to capture errors, warnings, and other information during the build and inference phases TRT_LOGGER = trt.Logger() def build_engine(onnx_file_path): # initialize TensorRT engine and parse ONNX model builder = trt.Builder(TRT_LOGGER) network = builder.create_network() parser = trt.OnnxParser(network, TRT_LOGGER) # parse ONNX with open(onnx_file_path, 'rb') as model: print('Beginning ONNX file parsing') parser.parse(model.read()) print('Completed parsing of ONNX file') # allow TensorRT to use up to 1GB of GPU memory for tactic selection builder.max_workspace_size = 1 &lt;&lt; 30 # we have only one image in batch builder.max_batch_size = 1 # use FP16 mode if possible if builder.platform_has_fast_fp16: builder.fp16_mode = True # generate TensorRT engine optimized for the target platform print('Building an engine...') engine = builder.build_cuda_engine(network) context = engine.create_execution_context() print(\"Completed creating Engine\") return engine, context # get sizes of input and output and allocate memory required for input data and for output data for binding in engine: if engine.binding_is_input(binding): # we expect only one input input_shape = engine.get_binding_shape(binding) input_size = trt.volume(input_shape) * engine.max_batch_size * np.dtype(np.float32).itemsize # in bytes device_input = cuda.mem_alloc(input_size) else: # and one output output_shape = engine.get_binding_shape(binding) # create page-locked memory buffers (i.e. won't be swapped to disk) host_output = cuda.pagelocked_empty(trt.volume(output_shape) * engine.max_batch_size, dtype=np.float32) device_output = cuda.mem_alloc(host_output.nbytes) stream = cuda.Stream() # preprocess input data host_input = np.array(preprocess_image(\"turkish_coffee.jpg\").numpy(), dtype=np.float32, order='C') cuda.memcpy_htod_async(device_input, host_input, stream) # run inference context.execute_async(bindings=[int(device_input), int(device_output)], stream_handle=stream.handle) cuda.memcpy_dtoh_async(host_output, device_output, stream) stream.synchronize() # postprocess results output_data = torch.Tensor(host_output).reshape(engine.max_batch_size, output_shape[0]) postprocess(output_data) The above codes is correctly work for one batch size of image, but I want to do for multi batch size, for this one thing that need to change : builder.max_batch_size = 1 and What are other things I have to change to work correctly for batch size more than one? In my opinion, the one things that I have to change from sync to async, right?: stream.synchronize() How I can to solve the problem for batch size more than one? My system: torch:1.2.0 torchvision:0.4.0 albumentations:0.4.5 onnx:1.4.1 opencv-python:4.2.0.34 cuda:10.0 ubuntu:18.04 tensorrt: 5.x/6.x Other solution is to use optimization profiler in TRT 7.x , But I want to know How I can to solve this problem with 5/6 versions, Is it possible?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to use TensorRt using the python API. I am trying to use it in multiple threads where the Cuda context is used with all the threads (everything works fine in a single thread). I am using docker with tensorrt:20.06-py3 image, and an onnx model, and Nvidia 1070 GPU. The multiple thread approach should be allowed, as mentioned here TensorRT Best Practices. I created the context in the main thread: cuda.init() device = cuda.Device(0) ctx = device.make_context() I tried two methods, first to build the engine in the main thread and use it in the execution thread. This case gives this error. [TensorRT] ERROR: ../rtSafe/cuda/caskConvolutionRunner.cpp (373) - Cask Error in checkCaskExecError&lt;false&gt;: 10 (Cask Convolution execution) [TensorRT] ERROR: FAILED_EXECUTION: std::exception Second, I tried to build the model in the thread it gives me this error: pycuda._driver.LogicError: explicit_context_dependent failed: invalid device context - no currently active context? The error appears when I call 'cuda.Stream()' I am sure that I can run multiple Cuda streams in parallel under the same Cuda context, but I don't know how to do it.",
        "answers": [
            [
                "I found a solution. The idea is to create a normal global ctx = device.make_context() Then in each execution thread do a: ctx.push() --- Execute Inference Code --- ctx.pop() The link for the source and full sample is here"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have a c++ code that gives image array output in GPU memory. I want to do further processing and image analytics using pycuda. I am trying to make GPU array as: For testing purpose, I have created an array in c++ as: const int arraySize = 5; const int a[arraySize] = { 1, 2, 3, 4, 5 }; int* dev_a = nullptr; cudaMalloc((void**)&amp;dev_a, arraySize * sizeof(int)); cudaMemcpy(dev_a, a, arraySize * sizeof(int), cudaMemcpyHostToDevice); printf(\"dev_a : %p\\n\", dev_a); Suppose, here I get GPU memory address as '0x7f3454800000'. I am using this address to create GPUarray as: from pycuda.gpuarray import GPUArray import numpy as np import pycuda.autoinit import pycuda.driver as drv from pycuda.compiler import SourceModule from pycuda.driver import PointerHolderBase drv.init() class Holder(PointerHolderBase): def __init__(self): super().__init__() self.gpudata = '0x7f5954800000' def get_pointer(self): return self.gpudata def __int__(self): return self.__index__() # without an __index__ method, arithmetic calls to the GPUArray backed by this pointer fail # not sure why, this needs to return some integer, apparently def __index__(self): return self.gpudata array = GPUArray((1,5), dtype=np.int32, gpudata=Holder()) print(array.get()) When I am running the code, I am getting error as: File \"array_test.py\", line 43, in &lt;module&gt; print(array.get()) File \"/home/govindam/anaconda3/envs/tf_c2/lib/python3.6/site-packages/pycuda/gpuarray.py\", line 305, in get _memcpy_discontig(ary, self, async_=async_, stream=stream) File \"/home/govindam/anaconda3/envs/tf_c2/lib/python3.6/site-packages/pycuda/gpuarray.py\", line 1309, in _memcpy_discontig drv.memcpy_dtoh(dst, src.gpudata) TypeError: No registered converter was able to produce a C++ rvalue of type unsigned long long from this Python object of type st How to give the memory address while creating GPUarray so that I am avoid copying from GPU to CPU and again to GPU?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to use PyCuda to convolve a Gaussian filter with an image. I've taken some code from the PyCuda documentation and a Cuda convolution kernel from a page online. For some reason, the resulting image comes out completely black. I believe that the image array and Gaussian filter array are being passed in incorrectly - when I try to use printf to print values from within the kernel, the values of the image are just '0.00...' and the values of the filter are very big numbers like '125529009160192000.000000'. I've tried flattening the arrays and explicitly setting them to C order, but this doesn't seem to help. I've also tried playing around with PyCuda GPUarrays, but haven't had any success. Thanks for taking a look! Here is my code: import pycuda.driver as cuda import pycuda.autoinit import math from pycuda.compiler import SourceModule from timeit import default_timer as timer from PIL import Image import numpy as np def make_k(sig): s = 65 out = np.zeros((s,s)) for x in range(s): for y in range(s): X = x-(s-1)/2 Y = y-(s-1)/2 gauss = 1/(2*np.pi*sig**2) * np.exp(-(X**2 + Y**2)/(2*sig**2)) out[x,y] = gauss a = np.sum(out) kernel = out/a return kernel def replication_pad(img, W, H, S, paddedW, paddedH): output = np.zeros((paddedH, paddedW)) output[:S, S:W+S] = img[0:1,:] output[S:H+S, :S] = img[:, 0:1] output[H+S:, S:W+S] = img[-1:,:] output[S:H+S, W+S:] = img[:, -1:] output[:S, :S] = img[0, 0] output[:S, paddedW-S:] = img[0, -1] output[paddedH-S:, :S] = img[-1, 0] output[paddedH-S:, paddedW-S:] = img[-1, -1] output[S:H+S, S:W+S] = img return output #d_f is the padded image #d_g is the filter #d_h is the filtering result mod = SourceModule(\"\"\" __global__ void convolution( const float *d_f, const unsigned int paddedW, const unsigned int paddedH, const float *d_g, const int S, float *d_h, const unsigned int W, const unsigned int H ) { // Set the padding size and filter size unsigned int paddingSize = S; unsigned int filterSize = 2 * S + 1; // Set the pixel coordinate const unsigned int j = blockIdx.x * blockDim.x + threadIdx.x + paddingSize; const unsigned int i = blockIdx.y * blockDim.y + threadIdx.y + paddingSize; // Print for debugging (on the first thread) if( i==paddingSize &amp;&amp; j==paddingSize) { //printf(\"%lf\", d_g[50]); printf(\"%lf\", d_f[100400]); } // The multiply-add operation for the pixel coordinate ( j, i ) if( j &gt;= paddingSize &amp;&amp; j &lt; paddedW - paddingSize &amp;&amp; i &gt;= paddingSize &amp;&amp; i &lt; paddedH - paddingSize ) { unsigned int oPixelPos = ( i - paddingSize ) * W + ( j - paddingSize ); d_h[oPixelPos] = 0.0; for( int k = -S; k &lt;=S; k++ ) { for( int l = -S; l &lt;= S; l++ ) { unsigned int iPixelPos = ( i + k ) * paddedW + ( j + l ); unsigned int coefPos = ( k + S ) * filterSize + ( l + S ); d_h[oPixelPos] += d_f[iPixelPos] * d_g[coefPos]; } } } } \"\"\") image = Image.open('spooky.jpg').convert('L') img_full = np.asarray(image, dtype='float') img = img_full[:1080,:1920] # 1080p resolution W = 1920 H = 1080 S = 32 paddedW = W + 2*S paddedH = H + 2*S img_padded = replication_pad(img, W, H, S, paddedW, paddedH) kernel = make_k(10) ker_cont = np.ascontiguousarray(kernel, dtype=\"float\") ker_gpu = cuda.mem_alloc(ker_cont.nbytes) cuda.memcpy_htod(ker_gpu, ker_cont) img_cont = np.ascontiguousarray(img_padded) img_gpu = cuda.mem_alloc(img_cont.nbytes) cuda.memcpy_htod(img_gpu, img_cont) img_og = np.ascontiguousarray(img) result_gpu = cuda.mem_alloc(img_og.nbytes) blockW = 32 blockH = 32 gridW = math.ceil(W/blockW) gridH = math.ceil(H/blockH) func = mod.get_function(\"convolution\") func(img_gpu, np.int_(paddedW), np.int_(paddedH), ker_gpu, np.int_(S), result_gpu, np.int_(W), np.int_(H), block = (blockW, blockH, 1), grid=(gridW, gridH)) host_output = np.empty_like(img_og) cuda.memcpy_dtoh(host_output, result_gpu) Image.fromarray(host_output).show() And here is the image I'm using: https://i.stack.imgur.com/8YvA6.jpg",
        "answers": [
            [
                "I needed to change dtypes of input image and input kernel from float64 to float32. Also needed to allocate output array with reference to float32 array for appropriate nbytes. This looked like: ker_cont = np.float32(ker_cont) img_cont = np.float32(img_cont) img_og = np.float32(img_og) result_gpu = cuda.mem_alloc(img_og.nbytes)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to implement a kernel that calculates average of percentages. Example- Taking the 3D array (in the code below) piece [[2,4],[3,6],[4,8]] and calculate (4+6+8)/((4+6+8)+(2+3+4)) Here's a colab notebook to run the following code quickly: https://colab.research.google.com/drive/1k_XfOVOYWOTnNQFA9Vo_H93D9l-xWO8K?usp=sharing # -*- coding: utf-8 -*- import numpy as np import pycuda.autoinit import pycuda.driver as cuda import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule # set dimentions ROWS = 3 COLS = 2 h_perms = np.array([[ [ 1,1], [ 1,1], [ 1,1] ],[ [ 2,7], [ 3,11], [ 4,13] ],[ [ 2,4], [ 3,6], [ 4,8] ],[ [ 2,7], [ 3,11], [ 4,13] ],[ [ 2,4], [ 3,6], [ 4,8] ],[ [ 1,1], [ 1,1], [ 1,1] ] ], dtype=np.float32).flatten() # send to device d_perms = gpuarray.to_gpu(h_perms) kernel = SourceModule(\"\"\" __global__ void calc(float *permutations, int *permutationShape, float *results) { __shared__ float c; __shared__ float b; int bIdx = blockIdx.y * gridDim.x + blockIdx.x; int tIdx = threadIdx.y * blockDim.x + threadIdx.x; int rowCount = permutationShape[0]; int colCount = permutationShape[1]; int i = (bIdx * rowCount * colCount) + (tIdx * colCount); c += permutations[i]; b += permutations[i+1]; __syncthreads(); results[bIdx] = b / (b + c); } \"\"\") calc = kernel.get_function('calc') # prepare results array d_results = gpuarray.zeros((6,1), np.float32) d_results = gpuarray.to_gpu(d_results) h_perms_shape = np.array([ROWS,COLS], np.int32); d_perms_shape = gpuarray.to_gpu(h_perms_shape); start = cuda.Event() end = cuda.Event() start.record() calc(d_perms, d_perms_shape, d_results, block=(ROWS,1,1), grid=(ROWS*COLS,1,1)) end.record() secs = start.time_till(end)*1e-3 print(secs) print(d_results) I expect getting this- array([[0.5 ], [0.775], [0.6666667], [0.775], [0.6666667], [0.5 ]], dtype=float32) But I get this- array([[0.5 ], [0.7777778], [0.6666667], [0.7777778], [0.6666667], [0.5 ]], dtype=float32) I'm trying to understand why the particular calculation for (7+11+13)/((7+11+13)+(2+3+4)) results with anything that is not 0.775",
        "answers": [
            [
                "The code you posted contains a memory race here: int i = (bIdx * rowCount * colCount) + (tIdx * colCount); c += permutations[i]; b += permutations[i+1]; Because b and c are in shared memory, you will have multiple threads attempting to read and write from/to the same memory locations simultaneously, and that is undefined behaviour in CUDA (except under extremely specific conditions which don't apply here). If I were writing this as a toy example, I might do it like this: __global__ void calc(float *permutations, int *permutationShape, float *results) { __shared__ float c; __shared__ float b; int bIdx = blockIdx.y * gridDim.x + blockIdx.x; int tIdx = threadIdx.y * blockDim.x + threadIdx.x; int rowCount = permutationShape[0]; int colCount = permutationShape[1]; int i = (bIdx * rowCount * colCount) + (tIdx * colCount); atomicAdd(&amp;c, permutations[i]); atomicAdd(&amp;b, permutations[i+1]); __syncthreads(); if (tIdx == 0) { results[bIdx] = b / (b + c);; } } In this code, atomicAdd ensures that the additions and memory transactions occur sequentially so that the memory race is avoided. This won't be a good solution for less trivial examples from a performance perspective (have a look at shared memory reduction techniques for that) but it should work as expected."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Using Python to drive CUDA, I want to schedule a Python host function asynchronous in a stream that runs after a kernel and memory copy has been taken place. Is there an equivalent to the CUDA C++ function CUresult cuLaunchHostFunc(CUstream hStream, CUhostFn fn, void* userData) in one of the Python libs (PyCuda, Numba, ...)? The Driver API function is here in the CUDA docs",
        "answers": [
            [
                "Is there an equivalent to the CUDA C++ function CUresult cuLaunchHostFunc(CUstream hStream, CUhostFn fn, void* userData) in one of the Python libs (PyCuda, Numba, ...)? Not in either of those two. None of the driver API based frameworks for CUDA I am aware of exposes cuLaunchHostFunc (PyCUDA, Numba, JCUDA). I want to schedule a Python host function asynchronous in a stream that runs after a kernel and memory copy has been taken place Nothing in the native CUDA driver API could ever support that. Tensorflow and Pytorch both have elaborate execution pipelining and callback mechanisms at a Python level which might get you something functionally similar to what you envisage. But it won't be done at a CUDA level, it will be at a higher level of abstraction."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I run a small kernel to use \"printf\" function in pycuda in google colab, I do not receive any error but I don't observe any output. I used Cuda.Context.synchronize() but it doesn't have an effect on my result!!",
        "answers": [
            [
                "You should add one line of code to your program: cudaDeviceSynchronize(); The code just forces the printf()s to flush."
            ]
        ],
        "votes": [
            -0.9999999
        ]
    },
    {
        "question": "As it's possible in C to index a negative array location and go out of the array bounds this code compiles and \"works\". __global__ void do_something_bad(int * in_a){ in_a[-1] = 666; // assign a value to an out of bounds memory location } My assumption is the above code is doing the following (please let me know if this assumption is wrong): GPU memory before: [0x00 = usually unused memory][0x01= Start of in_a][0x02 = in_a] .... GPU memory after: [0x00 = 666][0x01= Start of in_a][0x02 = in_a] .... In summary the memory before the the in_a array is being set the value. This memory before in_a could contain other important data but when I'm testing it containing nothing important and thus gives me no error or failing test. FYI: I'm using pycuda and am unit testing my code as a go. I'm trying to avoid creating silent unpredictable errors as a result of the above. Of course in the real world example -1 would have been calculated and I've simplified the code to just the problem I want to solve. How do I identify this error and force a detectable problem that my unit tests can pick up?",
        "answers": [
            [
                "To avoid a silent error in a kernel, I would use assertion if you're not on MacOS. Something like this: #include &lt;assert.h&gt; __global__ void do_something_bad(int* in_a){ int indx; indx = 0; // A valid index assert(indx &gt;= 0); // Lets the kernel continue in_a[indx] = 666; indx = -1; // An invalid index assert(indx &gt;= 0); // Sends an error to stderr in_a[indx] = 666; // This never gets executed } int main(){ int *a; cudaMalloc((void **)&amp;a, 10*sizeof(int)); do_something_bad&lt;&lt;&lt;1,1&gt;&gt;&gt;(a); cudaDeviceSynchronize(); } However, this may affect the performance of your program. From the Programming Guide: Assertions are for debugging purposes. They can affect performance and it is therefore recommended to disable them in production code. They can be disabled at compile time by defining the NDEBUG preprocessor macro before including assert.h"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I use PyCUDA's interface [1] over CUDA Unified Memory [2]. At some point I added random number generators [3] and stared to see dead kernels in Jupyter Notebook: I narrowed the problem down to the creation of random number generator. Or, to be precise, to the moment when I do this: import pycuda.curandom from pycuda import autoinit, driver import numpy as np gpu_data_1 = driver.managed_zeros(shape=5, dtype=np.int32, mem_flags=driver.mem_attach_flags.GLOBAL) gpu_generator = pycuda.curandom.XORWOWRandomNumberGenerator(pycuda.curandom.seed_getter_uniform) gpu_data_2 = driver.managed_zeros(shape=5, dtype=np.int32, mem_flags=driver.mem_attach_flags.GLOBAL) The code above fails without any error message, but if I put the gpu_generator = ... line one line higher or lower, it appears to work fine. I believe PyCUDA might somehow fail to execute the prepare call, which comes down to this kernel: extern \"C\" { __global__ void prepare(curandStateXORWOW *s, const int n, unsigned int *v, const unsigned int o) { const int id = blockIdx.x*blockDim.x+threadIdx.x; if (id &lt; n) curand_init(v[id], id, o, &amp;s[id]); } } Any idea what might be the problem?",
        "answers": [
            [
                "It is illegal in a pre-Pascal UM (Unified Memory) regime for host code to touch a managed allocation after a kernel has been launched, but before a cudaDeviceSynchronize() has been issued. I am guessing this code violates this rule. If I run your repro case on a Maxwell system I get this: $ cuda-memcheck python ./idontthinkso.py ========= CUDA-MEMCHECK ========= Error: process didn't terminate successfully ========= Fatal UVM CPU fault due to invalid operation ========= during write access to address 0x703bc1000 ========= ========= ERROR SUMMARY: 1 error That is the managed memory system blowing up. Placing a synchronization call between the random generator setup (which runs a kernel) and the zeros call (which touches managed memory) gets rid of it on my system: $ cat idontthinkso.py import pycuda.curandom from pycuda import autoinit, driver import numpy as np gpu_data_1 = driver.managed_zeros(shape=5, dtype=np.int32, mem_flags=driver.mem_attach_flags.GLOBAL) gpu_generator = pycuda.curandom.XORWOWRandomNumberGenerator(pycuda.curandom.seed_getter_uniform) autoinit.context.synchronize() gpu_data_2 = driver.managed_zeros(shape=5, dtype=np.int32, mem_flags=driver.mem_attach_flags.GLOBAL) $ cuda-memcheck python ./idontthinkso.py ========= CUDA-MEMCHECK ========= ERROR SUMMARY: 0 errors The UM regime you are in will vary depending on what GPU, driver and OS you use."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have this thread class built to run inference with TensorRT: class GPUThread(threading.Thread): def __init__(self, engine_path): threading.Thread.__init__(self) self.engine_path = engine_path self.engine = self.open_engine(engine_path) def run(self): cuda.init() #self.dev = cuda.Device(0) #self.ctx = self.dev.make_context() self.rt_run() #self.ctx.pop() #del self.ctx return def rt_run(self): with self.engine.create_execution_context() as context: inputs, outputs, bindings, stream = self.allocate_buffers(self.engine) # ... Retrieve image self.load_input(inputs[0].host, image) [output] = self.do_inference( context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream ) return def load_input(self, pagelocked_buffer, image): # ... Image transformations ... # Copy to the pagelocked input buffer np.copyto(pagelocked_buffer, crop_img) return def allocate_buffers(self, engine): inputs = [] outputs = [] bindings = [] stream = cuda.Stream() for binding in engine: size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size dtype = trt.nptype(engine.get_binding_dtype(binding)) # Allocate host and device buffers host_mem = cuda.pagelocked_empty(size, dtype) device_mem = cuda.mem_alloc(host_mem.nbytes) # Append the device buffer to device bindings. bindings.append(int(device_mem)) # Append to the appropriate list. if engine.binding_is_input(binding): inputs.append(HostDeviceMem(host_mem, device_mem)) else: outputs.append(HostDeviceMem(host_mem, device_mem)) return inputs, outputs, bindings, stream def run_inference(self, context, bindings, inputs, outputs, stream, batch_size=1): # Transfer input data to the GPU. [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs] # Run inference. context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle) # Transfer predictions back from the GPU. [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs] # Synchronize the stream stream.synchronize() # Return only the host outputs. return [out.host for out in outputs] When running the code above, I get the error: stream = cuda.Stream() pycuda._driver.LogicError: explicit_context_dependent failed: invalid device context - no currently active context? This function cuda.Stream() is called in allocate_buffers above. So I then try the below in run (note this is the commented out code above): self.dev = cuda.Device(0) self.ctx = self.dev.make_context() self.rt_run() self.ctx.pop() del self.ctx This causes my system to completely freeze when rt_run's create_execution_context is called. I'm guessing there are conflicts between making the PyCuda context and then creating the TensorRT execution context? I'm running this on a Jetson Nano. If I remove the create_execution_context code, I can allocate buffers and it seems that the context is active and found in the worker thread. However, I can't run inference without the TensorRT execution context. execute_async is not a method of self.ctx above. Note that none of these issues arise when running from the main thread. I can just use PyCuda's autoinit and create an execution context as in the above code. So in summary, in a worker thread, I can't allocate buffers unless I call self.dev.make_context but this causes the create_execution_context call to crash the system. If I don't call self.dev.make_context, I can't allocate buffers in the execution context as I get the error invalid device context when calling cuda.Stream() in allocate buffers. What I'm running: TensorRT 6 PyCuda 1.2 Jetson Nano 2019 (A02)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I tried to installing pycuda using https://github.com/jkjung-avt/tensorrt_demos/blob/master/ssd/install_pycuda.sh as i use python3 . So while installing I run into a error : Usage: configure.py [options] configure.py: error: no such option: --no-use-shipped-boost My system Configuration : Google Cloud - ubuntu 18.04 CUDA 10.0 TENSORRT 6x I'm trying to install Pycuda 2019.1.2 For installation reference, https://github.com/jkjung-avt/tensorrt_demos -&gt;&gt;&gt; Demo #4: YOLOv3",
        "answers": [
            [
                "According to this: Just remove the file \"${HOME}/src/pycuda-2019.1.2/siteconf.py\". So it should be like this: cd ${HOME}/src/pycuda-2019.1.2/ then rm siteconf.py Then you should be able to run the install again."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have an inference code in TensorRT(with python). I want to run this code in ROS but I get the below error when trying to allocate buffer: LogicError: explicit_context_dependent failed: invalid device context - no currently active context? The code works well out of the ROS package. A ROS node publishes an image and the given code get the image to do inference. The inference code is shown below: #!/usr/bin/env python # Revision $Id$ import rospy from std_msgs.msg import String from cv_bridge import CvBridge import cv2 import os import numpy as np import argparse import torch from torch.autograd import Variable from torchvision import transforms import torch.nn.functional as F import torch._utils from PIL import Image from sensor_msgs.msg import Image as ImageMsg import tensorrt as trt import pycuda.driver as cuda import pycuda.autoinit import random import sys import common import shutil from itertools import chain TRT_LOGGER = trt.Logger() # cuda.init() class ModelData(object): def __init__(self): self.MODEL_PATH = \"./MobileNet_v2_Final.onnx\" ## converted model from pytorch to onnx self.batch_size = 1 self.num_classes = 3 self.engine = build_int8_engine(self.MODEL_PATH, self.batch_size) self.context = self.engine.create_execution_context() ### ROS PART self.bridge_ROS = CvBridge() self.loop_rate = rospy.Rate(1) self.pub = rospy.Publisher('Image_Label', String, queue_size=1) print('INIT Successfully') def callback(self, msg): rospy.loginfo('Image received...') cv_image = self.bridge_ROS.imgmsg_to_cv2(msg, desired_encoding=\"passthrough\") inputs, outputs, bindings, stream = common.allocate_buffers(context.engine) [output] = common.do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream, batch_size=effective_batch_size) def listener(self): rospy.Subscriber(\"chatter\", ImageMsg, self.callback) while not rospy.is_shutdown(): rospy.loginfo('Getting image...') self.loop_rate.sleep() def build_int8_engine(model_file, batch_size=32): with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.OnnxParser(network, TRT_LOGGER) as parser: builder.max_batch_size = batch_size builder.max_workspace_size = common.GiB(1) with open(model_file, 'rb') as model: parser.parse(model.read(),) return builder.build_cuda_engine(network) if __name__ == '__main__': rospy.init_node(\"listener\", anonymous=True) infer = ModelData() infer.listener() The error comes from the below class in stream = cuda.Stream(): #!/usr/bin/env python # Revision $Id$ from itertools import chain import argparse import os import pycuda.driver as cuda import pycuda.autoinit import numpy as np import tensorrt as trt # Simple helper data class that's a little nicer to use than a 2-tuple. class HostDeviceMem(object): def __init__(self, host_mem, device_mem): self.host = host_mem self.device = device_mem def __str__(self): return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device) def __repr__(self): return self.__str__() # Allocates all buffers required for an engine, i.e. host/device inputs/outputs. def allocate_buffers(engine): inputs = [] outputs = [] bindings = [] stream = cuda.Stream() for binding in engine: size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size dtype = trt.nptype(engine.get_binding_dtype(binding)) # Allocate host and device buffers host_mem = cuda.pagelocked_empty(size, dtype) device_mem = cuda.mem_alloc(host_mem.nbytes) # Append the device buffer to device bindings. bindings.append(int(device_mem)) # Append to the appropriate list. if engine.binding_is_input(binding): inputs.append(HostDeviceMem(host_mem, device_mem)) else: outputs.append(HostDeviceMem(host_mem, device_mem)) ctx.pop() del ctx return inputs, outputs, bindings, stream # This function is generalized for multiple inputs/outputs. # inputs and outputs are expected to be lists of HostDeviceMem objects. def do_inference(context, bindings, inputs, outputs, batch_size=1): # Transfer input data to the GPU. [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs] # [cuda.memcpy_htod(inp.device, inp.host) for inp in inputs] # Run inference. context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle) # context.execute(batch_size=batch_size, bindings=bindings) # Transfer predictions back from the GPU. [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs] # [cuda.memcpy_dtoh(out.host, out.device) for out in outputs] # Synchronize the stream stream.synchronize() # Return only the host outputs. return [out.host for out in outputs] More info: TensorRT: 6.1.5 Python: 2.7 rosversion: 1.14.3 rosdistro: melodic",
        "answers": [
            [
                "You need to explicitly create Cuda Device and load Cuda Context in the worker thread i.e. your callback function, instead of using import pycuda.autoinit in the main thread, as follows import pycuda.driver as cuda import threading def callback(): cuda.init() device = cuda.Device(0) # enter your Gpu id here ctx = device.make_context() allocate_buffers() # load Cuda buffers or any other Cuda or TenosrRT operations ctx.pop() # very important if __name__ == \"__main__\": worker_thread = threading.Thread(target=callback()) worker_thread.start() worker_thread.join() Note: do not forget to remove import pycuda.autoinit in both modules This is also discussed in a question here"
            ],
            [
                "please init cuda. As answers above. import pycuda.driver as cuda in main.py or befor import cuda-XXX-process"
            ]
        ],
        "votes": [
            7.0000001,
            -1.9999999
        ]
    },
    {
        "question": "mod=SourceModule(\"\"\" __global__ void mat_ops(float *A,float *B) { /*formula to get unique thread index*/ int thrd= blockIdx.x*blockDim.x*blockDim.y+threadIdx.y*blockDim.x+threadIdx.x; B[]=A[]; } \"\"\") func = mod.get_function(\"mat_ops\") func(A_k, B_k, grid=(3,1,1),block=(4,4,1)) I have two 3D arrays float *A and float *B, each of size 4 X 4 X 3 in this PyCUDA kernel. What I am trying to do here is, traverse the 3D array column by column, instead of row by row. I am making use of a 1D Grid of 2D blocks. How do I do this ?",
        "answers": [
            [
                "To do this, you need to describe to layout of the array in memory to the CUDA kernel, and you need the correct indexing calculations in the kernel using the host side provided strides. A simple way to do this is to define a small helper class in CUDA which hides the bulk of the indexing and provides a simple indexing syntax. For example: from pycuda import driver, gpuarray from pycuda.compiler import SourceModule import pycuda.autoinit import numpy as np mod=SourceModule(\"\"\" struct stride3D { float* p; int s0, s1; __device__ stride3D(float* _p, int _s0, int _s1) : p(_p), s0(_s0), s1(_s1) {}; __device__ float operator () (int x, int y, int z) const { return p[x*s0 + y*s1 + z]; }; __device__ float&amp; operator () (int x, int y, int z) { return p[x*s0 + y*s1 + z]; }; }; __global__ void mat_ops(float *A, int sA0, int sA1, float *B, int sB0, int sB1) { stride3D A3D(A, sA0, sA1); stride3D B3D(B, sB0, sB1); int xidx = blockIdx.x; int yidx = threadIdx.x; int zidx = threadIdx.y; B3D(xidx, yidx, zidx) = A3D(xidx, yidx, zidx); } \"\"\") A = 1 + np.arange(0, 4*4*3, dtype=np.float32).reshape(4,4,3) B = np.zeros((5,5,5), dtype=np.float32) A_k = gpuarray.to_gpu(A) B_k = gpuarray.to_gpu(B) astrides = np.array(A.strides, dtype=np.int32) // A.itemsize bstrides = np.array(B.strides, dtype=np.int32) // B.itemsize func = mod.get_function(\"mat_ops\") func(A_k, astrides[0], astrides[1], B_k, bstrides[0], bstrides[1], grid=(4,1,1),block=(4,3,1)) print(B_k[:4,:4,:3]) Here I have chosen to make the source and destination arrays different sizes, just to show that the code is general and will work for any size arrays as long as the block size is sufficient. Note that there is no array bounds checking here on the device code side, you will need to add that for non-trivial examples. Note also that this should work correctly both for fortran and C ordered numpy arrays, because it uses the numpy stride values directly. Performance will be effected on the CUDA side because of memory coalescing issues, however. Note: this won't work for both fortran and C ordering without extending the helper class to take strides for all dimensions and changing the kernel to accept strides for all dimensions of the input and output arrays. From a performance perspective it would be better to write separate helper classes for fortran and C ordered arrays."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np dims=img_in.shape rows=dims[0] columns=dims[1] channels=dims[2] #To be used in CUDA Device N=columns #create output image matrix img_out=np.zeros([rows,cols,channels]) #Convert img_in pixels to 8-bit int img_in=img_in.astype(np.int8) img_out=img_out.astype(np.int8) #Allocate memory for input image,output image and N img_in_gpu = cuda.mem_alloc(img_in.size * img_in.dtype.itemsize) img_out_gpu= cuda.mem_alloc(img_out.size * img_out.dtype.itemsize) N=cuda.mem_alloc(N.size*N.dtype.itemsize) #Transfer both input and now empty(output) image matrices from host to device cuda.memcpy_htod(img_in_gpu, img_in) cuda.memcpy_htod(img_out_gpu, img_out) cuda.memcpy_htod(N_out_gpu, N) #CUDA Device mod=SourceModule(\"\"\" __global__ void ArCatMap(int *img_in,int *img_out,int *N) { int col = threadIdx.x + blockIdx.x * blockDim.x; int row = threadIdx.y + blockIdx.y * blockDim.y; int img_out_index=col + row * N; int i=(row+col)%N; int j=(row+2*col)%N; img_out[img_out_index]=img_in[] }\"\"\") func = mod.get_function(\"ArCatMap\") #for i in range(1,385): func(out_gpu, block=(4,4,1)) cuda_memcpy_dtoh(img_out,img_in) cv2_imshow(img_out) What I have here is a 512 X 512 image. I am trying to convert all the elements of the input image img_in to 8 bit int using numpy.astype. The same is being done for the output image matrix img_out. When I try to use cuda.mem_alloc(), I get an error saying that 'type int has no attribute called size' and 'type int has no attribute called dtype'. Also, I get an error called 'int has no attribute called astype'. Could you state any possible causes ?",
        "answers": [
            [
                "You are getting a python error. You defined N as N=dims[1] so its just a single value integer. You can not call the function size on integers, as well, they are of size 1. Similarly, you can not check which type an int is, because well, its an int. You are doing that in the call to cuda.mem_alloc. You dont need to allocate memory for a single int, you can just pass it by value. Define the kernel as __global__ void ArCatMap(int *img_in,int *img_out,int N) instead of passing a pointer."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I make following this original post : PyCuda code to invert a high number of 3x3 matrixes. The code suggested as an answer is : $ cat t14.py import numpy as np import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit # kernel kernel = SourceModule(\"\"\" __device__ unsigned getoff(unsigned &amp;off){ unsigned ret = off &amp; 0x0F; off &gt;&gt;= 4; return ret; } // in-place is acceptable i.e. out == in) // T = float or double only const int block_size = 288; typedef double T; // *** can set to float or double __global__ void inv3x3(const T * __restrict__ in, T * __restrict__ out, const size_t n, const unsigned * __restrict__ pat){ __shared__ T si[block_size]; size_t idx = threadIdx.x+blockDim.x*blockIdx.x; T det = 1; if (idx &lt; n*9) det = in[idx]; unsigned sibase = (threadIdx.x / 9)*9; unsigned lane = threadIdx.x - sibase; // cheaper modulo si[threadIdx.x] = det; __syncthreads(); unsigned off = pat[lane]; T a = si[sibase + getoff(off)]; a *= si[sibase + getoff(off)]; T b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; a -= b; __syncthreads(); if (lane == 0) si[sibase+3] = a; if (lane == 3) si[sibase+4] = a; if (lane == 6) si[sibase+5] = a; __syncthreads(); det = si[sibase]*si[sibase+3]+si[sibase+1]*si[sibase+4]+si[sibase+2]*si[sibase+5]; if (idx &lt; n*9) out[idx] = a / det; } \"\"\") # host code def gpuinv3x3(inp, n): # internal constants not to be modified hpat = (0x07584, 0x08172, 0x04251, 0x08365, 0x06280, 0x05032, 0x06473, 0x07061, 0x03140) # Convert parameters into numpy array # *** change next line between float32 and float64 to match float or double inpd = np.array(inp, dtype=np.float64) hpatd = np.array(hpat, dtype=np.uint32) # *** change next line between float32 and float64 to match float or double output = np.empty((n*9), dtype= np.float64) # Get kernel function matinv3x3 = kernel.get_function(\"inv3x3\") # Define block, grid and compute blockDim = (288,1,1) # do not change gridDim = ((n/32)+1,1,1) # Kernel function matinv3x3 ( cuda.In(inpd), cuda.Out(output), np.uint64(n), cuda.In(hpatd), block=blockDim, grid=gridDim) return output inp = (1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 1.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0) n = 2 result = gpuinv3x3(inp, n) print(result.reshape(2,3,3)) The result gives, on an initial 1D array containing 18 values (so 2 matrixes 3x3), the right inverted matrixes, i.e : [[[ 2. -0. -1. ] [-1. -0.33333333 1. ] [-0. 0.33333333 -0. ]] [[ 1. 0. 0. ] [ 0. 1. 0. ] [ 0. 0. 1. ]]] Main issue : I would like to understand in detail the working of this algorithm, especially how the kernel allows to use shared memory for the initial 1D vector and brings then optimization when I execute this code on a large number of 3x3 matrixes. I understand the line : size_t idx = threadIdx.x+blockDim.x*blockIdx.x; which gives the global index of current work-item identified by local threadIdx and blockIdx of the current working-group block. I understand that __shared__ T si[block_size]; represents a share array, i.e associated to work-group blocks : this is what we call Local Memory. On the other hand, I don't understand this following part of kernel code : __shared__ T si[block_size]; size_t idx = threadIdx.x+blockDim.x*blockIdx.x; T det = 1; if (idx &lt; n*9) det = in[idx]; unsigned sibase = (threadIdx.x / 9)*9; unsigned lane = threadIdx.x - sibase; // cheaper modulo si[threadIdx.x] = det; __syncthreads(); unsigned off = pat[lane]; c __syncthreads(); if (lane == 0) si[sibase+3] = a; if (lane == 3) si[sibase+4] = a; if (lane == 6) si[sibase+5] = a; __syncthreads(); Indeed, what's the role of sibase index defined by unsigned sibase = (threadIdx.x / 9)*9; and also, what is the utility of parameter lane defined by : unsigned lane = threadIdx.x - sibase; // cheaper modulo Finally, shifting are applied with : T a = si[sibase + getoff(off)]; a *= si[sibase + getoff(off)]; T b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; a -= b; But I don't see clearly the functionality. Same problem for me about this part : if (lane == 0) si[sibase+3] = a; if (lane == 3) si[sibase+4] = a; if (lane == 6) si[sibase+5] = a; The determinant is calculated in a weird way that I can't grasp, i.e : det = si[sibase]*si[sibase+3]+si[sibase+1]*si[sibase+4]+si[sibase+2]*si[sibase+5]; I am not beginner in OpenCL, but I am not enough expert to understand fully this kernel code.",
        "answers": [
            [
                "Preliminaries First, its important to understand the arithmetic of a 3x3 matrix inversion, see here (and below). The general methodology used for kernel design is to assign one matrix result element per thread. Therefore I will need 9 threads per matrix. Ultimately each thread will be responsible for computing one of the 9 numerical results, for each matrix. In order to compute two matrices, we then need 18 threads, 3 matrices require 27 threads. An ancillary task is to decide threadblock/grid sizing. This follows typical methods (overall problem size determines total number of threads needed), but we will make a specific choice of 288 for threadblock size, as this is a convenient multiple of both 9 (number of threads per matrix) and 32 (number of threads per warp in CUDA), which gives us a certain measure of efficiency (no wasted threads, no gaps in data storage). Since our thread strategy is one thread per matrix element, we must collectively solve the matrix inversion arithmetic using 9 threads. The major tasks are to compute the transposed matrix of cofactors, and then to compute the determinant, then do the final arithmetic (divide by the determinant) to compute each result element. Computation of the cofactors The first task is to compute the transposed matrix of cofactors of A, called M: |a b c| let A = |d e f| |g h i| |ei-fh ch-bi bf-ce| M = |fg-di ai-cg cd-af| |dh-eg bg-ah ae-bd| We have 9 threads for this task, and nine elements of matrix M to compute, so we will assign one thread to each element of M. Each element of M depends on multiple input values (a, b, c, etc.) so we shall first load each input value (there are 9, one per thread), into shared memory: // allocate enough shared memory for one element per thread in the block: __shared__ T si[block_size]; // compute a globally unique thread index, so each thread has a unique number 0,1,2,etc. size_t idx = threadIdx.x+blockDim.x*blockIdx.x; // establish a temporary variable that will use and reuse during thread processing T det = 1; // do a thread check to make sure that our next load will be in-bounds for the input array in if (idx &lt; n*9) // load one element per thread, 9 threads per matrix will load an entire matrix det = in[idx]; // for a given matrix (9 threads) compute the base offset into shared memory, where this matrix data (9 elements) will be stored. All 9 threads have the same base offset unsigned sibase = (threadIdx.x / 9)*9; // for each group of 9 threads handling a matrix, compute for each thread in that group, a group offset or \"lane\" from 0..8, so each thread in the group has a unique identifier/assignment in the group unsigned lane = threadIdx.x - sibase; // cheaper modulo // let each thread place its matrix element a,b,c, etc. into shared memory si[threadIdx.x] = det; // shared memory is now loaded, make sure all threads have loaded before any calculations begin __syncthreads(); now that each A matrix element (a, b, c, ...) is loaded into shared memory, we can start to compute the cofactors in M. Let's focus on a particular thread (0) and its cofactor (ei-fh). All of the needed matrix elements to compute this cofactor (e, i, f, and h) are now in shared memory. We need a method to load them in sequence, and perform the needed multiplications and subtractions. At this point we observe two things: each M element (cofactor) has a different set of 4 needed elements of A each M element (cofactor) follows the same general arithmetic, given four arbitrary elements of A, lets refer to them generically as X, Y, Z and W. The arithmetic is XY-ZW. I take the first element, multply it by the second, and then take the third and fourth element and multiply them together, then subtract the two products. Since the general sequence of operations (2, above) is the same for all 9 cofactors, we only need a method to arrange the loading of the 4 needed matrix elements. This methodology is encoded into the load patterns that are hard-coded into the example: hpat = (0x07584, 0x08172, 0x04251, 0x08365, 0x06280, 0x05032, 0x06473, 0x07061, 0x03140) There are 9 load patterns, each occupying a hexadecimal quantity, one load pattern per thread, i.e. one load pattern per M matrix element (cofactor). Within a particular A matrix, the matrix elements a, b, c etc. are (already) loaded into shared memory at group offsets of 0, 1, 2, etc. The load pattern for a given thread will allow us to generate the sequence of group offsets, needed to retrieve the matrix elements of A from their locations in shared memory, to be used in sequence to compute the cofactor assigned to that thread. Considering thread 0, and its cofactor ei-fh, how does the load pattern 0x7584 encode the needed pattern to select e, then i, then f, then h? For this we have a helper function getoff which takes a load pattern, and successively (each time it is called) strips off an index. The first time I call getoff with an argument of 0x7584, it \"strips off\" the index 4, returns that, and replaces the 0x7584 load pattern with 0x758 for the next usage. 4 corresponds to e. The next time I call getoff with 0x758 it \"strips off\" the index 8, returns that, and replaces 0x758 with 0x75. 8 corresponds to i. The next time produces the index 5, corresponding to f, and the last time produces the index 7, corresponding to h. With that description then we will walk through the code, pretending we are thread 0, and describe the process of computing ei-fh: // get the load pattern for my matrix \"lane\" unsigned off = pat[lane]; //load my temporary variable `a` with the first item indexed in the load pattern: T a = si[sibase + getoff(off)]; // multiply my temporary variable `a` with the second item indexed in the load pattern a *= si[sibase + getoff(off)]; //load my temporary variable `b` with the third item indexed in the load pattern T b = si[sibase + getoff(off)]; // multiply my temporary variable `b` with the fourth item indexed in the load pattern b *= si[sibase + getoff(off)]; // compute the cofactor by subtracting the 2 products a -= b; sibase, as already indicated in the first commented code section, is the base offset in shared memory where that A matrix elements are stored. The getoff function then adds to this base address to select the relevant input element. Computation of the determinant The numerical value of the determinant is given by: det(A) = det = a(ei-fh) - b(di-fg) + c(dh-eg) If we decompose this, we see that all terms are actually already computed: a,b,c: these are input matrix elements, in shared locations (group offsets) 0, 1, 2 ei-fh: cofactor computed by thread 0 di-fg: cofactor computed by thread 3 (with sign reversed) dh-eg: cofactor computed by thread 6 Now, every thread will need the value of the determinant because it will be used by each thread during computation of its final (result) element. Therefore we will have every thread in the matrix redundantly compute the same value (which is more efficient than computing it, say, in one thread, then broadcasting that value to the other threads). In order to facilitate this, we will need 3 of the already computed cofactors made available to all 9 threads. So we will select 3 (no longer needed) locations in shared memory to \"publish\" these values. We still need the values in locations 0, 1, 2 because we need the input matrix elements a, b, and c for calculation of the determinant. But we no longer need the input elements in locations 3, 4, or 5 for the remainder of our work, so we will reuse those: // we are about to change shared values, so wait until all previous usage is complete __syncthreads(); // load cofactor computed by thread 0 into group offset 3 in shared if (lane == 0) si[sibase+3] = a; // load cofactor computed by thread 3 into group offset 4 in shared if (lane == 3) si[sibase+4] = a; // load cofactor computed by thread 6 into group offset 5 in shared if (lane == 6) si[sibase+5] = a; // make sure shared memory loads are complete __syncthreads(); // let every thread compute the determinant (same for all threads) // a * (ei-fh) + b * -(fg-di) + c * (dh-eg) det = si[sibase]*si[sibase+3]+si[sibase+1]*si[sibase+4]+si[sibase+2]*si[sibase+5]; Calculation of final result This only involves (for each thread) dividing the previously computed cofactor for that thread, by the just-computed determinant, and storing that result: // another thread check: make sure this thread is actually doing useful work if (idx &lt; n*9) // take previously computed cofactor, divide by determinant, store result out[idx] = a / det;"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I tried calculating 53 * 53 with pycuda as follows: import numpy as np import pycuda.gpuarray as gpuarray import pycuda.autoinit a = gpuarray.to_gpu(np.array([53])) print((a**2).get()[0]) That prints out 2808 while the true answer is 2809. Where did I go wrong?",
        "answers": [
            [
                "That prints out 2808 while the true answer is 2809 No it does not: $ cat ohnoitdoesnt.py import numpy as np import pycuda.gpuarray as gpuarray import pycuda.autoinit a = gpuarray.to_gpu(np.array([53])) print((a**2).get()[0]) $ python ohnoitdoesnt.py Traceback (most recent call last): File \"ohnoitdoesnt.py\", line 6, in &lt;module&gt; print((a**2).get()[0]) File \"/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/gpuarray.py\", line 659, in __pow__ return self._pow(other,new=True) File \"/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/gpuarray.py\", line 643, in _pow func = elementwise.get_pow_kernel(self.dtype) File \"&lt;string&gt;\", line 2, in get_pow_kernel File \"/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/tools.py\", line 430, in context_dependent_memoize result = func(*args) File \"/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/elementwise.py\", line 559, in get_pow_kernel \"pow_method\") File \"/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/elementwise.py\", line 161, in get_elwise_kernel arguments, operation, name, keep, options, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/elementwise.py\", line 147, in get_elwise_kernel_and_types keep, options, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/elementwise.py\", line 75, in get_elwise_module options=options, keep=keep) File \"/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/compiler.py\", line 291, in __init__ arch, code, cache_dir, include_dirs) File \"/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/compiler.py\", line 255, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/compiler.py\", line 137, in compile_plain stderr=stderr.decode(\"utf-8\", \"replace\")) pycuda.driver.CompileError: nvcc compilation of /tmp/tmpaeIBGe/kernel.cu failed [command: nvcc --cubin -arch sm_52 -I/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/cuda kernel.cu] [stderr: kernel.cu(19): error: calling a __host__ function(\"std::pow&lt;long, long&gt; \") from a __global__ function(\"pow_method\") is not allowed kernel.cu(19): error: identifier \"std::pow&lt;long, long&gt; \" is undefined in device code 2 errors detected in the compilation of \"/tmp/tmpxft_00001674_00000000-6_kernel.cpp1.ii\". ] This isn't an unknown problem in CUDA and PyCUDA -- the CUDA math library doesn't overload integer argument versions of most functions. If we fix this and use a floating point type, it works as expected: $ cat ohnoitdoesnt.py import numpy as np import pycuda.gpuarray as gpuarray import pycuda.autoinit a = gpuarray.to_gpu(np.array([53], dtype=np.float32)) print((a**2).get()[0]) $ python ohnoitdoesnt.py 2809.0"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to implement user's @rkp solution to their own question of how to speed up sparse matrix multiplications with cython by using the pycuda library (please note this is their second solution in their post). After installing pycuda, pymetis etc and running their exact same code (in IDLE Python 3.5.2) I am getting: TypeError: 'numpy.float64' object cannot be interpreted as an integer It turns out the (reproducible) part that produces this error is: import numpy as np import pycuda.autoinit import pycuda.driver as drv import pycuda.gpuarray as gpuarray from pycuda.sparse.packeted import PacketedSpMV from pycuda.tools import DeviceMemoryPool from scipy.sparse import csr_matrix COUNT = 100 N = 5000 P = 0.1 DTYPE = np.int32 #construct objects np.random.seed(0) a_dense = np.random.rand(N, N).astype(DTYPE) a_dense[np.random.rand(N, N) &gt;= P] = 0 a_sparse = csr_matrix(a_dense) #PacketedSpMV produces the error spmv = PacketedSpMV(a_sparse, is_symmetric=False, dtype=DTYPE) And the full error: Traceback (most recent call last): File \"C:/Users/svobodov/Desktop/data/tests/cython/t.py\", line 23, in &lt;module&gt; spmv = PacketedSpMV(a_sparse, is_symmetric=False, dtype=DTYPE) File \"C:\\Python35\\lib\\site-packages\\pycuda\\sparse\\packeted.py\", line 185, in __init__ local_row_costs) File \"pkt_build_cython.pyx\", line 22, in pycuda.sparse.pkt_build_cython.build_pkt_data_structure TypeError: 'numpy.float64' object cannot be interpreted as an integer I initially thought this to be the cython-related double-precision error but this is obviously something different as it is expecting specifically an integer rather than float32.. I tried tweaking the pkt_build_cython.pyx but without any success or confidence that I did it properly. Any ideas on how to resolve this please?",
        "answers": [
            [
                "As identified in comments, this was a result of a missing integer cast within an internal routine in the PyCUDA codebase. The bug was actually fixed in 2018, so if you use any PyCUDA 2019 release, you should have the corrected code and this issue should not occur."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to run two inferences in a pipeline using Jetson Nano. The first inference is object detection using MobileNet and TensorRT. My code for the first inference is pretty much replicated from the AastaNV/TRT_Obj_Detection repository. The only difference being that I changed that code so that it resides inside a class Inference1. The second inference job uses the outputs of the first inference to run further analysis. For this inference, I use tensorflow (not TensorRT, but I assume it is called in the backend?) using a custom model. This model is loaded from a .pb file (frozen graph). Once loaded, the inference is performed by calling the session.run() command of tensorflow. If I run ONLY Inference1 or ONLY Inference2, the code runs properly without any errors. However, when I pipe them, I get the error [TensorRT] ERROR: cuda/caskConvolutionLayer.cpp (355) - Cuda Error in execute: 33 (invalid resource handle) From what I see in the log, the TensorRT serialized graph is loaded without any problems. Tensorflow is also imported and it recognizes my GPU. From my searching on the internet I have found that this problem maybe related to CUDA Contexts? I therefore show below how i have setup the CUDA context in my code below. The create_cuda_context is only called once during the initialization of the Inference1 class. The run_inference_for_single_image is called every iteration. Code: def create_cuda_context(self): self.host_inputs, self.host_outputs = [], [] self.cuda_inputs, self.cuda_outputs = [], [] self.bindings = [] self.stream = cuda.Stream() for binding in self.engine: size = trt.volume(self.engine.get_binding_shape(binding)) * self.engine.max_batch_size host_mem = cuda.pagelocked_empty(size, np.float32) cuda_mem = cuda.mem_alloc(host_mem.nbytes) self.bindings.append(int(cuda_mem)) if self.engine.binding_is_input(binding): self.host_inputs.append(host_mem) self.cuda_inputs.append(cuda_mem) else: self.host_outputs.append(host_mem) self.cuda_outputs.append(cuda_mem) self.context = self.engine.create_execution_context() def run_inference_for_single_image(self, image): ''' Copies the image (already raveled) input into GPU memory, performs the forward pass and copies the result back to CPU memory ''' np.copyto(self.host_inputs[0], image) cuda.memcpy_htod_async(self.cuda_inputs[0], self.host_inputs[0], self.stream) self.context.execute_async(bindings=self.bindings, stream_handle=self.stream.handle) cuda.memcpy_dtoh_async(self.host_outputs[1], self.cuda_outputs[1], self.stream) cuda.memcpy_dtoh_async(self.host_outputs[0], self.cuda_outputs[0], self.stream) self.stream.synchronize() return self.host_outputs[0] Log: WARNING:tensorflow:From /usr/lib/python3.6/dist-packages/graphsurgeon/DynamicGraph.py:4: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead. [TensorRT] INFO: Glob Size is 14049908 bytes. [TensorRT] INFO: Added linear block of size 5760000 [TensorRT] INFO: Added linear block of size 2880000 [TensorRT] INFO: Added linear block of size 409600 [TensorRT] INFO: Added linear block of size 218624 [TensorRT] INFO: Added linear block of size 61440 [TensorRT] INFO: Added linear block of size 57344 [TensorRT] INFO: Added linear block of size 30720 [TensorRT] INFO: Added linear block of size 20992 [TensorRT] INFO: Added linear block of size 9728 [TensorRT] INFO: Added linear block of size 9216 [TensorRT] INFO: Added linear block of size 2560 [TensorRT] INFO: Added linear block of size 2560 [TensorRT] INFO: Added linear block of size 1024 [TensorRT] INFO: Added linear block of size 512 [TensorRT] INFO: Found Creator FlattenConcat_TRT [TensorRT] INFO: Found Creator GridAnchor_TRT [TensorRT] INFO: Found Creator FlattenConcat_TRT [TensorRT] INFO: Found Creator NMS_TRT [TensorRT] INFO: Deserialize required 5159079 microseconds. Infering on input.mp4 WARNING:tensorflow:From /home/user/Desktop/SVM_TensorRT/deep_sort/tools/generate_detections.py:75: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead. 2018-01-29 02:01:38.254282: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1 2018-01-29 02:01:38.286962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.287300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: name: NVIDIA Tegra X1 major: 5 minor: 3 memoryClockRate(GHz): 0.9216 pciBusID: 0000:00:00.0 2018-01-29 02:01:38.287552: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2018-01-29 02:01:38.287744: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 2018-01-29 02:01:38.287983: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0 2018-01-29 02:01:38.288201: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0 2018-01-29 02:01:38.415478: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0 2018-01-29 02:01:38.484010: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0 2018-01-29 02:01:38.484668: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7 2018-01-29 02:01:38.485343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.486009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.486286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0 2018-01-29 02:01:38.665379: W tensorflow/core/platform/profile_utils/cpu_utils.cc:98] Failed to find bogomips in /proc/cpuinfo; cannot determine CPU frequency 2018-01-29 02:01:38.682935: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24f9ea50 executing computations on platform Host. Devices: 2018-01-29 02:01:38.683009: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt; 2018-01-29 02:01:38.764975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.765291: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x572614c0 executing computations on platform CUDA. Devices: 2018-01-29 02:01:38.765349: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): NVIDIA Tegra X1, Compute Capability 5.3 2018-01-29 02:01:38.766014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.766158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: name: NVIDIA Tegra X1 major: 5 minor: 3 memoryClockRate(GHz): 0.9216 pciBusID: 0000:00:00.0 2018-01-29 02:01:38.766716: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2018-01-29 02:01:38.766814: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 2018-01-29 02:01:38.766879: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0 2018-01-29 02:01:38.767002: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0 2018-01-29 02:01:38.767174: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0 2018-01-29 02:01:38.767311: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0 2018-01-29 02:01:38.767423: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7 2018-01-29 02:01:38.767731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.768049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:38.768136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0 2018-01-29 02:01:38.783718: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2018-01-29 02:01:41.046094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix: 2018-01-29 02:01:41.046260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187] 0 2018-01-29 02:01:41.046311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0: N 2018-01-29 02:01:41.054160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:41.054730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:972] ARM64 does not support NUMA - returning NUMA node zero 2018-01-29 02:01:41.112041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 85 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0, compute capability: 5.3) WARNING:tensorflow:From /home/user/Desktop/SVM_TensorRT/deep_sort/tools/generate_detections.py:76: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead. WARNING:tensorflow:From /home/user/Desktop/SVM_TensorRT/deep_sort/tools/generate_detections.py:80: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. [TensorRT] ERROR: CUDA cask failure at execution for trt_maxwell_scudnn_128x32_relu_small_nn_v1. [TensorRT] ERROR: cuda/caskConvolutionLayer.cpp (355) - Cuda Error in execute: 33 (invalid resource handle) [TensorRT] ERROR: cuda/caskConvolutionLayer.cpp (355) - Cuda Error in execute: 33 (invalid resource handle)",
        "answers": [
            [
                "I believe the two models you are trying to run both try to create CUDA Context. The first one initialize CUDA Context directly from TensorRT library, while the 2nd one initize new CUDA Context inside Tensorflow. When the 1st model tries to perform inferencing, it will use wrong CUDA Context, resulting with that error. If you are using the same TensorRT, Tensorflow (or other CUDA Libraries) for both models, it will be a lot easier to control CUDA Context. From my experience, Tensorflow and direct CUDA are not playing along well. I would suggest that you separate both models into different thread. That will ensure that both TensorRT and Tensorflow create and use their own different CUDA Contexts... (given that you are not running to OOM problem. I once tried to use both SSD+MobileNetV2 for object detection and another MobileNet for more classification on detected object on Jetson Nano. I faced the OOM and ended up running the 2nd model on CPU instead)."
            ],
            [
                "I had a similar error and that is what helped me in this case: Remove import pycuda.autoinit and do import pycuda.driver as cuda ... cuda.init() device = cuda.Device(0) cuda_driver_context = device.make_context() Wrap the piece of code that does inference with TensorRT like this: cuda_driver_context.push() # copy data to device memory, run inference, copy data from device memory cuda_driver_context.pop() See the related thread on NVidia's forum."
            ]
        ],
        "votes": [
            5.0000001,
            1e-07
        ]
    },
    {
        "question": "I wanted to use Mersenne Twister random generator inside pyCuda kernels for numerical experiment. Via Internet I found no simple examples of how to do it, so, I tried to construct something from Cuda documentation and pyCuda examples (pyCuda code below). How it can be done correctly? Thank you. code = \"\"\" #include &lt;curand_kernel.h&gt; #include &lt;curand_mtgp32_host.h&gt; #include &lt;curand_mtgp32dc_p_11213.h&gt; const int nstates = %(NGENERATORS)s; __device__ curandStateMtgp32 *devMTGPStates[nstates]; __device__ mtgp32_kernel_params *devKernelParams; curandMakeMTGP32Constants(mtgp32dc_params_fast_11213, devKernelParams); curandMakeMTGP32KernelState(devMTGPStates, mtgp32dc_params_fast_11213, devKernelParams, 64, %(seed)s); extern \"C\" { __global__ void generate_uniform(int N, int *result) { int tidx = threadIdx.x + blockIdx.x * blockDim.x; if (tidx &lt; nstates) { curandState_t s = *states[tidx]; for(int i = tidx; i &lt; N; i += blockDim.x * gridDim.x) { result[i] = curand_uniform(&amp;s); } *states[tidx] = s; } } } \"\"\" seed = 0 N = 256 * 64 nvalues = int(10**3) mod = SourceModule(code % { \"NGENERATORS\" : N, \"seed\": seed}, no_extern_c=True) CompileError: nvcc compilation of C:\\Users\\limen\\AppData\\Local\\Temp\\tmpspxyn4h9\\kernel.cu failed [command: nvcc --cubin -arch sm_50 -m64 -Ic:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\pycuda\\cuda kernel.cu] [stdout: kernel.cu ] [stderr: kernel.cu(10): error: this declaration has no storage class or type specifier kernel.cu(10): error: declaration is incompatible with \"curandStatus_t curandMakeMTGP32Constants(const mtgp32_params_fast_t *, mtgp32_kernel_params_t *)\" C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin/../include\\curand_mtgp32_host.h(367): here kernel.cu(10): error: a value of type \"mtgp32_params_fast_t *\" cannot be used to initialize an entity of type \"int\" kernel.cu(10): error: expected a \")\" kernel.cu(11): error: this declaration has no storage class or type specifier kernel.cu(11): error: declaration is incompatible with \"curandStatus_t curandMakeMTGP32KernelState(curandStateMtgp32_t *, mtgp32_params_fast_t *, mtgp32_kernel_params_t *, int, unsigned long long)\" C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin/../include\\curand_mtgp32_host.h(481): here kernel.cu(11): error: a value of type \"curandStateMtgp32 **\" cannot be used to initialize an entity of type \"int\" kernel.cu(11): error: expected a \")\" kernel.cu(21): error: identifier \"states\" is undefined 9 errors detected in the compilation of \"C:/Users/limen/AppData/Local/Temp/tmpxft_00001aa8_00000000-10_kernel.cpp1.ii\". ]",
        "answers": [
            [
                "I did the following: import pycuda.curandom print(dir(pycuda.curandom)) This gives all attributes that are present in the Python module pycuda.curandom. There was no mention of any Mersenne-Twister(MT) or MT-based Pseudo Random Number Generator. This indicates that MT isn't implemented in PyCUDA."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm a Python Programmer who recently started with PyCuda because I need to write a custom filter for image processing. I found tex2D and it seems very elegant to me for handling padding and out of range problems. My problem is that I am very confused about how I can pass data to the cuda kernel. For now I got this far: #!/usr/bin/env python3 \"\"\"minimal example: cuda kernel that returns the input using textures\"\"\" import numpy as np import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit from pycuda.tools import dtype_to_ctype # cuda kernel mod = SourceModule(\"\"\" #include &lt;pycuda-helpers.hpp&gt; texture&lt;fp_tex_float, 2&gt; my_tex; __global__ void return_input(const int input_width, const int input_height, float *output) { int row = blockIdx.x * blockDim.x + threadIdx.x; int col = blockIdx.y * blockDim.y + threadIdx.y; if(row &lt; input_height &amp;&amp; col &lt; input_width) { int index = col * input_width + row; output[index] = tex2D(my_tex, row, col); } } \"\"\") # get from cuda kernel return_input = mod.get_function('return_input') my_tex = mod.get_texref('my_tex') # setup texture shape = (5, 5) img_cpu = np.random.rand(*shape).astype(np.float32) print(img_cpu) img_gpu = cuda.matrix_to_array(img_cpu, order='C', allow_double_hack=True) my_tex.set_array(img_gpu) # setup output out_cpu = np.zeros((shape), dtype=np.float32) out_gpu = cuda.to_device(out_cpu) # build grid blocksize = 32 img_height, img_width = np.shape(img_cpu) grid = (int(np.ceil(img_height / blocksize)), int(np.ceil(img_width / blocksize)), 1) # call cuda kernel return_input(img_width, img_height, out_gpu, block=(blocksize, blocksize, 1), grid=grid) # copy back to host cuda.memcpy_dtoh(out_gpu, out_cpu) print(out_cpu)",
        "answers": [
            [
                "For everyone stumbling over the same Problem here my Solution: Cuda file named minimal_kernel.cu: #include &lt;pycuda-helpers.hpp&gt; texture&lt;float, 2&gt; my_tex; __global__ void return_input(const int input_width, const int input_height, float *output) { int row = blockIdx.x * blockDim.x + threadIdx.x; int col = blockIdx.y * blockDim.y + threadIdx.y; if(row &lt; input_height &amp;&amp; col &lt; input_width) { int index = col * input_width + row; output[index] = tex2D(my_tex, row, col); } } Python file: #!/usr/bin/env python3 \"\"\"minimal example: cuda kernel that returns the input using textures\"\"\" import numpy as np import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit # get from cuda kernel with open('./minimal_kernel.cu', 'r') as f: mod = SourceModule(f.read()) return_input = mod.get_function('return_input') my_tex = mod.get_texref('my_tex') # setup texture shape = (5, 5) img_in = np.random.rand(*shape).astype(np.float32) print(img_in) cuda.matrix_to_texref(img_in, my_tex, order='C') # setup output img_out = np.zeros(shape, dtype=np.float32) # build grid blocksize = 32 img_height, img_width = np.int32(np.shape(img_in)) grid = (int(np.ceil(img_height / blocksize)), int(np.ceil(img_width / blocksize)), 1) # call cuda kernel return_input(img_width, img_height, cuda.Out(img_out), texrefs=[my_tex], block=(blocksize, blocksize, 1), grid=grid) print(img_out)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to implement the quicksort parallelization by specifying the list separation snippet in two others compared to the pivo. I am having problems with the syntax and to save the pointer at the end of the two new lists. How do I get rid of the syntax errors and save the list sizes at the end of the kernel? import pycuda.autoinit import pycuda.driver as cuda from pycuda import gpuarray, compiler from pycuda.compiler import SourceModule import time import numpy as np def quickSort_paralleloGlobal(listElements: list) -&gt; list: if len(listElements) &lt;= 1: return listElements else: pivo = listElements.pop() list1 = [] list2 = [] kernel_code_template = \"\"\" __global__ void separateQuick(int *listElements, int *list1, int *list2, int pivo) { int index1 = 0, index2 = 0; int index = blockIdx.x * blockDim.x + threadIdx.x; int stride = blockDim.x * gridDim.x; for (int i = index; i &lt; %(ARRAY_SIZE)s; i+= stride) if (lista[i] &lt; pivo { list1[index2] = listElements[i]; index1++; } else { list2[index2] = listElements[i]; index2++; } } \"\"\" SIZE = len(listElements) listElements = np.asarray(listElements) listElements = listElements.astype(np.int) lista_gpu = cuda.mem_alloc(listElements.nbytes) cuda.memcpy_htod(lista_gpu, listElements) list1_gpu = cuda.mem_alloc(listElements.nbytes) list2_gpu = cuda.mem_alloc(listElements.nbytes) BLOCK_SIZE = 256 NUM_BLOCKS = (SIZE + BLOCK_SIZE - 1) // BLOCK_SIZE kernel_code = kernel_code_template % { 'ARRAY_SIZE': SIZE } mod = compiler.SourceModule(kernel_code) arraysQuick = mod.get_function(\"separateQuick\") arraysQuick(lista_gpu, list1_gpu, list2_gpu, pivo, block=(BLOCK_SIZE, 1, 1), grid=(NUM_BLOCKS, 1)) list1 = list1_gpu.get() list2 = list2_gpu.get() np.allclose(list1, list1_gpu.get()) np.allclose(list2, list2_gpu.get()) return quickSort_paralleloGlobal(list1) + [pivo] + quickSort_paralleloGlobal(list2) Here is the runtime error: Traceback (most recent call last): File \"C:/Users/mateu/Documents/GitHub/ppc_Sorting_and_Merging/quickSort.py\", line 104, in &lt;module&gt; print(quickSort_paraleloGlobal([1, 5, 4, 2, 0])) File \"C:/Users/mateu/Documents/GitHub/ppc_Sorting_and_Merging/quickSort.py\", line 60, in quickSort_paraleloGlobal mod = compiler.SourceModule(kernel_code) File \"C:\\Users\\mateu\\Documents\\GitHub\\ppc_Sorting_and_Merging\\venv\\lib\\site-packages\\pycuda\\compiler.py\", line 291, in __init__ arch, code, cache_dir, include_dirs) File \"C:\\Users\\mateu\\Documents\\GitHub\\ppc_Sorting_and_Merging\\venv\\lib\\site-packages\\pycuda\\compiler.py\", line 254, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"C:\\Users\\mateu\\Documents\\GitHub\\ppc_Sorting_and_Merging\\venv\\lib\\site-packages\\pycuda\\compiler.py\", line 137, in compile_plain stderr=stderr.decode(\"utf-8\", \"replace\")) pycuda.driver.CompileError: nvcc compilation of C:\\Users\\mateu\\AppData\\Local\\Temp\\tmpefxgkfkk\\kernel.cu failed [command: nvcc --cubin -arch sm_61 -m64 -Ic:\\users\\mateu\\documents\\github\\ppc_sorting_and_merging\\venv\\lib\\site-packages\\pycuda\\cuda kernel.cu] [stdout: kernel.cu ] [stderr: kernel.cu(10): error: expected a \")\" kernel.cu(19): warning: parsing restarts here after previous syntax error kernel.cu(19): error: expected a statement kernel.cu(5): warning: variable \"indexMenor\" was declared but never referenced kernel.cu(5): warning: variable \"indexMaior\" was declared but never referenced 2 errors detected in the compilation of \"C:/Users/mateu/AppData/Local/Temp/tmpxft_00004260_00000000-10_kernel.cpp1.ii\". ] Process finished with exit code 1",
        "answers": [
            [
                "There are a number of problems with your code. I don't think I will be able to list them all. However one of the central problems is that you have attempted to do a naive conversion of a serial quicksort into a thread-parallel quicksort, and such a simple conversion is not possible. To allow threads to work in a parallel fashion, while dividing up an input list into one of two separate output lists, requires a number of changes to your kernel code. However we can address most of the other issues by limiting your kernel launches to one thread each. With that idea, the following code appears to sort the given input correctly: $ cat t18.py import pycuda.autoinit import pycuda.driver as cuda from pycuda import gpuarray, compiler from pycuda.compiler import SourceModule import time import numpy as np def quickSort_paralleloGlobal(listElements): if len(listElements) &lt;= 1: return listElements else: pivo = listElements.pop() pivo = np.int32(pivo) kernel_code_template = \"\"\" __global__ void separateQuick(int *listElements, int *list1, int *list2, int *l1_size, int *l2_size, int pivo) { int index1 = 0, index2 = 0; int index = blockIdx.x * blockDim.x + threadIdx.x; int stride = blockDim.x * gridDim.x; for (int i = index; i &lt; %(ARRAY_SIZE)s; i+= stride) if (listElements[i] &lt; pivo) { list1[index1] = listElements[i]; index1++; } else { list2[index2] = listElements[i]; index2++; } *l1_size = index1; *l2_size = index2; } \"\"\" SIZE = len(listElements) listElements = np.asarray(listElements) listElements = listElements.astype(np.int32) lista_gpu = cuda.mem_alloc(listElements.nbytes) cuda.memcpy_htod(lista_gpu, listElements) list1_gpu = cuda.mem_alloc(listElements.nbytes) list2_gpu = cuda.mem_alloc(listElements.nbytes) l1_size = cuda.mem_alloc(4) l2_size = cuda.mem_alloc(4) BLOCK_SIZE = 1 NUM_BLOCKS = 1 kernel_code = kernel_code_template % { 'ARRAY_SIZE': SIZE } mod = compiler.SourceModule(kernel_code) arraysQuick = mod.get_function(\"separateQuick\") arraysQuick(lista_gpu, list1_gpu, list2_gpu, l1_size, l2_size, pivo, block=(BLOCK_SIZE, 1, 1), grid=(NUM_BLOCKS, 1)) l1_sh = np.zeros(1, dtype = np.int32) l2_sh = np.zeros(1, dtype = np.int32) cuda.memcpy_dtoh(l1_sh, l1_size) cuda.memcpy_dtoh(l2_sh, l2_size) list1 = np.zeros(l1_sh, dtype=np.int32) list2 = np.zeros(l2_sh, dtype=np.int32) cuda.memcpy_dtoh(list1, list1_gpu) cuda.memcpy_dtoh(list2, list2_gpu) list1 = list1.tolist() list2 = list2.tolist() return quickSort_paralleloGlobal(list1) + [pivo] + quickSort_paralleloGlobal(list2) print(quickSort_paralleloGlobal([1, 5, 4, 2, 0])) $ python t18.py [0, 1, 2, 4, 5] $ The next step in the porting process would be to convert your naive serial kernel to one that could operate in a thread-parallel fashion. One relatively simple approach would be to use atomics to manage all output data (both lists, as well as updates to the sizes of each list). Here is one possible approach: $ cat t18.py import pycuda.autoinit import pycuda.driver as cuda from pycuda import gpuarray, compiler from pycuda.compiler import SourceModule import time import numpy as np def quickSort_paralleloGlobal(listElements): if len(listElements) &lt;= 1: return listElements else: pivo = listElements.pop() pivo = np.int32(pivo) kernel_code_template = \"\"\" __global__ void separateQuick(int *listElements, int *list1, int *list2, int *l1_size, int *l2_size, int pivo) { int index = blockIdx.x * blockDim.x + threadIdx.x; int stride = blockDim.x * gridDim.x; for (int i = index; i &lt; %(ARRAY_SIZE)s; i+= stride) if (listElements[i] &lt; pivo) { list1[atomicAdd(l1_size, 1)] = listElements[i]; } else { list2[atomicAdd(l2_size, 1)] = listElements[i]; } } \"\"\" SIZE = len(listElements) listElements = np.asarray(listElements) listElements = listElements.astype(np.int32) lista_gpu = cuda.mem_alloc(listElements.nbytes) cuda.memcpy_htod(lista_gpu, listElements) list1_gpu = cuda.mem_alloc(listElements.nbytes) list2_gpu = cuda.mem_alloc(listElements.nbytes) l1_size = cuda.mem_alloc(4) l2_size = cuda.mem_alloc(4) BLOCK_SIZE = 256 NUM_BLOCKS = (SIZE + BLOCK_SIZE - 1) // BLOCK_SIZE kernel_code = kernel_code_template % { 'ARRAY_SIZE': SIZE } mod = compiler.SourceModule(kernel_code) arraysQuick = mod.get_function(\"separateQuick\") l1_sh = np.zeros(1, dtype = np.int32) l2_sh = np.zeros(1, dtype = np.int32) cuda.memcpy_htod(l1_size, l1_sh) cuda.memcpy_htod(l2_size, l2_sh) arraysQuick(lista_gpu, list1_gpu, list2_gpu, l1_size, l2_size, pivo, block=(BLOCK_SIZE, 1, 1), grid=(NUM_BLOCKS, 1)) cuda.memcpy_dtoh(l1_sh, l1_size) cuda.memcpy_dtoh(l2_sh, l2_size) list1 = np.zeros(l1_sh, dtype=np.int32) list2 = np.zeros(l2_sh, dtype=np.int32) cuda.memcpy_dtoh(list1, list1_gpu) cuda.memcpy_dtoh(list2, list2_gpu) list1 = list1.tolist() list2 = list2.tolist() return quickSort_paralleloGlobal(list1) + [pivo] + quickSort_paralleloGlobal(list2) print(quickSort_paralleloGlobal([1, 5, 4, 2, 0])) $ python t18.py [0, 1, 2, 4, 5] $ I'm not suggesting that the above examples are perfect or defect free. Also, I have not identified each and every change I made to your code. I suggest you study the differences between these examples and your posted code. I should also mention that this isn't a fast or efficient way to sort numbers on the GPU. I assume this is for a learning exercise. If you're interested in fast parallel sorting, you are encouraged to use a library implementation. If you want to do this from python, one possible implementation is provided by cupy"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am developing a program on NVIDIA Jetson Nano which capture a frame from webcam/video using opencv. Afterwards, it does some computation on the frame on gpu. This is done for each frame. To reduce run time, I am using mapped memory avoiding explicit copy between host, and device. The problem is I still have to copy the frame to the shared pointer address i.e. HOST to HOST copy which takes a good amount of time. How can I cut/reduce this time OR if rephrased how can I update frame data at the shared memory location each time efficently? Currently, I copy the frame to the shared address using np.copyto frame = cap.read() sharedadd = cuda.pagelocked_empty((1,3,500,500), dtype=np.float32),mem_flags=cuda.host_alloc_flags.DEVICEMAP) # allocate mapped memory np.copyto(sharedadd, frame.ravel()) # takes quite some time I tried using ctype pointer to dereference the shared memory location and assign it new value. However, it expects the frame to be a ctype pointer too. frame = cap.read() sharedadd = cuda.pagelocked_empty((1,3,500,500),dtype=np.float32),mem_flags=cuda.host_alloc_flags.DEVICEMAP) # allocate mapped memory c_float_p = ctypes.POINTER(ctypes.c_float) sharedptr = sharedadd.ctypes.data_as(c_float_p) ctypes.cast(ctypes.addressof(sharedptr), ?).contents # second argument is expected to be a ctype pointer to frame",
        "answers": [
            [
                "how can I update frame data at the shared memory location each time efficently? If I understand correctly, you want to access (and change) the content of the float array pointed to by shared_ptr. Given: frame = cap.read() sharedadd = cuda.pagelocked_empty((1,3,500,500),dtype=np.float32),mem_flags=cuda.host_alloc_flags.DEVICEMAP) # allocate mapped memory c_float_p = ctypes.POINTER(ctypes.c_float) sharedptr = sharedadd.ctypes.data_as(c_float_p) You can access the content directly by indexing shared_ptr: float_1 = shared_ptr[0] # read shared_ptr[0] = float_1 + 100.0 # write Here's a simple pure python (no cuda) example: &gt;&gt;&gt; import ctypes &gt;&gt;&gt; floats = [100.0, 200.0, 300.0] # array of python floats &gt;&gt;&gt; c_floats = (ctypes.c_float * len(floats))(*floats) # array of ctypes floats &gt;&gt;&gt; p_floats = ctypes.cast(c_floats, ctypes.POINTER(ctypes.c_float)) # pointer to array of ctypes floats &gt;&gt;&gt; p_floats[0] # read 100.0 &gt;&gt;&gt; p_floats[1] # read 200.0 &gt;&gt;&gt; p_floats[2] # read 300.0 &gt;&gt;&gt; p_floats[2] = 400.0 # write index 2 (was 300.0) &gt;&gt;&gt; p_floats[2] # read newly written index 400.0 &gt;&gt;&gt; c_floats[2] # check index 2 has been updated on the array itself 400.0 [edit] Can I do it by writing to all the indices at once? You can't slice from a ctypes pointer but you can use ctypes.memmove(). Following previous example: &gt;&gt;&gt; new_floats = [1000.0, 2000.0, 3000.0] &gt;&gt;&gt; c_new_floats = (ctypes.c_float * len(new_floats))(*new_floats) &gt;&gt;&gt; p_new_floats = ctypes.cast(c_new_floats, ctypes.POINTER(ctypes.c_float)) &gt;&gt;&gt; ctypes.memmove(p_floats, p_new_floats, len(new_floats) * ctypes.sizeof(ctypes.c_float)) # copy new floats to previous array through pointers. &gt;&gt;&gt; p_floats[0] 1000.0 &gt;&gt;&gt; p_floats[1] 2000.0 &gt;&gt;&gt; p_floats[2] 3000.0"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to pass a 2-D array of complex numbers into a PyCUDA kernel, and am getting unexpected results. Here's my test code: import numpy as np import pycuda.driver as cuda import pycuda.autoinit from pycuda import gpuarray from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" #include &lt;pycuda-complex.hpp&gt; #include &lt;stdio.h&gt; typedef pycuda::complex&lt;float&gt; cmplx; __global__ void myFunc(cmplx *A) { // A : input, array shape (), of type complex64 int ROWS = 3; int COLS = 2; printf(\"\\\\nKernel &gt;&gt;\"); for(int row = 0; row &lt; ROWS; row++) { printf(\"\\\\n\"); for(int col = 0; col &lt; COLS; col++) { printf(\"[row %d, col %d]: %f + %fi\",row, col, A[row,col].real(), A[row,col].imag()); printf(\"\\\\t\"); } } printf(\"\\\\n\\\\n\"); } \"\"\") A = np.zeros((3,2),dtype=complex) A[0,0] = 1.23 + 3.5j A[1,0] = 3.4 + 1.0j A_gpu = gpuarray.to_gpu(A.astype(np.complex64)) print(\"Host &gt;&gt;\") print(A_gpu) func = mod.get_function(\"myFunc\") func(A_gpu, block=(1,1,1), grid=(1, 1, 1) ) The results are as follows: Host &gt;&gt; [[1.23+3.5j 0. +0.j ] [3.4 +1.j 0. +0.j ] [0. +0.j 0. +0.j ]] Kernel &gt;&gt; [row 0, col 0]: 1.230000 + 3.500000i [row 0, col 1]: 0.000000 + 0.000000i [row 1, col 0]: 1.230000 + 3.500000i [row 1, col 1]: 0.000000 + 0.000000i [row 2, col 0]: 1.230000 + 3.500000i [row 2, col 1]: 0.000000 + 0.000000i Could anybody explain why the array in the kernel doesn't look like the one I pass to it?",
        "answers": [
            [
                "The indexing in your kernel code is broken (see here as to why). While A[row,col] is technically valid syntax in C++, it doesn't imply multidimensional array slicing as it does in Python. In fact, A[row,col] evaluates to A[row], so it should be obvious why the output of print statement doesn't match your expectations. Numpy arrays are storage contiguously in memory, and you must use your own indexing scheme to access the array. By default, numpy uses row major ordering for multidimensional arrays. This: mod = SourceModule(\"\"\" #include &lt;pycuda-complex.hpp&gt; #include &lt;stdio.h&gt; typedef pycuda::complex&lt;float&gt; cmplx; __global__ void myFunc(cmplx *A) { // A : input, array shape (), of type complex64 int ROWS = 3; int COLS = 2; printf(\"\\\\nKernel &gt;&gt;\"); for(int row = 0; row &lt; ROWS; row++) { printf(\"\\\\n\"); for(int col = 0; col &lt; COLS; col++) { printf(\"[row %d, col %d]: %f + %fi\",row, col, A[row*COLS+col].real(), A[row*COLS+col].imag()); printf(\"\\\\t\"); } } printf(\"\\\\n\\\\n\"); } \"\"\") will work as expected: %run complex_print.py Host &gt;&gt; [[ 1.23000002+3.5j 0.00000000+0.j ] [ 3.40000010+1.j 0.00000000+0.j ] [ 0.00000000+0.j 0.00000000+0.j ]] Kernel &gt;&gt; [row 0, col 0]: 1.230000 + 3.500000i [row 0, col 1]: 0.000000 + 0.000000i [row 1, col 0]: 3.400000 + 1.000000i [row 1, col 1]: 0.000000 + 0.000000i [row 2, col 0]: 0.000000 + 0.000000i [row 2, col 1]: 0.000000 + 0.000000i"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I was trying to run a code that is based off the following link https://documen.tician.de/pycuda/tutorial.html Running code in this link turned out to be fine. This is my version with similar definitions. Note that I was running under engine context since I want to run an engine.execute function. import pycuda.driver as cuda import pycuda.autoinit import tensorrt as trt import numpy as np from keras.datasets import mnist dims = (1, 28, 28) dims2 = (1, 1, 10) batch_size = 1000 nbytes = batch_size * trt.volume(dims) * np.dtype(np.float32).itemsize nbytes2 = batch_size * trt.volume(dims2) * np.dtype(np.float32).itemsize self.d_src = cuda.mem_alloc(nbytes) self.d_dst = cuda.mem_alloc(nbytes2) bindings = [int(self.d_src), int(self.d_dst)] (x_train, y_train), (x_test, y_test) = mnist.load_data() img_h = x_test.shape[1] img_w = x_test.shape[2] x_test = x_test.reshape(x_test.shape[0], 1, img_h, img_w) x_test = x_test.astype('float32') x_test /= 255 num_test = x_test.shape[0] output_size = batch_size * trt.volume(dims2) y = np.empty((num_test,output_size), np.float32) for i in range(0, num_test, batch_size): x_part = x_test[i : i + batch_size] y_part = y[i : i + batch_size] cuda.memcpy_htod(self.d_src, x_part) cuda.memcpy_dtoh(y_part, self.d_dst) However it failed at the memcpydtoh, yet memcpyhtod worked. File \"a.py\", line 164, in infer cuda.memcpy_dtoh(y_part, self.d_dst) pycuda._driver.LogicError: cuMemcpyDtoH failed: invalid argument Why is this the case? The definitions are similar to the code in the link.",
        "answers": [
            [
                "I have solved it anyway. The device allocation needs to be different for x_part and y_part since their sizes are different. So it works if I define output_size = trt.volume(dims2). The error message isn't very helpful to begin with &amp; made me think I inputted wrong arguments instead."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Is it possible to use a numpy array of shape (10, 3) like an array of 10 float3 inside a pycuda kernel? I'm trying to solve the problem of nearest point, with an array on point array_point of shape (10,3) where 10 are the point locations, for example array_point[0] is [x,y,z]. To solve this I really want to send to the kernel an float3* parameter, but I'm not sure how to do it. # for simplicity I will use a 4 point case with all handwritten directly # only mockup script, not really working... actually is the question import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule from pycuda import gpuarray, tools import numpy as np data = np.array([[1,2,3],[3,4,5],[7,8,9], [10,11,12]], dtype=np.float32) print(data.shape) data_gpu = gpuarray.to_gpu(data) // out must be like out_gpu[0] -&gt; 1 means point 0 nearest point is point 1 ... I Hope be clear with the main idea out_gpu = gpuarray.empty(4, np.int32) mod = SourceModule(\"\"\" __device__ float distance_not_sqrt(float3 p1, float3 p2) { return (p1.x - p2.x) * (p1.x - p2.x) + (p1.y - p2.y) * (p1.y - p2.y) + (p1.z - p2.z) * (p1.z - p2.z) ; } __global__ void find_closest(float3 *a, int*out) { int idx = threadIdx.x; int it; int it_min = -1; float dist_min = 1000.0; // more large than any real distance point for(it=0; it &lt; 4; it++){ if(it==idx)continue float dist = distance_not_sqrt(a[id], a[it]) if(dist &lt; dist_min){ dist_min = dist; it_min = it; } } out[idx] = it_min; } \"\"\") func = mod.get_function(\"find_closest\") func(data_gpu, out_gpu, block=(4,1,1)) print(out_gpu.get())",
        "answers": [
            [
                "Passing 10 x 3 numpy arrays will automatically work for float3 arrays, but you will need to make sure your contiguous dimension (dimension where elements are next to each other) is the one with the 3 in it. For example, a numpy array like so: x = np.array([[1,2,3],[1,2,3],[1,2,3],[1,2,3]],dtype=np.float32,order='C') is the same as 4 float3 values of (1,2,3), you can check this using np.ravel(order='K') x.ravel(order='K') array([1., 2., 3., 1., 2., 3., 1., 2., 3., 1., 2., 3.], dtype=float32) but if instead use fortran order (denoted with 'F', where 'C' is C order), the result won't be what we expect if we wanted 4 (1,2,3) float3s. x = np.array([[1,2,3],[1,2,3],[1,2,3],[1,2,3]],dtype=np.float32,order='F') and the result: x.ravel(order='K') array([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.], dtype=float32) This is because our contiguous dimension is the first one in fortran order (ie in x.shape == (4,3), the first dimension, 4, is our contiguous dimension in fortran order, and the last dimension is our contiguous dimension in C order) Your example should work minus some bugs (like id being used but never declared, assume you mean idx?)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am having an issue with kernel launch failure because too many resources are requested. I understand the error and that I can reduce my block size to avoid it but I am trying to get around that. I am working with a Nvidia Tesla K40c GPU. I am using pycuda to solve a system of PDEs. So, my goal is to do some local computation with each thread and then write into a shared memory array. I am fairly new to GPU computing but what I do know of the problem at hand is written below. This issue has to do with the commented out line of code in the snip below. I know that shared memory is ideal for inter-thread communication in a block and my shared memory works correctly until I try to write into it from a local variable which I am assuming is stored in registers. I am assuming this because I read that arrays less than a particular size, 16 floats if I remember correctly, MAY be stored in registers. Mine are of size 4. This is the goal anyways to avoid storing in global. __device__ void step(float *shared_state, int idx) { float dfdxy[NVC]={0}; get_dfdx(dfdxy,shared_state,idx); get_dfdy(dfdxy,shared_state,idx); __syncthreads(); //shared_state[idx+0*SGIDS] += dfdxy[0]; } Here is the trace. As I mentioned, I am familiar with the error. Traceback (most recent call last): File \"./src/sweep/sweep.py\", line 325, in &lt;module&gt; sweep(arr0,targs,order,block_size,euler.step,0) File \"./src/sweep/sweep.py\", line 109, in sweep gpu_speed(arr, source_mod, cpu_fcn, block_size,ops,num_tries=20) File \"./src/sweep/sweep.py\", line 175, in gpu_speed gpu_fcn(arr_gpu,grid=grid_size, block=block_size,shared=shared_size) File \"/home/walkanth/.conda/envs/pysweep/lib/python3.6/site-packages/pycuda/driver.py\", line 402, in function_call func._launch_kernel(grid, block, arg_buf, shared, None) pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch The issue is specifically this, when I run the code with the line commented. It says that I am using 32 registers. This is good, everything works because I am below the limit of 63. However, when I uncomment the line, the number of registers used jumps up to 70 and I suspect this is why the kernel launch fails. So, a couple questions. First, can anyone explain why this is happening? I have been searching for sometime and have fallen short. Second, if there is not a way around this. Does anyone know of some tips to reduce my register usage aside from decreasing the block_size? I have seen some older threads on nvidia dev talk about this but they seem to be pretty dated. Edit: So thanks to Michael on this post, I found out that the GPU I have actually has 255 registers per thread. So, the registers are not the problem. However, that makes me unsure of where the issue is coming from though. I thought it also beneficial to include that I am not using any specific compiler options. I tried -ptxas at one point but it did not change much. I don't want to reduce the blocksize block size because the number of calculations that I can make prior to needing outside information is dependent on the blocksize's minimum dimension (x or y). The larger the blocksize, the more calculations possible. Edit: So, to my understanding, I am still exceeding the total number of registers per SM which is cause for the issue. I need to reduce the registers used or the block size.",
        "answers": [
            [
                "The compiler is going to try to optimize the number of register instructions automatically; if you've written code that ultimately isn't storing information anywhere outside of a thread, then those instructions simply shouldn't be generated. That's probably why you're seeing a large change in the number of registers when you uncomment the line that writes to shared memory. However, according to https://developer.nvidia.com/cuda-gpus, the K40c is compute capability 3.5, and according to https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities, devices that are compute capability 3.5 can have up to 255 registers per thread, not 63. Thus, if you still only use 70 registers per thread then this probably isn't the issue. This is confirmed if you no longer get the error by reducing the block size; a reduction in block size reduces the number of threads in the block but shouldn't alter how many registers are used per thread, so it shouldn't fix your problem if you were actually running out of registers per thread. Without further knowledge of your compiler options, the rest of your kernel, and how you're launching it, we can't easily ascertain what the resource problem is. There are also limits on the number of registers per block and the number of registers per multiprocessor; if reducing the block size fixes the problem then it's probable that you're exceeding these thresholds... and need to reduce the block size. It's unclear why you don't want to reduce your block size, but it seems like you're just running up against a hardware limitation."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to run a simple pycuda program to update a list on the gpu. Following is my list. dm_count = [[0], [1, 2], [3, 4, 5], [6, 7, 8, 9]]. I have this list as the input and expected to update the input list in parallel. It throws an exception when I tries to allocate memory in the gpu using mem_alloc(). It give the Attribute Error saying the \"'list' object has no attribute 'nbytes'\". When I search for answers some says to convert the list in the form of an array and nbytes otherwise cannot be applied. It seems to support for the arrays in format [[1,1],[1,1],[2,4]] only. But I doesn't want to change the list. What is the way to allocate memory in gpu while keeping the list in its original format? I am not aware whether the memcpy_dtoh() also work correctly. How can I correct this program to yield the expected outcome? import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy dm_count = [[0], [1, 2], [3, 4, 5], [6, 7, 8, 9]] length = len(dm_count) mod = SourceModule(\"\"\" __global__ void UpdateMatrix(int **dm_count, int length) { int row = threadIdx.x + blockIdx.x*blockDim.x; int col = threadIdx.y + blockIdx.y*blockDim.y; if( (row &lt; length) &amp;&amp; (col&lt; row)){ dm_count[row][col] = 0 ; } } \"\"\") dm_gpu = cuda.mem_alloc(dm_count.nbytes) cuda.memcpy_htod(dm_gpu, dm_count) func = mod.get_function(\"updateMatrix\") func(dm_gpu, block=(length, length, 1)) result = numpy.empty_like(dm_count) cuda.memcpy_dtoh(result, dm_gpu) print(result) Expected Result: result = [[0], [0, 2], [0, 0, 5], [0, 0, 0, 9]] Error Message: Traceback (most recent call last): File \"test_pycuda.py\", line 55, in dm_gpu = cuda.mem_alloc(dm_count.nbytes) AttributeError: 'list' object has no attribute 'nbytes'",
        "answers": [
            [
                "I want to run a simple pycuda program to update a list on the gpu It is not possible to manipulate a python list in PyCUDA. In general, PyCUDA can only deal with numpy arrays with a limited set of dtypes, and similar types which support the Python buffer protocol. As a result, you could potentially re-write your code to use a numpy array of a suitable dtype as input to the kernel, although you would have to devise a representation of the jagged array which would be compatible with a contiguous numpy array. You would then need to write the CUDA kernel to use the format you devise (Note your current kernel is broken in a number of ways which mean that is would not work even if the list was accepted as an inout by PyCUDA)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Pycuda's gpuarray.dot() operation does not do the same thing as numpy.dot() operation. Is this on purpose? For example, the code below does a numpy.dot() then gpuarray.dot(). The former returns a 5x5 array and the latter a single number. import numpy as np import pycuda.autoinit import pycuda.gpuarray as gpuarray np.random.seed(1) print (\"\\nNUMPY: result of np.dot - OK\") a = np.array(2 * np.random.random((5, 5)) - 1) b = np.array(2 * np.random.random((5, 5)) - 1) a_b_dot = np.dot(a, b) print (type(a_b_dot), a_b_dot.shape) print (a_b_dot) print (\"\\nPYCUDA: result of gpuarray.dot - NOT OK\") a_gpu = gpuarray.to_gpu(a) b_gpu = gpuarray.to_gpu(b) a_b_dot = gpuarray.dot(a_gpu, b_gpu) print (type(a_b_dot), a_b_dot.shape) print (a_b_dot) The output is: NUMPY: result of np.dot - OK &lt;class 'numpy.ndarray'&gt; (5, 5) [[-0.4289689 -1.07826831 0.35264673 1.17316284 0.37989478] [-0.23539466 0.62140658 0.02890465 0.64194572 -0.90554719] [ 0.6308665 -0.5418927 0.15072667 1.53949101 -0.17648109] [-0.28165967 -1.06345895 0.17784186 -0.50902276 1.27061422] [ 0.15769648 0.01993701 -0.42621895 -0.07254009 -0.23463897]] PYCUDA: result of gpuarray.dot - NOT OK &lt;class 'pycuda.gpuarray.GPUArray'&gt; () -0.3611777016515303",
        "answers": [
            [
                "I'm pretty sure the reason why pycuda does this is because it would require pycuda to either have a dependency on cusparse/cublas (so it doesn't have to re-implement this stuff). The simplest way of extending this is to apply dot product across the entire thing and let the end user go and find their own matrix multiply library if they need something more advanced. If you actually want a dot product on matrix this way, you just use a matrix multiply, see the following example for proof: import numpy as np print(\"\\nNUMPY: result of np.dot - OK\") a = np.array(2 * np.random.random((5, 5)) - 1) b = np.array(2 * np.random.random((5, 5)) - 1) a_b_dot = np.dot(a, b) a_mul_b = np.matmul(a, b) print(type(a_b_dot), a_b_dot.shape) print(a_b_dot) print(type(a_mul_b), a_mul_b.shape) print(a_mul_b) NUMPY: result of np.dot - OK &lt;class 'numpy.ndarray'&gt; (5, 5) [[-0.12441477 -0.28175903 0.36632673 0.35687491 -0.25773564] [-0.57845471 -0.4097741 0.3505651 -0.23822489 1.17375904] [-0.19920533 -0.43918224 0.62438656 0.6326451 -0.27798801] [ 0.67128494 0.44472894 -0.57700879 -0.57246653 -0.0336262 ] [ 0.49149948 -0.65774616 1.09320886 0.76179777 -0.76590202]] &lt;class 'numpy.ndarray'&gt; (5, 5) [[-0.12441477 -0.28175903 0.36632673 0.35687491 -0.25773564] [-0.57845471 -0.4097741 0.3505651 -0.23822489 1.17375904] [-0.19920533 -0.43918224 0.62438656 0.6326451 -0.27798801] [ 0.67128494 0.44472894 -0.57700879 -0.57246653 -0.0336262 ] [ 0.49149948 -0.65774616 1.09320886 0.76179777 -0.76590202]] To do true matrix multiply you will either A: need to implement your own, or B: use scikit cuda (which both depends on pycuda and interops with it). In scikit cuda this is pretty much the same as numpy (ripped straight from scikit cuda docs) &gt;&gt;&gt; import pycuda.autoinit &gt;&gt;&gt; import pycuda.gpuarray as gpuarray &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; import skcuda.linalg as linalg &gt;&gt;&gt; import skcuda.misc as misc &gt;&gt;&gt; linalg.init() &gt;&gt;&gt; a = np.asarray(np.random.rand(4, 2), np.float32) &gt;&gt;&gt; b = np.asarray(np.random.rand(2, 2), np.float32) &gt;&gt;&gt; a_gpu = gpuarray.to_gpu(a) &gt;&gt;&gt; b_gpu = gpuarray.to_gpu(b) &gt;&gt;&gt; c_gpu = linalg.dot(a_gpu, b_gpu) &gt;&gt;&gt; np.allclose(np.dot(a, b), c_gpu.get()) True &gt;&gt;&gt; d = np.asarray(np.random.rand(5), np.float32) &gt;&gt;&gt; e = np.asarray(np.random.rand(5), np.float32) &gt;&gt;&gt; d_gpu = gpuarray.to_gpu(d) &gt;&gt;&gt; e_gpu = gpuarray.to_gpu(e) &gt;&gt;&gt; f = linalg.dot(d_gpu, e_gpu) &gt;&gt;&gt; np.allclose(np.dot(d, e), f) True Under the hood with scipy, you are using a cuda dll back end that converts things into ctypes and such, and you'll notice a lot lower level primitives for multiplication than numpy (sticking with 2 dimensions in most cases). If you do happen to need to use multiply matrix multiplies in a nd matrix, they will still be 2d, but you can do them in batch with your backends batch function, or mdot"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I was trying to test the output of an fft against a numpy fft for unit testing, I realized soon after when it failed, it wasn't because I had done something wrong, but skcuda literally doesn't produce the same answer. I knew they were going to be different by a bit, but at least one of the numbers is several orders of magnitude off of what numpy produces, and both allclose and almost_equal return massive errors (33% and 25% for rtol=1e-6, 16% for atol=1e-6). What am I doing wrong here? Can I fix this? Test file: import pycuda.autoinit from skcuda import fft from pycuda import gpuarray import numpy as np def test_skcuda(): array_0 = np.array([[1, 2, 3, 4, 5, 4, 3, 2, 1, 0]], dtype=np.float32) array_1 = array_0 * 10 time_domain_signal = np.array([array_0[0], array_1[0]], dtype=np.float32) fft_point_count = 10 fft_plan = fft.Plan(fft_point_count, np.float32, np.complex64, batch=2) fft_reserved = gpuarray.empty((2, fft_point_count // 2 + 1), dtype=np.complex64) fft.fft(gpuarray.to_gpu(time_domain_signal), fft_reserved, fft_plan) np.testing.assert_array_almost_equal( np.fft.rfft(time_domain_signal, fft_point_count), fft_reserved.get()) test_skcuda() Assertion failure: AssertionError: Arrays are not almost equal to 6 decimals (mismatch 25.0%) x: array([[ 2.500000e+01+0.000000e+00j, -8.472136e+00-6.155367e+00j, -1.193490e-15+2.331468e-15j, 4.721360e-01-1.453085e+00j, 2.664535e-15+0.000000e+00j, 1.000000e+00+0.000000e+00j],... y: array([[ 2.500000e+01+0.000000e+00j, -8.472136e+00-6.155367e+00j, 8.940697e-08+5.960464e-08j, 4.721359e-01-1.453085e+00j, 0.000000e+00+0.000000e+00j, 1.000000e+00+0.000000e+00j],... printed output: #numpy [[ 2.50000000e+01+0.00000000e+00j -8.47213595e+00-6.15536707e+00j -1.19348975e-15+2.33146835e-15j 4.72135955e-01-1.45308506e+00j 2.66453526e-15+0.00000000e+00j 1.00000000e+00+0.00000000e+00j] [ 2.50000000e+02+0.00000000e+00j -8.47213595e+01-6.15536707e+01j -1.11022302e-14+2.39808173e-14j 4.72135955e+00-1.45308506e+01j 3.55271368e-14+7.10542736e-15j 1.00000000e+01+0.00000000e+00j]] #skcuda [[ 2.5000000e+01+0.0000000e+00j -8.4721355e+00-6.1553669e+00j 8.9406967e-08+5.9604645e-08j 4.7213593e-01-1.4530852e+00j 0.0000000e+00+0.0000000e+00j 1.0000000e+00+0.0000000e+00j] [ 2.5000000e+02+0.0000000e+00j -8.4721359e+01-6.1553673e+01j 1.4305115e-06-4.7683716e-07j 4.7213597e+00-1.4530851e+01j 0.0000000e+00+1.9073486e-06j 1.0000000e+01+0.0000000e+00j]]",
        "answers": [
            [
                "The output of an FFT has an error that is relative to the magnitude of the input values. Each output element is computed from combining all input elements, and therefore it is their magnitudes that determine the precision of the result. You are computing two 1D FFTs in the same array. They each have different magnitude inputs, and therefore should have different magnitude tolerances. The following quick code demonstrates how you could implement this. I don't know how to tweak any of the functions in numpy.testing to do this. import numpy as np array_0 = np.array([[1, 2, 3, 4, 5, 4, 3, 2, 1, 0]], dtype=np.float32) array_1 = array_0 * 10 time_domain_signal = np.array([array_0[0], array_1[0]], dtype=np.float32) # numpy result a=np.array([[ 2.50000000e+01+0.00000000e+00j, -8.47213595e+00-6.15536707e+00j, -1.19348975e-15+2.33146835e-15j, 4.72135955e-01-1.45308506e+00j, 2.66453526e-15+0.00000000e+00j, 1.00000000e+00+0.00000000e+00j], [ 2.50000000e+02+0.00000000e+00j, -8.47213595e+01-6.15536707e+01j, -1.11022302e-14+2.39808173e-14j, 4.72135955e+00-1.45308506e+01j, 3.55271368e-14+7.10542736e-15j, 1.00000000e+01+0.00000000e+00j]]) # skcuda result b=np.array([[ 2.5000000e+01+0.0000000e+00j, -8.4721355e+00-6.1553669e+00j, 8.9406967e-08+5.9604645e-08j, 4.7213593e-01-1.4530852e+00j, 0.0000000e+00+0.0000000e+00j, 1.0000000e+00+0.0000000e+00j], [ 2.5000000e+02+0.0000000e+00j, -8.4721359e+01-6.1553673e+01j, 1.4305115e-06-4.7683716e-07j, 4.7213597e+00-1.4530851e+01j, 0.0000000e+00+1.9073486e-06j, 1.0000000e+01+0.0000000e+00j]]) # Tolerance for result array row relative to the mean absolute input values # 1e-6 because we're using single-precision floats tol = np.mean(np.abs(time_domain_signal), axis=1) * 1e-6 # Compute absolute difference and compare that to our tolearances diff = np.abs(a-b) if np.any(diff &gt; tol[:,None]): print('ERROR!!!')"
            ],
            [
                "this looks a lot like rounding errors, single-precision floats have ~8 decimal digits of precision (doubles have ~16) instead of using numpy.fft an alternative would be to use fftpack from scipy which supports single precision floats directly, e.g: from scipy import fftpack x = np.array([1, 2, 3, 4, 5, 4, 3, 2, 1, 0]) y = fftpack.fft( np.array([x, x * 10], dtype=np.float32) ) print(y[:,:6]) outputting: [[ 2.5000000e+01+0.0000000e+00j -8.4721355e+00-6.1553669e+00j 8.9406967e-08+5.9604645e-08j 4.7213593e-01-1.4530852e+00j 0.0000000e+00+0.0000000e+00j 1.0000000e+00+0.0000000e+00j] [ 2.5000000e+02+0.0000000e+00j -8.4721359e+01-6.1553673e+01j 1.1920929e-06+1.9073486e-06j 4.7213583e+00-1.4530851e+01j 0.0000000e+00+1.9073486e-06j 1.0000000e+01+0.0000000e+00j]] which looks to be much closer"
            ],
            [
                "Tiny result values of e-8 (float) and e-15 (double) from an FFT (given full scale input or output of anywhere near 1.0) are essentially equal to zero (plus numerical rounding noise). and zero + noise == zero + noise so your results might actually be the same."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I have tried to implement Element-wise multiplication of two numpy arrays by making similar GPU arrays and performing the operations. However, the resulting execution time is much slower than the original numpy pointwise multiplication. I was hoping to get a good speedup using the GPU. zz0 is complex128 type, (64,256,16) shape numpy array and xx0 is float64 type,(16,151) shape numpy array. Can someone please help me figure out what I am doing wrong with respect to the implementation: import sys import numpy as np import matplotlib.pyplot as plt import pdb import time import pycuda.driver as drv import pycuda.autoinit from pycuda.compiler import SourceModule from pycuda.elementwise import ElementwiseKernel import pycuda.gpuarray as gpuarray import pycuda.cumath import skcuda.linalg as linalg linalg.init() # Function for doing a point-wise multiplication using GPU def calc_Hyp(zz,xx): zz_stretch = np.tile(zz, (1,1,1,xx.shape[3])) xx_stretch = np.tile(xx, (zz.shape[0],zz.shape[1],1,1)) zzg = gpuarray.to_gpu(zz_stretch) xxg = gpuarray.to_gpu(xx_stretch) zz_Hypg = linalg.multiply(zzg,xxg) zz_Hyp = zz_Hypg.get() return zz_Hyp zz0 = np.random.uniform(10.0/5000, 20000.0/5000, (64,256,16)).astype('complex128') xx0 = np.random.uniform(10.0/5000, 20000.0/5000, (16,151)).astype('float64') xx0_exp = np.exp(-1j*xx0) t1 = time.time() #Using GPU for the calculation zz0_Hyp = calc_Hyp(zz0[:,:,:,None],xx0_exp[None,None,:,:]) #np.save('zz0_Hyp',zz0_Hyp) t2 = time.time() print('Time taken with GPU:{}'.format(t2-t1)) #Original calculation zz0_Hyp_actual = zz0[:,:,:,None]*xx0_exp[None,None,:,:] #np.save('zz0_Hyp_actual',zz0_Hyp_actual) t3 = time.time() print('Time taken without GPU:{}'.format(t3-t2))",
        "answers": [
            [
                "The first issue is that your timing metrics are not accurate. Linalg compiles cuda modules on the fly, and you may see code being compiles as you run it. I made some slight modifications to your code to reduce the size of the arrays being multiplied, but regardless, after two runs with no other improvements I saw massive gains in performance ex: Time taken with GPU:2.5476348400115967 Time taken without GPU:0.16627931594848633 vs Time taken with GPU:0.8741757869720459 Time taken without GPU:0.15836167335510254 However that is still much slower than the CPU version. The next thing I did was give a more accurate timing based upon where the actual computation is happening. You aren't tiling in your numpy version, so don't time it in your cuda version: REAL Time taken with GPU:0.6461708545684814 You also copy to the GPU, and include that in the calculation, but that in itself takes a non trivial amount of time, so lets remove that: t1 = time.time() zz_Hypg = linalg.multiply(zzg,xxg) t2 = time.time() ... REAL Time taken with GPU:0.3689603805541992 Wow, that contributed a lot. But we still are slower than the numpy version? Why? Remember when I said that numpy doesn't tile? It doesn't copy memory at all for broad casting. To get the real speed, you would have to: not Tile broadcast dimensions implement this in a kernel. Pycuda provides the utilities for kernel implementation, but its GPU array does not provide broadcasting. Essentially what you would have to do is this (DISCLAIMER: I haven't tested this, there are probably bugs, this is just to demonstrate approximately what the kernel should look like): #include &lt;pycuda-complex.hpp&gt; //KERNEL CODE constexpr unsigned work_tile_dim = 32 //instruction level parallelism factor, how much extra work to do per thread, may be changed but effects the launch dimensions. thread group size should be (tile_factor, tile_factor/ilp_factor) constexpr unsigned ilp_factor = 4 //assuming c order: // x axis contiguous out, // y axis contiguous in zz, // x axis contiguous in xx // using restrict because we know that all pointers will refer to different parts of memory. __global__ void element_wise_multiplication( pycuda::complex&lt;double&gt;* __restrict__ array_zz, pycuda::complex&lt;double&gt;* __restrict__ array_xx, pycuda::complex&lt;double&gt;* __restrict__ out_array, unsigned array_zz_w, /*size of w,z,y, dimensions used in zz*/ unsigned array_zz_z, unsigned array_zz_xx_y,/*size of y,x, dimensions used in xx, but both have same y*/ unsigned array_xx_x){ // z dimensions in blocks often have restrictions on size that can be fairly small, and sometimes can cause performance issues on older cards, we are going to derive x,y,z,w index from just the x and y indicies instead. unsigned x_idx = blockIdx.x * (work_tile_dim) + threadIdx.x unsigned y_idx = blockIdx.y * (work_tile_dim) + threadIdx.y //blockIdx.z stores both z and w and should not over shoot, and aren't used //shown for the sake of how to get these dimensions. unsigned z_idx = blockIdx.z % array_zz_z; unsigned w_idx = blockIdx.z / array_zz_z; //we already know this part of the indexing calculation. unsigned out_idx_zw = blockIdx.z * (array_zz_xx_y * array_xx_z); // since our input array is actually 3D, this is a different calcualation unsigned array_zz_zw = blockIdx.z * (array_zz_xx_y) //ensures if our launch dimensions don't exactly match our input size, we don't //accidently access out of bound memory, while branching can be bad, this isn't // because 99.999% of the time no branch will occur and our instruction pointer //will be the same per warp, meaning virtually zero cost. if(x_idx &lt; array_xx_x){ //moving over y axis to coalesce memory accesses in the x dimension per warp. for(int i = 0; i &lt; ilp_factor; ++i){ //need to also check y, these checks are virtually cost-less // because memory access dominates time in such simple calculations, // and arithmetic will be hidden by overlapping execution if((y_idx+i) &lt; array_zz_xx_y){ //splitting up calculation for simplicity sake out_array_idx = out_idx_zw+(y_idx+i)*array_xx_x + x_idx; array_zz_idx = array_zz_zw + (y_idx+i); array_xx_idx = ((y_idx+i) * array_xx_x) + x_idx; //actual final output. out_array[out_array_idx] = array_zz[array_zz_idx] * array_xx[array_xx_idx]; } } } } You will have to make the launch dimensions something like: thread_dim = (work_tile_dim, work_tile_dim/ilp_factor) # (32,8) y_dim = xx0.shape[0] x_dim = xx0.shape[1] wz_dim = zz0.shape[0] * zz0.shape[1] block_dim = (x_dim/work_tile_dim, y_dim/work_tile_dim, wz_dim) And there are several further optimizations you may be able to take advantage of: store global memory accesses in work tile in shared memory inside of kernel, this ensures that accesses to zz0s \"y\", but really x dimension are coallesced when put into shared memory, increasing performance, then accessed from shared memory (where coalescing doesn't matter, but bank conflicts do). See here on how to deal with that kind of bank conflict. instead of calculating eulers formula and expanding a double into a complex double, expand it inside of the kernel itself, use sincos(-x, &amp;out_sin, &amp;out_cos) to achieve the same result, but utilizing way less memory bandwidth (see here). But note, even doing this will likely not give you the performance you want (though will still likely be faster) unless you are on a higher end GPU with full double precision units, which aren't on most GPUs (most of the time it is emulated). Double precision floating point units take up a lot of space, and since gpus are used for graphics, they don't have much use for double precision. If you want higher precision than floating point, but want to take advantage of floating point hardware with out a 1/8 to 1/32 throughput hit of double, you can use the techniques used in this answer to achieve this on the gpu, getting you closer to 1/2 to 1/3 throughput."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "This is an extension of the discussion here: pycuda shared memory error \"pycuda._driver.LogicError: cuLaunchKernel failed: invalid value\" Is there a method in pycuda that is equivalent to the following C++ API call? #define SHARED_SIZE 0x18000 // 96 kbyte cudaFuncSetAttribute(func, cudaFuncAttributeMaxDynamicSharedMemorySize, SHARED_SIZE) Working on a recent GPU (Nvidia V100), going beyond 48 kbyte shared memory requires this function attribute be set. Without it, one gets the same launch error as in the topic above. The \"hard\" limit on the device is 96 kbyte shared memory (leaving 32 kbyte for L1 cache). There's a deprecated method Fuction.set_shared_size(bytes) that sounds promising, but I can't find what it's supposed to be replaced by.",
        "answers": [
            [
                "PyCUDA uses the driver API, and the corresponding function call for setting a function dynamic memory limits is cuFuncSetAttribute. I can't find that anywhere in the current PyCUDA tree, and therefore suspect that it has not been implemented."
            ],
            [
                "I'm not sure if this is what you're looking for, but this might help someone looking in this direction. The dynamic shared memory size in PyCUDA can be set either using: shared argument in the direct kernel call (the \"unprepared call\"). For example: myFunc(arg1, arg2, shared=numBytes, block=(1,1,1), grid=(1,1)) shared_size argument in the prepared kernel call. For example: myFunc.prepared_call(grid, block, arg1, arg2, shared_size=numBytes) where numBytes is the amount of memory in bytes you wish to allocate at runtime."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I'm studying the spread of an invasive species and am trying to generate random numbers within a PyCUDA kernel using the XORWOW random number generator. The matrices I need to be able to use as input in the study are quite large (up to 8,000 x 8,000). The error seems to occur inside get_random_number when indexing the curandState* of the XORWOW generator. The code executes without errors on smaller matrices and produces correct results. I'm running my code on 2 NVidia Tesla K20X GPUs. Kernel code and setup: kernel_code = ''' #include &lt;curand_kernel.h&gt; #include &lt;math.h&gt; extern \"C\" { __device__ float get_random_number(curandState* global_state, int thread_id) { curandState local_state = global_state[thread_id]; float num = curand_uniform(&amp;local_state); global_state[thread_id] = local_state; return num; } __global__ void survival_of_the_fittest(float* grid_a, float* grid_b, curandState* global_state, int grid_size, float* survival_probabilities) { int x = threadIdx.x + blockIdx.x * blockDim.x; // column index of cell int y = threadIdx.y + blockIdx.y * blockDim.y; // row index of cell // make sure this cell is within bounds of grid if (x &lt; grid_size &amp;&amp; y &lt; grid_size) { int thread_id = y * grid_size + x; // thread index grid_b[thread_id] = grid_a[thread_id]; // copy current cell float num; // ignore cell if it is not already populated if (grid_a[thread_id] &gt; 0.0) { num = get_random_number(global_state, thread_id); // agents in this cell die if (num &lt; survival_probabilities[thread_id]) { grid_b[thread_id] = 0.0; // cell dies //printf(\"Cell (%d,%d) died (probability of death was %f)\\\\n\", x, y, survival_probabilities[thread_id]); } } } } mod = SourceModule(kernel_code, no_extern_c = True) survival = mod.get_function('survival_of_the_fittest') Data setup: matrix_size = 2000 block_dims = 32 grid_dims = (matrix_size + block_dims - 1) // block_dims grid_a = gpuarray.to_gpu(np.ones((matrix_size,matrix_size)).astype(np.float32)) grid_b = gpuarray.to_gpu(np.zeros((matrix_size,matrix_size)).astype(np.float32)) generator = curandom.XORWOWRandomNumberGenerator() grid_size = np.int32(matrix_size) survival_probabilities = gpuarray.to_gpu(np.random.uniform(0,1,(matrix_size,matrix_size))) Kernel call: survival(grid_a, grid_b, generator.state, grid_size, survival_probabilities, grid = (grid_dims, grid_dims), block = (block_dims, block_dims, 1)) I expect to be able to generate random numbers within the range (0,1] for matrices up to (8,000 x 8,000), but executing my code on large matrices leads to an illegal memory access error. pycuda._driver.LogicError: cuMemcpyDtoH failed: an illegal memory access was encountered PyCUDA WARNING: a clean-up operation failed (dead context maybe?) cuMemFree failed: an illegal memory access was encountered Am I indexing the curandState* incorrectly in get_random_number? And if not, what else might be causing this error?",
        "answers": [
            [
                "The problem here is a disconnect between this code which determines the size of the state which the PyCUDA curandom interface allocates for its internal state and this code in your post: matrix_size = 2000 block_dims = 32 grid_dims = (matrix_size + block_dims - 1) // block_dims You seem to be assuming that PyCUDA will magically allocate enough state for whatever block and grid dimension you select in you code. That is obviously unlikely, particularly at large grid sizes. You either need to Modify your code to use the same block and grid sizes as the curandom module uses internally for whichever generator you choose to use, or Allocate and manage your own state scratch space so that you have enough state allocated to service the block and grid sizes you select I leave it as an exercise to the reader as to which one of these two approaches will work better in your application."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Getting the nvcc fatal : '--ptxas-options=-v': expected a number error when I try to build a Windows port of Faster-RCNN. You may reach the setup file (which is a Python script) directly from here. Software Environment: - CUDA v10.1 - VS 2019 - Python 3.7 - Windows 10",
        "answers": [
            [
                "This configuration line is no longer correct with CUDA 10.1: nvcc_compile_args = ['-O', '--ptxas-options=-v', '-arch=sm_35', '-c', '--compiler-options=-fPIC'] That will generate a nvcc compile command that looks like this: nvcc -O ... With CUDA 10.0 and prior, such a command was legal. With CUDA 10.1 it is not. This switch passes the optimization level for host code, so barring any reason not to, I would recommend passing -O3 here: nvcc_compile_args = ['-O3', '--ptxas-options=-v', '-arch=sm_35', '-c', '--compiler-options=-fPIC'] The relevant doc link is here"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I used TensorRT in python code. So I use PyCUDA. In the following inference code, there is an illegal memory access was encountered happened at stream.synchronize(). def infer(engine, x, batch_size, context): inputs = [] outputs = [] bindings = [] stream = cuda.Stream() for binding in engine: size = trt.volume(engine.get_binding_shape(binding)) * batch_size dtype = trt.nptype(engine.get_binding_dtype(binding)) # Allocate host and device buffers host_mem = cuda.pagelocked_empty(size, dtype) device_mem = cuda.mem_alloc(host_mem.nbytes) # Append the device buffer to device bindings. bindings.append(int(device_mem)) # Append to the appropriate list. if engine.binding_is_input(binding): inputs.append(HostDeviceMem(host_mem, device_mem)) else: outputs.append(HostDeviceMem(host_mem, device_mem)) img = np.array(x).ravel() np.copyto(inputs[0].host, 1.0 - img / 255.0) [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs] context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle) # Transfer predictions back from the GPU. [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs] # Synchronize the stream stream.synchronize() # Return only the host outputs. return [out.host for out in outputs] What could be wrong? EDIT: My program is combination of Tensorflow and TensorRT codes. The error happened only when I run self.graph = tf.get_default_graph() self.persistent_sess = tf.Session(graph=self.graph, config=tf_config) before running infer(). If I don't run the above two lines, I have no issue.",
        "answers": [
            [
                "The issue here is I have two python codes. Say tensorrtcode.py and tensorflowcode.py. tensorrtcode.py has only tensorrt codes. def infer(engine, x, batch_size, context): inputs = [] outputs = [] bindings = [] stream = cuda.Stream() for binding in engine: size = trt.volume(engine.get_binding_shape(binding)) * batch_size dtype = trt.nptype(engine.get_binding_dtype(binding)) # Allocate host and device buffers host_mem = cuda.pagelocked_empty(size, dtype) device_mem = cuda.mem_alloc(host_mem.nbytes) # Append the device buffer to device bindings. bindings.append(int(device_mem)) # Append to the appropriate list. if engine.binding_is_input(binding): inputs.append(HostDeviceMem(host_mem, device_mem)) else: outputs.append(HostDeviceMem(host_mem, device_mem)) img = np.array(x).ravel() np.copyto(inputs[0].host, 1.0 - img / 255.0) [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs] context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle) # Transfer predictions back from the GPU. [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs] # Synchronize the stream stream.synchronize() # Return only the host outputs. return [out.host for out in outputs] def main(): ..... infer(......) ..... Then tensorflowcode.py has only tensorflow apis and execute with session. self.graph = tf.get_default_graph() self.persistent_sess = tf.Session(graph=self.graph, config=tf_config) Problem is when I need to interface class from tensorflow to tensorrt class, declare the tensorflow code's class instance inside tensorrt's main as def main(): ..... t_flow_code=tensorflowclass() infer(......) ..... then I have error as illegal memory access was encountered happened at stream.synchronize() The problem is solved by adding another session at tensorrt just before t_flow_code=tensorflowclass(). I don't understand why I need as I have it own session for execution at tensorflow class. Why I need another session before class interface in tensorrt code."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Hi I am attempting to a simple 1D-FFT transform on a signal. This is what I tried: import numpy as np from scipy.signal import hilbert, chirp duration = 1.0 fs = 400.0 samples = int(fs*duration) t = np.arange(samples) / fs signal = chirp(t, 20.0, t[-1], 100.0) import pycuda.autoinit from pycuda import gpuarray import numpy as np from skcuda import fft def fft_gpu1(signal): x_gpu = gpuarray.to_gpu(signal) x_hat = gpuarray.empty_like(x_gpu, dtype=np.complex64) plan = fft.Plan(x_gpu.shape,np.float32,np.complex64) fft.fft(x_gpu, x_hat, plan) return x_hat.get() print(fft_gpu1(signal)) However, the result I get is: [nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj nan+nanj ... 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j 0. +0.j] Any insight on why im getting this issue would be greatly appreciated",
        "answers": [
            [
                "The signal should be of type float32, not float64: x_gpu = gpuarray.to_gpu(signal.astype(np.float32))"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am new to TensorRT and CUDA and I am trying to implement an inference server using TensorRT Python API. I follow the end_to_end_tensorflow_mnist and uff_ssd example and everything works ok. However, when I try to use the engine to make inference in multiple threads, I encounter some problems. So I was wondering what is the right way to run TensorRT in multiple threads. Here's what i have tried. First I create inference engine in the main thread. In the worker thread, I allocate memory space, CUDA Stream and execution context using the engine created in the main thread and make inference: import pycuda.autoinit # Create CUDA context import pycuda.driver as cuda # Main thread with open(\u201csample.engine\u201d, \u201crb\u201d) as f, trt.Runtime(TRT_LOGGER) as runtime: engine = runtime.deserialize_cuda_engine(f.read()) ... # Worker thread with engine.create_execution_context() as context: inputs, outputs, bindings, stream = common.allocate_buffers(engine) common.do_inference(context, inputs, outputs, bindings, stream) The above code produce the following error: pycuda._driver.LogicError: explicit_context_dependent failed: invalid device context - no currently active context? This sounds like there is no active CUDA context in the worker thread. So, I tried to create CUDA context manually in the worker thread: # Worker thread from pycuda.tools import make_default_context() cuda.init() # Initialize CUDA ctx = make_default_context() # Create CUDA context with engine.create_execution_context() as context: inputs, outputs, bindings, stream = common.allocate_buffers(engine) common.do_inference(context, inputs, outputs, bindings, stream) ctx.pop() # Clean up This time, it gives me another error: [TensorRT] ERROR: cuda/cudaConvolutionLayer.cpp (163) - Cudnn Error in execute: 7 [TensorRT] ERROR: cuda/cudaConvolutionLayer.cpp (163) - Cudnn Error in execute: 7 I understand the builder or runtime will be created with the GPU context associated with the creating thread. I guess this error is because the engine is associated with main thread but I use it in the worker thread, so my question is: Is that means I have to rebuild the engine in worker thread, which significantly increase inference overheat? Can I share GPU context between main thread and worker threads so that I don't have to create a new GPU context for each new request? If so, how to do that in pycuda? Any advice will be appreciated. Thanks!",
        "answers": [
            [
                "Refer to this. You need to modify the common.py as below if you want to do inference in multi-thread. Make context before triggering the GPU task: dev = cuda.Device(0) // 0 is your GPU number ctx = dev.make_context() and clean up after the GPU task: ctx.pop() del ctx"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have install pycuda and I am trying to test it with code below. import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.random.randn(4,4) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") func = mod.get_function(\"doublify\") func(a_gpu, block=(4,4,1)) a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print a_doubled print a I'm getting the following error: pytools.prefork.ExecError: error invoking 'nvcc --version': [Errno 2] No such file or directory",
        "answers": [
            [
                "It's working after adding the below lines in the .bashrc file export PATH=/usr/local/cuda-10.1/bin${PATH:+:${PATH}}$ export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Steps find .bashrc file. Add above lines to it. source .bashrc To Test run command \"nvcc --version\" link: https://askubuntu.com/questions/885610/nvcc-version-command-says-nvcc-is-not-installed helped"
            ],
            [
                "In case this error reported, open the compiler.py file and in the compile_plain() function add the following line: nvcc = '/usr/local/cuda/bin/' + nvcc the compiler.py file is located in: \"/anaconda3/lib/python3.7/site-packages/pycuda-2020.1-py3.7-linux-x86_64.egg/pycuda/compiler.py\" So the final code will be something like this: def compile_plain(source, options, keep, nvcc, cache_dir, target=\"cubin\"): from os.path import join assert target in [\"cubin\", \"ptx\", \"fatbin\"] nvcc = '/usr/local/cuda/bin/' + nvcc # --&gt; here is the new line if cache_dir: checksum = _new_md5() ... Save it and that's all"
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am starting to learn PyCUDA on Google Colab. I\u2019m trying to run the \"printf\" example. Everything works fine, but I do not get any output on the last line. How can I solve it? import pycuda.driver as drv import pycuda.autoinit from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void myfirst_kernel() { printf(\"Hello,PyCUDA!!!\"); } \"\"\") function = mod.get_function(\"myfirst_kernel\") function(block=(4,4,1)) # Flush context printf buffer cuda.Context.synchronize()",
        "answers": [
            [
                "drv.Context.synchronize() just make this change, it will work now."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am learning pycuda, but every time I finish running a programme, something still takes up some memory of GPU, how to clear memory of GPU, or how does pycuda run the gc?",
        "answers": [
            [
                "There is no such thing as garbage collection in PyCUDA. If you want to free the memory used by an allocation, you have to explicitly free it yourself. When a context is destroyed, all of the resources which were consumed by that context are freed. This process occurs at the GPU driver level and is transparent to the programmer."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to use pycuda to accelerate my neural net (I know tensorflow is easier to use for GPU acceleration, I just wanted to do it manually first as I am relatively new to neural networks), but whenever I pass an array to the GPU and have each thread print out the value of the array at the threadIdx, it prints zeros even though I set the array values. I have tried using an extremely simple kernel for testing that just prints the values of a one dimensional array, and I have tried changing the data type to float32. The basic kernel that I'm using for testing of this issue: test_mod = SourceModule(\"\"\" __global__ void test(float *a) { printf(\"%d: %d\\\\n\", threadIdx.x, a[threadIdx.x]); } \"\"\") The python code I'm using to create the array and initialize the kernel: a = np.asarray([4,2,1]) a = a.astype(np.float32) test_module = test_mod.get_function(\"test\") test_module(cuda.In(a), block=(3, 1, 1)) I expect it to print some order of 4, 2, and 1, but each thread prints a 0.",
        "answers": [
            [
                "The problem lies in the print statement within the kernel. The %d format specifier is intended for integers. It will not correctly format a floating point value. To fix it, modify the kernel like this: test_mod = SourceModule(\"\"\" __global__ void test(float *a) { printf(\"%d: %f\\\\n\", threadIdx.x, a[threadIdx.x]); } \"\"\") [Answer assembled from comments and added as a community wiki entry to try and get the question off the unaswered queue for the CUDA tag]."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a pycuda code that can run in a single process. Can python's multiple processes support running this code in multiple subprocesses? If I try, I will find that I made a mistake. Did I make a mistake? I tried to use python's process to implement a simple multi-process and found that it would go wrong. import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule from multiprocessing import Pool, Manager, Process def ffunc(i, return_dict, a, b, multiply_them): dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b), block=(400, 1, 1), grid=(1, 1)) return_dict[i] = dest if __name__ == '__main__': mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") multiply_them = mod.get_function(\"multiply_them\") aa = numpy.random.randn(2, 400).astype(numpy.float32) bb = numpy.random.randn(2, 400).astype(numpy.float32) manager = Manager() return_dict = manager.dict() jobs = [] for i in range(2): p = Process(target=ffunc, args=(i, return_dict, aa[i], bb[i], multiply_them)) jobs.append(p) p.start() for p in jobs: p.join() print(return_dict) Process Process-2: Traceback (most recent call last): File \"/home/vision/anaconda3/envs/py3b/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap self.run() File \"/home/vision/anaconda3/envs/py3b/lib/python3.6/multiprocessing/process.py\", line 93, in run self._target(*self._args, **self._kwargs) File \"/home/vision/lpx/AE23D/test_pycuda.py\", line 22, in ffunc block=(400,1,1), grid=(1,1)) File \"/home/vision/anaconda3/envs/py3b/lib/python3.6/site-packages/pycuda/driver.py\", line 382, in function_call func._set_block_shape(*block) pycuda._driver.LogicError: cuFuncSetBlockShape failed: initialization error Process Process-3: Traceback (most recent call last): File \"/home/vision/anaconda3/envs/py3b/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap self.run() File \"/home/vision/anaconda3/envs/py3b/lib/python3.6/multiprocessing/process.py\", line 93, in run self._target(*self._args, **self._kwargs) File \"/home/vision/lpx/AE23D/test_pycuda.py\", line 22, in ffunc block=(400,1,1), grid=(1,1)) File \"/home/vision/anaconda3/envs/py3b/lib/python3.6/site-packages/pycuda/driver.py\", line 382, in function_call func._set_block_shape(*block) pycuda._driver.LogicError: cuFuncSetBlockShape failed: initialization error {} Process finished with exit code 0 I'm not sure if pycuda can run in different processes. I look forward to your suggestions.",
        "answers": [
            [
                "Fortunately, I solved the problem. Add a line of code to the main function\uff1a multiprocessing.set_start_method('spawn')"
            ],
            [
                "CUDA should not be initialized before a fork. You can find more details here: https://forums.developer.nvidia.com/t/cuda8-0-bug-child-process-forked-after-cuinit-get-cuda-error-not-initialized-on-cuinit/45764 It's better to spawn a new process; hence, multiprocessing.set_start_method('spawn') works perfectly."
            ]
        ],
        "votes": [
            3.0000001,
            2.0000001
        ]
    },
    {
        "question": "Let us consider the CUDA code at CUDA's Mersenne Twister for an arbitrary number of threads and suppose that I want to convert it to a pyCUDA application. I know that I can use ctypes and CDLL, namely, cudart = CDLL(\"/usr/local/cuda/lib64/libcudart.so\") to use the cudart routines. However, I would also need to allocate, for example, a curandStateMtgp32 array whose definition is in curand_mtgp32.h, or else call curandMakeMTGP32Constants(mtgp32dc_params_fast_11213, devKernelParams); and use mtgp32dc_params_fast_11213 whose definition is in curand_mtgp32_host.h. How to deal with CUDA type definitions and values in pyCUDA?",
        "answers": [
            [
                "I solved the problem with reference to device side APIs as follows: I created a .dll containing two functions: MTGP32Setup() to setup the Mersenne Twister Generator and MTGP32Generation() to generate the random numbers; I called the above functions using ctypes. Source code for the .dll // --- Generate random numbers with cuRAND's Mersenne Twister #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;assert.h&gt; #include &lt;time.h&gt; #include &lt;cuda.h&gt; #include &lt;curand_kernel.h&gt; /* include MTGP host helper functions */ #include &lt;curand_mtgp32_host.h&gt; #define BLOCKSIZE 256 #define GRIDSIZE 64 curandStateMtgp32 *devMTGPStates; /********************/ /* CUDA ERROR CHECK */ /********************/ // --- Credit to http://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) { if (code != cudaSuccess) { fprintf(stderr, \"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line); if (abort) { exit(code); } } } void gpuErrchk(cudaError_t ans) { gpuAssert((ans), __FILE__, __LINE__); } /*************************/ /* CURAND ERROR CHECKING */ /*************************/ static const char *_curandGetErrorEnum(curandStatus_t error) { switch (error) { case CURAND_STATUS_SUCCESS: return \"CURAND_SUCCESS\"; case CURAND_STATUS_VERSION_MISMATCH: return \"CURAND_STATUS_VERSION_MISMATCH\"; case CURAND_STATUS_NOT_INITIALIZED: return \"CURAND_STATUS_NOT_INITIALIZED\"; case CURAND_STATUS_ALLOCATION_FAILED: return \"CURAND_STATUS_ALLOCATION_FAILED\"; case CURAND_STATUS_TYPE_ERROR: return \"CURAND_STATUS_TYPE_ERROR\"; case CURAND_STATUS_OUT_OF_RANGE: return \"CURAND_STATUS_OUT_OF_RANGE\"; case CURAND_STATUS_LENGTH_NOT_MULTIPLE: return \"CURAND_STATUS_LENGTH_NOT_MULTIPLE\"; case CURAND_STATUS_DOUBLE_PRECISION_REQUIRED: return \"CURAND_STATUS_DOUBLE_PRECISION_REQUIRED\"; case CURAND_STATUS_LAUNCH_FAILURE: return \"CURAND_STATUS_LAUNCH_FAILURE\"; case CURAND_STATUS_PREEXISTING_FAILURE: return \"CURAND_STATUS_PREEXISTING_FAILURE\"; case CURAND_STATUS_INITIALIZATION_FAILED: return \"CURAND_STATUS_INITIALIZATION_FAILED\"; case CURAND_STATUS_ARCH_MISMATCH: return \"CURAND_STATUS_ARCH_MISMATCH\"; case CURAND_STATUS_INTERNAL_ERROR: return \"CURAND_STATUS_INTERNAL_ERROR\"; } return \"&lt;unknown&gt;\"; } inline void __curandSafeCall(curandStatus_t err, const char *file, const int line) { if (CURAND_STATUS_SUCCESS != err) { fprintf(stderr, \"CURAND error in file '%s', line %d, error: %s \\nterminating!\\n\", __FILE__, __LINE__, \\ _curandGetErrorEnum(err)); \\ assert(0); \\ } } void curandSafeCall(curandStatus_t err) { __curandSafeCall(err, __FILE__, __LINE__); } /*******************/ /* iDivUp FUNCTION */ /*******************/ __host__ __device__ int iDivUp(int a, int b) { return ((a % b) != 0) ? (a / b + 1) : (a / b); } /*********************/ /* GENERATION KERNEL */ /*********************/ __global__ void generate_kernel(curandStateMtgp32 * __restrict__ state, float * __restrict__ result, const int N) { int tid = threadIdx.x + blockIdx.x * blockDim.x; for (int k = tid; k &lt; N; k += blockDim.x * gridDim.x) result[k] = curand_uniform(&amp;state[blockIdx.x]); } extern \"C\" { /**************************/ /* MERSENNE TWISTER SETUP */ /**************************/ __declspec(dllexport) void MTGP32Setup() { // --- Setup the pseudorandom number generator gpuErrchk(cudaMalloc(&amp;devMTGPStates, GRIDSIZE * sizeof(curandStateMtgp32))); mtgp32_kernel_params *devKernelParams; gpuErrchk(cudaMalloc(&amp;devKernelParams, sizeof(mtgp32_kernel_params))); curandSafeCall(curandMakeMTGP32Constants(mtgp32dc_params_fast_11213, devKernelParams)); curandSafeCall(curandMakeMTGP32KernelState(devMTGPStates, mtgp32dc_params_fast_11213, devKernelParams, GRIDSIZE, time(NULL))); } /*******************************/ /* MERSENNE TWISTER GENERATION */ /*******************************/ __declspec(dllexport) void MTGP32Generation(float * __restrict__ devResults, const int N) { // --- Generate pseudo-random sequence and copy to the host generate_kernel &lt;&lt; &lt;GRIDSIZE, BLOCKSIZE &gt;&gt; &gt; (devMTGPStates, devResults, N); gpuErrchk(cudaPeekAtLastError()); gpuErrchk(cudaDeviceSynchronize()); } } // Source code for the PyCUDA caller import os import sys import numpy as np import ctypes from ctypes import * import pycuda.driver as drv import pycuda.gpuarray as gpuarray import pycuda.autoinit lib = cdll.LoadLibrary('D:\\\\Project\\\\cuRAND\\\\mersenneTwisterDLL\\\\x64\\\\Release\\\\mersenneTwisterDLL.dll') N = 10 d_x = gpuarray.zeros((N, 1), dtype = np.float32) lib.MTGP32Setup() lib.MTGP32Generation(ctypes.cast(d_x.ptr, POINTER(c_float)), N) print(d_x) Host side APIs can be dealt with in a way similar to Calling host functions in PyCUDA."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am facing an issue of accuracy about my code which performs a number (128, 256, 512) of 4x4 matrix inversions. When I use the original version, i.e the numpy function np.linalg.inv or np.linalg.pinv, everything works fine. Unfortunately, with the CUDA code below, I get nan and inf values into inverted matrix. To be more explicit, I take this matrix to invert : 2.120771107884677649e+09 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 3.557266600921528288e+27 3.557266600921528041e+07 3.557266600921528320e+17 0.000000000000000000e+00 3.557266600921528041e+07 3.557266600921528288e+27 3.557266600921528041e+07 0.000000000000000000e+00 3.557266600921528320e+17 3.557266600921528041e+07 1.778633300460764144e+27 If I use classical numpy \"inv\", I get for the following inverted 3x3 matrix : 4.715266047722758306e-10 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 2.811147187396482366e-28 -2.811147186834252285e-48 -5.622294374792964645e-38 0.000000000000000000e+00 -2.811147186834252285e-48 2.811147187396482366e-28 -5.622294374230735768e-48 0.000000000000000000e+00 -5.622294374792964645e-38 -5.622294374230735768e-48 5.622294374792964732e-28 To check the validity of this inverse matrix, I have multiplied it by original matrix and the result is the identity matrix. But with CUDA GPU inversion, I get after the inversion this matrix : 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 -inf -inf -9.373764907941219970e-01 -inf inf nan -inf nan So, I woul like to increase the precision into my CUDA kernel or python code to avoid these nanand inf values. Here is the CUDA kernel code and calling part of my main code (I have commented the classical method with numpy inv function : # Create arrayFullCross_vec array arrayFullCross_vec = np.zeros((dimBlocks,dimBlocks,integ_prec,integ_prec)) # Create arrayFullCross_vec array invCrossMatrix_gpu = np.zeros((dimBlocks*dimBlocks*integ_prec**2)) # Create arrayFullCross_vec array invCrossMatrix = np.zeros((dimBlocks,dimBlocks,integ_prec,integ_prec)) # Build observables covariance matrix arrayFullCross_vec = buildObsCovarianceMatrix4_vec(k_ref, mu_ref, ir) \"\"\" # Compute integrand from covariance matrix for r_p in range(integ_prec): for s_p in range(integ_prec): # original version (without GPU) invCrossMatrix[:,:,r_p,s_p] = np.linalg.inv(arrayFullCross_vec[:,:,r_p,s_p]) \"\"\" # GPU version invCrossMatrix_gpu = gpuinv4x4(arrayFullCross_vec.flatten(),integ_prec**2) invCrossMatrix = invCrossMatrix_gpu.reshape(dimBlocks,dimBlocks,integ_prec,integ_prec) \"\"\" and here the CUDA kernel code and gpuinv4x4 function : kernel = SourceModule(\"\"\" __device__ unsigned getoff(unsigned &amp;off){ unsigned ret = off &amp; 0x0F; off = off &gt;&gt; 4; return ret; } const int block_size = 256; const unsigned tmsk = 0xFFFFFFFF; // in-place is acceptable i.e. out == in) // T = double or double only typedef double T; __global__ void inv4x4(const T * __restrict__ in, T * __restrict__ out, const size_t n, const unsigned * __restrict__ pat){ __shared__ T si[block_size]; size_t idx = threadIdx.x+blockDim.x*blockIdx.x; if (idx &lt; n*16){ si[threadIdx.x] = in[idx]; unsigned lane = threadIdx.x &amp; 15; unsigned sibase = threadIdx.x &amp; 0x03F0; __syncwarp(); unsigned off = pat[lane]; T a,b; a = si[sibase + getoff(off)]; a *= si[sibase + getoff(off)]; a *= si[sibase + getoff(off)]; if (!getoff(off)) a = -a; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; off = pat[lane+16]; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; off = pat[lane+32]; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; T det = si[sibase + (lane&gt;&gt;2)]*a; det += __shfl_down_sync(tmsk, det, 4, 16); // first add det += __shfl_down_sync(tmsk, det, 8, 16); // second add det = __shfl_sync(tmsk, det, 0, 16); // broadcast out[idx] = a / det; } } \"\"\") # python function for inverting 4x4 matrices # n should be an even number def gpuinv4x4(inp, n): # internal constants not to be modified hpat = ( 0x0EB51FA5, 0x1EB10FA1, 0x0E711F61, 0x1A710B61, 0x1EB40FA4, 0x0EB01FA0, 0x1E700F60, 0x0A701B60, 0x0DB41F94, 0x1DB00F90, 0x0D701F50, 0x19700B50, 0x1DA40E94, 0x0DA01E90, 0x1D600E50, 0x09601A50, 0x1E790F69, 0x0E391F29, 0x1E350F25, 0x0A351B25, 0x0E781F68, 0x1E380F28, 0x0E341F24, 0x1A340B24, 0x1D780F58, 0x0D381F18, 0x1D340F14, 0x09341B14, 0x0D681E58, 0x1D280E18, 0x0D241E14, 0x19240A14, 0x0A7D1B6D, 0x1A3D0B2D, 0x063D172D, 0x16390729, 0x1A7C0B6C, 0x0A3C1B2C, 0x163C072C, 0x06381728, 0x097C1B5C, 0x193C0B1C, 0x053C171C, 0x15380718, 0x196C0A5C, 0x092C1A1C, 0x152C061C, 0x05281618) # Convert parameters into numpy array # float32 \"\"\" inpd = np.array(inp, dtype=np.float32) hpatd = np.array(hpat, dtype=np.uint32) output = np.empty((n*16), dtype= np.float32) \"\"\" # float64 \"\"\" inpd = np.array(inp, dtype=np.float64) hpatd = np.array(hpat, dtype=np.uint32) output = np.empty((n*16), dtype= np.float64) \"\"\" # float128 inpd = np.array(inp, dtype=np.float128) hpatd = np.array(hpat, dtype=np.uint32) output = np.empty((n*16), dtype= np.float128) # Get kernel function matinv4x4 = kernel.get_function(\"inv4x4\") # Define block, grid and compute blockDim = (256,1,1) # do not change gridDim = ((n/16)+1,1,1) # Kernel function matinv4x4 ( cuda.In(inpd), cuda.Out(output), np.uint64(n), cuda.In(hpatd), block=blockDim, grid=gridDim) return output As you can see, I tried to increase accuracy of inverting operation by replacing np.float32 by np.float64 or np.float128 but problem remains. I have also replaced typedef float T; by typedef double T;but without success. How can I perform the right inversion of these matrices and mostly avoid the 'nan' and 'inf' values ? I think this is a real issue of precision but I can't find how to circumvent this problem.",
        "answers": [
            [
                "This question has previous related questions here and (to a lesser extent) here. It's unclear to me why the title of the question refers to 3x3, the bolded text in the question refers to to 3x3, but the presented problem is a 4x4 matrix inverse (and as stated to OP previously, this code can only be used to invert 4x4 matrices). I will proceed under the assumption that the example case is the desired case. According to my testing, the only thing that is necessary is to take the previous answer code and convert it to use double (or, in pycuda, float64) rather than float (or in pycuda, float32). I think this should be evident because the example matrix values exceed the range of a float32 type. Here is a worked example: $ cat t10.py import numpy as np # import matplotlib.pyplot as plt import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit # kernel kernel = SourceModule(\"\"\" __device__ unsigned getoff(unsigned &amp;off){ unsigned ret = off &amp; 0x0F; off = off &gt;&gt; 4; return ret; } const int block_size = 256; const unsigned tmsk = 0xFFFFFFFF; // in-place is acceptable i.e. out == in) // T = float or double only typedef double T; // *** change this typedef to convert between float and double __global__ void inv4x4(const T * __restrict__ in, T * __restrict__ out, const size_t n, const unsigned * __restrict__ pat){ __shared__ T si[block_size]; size_t idx = threadIdx.x+blockDim.x*blockIdx.x; if (idx &lt; n*16){ si[threadIdx.x] = in[idx]; unsigned lane = threadIdx.x &amp; 15; unsigned sibase = threadIdx.x &amp; 0x03F0; __syncwarp(); unsigned off = pat[lane]; T a,b; a = si[sibase + getoff(off)]; a *= si[sibase + getoff(off)]; a *= si[sibase + getoff(off)]; if (!getoff(off)) a = -a; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; off = pat[lane+16]; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; off = pat[lane+32]; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; T det = si[sibase + (lane&gt;&gt;2)]*a; det += __shfl_down_sync(tmsk, det, 4, 16); // first add det += __shfl_down_sync(tmsk, det, 8, 16); // second add det = __shfl_sync(tmsk, det, 0, 16); // broadcast out[idx] = a / det; } } \"\"\") # host code def gpuinv4x4(inp, n): # internal constants not to be modified hpat = ( 0x0EB51FA5, 0x1EB10FA1, 0x0E711F61, 0x1A710B61, 0x1EB40FA4, 0x0EB01FA0, 0x1E700F60, 0x0A701B60, 0x0DB41F94, 0x1DB00F90, 0x0D701F50, 0x19700B50, 0x1DA40E94, 0x0DA01E90, 0x1D600E50, 0x09601A50, 0x1E790F69, 0x0E391F29, 0x1E350F25, 0x0A351B25, 0x0E781F68, 0x1E380F28, 0x0E341F24, 0x1A340B24, 0x1D780F58, 0x0D381F18, 0x1D340F14, 0x09341B14, 0x0D681E58, 0x1D280E18, 0x0D241E14, 0x19240A14, 0x0A7D1B6D, 0x1A3D0B2D, 0x063D172D, 0x16390729, 0x1A7C0B6C, 0x0A3C1B2C, 0x163C072C, 0x06381728, 0x097C1B5C, 0x193C0B1C, 0x053C171C, 0x15380718, 0x196C0A5C, 0x092C1A1C, 0x152C061C, 0x05281618) # Convert parameters into numpy array # *** change next line between float32 and float64 to match float or double inpd = np.array(inp, dtype=np.float64) hpatd = np.array(hpat, dtype=np.uint32) # *** change next line between float32 and float64 to match float or double output = np.empty((n*16), dtype= np.float64) # Get kernel function matinv4x4 = kernel.get_function(\"inv4x4\") # Define block, grid and compute blockDim = (256,1,1) # do not change gridDim = ((n/16)+1,1,1) # Kernel function matinv4x4 ( cuda.In(inpd), cuda.Out(output), np.uint64(n), cuda.In(hpatd), block=blockDim, grid=gridDim) return output inp = (1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 1.0, 2.0, 2.0, 3.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.120771107884677649e+09, 0.0, 0.0, 0.0, 0.0, 3.557266600921528288e+27, 3.557266600921528041e+07, 3.557266600921528320e+17, 0.0, 3.557266600921528041e+07, 3.557266600921528288e+27, 3.557266600921528041e+07, 0.0, 3.557266600921528320e+17, 3.557266600921528041e+07, 1.778633300460764144e+27) n = 2 result = gpuinv4x4(inp, n) print(result) $ python t10.py [ -3.00000000e+00 -5.00000000e-01 1.50000000e+00 1.00000000e+00 1.00000000e+00 2.50000000e-01 -2.50000000e-01 -5.00000000e-01 3.00000000e+00 2.50000000e-01 -1.25000000e+00 -5.00000000e-01 -3.00000000e+00 -0.00000000e+00 1.00000000e+00 1.00000000e+00 4.71526605e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.81114719e-28 -2.81114719e-48 -5.62229437e-38 0.00000000e+00 -2.81114719e-48 2.81114719e-28 -5.62229437e-48 0.00000000e+00 -5.62229437e-38 -5.62229437e-48 5.62229437e-28] $ Apart from updating the input matrix to use the one supplied in this question, note that only 3 lines of code were changed from the previous answer to convert from 32-bit floating point to 64-bit floating point. Each of those 3 lines are marked by a comment that contains triple-asterisk (***) in it. Also, if you are concerned about accuracy beyond the first 9 or so digits displayed here, I haven't investigated that. It may be that this code is not numerically suitable for every case or need. As an aside, the code in the question also shows a float128 type in the python section. There is no native CUDA type that corresponds to that."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "Following a previous question ( Performing high number of 4x4 matrix inversion - PyCuda ), considering the inversion of 4x4 matrix, I would like to do the same but with 3x3 matrix. As @Robert Crovella said, this change implies a complete rewrite. Given the code shown below, I tried to test some things like putting zeros instead of values but this method doesn't seem to work. Here is the code working for a high number of 4x4 matrix inversion: $ cat t10.py import numpy as np import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit # kernel kernel = SourceModule(\"\"\" __device__ unsigned getoff(unsigned &amp;off){ unsigned ret = off &amp; 0x0F; off = off &gt;&gt; 4; return ret; } const int block_size = 256; const unsigned tmsk = 0xFFFFFFFF; // in-place is acceptable i.e. out == in) // T = float or double only typedef float T; __global__ void inv4x4(const T * __restrict__ in, T * __restrict__ out, const size_t n, const unsigned * __restrict__ pat){ __shared__ T si[block_size]; size_t idx = threadIdx.x+blockDim.x*blockIdx.x; if (idx &lt; n*16){ si[threadIdx.x] = in[idx]; unsigned lane = threadIdx.x &amp; 15; unsigned sibase = threadIdx.x &amp; 0x03F0; __syncwarp(); unsigned off = pat[lane]; T a,b; a = si[sibase + getoff(off)]; a *= si[sibase + getoff(off)]; a *= si[sibase + getoff(off)]; if (!getoff(off)) a = -a; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; off = pat[lane+16]; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; off = pat[lane+32]; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; if (getoff(off)) a += b; else a -=b; T det = si[sibase + (lane&gt;&gt;2)]*a; det += __shfl_down_sync(tmsk, det, 4, 16); // first add det += __shfl_down_sync(tmsk, det, 8, 16); // second add det = __shfl_sync(tmsk, det, 0, 16); // broadcast out[idx] = a / det; } } \"\"\") # python function for inverting 4x4 matrices # n should be an even number def gpuinv4x4(inp, n): # internal constants not to be modified hpat = ( 0x0EB51FA5, 0x1EB10FA1, 0x0E711F61, 0x1A710B61, 0x1EB40FA4, 0x0EB01FA0, 0x1E700F60, 0x0A701B60, 0x0DB41F94, 0x1DB00F90, 0x0D701F50, 0x19700B50, 0x1DA40E94, 0x0DA01E90, 0x1D600E50, 0x09601A50, 0x1E790F69, 0x0E391F29, 0x1E350F25, 0x0A351B25, 0x0E781F68, 0x1E380F28, 0x0E341F24, 0x1A340B24, 0x1D780F58, 0x0D381F18, 0x1D340F14, 0x09341B14, 0x0D681E58, 0x1D280E18, 0x0D241E14, 0x19240A14, 0x0A7D1B6D, 0x1A3D0B2D, 0x063D172D, 0x16390729, 0x1A7C0B6C, 0x0A3C1B2C, 0x163C072C, 0x06381728, 0x097C1B5C, 0x193C0B1C, 0x053C171C, 0x15380718, 0x196C0A5C, 0x092C1A1C, 0x152C061C, 0x05281618) # Convert parameters into numpy array inpd = np.array(inp, dtype=np.float32) hpatd = np.array(hpat, dtype=np.uint32) output = np.empty((n*16), dtype= np.float32) # Get kernel function matinv4x4 = kernel.get_function(\"inv4x4\") # Define block, grid and compute blockDim = (256,1,1) # do not change gridDim = ((n/16)+1,1,1) # Kernel function matinv4x4 ( cuda.In(inpd), cuda.Out(output), np.uint64(n), cuda.In(hpatd), block=blockDim, grid=gridDim) return output #example/test case inp = (1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 1.0, 2.0, 2.0, 3.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0) n = 2 result = gpuinv4x4(inp, n) print(result.reshape(2,4,4)) $ python t10.py [[-3. -0.5 1.5 1. ] [ 1. 0.25 -0.25 -0.5 ] [ 3. 0.25 -1.25 -0.5 ] [-3. -0. 1. 1. ]] [[ 1. 0. 0. 0. ] [ 0. 1. 0. 0. ] [ 0. 0. 1. 0. ] [ 0. 0. 0. 1. ]] I expect the same behavior, except that I am not longer working with 4x4 matrix but with 3x3 matrix. How can I adapt this code above to work with 3x3 matrix inversion? Update 1 Here the modifications that I made. I have modified the dimension and used direct formula from the link given by @Robert Crovella (https://ardoris.wordpress.com/2008/07/18/general-formula-for-the-inverse-of-a-3x3-matrix/). Below the code modified: import numpy as np import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit # kernel of 3x3 inversion kernel_3x3 = SourceModule(\"\"\" // in-place is acceptable i.e. out == in) // T = float or double only typedef float T; __global__ void inv3x3(const T * __restrict__ in, T * __restrict__ out, const size_t n, const unsigned * __restrict__ pat){ size_t ix = threadIdx.x; size_t idx = ix + blockDim.x*blockIdx.x; if (ix &lt; n*9){ T det = in[0+idx]*(in[4+idx]*in[8+idx]-in[7+idx]*in[5+idx]) - in[1+idx]*(in[3+idx]*in[8+idx]-in[6+idx]*in[5+idx]) + in[2+idx]*(in[3+idx]*in[7+idx]-in[6+idx]*in[4+idx]); out[0+idx] = (in[4+idx]*in[8+idx]-in[7+idx]*in[5+idx])/det; out[1+idx] = (in[2+idx]*in[7+idx]-in[1+idx]*in[8+idx])/det; out[2+idx] = (in[1+idx]*in[5+idx]-in[2+idx]*in[4+idx])/det; out[3+idx] = (in[6+idx]*in[5+idx]-in[3+idx]*in[8+idx])/det; out[4+idx] = (in[0+idx]*in[8+idx]-in[2+idx]*in[6+idx])/det; out[5+idx] = (in[2+idx]*in[3+idx]-in[0+idx]*in[5+idx])/det; out[6+idx] = (in[3+idx]*in[7+idx]-in[4+idx]*in[6+idx])/det; out[7+idx] = (in[1+idx]*in[6+idx]-in[0+idx]*in[7+idx])/det; out[8+idx] = (in[0+idx]*in[4+idx]-in[1+idx]*in[3+idx])/det; __syncwarp(); } } \"\"\") def gpuinv3x3 (inp, n): # internal constants not to be modified hpat = ( 0x0EB51FA5, 0x1EB10FA1, 0x0E711F61, 0x1A710B61, 0x1EB40FA4, 0x0EB01FA0, 0x1E700F60, 0x0A701B60, 0x0DB41F94, 0x1DB00F90, 0x0D701F50, 0x19700B50, 0x1DA40E94, 0x0DA01E90, 0x1D600E50, 0x09601A50, 0x1E790F69, 0x0E391F29, 0x1E350F25, 0x0A351B25, 0x0E781F68, 0x1E380F28, 0x0E341F24, 0x1A340B24, 0x1D780F58, 0x0D381F18, 0x1D340F14, 0x09341B14, 0x0D681E58, 0x1D280E18, 0x0D241E14, 0x19240A14, 0x0A7D1B6D, 0x1A3D0B2D, 0x063D172D, 0x16390729, 0x1A7C0B6C, 0x0A3C1B2C, 0x163C072C, 0x06381728, 0x097C1B5C, 0x193C0B1C, 0x053C171C, 0x15380718, 0x196C0A5C, 0x092C1A1C, 0x152C061C, 0x05281618) # Convert parameters into numpy array inpd = np.array(inp, dtype=np.float32) hpatd = np.array(hpat, dtype=np.uint32) output = np.empty((n*9), dtype= np.float32) # Get kernel function matinv3x3 = kernel_3x3.get_function(\"inv3x3\") # Define block, grid and compute blockDim = (81,1,1) # do not change gridDim = ((n/9)+1,1,1) # Kernel function matinv3x3 ( cuda.In(inpd), cuda.Out(output), np.uint64(n), cuda.In(hpatd), block=blockDim, grid=gridDim) return output #example/test case inp = (1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 1.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0) n = 2 result = gpuinv3x3(inp, n) print(result.reshape(2,3,3)) The first matrix is correctly inverted but not the second one (the identity matrix which has identity matrix as inverse) : [[[ 2. -0. -1. ] [-1. -0.33333334 1. ] [-0. 0.33333334 -0. ]] [[ 1. -0.5 -0. ] [ -inf 1. -1. ] [ nan nan 1. ]]] So, this issue doesn't seem to come from the kernel code but rather the batch size or something similar with dimensions of global 1D array (in my code, you can see the 2 3x3 matrices formatted as 1D array of 18 elements (inp = (1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 1.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0))). What's wrong in this code? Especially, the issue of bad inversion on the second matrix. Just a last point, an odd size of group doesn't imply issues for GPU processing?",
        "answers": [
            [
                "This answer will closely follow my answer on the 4x4 invert question, both in terms of answer layout and calculation method/kernel design. The formulas are described here. First, as before, we will show a CUDA C++ version with comparison to cublas: $ cat t432.cu #include &lt;iostream&gt; #include &lt;cublas_v2.h&gt; #include &lt;cstdlib&gt; // 3x3 matrix inversion // https://stackoverflow.com/questions/1148309/inverting-a-4x4-matrix // https://ardoris.wordpress.com/2008/07/18/general-formula-for-the-inverse-of-a-3x3-matrix/ // 9 threads per matrix to invert // 32 matrices per 288 thread block const unsigned block_size = 288; typedef double mt; #define cudaCheckErrors(msg) \\ do { \\ cudaError_t __err = cudaGetLastError(); \\ if (__err != cudaSuccess) { \\ fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\ msg, cudaGetErrorString(__err), \\ __FILE__, __LINE__); \\ fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\ exit(1); \\ } \\ } while (0) #include &lt;time.h&gt; #include &lt;sys/time.h&gt; #define USECPSEC 1000000ULL long long dtime_usec(unsigned long long start){ timeval tv; gettimeofday(&amp;tv, 0); return ((tv.tv_sec*USECPSEC)+tv.tv_usec)-start; } __device__ unsigned pat[9]; const unsigned hpat[9] = {0x07584, 0x08172, 0x04251, 0x08365, 0x06280, 0x05032, 0x06473, 0x07061, 0x03140}; __device__ unsigned getoff(unsigned &amp;off){ unsigned ret = off &amp; 0x0F; off &gt;&gt;= 4; return ret; } // in-place is acceptable i.e. out == in) // T = float or double only template &lt;typename T&gt; __global__ void inv3x3(const T * __restrict__ in, T * __restrict__ out, const size_t n){ __shared__ T si[block_size]; size_t idx = threadIdx.x+blockDim.x*blockIdx.x; T det = 1; if (idx &lt; n*9) det = in[idx]; unsigned sibase = (threadIdx.x / 9)*9; unsigned lane = threadIdx.x - sibase; // cheaper modulo si[threadIdx.x] = det; __syncthreads(); unsigned off = pat[lane]; T a = si[sibase + getoff(off)]; a *= si[sibase + getoff(off)]; T b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; a -= b; __syncthreads(); if (lane == 0) si[sibase+3] = a; if (lane == 3) si[sibase+4] = a; if (lane == 6) si[sibase+5] = a; __syncthreads(); det = si[sibase]*si[sibase+3]+si[sibase+1]*si[sibase+4]+si[sibase+2]*si[sibase+5]; if (idx &lt; n*9) out[idx] = a / det; } size_t nr = 2048; int main(int argc, char *argv[]){ if (argc &gt; 1) nr = atoi(argv[1]); const mt m2[] = {1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 1.0, 2.0, 2.0}; const mt i2[] = {2.0, 0.0, -1.0, -1.0, -0.33333334, 1.0, 0.0, 0.33333334, 0.0}; const mt m1[] = {1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0}; const mt i1[] = {1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0}; mt *h_d, *d_d; h_d = (mt *)malloc(nr*9*sizeof(mt)); cudaMalloc(&amp;d_d, nr*9*sizeof(mt)); cudaMemcpyToSymbol(pat, hpat, 9*sizeof(unsigned)); for (int i = 0; i &lt; nr/2; i++){ memcpy(h_d+i*2*9, m1, sizeof(m1)); memcpy(h_d+i*2*9+9, m2, sizeof(m2));} cudaMemcpy(d_d, h_d, nr*9*sizeof(mt), cudaMemcpyHostToDevice); long long t = dtime_usec(0); inv3x3&lt;&lt;&lt;((nr*9)/block_size)+1, block_size&gt;&gt;&gt;(d_d, d_d, nr); cudaDeviceSynchronize(); t = dtime_usec(t); cudaMemcpy(h_d, d_d, nr*9*sizeof(mt), cudaMemcpyDeviceToHost); for (int i = 0; i &lt; 2; i++){ for (int j = 0; j &lt; 9; j++) std::cout &lt;&lt; h_d[i*9 + j] &lt;&lt; \",\"; std::cout &lt;&lt; std::endl; for (int j = 0; j &lt; 9; j++) std::cout &lt;&lt; ((i==0)?i1[j]:i2[j]) &lt;&lt; \",\"; std::cout &lt;&lt; std::endl;} std::cout &lt;&lt; \"kernel time: \" &lt;&lt; t &lt;&lt; \" microseconds\" &lt;&lt; std::endl; cudaError_t err = cudaGetLastError(); if (err != cudaSuccess) std::cout &lt;&lt; cudaGetErrorString(err) &lt;&lt; std::endl; //cublas for (int i = 0; i &lt; nr/2; i++){ memcpy(h_d+i*2*9, m1, sizeof(m1)); memcpy(h_d+i*2*9+9, m2, sizeof(m2));} cudaMemcpy(d_d, h_d, nr*9*sizeof(mt), cudaMemcpyHostToDevice); cublasHandle_t h; cublasStatus_t cs = cublasCreate(&amp;h); if (cs != CUBLAS_STATUS_SUCCESS) std::cout &lt;&lt; \"cublas create error\" &lt;&lt; std::endl; mt **A, **Ai, *Aid, **Ap, **Aip; A = (mt **)malloc(nr*sizeof(mt *)); Ai = (mt **)malloc(nr*sizeof(mt *)); cudaMalloc(&amp;Aid, nr*9*sizeof(mt)); cudaMalloc(&amp;Ap, nr*sizeof(mt *)); cudaMalloc(&amp;Aip, nr*sizeof(mt *)); for (int i = 0; i &lt; nr; i++) A[i] = d_d + 9*i; for (int i = 0; i &lt; nr; i++) Ai[i] = Aid + 9*i; cudaMemcpy(Ap, A, nr*sizeof(mt *), cudaMemcpyHostToDevice); cudaMemcpy(Aip, Ai, nr*sizeof(mt *), cudaMemcpyHostToDevice); int *info; cudaMalloc(&amp;info, nr*sizeof(int)); t = dtime_usec(0); cs = cublasDmatinvBatched(h, 3, Ap, 3, Aip, 3, info, nr); if (cs != CUBLAS_STATUS_SUCCESS) std::cout &lt;&lt; \"cublas matinv error\" &lt;&lt; std::endl; cudaDeviceSynchronize(); t = dtime_usec(t); cudaMemcpy(h_d, Aid, nr*9*sizeof(mt), cudaMemcpyDeviceToHost); for (int i = 0; i &lt; 2; i++){ for (int j = 0; j &lt; 9; j++) std::cout &lt;&lt; h_d[i*9 + j] &lt;&lt; \",\"; std::cout &lt;&lt; std::endl; for (int j = 0; j &lt; 9; j++) std::cout &lt;&lt; ((i==0)?i1[j]:i2[j]) &lt;&lt; \",\"; std::cout &lt;&lt; std::endl;} std::cout &lt;&lt; \"cublas time: \" &lt;&lt; t &lt;&lt; \" microseconds\" &lt;&lt; std::endl; err = cudaGetLastError(); if (err != cudaSuccess) std::cout &lt;&lt; cudaGetErrorString(err) &lt;&lt; std::endl; return 0; } $ nvcc -o t432 t432.cu -lcublas $ ./t432 1,0,0,0,1,0,0,0,1, 1,0,0,0,1,0,0,0,1, 2,-0,-1,-1,-0.333333,1,-0,0.333333,-0, 2,0,-1,-1,-0.333333,1,0,0.333333,0, kernel time: 59 microseconds 1,0,0,0,1,0,0,0,1, 1,0,0,0,1,0,0,0,1, 2,0,-1,-1,-0.333333,1,0,0.333333,0, 2,0,-1,-1,-0.333333,1,0,0.333333,0, cublas time: 68 microseconds $ So this is perhaps slightly faster than cublas but not much, for this 2048 matrix test case, CUDA 10.0, Tesla P100, linux. Similar to the previous answer, here is a simplified (only 2 matrices) pycuda test case: $ cat t14.py import numpy as np # import matplotlib.pyplot as plt import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit # kernel kernel = SourceModule(\"\"\" __device__ unsigned getoff(unsigned &amp;off){ unsigned ret = off &amp; 0x0F; off &gt;&gt;= 4; return ret; } // in-place is acceptable i.e. out == in) // T = float or double only const int block_size = 288; typedef double T; // *** can set to float or double __global__ void inv3x3(const T * __restrict__ in, T * __restrict__ out, const size_t n, const unsigned * __restrict__ pat){ __shared__ T si[block_size]; size_t idx = threadIdx.x+blockDim.x*blockIdx.x; T det = 1; if (idx &lt; n*9) det = in[idx]; unsigned sibase = (threadIdx.x / 9)*9; unsigned lane = threadIdx.x - sibase; // cheaper modulo si[threadIdx.x] = det; __syncthreads(); unsigned off = pat[lane]; T a = si[sibase + getoff(off)]; a *= si[sibase + getoff(off)]; T b = si[sibase + getoff(off)]; b *= si[sibase + getoff(off)]; a -= b; __syncthreads(); if (lane == 0) si[sibase+3] = a; if (lane == 3) si[sibase+4] = a; if (lane == 6) si[sibase+5] = a; __syncthreads(); det = si[sibase]*si[sibase+3]+si[sibase+1]*si[sibase+4]+si[sibase+2]*si[sibase+5]; if (idx &lt; n*9) out[idx] = a / det; } \"\"\") # host code def gpuinv3x3(inp, n): # internal constants not to be modified hpat = (0x07584, 0x08172, 0x04251, 0x08365, 0x06280, 0x05032, 0x06473, 0x07061, 0x03140) # Convert parameters into numpy array # *** change next line between float32 and float64 to match float or double inpd = np.array(inp, dtype=np.float64) hpatd = np.array(hpat, dtype=np.uint32) # *** change next line between float32 and float64 to match float or double output = np.empty((n*9), dtype= np.float64) # Get kernel function matinv3x3 = kernel.get_function(\"inv3x3\") # Define block, grid and compute blockDim = (288,1,1) # do not change gridDim = ((n/32)+1,1,1) # Kernel function matinv3x3 ( cuda.In(inpd), cuda.Out(output), np.uint64(n), cuda.In(hpatd), block=blockDim, grid=gridDim) return output inp = (1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 1.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0) n = 2 result = gpuinv3x3(inp, n) print(result.reshape(2,3,3)) $ python t14.py [[[ 2. -0. -1. ] [-1. -0.33333333 1. ] [-0. 0.33333333 -0. ]] [[ 1. 0. 0. ] [ 0. 1. 0. ] [ 0. 0. 1. ]]] $ The above happens to be using double i.e. float64 in pycuda. Changing it to float i.e. float32 in pycuda involves changing the same 3 lines as described in this answer."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I want to generate random numbers with pyCUDA. To this end, I'm using the following code, which I'm running on the Kaggle virtual machine: import numpy as np import time import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule N = 10 from pycuda.curandom import XORWOWRandomNumberGenerator rng = XORWOWRandomNumberGenerator() d_x = rng.gen_uniform((N,), dtype = np.float32) My question is on how do I feed the random number generator with a seed. At the pyCUDA documentation page, it says that class pycuda.curandom.XORWOWRandomNumberGenerator(seed_getter=None, offset=0) Parameters: seed_getter \u2013 a function that, given an integer count, will yield an int32 GPUArray of seeds. offset \u2013 Starting index into the XORWOW sequence, given seed. What is an example of the seed_getter function?",
        "answers": [
            [
                "The curandom module has two built-in functions for generating random seeds: seed_getter_uniform which will return an array length N initialized with a single random seed, and seed_getter_unique which will return an array initialized with N different random seeds. Use one or the other depending on whether you want all internal generator instances to used the same seed or a unique seed."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a dataframe of the following format. df A B Target 5 4 3 1 3 4 I am finding the correlation of each column (except Target) with the Target column using pd.DataFrame(df.corr().iloc[:-1,-1]). But the issue is - size of my actual dataframe is (216, 72391) which atleast takes 30 minutes to process on my system. Is there any way of parallerize it using a gpu ? I need to find the values of similar kind multiple times so can't wait for the normal processing time of 30 minutes each time.",
        "answers": [
            [
                "Here, I have tried to implement your operation using numba import numpy as np import pandas as pd from numba import jit, int64, float64 # #------------You can ignore the code starting from here--------- # # Create a random DF with cols_size = 72391 and row_size =300 df_dict = {} for i in range(0, 72391): df_dict[i] = np.random.randint(100, size=300) target_array = np.random.randint(100, size=300) df = pd.DataFrame(df_dict) # ----------Ignore code till here. This is just to generate dummy data------- # Assume df is your original DataFrame target_array = df['target'].values # You can choose to restore this column later # But for now we will remove it, since we will # call the df.values and find correlation of each # column with target df.drop(['target'], inplace=True, axis=1) # This function takes in a numpy 2D array and a target array as input # The numpy 2D array has the data of all the columns # We find correlation of each column with target array # numba's Jit required that both should have same columns # Hence the first 2d array is transposed, i.e. it's shape is (72391,300) # while target array's shape is (300,) def do_stuff(df_values, target_arr): # Just create a random array to store result # df_values.shape[0] = 72391, equal to no. of columns in df result = np.random.random(df_values.shape[0]) # Iterator over each column for i in range(0, df_values.shape[0]): # Find correlation of a column with target column # In order to find correlation we must transpose array to make them compatible result[i] = np.corrcoef(np.transpose(df_values[i]), target_arr.reshape(300,))[0][1] return result # Decorate the function do_stuff do_stuff_numba = jit(nopython=True, parallel=True)(do_stuff) # This contains all the correlation result_array = do_stuff_numba(np.transpose(df.T.values), target_array) Link to colab notebook."
            ],
            [
                "You should take a look at dask. It should be able to do what you want and a lot more. It parallelizes most of the DataFrame functions."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I show you below an example of code using pycuda with \"kernel\" code included in itself (with SourceModule) import pycuda import pycuda.driver as cuda from pycuda.compiler import SourceModule import threading import numpy class GPUThread(threading.Thread): def __init__(self, number, some_array): threading.Thread.__init__(self) self.number = number self.some_array = some_array def run(self): self.dev = cuda.Device(self.number) self.ctx = self.dev.make_context() self.array_gpu = cuda.mem_alloc(some_array.nbytes) cuda.memcpy_htod(self.array_gpu, some_array) test_kernel(self.array_gpu) print \"successful exit from thread %d\" % self.number self.ctx.pop() del self.array_gpu del self.ctx def test_kernel(input_array_gpu): mod = SourceModule(\"\"\" __global__ void f(float * out, float * in) { int idx = threadIdx.x; out[idx] = in[idx] + 6; } \"\"\") func = mod.get_function(\"f\") output_array = numpy.zeros((1,512)) output_array_gpu = cuda.mem_alloc(output_array.nbytes) func(output_array_gpu, input_array_gpu, block=(512,1,1)) cuda.memcpy_dtoh(output_array, output_array_gpu) return output_array cuda.init() some_array = numpy.ones((1,512), dtype=numpy.float32) num = cuda.Device.count() gpu_thread_list = [] for i in range(num): gpu_thread = GPUThread(i, some_array) gpu_thread.start() gpu_thread_list.append(gpu_thread) I would like to use the same method but instead of using a \"kernel code\", I would like to do multiple calls of a function which is external (not a function like \"kernel code\"), i.e a classical function defined in my main program and which takes in argument different parameters shared by all the main program. Is it possible ? People who have practiced Matlab may know the function arrayfun where B = arrayfun(func,A) is a vector of results given by applying function funcfor each element of vector A. Actually, it is a version of what is commonly called the map function: I would like to do the same but with GPU/pycuda version. Update 1 Sorry, I forgot from the beginning of my post to say what I call an extern and classical function. Here is below an example of function which is used in main section : def integ(I1): function_A = aux_fun_LU(way, ecs, I1[0], I1[1]) integrale_A = 0.25*delta_x*delta_y*np.sum(function_A[0:-1, 0:-1] + function_A[1:, 0:-1] + function_A[0:-1, 1:] + function_A[1:, 1:]) def g(): for j in range(6*i, 6*i+6): for l in range(j, 6*i+6): yield j, l ## applied integ function to g() generator. ## Here I a using simple map function (no parallelization) if __name__ == '__main__': map(integ, g()) Update 2 Maybe a solution would be to call the extern function from a kernel code, benefiting as well of the high GPU power of multiple calls on kernel code. But how to deal with the returned value of this extern function to get it back into main program? Update 3 Here is below what I have tried: # Class GPUThread class GPUThread(threading.Thread): def __init__(self, number, some_array): threading.Thread.__init__(self) self.number = number self.some_array = some_array def run(self): self.dev = cuda.Device(self.number) self.ctx = self.dev.make_context() self.array_gpu = cuda.mem_alloc(some_array.nbytes) cuda.memcpy_htod(self.array_gpu, some_array) test_kernel(self.array_gpu) print \"successful exit from thread %d\" % self.number self.ctx.pop() del self.array_gpu del self.ctx def test_kernel(input_array_gpu): mod1 = SourceModule(\"\"\" __device__ void integ1(int *I1) { function_A = aux_fun_LU(way, ecs, I1[0], I1[1]); integrale_A = 0.25*delta_x*delta_y*np.sum(function_A[0:-1, 0:-1] + function_A[1:, 0:-1] + function_A[0:-1, 1:] + function_A[1:, 1:]); }\"\"\") func1 = mod1.get_function(\"integ1\") # Calling function func1(input_array_gpu) # Define couples (i,j) to build Fisher matrix def g1(): for j in range(6*i, 6*i+6): for l in range(j, 6*i+6): yield j, l # Cuda init if __name__ == '__main__': cuda.init() # Input gTotal lists some_array1 = np.array(list(g1())) print 'some_array1 = ', some_array1 # Parameters for cuda num = cuda.Device.count() gpu_thread_list = [] for i in range(num): gpu_thread = GPUThread(i, some_array1) #gpu_thread = GPUThread(i, eval(\"some_array\"+str(j))) gpu_thread.start() gpu_thread_list.append(gpu_thread) I get the following error at the execution: `Traceback (most recent call last): File \"/Users/mike/anaconda2/envs/py2cuda/lib/python2.7/threading.py\", line 801, in __bootstrap_inner self.run() File \"Example_GPU.py\", line 1232, in run self.array_gpu = cuda.mem_alloc(some_array.nbytes) NameError: global name 'some_array' is not defined` I can't see what's wrong with the variable 'some_array' and the line self.array_gpu = cuda.mem_alloc(some_array.nbytes) What can I try next?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to run code that I got off this site - https://documen.tician.de/pycuda/tutorial.html - it's what's below. import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import pycuda.autoinit import numpy a_gpu = gpuarray.to_gpu(numpy.random.randn(4,4).astype(numpy.float32)) a_doubled = (2*a_gpu).get() print(a_doubled) print(a_gpu) Basically, for the line \"import pycuda.driver as cuda\", I get the error message: File \"C:\\Users\\David\\Anaconda3\\lib\\site-packages\\pycuda\\driver.py\", line 5, in from pycuda._driver import * # noqa ModuleNotFoundError: No module named 'pycuda._driver' This makes sense, because when I look as the driver text file, I see the following lines try: from pycuda._driver import * # noqa except ImportError as e: if \"_v2\" in str(e): from warnings import warn warn(\"Failed to import the CUDA driver interface, with an error \" \"message indicating that the version of your CUDA header \" \"does not match the version of your CUDA driver.\") raise And there is in fact no text file called _driver in my pycuda folder. So how can I fix this? I thought I should have all the folders when I write \"pip install pycuda\" in my terminal.",
        "answers": [
            [
                "Depending on your OS make sure you have configured pycuda on correct cuda lib path.. Follow https://wiki.tiker.net/PyCuda/Installation/Windows carefully. HOWEVER: It looks like you want to install this on Anaconda, therefore I would advise you to follow this guide instead. https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_PyCUDA_On_Anaconda_For_Windows?lang=en"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm building a simple neural network in Python using Tensorflow and Keras. I need to implement this code to work on a GPU, using PyCuda. I plan on parallelizing learning all the epochs, however since Keras is very minimalistic, all epoch training (at least from my understanding) is done with one line: model.fit(train_images, train_labels, epochs=100) How would it be possible to \"extract\" something from this function, that could be fed to a PyCuda kernel function? This is my code so far: #TensorFlow and tf.keras import tensorflow as tf from tensorflow import keras #Helper libraries import numpy as np import matplotlib.pyplot as plt import cv2 print(tf.__version__) fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] train_images.shape len(train_labels) train_labels test_images.shape len(test_labels) plt.figure() plt.imshow(train_images[0]) plt.colorbar() plt.grid(False) plt.show() train_images = train_images / 255.0 test_images = test_images / 255.0 plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]]) plt.show() model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax) ]) model.compile(optimizer=tf.train.AdamOptimizer(), loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(train_images, train_labels, epochs=100)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have installed python 3.7.2 along with the following libraries: jupyter, pandas, numpy, pytools and pycuda. I'm working with Visual Studio Code. I'm trying to run the standard pyCuda example: # --- PyCuda initialization import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule # --- Create a 4x4 double precision array of random numbers import numpy a = numpy.random.randn(4,4) # --- Demote array to single precision a = a.astype(numpy.float32) # --- Allocate GPU device memory a_gpu = cuda.mem_alloc(a.nbytes) # --- Memcopy from host to device cuda.memcpy_htod(a_gpu, a) # --- Define a device function that doubles the input device array mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") # --- Define a reference to the __global__ function and call it func = mod.get_function(\"doublify\") func(a_gpu, block=(4,4,1)) # --- Copy results from device to host a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print(a_doubled) print(a) When I run this code, VSCode says that Module 'pycuda.driver' has no 'mem_alloc' member Module 'pycuda.driver' has no 'memcpy_htod' member Module 'pycuda.driver' has no 'memcpy_dtoh' member However, from the below figure, it seems that the module exists Any suggestion on how solving the problem? EDIT: SIMPLIFIED TEST CASE If I run # --- PyCuda initialization import pycuda.driver as cuda print(\"test\") then test is emitted in the console. If I run # --- PyCuda initialization import pycuda.driver as cuda # Initialize CUDA cuda.init() print(\"test\") nothing is emitted in the console and VSCode emits the following problem Module 'pycuda.driver' has no 'init' member",
        "answers": [
            [
                "The problem was an installation issue. I have just uninstalled the version of pycuda that I previously installed via python pip install pycuda and downloaded a precompiled binary from Christoph Golke page, while taking care of compatibility. For me, the correct file has been pycuda-2018.1.1+cuda100-cp37-cp37m-win_amd64 for python 3.7.2 64bits. Now, everything works correctly."
            ],
            [
                "If your code runs without AttributeErrors then it is likely that Visual Studio is giving false positives. If so, ignore them. This happens because doing static analysis on dynamic code doesn't always do the right thing."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am learning GPU programming on PyCUDA. I am a bit confused by the calculation of matrix operation on the blocks. Like the example below, I want to redo the calculation a = np.array([1,2,3,4,5,6]) c = a[:,np.newaxis] - a which should be c = [[0,-1,-2,-3,-4,-5], [1,0,-1,-2,-3,-4], [2,1,0,-1,-2,-3], [3,2,1,0,-1,-2]] on GPU. Follow the code below, if I allocate the same size for matrix and the block. Everything works fine. But to test computation in multiple blocks, I allocated 4 to the block size, things got wrong. I have checked the blockDim for each entry in output c. It shows some of the entries have 0 blockDim but they should be all 4. array([[4., 4., 4., 4., 4., 4.], [0., 0., 4., 4., 4., 4.], [0., 0., 4., 4., 4., 4.], [0., 0., 4., 4., 4., 4.], [4., 4., 4., 4., 4., 4.], [4., 4., 4., 4., 4., 4.]], dtype=float32) and the threadIdx.x shows wrong number at the same position. array([[0., 1., 2., 3., 0., 1.], [0., 0., 2., 3., 0., 1.], [0., 0., 2., 3., 0., 1.], [0., 0., 2., 3., 0., 1.], [0., 1., 2., 3., 0., 1.], [0., 1., 2., 3., 0., 1.]], dtype=float32) This is very strange. Repeatable code is as follows. import numpy as np from pycuda import compiler, gpuarray, tools import pycuda.driver as drv # -- initialize the device import pycuda.autoinit kernel_code_template = \"\"\" __global__ void com_t(float *a, float *c) { // 2D Thread ID int tx = blockDim.x*blockIdx.x + threadIdx.x; // Compute row index int ty = blockDim.y*blockIdx.y + threadIdx.y; // Compute column index // Pvalue is used to store the element of the matrix // that is computed by the thread float Pvalue = 0; // Each thread loads one row of M and one column of N, // to produce one element of P. float Aelement = blockDim.x; float Belement = 0; Pvalue = Aelement - Belement; // Write the matrix to device memory; // each thread writes one element c[ty * %(MATRIX_SIZE)s + tx] = Pvalue; } \"\"\" MATRIX_SIZE = 6 BLOCK_SIZE = 6 start = drv.Event() end = drv.Event() # # create a random vector a_cpu = np.array([i for i in range(MATRIX_SIZE)]).astype(np.float32) # compute reference on the CPU to verify GPU computation start.record() # start timing start.synchronize() c_cpu = a_cpu[:,np.newaxis] - a_cpu end.record() # end timing # calculate the run length end.synchronize() secs = start.time_till(end)*1e-3 print(\"CPU time:\") print(\"%fs\" % (secs)) # transfer host (CPU) memory to device (GPU) memory a_gpu = gpuarray.to_gpu(a_cpu) # create empty gpu array for the result (C = A * B) c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32) # get the kernel code from the template # by specifying the constant MATRIX_SIZE kernel_code = kernel_code_template % { 'MATRIX_SIZE': MATRIX_SIZE } # compile the kernel code mod = compiler.SourceModule(kernel_code) # get the kernel function from the compiled module matrixmul = mod.get_function(\"com_t\") start.record() # start timing # set grid size if MATRIX_SIZE%BLOCK_SIZE != 0: grid=(MATRIX_SIZE//BLOCK_SIZE+1,MATRIX_SIZE//BLOCK_SIZE+1,1) else: grid=(MATRIX_SIZE//BLOCK_SIZE,MATRIX_SIZE//BLOCK_SIZE,1) # call the kernel on the card matrixmul( # inputs a_gpu, # output c_gpu, grid = grid, # (only one) block of MATRIX_SIZE x MATRIX_SIZE threads block = (BLOCK_SIZE, BLOCK_SIZE, 1), ) end.record() # end timing end.synchronize() secs = start.time_till(end)*1e-3 print(\"GPU time:\") print(\"%fs\" % (secs)) # print the results print(\"-\" * 80) print(\"Matrix A (GPU):\") print(a_gpu.get()) print(\"-\" * 80) print(\"Matrix C (GPU):\") print(c_gpu.get()) print(\"-\" * 80) print(\"CPU-GPU difference:\") print(c_cpu - c_gpu.get()) np.allclose(c_cpu, c_gpu.get())",
        "answers": [
            [
                "Problem solved. The evaluation of matrix c should be put in a constrain like if((ty &lt;matrixsize) &amp;&amp; (tx &lt; matrixsize)) Otherwise, the over requested threads will be invoked and squeeze out the right entry of c."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a Python code (for implementing RayTracing) that I'm running in parallel with PyCuda. import pycuda.driver as drv import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np from stl import mesh import time my_mesh = mesh.Mesh.from_file('test_solid_py.stl') n = my_mesh.normals v0 = my_mesh.v0 v1 = my_mesh.v1 v2 = my_mesh.v2 v0_x = v0[:,0] v0_x = np.ascontiguousarray(v0_x) v0_y = v0[:,1] v0_y = np.ascontiguousarray(v0_y) v0_z = v0[:,2] v0_z = np.ascontiguousarray(v0_z) v1_x = v1[:,0] v1_x = np.ascontiguousarray(v1_x) v1_y = v1[:,1] v1_y = np.ascontiguousarray(v1_y) v1_z = v1[:,2] v1_z = np.ascontiguousarray(v1_z) v2_x = v2[:,0] v2_x = np.ascontiguousarray(v2_x) v2_y = v2[:,1] v2_y = np.ascontiguousarray(v2_y) v2_z = v2[:,2] v2_z = np.ascontiguousarray(v2_z) mod = SourceModule(\"\"\" #include &lt;math.h&gt; __global__ void intersect(float *origin,float *dir_x,float *dir_y,float *dir_z,float *v0_x,float *v0_y,float *v0_z,float *v1_x,float *v1_y,float *v1_z,float *v2_x,float *v2_y,float *v2_z,float *int_point_real_x, float *int_point_real_y,float *int_point_real_z) { using namespace std; unsigned int idx = blockDim.x*blockIdx.x + threadIdx.x; int count = 0; float v0_current[3]; float v1_current[3]; float v2_current[3]; float dir_current[3] = {dir_x[idx],dir_y[idx],dir_z[idx]}; float int_point[3]; float int_pointS[2][3]; int int_faces[2]; float dist[2]; dist[0] = -999; int n_tri = 105500; for(int i = 0; i&lt;n_tri; i++) { v0_current[0] = v0_x[i]; v0_current[1] = v0_y[i]; v0_current[2] = v0_z[i]; v1_current[0] = v1_x[i]; v1_current[1] = v1_y[i]; v1_current[2] = v1_z[i]; v2_current[0] = v2_x[i]; v2_current[1] = v2_y[i]; v2_current[2] = v2_z[i]; double eps = 0.0000001; float E1[3]; float E2[3]; float s[3]; for (int j = 0; j &lt; 3; j++) { E1[j] = v1_current[j] - v0_current[j]; E2[j] = v2_current[j] - v0_current[j]; s[j] = origin[j] - v0_current[j]; } float h[3]; h[0] = dir_current[1] * E2[2] - dir_current[2] * E2[1]; h[1] = -(dir_current[0] * E2[2] - dir_current[2] * E2[0]); h[2] = dir_current[0] * E2[1] - dir_current[1] * E2[0]; float a; a = E1[0] * h[0] + E1[1] * h[1] + E1[2] * h[2]; if (a &gt; -eps &amp;&amp; a &lt; eps) { int_point[0] = false; } else { double f = 1 / a; float u; u = f * (s[0] * h[0] + s[1] * h[1] + s[2] * h[2]); if (u &lt; 0 || u &gt; 1) { int_point[0] = false; } else { float q[3]; q[0] = s[1] * E1[2] - s[2] * E1[1]; q[1] = -(s[0] * E1[2] - s[2] * E1[0]); q[2] = s[0] * E1[1] - s[1] * E1[0]; float v; v = f * (dir_current[0] * q[0] + dir_current[1] * q[1] + dir_current[2] * q[2]); if (v &lt; 0 || (u + v)&gt;1) { int_point[0] = false; } else { float t; t = f * (E2[0] * q[0] + E2[1] * q[1] + E2[2] * q[2]); if (t &gt; eps) { for (int j = 0; j &lt; 3; j++) { int_point[j] = origin[j] + dir_current[j] * t; } //return t; } } } } if (int_point[0] != false) { count = count+1; int_faces[count-1] = i; dist[count-1] = sqrt(pow((origin[0] - int_point[0]), 2) + pow((origin[1] - int_point[1]), 2) + pow((origin[2] - int_point[2]), 2)); for (int j = 0; j&lt;3; j++) { int_pointS[count-1][j] = int_point[j]; } } } double min = dist[0]; int ind_min = 0; for (int i = 0; i &lt; 2; i++){ if (min &gt; dist[i]) { min = dist[i]; ind_min = i; } } if (dist[0] &lt; -998){ int_point_real_x[idx] = -999; int_point_real_y[idx] = -999; int_point_real_z[idx] = -999; } else{ int_point_real_x[idx] = int_pointS[ind_min][0]; int_point_real_y[idx] = int_pointS[ind_min][1]; int_point_real_z[idx] = int_pointS[ind_min][2]; } } \"\"\") n_rays = 20000 num_threads = 1024 num_blocks = int(n_rays/num_threads) origin = np.asarray([-2, -2, -2]).astype(np.float32) origin = np.ascontiguousarray(origin) rand_x = np.random.randn(n_rays) rand_y = np.random.randn(n_rays) rand_z = np.random.randn(n_rays) direction_x = np.ones((n_rays, 1)) * 3 direction_x = direction_x.astype(np.float32) direction_x = np.ascontiguousarray(direction_x) direction_y = np.ones((n_rays, 1)) * 4 direction_y = direction_y.astype(np.float32) direction_y = np.ascontiguousarray(direction_y) direction_z = np.ones((n_rays, 1)) * 5 direction_z = direction_z.astype(np.float32) direction_z = np.ascontiguousarray(direction_z) int_point_real_x = np.zeros((n_rays, 1)).astype(np.float32) int_point_real_x = np.ascontiguousarray(int_point_real_x) int_point_real_y = np.zeros((n_rays, 1)).astype(np.float32) int_point_real_y = np.ascontiguousarray(int_point_real_y) int_point_real_z = np.zeros((n_rays, 1)).astype(np.float32) int_point_real_z = np.ascontiguousarray(int_point_real_z) intersect = mod.get_function(\"intersect\") start = time.time() intersect(drv.In(origin), drv.In(direction_x),drv.In(direction_y),drv.In(direction_z),drv.In(v0_x),drv.In(v0_y),drv.In(v0_z), drv.In(v1_x),drv.In(v1_y),drv.In(v1_z), drv.In(v2_x), drv.In(v2_y), drv.In(v2_z), drv.Out(int_point_real_x),drv.Out(int_point_real_y),drv.Out(int_point_real_z), block=(num_threads, 1, 1), grid=((num_blocks+0), 1, 1)) finish = time.time() print(finish-start) I give as input some arrays whose size is 20k (dir_x, dir_y, dir_z) and I have as output 3 arrays (int_point_real_x,int_point_real_y,int_point_real_z) that have the same size as the above mentioned arrays (20k). If n_rays is a multiple of num_threads, e.g. n_rays=19456 and num_threads=1024, then int_point_real_x_y_z are correctly filled by the kernel. Otherwise, if n_rays is NOT a multiple of num_threads, e.g. n_rays=20000 (what I really need) and num_threads=1024, then int_point_real_x_y_z are filled by the kernel up to position 19455 and the 544 spots left in the array are not filled. Does anyone know if this is a rule of CUDA? If it's not, how could I modify my code in order to use an arbitrary size of input array (and not only multiple of num_threads)? Thanks",
        "answers": [
            [
                "your int(n_rays/num_threads) is rounding down to fix this, you need to round up and then put a condition into the kernel to enforce that idx is valid and \"do nothing\" if it's not. this will cause some cores to waste time, but your code looks pretty suboptimal anyway so it probably won't matter much"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Closed. This question needs to be more focused. It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 4 years ago. Improve this question I have a ray-tracing model where I fire 20k rays against a mesh object with ~100k triangular faces. To calculate coordinates of intersection point I wrote this function based on Moller-trumbore algorithm (https://en.wikipedia.org/wiki/M%C3%B6ller%E2%80%93Trumbore_intersection_algorithm): void MT_intersection(std::vector&lt;double&gt; origin, std::vector&lt;double&gt; dir, std::vector&lt;double&gt; v0, std::vector&lt;double&gt; v1, std::vector&lt;double&gt; v2, std::vector&lt;double&gt; &amp;int_point) { double eps = 0.0000001; std::vector&lt;double&gt; E1(3); std::vector&lt;double&gt; E2(3); std::vector&lt;double&gt; s(3); for (int i = 0; i &lt; 3; i++) { E1[i] = v1[i] - v0[i]; E2[i] = v2[i] - v0[i]; s[i] = origin[i] - v0[i]; } std::vector&lt;double&gt; h(3); h[0] = dir[1] * E2[2] - dir[2] * E2[1]; h[1] = -(dir[0] * E2[2] - dir[2] * E2[0]); h[2] = dir[0] * E2[1] - dir[1] * E2[0]; double a; a = E1[0] * h[0] + E1[1] * h[1] + E1[2] * h[2]; if (a &gt; -eps &amp;&amp; a &lt; eps) { int_point[0] = false; } else { double f = 1 / a; double u; u = f * (s[0] * h[0] + s[1] * h[1] + s[2] * h[2]); if (u &lt; 0 || u &gt; 1) { int_point[0] = false; } else { std::vector&lt;double&gt; q(3); q[0] = s[1] * E1[2] - s[2] * E1[1]; q[1] = -(s[0] * E1[2] - s[2] * E1[0]); q[2] = s[0] * E1[1] - s[1] * E1[0]; double v; v = f * (dir[0] * q[0] + dir[1] * q[1] + dir[2] * q[2]); if (v &lt; 0 || (u + v)&gt;1) { int_point[0] = false; } else { double t; t = f * (E2[0] * q[0] + E2[1] * q[1] + E2[2] * q[2]); if (t &gt; eps) { for (int i = 0; i &lt; 3; i++) { int_point[i] = origin[i] + dir[i] * t; } } } } } } I give as input the origin and the direction of my ray and 3 vectors with triangles vertices coordinates (v0,v1,v2). Then I use this function in a for loop with ~100k (number of triangles) repetitions inside another for loop with 20k (number of rays) repetitions. Since this code is very slow (it takes approx 2 days and a half to calculate everything), I want to run it in parallel with Cuda, hoping to reduce this time. Since I'm working with Python, I'm using PyCuda and I tried to write a C kernel with my \"MT_intersection\" function: import pycuda.driver as drv import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np from stl import mesh my_mesh = mesh.Mesh.from_file('sfera1.stl') n = my_mesh.normals v0 = my_mesh.v0 v1 = my_mesh.v1 v2 = my_mesh.v2 mod = SourceModule(\"\"\" #include &lt;math.h&gt; //#include &lt;vector&gt; __global__ void intersect(float *origin,float *dir,float *v0,float *v1,float *v2,float *int_point_real) { using namespace std; //#include &lt;vector&gt; //#include &lt;math.h&gt; int idx = threadIdx.x; //a[idx] *= 2; int count = 0; //std::vector&lt;double&gt; v0_current(3); float v0_current[3]; float v1_current[3]; float v2_current[3]; float dir_current[3] = {dir[idx][0],dir[idx][1],dir[idx][2]}; //std::vector&lt;double&gt; v1_current(3); //std::vector&lt;double&gt; v2_current(3); float int_point[3]; //std::vector&lt;float&gt; int_point(3); //std::vector&lt;std::vector&lt;float&gt;&gt; int_pointS; float int_pointS[2][3]; //std::vector&lt;std::vector&lt;double&gt;&gt; int_point; //std::vector&lt;int&gt; int_faces; int int_faces[2]; float dist[2]; //std::vector&lt;float&gt; dist; int n_tri = 960; for(int i = 0; i&lt;n_tri; i++) { for (int j = 0; j&lt;3; j++){ v0_current[j] = v0[i][j]; v1_current[j] = v1[i][j]; v2_current[j] = v2[i][j]; } double eps = 0.0000001; //std::vector&lt;float&gt; E1(3); float E1[3]; //std::vector&lt;float&gt; E2(3); float E2[3]; //std::vector&lt;float&gt; s(3); float s[3]; for (int j = 0; j &lt; 3; j++) { E1[j] = v1_current[j] - v0_current[j]; E2[j] = v2_current[j] - v0_current[j]; s[j] = origin[j] - v0_current[j]; } //std::vector&lt;float&gt; h(3); float h[3]; h[0] = dir[1] * E2[2] - dir[2] * E2[1]; h[1] = -(dir[0] * E2[2] - dir[2] * E2[0]); h[2] = dir[0] * E2[1] - dir[1] * E2[0]; float a; a = E1[0] * h[0] + E1[1] * h[1] + E1[2] * h[2]; if (a &gt; -eps &amp;&amp; a &lt; eps) { int_point[0] = false; //return false; } else { double f = 1 / a; float u; u = f * (s[0] * h[0] + s[1] * h[1] + s[2] * h[2]); if (u &lt; 0 || u &gt; 1) { int_point[0] = false; //return false; } else { //std::vector&lt;float&gt; q(3); float q[3]; q[0] = s[1] * E1[2] - s[2] * E1[1]; q[1] = -(s[0] * E1[2] - s[2] * E1[0]); q[2] = s[0] * E1[1] - s[1] * E1[0]; float v; v = f * (dir[0] * q[0] + dir[1] * q[1] + dir[2] * q[2]); if (v &lt; 0 || (u + v)&gt;1) { int_point[0] = false; //return false; } else { float t; t = f * (E2[0] * q[0] + E2[1] * q[1] + E2[2] * q[2]); if (t &gt; eps) { for (int j = 0; j &lt; 3; j++) { int_point[j] = origin[j] + dir_current[j] * t; } //return t; } } } } if (int_point[0] != false) { count = count+1; //int_faces.push_back(i); int_faces[count-1] = i; //dist.push_back(sqrt(pow((origin[0] - int_point[0]), 2) + pow((origin[1] - int_point[1]), 2) + pow((origin[2] - int_point[2]), 2))); //dist.push_back(x); dist[count-1] = sqrt(pow((origin[0] - int_point[0]), 2) + pow((origin[1] - int_point[1]), 2) + pow((origin[2] - int_point[2]), 2)); //int_pointS.push_back(int_point); for (int j = 0; j&lt;3; j++) { int_pointS[count-1][j] = int_point[j]; } } } double min = dist[0]; int ind_min = 0; for (int i = 0; i &lt; int_pointS.size(); i++){ if (min &gt; dist[i]) { min = dist[i]; ind_min = i; } } //dist_real[Idx] = dist[ind_min]; //int_point_real_x[Idx] = int_pointS[ind_min][0]; //int_point_real_y[Idx] = int_pointS[ind_min][1]; //int_point_real_z[Idx] = int_pointS[ind_min][2]; int_point_real[Idx][0] = int_pointS[ind_min][0]; int_point_real[Idx][1] = int_pointS[ind_min][1]; int_point_real[Idx][2] = int_pointS[ind_min][2]; } \"\"\") origin = np.asarray([1, 1, 1]).astype(np.float32) direction = np.ones((100, 3)).astype(np.float32) int_point_real = np.zeros((100, 3)).astype(np.float32) intersect = mod.get_function(\"intersect\") intersect(drv.In(origin), drv.In(direction), drv.In(v0), drv.In(v1), drv.In(v2), drv.Out(int_point_real), block=(512,1,1), grid=(64,1,1)) My idea is to run the 20k rays in parallel. This Python script is giving me different errors: kernel.cu(18): error: expression must have pointer-to-object type kernel.cu(18): error: expression must have pointer-to-object type kernel.cu(18): error: expression must have pointer-to-object type kernel.cu(34): error: expression must have pointer-to-object type kernel.cu(35): error: expression must have pointer-to-object type kernel.cu(36): error: expression must have pointer-to-object type kernel.cu(108): error: expression must have class type kernel.cu(118): error: expression must have pointer-to-object type kernel.cu(119): error: expression must have pointer-to-object type kernel.cu(120): error: expression must have pointer-to-object type kernel.cu(27): warning: variable \"int_faces\" was set but never used 10 errors detected in the compilation of \"C:/Users/20180781/AppData/Local/Temp/tmpxft_00000d44_00000000-10_kernel.cpp1.ii\". ] Any idea why? Does anyone know a smarter and more efficient way to calculate the intersection point when I have a lot of rays and a lot of faces?",
        "answers": [
            [
                "It seems all the errors are reported for lines where you try to double-index things. The line numbering is a little off, but from the warning kernel.cu(27): warning: variable \"int_faces\" was set but never used it can be deduced that the first few error messages refer to the following lines: float dir_current[3] = {dir[idx][0],dir[idx][1],dir[idx][2]}; [...] v0_current[j] = v0[i][j]; v1_current[j] = v1[i][j]; v2_current[j] = v2[i][j]; And it makes sense, because dir, v0, v1, and v2 are all defined as float *. Which is just a float pointer. You can index it once like dir[idx], yielding a float, but it doesn't make sense to index it (a mere float) again, like dir[idx][0]. After this point point, the line numbers are off again, but accepting my hypothesis, it can be assumed that these are the last 3 problematic lines: int_point_real[Idx][0] = int_pointS[ind_min][0]; int_point_real[Idx][1] = int_pointS[ind_min][1]; int_point_real[Idx][2] = int_pointS[ind_min][2]; And indeed, int_point_real is also just a pointer to a float. Also, notice how although int_pointS is referenced in several other lines, there are no errors reported for them, because that variable is correctly declared as a two-dimensional array (which can be indexed twice)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I've installed PyCUDA using pip. I tried this in two computers. One with a fresh install of Python 3.7.1 and one with Python 3.6.5. Everything fails after using PuCUDA with no error message. The minimum example is this: import sys import pycuda.driver as cuda import pycuda.autoinit # &lt;-- Comment in order for `print` to work if __name__ == '__main__': print('Print works') sys.stdout.write(\"Sys print works\") This doesn't print anything unless I remove pycuda.autoinit. Another example would be using printf: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule if __name__ == '__main__': mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void test() { printf(\"I am %d.%d\\\\n\", threadIdx.x, threadIdx.y); } \"\"\") func = mod.get_function(\"test\") func(block=(4, 4, 1)) This does not return any output also. I think that CUDA fails but nothing gets reported. My configuration: +--------------------+--------------------+ | PC1 | PC2 | +--------------------+--------------------+ | Python 3.6.5 | Python 3.7.1 | | Windows 10 | Windows 10 | | Cuda toolkit 9 | Cuda toolkit 10 | | GeForce GTX 1050 | GeForce GTX 1080 | | Visual Studio 2015 | Visual Studio 2015 | +--------------------+--------------------+ Drivers: GeForce Game Ready Driver Version : 418.91 WHQL Release Date : Wed Feb 13, 2019 I've noticed that this is a common problem but there is no solution.",
        "answers": [
            [
                "I'm not sure if this would help you out, but similar issue was solved by reinstall cuda(with anoconda). In case you need it, you can install it via conda console conda install -c anaconda cudatoolkit And to check if it runs well, type numba -s Hope this can help"
            ],
            [
                "I'm not sure but I hope this help you address the problem. Check whether you've installed CUDA toolkit on your Windows. Check whether PATH environments for CUDA is set properly. If you are using the latest version of VS, it may be difficult(or impossible) for PyCUDA to work with it. So if you want to install an older version of VS additionally on your current system, after your installation, you may need to specify the path to your new (old-versioned) compiler. For example, you can add this line into compiler function in site-packages\\pycuda\\compiler.py: options.extend(['-ccbin', 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\bin\\\\cl.exe'])"
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I want to parallelize my Python code and I'm trying to use PyCuda. What I saw so far is that you have to write a \"Kernel\" in C into your Python code. This Kernel is what is going to be parallelized. Am I right? Example (doubling an array of random numbers, from https://documen.tician.de/pycuda/tutorial.html): import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.random.randn(4, 4) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) # Kernel: mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") func = mod.get_function(\"doublify\") func(a_gpu, block=(4, 4, 1)) a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print(a_doubled) print(a) The point is that my Python code has classes and other things all suitable with Python and unsuitable with C (i.e. untranslatable to C). Let me clarify: my has 256 independent for-loops that I want to parallelize. These loops contain Python code that can\u2019t be translated to C. How can I parallelize an actual Python code with PyCuda without translating my code to C?",
        "answers": [
            [
                "You can't. PyCUDA doesn't support device side python, all device code must be written in the CUDA C dialect. Numba includes a direct Python compiler which can allow an extremely limited subset of Python language features to be compiled and run directly on the GPU. This does not include access to any Python libraries such as numpy, scipy, etc."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Code as below: cuda.init() dev = cuda.Device(0) context = dev.make_context() context.push() tfG = mod.get_texref(\"tfG\") tfG.set_array(cuda.np_to_array(Gs.astype(np.float32),order='C')) context.pop() gives me error message: --------------------------------------------------------------------------- LogicError Traceback (most recent call last) &lt;ipython-input-41-92ff15f5e108&gt; in &lt;module&gt;() 1 Gs = np.arange(100).reshape([10,10]) 2 context.push() ----&gt; 3 tfG.set_array(cuda.np_to_array(Gs.astype(np.float32),order='C')) 4 context.pop() LogicError: cuTexRefSetArray failed: peer access has not been enabled I read a similar thread but didn't solve this erroer P.S. with auto init, it will works: import pycuda.autoinit tfG = mod.get_texref(\"tfG\") Gs = np.arange(100).reshape([10,10]) tfG.set_array(cuda.np_to_array(Gs.astype(np.float32),order='C')) But my problem is that I want to use mpi for my program eventually so I need to figure out why it doesn't work.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am trying to pass output of some pycuda operation to the input of mxnet computational graph. I am able to achieve this via numpy conversion with the following code import pycuda.driver as cuda import pycuda.autoinit import numpy as np import mxnet as mx batch_shape = (1, 1, 10, 10) h_input = np.zeros(shape=batch_shape, dtype=np.float32) # init output with ones to see if contents really changed h_output = np.ones(shape=batch_shape, dtype=np.float32) device_ptr = cuda.mem_alloc(input.nbytes) stream = cuda.Stream() cuda.memcpy_htod_async(d_input, h_input, stream) # here some actions with d_input may be performed, e.g. kernel calls # but for the sake of simplicity we'll just transfer it back to host cuda.memcpy_dtoh_async(d_input, h_output, stream) stream.synchronize() mx_input = mx.nd(h_output, ctx=mx.gpu(0)) print('output after pycuda calls: ', h_output) print('mx_input: ', mx_input) However i would like to avoid the overhead of device-to-host and host-to-device memory copying. I couldn't find a way to construct mxnet.ndarray.NDArray directly from h_output. The closest thing that i was able to find is construction of ndarray from dlpack. But it is not clear how to work with dlpack object from python. Is there a way fo achieve NDArray &lt;-&gt; pycuda interoperability without copying memory via host?",
        "answers": [
            [
                "Unfortunately, it is not possible at the moment."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm using anaconda to regulate my environment, for a project i have to use my GPU for network training. I use pytorch for my project and i'm trying to get CUDA working. I installed cudatoolkit, numba, cudnn still, when i try this command: torch.cuda.is_available() I get \"False\" as output. This is my environment: # Name Version Build Channel blas 1.0 mkl bzip2 1.0.6 h470a237_2 conda-forge ca-certificates 2018.03.07 0 cairo 1.14.12 he6fea26_5 conda-forge certifi 2018.8.24 py35_1 cffi 1.11.5 py35he75722e_1 cloudpickle 0.5.5 py35_0 cudatoolkit 9.2 0 anaconda cudnn 7.2.1 cuda9.2_0 anaconda cycler 0.10.0 py_1 conda-forge cython 0.28.5 py35hf484d3e_0 anaconda dask-core 0.19.2 py35_0 dbus 1.13.0 h3a4f0e9_0 conda-forge decorator 4.3.0 py35_0 expat 2.2.5 hfc679d8_2 conda-forge ffmpeg 4.0.2 ha0c5888_1 conda-forge fontconfig 2.13.1 h65d0f4c_0 conda-forge freetype 2.9.1 h6debe1e_4 conda-forge gettext 0.19.8.1 h5e8e0c9_1 conda-forge giflib 5.1.4 h470a237_1 conda-forge glib 2.55.0 h464dc38_2 conda-forge gmp 6.1.2 hfc679d8_0 conda-forge gnutls 3.5.19 h2a4e5f8_1 conda-forge graphite2 1.3.12 hfc679d8_1 conda-forge gst-plugins-base 1.12.5 hde13a9d_0 conda-forge gstreamer 1.12.5 h61a6719_0 conda-forge harfbuzz 1.9.0 h08d66d9_0 conda-forge hdf5 1.10.2 hc401514_2 conda-forge icu 58.2 hfc679d8_0 conda-forge imageio 2.4.1 py35_0 intel-openmp 2019.0 118 jasper 1.900.1 hff1ad4c_5 conda-forge jpeg 9c h470a237_1 conda-forge kiwisolver 1.0.1 py35h2d50403_2 conda-forge libedit 3.1.20170329 h6b74fdf_2 libffi 3.2.1 hd88cf55_4 libgcc-ng 8.2.0 hdf63c60_1 libgfortran 3.0.0 1 conda-forge libgfortran-ng 7.3.0 hdf63c60_0 libiconv 1.15 h470a237_3 conda-forge libopenblas 0.3.3 h5a2b251_3 libpng 1.6.35 ha92aebf_2 conda-forge libstdcxx-ng 8.2.0 hdf63c60_1 libtiff 4.0.9 he6b73bb_2 conda-forge libuuid 2.32.1 h470a237_2 conda-forge libwebp 0.5.2 7 conda-forge libxcb 1.13 h470a237_2 conda-forge libxml2 2.9.8 h422b904_5 conda-forge llvmlite 0.24.0 py35hdbcaa40_0 matplotlib 3.0.0 py35h0b34cb6_1 conda-forge mkl 2019.0 118 mkl_fft 1.0.6 py35_0 conda-forge mkl_random 1.0.1 py35_0 conda-forge ncurses 6.1 hf484d3e_0 nettle 3.3 0 conda-forge networkx 2.1 py35_0 ninja 1.8.2 py35h6bb024c_1 numba 0.39.0 py35h04863e7_0 numpy 1.15.2 py35h1d66e8a_0 numpy-base 1.15.2 py35h81de0dd_0 olefile 0.46 py35_0 openblas 0.2.20 8 conda-forge opencv 3.4.1 py35h6fd60c2_1 opencv-python 3.4.3.18 &lt;pip&gt; openh264 1.7.0 0 conda-forge openssl 1.0.2p h14c3975_0 pandas 0.23.4 py35h04863e7_0 pcre 8.41 hfc679d8_3 conda-forge pillow 5.2.0 py35heded4f4_0 Pillow 5.3.0 &lt;pip&gt; pip 10.0.1 py35_0 pixman 0.34.0 h470a237_3 conda-forge pthread-stubs 0.4 h470a237_1 conda-forge pycparser 2.19 py35_0 pyparsing 2.2.2 py_0 conda-forge pyqt 5.6.0 py35h8210e8a_7 conda-forge python 3.5.6 hc3d631a_0 python-dateutil 2.7.3 py_0 conda-forge pytorch 0.4.1 py35_py27__9.0.176_7.1.2_2 pytorch pytz 2018.5 py35_0 pywavelets 1.0.0 py35hdd07704_0 qt 5.6.2 hf70d934_9 conda-forge readline 7.0 h7b6447c_5 scikit-image 0.14.0 py35hf484d3e_1 scipy 1.1.0 py35hfa4b5c9_1 setuptools 40.2.0 py35_0 sip 4.18.1 py35hfc679d8_0 conda-forge six 1.11.0 py35_1 conda-forge sqlite 3.25.2 h7b6447c_0 tk 8.6.8 hbc83047_0 toolz 0.9.0 py35_0 torchvision 0.1.9 py35h72e4c6f_1 soumith tornado 5.1.1 py35h470a237_0 conda-forge wheel 0.31.1 py35_0 x264 1!152.20180717 h470a237_1 conda-forge xorg-kbproto 1.0.7 h470a237_2 conda-forge xorg-libice 1.0.9 h470a237_4 conda-forge xorg-libsm 1.2.2 h8c8a85c_6 conda-forge xorg-libx11 1.6.6 h470a237_0 conda-forge xorg-libxau 1.0.8 h470a237_6 conda-forge xorg-libxdmcp 1.1.2 h470a237_7 conda-forge xorg-libxext 1.3.3 h470a237_4 conda-forge xorg-libxrender 0.9.10 h470a237_2 conda-forge xorg-renderproto 0.11.1 h470a237_2 conda-forge xorg-xextproto 7.3.0 h470a237_2 conda-forge xorg-xproto 7.0.31 h470a237_7 conda-forge xz 5.2.4 h14c3975_4 zlib 1.2.11 ha838bed_2 My desktop has a NVIDIA GeForce GTX 970 (so it is cuda available) Also for some reason, as you can see here: My graphics card doesn't show, however when using the lspci -v command, i can see my graphics card there. Don't know if that has something to do with it. Anyone knows how i can fix this?",
        "answers": [
            [
                "You need to install pytorch \"in one go\" using https://pytorch.org/get-started/locally/ to construct the anaconda command. With the standard configuration in anaconda, you get: conda install pytorch torchvision cudatoolkit=10.2 -c pytorch (Please always check on https://pytorch.org/get-started/locally/ whether this command is still up to date.) You seem to need the right cuda version 10.2 package to be aligned with what pytorch can handle. This is what is meant with @RussellGallop's helpful message. We can see that installing pytorch and cuda separately is not recommended, and that Anaconda installation is recommended, against your answer: Anaconda is our recommended package manager since it installs all dependencies. Uninstall and install better than repair In case of problems, you better uninstall all covered packages and apply https://pytorch.org/get-started/locally/ to get the command again, instead of trying to fix it with separate installations. Thus, if you want to uninstall, you need to use the exactly same command of the installation, but with \"conda uninstall\" instead. For the example above, the uninstall command would be: conda uninstall pytorch torchvision cudatoolkit=10.2 -c pytorch This needed uninstall \"in one go\" again is another hint at the sensitive installation of pytorch, and that separate installation is risky. See How can l uninstall PyTorch?)"
            ],
            [
                "Your CUDA install needs to be compatible with your NVidia drivers (which aren't installed by Anaconda). I had a similar thing with Anaconda on Windows. I ignored the torch.cuda.is_available() check and tried to send a tensor to the GPU with: device = torch.device(\"cuda\") torch.Tensor(1).to(device) This gave a more helpful message: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
            ],
            [
                "FIXED: I installed CUDA seperately, not through anaconda and now it works. If anyone knows why it doesn't work when installing cuda in anaconda, feel free to answer"
            ]
        ],
        "votes": [
            9.0000001,
            3.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am trying to install pycuda in computer with Windows 10 64bits, I installed the GPU Toolkit 9.1 and Anaconda 4.2 with python 3.5 64bits. I installed pycuda using the precompiled package: pycuda\u20112017.1.1+cuda9185\u2011cp35\u2011cp35m\u2011win_amd64.whl The installation in my Anaconda installation didn't mark any error, but when I try to run a simple example I have problems with the import: Python 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul 5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import pycuda.driver as cuda RuntimeError: module compiled against API version 0xb but this version of numpy is 0xa ImportError: numpy.core.multiarray failed to import During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\pycuda\\driver.py\", line 5, in &lt;module&gt; from pycuda._driver import * # noqa SystemError: &lt;class 'ImportError'&gt; returned a result with an error set Any idea?? UPDATE: I resolve my first problem adding the lib directory of the cuda library to my path: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\lib But now I have this error: Python 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul 5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import pycuda.driver as cuda &gt;&gt;&gt; import pycuda.autoinit Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\pycuda\\autoinit.py\", line 5, in &lt;module&gt; cuda.init() pycuda._driver.Error: cuInit failed: unknown error SOLVED I try installing Cudatoolkit 9.2, plus adding the C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\lib to path, update my nvidia driver, and install the pycuda pycuda\u20112018.1+cuda92148\u2011cp35\u2011cp35m\u2011win_amd64.whl Now my cuda codes function correctly.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a system with two GPUs, and am using Keras with Tensorflow backend. Gpu:0 is being allocated to PyCUDA, which is performing a unique operation which is fed forward to Keras, and changes with each batch. As such, I would like to run a Keras model on gpu:1 while leaving gpu:0 allocated to PyCUDA. Is there any way to do this? Looking through prior threads I've found several depreciated solutions.",
        "answers": [
            [
                "So I don't think that this feature is meaningfully implemented in Keras currently. Found a workaround that I recommend whereby you just create multiple processes using Python's default multiprocessing library. Note: Currently for this setup you need to spawn the new process, rather than fork it, to avoid a weird interaction with one of the PyCUDA backend libraries."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "can anybody help me to install pycuda on this Deep Learning Base AMI (Amazon Linux) Version 11.0, please? I use p2.xlarge type. I get this error: gcc: error trying to exec 'cc1plus': execvp: No such file or directory error: command 'gcc' failed with exit status 1 i tried to link the ccp1plus but no success: sudo ln -s /usr/libexec/gcc/x86_64-amazon-linux/4.8.5/cc1plus /usr/local/bin/ other solutions on the web can't help too. thank for your help, Looninho",
        "answers": [
            [
                "It looks like gcc is not included in that AMI. According to this StackOverflow question/answer, you may be able to just update the gcc symlink: ln /usr/bin/gcc46 /usr/bin/gcc"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am new to pycuda. In my code, each thread needs to calculate a row of a 2-D array. However, as I see in the examples, only the element-based distribution is used. How can I distribute it based on rows of the array?",
        "answers": [
            [
                ".... element-based distribution That is a very strange terminology to use, given that there is no concept of \"distribution\" at all in either CUDA or PyCUDA. In CUDA, how input data is treated by a given thread is completely at the discretion of the programmer, there are no predefined \"distributions\" of any sort. So in a standard CUDA C kernel (which is what you write in PyCUDA, it is really only an API wrapper and compilation system), you could do something like this for a row major ordered input: __global__ void kernel(float* array, int lda) { int tid = threadIdx.x + blockIdx.x * blockDim.x; int rowid = tid * lda; float* row = array + rowid; for(int col=0; col&lt;lda; col++) { row[col] = ....; } } [Obviously never compiled or tested, use at own risk] The setup code leaves row as a pointer to the first element of a given row of the input array which has a leading dimension of lda. Obviously, the code will change for column major storage, I leave that as an exercise for the reader."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am experimenting how to use cuda inside numba. However I have encountered something different from my expectation. Here is my code from numba import cuda @cuda.jit def matmul(A, B, C): \"\"\"Perform square matrix multiplication of C = A * B \"\"\" d=cuda.local.array((3,3),dtype=numba.float64) i, j = cuda.grid(2) if i &lt; C.shape[0] and j &lt; C.shape[1]: tmp = 0. for k in range(A.shape[1]): tmp += A[i, k] * B[k, j] C[i, j] = tmp This is the matrix function I self defined for testing using numba.cuda. Before running the tests, I also loaded the arrays in the following code: import numpy as np a=np.random.rand(2000,2000) b=np.random.rand(2000,2000) c=np.empty((2000,2000)) a1=cuda.to_device(a) b1=cuda.to_device(b) c1=cuda.to_device(c) Then I used the following code for experiment: from time import time count =0 start=time() for i in range(2000): matmul[(256,256),(16,16)](a1,b1,c1) count +=1 print(count) The for loops ran very fast for the first 1028 runs. However it ran very slow after the 1028th.What exactly caused this and how do I fix it. I am running on win10 by the way. Here is my cuda information called from numba.cuda from numba import cuda gpu = cuda.get_current_device() print(\"name = %s\" % gpu.name) print(\"maxThreadsPerBlock = %s\" % str(gpu.MAX_THREADS_PER_BLOCK)) print(\"maxBlockDimX = %s\" % str(gpu.MAX_BLOCK_DIM_X)) print(\"maxBlockDimY = %s\" % str(gpu.MAX_BLOCK_DIM_Y)) print(\"maxBlockDimZ = %s\" % str(gpu.MAX_BLOCK_DIM_Z)) print(\"maxGridDimX = %s\" % str(gpu.MAX_GRID_DIM_X)) print(\"maxGridDimY = %s\" % str(gpu.MAX_GRID_DIM_Y)) print(\"maxGridDimZ = %s\" % str(gpu.MAX_GRID_DIM_Z)) print(\"maxSharedMemoryPerBlock = %s\" % str(gpu.MAX_SHARED_MEMORY_PER_BLOCK)) print(\"asyncEngineCount = %s\" % str(gpu.ASYNC_ENGINE_COUNT)) print(\"canMapHostMemory = %s\" % str(gpu.CAN_MAP_HOST_MEMORY)) print(\"multiProcessorCount = %s\" % str(gpu.MULTIPROCESSOR_COUNT)) print(\"warpSize = %s\" % str(gpu.WARP_SIZE)) print(\"unifiedAddressing = %s\" % str(gpu.UNIFIED_ADDRESSING)) print(\"pciBusID = %s\" % str(gpu.PCI_BUS_ID)) print(\"pciDeviceID = %s\" % str(gpu.PCI_DEVICE_ID)) and the output is: name = b'GeForce GTX 1050 Ti' maxThreadsPerBlock = 1024 maxBlockDimX = 1024 maxBlockDimY = 1024 maxBlockDimZ = 64 maxGridDimX = 2147483647 maxGridDimY = 65535 maxGridDimZ = 65535 maxSharedMemoryPerBlock = 49152 asyncEngineCount = 2 canMapHostMemory = 1 multiProcessorCount = 6 warpSize = 32 unifiedAddressing = 1 pciBusID = 3 pciDeviceID = 0",
        "answers": [
            [
                "This is caused by the asynchronous launch queue associated with GPU kernel launches. When you tell numba to submit a GPU kernel: matmul[(256,256),(16,16)](a1,b1,c1) This request goes into a queue, and the CPU thread (i.e. python) that issued that kernel call can continue, even though the GPU kernel has not completed or even started yet. The CUDA runtime queues up these requests and issues them as the GPU is ready for more work. What you are witnessing initially during the very fast incrementing of your for-loop is the queue filling up with work requests. That is not representative of the actual time that the GPU requires to perform the work. Eventually the queue fills, and the CUDA runtime halts the CPU thread (i.e. python) at the point of kernel launch, until a queue slot opens up. At that point, the for-loop is allowed to proceed for one more iteration. It is at this point (perhaps around 1028 iterations) that you start to see a \"slow down\". Thereafter, the for-loop proceeds at approximately the rate at which GPU kernels are executed and removed from the processing queue. There is nothing to fix here; this is expected behavior. If you want the for-loop to proceed only at the rate at which GPU kernels are actually executed, then you should insert a synchronizing function in your for-loop. For example, numba provides numba.cuda.synchronize() So if you modify your for-loop as follows: for i in range(2000): matmul[(256,256),(16,16)](a1,b1,c1) cuda.synchronize() count +=1 print(count) You will see the for-loop proceed at the actual rate of GPU work completion, instead of the \"queue-filling\" rate."
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "when I install pycuda by this instruction: pip install pycuda but there is an error: src/cpp/cuda.hpp:14:10: fatal error: cuda.h: No such file or directory but I have installed the cuda toolkit.this is the result of nvcc -V [root@localhost include]# nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2018 NVIDIA Corporation Built on Tue_Jun_12_23:07:04_CDT_2018 Cuda compilation tools, release 9.2, V9.2.148 this is the result of install rpm downloaded in https://developer.nvidia.com/cuda-downloads [root@localhost include]# sudo dnf install cuda Last metadata expiration check: 0:05:09 ago on Wed 05 Sep 2018 10:08:35 PM EDT. Package cuda-1:9.2.148.1-2.fc28.x86_64 is already installed, skipping. Dependencies resolved. Nothing to do. Complete!",
        "answers": [
            [
                "In my case I met both issues: -lcurand not found and src/cpp/cuda.hpp:14:10: fatal error: cuda.h: No such file or directory And exporting C_INCLUDE_PATH didn't helped me. Instead I needed to export general version of C_INCLUDE_PATH -- CPATH: export CPATH=$CPATH:/usr/local/cuda/include export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/cuda/lib64"
            ],
            [
                "This is how I solved the problem on Jetson NANO: sudo pip3 install --global-option=build_ext --global-option=\"-I/usr/local/cuda-10.0/targets/aarch64-linux/include/\" --global-option=\"-L/usr/local/cuda-10.0/targets/aarch64-linux/lib/\" pycuda"
            ],
            [
                "You probably need to specify path to CUDA: export C_INCLUDE_PATH=${CUDA_HOME}/include:${C_INCLUDE_PATH} export LIBRARY_PATH=${CUDA_HOME}/lib64:$LIBRARY_PATH Please make sure that echo ${CUDA_HOME} does provide some sensible output."
            ],
            [
                "Find out where Cuda is installed on the system using find / - type d - name cuda 2&gt;/dev/null Use or which every location cuda is found on export PATH=/usr/local/cuda-VERSION/bin:$PATH Then pip install pycuda"
            ],
            [
                "Here is another solution that worked for me. This solution was taken from here. $ export PATH=/usr/local/cuda/bin:$PATH $ sudo apt-get install python-dev $ pip install numpy $ export CUDA_ROOT=/usr/local/cuda $ pip install pycuda"
            ]
        ],
        "votes": [
            16.0000001,
            15.0000001,
            5.0000001,
            4.0000001,
            2.0000001
        ]
    },
    {
        "question": "I am trying to execute the code here. I get the following error: orig: [0.36975162 0.08511397 0.16306844 0.4015488 0.25104857 0.30606773 0.24524205 0.13792656] Process Process-1: Traceback (most recent call last): File \"C:\\Program Files\\Python27\\lib\\multiprocessing\\process.py\", line 267, in _bootstrap self.run() File \"C:\\Program Files\\Python27\\lib\\multiprocessing\\process.py\", line 114, in run self._target(*self._args, **self._kwargs) File \"C:\\Users\\My\\Desktop\\test_codes\\pycuda4.py\", line 28, in func1 h = drv.mem_get_ipc_handle(x_gpu.ptr) LogicError: cuIpcGetMemHandle failed: operation not supported I am using Python 3.7, CUDA 9.2 in Windows 7 x64 environment. Is CUDA IPCMemoryHandle not supported in Windows? Or, am I missing something?",
        "answers": [
            [
                "What is documented here is that CUDA IPC functionality is only supported on Linux. However, the driver API (upon which PyCUDA is based) docs indicate: IPC functionality is restricted to devices with support for unified addressing on Linux and Windows operating systems. IPC functionality on Windows is restricted to GPUs in TCC mode Therefore if you can put your windows GPU in TCC mode (via nvidia-smi tool) then I think it should probably work/be supported. GeForce GPUs cannot be put in TCC mode. Most Titan and Quadro GPUs can be placed in TCC mode. Most Tesla GPUs on windows should automatically be in TCC mode. Note that putting your GPU in TCC mode means it cannot host a display any longer. In recent CUDA toolkits, IPC support has been added for both WDDM and TCC usage in windows (the underlying mechanisms are different). The CUDA IPC sample code demonstrates all flavors (linux, windows TCC, windows WDDM)."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am new to python and PYCUDA. In many PYCUDA examples I can find codes like this: import pycuda.driver as cuda cuda.mem_alloc(a.nbytes) But when I take a look into pycuda/driver.py, I can't find where mem_alloc is defined. I can only find mem_alloc_like, which calls mem_alloc(): def mem_alloc_like(ary): return mem_alloc(ary.nbytes) which tells me that mem_alloc must be somewhere in the libraries driver.py imports. However, the following are the imports I find in driver.py, and none of six, numpy, or sys has a function named as \"mem_alloc()\" from __future__ import absolute_import from __future__ import print_function import six try: from pycuda._driver import * # noqa except ImportError as e: if \"_v2\" in str(e): from warnings import warn warn(\"Failed to import the CUDA driver interface, with an error \" \"message indicating that the version of your CUDA header \" \"does not match the version of your CUDA driver.\") raise import numpy as np import sys",
        "answers": [
            [
                "pyCUDA is wrapper around CUDA driver API. This is how pyCUDA exposes those APIs write wrapper around driver API in C / C++ Expose those function to python using Boost::python because python's interpreter is implemented in C(Cpython) Compile those wrapper into a shared library that is _driver.so import the shared library that is the from pycuda._driver import *. But when I take a look into pycuda/driver.py, I can't find where mem_alloc is defined. It is defined in pycuda/src/cpp/cuda.hpp"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to learn CUDA and I'm a bit confused about calculating thread indices. Let's say I have this loop I'm trying to parallelize: ... for(int x = 0; x &lt; DIM_x; x++){ for(int y = 0; y &lt; DIM_y; y++){ for(int dx = 0; dx &lt; psize; dx++){ array[y*DIM_x + x + dx] += 1; } } } In PyCUDA, I set: block = (8, 8, 8) grid = (96, 96, 16) Most of the examples I've seen for parallelizing loops calculate thread indices like this: int x = blockIdx.x * blockDim.x + threadIdx.x; int y = blockIdx.y * blockDim.y + threadIdx.y; int dx = blockIdx.z * blockDim.z + threadIdx.z; if (x &gt;= DIM_x || y &gt;= DIM_y || dx &gt;= psize) return; atomicAdd(&amp;array[y*DIM_x + x + dx], 1) DIM_x = 580, DIM_y = 550, psize = 50 However, if I print x, I see that multiple threads with the same thread Id are created, and the final result is wrong. Instead, if I use this (3D grid of 3D blocks): int blockId = blockIdx.x + blockIdx.y * gridDim.x + gridDim.x * gridDim.y * blockIdx.z; int x = blockId * (blockDim.x * blockDim.y * blockDim.z) + (threadIdx.z * (blockDim.x * blockDim.y)) + (threadIdx.y * blockDim.x) + threadIdx.x; It fixes the multiple same thread Ids problem for x, but I'm not sure how I'd parallelize y and dx. If anyone could help me understand where I'm going wrong, and show me the right way to parallelize the loops, I'd really appreciate it.",
        "answers": [
            [
                "However, if I print x, I see that multiple threads with the same thread Id are created, and the final result is wrong. It would be normal for you to see multiple threads with the same x thread ID in a multi-dimensional grid, as it would also be normal to observe many iterations of the loops in your host code with the same x value. If the result is wrong, it has nothing to do with any of the code you have shown, viz: #include &lt;vector&gt; #include &lt;thrust/device_vector.h&gt; #include &lt;thrust/copy.h&gt; #include &lt;assert.h&gt; void host(int* array, int DIM_x, int DIM_y, int psize) { for(int x = 0; x &lt; DIM_x; x++){ for(int y = 0; y &lt; DIM_y; y++){ for(int dx = 0; dx &lt; psize; dx++){ array[y*DIM_x + x + dx] += 1; } } } } __global__ void kernel(int* array, int DIM_x, int DIM_y, int psize) { int x = blockIdx.x * blockDim.x + threadIdx.x; int y = blockIdx.y * blockDim.y + threadIdx.y; int dx = blockIdx.z * blockDim.z + threadIdx.z; if (x &gt;= DIM_x || y &gt;= DIM_y || dx &gt;= psize) return; atomicAdd(&amp;array[y*DIM_x + x + dx], 1); } int main() { dim3 block(8, 8, 8); dim3 grid(96, 96, 16); int DIM_x = 580, DIM_y = 550, psize = 50; std::vector&lt;int&gt; array_h(DIM_x * DIM_y * psize, 0); std::vector&lt;int&gt; array_hd(DIM_x * DIM_y * psize, 0); thrust::device_vector&lt;int&gt; array_d(DIM_x * DIM_y * psize, 0); kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(thrust::raw_pointer_cast(array_d.data()), DIM_x, DIM_y, psize); host(&amp;array_h[0], DIM_x, DIM_y, psize); thrust::copy(array_d.begin(), array_d.end(), array_hd.begin()); cudaDeviceSynchronize(); for(int i=0; i&lt;DIM_x * DIM_y * psize; i++) { assert( array_h[i] == array_hd[i] ); } return 0; } which when compiled and run $ nvcc -arch=sm_52 -std=c++11 -o looploop loop_the_loop.cu $ cuda-memcheck ./looploop ========= CUDA-MEMCHECK ========= ERROR SUMMARY: 0 errors emits no errors and passes the check of all elements against the host code in your question. If you are getting incorrect results, it is likely that you have a problem with initialization of the device memory before running the kernel. Otherwise I fail to see how incorrect results could be emitted by the code you have shown. In general, performing a large number of atomic memory transactions, as your code does, is not the optimal way to perform computation on the GPU. Using non-atomic transactions would probably need to rely on other a priori information about the structure of the problem (such as a graph decomposition or a precise description of the write patterns of the problem)."
            ],
            [
                "In a 3D grid with 3D blocks, the thread ID is: unsigned long blockId = blockIdx.x + blockIdx.y * gridDim.x + gridDim.x * gridDim.y * blockIdx.z; unsigned long threadId = blockId * (blockDim.x * blockDim.y * blockDim.z) + (threadIdx.z * (blockDim.x * blockDim.y)) + (threadIdx.y * blockDim.x) + threadIdx.x; Not the x you computed. The x is only the x index of that 3D matrix. There is a nice cheatsheet in this blog"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "After running the Visual Profiler, guided analysis tells me that I'm memory-bound, and that in particular my shared memory accesses are poorly aligned/accessed - basically every line I access shared memory is marked as ~2 transactions per access. However, I couldn't figure out why that was the case (my shared memory is padded/strided so that there shouldn't be bank conflicts), so I went back and checked the shared replay metric - and that says that only 0.004% of shared accesses are replayed. So, what's going on here, and what should I be looking at to speed up my kernel? EDIT: Minimal reproduction: import numpy as np import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.gpuarray as gp mod = SourceModule(\"\"\" (splitting the code block to get both Python and CUDA/C++ coloring) typedef unsigned char ubyte; __global__ void identity(ubyte *arr, int stride) { const int dim2 = 16; const int dim1 = 64; const int dim0 = 33; int shrstrd1 = dim2; int shrstrd0 = dim1 * dim2; __shared__ ubyte shrarr[dim0 * dim1 * dim2]; auto shrget = [shrstrd0, shrstrd1, &amp;shrarr](int i, int j, int k) -&gt; int{ return shrarr[i * shrstrd0 + j * shrstrd1 + k]; }; auto shrset = [shrstrd0, shrstrd1, &amp;shrarr](int i, int j, int k, ubyte val) -&gt; void { shrarr[i * shrstrd0 + j * shrstrd1 + k] = val; }; int in_x = threadIdx.x; int in_y = threadIdx.y; shrset(in_y, in_x, 0, arr[in_y * stride + in_x]); arr[in_y * stride + in_x] = shrget(in_y, in_x, 0); } \"\"\", (ditto) options=['-std=c++11']) #Equivalent to identity&lt;&lt;&lt;1, dim3(32, 32, 1)&gt;&gt;&gt;(arr, 64); identity = mod.get_function(\"identity\") identity(gp.zeros((64, 64), np.ubyte), np.int32(64), block=(32, 32, 1)) 2 transactions per access, shared replay overhead 0.083. Decreasing dim2 to 8 makes the problem go away, which I also don't understand.",
        "answers": [
            [
                "Partial answer: I had a fundamental misunderstanding of how shared memory banks worked (namely, that they are banks of around a thousand byte-banks each) and so didn't realize that they looped around, so that too much padding meant that 32 row elements might end up using each bank more than once. Presumably, though, that conflict just didn't come up every time - instead it came up, oh, about 85 times a block, from the numbers. I'll leave this here for a day in hopes of a more complete explanation, then close and accept this answer."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a linear int array arr, which is on CUDA global memory. I want to set sub-arrays of arr to defined values. The sub-array start indexes are given by the starts array, while the length of each sub-array is given in counts array. What I want to do is to set the value of sub-array i starting from starts[i] and continuing upto counts[i] to the value starts[i]. That is, the operation is: arr[starts[i]: starts[i]+counts[i]] = starts[i] I thought of using memset() in the kernel for setting the values. However, it is not getting correctly written ( the array elements are being assigned some random values). The code I am using is: #include &lt;stdlib.h&gt; __global__ void kern(int* starts,int* counts, int* arr,int* numels) { unsigned int idx = threadIdx.x + blockIdx.x*blockDim.x; if (idx&gt;=numels[0]) return; const int val = starts[idx]; memset(&amp;arr[val], val, sizeof(arr[0])*counts[idx]) ; __syncthreads(); } Please note that numels[0] contains the number of elements in starts array. I have checked the code with cuda-memcheck() but didn't get any errors. I am using PyCUDA, if it's relevant. I am probably misunderstanding the usage of memset here, as I am learning CUDA. Can you please suggest a way to correct this? Or other efficient way of doint this operation. P.S: I know that thrust::fill() can probably do this well, but since I am learning CUDA, I would like to know how to do this without using external libraries.",
        "answers": [
            [
                "The memset and memcpy implementations in CUDA device code emit simple, serial, byte values operations (and note that memset can't set anything other than byte values, which might be contributing to the problem you see if the values you are trying to set are larger than 8 bits). You could replace the memset call with something like this: const int val = starts[idx]; //memset(&amp;arr[val], val, sizeof(arr[0])*counts[idx]) ; for(int i = 0; i &lt; counts[idx]; i++) arr[val + i] = val; The performance of that code will probably be better than the built-in memset. Note also that the __syncthreads() call at the end of your kernel is both unnecessary, and a potential source of deadlock and should be removed. See here for more information."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm working with PyTorch and want to do some arithmetic on Tensor data with the help of PyCUDA. I can get a memory address of a cuda tensor t via t.data_ptr(). Can I somehow use this address and my knowledge of the size and data type to initialize a GPUArray? I am hoping to avoid copying the data, but that would also be an alternative.",
        "answers": [
            [
                "It turns out this is possible. We need a pointer do the data, which needs some additional capabilities: class Holder(PointerHolderBase): def __init__(self, tensor): super().__init__() self.tensor = tensor self.gpudata = tensor.data_ptr() def get_pointer(self): return self.tensor.data_ptr() def __int__(self): return self.__index__() # without an __index__ method, arithmetic calls to the GPUArray backed by this pointer fail # not sure why, this needs to return some integer, apparently def __index__(self): return self.gpudata We can then use this class to instantiate GPUArrays. The code uses Reikna arrays which are a subclass but should work with pycuda arrays as well. def tensor_to_gpuarray(tensor, context=pycuda.autoinit.context): '''Convert a :class:`torch.Tensor` to a :class:`pycuda.gpuarray.GPUArray`. The underlying storage will be shared, so that modifications to the array will reflect in the tensor object. Parameters ---------- tensor : torch.Tensor Returns ------- pycuda.gpuarray.GPUArray Raises ------ ValueError If the ``tensor`` does not live on the gpu ''' if not tensor.is_cuda: raise ValueError('Cannot convert CPU tensor to GPUArray (call `cuda()` on it)') else: thread = cuda.cuda_api().Thread(context) return reikna.cluda.cuda.Array(thread, tensor.shape, dtype=torch_dtype_to_numpy(tensor.dtype), base_data=Holder(tensor)) We can go back with this code. I have not found a way to do this without copying the data. def gpuarray_to_tensor(gpuarray, context=pycuda.autoinit.context): '''Convert a :class:`pycuda.gpuarray.GPUArray` to a :class:`torch.Tensor`. The underlying storage will NOT be shared, since a new copy must be allocated. Parameters ---------- gpuarray : pycuda.gpuarray.GPUArray Returns ------- torch.Tensor ''' shape = gpuarray.shape dtype = gpuarray.dtype out_dtype = numpy_dtype_to_torch(dtype) out = torch.zeros(shape, dtype=out_dtype).cuda() gpuarray_copy = tensor_to_gpuarray(out, context=context) byte_size = gpuarray.itemsize * gpuarray.size pycuda.driver.memcpy_dtod(gpuarray_copy.gpudata, gpuarray.gpudata, byte_size) return out Old answer from pycuda.gpuarray import GPUArray def torch_dtype_to_numpy(dtype): dtype_name = str(dtype)[6:] # remove 'torch.' return getattr(np, dtype_name) def tensor_to_gpuarray(tensor): if not tensor.is_cuda: raise ValueError('Cannot convert CPU tensor to GPUArray (call `cuda()` on it)') else: array = GPUArray(tensor.shape, dtype=torch_dtype_to_numpy(tensor.dtype), gpudata=tensor.data_ptr()) return array.copy() Unfortunately, passing an int as the gpudata keyword (or a subtype of pycuda.driver.PointerHolderBase as was suggested in the pytorch forum) seems to work on the surface, but many operations fail with seemingly unrelated errors. Copying the array seems to transform it into a useable format though. I think it is related to the fact that the gpudata member should be a pycuda.driver.DeviceAllocation object which cannot be instantiated from Python it seems. Now how to go back from the raw data to a Tensor is another matter."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "answer_array = np.zeros_like(self.redarray) answer_array_gpu = cuda.mem_alloc(answer_array.nbytes) redarray_gpu = cuda.mem_alloc(self.redcont.nbytes) greenarray_gpu = cuda.mem_alloc(self.greencont.nbytes) bluearray_gpu = cuda.mem_alloc(self.bluecont.nbytes) cuda.memcpy_htod(redarray_gpu, self.redcont) cuda.memcpy_htod(greenarray_gpu, self.greencont) cuda.memcpy_htod(bluearray_gpu, self.bluecont) cuda.memcpy_htod(answer_array_gpu, answer_array) desaturate_mod = SourceModule(\"\"\" __global__ void array_desaturation(float *a, float *b, float *c, float *d){ int index = blockIdx.x * blockDim.x + threadIdx.x; d[index] = ((a[index] + b[index] + c[index])/3); } \"\"\") func = desaturate_mod.get_function(\"array_desaturation\") func(redarray_gpu, greenarray_gpu, bluearray_gpu, answer_array_gpu, block=(self.gpu_threads, self.gpu_threads, self.blocks_to_use)) desaturated = np.empty_like(self.redarray) cuda.memcpy_dtoh(desaturated, answer_array_gpu) print(desaturated) print(\"Up to here\") I wrote this piece of code for finding the average of values on three arrays and save it in to a fourth array. The code is neither printing the result, nor the line saying \"Up to here\". What could be the error? Additional info: Redarray, greenarray and bluearray are float32 numpy arrays",
        "answers": [
            [
                "I know getting started with arrays in C, and especially in PyCUDA can be pretty tricky, it took me months to get a 2D sliding max algorithm working. In this example, you can't access array elements like you can in Python where you can just provide an index since you are passing a pointer to the memory address to the first element in each array. A useful example for how this works in C can be found here. You will also have to pass in the length for the arrays (assuming they are all equal so that we do not go out of bounds) and if they are of different lengths, all of them respectively. Hopefully, you can understand how to access your array elements via pointers in C from that link. Then @talonmies provides an nice example here for how to pass in a 2D array (this is the same as a 1D array since the 2D array gets flattened to a 1D one in memory on the GPU). However, when I was working with this, I never passed in the strides which @talonmies does, working like the TutorialsPoint tutorial says *(pointer_to_array + index) is correct. Providing a memory stride here will cause you to go out of bounds. Therefore my code for this would look more like: C_Code = \"\"\" __global__ void array_desaturation(float *array_A, float *array_B, float *array_C, float *outputArray, int arrayLengths){ int index = blockIdx.x * blockDim.x + threadIdx.x; if(index &gt;= arrayLengths){ // In case our threads created would be outwise out of the bounds for our memory, if we did we would have some serious unpredictable problems return; } // These variables will get the correct values from the arrays at the appropriate index relative to their unique memory addresses (You could leave this part out but I like the legibility) float aValue = *(array_A + index); float bValue = *(array_B + index); float cValue = *(array_C + index); *(outputArray + index) = ((aValue + bValue + cValue)/3); //Set the (output arrays's pointer + offset)'s value to our average value }\"\"\" desaturate_mod = SourceModule(C_Code) desaturate_kernel = desaturate_mod.get_function(\"array_desaturation\") desaturate_kernel(cuda.In(array_A), # Input cuda.In(array_B), # Input cuda.In(array_C), # Input cuda.Out(outputArray), # Output numpy.int32(len(array_A)), # Array Size if they are all the same length block=(blockSize[0],blockSize[1],1), # However you want for the next to parameters but change your index accordingly grid=(gridSize[0],gridSize[1],1) ) print(outputArray) # Done! Make sure you have defined all these arrays before ofc"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to install pycuda on google colab. I tried pip install pycuda but it gives WARNING: nvcc not in path. May need to set CUDA_INC_DIR for installation to succeed After reading some blog, I also did !export PATH=/usr/local/cuda/bin:$PATH and !export CUDA_ROOT=/usr/local/cuda but still it gives the same error. Also, while terminating installation it also displays In file included from src/cpp/cuda.cpp:1:0: src/cpp/cuda.hpp:14:10: fatal error: cuda.h: No such file or directory #include &lt;cuda.h&gt; ^~~~~~~~ compilation terminated. error: command 'x86_64-linux-gnu-gcc' failed with exit status 1",
        "answers": [
            [
                "This script installs the packages that are needed for pycuda too: !curl https://colab.chainer.org/install | sh -"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "in desaturate_image redarray_gpu = cuda.mem_alloc(self.redarray.nbytes) pycuda._driver.LogicError: cuMemAlloc failed: initialization error I get the above error on this line: redarray_gpu = cuda.mem_alloc(self.redarray.nbytes) What could be the reason?",
        "answers": [
            [
                "import pycuda.autoinit This import statement fixed the issue. My best guess is that the device was not initialized"
            ]
        ],
        "votes": [
            10.0000001
        ]
    },
    {
        "question": "error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\BIN\\\\x86_amd64\\\\cl.exe' failed with exit status 2 I'm trying to install PyCuda package for my program and I get the above error. The IDE I use is Pycharm. My pip version is 10.0.1. What could be the possible issue?",
        "answers": [
            [
                "From pycharm, goto settings -&gt; project Interpreter Click on + button on top right corner and you will get pop-up window of Available packages. Then search for pycuda python package. Then click on Install package to install the package."
            ],
            [
                "I solved it using these steps: Install Cuda Toolkit from NVIDIA website Open This link and download appropriate PyCuda whl file for your python version and system Install the downloaded package using Anaconda Open PyCharm and install the package using Interpreter settings Enjoy PyCuda Thanks"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am trying this code on my spyder(python3). import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1)) print (dest-a*b) But couldn't run the code. ExecError: error invoking 'nvcc --version': [Errno 2] No such file or directory: 'nvcc': 'nvcc' My nvcc -- version is nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Tue_Jan_10_13:22:03_CST_2017 Cuda compilation tools, release 8.0, V8.0.61 which nvcc /usr/local/cuda-8.0/bin/nvcc which spyder /home/anaconda3/bin/spyder $ which conda /home/anaconda3/bin/conda Device query : ./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"GeForce 940MX\" CUDA Driver Version / Runtime Version 9.0 / 8.0 echo $PATH /home/anaconda3/bin:/usr/local/cuda-8.0/bin:/home/bin:/home/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin I have tried running the code on terminal. Then its running , but from the sourceModule line itself it shows errors. \"/home/anaconda3/lib/python3.6/site-packages/pycuda-2017.1-py3.6-linux-x86_64.egg/pycuda/compiler.py\", line 255, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"/home/anaconda3/lib/python3.6/site-packages/pycuda-2017.1-py3.6-linux-x86_64.egg/pycuda/compiler.py\", line 137, in compile_plain stderr=stderr.decode(\"utf-8\", \"replace\")) pycuda.driver.CompileError: nvcc compilation of /tmp/tmpbdkxu33o/kernel.cu failed [command: nvcc --cubin -arch sm_50 -I/home/anaconda3/lib/python3.6/site-packages/pycuda-2017.1-py3.6-linux-x86_64.egg/pycuda/cuda kernel.cu] [stderr: In file included from /usr/local/cuda-8.0/bin/..//include/cuda_runtime.h:78:0, from &lt;command-line&gt;:0: /usr/local/cuda-8.0/bin/..//include/host_config.h:119:2: error: #error -- unsupported GNU version! gcc versions later than 5 are not supported! #error -- unsupported GNU version! gcc versions later than 5 are not supported! ^~~~~ ] Also i have tried on my Jupiter notebook and on other Cuda installed system. But couldn't identify the problem. Any help is appreciable.",
        "answers": [
            [
                "Its working on python2 virtual environment through terminal."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This question already has an answer here: How to bind a float* array to a 1D texture in cuda? (1 answer) Closed 5 years ago. I'm new to CUDA and my goal is to implement a simple 1D interpolation using PyCUDA and CUDA 1D textures. For testing purpose I just want a kernel which returns me the original image values (extracted from the texture) in an array. the problem is that tex1D(tex, pos); returns always 0. This is my CUDA kernel code: interp1 = \"\"\" #include &lt;stdint.h&gt; texture&lt;uint8_t, 1&gt; tex; __global__ void interp1(uint8_t *out) { unsigned int pos = blockIdx.x * blockDim.x + threadIdx.x; out[pos] = tex1D(tex, pos); } \"\"\" And this is my python code snipped, where I read in an test image, allocate memory on the GPU, copy the image onto the GPU, create my texture reference via set_address and call my kernel: ... img = cv2.imread(\"lena.jpg\", 0) img_in = pycuda.driver.to_device(img.flatten()) texref.set_address(img_in, img.nbytes) texref.set_format(pycuda.driver.array_format.UNSIGNED_INT8, 1) img_out = pycuda.driver.mem_alloc(img.nbytes) interp1_func(img_out, block=(512, 1, 1), grid=(7200, 1, 1)) # image is 1920 x 1920 context.synchronize() imgnew = np.zeros_like(img.flatten()) pycuda.driver.memcpy_dtoh(imgnew, img_out) imgnew = imgnew.reshape(img.shape) ... I hope someone can help me solving this issue.",
        "answers": [
            [
                "Because you have bound linear memory to the texture reference, you must use tex1Dfetch, rather than tex1D within the kernel to access the texture. Note that in this case, it also is not possible to perform interpolation, only lookups."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to run a pyCUDA code on a flask server. The file runs correctly directly using python3 but fails when the corresponding function is called using flask. Here is the relevant code: cudaFlask.py: import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule def cudaTest(): mod = SourceModule(\"\"\" int x = 4; \"\"\") print (\"done\") return if __name__ == \"__main__\": cudaTest() server.py (only the part which calls the function): @app.route('/bundle', methods=['POST']) def bundle_edges(): cudaTest() return \"success\" On running python cudaFlask.py I get the output done as expected but on starting the server and doing POST request at website/bundle I get the following error on the flask console: pycuda._driver.LogicError: cuModuleLoadDataEx failed: invalid device context - on the line mod = SourceModule... Where am I going wrong? There is a similar question out there but it has not been answered yet.",
        "answers": [
            [
                "Solved the issue with lazy loading in flask and making the context manually (i.e. without pycuda.autoinit in PyCUDA. Refer this for lazy loading in flask. My views.py file: import numpy as np import pycuda.driver as cuda from pycuda.compiler import SourceModule def index(): cuda.init() device = cuda.Device(0) # enter your gpu id here ctx = device.make_context() mod = SourceModule(\"\"\" int x = 4; \"\"\") ctx.pop() # very important print (\"done\") return \"success\""
            ],
            [
                "PyCUDA may not be compatible with WSGI web server contexts. You could possibly make it work if you use some kind of message queue like Celery, where the HTTP request places a job on the queue and the worker on the other side of the queue runs the CUDA program. Edit: A quick and easy way would be to use Python Subprocess check_output function In the web request: subprocess.check_output(['python', 'cudaFlask.py'])"
            ],
            [
                "According to your solution I changed my code from def print_device_info(): (free,total)=drv.mem_get_info() print(\"Global memory occupancy:%f%% free\"%(free*100/total)) for devicenum in range(cuda.Device.count()): device=drv.Device(devicenum) attrs=device.get_attributes() #Beyond this point is just pretty printing print(\"\\n===Attributes for device %d\"%devicenum) for (key,value) in attrs.iteritems(): print(\"%s:%s\"%(str(key),str(value))) to def print_device_info(): drv.init() device = drv.Device(0) # enter your gpu id here ctx = device.make_context() (free,total)=drv.mem_get_info() print(\"Global memory occupancy:%f%% free\"%(free*100/total)) attrs=device.get_attributes() #Beyond this point is just pretty printing print(\"\\n===Attributes for device %d\"%0) for (key,value) in attrs.items(): print(\"%s:%s\"%(str(key),str(value))) ctx.pop() and it works like a charm. Thank you so much for sharing your solution, this really made my day !"
            ],
            [
                "Starting the Flask application in non-threaded mode: app.run(host=HOST, port=PORT, debug=False,threaded=False)"
            ]
        ],
        "votes": [
            9.0000001,
            1e-07,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am trying to allocate device memory, copy to it, perform the calculations on the GPU, copy the results back and then free up the device memory I allocated. I wanted to make sure that I wasn't going over the limit and I wanted to see if I would have enough memory in the shared memory space to dump a few arrays. When I allocate device memory, there are no errors being returned. When I use cudaMemGetInfo to check the amount of memory allocated, it looks like one cudaMalloc hasn't allocated any memory. Also when I try to free the memory, it looks like only one pointer is freed. I am using the matlab Mexfunction interface to setup the GPU memory and launch the kernel. At this point, I'm not even calling into the kernel and just returning back a unit matrix for the results. cudaError_t cudaErr; size_t freeMem = 0; size_t totalMem = 0; size_t allocMem = 0; cudaMemGetInfo(&amp;freeMem, &amp;totalMem); mexPrintf(\"Memory avaliable: Free: %lu, Total: %lu\\n\",freeMem, totalMem); /* Pointers for the device memory */ double *devicePulseDelay, *deviceTarDistance, *deviceScattDistance, *deviceScatterers; double *deviceReceivedReal, *deviceReceivedImag; /* Allocate memory on the device for the arrays. */ mexPrintf(\"Allocating memory.\\n\"); cudaErr = cudaMalloc( (void **) &amp;devicePulseDelay, sizeof(double)*512); if (cudaErr != cudaSuccess) { mexPrintf(\"could not allocate memory to devicePulseDelay\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"devicePulseDelay: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); cudaErr = cudaMalloc( (void **) &amp;deviceTarDistance, sizeof(double)*512); if (cudaErr != cudaSuccess) { mexPrintf(\"could not allocate memory to deviceTarDistance\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceTarDistance: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); cudaErr = cudaMalloc( (void **) &amp;deviceScattDistance, sizeof(double)*999*512); if (cudaErr != cudaSuccess) { mexPrintf(\"could not allocate memory to deviceScattDistance\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceScattDistance: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); cudaErr = cudaMalloc( (void **) &amp;deviceScatterers, sizeof(double)*999); if (cudaErr != cudaSuccess) { mexPrintf(\"could not allocate memory to deviceScatterers\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceScatterers: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); cudaErr = cudaMalloc( (void **) &amp;deviceReceivedReal, sizeof(double)*999*512); if (cudaErr != cudaSuccess) { mexPrintf(\"could not allocate memory to deviceReceivedReal\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceReceivedReal: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); cudaErr = cudaMalloc( (void **) &amp;deviceReceivedImag, sizeof(double)*999*512); if (cudaErr != cudaSuccess) { mexPrintf(\"could not allocate memory to deviceReceivedImag\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceReceivedImag: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\", allocMem, totalMem,(freeMem - allocMem)); /* copy the input arrays across to the device */ mexPrintf(\"\\nCopying memory.\\n\"); cudaErr = cudaMemcpy(devicePulseDelay, pulseDelay, sizeof(double)*512,cudaMemcpyHostToDevice); if (cudaErr != cudaSuccess) { mexPrintf(\"could not copy to devicePulseDelay\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"devicePulseDelay: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); cudaErr = cudaMemcpy(deviceTarDistance, tarDistance, sizeof(double)*512,cudaMemcpyHostToDevice); if (cudaErr != cudaSuccess) { mexPrintf(\"could not copy to deviceTarDistance\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceTarDistance: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); cudaErr = cudaMemcpy(deviceScattDistance, scattDistance, sizeof(double)*999*512,cudaMemcpyHostToDevice); if (cudaErr != cudaSuccess) { mexPrintf(\"could not copy to deviceScattDistance\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceScattDistance: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); cudaErr = cudaMemcpy(deviceScatterers, scatterers, sizeof(double)*999,cudaMemcpyHostToDevice); if (cudaErr != cudaSuccess) { mexPrintf(\"could not copy to deviceScatterers\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceScatterers: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); /* call the kernel */ // launchKernel&lt;&lt;&lt;1,512&gt;&gt;&gt;(........); /* retireve the output */ cudaErr = cudaMemcpy(receivedReal, deviceReceivedReal, sizeof(double)*512*512,cudaMemcpyDeviceToHost); if (cudaErr != cudaSuccess) { mexPrintf(\"could not copy to receivedReal\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"receivedReal: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); cudaErr = cudaMemcpy(receivedImag, deviceReceivedImag, sizeof(double)*512*512,cudaMemcpyDeviceToHost); if (cudaErr != cudaSuccess) { mexPrintf(\"could not copy to receivedImag\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"receivedImag: Memory avaliable: Free: %lu, Total: %lu, Consumed: %lu\\n\",allocMem, totalMem,(freeMem - allocMem)); /* free the memory. */ mexPrintf(\"\\nFree'ing memory.\\n\"); cudaMemGetInfo(&amp;freeMem, &amp;totalMem); mexPrintf(\"Before freeing: Free %lu, Total: %lu\\n\", freeMem, totalMem); cudaErr = cudaFree(devicePulseDelay); if (cudaErr != cudaSuccess) { mexPrintf(\"could free devicePulseDelay\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"devicePulseDelay: Memory avaliable: Free: %lu, Total: %lu, Free'd: %lu\\n\",allocMem, totalMem,(allocMem - freeMem)); cudaErr = cudaFree(deviceTarDistance); if (cudaErr != cudaSuccess) { mexPrintf(\"could free deviceTarDistance\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceTarDistance: Memory avaliable: Free: %lu, Total: %lu, Free'd: %lu\\n\",allocMem, totalMem,(allocMem - freeMem)); cudaErr = cudaFree(deviceScattDistance); if (cudaErr != cudaSuccess) { mexPrintf(\"could free deviceScattDistance\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceScattDistance: Memory avaliable: Free: %lu, Total: %lu, Free'd: %lu\\n\",allocMem, totalMem,(allocMem - freeMem)); cudaErr = cudaFree(deviceScatterers); if (cudaErr != cudaSuccess) { mexPrintf(\"could free deviceScatterers\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceScatterers: Memory avaliable: Free: %lu, Total: %lu, Free'd: %lu\\n\",allocMem, totalMem,(allocMem - freeMem)); cudaErr = cudaFree(deviceReceivedReal); if (cudaErr != cudaSuccess) { mexPrintf(\"could free deviceReceivedReal\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceReceivedReal: Memory avaliable: Free: %lu, Total: %lu, Free'd: %lu\\n\",allocMem, totalMem,(allocMem - freeMem)); cudaErr = cudaFree(deviceReceivedImag); if (cudaErr != cudaSuccess) { mexPrintf(\"could free deviceReceivedImag\\n\"); mexPrintf(\"Error: %s\\n\",cudaGetErrorString(cudaErr)); } cudaMemGetInfo(&amp;allocMem, &amp;totalMem); mexPrintf(\"deviceReceivedImag: Memory avaliable: Free: %lu, Total: %lu, Free'd: %lu\\n\",allocMem, totalMem,(allocMem - freeMem)); Here is the output from this: Memory avaliable: Free: 2523959296, Total: 2818572288 Allocating memory. devicePulseDelay: Memory avaliable: Free: 2522910720, Total: 2818572288, Consumed: 1048576 deviceTarDistance: Memory avaliable: Free: 2522910720, Total: 2818572288, Consumed: 1048576 deviceScattDistance: Memory avaliable: Free: 2518716416, Total: 2818572288, Consumed: 5242880 deviceScatterers: Memory avaliable: Free: 2517667840, Total: 2818572288, Consumed: 6291456 deviceReceivedReal: Memory avaliable: Free: 2515570688, Total: 2818572288, Consumed: 8388608 deviceReceivedImag: Memory avaliable: Free: 2513473536, Total: 2818572288, Consumed: 10485760 Copying memory. devicePulseDelay: Memory avaliable: Free: 2513473536, Total: 2818572288, Consumed: 10485760 deviceTarDistance: Memory avaliable: Free: 2513473536, Total: 2818572288, Consumed: 10485760 deviceScattDistance: Memory avaliable: Free: 2513473536, Total: 2818572288, Consumed: 10485760 deviceScatterers: Memory avaliable: Free: 2513473536, Total: 2818572288, Consumed: 10485760 receivedReal: Memory avaliable: Free: 2513473536, Total: 2818572288, Consumed: 10485760 receivedImag: Memory avaliable: Free: 2513473536, Total: 2818572288, Consumed: 10485760 Free'ing memory. Before freeing: Free 2513473536, Total: 2818572288 devicePulseDelay: Memory avaliable: Free: 2513473536, Total: 2818572288, Free'd: 0 deviceTarDistance: Memory avaliable: Free: 2513473536, Total: 2818572288, Free'd: 0 deviceScattDistance: Memory avaliable: Free: 2513473536, Total: 2818572288, Free'd: 0 deviceScatterers: Memory avaliable: Free: 2514522112, Total: 2818572288, Free'd: 1048576 deviceReceivedReal: Memory avaliable: Free: 2514522112, Total: 2818572288, Free'd: 1048576 deviceReceivedImag: Memory avaliable: Free: 2514522112, Total: 2818572288, Free'd: 1048576 I feel like there is something obvious that i'm missing. Can anyone help explain what is going on? EDIT: platform is windows 7 with a Tesla C2050 GPu card.",
        "answers": [
            [
                "It is a pretty common misconception that malloc directly gets memory allocations from the host operating system when called, and free directly releases them back to the host operating when called. But they almost always don't work like that, instead the standard library maintains a circular list of free'd and malloc'd memory which is opportunistically expanded and contracted by interacting with the host OS (see some of the answers on How do malloc() and free() work? for more details if you are interested). Irrespective of how it works, this leads to a number of non-intuitive results, including the fact that it is usually impossible to allocate as much memory as the OS says is free, that allocations sometimes appear to not change the amount of free memory, and that free sometimes has no effect on the amount of memory the OS says is free. Although I have nothing but empirical evidence to support this, I believe CUDA works exactly the same way. The context maintains its own list of malloc'd and free'd memory, and will expand and contract the memory held in that list as host driver/window manager and the GPU itself allows. All hardware has a characteristic MMU page size, and there is evidence to suggest that the page size on NVIDIA GPUs is rather large. This implies there is rather coarse granularity in cudaMalloc calls, and means sometimes a malloc appears to not effect the amount of free memory or to consume much more memory than was requested, and sometimes free calls appear to have no effect (If you are interested, you can find a little tool which helps illustrate the page size behaviour of the CUDA driver here, although it was written for an early version of the CUDA API and might need a couple of changes to compile with modern versions). I believe this is the most likely explanation for the behaviour you are observing. Incidentally, if I run a simplified version of the code you posted on MacOS 10.6 with a GT200 family device: #include &lt;cstdio&gt; #define mexPrintf printf inline void gpuAssert(cudaError_t code, char *file, int line, bool abort=true) { if (code != cudaSuccess) { mexPrintf(\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line); if (abort) exit(code); } } #define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); } inline void gpuMemReport(size_t * avail, size_t * total, const char * title = 0, const size_t * free = 0, const bool sense = true) { char tstring[32] = { '\\0' }; gpuErrchk( cudaMemGetInfo(avail, total) ); if (free) { if (title) { strncpy(tstring, title, 31); } mexPrintf(\"%s Memory avaliable: Free: %zu, Total: %zu, %s: %zu\\n\", tstring, *avail, *total, (sense) ? \"Allocated\\0\" : \"Freed\\0\", (sense) ? (*free - *avail) : (*avail - *free)); } else { mexPrintf(\"Memory avaliable: Free: %zu, Total: %zu\\n\", *avail, *total); } } int main() { size_t freeMem = 0; size_t totalMem = 0; size_t allocMem = 0; gpuErrchk( cudaFree(0) ); gpuMemReport(&amp;freeMem, &amp;totalMem); double *devicePulseDelay, *deviceTarDistance, *deviceScattDistance, *deviceScatterers; double *deviceReceivedReal, *deviceReceivedImag; mexPrintf(\"Allocating memory.\\n\"); gpuErrchk( cudaMalloc( (void **) &amp;devicePulseDelay, sizeof(double)*512) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"devicePulseDelay:\", &amp;freeMem); gpuErrchk( cudaMalloc( (void **) &amp;deviceTarDistance, sizeof(double)*512) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"deviceTarDistance:\", &amp;freeMem); gpuErrchk( cudaMalloc( (void **) &amp;deviceScattDistance, sizeof(double)*999*512) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"deviceScattDistance:\", &amp;freeMem); gpuErrchk( cudaMalloc( (void **) &amp;deviceScatterers, sizeof(double)*999) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"deviceScatterers:\", &amp;freeMem); gpuErrchk( cudaMalloc( (void **) &amp;deviceReceivedReal, sizeof(double)*999*512) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"deviceReceivedReal:\", &amp;freeMem); gpuErrchk( cudaMalloc( (void **) &amp;deviceReceivedImag, sizeof(double)*999*512) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"deviceReceivedImag:\", &amp;freeMem); mexPrintf(\"\\nFree'ing memory.\\n\"); gpuMemReport(&amp;freeMem, &amp;totalMem); gpuErrchk( cudaFree(devicePulseDelay) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"devicePulseDelay:\", &amp;freeMem, false); gpuErrchk( cudaFree(deviceTarDistance) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"deviceTarDistance:\", &amp;freeMem, false); gpuErrchk( cudaFree(deviceScattDistance) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"deviceScattDistance:\", &amp;freeMem, false); gpuErrchk( cudaFree(deviceScatterers) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"deviceScatterers:\", &amp;freeMem, false); gpuErrchk( cudaFree(deviceReceivedReal) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"deviceReceivedReal:\", &amp;freeMem, false); gpuErrchk( cudaFree(deviceReceivedImag) ); gpuMemReport(&amp;allocMem, &amp;totalMem, \"deviceReceivedImag:\", &amp;freeMem, false); return 0; } I get a different result, but also one showing the same phenomena: Allocating memory. devicePulseDelay: Memory avaliable: Free: 202870784, Total: 265027584, Allocated: 1048576 deviceTarDistance: Memory avaliable: Free: 202870784, Total: 265027584, Allocated: 1048576 deviceScattDistance: Memory avaliable: Free: 198778880, Total: 265027584, Allocated: 5140480 deviceScatterers: Memory avaliable: Free: 197730304, Total: 265027584, Allocated: 6189056 deviceReceivedReal: Memory avaliable: Free: 193638400, Total: 265027584, Allocated: 10280960 deviceReceivedImag: Memory avaliable: Free: 189546496, Total: 265027584, Allocated: 14372864 Free'ing memory. Memory avaliable: Free: 189546496, Total: 265027584 devicePulseDelay: Memory avaliable: Free: 189546496, Total: 265027584, Freed: 0 deviceTarDistance: Memory avaliable: Free: 190595072, Total: 265027584, Freed: 1048576 deviceScattDistance: Memory avaliable: Free: 194686976, Total: 265027584, Freed: 5140480 deviceScatterers: Memory avaliable: Free: 195735552, Total: 265027584, Freed: 6189056 deviceReceivedReal: Memory avaliable: Free: 199827456, Total: 265027584, Freed: 10280960 deviceReceivedImag: Memory avaliable: Free: 203919360, Total: 265027584, Freed: 14372864 Which suggests that the behaviour is hardware/host OS dependent as well."
            ]
        ],
        "votes": [
            17.0000001
        ]
    },
    {
        "question": "I'm fighting with TensorRT (TensorRT 4 for python right now) since several weeks. I passed a lot of problems to get TensorRT running. The example code from NVIDIA works well for me : TensorRT MNIST example Now, i created my own network in tensorflow (a very simple one) for upscaling images, let's say (in HWC) 320x240x3 into 640x480x3 .The usual way by creating a frozen-graph and running an inferencer just based on Tensorflow gave me expected results but not by using TensorRT. I have a strange feeling about that i made something wrong by feeding the images into the GPU-memory (This would be probably an issue about pycuda and/or TensorRT). The worst case scenario would be that TensorRT destroys my network by the optimization process. I hope someone has just a little idea for saving my life. This is my Tensorflow-model (i just wrapped the functions): net = conv2d(input, 64, k_size=3, activation=tf.nn.relu, name='conv1') net = deconv2d(net, 3, k_size=5, activation=tf.tanh, stride=self.params.resize_factor, scale=self.params.resize_factor, name='deconv') This is the important snippet of my inferencer: import tensorrt as trt import uff from tensorrt.parsers import uffparser import pycuda.driver as cuda import numpy as np ... def _init_infer(self, uff_model): g_logger = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR) parser = uffparser.create_uff_parser() parser.register_input(self.input_node, (self.channels, self.height, self.width), 0) parser.register_output(self.output_node) self.engine = trt.utils.uff_to_trt_engine(g_logger, uff_model, parser, self.max_batch_size, self.max_workspace_size) parser.destroy() self.runtime = trt.infer.create_infer_runtime(g_logger) self.context = self.engine.create_execution_context() self.output = np.empty(self.output_size, dtype=self.dtype) # create CUDA stream self.stream = cuda.Stream() # allocate device memory self.d_input = cuda.mem_alloc(self.channels * self.max_batch_size * self.width * self.height * self.output.dtype.itemsize) self.d_output = cuda.mem_alloc(self.output_size * self.output.dtype.itemsize) self.bindings = [int(self.d_input), int(self.d_output)] def infer(self, input_batch, batch_size=1): # transfer input data to device cuda.memcpy_htod_async(self.d_input, input_batch, self.stream) # execute model self.context.enqueue(batch_size, self.bindings, self.stream.handle, None) # transfer predictions back cuda.memcpy_dtoh_async(self.output, self.d_output, self.stream) # synchronize threads self.stream.synchronize() return self.output And the executable snippet: ... # create trt inferencer trt_inferencer = TensorRTInferencer(params=params) img = [misc.imread('./test_images/lion.png')] img[0] = normalize(img[0]) img = img[0] # inferencing method result = trt_inferencer.infer(img) result = inormalize(result, dtype=np.uint8) result = result.reshape(1, params.height * 2, params.width * 2, 3) ... And the weird result by comparison :( upscaled lion TensorRT, Tensorflow, Original",
        "answers": [
            [
                "I got it now, finally. The problem was a wrong dimension and order of the input images and output. And for everyone who run into the same problem, this is the adopted executable snippet, dependent on my initialization: ... # create trt inferencer trt_inferencer = TensorRTInferencer(params=params) img = [misc.imread('./test_images/lion.png')] img[0] = normalize(img[0]) img = img[0] img = np.transpose(img, (2, 0, 1)) img = img.ravel() # inferencing method result = trt_inferencer.infer(img) result = inormalize(result, dtype=np.uint8) result = np.reshape(result, newshape=[3, params.height * 2, params.width * 2]) result = np.transpose(result, (1, 2, 0)) ..."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to juggle the context in the different GPUs to store different parameters in different GPUs. So what I have done is to initialize cuda, set the device, create the context (ctx1) for the device, then use curandom to generate a GPUarray of random values. After which I pop the context and follow the same steps, set it to another device, create a new context and generate another GPUarray of random values. So from what I understand, my x1 is stored in the memory of GPU1 and x2 is stored in the memory of GPU2. However, I realised that when I .pop() and .push() ctx1 and ctx2 respectively and vice versa, I am still able to access both x1 and x2. Is thus due to the Unified Virtual Addressing or enable peer access that allows me to access both x1 and x2 regardless of which context I'm in? import pycuda import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.curandom as curandom d = 2 ** 15 cuda.init() dev1 = cuda.Device(1) ctx1 = dev1.make_context() curng1 = curandom.XORWOWRandomNumberGenerator() x1 = curng1.gen_normal((d,d), dtype = np.float32) # so x1 is stored in GPU 1 ctx1.pop() # clearing ctx of GPU1 dev2 = cuda.Device(1) ctx2 = dev2.make_context() curng2 = curandom.XORWOWRandomNumberGenerator() x2 = curng2.gen_normal((d,d), dtype = np.float32) # so x2 is stored in GPU 2",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm using PyCuda to run a kernel that is expected to take at least two hours to complete, but it is failing after around one hour with the simple error of: pycuda._driver.Error: cuCtxSynchronize failed: unknown error I'm using Windows, and I added the registry key TdrDelay and set it to 120000000 to ensure that Windows is not timing out my kernel. This error doesn't happen when I adjust the parameters of the kernel so it is expected to complete in about 30 minutes. Why could the synchronize call be failing after the kernel has run for a long time? Could my graphics card be overheating and preemptively terminating the kernel? Could there be a CUDA setting that terminates a kernel if it runs for too long? Could running the kernel in NVidia Visual Profiler help figure out what the problem might be?",
        "answers": [
            [
                "I was able to get my long running kernel to complete without error by adding the registry key \"TdrLevel\" alongside \"TdrDelay\" and setting its value to 0."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "once tensorflow be active. it will make every cuda code crash even I use sess.close()... the error msg is: pycuda._driver.LogicError: cuFuncSetBlockShape failed: invalid resource handle The following code it a simple example cuda code run by pycuda: Once I add sess = tf.Session(). My cuda code crash. It work fine without sess = tf.Session(). import tensorflow as tf import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") ## tensorflow will make any other cuda code crash............ sess = tf.Session() sess.close() ## tensorflow will make any other cuda code crash............ multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them(drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1)) print (dest-a*b) print(\"finish\") Any suggestion? Thanks~~~",
        "answers": [
            [
                "I suspect same problem as here: Could pycuda and scikit-cuda work together? Short answer: Do not use import pycuda.autoinit; this creates a new CUDA context that you would ave to manage (push and pop) manually. Instead, use import pycuda.autoprimaryctx to retain the already existing primary CUDA context that most other CUDA applications use automatically."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a 3D matrix where the x-y plane(s) represent an image and the z-plane represents image layers. The issue is when I try to extract the first (or other layers) using idz, I do not get the expected results. It looks like the array, once in CUDA, has different indexes for x, y or z than what I expect (as in pycuda). I see this by the result array below. The following is a step by step process for this mini example (I used generic int numbers to represent my images to save uploading images and the entire code)! Here I import libraries and define image size and layers... import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy from pycuda.gpuarray import to_gpu row = 10 column = 10 depth = 5 Then I define my input 3D array and my output 2D array... #--==== Input 3D Array ====--- arrayA = numpy.full((row, column, depth), 0) #populate each layer with fixed values for i in range(depth): arrayA[:,:,i] = i + 1 arrayA = arrayA.astype(numpy.uint16) arrayA_gpu = cuda.mem_alloc(arrayA.nbytes) cuda.memcpy_htod(arrayA_gpu, arrayA) arrayA_Answer = numpy.empty_like(arrayA) #--==== Output 2D array container ====--- arrayB = numpy.zeros([row, column], dtype = numpy.uint16) arrayB_gpu = cuda.mem_alloc(arrayB.nbytes) cuda.memcpy_htod(arrayB_gpu, arrayB) arrayB_Answer = numpy.empty_like(arrayB) Next I define the CUDA kernal and function in pycuda mod = SourceModule(\"\"\" __global__ void getLayer(int *arrayA, int *arrayB) { int idx = threadIdx.x + (blockIdx.x * blockDim.x); // x coordinate (numpy axis 2) int idy = threadIdx.y + (blockIdx.y * blockDim.y); // y coordinate (numpy axis 1) int idz = 0; //The first layer, this can set in range from 0-4 int x_width = (blockDim.x * gridDim.x); int y_width = (blockDim.y * gridDim.y); arrayB[idx + (x_width * idy)] = arrayA[idx + (x_width * idy) + (x_width * y_width) * idz]; } \"\"\") func = mod.get_function(\"getLayer\") func(arrayA_gpu, arrayB_gpu, block=(row, column, 1), grid=(1,1)) Using standard pycuda commands, I extract the results (not what I expected) arrayA[:,:,0] = 10x10 matrix populated with 1's (good) print(arrayA_Answer[:,:,0]) [[1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1] [1 1 1 1 1 1 1 1 1 1]] arrayB[:,:] = 10x10 matrix populated with the following (bad), expected to be equal to arrayA[:,:,0]... print(arrayB_Answer) [[1 2 3 4 5 1 2 3 4 5] [1 2 3 4 5 1 2 3 4 5] [1 2 3 4 5 1 2 3 4 5] [1 2 3 4 5 1 2 3 4 5] [1 2 3 4 5 1 2 3 4 5] [1 2 3 4 5 1 2 3 4 5] [1 2 3 4 5 1 2 3 4 5] [1 2 3 4 5 1 2 3 4 5] [1 2 3 4 5 1 2 3 4 5] [1 2 3 4 5 1 2 3 4 5]]",
        "answers": [
            [
                "As discussed here, the numpy 3D storage order pattern is that the \"z\" (i.e. \"3rd\") index is the rapidly varying index, as you progress linearly through memory. Your code assumes that the first index (\"x\") is the rapidly varying one. Since your kernel is already organized for efficient (\"coalesced\") load/store behavior, you could address this by reordering the storage of your images/layers/slices in numpy. Here is a worked example: $ cat t10.py from __future__ import print_function import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy from pycuda.gpuarray import to_gpu row = 5 column = 10 depth = 10 #--==== Input 3D Array ====--- arrayA = numpy.full((row, column, depth), 0) my_slice=numpy.int32(3) # choose the layer #populate each layer with fixed values for i in range(row): arrayA[i,:,:] = i + 1 arrayA = arrayA.astype(numpy.int32) arrayA_gpu = cuda.mem_alloc(arrayA.nbytes) cuda.memcpy_htod(arrayA_gpu, arrayA) arrayA_Answer = numpy.empty_like(arrayA) #--==== Output 2D array container ====--- arrayB = numpy.zeros([column, depth], dtype = numpy.int32) arrayB_gpu = cuda.mem_alloc(arrayB.nbytes) cuda.memcpy_htod(arrayB_gpu, arrayB) arrayB_Answer = numpy.empty_like(arrayB) mod = SourceModule(\"\"\" __global__ void getLayer(int *arrayA, int *arrayB, int slice) { int idx = threadIdx.x + (blockIdx.x * blockDim.x); // x coordinate (numpy axis 2) int idy = threadIdx.y + (blockIdx.y * blockDim.y); // y coordinate (numpy axis 1) int idz = slice; //The \"layer\" int x_width = (blockDim.x * gridDim.x); int y_width = (blockDim.y * gridDim.y); arrayB[idx + (x_width * idy)] = arrayA[idx + (x_width * idy) + (x_width * y_width) * idz]; } \"\"\") func = mod.get_function(\"getLayer\") func(arrayA_gpu, arrayB_gpu, my_slice, block=(depth, column, 1), grid=(1,1)) cuda.memcpy_dtoh(arrayB_Answer,arrayB_gpu) print(arrayA[my_slice,:,:]) print(arrayB_Answer[:,:]) $ python t10.py [[4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4]] [[4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4] [4 4 4 4 4 4 4 4 4 4]] $ Note that I have also changed your use of uint16 to int32, to match the kernel type int."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have some complicated formula, which is easier to implement directly using CUDA code. On the other hand, I need to make use of the theano feature to build a neural network and train it separately. How can I safely use pycuda and theano together? The following code works on my machine: import numpy as np import pycuda.autoinit as cuauto import pycuda.driver as cuda import pycuda.compiler as cudacc import pycuda.gpuarray as gpuarray import theano import theano.tensor as T def get_pycuda_func(): mod = cudacc.SourceModule(\"\"\" __global__ void mul(double *dest, double *a, double *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") mul = mod.get_function(\"mul\") mul.prepare(\"PPP\") def f(a,b): N = len(a) gpu_a = gpuarray.to_gpu(a) gpu_b = gpuarray.to_gpu(b) c = gpuarray.empty((N,),dtype=np.float64) mul.prepared_call( (1,1,1),(N,1,1), c.gpudata, gpu_a.gpudata, gpu_b.gpudata ) return c.get() return f def get_theano_func(): a = T.vector('a') b = T.vector('b') c = a*b f = theano.function([a,b],c,allow_input_downcast=True) return f def get_cpu_func(): def f(a,b): return a*b return f if __name__ == \"__main__\": np.random.seed(12345) a = np.random.randn(400) b = np.random.randn(400) f_cuda = get_pycuda_func() f_cpu = get_cpu_func() f_theano = get_theano_func() for k in range(10): x = f_cuda(a,b) y = f_theano(a,b) z = f_cpu(a,b) print(k) print(np.allclose(x,z)) print(np.allclose(y,z)) Output: $ python3 test_theano_pycuda_simpler.py Using cuDNN version 7003 on context None Mapped name None to device cuda: GeForce GTX TITAN Black (0000:01:00.0) 0 True True 1 True True 2 True True 3 True True 4 True True 5 True True 6 True True 7 True True 8 True True 9 True True But if I make a more complicated theano computation, it does not work. The following DOES NOT WORK: import numpy as np import pycuda.autoinit as cuauto import pycuda.driver as cuda import pycuda.compiler as cudacc import pycuda.gpuarray as gpuarray import theano import theano.tensor as T def get_pycuda_func(): mod = cudacc.SourceModule(\"\"\" __global__ void mul(double *dest, double *a, double *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") mul = mod.get_function(\"mul\") mul.prepare(\"PPP\") def f(a,b): N = len(a) gpu_a = gpuarray.to_gpu(a) gpu_b = gpuarray.to_gpu(b) c = gpuarray.empty((N,),dtype=np.float64) mul.prepared_call( (1,1,1),(N,1,1), c.gpudata, gpu_a.gpudata, gpu_b.gpudata ) return c.get() return f floatX=theano.config.floatX def init_bias(size): tmp = np.random.rand(size) return theano.shared(np.asarray(tmp,dtype=floatX)) def init_weights(in_size,out_size): s = np.sqrt(2./(in_size+out_size)) tmp = np.random.normal(loc=0.,scale=s,size=(in_size,out_size)) return theano.shared(np.asarray(tmp,dtype=floatX)) def adam(params, gparams,learning_rate = 0.0001, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8): updates = [] t_pre = theano.shared(np.asarray(.0, dtype=theano.config.floatX)) t = t_pre + 1 a_t = learning_rate * T.sqrt(1 - beta2 ** t) / (1 - beta1 ** t) for (p,g) in zip(params, gparams): v = p.get_value(borrow = True) m_pre = theano.shared(np.zeros(v.shape, dtype = v.dtype), broadcastable = p.broadcastable) v_pre = theano.shared(np.zeros(v.shape, dtype = v.dtype), broadcastable = p.broadcastable) m_t = beta1 * m_pre + (1 - beta1) * g v_t = beta2 * v_pre + (1 - beta2) * g ** 2 step = a_t * m_t / (T.sqrt(v_t) + epsilon) p_update = p - step updates.append((m_pre, m_t)) updates.append((v_pre, v_t)) updates.append((p, p_update)) updates.append((t_pre, t)) return updates class test_network: def __init__(self,hidden=[100,100]): self.hidden = hidden self._create_params() self._create_train_func() self._create_func() def _create_params(self): hidden = self.hidden W0 = init_weights(1,hidden[0]) W1 = init_weights(hidden[0],hidden[1]) W2 = init_weights(hidden[1],1) b0 = init_bias(hidden[0]) b1 = init_bias(hidden[1]) b2 = init_bias(1) self.params = [ W0,W1,W2, b0,b1,b2, ] def predict(self,x): [ W0,W1,W2, b0,b1,b2, ] = self.params H0 = T.dot(x,W0) + b0 H0 = T.nnet.relu(H0) H1 = T.dot(H0,W1) + b1 H1 = T.nnet.relu(H1) ret = T.dot(H1,W2) + b2 return ret def _create_func(self): x = T.matrix('x') y = self.predict(x) self.f = theano.function([x],y,allow_input_downcast=True) def _create_train_func(self): y_in = T.matrix('y_in') x = T.matrix('x') y = self.predict(x) loss = T.mean((y-y_in)*(y-y_in)) grad_loss = T.grad(loss,self.params) updates = adam(self.params,grad_loss) self.train = theano.function(inputs=[x,y_in], outputs=loss, updates=updates, allow_input_downcast=True) def get_cpu_func(): def f(a,b): return a*b return f if __name__ == \"__main__\": np.random.seed(12345) a = np.random.randn(400) b = np.random.randn(400) f_cuda = get_pycuda_func() f_cpu = get_cpu_func() T = test_network() for k in range(10): x = f_cuda(a,b) z = f_cpu(a,b) print(k) print(np.allclose(x,z)) batch_size = 256 for k in range(1000): x = np.random.rand(batch_size) y = x*x x = x.reshape(batch_size,1) y = y.reshape(batch_size,1) loss = T.train(x,y) print(\"k=%d, loss=%g\" % (k,loss)) I would get: $ python3 test_theano_pycuda.py Using cuDNN version 7003 on context None Mapped name None to device cuda: GeForce GTX TITAN Black (0000:01:00.0) Traceback (most recent call last): File \"test_theano_pycuda.py\", line 160, in &lt;module&gt; x = f_cuda(a,b) File \"test_theano_pycuda.py\", line 32, in f gpu_b.gpudata File \"/usr/local/lib/python3.5/dist-packages/pycuda-2017.1.1-py3.5-linux-x86_64.egg/pycuda/driver.py\", line 447, in function_prepared_call func._set_block_shape(*block) pycuda._driver.LogicError: cuFuncSetBlockShape failed: invalid resource handle I am sure my test_theano_pycuda.py works because I have tested it by forcing theano to use CPU instead of cuda. (By modifying ~/.theanorc): From this. I bet it should be related to the problem that pycuda and theano are both creating a context within one process. In theano document, with gpuarray_cuda_context: pycuda_context = pycuda.driver.Context.attach() where does that gpuarray_cuda_context come from? Are there any workable example that I can test with?",
        "answers": [
            [
                "gpuarray_cuda_context here is just an existing context from a GpuArray Variable. For instance, you can find an example in theano/gpuarray/fft.py, where I think skcuda.misc.init() will call pycuda.driver.Context.attach() or do something similar."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am using the PyCUDA to implement the smooth_local_affine as shown here. It works well when I simply run the program on linux. But when I tried to import it under Flask context: from smooth_local_affine import smooth_local_affine from flask import Flask app = Flask(_name_) ... The following error occurs: ------------------------------------------------------------------- PyCUDA ERROR: The context stack was not empty upon module cleanup. ------------------------------------------------------------------- A context was still active when the context stack was being cleaned up. At this point in our execution, CUDA may already have been deinitialized, so there is no way we can finish cleanly. The program will be aborted now. Use Context.pop() to avoid this problem. Then I tried to add context.pop(),then another error occurs; Error in atexit._run_exitfuncs: Traceback (most recent call last): File \"/home/yifang/anaconda3/envs/python3/lib/python3.6/site-packages/pycuda-2017.1-py3.6-linux-x86_64.egg/pycuda/autoinit.py\", line 14, in _finish_up context.pop() pycuda._driver.LogicError: context::pop failed: invalid device context - cannot pop non-current context Anyone knows how to run PyCuda in Flask environment? Or maybe any alternative ways that I can use this smooth_local_affine feature without using PyCuda?",
        "answers": [
            [
                "Let me present one solution here because I have tried a lot of solutions but still not work. Fortunately I found one correct answer. some solutions like import pycuda.autoinit or cuda.init device = cuda.Device(0) ctx = device.make_context() inputs, outputs, bindings, stream = allocate_buffer() ctx.pop() these may work if you run the script as simple program, but it will raise context error if you run with flask or other web servers. According to my searching, the reason is possibally that flask server would spawn new thread when a request came in. The real solution in such a circumstance is quite simple and you should just add code like this: with engine.create_execution_context() as context: ctx = cuda.Context.attach() inputs, outputs, bindings, stream = allocate_buffer() ctx.detach() This works for me"
            ],
            [
                "Starting the Flask application in non-threaded mode worked for me app.run(host=HOST, port=PORT, debug=False,threaded=False)"
            ]
        ],
        "votes": [
            4.0000001,
            1e-07
        ]
    },
    {
        "question": "I have a kernel, how can I get the number of used registers per thread when launching the kernels? I mean in a PyCuda way. A simple example will be: __global__ void make_blobs(float* matrix, float2 *pts, int num_pts, float sigma, int rows, int cols) { int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; if (x &lt; cols &amp;&amp; y &lt; rows) { int idx = y*cols + x; float temp = 0.f; for (int i = 0; i &lt; num_pts; i++) { float x_0 = pts[i].x; float y_0 = pts[i].y; temp += exp(-(pow(x - x_0, 2) + pow(y - y_0, 2)) / (2 * sigma*sigma)); } matrix[idx] = temp; } } Is there anyway to get the number without crashing the program if the real number used has exceeded the max? The above is OK, it dose not exceed the max in my machine. I just want to get the number in a convenient way. Thanks!",
        "answers": [
            [
                "PyCuda already provides this as part of the Cuda function object. The property is called pycuda.driver.Function.num_regs. Below is a small example that shows how to use it: import pycuda.autoinit from pycuda.compiler import SourceModule kernel_src = \"\"\" __global__ void make_blobs(float* matrix, float2 *pts, int num_pts, float sigma, int rows, int cols) { int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; if (x &lt; cols &amp;&amp; y &lt; rows) { int idx = y*cols + x; float temp = 0.f; for (int i = 0; i &lt; num_pts; i++) { float x_0 = pts[i].x; float y_0 = pts[i].y; temp += exp(-(pow(x - x_0, 2) + pow(y - y_0, 2)) / (2 * sigma*sigma)); } matrix[idx] = temp; } }\"\"\" compiledKernel = SourceModule(kernel_src) make_blobs = compiledKernel.get_function(\"make_blobs\") print(make_blobs.num_regs) Note that you don't need to use SourceModule. You can also load the module from e.g. a cubin file. More details can be found in the documentation."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am currently trying to use PyCUDA on Debian 9. I already manage to make cuda work, and if I run: nvcc -ccbin clang-3.8 file.cu I compile the file correctly and I am able to run it. However, after I intalled pycuda using apt-get install python-pycuda And run a simple example from their website: import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1)) print dest-a*b But I receive the following error: CompileError Traceback (most recent call last) &lt;ipython-input-1-8e16128de7f2&gt; in &lt;module&gt;() 10 dest[i] = a[i] * b[i]; 11 } ---&gt; 12 \"\"\") 13 14 multiply_them = mod.get_function(\"multiply_them\") /usr/lib/python2.7/dist-packages/pycuda/compiler.pyc in __init__(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs) 263 264 cubin = compile(source, nvcc, options, keep, no_extern_c, --&gt; 265 arch, code, cache_dir, include_dirs) 266 267 from pycuda.driver import module_from_buffer /usr/lib/python2.7/dist-packages/pycuda/compiler.pyc in compile(source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs, target) 253 options.append(\"-I\"+i) 254 --&gt; 255 return compile_plain(source, options, keep, nvcc, cache_dir, target) 256 257 /usr/lib/python2.7/dist-packages/pycuda/compiler.pyc in compile_plain(source, options, keep, nvcc, cache_dir, target) 135 raise CompileError(\"nvcc compilation of %s failed\" % cu_file_path, 136 cmdline, stdout=stdout.decode(\"utf-8\", \"replace\"), --&gt; 137 stderr=stderr.decode(\"utf-8\", \"replace\")) 138 139 if stdout or stderr: CompileError: nvcc compilation of /tmp/tmpVgfyrm/kernel.cu failed [command: nvcc --cubin -arch sm_61 -I/usr/local/lib/python2.7/dist-packages/pycuda-2017.1.1-py2.7-linux-x86_64.egg/pycuda/cuda kernel.cu] [stderr: ERROR: No supported gcc/g++ host compiler found, but clang-3.8 is available. Use 'nvcc -ccbin clang-3.8' to use that instead. ] Anyone knows how I can add -ccbin clang-3.8 to pycuda??",
        "answers": [
            [
                "As per the documentation, you can specify compiler options to nvcc in two ways Set the default compiler options via the PYCUDA_DEFAULT_NVCC_FLAGS environment variable. Set the compiler options for a given SourceModule via a list passed using the options= keyword"
            ],
            [
                "For everyone with the problem, the solution is the one given b talonmies, using the options argument. The code I used was the following: import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\", options=[\"-ccbin\",\"clang-3.8\"]) multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1)) print dest-a*b or using: pycuda.compiler.DEFAULT_NVCC_FLAG = [\"-ccbin\",\"clang-3.8\"]"
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I have some pyCUDA code that keeps the GPU at 100% usage and seems to hog the GPU to the point that my screen only updates every second or so. Changing the block and grid sizes doesn't help. Each thread in the grid goes through a loop about 1.3 million times, and there are only around 6 blocks of 16 threads. If I make it a small loop there is no problem, but unfortunately it has to be that big and I see no good way to distribute the work into more blocks. Is there a way to limit the GPU usage of my program, or maybe change the priority of the screen? GTX 1060 on Windows.",
        "answers": [
            [
                "Is there a way to limit the GPU usage of my program, or maybe change the priority of the screen? In a word, no. The GPU cannot simultaneously run compute jobs and refresh the display. There is no concept of priority. If you have long running compute code, it will block the display from refreshing and the duration of that block is determined by the compute code. The driver only has one preemption mechanism, and that is the watch dog timer which will kill a long running compute activity on a display device. If you need screen responsiveness during compute operations, either vastly decrease the run time of an individual kernel launch, or get a second GPU and have one dedicated to the compute work and one for display."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "if i have any small pycuda code such as the one below, how can i access options of the nvcc compiler? for example if i want to set -maxrregcount 20 (or any other argument), how would i accomplish that&gt; import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1)) print dest-a*b",
        "answers": [
            [
                "well i found that the answer is rather simple all we need to do is add the code below.. mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\",options=['--maxrregcount=20'])"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I want to use pyCUDA on Microsoft azure. I tried installing it on Google Colab but it is not getting installed. I looked on to the pyCUDA documentation but couldn't find any discussion on it. Do anyone has any information regarding same? error: command 'x86_64-linux-gnu-gcc' failed with exit status 1 ---------------------------------------- Command \"/usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-PUtIQA/pycuda/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-uDKIjk-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-PUtIQA/pycuda/",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am writing code which can compare between numpy.fft.fft2 and pycuda but the results are not matching. Additionally pycuda results are ambiguous every time. data file : https://nofile.io/f/bjGRQGRVSCG/gauss.npy from pyfft.cuda import Plan import numpy as np from pycuda.tools import make_default_context import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import time import matplotlib.pyplot as plt cuda.init() context = make_default_context() data = np.load('gauss.npy') data_complex = data.astype(np.complex64) start_time = time.time() plan = Plan((32,32)) gpu_data = gpuarray.to_gpu(data_complex) plan.execute(gpu_data) result = gpu_data.get() print(\"--- %s seconds (FFT calculation pycuda)---\" % (time.time() - start_time)) start_time_3 = time.time() result_np = np.fft.fft2(data) #print(result_np) print(\"--- %s seconds (FFT calculation numpy.fft.fft)---\" % (time.time() - start_time)) context.pop() #plt.plot(result) #plt.plot(result_np) I'm starting to wonder whether we can even perform 2D FFT with pycuda?",
        "answers": [
            [
                "pyfft.cuda is almost assuredly using cufft, which does not compute FFTs in the same way as numpy's fft (IIRC, even scipy.fft and np.fft can produce different results). You should read the documentation for each library in order to understand the differences. You can definitely perform 2D FFTs with pycuda"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Using Cuda GPU programming in a college project and just wondering if a GPU has a possible block size of 1024 if you have 2 GPU's does that mean that that block size is doubled? And would this effect the implementation of the program do you need to access the GPU's individually?",
        "answers": [
            [
                "No, the block size is not doubled. Block size is usually related to maximum number of active warps per Streaming Multiprocessor on a GPU. If you are planning to write a program on multiple GPUs, you will need to set the active GPU device in your code (via cudaSetDevice()) everytime before calling any CUDA runtime functions. Each GPU will run its functions separately and asynchronously."
            ],
            [
                "I think what you're asking about is the maximum number of threads per block, which exists on a per-GPU basis. This means that even if you have two GPUs each with a maximum 1024 threads per block, the block size remains static. So to answer your question, no, block size is not doubled. You would still need to communicate with each GPU individually, unfortunately. You can see more about technical specifications such as threads per block here."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I have installed pycuda and scikit cuda via pip install on python 3.6 and have been trying to run the following example from scikit cuda: from __future__ import print_function import pycuda.autoinit import pycuda.driver as drv import pycuda.gpuarray as gpuarray import numpy as np import skcuda.linalg as culinalg import skcuda.misc as cumisc culinalg.init() # Double precision is only supported by devices with compute # capability &gt;= 1.3: import string import scikits.cuda.cula as cula demo_types = [np.float32, np.complex64] if cula._libcula_toolkit == 'premium' and \\ cumisc.get_compute_capability(pycuda.autoinit.device) &gt;= 1.3: demo_types.extend([np.float64, np.complex128]) for t in demo_types: print('Testing svd for type ' + str(np.dtype(t))) a = np.asarray((np.random.rand(50, 50) - 0.5) / 10, t) a_gpu = gpuarray.to_gpu(a) u_gpu, s_gpu, vh_gpu = culinalg.svd(a_gpu) a_rec = np.dot(u_gpu.get(), np.dot(np.diag(s_gpu.get()), vh_gpu.get())) print('Success status: ', np.allclose(a, a_rec, atol=1e-3)) print('Maximum error: ', np.max(np.abs(a - a_rec))) print('') which then prints: Traceback (most recent call last): File \"&lt;ipython-input-3-a4adfb24f4ca&gt;\", line 8, in &lt;module&gt; import skcuda.linalg as culinalg File \"C:\\Users\\moore\\downloadsl\\Continuum\\anaconda3\\lib\\site-packages\\skcuda\\linalg.py\", line 21, in &lt;module&gt; from . import cublas File \"C:\\Users\\moore\\downloadsl\\Continuum\\anaconda3\\lib\\site-packages\\skcuda\\cublas.py\", line 55, in &lt;module&gt; raise OSError('cublas library not found') OSError: cublas library not found I looked into the cublas.py and there is a list of versions and it only goes upto 7.5, so my question is threefold. Are the newest versions of pycuda and scikit-cuda compatible and if not, are there any analogous libraries for nvidia gpu acceleration of gpu code that is as easy as scikit-cuda, and if not what versions will I have to download in order to make this all work?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Is it possible somehow to use / launch the cudaLaunchCooperativeKernel api with pycuda? Hoping to achieve sync at grid level with such.",
        "answers": [
            [
                "Not at the moment, no. You have see for yourself here that the driver API version of that functionality doesn't appear anywhere in the current PyCUDA tree. I would opine that adding support would require a fairly major overhaul of the APIs and I am not confident that it will happen soon."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "My goal is to persist the full state of each iteration of a complex algorithm which also involves pseudo-random numbers generated via pycuda. In order to resume the algorithm at an arbitraty iteration and deterministically reproduce the same results, I need something similar to get_state() and set_state() from numpy.random.RandomState Considering this: from pycuda.curandom import XORWOWRandomNumberGenerator gen = XORWOWRandomNumberGenerator() How can I obtain the full state of gen in order to load it into numpy arrays? How to reproduce the exact same state of gen based on these previously obtained numpy arrays?",
        "answers": [
            [
                "I did not find an out-of-the-box solution. Thus, I derived the following class from XORWOWRandomNumberGenerator: from pycuda.curandom import XORWOWRandomNumberGenerator import pycuda.driver as drv import numpy class PersistableXORWOWRandomNumberGenerator(XORWOWRandomNumberGenerator): def get_state_size(self): from pycuda.characterize import sizeof data_type_size = sizeof(self.state_type, \"#include &lt;curand_kernel.h&gt;\") return self.block_count * self.generators_per_block * data_type_size def get_state(self, ary=None): if ary is None: ary = drv.from_device(self.state, (self.get_state_size(),), numpy.uint8) else: drv.memcpy_dtoh(ary, self.state) return ary def set_state(self, state): drv.memcpy_htod(self.state, state) It can be used like this: gen = PersistableXORWOWRandomNumberGenerator() # obtain the random state as a uint8 numpy array state_as_numpy = gen.get_state() # set the state from a uint8 numpy array gen.set_state(state_as_numpy)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "My python code has a gpu kernel function which is called multiple times in a for loop from host like this : for i in range: gpu_kernel_func(blocksize, grid) Since this function call requires communication between host and gpu device multiple times which is not efficient, I want to make this as gpu_kernel_function(){ for(){ computation } ; } But this requires extra step to make sure all the blocks in grid are in sync. According to dynamic parallelism, calling a dummy child kernel should ensure that every thread (in whole grid) should finish that child kernel before the code continues running. So I defined another kernel just like gpu_kernel_function and I tried this : GPUcode = ''' \\__global__ gpu_kernel_function() {... } \\__global__ dummy_child_kernel(){ ... } ''' gpu_kernel_function(){ for() { computation } ; dummy_child_kernel(void); } But I am getting this error \" nvcc fatal : Option '--cubin (-cubin)' is not allowed when compiling for a virtual compute architecture \" I am using Tesla P100 (compute 6.0), python 3.5, cuda.8.0.44. I am compiling my sourcemodule like this : mod = SourceModule(GPUcode, options=['-rdc=true' ,'-lcudart','-lcudadevrt','--machine=64'],arch='compute_60' ) I tried compute_35 too which gives same error.",
        "answers": [
            [
                "The error message is explicitly telling you what the issue is. compute_60 is a virtual architecture. You can't statically compile virtual architectures to machine code. They are intended for producing PTX (virtual machine assembler) for JIT translation to machine code by the runtime. PyCUDA compiles code to a binary payload (\"cubin\") using the CUDA toolchain and them loads it via the driver API into the CUDA context. Thus the error. You can fix the error by specifying a valid physical GPU target architecture. So you should modify the source module constructor call to something like this: mod = SourceModule(GPUcode, options=['-rdc=true','-lcudart','-lcudadevrt','--machine=64'], arch='sm_60' ) This should fix the compiler error. However, note that using dynamic parallelism requires device code linkage, and I am 99% sure that PyCUDA still doesn't support this, so you likely won't be able to do what you are asking about via a SourceModule. You could link your own cubin by hand using the compiler outside of PyCUDA and then load that cubin inside PyCUDA. You will find many examples of how to compile dynamic parallelism correctly if you search for them."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to implement a scan algorithm (Hillis-Steele) and I'm having some trouble understanding how to do it properly on CUDA. This is a minimal example using pyCUDA: import pycuda.driver as cuda import pycuda.autoinit import numpy as np from pycuda.compiler import SourceModule #compile cuda code mod = SourceModule(''' __global__ void scan(int * addresses){ for(int idx=1; idx &lt;= threadIdx.x; idx &lt;&lt;= 1){ int new_value = addresses[threadIdx.x] + addresses[threadIdx.x - idx]; __syncthreads(); addresses[threadIdx.x] = new_value; } } ''') func = mod.get_function(\"scan\") #Initialize an array with 1's addresses_h = np.full((896,), 1, dtype='i4') addresses_d = cuda.to_device(addresses_h) #Launch kernel and copy back the result threads_x = 896 func(addresses_d, block=(threads_x, 1, 1), grid=(1, 1)) addresses_h = cuda.from_device(addresses_d, addresses_h.shape, addresses_h.dtype) # Check the result is correct for i, n in enumerate(addresses_h): assert i+1 == n My question is about __syncthreads(). As you can see, I'm calling __syncthreads() inside the for loop and not every thread will execute that code the same number of times: ThreadID - Times it will execute for loop 0 : 0 times 1 : 1 times 2- 3 : 2 times 4- 7 : 3 times 8- 15 : 4 times 16- 31 : 5 times 32- 63 : 6 times 64-127 : 7 times 128-255 : 8 times 256-511 : 9 times 512-896 : 10 times There can be threads in the same warp with different number of calls to syncthreads. What will happen in that case? How can threads that are not executing the same code be synchronized? In the sample code, we start with an array full of 1's and we get in the output the index+1 as value for each position. It is computing the correct answer. Is it \"by chance\" or the code is correct? If this is not a proper use of syncthreads, how could I implement such algoritm using cuda?",
        "answers": [
            [
                "If this is not a proper use of syncthreads, how could I implement such algoritm using cuda? One typical approach is to separate the conditional code from the __syncthreads() calls. Use the conditional code to determine what threads will participate. Here's a simple transformation of your posted code, that should give the same result, without any violations (i.e. all threads will participate in every __syncthreads() operation): mod = SourceModule(''' __global__ void scan(int * addresses){ for(int i=1; i &lt; blockDim.x; i &lt;&lt;= 1){ int new_value; if (threadIdx.x &gt;= i) new_value = addresses[threadIdx.x] + addresses[threadIdx.x - i]; __syncthreads(); if (threadIdx.x &gt;= i) addresses[threadIdx.x] = new_value; } } ''') I'm not suggesting this is a complete and proper scan, or that it is optimal, or anything of the sort. I'm simply showing how your code can be transformed to avoid the violation inherent in what you have. If you want to learn more about scan methods, this is one source. But if you actually need a scan operation, I would suggest using thrust or cub."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a Python 3 program that involves the execution of a cuda kernel. The code runs fine when I launch it in the following configuration GeForce GTX 1080 Ti GPU Ubuntu 16.04 CUDA version 8.0.61 NVIDIA driver version 384.111 Python version 3.5.2 PyCUDA version (2017, 1, 1). However, when using a GeForce GTX 970 on the very same machine, I get this error: cuMemFree failed: the launch timed out and was terminated PyCUDA WARNING: a clean-up operation failed (dead context maybe?) Note that this error does not occur when I call the kernel with a rather small number of threads (i.e. with a small grid dimension at constant threads per block). In this post, Andreas explains the meaning of that error message: This means your context went away while PyCUDA was still talking to it. This will happen most often if you perform some invalid operation (such as access out-of-bounds memory in a kernel). In other words, it seems to indicate that something is wrong with the kernel I wrote. However, as the code does not raise an error when launched on the other GPU, I was wondering if other issues can raise the same error, too. So my questions are: Can the above error also be caused when running a correctly written kernel in an unfavourable environment? Can it be caused by a wrong combination of NVIDIA driver, CUDA version, PyCUDA version and GPU model? What do I have in general to consider regarding driver version, CUDA version, PyCUDA version and GPU model to assure that things function properly? I can understand that many people here are allergic to questions without code and minimal example. I tried to compose a simple example that would reproduce the error, but I couldn't. Kernels that would like double an input argument or so run fine up to the limit of memory errors... So I hope to just get some advice into what direction to look when searching the error.",
        "answers": [
            [
                "It was talonmies' comment to the question that lead me to the answer. The issue was that one of the cards (the GTX 970) was at the same time used for graphical output of the system. As explained here and here, this implies that there is a \"watchdog\" preventing CUDA kernels to run longer than some maximum time before they are stopped. The solution for me was to stop the X server by sudo service lightdm stop. Then, the program ran on both cards without error."
            ],
            [
                "Adding to Amos's answer above, for Linx 18.04 I had to use sudo service gdm stop. In addition, if that still doesn't work (it didn't for me), try opening a terminal using ctrl+alt+f3 and running your program through this."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am writing a cuda kernel to convert rgba image to gray scale image in pycuda, here is the PyCUDA code: import numpy as np import matplotlib.pyplot as plt import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule kernel = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void rgba_to_greyscale(const uchar4* const rgbaImage, unsigned char* const greyImage, int numRows, int numCols) { int y = threadIdx.y+ blockIdx.y* blockDim.y; int x = threadIdx.x+ blockIdx.x* blockDim.x; if (y &lt; numCols &amp;&amp; x &lt; numRows) { int index = numRows*y +x; uchar4 color = rgbaImage[index]; unsigned char grey = (unsigned char)(0.299f*color.x+ 0.587f*color.y + 0.114f*color.z); greyImage[index] = grey; } } \"\"\") However, the problem is how to relate uchar4* to numpy array. I know can modify my kernel function to accept int* or float*, and make it work. But I just wonder how to make the above kernel function to work in pycuda. Below is host code. def gpu_rgb2gray(image): shape = image.shape n_rows, n_cols, _ = np.array(shape, dtype=np.int) image_gray = np.empty((n_rows, n_cols), dtype= np.int) ## HERE is confusing part, how to rearrange image to match unchar4* ?? image = image.reshape(1, -1, 4) # Get kernel function rgba2gray = kernel.get_function(\"rgba_to_greyscale\") # Define block, grid and compute blockDim = (32, 32, 1) # 1024 threads in total dx, mx = divmod(shape[1], blockDim[0]) dy, my = divmod(shape[0], blockDim[1]) gridDim = ((dx + (mx&gt;0)), (dy + (my&gt;0)), 1) # Kernel function # HERE doesn't work because of mismatch rgba2gray ( cuda.In(image), cuda.Out(image_gray), n_rows, n_cols, block=blockDim, grid=gridDim) return image_gray Anyone have any ideas? Thanks!",
        "answers": [
            [
                "The gpuarray class has native support for CUDA's built in vector types (including uchar4). So you can create as gpuarray instance with the correct dtype for the kernel, and copy the host image to that gpuarray using buffers, then use the gpuarray as the kernel input argument. As an example (and if I understood your code correctly), something like this should probably work: import pycuda.gpuarray as gpuarray .... def gpu_rgb2gray(image): shape = image.shape image_rgb = gpuarray.empty(shape, dtype=gpuarray.vec.uchar4) cuda.memcpy_htod(image_rgb.gpudata, image.data) image_gray = gpuarray.empty(shape, dtype=np.uint8) # Get kernel function rgba2gray = kernel.get_function(\"rgba_to_greyscale\") # Define block, grid and compute blockDim = (32, 32, 1) # 1024 threads in total dx, mx = divmod(shape[1], blockDim[0]) dy, my = divmod(shape[0], blockDim[1]) gridDim = ((dx + (mx&gt;0)), (dy + (my&gt;0)), 1) rgba2gray ( image_rgb, image_gray, np.int32(shape[0]), np.int32(shape[1]), block=blockDim, grid=gridDim) img_gray = np.array(image_gray.get(), dtype=np.int) return img_gray this would take an image of 32 bit unsigned integers and copy them to an array of uchar4 on the GPU and then upcast the resulting array of uchar back to integers on the device."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I intended to write a kernel in PyCUDA to generate 2d Gaussian patches. However, values defined by me in the host change after copy them into device. Below is the code. import numpy as np import matplotlib.pyplot as plt import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit # kernel kernel = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void gaussian2D(float *output, float x, float y, float sigma, int n_rows, int n_cols) { int i = threadIdx.x + blockIdx.x * blockDim.x; int j = threadIdx.y + blockIdx.y * blockDim.y; printf(\"%d \", n_cols); if (i &lt; n_cols &amp;&amp; j &lt; n_rows) { size_t idx = j*n_cols +i; //printf(\"%d \", idx); } } \"\"\") # host code def gpu_gaussian2D(point, sigma, shape): # Convert parameters into numpy array x, y = np.array(point, dtype=np.float32) sigma = np.float32(sigma) n_rows, n_cols = np.array(shape, dtype=np.int) print(n_rows) output = np.empty((1, shape[0]*shape[1]), dtype= np.float32) # Get kernel function gaussian2D = kernel.get_function(\"gaussian2D\") # Define block, grid and compute blockDim = (32, 32, 1) # 1024 threads in total dx, mx = divmod(shape[1], blockDim[0]) dy, my = divmod(shape[0], blockDim[1]) gridDim = ((dx + (mx&gt;0)), (dy + (my&gt;0)), 1) # Kernel function gaussian2D ( cuda.Out(output), cuda.In(x), cuda.In(y), cuda.In(sigma), cuda.In(n_rows), cuda.In(n_cols), block=blockDim, grid=gridDim) return output point = (5, 5) sigma = 3.0 shape = (10, 10) result = gpu_gaussian2D(point, sigma, shape) After checking the print value of n_cols, it is NOT 10 as expected. anyone can help me, I cannot figure out what's going wrong here.",
        "answers": [
            [
                ".In() and .Out() are only used for buffers that will be passed via pointer parameters in the kernel (so only applicable to output here). Ordinary pass-by-value parameters can be used directly. $ cat t7.py import numpy as np # import matplotlib.pyplot as plt import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit # kernel kernel = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void gaussian2D(float *output, float x, float y, float sigma, int n_rows, int n_cols) { int i = threadIdx.x + blockIdx.x * blockDim.x; int j = threadIdx.y + blockIdx.y * blockDim.y; printf(\"%d \", n_cols); if (i &lt; n_cols &amp;&amp; j &lt; n_rows) { size_t idx = j*n_cols +i; //printf(\"%d \", idx); } } \"\"\") # host code def gpu_gaussian2D(point, sigma, shape): # Convert parameters into numpy array x, y = np.array(point, dtype=np.float32) sigma = np.float32(sigma) n_rows, n_cols = np.array(shape, dtype=np.int) print(n_rows) output = np.empty((1, shape[0]*shape[1]), dtype= np.float32) # Get kernel function gaussian2D = kernel.get_function(\"gaussian2D\") # Define block, grid and compute blockDim = (32, 32, 1) # 1024 threads in total dx, mx = divmod(shape[1], blockDim[0]) dy, my = divmod(shape[0], blockDim[1]) gridDim = ((dx + (mx&gt;0)), (dy + (my&gt;0)), 1) # Kernel function gaussian2D ( cuda.Out(output), x, y, sigma, n_rows, n_cols, block=blockDim, grid=gridDim) return output point = (5, 5) sigma = 3.0 shape = (10, 10) result = gpu_gaussian2D(point, sigma, shape) $ python t7.py 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to work with an array of strings(words) in CUDA. I tried flattening it by creating a single string, but then then to index it, I'd have to go through some of it each time a kernel runs. If there are 9000 words with a length of 6 characters, I'd have to examine 53994 characters in the worst case for each kernel call. So I'm looking for different ways to do it. Update: Forgot to mention, the strings are of different lengths, so I'd have to find the end of each one. The next thing I tried was copying each word to different memory locations, and then collect the addresses, and pass that to the GPU as an array with the following code: # np = numpy wordList = ['asd','bsd','csd'] d_words = [] for word in wordList: d_words.append(gpuarray.to_gpu(np.array(word, dtype=str))) d_wordList = gpuarray.to_gpu(np.array([word.ptr for word in d_words], dtype=np.int32)) ker_test(d_wordList, block=(1,1,1), grid=(1,1,1)) and in the kernel: __global__ void test(char** d_wordList) { printf(\"First character of the first word is: %c \\n\", d_wordList[0][0]); } The kernel should get an int32 array of pointers that point to the beginning of each word, effectively being a char** (or int**), but it doesn't work as I expect. What is wrong with this approach? Also what are the \"standard\" ways to work with strings in PyCUDA (or even in CUDA) in general? Thanks in advance.",
        "answers": [
            [
                "After some further thought, I've concluded that for this case of variable-length strings, using an \"offset array\" may not be much different than 2D indexing (i.e. double-pointer indexing), when considering the issue of data access within the kernel. Both involve a level of indirection. Here's a worked example demonstrating both methods: $ cat t5.py #!python #!/usr/bin/env python import time import numpy as np from pycuda import driver, compiler, gpuarray, tools import math from sys import getsizeof import pycuda.autoinit kernel_code1 = \"\"\" __global__ void test1(char** d_wordList) { (d_wordList[blockIdx.x][threadIdx.x])++; } \"\"\" kernel_code2 = \"\"\" __global__ void test2(char* d_wordList, size_t *offsets) { (d_wordList[offsets[blockIdx.x] + threadIdx.x])++; } \"\"\" mod = compiler.SourceModule(kernel_code1) ker_test1 = mod.get_function(\"test1\") wordList = ['asd','bsd','csd'] d_words = [] for word in wordList: d_words.append(gpuarray.to_gpu(np.array(word, dtype=str))) d_wordList = gpuarray.to_gpu(np.array([word.ptr for word in d_words], dtype=np.uintp)) ker_test1(d_wordList, block=(3,1,1), grid=(3,1,1)) for word in d_words: result = word.get() print result mod2 = compiler.SourceModule(kernel_code2) ker_test2 = mod2.get_function(\"test2\") wordlist2 = np.array(['asdbsdcsd'], dtype=str) d_words2 = gpuarray.to_gpu(np.array(['asdbsdcsd'], dtype=str)) offsets = gpuarray.to_gpu(np.array([0,3,6,9], dtype=np.uint64)) ker_test2(d_words2, offsets, block=(3,1,1), grid=(3,1,1)) h_words2 = d_words2.get() print h_words2 $ python t5.py bte cte dte ['btectedte'] $ Notes: for the double-pointer case, the only change from OP's example was to use the numpy.uintp type for the pointer as suggested in the comments by @talonmies I don't think the double-pointer access of data will necessarily be quicker or slower than the indirection associated with the offset lookup method. One other performance consideration would be in the area of copying data from host to device and vice versa. The double pointer method effectively involves multiple allocations and multiple copy operations, in both directions, I believe. For a lot of strings, this will be noticeable in the host/device data copy operations. Another possible merit of the offset method is that it is easy to determine the length of each string - just subtract two adjacent entries in the offset list. This could be useful so as to make it easy to determine how many threads can operate on a string in parallel, as opposed to having a single thread work on a string sequentially (or use a method in kernel code to determine string length, or pass the length of each string)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm relatively new to all of the things mentioned in the title, so bear with me. Currently I'm stuck with the conversion between python and C. Since CUDA kernels are written in C, so I can't just look at it the python way. As the documentation is rather limited, and overly complex for a beginner, I'd like to ask how pyCuda actually converts python(or numpy) arrays for use in C. For example, a string \"stuff\" is an array of characters in C, but in python it's an immutable string. However I can do the following: stuff = \"stuff\" d_stuff = cuda.mem_alloc(len(stuff)) cuda.memcpy_htod(d_stuff, stuff) And in the CUDA kernel, now I can use it as a char* d_stuff. However I cannot get it back the same way, as a python string is immutable. So doing the following will obviously raise an error: newstuff = \"\" cuda.memcpy_dtoh(newstuff, d_stuff) I know that these can be written as d_stuff = gpuarray.to_gpu(numpy.array(stuff)) # I need numpy, as the to_gpu expects an array newstuff = d_stuff.get() But I have no idea how it works, and what it does behind the scenes, so I would really appreciate, if anyone could explain how the conversions work briefly. (E.g. how does the second example give back a string) Also I have questions about arrays created with numpy. I've seen that they're widely used for GPUs but I don't know how they work. Does giving numpy a string create an array of characters in terms of C code, if yes, does an array of strings become char, or something else? (when translated to C of course) Writing CUDA code probably would be better with C only, but I'd like to explore the features of python, and I'm doing all this for learning purposes.",
        "answers": [
            [
                "I'd like to ask how PyCUDA actually converts python(or numpy) arrays for use in C. It doesn't. PyCUDA simply takes any object which supports the Python buffer protocol (typically a numpy array) and directly accesses its host memory buffer to transfer data to and from the GPU. No type conversion or data manipulation is ever performed. Types are directly inferred from the CTypes interface, (typically via a numpy dtype, given that numpy arrays are the usual data source). Does giving numpy a string create an array of characters in terms of C code, if yes, does an array of strings become char, or something else? That depends. For example doing this: ttt = np.asarray([ \"stuff\" + str(i) for i in range(0,20) ]) print( ttt.dtype, type(ttt[0]) ) |S7 &lt;type 'numpy.string_'&gt; Here numpy uses a special fixed length string data type whose length is calculated from the input data. This is effective a C ordered array of char[7]. See more here. PyCUDA automagically understands how to handle this because of the buffer protocol and the underlying direct mapping to a native C type. However you can also do this: ttt = np.asarray([ \"stuff\" + str(i) for i in range(0,20) ], dtype=object) print( ttt.dtype, type(ttt[0]) ) object &lt;type 'str'&gt; Here, the numpy array created contains Python objects (in this case a string). This is not something which can be used in PyCUDA, because the Python object doesn't have a direct representation in C."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I have some codes in python3 like this: import numpy as np import pycuda.driver as cuda from pycuda.compiler import SourceModule, compile import tensorflow as tf # create device and context cudadevice=cuda.Device(gpuid1) cudacontext=cudadevice.make_context() config = tf.ConfigProto() config.gpu_options.visible_device_list={}.format(gpuid2) sess = tf.Session(config=config) # compile from a .cu file cuda_mod = SourceModule(cudaCode, include_dirs = [dir_path], no_extern_c = True, options = ['-O0']) # in the .cu code a texture named \"map\" is defined as: # texture&lt;float4, cudaTextureType2D, cudaReadModeElementType&gt; map; texRef = cuda_mod.get_texref('map') # tex is a np.ndarray with shape 256*256*4, and it is the output of a tensorflow's graph by calling sess.run() tex = np.ascontiguousarray(tex).astype(np.float32) tex_gpu = cuda.make_multichannel_2d_array(tex, 'C') # error here!!!!! texRef.set_array(tex_gpu) and the error message: pycuda._driver.LogicError: cuTexRefSetArray failed: peer access has not been enabled The peer access error appeared when tensorflow is also on use (even if gpuid1 and gpuid2 are same), but everything goes right without tensorflow. I found that \"peer access\" has something to do with communicating between GPUs (devices). But what I'm doing here is just setting a numpy array to GPU memory as texture, so I think it has nothing to do with transferring data between different GPUs. So what's wrong with it? Thanks!",
        "answers": [
            [
                "It seems I have found the solution. When texRef.set_array(tex_gpu) is inserted between cudadevice.cudacontext.push() and cudadevice.cudacontext.pop() to explicitly switch the cuda context, everything goes okay."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm currently working on a dot product with pinned memory using PyCUDA. And I have a problem with big arrays. I'm working with: NVIDIA GTX 1060 CUDA 9.1 PyCUDA 2017.1.1 The code is: #!/usr/bin/env python import numpy as np import argparse import math import pycuda.autoinit import pycuda.driver as drv from pycuda.compiler import SourceModule from time import time dot_mod = SourceModule(\"\"\" __global__ void full_dot( double* v1, double* v2, double* out, int N ) { __shared__ double cache[ 1024 ]; int i = blockIdx.x * blockDim.x + threadIdx.x; cache[ threadIdx.x ] = 0.f; while( i &lt; N ) { cache[ threadIdx.x ] += v1[ i ] * v2[ i ]; i += gridDim.x * blockDim.x; } __syncthreads(); // required because later on the current thread is accessing // data written by another thread i = 1024 / 2; while( i &gt; 0 ) { if( threadIdx.x &lt; i ) cache[ threadIdx.x ] += cache[ threadIdx.x + i ]; __syncthreads(); i /= 2; //not sure bitwise operations are actually faster } #ifndef NO_SYNC // serialized access to shared data; if( threadIdx.x == 0 ) atomicAdd( out, cache[ 0 ] ); #else // no sync, what most likely happens is: // 1) all threads read 0 // 2) all threads write concurrently 16 (local block dot product) if( threadIdx.x == 0 ) *out += cache[ 0 ]; #endif } \"\"\") def main(args): dot = dot_mod.get_function(\"full_dot\") N = args.number BLOCK_SIZE = 1024 BLOCKS = int(math.ceil(N/BLOCK_SIZE)) THREADS_PER_BLOCK = BLOCK_SIZE # Time use of pinned host memory: x = drv.aligned_empty((N), dtype=np.float64, order='C') x = drv.register_host_memory(x, flags=drv.mem_host_register_flags.DEVICEMAP) x_gpu_ptr = np.intp(x.base.get_device_pointer()) # Time use of pinned host memory: y = drv.aligned_empty((N), dtype=np.float64, order='C') y = drv.register_host_memory(y, flags=drv.mem_host_register_flags.DEVICEMAP) y_gpu_ptr = np.intp(y.base.get_device_pointer()) # Time use of pinned host memory: z = drv.aligned_empty((1), dtype=np.float64, order='C') z = drv.register_host_memory(z, flags=drv.mem_host_register_flags.DEVICEMAP) z_gpu_ptr = np.intp(z.base.get_device_pointer()) z[:] = np.zeros(1) x[:] = np.zeros(N) y[:] = np.zeros(N) x[:] = np.random.rand(N) y[:] = x[:] x_orig = x.copy() y_orig = y.copy() start = time() dot(x_gpu_ptr, y_gpu_ptr, z_gpu_ptr, np.uint32(N), block=(THREADS_PER_BLOCK, 1, 1), grid=(BLOCKS,1)) times = time()-start print \"Average kernel GPU dot product execution time with pinned memory: %3.7f\" % np.mean(times) start = time() ydot=np.dot(x_orig,y_orig) times = time()-start print \"Average numpy dot product execution time: %3.7f\" % np.mean(times) print N,ydot,z[0] if __name__ == \"__main__\": parser = argparse.ArgumentParser(description=' ') parser.add_argument('-n', dest='number', type=long, help=\"Number of samples \", required=True) args = parser.parse_args() main(args) I have wrote this code that works well when sample array is of size aprox less than 1024*12, but for big arrays like 1024*1024 value gives a wrong result.- \u279c ./test_dot_pinned.py -n 16384 Average kernel GPU dot product execution time with pinned memory: 0.0001669 Average numpy dot product execution time: 0.0000119 16384 5468.09590706 5468.09590706 SIZE np.dot() GPU-dot-pinned \u279c ./test_dot_pinned.py -n 1048576 Average kernel GPU dot product execution time with pinned memory: 0.0002351 Average numpy dot product execution time: 0.0010922 1048576 349324.532564 258321.148593 SIZE np.dot() GPU-dot-pinned Thanks to everyone, I hope someone could help me.",
        "answers": [
            [
                "pycuda doesn't enforce any synchronization after a kernel launch. Normally, if you do a device-&gt;host copy of data after a kernel launch, the operation will force a synchronization, i.e. it will force the kernel to complete. But you have no such synchronization in your code. Since you are using pinned memory, as the kernel execution time grows (due to larger work size) eventually when you print out z[0] you are only getting a partial result, because the kernel is not finished at that point. A side effect of this also is that your kernel time measurement is not accurate. You can fix both of these by forcing the kernel to complete before you finish your time measurement: dot(x_gpu_ptr, y_gpu_ptr, z_gpu_ptr, np.uint32(N), block=(THREADS_PER_BLOCK, 1, 1), grid=(BLOCKS,1)) #add the next line of code: drv.Context.synchronize() times = time()-start"
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I am trying to load a 3d array into pycuda (i'm going to load images). I want each thread to handle all the channels of a single pixel using a for loop(this is an algorithmic requirement). So far I have this working: from pycuda.compiler import SourceModule mod = SourceModule(open(\"./cudacode.cu\").read()) multiply_them = mod.get_function(\"multiply_them\") rows,cols = 800,400 a = numpy.random.randn(rows,cols,3).astype(numpy.float32) b = numpy.random.randn(rows,cols,3).astype(numpy.float32) threads = 20 blocks = 16 dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b),np.int32(rows), np.int32(cols), # notice it is rows, cols block=(blocks,blocks,1), grid=(rows//blocks,cols//blocks)) print( dest- 2*a*b) print(np.unique( dest- 2*a*b)) and my cuda code is : __global__ void multiply_them(float *dest, float *a, float *b,int cols,int rows) # notice it is cols , rows and not rows, cols { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; if(row&lt;rows &amp;&amp; col &lt;cols){ dest[3* (row*cols + col)] = 2.0* a[3* (row*cols + col) ]* b[3* (row*cols + col) ] ; ## first channel of some pixel dest[3* (row*cols + col)+1] = 2.0* a[3* (row*cols + col)+1]* b[3* (row*cols + col)+1] ; ## second channel of that pixel dest[3* (row*cols + col)+2] = 2.0* a[3* (row*cols + col)+2]* b[3* (row*cols + col)+2] ; ## 3rd channel of that pixel } } notice that in my cuda function, the rows and columns are switched. This whole code works fine the way it is. I am able to say my code works fine because print(np.unique( dest- 2*a*b)) prints 0.0 The dest[3* (row*cols + col)] style of indexing , I found from here https://wiki.tiker.net/PyCuda/Examples/GlInterop My question is, why does max rows and max cols need to be switched in order for this to work ? And what is the more logically correct way to get to do what I want? I am new to cuda , so please be patient as I may ask questions that will probably be very stupid",
        "answers": [
            [
                "Numpy arrays use row-major ordering by default. Threads within CUDA blocks are numbered so that threadIdx.x is the fastest varying dimension, and threadIdx.z is the slowest (functionally equivalent to column major ordering). So if you wish to access row-major ordered data, as you would in a default ordered numpy array, and maintain access ordering which allows memory coalescing, then you should \"reverse\" the use of the x and y dimensions within a block. If your numpy array was column major ordered (order='F'), then you would not do this. That is why your code works correctly when your intuition is that it should not."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am doing a PyCuda code and I would like to get the property of the graphic card (like size of warps, max threads per block etc). So I went to this page : https://documen.tician.de/pycuda/driver.html And I saw this : Then I wrote in my code the following : import time import numpy as np from pycuda import driver, compiler, gpuarray, tools import math # -- initialize the device import pycuda.autoinit print(pycuda.driver.device_attribute.WARP_SIZE) But the print returns : WARP_SIZE Indeed he returns a str containing \"WARP_SIZE\" and not the integer representing the warp size. What am I doing wrong ?",
        "answers": [
            [
                "What you are printing out is the enumeration which needs to be passed to the device interface to retrieve that attribute. You want something like this: import time import numpy as np from pycuda import driver, compiler, gpuarray, tools import math # -- initialize the device import pycuda.autoinit dev = pycuda.autoinit.device print(dev.get_attribute(pycuda.driver.device_attribute.WARP_SIZE)) print(dev.get_attribute(pycuda.driver.device_attribute.MAX_BLOCK_DIM_X)) which does this: $ python device_attr.py 32 1024"
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I would like to understand why the following PyCUDA code doesn't work. The error that I have is: TypeError: invalid type on parameter #3 (0-based) And the error occurs on the block line of my call of the function. In the code it is at the line block = (MATRIX_SIZE,MATRIX_SIZE,1), 2 lines before the end. Does anyone know what is the mistake here? I tried a lot of things but I can't figure out. The CUDA code is working in C++, I am just trying to translate it in PyCUDA now and it is where it fails. import numpy as np from pycuda import driver, compiler, gpuarray, tools # -- initialize the device import pycuda.autoinit kernel_code_template = \"\"\" __global__ void MatMult(float* C, float* A, float*B, int dimAx, int dimBx, int dimCx, int dimCy) { int row = blockDim.y*blockIdx.y+threadIdx.y; int col = blockDim.x*blockIdx.x+threadIdx.x; double Result = 0; if (row&lt;=dimCy-1 &amp;&amp; col&lt;=dimCx-1) { for (int k = 0; k &lt; dimAx; k++) { Result += A[k + dimAx*row] * B[col + dimBx*k]; } C[col + row*dimCx] = Result; } } \"\"\" MATRIX_SIZE=3 # I create my variables : a_cpu=np.asarray([[0,1,2],[10,11,12],[20,21,22]]) b_cpu=np.asarray([[0,0,0],[1,2,3],[4,8,12]]) a_gpu = gpuarray.to_gpu(a_cpu) b_gpu = gpuarray.to_gpu(b_cpu) size_Ax=a_cpu.shape[1] size_Bx=b_cpu.shape[1] size_Ay=a_cpu.shape[0] size_Cx=size_Bx # Cx=Bx because of matrix product size_Cy=size_Ay # Cy=Ay # create empty gpu array for the result (C = A * B) c_gpu = gpuarray.empty((size_Cy, size_Cx), np.float32) # get the kernel code from the template kernel_code=kernel_code_template # compile the kernel code mod = compiler.SourceModule(kernel_code) # get the kernel function from the compiled module matrixmul = mod.get_function(\"MatMult\") # call the kernel on the card matrixmul( # outputs c_gpu, # inputs a_gpu, b_gpu, size_Ax,size_Bx,size_Cx,size_Cy, # (only one) block of MATRIX_SIZE x MATRIX_SIZE threads block = (MATRIX_SIZE,MATRIX_SIZE,1), )",
        "answers": [
            [
                "Your interpretation of the source of the error is incorrect. The error message: \"TypeError: invalid type on parameter #3 (0-based)\" is telling you that the fourth parameter size_Ax has an incorrect type. The error is not with the block argument. The reason for this is that PyCUDA enforces strict type safety when passing data to and from the GPU. Your kernel signature requires int values for dimAx, dimBx, dimCx, and dimCy,which are 32 bit. Python integers are 64 bit by default. You need to explicitly cast the arguments to the correct ctype, something like: matrixmul( # outputs c_gpu, # inputs a_gpu, b_gpu, np.int32(size_Ax),np.int32(size_Bx),np.int32(size_Cx),np.in32(size_Cy), # (only one) block of MATRIX_SIZE x MATRIX_SIZE threads block = (MATRIX_SIZE,MATRIX_SIZE,1), ) should work correctly."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to adapt the code found here: https://wiki.tiker.net/PyCuda/Examples/ThrustInterop ...to use cuda streams. (Please excuse that I'm new to c++, and have a few weeks experience with cuda only.) My main attempt and sticking point has been along the lines of adjusting the NVCC function like so to receive a cuda stream as an arg, and supply to the Thrust call: nvcc_function = FunctionBody( FunctionDeclaration(Value('void', 'my_sort'), [Value('CUdeviceptr', 'input_ptr'), Value('int', 'length'), Value('cudaStream_t','stream')]), Block([Statement('thrust::device_ptr&lt;float&gt; thrust_ptr((float*)input_ptr)'), Statement('thrust::sort(thrust::cuda::par.on(stream),thrust_ptr, thrust_ptr+length)')])) I'm getting the error \"'cudaStream_t' has not been declared\" (referring to the NVCC function argument). I've tried adding 'cuda_runtime.h' to both the host and device includes lists but to no avail.",
        "answers": [
            [
                "I am not familiar with pyCUDA or thrust but I am familiar with CUDA. One of the possible things that come to mind is that some reason the \"cuda_runtime.h\" might not be included despite being specified. Are you sure that the pyCUDA framework will indicate an error when it cannot find a specific include? Also another thing that caught my attention is that you are using CUdeviceptr which is a part of the driver api, whereas cudaStream_t is a part of the runtime api, which operates on a different level. From NVIDIA documentation, it seems that the driver api equivalent type would be CUstream. Source: http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__STREAM.html#group__CUDA__STREAM So the problem might be in mixing the functionality of apis on different levels. As I said though, I am not familiar with the exact framework you're using, those are just some suggestions that might or might not turn out useful. Good luck with debugging!"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I was trying to compute Local Binary Patterns for a image on my GPU, utilising cuda module in python for the same. But the results produced by execution of similar algorithm on CPU and GPU is producing different results. Can you help me figure out the problem ? Below is the snippet of code I was trying to execute : from __future__ import division from skimage.io import imread, imshow from numba import cuda import time import math import numpy # CUDA Kernel @cuda.jit def pointKernelLBP(imgGPU, histVec, pos) : ''' Computes Point Local Binary Pattern ''' row, col = cuda.grid(2) if row+1 &lt; imgGPU.shape[0] and col+1 &lt; imgGPU.shape[1] and col-1&gt;=0 and row-1&gt;=0 : curPos = 0 mask = 0 for i in xrange(-1, 2) : for j in xrange(-1, 2) : if i==0 and j==0 : continue if imgGPU[row+i][col+j] &gt; imgGPU[row][col] : mask |= (1&lt;&lt;curPos) curPos+=1 histVec[mask]+=1 #Host Code for computing LBP def pointLBP(x, y, img) : ''' Computes Local Binary Pattern around a point (x,y), considering 8 nearest neighbours ''' pos = [0, 1, 2, 7, 3, 6, 5, 4] curPos = 0 mask = 0 for i in xrange(-1, 2) : for j in xrange(-1, 2) : if i==0 and j==0 : continue if img[x+i][y+j] &gt; img[x][y] : mask |= (1&lt;&lt;curPos) curPos+=1 return mask def LBPHistogram(img, n, m) : ''' Computes LBP Histogram for given image ''' HistVec = [0] * 256 for i in xrange(1, n-1) : for j in xrange(1, m-1) : HistVec[ pointLBP(i, j, img) ]+=1 return HistVec if __name__ == '__main__' : # Reading Image img = imread('cat.jpg', as_grey=True) n, m = img.shape start = time.time() imgHist = LBPHistogram(img, n, m) print \"Computation time incurred on CPU : %s seconds.\\n\" % (time.time() - start) print \"LBP Hisogram Vector Using CPU :\\n\" print imgHist print type(img) pos = numpy.ndarray( [0, 1, 2, 7, 3, 6, 5, 4] ) img_global_mem = cuda.to_device(img) imgHist_global_mem = cuda.to_device(numpy.full(256, 0, numpy.uint8)) pos_global_mem = cuda.to_device(pos) threadsperblock = (32, 32) blockspergrid_x = int(math.ceil(img.shape[0] / threadsperblock[0])) blockspergrid_y = int(math.ceil(img.shape[1] / threadsperblock[1])) blockspergrid = (blockspergrid_x, blockspergrid_y) start = time.time() pointKernelLBP[blockspergrid, threadsperblock](img_global_mem, imgHist_global_mem, pos_global_mem) print \"Computation time incurred on GPU : %s seconds.\\n\" % (time.time() - start) imgHist = imgHist_global_mem.copy_to_host() print \"LBP Histogram as computed on GPU's : \\n\" print imgHist, len(imgHist)",
        "answers": [
            [
                "Now that you have fixed the glaringly obvious mistake in the original kernel code you posted, there are two problems which are preventing this code from working correctly. The first, and most serious, is a memory race in the kernel. This update of the histogram bins: histVec[mask]+=1 is not safe. Multiple threads in multiple blocks will attempt to read and write the same bin counters in global memory simultaneously. CUDA offers no guarantees of either correctness or repeatability in such circumstances. The simplest (but not necessarily the most performant, depending on your hardware) solution to this is to use atomic memory transactions. These do guarantee that the increment operation will be serialized, but of course that serialization implies some performance penalty. You can do this by changing the update code to something like: cuda.atomic.add(histVec,mask,1) Note that CUDA only supports 32 and 64 bit atomic memory transactions, so you will need to ensure the type of histVec is a compatible 32 or 64 bit integer type. This leads to the second problem, which is that you have defined the bin counter vector to be numpy.uint8. That means that even if you didn't have a memory race, you would only have 8 bits to store the counts, and they would quickly overflow for images of any meaningful size. So for both compatibility with atomic memory transactions and to prevent counter rollover, you will need to change the type of your counters. When I changed these things in the code you posted (and fixed the early missing code problem), I could get exact agreement between GPU and host code calculated histograms for a random 8 bit input array. The underlying parallel histogram problem is very well described for CUDA and there are a lot of examples and codebases you can study when you get to the point of worrying about performance, for example here."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm quite new to cuda and pycuda. I need a kernel that creates a matrix (of dimension n x d) out of an array (1 x d), by simply \"repeating\" the same array n times: for example, suppose we have n = 4 and d = 3, then if the array is [1 2 3] the result of my kernel should be: [1 2 3 1 2 3 1 2 3 1 2 3] (a matrix 4x3). Basically, it's the same as doing numpy.tile(array, (n, 1)) I've written the code below: kernel_code_template = \"\"\" __global__ void TileKernel(float *in, float *out) { // Each thread computes one element of out int y = blockIdx.y * blockDim.y + threadIdx.y; int x = blockIdx.x * blockDim.x + threadIdx.x; if (y &gt; %(n)s || x &gt; %(d)s) return; out[y * %(d)s + x] = in[x]; } \"\"\" d = 64 n = 512 blockSizex = 16 blockSizey = 16 gridSizex = (d + blockSizex - 1) / blockSizex gridSizey = (n + blockSizey - 1) / blockSizey # get the kernel code from the template kernel_code = kernel_code_template % { 'd': d, 'n': n } mod = SourceModule(kernel_code) TileKernel = mod.get_function(\"TileKernel\") vec_cpu = np.arange(d).astype(np.float32) # just as an example vec_gpu = gpuarray.to_gpu(vec_cpu) out_gpu = gpuarray.empty((n, d), np.float32) TileKernel.prepare(\"PP\") TileKernel.prepared_call((gridSizex, gridSizey), (blockSizex, blockSizey, 1), vec_gpu.gpudata, out_gpu.gpudata) out_cpu = out_gpu.get() Now, if I run this code with d equals a power of 2 &gt;= 16 I get the right result (just like numpy.tile(vec_cpu, (n, 1)) ); but if I set d equals to anything else (let's say for example 88) I get that every element of the output matrix has the correct value, except the first column: some entries are right but others have another value, apparently random, same for every wrong element, but different every run, and also the entries of the first column that have the wrong value are different every run. Example: [0 1 2 0 1 2 6 1 2 0 1 2 6 1 2 ...] I really can't figure out what is causing this problem, but maybe it's just something simple that I'm missing... Any help will be appreciated, thanks in advance!",
        "answers": [
            [
                "The bounds checking within your kernel code is incorrect. This if (y &gt; n || x &gt; d) return; out[y * d + x] = in[x]; should be: if (y &gt;= n || x &gt;= d) return; out[y * d + x] = in[x]; or better still: if ((y &lt; n) &amp;&amp; (x &lt; d)) out[y * d + x] = in[x]; All array valid indexing in the array lies on 0 &lt; x &lt; d and 0 &lt; y &lt; n. By allowing x=d you have undefined behaviour, allowing the first entry in the next row of the output array to be overwritten with an unknown value. This explains why sometimes the results were correct and other times not."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "We only have 4 GPU devices. and we have more than 4 users to run cuda program ,so before I run my program I want to check which device is not busy, or it will alloc memory fail. But I havent found a function to get this tag. I know when we want to use device we call \"cudaSetDevice()\" , so there must be a tag for each device. and that \"nvidia-smi\" can get more detail, include which proccess is using which device and how much memory it used. So who can help me?",
        "answers": [
            [
                "The values for cudaSetDevice start at 0 and then increase monotonically for each additional device. Alternatively you can set the environment variable CUDA_VISIBLE_DEVICES to select which device to use. (see https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/). To get information about what is using the device you need to use the driver API: http://docs.nvidia.com/cuda/cuda-driver-api/index.html"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "My system is as follows: System Environment: Windows 7 Professional anaconda 3 python 3.5.4 GPU: Quadr K2200 driver: 353.90 CUDA toolkit: 7.5 Visual studio: Visual studio community 2013 (Japanese version) pycuda binary file that I used for installation: pycuda-2016.1.2+cuda7518-cp35-cp35m-win_amd64.whl (downloaded from http://www.lfd.uci.edu/~gohlke/pythonlibs/#pycuda) Added PATH variables as instructed from here: https://github.com/drasmuss/hessianfree/wiki/PyCUDA-installation-on-Windows Additionally modified the nvcc.profile as shown here https://stackoverflow.com/a/19039177/7428707 Then tried to run the hello_gpu example code from here: https://documen.tician.de/pycuda/ Got the following error: The stderr message's formatting is not right. So I am unable to find out what the actual error is. I know there are a few related questions on here about this compilation error ( none of them have been answered, is there a solution at all ?) but the formatting problem in stderr has not been reported. So my question is : How do I see the stderr content ? In general, is there a solution for this compilation error ?",
        "answers": [
            [
                "I wasnt able to spend time on getting the stderr message to display. However I was able to solve the Compilation error message. I had to reconfigure the system a little. Anaconda: 5.0.1 with python 3.6.3 Cuda Toolkit: CUDA 8.0 Driver:376.51 for Quadro K2200 Visual Studio: Visual Studio community 2015 (custom installation: Visual C++ and Windows 10 SDK packages alone) Added the C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin in Path environment variable. Then created a new environment variable Include as follows: setx /M INCLUDE C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.10240.0\\ucrt Then in Anaconda command prompt, I changed the encoding to utf-8 ( default was Shift-JIS since my OS was japanese) chcp 65001 Then installed pycuda using pip pip install pycuda Installation was succesful and I was able to run the hello_gpu.py test code. However the nvcc compiler showed dozens of C4819 warnings (decoding warning because shift-JIS and utf-8 confusion) So I disabled the C4819 warning using the nvcc compiler command nvcc -Xcompiler \"/wd 4819\" references: pycuda installation guide.Clear pip installation UnicodeDecodeerror.Hiding C4819 warning All the references are in Japanese but I am giving them here in the hope they might help someone in similar situation as me."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to getting started with PyCUDA and parallel computing. I'm working on a 2012 iMac (High Sierra 10.13 NVIDIA GeForce GTX). This is a part of a simple tutorial I was trying: import pycuda import pycuda.driver as drv drv.init() print(\"%d device(s) found.\" % drv.Device.count()) But I get this error without any other detail: Traceback (most recent call last): File \"&lt;ipython-input-8-87f980c8ed7f&gt;\", line 4, in &lt;module&gt; drv.init() LogicError Also, the CUDA panel in preferences shows the annoying \"update required\" message even if both CUDA and GPU drivers are up to date...Don't know if the two problems are connected.",
        "answers": [],
        "votes": []
    },
    {
        "question": "the problem is not to use CUDA in processes, but that if the parent already has an initialized CUDA, then in processes can not initialize it. If you just make the initialization in the parent and throw it into the processes, but it does not work. Here is an example just in the processes: from pyfft.cuda import Plan import numpy import pycuda.driver as cuda from pycuda.tools import make_default_context import pycuda.gpuarray as gpuarray from multiprocessing import Process def do_this_fft(data): cuda.init() context = make_default_context() stream = cuda.Stream() plan = Plan((16, 16), stream=stream) gpu_data = gpuarray.to_gpu(data) plan.execute(gpu_data) result = gpu_data.get() del gpu_data print(result) context.pop() del plan,stream,context data = numpy.ones((16, 16), dtype=numpy.complex64) process_list=[Process(target=do_this_fft,args=(data,)).start() for i in range(2)] But if we write code like this, then nothing will work anymore from pyfft.cuda import Plan import numpy import pycuda.driver as cuda from pycuda.tools import make_default_context import pycuda.gpuarray as gpuarray from multiprocessing import Process def start_cuda_and_fft(data): cuda.init() context = make_default_context() stream = cuda.Stream() plan = Plan((16,16), stream=stream) context.pop() del plan,stream,context process_list=[Process(target=do_this_fft,args=(data,plan)).start() for i in range(2)] def do_this_fft(data): cuda.init() #&lt;---------------Error ''' File \"test.py\", line 35, in do_this_fft cuda.init() LogicError: cuInit failed: initialization error ''' context = make_default_context() stream = cuda.Stream() plan = Plan((16, 16), stream=stream) gpu_data = gpuarray.to_gpu(data) plan.execute(gpu_data) result = gpu_data.get() context.pop() del plan,stream,context print(result) data = numpy.ones((16, 16), dtype=numpy.complex64) process_list=[Process(target=do_this_fft,args=(data,)).start() for i in range(2)] The error is the following: File \"test.py\", line 35, in do_this_fft cuda.init() LogicError: cuInit failed: initialization error I have already tried to do the context, the plan and push into the process, all without consequences.",
        "answers": [
            [
                "We can't initialize CUDA before fork() so we need to spawn a new process"
            ],
            [
                "I had a similar issue, and solve it by adding a line of code on the main process, before start the subprocesses: multiprocessing.set_start_method('spawn') Source: [https://stackoverflow.com/a/55812288/8664574]"
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I'm trying to build an equality checker for two arrays that can I can run on my GPU using PyCUDA. Following the example given on the PyCUDA GPU Arrays documentation page, I attempted to write my own implementation. But whilst the below code works as expected for arithmetic, e.g. \"z[i] = x[i] + y[i]\", it returns erroneous output for the equality checker operand \"z[i] = x[i] == y[i]\". import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import pycuda.autoinit import numpy as np from pycuda.elementwise import ElementwiseKernel matrix_size = (5,) a = np.random.randint(2, size=matrix_size) b = np.random.randint(2, size=matrix_size) print a print b a_gpu = gpuarray.to_gpu(a) b_gpu = gpuarray.to_gpu(b) eq_checker = ElementwiseKernel( \"int *x, int *y, int *z\", \"z[i] = x[i] == y[i]\", \"equality_checker\") c_gpu = gpuarray.empty_like(a_gpu) eq_checker(a_gpu, b_gpu, c_gpu) print c_gpu Which prints out something like: [0 1 0 0 0] [0 1 1 1 0] [4294967297 4294967297 0 1 1] Does anyone understand why this error is occurring, or at least have an alternative PyCUDA method to achieve the desired function?",
        "answers": [
            [
                "Solved! The problem was that numpy automatically returns 64-bit integers, whereas PyCUDA only standardly accepts 32-bit integers. This is therefore fixed by specifying the type of ints numpy generates, such as: a = np.random.randint(2, size=matrix_size, dtype=np.int32) b = np.random.randint(2, size=matrix_size, dtype=np.int32) after which it works as expected."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.random.randn(4,4) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") I just installed CUDA 9.0 and pycuda, and I am following the tutorial to run the first cuda program. But it always turns out error: CompileError: nvcc compilation of c:\\users\\rl74173\\appdata\\local\\temp\\tmp6nww2c\\kernel.cu failed I did some research and find some answers to this before. So I add this before running: import os os.system(\"vcvarsamd64.bat\") But it is still error. I also see someone figure it out by adding line below to nvcc.profile COMPILER-BINDIR = C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin\\amd64 I installed visual studio community 2017,so in my case, I tried COMPILER-BINDIR = C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\atlmfc\\lib\\amd64 But it doesn't help.",
        "answers": [
            [
                "OK, so I fixed it for me. The problem is that running vcvars64.bat sets the path environment in a subshell... and then closes it, so the set path disappears again. What you want, is to change the path yourself: add the path to the cl.exe compiler file. For that, I referenced this post. In my case, I had to add this at the start to my .py file: import os if (os.system(\"cl.exe\")): os.environ['PATH'] += ';'+r\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64\" if (os.system(\"cl.exe\")): raise RuntimeError(\"cl.exe still not found, path probably incorrect\") I hope this works for you. Edit: you need to run a MSVS version compatible with CUDA. I.e. CUDA v9.0 doesn't support MSVS2017 and CUDA v9.1 only supports version 15.4, not later versions. Try if it works by running nvcc.exe from the Native Tools Command Prompt for Visual Studio."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Perhaps I'm missing something. I'm using pycuda and I have the following matrix m_gpu: &gt;&gt;&gt; m_gpu array([[ 0., 2., 4., 1., 3.], [ 2., 0., 5., 2., 1.], [ 4., 5., 0., 3., 2.], [ 1., 2., 3., 0., 4.], [ 3., 1., 2., 4., 0.]], dtype=float32) I print out the first column and second row for a sanity check: &gt;&gt;&gt; m_gpu[:,1] array([ 2., 0., 5., 2., 1.], dtype=float32) &gt;&gt;&gt; m_gpu[2,:] array([ 4., 5., 0., 3., 2.], dtype=float32) Then I do an element-wise comparison to return an array of max values, which is wrong as the 3rd element should be a '5' but it is a '1': &gt;&gt;&gt; gpuarray.maximum(m_gpu[:,1], m_gpu[2,:]) array([ 4., 5., 1., 3., 2.], dtype=float32) Could be a bug in gpuarray.maximum? Seems unlikely. I do the same operation using regular numpy and it returns the correct values.. Using pycuda-2017.1.1+cuda8061-cp27 on windows. Help? Thanks.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I am using pyCUDA for CUDA programming. I need to use random number inside kernel function. CURAND library doesn't work inside it (pyCUDA). Since, there is lot of work to be done in GPU, generating random number inside CPU and then transferring them to GPU won't work, rather dissolve the motive of using GPU. Supplementary Questions: Is there a way to allocate memory on GPU using 1 block and 1 thread. I am using more than one kernel. Do I need to use multiple SourceModule blocks?",
        "answers": [
            [
                "Despite what you assert in your question, PyCUDA has pretty comprehensive support for CUrand. The GPUArray module has a direct interface to fill device memory using the host side API (noting that the random generators run on the GPU in this case). It is also perfectly possible to use the device side API from CUrand in PyCUDA kernel code. In this use case the trickiest part is allocating memory for the thread generator states. There are three choices -- statically in code, dynamically using host memory side allocation, and dynamically using device side memory allocation. The following (very lightly tested) example illustrates the latter, seeing as you asked about it in your question: import numpy as np import pycuda.autoinit from pycuda.compiler import SourceModule from pycuda import gpuarray code = \"\"\" #include &lt;curand_kernel.h&gt; const int nstates = %(NGENERATORS)s; __device__ curandState_t* states[nstates]; __global__ void initkernel(int seed) { int tidx = threadIdx.x + blockIdx.x * blockDim.x; if (tidx &lt; nstates) { curandState_t* s = new curandState_t; if (s != 0) { curand_init(seed, tidx, 0, s); } states[tidx] = s; } } __global__ void randfillkernel(float *values, int N) { int tidx = threadIdx.x + blockIdx.x * blockDim.x; if (tidx &lt; nstates) { curandState_t s = *states[tidx]; for(int i=tidx; i &lt; N; i += blockDim.x * gridDim.x) { values[i] = curand_uniform(&amp;s); } *states[tidx] = s; } } \"\"\" N = 1024 mod = SourceModule(code % { \"NGENERATORS\" : N }, no_extern_c=True, arch=\"sm_52\") init_func = mod.get_function(\"_Z10initkerneli\") fill_func = mod.get_function(\"_Z14randfillkernelPfi\") seed = np.int32(123456789) nvalues = 10 * N init_func(seed, block=(N,1,1), grid=(1,1,1)) gdata = gpuarray.zeros(nvalues, dtype=np.float32) fill_func(gdata, np.int32(nvalues), block=(N,1,1), grid=(1,1,1)) Here there is an initialization kernel which needs to be run once to allocate memory for the generator states and initialize them with the seed, and then a kernel which uses those states. You will need to be mindful of malloc heap size limits if you want to run a lot of threads, but those can be manipulated via the PyCUDA driver API interface."
            ],
            [
                "There is one problem I have with the accepted answer. We have a name mangling there which is sort of nasty (these _Z10initkerneli and _Z14randfillkernelPfi). To avoid that we can wrap the code in the extern \"C\" {...} clause manually. code = \"\"\" #include &lt;curand_kernel.h&gt; const int nstates = %(NGENERATORS)s; __device__ curandState_t* states[nstates]; extern \"C\" { __global__ void initkernel(int seed) { .... } __global__ void randfillkernel(float *values, int N) { .... } } \"\"\" Then the code is still compiled with no_extern_c=True: mod = SourceModule(code % { \"NGENERATORS\" : N }, no_extern_c=True) and this should work with init_func = mod.get_function(\"initkernel\") fill_func = mod.get_function(\"randfillkernel\") Hope that helps."
            ]
        ],
        "votes": [
            6.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am benchmarking GPU matrix multiplication using PyCUDA, CUDAMat, and Numba and ran into some behavior I can't find a way to explain. I calculate the time it takes for 3 different steps independently - sending the 2 matrices to device memory, calculating the dot product, and copying the results back to host memory. The benchmarking for the dot product step is done in a loop since my applications will be doing many multiplications before sending the result back. As I increase the number of loops, the dot product time increases linearly just as expected. But the part I can't understand is that the time it takes to send the final result back to host memory also increases linearly with the loop count, even though it is only copying one matrix back to host memory. The size of the result is constant no matter how many matrix multiplication loops you do, so this makes no sense. It behaves as if returning the final result requires returning all the intermediate results at each step in the loop. Some interesting things to note are that the increase in time it takes has a peak. As I go above ~1000 dot products in a loop the time it takes to copy the final result back reaches a peak. Another thing is if inside the dot product loop I reinitialize the matrix that holds the result this behavior stops and the copy back time is the same no matter how many multiplies are done. For example - for i in range(1000): gc = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32) matrixmul(ga, gb, gc, grid=(MATRIX_SIZE // TILE_SIZE, MATRIX_SIZE // TILE_SIZE), block=(TILE_SIZE, TILE_SIZE, 1)) result = gc.get() The final thing to note is that this happens for both PyCUDA and Numba, but does not happen with CUDAMat. I can do a million multiplies and retrieving the final result will still take the same amount of time. CUDAMat has a built in matrix multiplication which could be why, but for PyCUDA and Numba I am using the matrix multiplication code provided in their own documentation. Here is my code for PyCUDA from __future__ import division import numpy as np from pycuda import driver, compiler, gpuarray, tools import time import pycuda.autoinit kernel_code_template = \"\"\" __global__ void MatrixMulKernel(float *A, float *B, float *C) { const int wA = %(MATRIX_SIZE)s; const int wB = %(MATRIX_SIZE)s; // Block index const int bx = blockIdx.x; const int by = blockIdx.y; // Thread index const int tx = threadIdx.x; const int ty = threadIdx.y; // Index of the first sub-matrix of A processed by the block const int aBegin = wA * %(BLOCK_SIZE)s * by; // Index of the last sub-matrix of A processed by the block const int aEnd = aBegin + wA - 1; // Step size used to iterate through the sub-matrices of A const int aStep = %(BLOCK_SIZE)s; // Index of the first sub-matrix of B processed by the block const int bBegin = %(BLOCK_SIZE)s * bx; // Step size used to iterate through the sub-matrices of B const int bStep = %(BLOCK_SIZE)s * wB; // The element of the block sub-matrix that is computed // by the thread float Csub = 0; // Loop over all the sub-matrices of A and B required to // compute the block sub-matrix for (int a = aBegin, b = bBegin; a &lt;= aEnd; a += aStep, b += bStep) { // Shared memory for the sub-matrix of A __shared__ float As[%(BLOCK_SIZE)s][%(BLOCK_SIZE)s]; // Shared memory for the sub-matrix of B __shared__ float Bs[%(BLOCK_SIZE)s][%(BLOCK_SIZE)s]; // Load the matrices from global memory to shared memory // each thread loads one element of each matrix As[ty][tx] = A[a + wA * ty + tx]; Bs[ty][tx] = B[b + wB * ty + tx]; // Synchronize to make sure the matrices are loaded __syncthreads(); // Multiply the two matrices together; // each thread computes one element // of the block sub-matrix for (int k = 0; k &lt; %(BLOCK_SIZE)s; ++k) Csub += As[ty][k] * Bs[k][tx]; // Synchronize to make sure that the preceding // computation is done before loading two new // sub-matrices of A and B in the next iteration __syncthreads(); } // Write the block sub-matrix to global memory; // each thread writes one element const int c = wB * %(BLOCK_SIZE)s * by + %(BLOCK_SIZE)s * bx; C[c + wB * ty + tx] = Csub; } \"\"\" MATRIX_SIZE = 512 TILE_SIZE = 8 BLOCK_SIZE = TILE_SIZE np.random.seed(100) a_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32) b_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32) kernel_code = kernel_code_template % { 'MATRIX_SIZE': MATRIX_SIZE, 'BLOCK_SIZE': BLOCK_SIZE, } mod = compiler.SourceModule(kernel_code) matrixmul = mod.get_function(\"MatrixMulKernel\") #copy to device memory total = time.clock() ga = gpuarray.to_gpu(a_cpu) gb = gpuarray.to_gpu(b_cpu) gc = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32) copy_to = time.clock() - total #matrix multiplication mult = time.clock() for i in range(1000): matrixmul(ga, gb, gc, grid=(MATRIX_SIZE // TILE_SIZE, MATRIX_SIZE // TILE_SIZE), block=(TILE_SIZE, TILE_SIZE, 1)) mult = time.clock() - mult #copy result back to host memory copy_from = time.clock() res = gc.get() copy_from = time.clock() - copy_from total = time.clock() - total #print out times for all 3 steps and the total time taken print(copy_to) print(mult) print(copy_from) print(total)",
        "answers": [
            [
                "GPU kernel launches are asynchronous. This means that the measurement you think you are capturing around the for-loop (the time it takes to do the multiplication) is not really that. It is just the time it takes to issue the kernel launches into a queue. The actual kernel execution time is getting \"absorbed\" into your final measurement of device-&gt;host copy time (because the D-&gt;H copy forces all kernels to complete before it will begin, and it blocks the CPU thread). Regarding the \"peak\" behavior, when you launch enough kernels into the queue, eventually it stops becoming asynchronous and begins to block the CPU thread, so your \"execution time\" measurement starts rising. This explains the varying peak behavior. To \"fix\" this, if you insert a pycuda driver.Context.synchronize() immediately after your for-loop, and before this line: mult = time.clock() - mult you will see your execution time increase as you increase the for loop, and your D-&gt;H copy time will remain constant."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to install pycuda to do some image processing in python. I followed the following link to install it : https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_PyCUDA_On_Anaconda_For_Windows?lang=en I feel I have installed everything right but when I run the sample code using pycharm: import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1)) print (dest-a*b) I get the error : ModuleNotFoundError: No module named 'pycuda.autoinit'; 'pycuda' is not a package When I run in the CMD, I get File \"C:\\Users\\Nitharshini\\Miniconda3\\lib\\site-packages\\pycuda\\au\u200c\u200btoinit.py\", line 2, in &lt;module&gt; import pycuda.driver as cuda File \"C:\\Users\\Nitharshini\\Miniconda3\\lib\\site-packages\\pycuda\\dr\u200c\u200biver.py\", line 5, in &lt;module&gt; from pycuda._driver import * # noqa ImportError: DLL load failed: The specified module could not be found. Any idea why this is happening?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Recently, I use pip to install the pyCUDA for my python3.4.3. But I found when I test the sample code\uff08https://documen.tician.de/pycuda/tutorial.html#getting-started\uff09, it can't print the result without any error message,the program can end. I can't understand what's wrong with this code or my python,thank you all for answer.This is my code: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy import random a =[random.randint(0,20) for i in range(20)] a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") func = mod.get_function(\"doublify\") func(a_gpu, block=(4,4,1)) a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print(a_doubled) print(a)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I want to parallelize a Python loop on GPU, but I don't want to use pyCUDA, because I need to do lots of thing myself. I am looking for something like OpenACC as in C++ for Python to implement the simple parallelization, but it seems no such thing. So I am thinking just using OpenACC in C++ and then system call a Python script, as in the code below. Will this work? Or is there any simple alternative without using pyCUDA? void foo(float*parameters){ %%system call python function with parameters as input } #pragma acc parallel loop for ( int i=0; i&lt;n; ++i) { foo(parameters[i]); //call on the device }",
        "answers": [
            [
                "No, this wont work. You can't execute a host system call from the device. For OpenACC device code, you can only call routines having the OpenACC \"routine\" directive, or a CUDA \"device\" routine."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to run multiple threads on GPUs using the Pycuda example MultipleThreads. When I run my python file, I get the following error message: (/root/anaconda3/) root@109c7b117fd7:~/pycuda# python multiplethreads.py Exception in thread Thread-5: Traceback (most recent call last): File \"/root/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner self.run() File \"multiplethreads.py\", line 22, in run test_kernel(self.array_gpu) File \"multiplethreads.py\", line 36, in test_kernel \"\"\") TypeError: 'module' object is not callable ------------------------------------------------------------------- PyCUDA ERROR: The context stack was not empty upon module cleanup. ------------------------------------------------------------------- A context was still active when the context stack was being cleaned up. At this point in our execution, CUDA may already have been deinitialized, so there is no way we can finish cleanly. The program will be aborted now. Use Context.pop() to avoid this problem. ------------------------------------------------------------------- Exception in thread Thread-6: Traceback (most recent call last): File \"/root/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner self.run() File \"multiplethreads.py\", line 22, in run test_kernel(self.array_gpu) File \"multiplethreads.py\", line 36, in test_kernel \"\"\") TypeError: 'module' object is not callable Aborted I have tried to change the way I import threading from import threading to from threading import Thread, but the error still persists. Could anyone be able to see what is the problem that I'm encountering?",
        "answers": [
            [
                "Problem has been solved. Just a minor error in the import command. Instead of: from pycuda.compiler import SourceModule I mistakenly wrote: import pycuda.compiler as SourceModule It was a bad mistake."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm new to CUDA and am trying to figure out whether PyCUDA (free) or NumbaPro CUDA Python (not free) would be better for me (assuming the library cost is not an issue). Both seem to require that you use their respective Python dialects. But, it seems that PyCUDA requires you to write a kernel function in C code, which would be more cumbersome than using NumbaPro, which seems to do all the hard work for you. Is this indeed the case? Would there be notable performance differences?",
        "answers": [
            [
                "let's talk about each one of these libraries: PyCUDA: PyCUDA is a Python programming environment for CUDA it give you access to Nvidia's CUDA parallel computation API from Python. PyCUDA is written in C++(the base layer) and Python,the C++ code will be executed on the NVIDIA chip, and Python code to compile, execute, and get the results of the C++ code and Automatically manages resources which make it one of powerful library CUDA. PyCUDA is slightly different from to PyOpenCl can be used to run code on a variety of platforms, including Intel, AMD, NVIDIA, and ATI chips. unlike PyCUDA which can be run on NVIDIA chips only: Python + CUDA = PyCUDA Python + OpenCL = PyOpenCL NUMBA/NumbaPro: NUMBA : NumbaPro or recently Numba (NumbaPro has been deprecated, and its code generation features have been moved into open-source Numba.) is an Open Source NumPy-aware optimizing compiler for Python sponsored by Anaconda, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code. Numba supports compilation of Python to run on either CPU or GPU hardware and it's fundamentally written in Python. it's easy to install and implement. As @Wang has mentioned, Pycuda is faster than Numba."
            ],
            [
                "numbapro now is numba on BSD license which is free also. according to this report pyOpenCL and pyCUDA is 5 times faster than numba"
            ]
        ],
        "votes": [
            14.0000001,
            14.0000001
        ]
    },
    {
        "question": "I'm trying to use some LAPACKE functions inside a CUDA kernel to solve small systems of linear equations. I have a main source file that contains the kernel function I want to call. Inside that kernel function I want to call the LAPACKE function LAPACKE_dgesv(), which is defined in a different source file. In my main source file I have included the header file lapacke.h which contains the declaration for LAPACKE_dgesv(). In addition I have edited lapacke.h to prepend __device__ to the function declaration of LAPACKE_dgesv(). I added the directory of the source file that contains the definition of LAPACKE_dgesv() to the include_dirs argument of the SourceModule call in my Python code. However when I run the code I get this error: ptxas fatal : Unresolved extern function 'LAPACKE_dgesv' My guess is that the source file containing the definition of LAPACKE_dgesv() is not being compiled. Is there a way to get PyCuda to compile multiple source files that contain device code? It seems that there would need to be a way for PyCuda to run the CUDA compiler with the --relocatable-device-code=true flag.",
        "answers": [
            [
                "No, you can't do this with SourceModule. There is an experimental DynamicSourceModule which has been added to the Master branch very recently and which probably can do what you want, although it isn't well documented and I have never used it. Otherwise, you can always statically compile and device link the code to a cubin file yourself outside of PyCUDA and just load the resulting device code via the standard APIs."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "CUDA has some built-in math functions, such as norm(). I want to create my own version of the norm() function, and use my version throughout the code. However when I define my own norm() function like so: __device__ float norm(float a, float b) { return sqrt(a*a+b*b); } I get the following compilation error: kernel.cu(9): error: more than one instance of overloaded function \"norm\" has \"C\" linkage Is there a way I can overload the norm() function, or do I have to just give my own function a unique name? I'm using PyCuda to compile my CUDA code.",
        "answers": [
            [
                "The problem here is the use of C linkage in your code. You may or may not be explicitly specifying extern \"C\"anywhere. Irrespective of whether you are, if you are using the PyCUDA SourceModule facility to compile your code, it is (un)helpfully, automagically bracketing the code you submit with extern \"C\". If you look at the documentation for SourceModule, you will see the option no_extern_c. Set that to True and this problem will go away. But note that everything you compile will now be compiled with C++ linkage and symbol mangling. You will have to adapt your Python code accordingly (see here for some of the gory details). And after that, read the other answer, which contains some very sage advice about the perils of overloading standard libraries and a best practice alternative."
            ],
            [
                "I'll make two suggestions in addition to @talonmies' answer - in case you do manage to get overloading working: General non-CUDA-specific advice: Avoid overloading the builtins / API functions of a library, unless that is absolutely necessary (which it isn't in your case). Reasons for this: Likely to confuse other readers of your code Mixing up \"wrapper\" code with builtins - it's not a \"clean\" way to code. If the builtins change, your code using the builtins+overloads is likely to also have to change, sometimes in ways you didn't expect. In your case, I would seriously consider having some namespace with your utility functions, e.g. namespace math { template &lt;typename T&gt; __device__ T norm(T a, T b) { return math::sqrt&lt;T&gt;(a*a+b*b); } } (of course you would need a math::sqrt template, which would abstract from the single-precision sqrtf(), double-precision sqrt() etc.)"
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001
        ]
    },
    {
        "question": "Is it possible to call __host__ functions in pyCUDA like you can __global__ functions? I noticed in the documentation that pycuda.driver.Function creates a handle to a __global__ function. __device__ functions can be called from a __global__ function, but __host__ code cannot. I'm aware that using a __host__ function pretty much defeats the purpose of pyCUDA, but there are some already made functions that I'd like to import and call as a proof of concept. As a note, whenever I try to import the __host__ function, I get: pycuda._driver.LogicError: cuModuleGetFunction failed: named symbol not found",
        "answers": [
            [
                "No it is not possible. This isn't a limitation of PyCUDA, per se, but of CUDA itself. The __host__ decorator just decays away to plain host code, and the CUDA APIs don't and cannot handle them in the same way that device code can be handled (note the the APIs also don't handle __device__ either, which is the true equivalent of __host__). If you want to call/use __host__ functions from Python, you will need to use one of the standard C++/Python interoperability mechanisms, like ctypes or SWIG or boost python, etc. EDIT: Since this answer was written five years ago, CUDA has added the ability to run host functions in CUDA streams via cuLaunchHostFunc (driver API) or cudaLaunchHostFunc. Unfortunately, at the time of this edit (June 2022), PyCUDA doesn't expose this functionality, so it still isn't possible in PyCUDA and the core message of the original answer is unchanged."
            ],
            [
                "Below, I'm providing a sample code to call CUDA APIs in pyCUDA. The code generates uniformly distributed random numbers and may serve as a reference to include already made functions (as the poster says and like CUDA APIs) in a pyCUDA code. import numpy as np import ctypes import pycuda.driver as drv import pycuda.gpuarray as gpuarray import pycuda.autoinit curand = CDLL(\"/usr/local/cuda/lib64/libcurand.so\") # --- Number of elements to generate N = 10 # --- cuRAND enums CURAND_RNG_PSEUDO_DEFAULT = 100 # --- Query the cuRAND version i = c_ulonglong() curand.curandGetVersion(byref(i)) print(\"curand version: \", i.value) # --- Allocate space for generation d_x = gpuarray.empty(N, dtype = np.float32) # --- Create random number generator gen = c_ulonglong() curand.curandCreateGenerator(byref(gen), CURAND_RNG_PSEUDO_DEFAULT) # --- Generate random numbers curand.curandGenerateUniform(gen, ctypes.cast(d_x.ptr, POINTER(c_float)), N) print(d_x)"
            ]
        ],
        "votes": [
            2.0000001,
            -0.9999999
        ]
    },
    {
        "question": "I've just started looking into Cuda and especially PyCuda. I'm currently using Anaconda on Windows 7. I have installed Pycuda using the Anaconda Prompt and tried the following code, which I copied directly from the PyCuda documentation web page. However I've got an CompileError. Does anyone have any suggestion? import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them(drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1),grid(1,1)) print(dest-a*b) Traceback (most recent call last): File \"&lt;ipython-input-2-06c8e60d26ae&gt;\", line 12, in &lt;module&gt; \"\"\") File \"C:\\Users\\Moritz\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 291, in __init__ arch, code, cache_dir, include_dirs) File \"C:\\Users\\Moritz\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 255, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"C:\\Users\\Moritz\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 137, in compile_plain stderr=stderr.decode(\"utf-8\", \"replace\")) CompileError: nvcc compilation of C:\\Users\\Moritz\\AppData\\Loca \\Temp\\tmpst8z9hvc\\kernel.cu failed",
        "answers": [
            [
                "Ok, solved the Problem. I've only had \"Microsoft Visual Studio Express\" installed, which, as it seems, does not support compiling 64bit applications. However, I've been running a 64bit Version of Anaconda on my PC. Installing Anaconda 32bit instead fixed the issue."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm using a 1kB .png file (i.e a 2048x2048 numpy array) to test my PyCuda program and it shows the program took about 1.57s to allocate and copy the data to the device. Is that normal to take such a long time? I wonder whether PyCuda and Cuda C have performance difference in the allocating and memcpy procedure? (As the kernel is still written in C when using PyCuda, the kernel executing time is about 0.17s so I feel it took too long to do the preparation.) Code to allocate and memcpy img_gpu = cuda.mem_alloc(img.nbytes) cuda.memcpy_htod(img_gpu, img) result_gpu = cuda.mem_alloc(result.nbytes) cuda.memcpy_htod(result_gpu, result) disX = np.array(disX).astype(np.int32) disY = np.array(disY).astype(np.int32) disX_gpu = cuda.mem_alloc(disX.nbytes) cuda.memcpy_htod(disX_gpu, disX) disY_gpu = cuda.mem_alloc(disY.nbytes) cuda.memcpy_htod(disY_gpu, disY)",
        "answers": [
            [
                "No, there is neglible difference in performance between PyCUDA (in the way you are using it) and \"native\" CUDA. The PyCUDA driver module is a very thin wrapper around the CUDA driver API, and you should expect them to perform the same. Context establishment and memory allocation are expensive operations, particularly on the Windows WDDM platform, and it would appear that all you are measuring is standard latency and setup overhead."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a pycuda program here that reads in an image from the command line and saves a version back with the colors inverted: import pycuda.autoinit import pycuda.driver as device from pycuda.compiler import SourceModule as cpp import numpy as np import sys import cv2 modify_image = cpp(\"\"\" __global__ void modify_image(int pixelcount, unsigned char* inputimage, unsigned char* outputimage) { int id = threadIdx.x + blockIdx.x * blockDim.x; if (id &gt;= pixelcount) return; outputimage[id] = 255 - inputimage[id]; } \"\"\").get_function(\"modify_image\") print(\"Loading image\") image = cv2.imread(sys.argv[1], cv2.IMREAD_UNCHANGED).astype(np.uint8) print(\"Processing image\") pixels = image.shape[0] * image.shape[1] newchannels = [] for channel in cv2.split(image): output = np.zeros_like(channel) modify_image( device.In(np.int32(pixels)), device.In(channel), device.Out(output), block=(1024,1,1), grid=(pixels // 1024 + 1, 1)) newchannels.append(output) finalimage = cv2.merge(newchannels) print(\"Saving image\") cv2.imwrite(\"processed.png\", finalimage) print(\"Done\") It works perfectly fine, even on larger images. However, in trying to expand the functionality of the program, I came across a really strange issue wherein adding a second variable argument to the kernel causes the program to completely fail, simply saving a completely black image. The following code does not work; import pycuda.autoinit import pycuda.driver as device from pycuda.compiler import SourceModule as cpp import numpy as np import sys import cv2 modify_image = cpp(\"\"\" __global__ void modify_image(int pixelcount, int width, unsigned char* inputimage, unsigned char* outputimage) { int id = threadIdx.x + blockIdx.x * blockDim.x; if (id &gt;= pixelcount) return; outputimage[id] = 255 - inputimage[id]; } \"\"\").get_function(\"modify_image\") print(\"Loading image\") image = cv2.imread(sys.argv[1], cv2.IMREAD_UNCHANGED).astype(np.uint8) print(\"Processing image\") pixels = image.shape[0] * image.shape[1] newchannels = [] for channel in cv2.split(image): output = np.zeros_like(channel) modify_image( device.In(np.int32(pixels)), device.In(np.int32(image.shape[0])), device.In(channel), device.Out(output), block=(1024,1,1), grid=(pixels // 1024 + 1, 1)) newchannels.append(output) finalimage = cv2.merge(newchannels) print(\"Saving image\") cv2.imwrite(\"processed.png\", finalimage) print(\"Done\") where the only difference is on two lines, the kernel header and it's call. The actual code of the kernel itself is unchanged, and yet this small addition completely breaks the program. Neither the compiler nor interpreter throw any errors. I have no idea how to begin to debug it, and am thoroughly confused.",
        "answers": [
            [
                "The device.In and relatives are designed for use with objects which support the Python buffer protocols (like numpy arrays). The source of your problem is using them to transfer non-buffer objects. Just pass your scalars with the correct numpy dtype directly to your kernel call. Don't use device.In. The fact this worked in the original case was a complete accident"
            ],
            [
                "Okay, so by changing the variable arguments to pointers in the kernel it fixed the code, i'm not sure how or why. Here is the modified version of the kernel; __global__ void modify_image(int* pixelcount, int* width, unsigned char* inputimage, unsigned char* outputimage) { int id = threadIdx.x + blockIdx.x * blockDim.x; if (id &gt;= *pixelcount) return; outputimage[id] = 255 - inputimage[id]; } The remainder of the code is unchanged. If anybody wants to explain why this is a successful fix, I would greatly appreciate it."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "I am trying to iterate through a 2D array in PyCUDA but I end up with repeated array values. I initially throw a small random integer array and that works as expected but when I throw an image at it, I see the same values over and over again. Here is my code img = np.random.randint(20, size = (4,5)) print \"Input array\" print img img_size=img.shape print img_size #nbtes determines the number of bytes for the numpy array a img_gpu = cuda.mem_alloc(img.nbytes) #Copies the memory from CPU to GPU cuda.memcpy_htod(img_gpu, img) mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void AHE(int *a, int row, int col) { int i = threadIdx.x+ blockIdx.x* blockDim.x; int j = threadIdx.y+ blockIdx.y* blockDim.y; if(i==0 &amp;&amp; j ==0) printf(\"Output array \"); if(i &lt;row &amp;&amp; j &lt; col) { printf(\" %d\",a[j + i*col]); } } \"\"\") col = np.int32(img.shape[-1]) row = np.int32(img.shape[0]) func = mod.get_function(\"AHE\") func(img_gpu, row, col, block=(32,32,1)) img_ahe = np.empty_like(img) cuda.memcpy_dtoh(img_ahe, img_gpu) Now when I replace the random integer array with an image converted to a numpy array I end up with this img = cv2.imread('Chest.jpg',0) img_size=img.shape print img_size #nbtes determines the number of bytes for the numpy array a img_gpu = cuda.mem_alloc(img.nbytes) #Copies the memory from CPU to GPU cuda.memcpy_htod(img_gpu, img) mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void AHE(int *a, int row, int col) { int i = threadIdx.x+ blockIdx.x* blockDim.x; int j = threadIdx.y+ blockIdx.y* blockDim.y; if(i==0 &amp;&amp; j ==0) printf(\"Output array \"); if(i &lt;row &amp;&amp; j &lt; col) { printf(\" %d\",a[j + i*col]); } } \"\"\") #Gives you the number of columns col = np.int32(img.shape[-1]) row = np.int32(img.shape[0]) func = mod.get_function(\"AHE\") func(img_gpu, row, col, block=(32,32,1)) img_ahe = np.empty_like(img) cuda.memcpy_dtoh(img_ahe, img_gpu)",
        "answers": [
            [
                "The problem here is that the image you are loading doesn't have pixel values stored as signed integers. This modification of your example works more as expected: import pycuda.driver as cuda from pycuda.compiler import SourceModule import numpy as np import cv2 import pycuda.autoinit img = cv2.imread('Chest.jpg',0) img_size=img.shape print img_size print img.dtype #nbtes determines the number of bytes for the numpy array a img_gpu = cuda.mem_alloc(img.nbytes) #Copies the memory from CPU to GPU cuda.memcpy_htod(img_gpu, img) mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void AHE(unsigned char *a, int row, int col) { int i = threadIdx.x+ blockIdx.x* blockDim.x; int j = threadIdx.y+ blockIdx.y* blockDim.y; if(i==0 &amp;&amp; j ==0) printf(\"Output array \"); if(i &lt;row &amp;&amp; j &lt; col) { int val = int(a[j + i*col]); printf(\" %d\", val); } } \"\"\") #Gives you the number of columns col = np.int32(img.shape[-1]) row = np.int32(img.shape[0]) func = mod.get_function(\"AHE\") func(img_gpu, row, col, block=(32,32,1)) img_ahe = np.empty_like(img) cuda.memcpy_dtoh(img_ahe, img_gpu) When run the code emits this: $ python image.py (681, 1024) uint8 Output array 244 244 244 244 244 244 244 244 244 244 244 244 244 244 244 244 244 244 245 245 245 246 246 246 246 246 246 246 246 246 246 246 244 244 244 244 244 244 244 244 245 245 245 245 245 245 245 245 244 244 245 245 245 246 246 246 [Output clipped for brevity] Note the dtype of the image - uint8. Your code is attempting to treat the stream of unsigned 8 bit values as integers. It should technically generate a runtime error on a full image because the kernel will read beyond the size of image as it reads 4 bytes per pixel instead of 1. However, you don't see this because you only run a single block, and your input image is presumably at least four times larger than the 32 x 32 size of the block you run. Incidentally, PyCUDA is extremely good at managing and enforcing type safety for CUDA calls, but your code neatly defeats every mechanism by which PyCUDA could detect a type mismatch in the kernel call. PyCUDA includes an excellent GPUarray class. You should familiarise yourself with it. If you had used a GPUarray instance here, you would have gotten type mismatch runtime errors which would have alerted you to the exact source of the problem the first time you tried to run it."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Let's say we allocated the same amount of memory on the device using two calls like so: pointerA = cuda_driver.mem_alloc(myArray.nbytes) pointerB = cuda_driver.mem_alloc(myArray.nbytes) Then we copy the same data onto the device for both DeviceAllocation objects. cuda_driver.memcpy_htod(pointerA, myArray) cuda_driver.memcpy_htod(pointerB, myArray) Is it a bad idea to then swap the two DeviceAllocation objects? For example they could be swapped like this: tempPointer = pointerA pointerA = pointerB pointerB = tempPointer",
        "answers": [
            [
                "No, it isn't a bad idea, quite the opposite. Pointer object exchange in PyCUDA is fully supported and is generally the right thing to do. In general, most device to device copies in CUDA code are unnecessary and could be replace by pointer exchange. About the only time that doesn't work is when the device to device transfer only copies part of the source memory to the destination memory (or vice versa)."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am trying to iterate through a 2D array in PyCUDA but I end up with repeated array values. I initially throw a small random integer array and that works as expected but when I throw an image at it, I see the same values over and over again. Here is my code img = np.random.randint(20, size = (4,5)) print \"Input array\" print img img_size=img.shape print img_size #nbtes determines the number of bytes for the numpy array a img_gpu = cuda.mem_alloc(img.nbytes) #Copies the memory from CPU to GPU cuda.memcpy_htod(img_gpu, img) mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void AHE(int *a, int row, int col) { int i = threadIdx.x+ blockIdx.x* blockDim.x; int j = threadIdx.y+ blockIdx.y* blockDim.y; if(i==0 &amp;&amp; j ==0) printf(\"Output array \"); if(i &lt;row &amp;&amp; j &lt; col) { printf(\" %d\",a[j + i*col]); } } \"\"\") col = np.int32(img.shape[-1]) row = np.int32(img.shape[0]) func = mod.get_function(\"AHE\") func(img_gpu, row, col, block=(32,32,1)) img_ahe = np.empty_like(img) cuda.memcpy_dtoh(img_ahe, img_gpu) Now when I replace the random integer array with an image converted to a numpy array I end up with this img = cv2.imread('Chest.jpg',0) img_size=img.shape print img_size #nbtes determines the number of bytes for the numpy array a img_gpu = cuda.mem_alloc(img.nbytes) #Copies the memory from CPU to GPU cuda.memcpy_htod(img_gpu, img) mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void AHE(int *a, int row, int col) { int i = threadIdx.x+ blockIdx.x* blockDim.x; int j = threadIdx.y+ blockIdx.y* blockDim.y; if(i==0 &amp;&amp; j ==0) printf(\"Output array \"); if(i &lt;row &amp;&amp; j &lt; col) { printf(\" %d\",a[j + i*col]); } } \"\"\") #Gives you the number of columns col = np.int32(img.shape[-1]) row = np.int32(img.shape[0]) func = mod.get_function(\"AHE\") func(img_gpu, row, col, block=(32,32,1)) img_ahe = np.empty_like(img) cuda.memcpy_dtoh(img_ahe, img_gpu)",
        "answers": [
            [
                "The problem here is that the image you are loading doesn't have pixel values stored as signed integers. This modification of your example works more as expected: import pycuda.driver as cuda from pycuda.compiler import SourceModule import numpy as np import cv2 import pycuda.autoinit img = cv2.imread('Chest.jpg',0) img_size=img.shape print img_size print img.dtype #nbtes determines the number of bytes for the numpy array a img_gpu = cuda.mem_alloc(img.nbytes) #Copies the memory from CPU to GPU cuda.memcpy_htod(img_gpu, img) mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void AHE(unsigned char *a, int row, int col) { int i = threadIdx.x+ blockIdx.x* blockDim.x; int j = threadIdx.y+ blockIdx.y* blockDim.y; if(i==0 &amp;&amp; j ==0) printf(\"Output array \"); if(i &lt;row &amp;&amp; j &lt; col) { int val = int(a[j + i*col]); printf(\" %d\", val); } } \"\"\") #Gives you the number of columns col = np.int32(img.shape[-1]) row = np.int32(img.shape[0]) func = mod.get_function(\"AHE\") func(img_gpu, row, col, block=(32,32,1)) img_ahe = np.empty_like(img) cuda.memcpy_dtoh(img_ahe, img_gpu) When run the code emits this: $ python image.py (681, 1024) uint8 Output array 244 244 244 244 244 244 244 244 244 244 244 244 244 244 244 244 244 244 245 245 245 246 246 246 246 246 246 246 246 246 246 246 244 244 244 244 244 244 244 244 245 245 245 245 245 245 245 245 244 244 245 245 245 246 246 246 [Output clipped for brevity] Note the dtype of the image - uint8. Your code is attempting to treat the stream of unsigned 8 bit values as integers. It should technically generate a runtime error on a full image because the kernel will read beyond the size of image as it reads 4 bytes per pixel instead of 1. However, you don't see this because you only run a single block, and your input image is presumably at least four times larger than the 32 x 32 size of the block you run. Incidentally, PyCUDA is extremely good at managing and enforcing type safety for CUDA calls, but your code neatly defeats every mechanism by which PyCUDA could detect a type mismatch in the kernel call. PyCUDA includes an excellent GPUarray class. You should familiarise yourself with it. If you had used a GPUarray instance here, you would have gotten type mismatch runtime errors which would have alerted you to the exact source of the problem the first time you tried to run it."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am new to PyCUDA and was going through some of the examples on the PyCUDA website. I am trying to figure out the logic behind certain lines of code and would really appreciate if someone explained the idea behind it. The below code snippet is from the PyCUDA website. Inside the function definition, I don't understand int idx = threadIdx.x + threadIdx.y*4; how the above line is being used to calculate the index of the array. Why are threadIdx.x and threadIdx.y added together and why is threadIdx.y multiplied by 4. For the function call to the GPU why is the block defined as 5,5,1. Since it is an array of 5x5 elements so in my understanding the block size should be 5,5 instead of 5,5,1. import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.random.randn(5,5) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) mod = SourceModule(\"\"\" __global__ void doubleMatrix(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") func = mod.get_function(\"doubleMatrix\") func(a_gpu, block=(5,5,1)) a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print (\"ORIGINAL MATRIX\") print a print (\"DOUBLED MATRIX AFTER PyCUDA EXECUTION\") print a_doubled",
        "answers": [
            [
                "The example you have posted seems to have come from (or been plagerised into) a book called \"Python Parallel Programming Cookbook\", which I hadn't heard of until five minutes ago. Honestly, if I were the authors of that book I would be ashamed to have included such a hacky, broken example. Here is a small modification to what you posted, and its output: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.random.randn(5,5) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) mod = SourceModule(\"\"\" __global__ void doubleMatrix(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2.f; } \"\"\") func = mod.get_function(\"doubleMatrix\") func(a_gpu, block=(5,5,1)) a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print a_doubled - 2.0*a [warning: Python 2 syntax] In [2]: %run matdouble.py [[ 0. 0. 0. 0. 0. ] [ 0. 0. 0. 0. 0. ] [ 0. 0. 0. 0. 0. ] [ 0. 0. 0. 0. 0. ] [ 0. -0.62060976 0.49836278 -1.60820103 1.71903515]] i.e. the code doesn't work as expected, and this is probably your source of confusion. The correct way to address a multidimensional array stored in linear memory (as numpy arrays are) is described in this very recent answer. Any sensible programmer would write the kernel in your example something like this: __global__ void doubleMatrix(float *a, int lda) { int idx = threadIdx.x + threadIdx.y * lda; a[idx] *= 2.f; } so that the leading dimension of the array is passed as an argument to the kernel (which in this case should be 5, not 4). Doing this gives the following: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.random.randn(5,5) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) mod = SourceModule(\"\"\" __global__ void doubleMatrix(float *a, int lda) { int idx = threadIdx.x + threadIdx.y * lda; a[idx] *= 2.f; } \"\"\") func = mod.get_function(\"doubleMatrix\") lda = numpy.int32(a.shape[-1]) func(a_gpu, lda, block=(5,5,1)) a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print a_doubled - 2.0*a which produces the expected result: In [3]: %run matdouble.py [[ 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0.]] For the function call to the GPU why is the block defined as 5,5,1. Since it is an array of 5x5 elements so in my understanding the block size should be 5,5 instead of 5,5,1. In CUDA, all blocks implictly have three dimensions. A block size of (5,5) is the same as a block size of (5,5,1). The last dimension can be ignored because it is one (i.e. all threads in the block have threadIdx.z = 1). What you shouldn't fall into the trap of doing is conflating the dimensions of the CUDA block or grid with the dimensions of the input array. Sometimes it is convenient to have them be the same, but equally it isn't necessary or even advisable to do so. A correctly written kernel in the style of BLAS for this example (assuming row major storage order) would probably look like this: __global__ void doubleMatrix(float *a, int m, int n, int lda) { int col = threadIdx.x + blockIdx.x * blockDim.x; int row = threadIdx.y + blockDim.y * blockDim.y; for(; row &lt; m; row += blockDim.y * gridDim.y) { for(; col &lt; n; col += blockDim.x * gridDim.x) { int idx = col + row * lda; a[idx] *= 2.f; } } } [Note: written in browser, not compiled or tested] Here any legal block and grid dimension will correctly process any sized input array total number of elements will fit into a signed 32 bit integer. If you run too many threads, some will do nothing. If you run too few threads, some will process multiple array elements. If you run a grid with the same dimensions as the input array, each thread will process exactly one input, as was the intention in the example you were studying. If you want to read about how to choose the most appropriate block and grid sizes, I suggest starting here."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have met an error when i was installing the pycuda,i tried many times but it didnt work,i already have compiled boost,installed the cuda8.0.here is the error traceback and siteconf.py. error trace back: running install running bdist_egg running egg_info writing requirements to pycuda.egg-info\\requires.txt writing pycuda.egg-info\\PKG-INFO writing top-level names to pycuda.egg-info\\top_level.txt writing dependency_links to pycuda.egg-info\\dependency_links.txt reading manifest file 'pycuda.egg-info\\SOURCES.txt' reading manifest template 'MANIFEST.in' warning: no files found matching 'doc\\source\\_static\\*.css' warning: no files found matching 'doc\\source\\_templates\\*.html' warning: no files found matching '*.cpp' under directory 'bpl-subset\\bpl_subset\\ boost' warning: no files found matching '*.html' under directory 'bpl-subset\\bpl_subset \\boost' warning: no files found matching '*.inl' under directory 'bpl-subset\\bpl_subset\\ boost' warning: no files found matching '*.txt' under directory 'bpl-subset\\bpl_subset\\ boost' warning: no files found matching '*.h' under directory 'bpl-subset\\bpl_subset\\li bs' warning: no files found matching '*.ipp' under directory 'bpl-subset\\bpl_subset\\ libs' warning: no files found matching '*.pl' under directory 'bpl-subset\\bpl_subset\\l ibs' writing manifest file 'pycuda.egg-info\\SOURCES.txt' installing library code to build\\bdist.win-amd64\\egg running install_lib running build_py running build_ext building '_driver' extension C:\\ProgramData\\Anaconda2\\MinGW\\bin\\gcc.exe -DMS_WIN64 -mdll -O -Wall -DBOOST_PYTHON_SOURCE=1 -DHAVE_CURAND=1 -DPYGPU_PACKAGE=pycuda -DBOOST_THREAD_DONT_USE_CHRONO=1 -DPYGPU_PYCUDA=1 -DBOOST_MULTI_INDEX_DISABLE_SERIALIZATION=1 -DBOOST_THREAD_BUILD_DLL=1 -Dboost=pycudaboost -DBOOST_ALL_NO_LIB=1 -Isrc/cpp -Ibpl-subset/bpl_subset \" -IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\include\" -IC:\\ProgramData\\Anaconda2\\lib\\site-packages\\numpy\\core\\include -IC:\\ProgramData\\Anaconda2\\include -IC:\\ProgramData\\Anaconda2\\PC -c src/cpp/cuda.cpp -o build\\temp.win-amd64-2.7\\Release\\src\\cpp\\cuda.o /EHsc gcc: error: /EHsc: No such file or directory error: command 'C:\\\\ProgramData\\\\Anaconda2\\\\MinGW\\\\bin\\\\gcc.exe' failed with exit status 1 siteconf.py: BOOST_INC_DIR = ['E:\\boost_1_63_0'] BOOST_LIB_DIR = ['E:\\\\boost_1_63_0\\\\stage\\\\lib'] BOOST_COMPILER = 'gcc43' USE_SHIPPED_BOOST = True BOOST_PYTHON_LIBNAME = ['libboost_python-vc100-mt-1_63'] BOOST_THREAD_LIBNAME = ['libboost_thread-vc100-mt-1_63'] CUDA_TRACE = False CUDA_ROOT = 'C:\\\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0' CUDA_ENABLE_GL = False CUDA_ENABLE_CURAND = True CUDADRV_LIB_DIR = ['${CUDA_ROOT}/lib', '${CUDA_ROOT}/lib/x64', '${CUDA_ROOT}/lib/stubs', '${CUDA_ROOT}/lib/x64/stubs'] CUDADRV_LIBNAME = ['cuda'] CUDART_LIB_DIR = ['${CUDA_ROOT}/lib', '${CUDA_ROOT}/lib/x64', '${CUDA_ROOT}/lib/stubs', '${CUDA_ROOT}/lib/x64/stubs'] CUDART_LIBNAME = ['cudart'] CURAND_LIB_DIR = ['${CUDA_ROOT}/lib', '${CUDA_ROOT}/lib/x64', '${CUDA_ROOT}/lib/stubs', '${CUDA_ROOT}/lib/x64/stubs'] CURAND_LIBNAME = ['curand'] CXXFLAGS = ['/EHsc'] LDFLAGS = ['/FORCE'] Could you offer me some help? thank you so much.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've just installed pyCuda, when i try to compile: import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") this is the result: Traceback (most recent call last): File \"&lt;stdin&gt;\", line 7, in &lt;module&gt; File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 265, in __init__ arch, code, cache_dir, include_dirs) File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 255, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\pycuda\\compiler.py\", line 137, in compile_plain stderr=stderr.decode(\"utf-8\", \"replace\")) pycuda.driver.CompileError: nvcc compilation of C:\\Users\\whyno\\AppData\\Local\\Temp\\tmpkv6oyxif\\kernel.cu failed [command: nvcc --cubin -arch sm_50 -m64 -Ic:\\program files\\anaconda3\\lib\\site-packages\\pycuda\\cuda kernel.cu] I've installed pyCuda using pip in an anaconda shell and i'm using microsoft visual studio 14.0. Follow these i've added ollowing line in nvcc.profile: COMPILER-BINDIR = C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64 but it returns always the same error. Thanks.",
        "answers": [
            [
                "Don't change nvcc.profile. You probably had the same issue I had. I edited compiler.py to output the stdout of the command call. I got \"nvcc fatal : Cannot find compiler 'cl.exe' in PATH\". So, if this is the same case for you, you need to add the path to cl.exe in your python file. In my case, I needed to add the following lines at the start of my code. import os if os.system(\"cl.exe\"): os.environ['PATH'] += ';'+r\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64\" if os.system(\"cl.exe\"): raise RuntimeError(\"cl.exe still not found, path probably incorrect\") Edit: you need to run a MSVS version compatible with CUDA. I.e. CUDA v9.0 doesn't support MSVS2017 and CUDA v9.1 only supports version 15.4, not later versions. Try if it works by running nvcc.exe from the Native Tools Command Prompt for Visual Studio."
            ],
            [
                "If you are using Windows make the following settings in Environment Variables: PATH: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\libnvvp C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\lib\\x64 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\extras\\CUPTI\\libx64 C:\\ProgramFiles(x86)\\MicrosoftVisualStudio\\2019\\Community\\VC\\Tools\\MSVC\\14.23.28105\\bin\\Hostx64\\x64\" CUDA_PATH: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2 CUDA_PATH_V8_0: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2 NVCUDASAMPLES_ROOT: C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.2 NVCUDASAMPLES8_0_ROOT: C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.2 After you finish, click OKs and restart the computer."
            ]
        ],
        "votes": [
            3.0000001,
            -0.9999999
        ]
    },
    {
        "question": "I'm trying to compile a PyCUDA kernel. The compilation fails with the error: pycuda.driver.CompileError: nvcc preprocessing of C:\\Users\\itay\\AppData\\Local\\Temp\\tmp78b6tln1.cu failed [command: nvcc --preprocess -arch sm_52 -m64 -Ic:\\users\\itay\\sources\\pythonmaps\\env\\lib\\site-packages\\pycuda\\cuda C:\\Users\\itay\\AppData\\Local\\Temp\\tmp78b6tln1.cu --compiler-options -EP] Running the same command from a command prompt works fine. The CompileError exception has a strerr attribute which contains the stderr of the compilation. It is empty. Only by placing a break-point inside PyCUDA did I find the actual error, which was reported by nvcc to stdout. Is there a way to get the compilation output without placing breakpoints in PyCUDA?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Is there any way to find the rank of element in a matrix row separately using CUDA or any functions for the same provided by NVidia?",
        "answers": [
            [
                "I don't know of a built-in ranking or argsort function in CUDA or any of the libraries I am familiar with. You could certainly build such a function out of lower-level operations using thrust for example. Here is a (non-optimized) outline of a possible solution approach using thrust: $ cat t84.cu #include &lt;thrust/device_vector.h&gt; #include &lt;thrust/copy.h&gt; #include &lt;thrust/sort.h&gt; #include &lt;thrust/sequence.h&gt; #include &lt;thrust/functional.h&gt; #include &lt;thrust/adjacent_difference.h&gt; #include &lt;thrust/transform.h&gt; #include &lt;thrust/iterator/permutation_iterator.h&gt; #include &lt;iostream&gt; typedef int mytype; struct clamp { template &lt;typename T&gt; __host__ __device__ T operator()(T data){ if (data == 0) return 0; return 1;} }; int main(){ mytype data[] = {4,1,7,1}; int dsize = sizeof(data)/sizeof(data[0]); thrust::device_vector&lt;mytype&gt; d_data(data, data+dsize); thrust::device_vector&lt;int&gt; d_idx(dsize); thrust::device_vector&lt;int&gt; d_result(dsize); thrust::sequence(d_idx.begin(), d_idx.end()); thrust::sort_by_key(d_data.begin(), d_data.end(), d_idx.begin(), thrust::less&lt;mytype&gt;()); thrust::device_vector&lt;int&gt; d_diff(dsize); thrust::adjacent_difference(d_data.begin(), d_data.end(), d_diff.begin()); d_diff[0] = 0; thrust::transform(d_diff.begin(), d_diff.end(), d_diff.begin(), clamp()); thrust::inclusive_scan(d_diff.begin(), d_diff.end(), d_diff.begin()); thrust::copy(d_diff.begin(), d_diff.end(), thrust::make_permutation_iterator(d_result.begin(), d_idx.begin())); thrust::copy(d_result.begin(), d_result.end(), std::ostream_iterator&lt;int&gt;(std::cout, \",\")); std::cout &lt;&lt; std::endl; } $ nvcc -arch=sm_61 -o t84 t84.cu $ ./t84 1,0,2,0, $"
            ],
            [
                "If you are in CUDA, the concept rank is not the same as the one on other languages as openmp or mpi. On that case you will need to go on a global block of the code you need to work with threadIdx.x and blockIdx.x parameters"
            ]
        ],
        "votes": [
            1.0000001,
            -0.9999999
        ]
    },
    {
        "question": "I'm very new to GPU programming and pyCUDA and have a pretty fundamental gap in my knowledge. I have spent quite a bit of time searching SO, looking at example code and reading supporting documentation for CUDA/pyCUDA but haven't found much diversity in the explanations and can't get my head around a few things. I am having trouble correctly defining block and grid dimensions. The code I am currently running is as follows, and aims to do element-wise multiplication of an array a by a float b: from __future__ import division import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np rows = 256 cols = 10 a = np.ones((rows, cols), dtype=np.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) b = np.float32(2) mod = SourceModule(\"\"\" __global__ void MatMult(float *a, float b) { const int i = threadIdx.x + blockDim.x * blockIdx.x; const int j = threadIdx.y + blockDim.y * blockIdx.y; int Idx = i + j*gridDim.x; a[Idx] *= b; } \"\"\") func = mod.get_function(\"MatMult\") xBlock = np.int32(np.floor(1024/rows)) yBlock = np.int32(cols) bdim = (xBlock, yBlock, 1) dx, mx = divmod(rows, bdim[0]) dy, my = divmod(cols, bdim[1]) gdim = ( (dx + (mx&gt;0)) * bdim[0], (dy + (my&gt;0)) * bdim[1]) print \"bdim=\",bdim, \", gdim=\", gdim func(a_gpu, b, block=bdim, grid=gdim) a_doubled = np.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print a_doubled - 2*a The code should print the block dimensions bdim and the grid dimensions gdim, as well as an array of zeroes. This works for small array sizes, for example, if rows=256 and cols=10 (as in the example above) the output is as follows: bdim= (4, 10, 1) , gdim= (256, 10) [[ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] ..., [ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.]] However, if I increase rows=512, I get the following output: bdim= (2, 10, 1) , gdim= (512, 10) [[ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] ..., [ 2. 2. 2. ..., 2. 2. 2.] [ 2. 2. 2. ..., 2. 2. 2.] [ 2. 2. 2. ..., 2. 2. 2.]] Indicating that the multiplication is happening twice for some elements of the array. However, if I force the block dimensions to bdim = (1,1,1), the problem no longer occurs and I get the following (correct) output for the larger array size: bdim= (1, 1, 1) , gdim= (512, 10) [[ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] ..., [ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.]] I don't understand this. What is happening here which means that this method of defining the block and grid dimensions is no longer appropriate as the array size is increased? Also, if block has dimensions (1,1,1) does this mean that the calculation is being performed serially? Thanks in advance for any pointers and help!",
        "answers": [
            [
                "You operate on 2D grid of 2D blocks. In your kernel you seem to assume that gridDim.x would return number of threads in x dimension of a grid. __global__ void MatMult(float *a, float b) { const int i = threadIdx.x + blockDim.x * blockIdx.x; const int j = threadIdx.y + blockDim.y * blockIdx.y; int Idx = i + j*gridDim.x; a[Idx] *= b; } The gridDim.x returns number of blocks r x direction of grid, not number of threads. In order to obtain number of threads in given direction you should multiply number of threads in a block with number of blocks in a grid in the same direction: int Idx = i + j * blockDim.x * gridDim.x"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am working with CUDA and 3D textures in python (using pycuda). There is a function called Memcpy3D which has the same members as Memcpy2D plus a few extras. In it it calls you to describe things such as width_in_bytes, src_pitch, src_height, height and copy_depth. This is what I am struggling with (in 3D) and its relevance with C or F style indexing. For instance, if I simply change the ordering from F to C in the working example below, it stops working - and I don't know why. First of all, I understand pitch to be how many bytes in memory it takes to move one index across in threadIdx.x (or the x direction, or a column). So for a float32 array of C shape (3,2,4), to move one value in x I expect to move 4 values in memory (as the indexing goes down the z axis first?). Therefore my pitch would be 4*32bits. I understand height to be the number of rows. (In this example, 3) I understand width to be the number of cols. (In this example, 2) I understand depth to be the number of z slices. (In this example, 4) I understand width_in_bytes to be the width of a row in x inclusive of the z elements behind it, i.e. a row slice, (0,:,:). This would be how many addresses in memory it takes to transverse one element in the y-direction. So when I change the ordering from F to C in the code below, and adapt the code to change the height/width values accordingly it still doesn't work. It just presents a logic failure which makes me think I'm not understanding the concept of pitch, width, height, depth correctly. Please educate me. Below is a full working script that copies an array to the GPU as a texture and copies the contents back. import pycuda.driver as drv import pycuda.gpuarray as gpuarray import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np w = 2 h = 3 d = 4 shape = (w, h, d) a = np.arange(24).reshape(*shape,order='F').astype('float32') print(a.shape,a.strides) print(a) descr = drv.ArrayDescriptor3D() descr.width = w descr.height = h descr.depth = d descr.format = drv.dtype_to_array_format(a.dtype) descr.num_channels = 1 descr.flags = 0 ary = drv.Array(descr) copy = drv.Memcpy3D() copy.set_src_host(a) copy.set_dst_array(ary) copy.width_in_bytes = copy.src_pitch = a.strides[1] copy.src_height = copy.height = h copy.depth = d copy() mod = SourceModule(\"\"\" texture&lt;float, 3, cudaReadModeElementType&gt; mtx_tex; __global__ void copy_texture(float *dest) { int x = threadIdx.x; int y = threadIdx.y; int z = threadIdx.z; int dx = blockDim.x; int dy = blockDim.y; int i = (z*dy + y)*dx + x; dest[i] = tex3D(mtx_tex, x, y, z); } \"\"\") copy_texture = mod.get_function(\"copy_texture\") mtx_tex = mod.get_texref(\"mtx_tex\") mtx_tex.set_array(ary) dest = np.zeros(shape, dtype=np.float32, order=\"F\") copy_texture(drv.Out(dest), block=shape, texrefs=[mtx_tex]) print(dest)",
        "answers": [
            [
                "Not sure I fully understand the problem in your code, but I'll attempt to clarify. In CUDA, width (x) refers to the fastest-changing dimension, height (y) is the middle dimension, and depth (z) is the slowest-changing dimension. The pitch refers to the stride in bytes required to step between values along the y dimension. In Numpy, an array defined as np.empty(shape=(3,2,4), dtype=np.float32, order=\"C\") has strides=(32, 16, 4), and corresponds to width=4, height=2, depth=3, pitch=16. Using \"F\" ordering in Numpy means the order of dimensions is reversed in memory. Your code appears to work if I make the following changes: #shape = (w, h, d) shape = (d, h, w) #a = np.arange(24).reshape(*shape,order='F').astype('float32') a = np.arange(24).reshape(*shape,order='C').astype('float32') ... #dest = np.zeros(shape, dtype=np.float32, order=\"F\") dest = np.zeros(shape, dtype=np.float32, order=\"C\") #copy_texture(drv.Out(dest), block=shape, texrefs=[mtx_tex]) copy_texture(drv.Out(dest), block=(w,h,d), texrefs=[mtx_tex])"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am very new to CUDA programming and am starting off with PyCUDA to get the basics. I studied the tutorials and have run a couple of simple test codes. The tests used only 1D arrays. When I tried to run the following code with 2D arrays, I am continuously getting a PyCUDA warning saying that the clean-up operation failed due to misaligned address. import pycuda.autoinit import pycuda.driver as drv import numpy as np from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply(float **dest) { const int i = threadIdx.x; const int j = threadIdx.y; dest[i][j] = 2.0*dest[i][j]; } \"\"\") a = np.random.randn(32, 32).astype(np.float32) multiply = mod.get_function(\"multiply\") multiply(drv.InOut(a), block=(32,32,1), grid=(1,1)) print(a) The error that I get when I run the above script is: Traceback (most recent call last): File \"cudaTest.py\", line 16, in &lt;module&gt; multiply(drv.InOut(a), block=(32,32,1), grid=(1,1)) File \"/users/gpu/local/python3.3/lib/python3.6/site-packages/pycuda-2016.1.2-py3.6-linux-x86_64.egg/pycuda/driver.py\", line 405, in function_call Context.synchronize() pycuda._driver.LogicError: cuCtxSynchronize failed: misaligned address PyCUDA WARNING: a clean-up operation failed (dead context maybe?) cuMemFree failed: misaligned address PyCUDA WARNING: a clean-up operation failed (dead context maybe?) cuModuleUnload failed: misaligned address I have checked other questions on SO and found a similar one. Following the answer given there, I tried specifying the size of the array a, but to no avail. I am running this on a cluster with two nVidia Tesla K10 GPUs. Since I have no root access, I had to install Python3 locally and add numpy, pyCUDA etc to the local installation. The cluster runs on Ubuntu 12.04.1 LTS. I am using Python 3.6.0 with PyCUDA 2016.1.2 and CUDA 6.0",
        "answers": [
            [
                "The problem here is that your understanding of what constitutes a \"2D array\" is incorrect. Numpy arrays (and by extension PyCUDA gpuarrays) are stored in pitched linear memory in row major order by default. Your kernel has been written to expect an array of pointers as in input, and is attempting to use floating point data as addresses, leading to the runtime addressing error you are seeing. To correct your kernel to work with the array, you would need to modify it to something like: mod = SourceModule(\"\"\" __global__ void multiply(float *dest, int lda) { const int i = threadIdx.x; const int j = threadIdx.y; float *p = &amp;dest[i * lda + j]; // row major order *p *= 2.0f; } \"\"\") Note that the array is passed as a pointer to a pitched linear allocation, not an array of row pointers. Because of this, you will need to pass the pitch of the array in elements to the kernel as well, so that the calling PyCUDA host code looks like: N = 8 a = np.random.randn(N, N).astype(np.float32) print(a) multiply = mod.get_function(\"multiply\") lda = np.int32(N) multiply(drv.InOut(a), lda, block=(N,N,1), grid=(1,1)) print(a) You should find this will now work correctly."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers. We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 6 years ago. Improve this question For a project that I'm working on, I'm supposed to brute force decrypt an AES-encrypted ciphertext given a portion of the key. The remaining keyspace for the ciphertext is 2^40. I'd like to run the decryption using CUDA (divide the keyspace over the GPU cores), but I can't seem to find a suitable CUDA AES library. I was wondering if there might be ways around this, such as running a C AES library decrypt in a kernel. Looking at this question suggests that this may not be possible. Another option - I currently have an implementation in python; would it be feasible to (learn and) use pyCuda to parallelize it, or would I run into the same problem as above using trying to use a python AES library function? Also any alternative suggestions to achieve what I'm trying to do would be greatly appreciated! Thanks!",
        "answers": [
            [
                "If you can't find a library that suits your needs (meaning it has a CUDA implementation of the functionalities you expect), you have to do your own implementation. However, if you have the sources in any other language, and as this problem seems to be pure mathematics, you should be able to write an \"equivalent\" in any other language. My suggestion is : First write your own C port of the Python implementation you have (\"classic\" CPU code) Then write an adaptation of this port with C and CUDA C to use one or several GPUs in the computation Notice I said \"adapt\" and not \"translate\", as it is really different than just switching from a language to another : you will create a project that uses GPUs in a specific kind of job and you have to consider all the differences between CPU and GPU programming. Anyway this might be a little off-topic for SO, as too broad about GPGPU and not enough focused on a specific problem on a source code from your own."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "OS: win10 VS: visual stadio2015 64bit CUDA: CUDA8.0 python: python2.7.12 64bit (pycuda) I followed this website, https://documen.tician.de/pycuda/tutorial.html#getting-started import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.random.randn(4,4) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu,a)#transfer the data to the GPU #executing a kernel #function: write code to double each entry in a_gpu. #we write the corresponding CUDA C code, and feed it into the constructor of pycuda.compiler.SourceModule mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") #If there aren\u2019t any errors, the code is now compiled and loaded onto the device. We find a reference to our pycuda.driver.Function and call it, specifying a_gpu as the argument, and a block size of 4x4: func = mod.get_function(\"doublify\") func(a_gpu, block=(4,4,1)) #Finally, we fetch the data back from the GPU and display it, together with the original a: a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print a_doubled print a but,failed with the error: Traceback (most recent call last): File \"G:/myworkspace/python2.7/cuda/test.py\", line 24, in &lt;module&gt; \"\"\") File \"D:\\python2.7\\lib\\site-packages\\pycuda\\compiler.py\", line 265, in __init__ arch, code, cache_dir, include_dirs) File \"D:\\python2.7\\lib\\site-packages\\pycuda\\compiler.py\", line 255, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"D:\\python2.7\\lib\\site-packages\\pycuda\\compiler.py\", line 137, in compile_plain stderr=stderr.decode(\"utf-8\", \"replace\")) CompileError: nvcc compilation of c:\\users\\gl\\appdata\\local\\temp\\tmp8poxqp\\kernel.cu failed [command: nvcc --cubin -arch sm_50 -m64 -Id:\\python2.7\\lib\\site-packages\\pycuda\\cuda kernel.cu] [stdout: nvcc fatal : Cannot find compiler 'cl.exe' in PATH ] Someone said to add the dir of cl.exe to environment. I did, and the error is the same. I'm new for CUDA. How could I solve this problem? Some advice? I did as @citizenSNIPS adviced: add the path to cl.exe, D:\\vs2015\\VC\\bin. INCLUDE = C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.10240.0\\ucrt. LIB = C:\\Program Files (x86)\\Windows Kits\\10\\Lib\\10.0.10240.0\\ucrt\\x64(I can't find C:\\Program Files (x86)\\Windows Kits\\10\\Lib\\10.0.10240.0\\um\\x64 in my computer). There had a new error as follow: raceback (most recent call last): File \"G:\\myworkspace\\python2.7\\cuda\\test.py\", line 24, in &lt;module&gt; \"\"\") File \"D:\\python2.7\\lib\\site-packages\\pycuda\\compiler.py\", line 265, in __init__ arch, code, cache_dir, include_dirs) File \"D:\\python2.7\\lib\\site-packages\\pycuda\\compiler.py\", line 255, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"D:\\python2.7\\lib\\site-packages\\pycuda\\compiler.py\", line 147, in compile_plain + (stdout+stderr).decode(\"utf-8\", \"replace\"), stacklevel=4) File \"D:\\python2.7\\lib\\idlelib\\run.py\", line 36, in idle_showwarning_subproc message, category, filename, lineno, line)) File \"D:\\python2.7\\lib\\idlelib\\PyShell.py\", line 65, in idle_formatwarning s += \"%s: %s\\n\" % (category.__name__, message) UnicodeEncodeError: 'ascii' codec can't encode characters in position 147-168: ordinal not in range(128) now I'm working for this problem, maybe it's because I did not add C:\\Program Files (x86)\\Windows Kits\\10\\Lib\\10.0.10240.0\\um\\x64?",
        "answers": [
            [
                "You could also have added the path to cl.exe in your python file. Downside is that you will have to change it if your MSVS version changes. Example: import os if (os.system(\"cl.exe\")): os.environ['PATH'] += ';'+r\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64\" if (os.system(\"cl.exe\")): raise RuntimeError(\"cl.exe still not found, path probably incorrect\") Edit: you need to run a MSVS version compatible with CUDA. I.e. CUDA v9.0 doesn't support MSVS2017 and CUDA v9.1 only supports version 15.4, not later versions. Try if it works by running nvcc.exe from the Native Tools Command Prompt for Visual Studio."
            ],
            [
                "you need to specify the path to cl.exe. go to \"Control Panel\\All Control Panel Items\\System\" and select \"advanced system settings\" select 'Environment Valiables'. under system variables, find PATH, click edit, and add the path to cl.exe. it should be: C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\ Make sure when you installed visual studios, you selected to install c++ compiler. it is not installed by default. if you didn't, re-run your visual studio installer and select to install the c++ compiler. once you finish with that, you might need to add the following system variables INCLUDE = C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.10240.0\\ucrt LIB = C:\\Program Files (x86)\\Windows Kits\\10\\Lib\\10.0.10240.0\\um\\x64 C:\\Program Files (x86)\\Windows Kits\\10\\Lib\\10.0.10240.0\\ucrt\\x64 see this thread here"
            ],
            [
                "I add these code in .py import sys reload(sys) sys.setdefaultencoding('utf8') and run, there is no error"
            ]
        ],
        "votes": [
            2.0000001,
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "For learning purposes, I wrote a small C Python module that is supposed to perform an IPC cuda memcopy to transfer data between processes. For testing, I wrote equivalent programs: one using theano's CudaNdarray, and the other using pycuda. The problem is, even though the test programs are nearly identical, the pycuda version works while the theano version does not. It doesn't crash: it just produces incorrect results. Below is the relevant function in the C module. Here is what it does: every process has two buffers: a source and a destination. Calling _sillycopy(source, dest, n) copies n elements from each process's source buffer to the neighboring process's dest array. So, if I have two processes, 0 and 1, processes 0 will end up with process 1's source buffer and processes 1 will end up with process 0's source buffer. Note that to transfer cudaIpcMemHandle_t values between processes, I use MPI (this is a small part of a larger project which uses MPI). _sillycopy is called by another function, \"sillycopy\" which is exposed in Python by the standard Python C API methods. void _sillycopy(float *source, float* dest, int n, MPI_Comm comm) { int localRank; int localSize; MPI_Comm_rank(comm, &amp;localRank); MPI_Comm_size(comm, &amp;localSize); // Figure out which process is to the \"left\". // m() performs a mod and treats negative numbers // appropriately int neighbor = m(localRank - 1, localSize); // Create a memory handle for *source and do a // wasteful Allgather to distribute to other processes // (could just use an MPI_Sendrecv, but irrelevant right now) cudaIpcMemHandle_t *memHandles = new cudaIpcMemHandle_t[localSize]; cudaIpcGetMemHandle(memHandles + localRank, source); MPI_Allgather( memHandles + localRank, sizeof(cudaIpcMemHandle_t), MPI_BYTE, memHandles, sizeof(cudaIpcMemHandle_t), MPI_BYTE, comm); // Open the neighbor's mem handle so we can do a cudaMemcpy float *sourcePtr; cudaIpcOpenMemHandle((void**)&amp;sourcePtr, memHandles[neighbor], cudaIpcMemLazyEnablePeerAccess); // Copy! cudaMemcpy(dest, sourcePtr, n * sizeof(float), cudaMemcpyDefault); cudaIpcCloseMemHandle(sourcePtr); delete [] memHandles; } Now here is the pycuda example. For reference, using int() on a_gpu and b_gpu returns the pointer to the underlying buffer's memory address on the device. import sillymodule # sillycopy lives in here import simplempi as mpi import pycuda.driver as drv import numpy as np import atexit import time mpi.init() drv.init() # Make sure each process uses a different GPU dev = drv.Device(mpi.rank()) ctx = dev.make_context() atexit.register(ctx.pop) shape = (2**26,) # allocate host memory a = np.ones(shape, np.float32) b = np.zeros(shape, np.float32) # allocate device memory a_gpu = drv.mem_alloc(a.nbytes) b_gpu = drv.mem_alloc(b.nbytes) # copy host to device drv.memcpy_htod(a_gpu, a) drv.memcpy_htod(b_gpu, b) # A few more host buffers a_p = np.zeros(shape, np.float32) b_p = np.zeros(shape, np.float32) # Sanity check: this should fill a_p with 1's drv.memcpy_dtoh(a_p, a_gpu) # Verify that print(a_p[0:10]) sillymodule.sillycopy( int(a_gpu), int(b_gpu), shape[0]) # After this, b_p should have all one's drv.memcpy_dtoh(b_p, b_gpu) print(c_p[0:10]) And now the theano version of the above code. Rather than using int() to get the buffers' address, the CudaNdarray way of accessing this is via the gpudata attribute. import os import simplempi as mpi mpi.init() # select's one gpu per process os.environ['THEANO_FLAGS'] = \"device=gpu{}\".format(mpi.rank()) import theano.sandbox.cuda as cuda import time import numpy as np import time import sillymodule shape = (2 ** 24, ) # Allocate host data a = np.ones(shape, np.float32) b = np.zeros(shape, np.float32) # Allocate device data a_gpu = cuda.CudaNdarray.zeros(shape) b_gpu = cuda.CudaNdarray.zeros(shape) # Copy from host to device a_gpu[:] = a[:] b_gpu[:] = b[:] # Should print 1's as a sanity check print(np.asarray(a_gpu[0:10])) sillymodule.sillycopy( a_gpu.gpudata, b_gpu.gpudata, shape[0]) # Should print 1's print(np.asarray(b_gpu[0:10])) Again, the pycuda code works perfectly and the theano version runs, but gives the wrong result. To be precise, at the end of the theano code, b_gpu is filled with garbage: neither 1's nor 0's, just random numbers as though it were copying from a wrong place in memory. My original theory regarding why this was failing had to do with CUDA contexts. I wondered if it was possible theano was doing something with them that meant that the cuda calls made in sillycopy were run under a different CUDA context than had been used to create the gpu arrays. I don't think this is the case because: I spent a lot of time digging deep in theano's code and saw no funny business being played with contexts I would expect such a problem to result in a bad crash, not an incorrect result, which is not what happens. A secondary thought is whether this has to do the fact that theano spawns several threads, even when using a cuda backend, which can be verified this by running \"ps huH p \". I don't know how threads might affect anything, but I have run out of obvious things to consider. Any thoughts on this would be greatly appreciated! For reference: the processes are launched in the normal OpenMPI way: mpirun --np 2 python test_pycuda.py",
        "answers": [],
        "votes": []
    },
    {
        "question": "My environment: I'm using Hortonworks HDP 2.4 with Spark 1.6.1 on a small AWS EC2 cluster of 4 g2.2xlarge instances with Ubuntu 14.04. Each instance has CUDA 7.5, Anaconda Python 3.5, and Pycuda 2016.1.1. in /etc/bash.bashrc I've set: CUDA_HOME=/usr/local/cuda CUDA_ROOT=/usr/local/cuda PATH=$PATH:/usr/local/cuda/bin On all 4 machines I can access nvcc from the command line for the ubuntu user, the root user, and the yarn user. My problem: I have a Python-Pycuda project I've adapted to run on Spark. It runs great on my local Spark installation on my Mac, but when I run it on AWS I get: FileNotFoundError: [Errno 2] No such file or directory: 'nvcc' since it runs on my Mac in local mode, my guess is that it is a configuration issue with CUDA/Pycuda in the worker processes but I'm really stumped as to what it could be. Any ideas? Edit: Below is a stack trace from one of the jobs failing: 16/11/10 22:34:54 INFO ExecutorAllocationManager: Requesting 13 new executors because tasks are backlogged (new desired total will be 17) 16/11/10 22:34:57 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 34, ip-172-31-26-35.ec2.internal, partition 16,RACK_LOCAL, 2148 bytes) 16/11/10 22:34:57 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ip-172-31-26-35.ec2.internal:54657 (size: 32.2 KB, free: 511.1 MB) 16/11/10 22:35:03 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 18, ip-172-31-26-35.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pytools/prefork.py\", line 46, in call_capture_output popen = Popen(cmdline, cwd=cwd, stdin=PIPE, stdout=PIPE, stderr=PIPE) File \"/home/ubuntu/anaconda3/lib/python3.5/subprocess.py\", line 947, in __init__ restore_signals, start_new_session) File \"/home/ubuntu/anaconda3/lib/python3.5/subprocess.py\", line 1551, in _execute_child raise child_exception_type(errno_num, err_msg) FileNotFoundError: [Errno 2] No such file or directory: 'nvcc' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/hadoop/yarn/local/usercache/ubuntu/appcache/application_1478814770538_0004/container_e40_1478814770538_0004_01_000009/pyspark.zip/pyspark/worker.py\", line 111, in main process() File \"/hadoop/yarn/local/usercache/ubuntu/appcache/application_1478814770538_0004/container_e40_1478814770538_0004_01_000009/pyspark.zip/pyspark/worker.py\", line 106, in process serializer.dump_stream(func(split_index, iterator), outfile) File \"/usr/hdp/2.4.2.0-258/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2346, in pipeline_func File \"/usr/hdp/2.4.2.0-258/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2346, in pipeline_func File \"/usr/hdp/2.4.2.0-258/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 317, in func File \"/home/ubuntu/pycuda-euler/src/cli_spark_gpu.py\", line 36, in &lt;lambda&gt; hail_mary = data.mapPartitions(lambda x: ec.assemble2(k, buffer=x, readLength = dataLength,readCount=dataCount)).saveAsTextFile('hdfs://172.31.26.32/genome/sra_output') File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 499, in assemble2 lmerLength, evList, eeList, levEdgeList, entEdgeList, readCount) File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 238, in constructDebruijnGraph lmerCount, h_kmerKeys, h_kmerValues, kmerCount, numReads) File \"./eulercuda.zip/eulercuda/eulercuda.py\", line 121, in readLmersKmersCuda d_lmers = enc.encode_lmer_device(buffer, partitionReadCount, d_lmers, readLength, lmerLength) File \"./eulercuda.zip/eulercuda/pyencode.py\", line 78, in encode_lmer_device \"\"\") File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/compiler.py\", line 265, in __init__ arch, code, cache_dir, include_dirs) File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/compiler.py\", line 255, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/compiler.py\", line 78, in compile_plain checksum.update(preprocess_source(source, options, nvcc).encode(\"utf-8\")) File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/compiler.py\", line 50, in preprocess_source result, stdout, stderr = call_capture_output(cmdline, error_on_nonzero=False) File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pytools/prefork.py\", line 197, in call_capture_output return forker[0].call_capture_output(cmdline, cwd, error_on_nonzero) File \"/home/ubuntu/anaconda3/lib/python3.5/site-packages/pytools/prefork.py\", line 54, in call_capture_output % ( \" \".join(cmdline), e)) pytools.prefork.ExecError: error invoking 'nvcc --preprocess -arch sm_30 -I/home/ubuntu/anaconda3/lib/python3.5/site-packages/pycuda/cuda /tmp/tmpkpqwoaxf.cu --compiler-options -P': [Errno 2] No such file or directory: 'nvcc' at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166) at org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:207) at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313) at org.apache.spark.rdd.RDD.iterator(RDD.scala:277) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313) at org.apache.spark.rdd.RDD.iterator(RDD.scala:277) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313) at org.apache.spark.rdd.RDD.iterator(RDD.scala:277) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)",
        "answers": [
            [
                "To close the loop on this, I finally worked my way through the problem. Note: I know this is not really a good nor permanent answer for most people however in my case I am running POC code for my dissertation and as soon as I get some final results I'm decommissioning the servers. I doubt this answer will be suitable or appropriate for most users. I ended up hardcoding the full path to nvcc into compile_plain() in Pycuda's compiler.py file. Partial listing: def compile_plain(source, options, keep, nvcc, cache_dir, target=\"cubin\"): from os.path import join assert target in [\"cubin\", \"ptx\", \"fatbin\"] nvcc = '/usr/local/cuda/bin/'+nvcc if cache_dir: checksum = _new_md5() Hopefully this points someone else in the proper direction."
            ],
            [
                "The error means that nvcc is not in PATH for the process that runs the code. Amazon ECS Container Agent Configuration - Amazon EC2 Container Service has instructions on how to set up environment variables for the cluster. For the same in Hadoop, there's Configuring Environment of Hadoop Daemons \u2013 Hadoop Cluster Setup."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm trying to implement a sparse matrix vector operation using pycuda. The only example I've been able to find for this is on their wiki, which implements a sparse solve routine, however I'm just interested in the matrix-vector multiplication part. The matrix is a block diagonal NxM matrix, and I have a dense length M vector. Below is some dummy code that does what I want, but should run (and fail) for anyone. It makes a block diagonal matrix using the scipy sparse routine that is 100, (100x7) matrices in a block diagonal format. I then call this 'PacketedSpMV' function which is the thing that fails with the error: only square matrices are supported I cannot find any useful documentation for what this function does, or if there is a different version that works for non square matrices. I also don't know what this permute thing is doing, however it was used in an answer to a different question on how to do a square matrix-vector operation. I also can't find any useful examples where, for example, once the spmv object has been created, I can update the contents with different matrices that have the same structure, rather than recalling this function every time (which seems to be slow). First things first though, is there no non-square way to do this? Or do I need to pad everything with zeros and do a sparse-sparse matrix-vector multiplication, which seems bizarre. Thanks, and apologies for the basic question. from __future__ import division import pycuda.autoinit import pycuda.driver as drv import pycuda.gpuarray as gpuarray import numpy from pycuda.sparse.packeted import PacketedSpMV import scipy.sparse as ss A=np.ones([100,10,7]) M = ss.block_diag(A, format=\"csr\") spmv = PacketedSpMV(M, False, M.dtype) rhs = numpy.random.rand(spmv.shape[0]).astype(spmv.dtype) rhs_gpu = gpuarray.to_gpu(rhs) xp = spmv.permute(rhs_gpu) yp = spmv(xp) y = spmv.unpermute(yp)",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to install pycuda on Linux Mint with a GeForce 960M and Cuda 8.0 installed. When I run the test_driver.py script it outputs the following error: ============================= test session starts ============================== platform linux2 -- Python 2.7.12, pytest-3.0.3, py-1.4.31, pluggy-0.4.0 rootdir: /home/milton/Downloads/pycuda-2016.1.2, inifile: collected 28 items test_driver.py ...................x.....F.. =================================== FAILURES =================================== ________________________ TestDriver.test_multi_context _________________________ args = (,), kwargs = {} pycuda = &lt;module 'pycuda' from '/home/milton/miniconda2/lib/python2.7/site-packages/pycuda-2016.1.2-py2.7-linux-x86_64.egg/pycuda/init.pyc'&gt; ctx = &lt;pycuda._driver.Context object at 0x7f540e39d758&gt; clear_context_caches = &lt;function clear_context_caches at 0x7f540ee26758&gt; collect =&lt;built-in function collect&gt; def f(*args, **kwargs): import pycuda.driver # appears to be idempotent, i.e. no harm in calling it more than once pycuda.driver.init() ctx = make_default_context() try: assert isinstance(ctx.get_device().name(), str) assert isinstance(ctx.get_device().compute_capability(), tuple) assert isinstance(ctx.get_device().get_attributes(), dict) inner_f(*args, **kwargs) ../../../miniconda2/lib/python2.7/site-packages/pycuda-2016.1.2-py2.7-linux-x86_64.egg/pycuda/tools.py:460: self = &lt;test_driver.TestDriver instance at 0x7f540c21fc20&gt; @mark_cuda_test def test_multi_context(self): if drv.get_version() &lt; (2,0,0): return if drv.get_version() &gt;= (2,2,0): if drv.Context.get_device().compute_mode == drv.compute_mode.EXCLUSIVE: E AttributeError: type object 'compute_mode' has no attribute 'EXCLUSIVE' test_driver.py:638: AttributeError ================ 1 failed, 26 passed, 1 xfailed in 6.92 seconds ================",
        "answers": [
            [
                "python driver compute mode only supports following modes: DEFAULT, PROHIBITED, EXCLUSIVE_PROCESS so please change this: if drv.Context.get_device().compute_mode == drv.compute_mode.EXCLUSIVE: to if drv.Context.get_device().compute_mode == drv.compute_mode.EXCLUSIVE_PROCESS: in your test_driver.py file"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Consider the following Python code: from numpy import float64 from pycuda import compiler, gpuarray import pycuda.autoinit # N &gt; 960 is crucial! N = 961 code = \"\"\" __global__ void kern(double *v) { double a = v[0]*v[2]; double lmax = fmax(0.0, a), lmin = fmax(0.0, -a); double smax = sqrt(lmax), smin = sqrt(lmin); if(smax &gt; 0.2) { smax = fmin(smax, 0.2)/smax ; smin = (smin &gt; 0.0) ? fmin(smin, 0.2)/smin : 0.0; smin = lmin + smin*a; v[0] = v[0]*smin + smax*lmax; v[2] = v[2]*smin + smax*lmax; } } \"\"\" kernel_func = compiler.SourceModule(code).get_function(\"kern\") kernel_func(gpuarray.zeros(3, float64), block=(N,1,1)) Executing this gives: Traceback (most recent call last): File \"test.py\", line 25, in &lt;module&gt; kernel_func(gpuarray.zeros(3, float64), block=(N,1,1)) File \"/usr/lib/python3.5/site-packages/pycuda/driver.py\", line 402, in function_call func._launch_kernel(grid, block, arg_buf, shared, None) pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch My setup: Python v3.5.2 with pycuda==2016.1.2 and numpy==1.11.1 on Ubuntu 16.04.1 (64-bit), kernel 4.4.0, nvcc V7.5.17. The graphics card is an Nvidia GeForce GTX 480. Can you reproduce this on your machine? Do you have any idea, what causes this error message? Remark: I know that, in principle, there is a race condition because all kernels try to change v[0] and v[2]. But the kernels shouldn't reach the inside of the if-block anyway! Moreover, I'm able to reproduce the error without the race condition, but it's much more complicated.",
        "answers": [
            [
                "It is almost certain that you are hitting a registers-per-block limit. Reading the relevant documentation, your device has a limit of 32k 32 bit registers per block. When the block size is larger than 960 threads (30 warps), your kernel launch requires too many registers and the launch fails. NVIDIA supply an excel spreadsheet and advice on how to determine the per thread the register requirement of your kernel and the limiting block sizes you can use for your kernel to launch on your device."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I recently started working with Pycuda. I tried executing some codes, but I get the similar kind of error for all of the codes that I have tried executing. import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __gloabal__ void adder(float * h_a,float * h_b,float * h_c) { const int idx= blockDim.x*blockIdx.x+threadIdx.x; const int idy= blockDim.y*blockIdx.y+threadIdx.y; h_c[idx][idy]=h_a[idx][idy]+h_b[idx][idy]; } \"\"\") adder=mod.get_function(\"adder\") h_a=numpy.random.randn(64,64) h_a=h_a.astype(numpy.float32) h_b=numpy.random.randn(64,64) h_b=h_b.astype(numpy.float32) h_c=numpy.zeros_like(h_a) d_a=cuda.memalloc(h_a.nbytes) d_b=cuda.memalloc(h_b.nbytes) d_c=cuda.memalloc(h_c.nbytes) cuda.memcpy_htod(d_a,h_a) cuda.memcpy_htod(d_b,h_b) adder(d_a,d_b,d_c,block=(64,64,1),grid=(1,1)) cuda.memcpy_dtoh(h_c,d_c) print h_c",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to store the kernel part of the code, with the 3 \"\"\" , in a different file. I tried saving it as a text file and a bin file, and reading it in, but I didn't find success with it. It started giving me an error saying \"\"\" is missing, or ) is missing. \"However, if i just copy paste the kernel code into cl.Program(, it works. So, is there a way to abstract long kernel code out into another file? This is specific to python, thank you! #Kernel function prg = cl.Program(ctx, \"\"\" __kernel void sum(__global double *a, __global double *b, __global double *c) { int gid = get_global_id(0); c[gid] = 1; } \"\"\").build() So pretty much everything inside \"\"\" \"\"\", the second argument of cl.Program() function, I wan't to move into a different file.",
        "answers": [
            [
                "Just put your kernel code in a plain text file, and then use open(...).read() to get the contents: foo.cl __kernel void sum(__global double *a, __global double *b, __global double *c) { int gid = get_global_id(0); c[gid] = 1; } Python code prg = cl.Program(ctx, open('foo.cl').read()).build()"
            ],
            [
                "The kernel code can also be stored in a .c file as foo.c. That way its easier to edit in editors like xcode. It can be read the same way prg = cl.Program(ctx, open('foo.c').read()).build()"
            ],
            [
                "If you have split your kernel code into multiple files then you also have to pass options to the cl.Program.build() function. So for example if your kernel is called 'kernel.cl' and it includes 'functions.h' then OpenCL compiler will exit with error that it cannot find 'functions.h'. To fix this pass '-I' option to build() as below: kernel_location = dirname(__file__) kernel = open(join(kernel_location, 'kernel.cl').read() program = cl.Program(context, kernel).build(options=['-I', kernels_location]) This assumes that your python code, 'kernel.cl' and 'functions.h' are in the same directory."
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I'm working on a project that involves creating CUDA kernels in Python. Numba works quite well (what these guys have accomplished is quite incredible), and so does PyCUDA. My problem is that I want to call a C device function from my Python generated kernel. I couldn't find a way to accomplish this. Numba can call CFFI modules but only in CPU code. In PyCUDA I can add my C device functions to the SourceModule, but I couldn't figure out how to include functions that already exist in another library. Is there a way to accomplish this?",
        "answers": [
            [
                "As far as I am aware, this isn't possible in either language. Neither exposes the necessary toolchain controls for separate compilation or APIs to do runtime linking of device code."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I've researched this topic quite a bit and can't seem to come to a conclusion. So I know OpenCL can be used for parallel processing using both the GPU and CPU (in contrast to CUDA). Since I want to do parallel processing with GPU and CPU, would it be better to use Multiprocessing module from python + PyOpenCL/PyCUDA for parallel processing or just use PyOpenCL for both GPU and CPU parallel programming? I'm pretty new to this but intuitively, I would imagine multiprocessing module from python to be the best possible way to do CPU parallel processing in Python. Any help or direction would be much appreciated",
        "answers": [
            [
                "I don't know if you already got your answer, but take in mind GPUs are designed for floating point operations, and execute a complete python processes in it could slower than you may expect for a GPU. Anyway, once you are new in parallell processing, you should start with multiprocessing module, once GPU programming, and OpenCL library itself, are diffcult to learn when you have no base. You may take a look here https://philipwfowler.github.io/2015-01-13-oxford/intermediate/python/04-multiprocessing.html"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using a combination of kivy to do some interactive scientific visualization that leverages NVIDIA's GPUs using pycuda. I am currently looking at using the GL Interop functionality so that after my CUDA code has modified an array, I can immediately draw that array without going through the slow process of getting the data from the GPU device to the host cpu, and then sending the data back to the GPU to be displayed as an OpenGL texture. In my attempt to do this, I have been reading through the pycuda interop examples such as the Sobel filter and the more simple Tea Pot example. In both examples, pixel buffer objects (PBOs) are used. As far as I understand, Kivy does not currently support or use PBOs, and so for simplicity's sake I would prefer to avoid using PBOs and just have my CUDA functions operate directly on OpenGL texture data. Is this possible? Is this a bad idea? A snippet of my current attempt to register a texture for being accessible to CUDA looks like this: self.cuda_access = pycuda.gl.RegisteredImage(texture_id, GL_TEXTURE_2D) self.mapping_obj = self.cuda_access.map() self.data, self.sz = self.mapping_obj.device_ptr_and_size() ...but on that last line, I get the following error pycuda._driver.LogicError: cuGraphicsResourceGetMappedPointer failed: resource not mapped as pointer ...which I am not sure how to persue. Many thanks in advance.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I recently downloaded the newest scikit for work with FFTs. However, I have run into a problem. I have data size and window size of 2^19. The size of array going into the fft function is 524288, which is far below the 2^27 element limit listed in the documentation. multiply_them = ElementwiseKernel( \"float *dest, float *a, float *b\", #{ #const int i = blockIdx.x +threadIdx.x; \"dest[i] = a[i] * b[i]\", #} \"linear_combination\") #multiplythem = mod.get_function(\"multiply_them\") gval1 = gpuarray.to_gpu(val1.astype(numpy.float32)) #gval1 = input * rescale * gain gwindow = gpuarray.to_gpu(window.astype(numpy.float32)) #gwindow = filtering window gval2 = gpuarray.to_gpu(numpy.zeros_like(gval1.get()))#.astype(np.float32)) #set up zero array #val2 = numpy.zeros_like(val1).astype(numpy.float32) multiply_them(gval2, gval1, gwindow) # block=(max_block_dim,1,1), grid=(grid_dim,1)) #gval2 = gval1 .* gwindow val1 = gval2.get() #retrieve val1 from GPU #gval1 = fft(gval1,fft_window_size); #gval1 = fftshift(gval1,1); #gval1 = abs(gval1); gval1 = gpuarray.to_gpu(val1) gval2 = gpuarray.to_gpu(numpy.empty(fft_window_size, numpy.complex64)) plan_forward = cu_fft.Plan(gval1.shape[0]*2, numpy.float32, numpy.complex64) cu_fft.fft(gval1, gval2, plan_forward) #val2 = scipy.fftpack.fft(val1,fft_window_size) val1 = gval2.get() Yet, when I run the code and check it with MATLAB and Scipy's FFT functions, the values trail off to zero half-way through the computations. I can't figure out how to increase the batch size and still have correct numbers. Some advice would be nice.",
        "answers": [],
        "votes": []
    },
    {
        "question": "Due to a seeming lack of a decent 2D histogram for CUDA (that I can find... pointers welcome), I'm trying to implement it myself with pyCUDA. Here's what the histogram should look like (using Numpy): Here's what I've got so far: code = ''' __global__ void histogram2d(const float *in_x, const float *in_y, const float *in_w, float *out) {{ int start = blockIdx.x * blockDim.x + threadIdx.x; float *block_out = &amp;out[{xres} * {yres} * {num_chans} * blockIdx.x]; for(int i = 0; i &lt; {length}; i++) {{ float x = in_x[start + i]; float y = in_y[start + i]; int w_idx = (start + i) * {num_chans}; int xbin = (int) (((x - {xmin}) / {xptp}) * {xres}); int ybin = (int) (((y - {ymin}) / {yptp}) * {yres}); if (0 &lt;= xbin &amp;&amp; xbin &lt; {xres} &amp;&amp; 0 &lt;= ybin &amp;&amp; ybin &lt; {yres}) {{ for(int c = 0; c &lt; {num_chans}; c++) {{ atomicAdd(&amp;block_out[(ybin * {xres} + xbin) * {num_chans} + c], in_w[w_idx + c]); }} }} }} }} '''.format(**args) ------ __global__ void histogram2d(const float *in_x, const float *in_y, const float *in_w, float *out) { int start = blockIdx.x * blockDim.x + threadIdx.x; float *block_out = &amp;out[50 * 50 * 4 * blockIdx.x]; for(int i = 0; i &lt; 100; i++) { float x = in_x[start + i]; float y = in_y[start + i]; int w_idx = (start + i) * 4; int xbin = (int) (((x - -10.0) / 20.0) * 50); int ybin = (int) (((y - -10.0) / 20.0) * 50); if (0 &lt;= xbin &amp;&amp; xbin &lt; 50 &amp;&amp; 0 &lt;= ybin &amp;&amp; ybin &lt; 50) { for(int c = 0; c &lt; 4; c++) { atomicAdd(&amp;block_out[(ybin * 50 + xbin) * 4 + c], in_w[w_idx + c]); } } } } There seems to be a problem with the indexing, but I haven't done much pure CUDA before, so I can't tell what it is. Here's what I think the equivalent python would be: def slow_hist(in_x, in_y, in_w, out, blockx, blockdimx, threadx): start = blockx * blockdimx + threadx block_out_addr = args['xres'] * args['yres'], args['num_chans'] * blockx for i in range(args['length']): x = in_x[start + i] y = in_y[start + i] w_idx = (start + i) * args['num_chans'] xbin = int(((x - args['xmin']) / args['xptp']) * args['xres']) ybin = int(((y - args['ymin']) / args['yptp']) * args['yres']) if 0 &lt;= xbin &lt; args['xres'] and 0 &lt;= ybin &lt; args['yres']: for c in range(args['num_chans']): out[(ybin * args['xres'] + xbin) * args['num_chans'] + c] += in_w[w_idx + c] All the code is viewable, including these images, at the Github page of this notebook (this cell is at the bottom). What am I doing wrong in this CUDA code? I've tried lots of little tweaks (striding the atomicAdd address by 1, 4, 8, 16, transposing outputs, etc.), but it seems like I'm missing something subtle, probably in how the pointer arithmetic is working. Any help would be appreciated.",
        "answers": [
            [
                "The array being allocated for the output array for the CUDA section used Numpy's default float64 instead of float32, so memory was twice as large as expected. Here's the new histogram output: I'd still greatly appreciate comments or answers that help explain why these histograms are all so different from each other."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am having a problem printing from within a pycuda kernel: the printf() function prints nothing. There was a similar question posted here by @username_4567 and also an example given here, which @harrism pointed to in his answer. However, I have implemented the code in the pycuda example and nothing gets printed (though with no errors). I am guessing the problem is that I am using a Kepler GPU and the example specifies that it only works on Fermi devices. Does anybody know how I can print information from within pycuda kernels using my Kepler GPU? There's probably a work-around I could design, which copies any data I want to print on to the CPU and then print via Python but I'd prefer to avoid that! I have searched the web for anyone having the same problem but I found nothing. I am using Python 3.5 (Anaconda build), Spyder as an IDE (launched from terminal) and an iMac with El Capitan. GPU is GeForce GT 755M.",
        "answers": [
            [
                "There is nothing wrong with the code in example you are trying to use, and it is perfectly suitable for use on a Kepler GPU. The problem is that the CUDA runtime uses a buffer for printf output which is only periodically flushed by the driver, and which needs to be triggered by any one of several API calls. I am guessing you are testing this in an interactive python shell. In that case you should add an explicit synchronization call to the code: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void say_hi() { printf(\"I am %d.%d\\\\n\", threadIdx.x, threadIdx.y); } \"\"\") func = mod.get_function(\"say_hi\") func(block=(4,4,1)) # Flush context printf buffer cuda.Context.synchronize() Alternatively, if you add a shebang line and run the unmodified code from a command prompt: $ cat hello_cuda.py #!/usr/bin/env python import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" #include &lt;stdio.h&gt; __global__ void say_hi() { printf(\"I am %d.%d\\\\n\", threadIdx.x, threadIdx.y); } \"\"\") func = mod.get_function(\"say_hi\") func(block=(4,4,1)) $ ./hello_cuda.py I am 0.0 I am 1.0 I am 2.0 I am 3.0 I am 0.1 I am 1.1 I am 2.1 I am 3.1 I am 0.2 I am 1.2 I am 2.2 I am 3.2 I am 0.3 I am 1.3 I am 2.3 I am 3.3 it will also work. In the latter case, it is the context cleanup triggered by the pycuda.autoinit module which automagically flushes the buffer."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am trying to follow the instructions to install CUDA (https://github.com/andersbll/cudarray). I am able to install the cudarray python package WITHOUT CUDA back-end following the instruction. The installation seems to be successful. When \"pip freeze\" I can see \"cudarray==0.1.dev0\" on the list. Though what I really want to set up is WITH CUDA back-end and am having troubles. Question #1: The instruction suggests that the INSTALL_PREFIX should be \"/usr/local\" and CUDA_PREFIX should be \"/usr/local/cuda\". My python is installed via Data/Anaconda, and the path to my site package is \"C:\\Users\\XYZ\\AppData\\Local\\Dato\\Dato Launcher\\Lib\\site-packages\". Does it mean my INSTALL_PREFIX should be set to \"C:/Users/XYZ/AppData/Local\" and my CUDA_PREFIX should be \"C:/Users/XYZ/AppData/Local/cuda\"? Question #2: From the installation instruction, it says I have to type \"make\" then \"make install\". But I don't know where and when to execute these commands. What I tried was cmd (into terminal) &gt; cd (to the setup.py path under the cudarray-master folder) &gt; make. Then I got \"'make' is not recognized as an internal or external command, operable program or batch file.\" =========================================================================== Edits/updates: Since my question I have installed GnuWin32, then I add its bin path as an environment variable. After I cd to the location and type make, I got this error: C:\\Users\\XYZ\\Desktop\\DeepArtist_Python\\cudarray-master&gt;make g++ -O3 -fPIC -Wall -Wfatal-errors -D_FORCE_INLINES -I./include -I/usr/local/cuda/include -c -o src/nnet/conv_bc01_matmul.o src/nnet/conv_bc01_matmul.cpp process_begin: CreateProcess(NULL, g++ -O3 -fPIC -Wall -Wfatal-errors -D_FORCE_INLINES -I./include -I/usr/local/cuda/include -c -o src/nnet/conv_bc01_matmul.o src/nnet/conv_bc01_matmul.cpp, ...) failed. make (e=2): The system cannot find the file specified. make: *** [src/nnet/conv_bc01_matmul.o] Error 2",
        "answers": [
            [
                "This doesn't directly answer your question; however, it should provide insight into your issue or you can directly follow the outlined steps to get PyCUDA installed: I have recently (within the past 4 months) successfully completed a PyCUDA install on my Win 10 machine. The steps I followed including links to the requisite python and PyCUDA libraries can be found here: Win 10 PyCUDA install instructions"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This is one of the standard example code we find every where... import time import numpy import pycuda.gpuarray as gpuarray import pycuda.cumath as cumath import pycuda.autoinit size = 1e7 t0 = time.time() x = numpy.linspace(1, size, size).astype(numpy.float32) y = numpy.sin(x) t1 = time.time() cpuTime = t1-t0 print(cpuTime) t0 = time.time() x_gpu = gpuarray.to_gpu(x) y_gpu = cumath.sin(x_gpu) y = y_gpu.get() t1 = time.time() gpuTime = t1-t0 print(gpuTime) the results are: 200 msec for cpu and 2.45 sec for GPU... more then 10X I'm running on win 10... vs 2015 with PTVS... Best regards... Steph",
        "answers": [
            [
                "It looks like pycuda introduces some additional overhead the first time you call the cumath.sin() function (~400ms on my system). I suspect this is due to the need to compile CUDA code for the function being called. More importantly, this overhead is independent of the size of the array being passed to the function. Additional calls to cumath.sin() are much faster, with CUDA code already compiled for use. On my system, the gpu code given in the question runs in about 20ms (for repeated runs), compared to roughly 130ms for the numpy code. I don't profess to know much at all about the inner workings of pycuda, so would be interested to hear other people's opinions on this."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using PyCuda to pass pairs of arrays to a cuda kernel via a pointer. The arrays are the output of a different kernel, so the data is already on the GPU. Within the kernel, I'm trying to access elements in each of the arrays to do a vector subtraction. The values that I'm getting for the elements in the array are not correct (h &amp; p are wrong in the code below). Can anyone help me see what am I doing wrong? My code: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np import time import cv2 from pycuda.tools import DeviceMemoryPool as DMP from scipy.spatial import distance import os import glob def get_cuda_hist_kernel(): #Make the kernel histogram_kernel = \"\"\" __global__ void kernel_getHist(unsigned int* array,unsigned int size, unsigned int* histo, float bucket_size, unsigned int num_bins, unsigned int* out_max) { unsigned int x = threadIdx.x + blockDim.x * blockIdx.x; if(x&lt;size){ unsigned int value = array[x]; unsigned int bin = floor(float(value) * bucket_size) - 1; //Faster Modulo 3 for channel assignment unsigned int offset = x; offset = (offset &gt;&gt; 16) + (offset &amp; 0xFFFF); offset = (offset &gt;&gt; 8) + (offset &amp; 0xFF); offset = (offset &gt;&gt; 4) + (offset &amp; 0xF); offset = (offset &gt;&gt; 2) + (offset &amp; 0x3); offset = (offset &gt;&gt; 2) + (offset &amp; 0x3); offset = (offset &gt;&gt; 2) + (offset &amp; 0x3); if (offset &gt; 2) offset = offset - 3; offset = offset * num_bins; bin += offset; atomicAdd(&amp;histo[bin + offset],1); } } __global__ void kernel_chebyshev(unsigned int* histo, unsigned int* prev_histo, unsigned int number, int* output) { const unsigned int size = 12; //Get all of the differences __shared__ int temp_diffs[size]; unsigned int i = threadIdx.x + blockDim.x * blockIdx.x; if (i &lt; size){ unsigned int diff = 0; unsigned int h = histo[i]; unsigned int p = prev_histo[i]; if (h &gt; p) { diff = h - p; } else { diff = p - h; } temp_diffs[i] = (int)diff; } __syncthreads(); output[number] = 0; atomicMax(&amp;output[number], temp_diffs[i]); } \"\"\" mod = SourceModule(histogram_kernel) return mod def cuda_histogram(ims, block_size, kernel): start = time.time() max_val = 4 num_bins = np.uint32(4) num_channels = np.uint32(3) bin_size = np.float32(1 / np.uint32(max_val / num_bins)) #Memory Pool pool = DMP() print 'Pool Held Blocks: ', pool.held_blocks #Compute block &amp; Grid dimensions bdim = (block_size, 1, 1) cols = ims[0].size rows = 1 channels = 1 dx, mx = divmod(cols, bdim[0]) dy, my = divmod(rows, bdim[1]) dz, mz = divmod(channels, bdim[2]) g_x = (dx + (mx&gt;0)) * bdim[0] g_y = (dy + (my&gt;0)) * bdim[1] g_z = (dz + (mz&gt;0)) * bdim[2] gdim = (g_x, g_y, g_z) #get the function func = kernel.get_function('kernel_getHist') func2 = kernel.get_function('kernel_chebyshev') #build list of histograms #send the histogram to the gpu hists = [] device_hists = [] for im in range(len(ims)): hists.append(np.zeros([num_channels * num_bins]).astype(np.uint32)) end = time.time() dur = end - start print(' '.join(['Prep Time: ', str(dur)])) start = time.time() #Copy all of the image data to GPU device_images = [] for im in range(len(ims)): #print('Allocating data for image :', im) #convert the image to 1D array of uint32s a = ims[im].astype(np.uint32) a = a.flatten('C') a_size = np.uint32(a.size) #allocate &amp; send im data to gpu device_images.append(pool.allocate(a.nbytes)) cuda.memcpy_htod(device_images[im], a) d_hist = pool.allocate(hists[im].nbytes) device_hists.append(d_hist) cuda.memcpy_htod(d_hist, hists[im]) differences = np.zeros(len(ims)).astype(np.uint32) device_diffs = pool.allocate(differences.nbytes) cuda.memcpy_htod(device_diffs, differences) for im in range(len(ims)): #run histogram function func(device_images[im], a_size, device_hists[im], bin_size, num_bins, block=(block_size,1,1), grid=gdim) cuda.Context.synchronize() device_hist_size = np.uint32(len(device_hists[im])) for im in range(1, len(ims)): number = np.uint32(im - 1) func2(device_hists[im], device_hists[im - 1], number, device_diffs, block=(32,1,1)) cuda.memcpy_dtoh(differences, device_diffs) print(differences) for im in range(len(ims)): #get histogram back cuda.memcpy_dtoh(hists[im], device_hists[im]) device_hists[im] = 0 end = time.time() dur = end - start print(' '.join(['Load, Compute, &amp; Gather Time: ', str(dur)])) pool.free_held() return differences def get_all_files(directory): pattern = os.path.join(directory, '*.jpg') files = [f for f in glob.glob(pattern)] return files if __name__ == \"__main__\": RESOURCES_PATH = \"../data/ims/\" MAX_IMS = 1000 direc = os.path.join(RESOURCES_PATH, '21JumpStreet', 'source_video_frames') files = get_all_files(direc) a = cv2.imread('t.png') ims = [cv2.imread(f) for f in files] print 'Shape of my image: ', ims[0].shape print 'Number of images to histogram: ', len(ims) block_size = 128 kernel = get_cuda_hist_kernel() start = time.time() num_diffs = len(ims) // MAX_IMS + 1 cuda_diffs = [] for i in range(num_diffs): first = i * MAX_IMS last = (i + 1) * MAX_IMS print(first) small_set = ims[first:last] print 'Small set size: ', str(len(small_set)) cuda_diffs.extend(cuda_histogram(small_set, block_size, kernel)) end = time.time() dur = end - start print(' '.join(['CUDA version took:', str(dur)])) start = time.time() cv_hists = [] for i in range(len(ims)): im = ims[i % len(ims)] h = [] for j in range(3): hist = cv2.calcHist([im], [j], None, [4], [0, 100]) h.extend(hist) cv_hists.append(h) #run Chebyshev on CPU: color_hist_diffs = np.array([distance.chebyshev(cv_hists[i-1], cv_hists[i]) \\ for i in range(len(cv_hists)) if i != 0]) print(color_hist_diffs) end = time.time() dur = end - start print(' '.join(['CPU &amp; cv2 version took:', str(dur)]))",
        "answers": [
            [
                "This was a bad question, as the error was elsewhere in my code. Sorry for the confusion."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Why would the code from PyCuda KernelConcurrency Example not run faster in 'concurrent' mode? It seems like there should be enough resources on my GPU... what am I missing? Here is the output from the 'concurrent' version, with line 63 uncommented: === Device attributes Name: GeForce GTX 980 Compute capability: (5, 2) Concurrent Kernels: True === Checking answers Dataset 0 : passed. Dataset 1 : passed. === Timing info (for last set of kernel launches) Dataset 0 kernel_begin : 1.68524801731 kernel_end : 1.77305603027 Dataset 1 kernel_begin : 1.7144639492 kernel_end : 1.80246400833 Here is the version with line 63 commented out. This should be no longer running concurrently, and should be significantly slower. It looks nearly the same to me (about 0.08 - 0.09 in both cases): === Device attributes Name: GeForce GTX 980 Compute capability: (5, 2) Concurrent Kernels: True === Checking answers Dataset 0 : passed. Dataset 1 : passed. === Timing info (for last set of kernel launches) Dataset 0 kernel_begin : 1.20230400562 kernel_end : 1.28966403008 Dataset 1 kernel_begin : 1.21827197075 kernel_end : 1.30672001839 Is there something I'm missing here? Is there another way to test concurrency?",
        "answers": [
            [
                "The only way to truly see what is happening with concurrent kernel execution is to profile the code. With the inner kernel launch loop as posted on the wiki: # Run kernels many times, we will only keep data from last loop iteration. for j in range(10): for k in range(n): event[k]['kernel_begin'].record(stream[k]) my_kernel(d_data[k], block=(N,1,1), stream=stream[k]) for k in range(n): # Commenting out this line should break concurrency. event[k]['kernel_end'].record(stream[k]) the profile trace looks like this: With the inner kernel launch loop like this (i.e. the kernel end events not pushed onto the stream within their own loop: # Run kernels many times, we will only keep data from last loop iteration. for j in range(10): for k in range(n): event[k]['kernel_begin'].record(stream[k]) my_kernel(d_data[k], block=(N,1,1), stream=stream[k]) # for k in range(n): # Commenting out this line should break concurrency. event[k]['kernel_end'].record(stream[k]) I get this profile: i.e. the kernels in the two execution streams are still overlapping. So the reason why the execution time doesn't change between the two examples is because the comment you are relying on is erroneous. Both cases yield kernel execution overlap (\"concurrency\"). I have no interest in understanding why that is the case, but that is the source of your confusion. You will need to look elsewhere for the source of poor performance in your code (which apparently doesn't use streams anyway so this entire question was a straw man)."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to write a cuda histogram function for use with Pycuda. The code seems to be iterating through more elements than are in the size of the array I'm passing in. To rule out errors in the bin computation, I created a very simple kernel where I pass a 2d array and add 1 to the first bucket of the histogram for each element processed. I am consistently getting more elements than are in my 2d array. The output should be [size_of_2d_array, 0,0,0]. I'm running on Ubuntu 15.04, python 2.7.9. When I try examples written by other people, they seem to work properly. What am I doing wrong? import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np #Make the kernel histogram_kernel = \"\"\" __global__ void kernel_getHist(unsigned int* array,unsigned int size, unsigned int lda, unsigned int* histo, float buckets) { unsigned int y = threadIdx.y + blockDim.y * blockIdx.y; unsigned int x = threadIdx.x + blockDim.x * blockIdx.x; unsigned int tid = y + lda * x; if(tid&lt;size){ //unsigned int value = array[tid]; //int bin = floor(value * buckets); atomicAdd(&amp;histo[0],1); } } \"\"\" mod = SourceModule(histogram_kernel) #2d array to analyze a = np.ndarray(shape=(2,2)) a[0,0] = 1 a[0,1] =2 a[1,0] = 3 a[1,1] = 4 #histogram stuff, passing but not using right now max_val = 4 num_bins = np.uint32(4) bin_size = 1 / np.uint32(max_val / num_bins) #send array to the gpu a = a.astype(np.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) #send the histogram to the gpu a_hist = np.ndarray([1,num_bins]) a_hist = a_hist.astype(np.uint32) a_hist = a_hist * 0 d_hist = cuda.mem_alloc(a_hist.nbytes) cuda.memcpy_htod(d_hist, a_hist) #get the function func = mod.get_function('kernel_getHist') #get size &amp; lda a_size = np.uint32(a.size) a_lda = np.uint32(a.shape[0]) #print size &amp; lda to check print(a_lda) print(a_size) #run function func(a_gpu, a_size, a_lda, d_hist, bin_size, block=(16,16,1)) #get histogram back cuda.memcpy_dtoh(a_hist, d_hist) #print the histogram print a_hist print a This code outputs the following: 2 4 [[6 0 0 0]] [[ 1. 2.] [ 3. 4.]] But, it should output: 2 4 [[4 0 0 0]] [[ 1. 2.] [ 3. 4.]] The histogram has too many elements, which leads me to believe that I'm doing something wrong with tid and size. Any thoughts?",
        "answers": [
            [
                "The problem here is that you are not calculating unique values of tid within the kernel. If you do some simple arithmetic on a piece of paper you should get this for blockDim.x = blockDim.y = 16 and lda = 2: x y tid 0 0 0 1 0 2 0 1 1 1 1 3 0 2 2 0 3 3 .. .. .. Note the last two are duplicate indices. This is why your code returns 6, there are 6 threads which satisfy tid &lt; size for size=4. You have two choices to fix this. One option is to compute a unique index correctly, something like: unsigned int y = threadIdx.y + blockDim.y * blockIdx.y; unsigned int x = threadIdx.x + blockDim.x * blockIdx.x; unsigned int gda = blockDim.y * gridDim.y; unsigned int tid = y + gda * x; should work. Alternatively, applying bounds on each dimension of the input array: unsigned int y = threadIdx.y + blockDim.y * blockIdx.y; unsigned int x = threadIdx.x + blockDim.x * blockIdx.x; unsigned int tid = y + lda * x; if ( (x &lt; M) &amp;&amp; (y &lt; N) ) { atomicAdd(&amp;histo[0], 1); } would also probably work."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I guess this is a rather easy question for an expert, yet I can't find any answers in the net. Given a simple case: The problem: listToProcess = [] for i in range(0, 10): listToProcess.append(i) This list shall be transfered to the GPU, for further processing. I would then go on with a common cuda procedure for mem-copy: import sys import pycuda.autoinit import pycuda.driver as cuda listToProcess_gpu = cuda.mem_alloc(sys.getsizeof(listToProcess)) cuda.memcpy_htod(listToProcess_gpu, listToProcess) and afterwards call the kernel itself. However lists have no buffer interfaces, so that memcpy_htod() crashes. I tried different approaches too but in the end it leads to The questions How does one transfer a list with its content from a python program to the GPU kernel ? How does one specify the data type of the list (i.e. a list of floats, or ints, or ...) for the kernel?",
        "answers": [
            [
                "The only way to do this is to create an object which supports the buffer protocol from the list, and pass that new object to PyCUDA. In practice, that probably means creating a numpy or PyCUDA native GPUarray array from the list and using that instead: import sys import pycuda.autoinit import pycuda.driver as cuda import numpy as np listToProcess = [] for i in range(0, 10): listToProcess.append(i) l2p = np.array(listToProcess, dtype=np.int32) listToProcess_gpu = cuda.mem_alloc(l2p.nbytes) cuda.memcpy_htod(listToProcess_gpu, l2p) This would imply that your list was homogeneous with respect to type. A numpy array with dtype of object won't work. You could, of course, put on a hair shirt and roll your own object with buffer protocol support using ctypes, but that would be reinventing the wheel given what PyCUDA supports natively."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am on windows machine with python 2.7 ( 32 bit ) and pycuda with cuda 7.5 whl installed . I get error while running a sample program to test pycuda. Traceback (most recent call last): File \"C:\\Users\\newbie\\Desktop\\roo.py\", line 82, in &lt;module&gt; \"\"\") File \"C:\\Python27\\lib\\site-packages\\pycuda\\compiler.py\", line 265, in __init__ arch, code, cache_dir, include_dirs) File \"C:\\Python27\\lib\\site-packages\\pycuda\\compiler.py\", line 255, in compile return compile_plain(source, options, keep, nvcc, cache_dir, target) File \"C:\\Python27\\lib\\site-packages\\pycuda\\compiler.py\", line 137, in compile_plain stderr=stderr.decode(\"utf-8\", \"replace\")) pycuda.driver.CompileError: nvcc compilation of c:\\users\\newbie\\appdata\\local\\temp\\tmplluyeq\\kernel.cu failed [command: nvcc --cubin -arch sm_35 -m32 -Ic:\\python27\\lib\\site-packages\\pycuda\\cuda kernel.cu] [stdout: kernel.cu ] [stderr: 'C:\\Program' is not recognized as an internal or external command, What could be the possible solution . Please help !",
        "answers": [
            [
                "With limited knowledge my first guess would be that the white space after Program in a path containing C:\\Program Files\\anything is not getting handled properly. Edit: Including solutions independent of the link. You may need to escape the space in the file path. THough, I am not sure about windows slashes due to primarily using linux. Example C:/Program\\ Files/foo/bar.exe Another method may be to wrap quotes around the path. Either C:\\\"Program files\"\\foo\\bar.exe or \"C:\\Program files\\foo\\bar.exe\" A thrid option could be replacing the space with hex such as %20 like in C:\\Program%20files\\foo\\bar.exe. In c++ I think you can replace whitespace with \\u0020. So this replacement method could be an avenue as well. Suggested answers on this post may be helpful: https://superuser.com/questions/432980/how-to-call-a-program-that-contains-space-in-filenameenter link description here"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a large matrix - 1045506 x 3 which I want to sort based on the 1st column. Since, it's a numpy matrix, I can use argsort to get the result mat_sorted = mat[mat[:,0].argsort()] It takes about 69ms to complete this step which seems a little to high to me. My question is if there is a way to speed up the computation using the GPU? I was looking at some libraries like pycuda, theano but I'm finding it hard to understand the documentation. Specially, the part around passing numpy arrays and matrices to the gpu. Any pointers will be extremely helpful.",
        "answers": [
            [
                "There is no native sorting functionality in PyCUDA or scikit-cuda that I am aware of. However, efficient parallel sorting on the GPU is a mostly solved problem, and you can find state-of-the-art sorting algorithms in the Thrust template library, which ships with every version of the toolkit since about CUDA 4.0. The PyCUDA wiki includes a complete example of calling the Thrust sort algorithm from PyCUDA."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Comparing multiple matrix multiplication calculations with pyopencl and pycuda show differences in performance. System: Ubuntu 14.04 with GeForce 920m Pyopencl code: #-*- coding: utf-8 -*- import pyopencl as cl import pyopencl.array from jinja2 import Template import time import numpy as np from scipy.sparse import csr_matrix KERNEL = Template(\"\"\" {{header}} #include &lt;pyopencl-complex.h&gt; __kernel void complex_mat_mul(__global const {{complex_type}} *a, __global const {{complex_type}} *b, __global {{complex_type}} *res) { int row = get_local_id(1); int col = get_local_id(0); int mat_id = get_group_id(0) * get_num_groups(0) + get_group_id(1); //printf(\"mat_id: %d, row: %d, col: %d ----- \", mat_id, row, col); {{complex_type}} entry = 0; for (int e = 0; e &lt; {{mat_dim}}; ++e) { entry += a[mat_id*{{mat_dim}}*{{mat_dim}} + row * {{mat_dim}} + e] * b[mat_id*{{mat_dim}}*{{mat_dim}} + e * {{mat_dim}} + col]; } res[mat_id*{{mat_dim}}*{{mat_dim}} + row * {{mat_dim}} + col] = entry; } \"\"\") def get_ctx_queue(devices=[0]): \"\"\" optain context and queue for spcified devices \"\"\" platform = cl.get_platforms()[0] platform_devices = platform.get_devices() ctx = cl.Context(devices=[platform_devices[x] for x in devices]) return (ctx, cl.CommandQueue(ctx)) data_types = { 'cfloat_t': np.complex64, 'cdouble_t': np.complex128, 'float': np.float32, 'double': np.float64 } def render_kernel(complex_type, real_type, mat_dim): header = \"\" if data_types[complex_type] == np.complex128: header = \"\"\" #pragma OPENCL EXTENSION cl_khr_fp64 : enable #define PYOPENCL_DEFINE_CDOUBLE \"\"\" templ = KERNEL.render( header=header, complex_type=complex_type, real_type=real_type, mat_dim=mat_dim, ) print(templ) return templ complex_type = 'cdouble_t' real_type = 'float' mat_dim = 25 mats_count = 200 # x*x ctx, queue = get_ctx_queue() program= cl.Program(ctx, render_kernel(complex_type, real_type, mat_dim)).build() mats_1 = np.array(np.random.rand(mats_count**2, mat_dim, mat_dim), dtype=data_types[complex_type]) mats_2 = np.array(np.random.rand(mats_count**2, mat_dim, mat_dim), dtype=data_types[complex_type]) start = time.time() numpy_result = np.array([np.dot(mats_1[i], mats_2[i]) for i in range(mats_count**2)]) print(\"numpy time: %.3f\" % (time.time()-start)) a = cl.array.to_device(queue, mats_1) b = cl.array.to_device(queue, mats_2) c = cl.array.to_device(queue, np.zeros((mats_count**2, mat_dim, mat_dim), dtype=data_types[complex_type])) start = time.time() program.complex_mat_mul(queue, (mats_count*mat_dim, mats_count*mat_dim, 1), (mat_dim, mat_dim, 1), a.data,b.data,c.data) queue.finish() queue.flush() result = c.get() print(\"opencl time: %.3f\" % (time.time()-start)) assert np.allclose(numpy_result.flatten(), result.flatten(), atol=0), \"FAIL opencl\" print(\"Success\") Pycuda code: #-*- coding: utf-8 -*- import pycuda.autoinit import pycuda.driver as drv from pycuda.compiler import SourceModule from jinja2 import Template import time import numpy as np from scipy.sparse import csr_matrix KERNEL = Template(\"\"\" #include &lt;stdio.h&gt; #include &lt;pycuda-complex.hpp&gt; typedef pycuda::complex&lt;float&gt; scmplx; typedef pycuda::complex&lt;double&gt; dcmplx; __global__ void complex_mat_mul(const {{complex_type}} *a, const {{complex_type}} *b, {{complex_type}} *res) { int row = threadIdx.y; int col = threadIdx.x; int mat_id = blockIdx.x * gridDim.x + blockIdx.y; //printf(\"mat_id: %d, row: %d, col: %d ----- \", mat_id, row, col); {{complex_type}} entry = 0; for (int e = 0; e &lt; {{mat_dim}}; ++e) { entry += a[mat_id*{{mat_dim}}*{{mat_dim}} + row * {{mat_dim}} + e] * b[mat_id*{{mat_dim}}*{{mat_dim}} + e * {{mat_dim}} + col]; } res[mat_id*{{mat_dim}}*{{mat_dim}} + row * {{mat_dim}} + col] = entry; } \"\"\") data_types = { 'scmplx': np.complex64, 'dcmplx': np.complex128, 'float': np.float32, 'double': np.float64 } def render_kernel(complex_type, real_type, mat_dim, block, gird): templ = KERNEL.render( complex_type=complex_type, real_type=real_type, mat_dim=mat_dim, blockDim_x=block[0], blockDim_y=block[1] ) print(templ) return templ complex_type = 'dcmplx' real_type = 'double' mat_dim = 25 mats_count = 200 # x*x block = (mat_dim,mat_dim,1) grid = (mats_count,mats_count) program = SourceModule(render_kernel(complex_type, real_type, mat_dim, block, grid)) complex_mat_mul = program.get_function(\"complex_mat_mul\") mats_1 = np.array(np.random.rand(mats_count**2, mat_dim, mat_dim), dtype=data_types[complex_type]) mats_2 = np.array(np.random.rand(mats_count**2, mat_dim, mat_dim), dtype=data_types[complex_type]) result = np.zeros((mats_count**2, mat_dim, mat_dim), dtype=data_types[complex_type]) start = time.time() numpy_result = np.array([np.dot(mats_1[i], mats_2[i]) for i in range(mats_count**2)]) print(\"numpy time: %.3f\" % (time.time()-start)) a = drv.In(mats_1) b = drv.In(mats_2) c = drv.Out(result) start = time.time() complex_mat_mul(a, b, c, block=block, grid=grid ) print(\"cuda time: %.3f\" % (time.time()-start)) assert np.array_equal(numpy_result.flatten(), result.flatten()), \"FAIL\" print(\"Success\") In single and double precision pyopencl performs at least 2 times faster. Changing the number of matrices doesn't change the result. Both kernels get called the same number of times and I suppose that the spots of the benchmark are appropriate. What am I missing?",
        "answers": [
            [
                "Running more tests gave the following results. Using the Kernel (opencl is equivalent except for the determination of row, col and mat_id): int row = threadIdx.y; int col = threadIdx.x; int mat_id = blockIdx.x * gridDim.x + blockIdx.y; for (int i=0; i &lt; 10; i++) { {{complex_type}} entry = 0; for (int e = 0; e &lt; {{mat_dim}}; ++e) { entry += a[mat_id*{{mat_dim}}*{{mat_dim}} + row * {{mat_dim}} + e] * b[mat_id*{{mat_dim}}*{{mat_dim}} + e * {{mat_dim}} + col]; } res[mat_id*{{mat_dim}}*{{mat_dim}} + row * {{mat_dim}} + col] = entry; } Added the extra loop to execute more floating point operations Measuring the performance includes writing and reading the buffers on the GPU (compared to the actual execution time, those transferoperations are very fast) For different matrix dimensions (constant number of matrices: 22500): For a different number of matrices (constant matrix dimension: 25): As in the comments discussed I shortly had better result with pycuda, when I included the initial buffer operations. But by adding the extra loop to increase floating point operations, the pycuda head start vanished. Thus leaving this question open, since we would expect pycuda to perform better."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm trying to process a 2D array using PyCUDA and I need the x,y coordinates of each thread. This question has been asked and answered here and here, but the linked solutions do not work for me for 2D data that exceeds my block size. Why? Here's the SourceModule I'm using to help figure this out: mod = SourceModule(\"\"\" __global__ void kIndexTest(float *M, float *X, float*Y) { int bIdx = blockIdx.x + blockIdx.y * gridDim.x; int idx = bIdx * (blockDim.x * blockDim.y) + (threadIdx.y * blockDim.x) + threadIdx.x; /* this array shows me the unique thread indices */ M[idx] = idx; /* these arrays should capture x, y for each unique index */ X[idx] = (blockDim.x * blockIdx.x) + threadIdx.x; Y[idx] = (blockDim.y * blockIdx.y) + threadIdx.y; } \"\"\") I'm executing the kernel like this: gIndexTest = mod.get_function(\"kIndexTest\") dims = (8, 8) M = gpuarray.to_gpu(numpy.zeros(dims, dtype=numpy.float32)) X = gpuarray.to_gpu(numpy.zeros(dims, dtype=numpy.float32)) Y = gpuarray.to_gpu(numpy.zeros(dims, dtype=numpy.float32)) gIndexTest(M, X, Y, block=(4, 4, 1), grid=(2, 2, 1)) M returns the correct index for all dimensions and all block/grid configurations I've tested. X and Y only return the correct coordinate values when the dimensions of X and Y are the same as the block dimensions, but do not return what I expect otherwise. For example, the above configuration yields: M: [[ 0. 1. 2. 3. 4. 5. 6. 7.] [ 8. 9. 10. 11. 12. 13. 14. 15.] [ 16. 17. 18. 19. 20. 21. 22. 23.] [ 24. 25. 26. 27. 28. 29. 30. 31.] [ 32. 33. 34. 35. 36. 37. 38. 39.] [ 40. 41. 42. 43. 44. 45. 46. 47.] [ 48. 49. 50. 51. 52. 53. 54. 55.] [ 56. 57. 58. 59. 60. 61. 62. 63.]] (correct) X: [[ 0. 1. 2. 3. 0. 1. 2. 3.] [ 0. 1. 2. 3. 0. 1. 2. 3.] [ 4. 5. 6. 7. 4. 5. 6. 7.] [ 4. 5. 6. 7. 4. 5. 6. 7.] [ 0. 1. 2. 3. 0. 1. 2. 3.] [ 0. 1. 2. 3. 0. 1. 2. 3.] [ 4. 5. 6. 7. 4. 5. 6. 7.] [ 4. 5. 6. 7. 4. 5. 6. 7.]] (not what I expect) Y: [[ 0. 0. 0. 0. 1. 1. 1. 1.] [ 2. 2. 2. 2. 3. 3. 3. 3.] [ 0. 0. 0. 0. 1. 1. 1. 1.] [ 2. 2. 2. 2. 3. 3. 3. 3.] [ 4. 4. 4. 4. 5. 5. 5. 5.] [ 6. 6. 6. 6. 7. 7. 7. 7.] [ 4. 4. 4. 4. 5. 5. 5. 5.] [ 6. 6. 6. 6. 7. 7. 7. 7.]] (not what I expect) Here's what I actually expect from X and Y: X: [[ 0. 1. 2. 3. 4. 5. 6. 7.] [ 0. 1. 2. 3. 4. 5. 6. 7.] [ 0. 1. 2. 3. 4. 5. 6. 7.] [ 0. 1. 2. 3. 4. 5. 6. 7.] [ 0. 1. 2. 3. 4. 5. 6. 7.] [ 0. 1. 2. 3. 4. 5. 6. 7.] [ 0. 1. 2. 3. 4. 5. 6. 7.] [ 0. 1. 2. 3. 4. 5. 6. 7.]] (only works when X dims = block dims) Y: [[ 0. 0. 0. 0. 0. 0. 0. 0.] [ 1. 1. 1. 1. 1. 1. 1. 1.] [ 2. 2. 2. 2. 2. 2. 2. 2.] [ 3. 3. 3. 3. 3. 3. 3. 3.] [ 4. 4. 4. 4. 4. 4. 4. 4.] [ 5. 5. 5. 5. 5. 5. 5. 5.] [ 6. 6. 6. 6. 6. 6. 6. 6.] [ 7. 7. 7. 7. 7. 7. 7. 7.]] (only works when Y dims = block dims) What do I not understand? Here's my deviceQuery: Device 0: \"GeForce GT 755M\" CUDA Driver Version / Runtime Version 7.5 / 6.5 CUDA Capability Major/Minor version number: 3.0 Total amount of global memory: 1024 MBytes (1073283072 bytes) ( 2) Multiprocessors, (192) CUDA Cores/MP: 384 CUDA Cores GPU Clock rate: 1085 MHz (1.09 GHz) Memory Clock rate: 2500 Mhz Memory Bus Width: 128-bit L2 Cache Size: 262144 bytes Maximum Texture Dimension Size (x,y,z) 1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096) Maximum Layered 1D Texture Size, (num) layers 1D=(16384), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(16384, 16384), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 1 copy engine(s) Run time limit on kernels: Yes Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device PCI Bus ID / PCI location ID: 1 / 0",
        "answers": [
            [
                "Everything is working \"as advertised\". The problem here is that you are mixing incompatible indexing schemes together, which is producing inconsistent results. If you wanted X and Y to appear as you expected, you would need to compute idx in a different way: __global__ void kIndexTest(float *M, float *X, float*Y) { int xidx = (blockDim.x * blockIdx.x) + threadIdx.x; int yidx = (blockDim.y * blockIdx.y) + threadIdx.y; int idx = (gridDim.x * blockDim.x * yidx) + xidx; X[idx] = xidx; Y[idx] = yidx; M[idx] = idx; } in this scheme, xidxand yidx are the grid x and y coordinates, and idx is the global index, all assuming column major ordering (i.e. x is the fastest varying dimension)."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I'm copying via pycuda some arrays on the GPU and then store the pointers to these arrays. How do I recuperate the data back? dist = np.zeros(numPoints).astype(np.float32) distAddress = [gpuarray.to_gpu(dist).ptr for i in range(100)] If I call the memcpy_dtoh function: buf = np.zeros(400).astype(np.float32) cuda.memcpy_dtoh(buf,distAddress[0]), (where type(distAddress[0]) is long) I get the following error: cuda.memcpy_dtoh(buf, distAddress[0]) LogicError: cuMemcpyDtoH failed: invalid argument What am I doing wrong? Thanks!",
        "answers": [
            [
                "I think if you are using GPUArrays the way to copy from device to host is with the .get() method. For example dist = np.zeros(num_points).astype(np.float32) dist_list = [gpuarray.to_gpu(dist) for i in range(100)] buf = dist_list[0].get()"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a pycuda gpuarray that I would like to feed to an opencv cuda function. As I understand there are currently no python bindings for the opencv 3 cv::cuda module. So I tried writing my own python wrapper for accessing cv::cuda functions (in my case cv::cuda::HOG class). Unfortunately I hit a road block when it comes to the central part of data exchange. I know that you can use a structure like static PyObject* my_function(PyObject* self, PyObject* args) { int test; PyArg_ParseTuple(args, \"t\", &amp;test); } To pass elementary data types, but I do not know how to do that with pointers on gpu memory (in my case gpuarray.gpudata). My understanding is that gpuarray.gpudata yields a pointer according to the dtype of the gpuarray but on the gpu. So on the python side things would look like this: input_gpu = gpuarray.zeros(shape=(3,3), dtype=numpy.float32) output_gpu = gpuarray.zeros(shape=(3,3), dtype=numpy.float32) my_function(input.gpudata, output.gpudata) On C++ side I would have to use cv:cuda::GpuMat, so how to I get from gpuarray.gpudata to cv:cuda::GpuMat and vice versa?",
        "answers": [],
        "votes": []
    },
    {
        "question": "I'm trying to install pycuda-2015.1.3 on my macbook pro. I've successfully installed CUDA, and I'm working with python 3.4. After entering terminal and going into the pycuda folder, I hit \"sudo make\" and get the following output: ctags -R src || true /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ctags: illegal option -- R usage: ctags [-BFadtuwvx] [-f tagsfile] file ... /usr/local/bin/python3.4 setup.py build --------------------------------------------------------------------------- Sorry, your build failed. Try rerunning configure.py with different options. --------------------------------------------------------------------------- Traceback (most recent call last): File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 2421, in _dep_map return self.__dep_map File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 2283, in __getattr__ raise AttributeError(attr) AttributeError: _DistInfoDistribution__dep_map During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 2412, in _parsed_pkg_info return self._pkg_info File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 2283, in __getattr__ raise AttributeError(attr) AttributeError: _pkg_info During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"setup.py\", line 229, in &lt;module&gt; main() File \"setup.py\", line 225, in main cmdclass={'build_py': build_py}) File \"/Users/shirgur/Downloads/pycuda-2015.1.3/aksetup_helper.py\", line 23, in setup setup(*args, **kwargs) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/distutils/core.py\", line 108, in setup _setup_distribution = dist = klass(attrs) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/setuptools/dist.py\", line 239, in __init__ self.fetch_build_eggs(attrs.pop('setup_requires')) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/setuptools/dist.py\", line 263, in fetch_build_eggs parse_requirements(requires), installer=self.fetch_build_egg File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 577, in resolve requirements.extend(dist.requires(req.extras)[::-1]) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 2232, in requires dm = self._dep_map File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 2423, in _dep_map self.__dep_map = self._compute_dependencies() File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 2445, in _compute_dependencies for req in self._parsed_pkg_info.get_all('Requires-Dist') or []: File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 2415, in _parsed_pkg_info self._pkg_info = Parser().parsestr(self.get_metadata(self.PKG_INFO)) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 1310, in get_metadata return self._get(self._fn(self.egg_info,name)).decode(\"utf-8\") File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pkg_resources.py\", line 1414, in _get stream = open(path, 'rb') FileNotFoundError: [Errno 2] No such file or directory: '/Users/shirgur/Library/Python/3.4/lib/python/site-packages/numpy-1.9.1.dist-info/METADATA' make: *** [all] Error 1",
        "answers": [
            [
                "I had two versions of numpy. After \"pip uninstall numpy\" and deleting al of the 1.9 files, i've installed bumpy from scratch. That's it!"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "windows 10, python 2.7 64 bit hello, following a guide to this step : pip install pipwin pipwin install pycuda gives me those options Package `pycuda` found in cache Choose version to download. [0] : 2014.1+cuda6514 [1] : 2015.1.3+cuda7518 no matter what i choose , i get the following error(last line): File \"c:\\users\\skpok\\anaconda2\\lib\\zipfile.py\", line 811, in _RealGetContents raise BadZipfile, \"File is not a zip file\" BadZipfile: File is not a zip file Anyone knows this mistake?",
        "answers": [
            [
                "try: pip install --no-cache-dir &lt;package_name&gt;, it will work when you try to pip install , first pip will check the pip cache for the package. if the package is found and fresh, pip will grap the .whl file for the package and try to install. this results in badzipfile as a .zip file is excepted. try pip install in verbose mode pip install &lt;some_package&gt; -vvvv. you can see that it will first try to install from cache"
            ],
            [
                "Updated: The download link below is expired. You can find the latest version of pycuda in http://www.lfd.uci.edu/~gohlke/pythonlibs/#pycuda ------------- I got the same error. Maybe the package is damaged. Downloading package . . . http://www.lfd.uci.edu/~gohlke/pythonlibs/WjTMc73K/pycuda-2015.1.3+cuda7518-cp27-none-win32.whl &lt;--- **damaged package?** You can download this package: http://www.lfd.uci.edu/~gohlke/pythonlibs/wkvprmqy/pycuda-2015.1.3+cuda7518-cp27-none-win32.whl and then: pip install \"pycuda-2015.1.3+cuda7518-cp27-none-win32.whl\" It would be OK."
            ],
            [
                "If it's already installed but older version try to upgrade like so: #upgrade pip install -U pycuda If didn't work try to uninstall and then install it again, like so: #uninstall pip uninstall pycuda #install pip install pycuda If didn't work then, Try to install Windows SDK and make sure your CUDA Toolkit is the latest, then install PyCuda again. see if that solve your problem."
            ]
        ],
        "votes": [
            30.0000001,
            4.0000001,
            1.0000001
        ]
    },
    {
        "question": "When I try to profile my pyCUDA application using nvvp, it works for the most part. I can click on \"Examine GPU Usage\" and view a number of analysis results / suggestions for my code, such as \"Low Compute / Memcpy Efficiency.\" However, everytime that nvvp runs the program to perform an analysis, I see the following warning. Some collected events, metrics or source-level results could not be associated with the session timeline. This may prevent event, metric and source-level results from being assigned to some kernels. It looks like I might be able to get more detailed analysis if I do something to fix this. Does anyone know how to associate \"collected events, metrics or source-level results with the session timeline\"?",
        "answers": [
            [
                "As seems in profiler documentation: \u25e6The Visual Profiler cannot correctly import profiler data generated by nvprof when the option --kernels kernel-filter is used. Visual Profiler reports a warning, \"Some collected events or source-level results could not be associated with the session timeline.\" One workaround is to use the nvprof option --kernels :::1 to profile the first invocation for all kernels. so you can try change this option"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "When I use any command with sudo the environment variables are not there. For example after setting HTTP_PROXY the command wget works fine without sudo. However if I type sudo wget it says it can't bypass the proxy setting.",
        "answers": [
            [
                "First you need to export HTTP_PROXY. Second, you need to read man sudo, and look at the -E flag. This works: $ export HTTP_PROXY=foof $ sudo -E bash -c 'echo $HTTP_PROXY' Here is the quote from the man page: -E, --preserve-env Indicates to the security policy that the user wishes to preserve their existing environment variables. The security policy may return an error if the user does not have permission to preserve the environment."
            ],
            [
                "The trick is to add environment variables to sudoers file via sudo visudo command and add these lines: Defaults env_keep += \"ftp_proxy http_proxy https_proxy no_proxy\" taken from ArchLinux wiki. For Ubuntu 14, you need to specify in separate lines as it returns the errors for multi-variable lines: Defaults env_keep += \"http_proxy\" Defaults env_keep += \"https_proxy\" Defaults env_keep += \"HTTP_PROXY\" Defaults env_keep += \"HTTPS_PROXY\""
            ],
            [
                "For individual variables you want to make available on a one off basis you can make it part of the command. sudo http_proxy=$http_proxy wget \"http://stackoverflow.com\""
            ],
            [
                "You can also combine the two env_keep statements in Ahmed Aswani's answer into a single statement like this: Defaults env_keep += \"http_proxy https_proxy\" You should also consider specifying env_keep for only a single command like this: Defaults!/bin/[your_command] env_keep += \"http_proxy https_proxy\""
            ],
            [
                "A simple wrapper function (or in-line for loop) I came up with a unique solution because: sudo -E \"$@\" was leaking variables that was causing problems for my command sudo VAR1=\"$VAR1\" ... VAR42=\"$VAR42\" \"$@\" was long and ugly in my case demo.sh #!/bin/bash function sudo_exports(){ eval sudo $(for x in $_EXPORTS; do printf '%q=%q ' \"$x\" \"${!x}\"; done;) \"$@\" } # create a test script to call as sudo echo 'echo Forty-Two is $VAR42' &gt; sudo_test.sh chmod +x sudo_test.sh export VAR42=\"The Answer to the Ultimate Question of Life, The Universe, and Everything.\" export _EXPORTS=\"_EXPORTS VAR1 VAR2 VAR3 VAR4 VAR5 VAR6 VAR7 VAR8 VAR9 VAR10 VAR11 VAR12 VAR13 VAR14 VAR15 VAR16 VAR17 VAR18 VAR19 VAR20 VAR21 VAR22 VAR23 VAR24 VAR25 VAR26 VAR27 VAR28 VAR29 VAR30 VAR31 VAR32 VAR33 VAR34 VAR35 VAR36 VAR37 VAR38 VAR39 VAR40 VAR41 VAR42\" # clean function style sudo_exports ./sudo_test.sh # or just use the content of the function eval sudo $(for x in $_EXPORTS; do printf '%q=%q ' \"$x\" \"${!x}\"; done;) ./sudo_test.sh Result $ ./demo.sh Forty-Two is The Answer to the Ultimate Question of Life, The Universe, and Everything. Forty-Two is The Answer to the Ultimate Question of Life, The Universe, and Everything. How? This is made possible by a feature of the bash builtin printf. The %q produces a shell quoted string. Unlike the parameter expansion in bash 4.4, this works in bash versions &lt; 4.0"
            ],
            [
                "Add code snippets to /etc/sudoers.d Don't know if this is available in all distros, but in Debian-based distros, there is a line at or near the tail of the /etc/sudoers file that includes the folder /etc/sudoers.d. Herein, one may add code \"snippets\" that modify sudo's configuration. Specifically, they allow control over all environment variables used in sudo. As with /etc/sudoers, these \"code snippets\" should be edited using visudo. You can start by reading the README file, which is also a handy place for keeping any notes you care to make: $ sudo visudo -f /etc/sudoers.d/README # files for your snippets may be created/edited like so: $ sudo visudo -f /etc/sudoers.d/20_mysnippets Read the \"Command Environment\" section of 'man 5 sudoers' Perhaps the most informative documentation on environment configuration in sudo is found in the Command environment section of man 5 sudoers. Here, we learn that a sudoers environment variables that are blocked by default may be \"whitelisted\" using the env_check or env_keep options; e.g. Defaults env_keep += \"http_proxy HTTP_PROXY\" Defaults env_keep += \"https_proxy HTTPS_PROXY\" Defaults env_keep += \"ftp_proxy FTP_PROXY\" And so, in the OP's case, we may \"pass\" the sudoer's environment variables as follows: $ sudo visudo -f /etc/sudoers.d/10_myenvwlist # opens the default editor for entry of the following lines: Defaults env_keep += \"http_proxy HTTP_PROXY\" Defaults env_keep += \"https_proxy HTTPS_PROXY\" # and any others deemed useful/necessary # Save the file, close the editor, and you are done! Get your bearings from '# sudo -V' The OP presumably discovered the missing environment variable in sudo by trial-and-error. However, it is possible to be proactive: A listing of all environment variables, and their allowed or denied status is available (and unique to each host) from the root prompt as follows: # sudo -V ... Environment variables to check for safety: ... Environment variables to remove: ... Environment variables to preserve: ... Note that once an environment variable is \"whitelisted\" as above, it will appear in subsequent listings of sudo -V under the \"preserve\" listing."
            ],
            [
                "If you have the need to keep the environment variables in a script you can put your command in a here document like this. Especially if you have lots of variables to set things look tidy this way. # prepare a script e.g. for running maven runmaven=/tmp/runmaven$$ # create the script with a here document cat &lt;&lt; EOF &gt; $runmaven #!/bin/bash # run the maven clean with environment variables set export ANT_HOME=/usr/share/ant export MAKEFLAGS=-j4 mvn clean install EOF # make the script executable chmod +x $runmaven # run it sudo $runmaven # remove it or comment out to keep rm $runmaven"
            ]
        ],
        "votes": [
            636.0000001,
            375.0000001,
            89.0000001,
            27.0000001,
            12.0000001,
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "i am trying install pycuda with cuda 7.0 no problem and run code nice but i can't install and run pycuda after install pycuda with Installing PyCUDA on Ubuntu Linux try run simple code with pycuda simple code is import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy mod = SourceModule(\"\"\" ___global___ void doublify(float *a) { int idx= threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") a = numpy.random.randn(4,4) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu,a) func = mod.get_function(\"doublify\") func(a_gpu, block=(4,4,1)) a_doubled = numpy.empty_like(a) cuda.memcpy_dtoh(a_doubled, a_gpu) print a_doubled print a but code not run and show this error Traceback (most recent call last): File \"/home/pupuol/Documents/educuda.py\", line 3, in &lt;module&gt; import pycuda . autoinit File \"/usr/local/lib/python2.7/dist-packages/pycuda-2015.1.3-py2.7- linux-x86_64.egg/pycuda/autoinit.py\", line 2, in &lt;module&gt; import pycuda.driver as cuda File \"/usr/local/lib/python2.7/dist-packages/pycuda-2015.1.3-py2.7- linux-x86_64.egg/pycuda/driver.py\", line 5, in &lt;module&gt; from pycuda._driver import * # noqa ImportError: libcurand.so.7.0: cannot open shared object file: No such file or directory please help me",
        "answers": [
            [
                "It appears to be a library path issue. Try this before you run python. export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64 # or somewhere else your cuda library is located"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have 2 simple matrices A and B and I'm calculating their multiplication. The arrays looks like this (using numpy as as mockup) A=np.array(([1,2,3],[4,5,6])).astype(np.float64) B=np.array(([7,8],[9,10],[11,12])).astype(np.float64) Here are the shapes of the Matrix A: (2, 3) B: (3, 2) Now, I am trying to do this using cublasDgemmBatched to get the product. I am confused on what my m,n,and k values should be when applying cublasDgemmBatched. Also, I'm not sure what my leading dimension (lda, ldb, ldc) of the array would be. There is a nice 3d example here but I can't seem to get this function to work on 2d matrices. Ideally, i would like to get the same results as np.dot.",
        "answers": [
            [
                "I don't have skcuda.blas to confirm this. But a more complete example might look like A = np.array(([1, 2, 3], [4, 5, 6])).astype(np.float64) B = np.array(([7, 8], [9, 10], [11, 12])).astype(np.float64) m, k = A.shape k, n = B.shape a_gpu = gpuarray.to_gpu(A) b_gpu = gpuarray.to_gpu(B) c_gpu = gpuarray.empty((m, n), np.float64) alpha = np.float64(1.0) beta = np.float64(0.0) a_arr = bptrs(a_gpu) b_arr = bptrs(b_gpu) c_arr = bptrs(c_gpu) cublas_handle = cublas.cublasCreate() cublas.cublasDgemm(cublas_handle, 'n','n', n, m, k, alpha, b_arr.gpudata, m, a_arr.gpudata, k, beta, c_arr.gpudata, m)"
            ],
            [
                "A very simple way to imitate np.dot() is using culinalg.dot() which uses cuBLAS behind, see skcuda.linalg.dot. Below, a simple example: import pycuda.autoinit import pycuda.gpuarray as gpuarray import pycuda.driver as drv import numpy as np import skcuda.linalg as culinalg import skcuda.misc as cumisc culinalg.init() A = np.array(([1, 2, 3], [4, 5, 6])).astype(np.float64) B = np.array(([7, 8, 1, 5], [9, 10, 0, 9], [11, 12, 5, 5])).astype(np.float64) A_gpu = gpuarray.to_gpu(A) B_gpu = gpuarray.to_gpu(B) C_gpu = culinalg.dot(A_gpu, B_gpu) print(np.dot(A, B)) print(C_gpu)"
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am trying to use scikit-cuda's wrappers for the cuSOLVER functions, in particular I want to execute cusolverDnSgesvd to compute full-matrix single precision SVD on a matrix of real numbers. Using the code here and here as a reference, I managed to get this far: import pycuda.autoinit import pycuda.driver as drv import pycuda.gpuarray as gpuarray import numpy as np from skcuda import cusolver handle = cusolver.cusolverDnCreate() m = 50 n = 25 a = np.asarray(np.random.random((m, n))) a_gpu = gpuarray.to_gpu(a) ldu = m ldvt = n s_gpu = gpuarray.empty(min(m, n), np.float32) u_gpu = gpuarray.empty((ldu, m), np.float32) vh_gpu = gpuarray.empty((n, n), np.float32) work_size = cusolver.cusolverDnSgesvd_bufferSize(handle, m, n) work = gpuarray.empty((m,n), np.float32) u_gpu, s_gpu, vh_gpu = cusolver.cusolverDnSgesvd( handle=handle, jobu='A', jobvt='A', m=m, n=n, A=a, lda=m, S=s_gpu, U=u_gpu, ldu=ldu, VT=vh_gpu, ldvt=ldvt, Work=work, Lwork=work_size, rwork=None, devInfo=0 ) But the code isn't working, probably because I'm messing up with types. Traceback (most recent call last): File \"/home/vektor/PycharmProjects/yancut/test_svd.py\", line 44, in &lt;module&gt; devInfo=0 File \"/home/vektor/anaconda3/lib/python3.4/site-packages/skcuda/cusolver.py\", line 577, in cusolverDnSgesvd int(A), lda, int(S), int(U), TypeError: only length-1 arrays can be converted to Python scalars How should I provide all the arguments so that the SVD is executed in a proper way? UPDATE1: After using this question as reference, I edited my code and I'm getting a new error. import pycuda.autoinit import pycuda.driver as drv import pycuda.gpuarray as gpuarray import numpy as np import ctypes from skcuda import cusolver rows = 20 cols = 10 a = np.asarray(np.random.random((rows, cols))) a_gpu = gpuarray.to_gpu(a.copy()) lda = rows u_gpu = gpuarray.empty((rows, rows), np.float32) v_gpu = gpuarray.empty((cols, cols), np.float32) s_gpu = gpuarray.empty(cols, np.float32) devInfo = gpuarray.zeros(1, np.int32) handle = cusolver.cusolverDnCreate() worksize = cusolver.cusolverDnSgesvd_bufferSize(handle, rows, cols) print(\"SIZE\", worksize) Workspace = gpuarray.empty(worksize, np.float32) svd_status = cusolver.cusolverDnSgesvd( handle=handle, jobu='A', jobvt='A', m=rows, n=cols, A=a_gpu.ptr, lda=rows, S=s_gpu.ptr, U=u_gpu.ptr, ldu=rows, VT=v_gpu.ptr, ldvt=cols, Work=Workspace.ptr, Lwork=worksize, rwork=Workspace.ptr, devInfo=devInfo.ptr ) status = cusolver.cusolverDnDestroy(handle) And I'm getting a new error Traceback (most recent call last): File \"/home/vektor/PycharmProjects/yancut/test_svd.py\", line 53, in &lt;module&gt; devInfo=devInfo.ptr File \"/home/vektor/anaconda3/lib/python3.4/site-packages/skcuda/cusolver.py\", line 579, in cusolverDnSgesvd Lwork, int(rwork), int(devInfo)) ctypes.ArgumentError: argument 2: &lt;class 'TypeError'&gt;: wrong type It now seems that I'm doing something wrong with devInfo",
        "answers": [
            [
                "From the documentation it looks like each of the matrices (so A, S, U, VT) need to be passed as device pointers. So for PyCUDA gpuarrays, pass A.ptr rather than A. etc and it should work."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm using Anaconda to install Theano on MacOSX (Mavericks 10.9 ), just like this post explains: \"How to make Theano operate on Mac Lion?\" theano.test() This command gives the same error as in the post above. It gives that error on an Ubuntu 14.1, System 76 as well. I am able to import commands from Theano; but I still would like to understand why theano.test() fails. The packages CUDA and Boost were already installed before running... (Reference: See section: \"Testing your Installation\" http://deeplearning.net/software/theano/install.html) As the post suggests, I assumed the fix would come from installing the XCode command line, homebrew, and pycuda. The first two were installed just fine. But pycuda fails: pip install pycuda ....gives the following error: &gt; src/cpp/cuda.cpp -o build/temp.macosx-10.5-x86_64-3.4/src/cpp/cuda.o &gt; In file included from src/cpp/cuda.cpp:1: &gt; &gt; src/cpp/cuda.hpp:14:10: fatal error: 'cuda.h' file not found &gt; #include &lt;cuda.h&gt; &gt; ^ &gt; 1 error generated. &gt; error: command 'gcc' failed with exit status 1 &gt; &gt; Command \"//anaconda/bin/python3 -c \"import setuptools, &gt;tokenize;__file__='/private/var/folders/5b/5g1stsns34x_7mgynxhhvf1h0000gn/T/pip-build-4raihcb4/pycuda/setup.py';exec(compile(getattr(tokenize, &gt; 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, &gt; 'exec'))\" install --record &gt; /var/folders/5b/5g1stsns34x_7mgynxhhvf1h0000gn/T/pip-kr_3ws22-record/install-record.txt &gt;&gt; --single-version-externally-managed --compile\" failed with error code 1 in &gt; /private/var/folders/5b/5g1stsns34x_7mgynxhhvf1h0000gn/T/pip-build-4raihcb4/pycuda It seems like the first error (gcc failed) is that the complier is not finding gcc. (Note again: I installed the MacOSX command line tools) I run which gcc this gives usr/bin/gcc I also tried : python configure.py --cuda-root=/usr/local/cuda --cuda-inc-dir=/Developer/NVIDIA/CUDA-5.5/include --cudart-lib-dir=/Developer/NVIDIA/CUDA-5.5/lib That didn't work as well. Has anyone else had this difficulty installing pycuda and can make a recommendation here? Thanks.",
        "answers": [
            [
                "The first error is not indicating that gcc failed to be found, rather it is saying that gcc reported an error and that error was that the file cuda.h could not be found. That error suggests you don't have the CUDA toolkit installed. You need to install this before proceeding to install PyCUDA. The CUDA toolkit can be downloaded from NVIDIA's website. The PyCUDA web page indicates that it has the following prerequisites: BOOST, CUDA, and Numpy."
            ],
            [
                "I resolved this same issue with a combination of two things: Making sure nvcc is in your PATH seems resolve the compilation errors. If you then encounter the following linking error: ld: file not found: @rpath/CUDA.framework/Versions/A/CUDA for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation) I fixed it changing the @rpath reference in libcuda.dylib to an absolute path: install_name_tool -change @rpath/CUDA.framework/Versions/A/CUDA \\ /Library/Frameworks/CUDA.framework/CUDA \\ /usr/local/cuda/lib/libcuda.dylib"
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am trying to parallelize my NN across two GPUs following https://github.com/uoguelph-mlrg/theano_multi_gpu. I have all the dependencies, but the cuda runtime initialization fails with the following message. ERROR (theano.sandbox.cuda): ERROR: Not using GPU. Initialisation of device 0 failed: cublasCreate() returned this error 'the CUDA Runtime initialization failed' Error when trying to find the memory information on the GPU: invalid device ordinal Error allocating 24 bytes of device memory (invalid device ordinal). Driver report 0 bytes free and 0 bytes total ERROR (theano.sandbox.cuda): ERROR: Not using GPU. Initialisation of device gpu failed: CudaNdarray_ZEROS: allocation failed. Process Process-1: Traceback (most recent call last): File \"/opt/share/Python-2.7.9/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap self.run() File \"/opt/share/Python-2.7.9/lib/python2.7/multiprocessing/process.py\", line 114, in run self._target(*self._args, **self._kwargs) File \"/u/bsankara/nt/Git-nt/nt/train_attention.py\", line 171, in launch_train clip_c=1.) File \"/u/bsankara/nt/Git-nt/nt/nt.py\", line 1616, in train import theano.sandbox.cuda File \"/opt/share/Python-2.7.9/lib/python2.7/site-packages/theano/__init__.py\", line 98, in &lt;module&gt; theano.sandbox.cuda.tests.test_driver.test_nvidia_driver1() File \"/opt/share/Python-2.7.9/lib/python2.7/site-packages/theano/sandbox/cuda/tests/test_driver.py\", line 30, in test_nvidia_driver1 A = cuda.shared_constructor(a) File \"/opt/share/Python-2.7.9/lib/python2.7/site-packages/theano/sandbox/cuda/var.py\", line 181, in float32_shared_constructor enable_cuda=False) File \"/opt/share/Python-2.7.9/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py\", line 389, in use cuda_ndarray.cuda_ndarray.CudaNdarray.zeros((2, 3)) RuntimeError: ('CudaNdarray_ZEROS: allocation failed.', 'You asked to force this device and it failed. No fallback to the cpu or other gpu device.') The relevant part of the code snippet is here: from multiprocessing import Queue import zmq import pycuda.driver as drv import pycuda.gpuarray as gpuarray def train(private_args, process_env, &lt;some other args&gt;) if process_env is not None: os.environ = process_env #### # pycuda and zmq environment drv.init() dev = drv.Device(private_args['ind_gpu']) ctx = dev.make_context() sock = zmq.Context().socket(zmq.PAIR) if private_args['flag_client']: sock.connect('tcp://localhost:5000') else: sock.bind('tcp://*:5000') #### # import theano stuffs import theano.sandbox.cuda theano.sandbox.cuda.use(private_args['gpu']) import theano import theano.tensor as tensor from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams import theano.misc.pycuda_init import theano.misc.pycuda_utils ... The error is triggered when it imports theano.sandbox.cuda. And this is where, I launch the training function as two processes. def launch_train(curr_args, process_env, curr_queue, oth_queue): trainerr, validerr, testerr = train(private_args=curr_args, process_env=process_env, ...) process1_env = os.environ.copy() process1_env['THEANO_FLAGS'] = \"cuda.root=/opt/share/cuda-7.0,device=gpu0,floatX=float32,on_unused_input=ignore,optimizer=fast_run,exception_verbosity=high,compiledir=/u/bsankara/.theano/NT_multi_GPU1\" process2_env = os.environ.copy() process2_env['THEANO_FLAGS'] = \"cuda.root=/opt/share/cuda-7.0,device=gpu1,floatX=float32,on_unused_input=ignore,optimizer=fast_run,exception_verbosity=high,compiledir=/u/bsankara/.theano/NT_multi_GPU2\" p = Process(target=launch_train, args=(p_args, process1_env, queue_p, queue_q)) q = Process(target=launch_train, args=(q_args, process2_env, queue_q, queue_p)) p.start() q.start() p.join() q.join() The import statement however seem to work if I try to initialize the gpu interactively in Python. I executed the first 20 lines of the train() and it worked fine there and also correctly assigned me to gpu0 as I requested.",
        "answers": [
            [
                "After digging around and running pdb, the original poster found the issue. Basically theano and pycuda were both competing to initialize the gpu, causing the problem. The solution is to first 'import theano', which would get a gpu and then attach to the specific context in pycuda. So, the import sections within train function would look like this: def train(private_args, process_env, &lt;some other args&gt;) if process_env is not None: os.environ = process_env #### # import theano related # We need global imports and so we make them as such theano = __import__('theano') _t_tensor = __import__('theano', globals(), locals(), ['tensor'], -1) tensor = _t_tensor.tensor import theano.sandbox.cuda import theano.misc.pycuda_utils #### # pycuda and zmq environment import zmq import pycuda.driver as drv import pycuda.gpuarray as gpuarray drv.init() # Attach the existing context (already initialized by theano import statement) ctx = drv.Context.attach() sock = zmq.Context().socket(zmq.PAIR) if private_args['flag_client']: sock.connect('tcp://localhost:5000') else: sock.bind('tcp://*:5000') [This answer was added as a community wiki entry from an edit made by the OP in a attempt to get this question off the unaswered list]."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm using pycuda to make a relativistic raytracer. Basically, for each \"pixel\" in a big 2D array we must solve a system of 6 ODEs using Runge Kutta. As each integration is independent of the rest it should be very easy. Other people has achieve it using C/C++ CUDA with excellent results (see this project). The problem arises in the fact that I do not know how is the best way of doing this. I'm writing a Kernel that does some Runge Kutta Steps and then return the results to the CPU. This Kernel is called a lot of times in order to get the whole ray integrated. The problem is for some reason is very slow. Of course, I know that memory transfers are really a Bottleneck in CUDA, but as this is really slow I'm starting to think that I'm doing something wrong. It would be great if you can recommend me best programming practices for this case. (Using pycuda). Some things that I'm wandering: Do I need to create a new context on reach Kernel call? There is a way to not have to transfer memory from GPU to CPU, that is, starting a Kernel, pausing it to get some information, restating it and repeat. Each RK4 iteration takes roughly half a second, which is insane (also compared with the CUDA code in the link that does some similar operation). And I think this is due to something wrong with the way I'm using pycuda, so if you can explain the best way to do such an operation in the best manner, it could be great!. To clarify: the reason I have to pause/restart the Kernel is because of the watchdog. Kernels of more than 10 seconds got killed by the watchdog. Thank you in advance!",
        "answers": [
            [
                "Your main question seems to be too general, and it's hard to give some concrete advice without seeing the code. I'll try to answer your subquestions (not an actual answer, but it's a bit long for a comment) Do I need to create a new context on reach Kernel call? No. There is a way to not have to transfer memory from GPU to CPU, that is, starting a Kernel, pausing it to get some information, restating it and repeat. Depends on what you mean by \"get some information\". If it means doing stuff with it on CPU, then, of course, you have to transfer it. If you want to use in another kernel invocation, then you don't need to transfer it. Each RK4 iteration takes roughly half a second, which is insane (also compared with the CUDA code in the link that does some similar operation). It really depends on the equation, the number of threads and the video card you are using. I can imagine a situation when one RK step would take that long. And I think this is due to something wrong with the way I'm using pycuda, so if you can explain the best way to do such an operation in the best manner, it could be great!. Impossible to say for sure without the code. Try to create some minimal demonstrating example, or, at the very least, post a link to a runnable (even if it's rather long) piece of code that illustrates your problem. As for PyCUDA, it's a very thin wrapper over CUDA, and all the programming practises that apply to the latter, apply to the former as well."
            ],
            [
                "I might help you with the memory handling, i.e. not having to copy from CPU to GPU during your iterations. I am evolving a system through time using euler timestepping, and the way I keep all my data on my GPU is given below. However, the problem with this is that once the first kernel has been launched, the cpu keeps executing the lines after it. I.e. the boundary kernel gets launched before the time evolution step. What I need is a way to synchronize things. I have tried doing it using strm.synchronize() (see my code) but it does not always work. If you have any ideas on this, I would really appreciate your input! Thanks! def curveShorten(dist,timestep,maxit): \"\"\" iterates the function image on a 2d grid through an euler anisotropic diffusion operator with timestep=timestep maxit number of times \"\"\" image = 1*dist forme = image.shape if(np.size(forme)&gt;2): sys.exit('Only works on gray images') aSize = forme[0]*forme[1] xdim = np.int32(forme[0]) ydim = np.int32(forme[1]) image[0,:] = image[1,:] image[xdim-1,:] = image[xdim-2,:] image[:,ydim-1] = image[:,ydim-2] image[:,0] = image[:,1] #np arrays i need to store things on the CPU, image is the initial #condition and final is the final state image = image.reshape(aSize,order= 'C').astype(np.float32) final = np.zeros(aSize).astype(np.float32) #allocating memory to GPUs image_gpu = drv.mem_alloc(image.nbytes) final_gpu = drv.mem_alloc(final.nbytes) #sending data to each memory location drv.memcpy_htod(image_gpu,image) #host to device copying drv.memcpy_htod(final_gpu,final) #block size: B := dim1*dim2*dim3=1024 #gird size : dim1*dimr2*dim3 = ceiling(aSize/B) blockX = int(1024) multiplier = aSize/float(1024) if(aSize/float(1024) &gt; int(aSize/float(1024)) ): gridX = int(multiplier + 1) else: gridX = int(multiplier) strm1 = drv.Stream(1) ev1 = drv.Event() strm2 = drv.Stream() for k in range(0,maxit): Kern_diffIteration(image_gpu,final_gpu,ydim, xdim, np.float32(timestep), block=(blockX,1,1), grid=(gridX,1,1),stream=strm1) strm1.synchronize() if(strm1.is_done()==1): Kern_boundaryX0(final_gpu,ydim,xdim,block=(blockX,1,1), grid=(gridX,1,1)) Kern_boundaryX1(final_gpu,ydim,xdim,block=(blockX,1,1), grid=(gridX,1,1))#,stream=strm1) Kern_boundaryY0(final_gpu,ydim,xdim,block=(blockX,1,1), grid=(gridX,1,1))#,stream=strm2) Kern_boundaryY1(final_gpu,ydim,xdim,block=(blockX,1,1), grid=(gridX,1,1))#,stream=strm1) if(strm1.is_done()==1): drv.memcpy_dtod(image_gpu, final_gpu, final.nbytes) #Kern_copy(image_gpu,final_gpu,ydim,xdim,block=(blockX,1,1), grid=(gridX,1,1),stream=strm1) drv.memcpy_dtoh(final,final_gpu) #device to host copying #final_gpu.free() #image_gpu.free() return final.reshape(forme,order='C')"
            ]
        ],
        "votes": [
            2.0000001,
            -0.9999999
        ]
    },
    {
        "question": "Is there anyone, who ever sent CUDA arrays over MPI via most recent mpy4py ( and pyCUDA 2015.1.3)? To send an array, one must convert respective data type to the contiguous buffer. This conversion is done using the following lambda: to_buffer = lambda arr: None if arr is None else lambda arr: arr.gpudata.as_buffer(arr.nbytes Complete script look as follows: import numpy from mpi4py import MPI import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import pycuda.autoinit import numpy to_buffer = lambda arr: None if arr is None else lambda arr: arr.gpudata.as_buffer(arr.nbytes) print \"pyCUDA version \" + str(pycuda.VERSION ) a_gpu = gpuarray.to_gpu(numpy.random.randn(4,4).astype(numpy.float32)) comm = MPI.COMM_WORLD rank = comm.Get_rank() comm.Bcast([ to_buffer(agpu , MPI.FLOAT], root=0) But unfortunately, all this beauty crashes with these errors: pyCUDA version (2015, 1, 3) Traceback (most recent call last): File \"./test_mpi.py\", line 21, in &lt;module&gt; comm.Bcast([ to_buffer( numpy.random.randn(4,4).astype(numpy.float32)) , MPI.FLOAT], root=0) File \"Comm.pyx\", line 405, in mpi4py.MPI.Comm.Bcast (src/mpi4py.MPI.c:66743) File \"message.pxi\", line 388, in mpi4py.MPI._p_msg_cco.for_bcast (src/mpi4py.MPI.c:23220) File \"message.pxi\", line 355, in mpi4py.MPI._p_msg_cco.for_cco_send (src/mpi4py.MPI.c:22959) File \"message.pxi\", line 111, in mpi4py.MPI.message_simple (src/mpi4py.MPI.c:20516) File \"message.pxi\", line 51, in mpi4py.MPI.message_basic (src/mpi4py.MPI.c:19644) File \"asbuffer.pxi\", line 108, in mpi4py.MPI.getbuffer (src/mpi4py.MPI.c:6757) File \"asbuffer.pxi\", line 50, in mpi4py.MPI.PyObject_GetBufferEx (src/mpi4py.MPI.c:6093) TypeError: expected a readable buffer object Any ideas what's going on? Maybe someone have alternative buffer conversion mantra? Thanks in advance!",
        "answers": [
            [
                "All that is needed is to call the MPI broadcast with a valid host memory buffer object or numpy array, for example: comm.Bcast( a_gpu.get(), root=0) in place of the lambda for transforming the DeviceAllocation object to a buffer object"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a 3D array and would like to transpose its first two dimensions (x &amp; y), but not the 3rd (z). On a 3D array A I want the same result as numpy's A.transpose((1,0,2)). Specifically, I want to get the \"transposed\" global threadIdx. The code below is supposed to write the transposed index at the untransposed location in 3D array A. It doesn't. Any advice? import numpy as np from pycuda import compiler, gpuarray import pycuda.driver as cuda import pycuda.autoinit kernel_code = \"\"\" __global__ void test_indexTranspose(uint*A){ const size_t size_x = 4; const size_t size_y = 4; const size_t size_z = 3; // Thread position in each dimension const size_t tx = blockDim.x * blockIdx.x + threadIdx.x; const size_t ty = blockDim.y * blockIdx.y + threadIdx.y; const size_t tz = blockDim.z * blockIdx.z + threadIdx.z; if(tx &lt; size_x &amp;&amp; ty &lt; size_y &amp;&amp; tz &lt; size_z){ // Flat index const size_t ti = tz * size_x * size_y + ty * size_x + tx; // Transposed flat index const size_t tiT = tz * size_x * size_y + tx * size_x + ty; A[ti] = tiT; } } \"\"\" A = np.zeros((4,4,3),dtype=np.uint32) mod = compiler.SourceModule(kernel_code) test_indexTranspose = mod.get_function('test_indexTranspose') A_gpu = gpuarray.to_gpu(A) test_indexTranspose(A_gpu, block=(2, 2, 1), grid=(2,2,3)) This is what is returned (not what I expected): A_gpu.get()[:,:,0] array([[ 0, 12, 9, 6], [ 3, 15, 24, 21], [18, 30, 27, 36], [33, 45, 42, 39]], dtype=uint32) A_gpu.get()[:,:,1] array([[ 4, 1, 13, 10], [ 7, 16, 28, 25], [22, 19, 31, 40], [37, 34, 46, 43]], dtype=uint32) A_gpu.get()[:,:,2] array([[ 8, 5, 2, 14], [11, 20, 17, 29], [26, 23, 32, 44], [41, 38, 35, 47]], dtype=uint32) This is what I expected (but was not returned): A_gpu.get()[:,:,0] array([[0, 4, 8, 12], [1, 5, 9, 13], [2, 6, 10, 14], [3, 7, 11, 15]], dtype=uint32) A_gpu.get()[:,:,1] array([[16, 20, 24, 28], [17, 21, 25, 29], [18, 22, 26, 30], [19, 23, 27, 31]], dtype=uint32) A_gpu.get()[:,:,2] ... Thanks,",
        "answers": [
            [
                "Creating the numpy array with strides that are consistent with the CUDA kernel code solves the problem. Default layout of a numpy array is not row, column, depth as my kernel assumes. However, the strides can be set when creating the array. The above kernel works fine if the array is created like this: nRows = 4 nCols = 4 nSlices = 3 nBytes = np.dtype(np.uint32).itemsize A = np.ndarray(shape=(nRows, nCols, nSlices), dtype=np.uint32, strides=(nCols*nBytes, 1*nBytes, nCols*nRows*nBytes)) The strides are the jumps in memory consecutive indices need to take for each dimension in bytes. E.g. from the 1st element in row 1 to the 1st element in row 2 there are nCols * nBytes, i.e. 16 bytes."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "When compiling a CUDA program that launches kernels on multiple devices, does nvcc internally compile a version of the kernel for each device? I ask this because I am trying to use PyCUDA and am struggling with why I have to compile (call SourceModule) the kernel code for each device I am about to launch the kernel on. Thanks for your help!",
        "answers": [
            [
                "The one word answer is No. The compiler doesn't know or need to know anything about the number of GPUs during compilation. The runtime API will automagically load code from binary payloads into each context without the compiler or programmer needing to do anything. If your code requires JIT recompilation, the driver will compile once and the cached machine code will be reused on subsequent contexts, if the hardware targets are the same. In PyCUDA, you are using the driver API, so context management is more manual. You have to load modules into the context of each GPU you are using. If you are using the source module feature, that means you need to submit code for each GPU. But (IIRC), PyCUDA does also do caching of its JIT compiled code done with nvcc. So even though you need to call the source module for every context, you shouldn't get compiler invocation every time, if the GPUs are the same. If this bothers you and you are not doing a lot of meta-programming, consider switching to pre-compiled cubins. You still need to load them into every context, but there is no compilation overhead at runtime."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Hi I'm following a neural net tutorial where the author seems to be using shared variables everywhere. From my understanding, a shared variable in theanos simply is a space in memory that can be shared by the gpu and cpu heap. Anyway, I have two matrices which I declare as shared variables and I want to perform some operation on them using function. (Question 1) I'd love it if someone could explain why function is usefull vs regular def function. Anyway, I'm setting up my definition like such: import theano import theano.tensor as T from theano import function import numpy as np class Transform: def __init__(self, dimg): dimg = dimg.astype(theano.config.floatX) self.in_t = theano.shared(dimg, name='dimg', borrow=True) def rotate(self, ox, oy, radians): value = np.zeros((2 * self.in_t.get_value().shape[0], 2 * self.in_t.get_value().shape[1])) out_t = theano.shared(value, name='b', dtype=theano.config.floatX), borrow=True) din = theano.tensor.dmatrix('a') dout = theano.tensor.dmatrix('b') def atest(): y = x + y return y f = function(inputs=[], givens={x: self.in_t, y: self.out_t}, outputs=atest) return f() The problem is that I have no idea how to use the shared variables in a regular function-output call. I understand that I can do updates via function([],..update=(shared_var_1, upate_function)). But how do I access them in my regular function?",
        "answers": [
            [
                "Theano beginner here, so I'm not sure that my answer will cover all the technical aspects. Answering your first question: you need to declare theano function instead of def function, because theano is like a \"language\" inside python and invoking theano.function you're compiling some ad-hoc C code performing your task under the hood. This is what makes Theano fast. From the documentation: It is good to think of theano.function as the interface to a compiler which builds a callable object from a purely symbolic graph. One of Theano\u2019s most important features is that theano.function can optimize a graph and even compile some or all of it into native machine instructions. About your second quetion, in order to access what's stored in your shared variable you should use shared_var.get_value() Check these examples: The value can be accessed and modified by the .get_value() and .set_value() methods. This code: a = np.array([[1,2],[3,4]], dtype=theano.config.floatX) x = theano.shared(a) print(x) Will output &lt;CudaNdarrayType(float32, matrix)&gt; But using get_value(): print(x.get_value()) It outputs [[ 1. 2.] [ 3. 4.]] Edit: to use shared variables in functions import theano import numpy a = numpy.int64(2) y = theano.tensor.scalar('y',dtype='int64') z = theano.tensor.scalar('z',dtype='int64') x = theano.shared(a) plus = y + z theano_sum = theano.function([y,z],plus) # Using shared variable in a function print(theano_sum(x.get_value(),3)) # Changing shared variable value using a function x.set_value(theano_sum(2,2)) print(x.get_value()) # Update shared variable value x.set_value(x.get_value(borrow=True)+1) print(x.get_value()) Will output: 5 4 5"
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I am on Windows 8.1, and have installed Cuda 5.5 and then installed PyCuda via building it from source. These steps seem to have worked correctly, however, when I do \"import pycuda.driver\" it complains that Error doesn't exist. I know that via this the Error class is supposed to be defined in pycuda.driver, as many other errors inherit from this. Yet it's not there. The only place I could find an error class was in cuda.hpp. So is there simply some flag I am missing? Or some include path that I need to add?",
        "answers": [
            [
                "I figured this out, it was because, when compiing, it threw an error then continued to finish the installation and installed it. And then when I tried again to build and install it just said it was successful. So if you are having the same problem, delete the build folder that is in the same directory as setup.py, and then you can build again and look for errors."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I'm using CUDA (in reality I'm using pyCUDA if the difference matters) and performing some computation over arrays. I'm launching a kernel with a grid of 320*600 threads. Inside the kernel I'm declaring two linear arrays of 20000 components using: float test[20000] float test2[20000] With these arrays I perform simple calculations, like for example filling them with constant values. The point is that the kernel executes normally and perform correctly the computations (you can see this filling an array with a random component of test and sending that array to host from device). The problem is that my NVIDIA card has only 2GB of memory and the total amount of memory to allocate the arrays test and test2 is 320*600*20000*4 bytes that is much more than 2GB. Where is this memory coming from? and how can CUDA perform the computation in every thread? Thank you for your time",
        "answers": [
            [
                "The actual sizing of the local/stack memory requirements is not as you suppose (for the entire grid, all at once) but is actually based on a formula described by @njuffa here. Basically, the local/stack memory require is sized based on the maximum instantaneous capacity of the device you are running on, rather than the size of the grid. Based on the information provided by njuffa, the available stack size limit (per thread) is the lesser of: The maximum local memory size (512KB for cc2.x and higher) available GPU memory/(#of SMs)/(max threads per SM) For your first case: float test[20000]; float test2[20000]; That total is 160KB (per thread) so we are under the maximum limit of 512KB per thread. What about the 2nd limit? GTX 650m has 2 cc 3.0 (kepler) SMs (each Kepler SM has 192 cores). Therefore, the second limit above gives, if all the GPU memory were available: 2GB/2/2048 = 512KB (kepler has 2048 max threads per multiprocessor) so it is the same limit in this case. But this assumes all the GPU memory is available. Since you're suggesting in the comments that this configuration fails: float test[40000]; float test2[40000]; i.e. 320KB, I would conclude that your actual available GPU memory is at the point of this bulk allocation attempt is somewhere above (160/512)*100% i.e. above 31% but below (320/512)*100% i.e. below 62.5% of 2GB, so I would conclude that your available GPU memory at the time of this bulk allocation request for the stack frame would be something less than 1.25GB. You could try to see if this is the case by calling cudaGetMemInfo right before the kernel launch in question (although I don't know how to do this in pycuda). Even though your GPU starts out with 2GB, if you are running the display from it, you are likely starting with a number closer to 1.5GB. And dynamic (e.g. cudaMalloc) and or static (e.g. __device__) allocations that occur prior to this bulk allocation request at kernel launch, will all impact available memory. This is all to explain some of the specifics. The general answer to your question is that the \"magic\" arises due to the fact that the GPU does not necessarily allocate the stack frame and local memory for all threads in the grid, all at once. It need only allocate what is required for the maximum instantaneous capacity of the device (i.e. SMs * max threads per SM), which may be a number that is significantly less than what would be required for the whole grid."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "In the example below, we have a contiguous array, and a view of the same array that is non-contiguous: shape = (5, 100) A = np.arange(np.product(shape)).reshape(shape) # Everything is contiguous at this point assert A.flags.c_contiguous == True # Since we're taking a view over the # row minor dimension, we'll have 5 # segments of 50 contiguous elements A[:,20:70].flags.c_contiguous == False # Each segment is contiguous A[0,20:70].flags.c_contiguous == True A[1,20:70].flags.c_contiguous == True A[2,20:70].flags.c_contiguous == True A[3,20:70].flags.c_contiguous == True A[4,20:70].flags.c_contiguous == True The view references 5 segments of 50 contiguous elements. If we look at a more general case shape = (30, 20, 70, 50) B = np.arange(np.product(shape)).reshape(shape) then, the following holds: B[0,:,:,:].flags.c_contiguous == True B[0,0,:,:].flags.c_contiguous == True B[0,0,0,:].flags.c_contiguous == True A partial solution, based on _UpdateContiguousFlags in flagobjects.c might be: def is_contiguous(ary): is_c_contig = True sd = ary.itemsize for i in reversed(range(ary.ndim)): dim = ary.shape[i] # Contiguous by default if dim == 0: return True if dim != 1: if ary.strides[i] != sd: is_c_contig = False sd *= dim return is_c_contig But I don't think this handles cases where the array is transposed. e.g.: B.transpose(3,0,1,2) Question: Is there a method of identifying the contiguous segments in a general NumPy view/array? I need this functionality in order to transfer data from a NumPy array to a GPU without copying data into pinned memory. Instead, I'd like to pin contiguous segments using cudaHostRegister and then do the transfer to the GPU. I'm aware of CUDA's support for pitched memory via MemCpy3D, but I'd like to handle more general cases. Edit 1: Added a basic solution to the question, and asked about the transpose case. Edit 2: Clarified removing the need to copy data into pinned memory.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I have a 3D numpy array of size 1000x1000x1000. I am looking for the indices of the values 1 in the entire array. The np.nonzero(array) is very slow for larger dataset as mine. I was wondering if there is a way to do it via pycuda. Or is there some other more efficient method.",
        "answers": [
            [
                "I have not used PyCuda before, but since I found a good example on how to use thrust in PyCuda, I came up with the following solution. Internally, it uses thrust::counting_iterator and thrust::copy_if to find the indices of the elements which are equal to 1. While this may be faster, there is a serious flaw in your whole problem: You have an array with 1 billion (1000000000) elements, which needs 4 GB memory when using 32bit integers. You will need another output array which is 4 GB as well. Even if your GPU has that much RAM, the input data needs to be copied to the GPU, which will take some time. If your array consists of mainly zero entries, you might be better off with using a sparse matrix format and only store non-zero entries. This will save memory and you do not have to search for non-zero entries at all. find_indices_thrust.py import pycuda import pycuda.autoinit import pycuda.gpuarray as gpuarray import numpy as np from codepy.cgen import * from codepy.bpl import BoostPythonModule from codepy.cuda import CudaModule #Make a host_module, compiled for CPU host_mod = BoostPythonModule() #Make a device module, compiled with NVCC nvcc_mod = CudaModule(host_mod) #Describe device module code #NVCC includes nvcc_includes = [ 'thrust/copy.h', 'thrust/device_vector.h', 'thrust/iterator/counting_iterator.h', 'thrust/functional.h', 'cuda.h', 'stdint.h' ] #Add includes to module nvcc_mod.add_to_preamble([Include(x) for x in nvcc_includes]) #NVCC function nvcc_function = FunctionBody( FunctionDeclaration(Value('void', 'find_indices'), [Value('CUdeviceptr', 'input_ptr'), Value('uint32_t', 'input_length'), Value('CUdeviceptr', 'output_ptr'), Value('uint32_t*', 'output_length')]), Block([Statement('thrust::device_ptr&lt;uint32_t&gt; thrust_input_ptr((uint32_t*)input_ptr)'), Statement('thrust::device_ptr&lt;uint32_t&gt; thrust_output_ptr((uint32_t*)output_ptr)'), Statement('using namespace thrust::placeholders'), Statement('*output_length = thrust::copy_if(thrust::counting_iterator&lt;uint32_t&gt;(0), thrust::counting_iterator&lt;uint32_t&gt;(input_length), thrust_input_ptr, thrust_output_ptr, _1==1)-thrust_output_ptr')])) #Add declaration to nvcc_mod #Adds declaration to host_mod as well nvcc_mod.add_function(nvcc_function) host_includes = [ 'boost/python/extract.hpp', ] #Add host includes to module host_mod.add_to_preamble([Include(x) for x in host_includes]) host_namespaces = [ 'using namespace boost::python', ] #Add BPL using statement host_mod.add_to_preamble([Statement(x) for x in host_namespaces]) host_statements = [ #Extract information from PyCUDA GPUArray #Get length 'tuple shape = extract&lt;tuple&gt;(gpu_input_array.attr(\"shape\"))', 'int input_length = extract&lt;int&gt;(shape[0])', #Get input data pointer 'CUdeviceptr input_ptr = extract&lt;CUdeviceptr&gt;(gpu_input_array.attr(\"ptr\"))', #Get output data pointer 'CUdeviceptr output_ptr = extract&lt;CUdeviceptr&gt;(gpu_output_array.attr(\"ptr\"))', #Call Thrust routine, compiled into the CudaModule 'uint32_t output_size', 'find_indices(input_ptr, input_length, output_ptr, &amp;output_size)', #Return result 'return output_size', ] host_mod.add_function( FunctionBody( FunctionDeclaration(Value('int', 'host_entry'), [Value('object', 'gpu_input_array'),Value('object', 'gpu_output_array')]), Block([Statement(x) for x in host_statements]))) #Print out generated code, to see what we're actually compiling print(\"---------------------- Host code ----------------------\") print(host_mod.generate()) print(\"--------------------- Device code ---------------------\") print(nvcc_mod.generate()) print(\"-------------------------------------------------------\") #Compile modules import codepy.jit, codepy.toolchain gcc_toolchain = codepy.toolchain.guess_toolchain() nvcc_toolchain = codepy.toolchain.guess_nvcc_toolchain() module = nvcc_mod.compile(gcc_toolchain, nvcc_toolchain, debug=True) length = 100 input_array = np.array(np.random.rand(length)*5, dtype=np.uint32) output_array = np.zeros(length, dtype=np.uint32) print(\"---------------------- INPUT -----------------------\") print(input_array) gpu_input_array = gpuarray.to_gpu(input_array) gpu_output_array = gpuarray.to_gpu(output_array) # call GPU function output_size = module.host_entry(gpu_input_array, gpu_output_array) print(\"----------------------- OUTPUT ------------------------\") print gpu_output_array[:output_size] print(\"-------------------------------------------------------\") generated code ---------------------- Host code ---------------------- #include &lt;boost/python.hpp&gt; #include &lt;cuda.h&gt; void find_indices(CUdeviceptr input_ptr, uint32_t input_length, CUdeviceptr output_ptr, uint32_t* output_length); #include &lt;boost/python/extract.hpp&gt; using namespace boost::python; namespace private_namespace_6f5e74fc4bebe20d5478de66e2226656 { int host_entry(object gpu_input_array, object gpu_output_array) { tuple shape = extract&lt;tuple&gt;(gpu_input_array.attr(\"shape\")); int input_length = extract&lt;int&gt;(shape[0]); CUdeviceptr input_ptr = extract&lt;CUdeviceptr&gt;(gpu_input_array.attr(\"ptr\")); CUdeviceptr output_ptr = extract&lt;CUdeviceptr&gt;(gpu_output_array.attr(\"ptr\")); uint32_t output_size; find_indices(input_ptr, input_length, output_ptr, &amp;output_size); return output_size; } } using namespace private_namespace_6f5e74fc4bebe20d5478de66e2226656; BOOST_PYTHON_MODULE(module) { boost::python::def(\"host_entry\", &amp;host_entry); } --------------------- Device code --------------------- #include &lt;thrust/copy.h&gt; #include &lt;thrust/device_vector.h&gt; #include &lt;thrust/iterator/counting_iterator.h&gt; #include &lt;thrust/functional.h&gt; #include &lt;cuda.h&gt; #include &lt;stdint.h&gt; void find_indices(CUdeviceptr input_ptr, uint32_t input_length, CUdeviceptr output_ptr, uint32_t* output_length) { thrust::device_ptr&lt;uint32_t&gt; thrust_input_ptr((uint32_t*)input_ptr); thrust::device_ptr&lt;uint32_t&gt; thrust_output_ptr((uint32_t*)output_ptr); using namespace thrust::placeholders; *output_length = thrust::copy_if(thrust::counting_iterator&lt;uint32_t&gt;(0), thrust::counting_iterator&lt;uint32_t&gt;(input_length), thrust_input_ptr, thrust_output_ptr, _1==1)-thrust_output_ptr; } ------------------------------------------------------- demo output ---------------------- INPUT ----------------------- [1 2 3 0 3 3 1 2 1 2 0 4 4 3 2 0 4 2 3 0 2 3 1 4 3 4 3 4 3 2 4 3 2 4 2 0 3 0 3 4 3 0 0 4 4 2 0 3 3 1 3 4 2 0 0 4 0 4 3 2 3 2 1 1 4 3 0 4 3 1 1 1 3 2 0 0 3 4 3 3 4 2 2 3 4 1 1 3 2 2 2 2 3 2 0 2 4 3 2 0] ----------------------- OUTPUT ------------------------ [ 0 6 8 22 49 62 63 69 70 71 85 86] -------------------------------------------------------"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I want to know if it is possible to get the options declared in parameters of SourceModule. How can i do it ? mod = SourceModule(src_device, nvcc='/opt/cuda65/bin/nvcc', options=['-DRANDPHILOX4x32_7','-DPROGRESSION']) Does it exist a method which do it, something like mod.get_options() Thanks in advance.",
        "answers": [
            [
                "No, as you can see here, compile options passed to the source module are not retained in the SourceModule object. Instead, they pass straight through to the compile system and there is no way to retrieve them after the compilation has been completed."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a set of rectangles overlapping each other. I need to detect that overlaps exist in a set of rectangles. If overlaps exist, then I need to update the coordinates so the set of rectangles do not overlap anymore. I wonder if there are existing python libraries suited for this task. This operation will be applied to million+ set of rectangles, so algorithm efficiency and leveraging GPU would be important as well.",
        "answers": [
            [
                "The shapely interface to GEOS may be the library for which you are looking, but I have never used it for this purpose. It may be easier to roll your own, in this case. The algorithm is a straightforward sweep-line algorithm, with an average complexity per rectangle proportional to log(N). A) Each rectangle is characterized by four coordinates, Left, Right, Top, Bottom. B) The rectangles will be processed in order of their Left coordinate c) An Interval Tree is used to maintain the top-bottom ranges for each rectangle whose Left edge has been encountered and Right edge has not yet been encountered D) A Priority Queue is maintained, ordered by Right edge, of all rectangles that are currently in the Interval Tree 1) Get the first or next rectangle to be processed. If no more are available, exit. 2) While any element on the Priority Queue has a priority less than or equal to the Left value of this rectangle, delete that element from the Priority Queue and the associated element from the Interval Tree. 3) Search the Interval Tree for overlap with the Top-Bottom range of this rectangle; process each overlap found. 4) Insert the Top-Bottom range of this rectangle into the Interval Tree, and add an element to the Priority Queue, with a priority set to the Right value from the rectangle, referring to the interval added to the Interval Tree. 5) Return to step 1) to get the next rectangle."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am working with large, nonuniform matrices and am having problems with what I believe to be mismatching on the elements. In example.py, get_simulated_ipp() builds echo and tx, two linear arrays of size 250000 and 25000 respectively. The code also hardcoded sr=25. My code is attempting to complex multiply tx into echo along different stretches, depending on specified ranges and value of sr. This will then be stored in an array S. After searching through some other people's examples, I found a way of building blocks and grids here that I thought would work well. I'm unfamiliar with C code, but have been trying to learn over the past week. Here is my code: #!/usr/bin/python #This iteration only works on the first and last elements, mismatching after that. # However, this doesn't result in any empty elements in S import numpy as np import example as ex import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule #pull simulated data and get info about it ((echo,tx)) = ex.get_simulated_ipp() ranges = np.arange(4000,6000).astype(np.int32) S = np.zeros([len(ranges),len(tx)],dtype=np.complex64) sr = ex.sr #copying input to gpu # will try this explicitly if in/out (in the function call) don't work block_dim_x = 8 #thread number is product of block dims, block_dim_y = 8 # want a multiple of 32 (warp multiple) blocks_x = np.ceil(len(ranges)/block_dim_x).astype(np.int32).item() blocks_y = np.ceil(len(tx)/block_dim_y).astype(np.int32).item() kernel_code=\"\"\" #include &lt;cuComplex.h&gt; __global__ void complex_mult(cuFloatComplex *tx, cuFloatComplex *echo, cuFloatComplex *result, int *ranges, int sr) { unsigned int block_num = blockIdx.x + blockIdx.y * gridDim.x; unsigned int thread_num = threadIdx.x + threadIdx.y * blockDim.x; unsigned int threads_in_block = blockDim.x * blockDim.y; unsigned long int idx = threads_in_block * block_num + thread_num; //aligning the i,j to idx, something is mismatched? int i = ((idx % (threads_in_block * gridDim.x)) % blockDim.x) + ((block_num % gridDim.x) * blockDim.x); int j = ((idx - (threads_in_block * block_num)) / blockDim.x) + ((block_num / gridDim.x) * blockDim.y); result[idx] = cuCmulf(echo[j+ranges[i]*sr], tx[j]); } \"\"\" ## want something to work like this: ## result[i][j] = cuCmulf(echo[j+ranges[i]*sr], tx[j]); #includes directory of where cuComplex.h is located mod = SourceModule(kernel_code, include_dirs=['/usr/local/cuda-7.0/include/']) complex_mult = mod.get_function(\"complex_mult\") complex_mult(cuda.In(tx), cuda.In(echo), cuda.Out(S), cuda.In(ranges), np.int32(sr), block=(block_dim_x,block_dim_y,1), grid=(blocks_x,blocks_y)) compare = np.zeros_like(S) #built to compare CPU vs GPU calcs txidx = np.arange(len(tx)) for ri,r in enumerate(ranges): compare[ri,:] = echo[txidx+r*sr]*tx print np.subtract(S, compare) At the bottom here, I've put in a CPU implementation of what I'm attempting to accomplish and put in a subtraction. The result is that the very first and very last elements come out as 0+0j, but the rest do not. The kernel is attempting to align an i and j to the idx so that I can traverse echo, ranges, and tx more easily. Is there a better way to implement something like this? Also, why might the result not come out as all 0+0j as I intend? Edit: Trying a little example to get a better grasp of how the arrays are being indexed with this block/grid configuration, I stumbled upon something very strange. Before, I tried to index the elements, I just wanted to run a little test multiplication. It seems like my block/grid covers all of the ary_in matrix, but the result ends up only doubling the top half of ary_in and the bottom half is returning whatever was left over from the bottom half calculation previously. If I change blocks_x to 4 so that I cover more space than needed, however, the doubling works fine. If I then run it with a 4x4 grid, with * 3 instead, it'll work out fine with ary_out as ary_in tripled. When I run it again with a 2x4 grid and only doubling, the top half of ary_out returns the doubled ary_in, but the bottom half returns the previous result in memory, a tripled value instead. I would understand this to be something in my index/block/grid mapping wrongly to the values, but I can't figure out what. ary_in = np.arange(128).reshape((8,16)) print ary_in ary_out = np.zeros_like(ary_in) block_dim_x = 4 block_dim_y = 4 blocks_x = 2 blocks_y = 4 limit = block_dim_x * block_dim_y * blocks_x * blocks_y mod = SourceModule(\"\"\" __global__ void indexing_order(int *ary_in, int *ary_out, int n) { unsigned int block_num = blockIdx.x + blockIdx.y * gridDim.x; unsigned int thread_num = threadIdx.x + threadIdx.y * blockDim.x; unsigned int threads_in_block = blockDim.x * blockDim.y; unsigned int idx = threads_in_block * block_num + thread_num; if (idx &lt; n) { // ary_out[idx] = thread_num; ary_out[idx] = ary_in[idx] * 2; } } \"\"\") indexing_order = mod.get_function(\"indexing_order\") indexing_order(drv.In(ary_in), drv.Out(ary_out), np.int32(limit), block=(block_dim_x,block_dim_y,1), grid=(blocks_x,blocks_y)) print ary_out FINAL EDIT: I figured out the problems. In the edit just above, the ary_in is by default an int64, mismatching with the int initialization in the C code of an int32. This only allocated half the amount of data needed on the GPU for the entire array, so only the top half was moved over and operated on. Adding a .astype(np.int32) solved this problem. This allowed me to figure out the the ordering of the indexing in my case and fix the main code with: int i = idx / row_len; int j = idx % row_len; I still don't understand how to get this working with non even division of block dimensions into the output array (e.g. 16x16), even with an if (idx",
        "answers": [
            [
                "I figured out the problems. In the edit just above, the ary_in is by default an int64, mismatching with the int initialization in the C code of an int32. This only allocated half the amount of data needed on the GPU for the entire array, so only the top half was moved over and operated on. Adding a .astype(np.int32) solved this problem. This allowed me to figure out the the ordering of the indexing in my case and fix the main code with: int i = idx / row_len; int j = idx % row_len;"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to understand shared memory by playing with the following code: import pycuda.driver as drv import pycuda.tools import pycuda.autoinit import numpy from pycuda.compiler import SourceModule src=''' __global__ void reduce0(float *g_idata, float *g_odata) { extern __shared__ float sdata[]; // each thread loads one element from global to shared mem unsigned int tid = threadIdx.x; unsigned int i = blockIdx.x*blockDim.x + threadIdx.x; sdata[tid] = g_idata[i]; __syncthreads(); // do reduction in shared mem for(unsigned int s=1; s &lt; blockDim.x; s *= 2) { if (tid % (2*s) == 0) { sdata[tid] += sdata[tid + s]; } __syncthreads(); } // write result for this block to global mem if (tid == 0) g_odata[blockIdx.x] = sdata[0]; } ''' mod = SourceModule(src) reduce0=mod.get_function('reduce0') a = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) reduce0(drv.In(a),drv.Out(dest),block=(400,1,1)) I can't see anything obviously wrong with this, but I keep getting synchronization errors and it doesn't run. Any help greatly appreciated.",
        "answers": [
            [
                "When you specify extern __shared__ float sdata[]; you are telling the compiler that the caller will provide the shared memory. In PyCUDA, that is done by specifying shared=nnnn on the line that calls the CUDA function. In your case, something like: reduce0(drv.In(a),drv.Out(dest),block=(400,1,1),shared=4*400) Alternately, you can drop the extern keyword, and specify the shared memory directly: __shared__ float sdata[400];"
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "I am using pycuda and i would like to know if there is an equivalent to the function cudaMemcpyToSymbol I would like to copy a constant from the host to the device like below import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy from sys import path from struct import * from gpustruct import GPUStruct if __name__ == '__main__': # list devices ndevices = cuda.Device.count() print '{} devices found'.format(ndevices) for i in xrange(ndevices): print ' ', cuda.Device(i).name() # compile device.cu mod = SourceModule(''' __device__ __constant__ int CONSTd; struct Results { float *A; float *B; float *C; }; struct fin { float *N; }; __global__ void test(Results *src,fin *dest){ int i=blockIdx.x *blockDim.x + threadIdx.x; src-&gt;C[i]=src-&gt;A[i]+src-&gt;B[i]+dest-&gt;N[i]+CONSTd; }''', nvcc='/opt/cuda65/bin/nvcc', ) kern = mod.get_function(\"test\") CONSTANTE=5 src_gpu = GPUStruct([(numpy.int32,'*A', numpy.ones(10,dtype=numpy.int32)),(numpy.int32,'*B', numpy.ones(10,dtype=numpy.int32)),(numpy.int32,'*C', numpy.zeros(10,dtype=numpy.int32))]) test_gpu = GPUStruct([(numpy.int32,'*N', numpy.array(10*[5],dtype=numpy.int32))]) #something like this: **cudaMemcpyToSymbol(CONSTd, &amp;CONSTANTE, sizeof(int));** src_gpu.copy_to_gpu() test_gpu.copy_to_gpu() kern(src_gpu.get_ptr(),test_gpu.get_ptr(),block=(10,1,1),grid=(1,1)) src_gpu.copy_from_gpu() print(src_gpu)",
        "answers": [
            [
                "The PyCUDA implementation directly follows the CUDA driver API, so you can use any driver API code you can find as a model, but there are two things required to make this work: Use the module function module.get_global() to retrieve the device pointer to the symbol within the compiled source module Use driver.memcpy_htod to copy values to that pointer. Note that the PyCUDA APIs require that objects support the Python buffer protocol. In practice this means you should be using numpy.ndarray or similar on the Python side. This is effectively what cudaMemcpyToSymbol does under the hood."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have my kernel which is like below: # compile device.cu mod = SourceModule(''' #include&lt;stdio.h&gt; __global__ void test(unsigned int* tab, unsigned int compteurInit) { unsigned int gID = threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * (blockIdx.x + blockIdx.y * gridDim.x)); tab[gID] = compteurInit; printf(\"%d \",tab[gID]); }''', nvcc='/opt/cuda65/bin/nvcc', ) and here is my host program kern = mod.get_function(\"test\") XGRID = 256 YGRID = 1 XBLOCK = 256 YBLOCK = 1 etat=np.zeros(XBLOCK * YBLOCK * XGRID * YGRID,dtype=np.uint) etat_gpu= gpuarray.to_gpu(etat) kern(etat_gpu,np.uint(10),block=(XBLOCK,YBLOCK,1),grid=(XGRID,YGRID,1)) print etat_gpu.get() when i print the result i have got some strange values whereas like this: [42949672970 42949672970 42949672970 ..., 0 0 0] but when i check the printed value in the kernel it seems good",
        "answers": [],
        "votes": []
    },
    {
        "question": "i am using pycuda and i write this program etat=np.zeros(XBLOCK * YBLOCK * XGRID * YGRID,dtype=np.uint) compteur_init=np.uint(0) clef_utilisateur=np.uint(SEED) config=clef_utilisateur compteur_init_gpu = cuda.mem_alloc(compteur_init.nbytes) etat_init_gpu=cuda.mem_alloc(etat.nbytes) cuda.memcpy_htod(compteur_init_gpu, compteur_init) cuda.memcpy_htod(etat_gpu, etat) when i compile i get this error message 'numpy.uint64' does not have the buffer interface what does it means exactly ??",
        "answers": [
            [
                "finally , i've solved the problem with the module gpuarray import pycuda.gpuarray as gpuarray etat=np.zeros(XBLOCK * YBLOCK * XGRID * YGRID,dtype=np.uint) etat_gpu= gpuarray.to_gpu(etat) kern(etat_gpu,np.uint(10),block=(XBLOCK,YBLOCK,1),grid=(XGRID,YGRID,1))"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to interface the sparse cuSOLVER routine cusolverSpDcsrlsvqr() (&gt;= CUDA 7.0) using PyCUDA and am facing some difficulties: I have tried wrapping the methods the same way the dense cuSolver routines are wrapped in scikits-cuda (https://github.com/lebedov/scikits.cuda/blob/master/scikits/cuda/cusolver.py). However, the code crashes with a segmentation fault when calling the cusolverSpDcsrlsvqr() function. Debugging with cuda-gdb (cuda-gdb --args python -m pycuda.debug test.py; run;bt) yields the following stacktrace, #0 0x00007fffd9e3b71a in cusolverSpXcsrissymHost () from /usr/local/cuda/lib64/libcusolver.so #1 0x00007fffd9df5237 in hsolverXcsrqr_zeroPivot () from /usr/local/cuda/lib64/libcusolver.so #2 0x00007fffd9e0c764 in hsolverXcsrqr_analysis_coletree () from /usr/local/cuda/lib64/libcusolver.so #3 0x00007fffd9f160a0 in cusolverXcsrqr_analysis () from /usr/local/cuda/lib64/libcusolver.so #4 0x00007fffd9f28d78 in cusolverSpScsrlsvqr () from /usr/local/cuda/lib64/libcusolver.so which is weird, since I do not call cusolverSpScsrlsvqr() nor do I think it should call a host function (cusolverSpXcsrissymHost). This is the code I'm talking about - thanks for your help: # ### Interface cuSOLVER PyCUDA import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import pycuda.autoinit import numpy as np import scipy.sparse as sp import ctypes # #### wrap the cuSOLVER cusolverSpDcsrlsvqr() using ctypes # cuSparse _libcusparse = ctypes.cdll.LoadLibrary('libcusparse.so') class cusparseMatDescr_t(ctypes.Structure): _fields_ = [ ('MatrixType', ctypes.c_int), ('FillMode', ctypes.c_int), ('DiagType', ctypes.c_int), ('IndexBase', ctypes.c_int) ] _libcusparse.cusparseCreate.restype = int _libcusparse.cusparseCreate.argtypes = [ctypes.c_void_p] _libcusparse.cusparseDestroy.restype = int _libcusparse.cusparseDestroy.argtypes = [ctypes.c_void_p] _libcusparse.cusparseCreateMatDescr.restype = int _libcusparse.cusparseCreateMatDescr.argtypes = [ctypes.c_void_p] # cuSOLVER _libcusolver = ctypes.cdll.LoadLibrary('libcusolver.so') _libcusolver.cusolverSpCreate.restype = int _libcusolver.cusolverSpCreate.argtypes = [ctypes.c_void_p] _libcusolver.cusolverSpDestroy.restype = int _libcusolver.cusolverSpDestroy.argtypes = [ctypes.c_void_p] _libcusolver.cusolverSpDcsrlsvqr.restype = int _libcusolver.cusolverSpDcsrlsvqr.argtypes= [ctypes.c_void_p, ctypes.c_int, ctypes.c_int, cusparseMatDescr_t, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_double, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p] #### Prepare the matrix and parameters, copy to Device via gpuarray # coo to csr val = np.arange(1,5,dtype=np.float64) col = np.arange(0,4,dtype=np.int32) row = np.arange(0,4,dtype=np.int32) A = sp.coo_matrix((val,(row,col))).todense() Acsr = sp.csr_matrix(A) b = np.ones(4) x = np.empty(4) print('A:' + str(A)) print('b: ' + str(b)) dcsrVal = gpuarray.to_gpu(Acsr.data) dcsrColInd = gpuarray.to_gpu(Acsr.indices) dcsrIndPtr = gpuarray.to_gpu(Acsr.indptr) dx = gpuarray.to_gpu(x) db = gpuarray.to_gpu(b) m = ctypes.c_int(4) nnz = ctypes.c_int(4) descrA = cusparseMatDescr_t() reorder = ctypes.c_int(0) tol = ctypes.c_double(1e-10) singularity = ctypes.c_int(99) #create cusparse handle _cusp_handle = ctypes.c_void_p() status = _libcusparse.cusparseCreate(ctypes.byref(_cusp_handle)) print('status: ' + str(status)) cusp_handle = _cusp_handle.value #create MatDescriptor status = _libcusparse.cusparseCreateMatDescr(ctypes.byref(descrA)) print('status: ' + str(status)) #create cusolver handle _cuso_handle = ctypes.c_void_p() status = _libcusolver.cusolverSpCreate(ctypes.byref(_cuso_handle)) print('status: ' + str(status)) cuso_handle = _cuso_handle.value print('cusp handle: ' + str(cusp_handle)) print('cuso handle: ' + str(cuso_handle)) ### Call solver _libcusolver.cusolverSpDcsrlsvqr(cuso_handle, m, nnz, descrA, int(dcsrVal.gpudata), int(dcsrIndPtr.gpudata), int(dcsrColInd.gpudata), int(db.gpudata), tol, reorder, int(dx.gpudata), ctypes.byref(singularity)) # destroy handles status = _libcusolver.cusolverSpDestroy(cuso_handle) print('status: ' + str(status)) status = _libcusparse.cusparseDestroy(cusp_handle) print('status: ' + str(status))",
        "answers": [
            [
                "Setting descrA to ctypes.c_void_p() and replacing cusparseMatDescr_t in the cusolverSpDcsrlsvqr wrapper with ctypes.c_void_p should solve the problem."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am trying to run pycuda on my windows 7 machine. I have installed the following- 1. Python 2.7.9 2. cuda_7.0.28_windows 3. numpy-1.9.2-win32-superpack-python2.7 4. pycuda-2014.1+cuda6514-cp27-none-win32 (whl from christopher gohlke's libraries page) 5. Visual Studio 2013 community edition All the above installations were successful but when I run the code below (its a long code but the import statements should be enough to describe the problem) from __future__ import division import numpy as np import pycuda.driver as drv from pycuda.compiler import SourceModule import pycuda.autoinit import numpy.testing I get the following error- Traceback (most recent call last): File \"D:\\trash\\cuda_test.py\", line 3, in &lt;module&gt; import pycuda.driver as drv File \"C:\\Python27\\lib\\site-packages\\pycuda\\driver.py\", line 2, in &lt;module&gt; from pycuda._driver import * # noqa ImportError: DLL load failed: The specified module could not be found. Any idea what could the problem be??",
        "answers": [
            [
                "The solution involved installing the version of CUDA supported by the library. I had a higher version. The pycuda library's filename can be decomposed as follows to identify the versions of CUDA and Python supported by it- Filename: pycuda-2014.1+cuda6514-cp27-none-win32 Python version supported: 2.7 (from cp27) CUDA version supported: 6.5.14 (from cuda6514)"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have a dictionary and i would like to know if is it possible to use it as a parameter of a kernel. for instance i have the cuda kernel signature __global__ void calTab(Tableaux) Tableaux is a C structure corresponding to typedef struct { float *Tab1; float *Tab2; } Tableaux; in python Tableaux correspond to the dictionary below: Tableaux={} Tableaux[\"Tab1\"]=[] Tableaux[\"Tab2\"]=[] is it possible to use the dictionary as the C structure without using a C API? Thank you in advance",
        "answers": [
            [
                "None of what you are proposing is possible. In PyCUDA, you cannot Pass a dictionary to a kernel Pass a list to a kernel Directly translate a dictionary to C++ structure in device code Directly translate a list to a C++ linear array in device PyCUDA can use Python classes as C++ structures and it has a numpy like array for use on the GPU. So points 3 and 4 are possible, but not as you would like to do them. Both techniques are discussed in the documentation,here for gpuarray and here for structures."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have created a C shared library which is used in python it works perfectly when i test it. I compile like this: gcc -shared -std=c99 -I/usr/include/python2.6 -fPIC -lpython2.6 -opymod.so pymod.c I have a cuda source code \"pymod.cu\" which is the same as \"pymod.c\" and i really don't know how to compile it with nvcc. Thank you in advance.",
        "answers": [
            [
                "You need to first compile using nvcc, then build the shared library using gcc: nvcc -c test.cu -o test.o -Xcompiler -fPIC gcc -shared -fPIC -o libtest.so test.o"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to launch a kernel in pyCUDA and then terminate the kernel by writing to a GPU global memory location. Here is a simple example kernel that I would like to be able to terminate at some point after it enters the infinite while loop: __global__ void countUp(u16 *inShot, u64 *counter) { while(inShot[0]) { counter[0]++; } } From what I have read about streams in CUDA, I should be able to launch this kernel after creating a stream and it will be non-blocking on the host, ie. I should be able to do stuff on the host after this kernel is launched and is running. I compile the above kernel to a cubin file and launch it in pyCUDA like so: import numpy as np from pycuda import driver, compiler, gpuarray, tools # -- initialize the device import pycuda.autoinit strm1 = driver.Stream() h_inShot = np.zeros((1,1)) d_inShot = gpuarray.to_gpu_async(h_inShot.astype(np.uint16), stream = strm1) h_inShot = np.ones((1,1)) h_counter = np.zeros((1,1)) d_counter = gpuarray.to_gpu_async(h_counter.astype(np.uint64), stream = strm1) testCubin = \"testKernel.cubin\" mod = driver.module_from_file(testCubin) countUp = mod.get_function(\"countUp\") countUp(d_inShot, d_counter, grid = (1, 1, 1), block = (1, 1, 1), stream = strm1 ) Running this script causes the kernel to enter an infinite while loop for obvious reasons. Launching this script from the ipython environment does not seem to return control to the host after the kernel launch (I can't input new commands as I guess its waiting for the kernel to finish). I would like control to return to the host so that I can change the value in GPU global memory pointer d_inShot and have the kernel exit the while loop. Is this even possible and if so, how do I do it in pyCUDA? Thanks.",
        "answers": [
            [
                "I figured this out, so am posting my solution. Even though asynchronous memcpy's are non-blocking, I discovered that doing a memcpy using the same stream as a running kernel does not work. My solution was to create another stream: strm2 = driver.Stream() and then change d_inShot like so: d_inShot.set_async(h_inShot.astype(np.uint16), stream = strm2) And this has worked for me."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to make a pycuda wrapper inspired by scikits-cuda library for some operations provided in the new cuSolver library of Nvidia. I want to solve a linear system of the form AX=B by LU factorization, to perform that first use the cublasSgetrfBatched method from scikits-cuda, that give me the factorization LU; then with that factorization I want to solve the system using cusolverDnSgetrs from cuSolve that I want to wrap, when I perform the computation return status 3, the matrices that supose to give me the answer don't change, BUT the *devInfo is zero, looking in the cusolver's documentation says: CUSOLVER_STATUS_INVALID_VALUE=An unsupported value or parameter was passed to the function (a negative vector size, for example). libcusolver.cusolverDnSgetrs.restype=int libcusolver.cusolverDnSgetrs.argtypes=[_types.handle, ctypes.c_char, ctypes.c_int, ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p] \"\"\" handle is the handle pointer given by calling cusolverDnCreate() from cuSolver LU is the LU factoriced matrix given by cublasSgetrfBatched() from scikits P is the pivots matrix given by cublasSgetrfBatched() B is the right hand matix from AX=B \"\"\" def cusolverSolveLU(handle,LU,P,B): rows_LU ,cols_LU = LU.shape rows_B, cols_B = B.shape B_gpu = gpuarray.to_gpu(B.astype('float32')) info_gpu = gpuarray.zeros(1, np.int32) status=libcusolver.cusolverDnSgetrs( handle, 'n', rows_LU, cols_B, int(LU.gpudata), cols_LU, int(P.gpudata), int(B_gpu.gpudata), cols_B, int(info_gpu.gpudata)) print info_gpu print status handle= cusolverCreate() #get the initialization of cusolver LU, P = cublasLUFactorization(...) B = np.asarray(np.random.rand(3, 3), np.float32) cusolverSolveLU(handle,LU,P,B) The output: [0] 3 What I'm doing wrong?",
        "answers": [
            [
                "Here is a full working example of how to use the library; the result is validated against that obtained using numpy's built-in solver: import ctypes import numpy as np import pycuda.autoinit import pycuda.gpuarray as gpuarray CUSOLVER_STATUS_SUCCESS = 0 libcusolver = ctypes.cdll.LoadLibrary('libcusolver.so') libcusolver.cusolverDnCreate.restype = int libcusolver.cusolverDnCreate.argtypes = [ctypes.c_void_p] def cusolverDnCreate(): handle = ctypes.c_void_p() status = libcusolver.cusolverDnCreate(ctypes.byref(handle)) if status != CUSOLVER_STATUS_SUCCESS: raise RuntimeError('error!') return handle.value libcusolver.cusolverDnDestroy.restype = int libcusolver.cusolverDnDestroy.argtypes = [ctypes.c_void_p] def cusolverDnDestroy(handle): status = libcusolver.cusolverDnDestroy(handle) if status != CUSOLVER_STATUS_SUCCESS: raise RuntimeError('error!') libcusolver.cusolverDnSgetrf_bufferSize.restype = int libcusolver.cusolverDnSgetrf_bufferSize.argtypes = [ctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p] def cusolverDnSgetrf_bufferSize(handle, m, n, A, lda, Lwork): status = libcusolver.cusolverDnSgetrf_bufferSize(handle, m, n, int(A.gpudata), n, ctypes.pointer(Lwork)) if status != CUSOLVER_STATUS_SUCCESS: raise RuntimeError('error!') libcusolver.cusolverDnSgetrf.restype = int libcusolver.cusolverDnSgetrf.argtypes = [ctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p] def cusolverDnSgetrf(handle, m, n, A, lda, Workspace, devIpiv, devInfo): status = libcusolver.cusolverDnSgetrf(handle, m, n, int(A.gpudata), lda, int(Workspace.gpudata), int(devIpiv.gpudata), int(devInfo.gpudata)) if status != CUSOLVER_STATUS_SUCCESS: raise RuntimeError('error!') libcusolver.cusolverDnSgetrs.restype = int libcusolver.cusolverDnSgetrs.argtypes = [ctypes.c_void_p, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p] def cusolverDnSgetrs(handle, trans, n, nrhs, A, lda, devIpiv, B, ldb, devInfo): status = libcusolver.cusolverDnSgetrs(handle, trans, n, nrhs, int(A.gpudata), lda, int(devIpiv.gpudata), int(B.gpudata), ldb, int(devInfo.gpudata)) if status != CUSOLVER_STATUS_SUCCESS: raise RuntimeError('error!') if __name__ == '__main__': m = 3 n = 3 a = np.asarray(np.random.rand(m, n), np.float32) a_gpu = gpuarray.to_gpu(a.T.copy()) lda = m b = np.asarray(np.random.rand(m, n), np.float32) b_gpu = gpuarray.to_gpu(b.T.copy()) ldb = m handle = cusolverDnCreate() Lwork = ctypes.c_int() cusolverDnSgetrf_bufferSize(handle, m, n, a_gpu, lda, Lwork) Workspace = gpuarray.empty(Lwork.value, dtype=np.float32) devIpiv = gpuarray.zeros(min(m, n), dtype=np.int32) devInfo = gpuarray.zeros(1, dtype=np.int32) cusolverDnSgetrf(handle, m, n, a_gpu, lda, Workspace, devIpiv, devInfo) if devInfo.get()[0] != 0: raise RuntimeError('error!') CUBLAS_OP_N = 0 nrhs = n devInfo = gpuarray.zeros(1, dtype=np.int32) cusolverDnSgetrs(handle, CUBLAS_OP_N, n, nrhs, a_gpu, lda, devIpiv, b_gpu, ldb, devInfo) x_cusolver = b_gpu.get().T cusolverDnDestroy(handle) x_numpy = np.linalg.solve(a, b) print np.allclose(x_numpy, x_cusolver)"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "i'm tryin to make a pycuda wrapper inspired by scikits-cuda library, for some operations provided in the new cuSolver library of Nvidia, first I need to perfom an LU factorization through cusolverDnSgetrf() op. but before that I need the 'Workspace' argument, the tool that cuSolver provides to get that is named cusolverDnSgetrf_bufferSize(); but when I use it, just crash and return a segmentation-fault. What I'm doing wrong? Note: I have already working this op with scikits-cuda but the cuSolver library use a lot this kind of argument and I want to compare the usage between scikits-cuda and my implementation with the new library. import numpy as np import pycuda.gpuarray import ctypes import ctypes.util libcusolver = ctypes.cdll.LoadLibrary('libcusolver.so') class _types: handle = ctypes.c_void_p libcusolver.cusolverDnCreate.restype = int libcusolver.cusolverDnCreate.argtypes = [_types.handle] def cusolverCreate(): handle = _types.handle() libcusolver.cusolverDnCreate(ctypes.byref(handle)) return handle.value libcusolver.cusolverDnDestroy.restype = int libcusolver.cusolverDnDestroy.argtypes = [_types.handle] def cusolverDestroy(handle): libcusolver.cusolverDnDestroy(handle) libcusolver.cusolverDnSgetrf_bufferSize.restype = int libcusolver.cusolverDnSgetrf_bufferSize.argtypes =[_types.handle, ctypes.c_int, ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p] def cusolverLUFactorization(handle, matrix): m,n=matrix.shape mtx_gpu = gpuarray.to_gpu(matrix.astype('float32')) work=gpuarray.zeros(1, np.float32) status=libcusolver.cusolverDnSgetrf_bufferSize( handle, m, n, int(mtx_gpu.gpudata), n, int(work.gpudata)) print status x = np.asarray(np.random.rand(3, 3), np.float32) handle_solver=cusolverCreate() cusolverLUFactorization(handle_solver,x) cusolverDestroy(handle_solver)",
        "answers": [
            [
                "The last parameter of cusolverDnSgetrf_bufferSize should be a regular pointer, not a GPU memory pointer. Try modifying the cusolverLUFactorization() function as follows: def cusolverLUFactorization(handle, matrix): m,n=matrix.shape mtx_gpu = gpuarray.to_gpu(matrix.astype('float32')) work = ctypes.c_int() status = libcusolver.cusolverDnSgetrf_bufferSize( handle, m, n, int(mtx_gpu.gpudata), n, ctypes.pointer(work)) print status print work.value"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm trying to initialize 2D surface in PyCUDA and fill it with values from NumPy 2D array.The idea, as I get it, is open drv.ArrayDescriptor, create drv.Array using this descriptor, copy data from NumPy array with drv.Memcpy2D, do set_array for SurfaceReference. But still I have pycuda._driver.LogicError at the last step. A minimal example of what I'm doing: import numpy as np import pycuda.driver as drv import pycuda.autoinit from pycuda.compiler import SourceModule mod = SourceModule(\"surface&lt;void, cudaSurfaceType2D&gt; fld_srf;\") def numpy2d_to_array(np_array): h, w = np_array.shape descr = drv.ArrayDescriptor() descr.width = w descr.height = h descr.format = drv.dtype_to_array_format(np_array.dtype) descr.num_channels = 1 descr.flags = 0 device_array = drv.Array(descr) copy = drv.Memcpy2D() copy.set_src_host(np_array) copy.set_dst_array(device_array) copy.width_in_bytes = copy.src_pitch = np_array.strides[0] copy.src_height = copy.height = h copy(aligned=True) return device_array fld = np.random.random_integers(-30, 30, (1920, 1080)).astype(np.int32) srf = mod.get_surfref('fld_srf') srf_arr = numpy2d_to_array(fld.copy()) srf.set_array(srf_arr) The code above throws following exception: Traceback (most recent call last): File \"./testsurface.py\", line 30, in &lt;module&gt; srf.set_array(srf_arr) pycuda._driver.LogicError: cuSurfRefSetArray failed: invalid value Any ideas how to do this correctly? Or at least why this error appears?",
        "answers": [
            [
                "It might have something to do with the flags: in the 3D case, you have to set descr.flags = drv.array3d_flags.SURFACE_LDST to allow binding to the surface according to this. I don't find the 2D equivalent for pycuda though."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm trying to work out a simple program with pycuda to test it and latter compare it to my opencl implementation. Yet, I'm having trouble adding 2 1D arrays. The problem is that I can't seem to find the correct ID of each element. My code is very simple: #!/usr/bin/env python # -*- coding: utf-8 -*- import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np #Host variables a = np.array([[1.0, 2,0 , 3.0]], dtype=np.float32) b = np.array([[4.0, 5,0 , 6.0]], dtype=np.float32) k = np.float32(2.0) #Device Variables a_d = cuda.mem_alloc(a.nbytes) b_d = cuda.mem_alloc(b.nbytes) cuda.memcpy_htod(a_d, a) cuda.memcpy_htod(b_d, b) s_d = cuda.mem_alloc(a.nbytes) m_d = cuda.mem_alloc(a.nbytes) #Device Source mod = SourceModule(\"\"\" __global__ void S(float *s, float *a, float *b) { int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int row = by * blockDim.y + ty; int col = bx * blockDim.x + tx; int dim = gridDim.x * blockDim.x; const int id = row * dim + col; s[id] = a[id] + b[id]; } __global__ void M(float *m, float *a, float k) { int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int row = by * blockDim.y + ty; int col = bx * blockDim.x + tx; int dim = gridDim.x * blockDim.x; const int id = row * dim + col; m[id] = k * a[id]; } \"\"\") #Vector addition func = mod.get_function(\"S\") func(s_d, a_d, b_d, block=(1,3,1)) s = np.empty_like(a) cuda.memcpy_dtoh(s, s_d) #Vector multiplication by constant func = mod.get_function(\"M\") func(m_d, a_d, k, block=(1,3,1)) m = np.empty_like(a) cuda.memcpy_dtoh(m, m_d) print \"Vector Addition\" print \"Expected: \" + str(a+b) print \"Result: \" + str(s) + \"\\n\" print \"Vector Multiplication\" print \"Expected: \" + str(k*a) print \"Result: \" + str(m) My output is: Vector Addition Expected: [[ 5. 7. 0. 9.]] Result: [[ 5. 7. 0. 6.]] Vector Multiplication Expected: [[ 2. 4. 0. 6.]] Result: [[ 2. 4. 0. 6.]] I don't really understand how this Index thing works in CUDA. I've found some documentation online that gave me some insight on how the grids, blocks and threads work, but still, I can't get it to work right. I must be missing something. Every piece of information is dearly appreciated.",
        "answers": [
            [
                "Your indexing seems fine, even if it's a bit overloaded for this small example (it would be enough to consider one dimension). The problem is, that your arrays a and b have 4 elements each. But your kernel functions only operate on the first 3 elements. Therefore is the result of the 4th element not as expected. Did you mean the following? a = np.array([[1.0, 2.0, 3.0]], dtype=np.float32) b = np.array([[4.0, 5.0, 6.0]], dtype=np.float32)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I've installed pycuda on a machine featuring a TESLA C2075. I'm running on Ubuntu 14.04 with the CUDA-6.0 compiler installed. Using python 2.7.9 (via the anaconda distribution) and numpy 1.9.0, I have installed pycuda 2014.1 from the ZIP file that Andreas Kloeckner provides on his website. (http://mathema.tician.de/software/pycuda/) Running the tests provided by that ZIP file goes all well except for the test_cumath.py file. I receive the following error: E AssertionError: (2.3841858e-06, 'cosh', &lt;type 'numpy.complex64'&gt;)` E assert &lt;built-in method all of numpy.bool_ object at 0x7f00747f3880&gt;()` E + where &lt;built-in method all of numpy.bool_ object at 0x7f00747f3880&gt; = 2.3841858e-06 &lt;= 2e-06.all test_cumath.py:54: AssertionError` ===== 1 failed, 27 passed in 12.57 seconds ===== Does anyone have a suggestion where this discrepancy between the GPU and CPU result for cosh comes from? Being just slightly over the tolerance of 2e-6 with that measured value of 2.38e-6 looks a bit weird to me. Especially, since the other tests succeed...?",
        "answers": [
            [
                "In the GPGPU/CUDA community it is indeed known that different hardware platforms and CUDA library versions might yield different results when using the same API. The differences are always small. So, there is some heterogeneity across platforms. Indeed, this makes it tedious to write tests based on numerical results. The classification of right and wrong becomes less sharp and one must answer \"what is good enough?\". One might consider this crazy and in many cases problematic, or even faulty. I think this should not be disputed here. What do you think, where did the 2e-6 tolerance come from in the first place? I'd say someone tried to find a trade-off between how much of a variance he/she thought is sufficiently correct and how much of a variance he/she expected, practically. In the CPU world 2e-6 is already large. Hence, here someone chose a large tolerance in order to account for an expected degree of heterogeneity among GPU platforms. In this case, practically, it probably means that the tolerance has not been chosen to reflect the real-world heterogeneity of GPU platforms. Having said this, the GPGPU community is also aware of the fact that an incredible amount of GPU cards is flaky (broken, basically). Before running serious applications, GPU cards must be exhaustively tested. Especially, a GPU card should produce reproducible results. Fluctuations are an indicator for a broken card. The Teslas usually are not affected as much as consumer cards, but even there we have seen it. Do you have a second GPU card of the same type? Does it produce the same results? Either you identify your GPU card as \"broken\" (by comparison with other cards of the same type) or you should submit a bug report to PyCUDA and tell them about the tolerance that does not suffice."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This question already has answers here: How do I install theano in Anaconda ver. 2.1 Windows 64 bit for Python 3.4? (3 answers) Closed 6 years ago. This Theano Installation is making me mad :( So, I've followed the instructions here on the most voted answer because it seemed like the most similar condiguration from mine and up-to-date version : Installing theano on Windows 8 with GPU enabled 1- I've installed Cuda v6.5, launched deviceQuery and it works fine. 2- I already have Visual Studio 2013 so I haven't installed Visual Studio 2010 3- &gt; At the time of writing, Theano on GPU only allows working with 32-bit floats and is primarily built for 2.7 version of Python. So i don't know exactly what is the current state now but I have a friend with the same configuration than mine and he managed to make it work so I guess it's possible. I've installed Python through Anaconda. 4- I've installed MinGW and Cygwin 5- I've fixed msvc9compiler.py 6- Here's the bottleneck : the PyCUDA Installation Here's what I've done: - I've used cygwin to extract the pycuda tar file - I've executed python configure.py through VS2013 x64 Native Tools Command Prompt than configured siteconfig.py as followed: BOOST_INC_DIR = [] BOOST_LIB_DIR = [] BOOST_COMPILER = 'gcc43' USE_SHIPPED_BOOST = True BOOST_PYTHON_LIBNAME = ['boost_python'] BOOST_THREAD_LIBNAME = ['boost_thread'] CUDA_TRACE = False CUDA_ROOT = 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v6.5' CUDA_ENABLE_GL = False CUDA_ENABLE_CURAND = True CUDADRV_LIB_DIR = ['${CUDA_ROOT}/lib', '${CUDA_ROOT}/lib/x64'] CUDADRV_LIBNAME = ['cuda'] CUDART_LIB_DIR = ['${CUDA_ROOT}/lib', '${CUDA_ROOT}/lib/x64'] CUDART_LIBNAME = ['cudart'] CURAND_LIB_DIR = ['${CUDA_ROOT}/lib', '${CUDA_ROOT}/lib/x64'] CURAND_LIBNAME = ['curand'] CXXFLAGS = ['/EHsc', '-DBOOST_NO_TYPEID'] LDFLAGS = ['/FORCE'] I've executed python setup.py build --compiler=\"msvc\" through VS2013 x64 Native Tools Command Prompt I've executed python setup.py install through VS2013 x64 Native Tools Command Prompt When I execute the little test in python, here's what's happening: PS C:\\users\\jmm\\desktop&gt; python test.py Vendor: Continuum Analytics, Inc. Package: mkl Message: trial mode expires in 29 days Traceback (most recent call last): File \"test.py\", line 7, in &lt;module&gt; a_doubled = (2*a_gpu).get() File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\gpuarray.py\", line 471, in __rm ul__ return self._axpbz(scalar, 0, result) File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\gpuarray.py\", line 333, in _axp bz func = elementwise.get_axpbz_kernel(self.dtype, out.dtype) File \"&lt;string&gt;\", line 2, in get_axpbz_kernel File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\tools.py\", line 423, in context _dependent_memoize result = func(*args) File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\elementwise.py\", line 417, in g et_axpbz_kernel \"axpb\") File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\elementwise.py\", line 157, in g et_elwise_kernel arguments, operation, name, keep, options, **kwargs) File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\elementwise.py\", line 143, in g et_elwise_kernel_and_types keep, options, **kwargs) File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\elementwise.py\", line 71, in ge t_elwise_module options=options, keep=keep) File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\compiler.py\", line 251, in __in it__ arch, code, cache_dir, include_dirs) File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\compiler.py\", line 241, in comp ile return compile_plain(source, options, keep, nvcc, cache_dir) File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\compiler.py\", line 73, in compi le_plain checksum.update(preprocess_source(source, options, nvcc).encode(\"utf-8\")) File \"C:\\Users\\jmm\\Anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg\\pycuda\\compiler.py\", line 52, in prepr ocess_source cmdline, stderr=stderr) pycuda.driver.CompileError: nvcc preprocessing of c:\\users\\jmm\\appdata\\local\\temp\\tmp32jnzb.cu failed [command: nvcc --preprocess -arch sm_30 -m64 -Ic:\\users\\jmm\\anaconda\\lib\\site-packages\\pycuda-2014.1-py2.7-win-amd64.egg \\pycuda\\cuda c:\\users\\jmm\\appdata\\local\\temp\\tmp32jnzb.cu --compiler-options -EP] [stderr: tmp32jnzb.cu 'C:\\Program' n'est pas reconnu en tant que commande interne ou externe, un programme ex\u00e9cutable ou un fichier de commandes. ] Could you please tell me why the hell this doesn't work ?",
        "answers": [
            [
                "I have written a practical guide for the whole process: https://my6266blog.wordpress.com/2015/01/21/installing-theano-pylearn2-and-even-gpu-on-windows/ Good luck! It's not that complicated, just follow the steps one by one."
            ],
            [
                "You probably need to add the path to the executables for Visual Studio in your nvcc.profile (you can find it in your CUDA bin folder. On my system: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v6.5\\bin). In my case, since I have Visual studio 2010, I added at the end of nvcc.profile: \"compiler-bindir = C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin\\amd64\""
            ],
            [
                "Actually you don't need to install pycuda to get theano working on your windows machine. I'm not an expert but I have Theano installed on Windows 8.1. This is my laptop config: 64-bit, nvcc/cuda 6.5, Python 2.7.9, WinPython-64bit-2.7.9.3, Windows 8.1, VS2013 and two graphic units (Intel HD Graphics 4600 and NVIDIA GeForce GT 750M)."
            ],
            [
                "The process was a pretty big hassle, so here is a tutorial for anyone that is interested: All of this has been tested using a clean install of Windows 8.1, with nothing else on it, though it should work fine if you don't have a clean install because this will install all the required versions of the software for you. You need 64-bit windows, 32 bit will not work. You will also need a CUDA compatible graphics card, so if you don\u2019t have one you\u2019re stuck for now, sadly. This means that you need a relatively modern NVIDIA graphics card, AMD will not work (it can run OpenCL but not CUDA because CUDA is lame and proprietary). I am installing this on Windows 8.1, but I suspect it should all still work on windows 7 as well. First download WinPython, (make sure to get python 2.7, version 2.7.10.3, this link points to there) and install it TO A PATH THAT DOES NOT HAVE SPACES IN IT. OTHERWISE THINGS BREAK. I made an Other folder in C:\\ (C:\\Other) and then made a folder named Python27 (C:\\Other\\Python27) and tell the installer to install it in there. Once it is done installing, you will need to add it to your path. Press the windows key and type environment variables, then click \u201cEdit the System Environment Variables\u201d, click Environment Variables in the windows that pops up, scroll down to Path, and then append C:\\Other\\Python27\\python-2.7.10.amd64 Or wherever else you installed WinPython to Then add a semicolon after it, so you get C:\\Other\\Python27\\python-2.7.10.amd64; This is how you add a specific path to the Path variables, in the future, I will just say to add it here and now give specific steps about how to do that. Note that if you update the system path, the current command prompt windows that are open won\u2019t get that update, and you will have to open a new command prompt window to actually have it use the new path. The purpose of a path is so your command prompt window knows where programs are, because if you call, say python in the command prompt, it will look through every folder in your path until it finds a python.exe. If it can\u2019t find any, it will get angry as it typically would if that program didn\u2019t exist. If you don\u2019t want to clutter your path variable/if your path variable is full, I put a tutorial here about how to make it so your path is just appended to when you open a command prompt window via a text file that stores all the paths, instead of having to edit the environment variable itself, if you are interested. You then need to add C:\\Other\\Python27\\python-2.7.10.amd64\\DLL; C:\\Other\\Python27\\python-2.7.10.amd64\\Scripts; to your path as well (again, or wherever else you installed python. For later on I will just say where I installed it and if you installed it somewhere else it should be pretty easy to just tweak the commands accordingly) Next, install visual studio 2015 community and visual studio 2013, making sure to install all the tools related to c++ development as well (using custom installation, then under Programming Languages). These don\u2019t need to be in a path without spaces, and they probably won\u2019t let you store them anywhere else anyway and that is OK. Add C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin; C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin\\amd64; C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\lib; C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\lib\\amd64; To your system path. Install NOT NEWEST DRIVER BECAUSE THEY ARE UNSTABLE, but instead 355.60 because it is known to be very reliable, and new enough. Then install the CUDA toolkit (it\u2019s also okay to store this in a path with spaces, it probably won\u2019t give you the option either, but even if it does, just let it store it in the default place it wants to store it to). Version 6.5 is needed because version 7 and above aren\u2019t supported by pycuda. If you have a GTX 9__ you will need to download CUDA from here instead. This will probably automatically append C:\\Program Files (x86)\\Windows Kits\\8.1\\Redist\\D3D\\x64; C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v7.5\\bin; C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v7.5\\libnvvp; C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common; To your path, if not you will need to do so now. Those are the only three things that can be stored to paths with spaces (these will be in like Program Files or Program Files (x86)), with everything else be very careful to store them to paths that don\u2019t have spaces. Download the boost binaries (1_55_0 for 64 bit which is the version this link points to), and run the installer, then select to store them to a path without spaces (I stored them to C:\\Other\\boost) Navigate to that directory in the command line, then run bootstrap.bat and then when it is done run .\\b2 This will start building, and take a long time, and use a lot of space (about 6 GB). It will probably say that 8 targets failed, 8 targets were skipped, and 1075 were updated. This is what one should expect, and is not a problem. Install Git-2.7.0-64-bit to some path without a space in it Choose to use Git from the Windows Command Prompt, checkout Windows-style, commit Unix-style endings, use Window\u2019s default console window, and do not enable file system caching. Add C:\\Other\\Git\\bin; To your system path Next, run the installer for VCForPython. Download pycuda source (pycuda-2015.1.3) Navigate inside that directory, then run python configure.py this will create a file named siteconf.py. Open this file, and it should look something like BOOST_INC_DIR = [] BOOST_LIB_DIR = [] BOOST_COMPILER = 'gcc43' USE_SHIPPED_BOOST = True BOOST_PYTHON_LIBNAME = ['boost_python-py27'] BOOST_THREAD_LIBNAME = ['boost_thread'] CUDA_TRACE = False CUDA_ROOT = 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v6.5' CUDA_INC_DIR = ['${CUDA_ROOT}/include'] CUDA_ENABLE_GL = False CUDA_ENABLE_CURAND = True CUDADRV_LIB_DIR = ['${CUDA_ROOT}/lib/Win32', '${CUDA_ROOT}/lib/x64'] CUDADRV_LIBNAME = ['cuda'] CUDART_LIB_DIR = ['${CUDA_ROOT}/lib/Win32', '${CUDA_ROOT}/lib/x64'] CUDART_LIBNAME = ['cudart'] CURAND_LIB_DIR = ['${CUDA_ROOT}/lib/Win32', '${CUDA_ROOT}/lib/x64'] CURAND_LIBNAME = ['curand'] CXXFLAGS = [] LDFLAGS = [] modify it so it looks like: BOOST_INC_DIR = ['C:/Other/boost'] BOOST_LIB_DIR = ['C:/Other/boost/lib64-msvc-12.0'] BOOST_COMPILER = 'msvc' USE_SHIPPED_BOOST = True BOOST_PYTHON_LIBNAME = ['boost_python-vc120-mt-1_55'] BOOST_THREAD_LIBNAME = ['boost_thread-vc110-mt-1_55'] CUDA_TRACE = False CUDA_ROOT = 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v6.5' CUDA_ENABLE_GL = False CUDA_ENABLE_CURAND = True CUDADRV_LIB_DIR = ['${CUDA_ROOT}/lib/Win32', '${CUDA_ROOT}/lib/x64'] CUDADRV_LIBNAME = ['cuda'] CUDART_LIB_DIR = ['${CUDA_ROOT}/lib/Win32', '${CUDA_ROOT}/lib/x64'] CUDART_LIBNAME = ['cudart'] CURAND_LIB_DIR = ['${CUDA_ROOT}/lib/Win32', '${CUDA_ROOT}/lib/x64'] CURAND_LIBNAME = ['curand'] CXXFLAGS = ['/DBOOST_PYTHON_STATIC_LIB', '/EHsc'] LDFLAGS = ['/LIBPATH:C:\\\\Other\\\\boost\\\\/lib64-msvc-12.0', '/FORCE'] then run python setup.py build followed by python setup.py install This should install pycuda for you =) To install Theano (with GPU enabled), download release 0.7, unzip it, navigate inside it using a command prompt, and then type python setup.py install then go and edit system environment variables, and create one named THEANO_FLAGS and set it\u2019s value to device=gpu,floatX=float32 Then open a new command prompt, and if you have completed all of the steps above, this should work well =) You can run the code here to make sure that you are actually running off the GPU."
            ],
            [
                "I was able to get Theano installed on my ASUS K501LX Windows 8.1 laptop, with an NVIDIA GeForce 950M GPU, without any hassle whatsoever. I largely followed Maor's post above from March 29th. I was actually shocked at how easy it was! All I needed was the Community Edition of Visual Studio 2013 and the CUDA 7 Toolkit. I then installed Anaconda 3.4 (I used the latest version that's out there now). The one modification I made to Maor's post was installing mingw, via conda install mingw libpython immediately after installing Anaconda. Also, since I am using Python 3, I had to change the flags parameter in .theanorc.txt to point to C:\\Anaconda3\\libs. Upon importing theano, it returned that it was using my GeForce GTX 950M device, and running the theano\\misc\\check_blas.py check returned no errors and carried out its tests on my GPU. Happy times!"
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001,
            1.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I am using reduction code basically exactly like the examples in the docs. The code below should return 2^3 + 2^3 = 16, but it instead returns 9. What did I do wrong? import numpy import pycuda.reduction as reduct import pycuda.gpuarray as gpuarray import pycuda.autoinit from pycuda.compiler import SourceModule as module newzeros = [{1,2,3},{4,5,6}] gpuSum = reduct.ReductionKernel(numpy.uint64, neutral=\"0\", reduce_expr=\"a+b\", map_expr=\"1 &lt;&lt; x[i]\", arguments=\"int* x\") mylengths = pycuda.gpuarray.to_gpu(numpy.array(map(len,newzeros),dtype = \"uint64\",)) sumfalse = gpuSum(mylengths).get() print sumfalse",
        "answers": [
            [
                "I just figured it out. The argument list used when defining the kernel should be unsigned long *x, not int *x. I was using 64-bit integers everywhere else and it messed it up."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am trying to implement a struct in my Pycuda code but i am getting out of bounds errors. I tried following this tutorial but am unable to get it working for my case. The problem is most probably due to improper use of pointers, e.g. the tutorial shows that the pointer memsize must be allocated rather than the data memsize. Hopefully someone here can give me some insight... Sample code: #!/usr/bin/env python #-*- coding:utf-8 -*- import numpy as np import pycuda.driver as cuda import pycuda.tools as tools import pycuda.autoinit from mako.template import Template from pycuda.compiler import SourceModule src_template = Template( \"\"\" struct Dist { %for s in xrange(ns): float *dist${s}; %endfor }; // return linear index based on x,y coordinate __device__ int get_index(int xcoord, int ycoord) { return ycoord + xcoord * ${ny}; }; __global__ void initialize(float *rho, float *ux, float *uy, Dist *ftmp) { int idx; float dens, velx, vely, vv, ev; for (int y = threadIdx.x + blockIdx.x * blockDim.x; y &lt; ${ny}; y += blockDim.x * gridDim.x) { for (int x = threadIdx.y + blockIdx.y * blockDim.y; x &lt; ${nx}; x += blockDim.y * gridDim.y) { if ((x &gt; 0) &amp;&amp; (x &lt; ${nx-1}) &amp;&amp; (y &gt; 0) &amp;&amp; (y &lt; ${ny-1})) { idx = get_index(x,y); dens = rho[idx]; velx = ux[idx]; vely = uy[idx]; vv = velx*velx + vely*vely; %for s in xrange(ns): // s = ${s}; \\vec{e}[${s}] = [${ex[s]},${ey[s]}] ev = ${float(ex[s])}f*velx + ${float(ey[s])}f*vely; ftmp-&gt;dist${s}[idx] = ${w[s]}f*dens*(1.0f+3.0f*ev+4.5f*ev*ev-1.5f*vv); %endfor } } } } \"\"\" ) class channelFlow: # initialize channelFlow def __init__(self, nx, ny): self.nx, self.ny = nx, ny max_threads_per_block = tools.DeviceData().max_threads self.blocksize = (ny if ny&lt;32 else 32, nx if nx&lt;32 else 32, 1) # threads per block self.gridsize = (ny/self.blocksize[0], nx/self.blocksize[1], 1) # blocks per grid self.ns = 9 self.w = np.array([4./9, 1./9, 1./9, 1./9, 1./9, 1./36, 1./36, 1./36, 1./36]) self.ex = np.array([0, 1, -1, 0, 0, 1, -1, -1, 1]) self.ey = np.array([0, 0, 0, 1, -1, 1, 1, -1, -1]) self.ctx = { 'nx': self.nx, 'ny': self.ny, 'ns': self.ns, 'w': self.w, 'ex': self.ex, 'ey': self.ey } dtype = np.float32 self.ftmp = np.zeros([self.nx,self.ny,self.ns]).astype(dtype) self.rho = np.zeros([self.nx,self.ny]).astype(dtype) self.ux = np.zeros([self.nx,self.ny]).astype(dtype) self.uy = np.zeros([self.nx,self.ny]).astype(dtype) self.ftmp_gpu = cuda.mem_alloc(self.ftmp.nbytes) self.rho_gpu = cuda.mem_alloc(self.rho.nbytes) self.ux_gpu = cuda.mem_alloc(self.ux.nbytes) self.uy_gpu = cuda.mem_alloc(self.uy.nbytes) def run(self): src = src_template.render(**self.ctx) code = SourceModule(src) initialize = code.get_function('initialize') self.rho[:,:] = 1. self.ux[:,:] = 0. self.uy[:,:] = 0. cuda.memcpy_htod(self.rho_gpu, self.rho) cuda.memcpy_htod(self.ux_gpu, self.ux) cuda.memcpy_htod(self.uy_gpu, self.uy) initialize( self.rho_gpu, self.ux_gpu, self.uy_gpu, self.ftmp_gpu, block=self.blocksize, grid=self.gridsize ) if __name__ == \"__main__\": sim = channelFlow(64,64); sim.run()",
        "answers": [
            [
                "I was able to properly implement a structure of arrays in pycuda using the GPUStruct python module available here and by fixing the improper use of pointers such that: ftmp-&gt;dist${s}[idx] = ${w[s]}f*dens*(1.0f+3.0f*ev+4.5f*ev*ev-1.5f*vv); was changed to: float *ftmp${s}_ptr = ftmp-&gt;dist${s}; ftmp${s}_ptr[idx] = ${w[s]}f*dens*(1.0f+3.0f*ev+4.5f*ev*ev-1.5f*vv); The revised code which shows the details of GPUStruct implementation: #!/usr/bin/env python #-*- coding:utf-8 -*- import numpy as np import pycuda.driver as cuda import pycuda.tools as tools import pycuda.autoinit from gpu_struct import GPUStruct from mako.template import Template from pycuda.compiler import SourceModule src_template = Template( \"\"\" struct Dist { %for s in xrange(ns): float *dist${s}; %endfor }; // return linear index based on x,y coordinate __device__ int get_index(int xcoord, int ycoord) { return ycoord + xcoord * ${ny}; }; __global__ void initialize(float *rho, float *ux, float *uy, Dist *ftmp) { int idx; float dens, velx, vely, vv, ev; for (int y = threadIdx.x + blockIdx.x * blockDim.x; y &lt; ${ny}; y += blockDim.x * gridDim.x) { for (int x = threadIdx.y + blockIdx.y * blockDim.y; x &lt; ${nx}; x += blockDim.y * gridDim.y) { if ((x &gt; 0) &amp;&amp; (x &lt; ${nx-1}) &amp;&amp; (y &gt; 0) &amp;&amp; (y &lt; ${ny-1})) { idx = get_index(x,y); dens = rho[idx]; velx = ux[idx]; vely = uy[idx]; vv = velx*velx + vely*vely; %for s in xrange(ns): // s = ${s}; \\vec{e}[${s}] = [${ex[s]},${ey[s]}] float *ftmp${s}_ptr1 = ftmp-&gt;dist${s}; ev = ${float(ex[s])}f*velx + ${float(ey[s])}f*vely; ftmp${s}_ptr1[idx] = ${w[s]}f*dens*(1.0f+3.0f*ev+4.5f*ev*ev-1.5f*vv); %endfor } } } } \"\"\" ) class channelFlow: # initialize channelFlow def __init__(self, nx, ny): self.nx, self.ny = nx, ny max_threads_per_block = tools.DeviceData().max_threads self.blocksize = (ny if ny&lt;32 else 32, nx if nx&lt;32 else 32, 1) # threads per block self.gridsize = (ny/self.blocksize[0], nx/self.blocksize[1], 1) # blocks per grid self.ns = 9 self.w = np.array([4./9, 1./9, 1./9, 1./9, 1./9, 1./36, 1./36, 1./36, 1./36]) self.ex = np.array([0, 1, -1, 0, 0, 1, -1, -1, 1]) self.ey = np.array([0, 0, 0, 1, -1, 1, 1, -1, -1]) self.ctx = { 'nx': self.nx, 'ny': self.ny, 'ns': self.ns, 'w': self.w, 'ex': self.ex, 'ey': self.ey } dtype = np.float32 self.ftmp = np.zeros([self.nx,self.ny,self.ns]).astype(dtype) self.rho = np.zeros([self.nx,self.ny]).astype(dtype) self.ux = np.zeros([self.nx,self.ny]).astype(dtype) self.uy = np.zeros([self.nx,self.ny]).astype(dtype) self.ftmp_gpu = GPUStruct([ (np.float32,'*dist0', self.ftmp[:,:,0]), (np.float32,'*dist1', self.ftmp[:,:,1]), (np.float32,'*dist2', self.ftmp[:,:,2]), (np.float32,'*dist3', self.ftmp[:,:,3]), (np.float32,'*dist4', self.ftmp[:,:,4]), (np.float32,'*dist5', self.ftmp[:,:,5]), (np.float32,'*dist6', self.ftmp[:,:,6]), (np.float32,'*dist7', self.ftmp[:,:,7]), (np.float32,'*dist8', self.ftmp[:,:,8]) ]) self.rho_gpu = cuda.mem_alloc(self.rho.nbytes) self.ux_gpu = cuda.mem_alloc(self.ux.nbytes) self.uy_gpu = cuda.mem_alloc(self.uy.nbytes) def run(self): src = src_template.render(**self.ctx) code = SourceModule(src) initialize = code.get_function('initialize') self.rho[:,:] = 1. self.ux[:,:] = 0. self.uy[:,:] = 0. self.ftmp_gpu.copy_to_gpu() cuda.memcpy_htod(self.rho_gpu, self.rho) cuda.memcpy_htod(self.ux_gpu, self.ux) cuda.memcpy_htod(self.uy_gpu, self.uy) initialize( self.rho_gpu, self.ux_gpu, self.uy_gpu, self.ftmp_gpu.get_ptr(), block=self.blocksize, grid=self.gridsize ) self.dens = np.zeros_like(self.rho) cuda.memcpy_dtoh(self.dens, self.rho_gpu) print self.dens if __name__ == \"__main__\": sim = channelFlow(64,64); sim.run()"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to implement in Python the following pattern for multi-CPU and single-GPU computation using pycuda and pyfft packages. I would like to have several processes (e.g. launched with multiprocessing.Pool()), with each of them able to perform FFTs using the GPU (using NVIDIA CUDA). However, I have the following problem: If I run too many processes or too many FFTs per process, the overall script remains on hold without terminating (and without computing all the FFTs that are due). From further investigations I suppose this is due to the memory limit on the GPU (currently 2048MB on NVIDIA GeForce GT 750M). It seems that the multiprocessing pool is not able to acquire the control back. Is there any way to avoid this? Since each process requires less than 2048 MB, I would like to have something like a queue where each process can book the usage of the GPU and, when a process releases the context, the next process in the queue starts using it. Is this doable? Alternatively, is it possible to force that only one process uses the GPU at a given time? I have tried separately these solutions but they do not work (or probably I have not implemented them correctly): synchronize the stream, with proc_stream.synchronize() clear context cache, with pycuda.tools.clear_context_caches() change the compute mode, with cuda.compute_mode = cuda.compute_mode.EXCLUSIVE Note: The solution 2. seems to free some memory, but it makes the computation way slower, and does not solve the problem: e.g. increasing the number of ffts to be computed, the script shows the same behaviour. Here the code. To start from a simple task, here each process computes 1 FFT (then one can use batch option in execute() to do more FFTs in a row). import multiprocessing import pycuda.driver as cuda import pycuda.gpuarray as gpuarray from pycuda.tools import make_default_context from pyfft.cuda import Plan def main(): # generates simple matrix, (e.g. image with a signal at the center) size = 4096 center = size/2 in_matrix = np.zeros((size, size), dtype='complex64') in_matrix[center:center+2, center:center+2] = 10. pool_size = 4 # integer up to multiprocessing.cpu_count() pool = multiprocessing.Pool(processes=pool_size) func = FuncWrapper(in_matrix, size) nffts = 16 # total number of ffts to be computed par = np.arange(nffts) results = pool.map(func, par) pool.close() pool.join() print results And here the function wrapper: class FuncWrapper(object): def __init__(self, matrix, size): self.in_matrix = matrix self.size = size print(\"Func initialized with matrix size=%i\" % size) def __call__(self, par): proc_id = multiprocessing.current_process().name # take control over the GPU cuda.init() context = make_default_context() device = context.get_device() proc_stream = cuda.Stream() # move data to GPU # multiplication self.in_matrix*par is just to have each process computing # different matrices in_map_gpu = gpuarray.to_gpu(self.in_matrix*par) # create Plan, execute FFT and get back the result from GPU plan = Plan((self.size, self.size), dtype=np.complex64, fast_math=False, normalize=False, wait_for_finish=True, stream=proc_stream) plan.execute(in_map_gpu, wait_for_finish=True) result = in_map_gpu.get() # free memory on GPU del in_map_gpu mem = np.array(cuda.mem_get_info())/1.e6 print(\"%s free=%f\\ttot=%f\" % (proc_id, mem[0], mem[1])) # release context context.pop() return par Now, with nffts=16 and pool_size=4 the script terminates correctly and gives this output: Func initialized with matrix size=4096 PoolWorker-1 free=1481.019392 tot=2147.024896 PoolWorker-2 free=1331.011584 tot=2147.024896 PoolWorker-3 free=1181.003776 tot=2147.024896 PoolWorker-4 free=1030.631424 tot=2147.024896 PoolWorker-1 free=881.074176 tot=2147.024896 PoolWorker-2 free=731.746304 tot=2147.024896 PoolWorker-3 free=582.418432 tot=2147.024896 PoolWorker-4 free=433.090560 tot=2147.024896 PoolWorker-1 free=582.754304 tot=2147.024896 PoolWorker-2 free=718.946304 tot=2147.024896 PoolWorker-3 free=881.254400 tot=2147.024896 PoolWorker-4 free=1030.684672 tot=2147.024896 PoolWorker-1 free=868.028416 tot=2147.024896 PoolWorker-2 free=731.713536 tot=2147.024896 PoolWorker-3 free=582.402048 tot=2147.024896 PoolWorker-4 free=433.090560 tot=2147.024896 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] But with nffts=18 and pool_size=4 the script does not terminate and gives this output, remaining stuck at the last line: Func initialized with matrix size=4096 PoolWorker-1 free=1416.392704 tot=2147.024896 PoolWorker-2 free=982.544384 tot=2147.024896 PoolWorker-1 free=1101.037568 tot=2147.024896 PoolWorker-2 free=682.991616 tot=2147.024896 PoolWorker-3 free=815.747072 tot=2147.024896 PoolWorker-4 free=396.918784 tot=2147.024896 PoolWorker-3 free=503.046144 tot=2147.024896 PoolWorker-4 free=397.144064 tot=2147.024896 PoolWorker-1 free=531.361792 tot=2147.024896 PoolWorker-1 free=397.246464 tot=2147.024896 PoolWorker-2 free=518.610944 tot=2147.024896 PoolWorker-2 free=397.021184 tot=2147.024896 PoolWorker-3 free=517.193728 tot=2147.024896 PoolWorker-4 free=397.021184 tot=2147.024896 PoolWorker-3 free=504.336384 tot=2147.024896 PoolWorker-4 free=149.123072 tot=2147.024896 PoolWorker-1 free=283.340800 tot=2147.024896 Many thanks for your help!",
        "answers": [],
        "votes": []
    },
    {
        "question": "want to do this programm on cuda. 1.in \"main.cpp\" struct Center{ double * Data; int dimension; }; typedef struct Center Center; //I allow a pointer on N Center elements by the CUDAMALLOC like follow .... #include \"kernel.cu\" .... center *V_dev; int M =100, n=4; cudaStatus = cudaMalloc((void**)&amp;V_dev,M*sizeof(Center)); Init&lt;&lt;&lt;1,M&gt;&gt;&gt;(V_dev, M, N); //I always know the dimension of N before calling My \"kernel.cu\" file is something like this #include \"cuda_runtime.h\" #include\"device_launch_parameters.h\" ... //other include headers to allow my .cu file to know the Center type definition __global__ void Init(Center *V, int N, int dimension){ V[threadIdx.x].dimension = dimension; V[threadIdx.x].Data = (double*)malloc(dimension*sizeof(double)); for(int i=0; i&lt;dimension; i++) V[threadIdx.x].Data[i] = 0; //For the value, it can be any kind of operation returning a float that i want to be able put here } I'm on visual studio 2008 and CUDA 5.0. When I Build my project, I've got these errors: error: calling a _host_ function(\"malloc\") from a _global_ function(\"Init\") is not allowed. I want to know please how can I perform this? (I know that 'malloc' and other cpu memory allocation are not allowed for device memory.",
        "answers": [
            [
                "malloc is allowed in device code but you have to be compiling for a cc2.0 or greater target GPU. Adjust your VS project settings to remove any GPU device settings like compute_10,sm_10 and replace it with compute_20,sm_20 or higher to match your GPU. (And, to run that code, your GPU needs to be cc2.0 or higher.)"
            ],
            [
                "You need the compiler parameter -arch=sm_20 and a GPU which supports it."
            ]
        ],
        "votes": [
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I am trying to set up a 3D loop with the assignment C(i,j,k) = A(i,j,k) + B(i,j,k) using Python on my GPU. This is my GPU: http://www.geforce.com/hardware/desktop-gpus/geforce-gt-520/specifications The sources I'm looking at / comparing with are: http://nbviewer.ipython.org/gist/harrism/f5707335f40af9463c43 http://nbviewer.ipython.org/github/ContinuumIO/numbapro-examples/blob/master/webinars/2014_06_17/intro_to_gpu_python.ipynb It's possible that I've imported more modules than necessary. This is my code: import numpy as np import numbapro import numba import math from timeit import default_timer as timer from numbapro import cuda from numba import * @autojit def myAdd(a, b): return a+b myAdd_gpu = cuda.jit(restype=f8, argtypes=[f8, f8], device=True)(myAdd) @cuda.jit(argtypes=[float32[:,:,:], float32[:,:,:], float32[:,:,:]]) def myAdd_kernel(a, b, c): tx = cuda.threadIdx.x ty = cuda.threadIdx.y tz = cuda.threadIdx.z bx = cuda.blockIdx.x by = cuda.blockIdx.y bz = cuda.blockIdx.z bw = cuda.blockDim.x bh = cuda.blockDim.y bd = cuda.blockDim.z i = tx + bx * bw j = ty + by * bh k = tz + bz * bd if i &gt;= c.shape[0]: return if j &gt;= c.shape[1]: return if k &gt;= c.shape[2]: return for i in xrange(0,c.shape[0]): for j in xrange(0,c.shape[1]): for k in xrange(0,c.shape[2]): # c[i,j,k] = a[i,j,k] + b[i,j,k] c[i,j,k] = myAdd_gpu(a[i,j,k],b[i,j,k]) def main(): my_gpu = numba.cuda.get_current_device() print \"Running on GPU:\", my_gpu.name cores_per_capability = {1: 8,2: 32,3: 192,} cc = my_gpu.compute_capability print \"Compute capability: \", \"%d.%d\" % cc, \"(Numba requires &gt;= 2.0)\" majorcc = cc[0] print \"Number of streaming multiprocessor:\", my_gpu.MULTIPROCESSOR_COUNT cores_per_multiprocessor = cores_per_capability[majorcc] print \"Number of cores per mutliprocessor:\", cores_per_multiprocessor total_cores = cores_per_multiprocessor * my_gpu.MULTIPROCESSOR_COUNT print \"Number of cores on GPU:\", total_cores N = 100 thread_ct = my_gpu.WARP_SIZE block_ct = int(math.ceil(float(N) / thread_ct)) print \"Threads per block:\", thread_ct print \"Block per grid:\", block_ct a = np.ones((N,N,N), dtype = np.float32) b = np.ones((N,N,N), dtype = np.float32) c = np.zeros((N,N,N), dtype = np.float32) start = timer() cg = cuda.to_device(c) myAdd_kernel[block_ct, thread_ct](a,b,cg) cg.to_host() dt = timer() - start print \"Wall clock time with GPU in %f s\" % dt print 'c[:3,:,:] = ' + str(c[:3,1,1]) print 'c[-3:,:,:] = ' + str(c[-3:,1,1]) if __name__ == '__main__': main() My result from running this is the following: Running on GPU: GeForce GT 520 Compute capability: 2.1 (Numba requires &gt;= 2.0) Number of streaming multiprocessor: 1 Number of cores per mutliprocessor: 32 Number of cores on GPU: 32 Threads per block: 32 Block per grid: 4 Wall clock time with GPU in 1.104860 s c[:3,:,:] = [ 2. 2. 2.] c[-3:,:,:] = [ 2. 2. 2.] When I run the examples in the sources, I see significant speedup. I don't think my example is running properly since the wall clock time is much longer than I would expect. I've modeled this mostly from the \"even bigger speedups with cuda python\" section in the first example link. I believe I've indexed correctly and safely. Maybe the problem is with my blockdim? or griddim? Or maybe I'm using the wrong types for my GPU. I think I read that they must be a certain type. I'm very new to this so the problem very well could be trivial! Any and all help is greatly appreciated!",
        "answers": [
            [
                "You are creating your indexes correctly but then you're ignoring them. Running the nested loop for i in xrange(0,c.shape[0]): for j in xrange(0,c.shape[1]): for k in xrange(0,c.shape[2]): is forcing all your threads to loop through all values in all dimensions, which is not what you want. You want each thread to compute one value in a block and then move on. I think something like this should work better... i = tx + bx * bw while i &lt; c.shape[0]: j = ty+by*bh while j &lt; c.shape[1]: k = tz + bz * bd while k &lt; c.shape[2]: c[i,j,k] = myAdd_gpu(a[i,j,k],b[i,j,k]) k+=cuda.blockDim.z*cuda.gridDim.z j+=cuda.blockDim.y*cuda.gridDim.y i+=cuda.blockDim.x*cuda.gridDim.x Try to compile and run it. Also make sure to validate it, as I have not."
            ],
            [
                "I don't see you using imshow, or show, so there is no need to import those. It doesn't appear as though you use your import of math (I didn't see any calls of math.some_function. Your imports from numba and numbapro seem repetitive. Your \"from numba import cuda\" overrides your \"from numbapro import cuda\", since it is subsequent to it. Your calls to cuda use the cuda in numba not numbapro. When you call \"from numba import *\", you import everything from numba, not just cuda, which seems to be the only thing you use. Also, (I believe) import numba.cuda is equivalent to from numba import cuda. Why not eliminate all your imports from numba and numbapro with a single \"from numba import cuda\"."
            ]
        ],
        "votes": [
            4.0000001,
            -0.9999999
        ]
    },
    {
        "question": "Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers. We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations. Closed 4 years ago. Improve this question Question: Is there an emulator for a Geforce card that would allow me to program and test CUDA without having the actual hardware? Info: I'm looking to speed up a few simulations of mine in CUDA, but my problem is that I'm not always around my desktop for doing this development. I would like to do some work on my netbook instead, but my netbook doesn't have a GPU. Now as far as I know, you need a CUDA capable GPU to run CUDA. Is there a way to get around this? It would seem like the only way is a GPU emulator (which obviously would be painfully slow, but would work). But whatever way there is to do this I would like to hear. I'm programming on Ubuntu 10.04 LTS.",
        "answers": [
            [
                "This response may be too late, but it's worth noting anyway. GPU Ocelot (of which I am one of the core contributors) can be compiled without CUDA device drivers (libcuda.so) installed if you wish to use the Emulator or LLVM backends. I've demonstrated the emulator on systems without NVIDIA GPUs. The emulator attempts to faithfully implement the PTX 1.4 and PTX 2.1 specifications which may include features older GPUs do not support. The LLVM translator strives for correct and efficient translation from PTX to x86 that will hopefully make CUDA an effective way of programming multicore CPUs as well as GPUs. -deviceemu has been a deprecated feature of CUDA for quite some time, but the LLVM translator has always been faster. Additionally, several correctness checkers are built into the emulator to verify: aligned memory accesses, accesses to shared memory are properly synchronized, and global memory dereferencing accesses allocated regions of memory. We have also implemented a command-line interactive debugger inspired largely by gdb to single-step through CUDA kernels, set breakpoints and watchpoints, etc... These tools were specifically developed to expedite the debugging of CUDA programs; you may find them useful. Sorry about the Linux-only aspect. We've started a Windows branch (as well as a Mac OS X port) but the engineering burden is already large enough to stress our research pursuits. If anyone has any time and interest, they may wish to help us provide support for Windows! Hope this helps. [1]: GPU Ocelot - https://code.google.com/archive/p/gpuocelot/ [2]: Ocelot Interactive Debugger - http://forums.nvidia.com/index.php?showtopic=174820"
            ],
            [
                "For those who are seeking the answer in 2016 (and even 2017) ... Disclaimer I've failed to emulate GPU after all. It might be possible to use gpuocelot if you satisfy its list of dependencies. I've tried to get an emulator for BunsenLabs (Linux 3.16.0-4-686-pae #1 SMP Debian 3.16.7-ckt20-1+deb8u4 (2016-02-29) i686 GNU/Linux). I'll tell you what I've learnt. nvcc used to have a -deviceemu option back in CUDA Toolkit 3.0 I downloaded CUDA Toolkit 3.0, installed it and tried to run a simple program: #include &lt;stdio.h&gt; __global__ void helloWorld() { printf(\"Hello world! I am %d (Warp %d) from %d.\\n\", threadIdx.x, threadIdx.x / warpSize, blockIdx.x); } int main() { int blocks, threads; scanf(\"%d%d\", &amp;blocks, &amp;threads); helloWorld&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(); cudaDeviceSynchronize(); return 0; } Note that in CUDA Toolkit 3.0 nvcc was in the /usr/local/cuda/bin/. It turned out that I had difficulties with compiling it: NOTE: device emulation mode is deprecated in this release and will be removed in a future release. /usr/include/i386-linux-gnu/bits/byteswap.h(47): error: identifier \"__builtin_bswap32\" is undefined /usr/include/i386-linux-gnu/bits/byteswap.h(111): error: identifier \"__builtin_bswap64\" is undefined /home/user/Downloads/helloworld.cu(12): error: identifier \"cudaDeviceSynchronize\" is undefined 3 errors detected in the compilation of \"/tmp/tmpxft_000011c2_00000000-4_helloworld.cpp1.ii\". I've found on the Internet that if I used gcc-4.2 or similarly ancient instead of gcc-4.9.2 the errors might disappear. I gave up. gpuocelot The answer by Stringer has a link to a very old gpuocelot project website. So at first I thought that the project was abandoned in 2012 or so. Actually, it was abandoned few years later. Here are some up to date websites: GitHub; Project's website; Installation guide. I tried to install gpuocelot following the guide. I had several errors during installation though and I gave up again. gpuocelot is no longer supported and depends on a set of very specific versions of libraries and software. You might try to follow this tutorial from July, 2015 but I don't guarantee it'll work. I've not tested it. MCUDA The MCUDA translation framework is a linux-based tool designed to effectively compile the CUDA programming model to a CPU architecture. It might be useful. Here is a link to the website. CUDA Waste It is an emulator to use on Windows 7 and 8. I've not tried it though. It doesn't seem to be developed anymore (the last commit is dated on Jul 4, 2013). Here's the link to the project's website: https://code.google.com/archive/p/cuda-waste/ CU2CL Last update: 12.03.2017 As dashesy pointed out in the comments, CU2CL seems to be an interesting project. It seems to be able to translate CUDA code to OpenCL code. So if your GPU is capable of running OpenCL code then the CU2CL project might be of your interest. Links: CU2CL homepage CU2CL GitHub repository"
            ],
            [
                "You can check also gpuocelot project which is a true emulator in the sense that PTX (bytecode in which CUDA code is converted to) will be emulated. There's also an LLVM translator, it would be interesting to test if it's more fast than when using -deviceemu."
            ],
            [
                "The CUDA toolkit had one built into it until the CUDA 3.0 release cycle. I you use one of these very old versions of CUDA, make sure to use -deviceemu when compiling with nvcc."
            ],
            [
                "https://github.com/hughperkins/cuda-on-cl lets you run NVIDIA\u00ae CUDA\u2122 programs on OpenCL 1.2 GPUs (full disclosure: I'm the author)"
            ],
            [
                "Be careful when you're programming using -deviceemu as there are operations that nvcc will accept while in emulation mode but not when actually running on a GPU. This is mostly found with device-host interaction. And as you mentioned, prepare for some slow execution."
            ],
            [
                "GPGPU-Sim is a GPU simulator that can run CUDA programs without using GPU. I created a docker image with GPGPU-Sim installed for myself in case that is helpful."
            ]
        ],
        "votes": [
            45.0000001,
            44.0000001,
            36.0000001,
            14.0000001,
            11.0000001,
            3.0000001,
            1.0000001
        ]
    },
    {
        "question": "I've installed python+pycuda (and other libraries) through this link: http://wiki.tiker.net/PyCuda/Installation/Linux But when I run test program, it says: Traceback (most recent call last): File \"test_driver.py\", line 17, in &lt;module&gt; import pycuda.gpuarray as gpuarray File \"/usr/local/lib/python2.7/dist-packages/pycuda-2014.1-py2.7-linux-x86_64.egg/pycuda/gpuarray.py\", line 3, in &lt;module&gt; import pycuda.elementwise as elementwise File \"/usr/local/lib/python2.7/dist-packages/pycuda-2014.1-py2.7-linux-x86_64.egg/pycuda/elementwise.py\", line 34, in &lt;module&gt; from pytools import memoize_method File \"/usr/local/lib/python2.7/dist-packages/pytools-2014.3.5-py2.7.egg/pytools/__init__.py\", line 5, in &lt;module&gt; from six.moves import range, zip, intern, input ImportError: cannot import name intern six is installed. I don't know what should I do!",
        "answers": [
            [
                "I was seeing this exact same issue on Ubuntu 14.04 but didn't want to override Ubuntu's version of six due to lots of finicky dependency issues. I thought it was odd that the pytools version in the error message (2014.3.5) didn't match the version from the Ubuntu apt repo (2013.5.7). It turns out that I had previously tried to install pycuda from source by checking out the git repository. I had also previously installed pip. Since pytools is listed as a requirement in pycuda's setup.py, pip installed its version of pytools (2014.3.5). And that's where the incompatibility between six and pytools originates. To solve the issue, I uninstalled both pycuda and pytools using pip remove pycuda pytools and then installed pycuda using apt-get which then auto-installed the compatible version of pytools. Just wanted to post this as an alternative solution in case anyone else prefers to keep the default Ubuntu version of six."
            ],
            [
                "On a OSX system I manage to solve the problem by upgrading the six package via pip. Namely $ pip install six --upgrade"
            ],
            [
                "I had the same error on Ubuntu 14.04 but neither of the tips above worked. This page recommends editing the file causing the error directly. So I edited /usr/local/lib/python2.7/dist-packages/pytools/__init__.py and changed the line: from six.moves import range, zip, intern, input Into try: from six.moves import range, zip, intern, input except ImportError: from six.moves import range, zip, input Not nice editing included library files, but it got around the error."
            ]
        ],
        "votes": [
            3.0000001,
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm developing a genetic cellular automata using PyCuda. Each cell will have a lot of genome data, along with cell parameters. I'm wondering what could be a most efficient way to 1) pass cells data to a CUDA kernel, then 2) to process this data. I began with one particularly bad (imo), yet still working solution. It was passing each parameter in a separate array, then process them with a switch-case and a lot of duplicate code. Then, realized that I could quickly end up with pretty large number of parameters per kernel function, and decide to rewrite it. Second solution was to store all bunch of cell's parameters in a single array with extra dimension. That was much more elegant in code, but surprisingly the code runs 10x slower! To make it more clear, the full list of data I need to be stored per cell: (Fc, Mc, Tc): 3x (int) - the cell's current 'flavor', mass and temperature (Rfc, Rmc, Rtc): 3x (int) - the cell's current registers (Fi, Mi, Ti) for each neighbour: 8*3x (int) - incoming values (Rfi, Rmi, Rti) for each neighbour: 8*3x (int) - incoming values gate orientation: 1x (uchar) execution pointer: 1x (uchar) current micro-operations memory: 32x (uchar) last step's micro-operations memory: 32x (uchar) I'm splitting an automata step in 2 phases. First (emit phase) is calculating (Fi, Mi, Ti) for each cell neighbours. Second (absorb phase) is blending 8x(Fi, Mi, Ti) values with current cells' states. No genome or registers implemented yet, but I need its data to be passed for future. So, the code for my first solution was: Mk = 64 Tk = 1000 emit_gpu = ElementwiseKernel(\"int3 *cells, int3 *dcells0, int3 *dcells1, int3 *dcells2, int3 *dcells3, int3 *dcells4, int3 *dcells5, int3 *dcells6, int3 *dcells7, int w, int h\", \"\"\" int x = i / h; int y = i % h; int3 cell = cells[i]; float M = (float) cell.y; float T = (float) cell.z; int Mi = (int) (fmin(1, T / Tk) * M); cells[i].y -= Mi; cells[i].z -= (int) (T * fmin(1, T / Tk) / 1); int Fi = cell.x; int Mbase = Mi / 8; int Mpart = Mi % 8; int Madd; int Ti = cell.z; int ii, xo, yo; for (int cc = 0; cc &lt; 9; cc++) { int c = (cc + Fi) % 9; if (c == 4) continue; xo = x + c%3 - 1; if (xo &lt; 0) xo = w + xo; if (xo &gt;= w) xo = xo - w; yo = y + c/3 - 1; if (yo &lt; 0) yo = h + yo; if (xo &gt;= w) yo = yo - h; ii = xo * h + yo; if (Mpart &gt; 0) { Madd = 1; Mpart--;} else Madd = 0; switch(c) { case 0: dcells0[ii] = make_int3(Fi, Mbase + Madd, Ti); break; case 1: dcells1[ii] = make_int3(Fi, Mbase + Madd, Ti); break; case 2: dcells2[ii] = make_int3(Fi, Mbase + Madd, Ti); break; case 3: dcells3[ii] = make_int3(Fi, Mbase + Madd, Ti); break; case 5: dcells4[ii] = make_int3(Fi, Mbase + Madd, Ti); break; case 6: dcells5[ii] = make_int3(Fi, Mbase + Madd, Ti); break; case 7: dcells6[ii] = make_int3(Fi, Mbase + Madd, Ti); break; case 8: dcells7[ii] = make_int3(Fi, Mbase + Madd, Ti); break; default: break; } } \"\"\", \"ca_prepare\", preamble=\"\"\" #define Tk %s \"\"\" % Tk) absorb_gpu = ElementwiseKernel(\"int3 *cells, int3 *dcells0, int3 *dcells1, int3 *dcells2, int3 *dcells3, int3 *dcells4, int3 *dcells5, int3 *dcells6, int3 *dcells7, int *img, int w, int h\", \"\"\" int3 cell = cells[i]; int3 dcell = dcells0[i]; cell = cell + calc_d(cell.x, cell.y, cell.z, dcell.x, dcell.y, dcell.z); cell.x = cell.x % 360; if (cell.x &lt; 0) cell.x += 360; dcell = dcells1[i]; cell = cell + calc_d(cell.x, cell.y, cell.z, dcell.x, dcell.y, dcell.z); cell.x = cell.x % 360; if (cell.x &lt; 0) cell.x += 360; if (cell.z &gt; Tk) cell.z = Tk; dcell = dcells2[i]; cell = cell + calc_d(cell.x, cell.y, cell.z, dcell.x, dcell.y, dcell.z); cell.x = cell.x % 360; if (cell.x &lt; 0) cell.x += 360; if (cell.z &gt; Tk) cell.z = Tk; dcell = dcells3[i]; cell = cell + calc_d(cell.x, cell.y, cell.z, dcell.x, dcell.y, dcell.z); cell.x = cell.x % 360; if (cell.x &lt; 0) cell.x += 360; if (cell.z &gt; Tk) cell.z = Tk; dcell = dcells4[i]; cell = cell + calc_d(cell.x, cell.y, cell.z, dcell.x, dcell.y, dcell.z); cell.x = cell.x % 360; if (cell.x &lt; 0) cell.x += 360; if (cell.z &gt; Tk) cell.z = Tk; dcell = dcells5[i]; cell = cell + calc_d(cell.x, cell.y, cell.z, dcell.x, dcell.y, dcell.z); cell.x = cell.x % 360; if (cell.x &lt; 0) cell.x += 360; if (cell.z &gt; Tk) cell.z = Tk; dcell = dcells6[i]; cell = cell + calc_d(cell.x, cell.y, cell.z, dcell.x, dcell.y, dcell.z); cell.x = cell.x % 360; if (cell.x &lt; 0) cell.x += 360; if (cell.z &gt; Tk) cell.z = Tk; dcell = dcells7[i]; cell = cell + calc_d(cell.x, cell.y, cell.z, dcell.x, dcell.y, dcell.z); cell.x = cell.x % 360; if (cell.x &lt; 0) cell.x += 360; if (cell.z &gt; Tk) cell.z = Tk; cells[i] = cell; img[i] = hsv2rgb(cell); \"\"\", \"ca_calc\", preamble=\"\"\" #include &lt;math.h&gt; #define Mk %s #define Tk %s __device__ int3 operator+(const int3 &amp;a, const int3 &amp;b) { return make_int3(a.x+b.x, a.y+b.y, a.z+b.z); } __device__ int3 calc_d(int Fc, int Mc, int Tc, int Fi, int Mi, int Ti) { int dF = Fi - Fc; if (dF &gt; 180) Fc += 360; if (dF &lt; -180) Fc -= 360; float sM = Mi + Mc; if (sM != 0) sM = Mi / sM; else sM = 0; dF = (int) (Fi - Fc) * sM; int dM = Mi; int dT = fabs((float) (Fi - Fc)) * fmin((float) Mc, (float) Mi) / Mk + (Ti - Tc) * sM; return make_int3(dF, dM, dT); } __device__ uint hsv2rgb(int3 pixel) { // skipped for brevity } \"\"\" % (Mk, Tk, RAM)) The second and current solution: Mk = 64 Tk = 1000 CELL_LEN = 120 # number of parameters per cell emit_gpu = ElementwiseKernel(\"int *cells, int w, int h\", \"\"\" int x = i / h; int y = i % h; int ii = i * CN; int Fc = cells[ii]; int Mc = cells[ii+1]; int Tc = cells[ii+2]; float M = (float) Mc; float T = (float) Tc; int Mi = (int) (fmin(1, T / Tk) * M); cells[ii+1] = Mc - Mi; cells[ii+2] = Tc - (int) (T * fmin(1, T / Tk)); int Mbase = Mi / 8; int Mpart = Mi % 8; int Madd; int iii, xo, yo; for (int cc = 0; cc &lt; 9; cc++) { int c = (cc + Fc) % 9; if (c == 4) continue; xo = x + c%3 - 1; if (xo &lt; 0) xo = w + xo; else if (xo &gt;= w) xo = xo - w; yo = y + c/3 - 1; if (yo &lt; 0) yo = h + yo; else if (xo &gt;= w) yo = yo - h; if (Mpart &gt; 0) { Madd = 1; Mpart--;} else Madd = 0; if (c &gt; 4) c--; iii = (xo * h + yo) * CN + 6 + c*3; cells[iii] = Fc; cells[iii+1] = Mbase + Madd; cells[iii+2] = Tc; } \"\"\", \"ca_emit\", preamble=\"\"\" #define Tk %s #define CN %s \"\"\" % (Tk, CELL_LEN)) absorb_gpu = ElementwiseKernel(\"int *cells, int *img, int w, int h\", \"\"\" int ii = i * CN; int Fc = cells[ii]; int Mc = cells[ii+1]; int Tc = cells[ii+2]; for (int c=0; c &lt; 8; c++){ int iii = ii + c * 3 + 6; int Fi = cells[iii]; int Mi = cells[iii+1]; int Ti = cells[iii+2]; int dF = Fi - Fc; if (dF &gt; 180) Fc += 360; if (dF &lt; -180) Fc -= 360; float sM = Mi + Mc; if (sM != 0) sM = Mi / sM; else sM = 0; dF = (int) (Fi - Fc) * sM; int dM = Mi; int dT = fabs((float) (Fi - Fc)) * fmin((float) Mc, (float) Mi) / Mk + (Ti - Tc) * sM; Fc += dF; Mc += dM; Tc += dT; Fc = Fc % 360; if (Fc &lt; 0) Fc += 360; if (Tc &gt; Tk) Tc = Tk; } cells[ii] = Fc; cells[ii+1] = Mc; cells[ii+2] = Tc; cells[ii+18] = (cells[ii+18] + 1) % 8; img[i] = hsv2rgb(Fc, Tc, Mc); \"\"\", \"ca_absorb\", preamble=\"\"\" #include &lt;math.h&gt; #define Mk %s #define Tk %s #define CN %s __device__ uint hsv2rgb(int hue, int sat, int val) { // skipped for brevity } \"\"\" % (Mk, Tk, CELL_LEN)) Both variants produce exactly the same CA behaviour, but latter is running much slower. GTX Titan: Field size: 1900x1080 cells Solution #1: ~200 steps/s Solution #2: ~20 steps/s GT 630M: Field size: 1600x900 cells Solution #1: ~7.8 steps/s Solution #2: ~1.5 steps/s Please feel free to play with both solutions' if you need: Solution #1 full source Solution #2 full source Any clues or advises are welcome: Why the performance is slowed down? Is it possible to raise the performance of solution #2 at least to the level of #1? Or another solution would be better?",
        "answers": [
            [
                "OK, I managed how to run second solution almost 15x faster. Following changes were made: Convert main parameters array from int to int4. This made it even faster than solution with int3. Although, extra space left unused (.w dimension). [3x speedup] Repack related parameters in WIDTHxHEIGHT groups. So, shape changed from (WIDTH, HEIGHT, N) to (N, WIDTH, HEIGHT). This made memory access more efficient, since elements inside groups tends to be processed together. [5x speedup] The optimized code looks like: Mk = 64 Tk = 1000 emit_gpu = ElementwiseKernel(\"int4 *cells, int w, int h, int cn\", \"\"\" int x = i / h; int y = i % h; int4 cell = cells[i]; int Fc = cell.x; int Mc = cell.y; int Tc = cell.z; float M = (float) Mc; float T = (float) Tc; int Mi = (int) (fmin(1, T / Tk) * M); cells[i] = make_int4(Fc, Mc - Mi, Tc - (int) (T * fmin(1, T / Tk)), 0); int Mbase = Mi / 8; int Mpart = Mi % 8; int Madd; int ii; int xo, yo; int cnn = 0; for (int dx = -1; dx &lt; 2; dx++) { xo = x + dx; if (xo &lt; 0) xo = w + xo; else if (xo &gt;= w) xo = xo - w; for (int dy = -1; dy &lt; 2; dy++) { if (dx == 0 &amp;&amp; dy == 0) continue; cnn += cn; yo = y + dy; if (yo &lt; 0) yo = h + yo; else if (yo &gt;= h) yo = yo - h; if (Mpart &gt; 0) { Madd = 1; Mpart--;} else Madd = 0; ii = (xo * h + yo) + cnn; cells[ii] = make_int4(Fc, Mbase + Madd, Tc, 0); } } \"\"\", \"ca_emit\", preamble=\"\"\" #define Tk %s #define CN %s \"\"\" % (Tk, CELL_LEN)) absorb_gpu = ElementwiseKernel(\"int4 *cells, int *img, int w, int h, int cn\", \"\"\" int ii = i; int4 cell = cells[i]; int Fc = cell.x; int Mc = cell.y; int Tc = cell.z; for (int c=0; c &lt; 8; c++){ ii += cn; cell = cells[ii]; int Fi = cell.x; int Mi = cell.y; int Ti = cell.z; int dF = Fi - Fc; if (dF &gt; 180) Fc += 360; if (dF &lt; -180) Fc -= 360; float sM = Mi + Mc; if (sM != 0) sM = Mi / sM; else sM = 0; dF = (int) (Fi - Fc) * sM; int dM = Mi; int dT = fabs((float) (Fi - Fc)) * fmin((float) Mc, (float) Mi) / Mk + (Ti - Tc) * sM; Fc += dF; Mc += dM; Tc += dT; Fc = Fc % 360; if (Fc &lt; 0) Fc += 360; if (Tc &gt; Tk) Tc = Tk; } cells[i] = make_int4(Fc, Mc, Tc, 0); img[i] = hsv2rgb(Fc, Tc, Mc); \"\"\", \"ca_absorb\", preamble=\"\"\" #include &lt;math.h&gt; #define Mk %s #define Tk %s __device__ uint hsv2rgb(int hue, int sat, int val) { // skipped for brevity } \"\"\" % (Mk, Tk)) Thanks to Park Young-Bae for clues on repacking and also to Alexey Shchepin for some optimization issues."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have designed a web site using Struts2. Now I have to call a function where image processing will be done. For that I have chosen to use pycuda. Can any one tell me steps and dependencies for installing pycuda. (I have to call this code from an action file). Does I need Jython also? I am using centOS.",
        "answers": [
            [
                "I'm not sure what image processing means here, but if you need to exploit the GPU to perform your calculations with Cuda, and you are using Java, then why using pycuda that is for Python ? Use JCuda or Cuda4J - the J is there to help you :)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I know how to create a global device function inside Host using np.array or np.zeros or np.empty(shape, dtype) and then using cuda.to_device to copy. Also, one can declare shared array as cuda.shared.array(shape, dtype) But how to create an array of constant size in the register of a particular thread inside gpu function. I tried cuda.device_array or np.array but nothing worked. I simply want to do this inside a thread - x = array(CONSTANT, int32) # should make x for each thread",
        "answers": [
            [
                "Numbapro supports numba.cuda.local.array(shape, type) for defining thread local arrays. As with CUDA C, whether than array is defined in local memory or register is a compiler decision based on usage patterns of the array. If the indexing pattern of the local array is statically defined and there is sufficient register space, the compiler will use registers to store the array. Otherwise it will be stored in local memory. See this question and answer pair for more information."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am currently using pycuda and scikits.cuda to solve linear equation A*x = b, where A is an upper/lower matrix. However the cublasStbsv routine requires a specific format. To give an example: if a lower matrix A = [[1, 0, 0], [2, 3, 0], [4, 5, 6]], then the input required by cublasStbsv should be [[1, 3, 6], [2, 5, 0], [4, 0, 0]], where rows are diagonal, subdiagonal1, subdiagonal2, respectively. If using numpy, this can be easily done by stride_tricks.as_strided, but I dont know how to do similar things with pycuda.gpuarray. Any help would be appreciated, thanks. I found pycuda.compyte.array.as_strided, but it cannot be applied to gpuarray.",
        "answers": [
            [
                "I got it done by using theano. First converted it to cudandarray, change stride and make a copy back to gpuarray. Just be careful about changes between Fortran and C order. update: finally got it done by using gpuarray.multi_take_put def make_triangle(s_matrix, uplo = 'L'): \"\"\"convert triangle matrix to the specific format required by cublasStbsv, matrix should be in Fortran order, s_matrix: gpuarray \"\"\" #make sure the dytpe is float32 if s_matrix.dtype != 'f': s_matrix = s_matrix.astype('f') dim = s_matrix.shape[0] if uplo == 'L': idx_tuple = np.tril_indices(dim) gidx = gpuarray.to_gpu(idx_tuple[0] + idx_tuple[1] * dim) gdst = gpuarray.to_gpu(idx_tuple[0] + idx_tuple[1] * (dim - 1)) return gpuarray.multi_take_put([s_matrix], gdst, gidx, (dim, dim))[0] else: idx_tuple = np.triu_indices(dim) gidx = gpuarray.to_gpu(idx_tuple[0] + idx_tuple[1] * dim) gdst = gpuarray.to_gpu(idx_tuple[0] + (idx_tuple[1] + 1) * (dim - 1)) return gpuarray.multi_take_put([s_matrix], gdst, gidx, (dim, dim))[0]"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I just want to call this code (sort algorithm provided by thrust) from python #include &lt;thrust/device_vector.h&gt; #include &lt;thrust/sort.h&gt; #include &lt;thrust/random.h&gt; #include &lt;iostream&gt; #include &lt;iomanip&gt; void initialize(thrust::device_vector&lt;int&gt;&amp; v) { thrust::default_random_engine rng(123456); thrust::uniform_int_distribution&lt;int&gt; dist(10, 99); // mean 10 std 99 for(size_t i = 0; i &lt; v.size(); i++) v[i] = dist(rng); } int main (void){ size_t N = 16; thrust::device_vector&lt;int&gt; keys(N); initialize(keys); print(keys); thrust::sort(keys.begin(), keys.end()); return 0; } So I found this example ThrustInterop.py. And with that I got problems by setting the toolchain properly so I looked under /usr/lib/python2.7/config/Makefile CC= clang CXX= clang++ CFLAGS= $(BASECFLAGS) -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 $(OPT) $(EXTRA_CFLAGS) CPPFLAGS= -I. -IInclude -I$(srcdir)/Include LDFLAGS= -Wl,-F. CONFIG_ARGS= '--prefix=/usr' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-dependency-tracking' '--enable-ipv6' '--with-system-expat' '--with-threads' '--enable-framework=/System/Library/Frameworks' '--enable-toolbox-glue' '--with-system-ffi' '--with-gcc=clang' 'CC=clang' 'CXX=clang++' 'CFLAGS=-g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32' 'LDFLAGS=-Wl,-F.' I tried changing that to: CC= gcc CXX= g++ CFLAGS= $(BASECFLAGS) -g -O2 -I/Users/Leo/local2/include -arch x86_64 -isysroot /Developer/SDKs/MacOSX10.5.sdk $(OPT) $(EXTRA_CFLAGS) CFLAGS= $(BASECFLAGS) -g -O2 -arch x86_64 -arch i386 -mmacosx-version-min=10.7 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.8.sdk $(OPT) $(EXTRA_CFLAGS) CFLAGS= $(BASECFLAGS) -g -O2 -arch x86_64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.7.sdk $(OPT) $(EXTRA_CFLAGS) CFLAGS= $(BASECFLAGS) -g -O2 -m32 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.7.sdk $(OPT) $(EXTRA_CFLAGS) CFLAGS= $(BASECFLAGS) -g -O2 -m64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.7.sdk $(OPT) $(EXTRA_CFLAGS) CPPFLAGS= -I. -IInclude -I$(srcdir)/Include -arch x86_64 -arch i386 -mmacosx-version-min=10.7 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.8.sdk LDFLAGS= -I/Users/Leo/local2/lib -arch x86_64 -isysroot /Developer/SDKs/MacOSX10.5.sdk LDFLAGS= -F/Library/Frameworks -framework CUDA -arch x86_64 -arch i386 -mmacosx-version-min=10.7 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.8.sdk LDFLAGS= -F/Library/Frameworks -framework CUDA -arch x86_64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.7.sdk LDFLAGS= -F/Library/Frameworks -framework CUDA -m32 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.7.sdk LDFLAGS= -F/Library/Frameworks -framework CUDA -m64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.7.sdk No matter what values I set, I moved from this: clang++ -fno-strict-aliasing -fno-common -dynamic -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -Wall -Wshorten-64-to-32 -g -Os -Wall -Wstrict-prototypes -pipe -framework CoreFoundation -bundle -undefined dynamic_lookup -Wl,-F. -u _PyMac_Error /System/Library/Frameworks/Python.framework/Versions/2.7/Python -DENABLE_DTRACE -DMACOSX -DNDEBUG -DNDEBUG -DENABLE_DTRACE -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -I/Users/Leo/pool2/include/ -I/usr/local/cuda//include /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/d2580b0673e96f4f0ae5c4039d0bc056/module.o /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/008d68d74904dace31665477f04b638b/gpu.o -L/System/Library/Frameworks/Python.framework/Versions/2.7/lib -L/Users/Leo/pool2/lib -L/usr/local/cuda//lib -lcuda -lcudart -lboost_python -lpython2.7 -ldl -o /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/d2580b0673e96f4f0ae5c4039d0bc056/codepy.temp.d2580b0673e96f4f0ae5c4039d0bc056.008d68d74904dace31665477f04b638b.module.so ld: warning: ignoring file /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/008d68d74904dace31665477f04b638b/gpu.o, file was built for i386 which is not the architecture being linked (x86_64): /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/008d68d74904dace31665477f04b638b/gpu.o Traceback (most recent call last): File \"ThrustInterop.py\", line 87, in &lt;module&gt; module = nvcc_mod.compile(gcc_toolchain, nvcc_toolchain, debug=True) File \"/Library/Python/2.7/site-packages/codepy/cuda.py\", line 108, in compile mod_name, **kwargs) File \"/Library/Python/2.7/site-packages/codepy/jit.py\", line 435, in link_extension return load_dynamic(mod_name, destination) ImportError: dlopen(/var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/d2580b0673e96f4f0ae5c4039d0bc056/codepy.temp.d2580b0673e96f4f0ae5c4039d0bc056.008d68d74904dace31665477f04b638b.module.so, 2): Library not loaded: libboost_python.dylib Referenced from: /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/d2580b0673e96f4f0ae5c4039d0bc056/codepy.temp.d2580b0673e96f4f0ae5c4039d0bc056.008d68d74904dace31665477f04b638b.module.so Reason: image not found To this: g++ -fno-strict-aliasing -g -O2 -g -fwrapv -O3 -Wall -framework CoreFoundation -bundle -undefined dynamic_lookup -u _PyMac_Error -DNDEBUG -I/Users/Leo/local2/include/python2.7 -I/Users/Leo/pool2/include/ -I/usr/local/cuda//include /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/5d0930e84f00eda6484e6423f88b81ae/module.o /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/0d69721d4d56892cb0c92be07d9ccc1d/gpu.o -L/Users/Leo/local2/lib -L/Users/Leo/pool2/lib -L/usr/local/cuda//lib -lcuda -lcudart -lboost_python -lpython2.7 -ldl -o /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/5d0930e84f00eda6484e6423f88b81ae/codepy.temp.5d0930e84f00eda6484e6423f88b81ae.0d69721d4d56892cb0c92be07d9ccc1d.module.so ld: warning: ignoring file /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/0d69721d4d56892cb0c92be07d9ccc1d/gpu.o, file was built for i386 which is not the architecture being linked (x86_64): /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/0d69721d4d56892cb0c92be07d9ccc1d/gpu.o Traceback (most recent call last): File \"ThrustInterop.py\", line 87, in &lt;module&gt; module = nvcc_mod.compile(gcc_toolchain, nvcc_toolchain, debug=True) File \"/Users/Leo/local2/lib/python2.7/site-packages/codepy/cuda.py\", line 95, in compile mod_name, **kwargs) File \"/Users/Leo/local2/lib/python2.7/site-packages/codepy/jit.py\", line 435, in link_extension return load_dynamic(mod_name, destination) ImportError: dlopen(/var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/5d0930e84f00eda6484e6423f88b81ae/codepy.temp.5d0930e84f00eda6484e6423f88b81ae.0d69721d4d56892cb0c92be07d9ccc1d.module.so, 2): Library not loaded: libboost_python.dylib Referenced from: /var/folders/n4/l3cggyqs1jj1_ltxl8v3dljh0000gn/T/codepy-compiler-cache-v5-uid501/5d0930e84f00eda6484e6423f88b81ae/codepy.temp.5d0930e84f00eda6484e6423f88b81ae.0d69721d4d56892cb0c92be07d9ccc1d.module.so Reason: image not found This is my .aksetup-defaults.py BOOST_COMPILER = 'gcc42' BOOST_INC_DIR = ['${HOME}/pool2/include/'] BOOST_LIB_DIR = ['${HOME}/pool2/lib'] BOOST_PYTHON_LIBNAME = ['boost_python'] BOOST_THREAD_LIBNAME = ['boost_thread'] CUDADRV_LIBNAME = ['cuda'] CUDADRV_LIB_DIR = ['${CUDA_ROOT}/lib'] CUDART_LIBNAME = ['cudart'] CUDART_LIB_DIR = ['${CUDA_ROOT}/lib'] CUDA_ENABLE_CURAND = True CUDA_ENABLE_GL = False CUDA_INC_DIR = ['${CUDA_ROOT}/include'] CUDA_ROOT = '/usr/local/cuda/' CUDA_TRACE = False CURAND_LIBNAME = ['curand'] CURAND_LIB_DIR = ['${CUDA_ROOT}/lib'] USE_SHIPPED_BOOST = True Do you know the simplest way to call this C code from Python?",
        "answers": [],
        "votes": []
    },
    {
        "question": "Here's the code: import pycuda.autoinit import pycuda.gpuarray as gpuarray import pycuda.driver as drv import numpy as np import scikits.cuda.linalg as culinalg import scikits.cuda.misc as cumisc culinalg.init() ag = gpuarray.to_gpu(np.random.rand(1000,1000)) bg = gpuarray.to_gpu(np.zeros((1000,1000)) bg[:,:] = ag I got the following error: TypeError: 'GPUArray' object does not support item assignment So any way to assign a matrix to another existing matrix in pycuda/ scikits.cuda ?",
        "answers": [
            [
                "Ok, this is not an elegant way, but a solution: Use the ElementwiseKernel of pycuda: import from pycuda.elementwise import ElementwiseKernel ele_assign = ElementwiseKernel(\"double *a,double *b\",\"a[i] = b[i]\",\"ele_assign\") ele_assign(bg,ag)"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Closed. This question needs to be more focused. It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 8 years ago. Improve this question For my science fair project I have to write a computationally-intensive algorithm that is well suited to parallelization. I have read about OpenCL and CUDA and it seems they are mainly used from C/C++. While it would not be that difficult for me to pick up a bit of C to write a simple main, I was wondering how big the performance hit would be if I used Java or Python bindings for my GPU computation? Specifically, I was more interested in the performance hit using CUDA because that's the framework I'm planning on using.",
        "answers": [
            [
                "In general, every time you add an abstraction layer you're loosing performance but, in the case of CUDA this is not completely true because, whether using Python or Java you'll end up writing your CUDA kernels on C/Fortran, so the performance in the GPU side will be the same as using C/Fortran (check some pyCUDA examples here) The bad news it that Java and Python will never achieve the performance of compiled languages such as C on certain tasks, see this SO answer for a more detailed discussion about this topic. Here is a good discussion about C versus Java, also on SO. There are many questions and discussions about performance comparison between interpreted and compiled languages, so I encourage you to read some of them."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am currently learning programming with GPU to improve the performance of machine learning algorithms. Initially I try to learn programming cuda with pure c, then I found pycuda which to me a wrapper of cuda library, and then I found theano and pylearn2 and got a little confused: I understand them in this way: pycuda: python wrapper for cuda library theano: similar to numpy but transparent to GPU and CPU pylearn2: deep learning package which build on theano and implemented several machine learning/deep learning model Since I am new to GPU programming, should I start learning from C/C++ implementation or starting from pycuda is enough, even starting from theano? E.g. I would like to implement randomForest model after learning the GPU programming.Thanks.",
        "answers": [
            [
                "Your understand is almost right. I would just add some remarks about Theano. It's much more than a Numpy which can run on the GPU. Theano is indeed is math expression compiler, which translates symbolic math expressions in highly optimized C/CUDA code, targeted for both CPU and GPU. The code it generates is often much more efficient than the one most programmers would write. Theano also can make symbolic differentiation (very useful for gradient based optimization) and has also a feature to achieve better numerical stability (which probably is something useful, though I don't know for real to what extent). It's very likely Theano will be enough to implement what you need. If you still decide to learn CUDA or PyCUDA, choose the one based no the language you will use, C++ or Python."
            ]
        ],
        "votes": [
            9.0000001
        ]
    },
    {
        "question": "Judging by past StackOverflow questions (e.g., here and here), it seems that one should be able to profile PyCUDA programs using nvvp. When I tried running nvvp in CUDA 6.5 on a PyCUDA script that runs without any problems from the command line, profiling failed with the following error: Unable to profile application. org.eclipse.core.rntime.CoreException: Reference to undefined variable args Has anyone successfully used nvvp 6.5 with PyCUDA? I'm using PyCUDA 2014.1 manually built against CUDA 6.5 on Ubuntu 14.04. The script itself has executable permissions and #!/usr/bin/env python at the top, and I specified the script path as the file to run.",
        "answers": [
            [
                "Apparently, nvvp can get confused by certain bash functions in one's environment (see this post for another example). I found the function by searching through the output of env and unset it using unset -f."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I wanted to check the difference in speed between cumath and ElementwiseKernel, since this example shows that Elementwise can perform faster than cumath. I am testing a different operation, where I would guess that Elementwise would be the faster method. import pycuda.autoinit import pycuda.driver as drv from pycuda import gpuarray from pycuda import cumath from pycuda.elementwise import ElementwiseKernel import numpy as np start = drv.Event() end = drv.Event() N = 10**6 a = 2*np.ones(N,dtype=np.float64) start.record() np.exp(a) end.record() end.synchronize() secs = start.time_till(end)*1e-3 print \"Numpy\",secs a_gpu = gpuarray.to_gpu(a) b_gpu = gpuarray.zeros_like(a_gpu) kernel = ElementwiseKernel( \"double *a,double *b\", \"b[i] = exp(a[i]);\", \"kernel\") start.record() # start timing kernel(a_gpu,b_gpu) end.record() # end timing end.synchronize() secs = start.time_till(end)*1e-3 print \"Kernel\",secs start.record() cumath.exp(a_gpu) end.record() end.synchronize() secs = start.time_till(end)*1e-3 print \"Cumath\", secs The first time I run it I get: Numpy 0.022 Kernel 0.143 Cumath 0.147 The second run in the same Python interpreter: Numpy 0.021 Kernel 0.138 Cumath 0.002 I understand that ElementwiseKernel and cumath are both slow on the first run, but I don't understand why ElementwiseKernel doesn't get any faster on the second run.",
        "answers": [],
        "votes": []
    },
    {
        "question": "I've been trying to access the Nvidia Performance Primitives library through Python, and I found a very useful tutorial last updated in 2011 at this site: http://openvidia.sourceforge.net/index.php/OpenVIDIA/python However, after downloading the CUDA 6.0 toolkit I can't seem to find any CUDA \".dll\" files at all (like those referenced near the start of the tutorial). Thanks to responses on here, I know the file names should be different to those in the tutorial, but I can't find any. Does anybody know an alternative method or command to import the library? Any help would be greatly appreciated, and if I've missed any key details then please let me know. Board: Jetson TK1 OS: L4T Ubuntu 14.04 (from https://developer.nvidia.com/jetson-tk1-support) Language: Python 2.7",
        "answers": [
            [
                "I just used the cdll.LoadLibrary() command from the ctypes library and called the \"libnppi.so\" and \"libcudart.so\" files. Worked perfectly, thanks for the help!"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am having some trouble with my Django/Celery/PyCuda setup. I am using PyCuda for some image processing on a Amazon EC2 G2 instance. Here is the info on my Cuda-capable GRID K520 card: Detected 1 CUDA Capable device(s) Device 0: \"GRID K520\" CUDA Driver Version / Runtime Version 6.0 / 6.0 CUDA Capability Major/Minor version number: 3.0 Total amount of global memory: 4096 MBytes (4294770688 bytes) ( 8) Multiprocessors, (192) CUDA Cores/MP: 1536 CUDA Cores GPU Clock rate: 797 MHz (0.80 GHz) Memory Clock rate: 2500 Mhz Memory Bus Width: 256-bit L2 Cache Size: 524288 bytes Maximum Texture Dimension Size (x,y,z) 1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096) Maximum Layered 1D Texture Size, (num) layers 1D=(16384), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(16384, 16384), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 2 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device PCI Bus ID / PCI location ID: 0 / 3 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.0, CUDA Runtime Version = 6.0, NumDevs = 1, Device0 = GRID K520 Result = PASS I am using a pretty out-of-the-box celery config. I have a set of tasks defined in utils/tasks.py, which are tested and work before attempting to use PyCuda. I installed PyCuda via pip. At the top of the file that I am having trouble with, I do my standard imports: from celery import task # other imports import os try: import Image except Exception: from PIL import Image import time #Cuda imports import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule import numpy A remote server initiates a task, which follows this basic workflow: @task() def photo_function(photo_id,...): print 'Got photo...' ... Do some stuff ... result = do_photo_manipulation(photo_id) return result def do_photo_manipulation(photo_id): im = Image.open(inPath) px = numpy.array(im) px = px.astype(numpy.float32) d_px = cuda.mem_alloc(px.nbytes) ... (Do stuff with the pixel array) ... return new_image This works if I run it in shell plus (ie, ./manage.py shell_plus) and if I run it as a standalone, outside-of-django-and-celery process. It's only in this context it fails, with the error: cuMemAlloc failed: not initialized I have looked at other solutions for a while, and tried putting the import statement to do the initialization in the function itself. I have also plugged in a wait() statement, to ensure it's not a problem of the gpu being ready to do work. Here is an answer that suggests the error comes from not importing pycuda.autoinit, which I have done: http://comments.gmane.org/gmane.comp.python.cuda/1975 Any help here would be appreciated! If I need to provide any more information, just let me know! EDIT: Here is the test code: def CudaImageShift(imageIn, mode = \"luminosity\" , log = 0): if log == 1 : print (\"----------&gt; CUDA CONVERSION\") # print \"ENVIRON: \" # import os # print os.environ print 'AUTOINIT' print pycuda.autoinit print 'Making context...' context = make_default_context() print 'Context created.' totalT0 = time.time() print 'Doing test run...' a = numpy.random.randn(4,4) a = a.astype(numpy.float32) print 'Test mem alloc' a_gpu = cuda.mem_alloc(a.nbytes) print 'MemAlloc complete, test mem copy' cuda.memcpy_htod(a_gpu, a) print 'memcopy complete' [2014-07-15 14:52:20,469: WARNING/Worker-1] cuDeviceGetCount failed: not initialized",
        "answers": [
            [
                "I believe the problem you experience is related to CUDA contexts. As of CUDA 4.0 a CUDA context is required per process and per device. Behind the scenes celery will spawn processes for the task workers. When a process/task starts it will not have a context available. In pyCUDA the context creation happens in the autoinit module. That's why your code will work if you run it as a standalone (no extra process is created and the context is valid) or if you put the import autoinit inside the CUDA task (Now the process/task will have a context, I believe you tried that already). If you want to avoid the import you may be able to use the make_default_context from pycuda.tools although I'm not very familiar with pyCUDA and how it handles context management. from pycuda.tools import make_default_context @task() def photo_function(photo_id,...): ctx = make_default_context() print 'Got photo...' ... Do some stuff ... result = do_photo_manipulation(photo_id) return result Beware that context creation is an expensive process. CUDA deliberately front loads a lot of work in the context in order to avoid non expected delays later on. That's why you have a stack of contexts that you can push/pop between host threads (but not between processes). If your kernel code is very fast you may experience delays because of the context create/destroy procedure."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm having a very weird problem with my program. Essentially I'm doing a matrix multiplication on part of a matrix. The program apparently runs fine on most cards cards but crashes on sm_35 Kepler (=GK110) cards. The initial program was written in PyCUDA, but I've since managed to boil it down to the following minimal example written in C: #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; #include &lt;cuda.h&gt; #include &lt;cuda_runtime.h&gt; #include &lt;cublas_v2.h&gt; int main(int argc, char **argv) { cudaError_t status; cublasStatus_t status_blas; CUresult status_drv; float *A = 0; float *B = 0; float *C = 0; float alpha = 1.0f; float beta = 0.0f; float *oldA, *oldB, *oldC; cublasHandle_t handle; int n = 131; int m = 2483; int k = 3; int i; CUcontext ctx; cuInit(0); status_drv = cuCtxCreate(&amp;ctx, 0, 0); if (status_drv != CUDA_SUCCESS) { fprintf(stderr, \"!!!! Context creation error: %d\\n\", status); return EXIT_FAILURE; } status_blas = cublasCreate(&amp;handle); if (status_blas != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \"!!!! CUBLAS initialization error\\n\"); return EXIT_FAILURE; } for (i = 0; i &lt; 5; ++i) { printf(\"Iteration %d\\n\", i); if (cudaMalloc((void **)&amp;B, m * k * sizeof(B[0])) != cudaSuccess) { fprintf(stderr, \"!!!! allocation error (allocate B)\\n\"); return EXIT_FAILURE; } if (cudaMalloc((void **)&amp;C, m * m * sizeof(C[0])) != cudaSuccess) { fprintf(stderr, \"!!!! allocation error (allocate C)\\n\"); return EXIT_FAILURE; } if (cudaMalloc((void **)&amp;A, n * m * sizeof(A[0])) != cudaSuccess) { fprintf(stderr, \"!!!! allocation error (allocate A)\\n\"); return EXIT_FAILURE; } int s = 3; float * A_slice = A + 128*m; status_blas = cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, m, s, &amp;alpha, A_slice, m, B, k, &amp;beta, C, m); if (status_blas != CUBLAS_STATUS_SUCCESS) { fprintf(stderr, \"!!!! kernel execution error.\\n\"); return EXIT_FAILURE; } if (i == 0) { oldA = A; oldB = B; oldC = C; } else if (i == 1) { status = cudaFree(oldA); if (status != cudaSuccess) { fprintf(stderr, \"!!!! allocation error (free A, %d)\\n\", status); return EXIT_FAILURE; } if (cudaFree(oldB) != cudaSuccess) { fprintf(stderr, \"!!!! allocation error (free B)\\n\"); return EXIT_FAILURE; } if (cudaFree(oldC) != cudaSuccess) { fprintf(stderr, \"!!!! allocation error (free C)\\n\"); return EXIT_FAILURE; } } } cublasDestroy(handle); cuCtxDestroy(ctx); return 0; } I only free memory in the 2nd iteration of the for loop to mimic the behavior of the original python program. The program will crash in the 2nd iteration of the for-loop when it tries to free \"A\", with cudaFree returning a cudaErrorIllegalAddress error. Concretely, the was tried on the following cards: NVS 5400M -&gt; no issues GTX560Ti -&gt; no issues Tesla S2050 -&gt; no issues unknown sm_30 card (see comments to this post) -&gt; no issues K40c -&gt; CRASH GTX 780 -&gt; CRASH K20m -&gt; CRASH I used a number of Linux machines with different distributions, some of them using CUDA 5.5 and some using CUDA 6.0. At least on the machines I have direct control over, all cards were using the 331 nvidia driver series. There are several things to note here: the order of the malloc calls matters. If I allocate A before B things run fine the numerical constants matter a bit. For some values (e.g. n=30) no crash occurs, for others there is a crash The order of the free/malloc calls matter. If I free the memory in the same iteration where I allocate, everything works just fine At this point I'm pretty desperate. I don't see why or where I'm doing anything wrong. If anyone could help me, I'd really appreciate it. EDIT: as pointed out in the comments, apparently it only fails to run on sm_35 (i.e., GK110 cards), but runs fine on sm_30 Kepler cards.",
        "answers": [
            [
                "This issue should be fixed in the CUDA 6.5 production release package, now available for download from http://www.nvidia.com/getcuda"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I tried to run following simple cublas example on both console environment and in Django framework. \"\"\" Demonstrates multiplication of two matrices on the GPU. \"\"\" import pycuda import pycuda.gpuarray as gpuarray import pycuda.driver as drv import numpy as np drv.init() #init pycuda driver current_dev = drv.Device(0) #device we are working on ctx = current_dev.make_context() #make a working context ctx.push() #let context make the lead import scikits.cuda.linalg as culinalg import scikits.cuda.misc as cumisc culinalg.init() # Double precision is only supported by devices with compute # capability &gt;= 1.3: import string demo_types = [np.float32] for t in demo_types: print 'Testing matrix multiplication for type ' + str(np.dtype(t)) if np.iscomplexobj(t()): a = np.asarray(np.random.rand(10, 5)+1j*np.random.rand(10, 5), t) b = np.asarray(np.random.rand(5, 5)+1j*np.random.rand(5, 5), t) c = np.asarray(np.random.rand(5, 5)+1j*np.random.rand(5, 5), t) else: a = np.asarray(np.random.rand(10, 5), t) b = np.asarray(np.random.rand(5, 5), t) c = np.asarray(np.random.rand(5, 5), t) a_gpu = gpuarray.to_gpu(a) b_gpu = gpuarray.to_gpu(b) c_gpu = gpuarray.to_gpu(c) temp_gpu = culinalg.dot(a_gpu, b_gpu) d_gpu = culinalg.dot(temp_gpu, c_gpu) temp_gpu.gpudata.free() del(temp_gpu) print 'Success status: ', np.allclose(np.dot(np.dot(a, b), c) , d_gpu.get()) print 'Testing vector multiplication for type ' + str(np.dtype(t)) if np.iscomplexobj(t()): d = np.asarray(np.random.rand(5)+1j*np.random.rand(5), t) e = np.asarray(np.random.rand(5)+1j*np.random.rand(5), t) else: d = np.asarray(np.random.rand(5), t) e = np.asarray(np.random.rand(5), t) d_gpu = gpuarray.to_gpu(d) e_gpu = gpuarray.to_gpu(e) temp = culinalg.dot(d_gpu, e_gpu) print 'Success status: ', np.allclose(np.dot(d, e), temp) ctx.pop() #deactivate again ctx.detach() #delete it In console environment, I succeeded. But when I wanted to run in django(I plugged the example as a function in get method of URL) , it gave me a segmentation fault(core dump). Does anyone know what might be the cause of this problem ? The traceback information of cuda-gdb is as following: 0 0x00007ffff782d267 in kill () from /lib/x86_64-linux-gnu/libc.so.6 1 0x000000000041f44e in ?? () 2 0x000000000052c6d5 in PyEval_EvalFrameEx () 3 0x000000000052cf32 in PyEval_EvalFrameEx () 4 0x000000000055c594 in PyEval_EvalCodeEx () 5 0x000000000052ca8d in PyEval_EvalFrameEx () 6 0x000000000056d0aa in ?? () 7 0x000000000052e1e6 in PyEval_EvalFrameEx () 8 0x000000000056d0aa in ?? () 9 0x000000000052e1e6 in PyEval_EvalFrameEx () 10 0x000000000056d0aa in ?? () 11 0x000000000052e1e6 in PyEval_EvalFrameEx () 12 0x000000000055c594 in PyEval_EvalCodeEx () 13 0x000000000052ca8d in PyEval_EvalFrameEx () 14 0x000000000052cf32 in PyEval_EvalFrameEx () 15 0x000000000055c594 in PyEval_EvalCodeEx () 16 0x000000000052ca8d in PyEval_EvalFrameEx () 17 0x000000000055c594 in PyEval_EvalCodeEx () 18 0x00000000005b7392 in PyEval_EvalCode () 19 0x0000000000469663 in ?? () 20 0x00000000004699e3 in PyRun_FileExFlags () 21 0x0000000000469f1c in PyRun_SimpleFileExFlags () 22 0x000000000046ab81 in Py_Main () thanks !",
        "answers": [
            [
                "I created a new process using subprocess to deal with CUDA calculation and that solved problem. The reason might be pycuda is not thread safe."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I've followed the PyCuda instructions here: http://wiki.tiker.net/PyCuda/Installation/Mac I'm trying to compile the following code: import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1)) print dest-a*b And I'm receiving the following error: &gt; python test.py Traceback (most recent call last): File \"test.py\", line 12, in &lt;module&gt; \"\"\") File \"/Library/Python/2.7/site-packages/pycuda-2013.1.1-py2.7-macosx-10.9-intel.egg/pycuda/compiler.py\", line 251, in __init__ arch, code, cache_dir, include_dirs) File \"/Library/Python/2.7/site-packages/pycuda-2013.1.1-py2.7-macosx-10.9-intel.egg/pycuda/compiler.py\", line 241, in compile return compile_plain(source, options, keep, nvcc, cache_dir) File \"/Library/Python/2.7/site-packages/pycuda-2013.1.1-py2.7-macosx-10.9-intel.egg/pycuda/compiler.py\", line 132, in compile_plain stderr=stderr.decode(\"utf-8\", \"replace\")) pycuda.driver.CompileError: nvcc compilation of /var/folders/xr/m_rf4dp96mn2tb4yxlwcft7h0000gp/T/tmpqQcztC/kernel.cu failed [command: nvcc --cubin -arch sm_30 -m64 -I/Library/Python/2.7/site-packages/pycuda-2013.1.1-py2.7-macosx-10.9-intel.egg/pycuda/cuda kernel.cu] [stderr: nvcc fatal : Path to libdevice library not specified ] I've searched google and found the following threads, but they don't help solve the issue; http://lists.tiker.net/pipermail/pycuda/2011-June/003244.html ... TIA!",
        "answers": [
            [
                "So the problem is that I hadn't installed a minor update from the nvidia control panel. After that and updating my bash_profile it works. Heh."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Do we have a GPU accelerated of version of numpy.max(X, axis=None) in Theano. I looked into the documentation and found theano.tensor.max(X, axis=None), but it is 4-5 times slower than the numpy implementation. I can assure you, it is not slow because of some bad choice of matrix size. Same matrix under theano.tensor.exp is 40 times faster than its numpy counterpart. Any suggestions?",
        "answers": [
            [
                "The previous answer is partial. The suggestion should not work, as the work around is the one used in the final compiled code. There is optimization that will do this transformation automatically. The title of the question isn't the same as the content. They differ by the axis argument. I'll answer both questions. If the axis is 0 or None we support this on the GPU for that operation for matrix. If the axis is None, we have a basic implementation that isn't well optimized as it is harder to parallelize. If the axis is 0, we have a basic implementation, but it is faster as it is easier to parallelize. Also, how did you do your timing? If you just make one function with only that operation and test it via the device=gpu flags to do your comparison, this will include the transfer time between CPU and GPU. This is a memory bound operation, so if you include the transfer in your timming, personnaly I don't expect any speed op for that case. To see only the GPU operation, use Theano profiler: run with the Theano flag profile=True."
            ],
            [
                "The max and exp operations are fundamentally different; exp (and other operations like addition, sin, etc.) is an elementwise operation that is embarrassingly parallelizable, while max requires a parallel-processing scan algorithm that basically builds up a tree of pairwise comparisons over an array. It's not impossible to speed up max, but it's not as easy as exp. Anyway, the theano implementation of max basically consists of the following lines (in theano/tensor/basic.py): try: out = max_and_argmax(x, axis)[0] except Exception: out = CAReduce(scal.maximum, axis)(x) where max_and_argmax is a bunch of custom code that, to my eye, implements a max+argmax operation using numpy, and CAReduce is a generic GPU-accelerated scan operation used as a fallback (which, according to the comments, doesn't support grad etc.). You could try using the fallback directly and see whether that is faster, maybe something like this: from theano.tensor.elemwise import CAReduce from theano.scalar import maximum def mymax(X, axis=None): CAReduce(maximum, axis)(X)"
            ]
        ],
        "votes": [
            5.0000001,
            3.0000001
        ]
    },
    {
        "question": "I'm getting a pycuda runtime error (very similar to the one at https://stackoverflow.com/questions/20078191/opencv-2-4-7-mac-osx-10-9-python-2-7-6-cuda-5-5) as below. The error when executing the example is cordelia:examples xxx$ python demo.py Traceback (most recent call last): File \"demo.py\", line 3, in &lt;module&gt; import pycuda.driver as cuda File \"/Users/xxx/canopy/lib/python2.7/site-packages/pycuda-2013.1.1-py2.7-macosx-10.6-x86_64.egg/pycuda/driver.py\", line 2, in &lt;module&gt; from pycuda._driver import * ImportError: dlopen(/Users/xxx/canopy/lib/python2.7/site-packages/pycuda-2013.1.1-py2.7-macosx-10.6-x86_64.egg/pycuda/_driver.so, 2): Library not loaded: @rpath/libcurand.dylib Referenced from: /Users/xxx/canopy/lib/python2.7/site-packages/pycuda-2013.1.1-py2.7-macosx-10.6-x86_64.egg/pycuda/_driver.so Reason: Incompatible library version: _driver.so requires version 1.1.0 or later, but libcurand.dylib provides version 0.0.0 and it shows up again on the command line, stopping my show for now: Enthought Canopy Python 2.7.3 | 64-bit | (default, Aug 8 2013, 05:37:06) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. import pycuda._driver Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; ImportError: dlopen(/Users/xxx/canopy/lib/python2.7/site-packages/pycuda-2013.1.1-py2.7-macosx-10.6-x86_64.egg/pycuda/_driver.so, 2): Library not loaded: @rpath/libcurand.dylib Referenced from: /Users/xxx/canopy/lib/python2.7/site-packages/pycuda-2013.1.1-py2.7-macosx-10.6-x86_64.egg/pycuda/_driver.so Reason: Incompatible library version: _driver.so requires version 1.1.0 or later, but libcurand.dylib provides version 0.0.0 Any ideas? Thanks!",
        "answers": [
            [
                "In fact the steps at http://wiki.tiker.net/PyCuda/Installation/Mac#Step_3:_Install_PyCUDA worked - phew!"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a for loop in python that I want to unroll onto a GPU. I imagine there has to be a simple solution but I haven't found one yet. Our function loops over elements in a numpy array and does some math storing the result in another numpy array. Each iteration adds some to this result array. A possible large simplification of our code might look something like this: import numpy as np a = np.arange(100) out = np.array([0, 0]) for x in xrange(a.shape[0]): out[0] += a[x] out[1] += a[x]/2.0 How can I unroll a loop like this in Python to run on a GPU?",
        "answers": [
            [
                "The place to start is http://documen.tician.de/pycuda/ the example there is import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1)) print dest-a*b You place the part of the code you want to parallelize in C code segment and call it from python. For you example the size of your data will need to be much bigger than 100 to make it worth while. You'll need some way to divide your data into block. If you wanted to add 1,000,000 numbers you could divide it into 1000 blocks. Add each block in the parallezed code. Then add the results in python. Adding things is not really a natural task for this type of parallelisation. GPUs tend to do the same task for each pixel. You have a task which need to operate on multiple pixels. It might be better to work with cuda first. A related thread is. Understanding CUDA grid dimensions, block dimensions and threads organization (simple explanation)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am using Canopy enthought on a machine without su access. Whenever i try to build any package dependent on python I get this error: /usr/bin/ld: cannot find -lpython2.7 collect2: ld returned 1 exit status error: command 'g++' failed with exit status 1 Any idea what's going wrong? I am running Debian OS. Thanks",
        "answers": [
            [
                "Depending on what version of Canopy you're using, try to set your LIBRARY_PATH variable. Example: export LIBRARY_PATH=~/Enthought/lib Then try to build the package. This worked for me but I don't know the root cause as to why Canopy virtual environment isn't setting this variable."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Can anyone explain to me why everytime I run this code, my computer freeze? from numbapro import cuda import numpy as np from timeit import default_timer as time n = 100 dtype = np.float32 @cuda.jit('void(float32[:,:], float32[:], float32[:])') def cu_matrix_vector(A, b, c): y, x = cuda.grid(2) if x &lt; n and y &lt; n: c[y] = 0.0 for i in range(n): c[y] += A[y, i] * b[i] A = np.array(np.random.random((n, n)), dtype=dtype) B = np.array(np.random.random((n, 1)), dtype=dtype) C = np.empty_like(B) blockDim = 32, 8 gridDim = (n + blockDim[0] - 1)/blockDim[0], (n + blockDim[1] - 1)/blockDim[1] print 'blockDim = (%d,%d)' %blockDim s = time() stream = cuda.stream() with stream.auto_synchronize(): dA = cuda.to_device(A,stream) dB = cuda.to_device(B,stream) dC = cuda.to_device(C,stream) cu_matrix_vector[(bpg, bpg), (tpb, tpb),stream](dA, dB, dC) dC.to_host(stream) e = time() tcuda = e - s print tcuda After hitting the code, my computer freezes. I'm not sure why. I appreciate all the help in advance.",
        "answers": [
            [
                "Array B should not be a 2D array: B = np.array(np.random.random((n, 1)), dtype=dtype) It should be 1D: B = np.array(np.random.random(n), dtype=dtype) Regarding the freezing, I assume you are using OSX. The CUDA driver should return an error code upon kernel launch error but, on OSX, it seems like the display manager will freeze."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Good morning all, I am kind of newbie with cuda/pyCuda, so probably this will have an easy solution employing some mechanism that I don't know.... I am employing pycuda to operate over pairs of values: I subtract the smallest from the biggest and then perform some time-consuming operations. As it must be repeated many times, it is well suited for GPUs. However, most of the times the result of the substraction is the same. Then, performing the time-consuming operations make no sense. what I do in the non-GPU version of my code is something like: myFunction(A,B): index=A-B try: value = myDictionary[index] except: value = expensiveOperation(index) myDictionary[index] = value return value As accessing the dictionary is much faster than expensiveOperation, and the value is found most of the times, I obtain a significant time gain. When porting this to GPUs, I can call to myFunction(A,B) with a high degree of parallelism, which is great. However, I don't know how could I employ this dictionary mechanism -or a similar one- to avoid redundant operations. any thoughts on this? Thanks for your help edit: I would like to know, is it possible to store the dictionary on the GPU, or should I copy it every time? If it's on the GPU, can it be accessed/edited by several cores at the same time? How should I implement it?",
        "answers": [
            [
                "You could try this: myFunction(A,B): index=A-B if index in myDictionary.keys(): value = myDictionary[index] else: value = expensiveOperation(index) myDictionary[index] = value return value"
            ],
            [
                "It seems your question is about implementing some sort of memoise facility inside GPU code. I don't think this is worth pursuing. In the GPU arithmetic operations are almost free, but memory access is very expensive (and random memory access even more so). Performing a dictionary/hash table look-up in GPU memory to retrieve an arithmetic result from a cache is almost guaranteed to be slower that the cost of just calculating the result. It sounds counter-intuitive, but that is the reality of GPU computing. In an interpreted language like Python, which is relatively slow, using a fast native memoisation mechanism makes a lot of sense, and memoising the results of a complete kernel function call also could yield useful performance benefits for expensive kernels. But memoisation inside CUDA doesn't seem all that useful."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "using Python33 on Windows 8.1 with Cuda toolkit 5.5 and hardware installed when trying to import and initialize the device with: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule &lt;--- this line causes the error I get the below error: Traceback (most recent call last): File \"C:\\Python33\\Lib\\site-packages\\pytools\\datatable.py\", line 1, in &lt;module&gt; from pytools import Record File \"C:\\Python33\\lib\\site-packages\\pytools\\__init__.py\", line 1249 print value, bin_nr, bin_starts ^ SyntaxError: invalid syntax Can anyone advise on this or suggest a work around?",
        "answers": [
            [
                "This appears to have been caused by attempting to use a version of PyCUDA without Python 3 support in Python 3. Python 3 support was officially added in PyCUDA 2013.1"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I am trying to use (and learn from) Mark Harris's optimized reduction kernel, by copying his source code into a simple pycuda application (full source of my attempt is listed below). Unfortunately, I run into one of the two following erros. The cuda kernel does not compile, throwing the following error message. kernel.cu(3): error: this declaration may not have extern \"C\" linkage If I include the argument no_extern_c=True into the line that compiles the kernel, the following error is raised: pycuda._driver.LogicError: cuModuleGetFunction failed: not found I have also tried wrapping the contents of modStr in extern \"C\" { [...] } with the no_extern_c variable set to either True or False, without any success. The problem appears to involve the line template &lt;unsigned int blockSize&gt; as if I comment the body of the function out it still raises errors. But I don't understand the problem well enough to have any more ideas about how to fix it. Any advice / suggestions / help would be much appreciated -- thanks in advance! from pylab import * import pycuda.gpuarray as gpuarray import pycuda.autoinit import pycuda.driver as drv from pycuda.compiler import SourceModule modStr = \"\"\" template &lt;unsigned int blockSize&gt; __global__ void reduce6(int *g_idata, int *g_odata, unsigned int n) { extern __shared__ int sdata[]; unsigned int tid = threadIdx.x; unsigned int i = blockIdx.x*(blockSize*2) + tid; unsigned int gridSize = blockSize*2*gridDim.x; sdata[tid] = 0; while (i &lt; n) { sdata[tid] += g_idata[i] + g_idata[i+blockSize]; i += gridSize; } __syncthreads(); if (blockSize &gt;= 512) { if (tid &lt; 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); } if (blockSize &gt;= 256) { if (tid &lt; 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); } if (blockSize &gt;= 128) { if (tid &lt; 64) { sdata[tid] += sdata[tid + 64]; } __syncthreads(); } if (tid &lt; 32) { if (blockSize &gt;= 64) sdata[tid] += sdata[tid + 32]; if (blockSize &gt;= 32) sdata[tid] += sdata[tid + 16]; if (blockSize &gt;= 16) sdata[tid] += sdata[tid + 8]; if (blockSize &gt;= 8) sdata[tid] += sdata[tid + 4]; if (blockSize &gt;= 4) sdata[tid] += sdata[tid + 2]; if (blockSize &gt;= 2) sdata[tid] += sdata[tid + 1]; } if (tid == 0) g_odata[blockIdx.x] = sdata[0]; } \"\"\" mod = SourceModule(modStr,no_extern_c=True) # With no_extern_c = True, the error is : # pycuda._driver.LogicError: cuModuleGetFunction failed: not found # With no_extern_c = False, the error is : # kernel.cu(3): error: this declaration may not have extern \"C\" linkage cuda_reduce_fn = mod.get_function(\"reduce6\") iData = arange(32).astype(np.float32) oData = zeros_like(iData) cuda_reduce_fn( drv.In(iData), drv.Out(oData), np.int32(32), block=(32,1,1), grid=(1,1)) print(iData) print(oData)",
        "answers": [
            [
                "It is illegal to have templated functions with C linkage in C++, which is why you get the error in the first case. In the second case, you get a not found error because you haven't actually instantiated the template anywhere I can see, so the compiler won't emit any output. When you do add an instance, you will get the same error, because the compiled code object for the device has a mangled name. You will need to use the mangled name in the get_function call. Paradoxically, you can't know the mangled name when JIT compiling from source, because you need to see the compiler output and that isn't know a priori (any of compiler messages, PTX, cubin or object files will give you the mangled name). If you want to work with templated kernels in PyCUDA, I recommend compiling them to cubin yourself with the toolchain, and then loading from cubin in PyCUDA to get known mangled names from the module."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am trying to learn CUDA and using PyCUDA to write a simple matrix multiplication code. For two 4x4 randomly generated matrices I get the following solution: Cuda: [[ -5170.86181641 -21146.49609375 20690.02929688 -35413.9296875 ] [-18998.5 -3199.53271484 13364.62890625 7141.36816406] [ 31197.43164062 21095.02734375 1750.64453125 11304.63574219] [ -896.64978027 18424.33007812 -17135.00390625 7418.28417969]] Python: [[ -5170.86035156 -21146.49609375 20690.02929688 -35413.9296875 ] [-18998.5 -3199.53271484 13364.62695312 7141.36816406] [ 31197.43164062 21095.02929688 1750.64404297 11304.63574219] [ -896.64941406 18424.33007812 -17135.00390625 7418.28417969]] Cuda-Python: [[-0.00146484 0. 0. 0. ] [ 0. 0. 0.00195312 0. ] [ 0. -0.00195312 0.00048828 0. ] [-0.00036621 0. 0. 0. ]] The error is of the order of 1e-3 and increases as I increase the size of matrices. I am not sure whether its a bug or not. My question is whether such a \"large\" error is a normal thing with int32 or I am doing something wrong? Here is the source code: matmul.py import numpy as np import time # import pycuda stuff import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule BLOCK_SIZE = 16 n = 4 ni = np.int32(n) # matrix A a = np.random.randn(n, n)*100 a = a.astype(np.float32) # matrix B b = np.random.randn(n, n)*100 b = b.astype(np.float32) # matrix B c = np.empty([n, n]) c = c.astype(np.float32) # allocate memory on device a_gpu = cuda.mem_alloc(a.nbytes) b_gpu = cuda.mem_alloc(b.nbytes) c_gpu = cuda.mem_alloc(c.nbytes) # copy matrix to memory cuda.memcpy_htod(a_gpu, a) cuda.memcpy_htod(b_gpu, b) # compile kernel mod = SourceModule(open(\"kernels.cu\", \"r\").read()) # get function matmul = mod.get_function(\"matmul\"); # set grid size if n%BLOCK_SIZE != 0: grid=(n/BLOCK_SIZE+1,n/BLOCK_SIZE+1,1) else: grid=(n/BLOCK_SIZE,n/BLOCK_SIZE,1) # call gpu function start = time.time() matmul(ni, a_gpu, b_gpu, c_gpu, block=(BLOCK_SIZE,BLOCK_SIZE,1), grid=grid); end = time.time() print \"Time: %.5f s\"%(end-start) # copy back the result cuda.memcpy_dtoh(c, c_gpu) print np.linalg.norm(c - np.dot(a,b)) print c print np.dot(a,b) print c - np.dot(a,b) kernels.cu __global__ void matmul(int n, const float *A, const float *B, float *C){ int tx = threadIdx.x; int ty = threadIdx.y; int bx = blockIdx.x; int by = blockIdx.y; int row = by*blockDim.y + ty; int col = bx*blockDim.x + tx; if(row &lt; n &amp;&amp; col &lt; n){ float val = 0.0; for(int i=0; i&lt;n; ++i){ val += A[row*n + i]*B[n*i + col]; } C[row*n + col] = val; } }",
        "answers": [
            [
                "Just adding to what Warren has said. I don't think there's anything wrong here. You're comparing the floating point results generated by two different machines (CPU and GPU). They are not guaranteed to be bitwise identical for the operations at the level you are thinking about, partly because the order of operations on the GPU is not necessarily the same as the order of operations on the GPU. As you increase the size of the matrices, you are increasing the number of values summed together, and your absolute error increases, since you are adding a bunch of small bit errors together. In general, these types of considerations should always come into play when comparing floating point results. Bitwise identical results are rarely to be expected from two different computations. And even something as simple as changing the order of operations can make it a different calculation, for floating point operations. You may want to read this paper especially section 2.2."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "When I try to use the script underneath to get the data back to the cpu, there is an error. I don't get an error when I try to put some values in \"ref\" if I would just put: ref[1] = 255; ref[0] = 255; ref[2] = 255; but if I do something like this: if (verschil.a[idx+idy*640]&gt;5){ ref[1] = 255; ref[0] = 255; ref[2] = 255; } the error message I get is: Traceback (most recent call last): File \"./zwartwit.py\", line 159, in &lt;module&gt; verwerking(cuda.InOut(refe),cuda.InOut(frame), block=(640, 1, 1)) File \"/usr/lib/python2.7/dist-packages/pycuda/driver.py\", line 374, in function_call func._launch_kernel(grid, block, arg_buf, shared, None) pycuda._driver.LogicError: cuLaunchKernel failed: invalid value Thanks for the help! ps, this is a symplified version of the script I was talking about. to get the same error, the // must be removed. import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy import cv2 from time import time,sleep mod = SourceModule(\"\"\" struct legear{ int a[307200];}; __global__ void totaal(int *ref){ int idx = threadIdx.x + blockIdx.x * blockDim.x; legear test; for (int idy=0;idy&lt;480;idy++){ if (idy &lt; 480){ if (idx &lt; 640){ if (ref[idx*3+idy*640*3]&gt;100){ test.a[idx+idy*640] = 255; } //if (test.a[idx+idy*640] == 255){ ref[idx*3+idy*640*3] = 255; ref[idx*3+idy*640*3+1] = 255; ref[idx*3+idy*640*3+2] = 255; //} } } } } \"\"\") camera = cv2.VideoCapture(0) im2 = numpy.zeros((768, 1024, 1 ),dtype=numpy.uint8) cv2.imshow(\"projector\", im2) key = cv2.waitKey(100) for i in range(0,8): refe = camera.read()[1] im2[500:502] = [100] cv2.imshow(\"projector\", im2) key = cv2.waitKey(100) verwerking = mod.get_function(\"totaal\") refe = refe.astype(numpy.int32) verwerking(cuda.InOut(refe), block=(640, 1, 1)) refe = refe.astype(numpy.uint8) cv2.imshow(\"test\", refe) cv2.waitKey(200) raw_input()",
        "answers": [
            [
                "The basic problem here is the size of test inside your kernel. As you have written it, every thread requires 1228800 bytes of local memory. The runtime must reserve that memory for every thread - so your code would require 750Mb of free memory to allocate for local memory on the device to support the 640 threads per block you are trying to launch. My guess is that your device doesn't have that amount of free memory. The reason why the code you have shown works without the if statement is down to compiler optimisation - in that case test isn't actually used for anything and the compiler simply removes it from the code, which eliminates the huge local memory footprint of the kernel and allows it to run. When you uncomment the if statement, test determines the state of a global memory write, thus the compiler cannot optimise it away and the kernel requires a large amount local memory to run. This is the compiler output I see with the kernel code as you posted it: &gt; nvcc -arch=sm_21 -Xptxas=\"-v\" -m32 -c wnkr_py.cu wnkr_py.cu wnkr_py.cu(7): warning: variable \"test\" was set but never used tmpxft_00000394_00000000-5_wnkr_py.cudafe1.gpu tmpxft_00000394_00000000-10_wnkr_py.cudafe2.gpu wnkr_py.cu wnkr_py.cu(7): warning: variable \"test\" was set but never used ptxas : info : 0 bytes gmem ptxas : info : Compiling entry function '_Z6totaalPi' for 'sm_21' ptxas : info : Function properties for _Z6totaalPi 0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads ptxas : info : Used 8 registers, 36 bytes cmem[0], 4 bytes cmem[16] tmpxft_00000394_00000000-5_wnkr_py.cudafe1.cpp tmpxft_00000394_00000000-15_wnkr_py.ii Note the compiler warning and the stack frame size. With the if statement active: &gt;nvcc -arch=sm_21 -Xptxas=\"-v\" -m32 -c wnkr_py.cu wnkr_py.cu tmpxft_000017c8_00000000-5_wnkr_py.cudafe1.gpu tmpxft_000017c8_00000000-10_wnkr_py.cudafe2.gpu wnkr_py.cu ptxas : info : 0 bytes gmem ptxas : info : Compiling entry function '_Z6totaalPi' for 'sm_21' ptxas : info : Function properties for _Z6totaalPi 1228800 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads ptxas : info : Used 7 registers, 36 bytes cmem[0] tmpxft_000017c8_00000000-5_wnkr_py.cudafe1.cpp tmpxft_000017c8_00000000-15_wnkr_py.ii Note the stack frame size changes to 1228800 bytes per thread. My quick reading of the code suggests that test doesn't need to be anything like as large as you have defined it for the code to run, but I leave the required size as an exercise to the reader...."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have some trouble with installing python modules. I wanna use gpu in a python script but I get some error while install modules 1- I install my graphic driver : Geforce GT 650M 2- install cuda_5.5.31_winvista_win7_win8_win8.1_notebook_x64.exe Now I want to install modules and these are errors : Pyrit Error : C:\\Users\\Ali\\Desktop\\pygpu\\pyrit-0.4.0&gt;python setup.py build running build running build_py running build_ext building 'cpyrit._cpyrit_cpu' extension error: Unable to find vcvarsall.bat C:\\Users\\Ali\\Desktop\\pygpu\\pyrit-0.4.0&gt;python setup.py install running install running build running build_py running build_ext building 'cpyrit._cpyrit_cpu' extension error: Unable to find vcvarsall.bat C:\\Users\\Ali\\Desktop\\pygpu\\pyrit-0.4.0&gt; cpyrit-cuda Error: C:\\Users\\Ali\\Desktop\\pygpu\\cpyrit-cuda-0.4.0&gt;python setup.py build The CUDA compiler and headers required to build kernel were not found. Trying to continue anyway... running build running build_ext 'nvcc' is not recognized as an internal or external command, operable program or batch file. None Failed to execute command 'nvcc -V' Traceback (most recent call last): File \"setup.py\", line 175, in &lt;module&gt; setup(**setup_args) File \"C:\\Python27\\lib\\distutils\\core.py\", line 152, in setup dist.run_commands() File \"C:\\Python27\\lib\\distutils\\dist.py\", line 953, in run_commands self.run_command(cmd) File \"C:\\Python27\\lib\\distutils\\dist.py\", line 972, in run_command cmd_obj.run() File \"C:\\Python27\\lib\\distutils\\command\\build.py\", line 127, in run self.run_command(cmd_name) File \"C:\\Python27\\lib\\distutils\\cmd.py\", line 326, in run_command self.distribution.run_command(command) File \"C:\\Python27\\lib\\distutils\\dist.py\", line 972, in run_command cmd_obj.run() File \"setup.py\", line 82, in run raise SystemError(\"Nvidia's CUDA-compiler 'nvcc' can't be \" \\ SystemError: Nvidia's CUDA-compiler 'nvcc' can't be found. C:\\Users\\Ali\\Desktop\\pygpu\\cpyrit-cuda-0.4.0&gt;python setup.py install The CUDA compiler and headers required to build kernel were not found. Trying to continue anyway... running install running build running build_ext 'nvcc' is not recognized as an internal or external command, operable program or batch file. None Failed to execute command 'nvcc -V' Traceback (most recent call last): File \"setup.py\", line 175, in &lt;module&gt; setup(**setup_args) File \"C:\\Python27\\lib\\distutils\\core.py\", line 152, in setup dist.run_commands() File \"C:\\Python27\\lib\\distutils\\dist.py\", line 953, in run_commands self.run_command(cmd) File \"C:\\Python27\\lib\\distutils\\dist.py\", line 972, in run_command cmd_obj.run() File \"C:\\Python27\\lib\\distutils\\command\\install.py\", line 563, in run self.run_command('build') File \"C:\\Python27\\lib\\distutils\\cmd.py\", line 326, in run_command self.distribution.run_command(command) File \"C:\\Python27\\lib\\distutils\\dist.py\", line 972, in run_command cmd_obj.run() File \"C:\\Python27\\lib\\distutils\\command\\build.py\", line 127, in run self.run_command(cmd_name) File \"C:\\Python27\\lib\\distutils\\cmd.py\", line 326, in run_command self.distribution.run_command(command) File \"C:\\Python27\\lib\\distutils\\dist.py\", line 972, in run_command cmd_obj.run() File \"setup.py\", line 82, in run raise SystemError(\"Nvidia's CUDA-compiler 'nvcc' can't be \" \\ SystemError: Nvidia's CUDA-compiler 'nvcc' can't be found. C:\\Users\\Ali\\Desktop\\pygpu\\cpyrit-cuda-0.4.0&gt; pycuda Error: C:\\Users\\Ali\\Desktop\\pygpu\\pycuda-2013.1.1&gt;python setup.py build *** WARNING: nvcc not in path. Traceback (most recent call last): File \"setup.py\", line 218, in &lt;module&gt; main() File \"setup.py\", line 88, in main conf[\"CUDA_INC_DIR\"] = [join(conf[\"CUDA_ROOT\"], \"include\")] File \"C:\\Python27\\lib\\ntpath.py\", line 96, in join assert len(path) &gt; 0 TypeError: object of type 'NoneType' has no len() C:\\Users\\Ali\\Desktop\\pygpu\\pycuda-2013.1.1&gt;python setup.py install *** WARNING: nvcc not in path. Traceback (most recent call last): File \"setup.py\", line 218, in &lt;module&gt; main() File \"setup.py\", line 88, in main conf[\"CUDA_INC_DIR\"] = [join(conf[\"CUDA_ROOT\"], \"include\")] File \"C:\\Python27\\lib\\ntpath.py\", line 96, in join assert len(path) &gt; 0 TypeError: object of type 'NoneType' has no len() C:\\Users\\Ali\\Desktop\\pygpu\\pycuda-2013.1.1&gt; numpy error: C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0&gt;python setup.py build Running from numpy source directory. non-existing path in 'numpy\\\\distutils': 'site.cfg' F2PY Version 2 blas_opt_info: blas_mkl_info: libraries mkl,vml,guide not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\Python 27\\\\libs'] NOT AVAILABLE openblas_info: libraries openblas not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\Python27\\\\l ibs'] NOT AVAILABLE atlas_blas_threads_info: Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', ' C:\\\\Python27\\\\libs'] NOT AVAILABLE atlas_blas_info: libraries f77blas,cblas,atlas not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\ Python27\\\\libs'] NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1522: User Warning: Atlas (http://math-atlas.sourceforge.net/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [atlas]) or by setting the ATLAS environment variable. warnings.warn(AtlasNotFoundError.__doc__) blas_info: libraries blas not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\Python27\\\\libs' ] NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1531: User Warning: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable. warnings.warn(BlasNotFoundError.__doc__) blas_src_info: NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1534: User Warning: Blas (http://www.netlib.org/blas/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [blas_src]) or by setting the BLAS_SRC environment variable. warnings.warn(BlasSrcNotFoundError.__doc__) NOT AVAILABLE non-existing path in 'numpy\\\\lib': 'benchmarks' lapack_opt_info: lapack_mkl_info: mkl_info: libraries mkl,vml,guide not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\Python 27\\\\libs'] NOT AVAILABLE NOT AVAILABLE atlas_threads_info: Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in C:\\Python27\\lib libraries lapack_atlas not found in C:\\Python27\\lib libraries ptf77blas,ptcblas,atlas not found in C:\\ libraries lapack_atlas not found in C:\\ libraries ptf77blas,ptcblas,atlas not found in C:\\Python27\\libs libraries lapack_atlas not found in C:\\Python27\\libs numpy.distutils.system_info.atlas_threads_info NOT AVAILABLE atlas_info: libraries f77blas,cblas,atlas not found in C:\\Python27\\lib libraries lapack_atlas not found in C:\\Python27\\lib libraries f77blas,cblas,atlas not found in C:\\ libraries lapack_atlas not found in C:\\ libraries f77blas,cblas,atlas not found in C:\\Python27\\libs libraries lapack_atlas not found in C:\\Python27\\libs numpy.distutils.system_info.atlas_info NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1428: User Warning: Atlas (http://math-atlas.sourceforge.net/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [atlas]) or by setting the ATLAS environment variable. warnings.warn(AtlasNotFoundError.__doc__) lapack_info: libraries lapack not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\Python27\\\\lib s'] NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1439: User Warning: Lapack (http://www.netlib.org/lapack/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [lapack]) or by setting the LAPACK environment variable. warnings.warn(LapackNotFoundError.__doc__) lapack_src_info: NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1442: User Warning: Lapack (http://www.netlib.org/lapack/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [lapack_src]) or by setting the LAPACK_SRC environment variable. warnings.warn(LapackSrcNotFoundError.__doc__) NOT AVAILABLE C:\\Python27\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'define_macros' warnings.warn(msg) running build running config_cc unifing config_cc, config, build_clib, build_ext, build commands --compiler opti ons running config_fc unifing config_fc, config, build_clib, build_ext, build commands --fcompiler opt ions running build_src build_src building py_modules sources creating build creating build\\src.win-amd64-2.7 creating build\\src.win-amd64-2.7\\numpy creating build\\src.win-amd64-2.7\\numpy\\distutils building library \"npymath\" sources No module named msvccompiler in numpy.distutils; trying from distutils error: Unable to find vcvarsall.bat C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0&gt;python setup.py install Running from numpy source directory. non-existing path in 'numpy\\\\distutils': 'site.cfg' F2PY Version 2 blas_opt_info: blas_mkl_info: libraries mkl,vml,guide not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\Python 27\\\\libs'] NOT AVAILABLE openblas_info: libraries openblas not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\Python27\\\\l ibs'] NOT AVAILABLE atlas_blas_threads_info: Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', ' C:\\\\Python27\\\\libs'] NOT AVAILABLE atlas_blas_info: libraries f77blas,cblas,atlas not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\ Python27\\\\libs'] NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1522: User Warning: Atlas (http://math-atlas.sourceforge.net/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [atlas]) or by setting the ATLAS environment variable. warnings.warn(AtlasNotFoundError.__doc__) blas_info: libraries blas not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\Python27\\\\libs' ] NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1531: User Warning: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable. warnings.warn(BlasNotFoundError.__doc__) blas_src_info: NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1534: User Warning: Blas (http://www.netlib.org/blas/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [blas_src]) or by setting the BLAS_SRC environment variable. warnings.warn(BlasSrcNotFoundError.__doc__) NOT AVAILABLE non-existing path in 'numpy\\\\lib': 'benchmarks' lapack_opt_info: lapack_mkl_info: mkl_info: libraries mkl,vml,guide not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\Python 27\\\\libs'] NOT AVAILABLE NOT AVAILABLE atlas_threads_info: Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in C:\\Python27\\lib libraries lapack_atlas not found in C:\\Python27\\lib libraries ptf77blas,ptcblas,atlas not found in C:\\ libraries lapack_atlas not found in C:\\ libraries ptf77blas,ptcblas,atlas not found in C:\\Python27\\libs libraries lapack_atlas not found in C:\\Python27\\libs numpy.distutils.system_info.atlas_threads_info NOT AVAILABLE atlas_info: libraries f77blas,cblas,atlas not found in C:\\Python27\\lib libraries lapack_atlas not found in C:\\Python27\\lib libraries f77blas,cblas,atlas not found in C:\\ libraries lapack_atlas not found in C:\\ libraries f77blas,cblas,atlas not found in C:\\Python27\\libs libraries lapack_atlas not found in C:\\Python27\\libs numpy.distutils.system_info.atlas_info NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1428: User Warning: Atlas (http://math-atlas.sourceforge.net/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [atlas]) or by setting the ATLAS environment variable. warnings.warn(AtlasNotFoundError.__doc__) lapack_info: libraries lapack not found in ['C:\\\\Python27\\\\lib', 'C:\\\\', 'C:\\\\Python27\\\\lib s'] NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1439: User Warning: Lapack (http://www.netlib.org/lapack/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [lapack]) or by setting the LAPACK environment variable. warnings.warn(LapackNotFoundError.__doc__) lapack_src_info: NOT AVAILABLE C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0\\numpy\\distutils\\system_info.py:1442: User Warning: Lapack (http://www.netlib.org/lapack/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [lapack_src]) or by setting the LAPACK_SRC environment variable. warnings.warn(LapackSrcNotFoundError.__doc__) NOT AVAILABLE C:\\Python27\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'define_macros' warnings.warn(msg) running install running build running config_cc unifing config_cc, config, build_clib, build_ext, build commands --compiler opti ons running config_fc unifing config_fc, config, build_clib, build_ext, build commands --fcompiler opt ions running build_src build_src building py_modules sources building library \"npymath\" sources No module named msvccompiler in numpy.distutils; trying from distutils error: Unable to find vcvarsall.bat C:\\Users\\Ali\\Desktop\\pygpu\\numpy-1.8.0&gt; Boost Error : C:\\Users\\Ali\\Desktop\\pygpu\\boost_1_55_0&gt;bootstrap.bat Building Boost.Build engine 'cl' is not recognized as an internal or external command, operable program or batch file. Failed to build Boost.Build engine. Please consult bootstrap.log for furter diagnostics. You can try to obtain a prebuilt binary from http://sf.net/project/showfiles.php?group_id=7586&amp;package_id=72941 Also, you can file an issue at http://svn.boost.org Please attach bootstrap.log in that case. C:\\Users\\Ali\\Desktop\\pygpu\\boost_1_55_0&gt; Someone here tell me how I install nvcc ,and where I can download it.",
        "answers": [
            [
                "You are missing several things here. That vcvarsall error is explained here, you'll have to set variables. Then you are missing nvcc as stated by WARNING: nvcc not in path. and similar messages. numpy has missing dependent libraries which you should install first. For boost you should follow the suggestion and use a prebuilt binary. All in all I'd say it's all about reading and understanding error messages. As a suggestion: follow the installation guides on the library websites, which will show you dependencies as well."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "There is a program written in C with some CUDA kernels and I need to adapt this program to run in python using PyCUDA. Now, in this C code there are some global variables defined and some of those kernels are accessing those variables via #ifdef. What I would like to do is to define those global variables in python and then just copy my CUDA C kernels code and run them using SourceModule scheme (let's say I'm just lazy). I imagine something like this: my_global_var=True mod=SourceModule(\"\"\" __global__ void func() { ... #ifdef my_global_var do something #endif } \"\"\") Is there any way to do that? Clearly, that naive attempt doesn't work at all. Should I use some kind of special pointer or something like that? In other words, what exacly happens when CUDA C kernel compiled with PyCUDA have #ifdef in it? Edit: Well, it is possible that I'm misusing the \"global variable\" term here. The C code looks more or less like this: #define X __global__ void func(...) { ... #ifdef X do something #endif } What I'm trying to achieve is to use that C code in PyCUDA without messing with it too much. So I would like to just copy \"global_ void func...\", put it in PyCUDA SourceModule and define that X (no matters if it is global variable or something different) in python (like in the first pseudocode). Is it possible?",
        "answers": [
            [
                "As with compiling any C or C++ code, external manipulation of preprocessor symbols (by which I mean outside of the code itself) requires passing additional arguments to the compiler. If you had this code: __global__ void func(...) { ... #ifdef X do something #endif } then the way to set X during compilation would be to pass -DX to the compiler, which defines the symbol X during the current preprocessor pass. You can do exactly the same thing in PyCUDA by passing options to the build using the options keyword in the SourceModule constructor. Compiler options are passed as a list, so your PyCUDA example could be implemented something like this: my_global_var=True .... build_options = [] if my_global_var: build_options.append('-Dmy_global_var') mod=SourceModule(\"\"\" __global__ void func() { ... #ifdef my_global_var do something #endif } \"\"\", options=build_options) [Disclaimer: Totally untested, use at own risk - I don't have a working PyCUDA installation ATM] Here, we simply build a list containing as many entries as there are options you need to pass to the compiler, and then instantiate a SourceModule instance with those options. After that, it should just work..."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have a CUDA kernel in which each address of a global array is written to 4 times by each thread. It seems that two threads in a warp can potentially write to the same address at the same time, causing the process to be non-coalesced. What about threads in a block? d_next_front[i*width + j+1] = 1; d_next[i*width + j-1] = 1; d_next[(i+1)*width + j] = 1; d_next[(i-1)*width + j] = 1;",
        "answers": [
            [
                "Do threads in a warp execute instructions in sync? Yes: Warps execute instructions in sync. What about threads in a block? No: A block is composed of one or more warps. The warps in a block are not necessarily synchronized, and usually won't be. Coalescing refers to the memory transactions associated with a single instruction, executed by a single warp. (There is no concept of coalescing that applies across multiple warps.) If all of these memory transactions lie within a single naturally aligned 128-byte segment of global memory, then the transactions will \"coalesce\" within the memory controller, and be satisfied by a single transaction to memory. There are many nuances to this behavior, but the statement is generally instructive for cc2.0 and newer devices. There may be some additional caveats to consider for pre-cc2.0 devices. It's impossible to look at your code and determine if the resultant transactions will coalesce. It would be necessary to know the definitions of i and j, especially as they relate to the built-in thread ID variables such as threadIdx.x Likewise, understanding whether two threads in a warp could write to the same location at the same time would require looking at much more code than what you have shown now."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Using the driver api precludes the usage of the runtime api in the same application ([1]) . Unfortunately cublas, cufft, etc are all based on the runtime api. If one wants dynamic kernel definition as in cuModuleLoad and cublas at the same time, what are the options? I have these in mind, but maybe there are more: A. Wait for compute capability 3.5 that's rumored to support peaceful coexistence of driver and runtime apis in the same application. B. Compile the kernels to an .so file and dlopen it. Do they get unloaded on dlcose? C. Attempt to use cuModuleLoad from the driver api, but everything else from the runtime api. No idea if there is any hope for this. I'm not holding my breath, because jcuda or pycuda are in pretty much the same bind and they probably would have figured it out already. [1] CUDA Driver API vs. CUDA runtime",
        "answers": [
            [
                "To summarize, you are tilting at windmills here. By relying on extremely out of date information, you seem to have concluded that runtime and driver API interoperability isn't supported in CUDA, when, in fact, it has been since the CUDA 3.0 beta was released in 2009. Quoting from the release notes of that version: The CUDA Toolkit 3.0 Beta is now available. Highlights for this release include: CUDA Driver / Runtime Buffer Interoperability, which allows applications using the CUDA Driver API to also use libraries implemented using the CUDA C Runtime. There is documentation here which succinctly describes how the driver and runtime API interact. To concretely answer your main question: If one wants dynamic kernel definition as in cuModuleLoad and cublas at the same time, what are the options? The basic approach goes something like this: Use the driver API to establish a context on the device as you would normally do. Call the runtime API routine cudaSetDevice(). The runtime API will automagically bind to the existing driver API context. Note that device enumeration is identical and common between both APIs, so if you establish context on a given device number in the driver API, the same number will select the same GPU in the driver API You are now free to use any CUDA runtime API call or any library built on the CUDA runtime API. Behaviour is the same as if you relied on runtime API \"lazy\" context establishment"
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "I'm trying to create a random number from a normal distribution inside a CUDA kernel, using PyCUDA (version 2011,2,2), in Python 2.7.3 on a Tesla C2075 (CUDA 5.0, V0.2.1221). However I am having issues in trying to simply include the curand_kernel.h header. I have created the most simple test case (shown below), but on compilation I receive the error. /usr/local/cuda/bin/../include/curand_kernel.h(568): error: this declaration may not have extern \"C\" linkage Despite searching around I unfortunately still don't fully understand what the problem is, which has made it hard in trying to resolve it. Any help would be greatly appreciated. import pycuda.driver as drv import pycuda.compiler import pycuda.autoinit import pycuda.gpuarray as gpuarray import pycuda.cumath as cumath from pycuda.compiler import SourceModule gpu_test_source = SourceModule(\"\"\" #include &lt;curand_kernel.h&gt; __global__ void test_kernel() { return; } \"\"\")",
        "answers": [
            [
                "Did you try the following?: gpu_test_source = SourceModule(\"\"\" #include &lt;curand_kernel.h&gt; extern \"C\"{ __global__ void test_kernel() { return; } } \"\"\", no_extern_c=True)"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm writing a Python package that does GPU computing using the PyCUDA library. PyCUDA needs to initialize a GPU device (usually by importing pycuda.autoinit) before any of its submodules can be imported. In my own modules I import whatever submodules and functions I need from PyCUDA, which means that my own modules are not importable without first initializing PyCUDA. That's fine mostly, because my package does nothing useful without a GPU present. However, now I want to write documentation and Sphinx Autodoc needs to import my package to read the docstrings. It works fine if I put import pycuda.autoinit into docs/conf.py, but I would like for the documentation to be buildable on machines that don't have an NVIDIA GPU such as my own laptop or readthedocs.org. What's the most elegant way to defer the of import my dependencies such that I can import my own submodules on machines that don't have all the dependencies installed?",
        "answers": [
            [
                "The autodoc mechanism requires that all modules to be documented are importable. When this requirement is a problem, mocking (replacing parts of the system with mock objects) can be a solution. Here is an article that explains how mock objects can be used when working with Sphinx: http://blog.rtwilson.com/how-to-make-your-sphinx-documentation-compile-with-readthedocs-when-youre-using-numpy-and-scipy/. The gist of the article is that it should work if you add something like this to conf.py: import mock # See http://www.voidspace.org.uk/python/mock/ MOCK_MODULES = ['module1', 'module2', ...] for mod_name in MOCK_MODULES: sys.modules[mod_name] = mock.Mock()"
            ],
            [
                "The usual method I've seen is to have a module-level function like foo.init() that sets up the GPU/display/whatever that you need at runtime but don't want automatically initialized on import. You might also consider exposing initialization options here: what if I have 2 CUDA-capable GPUs, but only want to use one of them?"
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am learning PyCUDA, and while going through the documentation on pycuda.gpuarray, I am puzzled by the difference between pycuda.driver.memcpy_htod (also _dtoh) and pycuda.gpuarray.to_gpu (also get) functions. According to gpuarray documentation, .get(). For example,transfer the contents of self into array or a newly allocated numpy.ndarray. If array is given, it must have the right size (not necessarily shape) and dtype. If it is not given, a pagelocked specifies whether the new array is allocated page-locked. Is this saying that .get() is implemented exactly the same way as pycuda.driver.memcpy_dtoh? Somehow, I think I am mis-interpreting it.",
        "answers": [
            [
                "pycuda.gpuarray.GPUArray.get() stores the GPUArray as a numpy.ndarray. pycuda.driver.memcpy_dtoh() and friends copy plain buffers between CPU and GPU memory without any processing of the data in the buffers."
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "I found an answer here, but it is not clear if I should reshape the array. Do I need to reshape the 2d array into 1d before passing it to pycuda kernel?",
        "answers": [
            [
                "There is no need to reshape a 2D gpuarray in order to pass it to a CUDA kernel. As I said in the answer you linked to, a 2D numpy or PyCUDA array is just an allocation of pitched linear memory, stored in row major order by default. Both have two members which tell you everything that you need to access an array - shape and strides. For example: In [8]: X=np.arange(0,15).reshape((5,3)) In [9]: print X.shape (5, 3) In [10]: print X.strides (12, 4) The shape is self explanatory, the stride is the pitch of the storage in bytes. The best practice for kernel code will be to treat the pointer supplied by PyCUDA as if it were allocated using cudaMallocPitch and treat the first element of stride as the byte pitch of the rows in memory. A trivial example might look like this: import pycuda.driver as drv from pycuda.compiler import SourceModule import pycuda.autoinit import numpy as np mod = SourceModule(\"\"\" __global__ void diag_kernel(float *dest, int stride, int N) { const int tid = threadIdx.x + blockDim.x * blockIdx.x; if (tid &lt; N) { float* p = (float*)((char*)dest + tid*stride) + tid; *p = 1.0f; } } \"\"\") diag_kernel = mod.get_function(\"diag_kernel\") a = np.zeros((10,10), dtype=np.float32) a_N = np.int32(a.shape[0]) a_stride = np.int32(a.strides[0]) a_bytes = a.size * a.dtype.itemsize a_gpu = drv.mem_alloc(a_bytes) drv.memcpy_htod(a_gpu, a) diag_kernel(a_gpu, a_stride, a_N, block=(32,1,1)) drv.memcpy_dtoh(a, a_gpu) print a Here some memory is allocated on the device, a zeroed 2D array is copied to that allocation directly, and the result of the kernel (filling the diagonals with 1) copied back to the host and printed. It isn't necessary to flatten or otherwise modify the shape or memory layout of the 2D numpy data at any point in the process. The result is: $ cuda-memcheck python ./gpuarray.py ========= CUDA-MEMCHECK [[ 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] ========= ERROR SUMMARY: 0 errors"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm using VS2008, Win XP, latest CUDA toolkit. I run pip install pycuda on windows and get following log from C:\\Documents and Settings\\User\\Application Data\\pip\\pip.log I get error LINK : fatal error LNK1181: cannot open input file 'cuda.lib' error: command '\"C:\\Program Files\\Microsoft Visual Studio 9.0\\VC\\BIN\\link.exe\"' failed with exit status 1181 I think I need to specify some path variable to cuda lib, but I don't understand what variable and why it don't set during instalation of cuda toolkit. UPDATE: I manged to resolve this issue installing prebuild pycuda from here , but maybe it will work slower because it wasn't compiled on my machine.",
        "answers": [
            [
                "In case someone is still looking for an answer: configure.py generates a siteconf.py file containing the paths to CUDA .lib files used to compile pycuda. However, it uses incorrect paths (at least when on Windows and using toolkit V7.5). Now this can be fixed multiple ways (make sure you've downloaded the pycuda package and decompressed it somewhere): 1. Modifying setup.py This is where the main culprit lies. These are the paths it currently uses: default_lib_dirs = [ \"${CUDA_ROOT}/lib\", \"${CUDA_ROOT}/lib64\", # https://github.com/inducer/pycuda/issues/98 \"${CUDA_ROOT}/lib/stubs\", \"${CUDA_ROOT}/lib64/stubs\", ] Currently Nvidia uses CUDA_PATH as environmental variable, and the .lib files are stored within a separate x64 or Win32 folder. You can either add these paths to the array, or just get rid of the incorrect ones default_lib_dirs = [\"${CUDA_PATH}/lib/x64\", \"${CUDA_PATH}/lib/Win32\"] now run py configure.py to generate the siteconf.py file. 2. Override settings generated by configure.py As mentioned, configure.py generates the siteconf.py file. You can call configure.py with optional parameters to override the default library folders (what we defined in setup.py). Partial output after running configure.py --help --cudadrv-lib-dir=DIR Library directories for Cudadrv (default: ${CUDA_PATH}/lib/x64) (several ok) --cudadrv-libname=LIBNAME Library names for Cudadrv (without lib or .so) (default: cuda) (several ok) --cudart-lib-dir=DIR Library directories for Cudart (default: ${CUDA_PATH}/lib/x64) (several ok) --cudart-libname=LIBNAME Library names for Cudart (without lib or .so) (default: cudart) (several ok) --curand-lib-dir=DIR Library directories for Curand (default: ${CUDA_PATH}/lib/x64) (several ok) --curand-libname=LIBNAME Library names for Curand (without lib or .so) (default: curand) (several ok) 3. Modify siteconf.py directly The easiest method. Just run py configure.py to generate a siteconf.py file with default paths, and edit that file afterwards. Later on I figured both these pages recommend doing just that: https://kerpanic.wordpress.com/2015/09/28/pycuda-windows-installation-offline/ https://wiki.tiker.net/PyCuda/Installation/Windows Finish the installation To wrap it all up, compile and install pycuda by running: py setup.py build py setup.py install (this will use the previously generated/modified siteconf.py file). That's it :) (If you are wondering why I wrote down all 3 methods instead of just the most easiest one, I actually found out about the siteconf.py and configure.py file after I messed around with default_lib_dirs in the setup.py file. Same for the two website links, I found those after manually solving the problem)"
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "I'm trying to call a CUDA kernel from another kernel, but get the following error : Traceback (most recent call last): File \"C:\\temp\\GPU Program Shell.py\", line 22, in &lt;module&gt; \"\"\") File \"C:\\Python33\\lib\\site-packages\\pycuda\\compiler.py\", line 262, in __init__ arch, code, cache_dir, include_dirs) File \"C:\\Python33\\lib\\site-packages\\pycuda\\compiler.py\", line 252, in compile return compile_plain(source, options, keep, nvcc, cache_dir) File \"C:\\Python33\\lib\\site-packages\\pycuda\\compiler.py\", line 134, in compile_plain cmdline, stdout=stdout.decode(\"utf-8\"), stderr=stderr.decode(\"utf-8\")) pycuda.driver.CompileError: nvcc compilation of c:\\users\\karste~1\\appdata\\local\\temp\\tmpgq8t45\\kernel.cu failed [command: nvcc --cubin -arch sm_35 -m64 -Ic:\\python33\\lib\\site-packages\\pycuda\\cuda kernel.cu] [stderr: kernel.cu(14): error: kernel launch from __device__ or __global__ functions requires separate compilation mode My understanding is that this is has to do with Dynamic Parallelism and the other question related to this error is due to a user without approppriate hardware. I have a GTX Titan, however, so it should be compatible. What am I missing? EDIT After adding \"options=['--cubin','-rdc=true' ,'-lcudart', '-lcudadevrt,','-Ic:\\python33\\lib\\site-packages\\pycuda\\cuda kernel.cu']\" to SourceModule, I get the following error: Traceback (most recent call last): File \"C:\\temp\\GPU Program Shell.py\", line 22, in &lt;module&gt; \"\"\", options=['--cubin','-rdc=true' ,'-lcudart', '-lcudadevrt,','-Ic:\\python33\\lib\\site-packages\\pycuda\\cuda kernel.cu']) File \"C:\\Python33\\lib\\site-packages\\pycuda\\compiler.py\", line 265, in __init__ self.module = module_from_buffer(cubin) pycuda._driver.LogicError: cuModuleLoadDataEx failed: not found -",
        "answers": [
            [
                "Python is compiling the CUDA code on the fly: nvcc --cubin -arch sm_35 -m64 -Ic:\\python33\\lib\\site-packages\\pycuda\\cuda kernel.cu In order to compile code containing dynamic parallelism, it's necessary to add specific switches to the compile command to enable separate compilation, device code linking, linking of the device runtime library, and the appropriate architecture target (sm_35). Some examples of valid nvcc command combinations are given in the programming guide section on dynamic parallelism. Your command line should look something like: nvcc --cubin -arch=sm_35 -m64 -rdc=true -Ic:\\python33\\lib\\site-packages\\pycuda\\cuda kernel.cu -lcudadevrt You may also wish to read the nvcc manual on separate compilation."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "When I try to run the following code, I get this error : Traceback (most recent call last): File \"C:\\temp\\GPU Program Shell.py\", line 28, in &lt;module&gt; dev=mod.get_function(\"lol\") File \"C:\\Python33\\lib\\site-packages\\pycuda\\compiler.py\", line 285, in get_function return self.module.get_function(name) pycuda._driver.LogicError: cuModuleGetFunction failed: not found Here's the code : mod = SourceModule(\"\"\" extern \"C\" { __device__ void lol(double *a) { a[0]=1; } __global__ void kernel(double *a) { const int r = blockIdx.x*blockDim.x + threadIdx.x; a[r] = 1; } } \"\"\") max_length = 5 a = numpy.zeros(max_length) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu, a) func = mod.get_function(\"kernel\") dev=mod.get_function(\"lol\") dev(a_gpu) newa = numpy.empty_like(a) cuda.memcpy_dtoh(newa, a_gpu) print(newa) print(a) As you can probably see, this is a slight modification of the PyCUDA tutorial code. My intent is to call this device function which is going to launch kernels and integrate things and generally make my life easier. I did a bit of googling and I knew that I had to put \"extern \"c\"\" into my code because of name mangling and have had success with this before when I was just using PyCUDA to launch a kernel instead of a device function. Along the same lines, if I change my code to launch the kernel instead of the device function, it does what I want it to. What am I missing here? Karsten A little bit more looking into the Device Interface Reference documentation and it seems like the function get_function only deals with global functions? Did I interpret that correctly? If so, am I able to do what I'm trying to do?",
        "answers": [
            [
                "You cannot call a __device__ function from host code. If you're indicating that the PyCUDA tutorial code shows how to do this, I'd like to see that tutorial. It's not clear to me what you're trying to accomplish with calling the __device__ function from host code that could not be done with an ordinary kernel (__global__) launch."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "FYI, I have 64 bit version of Python 2.7 and I followed the pycuda installation instruction to install pycuda. And I don't have any problem running following script. import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy a = numpy.random.randn(4,4) a = a.astype(numpy.float32) a_gpu = cuda.mem_alloc(a.nbytes) cuda.memcpy_htod(a_gpu,a) But after that, when executing this statement, mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y * 4; a[idx] *= 2; } \"\"\") I got the error messages CompileError: nvcc compilation of c:\\users\\xxxx\\appdata\\local\\temp\\tmpaoxt97\\kernel.cu failed [command: nvcc --cubin -arch sm_21 -m64 -Ic:\\python27\\lib\\site-packages\\pycuda\\cuda kernel.cu] [stderr: nvcc : fatal error : nvcc cannot find a supported version of Microsoft Visual Studio. Only the versions 2008, 2010, and 2012 are supported But I have VS 2008 and VS 2010 installed on the machine and set path and nvcc profile as instructed. Anybody tell me what's going on? UPDATE1: As cgohike pointed out, running following statements before the problematic statement will solve the problem. import os os.system(\"vcvarsamd64.bat\")",
        "answers": [
            [
                "Well, it was too early to call it final. Even with resolution from cgohike, I got the same error when I ran other script like this import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import pycuda.autoinit import numpy a_gpu = gpuarray.to_gpu(numpy.random.randn(4, 4)) print \"a_gpu =\" print a_gpu a_doubled = (2*a_gpu).get() print print \"a_doubled =\" print a_doubled And then I found this answer. So in my case, I added following line in nvcc.profile COMPILER-BINDIR = C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin\\amd64 After this, no more compiler error I got. Hope it helps others."
            ],
            [
                "Call \"C:\\Program Files (x86)\\Microsoft Visual Studio 9.0\\VC\\vcvarsall.bat\" amd64 or \"C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\vcvarsall.bat\" amd64 before python.exe. That will set all the necessary environment variables to use the 64 bit Visual Studio compiler from Python or the command line."
            ]
        ],
        "votes": [
            3.0000001,
            2.0000001
        ]
    },
    {
        "question": "I ran the deviceQuery and got the following result ./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"GeForce GTX 560 Ti\" CUDA Driver Version / Runtime Version 5.0 / 5.0 CUDA Capability Major/Minor version number: 2.1 Total amount of global memory: 1024 MBytes (1073283072 bytes) (8) Multiprocessors x ( 48) CUDA Cores/MP: 384 CUDA Cores GPU Clock rate: 1701 MHz (1.70 GHz) Memory Clock rate: 2052 Mhz Memory Bus Width: 256-bit L2 Cache Size: 524288 bytes Max Texture Dimension Size (x,y,z) 1D=(65536), 2D=(65536,65535), 3D= (2048,2048,2048) Max Layered Texture Size (dim) x layers 1D=(16384) x 2048, 2D=(16384,16384) x 2048 Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 32768 Warp size: 32 Maximum number of threads per multiprocessor: 1536 Maximum number of threads per block: 1024 Maximum sizes of each dimension of a block: 1024 x 1024 x 64 Maximum sizes of each dimension of a grid: 65535 x 65535 x 65535 Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 1 copy engine(s) Run time limit on kernels: Yes Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device PCI Bus ID / PCI location ID: 1 / 0 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 5.0, CUDA Runtime Version = 5.0, NumDevs = 1, Device0 = GeForce GTX 560 Ti My understanding is that I can create maximum of 65535 x 65535 x 65535 blocks with 1024 threads per block. Does that I can have 65535 x 65535 x 65535 x 1024 threads of maximum ? If not what is the maximum number of threads I can have ? Can anyone clarify this doubt ?",
        "answers": [
            [
                "Your understanding is correct. You can launch 65535 x 65535 x 65535 x 1024 threads theoretically but due to resource constraints you may be not able to hit the maximum."
            ],
            [
                "You can't just multiply all the maximum grid dimensions and assume that you can have that many threads, unfortunately. You have 8 MPs and a maximum number of threads per MP = 1536, so that makes 8 * 1536 = 12288 threads max."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "Consumer-grade Nvidia GPUs are expected to have about 1-10 soft memory errors per week. If you somehow manage to detect an error on a system without ECC (e.g. if the results were abnormal) what steps are necessary and sufficient to recover from it? Is it enough to just reload all of the data to the GPU (cuda.memcpy_htod in PyCuda), or do you need to reboot the system? What about the \"kernel\", rather than data?",
        "answers": [
            [
                "A soft memory error (meaning incorrect results due to noise of some kind), shouldn't require a reboot. Just rewind back to some known good position, reload data to the GPU and proceed."
            ],
            [
                "Of course, it depends on what was located in the memory that was corrupted. I have accidentally overwritten memory on GPUs that required a reboot to fix, so it seems that could happen if memory is randomly corrupted as well. I think the GPU drivers reside partially in GPU memory. For critical calculations, one can guard against soft memory errors by running the same calculation twice (including memory copies, etc) and comparing the result. Since the compute cards with ECC are often more than twice as expensive as the graphics cards, it may be less expensive to just purchase two graphics cards and run the same calculations on both and compare all results. That has the added benefit of enabling doubling the calculation speed for non-critical calculations."
            ]
        ],
        "votes": [
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "How do I get PyCuda to pull in the array of strings rather than one char string? If you uncomment the line within the C code, you'll see that its iterating through each character and not through each string. For now i'm just trying to calculate the length of each string, but will eventually make this into a word frequency counter. First step: pass in an array... Right now the desired output should be 25,27,44 import pycuda.driver as drv import pycuda.tools import pycuda.autoinit import numpy from pycuda.compiler import SourceModule # create an array of 1s lines = numpy.array(['ok this is the first line','number two line is this one','alright last line is in the third place here']) lines = numpy.array(lines) blocks = len(lines) block_size = 1 nbr_values = blocks * block_size # create a destination array that will receive the result a = numpy.zeros(nbr_values).astype(numpy.float32) dest = numpy.zeros_like(a) ###################### # SourceModele SECTION mod = SourceModule(\"\"\" __global__ void gpusin(float *dest, char *lines) { const int i = blockDim.x*blockIdx.x + threadIdx.x; dest[i] = sizeof (lines[i]); //dest[i] = lines[i]; //uncomment this line to see that its iterating through individual chars not strings } \"\"\") #Run the sourc model gpusin = mod.get_function(\"gpusin\") gpusin(drv.Out(dest), drv.In(lines), grid=(blocks,1), block=(block_size,1,1) ) print str(dest) print lines",
        "answers": [
            [
                "import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy # create an array of 1s data1 = ['Apple 12345 this is a long string again','Ding dong, it\"s Pyton!!','digity! gigity!'] lines = numpy.array( data1, dtype=str) linesGPU = cuda.mem_alloc(lines.size * lines.dtype.itemsize) cuda.memcpy_htod(linesGPU, lines) blocks = len(data1) threadsPerBlock = lines.dtype.itemsize nbr_values = lines.size * lines.dtype.itemsize # blocks * block_size print(\"lines size: \" + str(lines.size) + \" itemsize : \" + str(lines.dtype.itemsize)) # create a destination array that will receive the result dest = numpy.zeros((nbr_values,), dtype=numpy.str) destGPU = cuda.mem_alloc(dest.size * dest.dtype.itemsize) mod = SourceModule(\"\"\" __global__ void process(char **dest, char **line) { int tID = threadIdx.x ;//+ blockIdx.x * blockDim.x; dest[tID] = line[tID]; } \"\"\") #Run the sourc model gpusin = mod.get_function(\"process\") gpusin(destGPU, linesGPU, grid=(blocks,1), block=(threadsPerBlock,1,1)) cuda.memcpy_dtoh(dest, destGPU) print str(len(dest)) print dest"
            ]
        ],
        "votes": [
            -0.9999999
        ]
    },
    {
        "question": "I have a strange problem which origin I cannot determine: I have a working Kernel for some special Matrix-Vector-multiplication, which I want to speed up. Basically the big matrix (10^6 times 10^6) is constructed from few small matrices. So I want to put that data in shared memory. However when I try to add the shared memory, I only get the error: pycuda._driver.LogicError: cuLaunchKernel failed: invalid value So my working kernel is: #define FIELD_SIZE {field} #define BLOCK_SIZE {block} __global__ void MatrixMulKernel(double *gpu_matrix, double *gpu_b, double *gpu_y) { int tx = ... + threadIdx.x; if(tx &lt; FIELD_SIZE*FIELD_SIZE*BLOCK_SIZE) { ... multiplication ... } } And if I try to add the shared memory part it looks like #define FIELD_SIZE {field} #define BLOCK_SIZE {block} __global__ void MatrixMulKernel(double *gpu_matrix_ptr, double *gpu_b, double *gpu_y) { __shared__ double gpu_matrix[BLOCK_SIZE*BLOCK_SIZE*13]; int tx = ... + threadIdx.x; if(tx &lt; BLOCK_SIZE*BLOCK_SIZE*13) { gpu_matrix[tx] = gpu_matrix_ptr[tx]; } __syncthreads(); if(tx &lt; FIELD_SIZE*FIELD_SIZE*BLOCK_SIZE) { ... multiplication ... } } This is the only part I changed, so basically it has to be the gpu_matrix[tx] = gpu_matrix_ptr[tx] statement, right? But I fail to see how that should be. I basically tried to copy the tiled matrix-multiplication example from the pycuda examples. The invocation is: self.kernel.prepare([np.intp, np.intp, np.intp]) self.kernel.prepared_call(grid_shape, block_shape, self.matrix_gpu.gpudata, b_gpu.gpudata, y_gpu.gpudata) where matrix_gpu, b_gpu and y_gpu are pycuda.gpuarray instances. I hope that you can clear up some of my confusion...",
        "answers": [
            [
                "According to your description, the shared mem your allocated is too big. __shared__ double gpu_matrix[BLOCK_SIZE*BLOCK_SIZE*13]; shared mem is one of the hardware resources of cuda gpu. the total size is about 48KBytes, which you can not increase. CUDA actually provides a tool in the following dir to help you calculate the hardware resources you can use. $CUDA_ROOT/tools/CUDA_Occupancy_Calculator.xls On the other hand, the size of shared mem required by mat-vec-mul-like kernels should be able to reduce from O(BLOCK_SIZE^2) to O(BLOCK_SIZE). You may want to read code of some successful mat-vec-mul kernels such as MAGMA before implement your own."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "This question already has answers here: How do I choose grid and block dimensions for CUDA kernels? (3 answers) Closed 9 years ago. I got a question regarding the dimensions of the blocks and grids in (py)CUDA. I know that there are limits in the total size of the blocks, but not of the grids And that the actual blocksize influences the runtime. But what I'm wondering about is: Does it make a difference if I have a block of 256 threads, to start it like (256,1) or to start it like (128,2), like (64,4) etc. If it makes a difference: which is the fastest?",
        "answers": [
            [
                "Yes, it makes a difference. (256,1) creates a (1D) block of 256 threads in the X-dimension, all of which have a y-index of 0. (128,2) creates a (2D) block of 128x2 threads, ie. 128 in the x-dimension and 2 in the y-dimension. These threads will have an x-index ranging from 0 to 127 and a y-index ranging from 0 to 1 The structure of your kernel code must comprehend the thread indexing/numbering. For example if your kernel code starts with something like: int idx=threadIdx.x+blockDim.x*blockIdx.x; and doesn't create any other index variables, it's probably assuming a 1D threadblock and 1D grid. If, on the other hand, your kernel code starts with something like: int idx = threadIdx.x+blockDim.x*blockIdx.x; int idy = threadIdx.y+blockDim.y*blockIdx.y; It's probably expecting a 2D grid and 2D threadblocks. Generally speaking, the two approaches are not interchangeable, meaning you cannot launch a kernel that expects a 1D grid with a 2D grid and expect everything to work normally, and vice-versa."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I followed instructions here . I have installed all packages from http://www.lfd.uci.edu/~gohlke/pythonlibs/ (all the latest one). It seems I installed successfully. I ran the code below in Ipython: import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import pycuda.autoinit import numpy a_gpu = gpuarray.to_gpu(numpy.random.randn(4,4).astype(numpy.float32)) ## pass a_doubled = (2*a_gpu).get() ## the line can't be passed with Ipython and got this error: File \"C:\\Python27\\lib\\site-packages\\pycuda\\compiler.py\", line 137, in compile_plain lcase_err_text = (stdout+stderr).decode(\"utf-8\").lower() File \"C:\\Python27\\lib\\encodings\\utf_8.py\", line 16, in decode return codecs.utf_8_decode(input, errors, True) UnicodeDecodeError: 'utf8' codec can't decode byte 0xb8 in position 109: invalid start byte How to solve this issue? I have struggled several days.",
        "answers": [
            [
                "This appears to have been caused by an error handling issue inside PyCUDA when code contains unparseable unicode. The bug was fixed in late 2013 and should have been pushed in the PyCUDA 2014.1 release. [This answer was added as a community wiki entry to get this question off the unanswered list for the CUDA and PyCUDA tags]"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Based on my reading of the PyCUDA documentation, the samples and the book on CUDA by Kirk and Hwu, I have successfully implemented a CUDA C-based complex matrix multiplication program and have also written a version in PyCUDA. The C code produces the correct results, but the Python code doesn't. To be clear, the Python code is simply taken from the samples (MatrixMulTiled) and has been modified to handle complex numbers using cuComplexFloat from \"cuComplex.h\". Before this modification, it was correctly multiplying real-valued matrices. So I can't figure out the error. The Python code is # attempt to do matrix multiplication for complex numbers import pycuda.autoinit from pycuda import driver, compiler, gpuarray, tools import numpy as np from time import * kernel_code_template = \"\"\" #include &lt;cuComplex.h&gt; __global__ void MatrixMulKernel(cuFloatComplex *A, cuFloatComplex *B, cuFloatComplex *C) { const uint wA = %(MATRIX_SIZE)s; const uint wB = %(MATRIX_SIZE)s; // Block index const uint bx = blockIdx.x; const uint by = blockIdx.y; // Thread index const uint tx = threadIdx.x; const uint ty = threadIdx.y; // Index of the first sub-matrix of A processed by the block const uint aBegin = wA * %(BLOCK_SIZE)s * by; // Index of the last sub-matrix of A processed by the block const uint aEnd = aBegin + wA - 1; // Step size used to iterate through the sub-matrices of A const uint aStep = %(BLOCK_SIZE)s; // Index of the first sub-matrix of B processed by the block const int bBegin = %(BLOCK_SIZE)s * bx; // Step size used to iterate through the sub-matrcies of B const uint bStep = %(BLOCK_SIZE)s * wB; // The element of the block sub-matrix that is computed by the thread cuFloatComplex Csub = make_cuFloatComplex(0,0); // Loop over all the sub-matrices of A and B required to compute the block sub-matrix for (int a = aBegin, b = bBegin; a &lt;= aEnd; a += aStep, b += bStep) { // Shared memory for the sub-matrix of A __shared__ cuFloatComplex As[%(BLOCK_SIZE)s][%(BLOCK_SIZE)s]; // Shared memory for the sub-matrix of B __shared__ cuFloatComplex Bs[%(BLOCK_SIZE)s][%(BLOCK_SIZE)s]; // Load the matrices from global memory to shared memory; // each thread loads one element of each matrix As[ty][tx] = make_cuFloatComplex(cuCrealf(A[a + wA*ty + tx]),cuCimagf(A[a + wA*ty + tx])); Bs[ty][tx] = make_cuFloatComplex(cuCrealf(B[b + wB*ty + tx]),cuCimagf(B[b + wA*ty + tx])); // Synchronize to make sure the matrices are loaded __syncthreads(); // Multiply the two matrcies together // each thread computes one element of the block sub-matrix for(int k = 0; k &lt; %(BLOCK_SIZE)s; ++k) { Csub = cuCaddf(Csub,cuCmulf(As[ty][k],Bs[k][tx])); } // Synchronize to make sure that the preceding computation // is done before loading two new sub-matrices of A and B in the next iteration __syncthreads(); } // Write the block sub-matrix to global memory // each thread writes one element const uint c = wB * %(BLOCK_SIZE)s * by + %(BLOCK_SIZE)s * bx; C[c + wB*ty + tx] = make_cuFloatComplex(cuCrealf(Csub), cuCimagf(Csub)); } \"\"\" MATRIX_SIZE = 4 TILE_SIZE = 2 BLOCK_SIZE = TILE_SIZE a_cpu = np.zeros(shape=(MATRIX_SIZE,MATRIX_SIZE)).astype(np.complex) b_cpu = np.zeros(shape=(MATRIX_SIZE,MATRIX_SIZE)).astype(np.complex) a_cpu[:,:] = 1 + 1j*0 b_cpu[:,:] = 1 + 1j*2 # compute reference on the CPU to verify GPU computation t1 = time() c_cpu = np.dot(a_cpu, b_cpu) t2 = time() t_cpu = t2-t1 # transfer host (CPU) memory to device (GPU) memory a_gpu = gpuarray.to_gpu(a_cpu) b_gpu = gpuarray.to_gpu(b_cpu) # create empty gpuarry for the result (C = A * B) c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.complex) # get the kernel code from the template # by specifying the constant MATRIX_SIZE kernel_code = kernel_code_template % { 'MATRIX_SIZE': MATRIX_SIZE, 'BLOCK_SIZE': BLOCK_SIZE, } # compile the kernel code mod = compiler.SourceModule(kernel_code) # get the kernel function from the compiled module matrixmul = mod.get_function(\"MatrixMulKernel\") # call the kernel on the card t1 = time() matrixmul( # inputs a_gpu, b_gpu, # output c_gpu, # grid of multiple blocks grid = (MATRIX_SIZE/TILE_SIZE, MATRIX_SIZE/TILE_SIZE), # block of multiple threads block = (TILE_SIZE, TILE_SIZE, 1), ) t2 = time() t_gpu = t2-t1 # print the results print(\"-\" * 80) print(\"Matrix A (GPU): \") print(a_gpu.get()) print(\"-\" * 80) print(\"Matrix B (GPU): \") print(b_gpu.get()) print(\"-\" * 80) print(\"Matrix C (GPU): \") print(c_gpu.get()) print(\"-\" * 80) print(\"Matrix C (CPU): \") print(c_cpu) print(\"-\" * 80) print(\"CPU-GPU Difference: \") print(c_cpu-c_gpu.get()) print(\"CPU Time \", t_cpu) print(\"GPU Time \", t_gpu) np.allclose(c_cpu, c_gpu.get() ) The C-code is #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;cuComplex.h&gt; /* __global__ void MatrixMulKernel(...) * This is the main Matrix Multiplication Kernel * It is executed on the device (GPU). */ __global__ void MatrixMulKernel(cuFloatComplex *Md, cuFloatComplex *Nd, cuFloatComplex *Pd, int Width, int TILE_WIDTH) { // Calculate the row index of the Pd element and M int Row = blockIdx.y*TILE_WIDTH + threadIdx.y; // Calculate the column index of the Pd element and N int Col = blockIdx.x*TILE_WIDTH + threadIdx.x; cuFloatComplex Pvalue = make_cuFloatComplex(0,0); // each thread computes one element of the block sub-matrix for(int k = 0; k &lt; Width; k++) { Pvalue = cuCaddf(Pvalue,cuCmulf(Md[Row*Width + k],Nd[k*Width + Col])); } Pd[Row*Width + Col] = make_cuFloatComplex(cuCrealf(Pvalue),cuCimagf(Pvalue)); } /* void MatrixMultiplication(...) * This is the stub function for the matrix multiplication kernel * It is executed on the host. It takes inputs from the main() function * and declares memory, copies data to the device, invokes the kernel * and copies the result from the device back to the host. */ void MatrixMultiplication(cuFloatComplex *M, cuFloatComplex *N, cuFloatComplex *P, int Width, int TILE_WIDTH) { int size = Width*Width*sizeof(cuFloatComplex); cuFloatComplex *Md, *Nd, *Pd; // Transfer M and N to device memory cudaMalloc((void**) &amp;Md, size); cudaMemcpy(Md, M, size, cudaMemcpyHostToDevice); cudaMalloc((void**) &amp;Nd, size); cudaMemcpy(Nd, N, size, cudaMemcpyHostToDevice); // allocate P on the device cudaMalloc((void**) &amp;Pd, size); // setup the execution configuration dim3 dimGrid(Width/TILE_WIDTH, Width/TILE_WIDTH); dim3 dimBlock(TILE_WIDTH, TILE_WIDTH); // Launch the device computation kernel MatrixMulKernel&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(Md, Nd, Pd, Width, TILE_WIDTH); // Transfer P from device to host cudaMemcpy(P, Pd, size, cudaMemcpyDeviceToHost); // Free device matrices cudaFree(Md); cudaFree(Nd); cudaFree(Pd); } /* void printMatrix(..) * This is a function used to print the matrix */ void printMatrix(cuFloatComplex *M, int Width) { for(int i = 0; i &lt; Width; i++) { for(int j = 0; j &lt; Width; j++) { printf(\" %f+i%f\", cuCrealf(M[i*Width + j]), cuCimagf(M[i*Width + j])); } printf(\"\\n\"); } } int main() { /* Define dimension of matrix (Width x Width) */ int Width = 4; /* Define dimension of tile * This should be less than 512 */ int TILE_WIDTH = 2; /* Define pointers for row major matrices */ cuFloatComplex *M, *N, *P; M = (cuFloatComplex *)malloc(Width*Width*sizeof(cuFloatComplex)); N = (cuFloatComplex *)malloc(Width*Width*sizeof(cuFloatComplex)); P = (cuFloatComplex *)malloc(Width*Width*sizeof(cuFloatComplex)); /* Fill matrices arbitrarily */ for(int i = 0; i &lt; Width; i++) { for(int j = 0; j &lt; Width; j++) { M[i*Width + j] = make_cuFloatComplex(1,0); N[i*Width + j] = make_cuFloatComplex(1,2); P[i*Width + j] = make_cuFloatComplex(0,0); } } /* code to print matrices using helper function */ printf(\"Matrix M is \\n\\n\"); printMatrix(M, Width); printf(\"\\nMatrix N is \\n\\n\"); printMatrix(N, Width); /* Call the stub function for matrix multiplication */ MatrixMultiplication(M, N, P, Width, TILE_WIDTH); printf(\"\\nMatrix P is \\n\\n\"); printMatrix(P, Width); free(M); free(N); free(P); return 0; } The output of the Python code is Matrix C (GPU): [[ 1.59878214e-314 +1.59926782e-314j 1.59878214e-314 +1.59926782e-314j 1.59878214e-314 +1.59926782e-314j 1.59878214e-314 +1.59926782e-314j] [ 1.59878214e-314 +1.59926782e-314j 1.59878214e-314 +1.59926782e-314j 1.59878214e-314 +1.59926782e-314j 1.59878214e-314 +1.59926782e-314j] [ -9.01080877e+306 -5.19870527e+306j -1.45379609e+307 -8.65694841e+306j -4.14125486e+306 -2.15325816e+306j -5.83708063e+306 -3.25935506e+306j] [ -1.44828853e+306 -1.44828853e+306j -2.32949855e+306 -2.32949855e+306j -3.78945180e+306 -3.78945180e+306j -6.54203686e+306 -6.54203686e+306j]] -------------------------------------------------------------------------------- Matrix C (CPU): [[ 4.+8.j 4.+8.j 4.+8.j 4.+8.j] [ 4.+8.j 4.+8.j 4.+8.j 4.+8.j] [ 4.+8.j 4.+8.j 4.+8.j 4.+8.j] [ 4.+8.j 4.+8.j 4.+8.j 4.+8.j]] The C-output is Matrix P is 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 4.000000+i8.000000 I'd appreciate if someone could please point out the error in my Python code. I am trying to meet deadlines for my thesis, and all the rest of my code is in Python, so I have no time to port it to C. Thanks! ====================== EDIT: PROBLEM SOLVED This is most probably a precision issue. I fixed it by replacing the host transfer and empty matrix creation code with the following... # transfer host (CPU) memory to device (GPU) memory a_gpu = gpuarray.to_gpu(a_cpu.astype(np.complex64)) b_gpu = gpuarray.to_gpu(b_cpu.astype(np.complex64)) # create empty gpuarry for the result (C = A * B) c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.complex64) Hope this is helpful.",
        "answers": [
            [
                "This is most probably a precision issue. I fixed it by replacing the host transfer and empty matrix creation code with the following... # transfer host (CPU) memory to device (GPU) memory a_gpu = gpuarray.to_gpu(a_cpu.astype(np.complex64)) b_gpu = gpuarray.to_gpu(b_cpu.astype(np.complex64)) # create empty gpuarry for the result (C = A * B) c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.complex64) But I do find a problem in using PyCUDA to multiply large complex valued matrices. For instance, taking MATRIX_SIZE = 5040 and TILE_SIZE = 16 (which may not be a good choice from the standpoint of hardware), CUDA-C does manage to multiply the matrices, but PyCUDA crashes. Why should this happen?"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have two NVidia cards in my machine, and both are CUDA capable. When I run the example script to get started with PyCUDA seen here: http://documen.tician.de/pycuda/ i get the error nvcc fatal : Value 'sm_30' is not defined for option 'gpu-architecture' My computing GPU is compute capability 3.0, so sm_30 should be the right option for the nvcc compiler. My graphics GPU is only CC 1.2, so i thought maybe that's the problem. I've installed the CUDA 5.0 release for linux with no errors, and all the compiler components and python components. Is there a way to tell PyCUDA explicitly which GPU to use?",
        "answers": [
            [
                "nvcc isn't going to complain based on the specific GPUs you have installed. It will compile for whatever GPU type you tell it to compile for. The problem is you are specifying sm_30 which is not a valid option for --gpu-architecture when a --gpu-code option is also specified. You should be passing compute_30 for --gpu-architecture and sm_30 for --gpu-code Also be sure you have the correct nvcc in use and are not inadvertently using some old version of the CUDA toolkit. Once you have the compile problem sorted out, there is an environment variable CUDA_DEVICE that pycuda will observe to select a particular installed GPU. From here: CUDA_DEVICE=2 python my-script.py By the way someone else had your problem. Are you sure you don't have an old version of the CUDA toolkit laying around that PyCUDA is using?"
            ],
            [
                "I don't know about Python wrapper( or about Python in general), but in C++ you have WGL_NV_gpu_affinity NVidia extension which allows you to target a specific GPU. Probably you can write a wrapper for it in Python. EDIT: Now that I see you are actually running Linux, the solution is simpler (C++).You just need to enumerate XDisplay before context init. So basically the default GPU is usually targeted with Display string \"0.0\" To open display with second GPU you can do something like this: const char* gpuNum = \"0:1\"; if (!(_display = XOpenDisplay(gpuNum ))) { printf(\"error: %s\\n\", \"failed to open display\"); } else { printf(\"message: %s\\n\", \"display created\"); } ////here comes the rest of context setup...."
            ],
            [
                "At lest currently, it seems possible to just say import pycuda.driver as drv drv.Device(6).make_context() and this sets Device 6 as current context."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07,
            1e-07
        ]
    },
    {
        "question": "I have a working conjungate gradient method implementation in pycuda, that I want to optimize. It uses a self written matrix-vector-multiplication and the pycuda-native gpuarray.dot and gpuarray.mul_add functions Profiling the program with kernprof.py/line_profiler returned most time (&gt;60%) till convergence spend in one gpuarray.dot() call. (About .2 seconds) All following calls of gpuarray.dot() take about 7 microseconds. All calls have the same type of input vectors (size: 400 doubles) Is there any reason why? I mean in the end it's just a constant, but it is making the profiling difficult. I wanted to ask the question at the pycuda mailing list. However I wasn't able to subscribe with an @gmail.com adress. If anyone has either an explanation for the strange .dot() behavior or my inability to subscribe to that mailing list please give me a hint ;)",
        "answers": [
            [
                "One reason would be that Pycuda is compiling the kernel before uploading it. As far as I remember thought that should happen only the very first time it executes it. One solution could be to \"warm up\" the kernel by executing it once and then start the profiling procedure."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am calling this python file: import sys if __name__ == \"__main__\": print sys.argv[1] From PHP like this: $param = \"hello\"; $result = shell_exec(\"python /path/to/python/file.py \" . $param); echo $result; Which works great as hello is displayed when running the PHP file. However, when I import pyCuda (2012.1) into the python file, the output is no longer returned to PHP even though it is still shown when running the command from bash. This is the python file with pyCuda imported: from pycuda import driver as drv from pycuda import tools from pycuda import autoinit import sys if __name__ == \"__main__\": print sys.argv[1] Why does PHP no longer receive the output when pyCuda is imported and what is the best way to read the output from this python file in PHP? Many thanks!",
        "answers": [
            [
                "pycuda.autoinit registers an atexit autohandler to clean up the CUDA context it creates so that the python code exits gracefully. It is likely that this is terminating the application without flushing stdout, so the output never gets emitted. So either try explicit flushing of stdout: import sys print sys.argv[1] sys.stdout.flush() or don't use pycuda.autoinit and handle setup and pull down of the driver API context yourself."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I posted here not too long ago about a model I am trying to build using pycuda which solves About 9000 coupled ODEs. My model is too slow however and an SO member suggested that memory transfers from host to GPU is probably the culprit. Right now cuda is being used only to calculate the rate of change of each of the 9000 species I am dealing with. Since I am passing in an array from the host to the GPU to perform this calculation and returning an array from the GPU to integrate on the host I can see how this would slow things down. Would boost be the solution to my problem? From what I read, boost allows interoperability between c++ and python. It also includes c++ odeint , which I read, partnered with thrust allows quick reduction and integration all on the GPU. Is my understanding correct? Thank you, Karsten",
        "answers": [
            [
                "Yes, boost.odeint and boost.python should solve your problem. You can use odeint with Thrust. There are also some OpenCL libraries (VexCL, ViennaCL) which might be easier to use then Thrust. Have a look at thist paper for a comparions and for use cases of odeint on GPUs. Boost.python can do the communication between the C++ application and Python. Another approach would be a very slim command line application for solving the ODE (using boost.odeint) and which is entirely controlled by your python application."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am working on something that has highlighted the fact I don't have a firm grasp of how blocks and grids work in cuda. I have a 1000x10 matrix that I would like to traverse and fill in each element with a value. The kernel is like this: __global__ void myfun(float *vals,float *out, int M, int N) { int row = blockIdx.y*blockDim.y + threadIdx.y; int col = blockIdx.x*blockDim.x + threadIdx.x; int index = row*N + col; if( (row &lt; M ) &amp;&amp; (col &lt; N) ) { out[index] = index; } } where, M=1000 and N = 10. I don't know how to slice this up so that I can cover every element in the matrix. Since I need coverage for 1000*10 = 10,000 elements and given the limitations on the number of threads, I can't use block sizes of (10,1000,1). Using pycuda, I've tried things like block = (10,100,1), grid = (1,10) but I never get full coverage of the matrix elements. What's the right way to do this?",
        "answers": [
            [
                "Fix the block size, and keep the grid size dynamic. In this way, the kernel will cover each element of the matrix no matter what the values of M and N are. block = (8,8) grid = ((N + 7) / 8, (M + 7) / 8) Launch the kernel with this grid and block configuration. Keeping in limits of the device, you may change the block size if desired."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'm a graduate student in biophysics, trying to program a protein aggregation model using PyCUDA and Scipy's ODEInt. Within the past two weeks, I've gotten the code running, but it's very slow. Let me see if I can explain what my code does. I have an np array of N concentrations with each element being the concentration of the i+1 length polymer. I have a function that calculates the rate of change of the polymer concentrations using CUDA where each kernel calculates the rate of change of one specific length polymer. During this calculation, an (N-i-1) length array needs to be summed by the thread, drastically slowing down my code. Doing a little reading and Googling, I've come across parallel reduction as a way of invoking parallelism to make a serial calculation like an array sum go much faster. Of course I'm referring to Mark Harris' powerpoint slides. These were a great read and this looks like a potential way to drastically speed up my code, but I have a few questions : If the number of polymer species, N, needs to be ~ 8700-9000, is it conceivable to use CUDA to reduce these N arrays at the same time? Doing a quick calculation (again possible thanks to SO's great explanation of how to calculate the maximum number of concurrent threads), I get for my GTX Titan that I can have 15 * 64 * 32 = 30720 threads running at once. If I invoke my kernel on ~8960 kernels at a time, I should only have 21760 threads left to use, correct? Since it seems that you need at least (length of the array/2) threads to properly reduce it, then I'm doomed. I was thinking that perhaps I could use the remaining threads by dividing them up and reducing a few of the big arrays at a time in serial. I don't know...I'm just a physics grad student. I thought I'd ask the professionals before I embarked on a long journey in the wrong direction. Is it possible to easily and efficiently tell a kernel to reduce something? Thank you, Karsten Here's a representation of what I'm trying to do. fluxes and concs are np.arrays dcdt(concs, t) Call CUDA to calculate fluxes Thread 0 fluxes[i] = stuff + sum(concs[n] for n from 1 to 9000) 1 fluxes[i] = stuff + sum(concs[n] for n from 2 to 9000) 2 fluxes[i] = stuff + sum(concs[n] for n from 3 to 9000) ... N fluxes[i] = stuff You'll notice that the sum of the arrays that we've been talking about is basically a smaller version of the same array for each of the threads. This makes me wonder if this is something I should just do on the host.",
        "answers": [
            [
                "It's conceivable to use CUDA to reduce multiple arrays \"in parallel\". Reduction (summation) isn't a terribly compute-intensive operation, so if the data is not already resident on the GPU, then the cost to transfer the data to the GPU is likely to be a significant part (the majority) of the overall execution time. From your description, it's not clear if you're already doing this in some fashion on the GPU or if on the CPU. But if the data is on the GPU, then summing via parallel reduction will be fastest. Unless the data of a single array is larger than ~2GB, then the number of threads is not likely to be an issue. You could craft a kernel which simply reduces the arrays one after the other, in sequence. It seems you are saying there are N arrays, where N is around 9000. How big is each array? If the arrays are large enough, approximately all of the power of the GPU (roughly speaking) can be brought to bear on each individual operation, there's no significant penalty in that case to reducing the arrays, one after the other. The kernel then could be a basic parallel reduction, that looped over the arrays. Should be pretty straightforward. If you have roughly 9000 arrays to crunch, and it's not difficult to order your data in an interleaved fashion, then you might also consider an array of 9000 threads, where each thread sums the elements of a single array in a serial loop, pretty much the way you'd do it naively on CPU code. Data organization would be critical here, because the goal of all of this is to maximize utilization of available memory bandwidth. As the loop in each thread is picking up it's next data element to be summed, you would want to ensure that you have contiguous data reads amongst threads in a warp (coalesced access), thus implying an interleaved data storage arrangment amongst your N arrays. If that were the case, this approach would run quite fast as well. By the way, you might take a look at thrust which is relatively easy to use, and provides simple operations to do sum-reductions on arrays. As a prototype, it would be relatively easy to write a loop in thrust code that iteratively summed a sequence of arrays on the GPU."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I am implementing a password cracker for college work using PyCUDA. Everything seems to be working correctly except the implementation of the NTLM algorithm on CUDA. To test it out, I created a small module that launches a kernel with only 1 thread, hashes a value and returns it for comparison with the hash obtained on the CPU. Here is the code below: import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule import numpy from passlib.hash import nthash mod = SourceModule( \"\"\" #include &lt;string.h&gt; #include &lt;stdio.h&gt; #define INIT_A 0x67452301 #define INIT_B 0xefcdab89 #define INIT_C 0x98badcfe #define INIT_D 0x10325476 #define SQRT_2 0x5a827999 #define SQRT_3 0x6ed9eba1 __device__ void NTLM(char *, int, char*); //__device__ char hex_format[33]; __device__ __constant__ char itoa16[17] = \"0123456789ABCDEF\"; __global__ void NTBruteforce(char *hex_format){ int i; char test[4] = {'t', 'h', 'e', 'n'}; NTLM(test, 4, hex_format); } __device__ void NTLM(char *key, int key_length, char *hex_format) { unsigned int nt_buffer[16]; unsigned int output[4]; //Globals for rounds unsigned int a = INIT_A; unsigned int b = INIT_B; unsigned int c = INIT_C; unsigned int d = INIT_D; // Prepare the string for hash calculation int i; int length = key_length; //memset(nt_buffer, 0, 4); for (i = 0; i &lt; length / 2; i++) nt_buffer[i] = key[2 * i] | (key[2 * i + 1] &lt;&lt; 16); //padding if (length % 2 == 1) nt_buffer[i] = key[length - 1] | 0x800000; else nt_buffer[i] = 0x80; //put the length nt_buffer[14] = length &lt;&lt; 4; // NTLM hash calculation /* Round 1 */ a += (d ^ (b &amp; (c ^ d))) + nt_buffer[0]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[1]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[2]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[3]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); a += (d ^ (b &amp; (c ^ d))) + nt_buffer[4]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[5]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[6]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[7]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); a += (d ^ (b &amp; (c ^ d))) + nt_buffer[8]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[9]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[10]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[11]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); a += (d ^ (b &amp; (c ^ d))) + nt_buffer[12]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[13]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[14]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[15]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); /* Round 2 */ a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[0] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[4] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[8] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[12] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[1] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[5] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[9] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[13] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[2] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[6] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[10] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[14] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[3] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[7] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[11] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[15] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); /* Round 3 */ a += (d ^ c ^ b) + nt_buffer[0] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[8] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[4] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[12] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); a += (d ^ c ^ b) + nt_buffer[2] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[10] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[6] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[14] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); a += (d ^ c ^ b) + nt_buffer[1] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[9] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[5] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[13] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); a += (d ^ c ^ b) + nt_buffer[3] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[11] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[7] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[15] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); output[0] = a + 0x67452301; output[1] = b + 0xefcdab89; output[2] = c + 0x98badcfe; output[3] = d + 0x10325476; //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Convert the hash to hex (for being readable) //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ for(i=0; i&lt;4; i++) { int j = 0; unsigned int n = output[i]; //iterate the bytes of the integer for(; j&lt;4; j++) { unsigned int convert = n % 256; hex_format[i * 8 + j * 2 + 1] = itoa16[convert % 16]; convert = convert / 16; hex_format[i * 8 + j * 2 + 0] = itoa16[convert % 16]; n = n / 256; } } } \"\"\") expected = nthash.encrypt('then') data = numpy.array(expected) cleartext = numpy.zeros_like(data) cleartext_gpu = cuda.mem_alloc(data.nbytes) func = mod.get_function('NTBruteforce') func(cleartext_gpu, block=(1,1,1)) cuda.memcpy_dtoh(cleartext, cleartext_gpu) print 'Expected: {}'.format(expected.upper()) print \"GPU : {}\".format(cleartext.tostring()) The problem is that I get different results on consecutive runs. Sometimes I get the correct result a few times in a row, but the next time I run it (after 2-3 secs), the result is wrong. My output looks like this: Expected: 35B5C3F393D57F7836FF61514BCF1289 GPU : 90ABFDFAA5F9F1F25DAF679A3FC1331F Expected: 35B5C3F393D57F7836FF61514BCF1289 GPU : 4A3F30740C38FC259867716DF887349B Expected: 35B5C3F393D57F7836FF61514BCF1289 GPU : 2CA784517A80BBE10437EE88CFDEC269 Expected: 35B5C3F393D57F7836FF61514BCF1289 GPU : 35B5C3F393D57F7836FF61514BCF1289 Expected: 35B5C3F393D57F7836FF61514BCF1289 GPU : 35B5C3F393D57F7836FF61514BCF1289 Expected: 35B5C3F393D57F7836FF61514BCF1289 GPU : 8EA84AB098A6C8E37FFF1F6440127273 The above output is just an example of running the program a few times consecutively. As you can see I get the correct result sometimes (and sometimes consecutively as well) but other times the result is wrong and I don't understand why. I've tried re-installing the CUDA SDK (version 4.2.9) and rebooting my computer but the same thing happens. Using Windows 7 64-bit, Geforce GT240 Any ideas?",
        "answers": [
            [
                "You forgot to initialize nt_buffer. What you observed is a typical consequence of uninitialized variables: the junk in memory may vary from one run to the next, hence the inconsistent results. Simply changing the variable declaration line by: unsigned int nt_buffer[16] = { 0 }; should fix your issue (see this answer for information on C-style array initialization). Here is the complete (fix + error checking) CUDA/C++ code for those interested: #include &lt;string.h&gt; #include &lt;iostream&gt; #include &lt;stdio.h&gt; #define INIT_A 0x67452301 #define INIT_B 0xefcdab89 #define INIT_C 0x98badcfe #define INIT_D 0x10325476 #define SQRT_2 0x5a827999 #define SQRT_3 0x6ed9eba1 #define CUDA_CHECK_ERROR() __cuda_check_errors(__FILE__, __LINE__) #define CUDA_SAFE_CALL(err) __cuda_safe_call(err, __FILE__, __LINE__) inline void __cuda_check_errors(const char *filename, const int line_number) { cudaError err = cudaDeviceSynchronize(); if(err != cudaSuccess) { printf(\"CUDA error %i at %s:%i: %s\\n\", err, filename, line_number, cudaGetErrorString(err)); exit(-1); } } inline void __cuda_safe_call(cudaError err, const char *filename, const int line_number) { if (err != cudaSuccess) { printf(\"CUDA error %i at %s:%i: %s\\n\", err, filename, line_number, cudaGetErrorString(err)); exit(-1); } } __device__ void NTLM(char *, int, char*); __device__ __constant__ char itoa16[17] = \"0123456789ABCDEF\"; __global__ void NTBruteforce(char *hex_format){ char test[4] = {'t', 'h', 'e', 'n'}; NTLM(test, 4, hex_format); } __device__ void NTLM(char *key, int key_length, char *hex_format) { unsigned int nt_buffer[16] = { 0 }; unsigned int output[4] = { 0 }; //Globals for rounds unsigned int a = INIT_A; unsigned int b = INIT_B; unsigned int c = INIT_C; unsigned int d = INIT_D; // Prepare the string for hash calculation int i; int length = key_length; for (i = 0; i &lt; length / 2; i++) nt_buffer[i] = key[2 * i] | (key[2 * i + 1] &lt;&lt; 16); //padding if (length % 2 == 1) nt_buffer[i] = key[length - 1] | 0x800000; else nt_buffer[i] = 0x80; //put the length nt_buffer[14] = length &lt;&lt; 4; // NTLM hash calculation /* Round 1 */ a += (d ^ (b &amp; (c ^ d))) + nt_buffer[0]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[1]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[2]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[3]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); a += (d ^ (b &amp; (c ^ d))) + nt_buffer[4]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[5]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[6]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[7]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); a += (d ^ (b &amp; (c ^ d))) + nt_buffer[8]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[9]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[10]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[11]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); a += (d ^ (b &amp; (c ^ d))) + nt_buffer[12]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[13]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[14]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[15]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); /* Round 2 */ a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[0] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[4] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[8] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[12] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[1] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[5] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[9] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[13] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[2] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[6] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[10] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[14] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[3] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[7] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[11] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[15] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); /* Round 3 */ a += (d ^ c ^ b) + nt_buffer[0] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[8] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[4] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[12] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); a += (d ^ c ^ b) + nt_buffer[2] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[10] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[6] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[14] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); a += (d ^ c ^ b) + nt_buffer[1] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[9] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[5] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[13] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); a += (d ^ c ^ b) + nt_buffer[3] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[11] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[7] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[15] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); output[0] = a + 0x67452301; output[1] = b + 0xefcdab89; output[2] = c + 0x98badcfe; output[3] = d + 0x10325476; //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Convert the hash to hex (for being readable) //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ for(i=0; i&lt;4; i++) { int j = 0; unsigned int n = output[i]; //iterate the bytes of the integer for(; j&lt;4; j++) { unsigned int convert = n % 256; hex_format[i * 8 + j * 2 + 1] = itoa16[convert % 16]; convert = convert / 16; hex_format[i * 8 + j * 2 + 0] = itoa16[convert % 16]; n = n / 256; } } } int main() { char* d_hex; char h_hex[33] = \"\"; CUDA_SAFE_CALL(cudaMalloc(&amp;d_hex, 33 * sizeof(char))); NTBruteforce&lt;&lt;&lt;1, 1&gt;&gt;&gt;(d_hex); CUDA_CHECK_ERROR(); CUDA_SAFE_CALL(cudaMemcpy(h_hex, d_hex, 32 * sizeof(char), cudaMemcpyDeviceToHost)); CUDA_SAFE_CALL(cudaFree(d_hex)); h_hex[32] = '\\0'; std::cout &lt;&lt; h_hex &lt;&lt; std::endl; } which always returns 35B5C3F393D57F7836FF61514BCF1289. This was tested on Linux with CUDA 5.0, GeForce GT 650M and 319.12 beta drivers. Update Here is the file I used to test with PyCUDA. Note that I had to modify a few things: Escape the 2 \\n I added, else PyCUDA processes them... Add no_extern_c=True to SourceModule and put NTBruteforce in extern \"C\", else compilation fails for me (error: this declaration may not have extern \"C\" linkage). The complete PyCUDA program becomes: import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule import numpy from passlib.hash import nthash mod = SourceModule( \"\"\" #include &lt;string.h&gt; #include &lt;iostream&gt; #include &lt;stdio.h&gt; #define INIT_A 0x67452301 #define INIT_B 0xefcdab89 #define INIT_C 0x98badcfe #define INIT_D 0x10325476 #define SQRT_2 0x5a827999 #define SQRT_3 0x6ed9eba1 #define CUDA_CHECK_ERROR() __cuda_check_errors(__FILE__, __LINE__) #define CUDA_SAFE_CALL(err) __cuda_safe_call(err, __FILE__, __LINE__) inline void __cuda_check_errors(const char *filename, const int line_number) { cudaError err = cudaDeviceSynchronize(); if(err != cudaSuccess) { printf(\"CUDA error %i at %s:%i: %s\\\\n\", err, filename, line_number, cudaGetErrorString(err)); exit(-1); } } inline void __cuda_safe_call(cudaError err, const char *filename, const int line_number) { if (err != cudaSuccess) { printf(\"CUDA error %i at %s:%i: %s\\\\n\", err, filename, line_number, cudaGetErrorString(err)); exit(-1); } } __device__ void NTLM(char *, int, char*); __device__ __constant__ char itoa16[17] = \"0123456789ABCDEF\"; extern \"C\" { __global__ void NTBruteforce(char *hex_format){ char test[4] = {'t', 'h', 'e', 'n'}; NTLM(test, 4, hex_format); } } __device__ void NTLM(char *key, int key_length, char *hex_format) { unsigned int nt_buffer[16] = { 0 }; unsigned int output[4] = { 0 }; //Globals for rounds unsigned int a = INIT_A; unsigned int b = INIT_B; unsigned int c = INIT_C; unsigned int d = INIT_D; // Prepare the string for hash calculation int i; int length = key_length; for (i = 0; i &lt; length / 2; i++) nt_buffer[i] = key[2 * i] | (key[2 * i + 1] &lt;&lt; 16); //padding if (length % 2 == 1) nt_buffer[i] = key[length - 1] | 0x800000; else nt_buffer[i] = 0x80; //put the length nt_buffer[14] = length &lt;&lt; 4; // NTLM hash calculation /* Round 1 */ a += (d ^ (b &amp; (c ^ d))) + nt_buffer[0]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[1]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[2]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[3]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); a += (d ^ (b &amp; (c ^ d))) + nt_buffer[4]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[5]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[6]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[7]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); a += (d ^ (b &amp; (c ^ d))) + nt_buffer[8]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[9]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[10]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[11]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); a += (d ^ (b &amp; (c ^ d))) + nt_buffer[12]; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ (a &amp; (b ^ c))) + nt_buffer[13]; d = (d &lt;&lt; 7) | (d &gt;&gt; 25); c += (b ^ (d &amp; (a ^ b))) + nt_buffer[14]; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ (c &amp; (d ^ a))) + nt_buffer[15]; b = (b &lt;&lt; 19) | (b &gt;&gt; 13); /* Round 2 */ a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[0] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[4] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[8] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[12] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[1] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[5] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[9] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[13] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[2] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[6] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[10] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[14] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); a += ((b &amp; (c | d)) | (c &amp; d)) + nt_buffer[3] + SQRT_2; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += ((a &amp; (b | c)) | (b &amp; c)) + nt_buffer[7] + SQRT_2; d = (d &lt;&lt; 5) | (d &gt;&gt; 27); c += ((d &amp; (a | b)) | (a &amp; b)) + nt_buffer[11] + SQRT_2; c = (c &lt;&lt; 9) | (c &gt;&gt; 23); b += ((c &amp; (d | a)) | (d &amp; a)) + nt_buffer[15] + SQRT_2; b = (b &lt;&lt; 13) | (b &gt;&gt; 19); /* Round 3 */ a += (d ^ c ^ b) + nt_buffer[0] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[8] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[4] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[12] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); a += (d ^ c ^ b) + nt_buffer[2] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[10] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[6] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[14] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); a += (d ^ c ^ b) + nt_buffer[1] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[9] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[5] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[13] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); a += (d ^ c ^ b) + nt_buffer[3] + SQRT_3; a = (a &lt;&lt; 3) | (a &gt;&gt; 29); d += (c ^ b ^ a) + nt_buffer[11] + SQRT_3; d = (d &lt;&lt; 9) | (d &gt;&gt; 23); c += (b ^ a ^ d) + nt_buffer[7] + SQRT_3; c = (c &lt;&lt; 11) | (c &gt;&gt; 21); b += (a ^ d ^ c) + nt_buffer[15] + SQRT_3; b = (b &lt;&lt; 15) | (b &gt;&gt; 17); output[0] = a + 0x67452301; output[1] = b + 0xefcdab89; output[2] = c + 0x98badcfe; output[3] = d + 0x10325476; //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Convert the hash to hex (for being readable) //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ for(i=0; i&lt;4; i++) { int j = 0; unsigned int n = output[i]; //iterate the bytes of the integer for(; j&lt;4; j++) { unsigned int convert = n % 256; hex_format[i * 8 + j * 2 + 1] = itoa16[convert % 16]; convert = convert / 16; hex_format[i * 8 + j * 2 + 0] = itoa16[convert % 16]; n = n / 256; } } } \"\"\", no_extern_c=True) expected = nthash.encrypt('then') data = numpy.array(expected) cleartext = numpy.zeros_like(data) cleartext_gpu = cuda.mem_alloc(data.nbytes) func = mod.get_function('NTBruteforce') func(cleartext_gpu, block=(1,1,1)) cuda.memcpy_dtoh(cleartext, cleartext_gpu) print 'Expected: {}'.format(expected.upper()) print \"GPU : {}\".format(cleartext.tostring()) The result is, as expected: Expected: 35B5C3F393D57F7836FF61514BCF1289 GPU : 35B5C3F393D57F7836FF61514BCF1289"
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "When using a function from a SourceModule that depends on another function in the SourceModule, how do I pass it in the function call, i.e. what is \"???\" in the following code: import numpy import pycuda.autoinit import pycuda.driver as drv from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void make_square(float *in_array, float *out_array) { int i; int N = 5; for (i=0; i&lt;N; i++) { out_array[i] = pow(in_array[i],2); } } __global__ void make_square_add_one(float *in_array, float *out_array, void make_square(float *, float *)) { int N = 5; make_square(in_array,out_array); for (int i=0; i&lt;N; i++) out_array[i] = out_array[i] + 1; } \"\"\") make_square = mod.get_function(\"make_square\") make_square_add_one = mod.get_function(\"make_square_add_one\") in_array = numpy.array([1.,2.,3.,4.,5.]).astype(numpy.float32) out_array = numpy.zeros_like(in_array).astype(numpy.float32) make_square_add_one(drv.In(in_array), drv.Out(out_array), ??? , block = (1,1,1), grid = (1,1)) Thanks for any information.",
        "answers": [
            [
                "In the traditional CUDA execution model, __global__ functions are kernels, and they can't be passed as arguments to other kernels and they can't be called by other kernels. It looks like make_square should really be a device function, something like: __device__ void make_square(float *in_array, float *out_array) { int i; for (i=0; i&lt;5; i++) { out_array[i] = pow(in_array[i],2); } } which is then called from the running kernel as: __global__ void make_square_add_one(float *in_array, float *out_array) { int N = 5; make_square(in_array,out_array); for (int i=0; i&lt;N; i++) out_array[i] = out_array[i] + 1; } It is worth noting that this kernel is entirely serial and pretty much orthogonal to how CUDA kernels would be expected to be written."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "The code goes like this: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy dtype = numpy.float32 total_size = numpy.int32(1500) hist_rgb = numpy.random.randn(total_size) his_rgb = hist_rgb.astype(dtype) #print hist_rgb a_gpu = cuda.mem_alloc(hist_rgb.nbytes) #print a_gpu cuda.memcpy_htod(a_gpu, hist_rgb) numblock = 1500/256 print numblock mod = SourceModule(\"\"\" __global__ void enhance(float *a_gpu,int total){ int pixel= threadIdx.x+blockIdx.x*blockDim.x; if(pixel&lt;total){ a_gpu[pixel]*=2; } } \"\"\") func = mod.get_function(\"enhance\") func(a_gpu, total_size, block=(256,1,1), grid=(numblock,1)) result=numpy.empty_like(a_gpu) cuda.memcpy_dtoh(result, a_gpu) print result But I am getting errors in terminal as below: Segmentation fault (core dumped) I don't know what is the matter? Somebody help?",
        "answers": [],
        "votes": []
    },
    {
        "question": "How do I know the behavior of CUDA scheduler? Apart from testing it by varying the grid sizes, block sizes etc. in my application is there any vendor provided documentation that explains exactly in what fashion the blocks are distributed?",
        "answers": [
            [
                "It depends on the architecture you are working on. On the Fermi architecture, for example, you have a GigaThread global scheduler that distributes thread blocks to the Streaming Multiprocessors (SM) schedulers. For each SM, a Dual Warp scheduler schedules threads in groups of 32 parallel threads called warps. This is well explained in the NVIDIA White Paper on Fermi. I suggest also to take a look at this other document."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "It has to be a simple one, though I can't find an answer. I'm writing a program which has to calculate states of cellular automatons and in order to get a feeling how does CUDA works I tried to write a very simple program first. It takes a matrix, and every thread has to increment a value in its cell and in the cells which are above and below of this cell. So, if i give it the following matrix: [0 0 0 0 0 0 0] [0 0 0 0 0 0 0] [0 0 0 0 0 0 0] [0 0 0 0 0 0 0] [0 0 0 0 0 0 0] [0 0 0 0 0 0 0] [0 0 0 0 0 0 0] I expect to get the following result: [2 2 2 2 2 2 2] [3 3 3 3 3 3 3] [3 3 3 3 3 3 3] [3 3 3 3 3 3 3] [3 3 3 3 3 3 3] [3 3 3 3 3 3 3] [2 2 2 2 2 2 2] The first row has values of 2, because it doesn't have a row above which could increment values of first row one more time. And in a similar manner the last row has values of 2. But I'm getting a matrix which looks like this: [2 2 2 2 2 2 2] [3 3 3 3 3 3 3] [3 3 3 3 3 3 3] [3 3 3 3 2 2 2] [2 2 2 2 2 2 2] [2 2 2 2 3 3 3] [2 2 2 2 2 2 2] And I can't understand why there are values of 2 in the 4th, 5th and ath 6th row - there have to be 3, not 2. Here goes my code: import numpy import pycuda.autoinit import pycuda.driver as cuda from pycuda.compiler import SourceModule w = 7 mod = SourceModule(\"\"\" __global__ void diffusion( int* result, int width, int height) { int xIndex = blockDim.x * blockIdx.x + threadIdx.x; int yIndex = blockDim.y * blockIdx.y + threadIdx.y; int flatIndex = xIndex + width * yIndex; int topIndex = xIndex + width * (yIndex - 1); int bottomIndex = xIndex + width * (yIndex + 1); int inc = 1; result[flatIndex] += inc; result[bottomIndex] += inc; result[topIndex] += inc; } \"\"\") diff_func = mod.get_function(\"diffusion\") def diffusion(res): height, width = numpy.int32(len(res)), numpy.int32(len(res[0])) diff_func( cuda.InOut(res), width, height, block=(w,w,1) ) def run(res, step): diffusion(res) print res res = numpy.array([[0 \\ for _ in xrange(0, w)]\\ for _ in xrange(0, w)], dtype='int32') run(res, 0) One more interesting thing: if I comment one of the following lines: result[bottomIndex] += inc; result[topIndex] += inc; Everything works as expected and there aren't any unexpected values. It looks like in some cases CUDA can't work with values of three adjacent cells in one thread.",
        "answers": [
            [
                "You have what is know as a memory race: multiple independent threads are attempting to update the same value in memory at the same time. The CUDA memory model doesn't define what happens when two threads try to update the same memory location simultaneously. The solution is either to use atomic memory operations (see the CUDA programming guide for more information), or a different approach for updating adjacent cells (for example, colour the grid and update like coloured cells on separate passes through the grid)."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm a two week old mac user, so bear with me here. I'm trying to set EPD python up as my default python interpreter instead of the system python that came with the mac. It was my understanding that EPD does this automatically upon installation by modifying the .bash_profile, but after I installed EPD, the .bash_profile was unaltered and as far as I can see, system python is still the default interpreter. How do I go about changing this? The major reason why I want to change the default python to EPD is that all the packages I install are automatically placed into the /Library/Python/2.7/site-packages directory instead of the site-packages directory associated with EPD. In particular, I can't get PyCuda to install in EPD's site-packages directory. I hope this made some sort of sense. I'm lost and not sure where to go from here. Help is greatly appreciated.",
        "answers": [
            [
                "Adding the line export PATH=/Library/Frameworks/Python.framework/Versions/Current/bin:$PATH to your .bashrc file should work. In theory the EPD installer should have done this job... maybe an issue with permissions?"
            ],
            [
                "A few additional points. Normally the EPD installer makes the PATH changes by adding lines to the bottom of ~/.bash_profile ; if you are sure that it did not, you can also check ~/.profile . Perhaps you installed EPD as a different user? Since you say that you are new to Mac, I'll also mention that the PATH would not have been changed in any terminal windows that had already been opened before you installed EPD. Also, a crucial warning: since you've already installed 3rd-party packages into your system Python, please see this article: https://support.enthought.com/entries/22094157-OS-X-Conflict-with-installed-packages-in-earlier-Python-installation You may also find other useful articles in that same Knowledge Base site."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm working on a project where I distribute compute tasks to multiple python Processes each associated with its own CUDA device. When spawning the subprocesses, I use the following code: import pycuda.driver as cuda class ComputeServer(object): def _init_workers(self): self.workers = [] cuda.init() for device_id in range(cuda.Device.count()): print \"initializing device {}\".format(device_id) worker = CudaWorker(device_id) worker.start() self.workers.append(worker) The CudaWorker is defined in another file as follows: from multiprocessing import Process import pycuda.driver as cuda class CudaWorker(Process): def __init__(self, device_id): Process.__init__(self) self.device_id = device_id def run(self): self._init_cuda_context() while True: # process requests here def _init_cuda_context(self): # the following line fails cuda.init() device = cuda.Device(self.device_id) self.cuda_context = device.make_context() When I run this code on Windows 7 or Linux, I have no issues. When running the code on my MacBook Pro with OSX 10.8.2, Cuda 5.0, and PyCuda 2012.1 I get the following error: Process CudaWorker-1: Traceback (most recent call last): File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap self.run() File \"/Users/tombnorwood/pymodules/computeserver/worker.py\", line 32, in run self._init_cuda_context() File \"/Users/tombnorwood/pymodules/computeserver/worker.py\", line 38, in _init_cuda_context cuda.init() RuntimeError: cuInit failed: no device I have no issues running PyCuda scripts without forking new processes on my Mac. I only get this issue when spawning a new Process. Has anyone run into this issue before?",
        "answers": [
            [
                "This is really just an educated guess based on my experienced, but I suspect that the OS X implementation of CUDA (or possibly PyCuda) relies on some APIs that can't be used safely after fork, while the linux implementation does not.* Since the POSIX implementation of multiprocessing uses fork without exec to create child processes, this would explain why it fails on OS X but not linux. (And on Windows, there is no fork, just a spawn equivalent, so this isn't an issue.) The simplest solution would be to drop multiprocessing. If CUDA and PyCUDA are thread-safe (I don't know if they are), and your code is not CPU-bound (just GPU-bound), you might be able to just drop in threading.Thread in place of multiprocessing.Process and be done with it. Or you could consider one of the other parallel-processing libraries that provide similar APIs to multiprocessing. (There are a few people who use pp only because it always execs\u2026) However, it's pretty easy to hack up multiprocessing to exec/spawn a new Python interpreter and then do everything Windows-style instead of POSIX-style. (Getting every case right is difficult, but getting one specific use case right is easy.) Or, if you look at bug #8713, there's some work being done on making this work right in general. And there are working patches. Those patches are for 3.3, not 2.7, so you'd probably need a bit of massaging, but it shouldn't be very much. So, just cp $MY_PYTHON_LIB/multiprocessing.py $MY_PROJECT_DIR/mymultiprocessing.py, patch it, use mymultiprocessing in place of multiprocessing, and add the appropriate call to pick spawn/fork+exec/whatever the mode is called in the latest patch before you do anything else. * The OP says he suspected the same thing, so I probably don't need to explain this to him, but for future readers: It's not about a difference between Darwin and other Unixes, but about the fact that Apple ships a lot of non-Unix-y mid-level libraries like CoreFoundation.framework, Accelerate.framework, etc. that use unsafe-after-fork functionality (or just assert that they're not being used after a fork because Apple doesn't want to put in the rigorous testing that would be warranted before they could say \"as of 10.X, Foo.framework is safe after fork\"). Also, if you compare the way OS X and linux deal with graphics and other hardware, there's a lot more mid-level in-each-process-userspace going on in OS X."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I need help to know the size of my blocks and grids. I'm building a python app to perform metric calculations based on scipy as: Euclidean distance, Manhattan, Pearson, Cosine, joined other. The project is PycudaDistances. It seems to work very well with small arrays. When I perform a more exhaustive test, unfortunately it did not work. I downloaded movielens set (http://www.grouplens.org/node/73). Using Movielens 100k, I declared an array with shape (943, 1682). That is, users are 943 and 1682 films evaluated. The films not by a classifier user I configured the value to 0. With a much larger array algorithm no longer works. I face the following error: pycuda._driver.LogicError: cuFuncSetBlockShape failed: invalid value. Researching this error, I found an explanation of telling Andrew that supports 512 threads to join and to work with larger blocks it is necessary to work with blocks and grids. I wanted a help to adapt the algorithm Euclidean distance arrays to work from small to giant arrays. def euclidean_distances(X, Y=None, inverse=True): X, Y = check_pairwise_arrays(X,Y) rows = X.shape[0] cols = Y.shape[0] solution = numpy.zeros((rows, cols)) solution = solution.astype(numpy.float32) kernel_code_template = \"\"\" #include &lt;math.h&gt; __global__ void euclidean(float *x, float *y, float *solution) { int idx = threadIdx.x + blockDim.x * blockIdx.x; int idy = threadIdx.y + blockDim.y * blockIdx.y; float result = 0.0; for(int iter = 0; iter &lt; %(NDIM)s; iter++) { float x_e = x[%(NDIM)s * idy + iter]; float y_e = y[%(NDIM)s * idx + iter]; result += pow((x_e - y_e), 2); } int pos = idx + %(NCOLS)s * idy; solution[pos] = sqrt(result); } \"\"\" kernel_code = kernel_code_template % { 'NCOLS': cols, 'NDIM': X.shape[1] } mod = SourceModule(kernel_code) func = mod.get_function(\"euclidean\") func(drv.In(X), drv.In(Y), drv.Out(solution), block=(cols, rows, 1)) return numpy.divide(1.0, (1.0 + solution)) if inverse else solution For more details see: https://github.com/vinigracindo/pycudaDistances/blob/master/distances.py",
        "answers": [
            [
                "To size the execution paramters for your kernel you need to do two things (in this order): 1. Determine the block size Your block size will mostly be determined by hardware limitations and performance. I recommend reading this answer for more detailed information, but the very short summary is that your GPU has a limit on the total number of threads per block it can run, and it has a finite register file, shared and local memory size. The block dimensions you select must fall inside these limits, otherwise the kernel will not run. The block size can also effect the performance of kernel, and you will find a block size which gives optimal performance. Block size should always be a round multiple of the warp size, which is 32 on all CUDA compatible hardware released to date. 2. Determine the grid size For the sort of kernel you have shown, the number of blocks you need is directly related to the amount of input data and the dimensions of each block. If, for example, your input array size was 943x1682, and you had a 16x16 block size, you would need a 59 x 106 grid, which would yield 944x1696 threads in the kernel launch. In this case the input data size is not a round multiple of the block size, you will need to modify your kernel to ensure that it doesn't read out-of-bounds. One approach could be something like: __global__ void euclidean(float *x, float *y, float *solution) { int idx = threadIdx.x + blockDim.x * blockIdx.x; int idy = threadIdx.y + blockDim.y * blockIdx.y; if ( ( idx &lt; %(NCOLS)s ) &amp;&amp; ( idy &lt; %(NDIM)s ) ) { ..... } } The python code to launch the kernel could look like something similar to: bdim = (16, 16, 1) dx, mx = divmod(cols, bdim[0]) dy, my = divmod(rows, bdim[1]) gdim = ( (dx + (mx&gt;0)) * bdim[0], (dy + (my&gt;0)) * bdim[1]) ) func(drv.In(X), drv.In(Y), drv.Out(solution), block=bdim, grid=gdim) This question and answer may also help understand how this process works. Please note that all of the above code was written in the browser and has never been tested. Use it at your own risk. Also note it was based on a very brief reading of your code and might not be correct because you have not really described anything about how the code is called in your question."
            ],
            [
                "The accepted answer is correct in principle, however the code that talonmies has listed is not quite correct. The line: gdim = ( (dx + (mx&gt;0)) * bdim[0], (dy + (my&gt;0)) * bdim[1]) ) Should Be: gdim = ( (dx + (mx&gt;0)), (dy + (my&gt;0)) ) Besides an obvious extra parenthesis, gdim would produce way too many threads than what you want. talonmies had explained it right in his text that threads is the blocksize * gridSize. However the gdim he has listed would give you the total threads and not the correct grid size which is what is desired."
            ]
        ],
        "votes": [
            17.0000001,
            3.0000001
        ]
    },
    {
        "question": "I'm trying to compile some sources for working with my GPU. I use pycuda for this. When I compile source code, I receive some errors from Python: C:\\Users\\Dmitriy\\wcm&gt;python ws_gpu.py test.dcm Traceback (most recent call last): File \"ws_gpu.py\", line 2, in &lt;module&gt; import pycuda.gpuarray as gpu File \"C:\\Python27\\lib\\site-packages\\pycuda\\gpuarray.py\", line 3, in &lt;module&gt; import pycuda.elementwise as elementwise File \"C:\\Python27\\lib\\site-packages\\pycuda\\elementwise.py\", line 33, in &lt;module&gt; from pycuda.tools import context_dependent_memoize File \"C:\\Python27\\lib\\site-packages\\pycuda\\tools.py\", line 30, in &lt;module&gt; import pycuda.driver as cuda File \"C:\\Python27\\lib\\site-packages\\pycuda\\driver.py\", line 2, in &lt;module&gt; from pycuda._driver import * ImportError: DLL load failed: \u2550\u0445 \u044d\u0440\u0449\u0444\u0445\u044d \u0454\u044a\u0440\u0447\u0440\u044d\u044d\u221a\u0449 \u044c\u044e\u0444\u0454\u044b\u2116. Has anyone encountered a similar problem? How can I solve this? I use Windows 7 64-bit, last driver for my GPU (NVIDIA GT520M) and CUDA Toolkit v.5.0.",
        "answers": [
            [
                "This sort of error is almost always because of a broken PyCUDA installation. There is a library file called _driver.dll which provides bindings to the CUDA driver API. The error message is coming because that dll can either not be found or the libraries it depends on (ie. CUDA) can't be loaded. I can't say more because I can't read the cryllic error text you posted. One way of diagnosing this sort of problem is to try the following in an interactive python shell: import pycuda pycuda.__file__ The second command will tell you where the root path for the active PyCUDA installation is (taken from a mailing list post). Searching in that path will find you the locate of the _driver.dll and running a dependency diagnostic tool on the dll file will show what can or cannot be found. If that doesn't work I recommend taking your question to the PyCUDA mailing list."
            ]
        ],
        "votes": [
            8.0000001
        ]
    },
    {
        "question": "For numDraws up to 100000 (1M), I can do the following easily def Simple_Elt(numDraws): import numpy as np gauss = np.random.normal import pycuda.gpuarray as ga from pycuda.cumath import exp as gaexp import pycuda.autoinit from pycuda.elementwise import ElementwiseKernel npgausses = gauss(size=(numDraws,)).astype(np.float32) gausses = ga.to_gpu(npgausses) eltComp = \"\"\"p[i] = exp(p[i]);\"\"\" kernel = ElementwiseKernel(\"float *p\", eltComp, \"expnorm\") kernel(gausses) sumEN = ga.sum(gausses).get() Simple_Elt(1000000) However, for N = 10000000 (10M), I run out of GPU memory when transferring the random values to the GPU. I'd like to possibly solve two problems at one time: (1) efficiently use the GPU to generate the random numbers and (2) remove the size limitation. Now, I'm not sure the best way to do it. Code here shows how to make a custom normal random number generator using \"raw\" PyCUDA to do Box-Muller from CPU generated uniform randoms, but I think it would make more sense to use CURAND. However, using PyCuda's CURAND interface seems to give me the same size limitations (and, I believe it makes many random number generators which causes high overhead -- that's from the PyCUDA CURAND API document warning here. So, I suppose a possibility is to use PyCUDA with custom calls to underlying CURAND. This is all guess work. But, my real question is the best way to solve the two issues above. Examples, pointers, and suggestions are much appreciated.",
        "answers": [],
        "votes": []
    },
    {
        "question": "how can I iterate in two arrays? __global__ void euclidean(float *x, float *y, int dim_x, int dim_y, int ms, float *solution) { int idx = threadIdx.x + blockDim.x * blockIdx.x; int idy = threadIdx.y + blockDim.y * blockIdx.y; float result = 0.0; for (int iter = 0; iter &lt; ms; iter++) { float x_e = x[idy * ms + iter]; float y_e = y[idx * ms + iter]; result += (x_e * y_e); } } Input: X = [[1,2], [3,4], [5,6], [7,8], [9,10]] and Y = [[0,0], [1,1]] Expected Output: [[0, 3], [0, 7], [0, 11], [0, 15]. [0, 19]] How can I do this? My difficulty is to iterate on X and Y. Expected: [idx: 0 idy: 0 = 0] [idx: 1 idy: 0 = 3] [idx: 2 idy: 0 = 0] [idx: 3 idy: 0 = 7] [idx: 4 idy: 0 = 0] [idx: 0 idy: 1 = 11] [idx: 1 idy: 1 = 0] [idx: 2 idy: 1 = 15] [idx: 3 idy: 1 = 0] [idx: 4 idy: 1 = 19]",
        "answers": [
            [
                "I would do the following to multiply 2 matrices. This handles boundary conditions so should work on any grid/block size. // Compute C = A * B __global__ void matrixMultiply(float * A, float * B, float * C, int numARows, int numAColumns, int numBRows, int numBColumns, int numCRows, int numCColumns) { float cValue = 0; int Row = blockIdx.y * blockDim.y + threadIdx.y; int Col = blockIdx.x * blockDim.x + threadIdx.x; if ((Row &lt; numCRows) &amp;&amp; (Col &lt; numCColumns)) { for (int k = 0; k &lt; numAColumns; k++) { cValue += A[Row*numAColumns + k] * B[k*numBColumns + Col]; } C[Row*numCColumns + Col] = cValue; } } If you want a more efficient implementation you can also use the shared memory: // Compute C = A * B __global__ void matrixMultiplyShared(float * A, float * B, float * C, int numARows, int numAColumns, int numBRows, int numBColumns, int numCRows, int numCColumns) { __shared__ float ds_A[TILE_WIDTH_I][TILE_WIDTH_I]; __shared__ float ds_B[TILE_WIDTH_I][TILE_WIDTH_I]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int Row = by * TILE_WIDTH + ty; int Col = bx * TILE_WIDTH + tx; float cValue = 0; for (int m = 0; m &lt; (numAColumns/TILE_WIDTH); m++) { if (Row &lt; numARows &amp;&amp; m*TILE_WIDTH_I + tx &lt; numAColumns) { ds_A[ty][tx] = A[Row*numAColumns + m*TILE_WIDTH_I + tx]; } else { ds_A[ty][tx] = 0; } if (m*TILE_WIDTH_I + ty &lt; numBRows &amp;&amp; Col &lt; numBColumns) { ds_B[ty][tx] = B[(m*TILE_WIDTH_I + ty)*numBColumns + Col]; } else { ds_B[ty][tx] = 0; } __syncthreads(); if ((Row &lt; numCRows) &amp;&amp; (Col &lt; numCColumns)) { for (int k = 0; k &lt; TILE_WIDTH; k++) { cValue += ds_A[ty][k] * ds_B[k][tx]; } } __syncthreads(); } if ((Row &lt; numCRows) &amp;&amp; (Col &lt; numCColumns)) { C[Row*numCColumns + Col] = cValue; } }"
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "The following script was set-up for benchmark purposes. It computes the distance between N points using an Euclidean L2 norm. Three different routines are implemented: High-level solution using the scipy.spatial.distance.pdist function. Fairly low-level OpenMP powered scipy.weave.inline solution. pyCUDA powered GPGPU solution. Here are the benchmark results on a i5-3470 (16GB RAM) using a GTX660 (2GB RAM): ------------ Scipy Pdist Execution time: 3.01975 s Frist five elements: [ 0.74968684 0.71457213 0.833188 0.48084545 0.86407363] Last five elements: [ 0.65717077 0.76850474 0.29652017 0.856179 0.56074625] ------------ Weave Inline Execution time: 2.48705 s Frist five elements: [ 0.74968684 0.71457213 0.83318806 0.48084542 0.86407363] Last five elements: [ 0.65717083 0.76850474 0.29652017 0.856179 0.56074625] ------------ pyCUDA CUDA clock timing: 0.713028930664 Execution time: 2.04364 s Frist five elements: [ 0.74968684 0.71457213 0.83318806 0.48084542 0.86407363] Last five elements: [ 0.65717083 0.76850468 0.29652017 0.856179 0.56074625] ------------ I am a bit disappointed on the pyCUDA perfomance. Since I am new to CUDA, there is probably something I am missing here. So where is the crux of the matter ? Am I reaching the limits of global memory bandwidth ? Poor choice of block- and gridsizes ? import numpy,time,math import pycuda.autoinit import pycuda.driver as drv from pycuda.compiler import SourceModule from scipy.spatial.distance import pdist from scipy import weave def weave_solution(x): \"\"\" OpenMP powered weave inline. \"\"\" N,DIM = numpy.shape(x) L = ((N-1)**2+(N-1))/2 solution = numpy.zeros(L).astype(numpy.float32) ncpu = 4 weave_omp = {'headers' : ['&lt;omp.h&gt;'], 'extra_compile_args': ['-fopenmp'], 'extra_link_args' : ['-lgomp']} code = \\ r''' omp_set_num_threads(ncpu); #pragma omp parallel { int j,d,pos; float r=0.0; #pragma omp for for (int i=0; i&lt;(N-1); i++){ for (j=(i+1); j&lt;N; j++){ r = 0.0; for (d=0; d&lt;DIM; d++){ r += (x[i*DIM+d]-x[j*DIM+d])*(x[i*DIM+d]-x[j*DIM+d]); } pos = (i*N+j)-(i*(i+1)/2)-i-1; solution[pos] = sqrt(r); } } } ''' weave.inline(code,['x','N','DIM','solution','ncpu'],**weave_omp) return numpy.array(solution) def scipy_solution(x): \"\"\" SciPy High-level function \"\"\" return pdist(x).astype(numpy.float32) def cuda_solution(x): \"\"\" pyCUDA \"\"\" N,DIM = numpy.shape(x) N = numpy.int32(N) DIM = numpy.int32(DIM) L = ((N-1)**2+(N-1))/2 solution = numpy.zeros(L).astype(numpy.float32) start = drv.Event() end = drv.Event() mod = SourceModule(\"\"\" __global__ void distance(float *x,int N,int DIM,float *solution){ const int i = blockDim.x * blockIdx.x + threadIdx.x; int j,d,pos; float r=0.0; if ( i &lt; (N-1) ){ for (j=(i+1); j&lt;N; j++){ r = 0.0; for (d=0; d&lt;DIM; d++){ r += (x[i*DIM+d]-x[j*DIM+d])*(x[i*DIM+d]-x[j*DIM+d]); } pos = (i*N+j)-(i*(i+1)/2)-i-1; solution[pos] = sqrt(r); } } } \"\"\") func = mod.get_function(\"distance\") start.record() func(drv.In(x),N,DIM,drv.Out(solution),block=(192,1,1),grid=(192,1)) end.record() end.synchronize() secs = start.time_till(end)*1e-3 print \"CUDA clock timing: \",secs return solution if __name__ == '__main__': # Set up data points N = 25000 DIM = 3 x = numpy.random.rand(N,DIM).astype(numpy.float32) print \"-\"*12 # Scipy solution print \"Scipy Pdist\" stime = time.time() spsolution = scipy_solution(x) stime = time.time()-stime print \"Execution time: {0:.5f} s\".format(stime) print \"Frist five elements:\", spsolution[:5] print \"Last five elements:\", spsolution[-5:] print \"-\"*12 # Weave solution print \"Weave Inline\" wtime = time.time() wsolution = weave_solution(x) wtime = time.time()-wtime print \"Execution time: {0:.5f} s\".format(wtime) print \"Frist five elements:\", wsolution[:5] print \"Last five elements:\", wsolution[-5:] print \"-\"*12 # pyCUDA solution print \"pyCUDA\" ctime = time.time() csolution = cuda_solution(x) ctime = time.time()-ctime print \"Execution time: {0:.5f} s\".format(ctime) print \"Frist five elements:\", csolution[:5] print \"Last five elements:\", csolution[-5:] print \"-\"*12 Edit: I have added the hash bang line #!/usr/bin/env python at the top of the file and made it executable. After commenting out the computation using weave.inline and scipy.spatial.distance.pdist, the NVIDIA Visual Profiler promts the following results:",
        "answers": [
            [
                "Right now you have 192 threads each updating N-1 positions, you could easily launch more blocks/threads. What you want to do is instead of this loop for (j=(i+1); j&lt;N; j++){, replace it with N-1 threads doing just the inner loop. If you want to take it further you could have N-1 * DIM threads each doing the statement in the inner loop, store the result to shared memory and finally do an reduction on that. See Optimizing Parallel Reduction in CUDA Looking at this line: r += (x[i*DIM+d]-x[j*DIM+d])*(x[i*DIM+d]-x[j*DIM+d]); The memory transactions and pattern is not uniform and coalesced. Also do not know if nvcc will optimizes your expression to only two memory transactions instead of four shown here, as I do not know if pycuda passes -O3 to nvcc. Put (x[i*DIM+d]-x[j*DIM+d]) into a register variable to make sure and just square it yourself. Else you can also try to put #pragma unroll before each for loop to unroll them if possible."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I'm trying to perform Fitted Value Iteration (FVI) in python (involving approximating a 5 dimensional function using piecewise linear interpolation). scipy.interpolate.griddata works perfectly for this. However, I need to call the interpolation routine several thousand times (since FVI is a MC based algorithm). So basically, the set of points where the function is known is static (and large - say 32k), but the points i need to approximate (which are small perturbations of the original set) is very large (32k x 5000 say). Is there an implementation of what scipy.interpolate.griddata does that's been ported to CUDA? alternatively, is there a way to speed up the calculation somehow? Thanks.",
        "answers": [
            [
                "For piece-wise linear interpolation, the docs say that scipy.interpolate.griddata uses the methods of scipy.interpolate.LinearNDInterpolator, which in turn uses qhull to do a Delaunay tesellation of the input points, then performs standard barycentric interpolation, where for each point you have to determine inside which hypertetrahedron each point is, then use its barycentric coordinates as the interpolation weights for the hypertetrahedron node values. The tesellation is probably hard to parallelize, but you can access the CPU version with scipy.spatial.Delaunay. The other two steps are easily parallelized, although I don't know of any freely available implementation. If your known-function points are on a regular grid, the method described here is specially easy to implement in CUDA, and I have worked with actual implementations of it, albeit none publicly available. So I am afraid you are going to have to do most of the work yourself..."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Is-there a way to bind an array that is already on the gpu to a texture using PyCuda ? There is already a cuda.bind_array_to_texref(cuda.make_multichannel_2d_array(...), texref) that binds an array on the CPU to a texture, but I couldn't find the equivalent of cudaBindTextureToArray in PyCuda if the array is already on the device. For example, doing : myArray = [1, 2, 3] myArray_d = gpu.to_gpu(myArray) # then performs some computations on it, and then cuda.bind_texture_to_array(myArray_d, texref)",
        "answers": [
            [
                "If you want to bind an existing CUDA array in GPU memory to a texture reference, then pycuda.driver.TextureReference.set_array() is probably what you want. Note the PyCUDA is built on the driver API, so the call you are looking for is actually cuTexRefSetArray rather than cudaBindTextureToArray."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I made a very simple kernel below to practice CUDA. import pycuda.driver as cuda import pycuda.autoinit import numpy as np from pycuda.compiler import SourceModule from pycuda import gpuarray import cv2 def compile_kernel(kernel_code, kernel_name): mod = SourceModule(kernel_code) func = mod.get_function(kernel_name) return func input_file = np.array(cv2.imread('clouds.jpg')) height, width, channels = np.int32(input_file.shape) my_kernel_code = \"\"\" __global__ void my_kernel(int width, int height) { // This kernel trivially does nothing! Hurray! } \"\"\" kernel = compile_kernel(my_kernel_code, 'my_kernel') if __name__ == '__main__': for i in range(0, 2): print 'o' kernel(width, height, block=(32, 32, 1), grid=(125, 71)) # When I take this line away, the error goes bye bye. # What in the world? width -= 1 Right now, if we run the code above, execution proceeds through the first iteration of the for loop just fine. However, during the second iteration of the loop, I get the following error. Traceback (most recent call last): File \"outOfResources.py\", line 27, in &lt;module&gt; kernel(width, height, block=(32, 32, 1), grid=(125, 71)) File \"/software/linux/x86_64/epd-7.3-1-pycuda/lib/python2.7/site-packages/pycuda-2012.1-py2.7-linux-x86_64.egg/pycuda/driver.py\", line 374, in function_call func._launch_kernel(grid, block, arg_buf, shared, None) pycuda._driver.LaunchError: cuLaunchKernel failed: launch out of resources If I take away the line width -= 1, the error goes away. Why is that? Can't I change the parameter for a kernel the second time around? For reference, here is clouds.jpg.",
        "answers": [
            [
                "Though the error message isn't particularly informative, note that you need to pass in a correctly casted width variable. So something like: width = np.int32(width - 1) should work."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "A number of algorithms iterate until a certain convergence criterion is reached (e.g. stability of a particular matrix). In many cases, one CUDA kernel must be launched per iteration. My question is: how then does one efficiently and accurately determine whether a matrix has changed over the course of the last kernel call? Here are three possibilities which seem equally unsatisfying: Writing a global flag each time the matrix is modified inside the kernel. This works, but is highly inefficient and is not technically thread safe. Using atomic operations to do the same as above. Again, this seems inefficient since in the worst case scenario one global write per thread occurs. Using a reduction kernel to compute some parameter of the matrix (e.g. sum, mean, variance). This might be faster in some cases, but still seems like overkill. Also, it is possible to dream up cases where a matrix has changed but the sum/mean/variance haven't (e.g. two elements are swapped). Is there any of the three options above, or an alternative, that is considered best practice and/or is generally more efficient?",
        "answers": [
            [
                "I'll also go back to the answer I would have posted in 2012 but for a browser crash. The basic idea is that you can use warp voting instructions to perform a simple, cheap reduction and then use zero or one atomic operations per block to update a pinned, mapped flag that the host can read after each kernel launch. Using a mapped flag eliminates the need for an explicit device to host transfer after each kernel launch. This requires one word of shared memory per warp in the kernel, which is a small overhead, and some templating tricks can allow for loop unrolling if you provide the number of warps per block as a template parameter. A complete working examplate (with C++ host code, I don't have access to a working PyCUDA installation at the moment) looks like this: #include &lt;cstdlib&gt; #include &lt;vector&gt; #include &lt;algorithm&gt; #include &lt;assert.h&gt; __device__ unsigned int process(int &amp; val) { return (++val &lt; 10); } template&lt;int nwarps&gt; __global__ void kernel(int *inout, unsigned int *kchanged) { __shared__ int wchanged[nwarps]; unsigned int laneid = threadIdx.x % warpSize; unsigned int warpid = threadIdx.x / warpSize; // Do calculations then check for change/convergence // and set tchanged to be !=0 if required int idx = blockIdx.x * blockDim.x + threadIdx.x; unsigned int tchanged = process(inout[idx]); // Simple blockwise reduction using voting primitives // increments kchanged is any thread in the block // returned tchanged != 0 tchanged = __any(tchanged != 0); if (laneid == 0) { wchanged[warpid] = tchanged; } __syncthreads(); if (threadIdx.x == 0) { int bchanged = 0; #pragma unroll for(int i=0; i&lt;nwarps; i++) { bchanged |= wchanged[i]; } if (bchanged) { atomicAdd(kchanged, 1); } } } int main(void) { const int N = 2048; const int min = 5, max = 15; std::vector&lt;int&gt; data(N); for(int i=0; i&lt;N; i++) { data[i] = min + (std::rand() % (int)(max - min + 1)); } int* _data; size_t datasz = sizeof(int) * (size_t)N; cudaMalloc&lt;int&gt;(&amp;_data, datasz); cudaMemcpy(_data, &amp;data[0], datasz, cudaMemcpyHostToDevice); unsigned int *kchanged, *_kchanged; cudaHostAlloc((void **)&amp;kchanged, sizeof(unsigned int), cudaHostAllocMapped); cudaHostGetDevicePointer((void **)&amp;_kchanged, kchanged, 0); const int nwarps = 4; dim3 blcksz(32*nwarps), grdsz(16); // Loop while the kernel signals it needs to run again do { *kchanged = 0; kernel&lt;nwarps&gt;&lt;&lt;&lt;grdsz, blcksz&gt;&gt;&gt;(_data, _kchanged); cudaDeviceSynchronize(); } while (*kchanged != 0); cudaMemcpy(&amp;data[0], _data, datasz, cudaMemcpyDeviceToHost); cudaDeviceReset(); int minval = *std::min_element(data.begin(), data.end()); assert(minval == 10); return 0; } Here, kchanged is the flag the kernel uses to signal it needs to run again to the host. The kernel runs until each entry in the input has been incremented to above a threshold value. At the end of each threads processing, it participates in a warp vote, after which one thread from each warp loads the vote result to shared memory. One thread reduces the warp result and then atomically updates the kchanged value. The host thread waits until the device is finished, and can then directly read the result from the mapped host variable. You should be able to adapt this to whatever your application requires"
            ],
            [
                "I'll go back to my original suggestion. I've updated the related question with an answer of my own, which I believe is correct. create a flag in global memory: __device__ int flag; at each iteration, initialize the flag to zero (in host code): int init_val = 0; cudaMemcpyToSymbol(flag, &amp;init_val, sizeof(int)); In your kernel device code, modify the flag to 1 if a change is made to the matrix: __global void iter_kernel(float *matrix){ ... if (new_val[i] != matrix[i]){ matrix[i] = new_val[i]; flag = 1;} ... } after calling the kernel, at the end of the iteration (in host code), test for modification: int modified = 0; cudaMemcpyFromSymbol(&amp;modified, flag, sizeof(int)); if (modified){ ... } Even if multiple threads in separate blocks or even separate grids, are writing the flag value, as long as the only thing they do is write the same value (i.e. 1 in this case), there is no hazard. The write will not get \"lost\" and no spurious values will show up in the flag variable. Testing float or double quantities for equality in this fashion is questionable, but that doesn't seem to be the point of your question. If you have a preferred method to declare \"modification\" use that instead (such as testing for equality within a tolerance, perhaps). Some obvious enhancements to this method would be to create one (local) flag variable per thread, and have each thread update the global flag variable once per kernel, rather than on every modification. This would result in at most one global write per thread per kernel. Another approach would be to keep one flag variable per block in shared memory, and have all threads simply update that variable. At the completion of the block, one write is made to global memory (if necessary) to update the global flag. We don't need to resort to complicated reductions in this case, because there is only one boolean result for the entire kernel, and we can tolerate multiple threads writing to either a shared or global variable, as long as all threads are writing the same value. I can't see any reason to use atomics, or how it would benefit anything. A reduction kernel seems like overkill, at least compared to one of the optimized approaches (e.g. a shared flag per block). And it would have the drawbacks you mention, such as the fact that anything less than a CRC or similarly complicated computation might alias two different matrix results as \"the same\"."
            ]
        ],
        "votes": [
            4.0000001,
            3.0000001
        ]
    },
    {
        "question": "I am trying to compute the variance of a 2D gpu_array. A reduction kernel sounds like a good idea: http://documen.tician.de/pycuda/array.html However, this documentation implies that reduction kernels just reduce 2 arrays into 1 array. How do I reduce a single 2D array into a single value?",
        "answers": [
            [
                "I guess the first step is to define variance for this case. In matlab, the variance function on a 2D array returns a vector (1D-array) of values. But it sounds like you want a single-valued variance, so as others have already suggested, probably the first thing to do is to treat the 2D-array as 1D. In C we won't require any special steps to accomplish this. If you have a pointer to the array you can index into it as if it were a 1D array. I'm assuming you don't need help on how to handle a 2D array with a 1D index. Now if it's the 1D variance you're after, I'm assuming a function like variance(x)=sum((x[i]-mean(x))^2) where the sum is over all i, is what you're after (based on my read of the wikipedia article ). We can break this down into 3 steps: compute the mean (this is a classical reduction - one value is produced for the data set - sum all elements then divide by the number of elements) compute the value (x[i]-mean)^2 for all i - this is an element by element operation producing an output data set equal in size (number of elements) to the input data set compute the sum of the elements produced in step 2 - this is another classical reduction, as one value is produced for the entire data set. Both steps 1 and 3 are classical reductions which are summing all elements of an array. Rather than cover that ground here, I'll point you to Mark Harris' excellent treatment of the topic as well as some CUDA sample code. For step 2, I'll bet you could figure out the kernel code on your own, but it would look something like this: #include &lt;math.h&gt; __global__ void var(float *input, float *output, unsigned N, float mean){ unsigned idx=threadIdx.x+(blockDim.x*blockIdx.x); if (idx &lt; N) output[idx] = __powf(input[idx]-mean, 2); } Note that you will probably want to combine the reductions and the above code into a single kernel."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I am teaching myself CUDA from the ground up. I made this simple kernel that adds 1 to each of the relevant elements within a 2D array. The elements of the 2D array stem from the red channel of an image (zebra.jpg). from pycuda.compiler import SourceModule import matplotlib.image as img import pycuda.driver as cuda import pycuda.autoinit import numpy as np my_image = img.imread(\"zebra.jpg\")[:,:,0] block = (32, 32, 1) grid = (8, 8) if __name__ == '__main__': width, height = np.int32(my_image.shape) im = np.array(my_image) print 'original sum: ' + str(np.sum(im)) # Create the CUDA kernel, and run it. mod = SourceModule(\"\"\" __global__ void add1ToEverything(float* r, int w, int h) { int rowID = blockDim.y * blockDim.y + threadIdx.y; int colID = blockDim.x * blockIdx.x + threadIdx.x; if (rowID &gt; 0 &amp;&amp; rowID &lt; h - 2 &amp;&amp; colID &gt; 0 &amp;&amp; colID &lt; w - 2) r[rowID * w + colID] += 1.0; } \"\"\") func = mod.get_function('add1ToEverything') for i in range(0, 5): func(cuda.InOut(im), width, height, block=block, grid=grid) print 'new sum: ' + str(np.sum(im)) However, then I run this program, I get the following results. original sum: 1828815 new sum: 1828815 Why is my original sum identical to my new sum? Shouldn't the new sum be greater? Here is zebra.jpg.",
        "answers": [
            [
                "your problem is at the line: int rowID = blockDim.y * blockDim.y + threadIdx.y; it should be: int rowID = blockDim.y * blockIdx.y + threadIdx.y;"
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I have created a numpy array of float32s with shape (64, 128), and I want to send it to the GPU. How do I do that? What arguments should my kernel function accept? float** myArray? I have tried directly sending the array as it is to the GPU, but pycuda complains that objects are being accessed...",
        "answers": [
            [
                "Two dimensional arrays in numpy/PyCUDA are stored in pitched linear memory in row major order by default. So you only need to have a kernel something like this: __global__ void kernel(float* a, int lda, ...) { int r0 = threadIdx.y + blockDim.y * blockIdx.y; int r1 = threadIdx.x + blockDim.x * blockIdx.x; float val = a[r0 + r1*lda]; .... } to access a numpy ndarray or PyCUDA gpuarray passed by reference to the kernel from Python."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I am trying to teach myself CUDA. This has not been easy so far, but I don't give up easily either :) I have created a very simple program. It merely returns a value from the GPU. import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit import numpy as np returnValue = np.zeros(1) mod = SourceModule(\"\"\" __global__ void myVeryFirstKernel(float* returnValue) { returnValue[0] = 8.0; } \"\"\") func = mod.get_function('myVeryFirstKernel') func(cuda.InOut(returnValue), block=(1024, 1, 1), grid=(1, 1)) print str(returnValue[0]) However, the value that my program prints is 5.387879938e-315. That sure doesn't look like 8.0. Why is the wrong value getting returned from the GPU? I have tried altering the block size, which I don't think should anything (but who knows). I have also checked that the data type I am sending in (float64) matches my kernel.",
        "answers": [
            [
                "You have type conflicts - your kernel is expected a 32 bit single precision value, but you are passing a 64 bit double value to it. If you rewrite your code something like this: returnValue = np.zeros(1, dtype=np.float32) mod = SourceModule(\"\"\" __global__ void myVeryFirstKernel(float* returnValue) { returnValue[0] = 8.0f; } \"\"\") func = mod.get_function('myVeryFirstKernel') func(cuda.InOut(returnValue), block=(1024, 1, 1), grid=(1, 1)) print returnValue[0] so that everything is explicitly specified in single precision, you might have more luck."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I am teaching myself CUDA with pyCUDA. In this exercise, I want to send over a simply array of 1024 floats to the GPU and store it in shared memory. As I specify below in my arguments, I run this kernel on just a single block with 1024 threads. import pycuda.driver as cuda from pycuda.compiler import SourceModule import pycuda.autoinit import numpy as np import matplotlib.pyplot as plt arrayOfFloats = np.float64(np.random.sample(1024)) mod = SourceModule(\"\"\" __global__ void myVeryFirstKernel(float* arrayOfFloats) { extern __shared__ float sharedData[]; // Copy data to shared memory. sharedData[threadIdx.x] = arrayOfFloats[threadIdx.x]; } \"\"\") func = mod.get_function('myVeryFirstKernel') func(cuda.InOut(arrayOfFloats), block=(1024, 1, 1), grid=(1, 1)) print str(arrayOfFloats) Strangely, I am getting this error. [dfaux@harbinger CUDA_tutorials]$ python sharedMemoryExercise.py Traceback (most recent call last): File \"sharedMemoryExercise.py\", line 17, in &lt;module&gt; func(cuda.InOut(arrayOfFloats), block=(1024, 1, 1), grid=(1, 1)) File \"/software/linux/x86_64/epd-7.3-1-pycuda/lib/python2.7/site-packages/pycuda-2012.1-py2.7-linux-x86_64.egg/pycuda/driver.py\", line 377, in function_call Context.synchronize() pycuda._driver.LaunchError: cuCtxSynchronize failed: launch failed PyCUDA WARNING: a clean-up operation failed (dead context maybe?) cuMemFree failed: launch failed PyCUDA WARNING: a clean-up operation failed (dead context maybe?) cuModuleUnload failed: launch failed I have tried to debug this error by changing the type of elements I am sending to my GPU (instead of float64, I use float32 for instance). I have also tried altering my block and grid sizes to no avail. What could be wrong? What is a dead context? Any advice or ideas appreciated.",
        "answers": [
            [
                "One problem i see with your code is that you use extern __shared__ .. which means that you need to submit the size of the shared memory when you launch the kernel. In pycuda this is done by: func(cuda.InOut(arrayOfFloats), block=(1024, 1, 1), grid=(1, 1),shared=smem_size) where smem_size is the size of the shared memory in bytes. In your case smem_size = 1024*sizeof(float)."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I successfully created an operator+ between two float4 by doing : __device__ float4 operator+(float4 a, float4 b) { // ... } However, if in addition, I want to have an operator+ for uchar4, by doing the same thing with uchar4, i get the following error: \"error: more than one instance of overloaded function \"operator+\" has \"C\" linkage\" \" I get a similar error message when I declare multiple functions with the same name but different arguments. So, two questions : Polymorphism : Is-it possible to have multiple functions with the same name and different arguments in Cuda ? If so, why do I have this error message ? operator+ for float4 : it seems that this feature is already included by including \"cutil_math.h\", but when I include that (#include &lt;cutil_math.h&gt;) it complains that there is no such file or directory... anything particular I should do ? Note: I am using pycuda, which is a cuda for python. Thanks!",
        "answers": [
            [
                "Note the \"has \"C\" linkage\" in the error. You are compiling your code with C linkage (pyCUDA does this by default to circumvent symbol mangling issues). C++ can't support multiple definitions of the same function name using C linkage. The solution is to compile code without automatically generated \"extern C\", and explicitly specify C linkage only for kernels. So your code would looks something like: __device__ float4 operator+(float4 a, float4 b) { ... }; extern \"C\" __global__ void kernel() { }; rather than the standard pyCUDA emitted: extern \"C\" { __device__ float4 operator+(float4 a, float4 b) { ... }; __global__ void kernel() { }; } pycuda.compiler.SourceModule has an option no_extern_c which can be used to control whether extern \"C\" is emitted by the just in time compilation system or not."
            ]
        ],
        "votes": [
            4.0000001
        ]
    },
    {
        "question": "I wrote the following simple CUDA kernel: __global__ void pr_kernel(float* O, const float* I, const float* W, int N) { int x = threadIdx.x; float sum; int i; if (x &lt; N) { for (i = 0; i &lt; N; i++) { if (i == x) continue; sum += W[x*N+i] * I[x]; } O[x] = (0.15 / N) + 0.85 * sum; } } The variables are allocated in Python as follows: N = np.int32(4) W = np.float32(np.asarray( [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,1, 1, 0])) I = np.float32(np.asarray( [0.25, 0.25, 0.25, 0.25])) O = np.float32(np.zeros(N)) I'm transferring the variables using gpuarray.to_gpu, and I'm calling the kernel on a Tesla C2070 with the following line: pr_kernel(O_d, I_d, W_d, N_d, block=blocksize, grid=gridsize) Where: blocksize = (128, 1, 1) gridsize = (1, 1) I get the error message: pycuda.driver.LaunchError: cuLaunchKernel failed: launch out of resources. This happens even if I reduce blocksize to something like (8, 1, 1). I can run other CUDA programs on the GPU with a blocksize of (512, 1, 1) so I'm confident this is not due to a GPU configuration issue. What am I doing wrong? Thanks for any help.",
        "answers": [
            [
                "The problem was that I was transferring the integer N to the GPU using gpuarray.to_gpu, where I should have been directly passing N to the pr_kernel function."
            ],
            [
                "I got a similar problem when I used a different type in definition and as an argument to the kernel. Probably the fact that the latter required more resources generates an error."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I made a simple CUDA program for practice. It simply copies over data from one array to another: import pycuda.driver as cuda import pycuda.autoinit import numpy as np from pycuda.compiler import SourceModule # Global constants N = 2**20 # size of array a a = np.linspace(0, 1, N) e = np.empty_like(a) block_size_x = 512 # Instantiate block and grid sizes. block_size = (block_size_x, 1, 1) grid_size = (N / block_size_x, 1) # Create the CUDA kernel, and run it. mod = SourceModule(\"\"\" __global__ void D2x_kernel(double* a, double* e, int N) { int tid = blockDim.x * blockIdx.x + threadIdx.x; if (tid &gt; 0 &amp;&amp; tid &lt; N - 1) { e[tid] = a[tid]; } } \"\"\") func = mod.get_function('D2x_kernel') func(a, cuda.InOut(e), np.int32(N), block=block_size, grid=grid_size) print str(e) However, I get this error: pycuda._driver.LogicError: cuLaunchKernel failed: invalid value When I get rid of the second argument double* e in my kernel function and invoke the kernel without the argument e, the error goes away. Why is that? What does this error mean?",
        "answers": [
            [
                "Your a array does not exist in device memory, so I suspect that PyCUDA is ignoring (or otherwise handling) the first argument to your kernel invocation and only passing in e and N...so you get an error because the kernel was expecting three arguments and it has only received two. Removing double* e from your kernel definition might eliminate the error message you're getting, but your kernel still won't work properly. A quick fix to this should be to wrap a in a cuda.In() call, which instructs PyCUDA to copy a to the device before launching the kernel. That is, your kernel launch line should be: func(cuda.In(a), cuda.InOut(e), np.int32(N), block=block_size, grid=grid_size) Edit: Also, do you realize that your kernel is not copying the first and last elements of a to e? Your if (tid &gt; 0 &amp;&amp; tid &lt; N - 1) statement is preventing that. For the entire array, it should be if (tid &lt; N)."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I want to run a function on a large, 2D complex array (eventually 2*12x2*12 datapoints). However, pycuda does not work as expected. The ElementWise function doesn't work at 2d arrays, so I used the SourceModule function with block sizes. The problem is now that the C code on the GPU does not give the same result as the numpy calculation on the CPU. Very large and strange numbers are resulting. I'm using the following code. What's going wrong? #!/usr/bin/env python #https://github.com/lebedov/scikits.cuda/blob/master/demos/indexing_2d_demo.py \"\"\" Demonstrates how to access 2D arrays within a PyCUDA kernel in a numpy-consistent manner. \"\"\" from string import Template import pycuda import pycuda.autoinit import pycuda.gpuarray as gpuarray from pycuda.compiler import SourceModule import numpy as np from matplotlib import pyplot as plt # Set size A = 2**3 B = 2**3 N = A*B x_gpu = gpuarray.to_gpu(np.fromfunction(lambda x,y: (1.+x)*np.exp(1.j*y*np.pi/10), (A,B)) ) y_gpu = gpuarray.to_gpu(np.fromfunction(lambda x,y: 1.*x, (A,B)).astype( x_gpu.dtype)) d_gpu = gpuarray.to_gpu(np.zeros_like(x_gpu.get()))#.astype(np.float32)) func_mod_template = Template(\"\"\" // Macro for converting subscripts to linear index: #define INDEX(a, b) a*${B}+b #include &lt;pycuda-complex.hpp&gt; //__global__ void func(double *d,double *x,double *y, unsigned int N) { __global__ void func(pycuda::complex&lt;float&gt; *d,pycuda::complex&lt;float&gt; *x, pycuda::complex&lt;float&gt; *y) { // Obtain the linear index corresponding to the current thread: // unsigned int idx = blockIdx.y*blockDim.y*gridDim.x + blockIdx.x*blockDim.x*gridDim.y +threadIdx.x+threadIdx.y; unsigned int block_num = blockIdx.x + blockIdx.y * gridDim.x; unsigned int thread_num = threadIdx.y * blockDim.x + threadIdx.x; unsigned int threads_in_block = blockDim.x * blockDim.y; unsigned int idx = (threads_in_block * block_num + thread_num); // Convert the linear index to subscripts: unsigned int a = idx/${B}; unsigned int b = idx%${B}; // Use the subscripts to access the array: // d[INDEX(a,b)] = x[INDEX(a,b)]+y[INDEX(a,b)]; pycuda::complex&lt;float&gt; j(0,arg(x[idx])); pycuda::complex&lt;float&gt; i(abs(x[idx]),0); d[idx] = i * exp(j); } \"\"\") max_threads_per_block = pycuda.autoinit.device.get_attribute(pycuda._driver.device_attribute.MAX_THREADS_PER_BLOCK) max_block_dim = (pycuda.autoinit.device.get_attribute(pycuda._driver.device_attribute.MAX_BLOCK_DIM_X), pycuda.autoinit.device.get_attribute(pycuda._driver.device_attribute.MAX_BLOCK_DIM_Y), pycuda.autoinit.device.get_attribute(pycuda._driver.device_attribute.MAX_BLOCK_DIM_Z)) max_grid_dim = (pycuda.autoinit.device.get_attribute(pycuda._driver.device_attribute.MAX_GRID_DIM_X), pycuda.autoinit.device.get_attribute(pycuda._driver.device_attribute.MAX_GRID_DIM_Y), pycuda.autoinit.device.get_attribute(pycuda._driver.device_attribute.MAX_GRID_DIM_Z)) max_blocks_per_grid = max(max_grid_dim) block_dim = max_block_dim block_dim = (max_block_dim[0],1,1) grid_dim = (int(np.ceil(1.*x_gpu.shape[0]/block_dim[0])), int(np.ceil(1.*x_gpu.shape[1]/block_dim[1]))) print block_dim,grid_dim, N func_mod = \\ SourceModule(func_mod_template.substitute(max_threads_per_block=max_threads_per_block, max_blocks_per_grid=max_blocks_per_grid, A=A, B=B)) func = func_mod.get_function('func') func(d_gpu,x_gpu,y_gpu, block=block_dim, grid=grid_dim) print d_gpu.get()/x_gpu.get() #print 'Success status: ', np.allclose(x_np, x_gpu.get()) plt.imshow((d_gpu.get()/x_gpu.get()).real) plt.colorbar() plt.show()",
        "answers": [
            [
                "As an actual answer: changing the x_gpu line to x_gpu = gpuarray.to_gpu(np.fromfunction( lambda x,y: (1.+x)*np.exp(1.j*y*np.pi/10), (A,B)).astype(np.complex64) ) seems to fix the problem. Also, although ElementwiseKernel does not work with 2d arrays, you are using 2d-&gt;1d transformation anyway, so nothing really stops you from writing func = ElementwiseKernel( \"pycuda::complex&lt;float&gt; *d, pycuda::complex&lt;float&gt; *x, pycuda::complex&lt;float&gt; *y\", Template(\"\"\" // Convert the linear index to subscripts: unsigned int a = i/${B}; unsigned int b = i%${B}; // Use the subscripts to access the array: //d[INDEX(a,b)] = x[INDEX(a,b)]+y[INDEX(a,b)]; pycuda::complex&lt;float&gt; angle(0,arg(x[i])); pycuda::complex&lt;float&gt; module(abs(x[i]),0); d[i] = module * exp(angle); \"\"\").substitute(A=A, B=B), preamble=Template(\"\"\" #define INDEX(a, b) a*${B}+b \"\"\").substitute(A=A, B=B)) func(d_gpu, x_gpu, y_gpu) This way you will not need to juggle block/grid sizes because PyCUDA will handle this for you."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I've got a strange problem with cuda, In the below snippet, #include &lt;stdio.h&gt; #define OUTPUT_SIZE 26 typedef $PRECISION REAL; extern \"C\" { __global__ void test_coeff ( REAL* results ) { int id = blockDim.x * blockIdx.x + threadIdx.x; int out_index = OUTPUT_SIZE * id; for (int i=0; i&lt;OUTPUT_SIZE; i++) { results[out_index+i]=id; printf(\"q\"); } } } When I compile and run the code (via pycuda), it works as expected. When I remove the printf, then the results are weird - most of the array is populated correctly, but some of it seems completely random. here's the full python code: import numpy as np import string #pycuda stuff import pycuda.driver as drv import pycuda.autoinit from pycuda.compiler import SourceModule class MC: cudacodetemplate = \"\"\" #include &lt;stdio.h&gt; #define OUTPUT_SIZE 26 typedef $PRECISION REAL; extern \"C\" { __global__ void test_coeff ( REAL* results ) { int id = blockDim.x * blockIdx.x + threadIdx.x; int out_index = OUTPUT_SIZE * id; for (int i=0; i&lt;OUTPUT_SIZE; i++) { results[out_index+i]=id; //printf(\"q\"); } } } \"\"\" def __init__(self, size, prec = np.float32): #800 meg should be enough . . . drv.limit.MALLOC_HEAP_SIZE = 1024*1024*800 self.size = size self.prec = prec template = string.Template(MC.cudacodetemplate) self.cudacode = template.substitute( PRECISION = 'float' if prec==np.float32 else 'double') #self.module = pycuda.compiler.SourceModule(self.cudacode, no_extern_c=True, options=['--ptxas-options=-v']) self.module = SourceModule(self.cudacode, no_extern_c=True) def test(self, out_size): #try to precalc the co-efficients for just the elements of the vector that changes test = np.zeros( ( 128, out_size*(2**self.size) ), dtype=self.prec ) test2 = np.zeros( ( 128, out_size*(2**self.size) ), dtype=self.prec ) test_coeff = self.module.get_function ('test_coeff') test_coeff( drv.Out(test), block=(2**self.size,1,1), grid=( 128, 1 ) ) test_coeff( drv.Out(test2), block=(2**self.size,1,1), grid=( 128, 1 ) ) error = (test-test2) return error if __name__ == '__main__': p1 = MC ( 5, np.float64 ) err = p1.test(26) print err.max() print err.min() Basically, with the printf in the kernel, the err is 0 - without it it prints some random error (on my machine around 2452 (for the max), and -2583 (for the min)) I have no idea why. I've running cuda 4.2 on pycuda 2012.2 (windows 7 64bit) with a geforce 570. Thanks.",
        "answers": [
            [
                "This is most likely due to compiler optimization. You are setting a block of memory OUTPUT_SIZE in length to the loop-constant value of id. In my experience the compiler will optimize that to a memcpy or whathaveyou unless there is something else going on in the loop -- ie your print statement. Furthermore, if you do not utilize that block of memory the compiler may optimize the entire loop away. Try fiddling with your optimization levels and see if you have different results."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "This is my pycuda code.When I execute I get threading errors.I am trying to implement pycuda threads in this.I have searched google but couldn't find an answer.Respurces are also very limited for pycuda I'm the Internet.So guys please help me out. {{{#!python import pycuda import pycuda.driver as cuda from pycuda.compiler import SourceModule import threading import numpy class GPUThread(threading.Thread): def __init__(self, number, some_array): threading.Thread.__init__(self) self.number = number self.some_array = some_array def run(self): self.dev = cuda.Device(self.number) self.ctx = self.dev.make_context() self.array_gpu = cuda.mem_alloc(some_array.nbytes) cuda.memcpy_htod(self.array_gpu, some_array) test_kernel(self.array_gpu) print \"successful exit from thread %d\" % self.number self.ctx.pop() del self.array_gpu del self.ctx def test_kernel(input_array_gpu): mod = SourceModule(\"\"\" __global__ void f(float * out, float * in) { int idx = threadIdx.x; out[idx] = in[idx] + 6; } \"\"\") func = mod.get_function(\"f\") output_array = numpy.zeros((1,512)) output_array_gpu = cuda.mem_alloc(output_array.nbytes) func(output_array_gpu, input_array_gpu, block=(512,1,1)) cuda.memcpy_dtoh(output_array, output_array_gpu) return output_array cuda.init() some_array = numpy.ones((1,512), dtype=numpy.float32) num = cuda.Device.count() gpu_thread_list = [] for i in range(num): gpu_thread = GPUThread(i, some_array) gpu_thread.start() }}}",
        "answers": [
            [
                "You forgot to append the gpu thread.Just add gpu_thread_list.append(gpu_thread) after start() in your code and it will work.There are quite a lot of good tutorial for pycuda online.Check this."
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "I have this code here (modified due to the answer). Info 32 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads ptxas info : Used 46 registers, 120 bytes cmem[0], 176 bytes cmem[2], 76 bytes cmem[16] I don't know what else to take into consideration in order to make it work for different combinations of points \"numPointsRs\" and \"numPointsRp\" When ,for example, i run the code with Rs=10000 and Rp=100000 with block=(128,1,1),grid=(200,1) its fine. My computations: 46 registers*128threads=5888 registers . My card has limit 32768registers,so 32768/5888=5 +some =&gt; 5 block/SM (my card has limit 6). With the occupancy calculator i found that using 128 threads/block gives me 42% and am in the limits of my card. Also,the number of threads per MP is 640 (limit is 1536) Now,if i try to use Rs=100000 and Rp=100000 (for the same threads and blocks) it gives me the message in the title,with: cuEventDestroy failed: launch timeout cuModuleUnload failed: launch timeout 1) I don't know/understand what else is needed to be computed. 2) I can't understand how we use/find the number of the blocks.I can see that mostly,someone puts (threads-1+points)/threads ,but that still doesn't work. --------------UPDATED---------------------------------------------- After using driver.Context.synchronize() ,the code works for many points (1000000)! But ,what impact has this addition to the code?(for many points the screen freezes for 1 minute or more).Should i use it or not? --------------UPDATED2---------------------------------------------- Now,the code doesn't work again without doing anything! Snapshot of code: import pycuda.gpuarray as gpuarray import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np import cmath import pycuda.driver as drv import pycuda.tools as t #---- Initialization and passing(allocate memory and transfer data) to GPU ------------------------- Rs_gpu=gpuarray.to_gpu(Rs) Rp_gpu=gpuarray.to_gpu(Rp) J_gpu=gpuarray.to_gpu(np.ones((numPointsRs,3)).astype(np.complex64)) M_gpu=gpuarray.to_gpu(np.ones((numPointsRs,3)).astype(np.complex64)) Evec_gpu=gpuarray.to_gpu(np.zeros((numPointsRp,3)).astype(np.complex64)) Hvec_gpu=gpuarray.to_gpu(np.zeros((numPointsRp,3)).astype(np.complex64)) All_gpu=gpuarray.to_gpu(np.ones(numPointsRp).astype(np.complex64)) #----------------------------------------------------------------------------------- mod =SourceModule(\"\"\" #include &lt;pycuda-complex.hpp&gt; #include &lt;cmath&gt; #include &lt;vector&gt; typedef pycuda::complex&lt;float&gt; cmplx; typedef float fp3[3]; typedef cmplx cp3[3]; __device__ __constant__ float Pi; extern \"C\"{ __device__ void computeEvec(fp3 Rs_mat[], int numPointsRs, cp3 J[], cp3 M[], fp3 Rp, cmplx kp, cmplx eta, cmplx *Evec, cmplx *Hvec, cmplx *All) { while (c&lt;numPointsRs){ ... c++; } } __global__ void computeEHfields(float *Rs_mat_, int numPointsRs, float *Rp_mat_, int numPointsRp, cmplx *J_, cmplx *M_, cmplx kp, cmplx eta, cmplx E[][3], cmplx H[][3], cmplx *All ) { fp3 * Rs_mat=(fp3 *)Rs_mat_; fp3 * Rp_mat=(fp3 *)Rp_mat_; cp3 * J=(cp3 *)J_; cp3 * M=(cp3 *)M_; int k=threadIdx.x+blockIdx.x*blockDim.x; while (k&lt;numPointsRp) { computeEvec( Rs_mat, numPointsRs, J, M, Rp_mat[k], kp, eta, E[k], H[k], All ); k+=blockDim.x*gridDim.x; } } } \"\"\" ,no_extern_c=1,options=['--ptxas-options=-v']) #call the function(kernel) func = mod.get_function(\"computeEHfields\") func(Rs_gpu,np.int32(numPointsRs),Rp_gpu,np.int32(numPointsRp),J_gpu, M_gpu, np.complex64(kp), np.complex64(eta),Evec_gpu,Hvec_gpu, All_gpu, block=(128,1,1),grid=(200,1)) #----- get data back from GPU----- Rs=Rs_gpu.get() Rp=Rp_gpu.get() J=J_gpu.get() M=M_gpu.get() Evec=Evec_gpu.get() Hvec=Hvec_gpu.get() All=All_gpu.get() My card: Device 0: \"GeForce GTX 560\" CUDA Driver Version / Runtime Version 4.20 / 4.10 CUDA Capability Major/Minor version number: 2.1 Total amount of global memory: 1024 MBytes (1073283072 bytes) ( 0) Multiprocessors x (48) CUDA Cores/MP: 0 CUDA Cores //CUDA Cores 336 =&gt; 7 MP and 48 Cores/MP",
        "answers": [
            [
                "There are quite a few issues that you have to deal with. Answer 1 provided by @njuffa is the best general solution. I'll provide more feedback based upon the limited data you have provided. PTX output of 46 registers is not the number of registers used by your kernel. PTX is an intermediate representation. The offline or JIT compiler will convert this to device code. Device code may use more or less registers. Nsight Visual Studio Edition, the Visual Profiler, and the CUDA command line profiler can all provide you the correct register count. The occupancy calculation is not simply RegistersPerSM / RegistersPerThread. Registers are allocated based upon a granularity. For CC 2.1 the granularity is 4 registers per thread per warp (128 registers). 2.x devices can actually allocate at a 2 register granularity but this can lead to fragmentation later in the kernel. In your occupancy calculation you state My card has limit 32768registers,so 32768/5888=5 +some =&gt; 5 block/SM (my card has limit 6). I'm not sure what 6 means. Your device has 7 SMs. The maximum blocks per SM for 2.x devices is 8 blocks per SM. You have provided an insufficient amount of code. If you provide pieces of code please provide the size of all inputs, the number of times each loop will be executed, and a description of the operations per function. Looking at the code you may be doing too many loops per thread. Without knowing the order of magnitude of the outer loop we can only guess. Given that the launch is timing out you should probably approach debugging as follows: a. Add a line to the beginning of the code if (blockIdx.x &gt; 0) { return; } Run the exact code you have in one of the previously mentioned profilers to estimate the duration of a single block. Using the launch information provided by the profiler: register per thread, shared memory, ... use the occupancy calculator in the profiler or the xls to determine the maximum number of blocks that you can run concurrently. For example, if the theoretical block occupancy is 3 blocks per SM, and the number of SMs is 7 the you can run 21 blocks at a time which for you launch is 9 waves. NOTE: this assumes equal work per thread. Change the early exit code to allow 1 wave (21 blocks). If this launch times out then you need to reduce the amount of work per thread. If this passes then calculate how many waves you have and estimate when you will timeout (2sec on windows, ? on linux). b. If you have too many waves then reduce you have to reduce the launch configuration. Given that you index by gridDim.x and blockDim.x you can do this by passing in these dimensions as as parameters to your kernel. This will require tou to minimally change your indexing code. You will also have to pass a blockIdx.x offset. Change your host code to launch multiple kernels back to back. Since there should be no conflict you can rr launch these in multiple streams to benefit from overlap at the end of each wave."
            ],
            [
                "\"launch timeout\" would appear to indicate that the kernel ran too long and was killed by the watchdog timer. This can happen on GPUs that are also used for graphics output (e.g. a graphical desktop), where the task of the watchdog timer is to prevent the desktop from locking up for more than a few seconds. Best I can recall the watchdog time limit is on the order of 5 seconds or thereabouts. At any given moment, the GPU can either run graphics, or CUDA, so the watchdog timer is needed when running a GUI to prevent the GUI from locking up for an extended period of time, which renders the machine inoperable through the GUI. If possible, avoid using this GPU for the desktop and/or other graphics (e.g. don't run X if you are on Linux). If running without graphics isn't an option, to reduce kernel execution time to avoid hitting watchdog timer kernel termination, you will have to do less work per kernel launch, optimize the code so the kernel runs faster for the same amount of work, or deploy a faster GPU."
            ],
            [
                "To provide more inputs on @njuffa's answer, in Windows systems you can increase the launch timeout or TDR (Timeout Detection &amp; Recovery) by following these steps: 1: Open the options in Nsight Monitor. 2: Set an appropriate value for WDDM TDR Delay CUATION: If this value is small you may get timeout error and for higher values your screen will stay frozen until kernel finishes it's job. source"
            ]
        ],
        "votes": [
            3.0000001,
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "I ran SimpleSpeedTest.py from the PyCuda examples, producing the following output: Using nbr_values == 8192 Calculating 100000 iterations SourceModule time and first three results: 0.058294s, [ 0.005477 0.005477 0.005477] Elementwise time and first three results: 0.102527s, [ 0.005477 0.005477 0.005477] Elementwise Python looping time and first three results: 2.398071s, [ 0.005477 0.005477 0.005477] GPUArray time and first three results: 8.207257s, [ 0.005477 0.005477 0.005477] CPU time measured using : 0.000002s, [ 0.005477 0.005477 0.005477] The first four time measurements are reasonable, the last one (0.000002s) however is way off. The CPU result should be the slowest one but it is orders of magnitude faster than the fastest GPU method. So obviously the measured time must be wrong. This is strange since the same timing method seems to work fine for the first four results. So I took some code from SimpleSpeedTest.py and made a small test file [2], which produced: time measured using option 1: 0.000002s time measured using option 2: 5.989620s Option 1 measures the duration using pycuda.driver.Event.record() (as in SimpleSpeedTest.py), option 2 uses time.clock(). Again, option 1 is off while option 2 gives a reasonable result (the time it takes to run the test file is around 6s). Does anyone have an idea as to why this is happening? Since using option 1 is endorsed in SimpleSpeedTest.py, could it be my setup that is causing the problem? I am running a GTX 470, Display Driver 301.42, CUDA 4.2, Python 2.7 64, PyCuda 2012.1, X5650 Xeon [2] Test file: import numpy import time import pycuda.driver as drv import pycuda.autoinit n_iter = 100000 nbr_values = 8192 # = 64 * 128 (values as used in SimpleSpeedTest.py) start = drv.Event() # option 1 uses pycuda.driver.Event end = drv.Event() a = numpy.ones(nbr_values).astype(numpy.float32) # test data start.record() # start option 1 (inserting recording points into GPU stream) tic = time.clock() # start option 2 (using CPU time) for i in range(n_iter): a = numpy.sin(a) # do some work end.record() # end option 1 toc = time.clock() # end option 2 end.synchronize() events_secs = start.time_till(end)*1e-3 time_secs = toc - tic print \"time measured using option 1:\" print \"%fs \" % events_secs print \"time measured using option 2:\" print \"%fs \" % time_secs",
        "answers": [
            [
                "I contacted Andreas Kl\u00f6ckner and he suggested to synchronize on the start event, too. ... start.record() start.synchronize() ... And this seems to solve the issue! time measured using option 1: 5.944461s time measured using option 2: 5.944314s Apparently CUDA's behaviour changed in the last two years. I updated SimpleSpeedTest.py."
            ]
        ],
        "votes": [
            -0.9999999
        ]
    },
    {
        "question": "This is my pycuda code for rotation.I have installed the latest cuda drivers and I use a nvidia gpu with cuda support.I have also installed the cuda toolkit and pycuda drivers.Still I get this strange error. import pycuda.driver as cuda import pycuda.compiler import pycuda.autoinit import numpy from math import pi,cos,sin _rotation_kernel_source = \"\"\" texture&lt;float, 2&gt; tex; __global__ void copy_texture_kernel( const float resize_val, const float alpha, unsigned short oldiw, unsigned short oldih, unsigned short newiw, unsigned short newih, unsigned char* data) { unsigned int x = blockIdx.x * blockDim.x + threadIdx.x; unsigned int y = blockIdx.y * blockDim.y + threadIdx.y; if( (x &gt;= newiw) || (y &gt;= newih) ) return; unsigned int didx = y * newiw + x; float xmiddle = (x-newiw/2.) / resize_val; float ymiddle = (y-newih/2.) / resize_val; float sx = ( xmiddle*cos(alpha)+ymiddle*sin(alpha) + oldiw/2.) ; float sy = ( -xmiddle*sin(alpha)+ymiddle*cos(alpha) + oldih/2.); if( (sx &lt; 0) || (sx &gt;= oldiw) || (sy &lt; 0) || (sy &gt;= oldih) ) { data[didx] = 255; return; } data[didx] = tex2D(tex, sx, sy); } \"\"\" mod_copy_texture=pycuda.compiler.SourceModule( _rotation_kernel_source ) copy_texture_func = mod_copy_texture.get_function(\"copy_texture_kernel\") texref = mod_copy_texture.get_texref(\"tex\") def rotate_image( a, resize = 1.5, angle = 20., interpolation = \"linear\", blocks = (16,16,1) ): angle = angle/180. *pi a = a.astype(\"float32\") calc_x = lambda (x,y): (x*a.shape[1]/2.*cos(angle)-y*a.shape[0]/2.*sin(angle)) calc_y = lambda (x,y): (x*a.shape[1]/2.*sin(angle)+y*a.shape[0]/2.*cos(angle)) xs = [ calc_x(p) for p in [ (-1.,-1.),(1.,-1.),(1.,1.),(-1.,1.) ] ] ys = [ calc_y(p) for p in [ (-1.,-1.),(1.,-1.),(1.,1.),(-1.,1.) ] ] new_image_dim = ( int(numpy.ceil(max(ys)-min(ys))*resize), int(numpy.ceil(max(xs)-min(xs))*resize), ) cuda.matrix_to_texref(a, texref, order=\"C\") if interpolation == \"linear\": texref.set_filter_mode(cuda.filter_mode.LINEAR) gridx = new_image_dim[0]/blocks[0] if \\ new_image_dim[0]%blocks[0]==1 else new_image_dim[0]/blocks[0] +1 gridy = new_image_dim[1]/blocks[1] if \\ new_image_dim[1]%blocks[1]==0 else new_image_dim[1]/blocks[1] +1 output = numpy.zeros(new_image_dim,dtype=\"uint8\") copy_texture_func( numpy.float32(resize), numpy.float32(angle), numpy.uint16(a.shape[1]), numpy.uint16(a.shape[0]), numpy.uint16(new_image_dim[1]), numpy.uint16(new_image_dim[0]), cuda.Out(output),texrefs=[texref],block=blocks,grid=(gridx,gridy)) return output if __name__ == '__main__': import Image import sys def main( ): if len(sys.argv) != 2: print \"You should really read the source...\\n\\nUsage: rotate.py &lt;Imagename&gt;\\n\" sys.exit(-1) img = Image.open(sys.argv[1]).convert(\"L\") i = numpy.fromstring(img.tostring(),dtype=\"uint8\").reshape(img.size[1],img.size[0]) irot = rotate_image(i) rotimg = Image.fromarray(irot,mode=\"L\") rotimg.save(\"rotated.png\") rotimg.show() main() This is my error. ImportError: libboost_python-gcc43-mt-1_39.so.1.39.0: cannot open shared object file: No such file or directory Please help me fix this.",
        "answers": [
            [
                "Did you google the error before asking here?Anyways try this BoostInstallationHowto#LD_LIBRARY_PATH.Please google before you ask here.Hope this helps you."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I can't understand why the below program runs ok for block=N,1,1 but not for 1,1,N (result is invalid value ) or 1,N,1 ( result is 0,1,0.....0) or 10,50,1 (result is 0,1,0..0) (N=500). import pycuda.gpuarray as gpuarray import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np import random from pycuda.curandom import rand import cmath import pycuda.driver as drv N=500 a_gpu=gpuarray.to_gpu(np.zeros(N).astype(np.int32)) mod =SourceModule(\"\"\" #include &lt;cmath&gt; extern \"C\" { __global__ void myfunc(int *a,int N) { int idx=threadIdx.x; //+blockIdx.x*blockDim.x; if (idx&lt;N) a[idx]=idx; } } \"\"\",no_extern_c=1) #call the function(kernel) func = mod.get_function(\"myfunc\") func(a_gpu,np.int32(N), block=(N,1,1),grid=(1,1)) a=a_gpu.get() print(\"a = \",a) --------------EDIT---------------------------------------- Ok,i forgot that if i use int idx=threadIdx.y ,then i can use block(1,N,1) . But , then , always must i use this arrangement block(N,1,1) ? I must understand that! Thank you!",
        "answers": [
            [
                "The first dimension corresponds to threadIdx.x, the second with threadIdx.y and the third with threadIdx.z When you launch block(N,1,1) threadIdx.x goes from 0 to N, while threadIdx.y and threadIdx.z are always zero. When you launch block(1, N, 1) threadIdx.x is always zero, threadIdx.y goes from 0 to N. so instead of having idx = threadIdx.x; Change it to idx = blockDim.x * threadIdx.y + threadIdx.x; or more accurately (only if using block(X, Y, Z) with Z &gt; 1) idx = (blockDim.y * threadIdx.z + threadIdx.y) * blockDim.x + threadIdx.x;"
            ],
            [
                "the third value is limited to a small number like 2 or 3 if I remember! you should be able to use (1,N,1)."
            ]
        ],
        "votes": [
            1.0000001,
            -1.9999999
        ]
    },
    {
        "question": "I am using 63 registers/thread ,so (32768 is maximum) i can use about 520 threads.I am using now 512 threads in this example. (The parallelism is in the function \"computeEvec\" inside global computeEHfields function function.) The problems are: 1) The mem check error below. 2) When i use numPointsRp&gt;2000 it show me \"out of memory\" ,but (if i am not doing wrong) i compute the global memory and it's ok. -------------------------------UPDATED--------------------------- i run the program with cuda-memcheck and it gives me (only when numPointsRs&gt;numPointsRp): ========= Invalid global read of size 4 ========= at 0x00000428 in computeEHfields ========= by thread (2,0,0) in block (0,0,0) ========= Address 0x4001076e0 is out of bounds ========= ========= Invalid global read of size 4 ========= at 0x00000428 in computeEHfields ========= by thread (1,0,0) in block (0,0,0) ========= Address 0x4001076e0 is out of bounds ========= ========= Invalid global read of size 4 ========= at 0x00000428 in computeEHfields ========= by thread (0,0,0) in block (0,0,0) ========= Address 0x4001076e0 is out of bounds ERROR SUMMARY: 160 errors -----------EDIT---------------------------- Also , some times (if i use only threads and not blocks (i haven't test it for blocks) ) if for example i have numPointsRs=1000 and numPointsRp=100 and then change the numPointsRp=200 and then again change the numPointsRp=100 i am not taking the first results! import pycuda.gpuarray as gpuarray import pycuda.autoinit from pycuda.compiler import SourceModule import numpy as np import cmath import pycuda.driver as drv Rs=np.zeros((numPointsRs,3)).astype(np.float32) for k in range (numPointsRs): Rs[k]=[0,k,0] Rp=np.zeros((numPointsRp,3)).astype(np.float32) for k in range (numPointsRp): Rp[k]=[1+k,0,0] #---- Initialization and passing(allocate memory and transfer data) to GPU ------------------------- Rs_gpu=gpuarray.to_gpu(Rs) Rp_gpu=gpuarray.to_gpu(Rp) J_gpu=gpuarray.to_gpu(np.ones((numPointsRs,3)).astype(np.complex64)) M_gpu=gpuarray.to_gpu(np.ones((numPointsRs,3)).astype(np.complex64)) Evec_gpu=gpuarray.to_gpu(np.zeros((numPointsRp,3)).astype(np.complex64)) Hvec_gpu=gpuarray.to_gpu(np.zeros((numPointsRp,3)).astype(np.complex64)) All_gpu=gpuarray.to_gpu(np.ones(numPointsRp).astype(np.complex64)) mod =SourceModule(\"\"\" #include &lt;pycuda-complex.hpp&gt; #include &lt;cmath&gt; #include &lt;vector&gt; #define RowRsSize %(numrs)d #define RowRpSize %(numrp)d typedef pycuda::complex&lt;float&gt; cmplx; extern \"C\"{ __device__ void computeEvec(float Rs_mat[][3], int numPointsRs, cmplx J[][3], cmplx M[][3], float *Rp, cmplx kp, cmplx eta, cmplx *Evec, cmplx *Hvec, cmplx *All) { while (c&lt;numPointsRs){ ... c++; } } __global__ void computeEHfields(float *Rs_mat_, int numPointsRs, float *Rp_mat_, int numPointsRp, cmplx *J_, cmplx *M_, cmplx kp, cmplx eta, cmplx E[][3], cmplx H[][3], cmplx *All ) { float Rs_mat[RowRsSize][3]; float Rp_mat[RowRpSize][3]; cmplx J[RowRsSize][3]; cmplx M[RowRsSize][3]; int k=threadIdx.x+blockIdx.x*blockDim.x; while (k&lt;numPointsRp) { computeEvec( Rs_mat, numPointsRs, J, M, Rp_mat[k], kp, eta, E[k], H[k], All ); k+=blockDim.x*gridDim.x; } } } \"\"\"% { \"numrs\":numPointsRs, \"numrp\":numPointsRp},no_extern_c=1) func = mod.get_function(\"computeEHfields\") func(Rs_gpu,np.int32(numPointsRs),Rp_gpu,np.int32(numPointsRp),J_gpu, M_gpu, np.complex64(kp), np.complex64(eta),Evec_gpu,Hvec_gpu, All_gpu, block=(128,1,1),grid=(200,1)) print(\" \\n\") #----- get data back from GPU----- Rs=Rs_gpu.get() Rp=Rp_gpu.get() J=J_gpu.get() M=M_gpu.get() Evec=Evec_gpu.get() Hvec=Hvec_gpu.get() All=All_gpu.get() --------------------GPU MODEL------------------------------------------------ Device 0: \"GeForce GTX 560\" CUDA Driver Version / Runtime Version 4.20 / 4.10 CUDA Capability Major/Minor version number: 2.1 Total amount of global memory: 1024 MBytes (1073283072 bytes) ( 0) Multiprocessors x (48) CUDA Cores/MP: 0 CUDA Cores //CUDA Cores 336 =&gt; 7 MP and 48 Cores/MP",
        "answers": [
            [
                "When i use numPointsRp&gt;2000 it show me \"out of memory\" Now we have some real code to work with, let's compile it and see what happens. Using RowRsSize=2000 and RowRpSize=200 and compiling with the CUDA 4.2 toolchain, I get: nvcc -arch=sm_21 -Xcompiler=\"-D RowRsSize=2000 -D RowRpSize=200\" -Xptxas=\"-v\" -c -I./ kivekset.cu ptxas info : Compiling entry function '_Z15computeEHfieldsPfiS_iPN6pycuda7complexIfEES3_S2_S2_PA3_S2_S5_S3_' for 'sm_21' ptxas info : Function properties for _Z15computeEHfieldsPfiS_iPN6pycuda7complexIfEES3_S2_S2_PA3_S2_S5_S3_ 122432 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads ptxas info : Used 57 registers, 84 bytes cmem[0], 168 bytes cmem[2], 76 bytes cmem[16] The key numbers are 57 registers and 122432 bytes stack frame per thread. The occupancy calculator suggests that a block of 512 threads will have a maximum of 1 block per SM, and your GPU has 7 SM. This gives a total of 122432 * 512 * 7 = 438796288 bytes of stack frame (local memory) to run your kernel, before you have allocated a single of byte of memory for input and output using pyCUDA. On a GPU with 1Gb of memory, it isn't hard to imagine running out of memory. Your kernel has a enormous local memory footprint. Start thinking about ways to reduce it. As I indicated in comments, it is absolutely unclear why every thread needs a complete copy of the input data in this kernel code. It results in a gigantic local memory footprint and there seems to be absolutely no reason why the code should be written in this way. You could, I suspect, modify the kernel to something like this: typedef pycuda::complex&lt;float&gt; cmplx; typedef float fp3[3]; typedef cmplx cp3[3]; __global__ void computeEHfields2( float *Rs_mat_, int numPointsRs, float *Rp_mat_, int numPointsRp, cmplx *J_, cmplx *M_, cmplx kp, cmplx eta, cmplx E[][3], cmplx H[][3], cmplx *All ) { fp3 * Rs_mat = (fp3 *)Rs_mat_; cp3 * J = (cp3 *)J_; cp3 * M = (cp3 *)M_; int k=threadIdx.x+blockIdx.x*blockDim.x; while (k&lt;numPointsRp) { fp3 * Rp_mat = (fp3 *)(Rp_mat_+k); computeEvec2( Rs_mat, numPointsRs, J, M, *Rp_mat, kp, eta, E[k], H[k], All ); k+=blockDim.x*gridDim.x; } } and the main __device__ function it calls to something like this: __device__ void computeEvec2( fp3 Rs_mat[], int numPointsRs, cp3 J[], cp3 M[], fp3 Rp, cmplx kp, cmplx eta, cmplx *Evec, cmplx *Hvec, cmplx *All) { .... } and eliminate every byte of thread local memory without changing the functionality of the computational code at all."
            ],
            [
                "Using R=1000 and then block=R/2,1,1 and grid=1,1 everything ok If i try R=10000 and block=R/20,1,1 and grid=20,1 ,then it show me \"out of memory\" I'm not familiar with pycuda and didn't read into your code too deeply. However you have more blocks and more threads, so it will local memory (probably the kernel's stack, it's allocated per thread), shared memory (allocated per block), or global memory that gets allocated based on grid or gridDim. You can reduce the stack size calling cudeDeviceSetLimit(cudaLimitStackSize, N)); (the code is for the C runtime API, but the pycuda equivalent shouldn't be too hard to find)."
            ]
        ],
        "votes": [
            1.0000001,
            1.0000001
        ]
    },
    {
        "question": "In simple CUDA programs we can print messages by threads by including cuPrintf.h but doing this in PyCUDA is not explained anywhere. How to do this in PyCUDA?",
        "answers": [
            [
                "On Compute Capability 2.0 and later GPUs, cuPrintf.h is discouraged in favor of just using CUDA's built-in printf(). To use it, just #include &lt;stdio.h&gt; and call printf() just like on the host. The PyCUDA wiki has a specific example of this."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I'm newbie to PyCUDA. I want to call function declared with __device__ from function declared with __global__. How can I do this in pyCUDA? import pycuda.driver as cuda from pycuda.compiler import SourceModule import numpy as n import pycuda.autoinit import pycuda.gpuarray as gp d=gp.zeros(shape=(128,128),dtype=n.int32) h=n.zeros(shape=(128,128),dtype=n.int32) mod=SourceModule(\"\"\" __global__ void matAdd(int *a) { int px=blockIdx.x*blockDim.x+threadIdx.x; int py=blockIdx.y*blockDim.y+threadIdx.y; a[px*128+py]+=1; matMul(px); } __device__ void matMul( int px) { px=5; } \"\"\") m=mod.get_function(\"matAdd\") m(d,block=(32,32,1),grid=(4,4)) d.get(h) Above code is giving me following error 7-linux-i686.egg/pycuda/../include/pycuda kernel.cu] [stderr: kernel.cu(8): error: identifier \"matMul\" is undefined kernel.cu(12): warning: parameter \"px\" was set but never used 1 error detected in the compilation of \"/tmp/tmpxft_00002286_00000000-6_kernel.cpp1.ii\". ]",
        "answers": [
            [
                "You should declare your matMul function before refering to it. You could do it like this: __device__ void matMul( int px); // declaration __global__ void matAdd(int *a) { int px=blockIdx.x*blockDim.x+threadIdx.x; int py=blockIdx.y*blockDim.y+threadIdx.y; a[px*128+py]+=1; matMul(px); } __device__ void matMul( int px) // implementation { px=5; // by the way, this assignment does not propagate outside this function } , or just move whole matMul function to be before matAdd."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I'm having a difficult time installing pycuda. I am running on Ubuntu 12.04. I first installed the Enthought python distribution (even though I already had python on the computer). I added the enthought python location to the path in my .profile (and this works without issue, typing python or ipython from the command line correctly uses the enthought version). I then followed the instructions from http://wiki.tiker.net/PyCuda/Installation/Linux/Ubuntu (skipping steps 0-1 as I already had numpy and cuda installed). Everything seems to run and install fine. But when I try to actually use pycuda, it fails. For example, here's what happens when I try to run the hello_gpu.py example: :~/Downloads/pycuda-2012.1$ python examples/hello_gpu.py Traceback (most recent call last): File \"examples/hello_gpu.py\", line 1, in &lt;module&gt; import pycuda.driver as drv File \"/usr/lib/python_enthought/lib/python2.7/site-packages/pycuda-2012.1-py2.7-linux- x86_64.egg/pycuda/driver.py\", line 2, in &lt;module&gt; from pycuda._driver import * ImportError: /usr/lib/libboost_python-py27.so.1.46.1: undefined symbol: PyUnicodeUCS4_FromEncodedObject Alternatively, when I try to import the pycuda package from idle, I get a different error: Python 2.7.3 |EPD 7.3-1 (64-bit)| (default, Apr 11 2012, 17:52:16) [GCC 4.1.2 20080704 (Red Hat 4.1.2-44)] on linux2 Type \"credits\", \"demo\" or \"enthought\" for more information. &gt;&gt;&gt; import pycuda &gt;&gt;&gt; pycuda &lt;module 'pycuda' from 'pycuda/__init__.pyc'&gt; &gt;&gt;&gt; import pycuda.driver as drv Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"pycuda/driver.py\", line 2, in &lt;module&gt; from pycuda._driver import * ImportError: No module named _driver I suspect part of the issue is me somewhere not specifying the correct path to the python interpreter, but I cannot figure out where this error occurs. Any suggestions would be most welcome, I am out of ideas.",
        "answers": [
            [
                "This problem would appear to have been caused by conflicts between two different Python installations on the same system. The OP apparently solved this by uninstalling an Enthough Python installation and using only the system Python installation. There might well be other ways to solve this using a non-system Python installation, as suggested here [This answer was assembled from comments and added as a community wiki entry in the hope it will receive a vote and be removed from the unanswered list for the PyCUDA tag]"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "i installed sabayon linux and i tried to install pycuda but i am receiving the following error when i try su -c \"make install\" In file included from src/cpp/cuda.cpp:1:0: src/cpp/cuda.hpp:12:18: fatal error: cuda.h: No such file or directory compilation terminated. error: command 'x86_64-pc-linux-gnu-g++' failed with exit status 1 I must mention that i can compile with cuda. The same happens and when i try with sudo -E sh -c \"make install\" My .bashrc is : # Put your fun stuff here. export PATH=~/bin:$PATH export PATH=$PATH:$HOME/Matlab_2010b/bin export PATH=/opt/cuda/bin:$PATH export LD_LIBRARY_PATH=/opt/cuda/lib64:$LD_LIBRARY_PATH export CUDA_ROOT=/opt/cuda/bin (When i do echo $PATH as user it shows me : /opt/cuda/bin but when i try it as root it doesn't show anything and i had to do export PATH...again in order to show.)",
        "answers": [
            [
                "You have CUDA installed in a non-standard path and the PyCUDA installer doesn't know how to find the toolkit headers it needs to compile. As per the installation instructions, you need to do something like this: python configure.py --cuda-root=/opt/cuda then su -c \"make install\" this will ensure that the compilation can find the necessary driver API headers to build the support libraries and hardcode all the right paths into the PyCUDA python libraries so that everything works correctly."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I have a class written in C++ that uses also some definitions from cuda_runtime.h, this is a part from opensource project named ADOL-C, you can have a look here! This works when I'm using CUDA-C, but I want somehow to import this class in PyCUDA, if there is a possibility of doing that. So, I will use this class inside of kernels (not in 'main') to define specific variables that are used for computing the derivatives of a function. Is there any way for of passing this class to PyCUDA's SourceModule? I asked a similar question, but here I would like to explain a bit more that that. So, there is a solution compiling my C code using nvcc -cubin (thanks to talonmies) and then importing it with driver.module_from_file(), but, I would like to use SourceModule and write those kernels inside of a .py file, so it could be more user-friendly. My example would look something like this: from pycuda import driver, gpuarray from pycuda.compiler import SourceModule import pycuda.autoinit kernel_code_template=\"\"\" __global__ void myfunction(float* inx, float* outy, float* outderiv) { //defining thread index ... //declare dependent and independet variables as adoubles //this is a part of my question adtl::adouble y[3]; adtl::adouble x[3]; // ... } \"\"\" ... this is just an idea, but SourceModule will not know what are \"adouble's\", because they are defined in class definition adoublecuda.h, so I hope you understand my question now better. Does anyone have a clue where should I begin? If not, I will write this kernels in CUDA-C, and use nvcc -cubin option. Thanks for help!",
        "answers": [
            [
                "The PyCUDA SourceModule system is really only a way of getting the code you pass into a file, compiling that file with nvcc into a cubin file, and (optionally) loading that cubin file into the current CUDA context. The PyCUDA compiler module knows absolutely nothing about CUDA kernel syntax or code, and has (almost) no influence on the code that is compiled [the almost qualifier is because it can bracket user submitted code with an extern \"C\" { } declaration to stop C++ symbol mangling]. So to do what I think you are asking about, you should only require an #include statement for whatever headers your device code needs in the submitted string, and a suitable set of search paths in a python list passed via the include_dirs keyword option. If you do something like this: from pycuda import driver, gpuarray from pycuda.compiler import SourceModule import pycuda.autoinit kernel_code_template=\"\"\" #include \"adoublecuda.h\" __global__ void myfunction(float* inx, float* outy, float* outderiv) { //defining thread index ... //declare dependent and independet variables as adoubles //this is a part of my question adtl::adouble y[3]; adtl::adouble x[3]; // ... } \"\"\" module = SourceModule(kernel_code_template, include_dirs=['path/to/adoublecuda']) and it should automagically work (note untested, use at own risk)."
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "I'm a begginer in using pycuda, so far, I've learned some basic stuff how to write the kernels from book \"Cuda by example\", and my next task is to use a class, that is already written in C++, inside of kernels. How can I import this .h file to pycuda? Do I have to use boost python for transfering this .h so it can be imported like any other module in python? The thing is that I only need this special variables inside of a kernel. This class is used for computing the derivatives and I will write you a couple of lines so you can see how it's constructed: adoublecuda.h: namespace adtl { class adouble { public: // ctors __device__ __host__ inline adouble(); __device__ __host__ inline adouble(const double v); __device__ __host__ inline adouble(const adouble&amp; a); __device__ __host__ inline adouble operator - () const; __device__ __host__ inline adouble operator + () const; ...etc. I use this class inside of C-CUDA just typing the #include \"adoublecuda.h\" and now I want to include the same thing in PyCUDA. I'm only using this class inside of kernels (I don't need adouble variables inside of main). So will I have to use boost python in order to include this header file to PyCuda? Any advice would be appreciated, thank you for helping me!",
        "answers": [
            [
                "If it is only used inside device code, and you are not intending to do any metaprogramming, then you actually don't need to do anything with Python or PyCUDA to make it work. Just compile your device code to a CUBIN object (so use nvcc -cubin) and then load that CUBIN file using driver.module_from_file(). You can see a complete example of this in this older answer of mine."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "sorry if this all seem nooby and unclear, but I'm currently learning Netlogo to model agent-based collective behavior and would love to hear some advice on alternative software choices. My main thing is that I'd very much like to take advantage of PyCuda since, from what I understand, it enables parallel computation. However, does that mean I still have to write the numerical script in some other environment and implement the visuals in yet another one??? If so, my questions are: What numerical package should I use? PyEvolve, DEAP, or something else? It appears that PyEvolve is no longer being developed and DEAP is just a wrapper on the outdated(?) EAP. Graphic-wise, I find mayavi2 and vtk promising. The problem is, none of the numerical package seems to bind to these readily. Is there no better alternative than to save the numerical output to datafile and feed them into, say, mayavi2? Another option is to generate the data via Netlogo and feed them into a graphing package from (2). Is there any disadvantage to doing this? Thank you so much for shedding light on this confusion.",
        "answers": [
            [
                "You almost certainly do not want to use CUDA unless you are running into a significant performance problem. In general CUDA is best used for solving floating point linear algebra problems. If you are looking for a framework built around parallel computations, I'd look towards OpenCL which can take advantage of GPUs if needed.. In terms of visualization, I'd strongly suggest targeting a a specific data interchange format and then letting some other program do that rendering for you. The only reason I'd use something like VTK is if for some reason you need more control over the visualization process or you are looking for a real time solution."
            ],
            [
                "Probably the best choice for visualization would be to use an intermediate format and do it in another program. But for performance, i'd rather configure a JVM for a cluster and run NetLogo on it. I've not tried it yet but i'm thinking seriously to try NetLogo on a Beowulf style cluster. BTW, there is an ABM platform called Repast that is said to have Python interface if you're planning to implement your code in Python."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm writing a server with Twisted and pyCUDA. A restriction of how CUDA works is that I must access the CUDA context in the same thread I initialized it. However, the threadpool implementation of Twisted doesn't allow me to request a specific thread. For example, if I have multiple clients connected to the server, they will request some computation done with CUDA. Multiple operations will be requested with the same CUDA object (initialization is expensive). I wanted to use the deferToThread function, but this does not allow me to defer to a specific thread, just 'some' thread. What I would like to do, ideally, is to use a mechanism like deferToThread but to specify the thread that the code runs on. Any suggestions would be appreciated, maybe Twisted is the wrong way to go for this project.",
        "answers": [
            [
                "The CUDA Driver API has supported submitting working to a CUcontext (Driver API) from multiple thread through use of the functions cuCtxPushCurrent() and cuCtxPopCurrent(). Current for many releases. In CUDA 4.0 and beyond the CUDA Runtime supports submitting work to a device (CUcontext) from multiple OS threads or submitting work to multiple devices from a single OS thread using the function cudaSetDevice(). I'm not sure if this is exposed through pyCUDA."
            ],
            [
                "For your speciifc use-case with CUDA, Greg Smith's answer is probably the best. But, for others who happen upon this question looking for a way to do things in one particular thread in Twisted, Calendar and Contacts Server has an implementation of that functionality, and as you can see, it's not too complex."
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001
        ]
    },
    {
        "question": "I am beginner and installing pycuda2011.2.2 on ubuntu 11.10, but can't complete it. Cuda is 4.0. I have installed libraries: $ sudo apt-get install build-essential python-dev python-setuptools libboost-python-dev libboost-thread-dev -y calling configure.py like this: $ ./configure.py --cuda-root=/usr/local/cuda --cudadrv-lib-dir=/usr/lib --boost-inc-dir=/usr/include --boost-lib-dir=/usr/lib --boost-python-libname=boost_python-mt-py27 --boost-thread-libname=boost_thread-mt But, When i do: .....@ubuntu:~/pycuda-2011.2.2$ make -j 4 I get this error: /usr/bin/ld: cannot find -lcuda /usr/bin/ld: skipping incompatible /usr/local/cuda/lib/libcurand.so when searching for -lcurand why this error ? Thanks.",
        "answers": [
            [
                "You need to set the LDFLAGS environment variable so that the pycuda setup can find libcuda.so, which on ubuntu systems is in a non-standard location (/usr/lib/nvidia-current). The installation of pycuda 2012.1 is entirely distutils based, no Makefile involved. You install pycuda by running ./configure.py with the appropriate options followed by LDFLAGS=/usr/lib/nvidia-current python setup.py install."
            ],
            [
                "If you use some newer drivers for the nvidia card, like nvidia-313 (that's what I use), then the file libcuda.so (which is nicknamed lcuda, I don't know why) may not be in the cuda installation directory (which, by default, is /usr/lib/cuda). Instead, you might have to find it by yourself. Do: $ find /usr/lib/*/libcuda.so for me, the result is /usr/lib/nvidia-313-updates/libcuda.so so, when installing pycuda, I do: $ python configure.py --cuda-root=/usr/lib/nvidia-313-updates $ make $ sudo make install then, $ optirun python test/test_driver.py or $ optirun python some_program_which_imports_pycuda.py should work fine."
            ]
        ],
        "votes": [
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm trying to run pycuda introductory tutorial after installing Visual C++ Express 2010 and all kinds of Nvidia drivers, SDK, etc. I get to mod = SourceModule(\"\"\" __global__ void doublify(float *a) { int idx = threadIdx.x + threadIdx.y*4; a[idx] *= 2; } \"\"\") without errors. But this call in IPython yields CompileError: nvcc compilation of c:\\users\\koj\\appdata\\local\\temp\\tmpbbhsca\\kernel.cu failed [command: nvcc --cubin -arch sm_21 -m64 -IC:\\Python27\\lib\\site-packages\\pycuda\\..\\..\\..\\include\\pycuda kernel.cu] [stderr: nvcc fatal : Visual Studio configuration file '(null)' could not be found for installation at 'C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin/../..'] After this I installed MS Windows SDK without impact on this error, although I now seem to be able to choose x64 programs in Visual C++ Express 2010. So, what do I need to fix? (I used precompiled pycuda files.)",
        "answers": [
            [
                "In stackoverflow post CUDA linking error - Visual Express 2008 - nvcc fatal due to (null) configuration file the answer by Amitabh Mritunjai fixed this error. He explains the steps for Visual C++ Express 2010 while the most of the rest of the post is about 2008."
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "I tried to install PyCUDA using these instructions: http://wiki.tiker.net/PyCuda/Installation/Linux and at the point 3 I was compiling with: su -c \"make install\" have this output from console: running install running bdist_egg running egg_info writing requirements to pycuda.egg-info/requires.txt writing pycuda.egg-info/PKG-INFO writing top-level names to pycuda.egg-info/top_level.txt writing dependency_links to pycuda.egg-info/dependency_links.txt reading manifest file 'pycuda.egg-info/SOURCES.txt' reading manifest template 'MANIFEST.in' writing manifest file 'pycuda.egg-info/SOURCES.txt' installing library code to build/bdist.linux-x86_64/egg running install_lib running build_py running build_ext building '_driver' extension gcc -pthread -DDYNAMIC_ANNOTATIONS_ENABLED=1 -fwrapv -Wall -g -fPIC -DPYGPU_PACKAGE=pycuda -DHAVE_CURAND=1 -DBOOST_PYTHON_SOURCE=1 -DPYGPU_PYCUDA=1 -DBOOST_MULTI_INDEX_DISABLE_SERIALIZATION=1 -Dboost=pycudaboost -Isrc/cpp -Ibpl-subset/bpl_subset -I/usr/local/cuda/include -I/usr/lib/python3.2/site-packages/numpy/core/include -I/usr/lib/python3.2/site-packages/numpy/core/include -I/usr/include/python3.2mu -c src/cpp/cuda.cpp -o build/temp.linux-x86_64-3.2/src/cpp/cuda.o Inf ile included from src/cpp/cuda.cpp:1:0: src/cpp/cuda.hpp:12:18: fatal error: cuda.h: No such file or directory compilation terminated. error: command 'gcc' failed with exit status 1 What should I do? EDIT: My OS is: Arch Linux 3.3.2.1. GPU: Nvidia GF 9600M GS",
        "answers": [
            [
                "This one full command worked for me: sudo pip3 install --global-option=build_ext --global-option=\"-I/usr/local/cuda/include\" --global-option=\"-L/usr/local/cuda/lib64\" pycuda"
            ],
            [
                "Instead of su -c \"make install\" try sudo su - (to become root), and then, as root: make install (after navigating to the right directory)"
            ]
        ],
        "votes": [
            1e-07,
            -0.9999999
        ]
    },
    {
        "question": "TL;DR version: \"What's the best way to round-robin kernel calls to multiple GPUs with Python/PyCUDA such that CPU and GPU work can happen in parallel?\" with a side of \"I can't have been the first person to ask this; anything I should read up on?\" Full version: I would like to know the best way to design context, etc. handling in an application that uses CUDA on a system with multiple GPUs. I've been trying to find literature that talks about guidelines for when context reuse vs. recreation is appropriate, but so far haven't found anything that outlines best practices, rules of thumb, etc. The general overview of what we're needing to do is: Requests come in to a central process. That process forks to handle a single request. Data is loaded from the DB (relatively expensive). The the following is repeated an arbitrary number of times based on the request (dozens): A few quick kernel calls to compute data that is needed for later kernels. One slow kernel call (10 sec). Finally: Results from the kernel calls are collected and processed on the CPU, then stored. At the moment, each kernel call creates and then destroys a context, which seems wasteful. Setup is taking about 0.1 sec per context and kernel load, and while that's not huge, it is precluding us from moving other quicker tasks to the GPU. I am trying to figure out the best way to manage contexts, etc. so that we can use the machine efficiently. I think that in the single-gpu case, it's relatively simple: Create a context before starting any of the GPU work. Launch the kernels for the first set of data. Record an event for after the final kernel call in the series. Prepare the second set of data on the CPU while the first is computing on the GPU. Launch the second set, repeat. Insure that each event gets synchronized before collecting the results and storing them. That seems like it should do the trick, assuming proper use of overlapped memory copies. However, I'm unsure what I should do when wanting to round-robin each of the dozens of items to process over multiple GPUs. The host program is Python 2.7, using PyCUDA to access the GPU. Currently it's not multi-threaded, and while I'd rather keep it that way (\"now you have two problems\" etc.), if the answer means threads, it means threads. Similarly, it would be nice to just be able to call event.synchronize() in the main thread when it's time to block on data, but for our needs efficient use of the hardware is more important. Since we'll potentially be servicing multiple requests at a time, letting other processes use the GPU when this process isn't using it is important. I don't think that we have any explicit reason to use Exclusive compute modes (ie. we're not filling up the memory of the card with one work item), so I don't think that solutions that involve long-standing contexts are off the table. Note that answers in the form of links to other content that covers my questions are completely acceptable (encouraged, even), provided they go into enough detail about the why, not just the API. Thanks for reading!",
        "answers": [
            [
                "Caveat: I'm not a PyCUDA user (yet). With CUDA 4.0+ you don't even need an explicit context per GPU. You can just call cudaSetDevice (or the PyCUDA equivalent) before doing per-device stuff (cudaMalloc, cudaMemcpy, launch kernels, etc.). If you need to synchronize between GPUs, you will need to potentially create streams and/or events and use cudaEventSynchronize (or the PyCUDA equivalent). You can even have one stream wait on an event inserted in another stream to do sophisticated dependencies. So I suspect the answer to day is quite a lot simpler than talonmies' excellent pre-CUDA-4.0 answer. You might also find this answer useful. (Re)Edit by OP: Per my understanding, PyCUDA supports versions of CUDA prior to 4.0, and so still uses the old API/semantics (the driver API?), so talonmies' answer is still relevant."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "My program has two kernels and the second kernel should use the already uploaded input data and the results from the first kernel, so I can save the memory transfers. How would I archive this? This is how I launch my kernels: result = gpuarray.zeros(points, dtype=np.float32) kernel( driver.In(dataT),result,np.int32(points), grid = (blocks,1), block = (block_size, 1, 1), )",
        "answers": [
            [
                "In pycuda you won't transfer data to and from the device unless you explicitly request it. For example, if you allocate memory and transfer some data to the GPU with: result = float64(zeros( (height,width) ) result_device = gpuarray.to_gpu(result) The variable result_device is a reference to the data in the GPU. You can pass result_device to any other kernel without incurring a memory transfer back to the CPU. In this case a memory transfer will happen again when you call: result = result_device.get()"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have the below code written in php and have been reading up on Cuda to utilize the GPU processing power of my old Geforce 8800 Ultra. How do I convert this nested combinations test to Cuda parallel processing code (if even possible...)? The total combinations of the 2d arrays: $a, $b, $c, $d, $e quickly rise into the trillions... foreach($a as $aVal){ foreach($b as $bVal){ foreach($c as $cVal){ foreach($d as $dVal){ foreach($e as $eVal){ $addSum = $aVal[0]+$bVal[0]+$cVal[0]+$dVal[0]+$eVal[0]; $capSum = $aVal[1]+$bVal[1]+$cVal[1]+$dVal[1]+$eVal[1]; if($capSum &lt;= CAP_LIMIT){ $tempArr = array(\"a\" =&gt; $aVal[2],\"b\" =&gt; $aVal[2],\"c\" =&gt; $aVal[2], \"d\" =&gt; $aVal[2],\"e\" =&gt; $aVal[2],\"addTotal\" =&gt; $addSum,\"capTotal\" =&gt; $capSum); array_push($topCombinations, $tempArr); if(count($topCombinations) &gt; 1000){ $topCombinations = $ca-&gt;arraySortedDescend($topCombinations); array_splice($topCombinations, 900); } } } } } } }",
        "answers": [
            [
                "This is a very wide-open question. It requires conversion between languages as well as designing a parallel algorithm. I won't go into too much detail, but in a nutshell: How you parallelize it depends on the size of your arrays ($a - $e). If they are large enough, you could parallelize only the outer one or two loops across threads in a grid, and do the inner loops sequentially. If they are not super large, you might want to either flatten 2-3 of the outer loops or possibly implement them using 2D or 3D thread blocks and grids in CUDA."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "i'd tried for more than week to trace down a clear/clean install of pycuda on my win7 x64bit machine and found http://wiki.tiker.net/PyCuda/Installation/Windows which-i can surely say- the only page everyone keep ref to it and it's too vague. i am asking for any one who really use pycuda can help me setup it on my machine ,using it with visual studio 2010,,and any guide on installation and pdf resourcess",
        "answers": [
            [
                "The problem is the VS 10 compiler. Pycuda does not support this compiler. Someone should update pycuda for that. I am also trying to find a workaround!"
            ],
            [
                "I've installed pycuda on Windows 7 64bit a few of times using the instructions on that page. How easy the instructions are to follow will depend on your level of familiarity with, python, cuda, and C++. IMO, if this is the first time you are installing most of the required apps, go with the all binary approach: http://wiki.tiker.net/PyCuda/Installation/Windows#Windows_7_64-bit_with_Visual_Studio_Professional_2008_.28Strictly_Binary_Versions.29 Remove everything you have done so far and start from there."
            ]
        ],
        "votes": [
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "I have difficulties to use complex numbers in cuda,pycuda. I have this in C: #include &lt;complex&gt; typedef std::complex&lt;double&gt; cmplx; .... cmplx j(0.,1.); Also,in the same code: #include &lt;boost/python.hpp&gt; #include &lt;boost/array.hpp&gt; ... typedef std::vector&lt; boost::array&lt;std::complex&lt;double&gt;,3 &gt; &gt; ComplexFieldType; typedef std::vector&lt; boost::array&lt;double,3&gt; &gt; RealFieldType; ... __global__ void compute(RealFieldType const &amp; Rs,ComplexFieldType const &amp; M,..) ... How can i convert this to use it with pycuda? I tried sth like this (according to the book 'cuda by an example'): struct cuComplex { float real; float imag; cuComplex(float a,float b): real(a),imag(b){} cuComplex operator *(const cuComplex&amp; a) { return cuComplex(real*a.real -imag*a.imag ,imag*a.real +real*a.imag); } cuComplex operator +(const cuComplex&amp; a) { return cuComplex(real+a.real ,imag+a.imag); }; cuComplex j(0.,1.); //instead of cmplx j(0.,1.); __global__ void compute(float *Rs,cuComplex * M,..) //instead of RealFieldType const &amp; Rs,ComplexFieldType const &amp; M .... Some of the errors i take are: data member initializer is not allowed this declaration has no storage class or type specifier Thank you! ---------------------EDIT---------------------------------------------- I did the following using #include &lt;pycuda-complex.hpp&gt; (relative to the above) : pycuda::complex&lt;float&gt; cmplx; cmplx j(0.,1.); and as for typedef std::vector&lt; boost::array&lt;std::complex&lt;double&gt;,3 &gt; &gt; ComplexFieldType; and ComplexFieldType const &amp; M ,inside the global function, i tried just \"float *M \" or \"cmplx *M\". Until now , i am getting error : variable \"cmplx\" is not a type name If i use pycuda::complex cmplx; ,then i get: identifier \"cmplx\" is undefined name followed by \"::\" must be a class or namespace name Also: expression must have pointer-to-object type (but maybe this is from another part of code)",
        "answers": [
            [
                "It really isn't clear what you are actually trying to do (if you actually know yourself), and the question is getting progressively more confused as the edits and comments roll on. But to expand Andreas's answer a little, here is a simple, compilable piece of CUDA code which uses the pycuda native complex type correctly: #include &lt;pycuda-complex.hpp&gt; template&lt;typename T&gt; __global__ void kernel(const T * x, const T *y, T *z) { int tid = threadIdx.x + blockDim.x * blockIdx.x; z[tid] = x[tid] + y[tid]; } typedef pycuda::complex&lt;float&gt; scmplx; typedef pycuda::complex&lt;double&gt; dcmplx; template void kernel&lt;float&gt;(const float *, const float *, float *); template void kernel&lt;double&gt;(const double *, const double *, double *); template void kernel&lt;scmplx&gt;(const scmplx *, const scmplx *, scmplx *); template void kernel&lt;dcmplx&gt;(const dcmplx *, const dcmplx *, dcmplx *); This gives you single and double real and complex versions of the trivial kernel and compiles with nvcc something like this: $ nvcc -arch=sm_20 -Xptxas=\"-v\" -I$HOME/pycuda-2011.1.2/src/cuda -c scmplx.cu ptxas info : Compiling entry function '_Z6kernelIN6pycuda7complexIdEEEvPKT_S5_PS3_' for 'sm_20' ptxas info : Used 12 registers, 44 bytes cmem[0], 168 bytes cmem[2], 4 bytes cmem[16] ptxas info : Compiling entry function '_Z6kernelIN6pycuda7complexIfEEEvPKT_S5_PS3_' for 'sm_20' ptxas info : Used 8 registers, 44 bytes cmem[0], 168 bytes cmem[2] ptxas info : Compiling entry function '_Z6kernelIdEvPKT_S2_PS0_' for 'sm_20' ptxas info : Used 8 registers, 44 bytes cmem[0], 168 bytes cmem[2] ptxas info : Compiling entry function '_Z6kernelIfEvPKT_S2_PS0_' for 'sm_20' ptxas info : Used 4 registers, 44 bytes cmem[0], 168 bytes cmem[2] Perhaps this goes someway to answering your question...."
            ],
            [
                "Use #include &lt;pycuda-complex.hpp&gt; { pycuda::complex&lt;float&gt; x(5, 17); } Same interface as std::complex&lt;&gt;, in fact derived from the STLport version of that."
            ]
        ],
        "votes": [
            2.0000001,
            2.0000001
        ]
    },
    {
        "question": "[update] How can I output the warning messages from compiler.SourceModule(kernel_code)? With help of @flipchart I was able to pass the right parameters to NVCC trough PyCUDA, but I still don't know, where to access the compiler warnings. [original question] Using NVCC directly one can use the compiler switch -Wall*. How would one archive this in pycuda? I tried mod = compiler.SourceModule(kernel_code,options=['-Wall']), but the error message states: pytools.prefork.ExecError: error invoking 'nvcc --cubin -Wall -arch sm_11 -I/usr/local/lib/python2.6/dist-packages/pycuda-0.94.2-py2.6-linux-x86_64.egg/pycuda/../include/pycuda kernel.cu': status 255 invoking 'nvcc --cubin -Wall -arch sm_11 -I/usr/local/lib/python2.6/dist-packages/pycuda-0.94.2-py2.6-linux-x86_64.egg/pycuda/../include/pycuda kernel.cu': nvcc fatal : Unknown option 'Wall' The source problem is, that I spent a whole day debugging, because I overlooked an inexplicit conversion fromfloat to int. *Warnings from system-header with \"--compiler-options -Wall\" since CUDA 3.0",
        "answers": [
            [
                "The -Wall option is not a nvcc compiler option, but rather one that is passed on to the supporting compiler (g++ or cl.exe). You need to pass in the option --compiler-options -Wall to indicate to nvcc that the option is for the supporting compiler. In you python code: mod = compiler.SourceModule(kernel_code,options=['--compiler-options','-Wall']) PyCuda seems to want each option specified as a list item, otherwise it wraps the whole thing in quotes which nvcc doesn't like."
            ]
        ],
        "votes": [
            5.0000001
        ]
    },
    {
        "question": "I am a recreational pythonista who just got into pyCUDA. I am trying to figure out how to implement a linear interpolation (lerp) using pyCUDA. The CUDA CG function is: http://http.developer.nvidia.com/Cg/lerp.html My ultimate goal is a bilinear interpolation in pycuda from a set of weighted random points. I've never programmed C, or CUDA for that matter, and am learning as I go. This is how far I've gotten: import pycuda.autoinit import pycuda.driver as drv import pycuda.compiler as comp lerpFunction = \"\"\"__global__ float lerp(float a, float b, float w) { return a + w*(b-a); }\"\"\" mod = comp.SourceModule(lerpFunction) # This returns an error telling me a global must return a void. :( Any help on this would be fantastic!",
        "answers": [
            [
                "The error message is pretty explicit - CUDA kernels cannot return values, they must be declared void, and modifiable arguments passed as pointers. It would make more sense for your lerp implementation to be declared as a device function like this: __device__ float lerp(float a, float b, float w) { return a + w*(b-a); } and then called from inside a kernel for each value that requires interpolation. Your lerp function lacks a lot of \"infrastructure\" to be a useful CUDA kernel. EDIT: A really basic kernel along the same lines might look something like this: __global__ void lerp_kernel(const float *a, const float *b, const float w, float *y) { int tid = threadIdx.x + blockIdx.x*blockDim.x; // unique thread number in the grid y[tid] = a[tid] + w*(b[tid]-a[tid]); }"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have a code in c++.I modified it with cuda.But ,i want to use Pycuda. The code has for example : #include &lt;vector&gt; #include &lt;boost/array.hpp&gt; #include &lt;complex&gt; #include &lt;boost/python.hpp&gt; typedef std::complex&lt;double&gt; cmplx; typedef std::vector&lt; boost::array&lt;std::complex&lt;double&gt;,3 &gt; &gt; ComplexFieldType; typedef std::vector&lt; boost::array&lt;double,3&gt; &gt; RealFieldType; In Pycuda you can't use the libraries. How can i deal with that problem?",
        "answers": [
            [
                "For anyone who might be interested check this You can use #include &lt;pycuda-complex.hpp&gt;"
            ]
        ],
        "votes": [
            1e-07
        ]
    },
    {
        "question": "Although i had installed pycuda and using it ok,it started (without doing sth) not to work.So,i i tried to do the install again ,but when i am doing python configure.py --cuda-root=/usr/local/cuda/bin it gives me the error in the title. The nvcc file is in the above directory.",
        "answers": [
            [
                "pycuda is not finding nvcc. Did you try adding /usr/local/cuda/bin to your env PATH variable? That's the way I have this setup. Edit: As far as I can tell the configure.py doesn't call nvcc compiler it just creates the the makefile. I take that this problem happens when you run sudo -c \"make install\" which calls setup.py. A couple of things to try. Make sure that you have CUDA_ROOT set: echo $CUDA_ROOT If it's empty, set it with: export CUDA_ROOT=/usr/local/cuda/bin Try running the make command again. Now with the -E to preserve your env: sudo -E sh -c \"make install\""
            ],
            [
                "I encountered the same issue on a Slackware64 13.37. Install command su -c \"make install\" switches to root (0bv10u5Ly) thus CUDA_ROOT should be set in the root's profile. CUDA_ROOT is not an environment variable, it's used by the setup.py. Add /usr/local/cuda/bin to PATH and define CUDA_ROOT=/usr/local/cuda/bin then try to install again. This is the quick and dirty way but if none of above worked out for you like me, below will definitely work. (: Remove nvcc_path = search_on_path([\"nvcc\", \"nvcc.exe\"]) if nvcc_path is None: print(\"*** CUDA_ROOT not set, and nvcc not in path. Giving up.\") sys.exit(1) and set cuda_root_default = \"/usr/local/cuda/bin\" in setup.py file. Then try su -c \"make install\"."
            ],
            [
                "In my case, I had to set CUDA_ROOT=/usr/local/cuda because with /usr/local/cuda/bin path, it was not able find include folder and it was failing with error didn't find cuda.h."
            ]
        ],
        "votes": [
            11.0000001,
            2.0000001,
            2.0000001
        ]
    },
    {
        "question": "i am trying to learn pycuda and i have a few questions that i am trying to understand. I think my main question is how to communicate between pycuda and a function inside a cuda file. So,if I have a C++ file (cuda file) and in there i have some functions and i want to implement pycuda in one of them.For example ,lets say i want the function 'compute' which contains some arrays and do calculations on them.What would be my approach? 1) Initialize the arrays in python,allocate memory to GPU and transfer data to GPU. 2) Call the mod=SourceModule(\"\"\" global void ......\"\"\") from pycuda. Now, i want to ask :How i will handle this module?I will put all the 'compute' function in it?Because,if only do some calculations in 'global' ,i don't know how to communicate then between pycuda and c++ functions.How i will pass my results back to c++ file(cuda file). 3) In cuda we have the number of threads as 'blockDIm' and the number of blocks as 'gridDim'.In pycuda?We have block size ,block(4,4,1) which means 16 threads??And grid size, size(16,16) means 256 blocks? 4) I tried to do in pycuda an example from 'cuda by an example book' which adds vectors.The code is below: import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import scipy as sc N=50*1024 a=sc.arange(0,N).astype(sc.float32) a_gpu = cuda.mem_alloc(a.nbytes) #allocate memory on GPU cuda.memcpy_htod(a_gpu, a) #transfer data to the GPU b=sc.array([i**2 for i in range(0,N)]).astype(sc.float32) b_gpu = cuda.mem_alloc(b.nbytes)#allocate memory on GPU cuda.memcpy_htod(b_gpu, b) #transfer data to the GPU c=sc.zeros(N).astype(sc.float32) c_gpu = cuda.mem_alloc(c.nbytes)#allocate memory on GPU mod =SourceModule(\"\"\" __global__ void add(int*a,int *b,int *c){ int tid=threadIdx.x + blockIdx.x*gridDim.x; while (tid&lt;N){ c[tid]=a[tid]+b[tid]; tid+=blockDim.x*gridDim.x; } } \"\"\") #call the function(kernel) func = mod.get_function(\"add\") func(a_gpu,b_gpu,c_gpu, block=(16,16,1),grid=(16,16)) #transfer data back to CPU cuda.memcpy_dtoh(c, c_gpu) but it gives me an error: \"identifier \"N\" is undefined \" Thanks!",
        "answers": [
            [
                "The way I use pycuda and the way I think it is intended to be used is as a bridge interface between python and cuda. It's not a python-&gt;c++ interface tool. For that you will have to look at something like SWIG. I wouldn't use pycuda inside c++ code to interface with a GPU, instead I would prototype or design my application using pycuda and later move it to using c++ only. With that in mind I'll try to tackle you questions 1)With Pycuda you could also use the gpuarray module which will the allocation and transfer steps for you, so then you can just instantiate them and use them in the GPU: import pycuda.gpuarray as gpuarray a = gpuarray.arange(400, dtype=numpy.float32) b = gpuarray.arange(400, dtype=numpy.float32) #call Cuda function pass 'a' and 'b' resulta = a.get() resultb = b.get() 2)Again, pycuda is not a c++ interface.If you need the results to go from cuda-&gt;python-&gt;c++, I don't think you need python in the middle. 3)Yes block(4,4,1) is 16 threads and grid(16,16) is 256 blocks. Edit: To answer some of your comments: Yes a block(4,1,1) is one dimensional and block (4,4,1) is 2D. I fixed your code, you just had to pass N to the CUDA kernel. import pycuda.driver as cuda import pycuda.autoinit from pycuda.compiler import SourceModule import scipy as sc N=50*1024 a=sc.arange(0,N).astype(sc.float32) a_gpu = cuda.mem_alloc(a.nbytes) #allocate memory on GPU cuda.memcpy_htod(a_gpu, a) #transfer data to the GPU b=sc.array([i**2 for i in range(0,N)]).astype(sc.float32) b_gpu = cuda.mem_alloc(b.nbytes)#allocate memory on GPU cuda.memcpy_htod(b_gpu, b) #transfer data to the GPU c=sc.zeros(N).astype(sc.float32) c_gpu = cuda.mem_alloc(c.nbytes)#allocate memory on GPU mod = SourceModule(\"\"\" __global__ void add(int*a,int *b,int *c, int N){ int tid=threadIdx.x + blockIdx.x*gridDim.x; while (tid&lt;N){ c[tid]=a[tid]+b[tid]; tid+=blockDim.x*gridDim.x; } } \"\"\") #call the function(kernel) func = mod.get_function(\"add\") func(a_gpu,b_gpu,c_gpu, sc.int32(N), block=(16,16,1),grid=(16,16)) #transfer data back to CPU cuda.memcpy_dtoh(c, c_gpu) print c Another way of doing this is to use string substitution on the SourceModule: mod = SourceModule(\"\"\" __global__ void add(int*a,int *b,int *c){ const int N = %d; int tid=threadIdx.x + blockIdx.x*gridDim.x; while (tid&lt;N){ c[tid]=a[tid]+b[tid]; tid+=blockDim.x*gridDim.x; } } \"\"\" % (N)) One last note is that, when you are using Pycuda, it generally works as the glue that connects all the different pieces of working with CUDA together. It helps you compile allocate memory, run your kernel etc... As long as you are using it like this you will be fine."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I'm writing a kernel using PyCUDA. My GPU device only supports compute capability 1.1 (arch sm_11) and so I can only use floats in my code. I've taken great effort to ensure I'm doing everything with floats, but despite that, there is a particular line in my code that keeps causing a compiler error. The chunk of code is: // Gradient magnitude, so 1 &lt;= x &lt;= width, 1 &lt;= y &lt;= height. if( j &gt; 0 &amp;&amp; j &lt; im_width &amp;&amp; i &gt; 0 &amp;&amp; i &lt; im_height){ gradient_mag[idx(i,j)] = float(sqrt(x_gradient[idx(i,j)]*x_gradient[idx(i,j)] + y_gradient[idx(i,j)]*y_gradient[idx(i,j)])); } Here, idx() is a __device__ helper function that returns a linear index based on pixel indices i and j, and it only works with integers. I use it throughout and it doesn't give errors anywhere else, so I strongly suspect it's not idx(). The sqrt() call is just from the standard C math functions which support floats. All of the arrays involved, x_gradient , y_gradient, and gradient_mag are all float* and they are part of the input to my function (i.e. declared in Python, then converted to device variables, etc.). I've tried removing the extra cast to float in my code above, with no luck. I've also tried doing something completely stupid like this: // Gradient magnitude, so 1 &lt;= x &lt;= width, 1 &lt;= y &lt;= height. if( j &gt; 0 &amp;&amp; j &lt; im_width &amp;&amp; i &gt; 0 &amp;&amp; i &lt; im_height){ gradient_mag[idx(i,j)] = 3.0f; // also tried float(3.0) here } All of these variations give the same error: pycuda.driver.CompileError: nvcc said it demoted types in source code it compiled--this is likely not what you want. [command: nvcc --cubin -arch sm_11 -I/usr/local/lib/python2.7/dist-packages/pycuda-2011.1.2-py2.7-linux-x86_64.egg/pycuda/../include/pycuda kernel.cu] [stderr: ptxas /tmp/tmpxft_00004329_00000000-2_kernel.ptx, line 128; warning : Double is not supported. Demoting to float ] Any ideas? I've debugged many errors in my code and was hoping to get it working tonight, but this has proved to be a bug that I cannot understand. Added -- Here is a truncated version of the kernel that produces the same error above on my machine. every_pixel_hog_kernel_source = \\ \"\"\" #include &lt;math.h&gt; #include &lt;stdio.h&gt; __device__ int idx(int ii, int jj){ return gridDim.x*blockDim.x*ii+jj; } __device__ int bin_number(float angle_val, int total_angles, int num_bins){ float angle1; float min_dist; float this_dist; int bin_indx; angle1 = 0.0; min_dist = abs(angle_val - angle1); bin_indx = 0; for(int kk=1; kk &lt; num_bins; kk++){ angle1 = angle1 + float(total_angles)/float(num_bins); this_dist = abs(angle_val - angle1); if(this_dist &lt; min_dist){ min_dist = this_dist; bin_indx = kk; } } return bin_indx; } __device__ int hist_number(int ii, int jj){ int hist_num = 0; if(jj &gt;= 0 &amp;&amp; jj &lt; 11){ if(ii &gt;= 0 &amp;&amp; ii &lt; 11){ hist_num = 0; } else if(ii &gt;= 11 &amp;&amp; ii &lt; 22){ hist_num = 3; } else if(ii &gt;= 22 &amp;&amp; ii &lt; 33){ hist_num = 6; } } else if(jj &gt;= 11 &amp;&amp; jj &lt; 22){ if(ii &gt;= 0 &amp;&amp; ii &lt; 11){ hist_num = 1; } else if(ii &gt;= 11 &amp;&amp; ii &lt; 22){ hist_num = 4; } else if(ii &gt;= 22 &amp;&amp; ii &lt; 33){ hist_num = 7; } } else if(jj &gt;= 22 &amp;&amp; jj &lt; 33){ if(ii &gt;= 0 &amp;&amp; ii &lt; 11){ hist_num = 2; } else if(ii &gt;= 11 &amp;&amp; ii &lt; 22){ hist_num = 5; } else if(ii &gt;= 22 &amp;&amp; ii &lt; 33){ hist_num = 8; } } return hist_num; } __global__ void every_pixel_hog_kernel(float* input_image, int im_width, int im_height, float* gaussian_array, float* x_gradient, float* y_gradient, float* gradient_mag, float* angles, float* output_array) { ///// // Setup the thread indices and linear offset. ///// int i = blockDim.y * blockIdx.y + threadIdx.y; int j = blockDim.x * blockIdx.x + threadIdx.x; int ang_limit = 180; int ang_bins = 9; float pi_val = 3.141592653589f; //91 ///// // Compute a Gaussian smoothing of the current pixel and save it into a new image array // Use sync threads to make sure everyone does the Gaussian smoothing before moving on. ///// if( j &gt; 1 &amp;&amp; i &gt; 1 &amp;&amp; j &lt; im_width-2 &amp;&amp; i &lt; im_height-2 ){ // Hard-coded unit standard deviation 5-by-5 Gaussian smoothing filter. gaussian_array[idx(i,j)] = float(1.0/273.0) *( input_image[idx(i-2,j-2)] + float(4.0)*input_image[idx(i-2,j-1)] + float(7.0)*input_image[idx(i-2,j)] + float(4.0)*input_image[idx(i-2,j+1)] + input_image[idx(i-2,j+2)] + float(4.0)*input_image[idx(i-1,j-2)] + float(16.0)*input_image[idx(i-1,j-1)] + float(26.0)*input_image[idx(i-1,j)] + float(16.0)*input_image[idx(i-1,j+1)] + float(4.0)*input_image[idx(i-1,j+2)] + float(7.0)*input_image[idx(i,j-2)] + float(26.0)*input_image[idx(i,j-1)] + float(41.0)*input_image[idx(i,j)] + float(26.0)*input_image[idx(i,j+1)] + float(7.0)*input_image[idx(i,j+2)] + float(4.0)*input_image[idx(i+1,j-2)] + float(16.0)*input_image[idx(i+1,j-1)] + float(26.0)*input_image[idx(i+1,j)] + float(16.0)*input_image[idx(i+1,j+1)] + float(4.0)*input_image[idx(i+1,j+2)] + input_image[idx(i+2,j-2)] + float(4.0)*input_image[idx(i+2,j-1)] + float(7.0)*input_image[idx(i+2,j)] + float(4.0)*input_image[idx(i+2,j+1)] + input_image[idx(i+2,j+2)]); } __syncthreads(); ///// // Compute the simple x and y gradients of the image and store these into new images // again using syncthreads before moving on. ///// // X-gradient, ensure x is between 1 and width-1 if( j &gt; 0 &amp;&amp; j &lt; im_width){ x_gradient[idx(i,j)] = float(input_image[idx(i,j)] - input_image[idx(i,j-1)]); } else if(j == 0){ x_gradient[idx(i,j)] = float(0.0); } // Y-gradient, ensure y is between 1 and height-1 if( i &gt; 0 &amp;&amp; i &lt; im_height){ y_gradient[idx(i,j)] = float(input_image[idx(i,j)] - input_image[idx(i-1,j)]); } else if(i == 0){ y_gradient[idx(i,j)] = float(0.0); } __syncthreads(); // Gradient magnitude, so 1 &lt;= x &lt;= width, 1 &lt;= y &lt;= height. if( j &lt; im_width &amp;&amp; i &lt; im_height){ gradient_mag[idx(i,j)] = float(sqrt(x_gradient[idx(i,j)]*x_gradient[idx(i,j)] + y_gradient[idx(i,j)]*y_gradient[idx(i,j)])); } __syncthreads(); ///// // Compute the orientation angles ///// if( j &lt; im_width &amp;&amp; i &lt; im_height){ if(ang_limit == 360){ angles[idx(i,j)] = float((atan2(y_gradient[idx(i,j)],x_gradient[idx(i,j)])+pi_val)*float(180.0)/pi_val); } else{ angles[idx(i,j)] = float((atan( y_gradient[idx(i,j)]/x_gradient[idx(i,j)] )+(pi_val/float(2.0)))*float(180.0)/pi_val); } } __syncthreads(); // Compute the HoG using the above arrays. Do so in a 3x3 grid, with 9 angle bins for each grid. // forming an 81-vector and then write this 81 vector as a row in the large output array. int top_bound, bot_bound, left_bound, right_bound, offset; int window = 32; if(i-window/2 &gt; 0){ top_bound = i-window/2; bot_bound = top_bound + window; } else{ top_bound = 0; bot_bound = top_bound + window; } if(j-window/2 &gt; 0){ left_bound = j-window/2; right_bound = left_bound + window; } else{ left_bound = 0; right_bound = left_bound + window; } if(bot_bound - im_height &gt; 0){ offset = bot_bound - im_height; top_bound = top_bound - offset; bot_bound = bot_bound - offset; } if(right_bound - im_width &gt; 0){ offset = right_bound - im_width; right_bound = right_bound - offset; left_bound = left_bound - offset; } int counter_i = 0; int counter_j = 0; int bin_indx, hist_indx, glob_col_indx, glob_row_indx; int row_width = 81; for(int pix_i = top_bound; pix_i &lt; bot_bound; pix_i++){ for(int pix_j = left_bound; pix_j &lt; right_bound; pix_j++){ bin_indx = bin_number(angles[idx(pix_i,pix_j)], ang_limit, ang_bins); hist_indx = hist_number(counter_i,counter_j); glob_col_indx = ang_bins*hist_indx + bin_indx; glob_row_indx = idx(i,j); output_array[glob_row_indx*row_width + glob_col_indx] = float(output_array[glob_row_indx*row_width + glob_col_indx] + float(gradient_mag[idx(pix_i,pix_j)])); counter_j = counter_j + 1; } counter_i = counter_i + 1; counter_j = 0; } } \"\"\"",
        "answers": [
            [
                "Here's an unmistakable case of using doubles: gaussian_array[idx(i,j)] = float(1.0/273.0) * See the double literals being divided? But really, use float literals instead of double literals cast to floats - the casts are ugly, and I suggest they will hide bugs like this. -------Edit 1/Dec--------- Firstly, thanks @CygnusX1, constant folding would prevent that calculation - I didn't even think of it. I've tried to reproduce the environment of the error: I installed the CUDA SDK 3.2 (That @EMS has mentioned they seem to use in the lab), compiling the truncated kernel version above, and indeed nvopencc did optimize the above calculation away (thanks @CygnusX1), and indeed it didn't use doubles anywhere in the generated PTX code. Further, ptxas didn't give the error received by @EMS. From that, I thought the problem is outside of the every_pixel_hog_kernel_source code itself, perhaps in PyCUDA. However, using PyCUDA 2011.1.2 and compiling with that still does not produce a warning like in @EMS's question. I can get the error in the question, however it is by introducing a double calculation, such as removing the cast from gaussian_array[idx(i,j)] = float(1.0/273.0) * To get to the same python case, does the following produce your error: import pycuda.driver as cuda from pycuda.compiler import compile x=compile(\"\"\"put your truncated kernel code here\"\"\",options=[],arch=\"sm_11\",keep=True) It doesn't produce an error in my circumstance, so there is a possibility I simply can't replicate your result. However, I can give some advice. When using compile (or SourceModule), if you use keep=True, python will print out the folder where the ptx file is being generated just before showing the error message. Then, if you can examine the ptx file generated in that folder and looking where .f64 appears it should give some idea of what is being treated as a double - however, deciphering what code that is in your original kernel is difficult - having the simplest example that produces your error will help you."
            ],
            [
                "Your problem is here: angle1 = 0.0; 0.0 is a double precision constant. 0.0f is a single precision constant."
            ],
            [
                "(a comment, not an answer, but it is too big to put it as a comment) Could you provide the PTX code around the line where the error occurs? I tried compiling a simple kernel using the code you provided: __constant__ int im_width; __constant__ int im_height; __device__ int idx(int i,int j) { return i+j*im_width; } __global__ void kernel(float* gradient_mag, float* x_gradient, float* y_gradient) { int i = threadIdx.x; int j = threadIdx.y; // Gradient magnitude, so 1 &lt;= x &lt;= width, 1 &lt;= y &lt;= height. if( j &gt; 0 &amp;&amp; j &lt; im_width &amp;&amp; i &gt; 0 &amp;&amp; i &lt; im_height){ gradient_mag[idx(i,j)] = float(sqrt(x_gradient[idx(i,j)]*x_gradient[idx(i,j)] + y_gradient[idx(i,j)]*y_gradient[idx(i,j)])); } } using: nvcc.exe -m32 -maxrregcount=32 -gencode=arch=compute_11,code=\\\"sm_11,compute_11\\\" --compile -o \"Debug\\main.cu.obj\" main.cu got no errors. Using the CUDA 4.1 beta compiler Update I tried compiling your new code (I am working within CUDA/C++, not PyCUDA, but this shouldn't matter). Didn't catch the error either! Used CUDA 4.1 and CUDA 4.0. What is your version of CUDA installation? C:\\&gt;nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2011 NVIDIA Corporation Built on Wed_Oct_19_23:13:02_PDT_2011 Cuda compilation tools, release 4.1, V0.2.1221"
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I'm new to CUDA programming and I was wondering how the performance of pyCUDA is compared to programs implemented in plain C. Will the performance be roughly the same? Are there any bottle necks that I should be aware of? EDIT: I obviously tried to google this issue first, and was surprised to not find any information. i.e. I would have excepted that the pyCUDA people have this question answered in their FAQ.",
        "answers": [
            [
                "If you're using CUDA -- whether directly through C or with pyCUDA -- all the heavy numerical work you're doing is done in kernels that execute on the gpu and are written in CUDA C (directly by you, or indirectly with elementwise kernels). So there should be no real difference in performance in those parts of your code. Now, the initialization of arrays, and any post-work analysis, will be done in python (probably with numpy) if you use pyCUDA, and that generally will be significantly slower than doing it directly in a compiled language (though if you've built your numpy/scipy in such a way that it links directly to high-performance libraries, then those calls at least would perform the same in either language). But hopefully, your initialization and finalization are small fractions of the total amount of work you have to do, so that even if there is significant overhead there, it still hopefully won't have a huge impact on overall runtime. And in fact if it turns out that the python parts of the computation does hurt your application's performance, starting out doing your development in pyCUDA may still be an excellent way to get started, as the development is significantly easier, and you can always re-implement those parts of the code that are too slow in Python in straight C, and call those from python, gaining some of the best of both worlds."
            ],
            [
                "If you're wondering about performance differences by using pyCUDA in different ways, see SimpleSpeedTest.py included in the pyCUDA Wiki examples. It benchmarks the same task completed by a CUDA C kernel encapsulated in pyCUDA, and by several abstractions created by pyCUDA's designer. There's a performance difference."
            ],
            [
                "I've been using pyCUDA for a little while an I like prototyping with it because it speeds up the process of turning an idea into working code. With pyCUDA you will be writing the CUDA kernels using C++, and it's CUDA, so there shouldn't be a difference in performance of running that code. But there will be a difference in the performance of the code you write in Python to setup or use the results of the pyCUDA kernel vs the one you write in C."
            ],
            [
                "I was looking for an answer for the original question in this post and I see the problem Is deeper as I thought. I my experience, I compared Cuda kernels and CUFFT's written in C with that written in PyCuda. Surprisingly, I found that, on my computer, the performance of suming, multiplying or making FFT's vary from each implentatiom. For example, I got almost the same performance in cuFFT for vector sizes until 2^23 elements. However, suming and multiplying complex vectors show some troubles. The speed up obtained in C/Cuda was ~6X for N=2^17, whilst in PyCuda only ~3X. It also depends on the way that the sumation was performed. By using SourceModule and wrapping the Raw Cuda code, I found the problem that my kernel, for complex128 vectors, was limitated for a lower N (&lt;=2^16) than that used for gpuarray (&lt;=2^24). As a conclusion, is a good job testing and comparing the two sides of the problem and evaluate if it is convenient spend time in writing a Cuda script or gain readbility and pay the cost of a lower performance."
            ],
            [
                "Make sure you're using -O3 optimizations there and use nvprof/nvvp to profile your kernels if you're using PyCUDA and you want to get high performance. If you want to use Cuda from Python, PyCUDA is probably THE choice. Because interfacing C++/Cuda code via Python is just hell otherwise. You have to write a hell lot of ugly wrappers. And for numpy integration even more hardcore wrap-up code would be necessary."
            ]
        ],
        "votes": [
            19.0000001,
            6.0000001,
            4.0000001,
            2.0000001,
            1.0000001
        ]
    },
    {
        "question": "The PyCUDA help explains how to create an empty or zeroed array but not how to move(?) an existing numpy array into page-locked memory. Do I need to get a pointer for the numpy array and pass it to pycuda.driver.PagelockedHostAllocation? And how would I do that? UPDATE &lt;--sniped --&gt; UPDATE 2 Thanks talonmies for you help. Now the memory transfare is page-locked but the program ends with the following error: PyCUDA WARNING: a clean-up operation failed (dead context maybe?) cuMemFreeHost failed: invalid context This is the updated code: #!/usr/bin/env python # -*- coding: utf-8 -*- import numpy as np import ctypes from pycuda import driver, compiler, gpuarray from pycuda.tools import PageLockedMemoryPool import pycuda.autoinit memorypool = PageLockedMemoryPool() indata = np.random.randn(5).astype(np.float32) outdata = gpuarray.zeros(5, dtype=np.float32) pinnedinput = memorypool.allocate(indata.shape,np.float32) source = indata.ctypes.data_as(ctypes.POINTER(ctypes.c_float)) dest = pinnedinput.ctypes.data_as(ctypes.POINTER(ctypes.c_float)) sz = indata.size * ctypes.sizeof(ctypes.c_float) ctypes.memmove(dest,source,sz) kernel_code = \"\"\" __global__ void kernel(float *indata, float *outdata) { int globalid = blockIdx.x * blockDim.x + threadIdx.x ; outdata[globalid] = indata[globalid]+1.0f; } \"\"\" mod = compiler.SourceModule(kernel_code) kernel = mod.get_function(\"kernel\") kernel( driver.In(pinnedinput), outdata, grid = (5,1), block = (1, 1, 1), ) print indata print outdata.get() memorypool.free_held()",
        "answers": [
            [
                "You will need to copy the data from your source array to the array holding the page locked allocation returned from pycuda. The most straightforward way to do that is via ctypes: import numpy import ctypes x=numpy.array([1,2,3,4],dtype=numpy.double) y=numpy.zeros_like(x) source = x.ctypes.data_as(ctypes.POINTER(ctypes.c_double)) dest = y.ctypes.data_as(ctypes.POINTER(ctypes.c_double)) sz = x.size * ctypes.sizeof(ctypes.c_double) ctypes.memmove(dest,source,sz) print y The numpy.ctypes interface can be used to get a pointer to the memory used to hold an arrays data, and then the ctypes.memmove used to copy between two different ndarrays. All the usual caveats of working with naked C pointers apply, so some care is required, but it is straightforward enough to use."
            ],
            [
                "The memory block is still active. You might explicitly free the pinned array: print memorypool.active_blocks pinnedinput.base.free() print memorypool.active_blocks memorypool.free_held()"
            ],
            [
                "I've been doing this in a much simpler way: locked_ary = cuda.pagelocked_empty_like(ary, mem_flags=cuda.host_alloc_flags.DEVICEMAP) locked_ary[:] = ary The result has the right AlignedHostAllocation base, and timings are identical to what I get by using ctypes.memmove."
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "When I increase the unrolling from 8 to 9 loops in my kernel, it breaks with an out of resources error. I read in How do I diagnose a CUDA launch failure due to being out of resources? that a mismatch of parameters and an overuse of registers could be a problem, but that seems not be the case here. My kernel calculates the distance between n points and m centroids and selects for each point the closest centroid. It works for 8 dimensions but not for 9. When I set dimensions=9 and uncomment the two lines for the distance calculation, I get an pycuda._driver.LaunchError: cuLaunchGrid failed: launch out of resources. What do you think, could cause this behavior? What other iusses can cause an out of resources*? I use an Quadro FX580. Here is the minimal(ish) example. For the unrolling in the real code I use templates. import numpy as np from pycuda import driver, compiler, gpuarray, tools import pycuda.autoinit ## preference np.random.seed(20) points = 512 dimensions = 8 nclusters = 1 ## init data data = np.random.randn(points,dimensions).astype(np.float32) clusters = data[:nclusters] ## init cuda kernel_code = \"\"\" // the kernel definition __device__ __constant__ float centroids[16384]; __global__ void kmeans_kernel(float *idata,float *g_centroids, int * cluster, float *min_dist, int numClusters, int numDim) { int valindex = blockIdx.x * blockDim.x + threadIdx.x ; float increased_distance,distance, minDistance; minDistance = 10000000 ; int nearestCentroid = 0; for(int k=0;k&lt;numClusters;k++){ distance = 0.0; increased_distance = idata[valindex*numDim] -centroids[k*numDim]; distance = distance +(increased_distance * increased_distance); increased_distance = idata[valindex*numDim+1] -centroids[k*numDim+1]; distance = distance +(increased_distance * increased_distance); increased_distance = idata[valindex*numDim+2] -centroids[k*numDim+2]; distance = distance +(increased_distance * increased_distance); increased_distance = idata[valindex*numDim+3] -centroids[k*numDim+3]; distance = distance +(increased_distance * increased_distance); increased_distance = idata[valindex*numDim+4] -centroids[k*numDim+4]; distance = distance +(increased_distance * increased_distance); increased_distance = idata[valindex*numDim+5] -centroids[k*numDim+5]; distance = distance +(increased_distance * increased_distance); increased_distance = idata[valindex*numDim+6] -centroids[k*numDim+6]; distance = distance +(increased_distance * increased_distance); increased_distance = idata[valindex*numDim+7] -centroids[k*numDim+7]; distance = distance +(increased_distance * increased_distance); //increased_distance = idata[valindex*numDim+8] -centroids[k*numDim+8]; //distance = distance +(increased_distance * increased_distance); if(distance &lt;minDistance) { minDistance = distance ; nearestCentroid = k; } } cluster[valindex]=nearestCentroid; min_dist[valindex]=sqrt(minDistance); } \"\"\" mod = compiler.SourceModule(kernel_code) centroids_adrs = mod.get_global('centroids')[0] kmeans_kernel = mod.get_function(\"kmeans_kernel\") clusters_gpu = gpuarray.to_gpu(clusters) cluster = gpuarray.zeros(points, dtype=np.int32) min_dist = gpuarray.zeros(points, dtype=np.float32) driver.memcpy_htod(centroids_adrs,clusters) distortion = gpuarray.zeros(points, dtype=np.float32) block_size= 512 ## start kernel kmeans_kernel( driver.In(data),driver.In(clusters),cluster,min_dist, np.int32(nclusters),np.int32(dimensions), grid = (points/block_size,1), block = (block_size, 1, 1), ) print cluster print min_dist",
        "answers": [
            [
                "You're running out of registers because your block_size (512) is too large. ptxas reports that your kernel uses 16 registers with the commented lines: $ nvcc test.cu -Xptxas --verbose ptxas info : Compiling entry function '_Z13kmeans_kernelPfS_PiS_ii' for 'sm_10' ptxas info : Used 16 registers, 24+16 bytes smem, 65536 bytes cmem[0] Uncommenting the lines increases register use to 17 and an error at runtime: $ nvcc test.cu -run -Xptxas --verbose ptxas info : Compiling entry function '_Z13kmeans_kernelPfS_PiS_ii' for 'sm_10' ptxas info : Used 17 registers, 24+16 bytes smem, 65536 bytes cmem[0] error: too many resources requested for launch The number of physical registers used by each thread of a kernel limits the size of blocks you can launch at runtime. An SM 1.0 device has 8K registers that can be used by a block of threads. We can compare that to your kernel's register demands: 17 * 512 = 8704 &gt; 8K. At 16 registers, your original commented kernel just squeaks by: 16 * 512 = 8192 == 8K. When no architecture is specified, nvcc compiles kernels for an SM 1.0 device by default. PyCUDA may work the same way. To fix your problem, you could either decrease block_size (to say, 256) or find a way to configure PyCUDA to compile your kernel for an SM 2.0 device. SM 2.0 devices such as your QuadroFX 580 provide 32K registers, more than enough for your original block_size of 512."
            ]
        ],
        "votes": [
            8.0000001
        ]
    },
    {
        "question": "Why is this matrix transpose kernel faster, when the shared memory array is padded by one column? I found the kernel at PyCuda/Examples/MatrixTranspose. Source import pycuda.gpuarray as gpuarray import pycuda.autoinit from pycuda.compiler import SourceModule import numpy block_size = 16 def _get_transpose_kernel(offset): mod = SourceModule(\"\"\" #define BLOCK_SIZE %(block_size)d #define A_BLOCK_STRIDE (BLOCK_SIZE * a_width) #define A_T_BLOCK_STRIDE (BLOCK_SIZE * a_height) __global__ void transpose(float *A_t, float *A, int a_width, int a_height) { // Base indices in A and A_t int base_idx_a = blockIdx.x * BLOCK_SIZE + blockIdx.y * A_BLOCK_STRIDE; int base_idx_a_t = blockIdx.y * BLOCK_SIZE + blockIdx.x * A_T_BLOCK_STRIDE; // Global indices in A and A_t int glob_idx_a = base_idx_a + threadIdx.x + a_width * threadIdx.y; int glob_idx_a_t = base_idx_a_t + threadIdx.x + a_height * threadIdx.y; /** why does the +1 offset make the kernel faster? **/ __shared__ float A_shared[BLOCK_SIZE][BLOCK_SIZE+%(offset)d]; // Store transposed submatrix to shared memory A_shared[threadIdx.y][threadIdx.x] = A[glob_idx_a]; __syncthreads(); // Write transposed submatrix to global memory A_t[glob_idx_a_t] = A_shared[threadIdx.x][threadIdx.y]; } \"\"\"% {\"block_size\": block_size, \"offset\": offset}) kernel = mod.get_function(\"transpose\") kernel.prepare(\"PPii\", block=(block_size, block_size, 1)) return kernel def transpose(tgt, src,offset): krnl = _get_transpose_kernel(offset) w, h = src.shape assert tgt.shape == (h, w) assert w % block_size == 0 assert h % block_size == 0 krnl.prepared_call((w / block_size, h /block_size), tgt.gpudata, src.gpudata, w, h) def run_benchmark(): from pycuda.curandom import rand print pycuda.autoinit.device.name() print \"time\\tGB/s\\tsize\\toffset\\t\" for offset in [0,1]: for size in [2048,2112]: source = rand((size, size), dtype=numpy.float32) target = gpuarray.empty((size, size), dtype=source.dtype) start = pycuda.driver.Event() stop = pycuda.driver.Event() warmup = 2 for i in range(warmup): transpose(target, source,offset) pycuda.driver.Context.synchronize() start.record() count = 10 for i in range(count): transpose(target, source,offset) stop.record() stop.synchronize() elapsed_seconds = stop.time_since(start)*1e-3 mem_bw = source.nbytes / elapsed_seconds * 2 * count /1024/1024/1024 print \"%6.4fs\\t%6.4f\\t%i\\t%i\" %(elapsed_seconds,mem_bw,size,offset) run_benchmark() Output Quadro FX 580 time GB/s size offset 0.0802s 3.8949 2048 0 0.0829s 4.0105 2112 0 0.0651s 4.7984 2048 1 0.0595s 5.5816 2112 1 Code adopted",
        "answers": [
            [
                "The answer is shared memory bank conflicts. The CUDA hardware you are using arranges shared memory into 16 banks, and shared memory is sequentially \"striped\" across all of those 16 banks. If two threads try and access the same bank simultaneously, a conflict occurs and the threads must be serialized. This is what you are seeing here. By extending the stride of the shared memory array by 1, you are ensuring that the same column indices in successive rows of the shared array are on different banks, which eliminates most of the possible conflicts. This phenomena (and an associated global memory phenomena called partition camping) is discussed in great depth in the \"Optimizing Matrix Transpose in CUDA\" paper which ships with the SDK matrix transpose example. It is well worth reading."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I have to write in a PyCUDA function that gets two matrices Nx3 and Mx3, and return a matrix NxM, but I can't figure out how to pass by reference a matrix without knowing the number of columns. My code basically is something like that: #kernel declaration mod = SourceModule(\"\"\" __global__ void distance(int N, int M, float d1[][3], float d2[][3], float res[][M]) { int i = threadIdx.x; int j = threadIdx.y; float x, y, z; x = d2[j][0]-d1[i][0]; y = d2[j][1]-d1[i][1]; z = d2[j][2]-d1[i][2]; res[i][j] = x*x + y*y + z*z; } \"\"\") #load data data1 = numpy.loadtxt(\"data1.txt\").astype(numpy.float32) # Nx3 matrix data2 = numpy.loadtxt(\"data2.txt\").astype(numpy.float32) # Mx3 matrix N=data1.shape[0] M=data2.shape[0] res = numpy.zeros([N,M]).astype(numpy.float32) # NxM matrix #invoke kernel dist_gpu = mod.get_function(\"distance\") dist_gpu(cuda.In(numpy.int32(N)), cuda.In(numpy.int32(M)), cuda.In(data1), cuda.In(data2), cuda.Out(res), block=(N,M,1)) #save data numpy.savetxt(\"results.txt\", res) Compiling this I receive an error: kernel.cu(3): error: a parameter is not allowed that is, I cannot use M as the number of columns for res[][] in the declaretion of the function. I cannot either left the number of columns undeclared... I need a matrix NxM as an output, but I can't figure out how to do this. Can you help me?",
        "answers": [
            [
                "You should use pitched linear memory access inside the kernel, that is how ndarray and gpuarray store data internally, and PyCUDA will pass a pointer to the data in gpu memory allocated for a gpuarray when it is supplied as a argument to a PyCUDA kernel. So (if I understand what you are trying to do) your kernel should be written as something like: __device__ unsigned int idx2d(int i, int j, int lda) { return j + i*lda; } __global__ void distance(int N, int M, float *d1, float *d2, float *res) { int i = threadIdx.x + blockDim.x * blockIdx.x; int j = threadIdx.y + blockDim.y * blockIdx.y; float x, y, z; x = d2[idx2d(j,0,3)]-d1[idx2d(i,0,3)]; y = d2[idx2d(j,1,3)]-d1[idx2d(i,1,3)]; z = d2[idx2d(j,2,3)]-d1[idx2d(i,2,3)]; res[idx2d(i,j,N)] = x*x + y*y + z*z; } Here I have assumed the numpy default row major ordering in defining the idx2d helper function. There are still problems with the Python side of the code you posted, but I guess you know that already. EDIT: Here is a complete working repro case based of the code posted in your question. Note that it only uses a single block (like the original), so be mindful of block and grid dimensions when trying to run it on anything other than trivially small cases. import numpy as np from pycuda import compiler, driver from pycuda import autoinit #kernel declaration mod = compiler.SourceModule(\"\"\" __device__ unsigned int idx2d(int i, int j, int lda) { return j + i*lda; } __global__ void distance(int N, int M, float *d1, float *d2, float *res) { int i = threadIdx.x + blockDim.x * blockIdx.x; int j = threadIdx.y + blockDim.y * blockIdx.y; float x, y, z; x = d2[idx2d(j,0,3)]-d1[idx2d(i,0,3)]; y = d2[idx2d(j,1,3)]-d1[idx2d(i,1,3)]; z = d2[idx2d(j,2,3)]-d1[idx2d(i,2,3)]; res[idx2d(i,j,N)] = x*x + y*y + z*z; } \"\"\") #make data data1 = np.random.uniform(size=18).astype(np.float32).reshape(-1,3) data2 = np.random.uniform(size=12).astype(np.float32).reshape(-1,3) N=data1.shape[0] M=data2.shape[0] res = np.zeros([N,M]).astype(np.float32) # NxM matrix #invoke kernel dist_gpu = mod.get_function(\"distance\") dist_gpu(np.int32(N), np.int32(M), driver.In(data1), driver.In(data2), \\ driver.Out(res), block=(N,M,1), grid=(1,1)) print res"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "I have problems passing the right parameters to the prepare function (and to the prepared_call) to allocate of shared memory in PyCUDA. I understand the error message in this way, that one of the variables I pass to PyCUDA is a long instead of what I intended float32. But I cannot see, where the variable comes from. Furthermore does it seem to me, that the official example and the documentation of prepare contradict each other regarding if block needs to be None or not. from pycuda import driver, compiler, gpuarray, tools import pycuda.autoinit import numpy as np kernel_code =\"\"\" __device__ void loadVector(float *target, float* source, int dimensions ) { for( int i = 0; i &lt; dimensions; i++ ) target[i] = source[i]; } __global__ void kernel(float* data, int dimensions, float* debug) { extern __shared__ float mean[]; if(threadIdx.x == 0) loadVector( mean, &amp;data[0], dimensions ); debug[threadIdx.x]= mean[threadIdx.x]; } \"\"\" dimensions = 12 np.random.seed(23) data = np.random.randn(dimensions).astype(np.float32) data_gpu = gpuarray.to_gpu(data) debug = gpuarray.zeros(dimensions, dtype=np.float32) mod = compiler.SourceModule(kernel_code) kernel = mod.get_function(\"kernel\") kernel.prepare(\"PiP\",block = (dimensions, 1, 1),shared=data.size) grid = (1,1) kernel.prepared_call(grid,data_gpu,dimensions,debug) print debug.get() Output Traceback (most recent call last): File \"shared_memory_minimal_example.py\", line 28, in &lt;module&gt; kernel.prepared_call(grid,data_gpu,dimensions,debug) File \"/usr/local/lib/python2.6/dist-packages/pycuda-0.94.2-py2.6-linux-x86_64.egg/pycuda/driver.py\", line 230, in function_prepared_call func.param_setv(0, pack(func.arg_format, *args)) pycuda._pvt_struct.error: cannot convert argument to long",
        "answers": [
            [
                "I came across this same problem and it took my a while to work out the answer so here goes. The cause of the error message is that data_gpu is a GPUArray instance, i.e. you made it with data_gpu = gpuarray.to_gpu(data) To pass it to prepared_call you need to do data_gpu.gpudata to get the associated DeviceAllocation instance (i.e. effectively the pointer to the device memory location). Also, passing a block argument to prepare is now deprecated - so a correct invocation would be something like this: data_gpu = gpuarray.to_gpu(data) func.prepare( \"P\" ) grid = (1,1) block = (1,1,1) func.prepared_call( grid, block, data_gpu.gpudata )"
            ]
        ],
        "votes": [
            6.0000001
        ]
    },
    {
        "question": "Why does the transposed matrix look differently, when converted to a pycuda.gpuarray? Can you reproduce this? What could cause this? Am I using the wrong approach? Example code from pycuda import gpuarray import pycuda.autoinit import numpy data = numpy.random.randn(2,4).astype(numpy.float32) data_gpu = gpuarray.to_gpu(data.T) print \"data\\n\",data print \"data_gpu.get()\\n\",data_gpu.get() print \"data.T\\n\",data.T Output data [[ 0.70442784 0.08845157 -0.84840715 -1.81618035] [ 0.55292499 0.54911566 0.54672164 0.05098847]] data_gpu.get() [[ 0.70442784 0.08845157] [-0.84840715 -1.81618035] [ 0.55292499 0.54911566] [ 0.54672164 0.05098847]] data.T [[ 0.70442784 0.55292499] [ 0.08845157 0.54911566] [-0.84840715 0.54672164] [-1.81618035 0.05098847]]",
        "answers": [
            [
                "The basic reason is that numpy transpose only creates a view, which has no effect on the underlying array storage, and it is that storage which PyCUDA directly accesses when a copy is performed to device memory. The solution is to use the copy method when doing the transpose, which will create an array with data in the transposed order in host memory, then copy that to the device: data_gpu = gpuarray.to_gpu(data.T.copy())"
            ],
            [
                "In numpy, data.T doesn't do anything to the underlying 1D array. It simply manipulates the strides to obtain the transpose. This makes it a constant-time and constant-memory operation. It would appear that pycuda.to_gpu() isn't respecting the strides and is simply copying the underlying 1D array. This would produce the exact behaviour you're observing. In my view there is nothing wrong with your code. Rather, I would consider this a bug in pycuda. I've googled around, and have found a thread that discusses this issue in detail. As a workaround, you could try passing numpy.ascontiguousarray(data.T) to gpuarray.to_gpu(). This will, of course, create a second copy of the data in the host RAM."
            ]
        ],
        "votes": [
            6.0000001,
            6.0000001
        ]
    },
    {
        "question": "I'm getting an out-of-resources error when trying to launch a CUDA kernel (through PyCUDA), and I'm wondering if it's possible to get the system to tell me which resource it is that I'm short on. Obviously the system knows what resource has been exhausted, I just want to query that as well. I've used the occupancy calculator, and everything seems okay, so either there's a corner case not covered, or I'm using it wrong. I know it's not registers (which seems to be the usual culprit) because I'm using &lt;= 63 and it still fails with a 1x1x1 block and 1x1 grid on a CC 2.1 device. Thanks for any help. I posted a thread on the NVidia boards: http://forums.nvidia.com/index.php?showtopic=206261&amp;st=0 But got no responses. If the answer is \"you can't ask the system for that information\" that would be nice to know too (sort of... ;). Edit: The most register usage I've seen has been 63. Edited the above to reflect that.",
        "answers": [
            [
                "I think PyCUDA uses the CUDA driver API, so the following may be what is wrong: CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES can happen if you do not specify enough arguments, or you specify the wrong size for arguments, when using cuLaunch() to launch kernels. Since you are using PyCUDA, it could be pretty easy to mismatch the argument list required for a kernel and the arguments you are actually passing, so you might want to check how you are calling your kernels. I think that this is a poorly named error code in this situation..."
            ],
            [
                "See this answer CUDA maximum registers per thread: sm_12 vs sm_20 It seems 70 registers is too many registers."
            ]
        ],
        "votes": [
            6.0000001,
            1e-07
        ]
    },
    {
        "question": "When I create a new session and tell the Visual Profiler to launch my python/pycuda scripts I get following error message: Execution run #1 of program '' failed, exit code: 255 These are my preferences: Launch: python \"/pathtopycudafile/mysuperkernel.py\" Working Directory: \"/pathtopycudafile/mysuperkernel.py\" Arguments: [empty] I use CUDA 4.0 under Ubuntu 10.10. 64Bit. Profiling compiled examples works. p.s. I am aware of SO question How to profile PyCuda code in Linux?, but seems to be an unrelated problem. Minimal example pycudaexample.py: import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1)) pycuda.autoinit.context.detach() Example settings Error message",
        "answers": [
            [
                "There is something wrong with the way you are specifying the executable to the compute profiler. If I put a hash bang line at the top of your posted code: #!/usr/bin/env python and then give the python file executable permissions, the compute profiler runs the code without complaint and I get this:"
            ],
            [
                "There are two methods that you can use. Launch the Script Interpreter Launch python Arguments \"/pathtopycudafile/mysuperkernel.py\" Launch a Executable Script Launch \"/pathtopycudafile/mysuperkernel.py\" Arguments [blank] mysuperkernel.py must be executable (chmod +x) mysuperkenrel.py must have a #! to specify the path to the interpreter See @talonmies answer"
            ]
        ],
        "votes": [
            4.0000001,
            1.0000001
        ]
    },
    {
        "question": "I'm looking into speeding up my python code, which is all matrix math, using some form of CUDA. Currently my code is using Python and Numpy, so it seems like it shouldn't be too difficult to rewrite it using something like either PyCUDA or CudaMat. However, on my first attempt using CudaMat, I realized I had to rearrange a lot of the equations in order to keep the operations all on the GPU. This included the creation of many temporary variables so I could store the results of the operations. I understand why this is necessary, but it makes what were once easy to read equations into somewhat of a mess that difficult to inspect for correctness. Additionally, I would like to be able to easily modify the equations later on, which isn't in their converted form. The package Theano manages to do this by first creating a symbolic representation of the operations, then compiling them to CUDA. However, after trying Theano out for a bit, I was frustrated by how opaque everything was. For example, just getting the actual value for myvar.shape[0] is made difficult since the tree doesn't get evaluated until much later. I would also much prefer less of a framework in which my code much conform to a library that acts invisibly in the place of Numpy. Thus, what I would really like is something much simpler. I don't want automatic differentiation (there are other packages like OpenOpt that can do that if I require it), or optimization of the tree, but just a conversion from standard Numpy notation to CudaMat/PyCUDA/somethingCUDA. In fact, I want to be able to have it evaluate to just Numpy without any CUDA code for testing. I'm currently considering writing this myself, but before even consider such a venture, I wanted to see if anyone else knows of similar projects or a good starting place. The only other project I know that might be close to this is SymPy, but I don't know how easy it would be to adapt to this purpose. My current idea would be to create an array class that looked like a Numpy.array class. It's only function would be to build a tree. At any time, that symbolic array class could be converted to a Numpy array class and be evaluated (there would also be a one-to-one parity). Alternatively, the array class could be traversed and have CudaMat commands be generated. If optimizations are required they can be done at that stage (e.g. re-ordering of operations, creation of temporary variables, etc.) without getting in the way of inspecting what's going on. Any thoughts/comments/etc. on this would be greatly appreciated! Update A usage case may look something like (where sym is the theoretical module), where we might be doing something such as calculating the gradient: W = sym.array(np.rand(size=(numVisible, numHidden))) delta_o = -(x - z) delta_h = sym.dot(delta_o, W)*h*(1.0-h) grad_W = sym.dot(X.T, delta_h) In this case, grad_W would actually just be a tree containing the operations that needed to be done. If you wanted to evaluate the expression normally (i.e. via Numpy) you could do: npGrad_W = grad_W.asNumpy() which would just execute the Numpy commands that the tree represents. If on the other hand, you wanted to use CUDA, you would do: cudaGrad_W = grad_W.asCUDA() which would convert the tree into expressions that can executed via CUDA (this could happen in a couple of different ways). That way it should be trivial to: (1) test grad_W.asNumpy() == grad_W.asCUDA(), and (2) convert your pre-existing code to use CUDA.",
        "answers": [
            [
                "Have you looked at the GPUArray portion of PyCUDA? http://documen.tician.de/pycuda/array.html While I haven't used it myself, it seems like it would be what you're looking for. In particular, check out the \"Single-pass Custom Expression Evaluation\" section near the bottom of that page."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "Closed. This question needs debugging details. It is not currently accepting answers. Edit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question. Closed 8 years ago. Improve this question I am trying to run a pycuda program across two gpus. I have read a great post by Talonmies explaining how you do it with the threading library, the post also mentioned this is possible with mpi4py. When I run mpi4py with pycuda, program gives the error: self.ctx = driver.Device(gpuid).max_context pycuda._driver.logicError: cuDeviceGet failed: not initialized Perhaps this is due to my attempt to initalize two of the gpu devices simutanously. Does anyone have a very short example of how we can get 2 gpus working with mpi4py?",
        "answers": [
            [
                "For anyone who chances upon this question, here is a working mpi4py+pycuda example."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "How can I create an array in shared memory without modifying the kernel using templates as seen in the official examples. Or is using templates the official way? In PyOpenCL I can create an array in local memory with setting a kernel argument kernel.set_arg(1,numpy.uint32(a_width)) ... KERNEL_CODE = \"\"\" __kernel void matrixMul(__local float* A_temp,...) { ...} \"\"\"",
        "answers": [
            [
                "CUDA supports dynamic shared memory allocation at kernel run time, but the mechanism is a bit different to OpenCL. In the CUDA runtime API, a kernel using dynamically allocated/sized shared memory and the launch to size the memory uses the following syntax: __global__ void kernel(...) { extern __shared__ typename buffer[]; .... } .... kernel &lt;&lt;&lt; griddim, blockdim, sharedmem, streamID &gt;&gt;&gt; (...) where sharedmem is the total number of bytes per block which will be allocated to buffer. In PyCUDA, the same mechanism works something like this: mod = SourceModule(\"\"\" __global__ void kernel(...) { extern __shared__ typename buffer[]; .... } \"\"\") func = mod.get_function(\"kernel\") func.prepare(..., shared=sharedmem) func.prepared_call(griddim,blockdim,...) with the shared memory allocation size passed to the prepare method."
            ],
            [
                "I do not understand the question fully. I do not work with Python, but know OpenCL quite well. In OpenCL you have two possibilities to create shared/local memory buffers: 1) You add a kernel parameter as you have it in you question. 2) Do define a buffer statically within the kernel itself like: __local buffer[1024]; There are no other chances to do this with OpenCL. How you create the kernel code string to pass it to OpenCL is another question and related to Python. I am not an expert on this."
            ]
        ],
        "votes": [
            3.0000001,
            1e-07
        ]
    },
    {
        "question": "In a Linux system with multiple GPUs, how can you determine which GPU is running X11 and which is completely free to run CUDA kernels? In a system that has a low powered GPU to run X11 and a higher powered GPU to run kernels, this can be determined with some heuristics to use the faster card. But on a system with two equal cards, this method cannot be used. Is there a CUDA and/or X11 API to determine this? UPDATE: The command 'nvidia-smi -a' shows a whether a \"display\" is connected or not. I have yet to determine if this means physically connected, logically connected (running X11), or both. Running strace on this command shows lots of ioctls being invoked and no calls to X11, so assuming that the card is reporting that a display is physically connected.",
        "answers": [
            [
                "There is a device property kernelExecTimeoutEnabled in the cudaDeviceProp structure which will indicate whether the device is subject to a display watchdog timer. That is the best indicator of whether a given CUDA device is running X11 (or the windows/Mac OS equivalent). In PyCUDA you can query the device status like this: In [1]: from pycuda import driver as drv In [2]: drv.init() In [3]: print drv.Device(0).get_attribute(drv.device_attribute.KERNEL_EXEC_TIMEOUT) 1 In [4]: print drv.Device(1).get_attribute(drv.device_attribute.KERNEL_EXEC_TIMEOUT) 0 Here device 0 has a display attached, and device 1 is a dedicated compute device."
            ],
            [
                "I don't know any library function which could check that. However a one \"hack\" comes in mind: X11, or any other system component that manages a connected monitor must consume some of the GPU memory. So, check if both devices report the same amount of available global memory through 'cudaGetDeviceProperties' and then check the value of 'totalGlobalMem' field. If it is the same, try allocating that (or only slightly lower) amount of memory on each of the GPU and see which one fails to do that (cudaMalloc returning an error flag). Some time ago I read somewhere (I don't remember where) that when you increase your monitor resolution, while there is an active CUDA context on the GPU, the context may get invalidated. That hints that the above suggestion might work. Note however that I never actually tried it. It's just my wild guess. If you manage to confirm that it works, or that it doesn't, let us know!"
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers. Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it. Closed 9 years ago. Improve this question I want to learn how to do GPU programming over the summer, and I'm open to all languages/libraries but most interested in PyCuda. I am not a strong programmer; I can bang out most programs I want in Java, and understand the rudiments of C, but when I try anything complex in the latter a segfault or malloc error is almost certain. Thus, I really need a \"for dummies\" tutorial/guide/documentation. Ideally, a guide would work from the very basics of GPU programming through to fairly complicated scientific/numerical programming while explaining each detail with clarity and depth that doesn't take for granted any prior knowledge.",
        "answers": [
            [
                "Starting with PyCUDA doesn't eliminate the need to understand how CUDA works and how to program the GPU. Realistically you probably need to do all of the following, and in this order: Learn enough C to a least have a grasp of the syntax and a thorough understanding of pointers and memory concepts. The latter is really important because in CUDA you are always working with a non-uniform address space. There are dragons a plenty if you can't understand why pointers aren't portable and indirection of pointers in the wrong memory space can't work. Work through something like CUDA by example to get the hang of the basic ideas behind CUDA programming and how the APIs work. Do whatever \"Python for dummies\" and \"numpy for dummies\" tutorials you need to get up to speed with the Python end of things. Then PyCUDA will become completely self evident. It took me about an hour to digest PyCUDA coming from a background of already knowing how to write working CUDA code and working a lot with Python and numpy."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "I'll try and make this clear; I've got two classes; GPU(Object), for general access to GPU functionality, and multifunc(threading.Thread) for a particular function I'm trying to multi-device-ify. GPU contains most of the 'first time' processing needed for all subsequent usecases, so multifunc gets called from GPU with its self instance passed as an __init__ argument (along with the usual queues and such). Unfortunately, multifunc craps out with: File \"/home/bolster/workspace/project/gpu.py\", line 438, in run prepare(d_A,d_B,d_XTG,offset,grid=N_grid,block=N_block) File \"/usr/local/lib/python2.7/dist-packages/pycuda-0.94.2-py2.7-linux-x86_64.egg/pycuda/driver.py\", line 158, in function_call func.set_block_shape(*block) LogicError: cuFuncSetBlockShape failed: invalid handle First port of call was of course the block dimensions, but they are well within range (same behaviour even if I force block=(1,1,1), likewise grid. Basically, within multifunc, all of the usual CUDA memalloc etc functions work fine, (implying its not a context problem) So the problem must be with the SourceModuleing of the kernel function itself. I have a kernel template containing all my CUDA code that's file-scoped, and templating is done with jinja2 in the GPU initialisation. Regardless of whether that templated object is converted to a SourceModule object in GPU and passed to multifunc, or if its converted in multifunc the same thing happens. Google has been largely useless for this particular issue, but following the stack, I'm assuming the Invalid Handle being referred to is the kernel function handle rather than anything strange going on with the block dimensions. I'm aware this is a very corner-case situation, but I'm sure someone can see a problem that I've missed.",
        "answers": [
            [
                "The reason is context affinity. Every CUDA function instance is tied to a context, and they are not portable (the same applies to memory allocations and texture references). So each context must load the function instance separately, and then use the function handle returned by that load operation. If you are not using metaprogramming at all, you might find it simpler to compile your CUDA code to a cubin file, and then load the functions you need from the cubin to each context with driver.module_from_file. Cutting and pasting directly from some production code of mine: # Context establishment try: if (autoinit): import pycuda.autoinit self.context = None self.device = pycuda.autoinit.device self.computecc = self.device.compute_capability() else: driver.init() self.context = tools.make_default_context() self.device = self.context.get_device() self.computecc = self.device.compute_capability() # GPU code initialization # load pre compiled CUDA code from cubin file # Select the cubin based on the supplied dtype # cubin names contain C++ mangling because of # templating. Ugly but no easy way around it if self.computecc == (1,3): self.fimcubin = \"fim_sm13.cubin\" elif self.computecc[0] == 2: self.fimcubin = \"fim_sm20.cubin\" else: raise NotImplementedError(\"GPU architecture not supported\") fimmod = driver.module_from_file(self.fimcubin) IterateName32 = \"_Z10fimIterateIfLj8EEvPKT_PKiPS0_PiS0_S0_S0_jjji\" IterateName64 = \"_Z10fimIterateIdLj8EEvPKT_PKiPS0_PiS0_S0_S0_jjji\" if (self.dtype == np.float32): IterateName = IterateName32 elif (self.dtype == np.float64): IterateName = IterateName64 else: raise TypeError self.fimIterate = fimmod.get_function(IterateName) except ImportError: warn(\"Could not initialise CUDA context\")"
            ],
            [
                "Typical; as soon as I write the question I work it out. The issue was having the SourceModule operating outside of an active context. To fix it I moved the SourceModule invocation into the run function in the thread, below the cuda context setup. Leaving this up for a while because I'm sure someone else has a better explanation!"
            ]
        ],
        "votes": [
            5.0000001,
            1.0000001
        ]
    },
    {
        "question": "I've got a problem that I want to split across multiple CUDA devices, but I suspect my current system architecture is holding me back; What I've set up is a GPU class, with functions that perform operations on the GPU (strange that). These operations are of the style for iteration in range(maxval): result[iteration]=gpuinstance.gpufunction(arguments,iteration) I'd imagined that there would be N gpuinstances for N devices, but I don't know enough about multiprocessing to see the simplest way of applying this so that each device is asynchronously assigned, and strangely few of the examples that I came across gave concrete demonstrations of collating results after processing. Can anyone give me any pointers in this area? UPDATE Thank you Kaloyan for your guidance in terms of the multiprocessing area; if CUDA wasn't specifically the sticking point I'd be marking you as answered. Sorry. Perviously to playing with this implementation, the gpuinstance class initiated the CUDA device with import pycuda.autoinit But that didn't appear to work, throwing invalid context errors as soon as each (correctly scoped) thread met a cuda command. I then tried manual initialisation in the __init__ constructor of the class with... pycuda.driver.init() self.mydev=pycuda.driver.Device(devid) #this is passed at instantiation of class self.ctx=self.mydev.make_context() self.ctx.push() My assumption here is that the context is preserved between the list of gpuinstances is created and when the threads use them, so each device is sitting pretty in its own context. (I also implemented a destructor to take care of pop/detach cleanup) Problem is, invalid context exceptions are still appearing as soon as the thread tries to touch CUDA. Any ideas folks? And Thanks to getting this far. Automatic upvotes for people working 'banana' into their answer! :P",
        "answers": [
            [
                "You need to get all your bananas lined up on the CUDA side of things first, then think about the best way to get this done in Python [shameless rep whoring, I know]. The CUDA multi-GPU model is pretty straightforward pre 4.0 - each GPU has its own context, and each context must be established by a different host thread. So the idea in pseudocode is: Application starts, process uses the API to determine the number of usable GPUS (beware things like compute mode in Linux) Application launches a new host thread per GPU, passing a GPU id. Each thread implicitly/explicitly calls equivalent of cuCtxCreate() passing the GPU id it has been assigned Profit! In Python, this might look something like this: import threading from pycuda import driver class gpuThread(threading.Thread): def __init__(self, gpuid): threading.Thread.__init__(self) self.ctx = driver.Device(gpuid).make_context() self.device = self.ctx.get_device() def run(self): print \"%s has device %s, api version %s\" \\ % (self.getName(), self.device.name(), self.ctx.get_api_version()) # Profit! def join(self): self.ctx.detach() threading.Thread.join(self) driver.init() ngpus = driver.Device.count() for i in range(ngpus): t = gpuThread(i) t.start() t.join() This assumes it is safe to just establish a context without any checking of the device beforehand. Ideally you would check the compute mode to make sure it is safe to try, then use an exception handler in case a device is busy. But hopefully this gives the basic idea."
            ],
            [
                "What you need is a multi-threaded implementation of the map built-in function. Here is one implementation. That, with a little modification to suit your particular needs, you get: import threading def cuda_map(args_list, gpu_instances): result = [None] * len(args_list) def task_wrapper(gpu_instance, task_indices): for i in task_indices: result[i] = gpu_instance.gpufunction(args_list[i]) threads = [threading.Thread( target=task_wrapper, args=(gpu_i, list(xrange(len(args_list)))[i::len(gpu_instances)]) ) for i, gpu_i in enumerate(gpu_instances)] for t in threads: t.start() for t in threads: t.join() return result It is more or less the same as what you have above, with the big difference being that you don't spend time waiting for each single completion of the gpufunction."
            ]
        ],
        "votes": [
            21.0000001,
            3.0000001
        ]
    },
    {
        "question": "Anyone following CUDA will probably have seen a few of my queries regarding a project I'm involved in, but for those who haven't I'll summarize. (Sorry for the long question in advance) Three Kernels, One Generates a data set based on some input variables (deals with bit-combinations so can grow exponentially), another solves these generated linear systems, and another reduction kernel to get the final result out. These three kernels are ran over and over again as part of an optimisation algorithm for a particular system. On my dev machine (Geforce 9800GT, running under CUDA 4.0) this works perfectly, all the time, no matter what I throw at it (up to a computational limit based on the stated exponential nature), but on a test machine (4xTesla S1070's, only one used, under CUDA 3.1) the exact same code (Python base, PyCUDA interface to CUDA kernels), produces the exact results for 'small' cases, but in mid-range cases, the solving stage fails on random iterations. Previous problems I've had with this code have been to do with the numeric instability of the problem, and have been deterministic in nature (i.e fails at exactly the same stage every time), but this one is frankly pissing me off, as it will fail whenever it wants to. As such, I don't have a reliable way to breaking the CUDA code out from the Python framework and doing proper debugging, and PyCUDA's debugger support is questionable to say the least. I've checked the usual things like pre-kernel-invocation checking of free memory on the device, and occupation calculations say that the grid and block allocations are fine. I'm not doing any crazy 4.0 specific stuff, I'm freeing everything I allocate on the device at each iteration and I've fixed all the data types as being floats. TL;DR, Has anyone come across any gotchas regarding CUDA 3.1 that I haven't seen in the release notes, or any issues with PyCUDA's autoinit memory management environment that would cause intermittent launch failures on repeated invocations?",
        "answers": [
            [
                "Have you tried: cuda-memcheck python yourapp.py You likely have an out of bounds memory access."
            ],
            [
                "You can use nVidia CUDA Profiler and see what gets executed before the failure."
            ]
        ],
        "votes": [
            4.0000001,
            -0.9999999
        ]
    },
    {
        "question": "PyCUDA, for all its faults, usually has very good examples provided with it / downloadable from the wiki. But I couldn't find anything in the examples or in the documentation (or a cursory google search) demonstrating the PyCUDA way of dyanmically allocating workloads to multiple devices. Can anyone either hint me toward what I should be doing or point me to examples? One idea that popped into my head was using multiprocessing, generating a pool of N processes, each tied to one device, and then when the class is called (I have all my gpu functions in a separate class; probably not the best idea but it works) it round-robin's the multiprocesses. How good / retarded an idea is this? PS My dev machine is 1 GPU and my test machine in 4 GPU, so I need whatever solution to be able to deal with a dynamic number of devices (it also doesn't help that they're different compute capabilities, but thats life)",
        "answers": [
            [
                "Pycuda hasn't had any intrinsic multiple-GPU support because CUDA also hasn't had any intrinsic multiple-GPU support. This will change in CUDA 4.0 because the API has been changed to be thread safe and multi-GPU aware. But Pycuda doesn't yet have that support AFAIK. Even when it comes, each device has to be explicitly managed, and the workload divided by you. There is no automatic workload distribution or anything like that. For multi-GPU, I have normally used mpi4py. You could potentially use a multithreaded python scheme, with each thread opening a separate context in Pycuda. What works best will probably depend on how much communication is required between devices."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "As part of a larger project, I've come across a strangely consistent bug that I can't get my head around, but is an archetypical 'black box' bug; when running with cuda-gdb python -m pycuda.debug prog.py -args, it runs fine, but slow. If i drop pycuda.debug, it breaks. Consistently, at exactly the same point in multiple-kernel execution. To explain; I have (currently three) kernels, used in different grid and block arrangements to solve 'slices' of a larger optimisation problem. These strictly speaking should either work, or not, as the functions themselves are told nothing but 'here's some more data', and other than the contents of the data, don't know anything such as iteration number whether their input data is partitioned or not, and up until this one point, they perform perfectly. Basically, I can't see what's happening without pycuda.debug exposing the debugging symbols to GDB, but I also can't see the problem WITH pycuda.debug. What does pycuda actually do so I know what to look for in my kernel code?",
        "answers": [
            [
                "Almost nothing. It mostly sets compiler flags in the pycuda.driver module so that CUDA code gets compiled with the necessary debugging symbols and assembled in the way CUDA-gdb requires. The rest is a tiny wrapper that nicely encapsulates the pycuda libraries so thar everything works. The whole thing is about 20 lines of python, you can see the code in the source distribution if you want. The key thing here is that code run in the debugger spills everything from register and shared memory to local memory, so that the driver can read local program state. So if you have code that runs when built for the debugger and fails when built normally, it usually means there is a shared memory buffer overflow or pointer error which is causing the GPU equivalent of a segfault."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Anyone know likely avenues of investigation for kernel launch failures that disappear when run under cuda-gdb? Memory assignments are within spec, launches fail on the same run of the same kernel every time, and (so far) it hasn't failed within the debugger. Oh Great SO Gurus, What now?",
        "answers": [
            [
                "cuda-gdb spills all shared memory and registers to local memory. So when something runs ok built for debugging and fails otherwise, it usually means out of bounds shared memory access. cuda-memcheck might help, depending on what sort of card you are using. Fermi is better than older cards in that respect. EDIT: Casting my mind back to the bad old days, I remember having an ornery GT9500 which used to throw similar NV13 errors and have random code failures when running very memory intensive kernels with a lot of shared memory activity. Never when debugging. I put it down to bad hardware and moved on to a GT200, never to see a similar error since. One possibility might be bad hardware. Is this a G92 (9800GT or similar)?"
            ],
            [
                "CUDA GDB can make some of the cuda operations synchronous. Are you reading from a memory after has been initialized ? are you using Streams? Are you launching more than one kernel? Where and how does it fail ?"
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "PyCUDA's documentation mentions Driver Interface calls in passing, but I'm a bit think and can't see how to get information such as 'SHARED_SIZE_BYTES' out of my code. Can anyone point me to any examples of querying the device in this way? Is it possible to / How do I check the device state (eg between malloc/memcpy and kernel launch) to implement some machine-dynamic operations? (I want to be able to deal with devices that support multiple kernels in a 'friendly' way.",
        "answers": [
            [
                "Just for anyone else coming across this, spending half an hour with the CUDA API in one hand, and the PyCUDA documentation in another does wonders. Its much simpler than my initial experiments indicated. Runtime Kernel Info Incoming lazy lazy code ... kernel=mod.get_function(\"foo\") meminfo(kernel) ... def meminfo(kernel): shared=kernel.shared_size_bytes regs=kernel.num_regs local=kernel.local_size_bytes const=kernel.const_size_bytes mbpt=kernel.max_threads_per_block print(\"=MEM=\\nLocal:%d,\\nShared:%d,\\nRegisters:%d,\\nConst:%d,\\nMax Threads/B:%d\" % (local,shared,regs,const,mbpt)) Example Output =MEM= Local:24, Shared:64, Registers:18, Const:0, Max Threads/B:512 Static Device Info Incoming lazy lazy code import pycuda.autoinit import pycuda.driver as cuda (free,total)=cuda.mem_get_info() print(\"Global memory occupancy:%f%% free\"%(free*100/total)) for devicenum in range(cuda.Device.count()): device=cuda.Device(devicenum) attrs=device.get_attributes() #Beyond this point is just pretty printing print(\"\\n===Attributes for device %d\"%devicenum) for (key,value) in attrs.iteritems(): print(\"%s:%s\"%(str(key),str(value))) Example Output Global memory occupancy:70.000000% free ===Attributes for device 0 MAX_THREADS_PER_BLOCK:512 MAX_BLOCK_DIM_X:512 MAX_BLOCK_DIM_Y:512 MAX_BLOCK_DIM_Z:64 MAX_GRID_DIM_X:65535 MAX_GRID_DIM_Y:65535 MAX_GRID_DIM_Z:1 MAX_SHARED_MEMORY_PER_BLOCK:16384 TOTAL_CONSTANT_MEMORY:65536 WARP_SIZE:32 MAX_PITCH:2147483647 MAX_REGISTERS_PER_BLOCK:8192 CLOCK_RATE:1500000 TEXTURE_ALIGNMENT:256 GPU_OVERLAP:1 MULTIPROCESSOR_COUNT:14 KERNEL_EXEC_TIMEOUT:1 INTEGRATED:0 CAN_MAP_HOST_MEMORY:1 COMPUTE_MODE:DEFAULT MAXIMUM_TEXTURE1D_WIDTH:8192 MAXIMUM_TEXTURE2D_WIDTH:65536 MAXIMUM_TEXTURE2D_HEIGHT:32768 MAXIMUM_TEXTURE3D_WIDTH:2048 MAXIMUM_TEXTURE3D_HEIGHT:2048 MAXIMUM_TEXTURE3D_DEPTH:2048 MAXIMUM_TEXTURE2D_ARRAY_WIDTH:8192 MAXIMUM_TEXTURE2D_ARRAY_HEIGHT:8192 MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES:512 SURFACE_ALIGNMENT:256 CONCURRENT_KERNELS:0 ECC_ENABLED:0 PCI_BUS_ID:1 PCI_DEVICE_ID:0 TCC_DRIVER:0"
            ]
        ],
        "votes": [
            21.0000001
        ]
    },
    {
        "question": "I've got a large chunk of generated data (A[i,j,k]) on the device, but I only need one 'slice' of A[i,:,:], and in regular CUDA this could be easily accomplished with some pointer arithmetic. Can the same thing be done within pycuda? i.e cuda.memcpy_dtoh(h_iA,d_A+(i*stride)) Obviously this is completely wrong since theres no size information (unless inferred from the dest shape), but hopefully you get the idea?",
        "answers": [
            [
                "The pyCUDA gpuArray class supports slicing of 1D arrays, but not higher dimensions that require a stride (although it is coming). You can, however, get access to the underlying pointer in a multidimensional gpuArray from the gpuarray member, which is a pycuda.driver.DeviceAllocation type, and the size information from the gpuArray.dtype.itemsize member. You can then do the same sort of pointer arithmetic you had in mind to get something that the driver memcpy functions will accept. It isn't very pythonic, but it does work (or at least it did when I was doing a lot of pyCUDA + MPI hacking last year)."
            ],
            [
                "Is unlikely that is implemented in PyCuda. I can think to the following solutions: Copy the entire Array A in memory and make a numpy array from the interested slice. Create a Kernel that read the matrix and creates the desired slice. Rearrange the Produced Data in a way that you can read a slice at a time from pointer arithmetic."
            ]
        ],
        "votes": [
            2.0000001,
            1e-07
        ]
    },
    {
        "question": "Should be simple enough; I literally want to send an int to the a SourceModule kernel declaration, where the C function __global__......(int value,.....) with the value being declared and called... value = 256 ... ... func(value,...) But I'm getting a variety of errors from pycuda that I'm using the wrong type.",
        "answers": [
            [
                "The standard PyCUDA function interface requires argument have numpy dtypes, because it internally does mapping to C types under the hood. So for scalar arguments which are passed by value, you need to \"cast\" to a suitable numpy dtype first. Something like value = 256 va = numpy.int32(value) func(va) should work. If you are passing single precision floating point values or arrays, make sure to explicitly use a dtype of np.float32, because numpy uses double precision by default and you will wind up with similar errors."
            ]
        ],
        "votes": [
            9.0000001
        ]
    },
    {
        "question": "The PyCUDA documentation is a bit light on examples for those of us in the 'Non-Guru' class, but I'm wondering about the operations available for array operations on gpuarrays, ie. if I wanted to gpuarray this loop; m=np.random.random((K,N,N)) a=np.zeros_like(m) b=np.random.random(N) #example for k in range(K): for x in range(N): for y in range(N): a[k,x,y]=m[k,x,y]*b[y] The regular first-stop python reduction for this would be something like for k in range(K): for x in range(N): a[k,x,:]=m[k,x,:]*b But I can't see any simple way to do this with GPUArray, other than writing a custom elementwise kernel, and even then with this problem there would have to be looping constructs in the kernel and at that point of complexity I'm probably better off just writing my own full blown SourceModule kernel. Can anyone clue me in ?",
        "answers": [
            [
                "That is probably best done with your own kernel. While PyCUDA's gpuarray class is a really convenient abstraction of GPU memory into something which can be used interchangeably with numpy arrays, there is no getting around the need to code for the GPU for anything outside of the canned linear algebra and parallel reduction operations. That said, it is a pretty trivial little kernel to write. So trivial that it would be memory bandwidth bound - you might want to see if you can \"fuse\" a few like operations together to improve the ratio of FLOPS to memory transactions a bit. If you need some help with the kernel, drop in a comment, and I can expand the answer to include a rough prototype."
            ],
            [
                "You can also use the memcpy_dtod() method and the slicing functionality of gpuarrays. Its strange that normal assignment does not work. set() does not work because it assumes host to device transfer (using memcpy_htod()). for k in range(K): for x in range(N): pycuda.driver.memcpy_dtod(a[k,x,:].gpudata, (m[k,x,:]*b).gpudata, a[k,x,:].nbytes)"
            ]
        ],
        "votes": [
            2.0000001,
            2.0000001
        ]
    },
    {
        "question": "Simple enough start=cuda.Event() func(args,block=blockdims) cuda.memcpy_dtoh(d,h) end=cuda.Event() dur=start.time_till(end) print dur But I'm getting this error File \"gpu.py\", line 161, in gpu_test dur=start.time_till(end) pycuda._driver.LogicError: cuEventElapsedTime failed: invalid handle This is as far as I can tell from the docs the correct usage. Anyone got any idea what I'm doing wrong?",
        "answers": [
            [
                "Take a look at SimpleSpeedTest.py: start=cuda.Event() end=cuda.Event() start.record() # start timing func(args,block=blockdims) cuda.memcpy_dtoh(d,h) end.record() # end timing # calculate the run length end.synchronize() millis = start.time_till(end) print millis"
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Two problems I'm having with copperhead at the minute, which I suspect are related. Running a sample file (samples/axpy.py) generated lots of little warnings, but this one stood out. g++ -pthread -fno-strict-aliasing -g -O2 -g -fwrapv -O2 -Wall -fPIC -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Xlinker -export-dynamic -Wl,-O1 -Wl,-Bsymbolic-functions -DNDEBUG -I/usr/include/python2.6 -I/usr/local/lib/python2.6/dist-packages/copperhead-0.1a1-py2.6.egg/copperhead/include -I/usr/local/cuda/include /tmp/codepy-compiler-cache-v5-uid1000/202478034fea29b82d046b259bd6f43e/module.o /tmp/codepy-compiler-cache-v5-uid1000/fdcb04ede426b146cfce8894e99eeb57/gpu.o -L/usr/lib -L/usr/local/cuda/lib -L/usr/local/cuda/lib64 -lcuda -lcudart -lboost_python-gcc43-mt -lpthread -ldl -lutil -o /tmp/codepy-compiler-cache-v5-uid1000/202478034fea29b82d046b259bd6f43e/codepy.temp.202478034fea29b82d046b259bd6f43e.module.so /usr/bin/ld: skipping incompatible /usr/local/cuda/lib/libcudart.so when searching for -lcudart /usr/bin/ld: cannot find -lboost_python-gcc43-mt collect2: ld returned 1 exit status Few things to notice; g++ is has correctly picked up the correct CUDA lib dir to use (lib64), but doesn't see libcudart.so in there, which it is. /usr/local/cuda/lib64/libcudart.so.4.0.12: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, stripped Next, boost_python isn't found; I've heard from a few places that this is due to ubuntu breaking a package convention somewhere, and people has said they've fixed it by changing the pycuda configure options, but haven't said what to change it to... (Example) Anyone got any ideas for either the cudart or boot issues?",
        "answers": [
            [
                "It is finding libcudart. What you are seeing is only an informational warning: -L options are searched in order and the linker is finding the 32 bit version first, because you gave -L/usr/local/cuda/lib before -L/usr/loca/cuda/lib64. For the libboost_python problem, just link with -lboost_python. The Ubuntu systems I use (64 bit 10.04LTS with boost-python 1.40) have a series of cascading symbolic links to that canonical library name that make the linker find the correct library without any further intervention."
            ]
        ],
        "votes": [
            3.0000001
        ]
    },
    {
        "question": "Question more or less says it all. calling a host function(\"std::pow&lt;int, int&gt; \") from a __device__/__global__ function(\"_calc_psd\") is not allowed from my understanding, this should be using the cuda pow function instead, but it isn't.",
        "answers": [
            [
                "The error is exactly as the compiler is reported. You can't used host functions in device code, and that include the whole host C++ std library. CUDA includes its own standard library, described in the programming guide, but you should use either pow or fpow (taken from the C standard library, no C++ or namespaces). nvcc will overload the function with the cuda correct device function and inline the resulting code. Something like the following will work: #include &lt;math.h&gt; __device__ float func(float x) { return x * x * fpow(x, 0.123456f); } EDIT: The bit I missed the first time is the template specifier reported in the errors. Are you sure that you are passing either float or double arguments to pow? If you are passing integers, there is no overload function in the CUDA standard library, which is why it might be failing. If you need an integer pow function, you will have to roll your own (or do casting, but pow is a rather expensive function and I am certain some cascaded integer multiplication will be faster)."
            ]
        ],
        "votes": [
            7.0000001
        ]
    },
    {
        "question": "Something that isn't really mentioned anywhere (at least that I can see) is what library functions are exposed to inline CUDA kernels. Specifically I'm doing small / stupid matrix multiplications that don't deserve to be individually offloaded to the GPU but am offloading a larger section of the algorithm which includes this multiplication. Noone ever liked using their own linalg functions since someone has always done it better. TLDR What libraries can I play with while in inline kernels under PyCUDA?",
        "answers": [
            [
                "I don't know of any, and I always thought it would be useful to have. For the size of problems that I usually work with (small matrices and tensors that arise in the finite element method), I just wrote C++ templates to do the operations. Templating the functions allows the compiler to know the trip counts at compile time, and it can unroll loops and keep results or intermediate results in register, which tends to be very efficient for kernel throughput. So the matrix-matrix product gets declared as template &lt; typename Real, unsigned int l, unsigned int m, unsigned int n &gt; __device__ __host__ void matmul(const Real *a, const Real *b, Real *c) { for(int i=0; i&lt;l; i++) { for(int j=0; j&lt;n; j++) { Real dotprod = Real(0); for(int k=0; k&lt;m; k++) { dotprod += a[idx2c(i,k,l)] * b[idx2c(k,j,m)]; } c[idx2c(i,j,l)] = dotprod; } } } For the sort of sizes that crop up in my kernels (2x2, 3x3, 4x4, 8x8, 9x9), doing the above and letting the compile work things out seems to be as good as any other approach I have tried. Because at the thread level CUDA is effectively scalar, there aren't any vector primitives or stuff like that which can be used to accelerate these sort of small operations."
            ]
        ],
        "votes": [
            1.0000001
        ]
    },
    {
        "question": "Example code: import pycuda.autoinit import pycuda.driver as drv import numpy from pycuda.compiler import SourceModule mod = SourceModule(\"\"\" __global__ void multiply_them(float *dest, float *a, float *b) { const int i = threadIdx.x; dest[i] = a[i] * b[i]; } \"\"\") multiply_them = mod.get_function(\"multiply_them\") a = numpy.random.randn(400).astype(numpy.float32) b = numpy.random.randn(400).astype(numpy.float32) dest = numpy.zeros_like(a) multiply_them( drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1)) print dest-a*b Results: Traceback (most recent call last): File \"test.py\", line 12, in &lt;module&gt; \"\"\") File \"build/bdist.linux-x86_64/egg/pycuda/compiler.py\", line 238, in __init__ File \"build/bdist.linux-x86_64/egg/pycuda/compiler.py\", line 223, in compile File \"build/bdist.linux-x86_64/egg/pycuda/compiler.py\", line 149, in _find_pycuda_include_path ImportError: No module named pycuda Sounds simple enough, so lets test this. Python 2.7.1 (r271:86832, Feb 17 2011, 14:13:40) [GCC 4.3.4] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import pycuda &gt;&gt;&gt; pycuda &lt;module 'pycuda' from '/home/abolster/lib/python2.7/site-packages/pycuda-0.94.2-py2.7-linux-x86_64.egg/pycuda/__init__.pyc'&gt; &gt;&gt;&gt; Ok, thats weird... Long story short, even stepping through the file line by line into the python console, nothing goes wrong until the actual execution of the mod=SourceModule() line. (Final Traceback, I promise) /home/abolster/lib/python2.7/site-packages/pycuda-0.94.2-py2.7-linux-x86_64.egg/pycuda/compiler.pyc in _find_pycuda_include_path() 147 def _find_pycuda_include_path(): 148 from imp import find_module --&gt; 149 file, pathname, descr = find_module(\"pycuda\") 150 151 # Who knew Python installation is so uniform and predictable? ImportError: No module named pycuda So it looks like pycuda is getting different include dirs than runtime python, which shouldn't happen (as i understand it) Any ideas? (Sorry for the long question) Talonmies borought up a point about nvcc not being found; unless python is getting its envars from somewhere I can't think of, there's no reason it shouldn't : [bolster@dellgpu src]$ which nvcc ~/cuda/bin/nvcc",
        "answers": [
            [
                "Changing to Python 2.6 and reinstalling relevant modules fixed the problem for the OP."
            ],
            [
                "There is nothing wrong with the code you are trying to run - it should work. My guess is that nvcc cannot be found. Make sure that the path to the nvcc executable is set in your environment before you try using pycuda.compiler."
            ],
            [
                "I think you did not install the CUDA toolkit from nvidia and added the /usr/local/cuda/lib/ to LD_LIBRARY_PATH find the the .so of the pycuda module and give us the output of: &gt;lld pycuda.so"
            ]
        ],
        "votes": [
            3.0000001,
            1.0000001,
            1e-07
        ]
    },
    {
        "question": "I have a simple (tested) pycuda app and am trying to profile it. I've tried NVidia's Compute Visual Profiler, which runs the program 11 times, then emits this error: NV_Warning: Ignoring the invalid profiler config option: fb0_subp0_read_sectors Error : Profiler data file '/home/jguy/proj/gpu/tdbp/pyArch/temp_compute_profiler_0_0.csv' does not contain profiler output.This can happen when: a) Profiling is disabled during the entire run of the application. b) The application does not invoke any kernel launches or memory transfers. c) The application does not release resources (contexts, events, etc.). The program needs to be modified to properly free up all resources before termination. I also tried running \"CUDA_PROFILE python scriptname.py arg1\". It created a file containing: NV_Warning: Ignoring the invalid profiler config option: instructions # CUDA_PROFILE_LOG_VERSION 2.0 # CUDA_DEVICE 0 GeForce GTX 560 Ti # CUDA_PROFILE_CSV 1 # TIMESTAMPFACTOR fffff7003e38fec8 gpustarttimestamp,method,gputime,cputime,occupancy In case it's useful, I also have these environment vars set: CUDA_PROFILE_CONFIG=temp_cuda_profiler.conf CUDA_PROFILE_CSV=1 CUDA_PROFILE_LOG=profile.csv CUDA_PROFILE=1 and temp_cuda_profiler.conf contains gpustarttimestamp instructions Been googling for an hour or so. No luck. Thanks for any insights you can provide!",
        "answers": [
            [
                "When using import pycuda.autoinit, it is important to have pycuda.autoinit.context.detach() at the end of the program. This fixed the problem."
            ]
        ],
        "votes": [
            2.0000001
        ]
    },
    {
        "question": "I am in a project to process an image using CUDA. The project is simply an addition or subtraction of the image. May I ask your professional opinion, which is best and what would be the advantages and disadvantages of those two? I appreciate everyone's opinions and/or suggestions since this project is very important to me.",
        "answers": [
            [
                "General answer: It doesn't matter. Use the language you're more comfortable with. Keep in mind, however, that pycuda is only a wrapper around the CUDA C interface, so it may not always be up-to-date, also it adds another potential source of bugs, \u2026 Python is great at rapid prototyping, so I'd personally go for Python. You can always switch to C++ later if you need to."
            ],
            [
                "If the rest of your pipeline is in Python, and you're using Numpy already to speed things up, pyCUDA is a good complement to accelerate expensive operations. However, depending on the size of your images and your program flow, you might not get too much of a speedup using pyCUDA. There is latency involved in passing the data back and forth across the PCI bus that is only made up for with large data sizes. In your case (addition and subtraction), there are built-in operations in pyCUDA that you can use to your advantage. However, in my experience, using pyCUDA for something non-trivial requires knowing a lot about how CUDA works in the first place. For someone starting from no CUDA knowledge, pyCUDA might be a steep learning curve."
            ],
            [
                "Take a look at openCV, it contains a lot of image processing functions and all the helpers to load/save/display images and operate cameras. It also now supports CUDA, some of the image processing functions have been reimplemented in CUDA and it gives you a good framework to do your own."
            ],
            [
                "Alex's answer is right. The amount of time consumed in the wrapper is minimal. Note that PyCUDA has some nice metaprogramming constructs for generating kernels which might be useful. If all you're doing is adding or subtracting elements of an image, you probably shouldn't use CUDA for this at all. The amount of time it takes to transfer back and forth across the PCI-E bus will dwarf the amount of savings you get from parallelism. Any time you deal with CUDA, it's useful to think about the CGMA ratio (computation to global memory access ratio). Your addition/subtraction is only 1 float point operation for 2 memory accesses (1 read and 1 write). This ends up being very lousy from a CUDA perspective."
            ]
        ],
        "votes": [
            6.0000001,
            3.0000001,
            2.0000001,
            1e-07
        ]
    }
]