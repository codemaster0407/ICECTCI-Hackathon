,query,data
0,application-offloading,"Hi all,
I wonder if it is possible to offload a specific application to the SmartNIC? and how we can do that during runtime?
I have looked at the doca documentation but couldn’t find anything that would help.
Thank youIt may be possible to “offload” an entire application to the Bluefield-2 ARM, but it very much depends. Currently, we’re thinking about DOCA as application acceleration: Find pieces of your existing x86 applications where there is opportunity to have the NIC perform a portion of that work via DOCA libraries. Our first release of DOCA focuses mostly on deep packet inspection and regular expression acceleration.Thanks for the response, which doca libraries would be helpful to offload a portion of an application?Hi Diman,
How does one define offloading? Does running application originally intended for x86 over a BF2 device is an offload?
Both running applications over x86 and using the BF2 acceleration is possible, and running application over the ARM cores of the BF2 and using HW accelerators is possible.Does running applicatioHi Shauli,
The application is intended to run on the host but I want to offload it to the bluefield time to time. Similar to what Sean Choi is doing in Lambda nic paper , but he is using p4 programmingThere is a growing interest in serverless compute, a cloud computing model that automates infrastructure resource- allocation and management while billing customers only for the resources they use. Workloads like stream processing benefit from high...This is still a bit too wide in terms of me understanding what you’re trying to achieve.
If you’re trying to run a x86 based application on the host and use the regex capabilities of the BF2, it can easily be done.
If you’re trying to run the application on the ARM cores of the BF2 and be completely transparent to the host, it is also possible. I suggest following our reference applications which are both runnable on x86 and ARM.
I might need further clarification what “offload from time to time” actually means.further clarificatioThanks Shauli,
For the containerized applications, where we have multiple instances of the same container, is it possible to change where the pod runs time to time, for example, at some point we might decide to run more pods on x86 and at another time we might run more pods on the SmartNIC.
In the separated mode, this is possible since Kubernetes gives us the flexibility to do so using service mesh. But in the embedded mode the SmartNIC+ host are one node in the kubernetes cluster, and I’d like to know whether Doca provides us a feature to make the decision on where to run the containers during run-time.This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1,radius-aaa-configuration-questions,"Hello, I am running into some issues getting RADIUS AAA working. I am using a Windows NPS server for RADIUS. I only need to setup a Admin access to this switch. A read-only/monitor profile is not needed.I can see my authentication attempts being granted on the NPS server. With the correct policy being hit but I am never granted access to the switch. Do I need to send a specific VSA back to the switch to map user roles? I currently am not sending a VSA back to the switch from the RADIUS server.The one thing that confused me in the Mellanox documentation is the: “aaa authorization map” command. Is this needed for successful authentication? I attempted to add “aaa authorization map default-user admin” however there was no change in behavior.Current config is very basic:aaa authentication login default local radiusradius-server host 1.2.3.4Any help would be greatly appreciated.ThanksHi Kevin,You need to be sure you set the aaa authorization map order to tell it to check the RADIUS server first.(config) # aaa authorization map order remote-firstFrom the User Manual, this command does the following:Sets the user mapping behavior when authenticating users via RADIUSor TACACS+ or LDAP to one of three choices. The order determineshow the remote user mapping behaves. If the authenticated usernameis valid locally, no mapping is performed. The setting has the followingthree possible behaviors:local-only – maps all remote users to the user specified bythe command “aaa authorization map default-user <username>”. Any vendor attributes received by an authenticationserver are ignored.remote-first – if a local-user mapping attribute is returnedand it is a valid local username, it maps the authenticateduser to the local user specified in the attribute. Otherwise, ituses the user specified by the default-user command.remote-only – maps a remote authenticated user if theauthentication server sends a local-user mapping attribute.If the attribute does not specify a valid local user, no furthermapping is tried.Some helpful commands on the switch:show radius <<<----show radius server configuration on switchshow aaa <<<<-----shows default usershow usernames <<<<----shows usernames configured locally on switcshow users <<<-----shows which host is currently connected toshow aaa authentication attempts <<<----shows information on current and past login attempts as well as configurationRegards,KevinHi Kevin,I just hit the same issue and the solution was to change the aaa method order :aaa authentication login default local radiusreplace withaaa authentication login default radius localAs far as the radius servers are accessible, they will do the authentication. The drawback of that is that the second method (local) is not tried if the radius servers are up and running.This is a bit silly : there is no means to have the second method tried if the user account is not found on the first method listed (radius). The second method is only tried if all the radius servers fail.Also this imply that when you configure “aaa authentication login default local radius”, the radius method is never attempted because the local method may not fail…I hope this will be addressed on a futur Onyx release.Regards,AlainPowered by Discourse, best viewed with JavaScript enabled"
2,ubuntu-18-04-lts-kernel-4-15-0-140-generic-5-2-and-5-1-drivers-fail-to-install,"When using the 4.15.0-140-generic kernel I cannot get either of the 5x drivers to install:mlnx-en-5.1-1.0.4.0-ubuntu18.04-x86_64mlnx-en-5.2-1.0.4.0-ubuntu18.04-x86_64Both fail with the same error:Error: mlnx-en-dkms installation failed!Problem: mlx5_core: module file: /lib/modules/4.15.0-140-generic/kernel/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.ko, from package: linux-modules-4.15.0-140-generic.If I use an older kernel (4.15.0-112-generic), both drivers install fine.Is the 4.15.0-140 kernel supported?Hello Charlie,Thank you for posting your inquiry on the NVIDIA Networking Community.This is a known issue and is resolved in our latest MLNX_OFED version 5.3 GA which is available through our download site.Thank you and regards,~NVIDIA Networking Technical SupportWill the fix be applied to a Ethernet only driver?Powered by Discourse, best viewed with JavaScript enabled"
3,blue-field-dpu2-not-coming-up,"Hi
We are seeing EMMC issues and BFB install also failingLoading Linux 5.4.0-1017.16.gf565efa-bluefield …
Loading initial ramdisk …
[    4.042778] JBD2: Invalid checksum recovering block 716 in log
[    4.048645] EXT4-fs (mmcblk0p2): error loading journalBFB image fails :
Mellanox BlueField-2 A1 BL1 V1.1
ERROR:   Failed to load BL2R firmware.Dear NVIDIA team,Before the Monday meeting, wanted to provide the latest on the debugging done to try and bootup the DPU , please review the below and suggest if any other option is available to recover the emmc and bootup DPU.
The likely cause seems to be a corrupted partition in emmc as noticed in grub and bootup logs. Are there any other debug/recovery bfb images with nvidia to recover and bootup the DPU.We can discuss more during MondayWe have all doca packages installed/verified (sdk, runtime, tools, host-repo, rshim), minicom console working over pcie rshim (/dev/rshim0/console).
Host: Ubuntu 20.04.3 LTS (GNU/Linux 5.13.0-30-generic x86_64)
Rshim is Active, log belowWe can access the UEFI internal shell and grub prompt. Reading the emmc partitions indicates that the MBR might have got corrupted as per the log below.
grub> ls -l
Device proc: Filesystem type procfs - Sector size 512B - Total size 0KiB
Device hd0: No known filesystem detected - Sector size 512B - Total size
40747008KiB
Partition hd0,gpt2: Filesystem type ext* - Label writable' - Last modification time 2021-10-28 05:43:01 Thursday, UUID 2b8bddba-d258-4b86-94a5-2ca16043857b - Partition start at 52224KiB - Total size        40694767.5KiB              Partition hd0,gpt1: Filesystem type fat - Label system-boot’, UUID
7A48-5C8E - Partition start at 1024KiB - Total size 51200KiB
grub>We tried to boot the Recovery-Mode-Ubuntu(Advanced options for Ubuntu) from the grub, but it failed to boot with same emmc error (error log below). PS: Normal boot of Ubuntu also failed with same error(refer earlier mail)
GNU GRUB  version 2.04We tried to install and boot the latest official bfb published in nvidia website that also fails to boot the DPU (error log below).We tried to boot from grub by manually specifiying the vmlinuz and the initrd images from (hd0,gpt2)/boot/, it fails to boot.
grub> ls
(proc) (hd0) (hd0,gpt2) (hd0,gpt1)
grub> set root=(hd0,gpt2)
grub> ls /boot
efi/ initrd.img vmlinuz.old initrd.img.old vmlinuz grub/
initrd.img-5.4.0-1017.16.gf565efa-bluefield
config-5.4.0-1017.16.gf565efa-bluefield
System.map-5.4.0-1017.16.gf565efa-bluefield
vmlinuz-5.4.0-1017.16.gf565efa-bluefield
grub> linux /boot/vmlinuz-5.4.0-1017.16.gf565efa-bluefield
grub> initrd /boot/initrd.img-5.4.0-1017.16.gf565efa-bluefield
grub> bootThanks
NavinWhenever you see this, it usually means secure boot on the ATF failed.  BL1 which is the root of trust on the chip failed to validate BL2R so it could not load it.This particular card was an early sample card with secure boot enabled and the development keys installed. The signed image available for download on nvidia.com is signed with a different set of keys.  An unsigned BFB was providedPowered by Discourse, best viewed with JavaScript enabled"
4,connecting-to-mlnx-os-via-ssh-and-public-key-authentication,"I’m trying to connect to Mellanox SX6036 InfiniBand switches via SSH and public key authentication so that I don’t have to enter a password, but even after adding the public key of the host that initiates the ssh connection, I’m still asked to enter a password. I’ve added the public key via the following command:ssh client user username authorized-key sshv2 AAAA…When I check with show ssh client the correct key is shown but it looks like it is not used. Is there any way to debug this?
If relevant the switch I’m currently testing on is running  Software Version: PPC_M460EX 3.6.8012 2019-02-22 07:53:42 ppcHi Michael,Use*(config)# ssh client user admin authorized-key sshv2 “key”For example:l-supp-SX6506 [standalone: master] (config) # ssh client user admin authorized-key sshv2 “ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA2E3MMIbVu6vOmp8Sj+6AzKJlB/FgdT3IceDDnLWwbiZxrS0qRLwtyMw68yDm1caR8sDENna/xxcGBOsjRv25M5bAoNpauAiMWC9L/lypUxAbgerV0lc5SC1QsUfVTXvz5zPHSgAuhj7mk9+L4TXyIkEp16SHK8KRw2nZno5Z2YMFYn/a7/LSdRbXZLn4YBgjOdbEI9zUDKOJaqC8FMWzQ4ynyYr5wJ/CuGp2FwUIn2Jb+YyHKbF1h6S5yHAsVSzrgMOs+GcDBuXnjwHbGBu7uExUOKUcsgscFN+MO1B0HZZzNi/jQ+JWffN4JGX+9pqpx53H1u5iX11gj93O+I2JkQ== root@l-supp-05”Use "" show ssh client"" to verifyThanks a lot! Using the whole content of the public key file and enclosing it in double quotes and not just the key itself did the trick.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
5,does-the-switch-support-mac-vlan-functionality,"In the networking environment of mobile office or wireless access, when the user’s access VLAN is fixed, but the access port is not fixed, the MAC VLAN function can achieve that when the user accesses the network using any port of the access device, they can be divided into the same VLAN.The switch information is as follows
model: MSN2700-CS2F
version: 3.10.3004Hello zjf1I’m not sure I understand your question, but I would encourage you to check the Onyx user guide in regards Vlan. I hope you can find the answer there. Otherwise, please paraphrase your question or provide an example from another vendor if that helps explain the requesthttps://docs.nvidia.com/networking/display/Onyxv3102202/VLANsRegardsNVIDIA SupportHello, I have reviewed the link you provided me and it is not what I want.The scenario I want to implement is as follows:When the physical location of the terminal changes, regardless of which interface or switch it is connected to, it can obtain the IP address of the same network segment. Simply put, after the terminal changes location, regardless of which interface it is connected to, it can obtain the address of VLAN10 instead of VLAN20. In this case, MAC-VLAN needs to be used.

1683604438671856×867 95.2 KB
HelloThe document I provided is the user guide to the switch running Onyx NOS. If you have searched through it, and you have not found the feature you need ( MAC VLAN ), then it is not supported.I’m still not sure I understand what you want to implement. Onyx does not support dynamic access vlan allocation.  I don’t know if you are referring to a feature such as the following link (which is also a feature not implemented in Onyx)What is a MAC-based VLAN and how does it work with my managed switch?RegardsHelloSo it seems that this switch is not supported anymore.Yes, it is the function shown in this link. Does your company have a 100G switch that supports this function?HelloThe SN2700 is replaced by the SN3700C
SN3700C advantages:
•	128 x 10/25G are supported. Every 100G port can split to 4 without disabling neighbor ports
•	Larger buffer
•	Enhanced telemetry
•	Faster CPUAll cables/transceivers which were certified on the SN2410/2700 platforms are certified on the SN3420/3700CAs for the VLAN MAC feature, I would encourage you to contact your NVIDIA Sales representative with your use case and deployment size in other to file a Feature Request.Okay, thank you very much for the answerPowered by Discourse, best viewed with JavaScript enabled"
6,slow-iperf-speed-4gbit-between-2-mellanox-connectx-3-vpi-cards-with-40gps-link,"Hello,I have 2 mellanox connectx 3 vpi cards. They have been updated to latest firmware and installed one on centos7 and the other on windows 10.Both cards are on amd threadripper systems with pcie express gen4/3 at 8x or 16x.On windows I use the latest WinOF drivers.On linux I use the inbox drives because I can’t compile the ofed drivers for the 5.10 kernel.The link between the two cards is 40gps, tried on infiniband and ethernet mode.Problem is, even if the link is rated at 40gbs, real speed is 4gbs.What could be the problem?Thank youI had similar issues which are partly resolved but can’t seem to get it reliable with Windows in the mix.-make sure both are running Iperf2 (not 3)-Run with multiple threads (my setup maxes out at ~16-19 gbps though on a single thread)-increase window size to 128M-enable jumbo frames on both (eg 9000)-if you have a switch in between it needs to be set at least as high as the setting on the nodes. (eg 9014)In addition to Ruben’s suggestions:Windows OS Host controller driver for Cloud, Storage and High-Performance computing applications utilizing Mellanox’ field-proven RDMA and Transport OffloadsMellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)I know they have “Ntttcp” fo Linux as well so I would suggest using NTttcp test-toll between the two platformsUse NTTTCP to target the network for testing and minimize the use of other resources that could impact performance.Powered by Discourse, best viewed with JavaScript enabled"
7,100g-connectx-5-slow-speed-between-mac-to-proxmox-truenas-vm,"Hi I’m experiencing slow speed using100g connectX 5 between Mac to Proxmox TrueNAS VM.
My setup is as follows:on the Supermicro server i installed proxmox 7 hypervisor and TrueNAS VM. both mellanox firmware is on the latest version to date and on the same network (no VLANS). both adaptors show 100G link however when i use a iperf & iperf3 test i get only 9G speed between the MAC PRO to the TrueNAS and 27G between VM to VM. i also tried MTU 9000 and PTP bypassing the mikrotik switch but no change.it feels like my speed is capped somewhere.i’m out of ideas hope you can helpThank you
YarivHi All,Following to my OP i did more research looking for a macOS driver on NVIDIA, it doesn’t show it supported, is that correct?however i found a post on twitter that Mellanox Connect -x5 has a driver on the new macOS Ventura see link for the post
https://twitter.com/khronokernel/status/1539658235761655809macOS Ventura location

Screenshot 2023-02-28 at 13.30.221587×475 115 KB
Network

Screenshot 2023-02-28 at 13.32.56708×625 57.6 KB
after upgrading to macOS Ventura i get bit better speed 13G Mac > VM when using iperf2 however i’m unable to setup jambo frames, only max frame size available is 2034! even through terminal get an error that 2034 is the max i can go.
Screenshot 2023-02-28 at 13.37.16714×622 60.2 KB
i called apple they told me to ask NVIDIE although its an apple driver.is it a bug or settings, any idea how can i change / fix it?Thank youHi Yariv,Thank you for contacting us!
At this moment NVIDIA ConnectX Adapters are not supported on the Mac OS platform.
This hardware has not been tested on a Mac OS system and driver it self is an Apple driver.
As we have no knowledge of how this driver has been written and what features or test it has gone through we cannot offer any support on this issue.Thanks and have a great day!
Ilan.Powered by Discourse, best viewed with JavaScript enabled"
8,mellonax-loopback-test,"HiWe tried the loopback test for Mellanox cards, there are two cards with two ports each. Each port from a Ethernet cards are shorted with SFP28 DAC Cable. In Ubuntu OS, the ports are correctly updating as UP and DOWN.
To check the packet transmission we assigned the static ip and try to send the packets using ping command. The packets are transmitting but not receiving.Detail:
Card: NVIDIA® ConnectX®-6 Lx
Cable: 25G SFP+ DAC Cable - Twinax SFP CableLet me know anybody have a suggestionHi vairamhcet,Welcome, and thank you for posting your inquiry to the NVIDIA Developer Forums!With both ports connected in the same system, I assume you are assigning each interface an IP which is on the same subnet.In Linux, when multiple interfaces on a node belong to the same subnet, it can lead to unexpected network behavior (as Linux hosts may respond to incoming packets via a different interface than the packet came in on).In order to work around this situation, it is possible to use advanced routing and rules on the host side to guarantee that the Linux kernel will not accept traffic on a given interface if the destination IP is not defined on the incoming interface.Examples of this type of configuration can be found on the web, for example:
https://access.redhat.com/solutions/30564
https://access.redhat.com/solutions/31284But our recommendation is to use 2 separate systems to perform network testing and throughput measurements, as this type of test case is more representative of real-world capabilities.If you are still unable to make this work after reviewing the above material (and similar articles around the web), we recommend engaging our Enterprise Support team by creating a support case.Best regards,
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
9,whenever-the-machine-restarts-the-single-port-bidirectional-test-rate-will-decrease-by-15-or-an-error-will-be-reported,"MCX516A-CDAT, 100G dual port. On Supermicro 4124-GS-TNR, the general situation is that I use ib_write_bw for single-port bidirectional testing.
The first speed test, 196Gb/s is no problem, then I restarted, it became 165Gb/s directly, and then I restarted it may still be 165Gb/s, but after restarting again, it changed back to 196Gb/s
And when the speed measurement is 165Gb/s, it is often accompanied by ib_write_bw speed measurement stuck. It is stuck after the command is executed, maybe both ports are stuck, or the second port can run
MCX516A-CDAT, 100G dual port. On Supermicro 4124, the general situation is that I use ib_write_bw for single-port bidirectional testing.
The first speed test, 196Gb/s is no problem, then I restarted, it became 165Gb/s directly, and then I restarted it may still be 165Gb/s, but after restarting again, it changed back to 196Gb/s
And when the speed measurement is 165Gb/s, it is often accompanied by ib_write_bw speed measurement stuck. It is stuck after the command is executed, maybe both ports are stuck, or the second port can run
server [d1]:
ib_write_bw -d mlx5_0 -R --run_infinitely --report_gbits
client[d2]:
ib_write_bw -d mlx5_0 -R --run_infinitely -b --report_gbits 192.168.200.25When the test rate is normal, it shows the rate in the screenshot

image1669×1109 184 KB

When it is not normal, the machine port is stuck, unable to test, and then the following error is reported. And what I do is just restart the machine, and then execute mst start

e1ae04897a367d32e59316afded44581722×190 8.9 KB

Of course, it is also possible to test normally after restarting, but the rate will drop to 165Gb/s, and will not increase, and will maintain this rateI uploaded the file running ibdiagnet --pc --pm_pause_time 600 -P all=1 --get_phy_info --get_cable_info and running sysinfo-snapshot.py to the attachment
d1.zip (2.2 KB)
d2.zip (2.2 KB)
sysinfo-snapshot-v3.7.0-d1-20230410-112904.tgz (4.9 MB)
sysinfo-snapshot-v3.7.0-d2-20230410-112901.tgz (5.4 MB)Hello,It seems to be about ARP resolution tables issue due to the fact that you have 2 IPs from the same subnet:enp161s0f0np0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 4200
inet 192.168.200.27  netmask 255.255.255.0  broadcast 192.168.200.255
inet6 fe80::1270:fdff:fe30:b060  prefixlen 64  scopeid 0x20
ether 10:70:fd:30:b0:60  txqueuelen 1000  (Ethernet)
RX packets 6340  bytes 831225 (831.2 KB)
RX errors 0  dropped 5943  overruns 0  frame 0
TX packets 18  bytes 1356 (1.3 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0enp161s0f1np1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 4200
inet 192.168.200.28  netmask 255.255.255.0  broadcast 192.168.200.255
inet6 fe80::1270:fdff:fe30:b061  prefixlen 64  scopeid 0x20
ether 10:70:fd:30:b0:61  txqueuelen 1000  (Ethernet)
RX packets 6342  bytes 831404 (831.4 KB)
RX errors 0  dropped 5944  overruns 0  frame 0
TX packets 19  bytes 1416 (1.4 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0RoCE is affected in the connection establishment phase.The problem:“When a Linux box is connected to a network segment with multiple network cards, a potential problem with the link layer address to IP address mapping can occur. The machine may respond to ARP requests from both Ethernet interfaces. On the machine creating the ARP request, these multiple answers can cause confusion, or worse yet, non-deterministic population of the ARP cache. Known as ARP flux, this can lead to the possibly puzzling effect that an IP migrates non-deterministically through multiple link layer addresses. It’s important to understand that ARP flux typically only affects hosts which have multiple physical connections to the same medium or broadcast domain.” (2.1. Address Resolution Protocol (ARP))In general, when there are 2 interfaces on the same subnet there is no assurance as to which interface will be used to transmit traffic and the machine will accept traffic for either IP on either interface.In some cases Applications which use a specific interface for communication are expecting the same interface to be used on the return path.If you ping with -I dev , attempting to use a given interface, there is no guarantee the reply packet (if there even is one) will come back to the same interface, so pings done with -I dev may not work.RoCE connection establishment is also one of them.Illustration of the problem statement: Avoiding ARP Flux in Multi-Interface Linux HostsExplanation of all the ARP tunables: Chapter 2. Working with sysctl and kernel tunables Red Hat Enterprise Linux 7 | Red Hat Customer PortalExplanation:OS/kernel handles the arp resolution and routing tables and algorithms. When looking at standard routing table, without additional setting or effort, we will see several entries with same IP subnet pointing to different interfaces.OS will route the packets to chosen default interface without additional setting. Usually that interface will be the upper most in the routing table.In order to prevent from wrong arp resolution there is a need in source based routing configuration + correct sysctl settings.Or, even better- to use IPs from different subnets on each server (meaning first port on one subnet and second port on another subnet).Without those, arp might not be resolved at all on specific route or will be resolved on wrong interface.Even if arp table is populated correctly, but the actual handshake that is done for RDMA is done on a wrong interface due to wrong routing of standard IP packets, it might result in RDMA being issued on wrong interface and thus eventually in actual failures.Best Regards,
VikiI understand, I will try to reset according to what you said, but I still have a problem, sometimes after restarting, the speed test is 165Gb/s. why is this

image1623×471 147 KB
Hi, make sure that all the CPUs are free and that the previous job finished completely before running a new job.
Also, try to run with more queues (-q 2) or use taskset to bind the sender to close NUMA cores.Best Regards,
VikiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
10,issues-with-assigning-64-pkeys-for-each-server-in-cluster,"I am seeking help with an issue I am experiencing while working on a cluster using ConnectX-5 devices. I want to assign 64 PKeys for each server, but I’m running into some problems. Here’s what I have done so far:I would appreciate any guidance or suggestions to resolve this issue and successfully assign 64 PKeys for each server in the cluster. Thank you in advance for your assistance!Hi Gesrua,
You can use #smpquery pkeytables  .
Then you can find switch port only support 32 pkeytables.
Thanks,My switch is QM8790. I failed to find the limitation of 32 pkeys on any documents.I tried to use #smpquery, but the result is not as expected.There are only 8 pkeys capacity. However, in my case, 32 pkeys also seem to work.Hi,
you need use  #smpquery  pkeytables  
Not only with switch lid.Thanks,Sorry, I’m still confusing.I tried to run smpquery pkeytables / smpquery pkeytable. It didn’t work.I think it is because 1. pkeytables should be pkeytable, 2. there must be a <dest dr_path|lid|guid>.I cannot get your point. Do you mean the smpquery command needs to run on the switch? However, QM8790 is a externally managed switch. It seems that I cannot login to the switch and execute commands.Please see the result in my lab:
Anyhow,  switch port can support 32 pkeys.
[root@yl pkeys]# smpquery pkeytable 40 1
0: 0xffff 0x0000 0x0000 0X0000 0x0000 0x0000 0x0000 0x0000
8: 0x000 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000
16: 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000
24: 0x0000 0x0090 0x0009 0x0000 0x0000 0x0000 0x0000 0x0000
32 pkeys capacity for this portThis is because you are querying without a port indication – causing you to query the internal SMA port (aka port0) which holds 8 pkeys.Add a port indication and you’ll see the 32 pkeys…Example for the comment below:No port indication0: 0xffff 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x00008 pkeys capacity for this portIncl. port indication (port #1 in this case)0: 0xffff 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x00008: 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x000016: 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x000024: 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x0000 0x000032 pkeys capacity for this portThanks! I query with a port indication and get the correct result!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
11,mtu-size-on-sx6036,"I have an SX6036 and am using it between 2 windows 2016 servers as an Oracle RAC interconnect. I am having problems that might be related to the MTU size. I’m not 100% sure.Windows Event Viewer shows this error:“According to the configuration under the “Jumbo Packets” advanced property, the MTU configured for device Mellanox ConnectX-3 IPoIB Adapter #2 is 4092. The effective MTU is the supplied value + 4 bytes (for the IPoIB header). This configuration exceeds the MTU reported by OpenSM, which is 2048. This inconsistency may result in communication failures. Please change the MTU of IPoIB or OpenSM, and restart the driver.”So when login to the SX6036 and set the MTU to 2048, I get this error:“Error - valid MTU values for ipoib are 252, 508, 1020, 2044, 4092”What’s the solution for this? Can the network adapter be changed? Not sure what to do…I think I figured it out. I set the Jumbo Packet size (in Properties) to 2044 and in the switch to 2044 and the error message went away in the Event Viewer.I’m not yet sure if this has solved the problems.Hello McGinity,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on your update, your were able to align the MTU on the node on the default IPoIB MTU from the switch.If you want to increase the network performance, you can also change the IPoIB MTU on the switch to 4K and align the node to 4K.You can use following CLI syntax on the switch to configure the default IPoIB MTU to 4K:ib-sw [standalone: master] > enableib-sw [standalone: master] # configure terminalib-sw [standalone: master] (config) # ib partition Default mtu 4K forceib-sw [standalone: master] (config) # configuration writeib-sw [standalone: master] (config) # exitThank you and regards,~NVIDIA Networking Technical SupportYes, thanks for checking in. I got the MTU set and I also found the Mellanox WinMFT utility which updated the drivers. I am building a new Oracle RAC installation and chose IPoIB for the interconnect. I set this up on another server many years ago and it has been working nicely. In that installation, my switch did not run the subnet manager so I had to install it on the servers. This time, the switch has it.In that installation, I disabled the “Large Send Offload V2” for IPv4/6 for some reason. So I did the same in this new build. Also the in the Tunning Tab, I set it “Default”.Things seem to be working.I’m not sure if things are 100% ok. The MTU in the swtich and cards all match. I’m not sure if Oracle needs a partiular MTU size or the issues I’m seeing are unrelated. Looking at Oracle’s notes, they suggest an MTU size of 9000. The SX6036 cannot support this. If I turn off the Subnet Manager in the swtich and use the version which installs as a Windows Service can it support 9000?I have another question. Right now ib0 State MTU is 2044. When I try to set to 4092, it errors saying:“Error setting MTU for interface ib0: RTNETLINK answers: Invalid argument”I thought maybe it was because it needed updating so it is now update to PC_M460EX 3.6.8010 2018-08-20 18:04:16 ppc.I just clicked the “Expand Post” link and read your full answer. (I missed that link. )After running:ib-sw [standalone: master] > enableib-sw [standalone: master] # configure terminalib-sw [standalone: master] (config) # ib partition Default mtu 4K forceib-sw [standalone: master] (config) # configuration writeib-sw [standalone: master] (config) # exitib0 the MTU is now in 4092. I ran it via the web API in “System - Configuration - Execute CLI commands”I guess there is something not working when just doing it from “System-Interfaces”. Anyway, all good. I also made sure the 2 ports in “Ports” are set to 4k. And in the Windows, Adapter’s “Jumbo Packet” (in Properties - Configure - Advanced) is set to 4092.It looks like everything is working. Mellanox Infiniband is awesome!Powered by Discourse, best viewed with JavaScript enabled"
12,mellanox-sn3420-100gb-ports-amber-led-after-update-to-culumus-linux-ver-5-4,"Hi friends!
Got visual bug with 100gb ports after update to Cumulus ver.5.4.0
After update and reboot all services stopped and started Ok.
But after hard power off/On all updated Switches show strange solid amber indication on ports 100Gb.
Look at screen:
asfdgtrtf1131×524 96 KB
All services also started ok. No issues.
Ports have not yet been connected and not checked.
Confuses this chaotic garland of port indication…
What’s wrong?Hello! You upgraded to CL 5.4.0 but this sounds like a new install since no ports are connected yet, what version did you upgrade from and was it a package upgrade via ‘apt’ or an image upgrade via ‘onie-install’?Can you provide the output of net show interface or nv show interface and we can see if those ports are broken out or not? Also, for one of those ports showing amber LED please issue sudo l1-show swpXXsYYHello!
Flashed image - cumulus-linux-5.4.0-mlx-amd64.bin
Output come soon…
Screen before flashing:

34201216×618 60 KB
Hello!Captured output:
Linux cumulus 5.10.0-cl-1-amd64 #1 SMP Debian 5.10.162-1+cl5.4.0u1 (2023-01-20)                                                                                                                                    x86_64Welcome to NVIDIA Cumulus (R) Linux (R)ZTP in progress. To disable, do ‘ztp -d’cumulus@cumulus:mgmt:~$  net show interface
State  Name   Spd  MTU    Mode      LLDP  SummaryUP     lo     N/A  65536  Loopback        IP: 127.0.0.1/8
lo                                 IP: ::1/128
DN     eth0   N/A  1500   Mgmt            Master: mgmt(UP)
DN     swp1   N/A  9216   Default
DN     swp2   N/A  9216   Default
DN     swp3   N/A  9216   Default
DN     swp4   N/A  9216   Default
DN     swp5   N/A  9216   Default
DN     swp6   N/A  9216   Default
DN     swp7   N/A  9216   Default
DN     swp8   N/A  9216   Default
DN     swp9   N/A  9216   Default
DN     swp10  N/A  9216   Default
DN     swp11  N/A  9216   Default
DN     swp12  N/A  9216   Default
DN     swp13  N/A  9216   Default
DN     swp14  N/A  9216   Default
DN     swp15  N/A  9216   Default
DN     swp16  N/A  9216   Default
DN     swp17  N/A  9216   Default
DN     swp18  N/A  9216   Default
DN     swp19  N/A  9216   Default
DN     swp20  N/A  9216   Default
DN     swp21  N/A  9216   Default
DN     swp22  N/A  9216   Default
DN     swp23  N/A  9216   Default
DN     swp24  N/A  9216   Default
DN     swp25  N/A  9216   Default
DN     swp26  N/A  9216   Default
DN     swp27  N/A  9216   Default
DN     swp28  N/A  9216   Default
DN     swp29  N/A  9216   Default
DN     swp30  N/A  9216   Default
DN     swp31  N/A  9216   Default
DN     swp32  N/A  9216   Default
DN     swp33  N/A  9216   Default
DN     swp34  N/A  9216   Default
DN     swp35  N/A  9216   Default
DN     swp36  N/A  9216   Default
DN     swp37  N/A  9216   Default
DN     swp38  N/A  9216   Default
DN     swp39  N/A  9216   Default
DN     swp40  N/A  9216   Default
DN     swp41  N/A  9216   Default
DN     swp42  N/A  9216   Default
DN     swp43  N/A  9216   Default
DN     swp44  N/A  9216   Default
DN     swp45  N/A  9216   Default
DN     swp46  N/A  9216   Default
DN     swp47  N/A  9216   Default
DN     swp48  N/A  9216   Default
DN     swp49  N/A  9216   Default
DN     swp50  N/A  9216   Default
DN     swp51  N/A  9216   Default
DN     swp52  N/A  9216   Default
DN     swp53  N/A  9216   Default
DN     swp54  N/A  9216   Default
DN     swp55  N/A  9216   Default
DN     swp56  N/A  9216   Default
DN     swp57  N/A  9216   Default
DN     swp58  N/A  9216   Default
DN     swp59  N/A  9216   Default
DN     swp60  N/A  9216   Default
UP     mgmt   N/A  65575  VRF             IP: 127.0.0.1/8
mgmt                               IP: ::1/128cumulus@cumulus:mgmt:~$  nv show interface
Interface  State  Speed  MTU    Type      Remote Host  Remote Port  Summary
---------  -----  -----  -----  --------  -----------  -----------  -----------…
eth0       down          1500   eth
lo         up            65536  loopback                            IP Address:
127.0.0.1/8
IP Address:
::1/128
mgmt       up            65575  vrf                                 IP Address:
127.0.0.1/8
IP Address:
::1/128
swp1       down          9216   swp
swp2       down          9216   swp
swp3       down          9216   swp
swp4       down          9216   swp
swp5       down          9216   swp
swp6       down          9216   swp
swp7       down          9216   swp
swp8       down          9216   swp
swp9       down          9216   swp
swp10      down          9216   swp
swp11      down          9216   swp
swp12      down          9216   swp
swp13      down          9216   swp
swp14      down          9216   swp
swp15      down          9216   swp
swp16      down          9216   swp
swp17      down          9216   swp
swp18      down          9216   swp
swp19      down          9216   swp
swp20      down          9216   swp
swp21      down          9216   swp
swp22      down          9216   swp
swp23      down          9216   swp
swp24      down          9216   swp
swp25      down          9216   swp
swp26      down          9216   swp
swp27      down          9216   swp
swp28      down          9216   swp
swp29      down          9216   swp
swp30      down          9216   swp
swp31      down          9216   swp
swp32      down          9216   swp
swp33      down          9216   swp
swp34      down          9216   swp
swp35      down          9216   swp
swp36      down          9216   swp
swp37      down          9216   swp
swp38      down          9216   swp
swp39      down          9216   swp
swp40      down          9216   swp
swp41      down          9216   swp
swp42      down          9216   swp
swp43      down          9216   swp
swp44      down          9216   swp
swp45      down          9216   swp
swp46      down          9216   swp
swp47      down          9216   swp
swp48      down          9216   swp
swp49      down          9216   swp
swp50      down          9216   swp
swp51      down          9216   swp
swp52      down          9216   swp
swp53      down          9216   swp
swp54      down          9216   swp
swp55      down          9216   swp
swp56      down          9216   swp
swp57      down          9216   swp
swp58      down          9216   swp
swp59      down          9216   swp
swp60      down          9216   swp
cumulus@cumulus:mgmt:~$ sudo suWe trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:[sudo] password for cumulus:
root@cumulus:mgmt:/home/cumulus# l1-show swp1l1-show swp12
l1-show swp13
l1-show swp14
l1-show swp15
l1-show swp16
l1-show swp17
l1-show swp18
l1-show swp19
l1-show swp20
l1-show swp21
l1-show swp22
l1-show swp23
l1-show swp24
l1-show swp25
l1-show swp26
l1-show swp27
l1-show swp28
l1-show swp29
l1-show swp30
l1-show swp31
l1-show swp32
l1-show swp33
l1-show swp34
l1-show swp35
l1-show swp36
l1-show swp37
l1-show swp38
l1-show swp39
l1-show swp40
l1-show swp41
l1-show swp42
l1-show swp43
l1-show swp44
l1-show swp45
l1-show swp46
l1-show swp47
l1-show swp48
l1-show swp49
l1-show swp50
l1-show swp51
l1-show swp52
l1-show swp53
l1-show swp54
l1-show swp55
l1-show swp56
l1-show swp57
l1-show swp58
l1-show swp59
l1-show swp60
Port:  swp1
Module Info
Vendor Name: None                   PN: None
Identifier: None                    Type:
Configured State
Admin: Admin Up     Speed: Unknown! MTU: 9216
Autoneg: On                         FEC: Auto
Operational State
Link Status: Kernel: Down           Hardware: Down
Speed: Kernel: Unknown!             Hardware: N/A
Autoneg: On (Autodetect enabld)     FEC: None (down)
TX Power (mW): None
RX Power (mW): None
Topo File Neighbor: None, None
LLDP Neighbor:      None, None
Port Hardware State:
Compliance Code: N/A
Cable Type: N/A
Speed: N/A                          Autodetect: Enabled
Eyes: N/A                           Grade: 0
Troubleshooting Info: Cable is unplugged.Troubleshooting Info: Cable is unplugged.It seems to me that there are no problems,  just need to check the status of port by connecting the host.Hi Alex,Can you gather the output of sudo l1-show swp49 ? I see the output for swp1 but not the QSFP ports (swp49-60).If you don’t see anything in the l1-show for swp49, can you plug in an optic and without connecting a cable, see if the LEDs change? If they do not change, try connecting the cable to another port on the same or different switch.Hello Rmckenna,
u38.txt (60.6 KB)
root@cumulus:mgmt:/home/cumulus# l1-show swp49
Port:  swp49
Module Info
Vendor Name: None                   PN: None
Identifier: None                    Type:
Configured State
Admin: Admin Up     Speed: Unknown! MTU: 9216
Autoneg: On                         FEC: Auto
Operational State
Link Status: Kernel: Down           Hardware: Down
Speed: Kernel: Unknown!             Hardware: N/A
Autoneg: On (Autodetect enabld)     FEC: None (down)
TX Power (mW): None
RX Power (mW): None
Topo File Neighbor: None, None
LLDP Neighbor:      None, None
Port Hardware State:
Compliance Code: N/A
Cable Type: N/A
Speed: N/A                          Autodetect: Enabled
Eyes: N/A, N/A, N/A, N/A            Grade: 0, 0, 0, 0
Troubleshooting Info: Cable is unplugged.
None
root@cumulus:mgmt:/home/cumulus# l1-show swp50
Port:  swp50
Module Info
Vendor Name: None                   PN: None
Identifier: None                    Type:
Configured State
Admin: Admin Up     Speed: Unknown! MTU: 9216
Autoneg: On                         FEC: Auto
Operational State
Link Status: Kernel: Down           Hardware: Down
Speed: Kernel: Unknown!             Hardware: N/A
Autoneg: On (Autodetect enabld)     FEC: None (down)
TX Power (mW): None
RX Power (mW): None
Topo File Neighbor: None, None
LLDP Neighbor:      None, None
Port Hardware State:
Compliance Code: N/A
Cable Type: N/A
Speed: N/A                          Autodetect: Enabled
Eyes: N/A, N/A, N/A, N/A            Grade: 0, 0, 0, 0
Troubleshooting Info: Cable is unplugged.
NoneHi Community,Is there anyone here who encountered the same problem on the indication?hi Alex,What was the result after you plugged in the optics/cables?Hi Rmckenna,I’m not yet able to connect to the host to test.
So I see that the logs look good?Hi Rmckenna,Connected cable both ends into ports,  looks good, if the cable is disconnected the amber indication returns…
photo_2023-05-12_17-41-541280×720 120 KB

Cumulus.txt (5.6 KB)cumulus login: cumulus
Password:cumulus@cumulus:mgmt:~$ nv show  interface swp49
operational        appliedtype                      swp                swp
[acl]
evpn
multihoming
uplink                                   off
lldp
dcbx-ets-config-tlv     off
dcbx-ets-recomm-tlv     off
dcbx-pfc-tlv            off
[neighbor]              cumulus
ptp
enable                                     off
router
adaptive-routing
enable                                   off
ospf
enable                                   off
ospf6
enable                                   off
pbr
[map]
pim
enable                                   off
synce
enable                                     off
ip
igmp
enable                                   off
ipv4
forward                                  on
ipv6
enable                                   on
forward                                  on
neighbor-discovery
enable                                   on
[dnssl]
home-agent
enable                                 off
[prefix]
[rdnss]
router-advertisement
enable                                 on
fast-retransmit                        on
hop-limit                              64
interval                               600000
interval-option                        off
lifetime                               1800
managed-config                         off
other-config                           off
reachable-time                         0
retransmit-time                        0
router-preference                      medium
vrrp
enable                                   off
vrf                                        default
[gateway]
link
auto-negotiate          on                 on
duplex                  full               full
speed                   100G               auto
fec                     auto               auto
mtu                     9216               9216
[breakout]                                 1x
state                   up                 up
stats
carrier-transitions   2
in-bytes              2.52 KB
in-drops              0
in-errors             0
in-pkts               14
out-bytes             2.52 KB
out-drops             0
out-errors            0
out-pkts              14
mac                     9c:05:91:32:6b:10
pluggable
identifier              QSFP28
vendor-name             Mellanox
vendor-pn               MFA1A00-C010
vendor-rev              B3
vendor-sn               MT2311FT00443
ifindex                   52
cumulus@cumulus:mgmt:~$ nv show  interface swp57
operational        appliedtype                      swp                swp
[acl]
evpn
multihoming
uplink                                   off
lldp
dcbx-ets-config-tlv     off
dcbx-ets-recomm-tlv     off
dcbx-pfc-tlv            off
[neighbor]              cumulus
ptp
enable                                     off
router
adaptive-routing
enable                                   off
ospf
enable                                   off
ospf6
enable                                   off
pbr
[map]
pim
enable                                   off
synce
enable                                     off
ip
igmp
enable                                   off
ipv4
forward                                  on
ipv6
enable                                   on
forward                                  on
neighbor-discovery
enable                                   on
[dnssl]
home-agent
enable                                 off
[prefix]
[rdnss]
router-advertisement
enable                                 on
fast-retransmit                        on
hop-limit                              64
interval                               600000
interval-option                        off
lifetime                               1800
managed-config                         off
other-config                           off
reachable-time                         0
retransmit-time                        0
router-preference                      medium
vrrp
enable                                   off
vrf                                        default
[gateway]
link
auto-negotiate          on                 on
duplex                  full               full
speed                   100G               auto
fec                     auto               auto
mtu                     9216               9216
[breakout]                                 1x
state                   up                 up
stats
carrier-transitions   2
in-bytes              2.52 KB
in-drops              0
in-errors             0
in-pkts               14
out-bytes             2.52 KB
out-drops             0
out-errors            0
out-pkts              14
mac                     9c:05:91:32:6b:48
pluggable
identifier              QSFP28
vendor-name             Mellanox
vendor-pn               MFA1A00-C010
vendor-rev              B3
vendor-sn               MT2311FT00443
ifindex                   60
cumulus@cumulus:mgmt:~$Hi Alex,Thanks for the info! The ports don’t seem to have an issue coming online but that’s still strange. I don’t see anything wrong with the port from a software level, and no errors logged from hardware that we can see. You should open a support case for this issue so we can analyze the logs deeper and possibly RMA the switch if it’s a hardware error with the port LEDs.Powered by Discourse, best viewed with JavaScript enabled"
13,unable-to-identify-interface-for-packet-capture-on-the-mellanox-switch,"I am using tcpdump for capturing packets. I see the following options for interfaces on the switch.1.lo
2.mgmt0
3.mgmt1
4.mgmts0
5.mgmts1
6.vlan1I was trying to capture at vlan1 using the following command “tcpdump -w switch.pcap --time-stamp-precision=nano -s70 -i  vlan1” . But it only captures the ARP packets and not all the packets passing through the interface.I also try to capture at one of the ports “ethernet 1/1” using the command in the reference:
“Mellanox Interconnect Community”. But then it gives me the following error.ualloc-mlnx2 [standalone: master] (config) # tcpdump -i  eth1.0
tcpdump: eth1.0: No such device exists
(SIOCGIFHWADDR: No such device)Although I have the router port (see below)ualloc-mlnx2 [standalone: master] (config) # show interfaces ethernet 1/1Eth1/1:
Admin state                      : Enabled
Operational state                : Up
Last change in operational status: 3d and 12:21:50 ago (1 oper change)
Boot delay time                  : 0 sec
Description                      : N\A
Mac address                      : ec:0d:9a:5f:d8:3c
MTU                              : 1500 bytes (Maximum packet size 1522 bytes)
Fec                              : auto
Flow-control                     : receive off send off
Actual speed                     : 10 Gbps
Auto-negotiation                 : Disabled
Width reduction mode             : Unknown
Switchport mode                  : hybrid
MAC learning mode                : Enabled
Forwarding mode                  : inherited cut-throughCan you please tell me how can I capture all other packets on the switch?Thank you,
ParidhikaPlease review Cumulus Linux documentation on tcpdump here.tcpdump only captures control plane traffic.If you need to capture all traffic – use SPAN/ERSPAN.Regards,JonPowered by Discourse, best viewed with JavaScript enabled"
14,need-to-upgrade-nclu-command-line-select-local-file-from-switch-image-new-version,"Kindly share command line for select local file from switch image new versionHow to image file from ob server to switchHello Ramulu,I believe you are asking about CL 5.0, if so, please check the below sections of its user guide :Install-using-a-local-file: Installing a New Cumulus Linux Image | Cumulus Linux 5.0Installing-a-New-Cumulus-Linux-Image: Installing a New Cumulus Linux Image | Cumulus Linux 5.0Thanks,NVIDIA Enterprise Support / YousefPowered by Discourse, best viewed with JavaScript enabled"
15,doca-gpu-packet-processing-example,"Hi,
There is a problem when I try to run the GPU Packet Processing example in DOCA 2.0.
The topology is attached, I use a server with 2 NICs connected through an external cable.When I tried to run :
sudo ./doca_gpu_packet_processing -g b1:00.0 -n 31:00.1 -q 2 in terminal 1(the pcie address is where the NIC port ens786f1np1, assigned IP 192.168.202.10 locates)and ping the 192.168.202.10 from another NIC’s port ens785f0, 192.168.202.20.The result indicates the DOCA does not pass the ICMP message to GPU.Can anyone help?
Thanks a lot.
Solved myself by separating them into different namespaces.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
16,expanding-the-menus-in-documentation,"Q: As an avid fan of the docs I’m really happy with the snappy and modern website but something itches me. Whenever I’m using the docs I’m getting frustrated by the fact I can’t expand the menu items on the left without requesting the specific page/item. It would be awesome if the little > (greater than sign) only expands the menu instead of expanding + requesting the page. I’m no web developer or UI expert but it feels inefficient.A: It turns out you can expand the menu items without clicking the links (at least for Chrome and Firefox on MacOS) – just click the arrow. If that doesn’t work, try updating your browser.Powered by Discourse, best viewed with JavaScript enabled"
17,ips-example-application-failure-argument-handler,"Hi,I’m unable to get the DOCA IPS example application running on my Bluefield-2 DPU (NVIDIA SKU: 900-9D206-0063-ST2, DOCA: 2.0.2, BSP: 4.0.2, Firmware: 24.35.2000).After building the IPS application, compiling the rule file, and setting up the two scalable functions, I receive the following error when attempting to execute the IPS application.Of note, I am using the command specified from DOCA SDK IPS Application Documentation, but it seems the argument handler fails to parse the input correctly.Thanks in advance for any assistance.root@bluefield2DPU:/opt/mellanox/doca/applications/ips/bin# /opt/mellanox/doca/applications/ips/bin/doca_ips -a 0000:03:00.0,class=regex -a auxiliary:mlx5_core.sf.4,sft_en=1 -a auxiliary:mlx5_core.sf.5,sft_en=1 – --cdo ./compiler_output.cdo -p
EAL: Detected CPU lcores: 8
EAL: Detected NUMA nodes: 1
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: Probe PCI driver: mlx5_pci (15b3:a2d6) device: 0000:03:00.0 (socket -1)
TELEMETRY: No legacy callbacks, legacy socket not created
[03:57:03:771402][DOCA][ERR][ARGP:803]: Parameter “pci-addr” is mandatoryUsage: doca_ips [DPDK Flags] – [DOCA Flags] [Program Flags]DOCA Flags:
-h, --help                        Print a help synopsis
-v, --version                     Print program version information
-l, --log-level                   Set the log level for the program <CRITICAL=20, ERROR=30, WARNING=40, INFO=50, DEBUG=60>Program Flags:
-p, --print-match                 Prints FID when matched in DPI engine
-n, --netflow <source_id>         Collect netflow statistics and set source_id if value is set
-o, --output-csv            Path to the output of the CSV file
-c, --cdo                   Path to CDO file compiled from a valid PDD
-f, --fragmented                  Enables processing fragmented packets
-a, --pci-addr                    DOCA DPI device PCI address[03:57:03:771518][DOCA][ERR][IPS:86]: Failed to parse application input: Invalid inputHi,I was able to resolve the error. I needed to pass the DPU’s PCI address twice as an argument to the DOCA IPS application: once for DPDK and again for the DOCA IPS application (see usage structure below).I suggest the IPS application documentation mention this in their example command to run the application in case others run into similiar issues.Usage: doca_ips [DPDK Flags] -- [DOCA Flags] [Program Flags]Please see below for the final command I used to execute the IPS application./opt/mellanox/doca/applications/ips/bin/doca_ips -a 0000:03:00.0,class=regex -a auxiliary:mlx5_core.sf.4,sft_en=1 -a auxiliary:mlx5_core.sf.5,sft_en=1 -- -a 03:00.0 --cdo ./compiler_output.cdo -pPowered by Discourse, best viewed with JavaScript enabled"
18,rdma-not-working-with-connectx-6,"I am trying to get RoCEv2 working. I have two ConnectX-6 NICs (Mellanox Technologies MT2892 Family) connected to each other with an Ethernet cable. The firewall service is disabled.Using ibv_devinfo, ifconfig, show_gids, modinfo, lspci and ibdev2netdev all show that the NICs are operational and mounted to the correct drivers:But the following commands are not working: rping, ib_send_bw, ibv_rc_pingpong, qperf.When I use rping, it shows the following message:[root@localhost perftest]# rping -c -a 10.1.2.251 -C 10 -vcma event RDMA_CM_EVENT_ADDR_ERROR, error -19waiting for addr/route resolution state 1When I use ib_send_bw, I get the following on the client:[root@localhost perftest]# ./ib_send_bw 10.1.2.251Send BW TestDual-port : OFF Device : mlx5_0Number of qps : 1 Transport type : IBConnection type : RC Using SRQ : OFFPCIe relax order: ONibv_wr* API : ONTX depth : 128CQ Moderation : 1Mtu : 1024[B]Link type : EthernetGID index : 3Max inline data : 0[B]rdma_cm QPs : OFFData ex. method : EthernetFailed to modify QP 182 to RTRUnable to Connect the HCA’s through the link…and the following on the server:[root@localhost perftest]# ./ib_send_bwSend BW TestDual-port : OFF Device : mlx5_0Number of qps : 1 Transport type : IBConnection type : RC Using SRQ : OFFPCIe relax order: ONibv_wr* API : ONRX depth : 512CQ Moderation : 1Mtu : 1024[B]Link type : EthernetGID index : 3Max inline data : 0[B]rdma_cm QPs : OFFData ex. method : Ethernetethernet_read_keys: Couldn’t read remote addressUnable to read to socket/rdma_cmFailed to exchange data between server and clientsIf I pass the -R commandline option (to use RDMA), I get a different error message:[root@localhost perftest]# ./ib_send_bw -R 10.1.2.251Received 10 times ADDR_ERRORUnable to perform rdma_client functionUnable to init the socket connectionibv_rc_pingpong reports something very different:[root@localhost perftest]# ibv_rc_pingponglocal address: LID 0x0000, QPN 0x0000b7, PSN 0xd05bde, GID ::Failed to modify QP to RTRCouldn’t connect to remote QP[root@localhost perftest]# ibv_rc_pingpong 10.1.2.251local address: LID 0x0000, QPN 0x0000b7, PSN 0x0cf37d, GID ::client read/write: No space left on deviceCouldn’t read/write remote addressqperf also reports an issue with RDMA address lookup:[root@localhost perftest]# qperf -cm1 10.1.2.251 rc_bwrc_bw:unexpected event from RDMA CM: Address errorexpecting: Address resolvedMy question: are there any additional steps I am missing for configuration? I am using a RHEL 8.3 system with OFED @ MLNX_OFED_LINUX-5.5-1.0.3.2 (OFED-5.5-1.0.3).Here’s some additional information:modinfo:[root@localhost perftest]# modinfo mlx5_corefilename: /lib/modules/4.18.0-348.7.1.el8_5.x86_64/extra/mlnx-ofa_kernel/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.koifconfig:[root@localhost perftest]# ifconfigenp1s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500inet 10.1.2.250 netmask 255.0.0.0 broadcast 10.255.255.255ether b8:ce:f6:30:10:3a txqueuelen 1000 (Ethernet)RX packets 21 bytes 2934 (2.8 KiB)RX errors 0 dropped 0 overruns 0 frame 0TX packets 23 bytes 1764 (1.7 KiB)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@localhost perftest]# ifconfigenp1s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500inet 10.1.2.251 netmask 255.0.0.0 broadcast 10.255.255.255ether b8:ce:f6:30:10:3a txqueuelen 1000 (Ethernet)RX packets 21 bytes 2934 (2.8 KiB)RX errors 0 dropped 0 overruns 0 frame 0TX packets 23 bytes 1764 (1.7 KiB)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0ibv_devinfo:hca_id: mlx5_0transport: InfiniBand (0)fw_ver: 22.32.1010node_guid: b8ce:f603:0030:1044sys_image_guid: b8ce:f603:0030:1044vendor_id: 0x02c9vendor_part_id: 4125hw_ver: 0x0board_id: MT_0000000709phys_port_cnt: 1port: 1state: PORT_ACTIVE (4)max_mtu: 4096 (5)active_mtu: 1024 (3)sm_lid: 0port_lid: 0port_lmc: 0x00link_layer: Ethernetibdev2netdev:mlx5_0 port 1 ==> enp1s0 (Up)show_gids:[root@localhost perftest]# show_gidsDEV PORT INDEX GID IPv4 VER DEVmlx5_0 1 0 fe80:0000:0000:0000:bace:f6ff:fe30:1044 v1 enp1s0mlx5_0 1 1 fe80:0000:0000:0000:bace:f6ff:fe30:1044 v2 enp1s0mlx5_0 1 2 0000:0000:0000:0000:0000:ffff:0a01:02fb 10.1.2.251 v1 enp1s0mlx5_0 1 3 0000:0000:0000:0000:0000:ffff:0a01:02fb 10.1.2.251 v2 enp1s0n_gids_found=4DEV PORT INDEX GID IPv4 VER DEVmlx5_0 1 0 fe80:0000:0000:0000:bace:f6ff:fe30:103a v1 enp1s0mlx5_0 1 1 fe80:0000:0000:0000:bace:f6ff:fe30:103a v2 enp1s0mlx5_0 1 2 0000:0000:0000:0000:0000:ffff:0a01:02fa 10.1.2.250 v1 enp1s0mlx5_0 1 3 0000:0000:0000:0000:0000:ffff:0a01:02fa 10.1.2.250 v2 enp1s0n_gids_found=4Hello,Based on the information provided, there are a number of syntax and requirements not met when you had attempted to run these commands.For more information on the perftest collection, as well as the individual utilities themselves, please see the following community post:https://community.mellanox.com/s/article/perftest-packageAdditionally, more information on qperf and rping can be found here:https://community.mellanox.com/s/article/MLNX2-117-1581knhttps://community.mellanox.com/s/article/MLNX2-117-2807knqperf measures bandwidth and latency between two nodes. It can work over TCP/IP as well as the RDMA transports. On one of the nodes, qperf is typically run ...https://man7.org/linux/man-pages/man1/rping.1.htmlFor usage examples, please see the following output for each command as run from our lab systems.Regarding rping, you will need to specify a server instance of rping before running it on the client side.rping Server Side:# rping -s -a 10.10.10.10rping Client Side:# rping -c -a 10.10.10.10 -C 10 -vping data: rdma-ping-0: ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrping data: rdma-ping-1: BCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrsping data: rdma-ping-2: CDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstping data: rdma-ping-3: DEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuping data: rdma-ping-4: EFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvping data: rdma-ping-5: FGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwping data: rdma-ping-6: GHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxping data: rdma-ping-7: HIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyping data: rdma-ping-8: IJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyzping data: rdma-ping-9: JKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyzAclient DISCONNECT EVENT…To run ibv_rc_pingpong, in addition to running and specifying the client and server, you will need to specify the GID with the -g flag. We will provide the lab systems’ GID output for clarity in this example and others that require it.ibv_rc_pingpong Server side:# show_gidsDEV PORT INDEX GID IPv4 VER DEV— ---- ----- — ------------ — —mlx5_0 1 0 fe80:0000:0000:0000:9a03:9bff:fe13:f21c v1 enp129s0f0mlx5_0 1 1 fe80:0000:0000:0000:9a03:9bff:fe13:f21c v2 enp129s0f0mlx5_1 1 0 fe80:0000:0000:0000:9a03:9bff:fe13:f21d v1 enp129s0f1mlx5_1 1 1 fe80:0000:0000:0000:9a03:9bff:fe13:f21d v2 enp129s0f1mlx5_1 1 2 0000:0000:0000:0000:0000:ffff:0a0a:0a0a 10.10.10.10 v1 enp129s0f1mlx5_1 1 3 0000:0000:0000:0000:0000:ffff:0a0a:0a0a 10.10.10.10 v2 enp129s0f1n_gids_found=6# ibv_rc_pingpong -d mlx5_1 -g 3local address: LID 0x0000, QPN 0x000191, PSN 0xbba026, GID ::ffff:10.10.10.10ibv_rc_pingpong Client side:# show_gidsDEV PORT INDEX GID IPv4 VER DEV— ---- ----- — ------------ — —mlx5_0 1 0 fe80:0000:0000:0000:9a03:9bff:fe13:f414 v1 enp129s0f0mlx5_0 1 1 fe80:0000:0000:0000:9a03:9bff:fe13:f414 v2 enp129s0f0mlx5_1 1 0 fe80:0000:0000:0000:9a03:9bff:fe13:f415 v1 enp129s0f1mlx5_1 1 1 fe80:0000:0000:0000:9a03:9bff:fe13:f415 v2 enp129s0f1mlx5_1 1 2 0000:0000:0000:0000:0000:ffff:0a0a:0a0c 10.10.10.12 v1 enp129s0f1mlx5_1 1 3 0000:0000:0000:0000:0000:ffff:0a0a:0a0c 10.10.10.12 v2 enp129s0f1n_gids_found=6# ibv_rc_pingpong -d mlx5_1 -g 3 10.10.10.10local address: LID 0x0000, QPN 0x000191, PSN 0x846386, GID ::ffff:10.10.10.12remote address: LID 0x0000, QPN 0x000191, PSN 0xbba026, GID ::ffff:10.10.10.108192000 bytes in 0.01 seconds = 8031.37 Mbit/sec1000 iters in 0.01 seconds = 8.16 usec/iterTo run the ib_send_bw test, specify the device with the -d [DeviceName] and GID index for the configured port with the -x flag.ib_send_bw Server side:# ib_send_bw -R -d mlx5_1 -x 3************************************* Waiting for client to connect… *************************************ib_send_bw Client Side:# ib_send_bw -R -d mlx5_1 10.10.10.12 -x 3---------------------------------------------------------------------------------------Send BW TestDual-port : OFF Device : mlx5_1Number of qps : 1 Transport type : IBConnection type : RC Using SRQ : OFFPCIe relax order: ONibv_wr* API : ONTX depth : 128CQ Moderation : 1Mtu : 1024[B]Link type : EthernetGID index : 3Max inline data : 0[B]rdma_cm QPs : ONData ex. method : rdma_cm---------------------------------------------------------------------------------------local address: LID 0000 QPN 0x018a PSN 0x528956GID: 00:00:00:00:00:00:00:00:00:00:255:255:10:10:10:10remote address: LID 0000 QPN 0x018a PSN 0x419cbeGID: 00:00:00:00:00:00:00:00:00:00:255:255:10:10:10:12---------------------------------------------------------------------------------------#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]Conflicting CPU frequency values detected: 1500.000000 != 2350.000000. CPU Frequency is not max.65536 1000 2758.33 2758.32 0.044133---------------------------------------------------------------------------------------For qperf, you will need to run both the client and server for the included test:qperf Server Side:# qperfqperf Client Side:# qperf -cm1 10.10.10.10 rc_bwrc_bw:bw = 4.63 GB/secIf you require additional support beyond the usage guidance provided here, please open a support case. If you do not have a current/valid contract, please reach out to the team at Networking-contracts@nvidia.com for assistance in setting one up.Thank you,Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
19,network-access-in-the-embedded-mode,"Hi all,
I’m having difficulty for network access on the embedded mode.
My Bluefield2 smart nic can ping the host and host can ping bluefield2.
But I don’t have network connection.
Can some one explain how to use the ovs bridges to set the route for network connection?31d2a29e-d043-4f8b-94d0-5dd4d47198d7
Bridge ovsbr2
Port p1
Interface p1
Port pf1hpf
Interface pf1hpf
Port ovsbr2
Interface ovsbr2
type: internal
Port en3f1pf1sf0
Interface en3f1pf1sf0
Bridge ovsbr1
Port p0
Interface p0
Port ovsbr1
Interface ovsbr1
type: internal
Port en3f0pf0sf0
Interface en3f0pf0sf0
Port pf0hpf
Interface pf0hpf
ovs_version: “2.14.1”Hey there. This guide might help demystify all of these interfaces in the ovs bridge: vSwitch and Representors Model :: NVIDIA DOCA SDK DocumentationWhen in ECPF ownership mode, we see 2 representors for each one of the DPU’s network ports: one for the uplink, and another one for the host side PF (the PF representor created even if the PF is not probed on the host side). if you check the output of mlnx-sf -a show on the DPU you’ll see the kernel netdev and the representor names for the DPU itself.The Representor netdev devices are already connected (added to) the OVS bridge by default, so we only need to use netplan to assign an IP address to enp3s0f1s0 or enp3s0f0s0 (or both) and that should work. Here’s an example netplan config:demystifyThanks Justine for the clarification,
one follow up question: does the oob_net0 give us network connectivity only on the separated mode? How should we set the oob_net0 interface to give us network access? Our oob_net0 interface is connected to VLAN, but we still can’t perform apt-get update to finish doca installation.
Also in the link you shared with us, what are p3p1 and p4p2 interfaces? Are these internal interfaces that connect bluefield to the host? or the interfaces that connect bluefield to the switch?The oob_net0 interface is meant to be used to connect into an IMPI network that’s typically 1gbps copper. Most data centers have a low speed 1gbps out of band management network.For the apt update to work, DNS resolution must work and you must have a route to the internet. Do you have DNS servers configured and do you have a route to reach the configured DNS servers?Hi Justin! My name is Anu, and I work with Diman. So we have everything setup correctly now I think. This is the setup right now:Smartnic on embedded mode:Host ens4f0   x.x.231.115 <------> pf0hpf  x.x.231.116 SmartNIC
ens4f1   x.x.231.117 <------> pf1hpf  x.x.231.118pci@0000:a3:00.0  ens4f0       network        MT42822 BlueField-2 integrated ConnectX-6 Dx network controller
pci@0000:a3:00.1  ens4f1       network        MT42822 BlueField-2 integrated ConnectX-6 Dx network controllerWe used netplan to set the IP addresses and gateway. The DNS is the same as the host’s DNS. The host is able to ping the DNS server but from the smartnic , we are unable to ping the DNS server. We are behind proxy.And the ovs-vsctl show :
root@localhost:~# ovs-vsctl show
c8599771-cb29-432e-9087-070acfe56d20
Bridge ovsbr1
Port p0
Interface p0
Port ovsbr1
Interface ovsbr1
type: internal
Port en3f0pf0sf0
Interface en3f0pf0sf0
Port pf0hpf
Interface pf0hpf
Bridge ovsbr2
Port pf1hpf
Interface pf1hpf
Port p1
Interface p1
Port ovsbr2
Interface ovsbr2
type: internal
Port en3f1pf1sf0
Interface en3f1pf1sf0
ovs_version: “2.14.1”Can you tell us where we could be going wrong? The host can ping the DNS but the smartnic cannot ping the IP assigned to the host side as well it cannot ping the DNS.Also, as per this doc: Virtual Switch on BlueField SmartNIC - BlueField SW Manual v2.0.1.10841 - NVIDIA Networking DocsWe need to have openvswitch service working but for some reason, this service is not found on the smartnic ubuntu OS:root@localhost:~# service openvswitch start
Failed to start openvswitch.service: Unit openvswitch.service not found.root@localhost:~# systemctl restart openvswitch
Failed to restart openvswitch.service: Unit openvswitch.service not found.How do we enable openvswtich on the smartnic? Could this be the issue?systemctl restart openvswitchHi Anu,
Try $ systemctl status openvswitch-switch.serviceRegarding the ping issue.
The topology is not very clear to me.
The host is connected to a BF2 with a PCI slot. Did you connect the BF2 ports to a switch which has access to the DNS server? is it possible the server you’re using has an additional management port (not related to the BF2) which is allows access to the DNS server? Can you dump the routing table for both the host and the ARM device using ‘route -n’?Thanks!Dear Shaulia,Justin had a call with us and we were able to fix the issue. Thank you very much for your response.Thank you,
AnuThis topic was automatically closed 60 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
20,mig-support-in-bcm-enablement-failed-and-clients-auto-reset,"After enabling Mig on NVidia GPU client nodes and running below command , the client auto resets and after coming back online , it again goes to Disabled state of MIG .
Please let me know if you need more infoMore output commands@mdevries1 @gkloosterman , anything you guys can share on above issue ?Powered by Discourse, best viewed with JavaScript enabled"
21,cx5-25g-cards-not-linking-up-with-cisco-aci-randomly,"CX5 25G cards not linking up with Cisco ACI randomly -->CX5 cards with firmware 16.33.1048The issue is ports are getting in [close port state ] and not releasing even after reboot/ even after shut and no shut on the switch ports. Also -  other command like Mlxlink -d -a DN/ Mlxlink -d -a UP and OS ifup /ifdown does not down/up the interface. Its stuck at CLOSE PORT start.What does [STATE - CLOSE PORT] mean ? And how can we get out of this state?mlxlink -d 0000:xxxxxState                           : Close portPhysical state                  : ETH_AN_FSM_ENABLE
Speed                           : N/A
Width                           : N/A
FEC                             : N/A
Loopback Mode                   : No Loopback
Auto Negotiation                : ONEnabled Link Speed              : 0x38007013 (25G,10G,1G)
Supported Cable Speed           : 0x38007013 (25G,10G,1G)ThanksHello joe.maina4u,Thank you for posting your query on our community. The port can be down due to various reasons. Please try the below troubleshooting steps and see if it resolves the issue:Please let us know if it helps resolve the issue.Thanks,
-Nvidia Network SupportI’ve the exact same problem (not sure about the port state) with different CX 25G adapters (OEM HPE) for over a year now, mostly/only on Linux. Only removing the power completely from the server solved this is some cases. After months of trial and error we got feedback from Cisco and they pointed to the following Nvidia document.https://enterprise-support.nvidia.com/s/article/3rd-Party-Switches-Link-Is-Down-Due-to-Auto-NegotiationNetwork team now changed DFE delay and we have to see if it reappears or not. But as the experience with different types of Mellanox adapters was really bad during the last 18 months, we will not use any of them in new servers.ThanksPowered by Discourse, best viewed with JavaScript enabled"
22,ip-bgp-community-support,"Hello, I have a MSN2100-CB2F switch which is running Onyx 3.7.1134 version.I’m trying to deploy a route-map with set community statement on one of the BGP peers.Whenever i’m trying configure this, the machine won’t accept the input of xxxx:xxxx, the only option i have to set as a community is none.is the correct way to implement a set of communities on a bgp prefix is to set a standard ip community-list and use the set community-list  additive?another question, any way to use the redistribute connected\static with a route-policy attached to it?Dear David,Thank you for contacting NVIDIA Mellanox Technologies Technical Support,My name is Ori and I have respectfully received your request.Could you please share the exact command you used on the Mellanox?Just to make sure - The switch that you are trying to append the route map + community is the Mellanox?What the end goal is?Are you willing to add append specific community attribute to the NLRI announced by Mellanox to other peers or whether you want to enforce routing policy on the Mellanox while considering the community attribute populated by other peers?Thanks,Ori AcocaPowered by Discourse, best viewed with JavaScript enabled"
23,msx6012-mlag,"Hello,I have two 6012 switch, and want to configure MLAG.But when I followed the guide attached the MLAG-VIP is not working only one node became master and the other is stuck in unknown.The switches have version 3.6.8012Attached the 2 sh run and all the information I have with two .txt file.Please let me know if there is a know issue with this version or I did configured something working or is there a mlag issue with 3.6.8012 or the L3 eth license is not enough for mlag?Thank you very much in advance.BR,DominikMLNX-OS_VPI_User_Manual_3.6.8012.pdf (7.7 MB)MLNX2_sh_run_01.txt (11.1 KB)MLNX1_sh_run_01.txt (21.2 KB)Dear All,I just created 2 full sysdumps from the 2 sx6012 I attached them to the update.And noticed this in the logs: No mlag-vip - mgmt KA is not supportedDoes anyone know what this means?I tried to search online and in available documents, but I was unable to find any mention about this.Once again Thank you in advance.Br,DominikMar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: Cluster daemon launchedMar 1 16:34:38 MLNX2 pm[4337]: [pm.NOTICE]: Launched clusterd (Clustering Daemon) with pid 8865Mar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: mlnx_init_golden_profile: golden-profile = 4Mar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: mlnx_init_internal: Initial HA profile=mlag(mellanox_none)Mar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: mlnx_init_internal: 5 mDNS node configuration tuples:Mar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: mlnx_init_internal: ha-profile = mlagMar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: mlnx_init_internal: os-release = 3.6.8012Mar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: mlnx_init_internal: system-type = SX6012Mar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: mlnx_init_internal: golden-profile = 4Mar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: mlnx_init_internal: cpu-type = ppcMar 1 16:34:38 MLNX2 mlagd[5068]: TID 1251996816: [mlagd.NOTICE]: [MLAG_HEALTH.NOTICE] No mlag-vip - mgmt KA is not supportedMar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: set cluster master preference to 50Mar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: cluster mDNS enable""true""Mar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: Forking then execing binary /usr/bin/mDNSResponder with argv ""/usr/bin/mDNSResponder -d -a 10.0.24.11"".Mar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: Process with pid 8867: launched (not waiting for it)Mar 1 16:34:38 MLNX2 mDNSResponder[8867]: mDNSResponder starting upMar 1 16:34:38 MLNX2 clusterd[8865]: [clusterd.NOTICE]: dnode publish: startingMar 1 16:34:39 MLNX2 mDNSResponder[8867]: ResolveSimultaneousProbe: doneMar 1 16:34:39 MLNX2 last message repeated 5 timesMar 1 16:34:39 MLNX2 mlagd[5068]: TID 1251996816: [mlagd.NOTICE]: [MLAG_HEALTH.NOTICE] No mlag-vip - mgmt KA is not supportedMar 1 16:34:39 MLNX2 clusterd[8865]: [clusterd.NOTICE]: Cluster dnode publish: successMar 1 16:34:39 MLNX2 clusterd[8865]: [clusterd.NOTICE]: cl_disco_dnode_check_rename: hostid=123212341234 old_name=MLNX2 new_name=MLNX1(MLNX1-123212341234)Mar 1 16:34:39 MLNX2 clusterd[8865]: [clusterd.ERR]: cl_disco_master_resolve_blocking: Unable to join cluster my-vip master cpu=/ppc ha=/mlag cnt=0Mar 1 16:34:39 MLNX2 clusterd[8865]: [clusterd.NOTICE]: cl_disco_master_resolve_blocking: Restarting clusterd because unable to join cluster my-vip master cpu=/ppc ha=/mlagMar 1 16:34:39 MLNX2 clusterd[8865]: [clusterd.NOTICE]: Shutdown mDNSResponder with pid 8867Mar 1 16:34:39 MLNX2 mgmtd[4397]: [mgmtd.NOTICE]: Async: timed out getting external response for type query_request session 432 id 40794 from clusterd-8865Mar 1 16:34:39 MLNX2 mgmtd[4397]: [mgmtd.NOTICE]: Starting to dump backlogged messages: 0 in queueMar 1 16:34:39 MLNX2 mgmtd[4397]: [mgmtd.ERR]: md_system_get_layout_disk_names(), md_system.c:3027, build 1: Unexpected empty disk layout!Mar 1 16:34:39 MLNX2 mgmtd[4397]: [mgmtd.ERR]: md_system_iterate_disk(), md_system.c:3108, build 1: Error code 14001 (unexpected NULL) returnedMar 1 16:34:40 MLNX2 mlagd[5068]: TID 1251996816: [mlagd.NOTICE]: [MLAG_HEALTH.NOTICE] No mlag-vip - mgmt KA is not supportedMar 1 16:34:41 MLNX2 mlagd[5068]: TID 1251996816: [mlagd.NOTICE]: [MLAG_HEALTH.NOTICE] No mlag-vip - mgmt KA is not supportedMar 1 16:34:41 MLNX2 statsd[5006]: [statsd.NOTICE]: alarm 'cpu_util_indiv': triggered for rising clear for event cpu_util_indivMar 1 16:34:41 MLNX2 mgmtd[4397]: [mgmtd.NOTICE]: Event occurred (CPU utilization has fallen back to normal levels), but no mailhub configured.Mar 1 16:34:42 MLNX2 mlagd[5068]: TID 1251996816: [mlagd.NOTICE]: [MLAG_HEALTH.NOTICE] No mlag-vip - mgmt KA is not supportedHi Dominik.Do you have ping and SSH between the 2 mgmt0 ports of the switches?Please remove the mlag-vip config from switch #[[2:""(config)# no mlag-vip]​(config)# mlag-vip my-vipDear Eddie Shklaer,Thank you very much for the reply.Yes, ping and SSH works between the 2 nodes on mgmt0 to other mgmt0 ip address.The sh run indeed odd because I typed these to the CLI on BOTH switch:MLNX1: mlag-vip my-vip ip 10.0.24.12 /22 forceMLNX2: mlag-vip my-vip ip 10.0.24.12 /22 forceBut I saw that the MLNX2 has this in the config as You rightly pointed out:mlag-vip my-vip ip 0.0.0.0 /0 forceEither if I apply to the MLNX2 this Mlag config I get the same result: mlag-vip my-vipsh run shows: mlag-vip my-vip ip 0.0.0.0 /0 forceAnd sadly the Mlag-vip is still in desync Master/unkown.Thank you very much for your help.Please let me know if it is a known bug or I do miss configure something.Best Regards,Dominikother sysdumpJust saved the Logs files and it is showing like:cl_disco_master_resolve_blocking: Unable to join cluster my-vip master cpu=/ppc ha=/mlag cnt=0After I changed hostID it seems that the 2 msx6012 started to see each other still not in ha/Mlag-vip, but now new issue emerged: MLNX1_2_mlag_disk_errorMLNX1:Mar 1 18:24:20 MLNX1 clusterd[5239]: [clusterd.NOTICE]: session 41: accepted connection from IP 10.0.24.11Mar 1 18:24:20 MLNX1 clusterd[5239]: [clusterd.NOTICE]: CCL: Accepted connection from 10.0.24.11, port 38510Mar 1 18:24:20 MLNX1 clusterd[5239]: [clusterd.ERR]: HMAC verification failedMar 1 18:24:20 MLNX1 clusterd[5239]: [clusterd.ERR]: cl_verify_hmac_md5(), cl_comm.c:1508, build 1: Error code 14613 returnedMar 1 18:24:20 MLNX1 clusterd[5239]: [clusterd.ERR]: cl_ccl_msg_recv(), cl_comm.c:1559, build 1: HMAC verification failedMLNX2:Mar 1 17:55:15 MLNX2 clusterd[5532]: [clusterd.NOTICE]: Cluster dnode publish: successMar 1 17:55:15 MLNX2 clusterd[5532]: [clusterd.NOTICE]: Cluster master resolve: success: 10.0.24.10 60102 (MLNX1-123212341234)Mar 1 17:55:15 MLNX2 clusterd[5532]: [clusterd.NOTICE]: session 1: enabled listening on IP 10.0.24.11Mar 1 17:55:15 MLNX2 clusterd[5532]: [clusterd.NOTICE]: session 2: connected to IP 10.0.24.10Mar 1 17:55:15 MLNX2 clusterd[5532]: [clusterd.NOTICE]: master change event sent: falseMar 1 17:55:15 MLNX2 clusterd[5532]: [clusterd.NOTICE]: standby change event sent: falseMar 1 17:55:15 MLNX2 clusterd[5532]: [clusterd.NOTICE]: master vip: cleared any old vipMar 1 17:55:15 MLNX2 clusterd[5532]: [clusterd.NOTICE]: Master has been detected , update mDNSResponder enable state [Enabled]Mar 1 17:55:15 MLNX2 clusterd[5532]: [clusterd.NOTICE]: cl_config_mdns_enable: role=unknownMar 1 17:55:15 MLNX2 mDNSResponder[5533]: mDNSResponder allow traffic send value changed from : Allowed to : AllowedMar 1 17:55:16 MLNX2 mlagd[5066]: TID 1251996816: [mlagd.NOTICE]: [MLAG_HEALTH.NOTICE] No mlag-vip - mgmt KA is not supportedMar 1 17:55:16 MLNX2 mgmtd[4389]: [mgmtd.ERR]: md_system_get_layout_disk_names(), md_system.c:3027, build 1: Unexpected empty disk layout!Mar 1 17:55:16 MLNX2 mgmtd[4389]: [mgmtd.ERR]: md_system_iterate_disk(), md_system.c:3108, build 1: Error code 14001 (unexpected NULL) returnedsolved disk layout.Still issue with: HMAC verification failedDoes anybody had issues with HWMAC before?Mar 1 18:56:54 MLNX2 clusterd[5560]: [clusterd.NOTICE]: session 2: accepted connection from IP 10.0.24.10Mar 1 18:56:54 MLNX2 clusterd[5560]: [clusterd.NOTICE]: CCL: Accepted connection from 10.0.24.10, port 37635Mar 1 18:56:54 MLNX2 clusterd[5560]: [clusterd.ERR]: HMAC verification failedMar 1 18:56:54 MLNX2 clusterd[5560]: [clusterd.ERR]: cl_verify_hmac_md5(), cl_comm.c:1508, build 1: Error code 14613 returnedMar 1 18:56:54 MLNX2 clusterd[5560]: [clusterd.ERR]: cl_ccl_msg_recv(), cl_comm.c:1559, build 1: HMAC verification failedHi, please open a support case - it will be easier to troubleshootPowered by Discourse, best viewed with JavaScript enabled"
24,representors-on-bluefield2,"Hi all,
I moved one of our BF2 cards to a different PCI slot and reinstalled Ubuntu using bfb.bfb installs ok but for some reason the PF representors and VF representors dont get created automatically.P0 and P1 are listed as interfaces ok.Is this expected behaviour and if so how do I create the representors?I tried resetting the card using mlxfwreset  but this made no difference.Thanks in advance,
Paulhi paul,
what do you mean PF representors and VF representors are not created?
Could you share how you check(the commands) , the result and what you excepted?On the Arm side, when using ‘ip a’, are  the pf<port_number>hpf and pf<port_number>vf<function_number> not there?For pf representors, they should be there.
For vf representors,  VF should be created first on the host side. Then on the Arm side can see the VF representors.LeveiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
25,help-with-error-write-counter-to-semaphore-operation-not-permitted-on-linux,"Trying to get an older Connect-X4 Infiniband card (MT27700) working in CentOS 7.9 and after applying the MLNX_OFED package I get the error “write counter to semaphore: Operation not permitted” when trying an “mst status” command or a “mstconfig -d 41:00.0 query” command.  Is this an indication of a bad firmware, incompatible card?  Any help would be appreciated.  Thanks.Hello itengroc,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Based on the information provided, it looks like you are using an INBOX utility.We would recommend to install the latest MLNX_OFED GA version which contains the MFT package and use the ‘mlxconfig’ command to access the adapter.In some cases we see that the utilities provided by the OS vendor is somewhat outdated.You can download the latest MLNX_OFED GA driver through the following link → Linux InfiniBand DriversIf you are bounded by using the INBOX driver, you can install the MFT package separately. You can download the latest version through the following link → Mellanox Firmware Tools (MFT)Thank you and regards,
~NVIDIA Networking Technical SupportMvB - thanks for your input.  After much head-scratching I was able to determine that the issues with card were in fact not related to the card at all but to the TPM/Secure Boot options of the server.  After disabling them and rebooting, I was able to complete the driver install and firmware flash.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
26,ive-a-connectx6-mcx653106a-hda-ax-in-server-and-when-i-plugged-in-an-active-cable-it-gets-temperature-warning-message-high-temperature-on-sensors-with-bit-set-0-1-i-have-updated-the-connectx6-to-the-latest-firmware-20-29-2002-but-it-doesnt-help,"driver: mlx5_coreversion: 4.7-3.2.9firmware-version: 20.29.2002 (MT_0000000225)expansion-rom-version:bus-info: 0000:03:00.0Feb 23 14:35:58 localhost kernel: mlx5_core 0000:03:00.0: temp_warn:170:(pid 0): High temperature on sensors with bit set 0 1Feb 23 14:35:58 localhost kernel: mlx5_core 0000:03:00.1: temp_warn:170:(pid 0): High temperature on sensors with bit set 0 1Feb 23 14:39:08 localhost kernel: mlx5_core 0000:03:00.1: print_health_info:421:(pid 0): synd 0x10: High temperatureFeb 23 14:39:08 localhost kernel: mlx5_core 0000:03:00.0: print_health_info:421:(pid 0): synd 0x10: High temperatureHello Patrick,Thank you for posting your inquiry on the NVIDIA Networking Community.Make sure when using an active cable, you use one of the validated and supported cables based on your installed f/w RN → https://docs.mellanox.com/display/ConnectX6Firmwarev20292002/Firmware+Compatible+ProductsAlso please make sure, the adapter and transceiver is getting adequate cooling by increasing the fan-speed of the system.If you are using a validated/ supported cable and after increasing the fan speed of the system, you still are experiencing this issue, please open a support case (valid support contract needed), by sending an email to networking-support@nvidia.comThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
27,how-to-enable-connectx-5-nic-ipsec-offload-capability,"P/N MCX512A-ACATS/N MT2021K07389​After viewing the driver print, The Nic ipsec_offload capability bit is 0; (HCA Device Capabilities/ipsec_offload fields)Set the ipsec_offload bit to 1 with the SET_HCA_CAP command, then query it with QUERY_HCA_CAP and the bit still is 0.So ipsec esp offload can’t be turned on.​I would like to ask you how to turn on the hca ipsec_offload capability ,thank youHi Wang,Starting from OFED 5.2, IPsec full offload feature is supported in ConnectX-6-DX crypto card and Bluefield 2 crypto card. The Connectx-5 HCA card does not support this feature.Sophie.Is the ipsec Crypto Offload feature supported?Powered by Discourse, best viewed with JavaScript enabled"
28,how-to-upgrade-sn2410-to-cumulus-4-4-version,"I also checked the knowledge base says that SN2410 still support Cumulus Linux 4.X and 5.X.
Can anyone advise how to upgrade the OS ? Or I should install the OS image from beginning?Thanks,upgrade from CL 4.3.x to CL 4.4.x requires installing via Onie:https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-44/Installation-Management/Upgrading-Cumulus-Linux/#install-a-cumulus-linux-image-or-upgrade-packages
image1143×248 11.9 KB
Powered by Discourse, best viewed with JavaScript enabled"
29,how-to-flash-oem-firmware-for-connectx-4-lx,"We have a weird behaviour with Mellanox ConnectX-4 Lx cards that are in various routers and servers for an infra we are setting up.May 17 12:58:04 frr1 kernel: [ 6009.787720] bond1: An illegal loopback occurred on adapter (enp3s0f0)May 17 12:58:04 frr1 kernel: [ 6009.787720] Check the configuration to verify that all adapters are connected to 802.3ad compliant switch portsWe only have this problem with Mellanox cards and suspect a firmware issue.The driver embedded in the controller is very outdated : 14.21.2010 and 14.26.1040 for the other. There are ten releases ahead of these releases.We suspect that an upgrade of the firmware of the card will solve our issues.But when we go to the download page, they ask us the OPN number, which we don’t know : https://www.mellanox.com/support/firmware/connectx4lxenI would need either the latest driver’s binary or the correct OPN version for download.Our card are from iEi / QNAP and Gigabyte manufacturer.When we try to download the latest FW, you are asking us for the OPN number which we don’t know.Thanks for your earliest feedback.System has an LACP bond with mlx5_en based interfaces and the bonding driver repeatedly logs the following messages: Feb 27 10:08:42 localhost kernel: bond0: An illegal loopback occurred on adapter (eno1) Check the configuration to verify that all...Hello Bob,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, you are using an OEM adapter. For obtaining the latest provided and supported OEM f/w you need to reach out to the OEM vendor from where you purchased the adapter from. They will be able to provide you the correct f/w. The f/w downloads we provide on our website are related to the adapters with the Mellanox OPNs.Thank you and regards,~NVIDIA Networking Technical SupportWe have an issue, the manufacturer does not provide us with the binary so that we can properly flash the card.There are problems which are now clearly listed in Red Hat with the provided firmware and we REALLY need to update it in order to get the bond working.We have more or less identified the procedure but need an access to the binaries in order to flash the controller.Can you please help us ?Powered by Discourse, best viewed with JavaScript enabled"
30,howto-get-connectx-4-uefi-ib-working-with-dhcp,"Hi,we have a UEFI server with a dual port ConnectX-4 HCA (MT_2170111021) set to IB and would like to try booting via PXE. The server does write to the console indicating it’s trying to do PXE, but there’s is no indication whatsoever of any DHCP request, neither on the console nor on the network nor on the DHCP-server.DHCP works flawlessly once the server has booted (from disk) and configures the interfaces.We have other servers with older, non-UEFI flexboot roms, and they also work, so it’s not a network problem.In my humble opinion, all the necessary settings are there:UEFI_HII_EN True(1)LEGACY_BOOT_PROTOCOL PXE(1)EXP_ROM_UEFI_ARM_ENABLE False(0)EXP_ROM_UEFI_x86_ENABLE True(1)EXP_ROM_PXE_ENABLE True(1)Firmware is 12.28.2006 with UEFI 14.21.17.The server is a Supermicro X11SCL-F and the HCA is connected to a MSX6025F .How does one tell the HCA firmware to configure via DHCP ?The UEFI-settings can configure the interfaces under “Network Configuration”. Is this the place ?If so, it doesn’t help.Is there anything I’m missing ?Regards,NorbertHello,In order to boot via PXE , first of all need to make sure the device uefi interface is exposed in the boot menuif yes exposed so need to change the boot order to start from Mellanox device.so the place to tell HCA to configure DHCP is to go to boot menu on the BIOS and chose mellanox/Nvidia device to start booting instead boot over hardisk …If there is still an issue, please open a support ticket with Nvidia by sending an email to:networking-support@nvidia.comBest Regards,VikiWell, I wrote that it’s actually trying PXE via the Mellanox device, so it is selected as boot device. There is just no DHCP query seen anywhere.NorbertPowered by Discourse, best viewed with JavaScript enabled"
31,cumulus-linux-gui,"Netris.ai provides an intuitive GUI for Cumulus Linux (and SONiC).Just FYI in case you hadn’t heard of it.The Netris software also incorporates the use of a Mellanox SmartNIC and DPDK for full line rate network services including L4 Load Balancing:Kubernetes Load Balancer for On-Prem Private Clouds - a cloud-like automatic networking service for k8s clusters
Est. reading time: 7 minutes
Powered by Discourse, best viewed with JavaScript enabled"
32,are-there-any-limit-values-when-configuring-ovs-kernel-hardware-on-connectx-6-lx,"When configuring HWOL through ConnectX-6 LX and OVS, we confirmed that Conntack Offlaod works when using OVN ACL.However, when performing the TCP CPS test, the Netfiler Conntrack Table size exceeds 700k, it is not HW_OFFLOADed and is processed by OVS, causing a performance delay.Conntrack Table, OVS Conntrack Table, the number of Offload sessions decreased from 700k, and packets were dropped due to SoftIRQ spikes due to packet processing delays.There seems to be a Conntrack Table limit per NIC model.
Is it possible to share the limit values by ConnectX-6 LX, ConnectX-6, and ConnectX-6 DX?1.Hardware
Server: Dell R7615
CPU: AMD Epyc 9654P
Memory: 384GB
NUMA: 1
NIC: Connect-X 6LX2.Software Versions
OS: Ubuntu 22.04.2 LTS
Kernel: 5.15
Openstack Version: Yoga
OVN: 22.03
OVS: 2.17.5
MLNX OFED Driver: 5.8-2.0.3
Firmware: 26.35.1012 (DEL0000000031)3.SmartNIC Configuration
3-1. Driver
vport_match_mode: metadata
steering_mode: smfs
ct_action_on_nat_conns: disable
ct_labels_mapping: disable
ct_max_offloaded_conns: 42949672953-2. Devlink Param
num_of_groups: 153-3. MSTConfig
PF_NUM_PF_MSIX_VALID: False(0)
STRICT_VF_MSIX_NUM: False(0)
NUM_PF_MSIX_VALID: True(1)
NUM_PF_MSIX: 127
NUM_VF_MSIX: 127
PF_NUM_PF_MSIX: 63
DYNAMIC_VF_MSIX_TABLE: False(0)Powered by Discourse, best viewed with JavaScript enabled"
33,fail-to-start-l2fwd-nv,"Hi,Followed the instructions shown here: GitHub - NVIDIA/l2fwd-nv: l2fwd-nv provides an example of how to leverage your DPDK network application with the NVIDIA GPUDirect RDMA techonology.. The binary builds flawlessly but fails to start:[root@cu-l2fwd l2fwd-nv]# taskset -pc 1pid 1’s current affinity list: 2,7-9,11,13[root@cu-l2fwd l2fwd-nv]# ./build/l2fwdnv -l 2,7-9,11,13 -n 8 -a b5:00.1,txq_inline_max=0 – -m 1 -w 0 -b 64 -p 4 -v 0 -z 0************ L2FWD-NV ************EAL: Detected 24 lcore(s)EAL: Detected 1 NUMA nodesEAL: Detected static linkage of DPDKEAL: failed to parse device “b5:00.1”EAL: Unable to parse device ‘b5:00.1,txq_inline_max=0’EAL: Error - exiting with code: 1Cause: Invalid EAL arguments[root@cu-l2fwd l2fwd-nv]# ethtool -i enp1f1driver: mlx5_coreversion: 5.3-1.0.0firmware-version: 22.31.1014 (MT_0000000500)expansion-rom-version:bus-info: 0000:b5:00.1supports-statistics: yessupports-test: yessupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: yes[root@cu-l2fwd l2fwd-nv]# ethtool -l enp1f1Channel parameters for enp1f1:Pre-set maximums:RX: 0TX: 0Other: 512Combined: 24Current hardware settings:RX: 0TX: 0Other: 0Combined: 24[root@cu-l2fwd l2fwd-nv]# ethtool -a .Pause parameters for .:Cannot get device pause settings: No such device[root@cu-l2fwd l2fwd-nv]# ethtool -a enp1f1Pause parameters for enp1f1:Autonegotiate: offRX: offTX: off################Additional Info:controller-0:/home/sysadmin# lsmod | grep nvidianvidia_peermem 4467 0nvidia_modeset 1185108 0nvidia_uvm 995677 0nvidia 35219551 68 gdrdrv,nvidia_modeset,nvidia_peermem,nvidia_uvmdrm 441205 1 nvidiaib_core 349049 10 rdma_cm,ib_cm,iw_cm,mlx5_ib,ib_umad,nvidia_peermem,ib_uverbs,rdma_ucm,ib_ipoibcontroller-0:/home/sysadmin# mstflint -d b5:00.0 vReading Boot image component - OKFS4 failsafe image/0x00000018-0x0000001f (0x000008)/ (HW_POINTERS) - OK/0x00000020-0x00000027 (0x000008)/ (HW_POINTERS) - OK/0x00000028-0x0000002f (0x000008)/ (HW_POINTERS) - OK/0x00000030-0x00000037 (0x000008)/ (HW_POINTERS) - OK/0x00000038-0x0000003f (0x000008)/ (HW_POINTERS) - OK/0x00000040-0x00000047 (0x000008)/ (HW_POINTERS) - OK/0x00000500-0x0000053f (0x000040)/ (TOOLS_AREA) - OK/0x00001000-0x00003c33 (0x002c34)/ (BOOT2) - OK/0x00004000-0x0000401f (0x000020)/ (ITOC_HEADER) - OK/0x00006000-0x0001e87f (0x018880)/ (IRON_PREP_CODE) - OK/0x0001e880-0x0001e97f (0x000100)/ (FS3_RESET_INFO) - OK/0x0001ed00-0x004cee7f (0x4b0180)/ (MAIN_CODE) - OK/0x004cee80-0x004e5723 (0x0168a4)/ (PCIE_LINK_CODE) - OK/0x004e5780-0x004e616f (0x0009f0)/ (POST_IRON_BOOT_CODE) - OK/0x004e6180-0x00533807 (0x04d688)/ (PCI_CODE) - OK/0x00533880-0x0053522b (0x0019ac)/ (UPGRADE_CODE) - OK/0x00535280-0x0054daff (0x018880)/ (PHY_UC_CODE) - OK/0x0054db00-0x00560d3f (0x013240)/ (PHY_UC_CMD) - OK/0x00560d80-0x0056292b (0x001bac)/ (UNKNOWN:0xd) - OK/0x00562980-0x005631ab (0x00082c)/ (UNKNOWN:0xe) - OK/0x00563200-0x005635ff (0x000400)/ (IMAGE_INFO) - OK/0x00563600-0x005641ff (0x000c00)/ (FW_MAIN_CFG) - OK/0x00564200-0x0056467f (0x000480)/ (FW_BOOT_CFG) - OK/0x00564680-0x0056587f (0x001200)/ (HW_MAIN_CFG) - OK/0x00565880-0x00565e3f (0x0005c0)/ (HW_BOOT_CFG) - OK/0x00565e80-0x0056637f (0x000500)/ (PHY_UC_CONSTS) - OK/0x00566380-0x005664bf (0x000140)/ (IMAGE_SIGNATURE_256) - CRC IGNORED/0x00566500-0x00566dff (0x000900)/ (PUBLIC_KEYS_2048) - OK/0x00566e00-0x00566e8f (0x000090)/ (FORBIDDEN_VERSIONS) - OK/0x00566f00-0x0056713f (0x000240)/ (IMAGE_SIGNATURE_512) - CRC IGNORED/0x00567180-0x0056827f (0x001100)/ (PUBLIC_KEYS_4096) - OK/0x00568280-0x0056937f (0x001100)/ (FS4_RSA_PUBLIC_KEY) - OK/0x00569380-0x0056997f (0x000600)/ (FS4_RSA_4096_SIGNATURES) - CRC IGNORED/0x00569980-0x0061a1a7 (0x0b0828)/ (ROM_CODE) - OK/0x0061a200-0x0061abab (0x0009ac)/ (DBG_FW_INI) - OK/0x0061ac00-0x0061ac07 (0x000008)/ (DBG_FW_PARAMS) - OK/0x0061ac80-0x00642cff (0x028080)/ (CRDUMP_MASK_DATA) - OK-I- FW image verification succeeded. Image is bootable.controller-0:/home/sysadmin# mstflint -d b5:00.0 qImage type: FS4FW Version: 22.31.1014FW Release Date: 30.6.2021Product Version: 22.31.1014Rom Info: type=UEFI version=14.24.13 cpu=AMD64,AARCH64type=PXE version=3.6.403 cpu=AMD64Description: UID GuidsNumberBase GUID: 0c42a1030098c870 4Base MAC: 0c42a198c870 4Image VSD: N/ADevice VSD: N/APSID: MT_0000000500Security Attributes: secure-fwcontroller-0:/home/sysadmin# mstflint -d b5:00.1 qImage type: FS4FW Version: 22.31.1014FW Release Date: 30.6.2021Product Version: 22.31.1014Rom Info: type=UEFI version=14.24.13 cpu=AMD64,AARCH64type=PXE version=3.6.403 cpu=AMD64Description: UID GuidsNumberBase GUID: 0c42a1030098c870 4Base MAC: 0c42a198c870 4Image VSD: N/ADevice VSD: N/APSID: MT_0000000500Security Attributes: secure-fwcontroller-0:/home/sysadmin# mstvpd b5:00.1ID: ConnectX-6 Dx EN adapter card, 100GbE, Dual-port QSFP56, with PPS In/Out, PCIe 4.0 x16, Crypto and Secure BootPN: MCX623106PC-CDATEC: A2V2: MCX623106PC-CDATSN: MT2022X19516V3: 8ae07ad3b8abea1180000c42a198c870VA: MLX:MN=MLNX:CSKU=V2:UUID=V3:PCI=V0:MODL=CX623106PV0: PCIeGen4 x16controller-0:/home/sysadmin# mstvpd b5:00.0ID: ConnectX-6 Dx EN adapter card, 100GbE, Dual-port QSFP56, with PPS In/Out, PCIe 4.0 x16, Crypto and Secure BootPN: MCX623106PC-CDATEC: A2V2: MCX623106PC-CDATSN: MT2022X19516V3: 8ae07ad3b8abea1180000c42a198c870VA: MLX:MN=MLNX:CSKU=V2:UUID=V3:PCI=V0:MODL=CX623106PV0: PCIeGen4 x16Hi,Are you able to run testpmd to using similar syntax?Are you able to run l2fwd from DPDK and not external Nvidia version?What happens if you remove “txq_inline_max=0”?./app/dpdk-testpmd -l 3-5,8,11,13 --main-lcore=3 -a 0000:b5:00.0,txq_inline_max=512 – --port-numa-config=0,0 --socket-num=0 --burst=64 --txd=1024 --rxd=1024 --mbcache=512 --rxq=8 --txq=8 --forward-mode=mac -i --nb-cores=4 --txonly-multi-flow --mbuf-size=21762 ) using dpdk-l2fwd from dpdk 20.11.3 works:./examples/dpdk-l2fwd -l 2,6,7,9,10,12 -n 8 -a b5:00.1 – -p0x1 -q8check traffic:[root@cu-testpmd share]# ./rxtxbw.sh enp1f0enp1f0: 51312 Mbps RX | 51312 Mbps TX3 ) using l2fwd-nv Fails:[root@cu-l2fwd lib64]# pwd/opt/nvidia/cuBB/share/l2fwd-nv/external/dpdk/x86_64-native-linuxapp-gcc/install/lib64[root@cu-l2fwd lib64]# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD[root@cu-l2fwd lib64]# cd /opt/nvidia/cuBB/share/l2fwd-nv[root@cu-l2fwd l2fwd-nv]# cd build/[root@cu-l2fwd build]# ./l2fwdnv -l 2,6,7,9,10,12 -n 8 -a b5:00.1 – -p0x1 -q8************ L2FWD-NV ************EAL: Detected 24 lcore(s)EAL: Detected 1 NUMA nodesEAL: Detected static linkage of DPDKEAL: failed to parse device “b5:00.1”EAL: Unable to parse device ‘b5:00.1’EAL: Error - exiting with code: 1Cause: Invalid EAL arguments[root@cu-l2fwd build]#I would suggest to open an issue on project github page regarding the failure.OK will do. thanks.Powered by Discourse, best viewed with JavaScript enabled"
34,onie-static-ip-configuration,"Q: I have a switch which is in ONIE, I need to Install the OS so I need to assign the IP address for accessing Via SCP. I don’t have DHCP server to provide the IP. Now how can I set the Static IP for ONIE?A: ONIE supports various ways of installing the image: DHCP/web server, Web server, FTP/TFTP local file and USB. Without an IP, local file or USB may be the best way to go.
See Image Discovery and Execution — Open Network Install Environment documentationOr try this:Use SCP to copy the image file to the switch.
Run the installer manually.If you need to configure the gateway, the easiest way is to put your laptop on the same network as you selected above. If you can’t do that, run:Powered by Discourse, best viewed with JavaScript enabled"
35,install-doca-on-bluefield2-faild,"Hi, I want to install doca by sdkmanager on bluefiled 2.
but when came to  flash bulefield os,some errors below came out and this step can not succeed after a long timecould anyone help me?
Thanks.Powered by Discourse, best viewed with JavaScript enabled"
36,connectx-3-cards-ejectable-hot-plug,"Hello !
I use the ConnectX-3 Cards via a PCI-e 3.0x8 connector card in Windows 10, it works well. Unfortunately, Windows 10 recognized the ConnectX-3 Cards as an ejectable device. The WinOF driver 5.50.53000 has been correctly installed.
Does anyone knows how to fix it ?Hello autonomous120,Thank you for sharing your question.
This behavior of ConnectX-3 as an ejectable device has been seen in several Windows OS versions. It’s a normal behavior and can’t be changed. Furthermore, there is no need to change anything, you can use the adapter as for your needs normally.Best Regards,Nvidia Enterprise SupportPowered by Discourse, best viewed with JavaScript enabled"
37,hello-everyone-i-have-a-question-regarding-the-programmable-pipeline-for-new-network-flows-feature-of-connectx-6-smartnic-i-want-to-know-how-this-feature-can-be-manipulated,"In the datasheet paper of CX6 SmartNIC, it says that, ConnectX-6 offers intelligent flexible pipeline capabilities, including a programmable flexible parser and match-action tables that enable hardware offloads for future protocols.​At this end, I want utilize CX6 to implement my new idea about network protocols.In the designed protocol, the packet header will be rewrited according to the already-sent packet information.​So three questions are imposed for the implementation of the designed protocol:i) the SmartNIC has to identify all the packets;ii) the NIC has to store the already-sent statistics, e.g., counters may be needed;iii) and rewrite the packet header.​My question would be, how to implement the above-mentioned three questions on CX6 SmartNIC ?​​I’m looking forward to your answers!​Sincerely, yours​Hi long,Maybe you can refer to flex parser.In latest DPDK, the ‘rte flow’ provide some functions for flex.DPDK community provide more info about this.https://doc.dpdk.org/guides/prog_guide/rte_flow.html#data-matching-item-typesRegards,LeveiPowered by Discourse, best viewed with JavaScript enabled"
38,missing-full-offload-parameter-for-full-ipsec-offload-on-connectx-6-dx,"Hello,I’m trying to get full IPSec offloading to work on a ConnectX-6 DX card on Ubuntu 20.04.02.According to https://community.mellanox.com/s/article/ConnectX-6DX-Bluefield-2-IPsec-HW-Full-Offload-Configuration-Guide I should use the “full_offload” parameter to “ip xfrm” when setting this up. However, doing this fails with “Error: argument “full_offload” is wrong: unknown”. According to the documentations there is no such parameter (just “offload”).The same goes for the example swanctl config on the same article, “hw_offload=full” does not exist according to the documentation, only “yes, auto, no” are valid options.This leads me to the conclusion that the article expects me to use some specific software version / Kernel version.Please hint me to where I can find this software.Thanks!Hi Nils,Please follow the below updated article for full IPsec offload configuration. The instructions are for the Bluefiled card, but they should be similar for ConnectX-6 Dx.https://docs.nvidia.com/doca/sdk/east-west-overlay-encryption/index.htmlRegards,ChenHi Chen,Thanks a lot for your answer!The article you referenced shows quite nicely how to get a Mellanox version of strongswan up and running, that’s very helpful.However, it does not talk about the prerequisites for getting the full offload running: The kernel needs to support it, then configuration via ip xrm should also be possible.Please let me know how I can get kernel support for the full offloading on a standard Ubuntu 20.04 installation.Thanks again!Powered by Discourse, best viewed with JavaScript enabled"
39,error-in-the-log-msn2410,"May 19 11:00:17 Main-Switch statsd[7512]: [statsd.NOTICE]: (StatsLog) cpu-rate-limiter UC_router: 50 packets dropped by CPU rate-limiterMay 19 11:01:00 Main-Switch statsd[7512]: [statsd.NOTICE]: (StatsLog) cpu-rate-limiter UC_router: 132 packets dropped by CPU rate-limiterMay 19 07:34:28 Main-Switch cablemond[7605]: TID 140600186926848: [cablemond.WARNING]: Unknown identifier FF cableMay 19 08:34:29 Main-Switch cablemond[7605]: TID 140600186926848: [cablemond.WARNING]: Unknown identifier FF cableMay 19 09:34:49 Main-Switch cablemond[7605]: TID 140600186926848: [cablemond.WARNING]: Unknown identifier FF cableCan you tell us what causes these two errors?Hi Larsen,May 19 11:00:17 Main-Switch statsd[7512]: [statsd.NOTICE]: (StatsLog) cpu-rate-limiter UC_router: 50 packets dropped by CPU rate-limitedThe UC_router rate limiter has dropped 50 packets that were destined to the switch CPU,This control plance policer allows 1000 packets per second with 128 packets burst size.May 19 07:34:28 Main-Switch cablemond[7605]: TID 140600186926848: [cablemond.WARNING]: Unknown identifier FF cableOnce of the cables connected has an eeprom identifier field with FF value - invalid.you can check which cable with the below command:show interfaces ethernet transceiver briefPowered by Discourse, best viewed with JavaScript enabled"
40,how-to-reset-mlnx-qos-settings,"Hi all,I’ve seen mlnx_qos working properly to limit the traffic with sk_prio.If I want to reset all the qos settings, how to do that ? Even after rebooting the system, the traffic without sk_prio was limited. I’ve set all queue unlimited, but it did not work for no sk_proio.How to clear qos settings?*** here is logs running rate limit$ mlnx_qos -i enp182s0f0 -p 0,1,2,3,4,5,6,7 -r 6,6,6,6,6,6,6,6DCBX mode: OS controlledPriority trust state: dscpdscp2prio mapping:prio:0 dscp:07,06,05,04,03,02,01,00,prio:1 dscp:15,14,13,12,11,10,09,08,prio:2 dscp:23,22,21,20,19,18,17,16,prio:3 dscp:31,30,29,28,27,26,25,24,prio:4 dscp:39,38,37,36,35,34,33,32,prio:5 dscp:47,46,45,44,43,42,41,40,prio:6 dscp:55,54,53,52,51,50,49,48,prio:7 dscp:63,62,61,60,59,58,57,56,Receive buffer size (bytes): 262016,0,0,0,0,0,0,0,Cable len: 7PFC configuration:priority 0 1 2 3 4 5 6 7enabled 0 0 0 0 0 0 0 0buffer 0 0 0 0 0 0 0 0tc: 0 ratelimit: 6.0 Gbps, tsa: vendorpriority: 0tc: 1 ratelimit: 6.0 Gbps, tsa: vendorpriority: 1tc: 2 ratelimit: 6.0 Gbps, tsa: vendorpriority: 2tc: 3 ratelimit: 6.0 Gbps, tsa: vendorpriority: 3tc: 4 ratelimit: 6.0 Gbps, tsa: vendorpriority: 4tc: 5 ratelimit: 6.0 Gbps, tsa: vendorpriority: 5tc: 6 ratelimit: 6.0 Gbps, tsa: vendorpriority: 6tc: 7 ratelimit: 6.0 Gbps, tsa: vendorpriority: 7[root@vm1 ~]# iperf -c 1.1.1.112 -i 1Client connecting to 1.1.1.112, TCP port 5001TCP window size: 45.0 KByte (default)[ 3] local 1.1.1.111 port 38774 connected with 1.1.1.112 port 5001[ ID] Interval Transfer Bandwidth[ 3] 0.0- 1.0 sec 488 MBytes 4.09 Gbits/sec[ 3] 1.0- 2.0 sec 488 MBytes 4.09 Gbits/sec[ 3] 2.0- 3.0 sec 557 MBytes 4.67 Gbits/sec[ 3] 3.0- 4.0 sec 539 MBytes 4.52 Gbits/sec[ 3] 0.0- 4.0 sec 2.04 GBytes 4.35 Gbits/sec*** after rebooting the system, I’ve seen the default value, but vm traffic was limited by 1Gbps$ mlnx_qos -i enp182s0f0DCBX mode: OS controlledPriority trust state: pcpReceive buffer size (bytes): 262016,0,0,0,0,0,0,0,Cable len: 7PFC configuration:priority 0 1 2 3 4 5 6 7enabled 0 0 0 0 0 0 0 0buffer 0 0 0 0 0 0 0 0tc: 0 ratelimit: unlimited, tsa: vendorpriority: 1tc: 1 ratelimit: unlimited, tsa: vendorpriority: 0tc: 2 ratelimit: unlimited, tsa: vendorpriority: 2tc: 3 ratelimit: unlimited, tsa: vendorpriority: 3tc: 4 ratelimit: unlimited, tsa: vendorpriority: 4tc: 5 ratelimit: unlimited, tsa: vendorpriority: 5tc: 6 ratelimit: unlimited, tsa: vendorpriority: 6tc: 7 ratelimit: unlimited, tsa: vendor[root@vm1 ~]# iperf -c 1.1.1.112 -i 1Client connecting to 1.1.1.112, TCP port 5001TCP window size: 45.0 KByte (default)[ 3] local 1.1.1.111 port 41298 connected with 1.1.1.112 port 5001[ ID] Interval Transfer Bandwidth[ 3] 0.0- 1.0 sec 80.9 MBytes 678 Mbits/sec[ 3] 1.0- 2.0 sec 108 MBytes 903 Mbits/sec[ 3] 2.0- 3.0 sec 105 MBytes 882 Mbits/sec[ 3] 3.0- 4.0 sec 122 MBytes 1.03 Gbits/sec[ 3] 4.0- 5.0 sec 110 MBytes 926 Mbits/sec[ 3] 5.0- 6.0 sec 114 MBytes 959 Mbits/sec[ 3] 6.0- 7.0 sec 118 MBytes 990 Mbits/sec[ 3] 7.0- 8.0 sec 116 MBytes 971 Mbits/sec[ 3] 8.0- 9.0 sec 123 MBytes 1.03 Gbits/sec[ 3] 9.0-10.0 sec 132 MBytes 1.11 Gbits/sec[ 3] 0.0-10.0 sec 1.10 GBytes 948 Mbits/secBest Regrads,Jiho JungHi Jiho Jung,You can reset mlnx_qos settings by either changing them manually back to default, restart the driver or reboot the server.There are 2 ways to set qos parameters:Using mlnx_qosUsing LLDP DCBXThere could be some conflicts if both of them are used.Note: If you are using mlnx_qos to enable PFC, please ensure all firmware LLDP and DCBX parameters are disabled in the firmware.Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
41,difference-between-device-memory-programming-and-nvidia-peerdirect,"Hey there,so we have eitherbut what’s the difference?Any help please?Hi There,
Thanks for you questionDevice Memory Programming (DMP) and NVIDIA PeerDirect are both really nice technologies that help improve data transfer in high-performance computing. Let me break it down for you:Device Memory Programming (DMP) is an API that allows applications to use on-chip memory (like a special memory area on your graphics card or accelerator) for faster data transfers. By doing this, it reduces the time it takes to send and receive data compared to using regular system memory.On the other hand, NVIDIA PeerDirect is a technology developed by NVIDIA. It enables direct communication between devices (like GPUs or accelerators) using high-speed InfiniBand or RoCE connections. The best part is, it allows these devices to share computing power and use the interconnect at the same time, all without the need to copy data between them.In simple terms, DMP helps reduce latency by using special on-chip memory, while PeerDirect lets devices directly access each other’s memory for faster communication. Both technologies make high-performance computing even more efficient and speedy!Remember, DMP can work with various devices, while PeerDirect is specific to NVIDIA’s GPUs and accelerators. Hope this explanation helps you understand these awesome technologies better!Thanks,
Ilan.Thank you for clarification! That was important to understand. From my understanding, GPUDirect RDMA is based on DMP - is this correct? If one wants to utilize DMP for other devices than a GPU, one would need a ConnectX-5 NIC at least, right? As stated in the MLX_OFED manuals.You’re welcome! I’m glad I could help clarify things for you.Regarding GPUDirect RDMA and DMP:GPUDirect RDMA and Device Memory Programming (DMP) are related but different technologies.Regarding the ConnectX-5 NIC:You are correct that some features and functionalities related to RDMA and GPU communication may depend on the hardware and network interconnect used.As for DMP, the exact hardware requirements can vary depending on the implementation and vendor. In some cases, using DMP on devices other than GPUs may require specific network interface cards (NICs) with on-chip memory support, such as Mellanox ConnectX-5 NICs and above.Thanks,
Ilan.Powered by Discourse, best viewed with JavaScript enabled"
42,mst-start-for-bf2-dpu-fails,"root@ws2030077303:/home/user# mst start
Starting MST (Mellanox Software Tools) driver set
Loading MST PCI modulemodprobe: ERROR: could not insert ‘mst_pci’: Exec format errormst_pci driver not found
Unloading MST PCI module (unused) - Success
Unloading MST PCI configuration module (unused) - Successis there any way to identify the reason for failure , anything to be installed before this step .Powered by Discourse, best viewed with JavaScript enabled"
43,firmware-expansion-roms-and-driver-for-connectx-4-lx-on-system-with-uefi-bios,"Hello,I’ve been working on getting Connectx-4 Lx (MCX4421A-ACQN) to work with a non-legacy bios and have been running into a few issues during the process. I have the following questions:What is the correct drivers/firmware to use for this process if I want to embed the driver into my BIOS’ source code? I’m currently using the 14_22_14_RELEASE.efi file.Your product’s documentation mentions that the preboot option EXP_ROM_UEFI_x86_ENABLE needs to be enabled for the card to show up in as a boot option. I know this is the case when using the expansion rom method (i.e. burning .efirom file using flint), but is this also the case when using the 14_22_14_RELEASE.efi file?Similarly, I have only been able to get HII information to appear in bios menu when I enable the preboot option UEFI_HIIEN in mlxconfig. Is there a driver that I can embed into the bios that already has UEFI_HIIEN and EXP_ROM_UEFI_x86_ENABLE enabled? Or is there a method to make a .efi file with these preboot settings enabled?Is there a document that explains in detail what all your preboot options do for the Connectx4 Lx?Thanks,StephenHi Stephen,According to our records your account has a valid support contract with us , i suggest to open a support ticket at Networking-support@nvidia.comWe will be happy to assist and provide you with the best support.ThanksSamerPowered by Discourse, best viewed with JavaScript enabled"
44,ibportstate-disable-port-and-unrecoverable,"hello,We have Mellanox Infiniband ConnectX-5 VPI adapter card to test rdma, now i am in trouble.1 i find when use command ifdown, the IP disappeared, but the connection eastablished will have no affect, still have IO throgh port.Is there a solution to archive the effect like ifdown/ifup on Ethernet (easily disconnect the network and rdma IO)?2 well, i find a solution, use command ibportstate lid port disable ，it is really works, at lease no IO anymore.Unfortunately, this port can not recover to link-up, even after reboot.ibportstate lid port enable not work and mlxlink not work yet.How to make port link-up again?best regardsthanks
Catch1.jpg807×99 52.5 KB

Catch2.jpg1109×495 63.6 KB
Hi ,You can use the mlxlink command for that :Mlxlink -d  -a DNMlxlink -d  -a UPAnd also for toggling the linkThanks,Samer2 well, i find a solution, use command ibportstate lid port disable ，it is really works, at lease no IO anymore.Unfortunately, this port can not recover to link-up, even after reboot.ibportstate lid port enable not work and mlxlink not work yet.How to make port link-up again?I had exactly the same problem and this worked for me:mlxfwreset -d  -l 3 resetPowered by Discourse, best viewed with JavaScript enabled"
45,where-is-infiniband-sdk-for-connectx-5-on-windows-system,"Hi, I’m new on Mellanox RDMA programming.I tried to find the Windows SDK corresponding to ibverbs in the Linux environment, but it doesn’t seem to be included in WinOF-2.Where can I get the SDK for Windows and related documentation for programming Infiniband?Hello Juhwan,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately, with WinOF-2, the SDK is not supported anymore, as it only applies for adapters up to ConnectX-3.Instead of the IBAL API, all development needs to be done against Microsoft’s Network Direct Interface (NDI) API.The Network Direct Interface (NDI) architecture provides application developers with a networking interface that enables zero-copy data transfers between applications, kernel-bypass I/O generation and completion processing, and one-sided data transfer operations.NDI is supported by Microsoft and is the recommended method to write RDMA application. NDI exposes the advanced capabilities of the Mellanox networking devices and allows applications to leverage advances of RDMA.Both RoCE and InfiniBand (IB) can implement NDI.For further information, please refer to: http://msdn.microsoft.com/en-us/library/cc904397(v=vs.85).aspxFor code examples using NDI, you may refer to: https://msdn.microsoft.com/library/cc853440(v=vs.85).aspxThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
46,hi-mellanox-team-how-do-we-connect-sfp28-port-of-server-dual-port25gbps-lan-card-to-qsfp28-port-mellanox-switch-p-n-msn2100-bb2f,"We are having 25GbE, 2-port, SFP28 Network Adapter (Mellanox CX4) card in Nutanix HCI solution and we are going to connect this card with Mellanox switch P/N: MSN2100-BB2F.Can you please let us know how do n we connect Mellanox 25Gbps LAN card with Mellanox MSN2100-BB2F switch ? It is a 40Gbps LAN switch and let us know how do we connect QSFP28+ port to SFP28 ? Need your inputs.Hello Chintan,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, you can connect the SN2100 to a 25GbE adapter with a SFP28 cable suitable for your adapter → https://www.mellanox.com/related-docs/prod_cables/PB_MCP2M00-Axxx_25GbE_SFP28_DAC.pdfThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
47,rping-maximum-size,"I’m trying to verify RDMA connectivity between two machines using rping.  It works with message size up to 1444 bytes, but fails above that.  Where does this number come from?  I’m trying to get ksmbd to work, and some folks there suggested I need to make sure rping works with 32K message size.  It looks like the server received the ping properly, but the client does not receive a response.Each machine has a ConnectX-6.  The linux server uses MLNX-EN driver 5.7 installed with --vma option.  The Windows client uses WinOF2 driver 2.90.Server output:Client output:Powered by Discourse, best viewed with JavaScript enabled"
48,mellanox-nic-didnt-link-with-qsfp-cable,"Hi,We have Mellanox NIC(MCX414-GCAT ConnectX-4 EN Network Interface Card, 50GbE dual-port QSFP28, PCIe3.0 x8, tall bracket). We try to connect FPGA card with THDM to QSFP+ cable(FS-> Q-8LCA020, +0G QSFP+8LC A0C) but did not link. Can we find out if the cable is compatible with the card? Or Is there a solution?Thx.Hi ,Please refer to the ConnectX-4 Firmware release notes :https://docs.mellanox.com/display/ConnectX4Firmwarev12282006/Firmware+Compatible+ProductsThanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
49,setpci-unable-to-change-the-pcie-maximum-read-request-and-payload-values,"Hi there,I’m setting up the Mellanox CX-5 (MCX515A-CCAT) NIC for my server with Ubuntu 20.04 LTS. When running a traffic generator, It can only achieve 60% of the line rate speed (in the order of 90Mpps with 64B packets). By reading the DPDK manual, I knew that changing the PCIe MaxPayload and MaxReadReq might be helpful.As recommended, I first checked out the current value with setpci -s 0b:00.0 68.w, the value was 0000 (the value is actually pretty wired). When I used setpci -s 0b:00.0 68.w=3000 to change the MaxReadReq from 128 bytes to 512 bytes, it completed without error (echo $? give 0). However, when I rechecked the value with setpci -s 0b:00.0 68.w, the value was still 0000 without any changes.I totally have no idea what was wrong in my configurations. Could anyone help me with this? Thanks in advance.WendiHello loganwhitevon,Thank you for posting your query on our community. Please ensure to check the system logs for any errors related to the PCIe configuration.
Additionally, you could also try to modify the PCIe max read request value from the system’s BIOS settings. Make sure to save the changes and reboot the system to apply the modifications.If the issue still persists, I would like to request you to submit a support ticket for further troubleshooting. The support ticket can be opened by emailing "" Networking-support@nvidia.com "". Please note that an active support contract would be required for the same. For contracts information, please feel free to reach out to our contracts team at "" Networking-Contracts@nvidia.com ""Thanks & Regards,
~ Nvidia SupportHi @sribhargavid,Thank you so much for replying. The dmesg messages are shown as follows, and it seems everything works fine. Apart from the MaxReadReq register, I have tried to reset other registers of this NIC, it does not work either.Incidentally, my NIC runs on a VM ESXi VM with PCIe passthrough to the VM. ESXi version is 6.5, OS version is Ubuntu 20.04, MLXN_OFED driver version is MLNX_OFED_LINUX-5.8-2.0.3.0-ubuntu20.04-x86_64, and NIC firmware is burned with fw-ConnectX5-rel-16_35_2000-MCX515A-CCA_Ax_Bx-UEFI-14.28.16-FlexBoot-3.6.805.bin.Can you please help me with the reason? Thank you.Hello loganwhitevon,Thank you for the information. Please refer to this link on “How to configure Nvidia network device in Passthrough mode on VMware ESXi6.x”
The provided lspci o/p is gathered from the VM I presume. Have you ran lspci on the physical machine and compared the o/p to check for discrepancies. I would suggest you to check with VMware on what PCIe registers could be modified in Passthrough mode.Thanks & Regards,
~ Nvidia SupportThank you @sribhargavid. I have exactly followed the instructions you mentioned. However, I can still not able to change the value. Maybe the virtualization layer did some filters that filters out the parameter setting instruction. I have also posted a question on the ESXi forum, but unfortunately, got not response yet. Thank you for your information though, much appreciated.LoganPowered by Discourse, best viewed with JavaScript enabled"
50,connectx-5-rocev2-reliable-connection-ack-vlan-issue,"Trying to support RoCEv2 in our product.  We have a particular network configuration where we segregate traffic using internal vlans.  Incoming traffic comes on a particular internal vlan (let’s say vlan-10), and we expect outgoing traffic to go over an external vlan (vlan-id different from that of internal vlan.  let’s say vlan-500).
gid table has the correct mapping…, and IP’s in the packet is also right. i.e.,
CLIENT-FACING-CLUSTER-IP → vlan-500
INTERNAL-IP → vlan-10For Reliable connection Ack’s, looks like firmware is not using gid table to set vlan-id in the Ack.  Looks to be a bug.  Can anyone confirm?  Any workaround?For reference, gid table is set in kernel (mlx5_core_roce_gid_set) using this structure.
struct mlx5_ifc_roce_addr_layout_bits {
u8         source_l3_address[16][0x8];};For eg., for above vlan-id’s, incoming packet has
src-ip: CLIENT-IP
dest-ip: CLIENT-FACING-CLUSTER-IP
vlan-id: 10My expectation is that RC Ack be built as:
src-ip: CLIENT-FACING-CLUSTER-IP
dest-ip: CLIENT-IP
vlan-id: 500Why think The RC ACK on internal VLAN not correct? Are you using VF QinQ?If ACK on external VLAN, then how the APP ack? It run internal VLAN, right?If you do have requirement ACK on external VLAN, then try use Verbs API strip cvaln.We have particular network design due to a switch and a bunch of internal vlans, hence the data comes in on a particular vlan, and goes out in a different vlan as explained in example above.Will check/try the cvlan option here and get back.Thanks.@xiaofengl : We use SRQ, and I dont see a way of running above verb on it.  Any other suggestions?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
51,nvue-transition-from-nclu,"Were are just now getting to migrating our network from the 4.x series over to 5.x series.  In doing so, we’re setting up demo environments to make sure that after transition we won’t experience issues, but we have noticed that NCLU is gone, and replaced by NVUE.After doing some basics with NVUE we’re a bit blown away at how incomplete it seems.  NVUE is missing all kinds of commands we would need to configure these devices and if we hand-edit the files those edits just get overwritten the next time an NVUE command is used.By all intents and purposes, it feels like NVUE is a no-go in production until it’s more mature but then when we are ready, if we use it, then it will overwrite nearly all configs.I am hoping that others have blazed this trail before us and have some advice on migrating to NVUE?  It almost feels like we would be better to migrate to using ansible at this point but for a while there Cumulus and Ansible we not playing well together.  The entire transition to NVIDIA really feels like Cumulus is getting back burnered.For commands not yet support, you can use the Snippets feature for the supported config files.
For other text based configuration files, there is the option to use Flexible Snippets.  More info here:
https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-53/System-Configuration/NVIDIA-User-Experience-NVUE/NVUE-Snippets/NVUE uses its own data to build the config files, which is independent to the content of the config files.  Once you have used NVUE to apply a configuration, it is advisable not to switch between NVUE and editing the flat file.  You should stick to the same method for configuration, otherwise as you have noted, the config file will get overwritten next time NVUE config is applied.
Also see warning from:
https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-53/System-Configuration/NVIDIA-User-Experience-NVUE/NVUE-CLI/There are also various tools to assist with migrating from NCLU to NVUE. Eg.Convert NCLU to NVUEhttps://air.nvidia.com/migrate/Joshua if there are specific commands you’re looking for let me know and I can make sure they are addressed (I am the Product Manager for NVUE).Thanks @epulvino I think the problem is two fold.Here’s the command I was trying to run until I saw @erichan and their reply:nv set interface swp2 link speed 1000and this was the result of that command:Error at speed: '1000' is not one of ['auto']I utilized “1G” instead and it appears to not be an issue, but outside of NVUE there’s further issues with this.According to 5.3 documentation, on Spectrum chipset, it’s no longer accepted to use link speed specification, everything is essentially auto negotiated and detected but I have unfortunately had ports that will not report the link as up unless a port speed is specified.This is making more sense now in context. Thank you for your quick response. It sounds like we still have a documentation bug though as the link speed setting is still very much supported. What is not allowed in the future is setting link speed and breakout together in ports.conf file.In the future, ports.conf will only be used for breakout config and /etc/network/interfaces will only be used for speed config. To NVuE users this distinction will be invisible as the config will always go to the right files, but for folks doing manual file edits, at some point ports.conf will not support taking a speed specification along with a breakout at the same time. (There were race conditions where ports.conf speed setting would fight the interface speed setting, and the system would not know which one should be respected if they did not match).If you can point me to the documentation location that is tripping the understanding up, I can seek to make sure it is cleaned up.Have a good Sunday!
-EricHere’s where the confusion lies:https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-53/Layer-1-and-Switch-Ports/Interface-Configuration-and-Management/Switch-Port-Attributes/For NVIDIA Spectrum ASICs , MTU is the only port attribute you can directly configure. The Spectrum firmware configures FEC, link speed, duplex mode and auto-negotiation automatically, following a predefined list of parameter settings until the link comes up. However, you can disable FEC if necessary, which forces the firmware to not try any FEC options.If you read this the way I did, since I am on Spectrum chipset, then I cannot set link speed because it’s automatically set.  But as you can see I can set it, and without setting it the port won’t even register as upI see the confusion here, we’re updating the documentation now. Thank you for pointing this out!(You should see the documentation update within 24 hours when the next pipeline gets pushed.)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
52,driver-problem-with-carp-multicast-on-freebsd,"Hi,I’ve got 2 servers, one FreeNAS 11.3 (FreeBSD) and one Debian 10 and both are using ConnectX-3 (mcx311a-xcat). Both are connected to a switch using tagged vlan.On the Debian server everything works fine and the ConnectX-3 can send and recieve Unicast/Multicast traffic. But on the FreeNAS host only Unicast works in both direction and the ConnectX-3 can send but not receive CARP packets.The switch shouldn’t be the problem because if I use a Intel NIC instead of the ConnectX-3 on the FreeNAS host, receiving CARP packets is working fine.I also tried it with a direct connection (two ConnectX3 connected by DAC) without the switch between and the same problem.I also flashed the NICs to the latest firmware and swapped them bwtween the servers to verify that this is not a hardware problem.Are there any known Bugs that could cause this behavior?Or any settings I need to take care of to allow receiving multicast traffic like CARP on FreeBSD?Logs:Feb 23 08:41:45 BM-Homeserver kernel: mlx4_core: Mellanox ConnectX core driver v3.5.1 (April 2019)Feb 23 08:41:45 BM-Homeserver kernel: mlx4_core: Initializing mlx4_coreFeb 23 08:41:45 BM-Homeserver mlx4_core0: Unable to determine PCI device chain minimum BWFeb 23 08:41:45 BM-Homeserver kernel: mlx4_en mlx4_core0: Activating port:1Feb 23 08:41:45 BM-Homeserver mlxen0: Ethernet address: f4:52:14:89:23:b0Feb 23 08:41:45 BM-Homeserver kernel: mlx4_en: mlx4_core0: Port 1: Using 8 TX ringsFeb 23 08:41:45 BM-Homeserver kernel: mlx4_en: mlx4_core0: Port 1: Using 8 TX ringsFeb 23 08:41:45 BM-Homeserver kernel: mlxen0: link state changed to DOWNFeb 23 08:41:45 BM-Homeserver kernel: mlxen0: link state changed to DOWNFeb 23 08:41:45 BM-Homeserver kernel: mlx4_en: mlx4_core0: Port 1: Using 8 RX ringsFeb 23 08:41:45 BM-Homeserver kernel: mlx4_en: mlx4_core0: Port 1: Using 8 RX ringsFeb 23 08:41:45 BM-Homeserver kernel: mlx4_en: mlxen0: Using 8 TX ringsFeb 23 08:41:45 BM-Homeserver kernel: mlx4_en: mlxen0: Using 8 TX ringsFeb 23 08:41:45 BM-Homeserver kernel: mlx4_en: mlxen0: Using 8 RX ringsFeb 23 08:41:45 BM-Homeserver kernel: mlx4_en: mlxen0: Using 8 RX ringsFeb 23 08:41:45 BM-Homeserver kernel: mlx4_en: mlxen0: Initializing portFeb 23 08:41:45 BM-Homeserver kernel: mlx4_en: mlxen0: Initializing portFeb 23 08:41:45 BM-Homeserver mlxen0: tso4 disabled due to -txcsum.Feb 23 08:41:45 BM-Homeserver mlxen0: tso6 disabled due to -txcsum6.Feb 23 08:41:45 BM-Homeserver kernel: mlxen0: link state changed to UPFeb 23 08:41:45 BM-Homeserver kernel: mlxen0: link state changed to UPFeb 23 08:49:59 BM-Homeserver mlx5en: Mellanox Ethernet driver 3.5.1 (April 2019)Hi Björn,I see that you’re working with OFED 3.5.1, which is for ConnectX-4 NIC and above.Please try with v2.1.6 - note that it’s supported only on FreeBSD 10.1 OS.Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
53,sn2100,"Hello,
I currently have a problem that interfaces show a link by LED. I also see incoming multicast traffic with the command “show interface ethernet rates”. Autonegotation of the ports has also worked, but the operational state of the ports remains down.
What could be the reason for this?Thanks a lot
Harrytry:show interfaces ethernet 1/x link-diagnosticsPowered by Discourse, best viewed with JavaScript enabled"
54,both-connectx-5-vpi-adapter-and-mcx4121a-acat-connectx-4-lx-en-adapter-one-machine,"Powered by Discourse, best viewed with JavaScript enabled"
55,will-the-trx-10gsfp-sr-mlx-plugged-into-a-qxg-10g2sf-cx4-work-in-a-ubiquiti-environment,"Hi,Will the TRX-10GSFP-SR-MLX plugged into a QXG-10G2SF-CX4 work in a Ubiquiti environment? I am just looking to see what transceiver might work best. Here is the breakdown of Ubiquiti environmentUDM-Network 6.5.55UDM-Pro 1.11.0.3921USW-AggregationHi Brian,Both TRX-10GSFP-SR-MLX and QXG-10G2SF-CX4 are QNAP products, NOT NVIDIA Mellanox product. QXG-10G2SF-CX4 integrate NVIDIA product as controller module into their product.It’s better inquire QNAP. Below is more info related with them.https://atgbics.com/products/qnap-sfp-trx-10gsfp-sr-mlx-fibre-optic-transceiverhttps://www.qnap.com/en/product/qxg-10g2sf-cx4Regards,LeveiPowered by Discourse, best viewed with JavaScript enabled"
56,release-notes-for-nvidia-bright-cluster-manager-9-1-16,"Release notes for Bright 9.1-16== General ==
=New Features==Improvements==Fixed Issues=== CMDaemon ==
=New Features==Improvements==Fixed Issues=== Node Installer ==
=Improvements==Fixed Issues=== Cluster Tools ==
=Improvements==Fixed Issues=== Machine Learning ==
=New Features==Changes and Deprecated Features=== cm-clone-install ==
=Fixed Issues=== cm-scale ==
=Fixed Issues=== cm-wlm-setup ==
=Fixed Issues=== cmsh ==
=Fixed Issues=== jupyter ==
=Improvements==Fixed Issues=== licensing ==
=Fixed Issues=== slurm22.05 ==
=Improvements=Powered by Discourse, best viewed with JavaScript enabled"
57,as-described-in-datasheet-for-connectx-6-there-are-8-pfs-and-up-to-1k-vfs-per-port-however-the-parameter-num-of-vfs-can-only-be-set-to-127-at-max-and-there-are-two-physical-ports-so-how-can-we-configure-connectx-6-with-8-pfs-and-1k-vfs,"As described in datasheet, for ConnectX-6, there are 8 PFs and up to 1K VFs per port, however, the parameter [NUM_OF_VFS] can only be set to 127 at max and there are two physical ports. So, how can we configure ConnectX-6 with 8 PFs and 1K VFs.Hi Jianquan,There is a limitation for the number of VFs in the firmware, which is mentioned in the firmware’s release-notes.For example - ConnectX-6, FW 20.31.1014 RN:https://docs.mellanox.com/display/ConnectX6Firmwarev20311014/Known+IssuesAs you can see, the max number of VFs is 127 per port.This number can not be exceeded.Thanks,ChenThe following content is selected from datasheet of ConnectX-6Virtualization/Cloud NativeSingle Root IOV (SR-IOV) and VirtIO accelerationUp to 1 K VFs per port8 PFsAnd it is contradicts with the actual situation?Yes, the limitation is in the firmware.Thanks,ChenSo when will Mellanox release the firmware to support 1K VFs?Currently there’s no such plan.Thanks,ChenPowered by Discourse, best viewed with JavaScript enabled"
58,where-to-get-regex-version-5-8-for-bluefield-2,"Error: Invalid argument/option provided for --regex-version: 5.8Powered by Discourse, best viewed with JavaScript enabled"
59,dcqcn-config-with-cx6,"Hello Surinderjeet,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided and required, please review the following community articles:Even though the articles mention the ConnectX-4, it is similar for the ConnectX-6.As DCQCN configuration is driven from a f/w perspective, you can enable it with INBOX drivers as well. Be aware support for INBOX drivers is provided by the OS Distro vendor.If you need any further assistance, we recommended to open a NVIDIA Networking support ticket (Valid support contract required), by sending an email to networking-support@nvidia.comThank you and regards,~NVIDIA Networking Technical Supportthe site is not access, pls update the linkPowered by Discourse, best viewed with JavaScript enabled"
60,mellanox-technologies-mt2892-ethernet-mode,"Hi,
We are using the Mellanox Technologies MT2892 Family on ethernet mode:After the firmware update, we got a peek of 80 Gb/s; however, after 1/2 hours after the firmware upgrade top transfer speed was 3.58Gb/s,Any hints to fix this?tksHello @fabio.andrijauskas,Thank you for posting your query on our community. Please refer to the below link to ensure that you are using firmware compatible products:
https://docs.nvidia.com/networking/display/ConnectX6DxFirmwarev22371014/Firmware+Compatible+ProductsIf the issue still persists, I would like to request submitting a support ticket for further troubleshooting. The support ticket can be opened by emailing "" Networking-support@nvidia.com ""Please note that an active support contract would be required for the same. For contracts information, please feel free to reach out to our contracts team at "" Networking-Contracts@nvidia.com ""Thanks & Regards,
BhargaviPowered by Discourse, best viewed with JavaScript enabled"
61,how-to-set-rss-in-dpdk-18-11-about-mlx5,"how to set rss correctly, can let all queue receive packets ? this is my rss configure as follows:but only queue 0 receive packets, other queues display 0 packets;command as follows: /root/dpdk-18.11/app/test-pmd/testpmd -w 0000:3c:00.0 -l 0-9 -n 4 – --rxq=10 --txq=10 --rxd=256 --txd=4096 -i --nb-cores=9testpmd> show port xstats allrx_good_packets: 56
tx_good_packets: 18
rx_good_bytes: 4932
tx_good_bytes: 1767
rx_missed_errors: 0
rx_errors: 0
tx_errors: 0
rx_mbuf_allocation_errors: 0
rx_q0packets: 56
rx_q0bytes: 4932
rx_q0errors: 0
rx_q1packets: 0
rx_q1bytes: 0
rx_q1errors: 0
rx_q2packets: 0
rx_q2bytes: 0
rx_q2errors: 0
rx_q3packets: 0
rx_q3bytes: 0
rx_q3errors: 0
rx_q4packets: 0
rx_q4bytes: 0
rx_q4errors: 0
rx_q5packets: 0
rx_q5bytes: 0
rx_q5errors: 0
rx_q6packets: 0
rx_q6bytes: 0
rx_q6errors: 0
rx_q7packets: 0
rx_q7bytes: 0
rx_q7errors: 0
rx_q8packets: 0
rx_q8bytes: 0
rx_q8errors: 0
rx_q9packets: 0
rx_q9bytes: 0
rx_q9errors: 0Powered by Discourse, best viewed with JavaScript enabled"
62,bus-error-core-dump-on-bluefield-2,"I have been wondering the cause behind a “Bus Error (core dump)” message whenever I try to curl a package or a script. Has anyone faced similar issues on bluefield 2 cards?Hi,Sadly, it is a known issue due to a bug in OpenSSL, see full details here. We are actively monitoring the patch process of this bug, and when the fix will reach an official package we would install it as part of the BF2 image.The same issue also effects wget (not only curl). Please note that updating the config file itself will also affect IPSEC support, which relies on the same OpenSSL config file, and which expects the PKA acceleration.Hope that this answers your question.Powered by Discourse, best viewed with JavaScript enabled"
63,is-bluefield-smartnic-l3-cache-divide-into-some-slices,"From Intel Sandy Bridge and forward, the last level cache is divided into some slices.  I wonder if ARM L3 cache has a similar non-uniform characteristic and if bluefield-smartnic (e.g., BlueField MBF1L516A-CSCAT) has a a similar non-uniform characteristic.Hello,In BF-2 there is 6MB L3 cache that supports all cores.
We are not aware of ‘slices’.
Can you elaborate your question please?Best Regards,
VikiCan you elaborate your question pleaseI am very sorry for not explaining clearly the meaning of “slice”.  In fact, this may be the concept in Intel. Intel’s micro-architecture, from Sandy Bridge and forward, re-designed the LLC by dividing the LLC into multiple slices.The total size of the LLC in the above figure is 20MB, and the size of each slice is 2.5MB (20MB/8).  The CPU cores and all LLC slices are interconnected by a bi-directional bus. However, due to the differences in paths between a given core and the different slices (aka non-uniform cache architecture), accessing data stored in a closer slice is faster than accessing data stored in other slices (e.g., CPU 0 access slice 0 faster than other slices).I wonder if ARM L3 cache has a similar non-uniform characteristic and if bluefield-smartnic (e.g., BlueField MBF1L516A-CSCAT) has a similar non-uniform characteristic.Regards,I am very sorry for not explaining the “slices” clearly. I have re-uploaded the detailed explanation about “slices”.Hello,From the image you shared, you are asking about BF-1 and not BF-2 (MBF1L516A-CSCAT part number) which have 2 L3 caches (have 2 DRAM controllers), and it also has 16 cores.
It may be that there are some minor “distance” latency differences between “close” and “far” cores.Best Regards,
VikiCould you share some manuals or papers that introduce the L3 cache structure of the bluefield-smartnic (preferably BlueField MBF1L516A-CSCAT) in detail?Best RegardsPowered by Discourse, best viewed with JavaScript enabled"
64,does-this-connect-x-3-pro-card-3b00-support-infiniband,"My vendors have shipped as an Infiniband card a ConnectX-3 Pro card which identifies as an ethernet controller:3b:00.0 Ethernet controller: Mellanox Technologies MT27520 Family [ConnectX-3 Pro]ibv_devinfo says[root@dstorage13 ~]# ibv_devinfohca_id: mlx4_0transport: InfiniBand (0)fw_ver: 2.42.5000node_guid: 1c34:da03:00e5:5650sys_image_guid: 1c34:da03:00e5:5650vendor_id: 0x02c9vendor_part_id: 4103hw_ver: 0x0board_id: DEL2300000023phys_port_cnt: 2Device ports:port: 1state: PORT_DOWN (1)max_mtu: 4096 (5)active_mtu: 1024 (3)sm_lid: 0port_lid: 0port_lmc: 0x00link_layer: Ethernetport: 2state: PORT_DOWN (1)max_mtu: 4096 (5)active_mtu: 1024 (3)sm_lid: 0port_lid: 0port_lmc: 0x00link_layer: Ethernetand when I try to switch to Infiniband (which looks as though it might be possible) I getconnectx_port_configConnectX PCI devices :|----------------------------|| 1 0000:3b:00.0 ||----------------------------|Before port change:etheth|----------------------------|| Possible port modes: || 1: Infiniband || 2: Ethernet || 3: AutoSense ||----------------------------|Select mode for port 1 (1,2,3): 1Select mode for port 2 (1,2,3): 1WARNING: Illegal port configuration attempted,Please view dmesg for details.…[ 1916.433846] mlx4_core 0000:3b:00.0: Requested port type for port 1 is not supported on this HCA[ 1916.434592] mlx4_core 0000:3b:00.0: Requested port type for port 2 is not supported on this HCAI am guessing that Dell have just shipped an ethernet-only card but would be grateful for confirmation before I go back to them.Hi Martin,DEL2300000023 does not support InfiniBand, it’s an Ethernet-only card (not VPI).Regards,ChenThank you Chen!Hi all,do these DELL Ethernet-only cards work with SX6036 switches?Is the UPGR-6036-GW InfiniBand to Ethernet gateway software license needed on the switch to make them work at all?Kind regards,RichardPowered by Discourse, best viewed with JavaScript enabled"
65,gpudirect-rdma-at-the-ibverbs-level,"The goal is simple. I want to use GPU Direct RDMA at the ibverbs level. II do not want to use any cuda aware MPI implementation because I require a greater level of control. I want to perform transfers from a gpu to a remote Host using GPU Direct RDMA.After rummaging through a couple of scattered posts, my understanding is that I only need to install the nv_peer_mem module. Then ibverbs can differentiate between a GPU pointer and a main memory pointer automatically and perform necessary changes. So I basically can cudaMalloc(&gpu_ptr, size) then register the gpu pointer using ibv_reg_mr and continue normally through my application.Is this understanding correct?My setup if that helps:I am running a Ubuntu 18.04.4 machine with a Mellanox ConnectX-5 NIC and a V100 GPU. I have OFED 4.6, GPU Driver Version 455.32 , and CUDA Version:11.1.I am able to run CUDA kernels without issue. And I am able to run RDMA using ibverbs without issue. My main goal is to run RDMA to perform send and receive operations using GPU memory.Thanks for the helpHi,Right, to use rdma/gpu memory, you only need to register the GPU memory by ibv_reg_mem.You can find different sample of perftest package using flag --cuda to see how it is implemented.Google “perftest cuda”RegardsMarcHere a full testInstructions:rm -rf /etc/perftestcd /etcgit clone https://github.com/linux-rdma/perftest/cd perftest./autogen.sh./configure CUDA_PATH=/hpc/local/oss/cuda10.2/cuda-toolkit/ CUDA_H_PATH=/hpc/local/oss/cuda10.2/cuda-toolkit/include/cuda.hmakemake installThe output of the (GPU) memory allocation should look as below.[root@l-csi-1123s gdr]# ib_write_bw -d mlx5_0 -x 3 --tclass=96 --report_gbits --run_infinitely --disable_pcie_relaxed --CPU-freq --use_cuda=0initializing CUDAListing all CUDA devices in system:CUDA device 0: PCIe address is 1C:00CUDA device 1: PCIe address is 41:00Picking device No. 0[pid = 54188, dev = 0] device name = [Tesla V100S-PCIE-32GB]creating CUDA Ctxmaking it the current CUDA CtxcuMemAlloc() of a 131072 bytes GPU bufferallocated GPU buffer address at 00007fb9dfa00000 pointer=0x7fb9dfa00000Thanks for the prompt reply.Its good to confirm that there is nothing special in the code to be done and I can simply pass a GPU pointer to ibv_reg_mr.Your suggested full test:I went through your steps to perform the test but I get the --disable_pcie_relaxed option is not recognized. And running without it produces errors. I have removed most of the options and just used --use_cuda=0 which seems to work fine without any error.My main test that I am trying to pull off:I was using NCCL to test that GDR is working. Running the NCCL tests however yielded a completion status 0x4 error. So I read that I had to disable PCIe ACS which I did and now NCCL tests run fine.And I am also able to run my own GDR code for the following scenarios:RDMA WriteGPU → GPU (done)CPU → GPU (done)GPU → CPU (done)RDMA Write ImmediateGPU → GPU (done)CPU → GPU (done)GPU → CPU (done)RDMA ReadGPU → GPU (Segmentation Fault at ibv_poll_cq at receiver)CPU → GPU (Segmentation Fault at ibv_poll_cq at receiver)GPU → CPU (done)Send/ReceiveGPU → GPU (Segmentation Fault at ibv_poll_cq at receiver)CPU → GPU (Segmentation Fault at ibv_poll_cq at receiver)GPU → CPU (done)Send/Receive with immediateGPU → GPU (Segmentation Fault at ibv_poll_cq at receiver)CPU → GPU (Segmentation Fault at ibv_poll_cq at receiver)GPU → CPU (done)’ → ’ refers to communication across two different machines.CPU refers to memory allocated using normal malloc and GPU refers to memory allocated using cudaMalloc.Any Idea what may make ibv_poll_cq cause a segmentation fault for the above situations?I have been at this for a week now so I really appreciate any help.Hi,Please open a new case to NVIDIA Networking support.RegardsMarcPowered by Discourse, best viewed with JavaScript enabled"
66,mlx4-core-communication-channel-command-0x5-op-0x24-timed-out,"One of our Linux servers, running SUSE x86_64 Linux got the following error, and the network interfaces went down.Looking for any guidance on what this could be and how to address it:ct 10 21:36:32 mtb0120qpr5 kernel: [4238431.515819] mlx4_core 0000:00:14.0: communication channel command 0x5 (op=0x24) timed outOct 10 21:36:32 mtb0120qpr5 kernel: [4238431.515825] mlx4_core 0000:00:14.0: device is going to be resetOct 10 21:36:32 mtb0120qpr5 kernel: [4238431.515829] mlx4_core 0000:00:14.0: VF is sending reset request to FirmwareOct 10 21:36:32 mtb0120qpr5 kernel: [4238431.516494] mlx4_core 0000:00:14.0: VF Reset succeedOct 10 21:36:32 mtb0120qpr5 kernel: [4238431.516495] mlx4_core 0000:00:14.0: device was reset successfullyOct 10 21:36:32 mtb0120qpr5 kernel: [4238431.516496] <mlx4_ib> mlx4_ib_handle_catas_error: mlx4_ib_handle_catas_error was startedOct 10 21:36:32 mtb0120qpr5 kernel: [4238431.516507] <mlx4_ib> mlx4_ib_handle_catas_error: mlx4_ib_handle_catas_error endedOct 10 21:36:32 mtb0120qpr5 kernel: [4238431.516508] mlx4_en 0000:00:14.0: Internal error detected, restarting deviceOct 10 21:36:32 mtb0120qpr5 kernel: [4238431.516512] infiniband mlx4_0: ib_query_pkey failed (-5) for index 18Oct 10 21:36:32 mtb0120qpr5 kernel: [4238431.516516] infiniband mlx4_0: ib_query_port failed (-5)Oct 10 21:36:32 mtb0120qpr5 kernel: [4238431.795281] ib1: post_send_rss failed, error -5The following is the lspci for the IB card:00:14.0 0280: 15b3:1004Subsystem: 15b3:61b0Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- SERR- <PERR- INTx-Latency: 0Interrupt: pin A routed to IRQ 25Region 2: Memory at fa000000 (64-bit, prefetchable) [size=8M]Capabilities: [60] Express (v2) Endpoint, MSI 00DevCap: MaxPayload 512 bytes, PhantFunc 0, Latency L0s <64ns, L1 <1usExtTag- AttnBtn- AttnInd- PwrInd- RBE- FLReset+DevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported-RlxdOrd- ExtTag- PhantFunc- AuxPwr- NoSnoop- FLReset-MaxPayload 128 bytes, MaxReadReq 128 bytesDevSta: CorrErr- UncorrErr- FatalErr- UnsuppReq- AuxPwr- TransPend-LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM unknown, Latency L0 <64ns, L1 <1usClockPM- Surprise- LLActRep- BwNot-LnkCtl: ASPM Disabled; RCB 64 bytes Disabled- Retrain- CommClk-ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-LnkSta: Speed unknown, Width x0, TrErr- Train- SlotClk- DLActive- BWMgmt- ABWMgmt-DevCap2: Completion Timeout: Range ABCD, TimeoutDis+DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis-LnkCtl2: Target Link Speed: 2.5GT/s, EnterCompliance- SpeedDis-, Selectable De-emphasis: -6dBTransmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-Compliance De-emphasis: -6dBLnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete-, EqualizationPhase1-EqualizationPhase2-, EqualizationPhase3-, LinkEqualizationRequest-Capabilities: [9c] MSI-X: Enable+ Count=4 Masked-Vector table: BAR=2 offset=00002000PBA: BAR=2 offset=00003000Capabilities: [40] Power Management version 0Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0-,D1-,D2-,D3hot-,D3cold-)Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME-Kernel driver in use: mlx4_coreKernel modules: mlx4_coreThe following is a few of the Mellanox RPMs we have installed:mlnx-ofa_kernel-4.7-OFED.4.7.3.2.9.1.g457f064.sles11sp4mlnx-ofa_kernel-devel-4.7-OFED.4.7.3.2.9.1.g457f064.sles11sp4mlnx-ofa_kernel-modules-4.7-OFED.4.7.3.2.9.1.g457f064.kver.3.0.101_107_defaultThanks.Hello Greg,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the opcode, we researched internally and found that this issue was resolved a long time ago in f/w and driver update.Please update the f/w and driver to the latest version available, for ConnectX-3 adapters this is MLNX_OFED 4LTS 4.9 and f/w depending on the PSID of the adapter, version 2.4x.xxxx.If you still experiencing this issue after updating the driver and f/w, please do not hesitate to open a NVIDIA Networking Support Ticket by sending and email to networking-support@nvidia.comWe will gladly assist you through the support ticket.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
67,release-notes-for-nvidia-bright-cluster-manager-8-1-27,"Release notes for Bright 8.1-27== General ==
=Improvements=== CMDaemon ==
=Fixed Issues=== Node Installer ==
=Fixed Issues=== Machine Learning ==
=New Features==Improvements=Powered by Discourse, best viewed with JavaScript enabled"
68,centos-8-stream-centos-9-stream,"Hi,
I tried to figure out if there is a driver  and ofed stack available for Centos 8 stream and/or centos 9 stream ( ConnectX-5 and onwards) but came up short.  Also, the only thing that caught my eye on this topic on the forums was this posting from a while back already:Is there by any chance some news concerning this matter?Thanks for your insight and help!–
VesaHello Vesa.Simola,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.CentOS 8 Stream is supported in MLNX_OFED 5.6 GA as a Community Supported OS.The following link will provide you the information on how-to install → https://docs.nvidia.com/networking/display/MLNXOFEDv561033/Installing+MLNX_OFED#InstallingMLNX_OFED-InstallationonCommunityOperatingSystemscommunityMLNX_OFED versions < 5.6 do not contain support for CentOS StreamThank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
69,c76-with-4-18-0-348-23-1-1-ga8e8b87-el7-1-x86-64-install-mlnx-ofed-linux-5-4-1-0-3-0-faied,"configure:5445: checking if Linux was built with CONFIG_XEN
configure:5469: cp conftest.c build && env CROSS_COMPILE= make -d modules LD=ld CC=gcc -f /tmp/MLNX_OFED_LINUX-5.4-1.0.3.0-4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/mlnx_iso.11168/OFED_topdir/BUILD/mlnx-ofa_kernel-5.4/source/compat/build/Makefile MLNX_LINUX_CONFIG=/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/.config LINUXINCLUDE=-include generated/autoconf.h -I/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/arch/x86/include -Iarch/x86/include/generated -Iinclude -I/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/arch/x86/include/uapi -Iarch/x86/include/generated/uapi -I/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/include -I/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/include/uapi -Iinclude/generated/uapi -I/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/arch/x86/include -Iarch/x86/include/generated -I/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/arch/x86/include -I/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/arch/x86/include/generated -I/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/include -I/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/include -I/usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/include2 -include /usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/include/linux/kconfig.h -o tmp_include_depends -o scripts -o include/config/MARKER -C /usr/src/kernels/4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64 EXTRA_CFLAGS=-Werror-implicit-function-declaration -Wno-unused-variable -Wno-uninitialized  CROSS_COMPILE= M=/tmp/MLNX_OFED_LINUX-5.4-1.0.3.0-4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64/mlnx_iso.11168/OFED_topdir/BUILD/mlnx-ofa_kernel-5.4/source/compat/build >/dev/null 2>build/output.log; [ 0 -ne 0 ] && cat build/output.log 1>&2 && false || config/warning_filter.sh build/output.log
In file included from ././include/linux/compiler_types.h:58:0,
from :0:
include/linux/compiler-gcc.h:329:5: warning: “__has_attribute” is not defined [-Wundef]
#if __has_attribute(no_sanitize_address)
^
include/linux/compiler-gcc.h:329:20: error: missing binary operator before token “(”
#if __has_attribute(no_sanitize_address)
^
include/linux/compiler-gcc.h:335:52: error: missing binary operator before token “(”
#if defined(SANITIZE_THREAD) && __has_attribute(no_sanitize_thread)Hi,The default kernel version of CentOS 7.6 is 3.10.0-957 (CentOS - Wikipedia).The kernel version “4.18.0-348.23.1.1.ga8e8b87.el7.1.x86_64” which you are using is newer kernel and seems like a customzied one.Here are my suggestions:Upgrade gcc version to the latest version and try to “mlnxofedinstall --ovs-dpdk --upstream-libs --add-kernel-support --skip-distro-check”.Please try to downgrade the kernel version to the default version 3.10.0-957 ?Please try to use a newer version CenOS 8.6(or higher) which using the default 4.18.0-xxx kernel version?Longran Wei
Nvidia Support Teamwhen i upgrade gcc to 8.3, and mlnxofedinstall --ovs-dpdk --upstream-libs --add-kernel-support --skip-distro-check
image1532×959 210 KB
Powered by Discourse, best viewed with JavaScript enabled"
70,gid-table-limit,"We use ConnectX5 card.  I can see only 255 entries in GID table.  Is there a way to increase this number to say 1023 entries?  Is it configurable in any way?  For supporting large number of IPs, gid table size is becoming a blocker.  Or, does increasing GID table size come at cost?I see this in kernel for gid table size, but dont see how to configure or change.
“roce_address_table_size” in mlx5_ifc_roce_cap_bits.Hi jpaul2Could you please open a CASE?
we need to double-check it with our engineering. For that, we need CASE number.if you OFED source code, gid is defined for “const unsigned 8bit”1327 int mlx5_core_roce_gid_set(struct mlx5_core_dev *dev, unsigned int index,
1328                            u8 roce_version, u8 roce_l3_type, *const u8 gid,  <=============
1329                            const u8 *mac, bool vlan, u16 vlan_id, u8 port_num);/HyungKwangPowered by Discourse, best viewed with JavaScript enabled"
71,jetson-agx-connextx-6-dx-dpdk-performance-issue,"Hello,
I would like to understand what is the maximum performance is expected in Jetson AGX, ConnextX-6 DX and DPDK setup.
ConnextX-6 DX is installed in to PCIe slot.
Right now I got 40Gbps, when traffic is: one directional, packet size 8K, no memory copy.
In a case two ConnextX ports are receiving and two DPDK tasks(one per port) assigned to different LCOREs the performance is divided 20Gb/20Gb per LCORE.
There are no interrupts during the test.
It sounds as DRAM paging issue(hugepage…).Could someone provide directives how to improve the performance?Thanks in advanceWe not test DPDK on AGX and release performance result for that.If you are looking for solution or POC for AGX, you can open topic on AGX thread.There is our performance report for DPDK form DPDK.org list all test result and tuning guide. FYI.https://core.dpdk.org/perf-reports/1194.96 KBthanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
72,how-to-limit-fan-rpm-on-sn2010,"Hello,I’m looking to limit fan RPM on an SN2010 that will be installed near users, so it can’t be too loud. Similar to this question: How to reduce FANs RPM on Cumulus Linux (Switch SN2410)? - Cumulus Linux / General - NVIDIA Developer ForumsI’m running Cumulus Linux 4.3.0. The fae command and the fancontrol command don’t seem to be available.Thanks in advance for your help.Same thread elsewhere: how to mod fan speed of mellanox sn2100 or sn2010 ? | ServeTheHome ForumsHave you tried using mdreq instead?That previous thread was remarking on a specific bug affecting a specific switch model on a specific version of software which caused the fans to run at full tilt all the time.In Cumulus Linux, the fan speeds dynamically adjust themselves based on external temperatures.
As Attilla states, there is no supported method for lowering fan speeds.
Individual platforms set cutoff thresholds for these sensors and critical temperatures for each sensor in the platform files.
You can view them here: cat /lib/python2.7/dist-packages/cumulus/platforms/mlnx_SN2xxx.py
But changing these values can void your warranty as you are effectively preventing the box from appropriately cooling itself so I would not recommend doing this, even for testing.Thanks for your help. We’ve worked out a solution that will allow us to put it in a local network closet instead.Powered by Discourse, best viewed with JavaScript enabled"
73,need-onyx-3-8-2204,"Hello, can anyone help to obtaint ONYX 3.8.2204 for mlx-msn2010 ?Powered by Discourse, best viewed with JavaScript enabled"
74,about-vma-support-with-connectx-6-vpi-hcas,"Hi,Is there a possibility for a client-server application communicating through TCP sockets to benefit (in terms of throughput) from the VMA library on a RHEL8.3 (Ootpa) cluster interconnected through an infiniband network and Mellanox(Nvidia) ConnectX-6 VPI HCAs ?The documentation (A VMA Basic Usage · Mellanox/libvma Wiki · GitHub) appears to be silent even on the subject of the availability of this bypass library.
VPI support is mentionned here  [Introduction - MLNX_OFED v5.4-3.0.3.0 - NVIDIA Networking Docs (https://docs.nvidia.com/networking/display/MLNXOFEDv543030/Introduction)
but not for ConnectX-6 VPI.Our compute nodes are powered by 2 Intel Ice Lake processors (40 cores each).Software installed:rpm -qa | grep -E “rdma|infiniband|uverbs|ucx|vma” | grep -v “kernel-kernel”infiniband-diags-54mlnx1-1.54103.x86_64
ucx-devel-1.11.0-1.54103.x86_64
ucx-ib-1.11.0-1.54103.x86_64
librdmacm-54mlnx1-1.54103.x86_64
librdmacm-utils-54mlnx1-1.54103.x86_64
ucx-cma-1.11.0-1.54103.x86_64
rdma-core-devel-54mlnx1-1.54103.x86_64
ucx-rdmacm-1.11.0-1.54103.x86_64
rdma-core-54mlnx1-1.54103.x86_64
ucx-knem-1.11.0-1.54103.x86_64
ucx-1.11.0-1.54103.x86_64(No libvma currently installed …)Thanks for any information on the subject.Jean-MarcHello Jean-Marc,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Yes, As per official documentation → https://docs.nvidia.com/networking/display/VMAv940/NVIDIA+Messaging+Accelerator+(VMA)+Documentation+Rev+9.4.0“The NVIDIA® Messaging Accelerator (VMA) library accelerates latency-sensitive and throughput-demanding TCP and UDP socket-based applications by offloading traffic from the user-space directly to the network interface card (NIC) or Host Channel Adapter (HCA), without going through the kernel and the standard IP stack (kernel-bypass).”VMA will support both Ethernet and InfiniBand Technologies, and that includes the ConnectX-6 VPI adapter as well.See the documentation link for all relevant information and use cases related to VMA.Be aware, VMA specific support can only be obtained by an official VMA Support entitlement. The GitHub will give you some guidance but no support.Thank you and regards,
~NVIDIA Networking Technical SupportHi,Requirements for VMA v9.4.0 saysI guess the VMA library needs to be installed along with an upgrade
of our OFED package if this is to have any chance of  working ?Thanks for your reply.JMThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
75,vsan-rdma-question,"Im using vsan setup with rdma enabled.
I use connect x-6 dual port 100 g nics.From the switch end, i have changed the max mtu as 9000,however, from the  card, i see is max 4096(its printed on the card by default) why its showing like that?Note, i use DVS from vcenter where i have already set mtu as 9000.For rdma, previously it was showing as cee mode and tried to change it to ieee mode but no luck from esxi end.Now it says mode is invalid/unknown.Could you please help?Hi,Thanks for your question.
The maximum possible MTU for RDMA is 4k.
Please adjust it to 4k in all the relevant places in your RDMA environment.Best Regards,
AnatolyHi Anatoly,
Happy new year 2023 and thanks for the response.I have few queries. Could you please help with this?8 votes and 5 comments so far on Reddit2.If we disable the RDMA on 100G NIC,then the NIC will support 9000 MTU ?3.Do we need to upgrade the latest firmware version? If yes,please suggest the version no for VSAN 8.0Thanks,
RajCould you please update me the correct to change the RDMA mode in to IEEE?I used these command,but still it says invalid/unknown from ESXi/VSAN end//opt/mellanox/bin/mlxconfig -d mt4117_pciconf0 set LLDP_NB_DCBX_P1=1 LLDP_NB_RX_MODE_P1=2 LLDP_NB_TX_MODE_P1=2 LLDP_NB_DCBX_P2=1 LLDP_NB_RX_MODE_P2=2 LLDP_NB_TX_MODE_P2=2 DCBX_WILLING_P1=1 DCBX_IEEE_P1=1 DCBX_CEE_P1=0 DCBX_WILLING_P2=1 DCBX_IEEE_P2=1 DCBX_CEE_P2=0Thanks
rajPowered by Discourse, best viewed with JavaScript enabled"
76,how-to-setup-and-deploy-sharp-enabled-network-in-k8s-docker-environment,"Hi there,
Is there any tutorial about how to setup and deploy SHARP in a docker environment?
Should I setup sharp_am/sharpd service inside docker or in the physical hosts?Powered by Discourse, best viewed with JavaScript enabled"
77,nvmeof-target-offload-setup-on-connect-x,"Hi All,I am following the guide to setup NVMEof Target Offload.
https://enterprise-support.nvidia.com/s/article/howto-configure-sr-iov-for-connect-ib-connectx-4-with-kvm--infiniband-xFollowing the same steps executing the step.
echo 1 > /sys/kernel/config/nvmet/subsystems/testsubsystem/attr_offloadI am getting an error permission denied. Running all commands in the root mode.testsubsystem does not contain any file with name attr_offload. We also cannot create a new file in this directory since it is /sys/kernel which contains temporary files generated by Kernel.Using the Ofed version 5.9 with --with-nvmf flags. Centos 8.3 version.Any help regarding this would be helpful.Regards,
Umer IdreesAble to resolve this issue by using the following flags while installing ofed.sudo ./mlnxofedinstall --kmp --add-kernel-support --skip-repo --with-nvmfThe KMP flag can be used to compile the necessary kernel modules for the installed kernel version.Powered by Discourse, best viewed with JavaScript enabled"
78,connectx-4-speed-issue-esxi6-7,"Hi,
We have issue in connectx-4 25Gbps speed, its shows 25Gbps in Esxi but practically the throughput is not going more 10Gbps. We test with different scenarios.Powered by Discourse, best viewed with JavaScript enabled"
79,application-recognition-signature-files,"Are there any readily available signature files or even pre-compiled CDO files that can be used with the AR   sample application?Hey there! We’re looking into this for you. The short answer is yes, but we can’t package and bundle it without a small licensing agreement. I’ll message you directly for more info!Hi,
are there plans to release the signatures publicly?
Thanks!Yes there are plans to publish a set of signatures soon that aren’t bound by licensing and legal requirements. Stay tuned.Hey jubetz,Is it already possible to get access to the signature file? Can you DM me?Thanks!Any more news? I could really use that, tooIn the DOCA 1.3, we’ve included a more useful sample ruleset for the application recognition application.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
80,any-way-to-get-real-time-power-consumption-of-bluefield2-smartnic-card,"We intend to get the real-time power consumption of bluefield2 SmartNic cards. Looking through bluefield documents, we do find some clues for real-time power, such as getting via BMC, or reading particular PCIe registers. But none of them can actually work. We want to confirm if power monitoring is really NOT supported in the hardware design, or maybe there are some undocumented methods to get it, e.g. via undocumented ipmi raw commands, or via i2c links. Any comments will be appreciated. Thanks!Below are methods we have tried:It is mentioned in the BlueField BMC document that BMC has the ability of Environmental monitoring – voltage/current/power. But we only see the voltages from BMC sensor information.From the Nvidia Adapters Programmer’s Reference Manual (PRM), we find there is a set of PCIe registers for Voltage and Power (MVCAP, MVCR, MSPS, MCPP). But we failed to get valid register values with the tool mlxreg.Hi @lei.wang9,Currently, the BMC can get voltage but not power.  Conceivable it is possible but not part of any planned feature.  On BF-2, we also don’t monitor every voltage rail on the board.
On BF-3, we monitor the power from the Arm but not on BF-2.
Those registers in the PRM are dependent on hardware on the board to support such which is not available on BF-2.Thanks,
ChenThank you for the confirmation!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
81,openvswitch-ofed-verbs-device-not-found,"I am following along the doc at Open vSwitch with DPDK — Open vSwitch 3.0.90 documentation to see OpenVswitch work with DPDK and a CX5 25GBE adapter.  It all builds fine but when attempting to add the adapter it errors out (log entries below).   Lots and lots of Google search suggests that this is some issue related to the OFED drivers but seemingly nobody has ever encountered it before, or at least posted about it.I’ve no clue where to start with this one.   The system is a fresh install of Rocky 8.7.  OFED is MLNX_OFED_LINUX-5.8-1.0.1.1-rhel8.7-x86_64.  DPDK is dpdk-stable-21.11.2 and OVS is openvswitch-2.17.3.  There are no special build options or attempts to optimize anything at this point.  Just a fresh system following along step by step the docs at the OpenVswitch url above.I’d be most appreciative if someone can give me a pointer to get back on track!Thanks!
-JSystem:Rocky Linux release 8.7 (Green Obsidian)Linux VH04 4.18.0-425.3.1.el8.x86_64 #1 SMP Wed Nov 9 20:13:27 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
[root@VH04 dpdk-stable-21.11.2]#OFED was installed with this command:
./mlnxofedinstall -vv --upstream-libs --dpdk --add-kernel-supportThe failing OpenVSwitch command was
ovs-vsctl add-port br0 port0 – set Interface port0 type=dpdk options:dpdk-devargs=0000:81:00.0This is the Mellanox/NVidia card81:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]
81:00.1 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]OVS thinks DPDK was initialized:
#ovs-vsctl get Open_vSwitch . dpdk_initialized
trueThis is a tail from the ovs log:2022-11-21T21:20:50.144Z|00071|memory|INFO|peak resident set size grew 805% in last 10.2 seconds, from 76524 kB to 692292 kB
2022-11-21T21:20:50.144Z|00072|memory|INFO|handlers:53 idl-cells:96 ports:1 revalidators:19 rules:5
2022-11-21T21:22:11.368Z|00073|dpdk|INFO|EAL: Probe PCI driver: mlx5_pci (15b3:1017) device: 0000:81:00.0 (socket 1)
2022-11-21T21:22:11.368Z|00074|dpdk|ERR|mlx5_common: Verbs device not found: 0000:81:00.0
2022-11-21T21:22:11.368Z|00075|dpdk|ERR|mlx5_common: Failed to initialize device context.
2022-11-21T21:22:11.368Z|00076|dpdk|ERR|EAL: Driver cannot attach the device (0000:81:00.0)
2022-11-21T21:22:11.368Z|00077|dpdk|ERR|EAL: Failed to attach device on primary process
2022-11-21T21:22:11.368Z|00078|netdev_dpdk|WARN|Error attaching device ‘0000:81:00.0’ to DPDK
2022-11-21T21:22:11.368Z|00079|netdev|WARN|port0: could not set configuration (Invalid argument)
2022-11-21T21:22:11.368Z|00080|dpdk|ERR|Invalid port_id=32Loaded modules:ib_cm                 118784  2 rdma_cm,ib_ipoib
ib_core               442368  8 rdma_cm,ib_ipoib,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm
ib_ipoib              155648  0
ib_umad                28672  0
ib_uverbs             155648  2 rdma_ucm,mlx5_ib
libahci                40960  1 ahci
libata                266240  2 libahci,ahci
libcrc32c              16384  5 nf_conntrack,nf_nat,openvswitch,nf_tables,xfs
mlx5_core            2105344  1 mlx5_ib
mlx5_ib               462848  0
mlx_compat             16384  11 rdma_cm,ib_ipoib,mlxdevm,iw_cm,ib_umad,ib_core,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm,mlx5_core
mlxdevm               180224  1 mlx5_core
mlxfw                  32768  1 mlx5_core
nf_tables             180224  235 nft_ct,nft_reject_inet,nft_fib_ipv6,nft_fib_ipv4,nft_chain_nat,nf_tables_set,nft_reject,nft_fib,nft_fib_inet
nft_fib                16384  3 nft_fib_ipv6,nft_fib_ipv4,nft_fib_inet
nft_fib_inet           16384  1
nft_fib_ipv4           16384  1 nft_fib_inet
nft_fib_ipv6           16384  1 nft_fib_inet
pci_hyperv_intf        16384  1 mlx5_core
psample                20480  1 mlx5_core
tls                   110592  1 mlx5_coreI should have included this info as well:0000:81:00.0 ‘MT27800 Family [ConnectX-5] 1017’ drv=vfio-pci unused=mlx5_core
0000:81:00.1 ‘MT27800 Family [ConnectX-5] 1017’ drv=vfio-pci unused=mlx5_coreWell, seems the answer to this was in a little note in this article:https://mymellanox.force.com/mellanoxcommunity/s/article/MLNX2-117-6278knFor anyone else having this problem, it seems you must not unbind the mlx5_core.  IOW, skip this step in the DPDK how-to doc:$DPDK_DIR/usertools/dpdk-devbind.py --bind=vfio-pci eth1   ← just don’t do it!NVIDIA MLX DPDK driver not base on VFIO/UIO like INTEL NIC. It base on kernel infiniband verbs layer.And please read all of below before you run DPDK on MLX devicehttps://doc.dpdk.org/guides/nics/mlx5.htmlThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
82,hi-how-do-i-change-name-management-ip-and-description-of-sx1024-switch,"Hi,Bought SX1024 Switch for my homelab, I can log in but if I reset it by the button it seems to remeber everything and doesnt run configuration wizard. Even it seems to be reloading some configuration after restart. Is there a way to start and run configuration wizard manualy after logging in?Or at least change the management IP and name etc.This is what I see now:10gswitch06 [my-mlag-vip-domain-3: master] #Thanks for helpSorted it myself…configure terminalshow interfaces mgmt0show interfaces mgmt1determined the ip lease, on I go…Hi Libor,Do I understand correctly that you have this figured out?CharlesPowered by Discourse, best viewed with JavaScript enabled"
83,what-is-the-main-difference-between-connectx-4-and-connectx-5ex,"What is the main difference between ConnectX 4 and ConnectX 5Ex?My systems require 2x100G QSFP28 ports to support 100GBase-CR4 DAC.What will be the suitable PCIe card to use? ConnectX 4 or 5Ex?Can connectx 4 and 5Ex supports 4x25G mode?Hello Bala,Thank you for posting your inquiry on the NVIDIA Networking Community.Both adapters support 100GBase-CR DAC. Please review the supported and tested cables through the following links:The adapter which will give you the highest performance, that is the ConnectX-5 Ex as it is PCIe 4.0 x16. The ConnectX-4 is only available in PCIe 3.0 x16The following link will provide the PB for the adapters in question:Our adapters do not support the feature to split the port into 4x25GbE is this is only available on our switches.Thank you and regards,~NVIDIA Networking Technical SupportThanks for your response. Hope connectX-5 adapter will support the feature or accept 4x25GbE that comes from switch or MAC. I assume within one of 100G port at connect x5 adapter side can operate all 4 RF channels(high speed data) independently or 100G as a whole?Refer drawing below. Kindly let me know.
Mellanox693×319 20.8 KB
Powered by Discourse, best viewed with JavaScript enabled"
84,what-is-the-caused-to-make-single-port-50gbps-only-on-connectx-6-dx-nic,"Hi sir,I’m using ConnectX-6 Dx under Windows Server 2022, with the QSFP28 cable connect between two ConnectX-6 Dx card, the performance of single port I got is only about 50Gbps,NIC: MCX623106AC-CDAT (plug on PCIe gen4 x16 slot)
NIC FW: v22.35.1012
NIC Driver: vv3.10.50000
Cable: FS Q28-PC03 Passive Twinax Cable Network Cable 3M
OS: Windows Server 2022 DatacenterI’d like to know if any settings need be set to get signle port with 100Gbps performance, or how I can resolve this problem?Thanks,
JackyHi Jacky,From your description, I am not sure what is your detail test steps.   I agree with you that the test result 50Gbps is not acceptable.So I think you can have a look at these 2 document links about the performance tuning at Windows.
https://docs.nvidia.com/networking/display/winof2v240/Performance+Tuning
https://support.mellanox.com/s/article/os-and-vm-performance-tuning-for-windowsPlease help to go through all the steps/suggestions in the 2 documents above.
If the performance problem still exist, then send the detail test steps from your servers side and with the deteil command output. I will help to investigate.Thanks!
Longran WeiHi longanw,I got ~93Gbps after installing the driver with custom installation(with performance tool) and then execute the MlxNdPerf.exe, suppose the issue is resolved by the tool.But when I execute the performance test on both ports at the same time, I only got ~80Gbps.Thanks,
JackyPowered by Discourse, best viewed with JavaScript enabled"
85,mcx354a-qcbtb-fails-to-detect-transceiver,"We recently bought two MCX354A-QCBTB.They are set to Ethernet in the drivers tools. Their ethernet ports show up fine in Windows Server (We are on 2019).Regardless, both ports on both card fail to detect all our passive DAC cables. Said cables work fine on a MCX314A-BCCT.
The cables are from FS.com, with a transceiver encoded for Mellanox NIC.The chances that both ports on both card would fail is quite unlikely. What options are we missing that makes those cards report no cable attached?Usually in cases like this the cable’s eeprom will be missing some value that’s necessary for linkup.You will need to open a case with mlnx support and provide the register dumps from the cards in both good and bad scenarios.What is the FW version you have on both cards ?We figured it out.For some reason, the passive DAC isn’t properly detected when in 10Gb Ethernet mode, and the MCX354A-QCBTB is only supporting 40Gb InfiniBand or 10Gb Ethernet.We flashed to the MCX354A-FCB firmware (That supports 40Gb Ethernet), and now everything works fine.
We contacted the maker of the transceivers to see why they don’t work in 10Gb mode.While now everything should be in 40Gb mode, our iPerf test shows it’s probably not really running 40Gb.
I was under the impression both MCX354A-QCBTB and MCX354A-FCB were the same hardware with different firmware. Is the MCX354A-QCBTB hardware limited to 10Gb regardless if it’s handshake to 40?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
86,operation-ib-wr-reg-mr-fails-with-ib-wc-mw-bind-err,"We have kernel module program which uses IB network for data transfer. This works perfectly on Debian and now we are trying to port that module onto FreeBSD and facing issue with memory registration.	uname -a
FreeBSD test1 14.0-CURRENT FreeBSD 14.0-CURRENT #0 main-n259630-0ca90ed42a49: Sat Jul 28 09:40:05 IST 2001     root@test1:/usr/obj/usr/src/amd64.amd64/sys/GENERIC amd64Our program creates pd,mr(using ib_alloc_mr), cqs and qp(with type IB_QPT_RC). Then moves qp to states IB_QPS_INIT->IB_QPS_RTR->IB_QPS_RTS.
Gets dma_address for the malloced address by call ib_dma_map_single(). Plugs this DMA address into sg using sg_dma_address and sg_dma_length.
And then maps mr and sg using call ib_map_mr_sg() and calls  to these succeed.Then program posts IB_WR_REG_MR work request as below: (Simulates ibv_reg_mr())wr.opcode = IB_WR_REG_MR;
wr.send_flags = IB_SEND_SIGNALED;
wr.mr =  mr;
wr.key =  rkey;
wr.access = 0;
error = ib_post_send( qp, &wr, &bad_wr);
ib_post_send is successful. Then it waits for completion on call  ib_poll_cq(cq, 1, &wc);
It is failing at ASSERT(wc.status == IB_WC_SUCCESS)  as we have wc.status set to IB_WC_MW_BIND_ERR.QUESTIONS:(kgdb) p wc
$1 = {{wr_id = 0, wr_cqe = 0x0}, status = IB_WC_MW_BIND_ERR, opcode = -511, vendor_err = 120, byte_len = 0, qp = 0xfffff80040687800,
ex = {imm_data = 8, invalidate_rkey = 8}, src_qp = 0, wc_flags = -2096742349, pkey_index = 65535, slid = 65535, sl = 161 ‘\241’,
dlid_path_bits = 0 ‘\000’, port_num = 0 ‘\000’, smac = “\000\000\000\000\000\b”, vlan_id = 0, network_hdr_type = 0 ‘\000’}our register WR is as below
(kgdb) p wr
$3 = {wr = {next = 0x0, {wr_id = 0, wr_cqe = 0x0}, sg_list = 0x0, num_sge = 0, opcode = IB_WR_REG_MR, send_flags = 2, ex = {
imm_data = 0, invalidate_rkey = 0}}, mr = 0xfffff8004068ad80, key = 1857536, access = 0}QP from dump(kgdb) p * qp
$7 = {device = 0xfffffe011e04b000, pd = 0xfffff8000b1c4600, send_cq = 0xfffff80040687c00, recv_cq = 0xfffff80040687c00, mr_lock = {
m = {lock_object = {lo_name = 0xffffffff82fe57e0 “lnxspin”, lo_flags = 16842752, lo_data = 0, lo_witness = 0x0}, mtx_lock = 0}},
srq = 0x0, xrcd = 0x0, xrcd_list = {next = 0x0, prev = 0x0}, usecnt = {counter = 0}, open_list = {next = 0x0, prev = 0x0},
real_qp = 0xfffff80040687800, uobject = 0x0, event_handler = 0x0, qp_context = 0xfffff8000cf3a440, qp_num = 349, max_write_sge = 6,
max_read_sge = 6, qp_type = IB_QPT_RC, rwq_ind_tbl = 0x0, port = 0 ‘\000’}
(kgdb)MR related to this WR  is
kgdb) p *wr->mr
$9 = {device = 0xfffffe011e04b000, pd = 0xfffff8000b1c4600, lkey = 1857536, rkey = 1857536, iova = 18446741879489164128,
length = 1512, page_size = 4096, type = IB_MR_TYPE_MEM_REG, need_inval = false, {uobject = 0x0, qp_entry = {next = 0x0,
prev = 0x0}}, dm = 0x0, sig_attrs = 0x0}
(kgdb)(kgdb) p /x *rfmr->sg
$10 = {page_link = 0x2, offset = 0x360, length = 0x5e8, dma_address = 0x4425f360, dma_map = 0x0}Information on mlx device:
root@test1:~ # ibstat
CA ‘mlx5_0’
CA type: MT4124
Number of ports: 1
Firmware version: 20.31.1014
Hardware version: 0
Node GUID: 0x005056fffe8b6f9a
System image GUID: 0xb88303ffff8bf28c
Port 1:
State: Active
Physical state: LinkUp
Rate: 40
Base lid: 0
LMC: 0
SM lid: 0
Capability mask: 0x04010000
Port GUID: 0x025056fffe8b6f9a
Link layer: Ethernet
CA ‘mlx5_1’
CA type: MT4124
Number of ports: 1
Firmware version: 20.31.1014
Hardware version: 0
Node GUID: 0x005056fffe8b0996
System image GUID: 0xb88303ffff8bf28c
Port 1:
State: Active
Physical state: LinkUp
Rate: 40
Base lid: 0
LMC: 0
SM lid: 0
Capability mask: 0x04010000
Port GUID: 0x025056fffe8b0996
Link layer: Ethernet
root@test1:~ # pciconf -lv  | grep mlx -C 3
device     = ‘VMXNET3 Ethernet Controller’
class      = network
subclass   = ethernet
mlx5_core0@pci0:19:0:0: class=0x020000 rev=0x00 hdr=0x00 vendor=0x15b3 device=0x101c subvendor=0x1590 subdevice=0x02af
vendor     = ‘Mellanox Technologies’
device     = ‘MT28908 Family [ConnectX-6 Virtual Function]’
class      = network
subclass   = ethernet
mlx5_core1@pci0:27:0:0: class=0x020000 rev=0x00 hdr=0x00 vendor=0x15b3 device=0x101c subvendor=0x1590 subdevice=0x02af
vendor     = ‘Mellanox Technologies’
device     = ‘MT28908 Family [ConnectX-6 Virtual Function]’
class      = network
root@test1:~ # sysctl -a | grep Mellanox
mlx5: Mellanox Core driver 3.7.1 (November 2021)ugen1.1:  at usbus1
mlx5: Mellanox Core driver 3.7.1 (November 2021)ugen1.1:  at usbus1
mlx5: Mellanox Core driver 3.7.1 (November 2021)ugen1.1:  at usbus1
mlx5: Mellanox Core driver 3.7.1 (November 2021)ugen1.1:  at usbus1
mlx5: Mellanox Core driver 3.7.1 (November 2021)ugen1.1:  at usbus1
dev.mlx5_core.1.%desc: Mellanox Core driver 3.7.1 (November 2021)
dev.mlx5_core.0.%desc: Mellanox Core driver 3.7.1 (November 2021)Hello suresh.pujar,Welcome, and thank you for posting your inquiry to the NVIDIA Developer Forums.Q) Can you tell us why we get this error?
A) Our exposure to BSD applications is limited. This type of issue would need to be investigated via a support case, however do bear in mind that programming assistance / code review is outside of the scope of NVIDIA Enterprise Support. If assistance is needed with programming / code review, we would recommend engaging our Sales and Solutions team so they can understand your goals and set you up with a solution that fits your specific business needs.Q) Any documentation on IB calls used in kernel mode?
A) Unfortunately no, this is dealing with the inner workings of the driver and may be proprietary information. This request will also need to be run through our Solutions team.To get in touch with our Sales and Solutions team, please use the following web form:Get your questions answered. Search for products or services by category and get routed to the correct customer support person.To open an NVIDIA support ticket, please use the following link:
https://enterprise-support.nvidia.com/s/create-case
Note that engaging NVIDIA support will require support entitlement.
You will be prompted to enter your entitlement information (serial number of device, support entitlement certificate number, etc) at that link.Thanks, and best regards;
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
87,mlx5-1-switch-infiniband-to-ethernet,"Hi, I have Mellanox Technologies MT27800 Family [ConnectX-5] adapter in a server.Installed OS: Fedora 35.Without drivers I can seeibp175s0f0 and ibp175s0f1 network adapters with link_layer: InfiniBand status.After RPMS_ETH installation there is no network adapters.ibstatus shows:Infiniband device ‘mlx5_0’ port 1 status:base lid: 0xffffsm lid: 0x0state: 1: DOWNphys state: 3: Disabledrate: 10 Gb/sec (4X SDR)link_layer: InfiniBandInfiniband device ‘mlx5_1’ port 1 status:base lid: 0xffffsm lid: 0x0state: 1: DOWNphys state: 2: Pollingrate: 10 Gb/sec (4X SDR)link_layer: InfiniBandPlease help, how can I switch mlx device to Ethernet state?Hello Andrey,Thank you for posting your inquiry on the NVIDIA Networking Community.You can change the port type with the following instructions:After the reset, the port link type is set to ‘Ethernet’Thank you and regards,~NVIDIA Networking Technical SupportThanks. Tried to find this info in any docs. No success.Aftermstconfig -d mlx5_0 set LINK_TYPE_P1=2mstfwreset -d mlx5_0 -l3 -y resetmstconfig -d mlx5_1 set LINK_TYPE_P1=2mstfwreset -d mlx5_1 -l3 -y resetI got new wired connection in my system.weird, but second port is still InfiniBand.Infiniband device ‘mlx5_0’ port 1 status:link_layer: EthernetInfiniband device ‘mlx5_1’ port 1 status:link_layer: InfiniBandConfigurations: Next Boot NewLINK_TYPE_P1 ETH(2) ETH(2)This commands did not help.mstconfig -d mlx5_1 set LINK_TYPE_P1=2mstfwreset -d mlx5_1 -l3 -y resetOh, after query found port2mstconfig -d mlx5_0 set LINK_TYPE_P2=2Powered by Discourse, best viewed with JavaScript enabled"
88,problems-with-jumbo-frames-in-a-sn2010,"Hello there​​​I have set the MTU in the Port to 9126 BT when I Test with Ping and setting the size to 8500 nothing happens.​I Hope for getting some advices​​best regards​Kilian​​Kilian - a lot of details are missing.what is the topology? IP addresses? configuration?Powered by Discourse, best viewed with JavaScript enabled"
89,latest-kernel-officially-supported-by-mlnx-ofed-lts-4-9-4-0-8-0,"Specifically interested in whether MLNX_OFED LTS 4.9-4.0.8.0 officially supports running on 5.10 kernel with --add-kernel-support. And if not, if there are any plans to support a kernel beyond 5.4 in the future.Hello,Unfortunately, MLNX_OFED LTS does not currently support the 5.10 kernel in any of the distributions’ releases supported.The list of supported operating systems by version and latest tested and supported kernels as implemented by each OS vendor can be found in the “General Support” section of the MLNX_OFED LTS Release Notes.To view the Supported OS matrix for the latest LTS release, MLNX_OFED v4.9-4.0.8.0 LTS, please visit the following link:https://docs.mellanox.com/display/MLNXOFEDv494080/General+Support+in+MLNX_OFEDAs support is added for each kernel and OS/distribution tested, it will be included in these listings.If you are using a ConnectX-4 or newer adapter that is compatible with MLNX_OFED 5.4-3.0.3.0 and do not require the older experimental verbs, you may wish to utilize the non-LTS version that does include support for many newer kernel implementations.Thank you,Thanks Hilary,I need to use functionality that only exists in the LTS version. Is there any expectation that support for newer kernels would be added to LTS in the future or would LTS effectively “die” when kernel 5.4 reaches end of life?Powered by Discourse, best viewed with JavaScript enabled"
90,activating-wake-on-lan-for-connectx-3-mcx311a-xcat,"I was wondering if there is a way to enable WOL for the MCX311A-XCAT HCA? Right now I’m using the builtin Linux driver and ethtool only shows d as available WOL option. Do I need to use the mlnx_en driver to activate WOL?Hello Lennart,Thank you for posting your inquiry on the NVIDIA Networking Community.Please the instructions provide through the following link to enable WOL on the ConnectX-3 → https://docs.nvidia.com/networking/display/MFTv4140/MFT+Supported+Configurations+and+ParametersThe table shows the configurable parameter → WOL_MAGIC_EN_P1/P2Install the MFT tools from our website (Mellanox Firmware Tools (MFT)) and use the ‘mlxconfig’ command to enable WOL on the ConnectX-3. → # mlxconfig -d mt4099_pci_cr0 set WOL_MAGIC_EN_P<port #>=1After changing this setting please reboot the node to make the setting active.Please make sure the adapter is running the latest f/w version available.Thank you and regards,~NVIDIA Networking Technical SupportSo I just tested this, I ran mlxup to update the fw and gotsudo ./mlxupQuerying Mellanox devices firmware …Device #1:Device Type: ConnectX3Part Number: MCX311A-XCA_AxDescription: ConnectX-3 EN network interface card; 10GigE; single-port SFP+; PCIe3.0 x4 8GT/s; RoHS R6PSID: MT_1170110023PCI Device Name: 0000:07:00.0Port1 MAC: …Port2 MAC: …Versions: Current AvailableFW 2.42.5000 2.42.5000PXE 3.4.0752 3.4.0752Status: Up to dateSo seems to be up to date. however when querying mlxconfig / mstconfig for the flag, it’s not listed and attempting to set the flag, I getsudo mstconfig -d 07:00.0 set WOL_MAGIC_EN_P1=1 3 ↵Device #1:Device type: ConnectX3Device: 07:00.0Configurations: Next Boot New-E- Device doesn’t support WOL_MAGIC_EN_P1 configurationsudo mstconfig -d 07:00.0 q 3 ↵Device #1:Device type: ConnectX3Device: 07:00.0Configurations: Next BootSRIOV_EN False(0)NUM_OF_VFS 0LOG_BAR_SIZE 0BOOT_OPTION_ROM_EN_P1 False(0)BOOT_VLAN_EN_P1 False(0)BOOT_RETRY_CNT_P1 0LEGACY_BOOT_PROTOCOL_P1 None(0)BOOT_VLAN_P1 0BOOT_OPTION_ROM_EN_P2 False(0)BOOT_VLAN_EN_P2 False(0)BOOT_RETRY_CNT_P2 0LEGACY_BOOT_PROTOCOL_P2 None(0)BOOT_VLAN_P2 0IP_VER_P1 IPv4(0)IP_VER_P2 IPv4(0)CQ_TIMESTAMP False(0)mlxconfig just says that it can’t query the current device configuration. So I guess my card doesn’t support WOL?Powered by Discourse, best viewed with JavaScript enabled"
91,release-notes-for-nvidia-bright-cluster-manager-9-2-8,"Release notes for Bright 9.2-8== General ==
=Improvements==Fixed Issues=== CMDaemon ==
=New Features==Improvements==Fixed Issues=== Head Node Installer ==
=Fixed Issues=== Machine Learning ==
=New Features==Improvements=== cm-wlm-setup ==
=New Features=Powered by Discourse, best viewed with JavaScript enabled"
92,sn2100-mlag-problem,"I have had this on a 2700 MLAG Cluster also, after years or running, 1 Switch takes all Ports down except for IPL. I see this in the log.
After Reboot, everything is fine again. Whats the Problem ?ec 14 09:39:48 mel2100-148 mlagd[7854]: TID 139898656126720: [mlagd.NOTICE]: [MLAG_COMM_LAYER_WRAPPER.NOTICE] Communication layer wrapper stop for tcp port 51236 , role 1, cause 0
Dec 14 09:39:48 mel2100-148 mlagd[7854]: TID 139898664519424: [mlagd.NOTICE]: [MLAG_COMM_LAYER_WRAPPER.NOTICE] Communication layer wrapper stop for tcp port 51235 , role 1, cause 0
Dec 14 09:39:48 mel2100-148 mlagd[7854]: TID 139898664519424: [mlagd.NOTICE]: [MLAG_COMM_LAYER_WRAPPER.NOTICE] TCP client session stop for handle 57
Dec 14 09:39:48 mel2100-148 mlagd[7854]: TID 139898643699456: [mlagd.NOTICE]: [MLAG_TUNNELING.NOTICE] Peer 0 Down
Dec 14 09:39:48 mel2100-148 mlagd[7854]: TID 139898664519424: [mlagd.NOTICE]: [PORT_MANAGER.NOTICE] Port manager peer state change [3]
Dec 14 09:39:48 mel2100-148 mlagd[7854]: TID 139898664519424: [mlagd.NOTICE]: [STP_MANAGER.NOTICE] Received PEER STATUS CHANGE event - PEER_DOWN_WAITHi Tim,I think that this one should be analysed by reviewing the 2 switches sysdumps and by the Nvidia GTS  team.
can you open a support case?I cannot, its a very old device and we dont have any support contract for this. And on the other hand, i havent been so happy with support elsewhere, and when this issue occured on a 2700 MLAG Cluster which was under support, there was no solution either. Im just unhappy this happens all of a sudden under 3.9 or 3.10 like it happend years ago under 3.6 or so. Same behaviour, no solution or clue.I will prolly end up exchanging those devices against another brand which gives me less headachePowered by Discourse, best viewed with JavaScript enabled"
93,trying-to-get-to-the-admin-gui-on-a-new-sn2010-i-can-ssh-but-not-web-gui,"Hi all,I have a newish SN2010 switch that I reset (pushed the reset button for 15s). I logged in an ran ‘configure jump-start’ and put on a temp IP, etc. I can access the switch via ssh without issue, but when I try and access the web GUI the browser, it just times out. As this is a test environment, the switch isn’t on the network yet, I was connecting to it straight from my laptop. I also tried via a small switch, but obviously, no difference.Any ideas would be greatly appreciated.Hi David,Can you try to use https and let us know if that helpedWith the laptop connected directly to OOB mgmt0 interface. Configure the IP’s correctly in the same subnet and then try to use httpse.g. https://10.10.10.1Thanks,Pratik PandePowered by Discourse, best viewed with JavaScript enabled"
94,mlx5-linux-counters-vs-mlx5-ethtool-counters,"I was going through two pages regarding MLX5 LINUX COUNTERS and MLX5 ETHTOOL COUNTERS.1)ESPCommunity2)/understanding-mlx5-ethtool-countersCan anyone tell what are the differences these two counters (MLX5 LINUX COUNTERS vs MLX5 ETHTOOL COUNTERS)?Hello,The difference is that under this page:
https://enterprise-support.nvidia.com/s/article/understanding-mlx5-linux-counters-and-status-parameters
You can also find the hw_counters which is relevant for RDMA traffic only (Ethernet and Infiniband), and these counters are not present under ethtool because these are the kernel bypass counters.Best Regards,
VikiPowered by Discourse, best viewed with JavaScript enabled"
95,performance-test-with-rocev2,"Hi,CPU : Intel
Card : ConnectX-5 EN
O/S : Ubuntu 22.04(64-bit)
Driver : MLNX_OFED_LINUX-5.8-1.1.2.1-ubuntu22.04-x86_64I am testing performance with RoCEv2. The link speed is 100Gbps.Settings for enp202s0f0np0:
Supported ports: [ FIBRE ]
Supported link modes:   1000baseKX/Full
10000baseKR/Full
40000baseKR4/Full
40000baseCR4/Full
40000baseSR4/Full
40000baseLR4/Full
25000baseCR/Full
25000baseKR/Full
25000baseSR/Full
50000baseCR2/Full
50000baseKR2/Full
100000baseKR4/Full
100000baseSR4/Full
100000baseCR4/Full
100000baseLR4_ER4/Full
Supported pause frame use: Symmetric
Supports auto-negotiation: Yes
Supported FEC modes: None        RS      BASER
Advertised link modes:  100000baseKR4/Full
100000baseSR4/Full
100000baseCR4/Full
100000baseLR4_ER4/Full
Advertised pause frame use: No
Advertised auto-negotiation: No
Advertised FEC modes: None
Speed: 100000Mb/s
Duplex: Full
Auto-negotiation: off
Port: FIBRE
PHYAD: 0
Transceiver: internal
Supports Wake-on: d
Wake-on: d
Current message level: 0x00000004 (4)
link
Link detected: yesThis  is a log from “show_gids” command.=== PC1 ===
mlx5_2  1       2       0000:0000:0000:0000:0000:ffff:c0a8:6714 192.168.103.20          v1      enp202s0f0np0
mlx5_2  1       3       0000:0000:0000:0000:0000:ffff:c0a8:6714 192.168.103.20          v2      enp202s0f0np0=== PC2 ===
mlx5_2  1       2       0000:0000:0000:0000:0000:ffff:c0a8:671e 192.168.103.30          v1      enp202s0f0np0
mlx5_2  1       3       0000:0000:0000:0000:0000:ffff:c0a8:671e 192.168.103.30          v2      enp202s0f0np0I tested the performance by running “ibv_rc_pingpong” as shown below, and the performance was about 13Gbps.  Need other testing options to see 100 Gbps performance?root@pc1:~# ibv_rc_pingpong -d mlx5_2 -g 3
local address:  LID 0x0000, QPN 0x00010d, PSN 0x45844e, GID ::ffff:192.168.103.20
remote address: LID 0x0000, QPN 0x00004b, PSN 0x58f27e, GID ::ffff:192.168.103.30
8192000 bytes in 0.00 seconds = 13487.55 Mbit/sec
1000 iters in 0.00 seconds = 4.86 usec/iterroot@pc2:~# ibv_rc_pingpong -d mlx5_2 -g 3 192.168.103.20
local address:  LID 0x0000, QPN 0x00004b, PSN 0x58f27e, GID ::ffff:192.168.103.30
remote address: LID 0x0000, QPN 0x00010d, PSN 0x45844e, GID ::ffff:192.168.103.20
8192000 bytes in 0.00 seconds = 14362.48 Mbit/sec
1000 iters in 0.00 seconds = 4.56 usec/iterUltimately my goal is to get maximum transmit/receive throughput between PC1 and PC2 using RoCE.
In order to utilize all of the 100Gbps B/W, sending and receiving should be done in multi-thread, but I want to implement it in single-thread.Use perftest package (ib_write_bw etc.)Pin the application threads to the NUMA closest to the card.Disable C-states to prevent CPU from slowing down.Thank you for your comment.HiPlease go through the performance tunning guide.https://enterprise-support.nvidia.com/s/article/performance-tuning-for-mellanox-adaptersPowered by Discourse, best viewed with JavaScript enabled"
96,docker-image-build-disconnects-oob-network,"Hello AllI am trying to deploy some dockers on BlueField-2, but after lauching docker build -t <image> . my OOB network gets disconnectedHas anyone run into such issue ?Thank you
PeterHi Peter,The ssh disconnect during the docker build command is not an expected/known issue, may be caused by various system related reasons (high oob interface or system load, and may require a much deeper investigation.
In order to get a more information about the BF2 status during the docker build operation, we’d recommend you to connect the BF2 adapter via console from the host the card is physically connected to. This will allow you to stay connected, even if ssh connection is lost.
i.e.
minicom -D /dev/rshim0/consoleBest Regards,
AnatolyPowered by Discourse, best viewed with JavaScript enabled"
97,couldnt-find-vendor-err-list-and-hard-to-debug,"I’m using a MBF2H516A-EEEOT and encountered a problem when polling a CQ. It shows that the status of wc is 2 and the vendor_error is 104. I tried to figure out what happened, but couldn’t find what does the vendor error stand for. Does exist such a list can show me what the vendor error means?Hello @yangfisher01 ,This syndrome indicates the driver issued a WQE that was malformed in size.
To further examine this matter, additional data and logs are required. Please create a support case according to your entitlement, by emailing [EnterpriseSupport@nvidia.com]. We will be happy to assist.Best regards,
ChenThank you for your help!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
98,mlx5-core-cqe-error-on-kernel-log-vendor-syndrome-0xf9,"Hello,I’m getting a lot of kernel error logs while forwarding packets using AF_XDP in ZEROCOPY mode in ConnectX-5 NICs & it seems to be a lot of packet loss.NIC: Mellanox Technology MT27800 Family [ConnextX-5]
firmware: 16.35.1012
Processor: AMD EPYC 7452 32-core Processor
Distribution:Debian 12rc3
Kernel: 6.1.0-9-amd64kernel logs for your reference,[Thu May 25 03:42:53 2023] mlx5_core 0000:81:00.1 enp129s0f1np1: Error cqe on cqn 0xc53, ci 0x362, qn 0x4b8e, opcode 0xd, syndrome 0x5, vendor syndrome 0xf9
[Thu May 25 03:42:53 2023] 00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000030: 00 00 00 00 45 00 f9 05 00 00 4b 8e e0 7e 50 d3
[Thu May 25 03:42:53 2023] mlx5_core 0000:81:00.1 enp129s0f1np1: Error cqe on cqn 0xc53, ci 0x363, qn 0x4b8e, opcode 0xd, syndrome 0x5, vendor syndrome 0xf9
[Thu May 25 03:42:53 2023] 00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000030: 00 00 00 00 45 00 f9 05 00 00 4b 8e e0 7f 50 d3
[Thu May 25 03:42:53 2023] mlx5_core 0000:81:00.1 enp129s0f1np1: Error cqe on cqn 0xc53, ci 0x364, qn 0x4b8e, opcode 0xd, syndrome 0x5, vendor syndrome 0xf9
[Thu May 25 03:42:53 2023] 00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000030: 00 00 00 00 45 00 f9 05 29 00 4b 8e e0 80 81 d3
[Thu May 25 03:42:53 2023] mlx5_core 0000:81:00.1 enp129s0f1np1: Error cqe on cqn 0xc53, ci 0x365, qn 0x4b8e, opcode 0xd, syndrome 0x5, vendor syndrome 0xf9
[Thu May 25 03:42:53 2023] 00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
[Thu May 25 03:42:53 2023] 00000030: 00 00 00 00 45 00 f9 05 29 00 4b 8e e0 8d 8d d3Hi Gopinath,Thank you for posting your query on NVIDIA community.Based on the information shared so far, it is unclear if this issue is experienced when using MLNX OFED driver or not. If not in use, I would like to request installing the MLNX OFED driver based on the supported OS mentioned at —> Linux InfiniBand DriversDebian 12 is currently not supported.If you experience issues after using a supported OS and MLNX OFED driver, I would like to request opening a support ticket by emailing to Networking-support@nvidia.com in order to perform additional debug. Please note, a valid support contract is needed for opening support ticket. The contracts team can be reached on Networking-contracts@nvidia.comThanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
99,problem-about-ofed-installation-on-uos20-1050e,"I’m having trouble installing OFED on my server.Here is my server’s Linux distribution :
$ uname -r
4.19.90-2211.5.0.0178.23.uel20.aarch64Here is the OFED version I use :
MLNX_OFED_LINUX-5.9-0.5.6.0-uos20.1040-aarch64.tgzHere is my installing procedure :
$ ./mlnx_add_kernel_support.sh -m ./ --make-tgz
Note: This program will create MLNX_OFED_LINUX_TGZ for uos20 under /tmp directory.
Do you want to continue?[y/N]:y
See log file /tmp/mlnxiso.290179_1ogs/mlnx_ofed_iso.290179.logWARNING: The current MLNX_OFED_LINUX is intended for uos20.1040 !
You may need to use the’–skip-distro-checkflag to install the resulting MLNX_OFED_LINUX on this system.Checking if all needed packages are installed…
Building MLNX_OFED_LINUX_RPMS.Please wait…ERROR: Failed executing""MLNX_OFED_SRC-5.9-0.5.6.0/install.pl --tmpdir /tmp/mlnx_iso.290179 logs --kernel-only --kernel 4.19.90-2211.5.0.0178.23.uel20.aarch64 --kernel-sources /lib/modules/4.19.90-2211.5.0.0178.23.ue120.aarch64/build/ --builddir /tmp/mlnx iso.290179 --disable-kmp --build-only""
ERROR:See /tmp/mlnx_iso.290179_logs/mlnx_ofed_iso.290179.1ogAnd here is the error log :
$ cat /tmp/mlnx_iso.290179 logs/mlnx_ofed_iso.290179.logBelow is the list of OFED packages that you have chosen
(some may have been added by the installer due to package dependencies):ofed-scripts mlnx-tools
mlnx-ofed-kernel-utils
mlnx-ofed-kernel-modules
kernel-mft-modules
knem-modulesChecking SW Requirements…
One or more required packages for installing OFED-internal are missing.
Attempting to install the following missing packages:
coreutils dpatch swig kmod grep debhelper lsof pciutils gcc python3 libltdl-dev bzip2 pkg- config quilt build-essential graphviz chrpath procps autotools-dev python3-distutils perl m4 ethtool dh-autoreconf dh-python automake autoconf make
Failed command: apt-get install -y -o Dpkg::Options::=‘–force-confdef’-o Dpkg::Options::=’–force-confold’coreutils dpatch swig kmod grep debhelper lsof pciutils gcc python3 lib ltdl-dev bzip2 pkg-config quilt build-essential graphviz chrpath procps autotools-dev pyth on3-distutils perl m4 ethtool dh-autoreconf dh-python automake autoconf makeSo,I think my problem now is I can’t find a appropriate ofed version for my srever.My server is in uos series but it is using yum for package installing. Therefore, the kernel_add script leads to failure.
How can I find a suitable ofed version for my server?Powered by Discourse, best viewed with JavaScript enabled"
100,replace-connectx-3-fdr10-adapter-with-a-connectx-5-edr-adapter,"Hi,We have a host with a ConnectX-3 adapter connected to a SX6036 InfiniBand Switch (https://www.mellanox.com/related-docs/prod_ib_switch_systems/PB_SX6036.pdf) using FDR10 cable (https://store.mellanox.com/products/nvidia-mc2206310-030-active-optical-cable-infiniband-qdr-fdr10-40gb-s-qsfp-30m.html).We would like to know if the ConnectX-3 FDR10 adapter can be replaced with a ConnectX-5 EDR adapter using the same cable.Thank you.Hello Ubay,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, the following link provides the supported FDR10 cables for connecting a ConnectX-5 EDR adapter → https://docs.mellanox.com/display/ConnectX5Firmwarev16292002/Firmware+Compatible+Products#FirmwareCompatibleProducts-ValidatedandSupportedFDR10CablesIf the cable you are using is in the list, it is supported and validated. Please make sure that the adapter and the switch are running the latest f/w and s/w code.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
101,create-monitor-failed-on-pipe-using-doca-switch,"Hi everyone,I have been trying to use doca_switch version 1.5.1) to create flows and monitor the counter of entries. I have been using the following command line:create monitor flags=4
create pipe port_id=0,root_enable=1,monitor=1However, following error message appears:
[ERR][dpdk_pipe_common] counter: configured number of resources is 0
[ERR][dpdk_pipe_legacy] failed cresting pipe of type 0: verification rc=-22If I am reading the code correctly, doca_switch is not using shared counter by a counter per entry. But why is above error showing up?Many thanks in advance
KyleHi everyone,The monitor function is working now:
It needs to add following to lines to the switch_init() function:flow_cfg.resource.nb_counters=8192;
flow_cfg.resource.nb_meters=8192Best regards
KyleThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
102,as-specified-on-spec-sheet-connectx-6-family-adapters-support-up-to-1000-vf-per-port-max-up-to-1k-vf-per-port-is-it-true-for-all-cards-i-m-targetting-to-buy-nvidia-connectx-6-mcx623102an-adat-or-nvidia-connectx-6-mcx623106an-cdat-thank-you,"In other hand is it possible to use VF with linux bridge ?Hi Auranext,Mellanox adapters are capable of exposing up to 127 virtual instances (Virtual Functions (VFs)) for each port in the Mellanox ConnectX family cards.Please refer to the Firmware’s Release Notes of the ConnectX card, under the “known issues” section for rate limit per VF.For example (ConnectX-6):https://docs.mellanox.com/display/ConnectX6Firmwarev20291016/Known+IssuesRegards,ChenPowered by Discourse, best viewed with JavaScript enabled"
103,mellanox-driver-bugcheck-0x50,"On a Windows server 2016, a Mellanox driver v5.35 that comes with the image randomly could cause a Bugcheck 0x50 (BSOD) on the VM. This is fixed in v5.46 and above. Please help me to locate a download link for a newer version of the driver. Thanks in advance!Hi nikolayYou can download it below.
Mellanox OFED for Windows - WinOF / WinOF-2 (nvidia.com)/HyungKwangthan you! I wasn’t 100% convinced that that’s the driver package but I am now.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
104,connectx-6-dx-testpmd-no-probed-ethernet-devices,"I have a centos 7.x based DPDK 22.11 build on a machine with two mellanox nics. One is ConnectX-5 Ex and other is ConnectX-6 Dx.They both throw No probed ethernet devices on the latest DPDK version 22.11 on Centos 7. If one upgrades to CentOS 8, they work. But I need to remain on CentOS 7. Is there a workaround.The workaround suggested in other threads does not cover meson builds.[root@bminstance dpdk-22.11]# dpdk-devbind.py -s0000:4b:00.1 ‘MT2892 Family [ConnectX-6 Dx] 101d’ drv=vfio-pci unused=mlx5_core
0000:98:00.1 ‘MT28800 Family [ConnectX-5 Ex] 1019’ drv=vfio-pci unused=mlx5_core0000:4b:00.0 ‘MT2892 Family [ConnectX-6 Dx] 101d’ if=ens300f0 drv=mlx5_core unused=vfio-pci Active
0000:98:00.0 ‘MT28800 Family [ConnectX-5 Ex] 1019’ if=ens800f0 drv=mlx5_core unused=vfio-pci0000:00:01.0 ‘Device 0b00’ unused=vfio-pci
0000:00:01.1 ‘Device 0b00’ unused=vfio-pci
0000:00:01.2 ‘Device 0b00’ unused=vfio-pci
0000:00:01.3 ‘Device 0b00’ unused=vfio-pci
0000:00:01.4 ‘Device 0b00’ unused=vfio-pci
0000:00:01.5 ‘Device 0b00’ unused=vfio-pci
0000:00:01.6 ‘Device 0b00’ unused=vfio-pci
0000:00:01.7 ‘Device 0b00’ unused=vfio-pci
0000:80:01.0 ‘Device 0b00’ unused=vfio-pci
0000:80:01.1 ‘Device 0b00’ unused=vfio-pci
0000:80:01.2 ‘Device 0b00’ unused=vfio-pci
0000:80:01.3 ‘Device 0b00’ unused=vfio-pci
0000:80:01.4 ‘Device 0b00’ unused=vfio-pci
0000:80:01.5 ‘Device 0b00’ unused=vfio-pci
0000:80:01.6 ‘Device 0b00’ unused=vfio-pci
0000:80:01.7 ‘Device 0b00’ unused=vfio-pci[root@bminstance dpdk-22.11]# dpdk-testpmd -l 0-3 -n 4 -a 98:00.1 – -i --nb-cores=2
EAL: Detected CPU lcores: 72
EAL: Detected NUMA nodes: 2
EAL: Detected static linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘VA’
EAL: VFIO support initialized
EAL: DPDK is running on a NUMA system, but is compiled without NUMA support.
EAL: This will have adverse consequences for performance and usability.
EAL: Please use --legacy-mem option, or recompile with NUMA support.
TELEMETRY: No legacy callbacks, legacy socket not created
testpmd: No probed ethernet devices
Interactive-mode selected
testpmd: create a new mbuf pool <mb_pool_0>: n=171456, size=2176, socket=0
testpmd: preferred mempool ops selected: ring_mp_mc
Done
testpmd>hi vikasdHave you ever install ofed in your centos 7?
If the driver install correct, should not have such issue.
here is the driver download link:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)Yes, I have OFED 5.9. Here are the fw versionsibv_devinfo
hca_id:	mlx5_0
transport:			InfiniBand (0)
fw_ver:				22.31.1660
node_guid:			08c0:eb03:004a:5f8a
sys_image_guid:			08c0:eb03:004a:5f8a
vendor_id:			0x02c9
vendor_part_id:			4125
hw_ver:				0x0
board_id:			ORC0000000007
phys_port_cnt:			1
port:	1
state:			PORT_ACTIVE (4)
max_mtu:		4096 (5)
active_mtu:		4096 (5)
sm_lid:			0
port_lid:		0
port_lmc:		0x00
link_layer:		Ethernethca_id:	mlx5_2
transport:			InfiniBand (0)
fw_ver:				16.29.1436
node_guid:			08c0:eb03:0072:5580
sys_image_guid:			08c0:eb03:0072:5580
vendor_id:			0x02c9
vendor_part_id:			4121
hw_ver:				0x0
board_id:			ORC0000000003
phys_port_cnt:			1
port:	1
state:			PORT_ACTIVE (4)
max_mtu:		4096 (5)
active_mtu:		1024 (3)
sm_lid:			0
port_lid:		0
port_lmc:		0x00
link_layer:		Ethernethi vikasdWe need more information for further debug this issue.
I suggest you collect the sysinfo-snapshot log and contact networking-support@nvidia.com for solve such issue.Thanks, just send the output to the alias you mention.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
105,mlnx-ofed-drivers-for-mt27520-connectx-3-pro-for-debian-11-bullseye,"I have a dual port MT27520 40Gb ConnectX-3 Pro card running on a Dell R630 with Proxmox 7.1. The underlying OS is Debian 11 Bullseye. The system is configured for SR-IOV, but the card does is not splitting the ports. I understand I need the 4.9-4.1.7.0 software for this card, but the driver is only available for Debian 10. I see that the newer cards have gotten Debian 11 support this week. When will this card have the driver available?Hello jeff80,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.The upcoming MLNX_OFED 4.9 support will have support for Debian 11.2. It is targeted in June.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
106,nftables-iptables-tc-and-changing-source-ip,"HiAre there plans to add nftables support to Cumulus?I was looking for a way to change the source IP of packets without connection tracking. Nftables seems to let you change the packet header without connection tracking, but not iptables.In the end I used tc, but the header changes happen in software not hardware.Hi ,I didn’t find any plans of supporting , if you have a good use case for it - please contact your Nvidia SE/sales team to push such a feature.Powered by Discourse, best viewed with JavaScript enabled"
107,mlnx5-nic-testpmd-tx-pp-wqe-index-ignore-feature-is-required-for-packet-pacing,"Hi,I am trying to run dpdk testpmd with Mellanox ConnectX4 Lx (mlx5 driver).I am specifying the tx_pp parameter to provide the packet send scheduling on mbuf timestamps, but the testpmd fails with the following error:EAL: Detected 36 lcore(s)EAL: Detected 2 NUMA nodesEAL: Multi-process socket /var/run/dpdk/rte/mp_socketEAL: Selected IOVA mode ‘PA’EAL: No available hugepages reported in hugepages-1048576kBEAL: Probing VFIO support…EAL: VFIO support initializedEAL: Probe PCI driver: mlx5_pci (15b3:1015) device: 0000:3b:00.0 (socket 0)mlx5_pci: No available register for Sampler.mlx5_pci: WQE index ignore feature is required for packet pacingmlx5_pci: probe of PCI device 0000:3b:00.0 aborted after encountering an error: No such devicecommon_mlx5: Failed to load driver = mlx5_pci.EAL: Requested device 0000:3b:00.0 cannot be usedEAL: Bus (pci) probe failed.EAL: No legacy callbacks, legacy socket not createdtestpmd: No probed ethernet devicesThe error messages suggest that “WQE index ignore feature is required for packet pacing”.Anyone knows the reason of this error and how to solve it?I know that WQE is related to RDMA (InfiniBand/RoCE) but I do not understand how it is related to DPDK.I have followed the MLX5 poll mode driver guide (http://doc.dpdk.org/guides/nics/mlx5.html).Hi Alessandro,The feature is supported starting from ConnectX-6 Dx, and it’s not supported for ConnectX-4 Lx.Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
108,mlx5-core-poll-health-raise-an-error-devices-health-compromised-reached-miss-count,"After I create several VF on ConnectX5 Adapter port 0, I got the following system message:
[ 1810.527156] mlx5_core 0000:51:00.3: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1811.487131] mlx5_core 0000:51:00.5: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1812.767131] mlx5_core 0000:51:00.6: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1812.831130] mlx5_core 0000:51:01.0: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1812.841027] mlx5_core 0000:51:00.7: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1815.007129] mlx5_core 0000:51:01.1: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1815.519130] mlx5_core 0000:51:01.3: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1816.159129] mlx5_core 0000:51:01.2: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1816.415130] mlx5_core 0000:51:01.6: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1816.543131] mlx5_core 0000:51:01.4: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1817.119130] mlx5_core 0000:51:01.5: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1818.271130] mlx5_core 0000:51:01.7: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1819.551131] mlx5_core 0000:51:02.0: poll_health:853:(pid 0): device’s health compromised - reached miss count
[ 1819.561031] mlx5_core 0000:51:02.1: poll_health:853:(pid 0): device’s health compromised - reached miss countWhat are those errors mean and impact ？I update the firmware and driver but it seems not help.Firmware version: 16.34.1002
Driver version: 5.7-1.0.2.0
OS: RHEL 8.3Hi,The print log “mlx5_core 0000:51:00.6: poll_health:853:(pid 0): device’s health compromised - reached miss count” is a driver warning messages.  So this message should not report any fatal error on FW.But, this “device’s health compromised - reached miss count” should not be the only log appears in your server messages log.   In most cases, it will comes together with other log message, like “synd 0x1: firmware internal error” or “print_health_info:466:(pid 0): ext_synd 0x8a02”.So the suggesetions are:If there is many firmware error log before/after the “device’s health compromised - reached miss count”, you need to share all those log to us.If you did update firmware and driver, please do a AC power cycle on this server, and see if the “device’s health compromised - reached miss count” still exist at server boot up.Longran Wei
Nvidia Support Teamhi，
Thanks for reply, one of my system’s log is attached.and I also find these are same error reported on my vf port that just created .dmesg.zip (35.4 KB)Hi lvzhipeng,Please try to execute this command: “flint -d 51:00.0 -ocr hw query”.If you can see the  “QuadEn                0”,
You can try to run “flint -d 51:00.0 -ocr hw set QuadEn=1”Then you can try to reboot and see if the error still exist or not.   Please let me know the test result.Longran Wei
Nvidia Support TeamThe “QuadEn” value is 0, but the set opration  is not supported.-W- Firmware flash cache access is enabled. Running in this mode may cause the firmware to hang.
HW Info:
HwDevId                 525
HwRevId                 0x0
Flash Info:
Type                    GD25LBxxx
TotalSize               0x1000000
Banks                   0x1
SectorSize              0x1000
WriteBlockSize          0x10
CmdSet                  0x80
QuadEn                  0
DummyCycles             15
Flash0.WriteProtected   Disabled
JEDEC_ID                0x1840c8
2. set failed even if i use the mst dev
[root@localhost ~]# flint -d 51:00.0 -ocr hw set QuadEn=1-W- Firmware flash cache access is enabled. Running in this mode may cause the firmware to hang.
-E- Unknown option “set” for the “Hw” command. you can use query.
[root@localhost ~]# flint -d /dev/mst/mt4119_pciconf0 -ocr hw set QuadEn=1-W- Firmware flash cache access is enabled. Running in this mode may cause the firmware to hang.
-E- Unknown option “set” for the “Hw” command. you can use query.I haved changed the value of “QuadEn” to 1, but it didn’t work as I still can see the error after reboot.
[root@localhost ~]# flint -d 51:00.0 -ocr hw querydmesg_with_QuadEn.log (5.3 KB)Hi lvzhipeng,Thanks for your update.From the dmesg log you sent, I suggest you can try to change this NIC to another PCI slot and reboot the server again.If problem still exist, and this NIC is still in warrenty, then you can submit a case ticket to our support portal(not forum) to ask for a RMA. (Our engineer should ask a few questions before it begin the RMA process)Thanks!
Longran Wei
Nvidia Support TeamPowered by Discourse, best viewed with JavaScript enabled"
109,hardware-vdpa-offloading,"Hi,I am interested in the vDPA techniques, but there are some confusing things while I am trying to understand. (https://docs.nvidia.com/networking/pages/viewpage.action?pageId=39279792#OVSOffloadUsingASAP²Direct-hwvdpa)Here are my questions:(H/W offloading data path) vDPA separates data path and vendor specific control path. In the hardware vDPA offloading with qemu, how packets are delivered from NIC to VM?(H/W offloading control path) How does the control path change once the H/W offloading is enabled?Is DPDK necessary? I mean, is DPDK theoretically unavoidable for hardware offloaded vDPA?Any material or paper is fine.Thanks!Hi bk-2,Thanks for posting your inquiry to the NVIDIA Developer Forums.1). Regarding offloading path with qemu, data appears to be limited: https://www.qemu.org/docs/master/interop/vhost-vdpa.html. As qemu is community-driven software, we recommend engaging the qemu development community for better understanding.2). On the link you referenced, in the overview section, the following statement is made:""The traditional ASAP2 hardware data plane is built over SR-IOV virtual functions (VFs), so that the VF is passed through directly to the VM, with the Mellanox driver running within the VM. An alternate approach that is also supported is vDPA (vhost Data Path Acceleration). vDPA allows the connection to the VM to be established using VirtIO, so that the data-plane is built between the SR-IOV VF and the standard VirtIO driver within the VM, while the control-plane is managed on the host by the vDPA application. ""3). As documented for our solutions, DPDK is a requirement. Other vendors / solutions may have other methods of implementation, but implementation of vDPA without DPDK is currently an unsupported configuration for
our drivers and hardware. That’s not to say ‘it definitely can’t ever be done’, but at current this is a requirement for MLNX_OFED and our adapters.Hope that helps clarify things.Thanks,
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
110,installing-mellanox-connectx-5-driver-on-ubuntu-but-in-airgapped-enviroment,"Hi,
We are working alot in airgapped enviroments, and have to upgrade a  Mellanox connectx 5 driver. but the software available on the webpage wants to download alot from apt. is there any solution to get around this?/Mortenhi Mortel,
What do you mean in airgapped enviroment ？Generally, we can download ConnectX5 driver from:
Linux InfiniBand DriversFind ‘Download’ → ‘Current Version’ ， select the version,  OS, then you can find a suitable version.We can get a full OFED package (tgz file)
Upload the package to ubuntu system. uncompress it then use ‘mlnxofedinstall’ command to install it.Regards,
LeveiHi, thanks for you reply.
By airgapped, i mean we have no internet connection on the server. and are not able to have an internet connection.
So all dependencies (etc. GCC) cannot be downloaded.
We have tried to get the full OFED package, and run the mlnxofedinstall, but fails when it want to download dependencies.
We would just like to have the ethernet MLX_EN driver, as a standalone.
is that possible?Best Regards
MortenPowered by Discourse, best viewed with JavaScript enabled"
111,bridge-over-vfs,"HiI did not find this information in the documentation, so I would like toknow if it is possible to create a bridge on a carddivided into severalVFs.What I mean is:1 - Divide a Mellanox board into 4 VFs per port;2 - Over a VF, create a bridge and use it for traffic.I ask this because I’m trying to run the VMs on RHEV this way and I’m notgetting it.Thank you all.Hi Jorge,Yes, it is possible.You just need to create the bridge and add the VF interface.Please review SR-IOV section in MLNX_OFED User Manual for the required steps to configure the VFs.https://docs.mellanox.com/pages/viewpage.action?pageId=39285051Note that it’s possible to offload the traffic using ASAP^2 technology, which describes here:https://docs.mellanox.com/pages/viewpage.action?pageId=39285091Regards,ChenHi Chen, thank you for your help!!Yes, with Openvswitch I already use it, both on connectx3 and connectx4 and it works.I would like to know if with normal Linux bridge, that is, without OpenvSwitch.Here I also use RHEV and I would like to use it, however RHEV does not support OpenvSwitch.Powered by Discourse, best viewed with JavaScript enabled"
112,problem-loading-mlx5-core,"I installed a system with Oracle Linux 8.6 to install the official updates. After yum update I tried to compile and install the Mellanox OFED drivers.After reboot the mlx5_core not loading, I see the following message in dmesg:[   19.308216] ------------[ cut here ]------------
[   19.308218] WARNING: CPU: 0 PID: 1886 at net/core/devlink.c:8047 devlink_alloc+0x37/0x1c3
[   19.308218] Modules linked in: mlx5_core(OE+) mlxfw(OE) mlxdevm(OE) ib_uverbs(OE) psample ib_core(OE) mlx_compat(OE) tls pci_hyperv_intf nft_fib_inet nft_fib_ipv4 nft_fib_ipv6 nft_fib nft_reject_inet nf_reject_ipv4 nf_reject_ipv6 nft_reject bonding nft_ct nf_tables_set sunrpc nft_chain_nat nf_nat nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 rfkill ip_set vfat fat amd64_edac_mod edac_mce_amd kvm_amd kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel crypto_simd cryptd glue_helper pcspkr ipmi_ssif ipmi_si sp5100_tco ipmi_devintf ccp i2c_piix4 k10temp ipmi_msghandler acpi_cpufreq sch_fq_codel knem(OE) xfs libcrc32c raid1 sd_mod ast sg drm_vram_helper ttm drm_kms_helper syscopyarea ahci sysfillrect igb sysimgblt libahci fb_sys_fops nvme drm libata tg3 dca nvme_core i2c_algo_bit pinctrl_amd dm_mirror dm_region_hash dm_log dm_mod fuse [last unloaded: mlx_compat]
[   19.308233] CPU: 0 PID: 1886 Comm: kworker/0:3 Tainted: G        W  OE     5.4.17-2136.318.7.1.el8uek.x86_64 #2
[   19.308233] Hardware name: GIGABYTE R282-Z94-00/MZ92-FS1-00, BIOS R25 10/11/2021
[   19.308235] Workqueue: events work_for_cpu_fn
[   19.308236] RIP: 0010:devlink_alloc+0x37/0x1c3
[   19.308237] Code: 85 ff 0f 84 a2 01 00 00 48 83 3f 00 74 25 48 83 7f 08 00 74 1e 48 8b 97 70 01 00 00 48 8d 42 ff 48 83 f8 06 0f 86 48 01 00 00 <0f> 0b 31 db e9 33 01 00 00 48 83 bf 80 01 00 00 00 74 0a 48 83 bf
[   19.308238] RSP: 0018:ffffb70ddd05fdd8 EFLAGS: 00010286
[   19.308238] RAX: ffffffffffffffff RBX: ffff97623a66f000 RCX: 0000000000000000
[   19.308239] RDX: 0000000000000000 RSI: 000000000002e548 RDI: ffffffffc0df1c20
[   19.308239] RBP: ffffb70ddd05fde8 R08: 0000000000000000 R09: 000073746e657665
[   19.308240] R10: 8080808080808080 R11: ffff97626ce2f080 R12: ffffb70dddfc7ab0
[   19.308240] R13: ffff97623a66f000 R14: ffff97623a66f0b0 R15: ffffffffc0dedfe0
[   19.308241] FS:  0000000000000000(0000) GS:ffff97626ce00000(0000) knlGS:0000000000000000
[   19.308241] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[   19.308242] CR2: 000055e5c454af44 CR3: 00008107c7c0a000 CR4: 0000000000340ef0
[   19.308242] Call Trace:
[   19.308275]  mlx5_devlink_alloc+0x1a/0x20 [mlx5_core]
[   19.308302]  probe_one+0x27/0x331 [mlx5_core]
[   19.308304]  local_pci_probe+0x47/0x98
[   19.308305]  work_for_cpu_fn+0x1a/0x25
[   19.308306]  process_one_work+0x1bb/0x3a9
[   19.308308]  worker_thread+0x1e1/0x3b2
[   19.308309]  kthread+0x120/0x136
[   19.308310]  ? create_worker+0x1b0/0x1ab
[   19.308311]  ? __kthread_cancel_work+0x50/0x46
[   19.308312]  ret_from_fork+0x2b/0x36
[   19.308314] —[ end trace 4a9f19eb93f9ef33 ]—
[   19.308315] mlx5_core 0000:01:00.1: devlink alloc failed
[   19.308320] mlx5_core: probe of 0000:01:00.1 failed with error -12Did anyone faced with the same situation?With OS reinstall and driver install before update I can solve the problem but for later updates it should be good to know what is the issue.Hello @lpopovics,Thank you for posting your query on our community. Assuming that you have installed Mellanox OFED 5.9 drivers, the default kernel for Oracle Linux 8.6 is 5.4.17-2136.307.3.1.el8uek.x86_64. If your kernel version does not match the default kernel, please use the --add-kernel-support flag while installing the drivers. Please refer to the RN here - https://docs.nvidia.com/networking/display/MLNXOFEDv590560/General+Support
Hope this answers your question.Thanks,
BhargaviThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
113,security-implications-of-log4j-in-cuda,"Log4j was recently assigned a widely reported CVE vulnerability: CVE - CVE-2021-44228CUDA drivers seem to contain an unpatched log4j version under
libnvvp/plugins/org.apache.ant_1.9.2.v201404171502/lib/ant-apache-log4j.jar.What is the impact of this dependency and will there be a security release of CUDA drivers?[Security Notice: NVIDIA Response to Log4j Vulnerabilities - December 2021 | NVIDIA]I think they are working now…We deploy ML Models on Production using Tesla T4 and K80 GPUs. I have also found the existence of a similar file at /usr/local/cuda-11.2/libnvvp/plugins/org.apache.ant_1.9.2.v201404171502/lib/ant-apache-log4j.jarTwo questions:Why is CUDA using log4j? There should be some configuration file for configuring logging if logging is the answer.What version of Log4J is CUDA using? Most JARs have versions declared in the filename itself.I think this is used in the cave-based performance profiler -but not in computing itself per se.typo—in the java-based performance profilerOur security and product teams are actively investigating this issue.Please continue to monitor the Security Notice for the latest updates at:
https://nvidia.custhelp.com/app/answers/detail/a_id/5294Thank you for investigating the risks posed by this log4j vulnerability. The Security Notice linked does not (yet) explain the presence of the ant-apache-log4j.jar library in the CUDA library, nor inform us as to whether it presents a risk. When you get a chance, please update the Security Notice so those of us who use your CUDA library may properly understand our risk. Thank you!+1 to the earlier comments.We see /usr/local/cuda-11.2/libnvvp/plugins/org.apache.ant_1.9.2.v201404171502/lib/ant-apache-log4j.jar in our environment, and according to the Maven listing it looks like this version contains log4j-1.2.13.This wouldn’t be affected by CVE-2021-44228 but it is affected by CVE-2019-17571, still a critical vulnerability.Are there any plans or timelines in place to fix this issue?Your comment about Maven is of interest. I was wondering about that --since I couldn’t tell from the jar files in the org.apache.ant_1.9.2 v201404171502/lib subdirectory. I agree this is likely of lesser risk. But I was concerned about comments about indirect exploits in the Carnegie Mellon analysis about lib4j 1.X: VU#930724 - Apache Log4j allows insecure JNDI lookups but ----we may be OK: VU#930724 - Apache Log4j allows insecure JNDI lookupsEssentially: Log4j 1.x does not have Lookups so the risk is lower. Applications using Log4j 1.x are only vulnerable to this attack when they use JNDI in their configuration. A separate CVE (CVE-2021-4104) has been filed for this vulnerability. To mitigate: audit your logging configuration to ensure it has no JMSAppender configured. Log4j 1.x configurations without JMSAppender are not impacted by this vulnerability. -This is their analysis: I may be off base but doing a jar tvf on the *.jars under the apache lib subdirectory I don’t see JMSAppender class paths defined or any configuration files in the Nvidia install so: SUMMARY if JNDI is not  used in the configuration and JMSAppender is not configured—I don’t think logging is going to occur–and the indirect vulnerability is not present. This is as far as I have gotten in my naive analysis. I will await word from NVIDIA postings. I do wonder why End of Life apache components are present in fairly recent NVIDIA-cuda releases. Presently I have chosen to delete the apache EOL components under the CUDA 10.1- CUDA 11.2 releases to sidestep the issue for the moment-as this does not affect GPU computing capability.GRID vGPU license manager is reported to be affected by the 1.x vulnerability.
The fix is simple: replace the library with a fixed version and then restart the process.Powered by Discourse, best viewed with JavaScript enabled"
114,connectx-5-dpdk-performance-degradation-when-enabling-jumbo-scatter-and-multi-segs,"I’m using a ConnectX-5 nic.
I have a DPDK application on which I want to support jumbo packets.
To do that I add rx offload capabilities: DEV_RX_OFFLOAD_JUMBO_FRAME, DEV_RX_OFFLOAD_SCATTER
And tx offload capabilities: DEV_TX_OFFLOAD_MULTI_SEGS
I also make the max_rx_pkt_len higher so it will accept jumbo packets (9k).
I’ve noticed that adding these offload capabilities + increasing the max_rx_pkt_len harms performance.
For example, without those offload flags, I can redirect 80Gbps packets of size 512 without any drop.
Using those flags reduce to ~55Gbps without losses.
I’m using DPDK 19.11.6.
Currently in this test I do not send jumbo packets. I just want to understand how it affects the average packet size.Is this expected? Using those offload flags should degrade performance?
ThanksSame problem here with DEV_TX_OFFLOAD_MULTI_SEGS. Quite a huge performance degradation even if you don’t actually send segments that would be split and only define a single segment type.Powered by Discourse, best viewed with JavaScript enabled"
115,mellanox-connectx-5-en-25gb-dual-port-spf-rdma-does-not-work-properly,"Good day,
I have a problem with RDMA and CPU utilization. My setup consists of 2x Dell AX-750 servers with 4x 25GB Mellanox Connect-X5 cards each. The servers are wired together with 4x 25GB DAC cables.I get a speed of 80-95 Gbps with ntttcp.I tested the whole thing with RDMA and without RDMA but I see almost no change in the CPU load. The core aspect of RDMA is supposed to be the reduction of CPU load but I see nothing of it. I see in the Windows Admin Center that RDMA activity is present but why is the CPU load not reduced?Driver information Windows 2019 core :3.0.25668.0RDMA is enabled on the HYPER V switch. In the BIOS RDMA is enabled am I missing something?I have a problem with RDMA and CPU utilization. My setup consists of 2x Dell AX-750 servers with 4x 25GB Mellanox Connect-X5 cards each. The servers are wired together with 4x 25GB DAC cables.I get a speed of 80-95 Gbps with ntttcp.I tested the whole thing with RDMA and without RDMA but I see almost no change in the CPU load. The core aspect of RDMA is supposed to be the reduction of CPU load but I see nothing of it. I see in the Windows Admin Center that RDMA activity is present but why is the CPU load not reduced?Driver information Windows 2019 core :3.0.25668.0RDMA is enabled on the HYPER V switch. In the BIOS RDMA is enabled am I missing something?While RDMA can significantly reduce CPU utilization in certain workloads, it is not a silver bullet solution and there can be several factors that impact its effectiveness. Here are some things you can check to help identify the cause of your high CPU utilization:Overall, the effectiveness of RDMA will depend on the specific workload and system configuration, and it may not always lead to a significant reduction in CPU utilization. However, by verifying that RDMA is properly configured and experimenting with different settings, you may be able to improve performance and reduce CPU utilization.Powered by Discourse, best viewed with JavaScript enabled"
116,ndivia-peer-mem-error-could-not-insert-nv-peer-mem-invalid-argument,"Hi all,I am facing this problem when trying to install nv_peer-memory-dkms in ubuntu-22.04Any help is welcome.Not sure where is this deb from “nvidia-peer-memory_1.2-0_all.deb”. likely not compatible with kernel 5.19.0-42So far, you can get nv_peer_mem from GPU driver and OFED driver.Just start with systemctl is OK.Or, you can git clone from github and build install.Contribute to Mellanox/nv_peer_memory development by creating an account on GitHub.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
117,performance-degradation-with-hairpin-and-asynchronous-api-in-dpdk,"Hi.
I wrote a network function which uses hairpin queues to avoid DMA with DPDK library.
But with only one hairpin queue (self pinned), my connectX-6 shows not full link speed (~164Gbps).
When I use more than two hairpin queues, it shows full link speed (~200Gbps).Also, when I use asynchronous rte_flow API which requires dv_flow_en=2, the performance drops more.
It became almost 10Gbps lower than when using synchronous API.Is there any one who knows the reason?Thank you in advance.Hi @cerotyki ,You can try increasing the hairpin buffer size by modifying this parameter: “hp_buf_log_sz”. The maximum value is 19. See: https://doc.dpdk.org/guides/nics/mlx5.htmlThe NIC can be loaded by these HW offloading features. We have internal debugging tools for such use cases that may help to locate the bottleneck / hot-spots in the HW. I recommend initiating a support case (based on your entitlement) for further debug.Regards,
ChenThank you for your help.I set hp_buf_log_sz as 19, so now one hairpin queue shows full link speed.Powered by Discourse, best viewed with JavaScript enabled"
118,help-debugging-example-p4-programs-on-bluefield-2-cards-in-dell-server-running-ubuntu-20-04,"I have two Dell servers each with a BlueField-2 card.  They are connected to each other via 100Gbps port.I am running one of the examples here but it doesnt seem to be receiving traffic. How do I confirm this example works?root@SERVERNAME:/home/ubuntu# /opt/mellanox/doca/examples/simple_fwd_vnf/bin/doca_simple_fwd_vnf -a auxiliary:mlx5_core.sf.2,sft_en=1 -a auxiliary:mlx5_core.sf.3,sft_en=1 – --nr_queues=4 --stats_timer=2
EAL: Detected 8 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: No available hugepages reported in hugepages-32768kB
EAL: No available hugepages reported in hugepages-64kB
EAL: No available hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL: No legacy callbacks, legacy socket not created
[19:15:53:078975][DOCA][I][SIMPLE_FWD_VNF]: core 1 process queue 1 start
[19:15:53:079103][DOCA][I][SIMPLE_FWD_VNF]: core 3 process queue 3 start
[19:15:53:079054][DOCA][I][SIMPLE_FWD_VNF]: core 2 process queue 2 start
[19:15:53:079173][DOCA][I][SIMPLE_FWD_VNF]: core 4 process queue 4 start
[19:15:53:079231][DOCA][I][SIMPLE_FWD_VNF]: core 5 process queue 5 start
[19:15:53:079486][DOCA][I][SIMPLE_FWD_VNF]: core 6 process queue 6 start
[19:15:53:079549][DOCA][I][SIMPLE_FWD_VNF]: core 0 process queue 0 startIt seems that the hugepages have not been allocated. Could you try running the below before running the application?sudo echo 2048 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepagesPlease refer to this link for the NVIDIA DOCA Simple Forward VNF Application Guide- Simple Forward VNF :: NVIDIA DOCA SDK Documentation
Troubleshooting Guide - Troubleshooting Guide :: NVIDIA DOCA SDK DocumentationPlease let me know if that works.Powered by Discourse, best viewed with JavaScript enabled"
119,infiniband-bonding,"Hi everyone,We have a handful of servers running gluster, equipped with either CX354A or CX456A Infiniband adapters. I am thinking about bonding to fully utilize the two ports each of them has.However, it looks like mode=1 is the only one working while the network wouldn’t start at all when setting to mode=0. https://community.mellanox.com/s/article/howto-create-linux-bond--lag--interface-over-infiniband-network said that mode=1 (active-backup) is the only meaningful bonding policy in IPoIB. Can somebody explain a little bit about it. Why is mode=1 the only meaningful one?Besides, does that work for RDMA as well?Thanks,WadeAs InfiniBand works based on LID to LID routing the only supported mode is 1 that is active-backup.Yes, it works with RDMA.-MamathaPowered by Discourse, best viewed with JavaScript enabled"
120,no-ping-with-connectx-5-vpi-socket-direct-on-esxi-7-0-u2,"Hi guys,I have 2 servers interconnecting switchless with ConnectX-5 VPI 100Gb ETH/IB Socker-Direct (2x8 PCIe), all well configured and I can also ping between Linux VMs (OFED Linux driver) from both hosts (via PCI passthrough the nic).
But when toggling off the passthrough mode from esxi 7.0U2 (vsphere / vcenter 7 U3) I cannot ping between my vSS. I dont unterstand it, because the same vSS work with other nics, mtu is set on 1500 and Im confused about esxi status: link up but no ping between hosts possible?! I tried to ping vmk and other maschines from the second host. I used 4.21.x driver and 4.19.x ethernet drivers… i also tried with OFED (but it is not supported on esxi 7).does anyone have experience how to connect these two cards in VMware environment?thanks!Hello,Are you using the VMware inbox driver or the Mellanox\NVIDIA drivers?
Please provide the full version of the drivers you are using.Can you please provide the output for this command by running from the ESXi CLIesxcli network nic listThanks,
Ilan.Hello @ipavis ,im using inbox and now try going back to async 4.21.71.1/101 with firmware 16.19.1016 (also tried 16.32.x)this is my output:
[root@esxi01:~] esxcli network nic list
Name    PCI Device    Driver      Admin Status  Link Status   Speed  Duplex  MAC Address         MTU  Descriptionvmnic0  0000:18:00.0  ixgben      Up            Up            10000  Full    0c:c4:7a:f3:ef:02  1500  Intel(R) 82599 10 Gigabit Dual Port Network Connection
vmnic1  0000:18:00.1  ixgben      Up            Down              0  Half    0c:c4:7a:f3:ef:03  1500  Intel(R) 82599 10 Gigabit Dual Port Network Connection
vmnic2  0000:af:00.0  bnxtnet     Up            Up            10000  Full    bc:97:e1:2a:2e:d0  1500  Broadcom BCM57412 NetXtreme-E 10Gb RDMA Ethernet Controller
vmnic3  0000:af:00.1  bnxtnet     Up            Up            10000  Full    bc:97:e1:2a:2e:d1  1500  Broadcom BCM57412 NetXtreme-E 10Gb RDMA Ethernet Controller
vmnic4  0000:b0:00.0  bnxtnet     Up            Up            10000  Full    bc:97:e1:2a:29:00  1500  Broadcom BCM57412 NetXtreme-E 10Gb RDMA Ethernet Controller
vmnic5  0000:b0:00.1  bnxtnet     Up            Up            10000  Full    bc:97:e1:2a:29:01  1500  Broadcom BCM57412 NetXtreme-E 10Gb RDMA Ethernet Controller
vmnic6  0000:86:00.0  nmlx5_core  Up            Down              0  Half    f4:52:14:22:01:80  1500  Mellanox Technologies ConnectX-5 VPI adapter card with Multi-Host Socket Direct supporting dual-socket server EDR IB (100Gb/s) and 100GbE dual-port QSFP28 2x PCIe3.0 x8 35cm harness (MCX556M-ECAT-S35A)
vmnic7  0000:86:00.1  nmlx5_core  Up            Up           100000  Full    f4:52:14:22:01:81  9000  Mellanox Technologies ConnectX-5 VPI adapter card with Multi-Host Socket Direct supporting dual-socket server EDR IB (100Gb/s) and 100GbE dual-port QSFP28 2x PCIe3.0 x8 35cm harness (MCX556M-ECAT-S35A)
vmnic8  0000:3c:00.0  nmlx5_core  Up            Down              0  Half    f4:52:14:22:01:86  1500  Mellanox Technologies ConnectX-5 VPI adapter card with Multi-Host Socket Direct supporting dual-socket server EDR IB (100Gb/s) and 100GbE dual-port QSFP28 2x PCIe3.0 x8 35cm harness (MCX556M-ECAT-S35A)
vmnic9  0000:3c:00.1  nmlx5_core  Up            Up           100000  Full    f4:52:14:22:01:87  9000  Mellanox Technologies ConnectX-5 VPI adapter card with Multi-Host Socket Direct supporting dual-socket server EDR IB (100Gb/s) and 100GbE dual-port QSFP28 2x PCIe3.0 x8 35cm harness (MCX556M-ECAT-S35A)Powered by Discourse, best viewed with JavaScript enabled"
121,creating-kubernetes-cluster-on-bcm-exception-version-of-the-local-path-provisioner-0-0-23-is-too-new,"HI TeamBelow is the error in setting up of K8s in BCMI managed to fix the above but my cmd service on compute nodes are failing after rebootIt looks like you upgraded your cmdaemon package, but did not upgrade cuda-dcgm-libs. Bright switched to a newer version of DCGM with 9.2-9.Best regards,Martijn@mdevries1I have reinstalled Master and Slaves again , its a fresh OS now . 1st thing I tried on it is Kubernetes Cluster , below is the screenshot of failure , how can I ignore permissions manager or download 0.1.1 version from where ?

image1106×616 28.3 KB
Same thing happens to another package , local path provisioner version , required is 0.0.20 and installed is 0.0.23My CM iso is Rocky linux 8.6FYI
I upgraded cluster-tools and cmdaemon , still I am getting the same error . How can i get kubernetes version highe r than 1.21 . I think that is the issueHi,The cm-setup package should also be updated. We have updated the error message to mention cm-setup instead of cluster-tools.The Bright version on your cluster appears to be really old. I don’t see any concrete version numbers, but the typo in the error message was fixed in 9.2-6, released in October. So the cluster must be at an even older state. It might be useful to update even more than just cm-setup, cmdaemon, cluster-tools, both on the headnode and in the software images.Cheers,
Geertcm-setup, cmdaemon, cluster-toolsUpdating now Geert…hope I can see k8s 1.24 :)
image1670×663 19.3 KB
:( , what I am supposed to do correctly , please let me know detailed steps
image1230×469 18.3 KB
cm-setup, cmdaemon, cluster-toolsManaged to roll forward …
image1665×760 26.6 KB
Got it…hope it doesn’t mess up now :)
image1073×174 4.96 KB
@gkloosterman @mdevries1
Is the license serving down ??
image807×341 10.3 KB
creating a k8s cluster is like a war…now my nodes are not starting kublet service
image1670×580 30.9 KB

image1191×650 33.3 KB

image1670×399 17.4 KB


image1068×415 19.3 KB
Do I need to create different images for each worker node or default image will work ?The Kubelet error you see is because the cmdaemon package is too old.  The older version you’re running does not know yet about the kubelet flags that were removed with k8s v1.24.Note you need to update cmdaemon both on the headnode and in the software images.  Also you need to make sure the nodes get the updated cmdaemon package and the cmd service is restarted on all nodes. Either by rebooting or by a combination of imageupdate using cmsh or Bright View to synchronize the updated package and something like pdsh -g computenode systemctl restart cmd to restart the service on all nodes.Updating on Headnode and Compute Nodes …Headnode
image1670×468 12.6 KB
Compute Node
image1669×530 14.3 KB
and i think its working :) , let me check further
image708×863 18.1 KB

image864×349 9.99 KB

Thanks guys for all the assistance @gkloosterman @mdevries1I have 2 more queries if you could give me a hand@gkloosterman @mdevries1 , should I create a new post for the above queries ?Yes it makes sense to create a new post for a new topic.Created , please checkPowered by Discourse, best viewed with JavaScript enabled"
122,connectx-4-25gbps-speed-issue,"Hi,
We have issue in connectx-4 25Gbps speed, its shows 25Gbps in Esxi but practically the throughput is not going more 10Gbps. We test with different scenarios. Please help.You might check that other components of connection -cables/switch and switch’s port support higher speed and configured properly.
Connecting it directly to other host switch 25 Gbps or higher speed may narrow the issue to specific component.Its already verified from cisco. Is firmware or drivers parameters effect speed of NIC?What kind of test you are running?
I would suggest to use ib_read_bw/ib_write_bw, that is part of perftest package, inside VM and connects port to the other host to eliminate switch from the picture and also bypass the kernel.
Another test from VM, that must have linux installed  and latest MOFED package, is to execute
run_perftest_loopback 1 2 ib_write_bw
and see what is the output. If it still showing around 10Gbps, there might be limitation in the hardware, that most likely will be a server. For example,  PCI speed and width are not expected.
In any case, there are little details about your setup in your questions - exact card PSID, firmware version, ESXi version, what are the tests, cable P/N, etc.Powered by Discourse, best viewed with JavaScript enabled"
123,nvidia-mib,"I’m trying to use the SNMP protocol to create a web-based monitoring interface. All the things that we want to monitor using this protocol require MIBs. For instance, if you search on Google, you can find MIBs such as HOST-RESOURCES-MIB, which can help us determine CPU and memory usage on a PC. To monitor GPU usage, we need to have the MIB for our graphics card. In our case, all of our PCs have NVIDIA graphics cards, but we haven’t been able to find the MIB for NVIDIA. Can you help us find it? Thank you.Hello @rafaelaviladelgado and welcome to the NVIDIA developer forums.NVIDIA GPUs are not exactly networked devices and as such don’t have dedicated MIB files.To get specific GPU information you will need to go through the OS services and create your own SNMP MIB extension that sends this data to your server. You can use for example nvidia-smito monitor this data or use some open source library that already implements SNMP wrappers for this or similar tools.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
124,bluefield-2-0-2-installation-fails,"I’m trying the “default” installation for DOCA 2.0.2, that is not changing the selection and running with sdkmanager --cli install --logintype devzone --product DOCA --version 2.0.2 --targetos Linux --host --target BLUEFIELD2_DPU_TARGETS --flash all    but I got this error :Anybody had that before?That’s the install summary :I couldn’t find a relevant detailed log…So I removed everything doca and OFED related, rebooted, and now I get stuck in image flashingI got a message telling me this is longer than usual.Funnily, just running sudo bfb-install --rshim rshim0 --bfb DOCA_2.0.2_BSP_4.0.3_Ubuntu_22.04-8.23-04.prod.bfb with the image downloaded online works…But now I can’t find the default password of that latest image… And anyway I’m not sure there’s nothing to do after bfb-install as the DOCA install is cut short.Powered by Discourse, best viewed with JavaScript enabled"
125,can-i-install-cuda-directly-on-my-bluefield-2,"Hi, everyone! I’m an undergraduate studying offloading NN inference on DPU. My professor told me that I should use tensorRT to accelerating my NN inference. But as far as I know, tensorRT depends on cuda and I can’t find cuda for arm64 architecture. Does installing cuda on dpu mean that installing cuda on gpu and expose it to dpu. Anyone helps, early thanks.Powered by Discourse, best viewed with JavaScript enabled"
126,can-i-safely-ignore-those-windows-warnings,"Hi there,Got 2 Mellanox ConnectX-4 MCX4121A-ACAT NICs installed on a Cisco UCS C240 M5SX with Windows Server 2016.Events in Windows Server are full of this message :Mellanox ConnectX 4 dual port 10/25G Ethernet NIC #4 Firmware version 14.24.1602 is below the minimum FW version recommended for this driver.Minimum recommended Firmware version for this driver: 14.31.392.It is recommended to upgrade the FW, for more details, please refer to WinOF-2 User Manual.I tried upgrade the firmware but can’t find the proper firmware since the PSID is Cisco-related.I had contact with Cisco before this thread and for us, the driver version and firmware version is compatible. It seems that Windows don’t like it.Cisco will not help me anymore.I’m wondering if someone had this message before and if I should pay attention to it.Thanks !Hello Kévène,Thank you for posting your question on the Mellanox Community.When using Mellanox OEM adapters, the OEM is the channel who will provide the certified s/w and f/w for the adapter. When installing the driver from our website, you are running into the issue, that the f/w of the adapter is not being updated as we do not provide OEM adapter f/w in our driver bundle.It is strange to say that Cisco does not want to assist you any further, as first this is their responsibility and second, it is an easy answer for them to your question.In your issue, please install the driver bundle provided by Cisco for your OS → Software Download - Cisco SystemsThis driver bundles contains the correct f/w version for your PSID. You need to re-install the driver for the adapter through, so the f/w can be updated through the WinOF-2 driver executable.Thanks and regards,~Mellanox Technical SupportHi Martijn and thanks for your answer.Done, but unfortunately, this updates the driver but not the firmware, so the warning remains.Can you help me about this ?Thanks !Powered by Discourse, best viewed with JavaScript enabled"
127,how-to-enable-sx6012-running-eth-single-switch-profile-with-vlan-filtering-in-l3-mode,"bind-point rifno bind-point rifChanges the ACL table bind point from L2 port mode to L3 port.The no form of the command resets this parameter to its default.History3.6.5000Exampleswitch (config mac access-list my-mac-list)# bind-point rifSX6012 [standalone: master] (config interface vlan 85) # ipv4 port access-group test% Access-list [test] doesn’t match binding point.SX6012 [standalone: master] (config ipv4 access-list test) # bind-point rif% Unrecognized command “bind-point”.bind-point rif is nt supported for SwitchX based systems. Only Spectrum 1,2,3 Systems.Powered by Discourse, best viewed with JavaScript enabled"
128,is-it-possible-to-use-connectx-4-5-6-and-an-appropriate-switch-to-create-a-50gbe-or-100gbe-infiniband-between-4-desktop-computers-with-i9-12900k-z690-ddr5-motherboard-and-rtx-a6000-a100-not-sure-if-enough-pcie-lanes-and-if-viable-between-desktops,"We are going to buy new deep learning hardware. To accommodate multiple users, the ideal setup would be 4 individual machines with A100 GPU on each. When occasionally we need to train a very large model, we would like to use fast infiniband for distributed training (if this is possible between 4 desktops)Hello Max,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we do see no issues in facilitating 50GbE (Ethernet) or 100Gb/s (InfiniBand) on this setup, as long as the system board can facilitate PCIe Gen3 x16 slots to house them.For the overall solutions, we do recommend to open a support ticket with NVIDIA Enterprise support, to have the Sales team validate the configuration based on your requirements. Most important aspect related to desktop stations is the cooling for the components.Majority of our certification is only done on server-grade platforms.You can send an email to enterprisesupport@nvidia.comThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
129,how-to-connect-sn-2100-switches-via-vxlan-over-a-campus-network,"We are operating two server clusters at different regions of a university campus. Each cluster is equipped with SN-2100 (ONXY) switches and each cluster has a dedicated IP subnet assigned by the university’s computing centre. The university provides a 40GBits network backbone interconnecting both clusters on layer 3.In order to consolidate some functionality in the clusters, we are aiming at connecting both clusters on layer 2, ideally using VXLAN.While the SN2100 switches claim to support VXLAN they seem to do so only in a very limited manner. At least all tutorials/documentation I found for VXLAN on that switch seems to assume that the involved switches are directly connected or that I am able to control the the routers between them. Both prerequisites are not fulfilled in our set-up.Is there an undocumented approach how I can still establish the tunnel between the two switches?Hello Jörg,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we recommend to open a NVIDIA Networking Support ticket to have our Professional Services advise you on a proper solution based on your current topology. You can open a ticket by sending an email to support@mellanox.comThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
130,how-to-set-rss-hash-calculation-from-inner-layer-not-from-outer-layer-for-tunneling-trafic,"Hi, sorry for my english, it is not my native language.I use dpdk and mellanox card ConnectX-5 and I try to have a hash value from the driver. I need to have the hash value calculated form inner layer for tunneling traffic. For now, the hash value reading from the mbuf struct is calcultated from outter layer (for vxlan or gre).I try to add the flag ETH_RSS_LEVEL_INNER_MOST to the hash functions in the initalising structure rte_eth_rss_conf used by the rte_eth_dev_configure. This flag is not supported.How can I have get a hash value based on the inner layers?I missed something, maybe I can’t have this hashvalue, any help will be really appreciated.DPDK version : 20.11MLNX_OFED version : 5.2-1.0.4.0ThanksHiwe have a list of supported RSS hashing types in DPDK + their definitions :https://doc.dpdk.org/guides/nics/mlx5.htmlIPv4, IPv6, TCPv4, TCPv6, UDPv4 and UDPv6 RSS on any number of queues.RSS using different combinations of fields: L3 only, L4 only or both, and source only, destinationThanks,SamerHello Samer, thanks for your reply,Regarding on https://doc.dpdk.org/guides/nics/mlx5.html :I can’t get the hash value based on the inner layers for tunnels.Is it really supported ? or I don’t understand this features.Thanks,Regards.NicolasBump.Any response on this question? Also what would be the behavior if the ETH_RSS_VXLAN RSS offload was to be selected? On what parameters would the hash be calculated? Would the VNI be used? Would outer header be used or inner?Powered by Discourse, best viewed with JavaScript enabled"
131,connectx-6-dx-packet-drop-when-enabling-rss-rxqueues,"TLDR:Testsetup:Issue:running testpmd without rss support reaches full 100 Gbps throughputOK (/)enabling (symmetric) RSS reaches full 100 Gbps throughput for UDP traffic profilesOK (/)using RSS-IP support drops performance > 50 % (/)Questions:RELATED Issues:issue might be related to. See the same RX-drop upon reaching 54Mpps ONLY when using TCP-traffic.https://mymellanox.force.com/mellanoxcommunity/s/question/0D51T00008yTVgjSAG/how-to-improve-the-performance-of-flow-steering-on-connectx5DPDK patchset:{code}diff --git a/app/test-pmd/parameters.c b/app/test-pmd/parameters.cindex f9185065af…7f6d29c16e 100644— a/app/test-pmd/parameters.c+++ b/app/test-pmd/parameters.c@@ -1095,7 +1095,7 @@ launch_args_parse(int argc, char** argv)if (!strcmp(lgopts[opt_idx].name, “forward-mode”))set_pkt_forwarding_mode(optarg);if (!strcmp(lgopts[opt_idx].name, “rss-ip”))if (!strcmp(lgopts[opt_idx].name, “rss-udp”))rss_hf = RTE_ETH_RSS_UDP;if (!strcmp(lgopts[opt_idx].name, “rss-level-inner”))diff --git a/app/test-pmd/testpmd.c b/app/test-pmd/testpmd.cindex 55eb293cc0…846cb35a5f 100644— a/app/test-pmd/testpmd.c+++ b/app/test-pmd/testpmd.c@@ -86,6 +86,20 @@#define EXTMEM_HEAP_NAME “extmem”#define EXTBUF_ZONE_SIZE RTE_PGSIZE_2Muint8_t symmetric_rss_key[40] = {0x6D, 0x5A, 0x6D, 0x5A,0x6D, 0x5A, 0x6D, 0x5A,0x6D, 0x5A, 0x6D, 0x5A,0x6D, 0x5A, 0x6D, 0x5A,0x6D, 0x5A, 0x6D, 0x5A,0x6D, 0x5A, 0x6D, 0x5A,0x6D, 0x5A, 0x6D, 0x5A,0x6D, 0x5A, 0x6D, 0x5A,0x6D, 0x5A, 0x6D, 0x5A,0x6D, 0x5A, 0x6D, 0x5A};uint16_t verbose_level = 0; /**< Silent by default. */int testpmd_logtype; /**< Log type for testpmd logs */@@ -3757,7 +3771,8 @@ init_port_config(void)return;if (nb_rxq > 1) {port->dev_conf.rx_adv_conf.rss_conf.rss_key = symmetric_rss_key;port->dev_conf.rx_adv_conf.rss_conf.rss_key_len = 40;port->dev_conf.rx_adv_conf.rss_conf.rss_hf =rss_hf & port->dev_info.flow_type_rss_offloads;} else {{code}Hi Tobias,As you know,’ --rss-udp’ is NOT only for UDP, include ipv4/ipv6 and UDP.and ‘–rss-ip’ is only for ipv4/ipv6.​About rx-drop issue, maybe first check if the RSS works or not.Use ‘show port xstat’ to check if all queues receive packets.If not, check the receiving packet attributes.If the 5 tuples of all packets are the same, the RSS won’t work.Regards,Levei​Hi @Levei Luo​ ,please check my initial post again.In my case --rss-ip has been customized and addsrss_hf = ETH_RSS_IP | ETH_RSS_TCP | ETH_RSS_UDP | ETH_RSS_SCTP;instead of initialrss_hf = RTE_ETH_RSS_IP;. Just imagine we added a new cli-flag rss-tcp-udp-sctp !Also with regards to your ideas:Please involve 3rd-level support here and provide a working configuration for enabled symmetric rss.Any other card vendor can easily run symmetric rss on 100 Gbps onwards.What am I missing here? ThanksHi @Levei Luo​ , any updates? Please check my previous post.Hey Tobias R (Partner)I have a similar problem.NVIDIA Developer ForumsNVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
132,doca-flow-pipe-acl-and-icmp,"We are working on a DOCA_FLOW application and working with ACL pipes (DOCA_FLOW_PIPE_ACL) under DOCA 2.0.2. We find that we cannot add flow entries for matching IPv4 ICMP traffic. Flow additions are rejected with “Unsupported Protocol” errors if flow L4 type is set to DOCA_FLOW_L4_TYPE_EXT_ICMP.   We CAN match TCP and UDP traffic successfully so I believe we are using ACL pipes correctly overall.How might one match ICMP traffic in an ACL type pipe?@xiaofengl - perhaps you can check this; it seems like something you might know.Thanks!
-JI recall DOCA flow API not support ICMP header match on old version.But I checked latest API, it already have.https://docs.nvidia.com/doca/sdk/flow-programming-guide/index.html#doca-flow-header-formatstruct doca_flow_header_format {
struct doca_flow_header_eth eth;
uint16_t l2_valid_headers;
struct doca_flow_header_eth_vlan eth_vlan[DOCA_FLOW_VLAN_MAX];
enum doca_flow_l3_type l3_type;
union {
struct doca_flow_header_ip4 ip4;
struct doca_flow_header_ip6 ip6;
};
enum doca_flow_l4_type_ext l4_type_ext;
union {
struct doca_flow_header_icmp icmp;
struct doca_flow_header_udp udp;
struct doca_flow_header_tcp tcp;
};
};https://docs.nvidia.com/doca/sdk/flow-programming-guide/index.html#doca-flow-match@XiaofenglYep, this is what we are trying to use.  Unfortunately it returns error.Do you have access to enterprise support tickets?  If so, can you take a look at my case 00603711 which has an attached code sample.  I wonder if you might have thoughts on this…-JI have a short review,DOCA_FLOW_L4_TYPE_EXT_ICMP, may not define on runtime.We do have ICMP match API,doca\libs\doca_flow\doca_flow_net.henum doca_flow_l4_type_ext {
DOCA_FLOW_L4_TYPE_EXT_NONE = 0,
/< l4 ext type is not set */
DOCA_FLOW_L4_TYPE_EXT_TCP,
/< l4 ext type is tcp */
DOCA_FLOW_L4_TYPE_EXT_UDP,
/< l4 ext type is udp */
DOCA_FLOW_L4_TYPE_EXT_ICMP,
/< l4 ext type is icmp */
DOCA_FLOW_L4_TYPE_EXT_ICMP6,
/**< l4 ext type is icmp6 */
};Where you run doca_flow (host/dpu)? you need check if 2.0.2 runtime libs install properly.I am on vacation so far.I see 00603711 handled by James Tau, he could check that to help you.@xiofenglThank you for looking at this.  To clarify, you comment“DOCA_FLOW_L4_TYPE_EXT_ICMP, may not define on runtime.”Does it mean L4 type is not allowed in individual match entries and can only be specified in pipe config, doca_flow_pipe_create(), call?  In other words, this field must be constant for whole pipe?If so, this probably explain my issue.  So, if I need to filter some ICMP, TCP, and UDP, then best approach is to chain three pipes, one for each L4, using fwd_miss on each one to reach the next?I will try it today on my lab.Also, you ask if we run DOCA 2.0.2 on host or dpu.  Answer is we run on DPU.  For test, we compile code on DPU directly and run there via SSH.Thank you for replying even in your holiday; I really appreciate the help.  I just got back from holiday myself, yesterday.-JThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
133,ats-pri-and-pasid-capabilities-support-on-connectx-7,"I am working with ConnectX-7, after updating firmware version to 28.37.1014 and doing below step
mlxconfig -d /dev/mst/mt4129_pciconf0 set ATS_ENABLED=1
I see ATS capability listed in lscpi output but I don’t see ATS requests raised by the device, as I do not see PCIe RC PMU(Performance monitor unit) ATS counters getting incremented. Could someone confirm device supports ATS requests.And, also I do not see PRI and PASID capabilities listed in the device’s config space, should I need to enable something more to support these capabilities?For your information I am using the latest Kernelv6.3 which has software support to these PCIe capabilities.Thanks,
AnilHello,It seems that you enabled it properly on the adapter, so please check the BIOS configuration or check with the server vendor.This are the needed commands to enable the ATS:to check that its enabled:
3.       mlxconfig -d  q | grep ATS_ENABLED
you should see this:
·         mlxconfig -d /dev/mst/mt4123_pciconf0 q | grep ATS_ENABLED
ATS_ENABLED                         True(1)
to see it in the config space:
4.       lspci -s  -vvvxxx | grep ATSyou should see this:·         Capabilities: [480 v1] Address Translation Service (ATS)
ATSCap: Invalidate Queue Depth: 00
ATSCtl: Enable-, Smallest Translation Unit: 00If all the above is set then you should check the BIOS and kernel.Best Regards,
VikiThanks for your reply @vikiz.After doing steps 1,2.
I see ATS capability got enabled on the card.  and have confirmed as suggested.#mlxconfig -d /dev/mst/mt4129_pciconf0 q | grep ATS_ENABLED
ATS_ENABLED                                 True(1)#lspci -s 000b:01:00.0 -vvv | grep ATS
Capabilities: [480 v1] Address Translation Service (ATS)
ATSCap: Invalidate Queue Depth: 00
ATSCtl: Enable+, Smallest Translation Unit: 00After enabling ATS  ATSCtl: Enable+,  I do not see ATS requests raised by the device, I have confirmed this  by monitoring PCIe RC PMU(Performance monitor unit) ATS counters. Kindly please confirm firmware version 28.37.1014 have ATS support.And even after enabling the ATS capability mentioned above,  I do not see PRI and PASID capabilities listed in the device’s config space.  Should I need to enable something more? Linux software stack expects the endpoint device capability to be listed in first place so that it will enable and use these capabilities.Thanks,
AnilAny update on this request?Hi,
I see no further support here, can you tag/assign this ticket to right forum/technical team or guide me how to reach them. We bought couple of ConnectX-7 cards for ATS/PRI capabilities, we would need support to enable and use those capabilities.Thanks,
AnilPowered by Discourse, best viewed with JavaScript enabled"
134,connectx-6-dx-22-37-1014,"Hi,Is there any known issue with ConnectX 7 22.37.1014? I have noticed that the VF driver fails to ping external network.It pings fine using the earlier firmware which is 22.36.1010.Thanks,
MarcHello Marc,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Currently we have no record regarding a known issue based on your description.Actually, there was a known ‘ping’ issue related to VF based on 22.36.1010, but that was resolved.
image1009×146 8.3 KBTo better triage this issue, we recommend to open an official NVES support ticket.Thank you and regards,
~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
135,inline-ipsec-offload-support-on-crypto-enabled-bluefield-2,"Hi,
I was trying to run ipsec-gw DPDK application (22.07) with inline-crypto-mode on BlueField-2 (crypto-enabled), but it failed with the following error.    Is inline crypto mode supported on BlueField 2?sudo dpdk-ipsec-secgw -l 0,1 -n 4 --vdev “crypto_null” -a 03:00.0 -a 03:00.1 – -p 0x3 -P -u 0x3 --transfer-mode poll -f ~/default.cfg --config=“(0,0,0),(1,0,1)”EAL: Detected CPU lcores: 8
EAL: Detected NUMA nodes: 1
EAL: Detected static linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘VA’
EAL: VFIO support initialized
mlx5_common: DevX create q counter set failed errno=22 status=0 syndrome=0
EAL: Probe PCI driver: mlx5_pci (15b3:a2d6) device: 0000:03:00.0 (socket 0)
mlx5_common: DevX create q counter set failed errno=22 status=0 syndrome=0
EAL: Probe PCI driver: mlx5_pci (15b3:a2d6) device: 0000:03:00.1 (socket 0)
mlx5_common: DevX create q counter set failed errno=22 status=0 syndrome=0
CRYPTODEV: Creating cryptodev crypto_nullCRYPTODEV: Initialisation parameters - name: crypto_null,socket id: 0, max queue pairs: 8
Promiscuous mode selected
librte_ipsec usage: disabled
replay window size: 0
ESN: disabled
SA flags: 0
Frag TTL: 10000000000 ns
lcore/cryptodev/qp mappings:
Inbound cdev mapping: lcore 0 using cdev 0 qp 0 (cdev_id_qp 0)
Inbound cdev mapping: lcore 1 using cdev 0 qp 1 (cdev_id_qp 0)Allocated mbuf pool on socket 0
CRYPTODEV: elt_size 64 is expanded to 272Allocated session pool on socket 0
Allocated session priv pool on socket 0
Number of mbufs in packet pool 10880
Configuring device port 0:
Address: 02:CA:F9:64:0E:2F
Creating queues: nb_rx_queue=1 nb_tx_queue=2…
EAL: Error - exiting with code: 1
Cause: Error: port 0 required RX offloads: 0x800e, available RX offloads: 0x18620fContent of configuration file:sp ipv4 in esp protect 105 pri 1 dst 192.168.115.0/24 sport 0:65535 dport 0:65535#SA rules
sa in 105 cipher_algo aes-128-cbc cipher_key a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0 auth_algo sha1-hmac auth_
key a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:a0:0 mode ipv4-tunnel src 192.168.115.10 dst 192.168.115.20
type inline-crypto-offload port_id 0#Routing rules
rt ipv4 dst 192.168.115.0/24 port 1I tried full IPsec offload in switchdev mode with ip xfrm, and it works fine.  It’s just that I cannot enable full IPsec offload in DPDK.  Any help will be appreciated.Thanks,
-ChangHello,inline crypto mode is not supported on BlueField-2.
IPsec is supported in kernel (xfrm). Nvidia will have Beta support via DOCA software later this year.Best Regards,
VikiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
136,can-rdma-be-used-with-lacp,"Hello Team,My question is : Can RDMA be used with lacp on windows servers with Connectx5-6 nics. I’ve checked the RDMA and RoCE terms little bit and i couldn’t able to see any article for link-redundancy ? Do you have any manual or installation guide for link redundancies  ? In near future we are planning to build a IP SAN with Mellanox and redundancy is mandatory for us.Waiting for your feedback.
Have a nice dayHi sezgink059,Microsoft recommends using RDMA with Switch Embedded Teaming (SET) and SMB Direct for RDMA link aggregation.  SET is a new teaming mechanism that Microsoft would like to replace traditional Windows LBFO teaming.  You will need to enable Hyper-V guest support and create a Hyper-V vSwitch for SET, even you only use the team for physical workload link MSSQL on metal.After SET vSwitch is created, you can create a vNIC with option “-AllowManagementOS” to create a vNIC on host Windows OS.  This vNIC is then running on pNIC team, fault tolerated, bandwidth aggregated, and running on low latency RDMA, but it still needs whole brunch of tuning, including pNIC CPU affinity, VMQ/RSS, pNIC driver configuration, SMB server/client setting, etc.  The higher the NIC speed the more important those tuning will impact outcome performance.  I spent months to figure most of them out and result is a super stable Hyper-V HCI that has run for 4-5 years already without major instability and performance problem.  My NIC are are just the old ConnectX-3 Pro only.  I believe your ConnectX-5 (or 6?) will give better result.  Please avoid mixing pNIC models and speed for team members of a single vSwitch.SET can be done via PowerShell, Windows Admin Center or System Center SCVMM.  One of the requirements for SET is the teaming mode set to be “switch independent” instead of LACP, that means RDMA is not preferred to be used in conjunction with LACP in Microsoft network teaming.  Once pNICs are added into a SET teamed vSwitch, you can fine tune the team pNIC to vNIC mapping via PowerShell.If you are looking for an IP SAN, you can consider Windows native Storage Space Direct (S2D) on Windows Failover Cluster, it has a lot of exciting storage feature like automatic tiering, deduplication, volume encryption, sync/async volume replication etc., it is also a world SDS performance record holder in this specific area.  The most important point is: it is free from Windows Server OS license.You can google for “Microsoft S2D deployment guide” and search for some server hardware vendors’ reference architecture and step-by-step implementation walkthrough.Cheers.Powered by Discourse, best viewed with JavaScript enabled"
137,doca-1-1-is-available-now,"Announcing the release of DOCA 1.1 | Bluefield OS v3.7 | DOCA SDK v0.2PlatformsKey FeaturesNVIDIA DOCA is an EA program. To get started with DOCA for DPUs, sign up for DOCA early access here: https://developer.nvidia.com/nvidia-doca-sdk-early-accessCheck out more details about this release on the blog: https://developer.nvidia.com/blog/nvidia-continues-bluefield-dpu-doca-momentum-with-the-release-of-doca-1-1/Powered by Discourse, best viewed with JavaScript enabled"
138,nvmet-hw-offload-issue,"Does anyone who had the same issue when enabling hw offload for nvmet?314.425940] nvmet: adding nsid 1 to subsystem testsub
[  314.427031] nvmet: adding nsid 2 to subsystem testsub
[  314.428111] nvmet: adding nsid 3 to subsystem testsub
[  314.429861] nvmet: adding nsid 4 to subsystem testsub
[  314.431758] nvmet: adding nsid 5 to subsystem testsub
[  314.432838] nvmet: adding nsid 6 to subsystem testsub
[  314.433888] nvmet: adding nsid 7 to subsystem testsub
[  314.435059] nvmet: adding nsid 8 to subsystem testsub
[  314.436122] nvmet: adding nsid 9 to subsystem testsub
[  314.437276] nvmet: adding nsid 10 to subsystem testsub
[  314.441046] nvmet_rdma: enabling port 1 (192.168.200.13:4420)
[  322.952806] nvmet: creating nvm controller 1 for subsystem testsub for NQN nqn.2014-08.org.nvmexpress:uuid:e9184000-7983-11ec-8000-d85ed34e1861.
[  324.192974] nvme nvme10: creating 63 I/O queues.
[  335.349806] nvmet_rdma: using dynamic staging buffer 00000000191efefc
[  335.403750] nvme 0000:21:00.0: Failed to get peer resource xrq=00000000a7817acc be_ctrl=000000009341a12e
[  335.403790] nvmet_rdma: failed to get XRQ for queue (1)
[  335.403806] nvmet: failed to install queue 1 cntlid 1 ret 4006
[  336.547842] nvme nvme10: mapped 63/0/0 default/read/poll queues.
[  336.607738] nvme nvme10: Connect command failed, error wo/DNR bit: 6
[  336.608835] nvme nvme10: failed to connect queue: 1 ret=16390
[  347.356444] nvme nvme10: rdma connection establishment failed (-104)Hi @lpopovics ,Please ensure that you refer to this documentation for configuring the nvme-of target offload: ESPCommunity.When configuring the target, it is important to note that the command “modprobe nvme num_p2p_queues=1” should be executed.
This command sets the number of I/O queues that can be used for peer-to-peer.To determine the actual number of I/O queues available for peer-to-peer, you can read the “num_p2p_queues” sysfs entry.
If the command “cat /sys/block/<nvme_device>/device/num_p2p_queues” shows zero, it is necessary to remove the nvme module before setting “modprobe nvme num_p2p_queues=1”.
This can be done using the command “modprobe -rv nvme”.Best regards,
ChenI set the num_p2p_queues to 30, as I have 10 devices per host and communicating with two other hosts. Is there any documentation available for the parameters how I can tune them according my setup? I used the setup of the guide which available on the portal, but not every parameter available what the documentation refers. (offload_mem_start)
https://mellanox.my.site.com/mellanoxcommunity/s/article/howto-configure-nvme-over-fabrics--nvme-of--target-offloadOne addition, on the target host I see these messages:
[  105.487209] nvmet_rdma: enabling port 1 (192.168.200.11:4420)
[  114.488300] nvmet: creating nvm controller 1 for subsystem delkvmsrv01 for NQN nqn.2014-08.org.nvmexpress:uuid:e9184000-7983-11ec-8000-d85ed34e19d9.
[  127.454630] nvmet_rdma: using dynamic staging buffer 000000004289d44f
[  127.508490] nvme 0000:21:00.0: Failed to get peer resource xrq=000000002642df80 be_ctrl=000000001efd7ba9
[  127.508544] nvmet_rdma: failed to get XRQ for queue (1)
[  127.508562] nvmet: failed to install queue 1 cntlid 1 ret 4006
[  145.663147] nvmet: creating nvm controller 1 for subsystem delkvmsrv01 for NQN nqn.2014-08.org.nvmexpress:uuid:e9184000-7983-11ec-8000-d85ed34e19d9.
[  158.440082] nvme 0000:21:00.0: Failed to get peer resource xrq=000000002642df80 be_ctrl=000000001efd7ba9
[  176.894546] nvmet: creating nvm controller 1 for subsystem delkvmsrv01 for NQN nqn.2014-08.org.nvmexpress:uuid:e9184000-7983-11ec-8000-d85ed34e19d9.
[  189.904877] nvme 0000:21:00.0: Failed to get peer resource xrq=000000002642df80 be_ctrl=000000001efd7ba9
[  208.125623] nvmet: creating nvm controller 1 for subsystem delkvmsrv01 for NQN nqn.2014-08.org.nvmexpress:uuid:e9184000-7983-11ec-8000-d85ed34e19d9.
[  221.037074] nvme 0000:21:00.0: Failed to get peer resource xrq=000000002642df80 be_ctrl=000000001efd7ba9
[  239.356015] nvmet: creating nvm controller 1 for subsystem delkvmsrv01 for NQN nqn.2014-08.org.nvmexpress:uuid:e9184000-7983-11ec-8000-d85ed34e19d9.
[  252.141791] nvme 0000:21:00.0: Failed to get peer resource xrq=000000002642df80 be_ctrl=000000001efd7ba9
[  270.075432] nvmet: creating nvm controller 1 for subsystem delkvmsrv01 for NQN nqn.2014-08.org.nvmexpress:uuid:e9184000-7983-11ec-8000-d85ed34e19d9.
[  282.453918] nvme 0000:21:00.0: Failed to get peer resource xrq=000000002642df80 be_ctrl=000000001efd7ba9
[  300.283618] nvmet: creating nvm controller 1 for subsystem delkvmsrv01 for NQN nqn.2014-08.org.nvmexpress:uuid:e9184000-7983-11ec-8000-d85ed34e19d9.
[  313.068466] nvme 0000:21:00.0: Failed to get peer resource xrq=000000002642df80 be_ctrl=000000001efd7ba9
[  325.752641] nvmet: ctrl 1 keep-alive timer (5 seconds) expired!
[  325.753098] nvmet: ctrl 1 fatal error occurred!
[  660.610330] nvmet: creating nvm controller 1 for subsystem delkvmsrv01 for NQN nqn.2014-08.org.nvmexpress:uuid:e9184000-7983-11ec-8000-d85ed34e19d9.Powered by Discourse, best viewed with JavaScript enabled"
139,switchdev-not-possible-on-bluefield-2,"Hello, as part of an evaluation process I am trying to configure an OVS Offload on a Bluefield-2 25GbE Card according to the following guide (specifically the guide linked under the section “Enabling OVS-DPDK Hardware Offload”):However I am not able to configure the SwitchDev mode according to the section: "" SwitchDev Configuration"". Trying to do so results in an “Operation not permitted” response. Help regarding this configuration would be much appreciated.Host OS version: CentOS stream 8 (kernel: 4.18.0-193.28.1.el8_2.x86_64)Bluefield Info:
Type: Bluefield2
Name: MBF2H322A-AEEO_Ax_Bx
Description: BlueField-2 P-Series DPU 25GbE Dual-Port SFP56; PCIe Gen4 x8; Crypto Enabled; 8GB on-board DDR; 1GbE OOB management; HHHLDPU no need configure switchdev, it enable and configured default.Thanks for the clarification. I was not aware of that.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
140,cc-mgr-not-present-in-all-lts-4-9-versions-of-mlnx-ofed,"Hello,cc_mgr was present in MLNX OFED 4.9-0.1.7.0 but wasn’t in any versions after that (ex 4.9-2.2.4.0 or the latest 4.9-4.1.7.0).Because I didn’t find anything in the release not about it, is there any reason why it disappeared?Regards,
Mathieuhi Mathieu,
It is integrated into OpenSM. Now use ‘cc policy’ for congestion control.
There is config file under opensm: /etc/opensm/cc-policy.conf .Regards,
LeveiHi Levei,Sorry for the late reply, thanks for getting back to me about that!Regards,
MathieuPowered by Discourse, best viewed with JavaScript enabled"
141,gpudirect-rdma-on-x86-linux-pc-driver-build-issue,"I am trying to do GPUDirect RDMA mentioned the below link. I am using an X86 linux PC with Quadro M4000 GPU and CUDA 12.0 tool kit.GPUDirect_RDMA
When ran command “./build-for-pc-native.sh”, get the following error:
“”""
Building modules, stage 2.
**  MODPOST 1 modules**
FATAL: parse error in symbol dump file
scripts/Makefile.modpost:92: recipe for target ‘__modpost’ failed
make[2]: *** [__modpost] Error 1
Makefile:1678: recipe for target ‘modules’ failed
make[1]: *** [modules] Error 2
make[1]: Leaving directory ‘/usr/src/linux-headers-5.4.0-136-generic’
Makefile:19: recipe for target ‘modules’ failed
make: *** [modules] Error 2
“”""To avoid the above error, I ran “./build-for-any-no-cuda-native.sh” this command the module is built and loaded.
But again, I get error when I ran the command “client-applications/./build-for-pc-native.sh”,
make: Nothing to be done for ‘default’.Ignoring this error when I ran the data acess test given the above github link, I do not get an error while running the command “./rdma-malloc”. But, I get the following error, when I ran this command “./rdma-cuda”,
ioctl(PIN_CUDA src) failed: ret=-1 errno=22.Again when I did the ""set leds test, if I use command ""./set-leds 7, I get an error “open() failed: Permission denied”, but when I use “sudo” before the command, i don’t get an error.Kindly, anyone who knows reply to this thread for the above errors and why they are occuring?GPU Driect support on TESLA GPU.@xiaofengl  GPU Driect support on TESLA GPU?
I think you forgot provide any link or I did not get your answer.Kindly, anyone who knows the cause for the above errors and why they are occurring? reply to this thread.Kindly reply,
Driver version - NVIDIA-Linux-x86_64-525.78.01
kernel version - 5.4.0-137-generic
These kernel and driver versions are compatible to build the picoevb module or not?Powered by Discourse, best viewed with JavaScript enabled"
142,how-do-i-change-the-mac-address-for-a-mcx4131a-gcat,"How do I change the MAC address for aMCX4131A-GCAT? Trying to change it using FLINT results in a cryptic error. Using Ubuntu 20.04 with the newest MFT software installed.The reason I need to change the MAC address is that somehow it became corrupted and it now shows as 0000001234.Any help with this would be appreciated.Hi Charles,Please follow the below guide and let me know if it helps.https://docs.mellanox.com/pages/viewpage.action?pageId=32411642Regards,ChenThank you so much for your response. I’m sure I would have eventually stumbled upon the correct documents, but you link makes my job so much easier.Chuck RenfroeHardware Technical Lead(270) 801-3015the command listed on the page you send me results in the following error:For Example:#flint -d /dev/mst/mt4117_pciconf0 -ocr hw set Flash0.WriteProtected=disabled-W- Firmware flash cache access is enabled. Running in this mode may cause the firmware to hang.-E- Command “Hw” requires at most 1 arguments, but 2 arguments were givenAny ideas what I’m doing wrong? Keep in mind I followed the example in the link you provided exactly.Powered by Discourse, best viewed with JavaScript enabled"
143,installing-doca-on-bluefield-dpu,"Hi all,
I’m trying to redo instructions to upgrade to doca v1.1
When trying to install Doca on Bluefield DPU, my bluefield does not have network access, I have setup the firewall, but still unable to do "" apt-get update"", do I have to change the mode to separated mode?
Because in Bluefield 1 we had the same issue and were not able to fix proxy setting in the embedded modeIdeally the oob_net0 would be connected into a separate out of band management network. But in embedded mode, you can still configure an interface on the DPU and assign and IP address and route to go out through the OVS bridges for network access.Thanks @jubetz , can you please clarify how to set OVS bridges for network access? the problem I have is that I don’t see p2p1 p2p2 interfaces in the document, what are these interfaces that we need to assign IP addresses?
Here are the interfaces I see on my first Bluefield2 card, is p2p1 equal to p0 and p2p2 equal to p1?:
root@localhost:~# ifconfig
en3f0pf0sf0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
ether d2:48:c6:80:c0:ec  txqueuelen 1000  (Ethernet)
RX packets 61  bytes 15582 (15.5 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 4711  bytes 738442 (738.4 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0en3f1pf1sf0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
ether 8e:88:f4:78:8a:0d  txqueuelen 1000  (Ethernet)
RX packets 63  bytes 15762 (15.7 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 4704  bytes 737784 (737.7 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0enp3s0f0s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet6 fe80::d:58ff:fe0b:1f67  prefixlen 64  scopeid 0x20
ether 02:0d:58:0b:1f:67  txqueuelen 1000  (Ethernet)
RX packets 2134  bytes 564336 (564.3 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 61  bytes 15582 (15.5 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0enp3s0f1s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet6 fe80::73:e3ff:fe80:e0b  prefixlen 64  scopeid 0x20
ether 02:73:e3:80:0e:0b  txqueuelen 1000  (Ethernet)
RX packets 2132  bytes 563652 (563.6 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 63  bytes 15762 (15.7 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
inet 127.0.0.1  netmask 255.0.0.0
inet6 ::1  prefixlen 128  scopeid 0x10
loop  txqueuelen 1000  (Local Loopback)
RX packets 2457  bytes 194358 (194.3 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 2457  bytes 194358 (194.3 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0oob_net0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
ether b8:ce:f6:a8:83:04  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0p0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet6 fe80::bace:f6ff:fea8:82fe  prefixlen 64  scopeid 0x20
ether b8:ce:f6:a8:82:fe  txqueuelen 1000  (Ethernet)
RX packets 6140  bytes 932569 (932.5 KB)
RX errors 0  dropped 4  overruns 0  frame 0
TX packets 171  bytes 24267 (24.2 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0p1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet6 fe80::bace:f6ff:fea8:82ff  prefixlen 64  scopeid 0x20
ether b8:ce:f6:a8:82:ff  txqueuelen 1000  (Ethernet)
RX packets 6136  bytes 932141 (932.1 KB)
RX errors 0  dropped 1  overruns 0  frame 0
TX packets 173  bytes 24475 (24.4 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0pf0hpf: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet6 fe80::e408:bff:fed4:3ce4  prefixlen 64  scopeid 0x20
ether e6:08:0b:d4:3c:e4  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 4641  bytes 733296 (733.2 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0pf1hpf: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet6 fe80::2c61:10ff:fef5:5fd0  prefixlen 64  scopeid 0x20
ether 2e:61:10:f5:5f:d0  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 4640  bytes 733232 (733.2 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0tmfifo_net0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 192.168.100.2  netmask 255.255.255.252  broadcast 192.168.100.3
inet6 fe80::21a:caff:feff:ff01  prefixlen 64  scopeid 0x20
ether 00:1a:ca:ff:ff:01  txqueuelen 1000  (Ethernet)
RX packets 8891  bytes 726002 (726.0 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 7704  bytes 857035 (857.0 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0terface on the DPU and assign and IP address andHere is the ovs bridges:
ovs-vsctl show
31d2a29e-d043-4f8b-94d0-5dd4d47198d7
Bridge ovsbr2
Port p1
Interface p1
Port pf1hpf
Interface pf1hpf
Port ovsbr2
Interface ovsbr2
type: internal
Port en3f1pf1sf0
Interface en3f1pf1sf0
Bridge ovsbr1
Port p0
Interface p0
Port ovsbr1
Interface ovsbr1
type: internal
Port en3f0pf0sf0
Interface en3f0pf0sf0
Port pf0hpf
Interface pf0hpf
ovs_version: “2.14.1”Powered by Discourse, best viewed with JavaScript enabled"
144,performance-interference-of-two-regexes-when-running-regex-accelerators-on-bluefield-2,"Hi,I am exploring the feasibility of running two regex workloads on the same regex accelerator. The testing program I used is RXPBench from the official site.The results show that there is an interference between the  MATCH TO BYTE RATIO of two regexes. For example, regex1’s MATCH TO BYTE RATIO is 0 when it runs alone; when I run another regex regex2 whose MATCH TO BYTE RATIO is 30.54, regex1’s MATCH TO BYTE RATIO also becomes ~30.The results make me guess that the regex accelerator doesn’t differentiate the rule sets from multiple applications, in other words, it’s stateless. Everytime the regex accelerator will search all the rule sets in its hardware rule buffer despite that the buffer stores the rules of multiple applications.Of course, that’s just a guess, could you please help to explain the root reason for this? Or does the regex accelerator have the ability to support multiple applications without any interference?Best regards. :)Powered by Discourse, best viewed with JavaScript enabled"
145,mlnx-ofed-not-getting-installed-on-ubuntu-20-04-with-latest-kernel,"I’ve Ubuntu 20.04 installed with 5.4.0-109-generic kernel . OFED gets installed on it successfully.
Once i upgrade to latest kernel or use Ubuntu 20.04.4, OFED doesn’t get installed on it… any ideas?Hello susingh,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Based on the kernel version, we recommend to us the latest MLNX_OFED 5.6 GA version and see if the driver installs without any issues.If you still are experiencing issue, we recommend to open a NVIDIA Networking Support ticket (valid support contract needed) so we can triage this through a support ticket.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
146,sn2010-firmware-when-moving-from-onyx-to-cumulus,"After reading through some specific docs about migrating a switch with Onyx pre-installed to Cumulus, it mentioned that we would need to likely update the firmware because the pre-installed firmware could be allowing only NVIDIA (Mellanox) Transceivers and that by updating the firmware would unlock this.I’ve tried to use the download tool online but after entering the PSID it states it can’t find the PSID.  I am at a loss.  I have switches setup for testing this NoS migration and I can get links configured and UP but they’re not passing anything no matter what I try and when I ran into that small blurb I have come to suspect that the issue is that the firmware needs to be updated because I am NOT using Mellanox transceiversHello, if you need to change from the onyx system to the cumulus system, you need to find our partner for quotation, and then purchase the licenseClosing the loop on this.The SN2010 doesn’t use firmware for modules, nothing need to be changed to migrate to Cumulus from ONYX.Also didn’t need Mellanox transceivers, what I discovered is that these 10/25 ports can operate at 1G, but only if the SFP handles the transition.  I could not get it operating at 1G with a 1G transceiver.   I did have to install a 1/10 transceiver and then allow it handle transition to 1G.  In our broadcom switches we have a lot of 1G connections coming from 10G ports and the transceivers are 10/100/1000 so I just assumed that because the Spectrum switches have a 10/25 that it COULD also do 1/10/25 but that’s simply not the case.Hello,Do you have any questions that need help?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
147,behavior-of-mellanox-nic,"We generated 8 virtual functions for each PF (MELLANOX NIC). We used those VFs to deploy the virtual machine. We are unable to shut off the VM ports by shutting down the physical link.Hi,Please follow the suggestions below:There is a firmware feature called keep link up, that means the physical link stays up even if you put the interface down.  Please make sure the mlxconfig “KEEP_ETH_LINK_UP_P1” or “KEEP_IB_LINK_UP_P1” as expected.Use the “# mlxconfig -d xx:xx.x set KEEP_ETH_LINK_UP_P1=0” to configure, and use “mlxfwreset -d xx:xx.x r -y” to reset the FW and make it work.If you want to test the PF port down and check VF state, please don’t use the ip link down or ifdown or ifconfig down on PF port.  you should disable/shutdown the correpsonding port on the switch and check.Use the “ip link show” command to see if the vf port link-state is “auto”.https://man7.org/linux/man-pages/man8/ip-link.8.htmlHi,Thanks for the response. We have set vf link-state auto, but still we are observing the issue.mlxconfig is not working on compute node. We tried to install by downloading the pkg from mellanox official site but no luckCould you please help us to proceed further?Thanks,
SusmithaPlease download and install the mft 4.23.0 version onto your system, then you can use the mlxconfig to modify the fw config.The Mellanox Firmware Tools (MFT) package is a set of firmware management toolsPowered by Discourse, best viewed with JavaScript enabled"
148,doca-sdk-libaries-and-drivers-depends-linunx-headers-bluefield-5-4-0-1042-41-but-5-4-0-1044-43-is-to-be-installled,"Hi, I want to install DOCA(1.4) by sdkmanager on bluefield-2.
When I install DOCA  SDK 1.4 version, installation failed with attached message.
DOCA SDK Libaries and Drivers : Depends : linunx-headers-bluefield(=5.4.0.1042.41) but 5.4.0.1044.43 is to be installledSDKM_logs_2022-08-30_11-22-52.zip (241.9 KB)Powered by Discourse, best viewed with JavaScript enabled"
149,any-mellanox-1g-baset-switch-series,"Does mellanox still have the SN2201 (48 x 1G BaseT) switch in its active portafolio? If not, is there any Mellanox BaseT switch series currently alive?Thank you!We wind up just using SFP to BaseT converter modules.   Obviously adding a point of failure but then again most of our connections are bonded.Powered by Discourse, best viewed with JavaScript enabled"
150,sx6012-igmp-snooping-warning,"Once I enable igmp snooping，the following message will show up in the log and repeat every two minutesissd[5045]: TID 1423378496: [issd.WARNING]: NPAPI_WRN: warning RxMgrLowHandleMain Failed Processing RxGddProcessRecvInterruptEventThis message means that the switch recieved an igmp message that it can’t process for some reason.Further troubleshooting should be done with opening a case to the Nvidia technical support team at.Nvidia Support Admin networking-support@nvidia.comPowered by Discourse, best viewed with JavaScript enabled"
151,cx-4-ethernet-cx416a-rhel7-inbox-cannot-get-roce-to-initialize-anyone-have-a-good-how-to,"I have a CentOS 7.6 environment with CX4 Ethernet only card. Using MLNX 4.6-1.0.1 card drivers and inbox rdma.The CX4 is functioning fine in Ethernet mode, has an ipv4 address, passes traffic, no problem.I set “options roce_mode=2” in /etc/modprobe.d/mlx4.conf and verified after module loading by catting /sys/module/mlx4_core/parameters/roce_mode that indicates “2”.When I start the inbox rdma services, the other non-Mellanox interfaces load their RoCE modules and appear functional in ibstatus. The Mellanox interface does not.I can see that mlx4_code, mlx5_core, mlx4_ib and mlx5_ib modules are loaded but the _ib modules are not bound to any interfaces.If I try to create their instances in configfs it fails.
mkdir /sys/kernel/config/rdma_cm/mlx5_0/
mkdir: cannot create directory ‘/sys/kernel/config/rdma_cm/mlx5_0/’: No such devicemkdir /sys/kernel/config/rdma_cm/mlx4_0
mkdir: cannot create directory ‘/sys/kernel/config/rdma_cm/mlx4_0’: No such deviceThe two onboard Intel i40e interfaces immediately initialize fine, load i40iw kernel module and appear as RDMA interfaces in ibstatus when I start inbox RDMA services so I know the RDMA services environment is functional. For some reason the CX416A will not come up as RDMA. Is there anything specific that has to be set in the CX416A with mstconfig? Is there a NUM_OF_VFS requirement or other card NVRAM setting required?Does anyone have a how-to or know the magic incantation to get a CX416A to function in RoCE mode?ThanksCard NVRAM:/etc/modprobe.d/Hello @jeff.johnson,Thank you for posting your query on our community. Please note that we provide support for MLNX_OFED drivers. If using inbox drivers, you will need to reach out to the OS vendor for further assistance.ConnectX-4 uses mlx5 driver. Our mlx5 driver for ConnectX-4/5 will support RoCEv2 and RoCEv1. By default when using RDMA_CM in conjunction with our mlx5 driver for ConnectX-4/5 it uses RoCEv2.
To change the default RoCE mode for RDMA_CM, you will need to use the cma_roce_mode command.For ex:
To check the default RoCE mode,IB/RoCE V1To set the default RoCE mode, use -m 2 parameter.RoCE V2Hope this answers your question.Regards,
BhargaviPowered by Discourse, best viewed with JavaScript enabled"
152,a-issue-when-compiling-dpdk-on-dpu,"Hi, there was a issue when I  was compiling DPDK 21.11.1.It’s shown  below.

屏幕截图 2022-07-26 2151192332×620 30.7 KB

I have tried this operation on three kinds of devices,like arm on bf2 dpu,x86 host with dpu ,x86 host with connect x6 dx, and I met the same problem. So I had to update the  meson file on the path drivers/regex/octeontx2/meson.build .I know this may lead the rxp_compiler not included,but I compiled dpdk successfully.And I wonder why this problem happen when using mlx devices.Powered by Discourse, best viewed with JavaScript enabled"
153,link-problem-between-sfp28-and-sfp,"Computer: Windows 11 22H2 with March 2023 updates (22621.1413)
Product: Connect-X 5
Driver: 3.20.25915.0
Firmware: 16.35.1012Cannot SFP28 10G/25G get working with SFP+ 10G in Cisco switch.Setting “mlxlink -d mt4119_pciconf0” --link_mode_force -speeds 10G ends up with “Cable speed not enabled”.
With no speed specified (mlx5cmd -LinkSpeed -Name “blabla” -Set 0) it ends up with “Negotiation Failure”.
Cable is OM4, length is 30 meters.Using Cisco’s SFP+ 10G in Mellanox ConnectX-5 card establishes link and network connection is functional.Hi marian.hercek,Thank you for posting your question on this community thread.Please verify if the cable and SFP connectors you are using are listed under the Firmware Compatible Products list for ConnectX-5 firmware version 16.35.1012 release notes documentation.
https://docs.nvidia.com/networking/display/ConnectX5Firmwarev16351012/Firmware+Compatible+ProductsBest regards,Nvidia supportPowered by Discourse, best viewed with JavaScript enabled"
154,how-to-configure-tos-in-rss-rule-using-ethtool,"Hi, I am trying to steer ingress traffic according to tos value using ethtool but I got the following error. If anyone has any ideas, could you please share with me? Thanks!sudo ethtool --config-ntuple ens1f1np1 flow-type udp4 tos 0 action 2
rmgr: Cannot insert RX class rule: Invalid argumentother info:
OS: Ubuntu 22.04.1 LTS
Kernel:  5.16.0-051600rc5-generic
NIC: Two Dual-port Mellanox ConnectX-4 25 GB NIC (PCIe v3.0, 8 lanes
driver: mlx5_core
version: 5.16.0-051600rc5-generic
firmware-version: 14.18.2030 (HP_2420110034)
expansion-rom-version:
bus-info: 0000:03:00.1
supports-statistics: yes
supports-test: yes
supports-eeprom-access: no
supports-register-dump: no
supports-priv-flags: yesPowered by Discourse, best viewed with JavaScript enabled"
155,hwmon-monitoring-for-mlx3-and-mlx4,"Hi,some of the NICs I use (most notably Solarflare NICs), do have this awesome feature,that expose temperatures and voltages, using hwmon Linux infrastructure.It makes it very easy to monitor some vital things of the NIC using sensors program.I wish the Mellanox NICs had the same feature. The reason beingthey often can run hot, and have only passive cooling, andrequire airflow from the computer case. But without monitoring,it is hard to determine if that is the case.I am aware that there are tools, that can view the temperature.The problem is they are not packaged in any distro,and cannot be easily hooked to existing monitoring systems(collectd, prometheus node_exporter), that already just usehwmon automatically.Hello Baryluk,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately, the only value we expose is the ASIC temperature, which you can read through the ‘mget_temp’ tool (provided by Mellanox Firmware Tools → Mellanox Firmware Tools (MFT)). In any other case, the adapter f/w will print temperature or voltage related in the system messages file when exceed the threshold.Thank you and regards,~NVIDIA Networking Technical SupportI know all this. I even said so in my original post.Please expose temperatures and voltages using hwmon, like everybody else.Powered by Discourse, best viewed with JavaScript enabled"
156,bluefield-2-doca-flow-sample-wont-run-on-dpu,"Just getting started with BlueField-2 & DOCA, trying to run one of the sample apps just to “see it work” and get a baseline before really digging into things.The sample built fine but won’t run.  The sample is “flow_hairpin”.Perhaps someone can point me in the right direction?Here are the details and error message:The card has been setup thusly:root@localhost:/opt/mellanox/doca/samples/doca_flow/flow_hairpin# mlxconfig -d 0000:03:00.0 s PF_BAR2_ENABLE=0 PER_PF_NUM_SF=1 PF_TOTAL_SF=236Device type:    BlueField2
Name:           MBF2H332A-AEEO_Ax_Bx
Description:    BlueField-2 P-Series DPU 25GbE Dual-Port SFP56; PCIe Gen4 x8; Crypto Enabled; 16GB on-board DDR; 1GbE OOB management; HHHL
Device:         0000:03:00.0Configurations:                                      Next Boot       New
PF_BAR2_ENABLE                              False(0)        False(0)
PER_PF_NUM_SF                               True(1)         True(1)
PF_TOTAL_SFThe sample is being run with this command line:./build/doca_flow_hairpin -a auxiliary:mlx5_core.sf.2,dv_flow_en=2 -a auxiliary:mlx5_core.sf.3,dv_flow_en=2  – -l 60The error message that appears to be at the root of the issue is:“mlx5_net: [mlx5dr_action_create_generic]: Cannot create HWS action since HWS is not supported”And the full run output is:root@localhost:/opt/mellanox/doca/samples/doca_flow/flow_hairpin# ./build/doca_flow_hairpin -a auxiliary:mlx5_core.sf.2,dv_flow_en=2 -a auxiliary:mlx5_core.sf.3,dv_flow_en=2  – -l 60
EAL: Detected 8 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL: No legacy callbacks, legacy socket not created
[00:54:05:578715][DOCA][DBG][NUTILS:507]: Port 0 MAC: 02 7a db 5b 5e 7c
[00:54:05:651509][DOCA][DBG][NUTILS:507]: Port 1 MAC: 02 ab 52 2b 30 a9
[00:54:05:653878][DOCA][INF][engine_model:73]: engine model defined with mode=vnf
[00:54:05:653909][DOCA][INF][engine_model:75]: engine model defined with nr_pipe_queues=8
[00:54:05:653929][DOCA][INF][engine_model:76]: engine model defined with pipe_queue_depth=0
[00:54:05:654180][DOCA][INF][engine_field_mapping:96]: Engine field mapping initialized with 3 focus 12 protocols
[00:54:05:654215][DOCA][INF][engine_shared_resources:94]: Engine shared resources initialized successfully
[00:54:05:654250][DOCA][INF][dpdk_engine:437]: queue depth is zero, set it to default 128.
[00:54:05:654307][DOCA][INF][encap_table:119]: encap table created
[00:54:05:654417][DOCA][DBG][dpdk_table_hws:870]: Initialized dpdk table work module to be HW steering
[00:54:05:654443][DOCA][INF][dpdk_table:70]: Initializing dpdk table successfully
[00:54:05:654463][DOCA][DBG][dpdk_flow_hws:33]: Initialized dpdk flow work module to be HW steering
[00:54:05:654487][DOCA][INF][dpdk_flow:82]: Initializing dpdk flow successfully
[00:54:05:654513][DOCA][INF][engine_shared_resources:133]: Allocated 16 shared resources of type 2
[00:54:05:654533][DOCA][INF][dpdk_resource_manager:184]: Dpdk resource manager register completed
[00:54:05:654585][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.eth.dst_mac, offset=0)
[00:54:05:654614][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.eth.src_mac, offset=6)
[00:54:05:654634][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.eth.type, offset=12)
[00:54:05:654659][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.eth.dst_mac, offset=0)
[00:54:05:654680][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.eth.src_mac, offset=6)
[00:54:05:654700][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.eth.type, offset=12)
[00:54:05:654720][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.eth_vlan.tci, offset=0)
[00:54:05:654745][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.eth_vlan.tci, offset=0)
[00:54:05:654766][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv4.src_ip, offset=12)
[00:54:05:654782][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv4.dst_ip, offset=16)
[00:54:05:654804][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv4.next_proto, offset=9)
[00:54:05:654824][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv4.src_ip, offset=12)
[00:54:05:654849][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv4.dst_ip, offset=16)
[00:54:05:654871][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv4.next_proto, offset=9)
[00:54:05:654891][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv6.src_ip, offset=8)
[00:54:05:654910][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv6.dst_ip, offset=24)
[00:54:05:654935][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv6.next_proto, offset=6)
[00:54:05:654955][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv6.src_ip, offset=8)
[00:54:05:654975][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv6.dst_ip, offset=24)
[00:54:05:654999][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv6.next_proto, offset=6)
[00:54:05:655020][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.udp.src_port, offset=0)
[00:54:05:655040][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.udp.dst_port, offset=2)
[00:54:05:655059][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.udp.src_port, offset=0)
[00:54:05:655083][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.udp.dst_port, offset=2)
[00:54:05:655109][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.tcp.src_port, offset=0)
[00:54:05:655129][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.tcp.dst_port, offset=2)
[00:54:05:655154][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.tcp.flags, offset=13)
[00:54:05:655173][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.tcp.src_port, offset=0)
[00:54:05:655193][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.tcp.dst_port, offset=2)
[00:54:05:655218][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.tcp.flags, offset=13)
[00:54:05:655239][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.tunnel.vxlan.vni, offset=4)
[00:54:05:655263][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.tunnel.gre.key, offset=0)
[00:54:05:655288][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.tunnel.gtp.teid, offset=4)
[00:54:05:655307][DOCA][INF][dpdk_layer:260]: Dpdk layer register completed
[00:54:05:655331][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.eth.dst_mac, offset=42, len=6)
[00:54:05:655356][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.eth.src_mac, offset=36, len=6)
[00:54:05:655381][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.eth.type, offset=48, len=2)
[00:54:05:655403][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.eth.dst_mac, offset=150, len=6)
[00:54:05:655423][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.eth.src_mac, offset=144, len=6)
[00:54:05:655448][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.eth.type, offset=156, len=2)
[00:54:05:655470][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.eth_vlan.tci, offset=50, len=2)
[00:54:05:655491][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.eth_vlan.tci, offset=158, len=2)
[00:54:05:655515][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv4.src_ip, offset=56, len=4)
[00:54:05:655536][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv4.dst_ip, offset=76, len=4)
[00:54:05:655555][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv4.next_proto, offset=92, len=1)
[00:54:05:655576][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv4.src_ip, offset=164, len=4)
[00:54:05:655600][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv4.dst_ip, offset=184, len=4)
[00:54:05:655621][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv4.next_proto, offset=200, len=1)
[00:54:05:655645][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv6.src_ip, offset=56, len=16)
[00:54:05:655667][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv6.dst_ip, offset=76, len=16)
[00:54:05:655689][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv6.next_proto, offset=92, len=1)
[00:54:05:655713][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv6.src_ip, offset=164, len=16)
[00:54:05:655734][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv6.dst_ip, offset=184, len=16)
[00:54:05:655755][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv6.next_proto, offset=200, len=1)
[00:54:05:655779][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.udp.src_port, offset=94, len=2)
[00:54:05:655800][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.udp.dst_port, offset=96, len=2)
[00:54:05:655820][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.udp.src_port, offset=202, len=2)
[00:54:05:655846][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.udp.dst_port, offset=204, len=2)
[00:54:05:655867][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.tcp.src_port, offset=94, len=2)
[00:54:05:655887][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.tcp.dst_port, offset=96, len=2)
[00:54:05:655911][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.tcp.flags, offset=93, len=1)
[00:54:05:655934][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.tcp.src_port, offset=202, len=2)
[00:54:05:655954][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.tcp.dst_port, offset=204, len=2)
[00:54:05:655978][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.tcp.flags, offset=201, len=1)
[00:54:05:655998][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.vxlan.vni, offset=104, len=3)
[00:54:05:656018][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.gre.key, offset=108, len=4)
[00:54:05:656038][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.gre.protocol, offset=106, len=2)
[00:54:05:656063][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.gtp.teid, offset=104, len=4)
[00:54:05:656083][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.nisp.hdr, offset=104, len=40)
[00:54:05:656108][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.audp.hdr, offset=104, len=24)
[00:54:05:656128][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.esp.spi, offset=104, len=4)
[00:54:05:656149][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.esp.sn, offset=108, len=4)
[00:54:05:656213][DOCA][INF][doca_flow_layer:466]: Doca flow layer initialized
[00:54:05:656234][DOCA][INF][doca_flow:526]: Doca flow initialized successfully
[00:54:05:657054][DOCA][INF][utils_hash_table:123]: hash table a_tmplt_t port 0 created
[00:54:05:657130][DOCA][INF][utils_hash_table:123]: hash table p_tmplt_t port 0 created
[00:54:05:657208][DOCA][INF][utils_hash_table:123]: hash table dpdk_tbl_mgr port 0 created
[00:54:05:657335][DOCA][INF][utils_hash_table:123]: hash table grp_fwd port 0 created
[00:54:05:657358][DOCA][INF][dpdk_port:167]: Dpdk port 0 initialized successfully with 9 queues
mlx5_net: [mlx5dr_action_create_generic]: Cannot create HWS action since HWS is not supported
[00:54:05:714997][DOCA][ERR][dpdk_flow_hws_legacy:143]: failed to configure flow hws port 0 - rte flow configure, type 1 message: fail to configure port
[00:54:05:715066][DOCA][ERR][dpdk_engine:1694]: failed to start port 0 - init port, ret=-1
[00:54:05:715146][DOCA][INF][utils_hash_table:151]: hash table destroyed
[00:54:05:715581][DOCA][INF][utils_hash_table:151]: hash table destroyed
[00:54:05:715639][DOCA][INF][utils_hash_table:151]: hash table destroyed
[00:54:05:715673][DOCA][INF][utils_hash_table:151]: hash table destroyed
[00:54:05:715692][DOCA][INF][dpdk_port:230]: Dpdk port 0 destroyed successfully with 9 queues
[00:54:05:715723][DOCA][ERR][flow_common:82]: Failed to start port - dpdk port start failed (0)
[00:54:05:715747][DOCA][ERR][FLOW_HAIRPIN:139]: Failed to init DOCA ports
[00:54:05:715774][DOCA][INF][doca_flow_layer:478]: Doca flow layer destroyed
[00:54:05:715794][DOCA][INF][dpdk_resource_manager:191]: Dpdk resource manager unregister completed
[00:54:05:715814][DOCA][INF][dpdk_flow:205]: Cleanup dpdk flow
[00:54:05:715833][DOCA][DBG][dpdk_flow_hws:69]: Cleanup dpdk flow HW steering module
[00:54:05:715852][DOCA][INF][dpdk_table:77]: Cleanup dpdk table
[00:54:05:715871][DOCA][DBG][dpdk_table_hws:877]: Cleanup dpdk table HW steering module
[00:54:05:715890][DOCA][INF][dpdk_layer:272]: Dpdk layer unregister completed
[00:54:05:715917][DOCA][INF][dpdk_resource_manager:191]: Dpdk resource manager unregister completed
[00:54:05:715936][DOCA][INF][dpdk_flow:205]: Cleanup dpdk flow
[00:54:05:715954][DOCA][DBG][dpdk_flow_hws:69]: Cleanup dpdk flow HW steering module
[00:54:05:715972][DOCA][INF][dpdk_table:77]: Cleanup dpdk table
[00:54:05:715990][DOCA][DBG][dpdk_table_hws:877]: Cleanup dpdk table HW steering module
[00:54:05:716011][DOCA][INF][dpdk_layer:272]: Dpdk layer unregister completed
[00:54:05:716037][DOCA][INF][encap_table:136]: encap table destroyed
[00:54:05:716060][DOCA][INF][engine_shared_resources:243]: Cleanup 16 shared resources of type 2 completed
[00:54:05:716082][DOCA][INF][engine_field_mapping:104]: Engine field mapping destroyed
[00:54:05:716105][DOCA][INF][engine_model:150]: engine model destroyed
[00:54:05:716125][DOCA][INF][doca_flow:542]: Doca flow destroyed
[00:54:05:716144][DOCA][ERR][FLOW_HAIRPIN::MAIN:72]: flow_hairpin sample encountered errors
Tx port 0 is already stopped
[00:54:05:716219][DOCA][ERR][NUTILS:104]: Failed to bind hairpin queues (-16)
[00:54:05:716242][DOCA][ERR][NUTILS:191]: Disabling hairpin queues failed: err=21, port=0
Tx port 0 is already stopped
[00:54:05:716294][DOCA][ERR][NUTILS:117]: Failed to bind hairpin queues (-16)
[00:54:05:716316][DOCA][ERR][NUTILS:191]: Disabling hairpin queues failed: err=21, port=1
Device with port_id=0 already stopped
Segmentation fault (core dumped)
root@localhost:/opt/mellanox/doca/samples/doca_flow/flow_hairpin#Hopefully I am just missing something simple…!Thanks!
-JHi,I’ve got the same issue, I just updated a firmware using mlxofedinstall and It helps to me. Can you share firmware version?Hi Yavtuk -Thanks for the reply.  I believe our firmware is current.  Here are the version details…root@localhost:/opt/mellanox/doca/samples/doca_flow/flow_hairpin# flint -d  /dev/mst/mt41686_pciconf0 query
Image type:            FS4
FW Version:            24.37.1300
FW Version(Running):   24.35.2000
FW Release Date:       11.5.2023
Product Version:       24.35.2000
Rom Info:              type=UEFI Virtio net version=21.4.10 cpu=AMD64,AARCH64
type=UEFI Virtio blk version=22.4.10 cpu=AMD64,AARCH64
type=UEFI version=14.28.16 cpu=AMD64,AARCH64
type=PXE version=3.6.805 cpu=AMD64
Description:           UID                GuidsNumber
Base GUID:             b8cef60300677860        14
Base MAC:              b8cef6677860            14
Image VSD:             N/A
Device VSD:            N/A
PSID:                  MT_0000000540
Security Attributes:   N/A
root@localhost:/opt/mellanox/doca/samples/doca_flow/flow_hairpin#-JHi @IamAries, I check the sample with your options and It works on my environment.let me describe how I setup env step by step:Also, you should perform power cycle becauseFW Version: 24.37.1300
FW Version(Running): 24.35.2000Hey Yavtuk -Thank you for taking the time to run the sample and send your logs.  It turns out issue was I did not specify the proper sub-function in the command line arg.  Your log showed it right away.  I now have things working properly.Thanks again
-JHey Yavtuk -Well, seems I spoke too soon.  If you don’t mind, I could use a little more assistance.  I can now get the samples to run without errors but I just cannot see it actually work!The test scenario I have is running a ping on the network segment connected to port 1.  I run either the flow_hairpin or flow_drop samples which run with no error but do not seem to have any effect.Running tcpdump on the host machine on my ens2f1 port I can see the ping traffic coming in.  Looking at the ovs switch flows I can see the packet counters increasing.  It would seem the DPU is passing the traffic through ovs and up to the host – ok.   I then run the flow_drop sample application.  I would expect, at least, the traffic going up to the host to stop as presumably the flow_drop app should be passing all traffic now to the second port on the DPU (and not up to the host).   This does not happen.Looking at the code for this sample it appears that its intent is to pass all traffic between the ports except for the traffic that it attempts to block (appears to be traffic destined to 8.8.8.8 on port 80).  Not seeing traffic emanating from my port 0, and at the same time seeing the traffic continue to appear at host would seem to suggest something isn’t working.I also note that my starting condition does not appear to matching yours.  Your log file output seems to snow no sf’s existing before you added two for use by the sample (your output of “/opt/mellanox/iproute2/sbin/mlxdevm port show” was blank).  On my system there are already two existing sf’s that I did not create.  I am not sure what accounts for this difference.  I just did a reinstall of the SDK & re-flash of the card to attempt to clear out anything that may have been pre-existing somehow but result is still the same.Here is what my system looks like:/opt/mellanox/iproute2/sbin/mlxdevm port show
pci/0000:03:00.0/229376: type eth netdev en3f0pf0sf0 flavour pcisf controller 0 pfnum 0 sfnum 0
function:
hw_addr 02:f4:65:74:e3:69 state active opstate attached roce true max_uc_macs 128 trust off
pci/0000:03:00.0/229377: type eth netdev en3f0pf0sf4 flavour pcisf controller 0 pfnum 0 sfnum 4
function:
hw_addr 00:00:00:00:10:00 state active opstate attached roce true max_uc_macs 128 trust on
pci/0000:03:00.1/294912: type eth netdev en3f1pf1sf0 flavour pcisf controller 0 pfnum 1 sfnum 0
function:
hw_addr 02:f5:45:34:a0:38 state active opstate attached roce true max_uc_macs 128 trust off
pci/0000:03:00.1/294913: type eth netdev en3f1pf1sf5 flavour pcisf controller 0 pfnum 1 sfnum 5
function:
hw_addr 00:00:00:00:20:00 state active opstate attached roce true max_uc_macs 128 trust on(sf4 and sf5 are the ones I added.  the two sf0’s are the ones that were already existing)And my command line to run the flow_drop example:./build/doca_flow_drop -a auxiliary:mlx5_core.sf.5,dv_flow_en=2 -a auxiliary:mlx5_core.sf.4,dv_flow_en=2 – -l 60And to run the hairpin:./build/doca_flow_hairpin -a auxiliary:mlx5_core.sf.5,dv_flow_en=2 -a auxiliary:mlx5_core.sf.4,dv_flow_en=2 – -l 60Might you have any thoughts that might help me here?Thanks again for your time
-J
config.log (2.7 KB)
flow_drop.log (25.9 KB)Hi @IamAries, I don’t have much experience with openvswitch.
It’s hard to say but I can try to find time to check it locally.Can you share me the rules for ovs bridges?sudo ovs-ofctl dump-flows ovsbr1
sudo ovs-ofctl dump-flows ovsbr2BTW, you can delete SFs manually
/opt/mellanox/iproute2/sbin/mlxdevm port del pci/0000:03:00.0/xxxxxxbased on config log, it seems wrong ovs configurationb8a82ad1-cabf-4a6f-829b-2b1c05c5e06b
Port en3f1pf1sf0
Interface en3f1pf1sf0this ports aren’t trusted in your setuppci/0000:03:00.0/229376: type eth netdev en3f0pf0sf0 flavour pcisf controller 0 pfnum 0 sfnum 0
function:
hw_addr 02:f4:65:74:e3:69 state active opstate attached roce true max_uc_macs 128 trust offHere are the OVS flows:root@localhost:/opt/mellanox/doca/samples/doca_flow/flow_drop# ovs-ofctl dump-flows ovsbr1
cookie=0x0, duration=22826.342s, table=0, n_packets=380, n_bytes=118022, priority=0 actions=NORMAL
root@localhost:/opt/mellanox/doca/samples/doca_flow/flow_drop# ovs-ofctl dump-flows ovsbr2
cookie=0x0, duration=22829.571s, table=0, n_packets=15970, n_bytes=1427430, priority=0 actions=NORMAL
root@localhost:/opt/mellanox/doca/samples/doca_flow/flow_drop#My assumption had been, though, that if traffic was being hairpin in the DPU that it would never come up to OVS because this is offloaded, is it not so?The untrusted SF’s are the first two that were not created by me.  I did not specify either of these in my CLI args so presumably the sample is not attempting to use these.I will try to remove them shortly and see if it makes any difference.If you do find time to try it on yours I would most appreciate it.  Again, thank you for taking the time to help me get started here; I really appreciate it.-JI add some rules to openvswitch for ingress traffic:ingress packet  → p0 → ovsbr1 → en3f0pf0sf2 → app → en3f0pf0sf1 → ovsbr2 → pf0hpf → host
sudo ovs-ofctl add-flow ovsbr1 in_port=p0,actions=output: en3f0pf0sf2
sudo ovs-ofctl add-flow ovsbr2 in_port= en3f0pf0sf1,actions=output: pf0hpfI can see the packets inside my app. You can try to add the same rules for P1This appear to maybe be where my issue was.Setting up rules similar to yours in ovs now seems to allow the examples to pass traffic in a way that I would expect.What is curious is that ovs does not show the flows as hardware offloaded.  Should it?  Or is it implied that they are offloaded because we reference the representers?Here are the output of dump-flow on my ovs – traffic is passing successfully, but how to verify that this traffic is actually staying in the chip, not coming into the ARM cores?(this is a run of the flow_drop example… so all traffic should either drop in hardware, or pass to other physical port in hardware… nothing up to the ARM cores)root@localhost:/home/ubuntu# ovs-appctl dpctl/dump-flows
recirc_id(0),in_port(5),eth(src=e8:eb:d3:8c:08:6a,dst=00:00:00:00:fe:00),eth_type(0x0800),ipv4(frag=no), packets:1518, bytes:127512, used:0.010s, actions:6
recirc_id(0),in_port(4),eth(src=02:f5:45:34:a0:38,dst=ff:ff:ff:ff:ff:ff),eth_type(0x0800),ipv4(frag=no), packets:0, bytes:0, used:9.440s, actions:2,3
recirc_id(0),in_port(4),eth(src=e8:eb:d3:8c:08:6a,dst=00:00:00:00:fe:00),eth_type(0x0800),ipv4(frag=no), packets:28, bytes:2352, used:0.010s, actions:2,3
root@localhost:/home/ubuntu#
root@localhost:/home/ubuntu# ovs-ofctl dump-flows  br0
cookie=0x0, duration=164.480s, table=0, n_packets=1576, n_bytes=132398, in_port=p0 actions=output:en3f1pf1sf4
cookie=0x0, duration=1704.893s, table=0, n_packets=4251, n_bytes=356274, priority=0 actions=NORMAL
root@localhost:/home/ubuntu# ovs-ofctl dump-flows  br1
cookie=0x0, duration=1460.190s, table=0, n_packets=9106, n_bytes=928786, in_port=p1 actions=output:en3f1pf1sf0
cookie=0x0, duration=1705.415s, table=0, n_packets=1423, n_bytes=126236, priority=0 actions=NORMAL
root@localhost:/home/ubuntu#-J""What is curious is that ovs does not show the flows as hardware offloaded. Should it? Or is it implied that they are offloaded because we reference the representers?Here are the output of dump-flow on my ovs – traffic is passing successfully, but how to verify that this traffic is actually staying in the chip, not coming into the ARM cores?""I am thinking about the same issue and I am going to investigate it. let me know if you find something about itOkay, yes I will do so!  Thanks for all of your help; it sure sped up my getting things going!-JYou are very welcome 🙂This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
157,is5030-and-sx6036-questions-about-licenses,"Hi, after I had a “bad sale” with a Mellanox IS5030 I learned that I need one with a license for the “subnet manager” and Fabric IT and I must ask the seller if there is one… can pls someone explain which funcitnality is covered by the license calledLK2_EFM_Config_ ?I got response from two companies who sell used IS5030 with that license. The IS5030 obviously was sold without license before 2012, and most models built in 2012 or after do have a sticker on the pull out strip.And does the SX6036 provide management functionality out of the box (and 36 ports and 40 GBe) or do I also have to take care for licensed / unlicensed features? As I heard the “ethernet gateway” is in fact a separate license… what is this license exactly doing? Routing Ethernet to the 1 GBit RJ45 interface in case a SFP+ connect with 10 GBE to a native ethernet card is used? There are some split cables they split 1x 40 Gbit to 4x10 GBeHello Andreas,Thank you for posting your inquiry on the NVIDIA Networking Community.Please see the following link regarding to the LK2_EFM FabricIT manager → https://www.mellanox.com/pdf/prod_ib_switch_systems/pb_FabricIT_EFM.pdfFor the SX6036 switch, even though it is a VPI switch, for L2-L3 Ethernet functionality, you need to purchase the Gateway license for this functionality. You can purchase the license through our webstore → https://store.mellanox.com/products/nvidia-upgr-6036-gw-l2-l3-ethernet-gateway-upgrade-for-6036-series-switch.htmlThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
158,doca-examples-url-filter,"Hi all,
I’m new to DOCA SDK. I’m trying to run the example applications but I’m getting the following error, can anyone help?
$ sudo /opt/mellanox/doca/examples/url_filter/bin/doca_url_filter -a 0000:03:00.0,class=regex -a auxiliary:mlx5_core.sf.2,sft_en=1 -a auxiliary:mlx5_core.sf.4,sft_en=1 -c3 – -p -l 3
EAL: Detected 8 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: No available hugepages reported in hugepages-32768kB
EAL: No available hugepages reported in hugepages-64kB
EAL: No available hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL:   Device is not NUMA-aware, defaulting socket to 0
EAL: Probe PCI driver: mlx5_pci (15b3:a2d6) device: 0000:03:00.0 (socket 0)
mlx5_net: dr_create_flow_tbl failed
mlx5_net: dr_create_flow_tbl failed
EAL: No legacy callbacks, legacy socket not created
[09:23:12:180936][DOCA][E][FOFLD]: Forward to SFT IPV4-UDP failed, error=SFT was not initializedSome other information that might be helpful:
$ sudo mlnx-sf -a showSF Index: pci/0000:03:00.0/229408
Parent PCI dev: 0000:03:00.0
Representor netdev: en3f0pf0sf0
Function HWADDR: 02:6f:2d:87:5a:24
Auxiliary device: mlx5_core.sf.2
netdev: enp3s0f0s0
RDMA dev: mlx5_2SF Index: pci/0000:03:00.0/229409
Parent PCI dev: 0000:03:00.0
Representor netdev: en3f0pf0sf4
Function HWADDR: 52:17:06:71:a8:87
Auxiliary device: mlx5_core.sf.4
netdev: enp3s0f0s4
RDMA dev: mlx5_4SF Index: pci/0000:03:00.1/294944
Parent PCI dev: 0000:03:00.1
Representor netdev: en3f1pf1sf0
Function HWADDR: 02:e0:69:a4:6b:ed
Auxiliary device: mlx5_core.sf.3
netdev: enp3s0f1s0
RDMA dev: mlx5_3$ numastat -mc | egrep “Node|Huge”
Token Node not in hash table.
Token Node not in hash table.
Token Node not in hash table.
Token Node not in hash table.
Token Node not in hash table.
Node 0 Total
AnonHugePages         0     0
HugePages_Total    4096  4096
HugePages_Free     3762  3762
HugePages_Surp        0     0Hi! Can you share your output of ovs-vsctl show? I am trying to repeat this in my lab and want to make sure I’ve got this setup the same way.Thanks!It might be possible that you’re on the DOCA 1.0 release and following the DOCA 1.1 (SDK 0.2) guide for running the sample application. Stateful flow table support was added in DOCA 1.1See if these instructions work:NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation./opt/mellanox/doca/examples/url_filter/bin/doca_url_filter -a 0000:03:00.0,class=regex:eth,representor=[65535],sft_en=0 –pAlternatively, upgrading or reimaging to the DOCA 1.1 release should resolve this one.Hi jubetz,Hi jubetz,Hi jubetz,
I don’t know how to check the version number of DOCA. Could you tell me?
But I think I should install DOCA 1.1.
I installed it through nvidia_sdk.
https://developer.nvidia.com/networking/doca/getting-started
And the version downloaded on the host side looks like v1.1.Hi,The SFT error you received is usually shown when supplying the DOCA application an SF (Scalable Function) without first setting them to “trusted”. You can find more information about it in the Scalable Function Documentation.As for the DOCA version, it looks like you installed DOCA 1.1, meaning you have everything needed for testing the application. The configuration steps should be:Good luck!Hi eitkin, thank you very much. Now it works fine.Powered by Discourse, best viewed with JavaScript enabled"
159,trying-to-run-nvme-over-ib-client-installed-ofed-5-1-2-5-8-0-on-rhel-8-3-kernel-4-18-0-240-el8-x86-64-using-mlnxofedinstall-with-nvmf-add-kernel-support-command-failed-to-load-nvme-rdma-driver-getting-a-bunch-of-22-errors-in-dmesg,"I did not re-compile the linux kernel with NVMe block device support. Questions. 1) Is this Linux/OFED combo supported for NVMe over IB operation? 2) Do I have to add the NVMe block device driver by re-building the Linux kernel?Hi,See the following links for a full description on how to configure NVME over fabrics and its prerequisites:https://community.mellanox.com/s/article/howto-compile-linux-kernel-for-nvme-over-fabricshttps://community.mellanox.com/s/article/howto-configure-nvme-over-fabrics#jive_content_id_PrerequisitesYou will find there all the answers to your questions.RegardsMarcPowered by Discourse, best viewed with JavaScript enabled"
160,network-interface-renaming-between-mofed-4-9-3-1-5-0-and-modef-5-4,"Installing MOFED 4.9.3.1.5.0 renames interfaces from eth0/eth1 to enp1s0f0 and enp1s0f1. However, when using MOFED 5.4 it renames the same interfaces to enp1s0f1np0 and enp1s0f1np1. Why this change, and is there a way for me to control that?Hello Larry,Thank you for posting your inquiry on the NVIDIA Networking Community.As of MLNX_OFED 5.4, the following customer affected changes were made → https://docs.mellanox.com/display/MLNXOFEDv541030/Changes+and+New+Features#ChangesandNewFeatures-CustomerAffectingChangesudev Rules:As of version 5.4, the driver is set so that udev rules will change the names of network interfaces created from NVIDIA adapters.The udev rules are shipped to ""/lib/udev/rules.d""and may be overridden by placing a file with the same name in ""/etc/udev/rules.d"".Example:/etc/udev/rules.d/82-net-setup-link.rulesYou can find more information regarding to persistent naming through the following link → https://docs.mellanox.com/display/MLNXOFEDv541030/Ethernet+Interface#EthernetInterface-PersistentNamingPersistentNamingThank you and regards,~NVIDIA Networking Technical SupportPerfect answer with one exception Martijn. I’m wondering how to restore the previous naming convention for the purposes of consistency in the cluster for people attempting or have been utilizing the previous naming convention as a part of the OMPI command lines?Powered by Discourse, best viewed with JavaScript enabled"
161,innova2-flex-app-not-finding-connectx-device,"I have installed the Nvidia OFED driver and made the innova2 flex app. I then run it as follows:The application fails to find the device.
This is my output from lspci:Any ideas? ThanksThe ConnectX device (mlx5_fpga_tools) is created by running sudo insmod /usr/lib/modules/5.8.0-43-generic/updates/dkms/mlx5_fpga_tools.koThe BOPE device is created by running sudo ~/Innova_2_Flex_Open_18_12/driver/make_deviceTry the following:I have some detailed notes regarding the Innova-2.Powered by Discourse, best viewed with JavaScript enabled"
162,package-digest-query-fails-for-mft-rpm-file-if-run-on-aarch64-but-not-x86-64,"We’ve not been using the mlnx_ofed stack.  We’re required to mirror packages locally and ‘createrepo’ against that before provisioning downstream.  All compute nodes on all HPC Asset clusters install the Infiniband Support package group.  Through last week this did not pull in ‘mft’ as a dependency because the mlnx_ofed repo was not enabled.  This week we’re trying to add knem, kmod-knem, and ucx-knem to the compute image, so we enabled the NVidia OFED repodef for the clients;  this points to a cluster-local createrepo’d mirror of {new_users_can’t_post_multiple_links}://linux.mellanox.com/public/repo/mlnx_ofed/latest/rhel8.7/ with the clients’ repodef referencing {releasever} and {basearch} as usual.Enabling dnf searching into the mlnx_ofed stack caused the Infiniband Support package group install to fail with no-digest for the NVidia arm64 mft package.  We can easily verify this on an aarch64 system but, if we try to verify it on an x86_64 system against the same package files, the digests are OK.Why are the SHA256 and MD5 digests of a static aarch64 package file visible on an x86_64 box but not visible, for the same static file, on an aarch64 box running the same base os?Here’s a chroot session into the image being built on an aarch64 system, starting with the minimal bits required to support chroot and dnf:Here’s the manual download and digest check, on the same aarch64 system and in the same chroot session, showing inability to see the SHA256 and MD5 digests of the mft package but ability to see them in (arbitrarily chosen) perl-Digest-SHA:In case the problem is in the [as of yet minimal] system image or chroot session, here’s the same check on the same system, outside of the chroot and new system image, accessing the same package files:And the screwy part:  here’s the same check done with the same files on the x86_64 infrastructure node responsible for serving the local package repo (built from a mirror of Index of /public/repo/mlnx_ofed/latest/rhel8.7 with ‘createrepo’ run against it).  These are the same package files as before, for aarch64, but with the check run on an x86_64 system:If I force mft onto the build (rpm --nodigest -i) then it installs and I can at least start the groupinstall for ‘Infiniband Support’, but this then throws unpack / digest mismatch errors for select other packages in the NVidia OFED stack.  These are not digest-missing or digest-parsable errors, the installing platform can download and verify digests on each of these packages, these appear to be “real” content-validation errors:Error unpacking rpm package sharp-3.2.0.MLNX20230122.a97f1d1c-1.59056.aarch64
error: unpacking of archive failed on file /etc/ld.so.conf.d/sharp.conf;63f94125: cpio: Digest mismatch
error: sharp-3.2.0.MLNX20230122.a97f1d1c-1.59056.aarch64: install failedError unpacking rpm package hcoll-4.8.3221-1.59056.aarch64
error: unpacking of archive failed on file /etc/ld.so.conf.d/hcoll.conf;63f94125: cpio: Digest mismatch
error: hcoll-4.8.3221-1.59056.aarch64: install failedError unpacking rpm package openmpi-4.1.5rc2-1.59056.aarch64
error: unpacking of archive failed on file /usr/mpi/gcc/openmpi-4.1.5rc2/bin/aggregate_profile.pl;63f94125: cpio: Digest mismatch
error: openmpi-4.1.5rc2-1.59056.aarch64: install failedError unpacking rpm package mlnx-ofed-all-5.9-0.5.6.0.rhel8.7.noarch
error: unpacking of archive failed on file /usr/share/doc/mlnx-ofed-all/mlnx-ofed-all-release;63f94125: cpio: Digest mismatch
error: mlnx-ofed-all-5.9-0.5.6.0.rhel8.7.noarch: install failedSkipped in favor of libibverbs-59mlnx44-1.59056.aarch64 and perftest-4.5-0.20.gac7cca5.59056.aarch64, OK and expected.(See previous digest mismatch errors.)Hi,Thank you for your patience .
Did you try to download the package from our website and install it ?The Mellanox Firmware Tools (MFT) package is a set of firmware management toolsThanks,
NVIDIA Enterprise SupportYes, the packages are downloading intact.  The local mirror is created with the same recursive wget method you find in RHEL/CentOS/Rocky/Alma for cases where there’s no upstream rsync service.  The “createrepo” is only there for when a mirroring pass falls inside of an NVidia update of the upstream tree (ie, resulting in the mirroring seeing empty repodata directories).  And besides, as the blockquotes show the problem isn’t the metadata at all, that’s working just fine.  It’s something with visibility of digests, on only the aarch64 side of the fence, and only for a very small number of packages in the mlnx_ofed collection.  The mft, hcoll, sharp, and openmpi packages obviously have their digests, the x86_64 systems can see that via ‘rpm -K’ against the static aarch64 rpm files.  The aarch64 platform can see the same for all of the other 100+ packages in the collection, but not for mft, and not for hcoll, openmpi, sharp, or mlnx-ofed-all when unpacking the rpm.HiWe suspect that the issue might be OS Kernel issue for RH8.7 , we can see your image version
is 4.18.0-425.10.1.el8_7 but we tested in a newer version 4.18.0-425.14.1.el8_7 and it was ok as bellow:uname -a
4.18.0-425.14.1.el8_7.aarch64 #1 SMP Mon Feb 13 10:41:20 EST 2023 aarch64 aarch64 aarch64 GNU/Linuxrpm -Kvv --nosignature mft-4.23.0-104.arm64.rpm 2>&1 |grep -i digest
Header SHA1 digest: OK
Header SHA1 digest: OK
Header SHA1 digest: OK
Header SHA1 digest: OK
MD5 digest: OKPlease give it a try.Thanks,
SamerIt’ll be a bit before I can test against the .13.1 kernel, it’s not yet in the EL mirror as of last night (Rocky 8).With the compute nodes running the latest EL8.7 stable kernel (4.18.0-425.13.1.el8_7.aarch64) the problem persists.  I can’t run a later kernel without leaving the Customer “blessed” EL stack and don’t currently have the resources to isolate a node for testing prerelease kernels.  I’ll put something together but it won’t happen right away…The issue went away briefly while running  the 4.18.0-425.13+ kernel, but is back again.  The kernel of the system building the image is running ‘chroot … dnf install … mlnx-ofed-hpc’, using kernel 4.18.0-425.19.2.el8_7.aarch64.  The kernel in the image itself is 4.18.0-477.10.1.el8_8.aarch64.  Both produce the same failure.  One new bit:  previously it was only the ‘mft’ packages, all other packages in the both the x86_64 and aarch64 sides of both the ‘hpc-sdk’ and ‘public’ roots did not produce this issue.  Now there are 8 more.I’ve tested all packages mirrored from all three urls below and found no digest errors for any packages other than these.  Each of the corresponding183 package files return the same NOTFOUND for the SHA256 and MD5 digest fields on the indicated aarch64 kernel but not the x86_64 equivalent.  Neither x86_64 nor aarch64 kernels produce this error for the other 557 files currently present in the nvidia mlnx-ofed and hpc-sdk stacks.  If you discount various combinations of distro, distrover, basearch, and version, you get 9 products:FWIW here’s the complete list of files exhibiting this issue on aarch64 but not x86_64, from which the above list was compiled.  The same aarch64 system producing digest errors for these did not produce digest errors for 557 other packages in the same upstream sources, urls, and repositories that provide these.Background follows, updated for multiple changes since the OP.Source urls for the local NVidia repos…… are combined under the ‘nvidia’ local module name…… and write to a unified rootdir for nvidia repos…… resulting in the following mft*rpm files as of ~0430 today, Fri 26 May 2023…… all of which have good digests for the mirroring host (kernel 4.18.0-425.13.1.el8_7.x86_64)…… but have unparsable SHA256 and MD5 digests on the image building node (kernel 4.18.0-425.19.2.el8_7.aarch64)Will someone PLEASE look at this ticket.  This is NOT THE FIPS ISSUE, that issue is architecture agnostic, this issue is architecture specific.Powered by Discourse, best viewed with JavaScript enabled"
163,release-notes-for-nvidia-bright-cluster-manager-9-2-5,"== General ==
=  New Features=  Improvements== CMDaemon ==
=  New Features=  Improvements= Fixed Issues== Bright View ==
=  Fixed Issues== Head Node Installer ==
=  Fixed Issues== User Portal ==
=  Fixed Issues== cm-image ==
=  Fixed Issues== cm-kubernetes-setup ==
=  Improvements=  Fixed Issues== cm-wlm-setup ==
=  Fixed Issues== cmsh ==
=  Improvements=  Fixed Issues== hwloc ==
=  Improvements
-Update cm-hwloc2 to 2.7.1== pythoncm ==
=  Improvements== slurm22.05 ==
=  ImprovementsPowered by Discourse, best viewed with JavaScript enabled"
164,unable-to-use-mellanox-cx5-vf-in-my-dpdk-application,"Hi all,
I have a VM running on top of VMWare ESXi which gets a Mellanox VF exposed.
This is the output of dpdk-devbind -s inside the VM, after installing the OFED v5.4:However when I try to start my DPDK application on it I get:I’m trying to understand this “net_mlx5: unable to recognize master/representors on the multiple IB devices” but I cannot fix it… any suggestion?thanks a lot for any hintReplying to myself: it turns out that my application was running inside a container having its own network namespace, where the network interface device (e.g. “enp1s0f0”) was not visible.
What I discovered is that even when using DPDK and PCI addresses for NIC port selection, apparently Mellanox mlx5_core driver still needs to have visibility on the network interface (e.g. “enp1s0f0”). By using the host-device plugin (GitHub - containernetworking/plugins: Some reference and example networking plugins, maintained by the CNI team.) the network interface has been made visible inside the container (a Kubernetes POD) and that fixed this error message from DPDK.Hi Francesco,Thank you for updating the forum on the fix details.Thanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
165,mellanox-mcx516a-ccat-connectx-5-support-enabling-or-disabling-function-for-rx-port,"Trying to control the rx/tx of the card in code-level or Linux. Let me know if you have any features that apply. Thank you.I am not clear on your request though features/functionalities/settings are either hard/soft coded (HW/FW/driver) and the code(s) are proprietary to Nvidia.
However, depending on what you are looking for, there are some settings/features that can be enabled/disabled/modified at the OS/MLNX_OFED, FW level.
You can reference to our MLNX_OFED UM (ethtool, mlxconfig, echo etc…) though no code-level related.Powered by Discourse, best viewed with JavaScript enabled"
166,what-is-eswitch-inline-mode-and-how-can-i-use-it-to-get-performance-boost,"I’ve been playing around devlink and saw there is an eswitch inline-mode that you use for packet steering/acceleration. I have Connect X-5, but I can’t figure out when inline modes can be changed and when these cannot. Because sometimes they does and sometimes I get not supported error.Also, how much performance boost does it provide in all modes?Hi Zaid,eswitch is supported on kernels starting from kernel version 4.9; for Linux distrokernels earlier than 4.9.0, eswitch is supported only on RHEL7.x and on XenServer7.1 CU2.if you are looking into deploying ASAP2 solution for OVS hardware offload & accelerate switch and packet processing, we have a section in our user manual as well as several posts in our community.(Support in ASAP2—Accelerated Switch and Packet Processing®).NVIDIA Accelerated Switching And Packet Processing (ASAP2) technology allows OVS offloading byhandling OVS data-plane in ConnectX-5 onwards NIC hardware (Embedded Switch or eSwitch) whilemaintaining OVS control-plane unmodified. As a result, you observe significantly higher OVSperformance without the associated CPU load.Sophie.Powered by Discourse, best viewed with JavaScript enabled"
167,doca-secure-channel-failed-to-open-comm-channel,"Hi!I’m trying to run the secure channel application on the DPU after upgrading DOCA to 1.5. I get the following error message:Any idea why it is not able to create a representor address?I’m having the exact same problem, do you have any solution yet?Powered by Discourse, best viewed with JavaScript enabled"
168,gtpu-decap-support-in-simple-forward-implementation,"Does Simple Forward VNF :: NVIDIA DOCA SDK Documentation support GTPU decap?Questions:There exist simple_fwd_pinfo_decap() in simple_fwd_pkt.c which states/show only support for DOCA_FLOW_TUN_VXLAN and “decap for GRE not supported” and no mention of GTPU – Is GTPU decap supported?simple_fwd_build_gtp_pipe() in  simple_fwd.c show “actions.decap = true;” but I do not observe GTPU decap – Is “actions.decap = true;” supported for GTP and does it function correctly in causing the headers that were defined in the doca_flow_match to be removed from the packet ?I forgot to add:My NIC is BlueField-2 P-Series SmartNIC 25GbE Dual-Port SFP56,DOCA_v1.2.1_BlueField_OS_Ubuntu_20.04-5.4.0-1023-bluefield-5.5-2.1.7.0-3.8.5.12027-1.signed-aarch64Powered by Discourse, best viewed with JavaScript enabled"
169,error-code-16-at-startup-on-mcx516-gcat-once-in-a-while,"Once in a while we are seeing an error code of -16 when booting the system. It has happened on multiple systems (initially we thought maybe it was just one flaky card). Rebooting pretty much always seems to fix it, but having to monitor for it and reboot isn’t great. We are currently running 4.9.185 kernel in case this is a driver problem. We have seen this on firmware 10.27.2008, and some earlier versions too. I upgraded to 10.28.2006 yesterday and haven’t seen it yet there, although I am not sure how many attemps I have to make to be sure. Also with 10.28.2006 (and 10.28.1002) we get strange errors at boot like this:Dec 4 09:18:43 c1-xca2 kernel: mlx5_core 0000:af:00.0: firmware version: 16.28.1002Dec 4 09:18:43 c1-xca2 kernel: mlx5_core 0000:af:00.0: Rate limit: 127 rates are supported, range: 0Mbps to 48828MbpsDec 4 09:18:43 c1-xca2 kernel: mlx5_core 0000:af:00.1: firmware version: 16.28.1002Dec 4 09:18:44 c1-xca2 kernel: mlx5_core 0000:af:00.1: Rate limit: 127 rates are supported, range: 0Mbps to 48828MbpsDec 4 09:18:44 c1-xca2 kernel: mlx5_core 0000:af:00.0: MLX5E: StrdRq(0) RqSz(1024) StrdSz(1) RxCqeCmprss(0)Dec 4 09:18:44 c1-xca2 kernel: mlx5_core 0000:af:00.1: MLX5E: StrdRq(0) RqSz(1024) StrdSz(1) RxCqeCmprss(0)Dec 4 09:18:44 c1-xca2 kernel: mlx5_core 0000:af:00.0 temp: renamed from eth3Dec 4 09:18:44 c1-xca2 kernel: mlx5_core 0000:af:00.1 eth3: renamed from eth4Dec 4 09:18:44 c1-xca2 kernel: mlx5_core 0000:af:00.0 eth4: renamed from tempDec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: device’s health compromised - reached miss countDec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: assert_var[0] 0xfffffffcDec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: assert_var[1] 0x00000001Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: assert_var[2] 0x00000000Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: assert_var[3] 0x00000000Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: assert_var[4] 0x00000000Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: assert_exit_ptr 0x00991a18Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: assert_callra 0x009919c4Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: fw_ver 1.28.1002Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: hw_id 0x0000020dDec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: irisc_index 5Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: synd 0x1: firmware internal errorDec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.0: ext_synd 0x8bb4Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: device’s health compromised - reached miss countDec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: assert_var[0] 0xfffffffcDec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: assert_var[1] 0x00000001Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: assert_var[2] 0x00000000Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: assert_var[3] 0x00000000Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: assert_var[4] 0x00000000Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: assert_exit_ptr 0x00991a18Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: assert_callra 0x009919c4Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: fw_ver 1.28.1002Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: hw_id 0x0000020dDec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: irisc_index 5Dec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: synd 0x1: firmware internal errorDec 4 09:18:53 c1-xca2 kernel: mlx5_core 0000:af:00.1: ext_synd 0x8bb4Dec 4 09:19:09 c1-xca2 kernel: mlx5_core 0000:af:00.1 eth3: Link upAny suggestion as to why the 2 latest firmwares do that? Every system I have tried either of the 16.28.x firmwares on do that (all using 4.9.185 kernel of course). I was trying the updated firmware since I thought perhaps 2100377 could explain the -16 error we sometimes see on boot when the driver fails to start the card and so far I haven’t seen it with 10.28.x but I am seeing those other strange errors (and neither our QA or customers will like seeing those in the logs. Someone will ask questions even if it is working).Hello Lennart,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we want to continue to debug this issue through a NVIDIA Networking Technical Support ticket. You currently have a valid support contract so when you send an email to support@mellanox.com it will open a support ticket which will be handled by one of our support engineers. Also please note in the email which time zone you are residing so we can re-route the ticket to your local support center.Thank you and regards,~NVIDIA Networking Technical SupportOK, I have sent of an email with as much details as I could think of.Hi Martijn, sorry about that, could you able to share the root cause and solution of this symptom, ​I also found the same symptom from my side, appreciate.Powered by Discourse, best viewed with JavaScript enabled"
170,looking-for-data-on-the-455a-ecats-heatsink,"We need to upgrade the heatsink on the 455A adapters to increase dissipation efficiency for our application.Thank you.We need to upgrade the heatsink on the 455A adapters to increase dissipation efficiency for our application.Hi,
we do not understand the nature of your question. NVIDIA does not provide technical information on internal components of the system.
Thanks,
SuoThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
171,issue-using-the-media-receiver-to-display-2110-20-video,"I am using the media_receiver to display video transmitted from a Matrox ConvertIP device to a connectX card.
The image received appears to have its lines shifted.

SampleImage1920×1080 51.2 KB
When I use the media_sender to send an image, the image is displayed as it should.
Any idea what could be causing this? Where should I look?Hi shachara,Welcome, and thank you for posting your inquiry to the NVIDIA Developer Forums!For further assistance with this issue, please open a support ticket with NVIDIA Enterprise Experience at the following link: https://enterprise-support.nvidia.com/s/create-caseOur Enterprise Technical Support team will be able to assist you further.Thanks,
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
172,cloning-microsd-card-to-usb,"I would like to clone the microsd card via dd so that I can make a backup microsd card. There is no room on the microsd card to clone itself (i.e. I m over 50% full - even with gzip compression).
So I put a large volume usb3.1 stick in one of the usb ports. Via parted, I see the disc. It is not mounted. It is in /dev/sda.
I want to do something like:
sudo dd if=/dev/microsd conv=sync,noerror bs=64K | gzip -c > /dev/sda/backup_image.img.gz1- Since this is not NTFS, the largest file is 4gb and my clone will surly be larger. Should I format the disc to NTFS?  Perhaps exFat is better?
2- In order to do the dd, the disc is supposed to be not mounted, correct?If anyone can give direct comments on how I should do this (fat32 will not be good) I’d appreciate advice. Thanks.Powered by Discourse, best viewed with JavaScript enabled"
173,based-on-connectx-6-what-is-the-different-of-virtio-acceleration-through-vf-relay-software-hardware-vdpa-and-virtio-acceleration-through-hardware-vdpa-which-is-in-documents-of-mlnx-en-documentation-rev-5-3-1-0-0-1-07-09-2021-pdf,"In document [MLNX_EN Documentation Rev 5.3-1.0.0.1__07_09_2021.pdf], there are two sections, called [VirtIO Acceleration through VF Relay (Software & Hardware vDPA)] and [VirtIO Acceleration through Hardware vDPA] respectively, I’m confused what is the difference of these two modes.Hi Jianquan,vDPA allows the connection to the VM to be established using VirtIO, so that the data-plane is built between the SR-IOV VF and the standard VirtIO driver within the VM, while the control-plane is managed on the host by the vDPA application.Two flavors of vDPA are supported, Software vDPA and Hardware vDPA.In general, SW solution uses emulated driver which simulates HW supports, wastes more CPUs and can work on older NICs which don’t support HW plugin.For more information, please refer to the below article, sections: VirtIO Acceleration through VF Relay (Software vDPA) and VirtIO Acceleration through Hardware vDPA.https://docs.mellanox.com/m/view-rendered-page.action?abstractPageId=52011200#Thanks,ChenI know what you mean, Hardware vDPA is better in performance compared with software vDPA. What confuse me most is the difference between [VirtIO Acceleration through VF Relay (Software & Hardware vDPA)] and [VirtIO Acceleration through Hardware vDPA], Is VF Relay with Hardware vDPA is different with Hardware vDPA? What’s the difference?I have read the attached link page​, and have practice with ConnectX-6 in the lab, could you please give me more information/documents about the hardware acceleration principle, or simply, how hardware vDPA works, thank you.We support hardware vDPA from CX6Dx.While both work in accelerating virtio, the difference is quite big:ConnectX-6Dx has a dedicated processing unit (ACE) which can emulate virtio software.Unlike previous different solutions, virtio datapath is not handled by software, but directly by hardware.This provides several advantages:VF relay was gen1 solution, which can also run on ConnectX-5.With VF Relay, the software (OVS) opens a dpdk port which translates between phy port to virtIO port. It takes packets from the Rx queue and sends them to the suitable Tx queue, and allows transfer of packets from virtIO guest. DPDK mlx5 rx_burst/tx_burst functions are used for datapath. This approach is costly on CPU but provides better performance than legacy virtio solution.Thanks,ChenSorry, I’m a little confused. In sentence​ [please refer to the below article, sections: VirtIO Acceleration through VF Relay (Software vDPA) and VirtIO Acceleration through Hardware vDPA.], you just said two situations, VF Relay based on software vDPA and VirtIO Acceleration through Hardware vDPA. However, in my oppion, there are three situations:1. VF Relay based on software vDPA;2. VF Relay based on hardware vDPA;3. VirtIO Acceleration through Hardware vDPA;The section name is [VirtIO Acceleration through VF Relay (Software & Hardware vDPA)], so I think the first two situations are included in this section.The difference between situation 1 and situation 2 is easy to distinguish, but what is the difference between situation 2 and situation 3.In situation 2, Vitual Fumction is exposed in host machine, while in situation 3, just Virtio-net port is exposed in host machine?VF relay is SW solution only (no HW in VF relay).VirtIO Acceleration through Hardware vDPA is HW based, on ConnectX-6 and BF-2.The difference between the SW solution (VF relay) and HW solution is described in my previous comment.Thanks,ChenThat means, there are some mistakes in section name of [VirtIO Acceleration through VF Relay (Software & Hardware vDPA)] ?In the document, command [# ovs-vsctl add-port br0 vdpa0 – set Interface vdpa0 type=dpdkvdpa options:vdpa-socket-path=/tmp/sock-virtio0 options:vdpa-accelerator-devargs=0000:5e:00.2 options:dpdk-devargs=0000:5e:00.0,representor=[0] options:vdpa-max-queues=8 [options:vdpa-sw=true]] isused for VF Relay based Software/Hardware vDPA through adding [options:vdpa-sw=true] or not. While in section Running Hardware vDPA, command [./vdpa -w ,class=vdpa --log-level=pmd,info – -i] is needed.The document is attached, in page 152 and page 156.Powered by Discourse, best viewed with JavaScript enabled"
174,i-cannot-use-sdkmanager-to-install-doca,"I am following the document to install DOCA by using sdkmanager.
I am using a r7525 type of machine from cloudlab.
The hardware settings are:
CPU: two 32-core AMD 7542 at 2.9GHz
RAM: 512GB ECC Memory(16 X 32 GB 3200MHz DDR4)
DISK: One 2TB 7200 RPM 6G SATA HDD
NIC: Dual-port Mellanox ConnectX-5 25 Gb NIC (PCIe v4.0)
NIC:Dual-port Mellanox BlueField2 100 Gb SmartNIC
GPU:Two NVIDIA GV100GL (Tesla V100S PCIe 32GB)The software settings are:
Ubuntu 20.04 LTS
Linux 5.4.0-100-generic
sdkmanager version: 1.9.0.10816 /  1.9.3.10904I tried with both old/new version of sdkmanager. None of them can work.The command to install DOCA:
sdkmanager --cli install --logintype devzone --product DOCA --host --target BLUEFIELD2_DPU_TARGETS --targetos Linux --version 1.5 --flash allError Message:
No such product ‘DOCA’. Please select one of the following products: JetsonI wonder whether there are some updates to sdkmanager and whether it does not support DOCA installation any more.Thanks!Please follow DOCA maunall to install.https://docs.nvidia.com/doca/sdk/installation-guide-for-linux/index.html#installing-software-on-hostHi @xiaofengl, does it mean that sdkmanager is not working properly?Sure. I have the same question with yavtul. Does it mean that sdkmanager cannot be used to install DOCA?ThanksSDKManager can install DOCA.  I have been using cli method with sdkmanager to install both 1.5.1 and 2.0.2 of DOCA.Here is command line I use…DOCA 2.0.2:sdkmanager --cli install --logintype devzone --product DOCA --targetos Linux --host --target BLUEFIELD2_DPU_TARGETS --flash all --version 2.0.2-or-DOCA 1.5.1sdkmanager --cli install --logintype devzone --product DOCA --targetos Linux --host --target BLUEFIELD2_DPU_TARGETS --flash all --version 1.5.1My host is Rocky Linux 8.6Does this help?-JHi,Thanks for the answer. I wonder what is the version of your sdkmanager. And have you ever installed on ubuntu using the same cli?ThanksHi cxinyicThe version I am using is:[VH02 ~]$ sdkmanager --ver
1.9.2.10899
[VH02 ~]$Originally I installed:sdkmanager_1.9.1-10844.x86_64.rpmIt then updated itself to the version listed above.Though we run Rocky mostly here, I did install all of this on Ubuntu while troubleshooting an unrelated issue to ensure that using Rocky wasnt the cause of my issue (it wasn’t).  The install on Ubuntu worked fine using the same command line I posted earlier.-J@IamAries do you have early access or something like that? because I got the same error,  only jetson is available. I tried using gui and cli but I see only the one platform and can’t choose DOCAsdkmanager --cli downloadonly --staylogin true --logintype devzone --product DOCA --targetos Linux --host --target BLUEFIELD2_DPU_TARGETS --flash all --version 2.0.2Authenticating with NVIDIA server…
Login succeeded.
Loading user information…
User information loaded successfully.
Loading server data…
Server data loaded successfully.
No such product ‘DOCA’. Please select one of the following products: Jetsonsdkmanager --ver
1.9.2.10899@yavtukWe do have early access.  I had assumed you and original poster as well as I thought I saw in other messages you are on DOCA 2.0.2?  How did you install your current setup - manually?What do you get if you run:[VH02 ~]$ sdkmanager  --cli --product DOCA --showallversions --version 2.0.2 --targetos linux --hostFor me I get many versions of JetPack and DOCA.  The list is a bit long to post, but do you not have any DOCA versions appearing?-JI think the early access is the reason. I installed it manually.sdkmanager --cli --product DOCA --showallversions --version 2.0.2 --targetos linux --host
Authenticating with NVIDIA server…
Login succeeded.
Loading user information…
User information loaded successfully.
Loading server data…
Server data loaded successfully.
No ‘DOCA’ product is installed on your host.@xiaofengl Can you confirm that use is only possible with a paid subscription?That is curious indeed.  I would think that if you didn’t have access, it would not let you download the DOCA sdk by any method (not just block sdkmanager).Anyway, shall be interesting to know what the answer is.-JI followed the manual installation and it can work. Thanks. But still, I am curious why the sdkmanager cannot work.Powered by Discourse, best viewed with JavaScript enabled"
175,non-standard-connectx-6-dx-firmware,"We purchased some ConnectX-6 Dx NICs (CX623106A) from a vendor on eBay. They look legit and appear to operate as the dual 100 GbE NICs they should be. But when we went to update the firmware, the firmware would not update as the cards have a non-standard PSID of CIS0000000008.Is there a way to flash a standard NVIDIA firmware over this load?Here is what we see currently:Thanks in advance for any guidance.
ShepThese seem like Cisco/MLNX branded cards.Upgrade versions should arrive from Cisco.I think their latest release for this card is under:https://www.cisco.com/c/en/us/td/docs/unified_computing/ucs/release/firmware_files/4_2/b_ucs_c-series-firmware-files_4_2/m_firmware_files_4_2_3.htmlCisco-MLNX MCX623106AS-CDAT 2x100GbE QSFP56 PCIe NICThank you for that @dwaxman ! I would have guessed CIS ~= “cisco”, but wouldn’t have found the link you provided.I grabbed the iso from the Cisco site and blew out the file system to see what is in the /firmware/Common/Mellanox_Technologies directories. I found three ConnectX6Dx firmwares as such:But when I try flint on any of these, I get a “Invalid Image signature” error.
Drats! I really dont want to recreate the whole Cisco update universe.These boards seem to work fine; just a little uncomfortable not having the mainline Nvidia firmware (or even the latest Cisco-branded fw). Morale of the story - stick with Nvidia NICs bought as Nvidia NICs; as even a legit-looking board from eBay may have other firmware.It looks like a secured FW image. So I’m not sure it’ll allow you to burn any other image onto the card’s flash.Hello, I have the same card also from ebay. I’m unable to make it work at 100Gb. It is able to connect at 40Gbps and the strangest thing is the switch shows the port as 100Gb.
lshw shows this:From ethtool i can see it also shows speed 40Gbps.
I installed the latest driver 23.04-1.1.3 Any advice on why the card is not able to do 100G?Powered by Discourse, best viewed with JavaScript enabled"
176,md5-switch-image,"is it possible to md5 checksum the image after its copied on the switch ? Just as an extra precaution step before upgrading the switch.Hi Dropbrick,
there is no cli command for it but the install command itself  (image install )is checking for integrity before actually installing so it will fail if the checksum is wrongPowered by Discourse, best viewed with JavaScript enabled"
177,are-there-registers-or-any-other-options-that-can-be-set-which-will-affect-performance,"As we all know, there are many registers (like msr in x86 CPU) on CPU which can be set with tools or set with BIOS. The value in registers change the action of CPU computing, which affects the performance.​Are there any registers of this kind on the SoC of ConnectX-5? How to read and write to them?What’s more, is there any knobs (for example, knobs which indicate the buffer or queue) on SoC of ConnectX-5 or driver or protocal of this network card?​What I wanna do:The general settings of ConnectX-5 may not suit for every kind of workload. I wanna find knobs which may affect the actions of ConnectX-5 and tune them.I have read linux tuning from the aspect of TCP/UDP/kernel/NUMA bind and so on. But I wanna tune ConnectX-5 with OFED with HPC workload running.So, are there knobs of ConnectX-5? (Not only limited to registers on SoC, driver, protocal stack)Hello Zh,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, the adapter f/w controls all the features and OOB tunes the adapter for the best performance possible,Tuning the adapter to interact with the OS, is done through the Performance Tuning Guidelines provided through the following link → https://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
178,the-meaning-of-flow-control-update-watchdog-timer-expired-messages,"Sometimes, the messages like “log_trap_info: Received Generic Notice: type:1 num:131 (Flow Control Update watchdog timer expired) Producer:2 (Switch) from LID:11 Port 7 TID:0x00000500070083” are logged three times continuosly in opensm.log file.At that time, there isn’t any network issue.If someone know the meaning of these messages and why these messages are logged, please inform me.Belows are HW/SW information.Switch: IB HDR QM8790Adapter: HCA 1 port / 2 portsOFED version: 4.6.1.0.1Adapter FW version: 20.28.1002Switch FW version: 27.2008.1300Thank you,Sung-JaeThis message means there is physical error , like symbol error caused the Flow control message time out , , you can check use #perfquery 11 7 to check the counter.And you can also collect a ibdiagnet and open a case if you have support contractthanksPowered by Discourse, best viewed with JavaScript enabled"
179,kernel-crash-issue-after-installing-mlnx-ofed-23-04-1-1-3-0-driver,"A kernel crash occurred after installing the mlnx_ofed 23.04-1.1.3.0 driver.
The system log is as follows.Hi kyoonRegarding the log “Dropping C-tag vlan stripping offload due to S-tag vlan”
C-Tag feature enabled by default. So when VM or other application disable it. so when the end application can not read the packets, it displays the log messages. i think ethoool can change the settings.Regarding the meaning of “Disabling HW_VLAN CTAG FILTERING, not supported in switchdev mode”, could you please open the CASE?Regarding the kernel crash issue, you must open a ticket as well./HyungKwangHi hyungkwangHow do I open a case?This configuration worked fine when using 5.8-2.0.3.
The eswitch configuration is registered as a systemd service and is done at boot time.
Passes additional logs of conflicts between the driver and the kernel.
The kernel version is “Linux 5.15.0-60-generic”.thanksHow do I open a case?To open a CASE, please contact your NVIDIA regional sales representive or Nvidia partner, and get an valid license to get technical support per your purchased product, and then ask partner or sales about “how to open a technical case?”Powered by Discourse, best viewed with JavaScript enabled"
180,vivado-board-files-for-innova-2-flex-cards,"Hello everyone,
Are there Vivado board files for MNV303212A-ADLT and MNV303611A-EDLT card?innova2_mnv303611a_xcku15p_xdma is a complete XDMA demo for the MNV303611A-EDLT.Here is a minimal constraints .xdc file taken from the official Innova-2 constraints package which should work with the MNV303611A-EDLT:Powered by Discourse, best viewed with JavaScript enabled"
181,hgx-h100-ib-hdr-200-is-it-possible,"Hi there!I have a customer who wants to use a new generation of systems based on HGX H100 with previous generation of IB network - HDR200, because he has already bought network equipment.
Perhaps someone can tell me which network adapters are better to use in this case? (with the possibility of transition to 400 NDR IB network equipment in the future)HDR 200 IB switches has QSFP56 ports
Single port ConnectX-7 supports only OSFP (incompatible with QSFP56)
Dual port ConnectX-7 supports QSFP112, but I have doubts how many adapters to use:Or better will be use ConnectX-6 for HDR with H100NVIDIA ConnectX-7 Adapter Cards User Manualhttps://docs.nvidia.com/networking/display/ConnectX7VPI/NVIDIA+ConnectX-7+Adapter+Cards+User+ManualHello Bayonetx,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.For this type in inquiries, we recommend you to contact your NVIDIA Sales representative, to make sure he can validate the technical request and that it is supported.Thank you and regards,
~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
182,doca-flow-how-to-match-icmp,"How would one go about matching ICMP traffic with doca_flow?Using the samples I can match TCP traffic, for example by setting my doca_flow_match fields like:match.out_l4_type = DOCA_PROTO_TCP;However, there appear to be DOCA_PROTO_ defines for TCP and UDP.  I did notice that the define values correspond to the IANA protocol numbers for these TCP - 6, and UDP - 17.  So I tried specifying 1 as ICMP is 1 but that resulted in an error.So, how might one go about setting up a match for ICMP traffic?-JThis was sorted out.  ICMP is supported on 2.0.2.  We had been working with 1.5.1.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
183,how-does-the-mlx5-driver-use-the-iova-generated-by-iommu-as-dma-address,"I have enabled IOMMU on the physical machine(iommu=pt intel_iommu=on). I expect the RDMA NIC to use the IOVA allocated by the IOMMU module for DMA after enabling IOMMU. However, in reality, the RDMA NIC does not use the IOVA for DMA：
image1573×705 75.9 KB
I found through reading the kernel source code that ib_dma_map_sgtable_attrs() is called in ib_umem_get to obtain the DMA address for each scatterlist (sg) entry. During the process of obtaining the DMA address, it first checks whether dev->ops is empty, and then decides whether to assign the DMA address to IOVA. Currently, it was debugged that dev->ops is empty at this point, and the DMA address is not assigned to IOVA.Does the mlx5 driver perform any other actions (such as assigning dev->ops) when obtaining the struct device *dev to be able to use IOMMU?Hi Kuao,Thank you for posting your query on NVIDIA Forum.Based on internal check, this requires a deeper dig into the driver code and unfortunately, we cannot perform debug over the forum.As this is programming related, a support ticket with valid support contract in place will be required for further debug. Support ticket can be opened by emailing Networking-support@nvidia.comFor details on contracts, please feel free to contact our contracts team at Networking-contracts@nvidia.comThanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
184,i-have-the-performance-issue-of-connectx-4-lx-on-ubuntu-20-04lts-kernel-5-4-0-84-generic-could-you-please-help-us-for-fix-the-issue,"
mellanox_card_result1604×780 39.2 KB
I have the performance issue of ConnectX-4 Lx on ubuntu 20.04LTS, kernel 5.4.0-84-generic. The scenario is Server-01 direct connect to Server-02 single port 25G via Juniper Switch (Layer 2). Then we test the performance with iperf3 get the result is bandwidth not stable (Throughput not maximum 25G). We checked the configure on Juniper switch is normally. Which we upgrade the kernel from 5.4 to 5.11 and try to test again have the same issue. Then we try test as same as the Server-01 and Server-02 with the Intel Card (XXV710) to get the result is stable (24.7G - 24.8G). Could you please help us for fix the issue?.More Informantion :Server-01:~# modinfo mlx5_corefilename: /lib/modules/5.4.0-84-generic/updates/dkms/mlx5_core.koalias: auxiliary:mlx5_core.eth-repalias: auxiliary:mlx5_core.ethversion: 5.4-1.0.3license: Dual BSD/GPLdescription: Mellanox 5th generation network adapters (ConnectX series) core driverauthor: Eli Cohen eli@mellanox.comsrcversion: 7B67674E4C53E7F9FEBDFFFalias: pci:v000015B3d0000A2DCsvsdbcsci*alias: pci:v000015B3d0000A2D6svsdbcsci*alias: pci:v000015B3d0000A2D3svsdbcsci*alias: pci:v000015B3d0000A2D2svsdbcsci*alias: pci:v000015B3d00001021svsdbcsci*alias: pci:v000015B3d0000101Fsvsdbcsci*alias: pci:v000015B3d0000101Esvsdbcsci*alias: pci:v000015B3d0000101Dsvsdbcsci*alias: pci:v000015B3d0000101Csvsdbcsci*alias: pci:v000015B3d0000101Bsvsdbcsci*alias: pci:v000015B3d0000101Asvsdbcsci*alias: pci:v000015B3d00001019svsdbcsci*alias: pci:v000015B3d00001018svsdbcsci*alias: pci:v000015B3d00001017svsdbcsci*alias: pci:v000015B3d00001016svsdbcsci*alias: pci:v000015B3d00001015svsdbcsci*alias: pci:v000015B3d00001014svsdbcsci*alias: pci:v000015B3d00001013svsdbcsci*depends: mlx_compat,tls,auxiliary,mlxdevm,pci-hyperv-intf,mlxfw,psampleretpoline: Yname: mlx5_corevermagic: 5.4.0-84-generic SMP mod_unload modversionsparm: guids:charpparm: node_guid:guids configuration. This module parameter will be obsolete!parm: num_of_groups:Eswitch offloads number of big groups in FDB table. Valid range 1 - 1024. Default 15 (uint)parm: debug_mask:debug mask: 1 = dump cmd data, 2 = dump cmd exec time, 3 = both. Default=0 (uint)parm: prof_sel:profile selector. Valid range 0 - 3 (uint)parm: probe_vf:probe VFs or not, 0 = not probe, 1 = probe. Default = 1 (bool)Server-01:~# uname -r5.4.0-84-genericServer-01:~# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianAddress sizes: 43 bits physical, 48 bits virtualCPU(s): 192On-line CPU(s) list: 0-191Thread(s) per core: 2Core(s) per socket: 48Socket(s): 2NUMA node(s): 2Vendor ID: AuthenticAMDCPU family: 23Model: 49Model name: AMD EPYC 7642 48-Core ProcessorStepping: 0Frequency boost: enabledCPU MHz: 1499.440CPU max MHz: 2300.0000CPU min MHz: 1500.0000BogoMIPS: 4599.98Virtualization: AMD-VL1d cache: 3 MiBL1i cache: 3 MiBL2 cache: 48 MiBL3 cache: 512 MiBNUMA node0 CPU(s): 0-47,96-143NUMA node1 CPU(s): 48-95,144-191Vulnerability Itlb multihit: Not affectedVulnerability L1tf: Not affectedVulnerability Mds: Not affectedVulnerability Meltdown: Not affectedVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccompVulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitizationVulnerability Spectre v2: Mitigation; Full AMD retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB fillingVulnerability Srbds: Not affectedVulnerability Tsx async abort: Not affectedFlags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibrsibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgifumip rdpid overflow_recov succor smcaHII suggest to apply tuning on the Server/BIOS/Kernel/OS level following our guidehttps://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersIf this won’t help , feel free to open support case by sending email to Networking-support@nvidia.comThanks,SamerHiThanks you for your response. So i think driver is not compatibility with Ubuntu 20.04. Because I try to test on same hardware with CentOS 7 haven’t the issue. So i will try again follow as your recommend.Powered by Discourse, best viewed with JavaScript enabled"
185,release-notes-for-nvidia-bright-cluster-manager-9-2-13,"Release notes for Bright 9.2-13== General ==
=Improvements==Fixed Issues=== CMDaemon ==
=Fixed Issues=== Node Installer ==
=Fixed Issues=== Head Node Installer ==
=Fixed Issues=== cmha-setup ==
=Fixed Issues=== jupyter ==
=Fixed Issues=Powered by Discourse, best viewed with JavaScript enabled"
186,getting-started-with-connectx-5-100gb-s-adapter-for-windows-2019-server,"I attached ConnectX-5 to 2 Windows 2019 servers and installed the WinOF-2 drivers. After the installation was completed, the ConnecX-5 cards installed in the two servers were connected with Infiniband cables.
I use “MLNX_WinOF2-3_20_50010_All_x64.exe”.In device manager each ConnectX-5 is displayed in network adpater. However, the link between the two units is not connected. How can I make it a link?  There is no opensm.exe.Addiitionally…I bought 2 ConnectX-5s. One ConnectX-5 was installed on each of the two servers. And, the ConnectX-5 cards mounted on the two servers were connected with a 100G cable.Currently, when Link_Type_P1=2, the LED of the card turns on and the connection is made normally.But, when Link_Type_P1=1, the card’s LED doesn’t turn on. That is, there is no connection between the cards.I’m not ethernet type, I want to use IB type.How can I set and use IB type with ConnecX-5 in Windows server environment?Like OpenSM in ConnectX-3, what additional settings do I need to set to connect the link? Please let me know how to set it up.Hi,
Thank you for contacting us.I think you should check to use proper firmware version of ConnectX-5 and cable type (MCP1600-E).
Please check the LINKs below
https://docs.nvidia.com/networking/display/winof2v320/Firmware+Upgrade
https://docs.nvidia.com/networking/display/winof2v320/InfiniBand+Network1240.89 KBThank you,
NVIDIA Network SupportAfter installing the WinMFT_x64_4_23_0_104.exe program on the server, the MLNX_WinOF2-3_20_50010_All_x64.exe program was installed.
When installing the MLNX_WinOF2-3_20_50010_All_x64.exe program, the firmware was automatically updated.After installing the driver, set Link_Type_P1=2 and restart the servers. In that case, the orange LED on the card lit up before both servers booted up. When the server booting is complete, the LED turns green and communication is established between the two servers through ConnectX-5.However, after setting Link_Type_P1=1, the servers were restarted. In that case, the orange LED on the card lit up before both servers booted up. Even if the server boots up, the LED turns orange and communication between the two servers is not possible through ConnectX-5.When using ConnectX-3, the LED turned orange even after booting after setting to IB mode. However, when I set up and started openSM, the LED turned green. Does ConnectX-5 not have the same function as OpenSM in Windows Server? Or is there another way?Powered by Discourse, best viewed with JavaScript enabled"
187,i-have-a-problem-with-ubuntu-14-04-6-lts-and-a-connectx-4-ethernet-card-weve-tried-with-various-mlnx-ofed-linux-4-and-5-and-we-cannot-see-the-ports-can-you-help-me-please,"Distributor ID: UbuntuDescription: Ubuntu 14.04.6 LTSRelease: 14.04Codename: trustyHe probado con dos versiones de drivers:mlnx-en-4.9-3.1.5.0-ubuntu14.04-x86_64.isoMLNX_OFED_LINUX-5.3-1.0.0.1-ubuntu14.04-x86_64.isoY en la instalación de cada driver verificar que el firmware sea e ultimo disponible si no lo tiene instalado lo instala por lo que el de acuerdo a esto el firmware es el último.Reviso con lspci y obtengo esto:01:00.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]01:00.1 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]De acuerdo al manual una vez instalado los driver debe reiniciar el servicio para ver los puertos:arca@optiplex-7040:~/Downloads$ sudo /etc/init.d/mlnx-en.d restart[sudo] password for arca:Unloading NIC driver: [ OK ]Loading NIC driver: [ OK ]La instalación en ambos casos termina sin errores pero aún no tengo disponible los puertos, solamente los puertos ya existentes están disponibles:1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: eth0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000link/ether 90:e2:ba:b8:7b:d0 brd ff:ff:ff:ff:ff:ff3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000link/ether 48:4d:7e:b0:02:83 brd ff:ff:ff:ff:ff:ff4: eth2: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000link/ether 90:e2:ba:b8:7b:d1 brd ff:ff:ff:ff:ff:ffAlgo que llama mi atención es que en los registros de sistema cuando reinicio el servicio tengo los siguientes mensajes :May 10 11:10:08 optiplex-7040 kernel: [153550.957504] Compat-mlnx-ofed backport release: 7e619caMay 10 11:10:08 optiplex-7040 kernel: [153550.957507] Backport based on mlnx_ofed/mlnx-ofa_kernel-4.0.git 7e619caMay 10 11:10:08 optiplex-7040 kernel: [153550.957508] compat.git: mlnx_ofed/mlnx-ofa_kernel-4.0.gitMay 10 11:10:08 optiplex-7040 kernel: [153550.978037] mlx5_core 0000:01:00.0: Missing registers BAR, abortingMay 10 11:10:08 optiplex-7040 kernel: [153550.978041] mlx5_core 0000:01:00.0: mlx5_pci_init:1055:(pid 25366): error requesting BARs, abortingMay 10 11:10:08 optiplex-7040 kernel: [153550.978080] mlx5_core 0000:01:00.0: init_one:2142:(pid 25366): mlx5_pci_init failed with error code -19May 10 11:10:08 optiplex-7040 kernel: [153550.978334] mlx5_core 0000:01:00.1: Missing registers BAR, abortingMay 10 11:10:08 optiplex-7040 kernel: [153550.978338] mlx5_core 0000:01:00.1: mlx5_pci_init:1055:(pid 25366): error requesting BARs, abortingMay 10 11:10:08 optiplex-7040 kernel: [153550.978375] mlx5_core 0000:01:00.1: init_one:2142:(pid 25366): mlx5_pci_init failed with error code -19-JavierHello Javier,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, the issue you are facing is not related to the adapter. It is a system BIOS related issue. The system BIOS was not able to setup the BAR address on our devices and as a result the driver failed at startup.Our recommendation is to update the system BIOS to latest version available, reset all BIOS settings to system default.If the issue still occurs after these recommendation, please contact the system vendor.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
188,mellanox-flex2-smart-nic-unable-to-set-guid-and-mac,"Hi, I’ve a mellanox flex2 smart nic (bought used) that have no guid and mac set.I’ve tried to reflash the firmware and setting up mac and guid without success.If i query the card i see:flint -d /dev/mst/mt4119_pciconf0 qImage type: FS4FW Version: 16.28.2006FW Release Date: 15.9.2020Product Version: 16.28.2006Rom Info: type=UEFI version=14.21.17 cpu=AMD64type=PXE version=3.6.102 cpu=AMD64Description: UID GuidsNumberBase GUID: N/A 12Base MAC: N/A 12Image VSD: N/ADevice VSD: N/APSID: MT_0000000158Security Attributes: N/AI’ have tried this command :flint -d /dev/mst/mt4119_pciconf0 -uid e41d2d0300xxxxxx -ocr sg guids_num=4 step_size=1mlxfwreset -d /dev/mst/mt4119_pciconf0 rAlso i’ve tried to edit the image and reflash the card without success. Can i do?Another question :How can i check if the board work correctly ? Can be done a test to it? ( nic and fpga)? thanks.Hi,Please use the below commands :Thanks,SamerThanks for reply,I’ve tried your method without success. Seems that device have non GUID and MAC.Querying Mellanox devices firmware …Device #1:Device Type: ConnectX5Part Number: MNV303212A-ADL_Ax_BxDescription: Innova-2 Flex for Application Acceleration; ConnectX-5 Eth Adapter dual Port SFP 25GbE; Xilinx KU15P; PCI4.0 x8; HHHL; active cooling tall bracket; ROHS R6PSID: MT_0000000158PCI Device Name: /dev/mst/mt4119_pciconf0Base GUID: N/ABase MAC: N/AVersions: Current AvailableFW 16.28.2006 16.28.2006PXE 3.6.0102 3.6.0102UEFI 14.21.0017 14.21.0017Status: Up to dateroot@mellanox:/home/cicl/Downloads# flint -d /dev/mst/mt4119_pciconf0 -ocr -guid 0xe41d2d0300570fc0 -mac 0x0000e41d2d570fc0 sg-W- Firmware flash cache access is enabled. Running in this mode may cause the firmware to hang.Updating GUID section - OK-I- To load new configuration run mlxfwreset or reboot machineroot@mellanox:/home/cicl/Downloads# mlxfwreset -d /dev/mst/mt4119_pciconf0 -s -l 3 resetRequested reset level for device, /dev/mst/mt4119_pciconf0:3: Driver restart and PCI resetContinue with reset?[y/N] y-I- Sending Reset Command To Fw -Done-I- Resetting PCI -Done-I- Restarting MST -Done-I- FW was loaded successfully.root@mellanox:/home/cicl/Downloads# ./mlxupQuerying Mellanox devices firmware …Device #1:Device Type: ConnectX5Part Number: MNV303212A-ADL_Ax_BxDescription: Innova-2 Flex for Application Acceleration; ConnectX-5 Eth Adapter dual Port SFP 25GbE; Xilinx KU15P; PCI4.0 x8; HHHL; active cooling tall bracket; ROHS R6PSID: MT_0000000158PCI Device Name: /dev/mst/mt4119_pciconf0Base GUID: N/ABase MAC: N/AVersions: Current AvailableFW 16.28.2006 16.28.2006PXE 3.6.0102 3.6.0102UEFI 14.21.0017 14.21.0017Status: Up to datePowered by Discourse, best viewed with JavaScript enabled"
189,ipoib-enhanced-mode-issue,"Hello everybody,According to the documentation (https://docs.nvidia.com/networking/pages/viewpage.action?pageId=25155266), I quote “Enhanced IPoIB feature enables offloading ULP basic capabilities to a lower vendor specific driver, in order to optimize IPoIB data path”.The first thing I’d like to ask is why I am stuck on MTU 2044 when switched in enhanced mode (and then the ibX mode switch automatically to datagram), while the documentation talks about 4k MTU (but only defining it in a “partitions”… I do not even have opensm set-up to define a partition; subnet manager is running on fabric switch). Of course all ports of my IB Switch have MTU 4k.The second question is why I am getting poorer CPU-System performances (I mean higher CPU system) and lower bandwidth in datagram/enhanced, compared to connected/not-enhanced (I could guess the low BW is caused by low MTU… but I have not much experience on this topic; trying to learning right now).My HW is Nvidia ConnectX-5 on one node, ConnectX-6 on the other node, OFED 5.8-1.1.2.1, RH7.9, 3.10.0-1160.49.1 kernel, FDR cables, EDR Nvidia fabric switch.To check that I am in enhanced mode I do:[root@sf-daq-8 ~]# cat /sys/class/net/ib0/modeHi - assuming your Switch that is running the SM is using MLNX-OS (the only option :)) - you can configure the default partition for 4K mtu with the below config:(config)# ib partitions Default mtu 4K force
https://docs.nvidia.com/networking/display/MLNXOSv3105000/SM+Commands#SMCommands-mtuMorning Eddies,indeed it was effective: instantly, after issuing that command from within the MLNX-OS shell, the ibX ports of my nodes switched to 4092 MTU themselves (as expected).Thanks a lot for the fast support.
I guess the case can be closed, as for now I need to repeat my throughput tests with the extended MTU.
In case, if needed, I will open another ticket more specific to enhanced ipoib mode.Regards,Alvise DorigoPowered by Discourse, best viewed with JavaScript enabled"
190,linux-soft-lockup-in-mellanox-driver,"Hello, we are seeing a lot of softlockups in mellanox driver. Just wondering if its a known issue or what might be causing it. it often happens after this error.kernel: [736550.054087] mlx5_core 0000:41:00.1 mcx3p1: Failed to get min RX wqes on Channel[20] RQN[0x26e9] wq cur_sz(1) min_rx_wqes(2)Jun 30 00:21:57 bcn01-data01 kernel: [736550.054091] mlx5_core 0000:41:00.1 mcx3p1: RX timeout on channel: 20, ICOSQ: 0x26e7 RQ: 0x26e9, CQ: 0x48dJun 30 00:21:57 bcn01-data01 kernel: [736550.065995] mlx5_core 0000:41:00.1 mcx3p1: EQ 0x1b: Cons = 0x8ca620b, irqn = 0xc7and EIP always points to “mlx5e_poll_ico_cq+0xda/0x380” function.we are using Debian.Linux host 5.10.0-7-amd64 #1 SMP Debian 5.10.40-1 (2021-05-28) x86_64 GNU/Linuxafter this Soft lockup occurs and does not recover until the system is rebooted.Jun 30 00:21:58 bcn01-data01 kernel: [736551.054043] rcu: INFO: rcu_sched detected stalls on CPUs/tasks:Hi,Nothing that pops up with this prints. First, be sure you are using latest HCA firmware.If not using Mellanox OFED, raise discussion with OS vendor if using official vendor kernel or on the kernel forum if using the one from kernel.orgIf using AMD CPU, double check that you have iommu=pt in grub configuration.If using Mellanox OFED and latest firmware you might open an official support ticket, however you or your organization must have a valid support contract with Nvidia.Powered by Discourse, best viewed with JavaScript enabled"
191,cx-6-nof-offload-parameter-configuration,"How to configure the ‘param_offload_queues’ as well as  ‘param_inline_data_size’ when offload is enabled?
Is there a manual to explain how to configure all the parameters?Hello @caesarroot,Thank you for posting your query on our community. The below article shows how to configure NVMe-oF target offload for ConnectX-5(or later) adapters.
https://enterprise-support.nvidia.com/s/article/howto-configure-nvme-over-fabrics--nvme-of--target-offloadHope this helps.Thanks & Regards,
BhargaviPowered by Discourse, best viewed with JavaScript enabled"
192,crypto-enabled-disabled-blue-field2,"I am using the node with smartNIC on cloudlab. Here is the hardware description for it:
Device Type:      BlueField2
Part Number:      MBF2H516A-CENO_Ax
Description:      Bluefield-2 SmartNIC 100GbE Dual-Port QSFP56; PCIe Gen4 x16; Crypto Disabled; 16GB on-board DDR; 1GbE OOB management; FHHLIt has crypto disabled.
I wonder whether that is from the hardware restriction like the smartNIC does not have the hardware accelerator or if there is  a way to change it into a crypto enabled state.Hi @cxinyic ,Please refer to:“The BlueField-2 DPU crypto-enabled versions include a BlueField-2 IC which supports accelerated cryptographic operations. In addition to specialized instructions for bulk cryptographic processing in the Arm cores, an offload hardware engine accelerates public-key cryptography, and random number generation is enabled.”Best Regards,
ChenThanks for the information!Without specific context, it’s hard to determine the implications of this change. Users and stakeholders should closely review the provided link to understand the implications and rationale behind this decision. It’s crucial to stay informed and consider all aspects before forming any opinions. Transparency and communication are vital in such matters to ensure a smooth transition, if applicable.Powered by Discourse, best viewed with JavaScript enabled"
193,release-notes-for-nvidia-bright-cluster-manager-9-0-20,"Release notes for Bright 9.0-20== General ==
=New Features==Fixed Issues=== CMDaemon ==
=New Features==Fixed Issues=== Cluster Tools ==
=Improvements=== Machine Learning ==
=New Features=== cm-clone-install ==
=New Features== Fixed Issues=== cm-kubernetes-setup ==
=Fixed Issues=== cm-scale ==
=Fixed Issues=== cm-setup ==
=Fixed Issues=== cmsh ==
=New Features=== jupyter ==
=Fixed Issues=== openpbs20 ==
=Fixed Issues=== openpbs22.05 ==
=Fixed Issues=== pbspro2020 ==
=Fixed Issues=== pbspro2021 ==
=Fixed Issues=== pbspro2022 ==
=Fixed Issues=== slurm22.05 ==
=Improvements=Powered by Discourse, best viewed with JavaScript enabled"
194,mellanox-download-center-offline,"Hi there,
I’m trying to download the newest MLNX-OS for our  SB7700 switch.
That’s what I get when I click ‘Download latest’ on the product site:

image1078×469 26.6 KB

https://support.mellanox.com/s/productdetails/a2v50000000XoRSAA0/sb7700
Help appreciated.Powered by Discourse, best viewed with JavaScript enabled"
195,setting-cx4-lx-adapter-in-mode-switchdev,"Hi,Trying to get offloading (vlan) to work, but can’t seem to get the adapter in to mode switchdev,
Running Ubuntu 20.04, latest firmware on the adapter. Tried to remove the adapter from openvswitch, but it reveals the same result. I know the openvswitch version is outdated for offloading but does it matter when i am only trying to set the card in the correct mode?
Followed an openstack based guide, have a working SR-IOV setup failing on the step 2 (setting mode switchdev).
https://docs.openstack.org/neutron/latest/admin/config-ovs-offload.html#configure-open-vswitch-hardware-offloadingroot@aio1:~# ofed_info -s
mlnx-en-5.9-0.5.6.0:root@aio1:~# uname -r
5.4.0-139-genericroot@aio1:~# devlink dev eswitch set pci/0000:04:00.1 mode switchdev
Error: mlx5_core: Failed setting eswitch to offloads.
devlink answers: Invalid argument
root@aio1:~#root@aio1:~# ethtool -i enp4s0f1np1
driver: mlx5_core
version: 5.9-0.5.5
firmware-version: 14.32.1010 (MT_2420110034)
expansion-rom-version:
bus-info: 0000:04:00.1
supports-statistics: yes
supports-test: yes
supports-eeprom-access: no
supports-register-dump: no
supports-priv-flags: yes
root@aio1:~# dpkg -l | grep iproute
ii  iproute2                              5.5.0-1ubuntu1                        amd64        networking and traffic control tools
root@aio1:~# dpkg -l openvswitch-switch
Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
||/ Name               Version                Architecture Description
++±==================-======================-============-===================================
ii  openvswitch-switch 2.16.4-0ubuntu1~cloud0 amd64        Open vSwitch switch implementationsGreatful for any tips or pointers,
Wbr
///MathiasHi Mathias,Please ensure all VF devices are unbound before setting the PF device in switchdev mode. For example:echo 0000:04:00.2 > /sys/bus/pci/drivers/mlx5_core/unbindThanks,
ChenHi,Thank you for your reply, this was very true on another card i did the same thing to.
Not until every last one of the VF’s was unbound could i change to switch mode on a ConnectX 6 DX. I’ll give the Cx4 lx a try once its no longer in use.
Thanks again,
WBR
///MathiasThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
196,rdma-in-windows-11,"Does SMB over RDMA work in Windows 11 (client) with ConnectX-6 cards? If yes, what drivers are required and how do I get this to work? I want to transfer data from workstation machines running Windows 11 to and from a NASHello Fabianstimpfle,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Yes, Windows 11 supports RDMA as part of SMBv2/3. For the ConnectX-6, you need to install the Win-OF2 driver which you can download from the following link → Mellanox OFED for Windows - WinOF / WinOF-2For configuration of your NAS, we recommend to reach out to your NAS provider to see if they support SMBv2/3 in their NAS OS.For the client config, please review the following UM section → https://docs.nvidia.com/networking/display/winof2v290/Storage+ProtocolsThank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
197,sha-offload-dpu-error,"I am trying to use the SHA_OFFLOAD engine from one bluefield-2 we have hosted in our server.I access to the /opt/mellanox/doca/infrastructure/doca_sha_offload_engine and when I run:
openssl engine dynamic -pre NO_VCHECK:1 -pre SO_PATH:${DOCA_DIR}/infrastructure/doca_sha_offload_engine/libdoca_sha_offload_engine.so -pre LOAD -vvv -t -cI get the next error:
(dynamic) Dynamic engine loading support
[Success]: NO_VCHECK:1
[Success]: SO_PATH:/opt/mellanox/doca//infrastructure/doca_sha_offload_engine/libdoca_sha_offload_engine.so
[Success]: LOAD
Loaded: (doca_sha_offload_engine) Openssl SHA offloading engine based on doca_sha
[SHA1, SHA256, SHA512]
[16:05:10:446557][DOCA][ERR][DOCA_SHA_OFFLOAD_LIB:380]: No suitable DOCA device found!
[16:05:10:496083][DOCA][ERR][DOCA_SHA_OFFLOAD_LIB:380]: No suitable DOCA device found!
[ unavailable ]
set_pci_addr: set the pci address of the doca_sha_engine, for example, 03:00.0
(input flags): STRING
What can I do to solve that bug?Powered by Discourse, best viewed with JavaScript enabled"
198,why-does-sn2410-show-higher-speeds-available-than-label-suggests,"I have an SN2410 (MSN2410-BB2F) which says that it has 48 10GbE ports and 8 100GbE ports. However, in switch management, I am able to set the SFP28 ports to 25Gb and the ports appear to connect at these speeds. Would this have been an upgrade purchased by the previous owner of the switch, or perhaps an upgrade unlocked with a later version of the software? Would the switch have the backplane necessary to accommodate 25Gb connections?Hello NewtonSN2410 support up to 25G in the first 48 ports. The backplane already accommodates this speeds . You can find additional data from the switch in its data sheethttps://www.mellanox.com/related-docs/prod_eth_switches/PB_SN2410.pdfAdolfoPerhaps I should have said SN2410B. The data sheet suggests that my switch model is 10GbE/100GbE. Does the MSN2410-BB2F support 25GbE? I just want to be sure before deployment.Likewise, I have another MSN2410-BB2F which says 10GbE/40GbE and I don’t even see that as an available option. Similarly, it appears to function at 25GbE/100GbE.Thank you!Powered by Discourse, best viewed with JavaScript enabled"
199,getting-started-with-doca-for-connectx-6-dx,"Looking at DOCA 1.5 LTS release notes it mentions DOCA onwards supports ConnectX. However when I’m trying to install DOCA through SDK  I dont see any option for ConnectX as host. Can anybody guide or refer me to the resources which I might have missed or overlooked.I have a machine with ConnectX 6 card that Im trying to setup for DPDK based applicationThanksI guess you misinterpreted the saying. As far as I know, DOCA is for the BlueField that embed ConnectX NICs. The BF2 has a ConnectX 6 DX, and the 3 has a CX7.If you only have a ConnectX NIC the right path is to use DPDK directly, and installing the OFED instead of DOCA.Powered by Discourse, best viewed with JavaScript enabled"
200,sft-init-failed,"When I was trying to run the doca url filter on host through vf.The Trouble shooting give suggestion this error below may be relevant to too many cores.
sudo ./doca_url_filter -a 01:00.0 -a 0000:01:00.3 -a 0000:01:00.4 -l 0,1 – -p
[sudo] password for egaoyuc:
EAL: Detected 56 lcore(s)
EAL: Detected 2 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘VA’
EAL: 2048 hugepages of size 2097152 reserved, but no mounted hugetlbfs found for that size
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL: Probe PCI driver: mlx5_pci (15b3:a2d6) device: 0000:01:00.0 (socket 0)
EAL: Probe PCI driver: mlx5_pci (15b3:101e) device: 0000:01:00.3 (socket 0)
EAL: Probe PCI driver: mlx5_pci (15b3:101e) device: 0000:01:00.4 (socket 0)
EAL: No legacy callbacks, legacy socket not created
port-0: SFT init failed err=-12
[11:49:25:811302][DOCA][ERR][NUTILS:286]: SFT init failed, ret=-12But when I  do this -l 0
sudo ./doca_url_filter -a 01:00.0 -a 0000:01:00.3 -a 0000:01:00.4 -l 0 – -p
EAL: Detected 56 lcore(s)
EAL: Detected 2 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘VA’
EAL: 2048 hugepages of size 2097152 reserved, but no mounted hugetlbfs found for that size
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL: Probe PCI driver: mlx5_pci (15b3:a2d6) device: 0000:01:00.0 (socket 0)
EAL: Probe PCI driver: mlx5_pci (15b3:101e) device: 0000:01:00.3 (socket 0)
EAL: Probe PCI driver: mlx5_pci (15b3:101e) device: 0000:01:00.4 (socket 0)
EAL: No legacy callbacks, legacy socket not created
[11:54:06:051766][DOCA][ERR][NUTILS:622]: At least 2 cores are needed for the application to run, available_cores=1
[11:54:06:051794][DOCA][ERR][URL_FILTER:275]: Failed to update application ports and queues: DOCA Driver call failure
It suggests this way. Can anyone help?After trying, I set the running command as :
./doca_url_filter -a 0000:01:00.3,class=regex:eth,representor=[196609],sft_en=0 -a 0000:01:00.4,class=regex:eth,representor=[196610],sft_en=0 -l 0-7 – -pAnd now the new error is :

image1461×316 21.7 KB
Help please!mark,same questionPowered by Discourse, best viewed with JavaScript enabled"
201,connectx-6dx-maximum-number-of-tls-ipsec-offloads,"What is the maximum number of ipsec tunnels that can be offloaded on ConnectX.6DX?what about TLS Connection?Hi Alivin,Please refer to the below guide :https://community.mellanox.com/s/article/ConnectX-6DX-Bluefield-2-IPsec-HW-Full-Offload-Configuration-GuideThanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
202,how-to-acl-log,"I want to output a processing message for the ACL to the syslog. Is it possible?I need advice on NCLU or CLI commands.could not find the option for logging in the NCLU command.i using Cumulus Linux 4.1This is not currently possible as far as I know.
ACLs are defined in the control-plane and then offloaded to the ASIC for processing. In linux it is possible to build log messages which are triggered when a packet is processed matching the rule but as the processing is done by the ASIC in Cumulus, the ASIC does not recognize the request to generate an ACL/Match log rule.Powered by Discourse, best viewed with JavaScript enabled"
203,driver-v4-9-5-1-0-module-verification-failed-signature-and-or-required-key-missing-tainting-kernel,"I’m attempting to install the Mellanox drivers certified to be used with ConnectX-3 card on Redhat 8.6 for a HPC cluster running Bright 9.2 image.  The resulting driver does not load into the kernel and I cannot see the infiniband device.  The error messages are below.  The driver is 4.9-5.1.0mlx_compat: loading out-of-tree module taints kernel.
mlx_compat: module verification failed: signature and/or required key missing - tainting kernelMake sure you use kernel version is 4.18.0-372.9.1.el8.x86_64.And not enable uefi scure boot or try sign kernel module.https://docs.nvidia.com/networking/display/MLNXOFEDv495100/UEFI+Secure+BootI have verified my kernel is: 4.18.0-372.19.1.el8_6.x86_64I tried the instructions in the link provided, but I’m still not able to get this to work.
The first step in the instructions says to import the public key module, which I have done. I was able to reboot, it asked for the password for the public key, and I provided that.  The kernel is still tainted.  I then tried to re-install the driver.  Still tainted.I then tried to follow the instruction to strip the signature from the kernel modules, but it’s unable to work because the command given doesn’t return any results.rpm -qa | grep -E “kernel-ib|mlnx-ofa_kernel|iser|srp|knem|mlnx-rds|mlnx-nfsrdma|mlnx-nvme|mlnx-rdma-rxe” | xargs rpm -ql | grep “.ko$”Results in nothing returned, therefore the strip command doesn’t have anything to work with.I’m not sure where I need to go from here. I don’t see how to “resign” the module from these instructions, if that’s an option?Try disable UEFI SCURE boot from BIOS. Or need re-config kernel to disable scure boot kernel lock down.Have you check if the driver module eg. mlx5_core.ko load failure?Usually if you enable scure boot on kernel it will just WARNING tainted. And not prevent load driver, unless enable scure boot combine with kernel lockdown.I have verified in the BIOS that secure boot is NOT enabled.  I also verified this in Linux with /usr/bin/mokutil --sb-state which comes back with SecureBoot disabled.After installation of the driver, tried running hca_self_test.ofed which comes back with errors---- Performing Adapter Device Self Test ----
Number of CAs Detected … 1
PCI Device Check … PASS
Host Driver RPM Check … FAIL
REASON: no RPMs found for currently booted kernel 4.18.0-372.19.1.el8_6.x86_64
Kernel Arch … x86_64
Host Driver Version … NA
Firmware Check on CA #0 (VPI) … NA
Host Driver Initialization … NA
Number of CA Ports Active … NA
Error Counter Check … NA
Kernel Syslog Check … NA
Node GUID on CA #0 (VPI) … NA
------------------ DONE ---------------------lsmod | grep mlx
mlx5_core            1417216  0
mlxfw                  24576  1 mlx5_core
tls                   102400  1 mlx5_core
mlx4_en               159744  0
mlx4_ib                16384  0
mlx4_core             413696  1 mlx4_en
mlx_compat             16384  4 mlx4_core,mlx4_ib,mlx4_en,mlx5_coreAttempted to re-install the driver with --add-kernel-support which failsNote: This program will create mlnx-en TGZ for rhel8.6 under /tmp/mlnx-en-4.9-5.1.0.0-4.18.0-372.19.1.el8_6.x86_64 directory.
See log file /tmp/mlnx-en-4.9-5.1.0.0-4.18.0-372.19.1.el8_6.x86_64/mlnx_iso.7685_logs/mlnx_ofed_iso.7685.logChecking if all needed packages are installed…
Building mlnx-en RPMS . Please wait…ERROR: Failed executing “MLNX_EN_SRC-4.9-5.1.0.0/install.pl --tmpdir /tmp/mlnx-en-4.9-5.1.0.0-4.18.0-372.19.1.el8_6.x86_64/mlnx_iso.7685_logs --kernel-only --kernel 4.18.0-372.19.1.el8_6.x86_64 --kernel-sources /lib/modules/4.18.0-372.19.1.el8_6.x86_64/build --builddir /tmp/mlnx-en-4.9-5.1.0.0-4.18.0-372.19.1.el8_6.x86_64/mlnx_iso.7685 --disable-kmp --build-only --distro rhel8.6”
ERROR: See /tmp/mlnx-en-4.9-5.1.0.0-4.18.0-372.19.1.el8_6.x86_64/mlnx_iso.7685_logs/mlnx_ofed_iso.7685.log
Failed to build mlnx-en for 4.18.0-372.19.1.el8_6.x86_64Numerous errors in the debug log states that there are attempts to redefine kvmalloc in all sorts of source files causing make to fail.:21: error: redefinition of ‘kvmalloc’
static inline void *kvmalloc(size_t size, gfp_t flags)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
204,migration-using-rdma-with-mellanox-connectx3-pro-en,"Hello everyone,I have encountered a problem regarding the migration process using RDMA, which seems to have a longer total time compared to TCP.Here are the relevant details:Steps taken:I’m uncertain about what might be causing this discrepancy. Any insights or suggestions would be greatly appreciated. Thank you.Powered by Discourse, best viewed with JavaScript enabled"
205,connectx-5-and-raw-ethernet-bw-in-perftest-suite-basic-config-question,"In GitHub - linux-rdma/perftest: Infiniband Verbs Performance Tests there’s the raw_ethernet_bw task. My questions are:Since raw_ethernet_bw is sending raw UDP/TCP packets (not over IB) the MTU size needn’t be restricted to 256/512/1024/2048/4096 which are IB restrictions correct?I have a ConnectX-5 NIC. So should I be calling, for example, mlx5dv_query_port (/usr/includes/infiniband/mlx5dv.h) to get port info or should I be calling ibv_query_port (/usr/includes/infiniband/verbs.h). Is mlx5dv.h just a helper layer on top of ibverbs?Since perftest’s config and setup mixes together IB and ethernet, it’s hard to tell what applies for ethernet and what’s ultimately meant for IB only.Hi @7532yahoo,Regards,
ChenPowered by Discourse, best viewed with JavaScript enabled"
206,implementation-of-fec-and-digital-fiber-nonlinear-distortion-mitigation,"Hello for research purposesis there any possibility in any of mellanox network card that we can implement the FEC and digital fiber distortion mitigation on GPU, using Mellanox card output or digital pre distortion mitigation on transmitted signal and pre FEC coding on GPU on transmitter side on GPU and then send them to Mellanox card?thank you so muchHi,Could you please open support ticket via Networking-support@nvidia.comand explain the motivation and what exactly are you trying to achieve ?Thanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
207,nvidia-connectx6-smartnic,"I had a few questions regarding the Nvidia ConnectX6 smartNIC.Powered by Discourse, best viewed with JavaScript enabled"
208,missing-directory-with-sr-iov-on-connectx-4-infinband-with-centos8-and-ofed-5-1-2-5-8-0,"I’ve been following https://support.mellanox.com/s/article/howto-configure-sr-iov-for-connect-ib-connectx-4-with-kvm–infiniband-x so I already know about these instructions.I’m on step 6 where the GUID, port and policy are set.However the target ‘directory’ that holds the VF configurations is missing:We have a subnet manager running elsewhere, but have tried with opensmd running and not running with the same resullt.The base system is CentOS8:The driver version is:We are not going to upgrade the drivers to 5.2.x as we have other systems on CentOS7 and older drivers with working SR-IOV, so we’re just trying to work through the CentOS8 changes first.Hi,Please refer to the updated documentation of SR-IOV in the latest MLNX_OFED 5.2-2.2.0.0https://docs.mellanox.com/pages/viewpage.action?pageId=43718746Thanks,SamerThis answer was not helpful. I’ve already read this document and the document that is specific for the driver I am using, they’re like the first hits when googling SR-IOV & InfiniBand issues. It did not contain any troubleshooting instructions that would have diagnosed the problem. The document I linked previously actually contains a few more steps that are helpful (such as how to reset the card).I have solved this issue BTW, the hosts were booting the wrong kernel and still using the CentOS ‘inbox’ driver. The key was figuring that out.Response from a ‘good’ host:[root@node001 ~]# modinfo mlx5_core | grep signersigner: Mellanox Technologies signing keyResponse from a ‘bad’ host with missing SR-IOV directory:[root@node002 ~]# modinfo mlx5_core | grep signersigner: CentOS kernel signing keyThese hosts boot using PXE so I went and checked the host that served their boot images and checked and fixed that it was serving the correct kernel to them. While this part is out-of-scope for Mellanox support, I do think it’s reasonable that they could provide troubleshooting steps to establish if your host is running the correct kernel and loading the correct kernel modules, rather that just ‘RTFM’Powered by Discourse, best viewed with JavaScript enabled"
209,innova-2-flex-open-bundle-download,"Hi! Sorry again for disturb.I’m was unable to download this bundle. Where is located?Thanks.Anyone can help me?Hi,Here the link,Maximize network efficiency and scalability with the NVIDIA Mellanox Innova-2 Flex Open Programmable SmartNIC. The advanced ConnectX®-5 InfiniBand and Ethernet network controller ASIC achieves maximum efficiency in RoCE, ASAP² and SR-IOV.RegardsMarcThank you! There was a problem with my browser that’s not shown the download page!Thank youPowered by Discourse, best viewed with JavaScript enabled"
210,problem-running-doca-reference-application-dns-filter-segmentation-fault,"I’m trying to run DNS Filter descripted in NVIDIA DOCA DNS Filter Application Guide on Bluefield2  but failed.
I followed instruction in NVIDIA BlueField DPU Scalable Function User Guide to create two SFs and ran /opt/mellanox/doca/applications/dns_filter/bin/doca_dns_filter, it failed to start and throws a Segmentation Fault error.The command I used to run the app:
cd /opt/mellanox/doca/applications/dns_filter/bin
sudo ./doca_dns_filter --json dns_filter_params.jsonHere is the output of doca_dns_filter:
EAL: Detected 8 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL: No legacy callbacks, legacy socket not created
[04:28:16:517144][DOCA][DBG][NUTILS:507]: Port 0 MAC: 02 25 f2 8d a2 4c
[04:28:16:555278][DOCA][DBG][NUTILS:507]: Port 1 MAC: 02 25 f2 8d a2 5c
[04:28:16:560638][DOCA][INF][engine_model:73]: engine model defined with mode=vnf
[04:28:16:560746][DOCA][INF][engine_model:75]: engine model defined with nr_pipe_queues=3
[04:28:16:560784][DOCA][INF][engine_model:76]: engine model defined with pipe_queue_depth=0
[04:28:16:561143][DOCA][INF][engine_field_mapping:96]: Engine field mapping initialized with 3 focus 11 protocols
[04:28:16:561205][DOCA][INF][engine_shared_resources:94]: Engine shared resources initialized successfully
[04:28:16:561254][DOCA][INF][dpdk_engine:433]: queue depth is zero, set it to default 128.
[04:28:16:561322][DOCA][INF][encap_table:119]: encap table created
[04:28:16:561421][DOCA][DBG][dpdk_table_hws:858]: Initialized dpdk table work module to be HW steering
[04:28:16:561453][DOCA][INF][dpdk_table:70]: Initializing dpdk table successfully
[04:28:16:561476][DOCA][DBG][dpdk_flow_hws:33]: Initialized dpdk flow work module to be HW steering
[04:28:16:561503][DOCA][INF][dpdk_flow:82]: Initializing dpdk flow successfully
[04:28:16:561539][DOCA][INF][engine_shared_resources:133]: Allocated 16 shared resources of type 2
[04:28:16:561570][DOCA][INF][dpdk_resource_manager:184]: Dpdk resource manager register completed
[04:28:16:561631][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.eth.dst_mac, offset=0)
[04:28:16:561657][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.eth.src_mac, offset=6)
[04:28:16:561688][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.eth.type, offset=12)
[04:28:16:561716][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.eth.dst_mac, offset=0)
[04:28:16:561747][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.eth.src_mac, offset=6)
[04:28:16:561778][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.eth.type, offset=12)
[04:28:16:561811][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.eth_vlan.tci, offset=0)
[04:28:16:561830][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.eth_vlan.tci, offset=0)
[04:28:16:561862][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv4.src_ip, offset=12)
[04:28:16:561887][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv4.dst_ip, offset=16)
[04:28:16:561914][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv4.next_proto, offset=9)
[04:28:16:561942][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv4.src_ip, offset=12)
[04:28:16:561970][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv4.dst_ip, offset=16)
[04:28:16:561994][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv4.next_proto, offset=9)
[04:28:16:562018][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv6.src_ip, offset=8)
[04:28:16:562039][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv6.dst_ip, offset=24)
[04:28:16:562056][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.ipv6.next_proto, offset=6)
[04:28:16:562079][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv6.src_ip, offset=8)
[04:28:16:562110][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv6.dst_ip, offset=24)
[04:28:16:562136][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.ipv6.next_proto, offset=6)
[04:28:16:562160][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.udp.src_port, offset=0)
[04:28:16:562182][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.udp.dst_port, offset=2)
[04:28:16:562210][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.udp.src_port, offset=0)
[04:28:16:562237][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.udp.dst_port, offset=2)
[04:28:16:562267][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.tcp.src_port, offset=0)
[04:28:16:562296][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.tcp.dst_port, offset=2)
[04:28:16:562320][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.outer.tcp.flags, offset=13)
[04:28:16:562343][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.tcp.src_port, offset=0)
[04:28:16:562360][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.tcp.dst_port, offset=2)
[04:28:16:562382][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.inner.tcp.flags, offset=13)
[04:28:16:562409][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.tunnel.vxlan.vni, offset=4)
[04:28:16:562438][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.tunnel.gre.key, offset=0)
[04:28:16:562462][DOCA][DBG][dpdk_layer:58]: Registered dpdk field opcode=match.packet.tunnel.gtp.teid, offset=4)
[04:28:16:562485][DOCA][INF][dpdk_layer:260]: Dpdk layer register completed
[04:28:16:562525][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.eth.dst_mac, offset=42, len=6)
[04:28:16:562553][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.eth.src_mac, offset=36, len=6)
[04:28:16:562587][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.eth.type, offset=48, len=2)
[04:28:16:562607][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.eth.dst_mac, offset=150, len=6)
[04:28:16:562638][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.eth.src_mac, offset=144, len=6)
[04:28:16:562667][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.eth.type, offset=156, len=2)
[04:28:16:562700][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.eth_vlan.tci, offset=50, len=2)
[04:28:16:562737][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.eth_vlan.tci, offset=158, len=2)
[04:28:16:562767][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv4.src_ip, offset=56, len=4)
[04:28:16:562798][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv4.dst_ip, offset=76, len=4)
[04:28:16:562825][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv4.next_proto, offset=92, len=1)
[04:28:16:562856][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv4.src_ip, offset=164, len=4)
[04:28:16:562889][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv4.dst_ip, offset=184, len=4)
[04:28:16:562918][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv4.next_proto, offset=200, len=1)
[04:28:16:562952][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv6.src_ip, offset=56, len=16)
[04:28:16:562972][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv6.dst_ip, offset=76, len=16)
[04:28:16:563004][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.ipv6.next_proto, offset=92, len=1)
[04:28:16:563032][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv6.src_ip, offset=164, len=16)
[04:28:16:563064][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv6.dst_ip, offset=184, len=16)
[04:28:16:563093][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.ipv6.next_proto, offset=200, len=1)
[04:28:16:563127][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.udp.src_port, offset=94, len=2)
[04:28:16:563162][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.udp.dst_port, offset=96, len=2)
[04:28:16:563194][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.udp.src_port, offset=202, len=2)
[04:28:16:563222][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.udp.dst_port, offset=204, len=2)
[04:28:16:563256][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.tcp.src_port, offset=94, len=2)
[04:28:16:563276][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.tcp.dst_port, offset=96, len=2)
[04:28:16:563302][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.outer.tcp.flags, offset=93, len=1)
[04:28:16:563329][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.tcp.src_port, offset=202, len=2)
[04:28:16:563357][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.tcp.dst_port, offset=204, len=2)
[04:28:16:563389][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.inner.tcp.flags, offset=201, len=1)
[04:28:16:563418][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.vxlan.vni, offset=104, len=3)
[04:28:16:563451][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.gre.key, offset=108, len=4)
[04:28:16:563486][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.gre.protocol, offset=106, len=2)
[04:28:16:563516][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.gtp.teid, offset=104, len=4)
[04:28:16:563552][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.nisp.hdr, offset=104, len=40)
[04:28:16:563586][DOCA][DBG][doca_flow_layer:66]: Registered field opcode=match.packet.tunnel.audp.hdr, offset=104, len=24)
[04:28:16:563693][DOCA][INF][doca_flow_layer:455]: Doca flow layer initialized
[04:28:16:563729][DOCA][INF][doca_flow:407]: Doca flow initialized successfully
[04:28:16:564818][DOCA][INF][utils_hash_table:123]: hash table a_tmplt_t port 0 created
[04:28:16:564952][DOCA][INF][utils_hash_table:123]: hash table p_tmplt_t port 0 created
[04:28:16:565065][DOCA][INF][utils_hash_table:123]: hash table dpdk_tbl_mgr port 0 created
[04:28:16:565246][DOCA][INF][utils_hash_table:123]: hash table grp_fwd port 0 created
[04:28:16:565279][DOCA][INF][dpdk_port:167]: Dpdk port 0 initialized successfully with 4 queues
Segmentation faultand params.json used to run the app:
{
“doca_dpdk_flags”:{
// -a - Add a device to the allow list.
“devices”:[
{
“device”: “sf”,
“id”: “4”,
“sft”: false,
“hws”: true,
},
{
“device”: “sf”,
“id”: “5”,
“sft”: false,
“hws”: true,
},
],},
“doca_general_flags”:{
// -l - sets the log level for the application DEBUG=60, CRITICAL=20
“log-level”: 60,
},
“doca_program_flags”:{}
}My DOCA Version is 1.5.0055
And some SF info which may be helpful:Output of running “devlink dev show”:
pci/0000:03:00.0
pci/0000:03:00.1
auxiliary/mlx5_core.sf.2
auxiliary/mlx5_core.sf.3
auxiliary/mlx5_core.sf.4
auxiliary/mlx5_core.sf.5Output of running “/opt/mellanox/iproute2/sbin/mlxdevm port show”
pci/0000:03:00.0/229408: type eth netdev en3f0pf0sf0 flavour pcisf controller 0 pfnum 0 sfnum 0
function:
hw_addr 02:0a:c3:28:ee:37 state active opstate attached roce true max_uc_macs 128 trust off
pci/0000:03:00.0/229409: type eth netdev en3f0pf0sf4 flavour pcisf controller 0 pfnum 0 sfnum 4
function:
hw_addr 02:25:f2:8d:a2:4c state active opstate attached roce true max_uc_macs 128 trust on
pci/0000:03:00.0/229410: type eth netdev en3f0pf0sf5 flavour pcisf controller 0 pfnum 0 sfnum 5
function:
hw_addr 02:25:f2:8d:a2:5c state active opstate attached roce true max_uc_macs 128 trust on
pci/0000:03:00.1/294944: type eth netdev en3f1pf1sf0 flavour pcisf controller 0 pfnum 1 sfnum 0
function:
hw_addr 02:e8:ed:48:15:07 state active opstate attached roce true max_uc_macs 128 trust offAnd I’m sure that I compiled regex rules to /tmp and mlx-regex service works properly.Anyone know what’s wrong with this and how to make the app run correctly?
Any comments will be much appreciated.Powered by Discourse, best viewed with JavaScript enabled"
211,mst-sdk-api,"Hi.
Where can I find the API for the mst package (mtcr library)?
I’m fine if you will point me the git project for mst tools like i2c
Thanks.Hello @dimax.main,You can refer to the official GitHub repository of the mstflint project, the mstflint project provides tools and libraries for managing and configuring Mellanox devices, including the mst package.You can find the mstflint project on GitHub using the following link:Mstflint - an open source version of MFT (Mellanox Firmware Tools) - GitHub - Mellanox/mstflint: Mstflint - an open source version of MFT (Mellanox Firmware Tools)Hope this answers your question.Regards,
BhargaviThanks.
It helped a bit but it has nothing about the mtusb device or i2c access.Anyway I managed to build my application using WinMFT SDK.
But I still have an issue with opening a mtusb device, although I know that device is connected and mst status can see it:
My application name is sprutm. I placed it into the WinMFT folder. I run command shell as Administrator:
image.png981×629 10.5 KB
What do you think can be an issue?
I believe it is something with security on Windows.
Let me know if you need more details.
Thanks. Regards.Hello.
Any updates here? We can not move forward with our development without your help.
Thanks.App refers to the device linux path.Try only mtusb-1 without /dev/mst.Thanks but it does not help. From what I can see both ways are supported but for none works:

image981×576 16 KB
Because I still can’t find any reference on how to use MFT package with mtusb-1 I’m not sure that below code is correct:Also it is strange that errno is not set.Can you please scan the mtusb device?mlxi2c -d <mtusb_dev> scanHi. I run it with MTUSB-1 connected to USB but not connected to the Mellanox board.

image981×576 8.24 KB
But I do not understand how it can help.
My issue is not on i2c side. I can not open mtusb device with mopen. mopen fails without errno update but returns a zero file descriptor.Powered by Discourse, best viewed with JavaScript enabled"
212,issue-with-vlan-pids-vpid,"so we have a config, where we have openstack deployed, as following, where the hosts on bond0.1648 10.16.48.X/24 cannot ping their own gateway or each other. however all other vlans work as expected. i am trying to debug/fix the issue, could anyone with openstack knowledge / a cumulus please enlighten me if you see the mistake?from what i can tell, not being a cumulus expert, can abridge-pvid 1648
bridge-vids 1680 1696 1648 1672(contain the same untagged/tagged vlan ?I also noted that
vlan1648 shows   address-virtual 44:38:39:FF:00:02 10.16.48.1where all others showaddress-virtual 44:38:39:FF:00:03 10.16.72.1
address-virtual 44:38:39:FF:00:03 10.16.96.1
address-virtual 44:38:39:FF:00:07 10.16.64.1shouldnt they all be the same MAC ??best case scenerio from my perspective is
move all pvid to 1and all  address-virtual  to mac 44:38:39:FF:00:03Ive included both the openstack config, and below the cumulus config, any help or insight is welcome and appreciated.and cumulus hosts exampleinterface swp12
mtu 9000interface openstack-infra
bond-slaves swp12
bridge-pvid 1648
bridge-vids 1680 1696 1648 1672
clag-id 12
mstpctl-bpduguard yes
mstpctl-portadminedge yes
bond-lacp-bypass-allow 1
mtu 9000from what i can tell, not being a cumulus expert, can abridge-pvid 1648
bridge-vids 1680 1696 1648 1672(contain the same untagged/tagged vlan ?I also noted that
vlan1648 shows address-virtual 44:38:39:FF:00:02 10.16.48.1where all others showaddress-virtual 44:38:39:FF:00:03 10.16.72.1
address-virtual 44:38:39:FF:00:03 10.16.96.1
address-virtual 44:38:39:FF:00:07 10.16.64.1shouldnt they all be the same MAC ??best case scenerio from my perspective is
move all pvid to 1and all address-virtual to mac 44:38:39:FF:00:03Ive included both the openstack config, and below the cumulus config, any help or insight is welcome and appreciated.Hello ScottYour approach to change the PVID to 1 on those ports will most likelly address your connectivity issues, let me elaborate why:The command “bridge-pvid 1648” under the bond interface sets the cumulus switchport to process DOT1Q untagged a RX/TX packets and assign them to vlan 1648The command ""bridge-vids 1680 1696 1648 1672 "" under the bond interface sets the cumulus switchport to process DOT1Q and untagged traffic for vlans 1680, 1696, 1648 and 1672. It’s the list of vlans that should be allowed on the portIf you change pvid under the port(s) to 1, it will assign untagged packets to vlan 1, and  if the other “bridge-vid” command remains unchanged, it will accept and process taggeg packets for vlan1648. You server configuration makes me suspect you are tagging traffic for interface br-mgmt which has bond0.1648 assignedAs for the virtual mac addresses, there is no need to make them all “44:38:39:FF:00:03”. Changing the PVID should suffice. MAC OUI 44:38:39 is reserved by cumulus for Peerlink/Clag and the intend is to avoid having a host duplicate the MAC addressYou can find the vlan documentation and confiugraiton guide here:
https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-42/Layer-2/Ethernet-Bridging-VLANs/VLAN-aware-Bridge-Mode/Great thanks for the info, and the fast reply, ill give it a shotPowered by Discourse, best viewed with JavaScript enabled"
213,driver-for-mellanox-technologies-mt25408a0-fcc-qi-connectx,"Hi all,I want to learn some basics about RDMA and I got several machines that are equipped with very old NIC, which is “Mellanox Technologies MT25408A0-FCC-QI ConnectX”. However, I cannot find a proper driver for it in the internet. Does anyone have an idea of how can I proceed with this old hardware? Many thanks!Hello lixl666,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.ConnectX adapters are end-of-life and end-of-service for awhile now. Our latest drivers (MLNX_OFED/EN), the Linux Distros INBOX drivers and the latest UpStream kernel modules, do not contain any support for the ConnectX adapter anymore.You can try to see with loading the mlx4_core and and mlx4_en modules, if the adapter is configurable.Our recommendation is to find a more recent adapter like the ConnectX-4, which is still supported.Thank you and regards,
~NVIDIA Networking Technical SupportDear MvB,I have tried to install “MLNX_OFED_LINUX-4.9-0.1.7.0-ubuntu18.04-x86_64.iso” in Ubuntu 18.04. During the installation, an error occured for package “NEO-Host”:
image1410×314 112 KB
If I skip this package by ./mlnxofedinstall --force --without-neohost-backend, it can finish the installation. I just wonder the usage of this package. Is it OK that I skip the installation of this package?Thank you very much!Hello lixl666,You can skip this package as it will not support your adapter.As you mentioned that you are using a ConnectX adapter, MLNX_OFED 4.9 LTS has no support for your ConnectX adapter.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
214,questions-about-backup-configuration-switch-sn2010-in-rconfig,"
2953×549 53.3 KB

11176×591 104 KB
Greetings!I’m setting up the rConfig backup system for saving configuration Mellanox Model SN2010 switch. Only the first page of the configuration is saved.The connection template is taken from here:I added pagingCmd: “terminal length 0” and resetPagingCmd: “terminal length 999” commands to Connection Templates, but it didn’t help.Tell me, please, how can I configure the template so that the all pages of configuration is saved?If you compare it with the console, you can see that 23 of the 29 lines of the first page are saved.
3873×517 44.1 KB
try the below before running “show running-config”enable#config t(config) # no cli session paging enableThank you very much!i use this command:no cli default paging enableand it fixed. all config are saved.Powered by Discourse, best viewed with JavaScript enabled"
215,mellonox-switch-mellonx-performance-issues,"Hi all,we are working with the setup having the 3 mellanox cards(versions are mlx5_0)are connected over the Mellonox switch(MSN2100-CB2F Model MSN2100 Spectrum™ based 100GbE, 1U Open Ethernet Switch with MLNX-OS, 16 QSFP28 ports,2AC PSUsx86 2Core),when we pump the traffic from two  Mellonox hosts(In client mode) to one Mellonox host (in server mode )using the perftest(ib_write_bw)we are observed one of the Mellonox client,we observed the performance drop was ~3 to 4 Gbps and the other Mellonox client was giving  ~85 Gbps.we are also observed the one of the Mellonox client have the pkt_Seq_err counter  also changing from iteration to iteration.This experiment we are carried with in all the mellnox hosts PFC was enabled.Can anyone suggest me why this was happening.Hi Pkashire,have you enabled any sort of congestion control mechanism on the switch side?
not sure if you are using Onyx or Cumulus but please review:https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-50/Layer-1-and-Switch-Ports/Quality-of-Service/RDMA-over-Converged-Ethernet-RoCE/Or
Onyx:https://docs.nvidia.com/networking/pages/viewpage.action?pageId=71023238Powered by Discourse, best viewed with JavaScript enabled"
216,enabling-the-http-rest-api,"Problem
I am running Cumulus VX with in GNS3 and can connect just fine. But I’m having issues enabling the API. I keep getting this error after updating the nginx-restapi-chassis.conf file and running the test to check.nginx: [emerg] ""limit_req_zone"" directive is not allowed here in /etc/nginx/sites-available/nginx-restapi-chassis.conf:21SolutionTo enable the HTTP API, first make sure you are editing the correct file, which is named /etc/nginx/sites-available/nginx-restapi.conf (the file listed above is for chassis platforms only), then follow these steps:
enable_HTTP_API_for_NCLU926×685 77.6 KB
For more information, read the Cumulus Linux user guide.i am also facing same problem..Hey there did the solution work for you? You are editing the correct file?Thanks!Powered by Discourse, best viewed with JavaScript enabled"
217,unable-to-verify-integrity-of-bluefield-os-image,"Hello,The expected sha256sum of the DOCA Bluefield OS image DOCA_2.0.2_BSP_4.0.2_Ubuntu_22.04-6.23-04.prod.bfb is 1ef541b1f28b1d1681a0541a6cd26e548e6336b84b66212ececa38c919b0cfaf, according to https://developer.nvidia.com/networking/doca.However, an actual checksum is 66c426324db6477372414552ac9280001492fdcae7af13a5b036b2eecd66299b. I re-downloaded the image several times, but I got the same result.Does this reproduce on your computer?Thanks.PS 1: You can try downloading the image from NVIDIA DOCA SDK | NVIDIA Developer > 2.0.2 > DOCA + BlueField OS Ubuntu Server 22.04 Image.PS 2: I downloaded the image over HTTPS, so it is highly unlikely that a firewall or any other middleboxes modified the image.Powered by Discourse, best viewed with JavaScript enabled"
218,flashed-wrong-firmware-mhrh2a-xsr,"I recently bought a ConnectX-2 MHRH2A-XSR Card with the Intention to use it as a 10 GbE NIC. At first i was not able to get the Card or one of its Ports to work in Ethernet mode. Not realising at the time it was probably a driver problem (The newer OFED versions dont seem to support such old cards anymore), I tried to update the formware. On the official Firmware Download page I only saw a firmware for the MHRH2A-XTR. But I did read in a random forum, that the “T” only stands for “tall bracket” and the “S” for “short bracket”. That is why i just flashed the MHRH2A-XTR onto my card. Now I cant get the card to work in eth mode at all. I tried at least 5 OS (Debian, Ubuntu, windows in different versions) with different driver versions, but no matter what I try, the card wont work in eth mode. In Linux I get some type of “firmware error” after which the specified port is unusable and in windows almost the same behavior.Now: Could flashing the (slightly) wrong firmware be the problem? And if yes where do I get the right Firmware now?(Of course i wasnt smart enough to pull backup first…)Hello Moritz,Thank you for posting your inquiry on the NVIDIA Networking Community.The ConnectX-2 is an adapter which is EOL and EOS for a long time already. We recommend to move to a more recent adapter, for example ConnectX-4 or ConnectX-5I found the latest f/w version for your adapter. You can download through the following direct download link → https://content.mellanox.com/firmware/fw-ConnectX2-rel-2_9_1810-MHRH2A_A2.bin.zipThis firmware is for the following PSID:Image type: FS2FW Version: 2.9.1810MIC Version: 1.5.0Device ID: 26418Description: Node Port1 Port2 Sys imageGUIDs: 0002c9000100d050 0002c9000100d051 0002c9000100d052 0002c9000100d050MACs: 0002c9000001 0002c9000002VSD: n/aPSID: MT_0F90120008As this is an older adapter, configuration of the port for IB and Ethernet needs to be done through the ‘connectx_port_config’ utility.You can use the following link as reference → File: mlx4_release_notes.txt | Debian SourcesThank you and regards,~NVIDIA Networking Technical SupportHi!Thank you for your answer!I successfully flashed the new firmware onto the card. However I am not able to change the port type to eth on the card. I tried both the mlnx-en and the ofed driver, however both of them still produce this error:[ 961.669548] mlx4_core 0000:04:00.0: command 0x9 failed: fw status = 0x8[ 961.669593] mlx4_en: 0000:04:00.0: Port 2: Failed Initializing portI used the 4.9 LTS version for both drivers. I am starting to think that there is something physically wrong with this card. My card does not appear in the mlx4 release notes as one of the unsupported cards but it produces the given error anyway.Powered by Discourse, best viewed with JavaScript enabled"
219,the-device-does-not-seem-to-be-present-delaying-initialization-need-ifup-device-manually,"We have issues with bringing up Ethernet Mellanox MT27710 (ConnectX-4 Lx) upon reboot with CentOS 7.9.mlx5_ib and mlx5_core drivers are all installed and the ethernet is recognized. We can manually bring up the ethernet. Does the issue seem to be on the initial kernel runtime upon reboot?Any suggestions on how we can fix this?Hi,What kind of error do you see in your dmesg ?Do you use MOFED ?If yes which version ?Reboot your machine.Load the driver (/etc/init.d/openibd restart)Modifiy your initramfs, by running mkinitrd command to update the initramfs with the loaded kernel modules driverRebootIf it persists, please open a caseRegardsMarcPowered by Discourse, best viewed with JavaScript enabled"
220,ucx-error-with-driver-5-1-2-5-8-on-rhel-7-9,"I get an error when executing ‘ucx_info -d’ as normal user:ucx_info -d…[1608791980.432700] [drp-srcf-mon001:17816:0] ib_iface.c:961 UCX ERROR ibv_create_cq(cqe=4096) failed: Cannot allocate memory…Note that the same command looks OK when running as root:root> ucx_info -dCurrent setup:ethtool -i ib0driver: mlx5_core[ib_ipoib]version: 5.1-2.5.8firmware-version: 20.28.2006 (MT_0000000222)expansion-rom-version:bus-info: 0000:01:00.0supports-statistics: yessupports-test: yessupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: yesuname -aLinux drp-srcf-cmp034 3.10.0-1160.6.1.el7.x86_64 #1 SMP Wed Oct 21 13:44:38 EDT 2020 x86_64 x86_64 x86_64 GNU/Linuxrpm -qa | grep ucxucx-cma-1.9.0-1.51258.x86_64ucx-1.9.0-1.51258.x86_64ucx-knem-1.9.0-1.51258.x86_64ucx-devel-1.9.0-1.51258.x86_64ucx-rdmacm-1.9.0-1.51258.x86_64ucx-ib-1.9.0-1.51258.x86_64cat /etc/redhat-releaseRed Hat Enterprise Linux Server release 7.9 (Maipo)This error is currently preventing me from running mpirun using UCX.Thank you very much for your help in this matter,AmedeoHi,Can you provide the full output ?Is your issue similar to this one :### Describe the bug
ucx_info -d 
shows various errors (depending on the ucx v…ersion) on nodes with connectx-6 hca. I tried several versions of ucx but didn't succeed using it with connectx-6. Before posting lots of details: is connectx-6 supported at all? 

```
Some Errors for ucx_info:
ucx 1.4.0:
[1575906412.855268] [max-exfl200:137500:0]       ib_iface.c:947  UCX  ERROR Invalid active_width on mlx5_0:1: 16
ucx 1.6.1:
[1575906773.636572] [max-exfl200:138596:0]     ib_mlx5_dv.c:157  UCX  ERROR ibv_create_cq() failed: Invalid argument
ucx 1.7.0
[1575908314.020997] [max-exfl200:168320:0]       ib_iface.c:618  UCX  ERROR ibv_create_cq(cqe=4096) failed: Invalid argument


Short extract from ibv_devinfo -v:
hca_id:        mlx5_0
        transport:                        InfiniBand (0)
        fw_ver:                                20.26.1040
        node_guid:                        b859:9f03:004e:bd14
        sys_image_guid:                        b859:9f03:004e:bd14
        vendor_id:                        0x02c9
        vendor_part_id:                        4123
        hw_ver:                                0x0
        board_id:                        MT_0000000222
        phys_port_cnt:                        1
        max_mr_size:                        0xffffffffffffffff
        page_size_cap:                        0xfffffffffffff000
        max_qp:                                262144
        max_qp_wr:                        32768

```If yes, a workaround is suggested thereRegardsMarcHi,We are experiencing the same issue in one of our clusters with ConnectX-6 cards. Our configurations are as follows:In our case, the problem is not limited to UCX but also impacting other transports like OFI and verbs. We notice a lot of message in dmesg/syslog like these:andThe user-level MPI error is:We currently have no way to mitigate the situation except rebooting the affected nodes. The problem seems to appear randomly on a subset of the nodes.Is this a known firmware/ofed issue and what triggers it?Any help will be appreciated. Please let us know if you need more info.Best regards,Amiya.We fixed this error on our system by adding the file /etc/security/limits.d/rdma.conf containing:soft memlock unlimitedhard memlock unlimitedPowered by Discourse, best viewed with JavaScript enabled"
221,gtp-tunnel-cant-be-decap,"I try to use DOCA(1.5.1) to decap GTPU but I got error when I match the tunnel type equal GTPUI set the encap mode to none and also set FLEX_PARSER_PROFILE_ENABLE=3
image1119×155 44.2 KB
Powered by Discourse, best viewed with JavaScript enabled"
222,how-to-improve-the-performance-of-flow-steering-on-connectx-5,"On one of our tests, we realized that using flow steering was significantly reducing the performance of our application. We have been able to confirm that with a simple benchmarking tool that simply counts packets. We are testing with an MCX516A-CCA_Ax NIC.In our test, we are sending 100Mpps to our test machine. Without any flow rules, we are able to receive 100Mpps. But when we set up flow steering and flow isolation, we only get about 54Mpps. In this case, we simply set up a flow matching all UDP traffic with an RSS action with 16 queues.Looking at the statistics, we can see rx_discards_phy for the missing packets, apparently indicating a hardware bottleneck.Until now, our application was never fast enough to witness this bottleneck. And we operated under the assumption that flow steering would be able to reach line rate. Apparently, we may have been wrong.So here are my questions:Is these limitations reasonable or should we be able to receive 100Mpps even with flow steering rules?(if not) Is there any DPDK or Mellanox configuration that we should use to speed up flow steering?We are using DPDK 19.11 and RHEL 7.9 drivers. I upgraded the firmware to the latest version to no avail and I tried switching to OFED drivers, again to no avail.Thank you very much for any help that you could provideHi,Please, refer to this link http://fast.dpdk.org/doc/perf/DPDK_21_05_Mellanox_NIC_performance_report.pdf as for tuning and for performance results. You should be getting 148 pps and not 100 with 64b messages. If 100 is maximum of what you are receiving, it might be a tuning or application issue. After reaching the numbers from document, you might use testpmd (tested and verified) to utilize steering rules.As a side not, please use latest MOFED v5.4 and firmware and latest stable DPDK in order to be sure that you are using software with latest fixesIf the issue is still happening, please put more details about the setup, command lines, a way flow steering is configured, traffic pattern, your host configuration. Be sure to use testpmd as we cannot troubleshoot custom code.We should be able to run basic troubleshooting on this forum, however if the issue require more serious debugging like running debug tools, collecting logs, executing different tests, that can be done only if your organization has a software support contract with Nvidia.Hi AlekseyThe 100Mpps is not the maximum, it’s just one test we are doing. But we should also get 100Mpps with flow steering on. I see nothing related to flow steering or rte flow rules in the performance reports.I have tested with recent OFED drivers as well, but it made no difference. We can’t test with recent versions of DPDK right now, that’s a ton of work for us to port.But maybe a more direct question: Should we be able to reach line rate results with RTE flow rules?Hey @Baptiste Wicht​ ,we are seeing a comparable issue however only on TCP based traffic. UDP traffic reaches line-rate.NVIDIA Developer ForumsHave you had a chance to further debug the issue?Powered by Discourse, best viewed with JavaScript enabled"
223,sn2010-connectx-4lx-mlag-rdma-with-hyper-v-cable-recommendations-with-this-configuration,"Hello, just purchased a pair of SN2010s to configure as an MLAG pair ToR switch set for Hyper-v clusters with ConnectX-4LX Nics. Looking to understand if there was any reason not to use the 100GbE to 4x25GbE Direct Attach Copper splitter cables in this setup as there are 4 servers in a cluster, and with Hyper-V/MLAG/RDMA/RoCE, would either -N ( no FEC ) or -L ( FEC ) be more or less preferred?Powered by Discourse, best viewed with JavaScript enabled"
224,infiniband-mtu-value-cannot-be-changed,"type : CX516A
OS : CentOS 7.6
I have the following configuration and I want to set the MTU value of IB to 65520 but it is not updated.CONNECTED_MODE described in ifcfg-ib0 is set to YES.
/etc/sysconfig/network-scripts/ifcfg-ib0
CONNECTED_MODE=yes
TYPE=InfiniBand
MTU=65520
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
IPADDR=
PREFIX=24
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=ib0
UUID=b1f0e139-55a4-4df3-b860-78806d60c84d
DEVICE=ib0
ONBOOT=yesopenibd opensmd service is enabled.When the ibstat command is executed, port1 is displayed as two.CA ‘mlx5_0’
CA type: MT4119
Number of ports: 1
Firmware version: 16.33.1048
Hardware version: 0
Node GUID: 0x1070fd0300242dd0
System image GUID: 0x1070fd0300242dd0
Port 1:
State: Down
Physical state: Disabled
Rate: 40
Base lid: 0
LMC: 0
SM lid: 0
Capability mask: 0x00010000
Port GUID: 0x1270fdfffe242dd0
Link layer: Ethernet
CA ‘mlx5_1’
CA type: MT4119
Number of ports: 1
Firmware version: 16.33.1048
Hardware version: 0
Node GUID: 0x1070fd0300242dd1
System image GUID: 0x1070fd0300242dd0
Port 1:
State: Down
Physical state: Disabled
Rate: 10
Base lid: 65535
LMC: 0
SM lid: 0
Capability mask: 0xa651e84a
Port GUID: 0x1070fd0300242dd1
Link layer: InfiniBandThe IP address can be set, but the MTU value cannot be changed and remains “4092”.eno1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet <>  netmask <>  broadcast <>
inet6 fe80::6d9:f5ff:febc:6e26  prefixlen 64  scopeid 0x20
ether 04:d9:f5:bc:6e:26  txqueuelen 1000  (Ethernet)
RX packets 24239  bytes 1982868 (1.8 MiB)
RX errors 0  dropped 1448  overruns 0  frame 0
TX packets 32145  bytes 38036388 (36.2 MiB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
device memory 0x90120000-9013ffffenp24s0f0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
inet <>  netmask <>  broadcast <>
ether 10:70:fd:24:2d:d0  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0ib0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 4092
inet <>  netmask <>  broadcast <>
Infiniband hardware address can be incorrect! Please read BUGS section in ifconfig(8).
infiniband 00:00:0A:77:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00  txqueuelen 256  (InfiniBand)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
inet 127.0.0.1  netmask 255.0.0.0
inet6 ::1  prefixlen 128  scopeid 0x10
loop  txqueuelen 1000  (Local Loopback)
RX packets 52  bytes 4311 (4.2 KiB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 52  bytes 4311 (4.2 KiB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0Can you tell us if there are other setting points?Regards,
Chikara.Correction of card model number.
CX556A-ECATHi
In order to set ConnectX-5 card as connected mode, please disable the Enhanced IPoIB
To disable Enhanced IPoIB:Add the following entry:
options ib_ipoib ipoib_enhanced=0Stop and start openibd service:
#/etc/init.d/openibd stop
#/etc/init.d/openibd startVerify parameter is disabled: (The value should be 0)
#cat /sys/module/ib_ipoib/parameters/ipoib_enhancedVerify the mode:
#cat /sys/class/net/ib0/modeVerify parameter is disabled: (The value should be 0)zhang4-san ,
MTU update is available.
ib0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 65520I appreciate your cooperation,It was very helpful.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
225,firmware-24-30-1004,"Hi,in my recent thread we determined that our cards(MBF2M332A-AEEO_Ax) are an older model.As I’m still trying to figure out the reason behind an mlx5_core timeout during boot I was trying to locate the latest firmware for the device. mlxfwmanager does not find any newer firmware, but the EoL notification states that there should be a more recent version 24_30_1004. That version is also not available on the website. Where can I download this version of the firmware?Thanks,
RaphaelEoL Notification: https://network.nvidia.com/pdf/eol/LCR-000770.pdfBoot Error:mlxfwmanagerPowered by Discourse, best viewed with JavaScript enabled"
226,global-pause-issues-increasing-tx-pause-ctrl-phy-with-multiple-queue-pairs,"the data source is an FPGA system with 100gb MAC IP (xilinx cmac). RoCEv2 packets (UC) are correctly generated on the fly.Receiver is a connectx-5, connected directly by fiber (no switch)Any hints on this subject is welcome…Hello Raphael,It seems that your organization already has a related ticket opened with us. Please, continue to work on it using official support channel.Helloissue solved (incorrect PSN)Powered by Discourse, best viewed with JavaScript enabled"
227,ib-card-ports-are-down-or-polling,"I have two cards installed now for loopback tests. I see each card has two slots, and under lspci, I see each card has ethernet and infiniband, total of 2 ethernet + 2 IB.I opensm started but getting following:/var/log/opensm.log:Any idea?I set guid in /etc/rdma/opensm.conf from 0x0000 to one of the ports and now getting following on /var/log/opensm.logibportstate:Web search does reveal someone suggested connecting cable and port should be up by itself.
Configuration is i have both cards in same system and connected 2nd port of each card by a IB cable (direct connection and for ethernet needs cross over in this situation) however from what i found, cross over cable is not applicable to IB and any cable should work either by switch or direct connection. Is that true?I have encountered the same problem as you, did you solve this problem later, if yes, I hope you can give me some helpPowered by Discourse, best viewed with JavaScript enabled"
228,dual-port-100g-on-pci3-0-bus-bandwidth,"Hi,
I’m confused about the speed of the ports of some Mellanox cards and the PCIe slot bandwidth they are installed on.As I understand it a single 100G port is full duplex right ? Meaning you could have 100 + 100 G of traffic going on both directions.How can you have a ConnectX-5 100Gbit dual port card sitting on PCIe 3.0 x16 slots ? PCIe 3.0 x16 can give you ~128Gbit of bandwidth so how is it possible to use the full bandwidth of this card on such slots ? As bidirectionally it could push 200 + 200 G right ?I’m probably missing something obvious so asking for help.Thanksor the MCX414A-GCAT for that matter, dual port 50Gbit on x8 PCIe 3.0 which should have up to ~64Gbit of bandwidth…@dropbrick: While looking for information on a similar topic, I found the following page: Specifications - ConnectX-5 InfiniBand/VPI - NVIDIA Networking Docs
Quoting from there:Note: PCIe 3.0 x16 bus can supply a maximum bandwidth of 128Gb/s only (=16 * 8GT/s, including overhead), and therefore cannot support 200Gb/s when both network ports of MCX556A-ECAT run at 100Gb/s.Powered by Discourse, best viewed with JavaScript enabled"
229,my-iblinkinfo-dont-work-on-host,"As the title says, I execute sudo iblinkinfo on the host-side of server A and it reports the error:ibwarn: [9475] _do_madrpc: send failed; Invalid argumentibwarn: [9475] mad_rpc: _do_madrpc failed; dport (DR path slid 0; dlid 0; 0)/var/tmp/rdma-core/rdma-core-54mlnx1/libibnetdisc/ibnetdisc.c:811; Failed to resolve selfdiscover failedHowever, if I execute the same command on the dpu(Bluefield 2) side of server A, it succeeds.In fact, when I send RDMA requests from another server B to the host side and dpu side of server A, they fail on the host side of A and succeed on the dpu side of A.Could someone please provide me some ideas to solve the problem, thanks a lot.P.S.Powered by Discourse, best viewed with JavaScript enabled"
230,why-performance-of-winof-2-in-multi-threads-is-very-poor,"Hi, All:I’m working on Windows RDMA(Network Direct) with WinOF-2 driver installed. I found the write and send API is
extremely slow in multi-threads scenario using one QP, it seems there may be locks inside these APIs implementation.latency of APIs call are roughly as belowing:Why is this ?
Can I got source codes of WinOF-2 driver somewhere ?References:Hello @johnpub,Thank you for posting your query on our community.Regarding your concern for poor throughput, single QP does not provide full line rate. Our design is built for running multiple QPs in parallel thus compensating the single QP rate limit. It is a design limitation, and we don’t have any tuning for it. So, to achieve the full throughput, we would recommend testing with two QPs.Regarding your question about WinOF-2 source code, we would like to inform you that it is not publicly available. Our engineering team cannot provide it unless there is a special justification for such a request.If you require further assistance on this, I would suggest you to open a support case for further investigation of the issue. The support ticket can be opened by emailing ""Networking-support@nvidia.com ""Please note that an active support contract would be required for the same. If you do not have a current support contract, please reach out to our Contracts team at networking-contracts@nvidia.comThank you,
-Nvidia Network SupportHi, does this reply means I should not use one QP in multi-threads, for Sending and Writing ?
I may not need to full line rate or achieve the full throughput, one QP is enough for my upper layer APPs, but must with multi-threads supported.What  I care is:  are there some methods to optimize latency of Send or Write API in multi-threads environment, the latency of one single call  seems to linearly grows along with the number of threads, which really upset me.Powered by Discourse, best viewed with JavaScript enabled"
231,degrade-throughout-when-one-of-vls-in-congested-status,"Hello, we got a problem.
There are 5 servers, each connected to one 40 ports switch with one 200Gbps IB card (HDRx4).
Lets named server as A, B, C, D, E.Test topology:
C <---- A -----> B
run following command:Test Result shows as following:Then, we add more traffic to node C, make C as a congested node. Test topology:running following commands:Test result shows as following:My question is, why A->B is 80Gbps? Shouldn’t it be about 130Gbps?Powered by Discourse, best viewed with JavaScript enabled"
232,sx6036-switchs-and-hdr-cables,"Hello. I’m working on a project were we will dploy a brand new HPE Server that came with what are called “HPE 2m IB HDR QSFP56 Copper Cbl”. These cables are supposed to support both 100 and 200Gb connection.What I’m not sure is if I can use these cables with a old SX6036 Switch. Can someone tell-me if these are compatible?Thanks in advance.Powered by Discourse, best viewed with JavaScript enabled"
233,becn-marking-doesnt-work,"Hi,I’m testing my InfiniBand network consisting of a SB7800 EDR switch and a couple of nodes with ConnectX-6 HCA.In particular, I just want to see if congestion control via FECN and BECN works as I expected.By creating an incast situation to a target node, I could see FECN marking by the switch works correctly.“perfquery --rcvcc” at the target node shows me that PortPktRcvFECN has increased after the test.Also, packet sniffing using ibdump indicates some of the packets have a FECN mark.But, I couldn’t see any packet with BECN which is supposed to be generated by the target node.And, “perfquery --rcvcc” at the source nodes also indicates that no packet with BECN has been received.I tried many different settings, but it didn’t solve the problem.(Turning off AR, changing link speed, etc.)Could you tell me what I did wrong?Thanks in advance.What are the firmware version you are using on switch and HCAs? What is the OFED version?Check that you have “mlnx_congestion_control 2” set in opensm.conf configuration.What is the test you are running? ib_write_bw/ib_read_bw? try both.It works on my setup:PortSelect:…1CounterSelect:…0x0000PortPktRcvFECN:…0PortPktRcvBECN:…1PortSelect:…1CounterSelect:…0x0000PortPktRcvFECN:…163587PortPktRcvBECN:…4515960PortSelect:…1CounterSelect:…0x0000PortPktRcvFECN:…0PortPktRcvBECN:…163627Not all FECN will lead to BECN, for example:-Multicast Packets-ACK Packets-CN PacketsIt also can be that a target node is sending CN packet back . Check if BECN bit is set in BETHFor additional details, please check IB specification, Vol 1.4. “A10.2.2 CA BEHAVIOR”.If you still seeing the behaviour different then described in IB specification, please open a support case with provide all the details of the test, topology and log files, as your organization has a valid support contract.It works!! I didn’t know CC setting should be configured in opensm configuration. Thank you very much!!Powered by Discourse, best viewed with JavaScript enabled"
234,i-cant-install-mlnx-ofex-linux-5-2-2-2-0-0-4-15-0-20-generic,"i’m using ubuntu18.04and device is connectX-4 Lxuname -r is 4.15.0-20-generici used sudo ./mlnxofedinstall --with-nfsrdma --with-nvmf --enable-gds --add-kernel-supportthe result isERROR: Failed executing “MLNX_OFED_SRC-5.2-2.2.0.0/install.pl --tmpdir /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.22959_logs --kernel-only --kernel 4.15.0-20-generic --kernel-sources /lib/modules/4.15.0-20-generic/build --builddir /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.22959 --without-dkms --without-debug-symbols --enable-gds --build-only --distro ubuntu18.04”ERROR: See /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.22959_logs/mlnx_ofed_iso.22959.logFailed to build MLNX_OFED_LINUX for 4.15.0-20-genericand fail log like thisn file included from include/linux/printk.h:350:0,from /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/include/linux/printk.h:7,from include/linux/kernel.h:14,from /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/include/linux/kernel.h:6,from include/linux/uio.h:12,from include/linux/socket.h:8,from /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/include/linux/socket.h:4,from ./include/uapi/linux/if.h:25,from /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/include/linux/compat-2.6.h:11,from :0:/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/net/sunrpc/xprtrdma/verbs.c:1267:14: error: ‘struct rpcrdma_xprt’ has no member named ‘rx_ia’; did you mean ‘rx_ep’?r_xprt->rx_ia.ri_id->device->name, mr->mr_sg, mr->mr_nents);^include/linux/dynamic_debug.h:128:10: note: in definition of macro ‘dynamic_pr_debug’##VA_ARGS); \^~~~~~~~~~~/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/net/sunrpc/xprtrdma/verbs.c:1266:4: note: in expansion of macro ‘pr_debug’pr_debug(“rpcrdma_nvfs_unmap_data device %s mr->mr_sg: %p , nents: %d\n”,^LD [M] /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/drivers/infiniband/hw/mlx4/mlx4_ib.oCC [M] /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/drivers/infiniband/ulp/iser/iscsi_iser.oscripts/Makefile.build:332: recipe for target ‘/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/net/sunrpc/xprtrdma/verbs.o’ failedmake[5]: *** [/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/net/sunrpc/xprtrdma/verbs.o] Error 1make[5]: *** Waiting for unfinished jobs…CC [M] /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/drivers/infiniband/hw/mlx5/cmd.o/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/net/sunrpc/xprtrdma/frwr_ops.c: In function ‘frwr_mr_recycle’:/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/net/sunrpc/xprtrdma/frwr_ops.c:91:39: error: ‘struct rpcrdma_xprt’ has no member named ‘rx_ia’; did you mean ‘rx_ep’?if (rpcrdma_nvfs_unmap_data(r_xprt->rx_ia.ri_id->device->dma_device,^~~~~rx_epIn file included from include/linux/printk.h:350:0,from /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/include/linux/printk.h:7,from include/linux/kernel.h:14,from /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/include/linux/kernel.h:6,from include/linux/uio.h:12,from include/linux/socket.h:8,from /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/include/linux/socket.h:4,from ./include/uapi/linux/if.h:25,from /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/include/linux/compat-2.6.h:11,from :0:/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/net/sunrpc/xprtrdma/frwr_ops.c:94:14: error: ‘struct rpcrdma_xprt’ has no member named ‘rx_ia’; did you mean ‘rx_ep’?r_xprt->rx_ia.ri_id->device->name, mr->mr_sg, mr->mr_nents);^include/linux/dynamic_debug.h:128:10: note: in definition of macro ‘dynamic_pr_debug’##VA_ARGS); \^~~~~~~~~~~/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/net/sunrpc/xprtrdma/frwr_ops.c:93:4: note: in expansion of macro ‘pr_debug’pr_debug(“rpcrdma_nvfs_unmap_data device %s mr->mr_sg: %p , nents: %d\n”,^/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/net/sunrpc/xprtrdma/frwr_ops.c: In function ‘frwr_map’:/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.15.0-20-generic/mlnx_iso.60350/mlnx-ofed-kernel/mlnx-ofed-kernel-5.2/net/sunrpc/xprtrdma/frwr_ops.c:341:36: error: ‘ia’ undeclared (first use in this function); did you mean ‘i’?dma_nents = rpcrdma_nvfs_map_data(ia->ri_id->device->dma_device,^~i​what should i do…?mlnx-ofed-kernel.debbuild.log (453 KB)Hi,based on my check, the OFED and your kernel versions are compatible, so you can try as following:Please make sure there is no previous OFED versions are installed, and if installed, please uninstall them prior to running the new OFED installation.Try running the installation without --add-kernel-support optionRegards,Anatolyyour second option worked very well thanks for your help!!Powered by Discourse, best viewed with JavaScript enabled"
235,url-filter-cannot-recompile,"Meanwhile, I found the issue regarding my previous message here!
The forum did not allow me to answer because I asked too many questions, and no one answered :D nice feature :PDropping the packet after the match is hardcoded in the source code.
In the main function:
The drop_on_match() function

image791×227 6.84 KB
I have tried to modify the source code, but then I could not recompile it :(
I followed the instructions here URL filter.
After issuing the ninja -C /tmp/build command, the compiler says it cannot find rte_sft.h.
I also set the LD_LIBRARY_PATH variable and issue ldconfig, but still not found. I am not a meson/ninja expert, so I blindly tried to copy the rte_sft.h file found /opt/mellanox/dpdk/include/dpdk/rte_sft.h to all libraries to all directories the compiler tries to use, such as /usr/local/include.
The original error of not finding the rte_sft.h disappeared but now many things are still undefined, such as
/opt/mellanox/doca/examples/url_filter/src/url_filter.c:389: undefined reference to 'rte_sft_fini'Is there any less hacky way to recompile url_filter?Did you export both environment variables when you tried to build?Could you share the output from the build failure?Thanks!@jubetz , thank you for your support!
PKG_CONFIG_PATH setting solved the issue! I can modify the source code and recompile it. I will share my experience here, where my original problem occurred regarding the dropping :)Speaking of this extra ENV VAR, if you have permission, it would be nice to update the official description,  too.Thank you so much :)The official description was updated in this week’s update (DOCA 1.1), and some of the environment variables are now taken care off by the BFB itself so users won’t need to configure them themselves.Thank you again for your helpful suggestions and your patience.Powered by Discourse, best viewed with JavaScript enabled"
236,sudo-apt-install-doca-tools-fails,"While installing doca-tools on Host PC running Ubuntu 20.04, the command “sudo apt install doca-tools” failes with the following output:Does anyone know how to fix this??Powered by Discourse, best viewed with JavaScript enabled"
237,about-the-performance-bottleneck-of-qm8790-switcher,"I have the same CX6 infiniband card installed on both nodes.When I directly connect two nodes with EDR cable, the read and write performance is 97Gb/Sec, MsgRate[Mpps] is 0.183.But when I use QM8790 switch to connect two nodes, the read and write performance drops to 65.99Gb/Sec, MsgRate [Mpps] is 0.125.Who can help me please?Please make sure you have the latest QM8790 FW installedUpdating Firmware for Quantum™ Based InfiniBand Switch PlatformsPowered by Discourse, best viewed with JavaScript enabled"
238,app-shield-agent-attestation-failure,"I recently learned app shield API and tried to run the app shield agent.
I followed the app shield agent steps in the documentation and tested with the apache service process.
However, it keeps failing due to attestation failure.root@localhost:/home/ubuntu/apsh# ./doca_app_shield_agent -p 2270 -e hash.zip -m mem_regions.json -o symbols.json -f MT2125X06703MLNXS0D0F0VF1 -d mlx5_0 -t 3 -s linux
[11:02:13:332792][DOCA][INF][APSH_APP:114]: start attestation on pid=2270
[11:02:14:284910][DOCA][INF][APSH_APP:144]: attestation failedThere is no more information than the log above, even if I increase the log level, and also, no telemetry file is output.Is there any solution to solve this problem?
Or is there a process condition for the app shield agent to work?Also, if I test with PF1, it fails to open representor device.root@localhost:/home/ubuntu/apsh# ./doca_app_shield_agent -p 2270 -e hash.zip -m mem_regions.json -o symbols.json -f MT2125X06703MLNXS0D0F1VF1 -d mlx5_0 -t 3 -s linux
[11:03:38:245879][DOCA][ERR][COMMON:197]: Matching device not found.
[11:03:38:258364][DOCA][ERR][APSH_APP::Core:441]: Failed to open representor device
[11:03:38:258406][DOCA][ERR][APSH_APP:80]: Failed to init application: Requested Resource Not FoundIs there any relation between PF1 failure and attestation failure of PF0VF1?I figured out how to view the telemetry service and found that the initial attestation succeeded.
I’m now thinking there may be a bug in the attestation.For the test, I disabled the attestation checking logic and printed the values involved in checking(DOCA_APSH_ATTESTATION_PAGES_PRESENT, DOCA_APSH_ATTESTATION_MATCHING_HASHES, and additionally, DOCA_APSH_ATTESTATION_PAGES_NUMBER).
I also printed DOCA_APSH_ATTESTATION_PATH_OF_MEMORY_AREA and “hash data is not present” message when DOCA_APSH_ATTESTATION_HASH_DATA_IS_PRESENT is false.This is the log of one attestation iteration of the sshd process with the above conditions.
A few lines of the log have been omitted for ease of understanding.[17:18:59:410809][DOCA][INF][APSH_APP:121]: start attestation on pid=2164
[17:18:59:661111][DOCA][INF][APSH_APP:132]: runtime_file_ind: 0, att_count: 62
[17:18:59:661224][DOCA][INF][APSH_APP:137]: page_number: 189, page_present: 145, matching_hashes: 145
[17:18:59:661251][DOCA][INF][APSH_APP:140]: path of memory area: sshd
[17:18:59:661299][DOCA][INF][APSH_APP:132]: runtime_file_ind: 1, att_count: 62
[17:18:59:661323][DOCA][INF][APSH_APP:137]: page_number: 3, page_present: 3, matching_hashes: 3
[17:18:59:661345][DOCA][INF][APSH_APP:140]: path of memory area: sshd
[17:18:59:661391][DOCA][INF][APSH_APP:132]: runtime_file_ind: 2, att_count: 62
[17:18:59:661418][DOCA][INF][APSH_APP:137]: page_number: 1, page_present: 1, matching_hashes: 1
[17:18:59:661446][DOCA][INF][APSH_APP:140]: path of memory area: libnss_files-2.27.so
[17:18:59:661491][DOCA][INF][APSH_APP:132]: runtime_file_ind: 3, att_count: 62
[17:18:59:661515][DOCA][INF][APSH_APP:137]: page_number: 1, page_present: 1, matching_hashes: 1
[17:18:59:661558][DOCA][INF][APSH_APP:140]: path of memory area: libnss_nis-2.27.so
[17:18:59:661608][DOCA][INF][APSH_APP:132]: runtime_file_ind: 4, att_count: 62
[17:18:59:661627][DOCA][INF][APSH_APP:137]: page_number: 1, page_present: 1, matching_hashes: 1
[17:18:59:661652][DOCA][INF][APSH_APP:140]: path of memory area: libnss_compat-2.27.so
[17:18:59:661697][DOCA][INF][APSH_APP:132]: runtime_file_ind: 5, att_count: 62
[17:18:59:661721][DOCA][INF][APSH_APP:137]: page_number: 20, page_present: 16, matching_hashes: 0
[17:18:59:661745][DOCA][INF][APSH_APP:140]: path of memory area: libgpg-e
[17:18:59:661790][DOCA][INF][APSH_APP:147]: hash data is not present
[17:18:59:661813][DOCA][INF][APSH_APP:132]: runtime_file_ind: 6, att_count: 62
[17:18:59:661835][DOCA][INF][APSH_APP:137]: page_number: 1, page_present: 1, matching_hashes: 0
[17:18:59:661858][DOCA][INF][APSH_APP:140]: path of memory area: libgpg-e
[17:18:59:661905][DOCA][INF][APSH_APP:147]: hash data is not present
[17:18:59:661930][DOCA][INF][APSH_APP:132]: runtime_file_ind: 7, att_count: 62
[17:18:59:661954][DOCA][INF][APSH_APP:137]: page_number: 23, page_present: 15, matching_hashes: 15
[17:18:59:661977][DOCA][INF][APSH_APP:140]: path of memory area: libresolv-2.27.so
[17:18:59:662026][DOCA][INF][APSH_APP:132]: runtime_file_ind: 8, att_count: 62
[17:18:59:662049][DOCA][INF][APSH_APP:137]: page_number: 1, page_present: 1, matching_hashes: 1
[17:18:59:662078][DOCA][INF][APSH_APP:140]: path of memory area: libresolv-2.27.so
…
[17:18:59:664154][DOCA][INF][APSH_APP:132]: runtime_file_ind: 30, att_count: 62
[17:18:59:664178][DOCA][INF][APSH_APP:137]: page_number: 1, page_present: 1, matching_hashes: 1
[17:18:59:664203][DOCA][INF][APSH_APP:140]: path of memory area: libcap-ng.so.0.0.0
[17:18:59:664254][DOCA][INF][APSH_APP:132]: runtime_file_ind: 31, att_count: 62
[17:18:59:664277][DOCA][INF][APSH_APP:137]: page_number: 1, page_present: 1, matching_hashes: 1
[17:18:59:664301][DOCA][INF][APSH_APP:140]: path of memory area: libnsl-2.27.so
[17:18:59:664349][DOCA][INF][APSH_APP:132]: runtime_file_ind: 32, att_count: 62
[17:18:59:664374][DOCA][INF][APSH_APP:137]: page_number: 487, page_present: 417, matching_hashes: 0
[17:18:59:664401][DOCA][INF][APSH_APP:140]: path of memory area: libc-2.2
[17:18:59:664445][DOCA][INF][APSH_APP:147]: hash data is not present
[17:18:59:664471][DOCA][INF][APSH_APP:132]: runtime_file_ind: 33, att_count: 62
[17:18:59:664494][DOCA][INF][APSH_APP:137]: page_number: 4, page_present: 4, matching_hashes: 0
[17:18:59:664517][DOCA][INF][APSH_APP:140]: path of memory area: libc-2.2
[17:18:59:664563][DOCA][INF][APSH_APP:147]: hash data is not present
[17:18:59:664584][DOCA][INF][APSH_APP:132]: runtime_file_ind: 34, att_count: 62
[17:18:59:664607][DOCA][INF][APSH_APP:137]: page_number: 3, page_present: 3, matching_hashes: 3
[17:18:59:664630][DOCA][INF][APSH_APP:140]: path of memory area: libcom_err.so.2.1
[17:18:59:664682][DOCA][INF][APSH_APP:132]: runtime_file_ind: 35, att_count: 62
[17:18:59:664705][DOCA][INF][APSH_APP:137]: page_number: 1, page_present: 1, matching_hashes: 1
[17:18:59:664733][DOCA][INF][APSH_APP:140]: path of memory area: libcom_err.so.2.1
…
[17:18:59:666741][DOCA][INF][APSH_APP:132]: runtime_file_ind: 58, att_count: 62
[17:18:59:666762][DOCA][INF][APSH_APP:137]: page_number: 41, page_present: 41, matching_hashes: 41
[17:18:59:666791][DOCA][INF][APSH_APP:140]: path of memory area: ld-2.27.so
[17:18:59:666835][DOCA][INF][APSH_APP:132]: runtime_file_ind: 59, att_count: 62
[17:18:59:666858][DOCA][INF][APSH_APP:137]: page_number: 1, page_present: 1, matching_hashes: 1
[17:18:59:666880][DOCA][INF][APSH_APP:140]: path of memory area: ld-2.27.so
[17:18:59:666923][DOCA][INF][APSH_APP:132]: runtime_file_ind: 60, att_count: 62
[17:18:59:666946][DOCA][INF][APSH_APP:137]: page_number: 3, page_present: 1, matching_hashes: 0
[17:18:59:666968][DOCA][INF][APSH_APP:140]: path of memory area: Anonymous Mapping
[17:18:59:667011][DOCA][INF][APSH_APP:132]: runtime_file_ind: 61, att_count: 62
[17:18:59:667034][DOCA][INF][APSH_APP:137]: page_number: 1, page_present: 1, matching_hashes: 1
[17:18:59:667056][DOCA][INF][APSH_APP:140]: path of memory area: vdso
[17:18:59:667099][DOCA][INF][APSH_APP:156]: telemetry enabled
[17:18:59:667138][DOCA][INF][APSH_APP:173]: attestation passDOCA_APSH_ATTESTATION_MATCHING_HASHES shows value 0 (runtime_file_ind: 5, 6, 32, 33, 60), and this incurs attestation checking logic to fail and break the loop.
For runtime_file_ind 5, 6, 32, 33, DOCA_APSH_ATTESTATION_HASH_DATA_IS_PRESENT is false, which means there is no hash data for the attestation and DOCA_APSH_ATTESTATION_PATH_OF_MEMORY_AREA shows an invalid lib name.
For example, runtime_file_ind 5 and 6 indicate libgpg-e for the path of memory area, but the correct lib name is libgpg-error.so.0.22.0.
This can be checked in /proc//maps file and in hash.zip file.
(I have no clue about runtime_file_ind 60, which shows “Anonymous Mapping” for the path of memory area.)Therefore, I inferred that the hash could not be retrieved because the path of memory area and the library in hash.zip do not match by name and it incurs matched hashes to be 0.
Is there any way to fix this bug, or can this bug be fixed?Powered by Discourse, best viewed with JavaScript enabled"
239,cumulus-radius-configuration,"Hi Everyone,I want to ssh to a Cumulus Mellanox switch and use radius to authenticate the user. I also want to create a Fallback user. Are there any ready-made configuration commands? Can you help me?Thank youRegardsRadius authentication is not yet supported by the NVUE API/CLI in Cumulus Linux 5.0+.Note: Radius is still supported via the classical Linux methods shown in our documentation → RADIUS AAA | Cumulus Linux 5.4Hi Epulvino,Actually I want Cumulus CLI to work according to user and password in Radius when connecting.Can’t this be done? Can’t I do user authentication with Radius? We using 4.2 versiyon.RegardsPowered by Discourse, best viewed with JavaScript enabled"
240,gpudirect-in-pci-passthrough-configuration,"Hello all,We have a setup in one Workstation with two Virtual-Machines. Each machine has a GPU, and they form a sort of a processing-visualization pipeline: the results processed in the linux VM are then visualized in Windows.
InnovATe-Pipeline749×361 16.5 KB
The communication between the two VMs happens through the Hypervisor (virtual-network). This involves copying the information from one GPU to the linux-vm, and then to the windows-VM and then to the other GPU. This is CPU intensive and we would like to explore the GPU Direct technology for it.The question is:We this we want to offload some of the load on the CPU/Hypervisor.Thanks.I am curious about why you need two systems for one Task. Is it possible to migrate the application on Linux to Windows since we have CUDA, TensorRT supported on windowns?Unfortunately, that part of the design is due to legacy code. In a future a restructuring would be possible, for now we have to work with this setup.Since this is a legacy implementation, what is your legacy HW set up for these two OS applications?Well, the HW itself can be upgraded. We are using now a RTX4000 in Linux, and a GTX 1070 in Windows. But we are more interested in the general question of whether or not a GPUDirect link could be done between two GPUs in the same system, to avoid overloading the CPU with copying of the data.It is only possible when using the same operating systems for two GPUs inside one system.Hi @sluo , I have a question that builds on @fabian.melendez 's post.It is only possible when using the same operating systems for two GPUs inside one system.To confirm, if I am running two VMs, both running Linux, each with it’s own independent GPU assigned via PCIe Passthrough, I should be able to initiate GPUDirect communication for GPU Direct RDMA transfers over PCIe, not hitting shared CPU memory?As followup: Would this work with two Windows VMs? Would GPU Direct between the two VMs over a physical NVLink be supported on either Linux or Windows, to accelerate transfers?I’ve wondered about this for a long time, hopefully you’ve run across some use cases like this.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
241,bluefield-2-dpu-handling-only-500mbps-traffic,"Hello all,
I am generating a 11GBps  (Giga BYTES per second) UDP traffic using the CISCO Trex tool, to evaluate the performance of the BF2 DPU.
I am trying to make my DPU configuration as simple as possible by connecting the p1 and pf1hpf directly via an OVS bridge. I am making the bridge as shown below:my  bridge configuration is as follows:then I am monitoring the traffic using the system activity report tool (sar) using this command:I notice that the OVS bridge is only able to forward a maximum traffic of 500MBps as per the screenshot below:
Screenshot from 2023-05-09 18-34-071024×382 62.5 KB
as you notice from the screen shot above the traffic arriving on p1 is 10.9GBps while the traffic arriving on pf1hpf is only maxing out at 500MBps.could you please explain why this is happening, and how to go about fixing this issue.hi zoxerusI suspect maybe the traffic didn’t offload.
please check with such link make sure offload is enabled:
https://docs.nvidia.com/networking/display/BlueFieldDPUOSv380/Virtual+Switch+on+BlueField+DPU#VirtualSwitchonBlueFieldDPU-EnablingOVSHWOffloadingEnablingOVSHWOffloadingIf still have issue, or need further DPU performance tunning please contact networking-support@nvidia.comThank you
Meng, ShiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
242,cumulus-switchd-and-32-routes-in-software,"HiIt seems that adding /32 routes in Cumulus / switchd causes routing to happen in software.We can add a /24 route, everything is fast and seems to be in hardware. Then we test with a /32 route to the same IP and things slow down to 4Mbps.Where can I read about when /32 routes trigger software routing? I believe the 4Mbps we are seeing is related to L3 COPP limits.Testing with SN2010, Cumulus 5.3.If I do the same on Linux / SwitchDev (not switchd) it seems to work at hardware speeds.We should get a case opened to track this. This should not be the case.Outside of bandwidth testing, did you confirm that the packets are actually software switched by doing a tcpdump on the switch to see if the CPU sees the packets?Also, are you using any other overlays? (ie. VXLAN, EVPN, etc) Or is this just a simple L3 static route?I can see the packets with tcpdump, so fairly sure routing is happening in software.Also if I change the value of copp.l3_local.rate in /etc/cumulus/control-plane/policers.conf it the download speed changes, so I think it’s triggering a COPP rule.Doing very basic L3 routing, still in a ‘lab’ style setup. Have tried with vlan aware and traditional bridges. As soon as we add a /32 route things break.Can you share some configuration on what you’re doing? As @rdarbha said though, this would definitely be a support case.Hi, the config is trivial, SN2010, cumulus 5.3Set up /24 network, PC can route via switch and get full hardware speeds.Add a /32 route to the PC (same IP), speed drops to 4Mbps and things happen in software.I think we’ll just use Debian + SwitchDev, that’s working as expected.Sure! Everything is fine. :-)Powered by Discourse, best viewed with JavaScript enabled"
243,how-to-forward-broadcast-subnet-or-global-between-vlans,"Hi:I am using switch SN-2010.
I’ve created 2 vlans as:
interface vlan 10 ip address 192.168.1.1/24 primary
interface vlan 20 ip address 192.168.2.1/24 primaryand defined their switchport range, for example:
valn 10:  1/1 - 1/6
valn 20:  1/7 - 1/12For now, all swithports are in access mode.Device on vlan10 port is 192.168.1.2, gateway 192.168.1.1 for 192.168.2.0/24
Device on vlan20 port is 192.168.2.2.Unicast packets are forwarded properly.However, our user case requires broadcast packets (global broadcast 255.255.255.255, or subnet broadcast 192.168.1.255) to be forwarded from vlan 10 to vlan 20.What should I do to achieve this?Thanks
KevinPowered by Discourse, best viewed with JavaScript enabled"
244,skyway-appliance,"Anyone have a NVIDIA Skyway appliance in production? We are having issues and NVIDIA support is not very helpful.Have you opened a case?Colin,Your case was closed due to no response from your side. Did you received all the updates?
We will re-open the case so the engineer can reach out to you by phone.Thank you,
~NVEX Global Technical SupportI added a comment to the NVIDIA Case. ThanksHello Colin,I moved your case to another engineer (also named Colin) who will assist you further. He will contact you today for setting up a remote-session to understand the issue better why the links are not coming up as EDR.Lets keep the conversation from now on in the case as I am monitoring the case as well and I can respond to you through the case.Thank you,
~MartijnNVEX Global Technical Support ManagmentPowered by Discourse, best viewed with JavaScript enabled"
245,gpu-has-fallen-off-the-bus-requires-your-serious-attention,"On Recent Linux Distro/Kernels, when the machine goes into idle or used very slightly with very low CPU processing, suddenly the “GPU falls off the bus” and the machine freezes, ssh on the machine you can access it, but it’s now working without a display card!And I want here to raise to your attention that this is NEITHER a BIOS nor a PS issue, the same cards on the same workstations, with the same BIOS version and same power supply work as expected on older Distro/Kernels…
Anyway, it’s doesn’t seem to be a driver issue too! And it happens even when the NVidia driver isn’t installed!However, the issue requires some attention from your engineers to figure out why the RTX cards have such issue with the recent linux distro! It’s your product at the end, and we expect you to troubleshoot the issue and tell us what to do!FYI, all the issue is not something new! the intel idle c_state is behind it, loading the kernel with idle=nomwait (which disables the intel idle driver and uses the ACPI driver instead fixes the issue) however, this is consuming too much energy and makes the workstation really noisy and probably hotter!So, this is the case, if you will continue assuming that it’s something wrong with our hardware, the issue will never get resolved! If there is something wrong on our hardware, then it’s your PCIe cards design lacking some sort of power regulator, or requires a firmware update that allows the card to deal with the power reduction that happens when the system kernel activate the processor idle via the c_state levels.I may share with you what I did, which could help you have a clue about where the bug lives… my tests led me to doubt about the glibc! Even if this doesn’t make sens, but here is why:From the tests I’ve made, it works fine on old disto (with old glibc) regardless to if the kernel is recent or old.
And it fails on recent Distro, with recent glibc, regardless to the kernel version if it’s recent or old.Obviously, the only common thing between the failure scenarios, is glibc, as I understand, nothing else in the OS is involved in such thing, it’s the kernel and the hardware, since both were proven to be ok, then my doubts goes to some sort of a bug in glibc! Even when I’m not sure if glibc could be involved in this!hi,could you try disable PCIe ASPM in BIOS and set ‘pcie_aspm=off’ in kernel ?Regards,
LeveiThanks…
Interesting feature! I didn’t know about it… I will give it a try and let you know.unfortunately this didn’t help, the card doesn’t wake up after idle (although this kernel option as I read was supposed to disable the pcie idle!)…And I have a doubt here that when the display goes off on idle, the card doesn’t wake up before detecting the display back on again, which doesn’t happen for some reason! I don’t know if this make any sense… could such thing be an OS related thing?!Powered by Discourse, best viewed with JavaScript enabled"
246,measure-the-performance-of-the-accelerators-on-bluefield-1-card,"Hi, I am trying to test the raw performance of the accelerators on BlueField-1. The DPU version is MBF1L516A-CSNAT, and the related data sheet should be this.According to the data sheet, there are no regex and crypto accelerators. Could anybody please point out that what kind of accelerators I can test on this DPU? Thanks. :)Hi user52115,Thanks for posting your inquiry to the NVIDIA Developer Forums.Per the datasheet, there are public key accelerators supported with the Bluefield-1, as it contains H/W RNG and entropy source - you can benchmark public key acceleration with the tools included in the libpka suite.IE, within the ARM environment, as root:
/usr/lib/aarch64-linux-gnu/libpka1/pka_test_performance -c RSA_VERIFY -v 3You can also see that GPUDirect and NVMEoF acceleration is supported with this hardware - though these solutions aren’t one-size-fits-all. It may be beneficial for you to engage our support and solutions teams for more details, via https://support.mellanox.comBest,
NVIDIA Networking SupportHi,Thanks very much for your suggestions. :)I tried the pka test on the BlueField ARM OS, but it is failed. The log is shown below.Could you please help to check how to benchmark the PKA performance correctly? BTW, do you have any insights what the performance will be if multiple workloads use the same pka accelerator at the same time? Thanks. :)Best regards.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
247,how-to-use-non-root-to-start-testpmd-of-dpdk-17-11-with-mlx5-driver,"I can start testpmd of dpdk 17.11 with root user，after install MLNX_OFED driver。But can not start testpmd with non-root user。Why can not start testpmd with non-root?EAL: Detected 128 lcore(s)
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL: PCI device 0000:41:00.0 on NUMA socket 0
EAL:   probe driver: 15b3:1015 net_mlx5
Interactive-mode selected
USER1: create a new mbuf pool <mbuf_pool_socket_0>: n=203456, size=2304, socket=0
Configuring Port 0 (socket 0)
net_mlx5: mlx5_txq.c:395: mlx5_txq_ibv_new(): port 0 Tx queue 0 QP creation failure
net_mlx5: mlx5_trigger.c:185: mlx5_dev_start(): port 0 Tx queue allocation failed: Cannot allocate memory
Fail to start port 0
Please stop the ports first
DoneHi,Welcome to the NVIDIA Developer forums! This is the VGPU category, your issue looks to be related to Mellanox drivers. I am moving this to the appropriate category so the support team has visibility.oHi ,It looks like hugepages issue related to non-root user
You can refer to the below page
https://doc.dpdk.org/guides/linux_gsg/enable_func.htmlHugepages must be reserved as root before running the application as non-root.
If the driver requires using physical addresses ¶, the executable file must be granted additional capabilities:SYS_ADMIN to read /proc/self/pagemaps
IPC_LOCK to lock hugepages in memoryThanks,
SamerHi,I tried it “https://doc.dpdk.org/guides/linux_gsg/enable_func.html”，but it does not work.
Now,it have the below info after starting testpmd with non-root,EAL: Detected 128 lcore(s)
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL: PCI device 0000:41:00.0 on NUMA socket 0
EAL:   probe driver: 15b3:1015 net_mlx5
Interactive-mode selected
USER1: create a new mbuf pool <mbuf_pool_socket_0>: n=203456, size=2304, socket=0
Configuring Port 0 (socket 0)
net_mlx5: mlx5_flow.c:2211: mlx5_flow_create_drop_queue(): port 0 cannot allocate QP for drop queue
net_mlx5: mlx5_trigger.c:179: mlx5_dev_start(): port 0 drop queue allocation failed: Operation not permitted
Fail to start port 0
Please stop the ports first
Done
testpmd>
企业微信截图_16605466718757873×735 22.5 KB
ThanksHi,Call ibv api  at the dpdk mlx5 driver with non-root always fail.
Do you heard it?Thanks,LouyqPowered by Discourse, best viewed with JavaScript enabled"
248,install-doca-on-bluefield-2-failed,"Hi, I follow  the DOCA SDK DOCUMENTATION（v1.4.0）to install software for my BlueFiled 2 DPU (MBF2M516A-EEEO).
But I have some problems at step 3.3.1 Installing Full DOCA Image On DPU, following is the logs
1. the bfd-install failed to execute with “cat: write error: Connection timed out”2. rshim console shows “Memory Device: 0 BIST Failed” and “DDR BIST POST failed!”3. my host os is ubuntu 20.04 with Linux 5.4.0-26-generic kernelThank you for your reply.Could you try POWER CYCLE BF2 AND SERVER, then re-burn by bfb-install.POWER CYCLE BF2Thanks a lot.
I have rebooted the host and even put the BF2 on other hosts but still getting the same error, I think maybe there is something wrong with my BF2.Does BF2 can boot normally? Or just can’t burn new DOCA image?I think the BF2 do not work normally, after setting ip to tmfifo_net0, I can not ssh to BF2OK, please check below,If not “DPU is ready”, you can open support ticket from ESP portal for RMA,https://enterprise-support.nvidia.com/s/Ok, thank youThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
249,lacp-on-mellanox-mlnx-os-msb7800-infiniband-switch,"Hi guys.I would like to implement LACP on the infiniband switch Mellanox MLNX-OS MSB7800 as it is explained on: https://enterprise-support.nvidia.com/s/article/howto-configure-lacp-on-mellanox-switches but the os command: “LACP” doesn’t exist in the switch. Could you tell me if it is possible? and how to implement it?
The current firmware on the swith is: X86_64 3.9.2400 2021-03-24 14:34:04 x86_64Thank you in advance for your reply.No LACP for Infiniband. You are looking at an Ethernet related post.HI @dwaxman , thank you for your reply. However here is explained how to implement bonding with infiniband networks: https://enterprise-support.nvidia.com/s/article/howto-create-linux-bond–lag–interface-over-infiniband-network. Is that possible just from the server side? thank you in advance.Hi,Yes – this is IPoIB bond that is done for high availability.
It doesn’t allow for active/active configuration that’ll work using LACP towards the switch etc.thanks,DanThank you @dwaxman for your reply.
We just have an additional question for you:  Is there any bonding policy that we should use to implement it?If you mean to use LACP in IB – it is not possible to do that.As stated in the doc you referred to, IPoIB bond mode is only for active/backup (bonding mode number 1, or “active-backup”) – hence not requiring anything on the switch configuration.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
250,the-problems-of-connected-or-datagram-in-ipoib,"according to the https://docs.nvidia.com/networking/pages/viewpage.action?pageId=64306551#IPoverInfiniBand(IPoIB)-EnhancedIPoIB , try to change datagram in /sys/class/net/ib/mode , but im failed .Im setting 'SET_IPOIB_CM=yes'and /etc/init.d/openibd restart ,then found which in the mode is datagram . Use echo connected > /sys/class/net/ib/mode ,also failed, i need help .Powered by Discourse, best viewed with JavaScript enabled"
251,where-to-get-mlx5-ifc-h-and-device-h-for-windows-10-compile,"Trying to do a windows version of this example:https://community.mellanox.com/s/article/basic-verbs-programming–show-devices—code-example?t=1631645044907 In …\MLNX_WinOF2_DevX_SDK\inc\mlx5_ifc_devx. h there are the following references:#include <…/drivex/mlx5/inc/mlx5_ifc.h>#include <…/drivex/mlx5/inc/device.h>I’m guessing these are included in a driver for windows but I can’t find it.Powered by Discourse, best viewed with JavaScript enabled"
252,using-ethernet-and-rdma-simultaneously,"Hello, is there any possibilities to use ethernet and RDMA simultaneously on Mellanox Infiniband network cards?If not, is possible to use ethernet and SR-IOV simultaneously?Thank youHi,If you have a VPI card, you can configure one port as Ethernet and one port as InfiniBand.You should have MFT package installed on the host (should be installed by default if you have MLNX_OFED driver, if not you can download it from our website).The protocol types are:Port Type 1 = IBPort Type 2 = EthernetFor example:Regarding the second questions - it is possible to use Ethernet and SR-IOV simultaneously.Please refer the below article.https://docs.mellanox.com/pages/viewpage.action?pageId=39264752Best Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
253,connectx6-mlx5-kernel-driver-strange-behavior,"HiI’ve been testing the speed of my 100g setup with iperf3 and I have an unexplained ‘issue’.What I’m doing:Out of the box the aggregate speed is then ~45gbit.One I change any settings that relate to hardware, like the rx/tx buffers (default 1024)  to any higher or lower number, and then revert to default, then the performance bumps to ~90gbit.
eg,
default (boot) (rx and tx are both set to 1024)  => 45gbit
ethtool -G enp129s0f0np0 rx 100 tx 100     → ~90gbit
ethtool -G enp129s0f0np0 rx 1024 tx 1024 → ~90gbitSimilar behavior is observed when just changing the port MTU from 1500 (default) to 9000 and back to 1500.The “problem” is somewhere on the RX data path I believe.
Can someone please help me make some sense of this?Linux node113 5.15.0-40-generic #43-Ubuntu SMP Wed Jun 15 12:54:21 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux[    0.805346] pci 0000:81:00.0: [15b3:101b] type 00 class 0x020000
[    0.805475] pci 0000:81:00.0: reg 0x10: [mem 0x5816e000000-0x5816fffffff 64bit pref]
[    0.805761] pci 0000:81:00.0: reg 0x30: [mem 0xb2300000-0xb23fffff pref]
[    0.806357] pci 0000:81:00.0: PME# supported from D3cold
[    0.844530] pci 0000:81:00.0: Adding to iommu group 123
[    1.896377] mlx5_core 0000:81:00.0: firmware version: 20.33.1048
[    1.899670] mlx5_core 0000:81:00.0: 252.048 Gb/s available PCIe bandwidth (16.0 GT/s PCIe x16 link)
[    2.254623] mlx5_core 0000:81:00.0: Rate limit: 127 rates are supported, range: 0Mbps to 97656Mbps
[    2.256238] mlx5_core 0000:81:00.0: E-Switch: Total vports 2, per vport: max uc(128) max mc(2048)
[    2.260273] mlx5_core 0000:81:00.0: Port module event: module 0, Cable plugged
[    2.261886] mlx5_core 0000:81:00.0: mlx5_pcie_event:295:(pid 10): PCIe slot power capability was not advertised.
[    2.287577] mlx5_core 0000:81:00.0: mlx5_fw_tracer_start:821:(pid 588): FWTracer: Ownership granted and active
[    2.730661] mlx5_core 0000:81:00.0: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)
[    2.924007] mlx5_core 0000:81:00.0: Supported tc offload range - chains: 4294967294, prios: 4294967295
[    3.155111] mlx5_core 0000:81:00.0 enp129s0f0np0: renamed from eth0
[    6.507168] mlx5_core 0000:81:00.0 enp129s0f0np0: Link uproot@node113:~/ofed# mstlink -d 81:00.0 --cable --ddmState                           : Active
Physical state                  : ETH_AN_FSM_ENABLE
Speed                           : 100G
Width                           : 4x
FEC                             : Standard RS-FEC - RS(528,514)
Loopback Mode                   : No Loopback
Auto Negotiation                : ONEnabled Link Speed (Ext.)       : 0x000007f2 (100G_2X,100G_4X,50G_1X,50G_2X,40G,25G,10G,1G)
Supported Cable Speed (Ext.)    : 0x00000200 (100G_4X)Status Opcode                   : 0
Group Opcode                    : N/A
Recommendation                  : No issue was observed.Temperature                     : 48C
Voltage                         : 0.3292V
Channels                        : Channel 1     ,Channel 2     ,Channel 3     ,Channel 4
RX Power                        : 2.000dBm      ,2.000dBm      ,2.000dBm      ,2.000dBm
TX Power                        : 2.000dBm      ,2.000dBm      ,2.000dBm      ,2.000dBm
TX Bias                         : 40.632mA      ,40.496mA      ,39.678mA      ,40.906mADevice type:    ConnectX6
Name:           MCX653106A-ECA_Ax
Description:    ConnectX-6 VPI adapter card; H100Gb/s (HDR100; EDR IB and 100GbE); dual-port QSFP56; PCIe3.0 x16; tall bracket; ROHS R6
Device:         81:00.0Configurations:                              Next Boot
MEMIC_BAR_SIZE                      0
MEMIC_SIZE_LIMIT                    _256KB(1)
HOST_CHAINING_MODE                  DISABLED(0)
HOST_CHAINING_CACHE_DISABLE         False(0)
HOST_CHAINING_DESCRIPTORS           Array[0…7]
HOST_CHAINING_TOTAL_BUFFER_SIZE     Array[0…7]
FLEX_PARSER_PROFILE_ENABLE          0
FLEX_IPV4_OVER_VXLAN_PORT           0
ROCE_NEXT_PROTOCOL                  254
ESWITCH_HAIRPIN_DESCRIPTORS         Array[0…7]
ESWITCH_HAIRPIN_TOT_BUFFER_SIZE     Array[0…7]
PF_BAR2_SIZE                        0
NON_PREFETCHABLE_PF_BAR             False(0)
VF_VPD_ENABLE                       False(0)
PF_NUM_PF_MSIX_VALID                False(0)
PER_PF_NUM_SF                       False(0)
STRICT_VF_MSIX_NUM                  False(0)
VF_NODNIC_ENABLE                    False(0)
NUM_PF_MSIX_VALID                   True(1)
NUM_OF_VFS                          0
NUM_OF_PF                           2
PF_BAR2_ENABLE                      False(0)
SRIOV_EN                            False(0)
PF_LOG_BAR_SIZE                     5
VF_LOG_BAR_SIZE                     1
NUM_PF_MSIX                         63
NUM_VF_MSIX                         11
INT_LOG_MAX_PAYLOAD_SIZE            AUTOMATIC(0)
PCIE_CREDIT_TOKEN_TIMEOUT           0
ACCURATE_TX_SCHEDULER               False(0)
PARTIAL_RESET_EN                    False(0)
RESET_WITH_HOST_ON_ERRORS           False(0)
DISABLE_SLOT_POWER_LIMITER          True(1)
ADVANCED_POWER_SETTINGS             True(1)
CQE_COMPRESSION                     BALANCED(0)
IP_OVER_VXLAN_EN                    False(0)
MKEY_BY_NAME                        False(0)
PRIO_TAG_REQUIRED_EN                False(0)
UCTX_EN                             True(1)
PCI_ATOMIC_MODE                     PCI_ATOMIC_DISABLED_EXT_ATOMIC_ENABLED(0)
TUNNEL_ECN_COPY_DISABLE             False(0)
LRO_LOG_TIMEOUT0                    6
LRO_LOG_TIMEOUT1                    7
LRO_LOG_TIMEOUT2                    8
LRO_LOG_TIMEOUT3                    13
LOG_TX_PSN_WINDOW                   7
LOG_MAX_OUTSTANDING_WQE             7
TUNNEL_IP_PROTO_ENTROPY_DISABLE     False(0)
ICM_CACHE_MODE                      DEVICE_DEFAULT(0)
TX_SCHEDULER_BURST                  0
LOG_DCR_HASH_TABLE_SIZE             11
DCR_LIFO_SIZE                       16384
LINK_TYPE_P1                        ETH(2)
LINK_TYPE_P2                        ETH(2)
ROCE_CC_PRIO_MASK_P1                255
ROCE_CC_PRIO_MASK_P2                255
CLAMP_TGT_RATE_AFTER_TIME_INC_P1    True(1)
CLAMP_TGT_RATE_P1                   False(0)
RPG_TIME_RESET_P1                   300
RPG_BYTE_RESET_P1                   32767
RPG_THRESHOLD_P1                    1
RPG_MAX_RATE_P1                     0
RPG_AI_RATE_P1                      5
RPG_HAI_RATE_P1                     50
RPG_GD_P1                           11
RPG_MIN_DEC_FAC_P1                  50
RPG_MIN_RATE_P1                     1
RATE_TO_SET_ON_FIRST_CNP_P1         0
DCE_TCP_G_P1                        1019
DCE_TCP_RTT_P1                      1
RATE_REDUCE_MONITOR_PERIOD_P1       4
INITIAL_ALPHA_VALUE_P1              1023
MIN_TIME_BETWEEN_CNPS_P1            4
CNP_802P_PRIO_P1                    6
CNP_DSCP_P1                         48
CLAMP_TGT_RATE_AFTER_TIME_INC_P2    True(1)
CLAMP_TGT_RATE_P2                   False(0)
RPG_TIME_RESET_P2                   300
RPG_BYTE_RESET_P2                   32767
RPG_THRESHOLD_P2                    1
RPG_MAX_RATE_P2                     0
RPG_AI_RATE_P2                      5
RPG_HAI_RATE_P2                     50
RPG_GD_P2                           11
RPG_MIN_DEC_FAC_P2                  50
RPG_MIN_RATE_P2                     1
RATE_TO_SET_ON_FIRST_CNP_P2         0
DCE_TCP_G_P2                        1019
DCE_TCP_RTT_P2                      1
RATE_REDUCE_MONITOR_PERIOD_P2       4
INITIAL_ALPHA_VALUE_P2              1023
MIN_TIME_BETWEEN_CNPS_P2            4
CNP_802P_PRIO_P2                    6
CNP_DSCP_P2                         48
LLDP_NB_DCBX_P1                     False(0)
LLDP_NB_RX_MODE_P1                  OFF(0)
LLDP_NB_TX_MODE_P1                  OFF(0)
LLDP_NB_DCBX_P2                     False(0)
LLDP_NB_RX_MODE_P2                  OFF(0)
LLDP_NB_TX_MODE_P2                  OFF(0)
DCBX_IEEE_P1                        True(1)
DCBX_CEE_P1                         True(1)
DCBX_WILLING_P1                     True(1)
DCBX_IEEE_P2                        True(1)
DCBX_CEE_P2                         True(1)
DCBX_WILLING_P2                     True(1)
KEEP_ETH_LINK_UP_P1                 True(1)
KEEP_IB_LINK_UP_P1                  False(0)
KEEP_LINK_UP_ON_BOOT_P1             False(0)
KEEP_LINK_UP_ON_STANDBY_P1          False(0)
DO_NOT_CLEAR_PORT_STATS_P1          False(0)
AUTO_POWER_SAVE_LINK_DOWN_P1        False(0)
KEEP_ETH_LINK_UP_P2                 True(1)
KEEP_IB_LINK_UP_P2                  False(0)
KEEP_LINK_UP_ON_BOOT_P2             False(0)
KEEP_LINK_UP_ON_STANDBY_P2          False(0)
DO_NOT_CLEAR_PORT_STATS_P2          False(0)
AUTO_POWER_SAVE_LINK_DOWN_P2        False(0)
NUM_OF_VL_P1                        _4_VLs(3)
NUM_OF_TC_P1                        _8_TCs(0)
NUM_OF_PFC_P1                       8
VL15_BUFFER_SIZE_P1                 0
NUM_OF_VL_P2                        _4_VLs(3)
NUM_OF_TC_P2                        _8_TCs(0)
NUM_OF_PFC_P2                       8
VL15_BUFFER_SIZE_P2                 0
DUP_MAC_ACTION_P1                   LAST_CFG(0)
UNKNOWN_UPLINK_MAC_FLOOD_P1         False(0)
SRIOV_IB_ROUTING_MODE_P1            LID(1)
IB_ROUTING_MODE_P1                  LID(1)
DUP_MAC_ACTION_P2                   LAST_CFG(0)
UNKNOWN_UPLINK_MAC_FLOOD_P2         False(0)
SRIOV_IB_ROUTING_MODE_P2            LID(1)
IB_ROUTING_MODE_P2                  LID(1)
PF_TOTAL_SF                         0
PF_SF_BAR_SIZE                      0
PF_NUM_PF_MSIX                      63
ROCE_CONTROL                        ROCE_ENABLE(2)
PCI_WR_ORDERING                     per_mkey(0)
MULTI_PORT_VHCA_EN                  False(0)
PORT_OWNER                          True(1)
ALLOW_RD_COUNTERS                   True(1)
RENEG_ON_CHANGE                     True(1)
TRACER_ENABLE                       True(1)
IP_VER                              IPv4(0)
BOOT_UNDI_NETWORK_WAIT              0
UEFI_HII_EN                         True(1)
BOOT_DBG_LOG                        False(0)
UEFI_LOGS                           DISABLED(0)
BOOT_VLAN                           1
LEGACY_BOOT_PROTOCOL                PXE(1)
BOOT_RETRY_CNT                      NONE(0)
BOOT_INTERRUPT_DIS                  False(0)
BOOT_LACP_DIS                       True(1)
BOOT_VLAN_EN                        False(0)
BOOT_PKEY                           0
P2P_ORDERING_MODE                   DEVICE_DEFAULT(0)
ATS_ENABLED                         False(0)
DYNAMIC_VF_MSIX_TABLE               False(0)
EXP_ROM_UEFI_ARM_ENABLE             True(1)
EXP_ROM_UEFI_x86_ENABLE             True(1)
EXP_ROM_PXE_ENABLE                  True(1)
ADVANCED_PCI_SETTINGS               False(0)
SAFE_MODE_THRESHOLD                 10
SAFE_MODE_ENABLE                    True(1)Identifier                      : QSFP28
Compliance                      : 100GBASE-LR4 or 25GBASE-LR
Cable Technology                : 1310 nm DFB
Cable Type                      : Optical Module (separated)
OUI                             : Other
Vendor Name                     : FINISARCORP.
Vendor Part Number              : FTLC1154RDPL-A5
Vendor Serial Number            : U6EAGPE
Rev                             : A0
Wavelength [nm]                 : 1302
Transfer Distance [m]           : 0
Attenuation (5g,7g,12g) [dB]    : N/A
FW Version                      : N/A
Digital Diagnostic Monitoring   : Yes
Power Class                     : 3.5 W max
CDR RX                          : ON,ON,ON,ON
CDR TX                          : ON,ON,ON,ON
LOS Alarm                       : N/A
Temperature [C]                 : 51 [-5…75]
Voltage [mV]                    : 3288.9 [2970…3630]
Bias Current [mA]               : 40.596,40.122,39.750,40.724 [25…55]
Rx Power Current [dBm]          : 2,2,2,2 [-14…6]
Tx Power Current [dBm]          : 2,2,4,2 [-8…8]It is very strange, with default ring buffers (1024) we on 45gbits have drops, after increase to 8k we can push ~80-85gbps.  MTU not changed, i think it can change if router supported.Powered by Discourse, best viewed with JavaScript enabled"
254,file-scan-example-set-max-number-of-matches,"Hello,
I’m trying to use the File Scan example with a specific number of matches.
Specifically, I would like to stop the matching process at the first match.
I have tried to use
regex_cfg.nb_max_matches = 1;
rte_regexdev_configure(app_cfg->device_id, &regex_cfg);
but the rte_regexdev_dequeue_burst() still returns a > 1 number of matches.
Is there a way to stop the matching process when a match occurs without going further?Thanks,FIlippoPowered by Discourse, best viewed with JavaScript enabled"
255,ofed-5-6-install-pl-dracut-failures-on-rockylinux-8-5,"Hi, trying to install OFED 5.6-1.0.3.3 using the recommended install.pl method (with --hpc flag) on RockyLinux 8.5 Generic Cloud image says “Installation successful” but actually has a load of errors like:any suggestions for what’s wrong here. Currently running it again with selinux set to permissive but that’ll take 2 hours so someone might have a better idea in that time!Hmm. Looks like --all might work, so presumably this is a bug.Hello Steveb1,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Based on the information provided, we are not able to reproduce the issue in our lab. See attached install log.
general.log (74.4 KB)We used the regular install ISO from Rocky Linux and followed the UM in detail, related to the section on how-to install on Community OS Systems → https://docs.nvidia.com/networking/display/MLNXOFEDv561033/Installing+MLNX_OFED#InstallingMLNX_OFED-InstallationonCommunityOperatingSystemscommunityAlso the ‘dracut’ message is not appearing on our lab system as ‘/etc/rdma/modules/infiniband.conf’ does not belong to MLNX_OFED but is part of the OS INBOX drivers.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
256,sn2000-series-max-tc-rules,"HiHow many tc rules can you install on an SN2010 using the mlxsw hardware tc offload features?Contribute to Mellanox/mlxsw development by creating an account on GitHub.Hi JoeAbout 5600 rules with default settings.There are two limiting factors:Size of TCAMNumber of countersTo mitigate the first, use chain templates with only the keys you need:https://github.com/Mellanox/mlxsw/wiki/ACLs#chain-templatesThis will result in each rule occupying less space in the TCAMTo mitigate the second, disable statistics:https://github.com/Mellanox/mlxsw/wiki/ACLs#suppressing-per-filter-statisticsPowered by Discourse, best viewed with JavaScript enabled"
257,connectx-5-bit-rate-expected-with-winsock-send-and-recv-versus-roce,"Can a ConnecX-5 adapter improve non-RoCE throughput for a legacy application using simple “send” and “recv” Winsock calls.  For testing, I’m using a simple application that repeatedly passes a buffer ranging from tens of MB to one GB to Winsock APIs “send” and “recv”, where “recv” is called with flag MSG_WAITALL.  We are hoping for speeds of around 25 Gbps without changing the application to use RoCE, but I’m only seeing speeds on the order of 8.5 to 9.0 Gbps.  The application provides about 9.5 Gbps on a setup with old Intel X520 10GbE adapters.On the same setup, your nd_read_bw.exe and nd_write_bw.exe performance tests show speeds of 96 Gbps.Should “send” and “recv” calls on Windows be able to achieve more than 10 Gbps throughput with ConnectX-5 adapters?  If so, can you send me some suggestions?Powered by Discourse, best viewed with JavaScript enabled"
258,help-finding-the-latest-drivers-for-my-connectx4lx-card-on-lenovo,"Hi,I am trying to find firmware (version 14.31.1014 to be specific) for my ConnectX4LX on lenovo server. This is a linux box. Can you point us in the right direction?I couldn’t find the specific part number and PSID in nvidia or lenovo website. I tried auto updating using  mlxup --online but no luckDevice Type:      ConnectX4LX
Part Number:      01GR252_Ax
Description:      Mellanox ConnectX-4 Lx 2x25GbE PCIe Adapter
PSID:             LNV2420110034
PCI Device Name:  /dev/mst/mt4117_pciconf1
Versions:         Current        Available
FW             14.30.1004     N/A
PXE            3.6.0301       N/A
UEFI           14.23.0017     N/AStatus:           No matching image foundThe NIC is a Lenovo branded NIC.
The official FW should be on their site.Hi rohith.balusu2,For the Firmware of Lenovo OEM adapters, you need to seek assistance from Lenovo side.
Nvidia doesn’t offer Firmware for OEM products. That’s the reason you couldn’t find the firmware on the Nvidia official website.Thanks,
YuyingThank You.
I will follow up with lenovo.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
259,how-to-mitigate-the-degradation-of-mellanox-connectx-5-forwarding-performance-when-multi-segs-is-activated,"In order to test the performance degradation of packet fragmentation and multi-segment transmission.
I used DPDK 23.03 and add rx offload capabilities: DEV_RX_OFFLOAD_SCATTER
and tx offload capabilities: DEV_TX_OFFLOAD_MULTI_SEGS.In addition, I change the size of the mbuf so that multiple mbufs can store a data packet together.  Surprisingly, when we use 512B mbuf to store 256B packets, the forwarding bandwidth of Mellanox will still decrease. However, the Mbuf with a size of 512B can store 256B packets without using multiple segments. So this performance drop is not due to packet fragmentation.So what is the reason for this performance drop? Is there any way to suppress this performance drop？The test results can be found in the attachment.
test-THROUGHPUT.pdf (13.6 KB)A supplementary note, working with the OP. In the graph the SEGS refer to the segment size of RTE_ETH_RX_OFFLOAD_BUFFER_SPLIT.That means even if we have a huge, single segment but send 64byte packets, there is a huge performance degradation.Powered by Discourse, best viewed with JavaScript enabled"
260,hairpin-error-in-dpdk-testpmd-with-connectx-4-lx,"Hi,I would like to use hairpin feature with my ConnectX-4 Lx card with DPDK testpmd application however I’m facing below error message in mlx5 drivers. Any suggestions on solution?./dpdk-testpmd -l 14,15 -a 0000:65:00.0 -a 0000:65:00.1 – -i --total-num-mbufs=5000 --rxq=2 --rxd=512 --txq=2 --txd=512 --hairpinq=1 --hairpin-mode=0x12Configuring Port 0 (socket 0)
common_mlx5: Failed to create SQ using DevX
mlx5_pci: Port 0 tx hairpin queue 2 can’t create SQ object.
mlx5_pci: port 0 Tx queue allocation failed: Invalid argument
Fail to start port 0
Configuring Port 1 (socket 0)
common_mlx5: Failed to create SQ using DevX
mlx5_pci: Port 1 tx hairpin queue 2 can’t create SQ object.
mlx5_pci: port 1 Tx queue allocation failed: Invalid argument
Fail to start port 1Error is caused by this function: mlx5dv_devx_obj_create, it returns as NULL by not creating requested object.OFED version: 5.4-3.4.0.0
DPDK version: 20.11.5Powered by Discourse, best viewed with JavaScript enabled"
261,help-finding-the-latest-drivers-for-my-connect-x-4-card,"Hi,I am only able to find an older version of driver for our Dell branded Mellanox cards. Below is what we have today and we’d like to upgrade to latest firmware. Can you point us in the right direction?WE tried using the auto update function but that did not work. This is a linux box.mlxup --queryDevice #1:Device Type: ConnectX4Part Number: 0068F2_0NNJ2M_AxDescription: Mellanox ConnectX-4 Dual Port EDR PCIE Adapter LPPSID: DEL2190110032PCI Device Name: /dev/mst/mt4115_pciconf0Base MAC: 0c42a1b29658Versions: Current AvailableFW 12.17.2052 N/APXE 3.4.0903 N/AStatus: No matching image foundIt seems that it can’t update the firmware.Hello Glenn,Thank you for posting your inquiry on the NVIDIA Networking Community.For Dell Mellanox OEM cards, Dell provides a link on their support website where to download the latest and greatest based on the platform you are using the adapter in.For your convenience I provided the direct download link below.You can download the latest version of the f/w for this PSID through the following direct link → https://www.mellanox.com/downloads/firmware/fw-ConnectX4-rel-12_28_4512-0068F2_0NNJ2M_Ax-FlexBoot-3.6.203.bin.zipYou can update the f/w with the ‘mlxup’ command through the following syntax → # mlxup -d /dev/mst/mt4115_pciconf0 -i <path to f/w bin file> -y burnThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
262,app-may-influence-ovs-offload,"Hi all,
With running DNS_Filter app on the host1, I use iperf to test the bindwith. Host1 to Host2 is 23.5Gbits/s. But Host2 to Host1 is about 5-7Gbits/s. I don’t know why this phenomenon occurs. I guess that ovs offload is wrong. But I can not solve it.
Besides that, I use ib_send_bw to test rdma. It does not work. Is rdma not working when app (e.g. dns_filer, url_filter etc.) is running?P/N: MBF2M332A-AENOTPowered by Discourse, best viewed with JavaScript enabled"
263,how-to-reduce-fans-rpm-on-cumulus-linux-switch-sn2410,"Hi there, hope you guys are doing great!I’m using a Cumulus Linux in a SN2410 Switch, and I am wondering how can I reduce/decrease all fans RPM?
Is there any command or configuration file where I can set the fan speed in RPM? I’ve noticed that all of them are running in the maximum speed by default, which makes a loud noise.Is there any example?Thanks for your attention!They are probably not even running at full speed, that only happens when it gets very hot. The speed is dynamically adjusted based on the temperature and you cannot change that in any supported way. This is to prevent various components from overheating.So @attilla , they are running at full speed all time the switch is powered. FANs speed is not being dynamically adjusted. Every time I check the sensors command, it returns the same RPM for all fans (that is the same of the maximum speed or even higher). My lab temperature is controlled and below 23 °C, so its not “hot” in the lab.I would like to know how can I make an speed divider ( I believe they are all PWM controlled). Are you sure that Isn’t there any configuration that I can write in the file located at /etc/sensors/ ?I just want to do it because the noise is too loud in the lab and outside it, and it is not being healthy for other employers to work around it.Thanks,Hi Phzera,This is not correct then, because they should automatically adjust and your environment doesn’t seem to hot. Which CL version are you running, because there has been a specific version that had an issue around this.Obviously there are non supported ways, but that is not recommended. I would first look if the above is applicable to you.Hi @attilla,
I’m running on CL 4.2.1.I think that is exactly the version that is affected by the issue, so I would suggest you install 4.4 or if possible for you 5.0 (you would have to move to NVUE though).OK @attilla , I’ll try to upgrade to CL 4.4.Thanks for the support.Powered by Discourse, best viewed with JavaScript enabled"
264,error-while-installing-mofed-for-centos-8-3,"Hello,I’ve upgraded my OS to CentOS 8.3 in order to reinstall MOFED (5.1-2.5.8.0). However, I’m getting this error message:Current operation system is not supported!I’m guessing that it refers to the OS but I had never seen this message before. The command is:sudo ./mlnxofedinstall --kernel $KERNEL --kernel-sources /usr/src/kernels/${KERNEL} --add-kernel-supportwhere KERNEL is 4.18.0-240.1.1.el8_3.x86_64, and the kernels are located at /usr/src/kernels/4.18.0-240.1.1.el8_3.x86_64 with the following structure:arch crypto fs ipc lib mm samples sound usrblock drivers include Kconfig Makefile Module.symvers scripts System.map virtcerts firmware init kernel Makefile.rhelver net security tools vmlinux.idI really don’t see anything wrong here so I’m unsure why of the error message.Thanks.Hi Arturo,Could you please try the below command:./mlnxofedinstall --add-kernel-support --force --skip-distro-checkThanks,SamerHello Samer,It’s still producing the same error.ArturoHi Arturo,Please provide the below outputs :For further investigation, i suggest to open support ticket networking-support@nvidia.comThanks,SamerHi Samer,The requested info:1 - 4.18.0-240.1.1.el8_3.x86_642 - CentOS Linux release 8.3.20113 - I double checked (because it was my recollection) but there’s no log as nothing has happened. The installation stops right after calling mlxnofedinstall.Thanks.ArturoPowered by Discourse, best viewed with JavaScript enabled"
265,i-need-help-with-using-infiniband-in-my-data-center,"HiTo compute data in direct access memory, I want to buy hardware that supports InfiniBand RDMA in my data center.I watched a video on how the RDMA Protocol works.I have some questions :The rack network is busy for 480 GB data transfer before the compute task.To remove transfer and direct computation, I am checking whether RDMA will be useful.I would like to discuss hardware networking with RDMA.My rack having :For RDMA storage (such as NVME) is important or RAM?What hardware is suitable for RDMA computation without transfer?I focus on the best IB hardware, such as :Hi Magdalena,Where are you located? I want to forward your email to a regional sales department that can help and answer questions and provide a solution.Powered by Discourse, best viewed with JavaScript enabled"
266,rshim-shows-another-backend-already-attached-and-tmfifo-net0-cannot-be-found,"I have a BlueField-2 DPU installed on my server, but when I try to start rshim, it says “another backend already attached”. I don’t even have other PCIe devices attached other than the DPU. What could go wrong?below are the logs of systemctl status rshim:shujunyi@poweredge0-PowerEdge-R740:~$ sudo systemctl status rshim● rshim.service - rshim driver for BlueField SoCLoaded: loaded (/lib/systemd/system/rshim.service; enabled; vendor preset: enabled)Active: active (running) since Mon 2021-09-27 19:17:28 CST; 2min 31s agoDocs: man:rshim(8)Process: 2979 ExecStart=/usr/sbin/rshim $OPTIONS (code=exited, status=0/SUCCESS)Main PID: 3043 (rshim)Tasks: 2 (limit: 6143)CGroup: /system.slice/rshim.service└─3043 /usr/sbin/rshim9月 27 19:17:28 poweredge0-PowerEdge-R740 systemd[1]: Starting rshim driver for BlueField SoC…9月 27 19:17:28 poweredge0-PowerEdge-R740 systemd[1]: Started rshim driver for BlueField SoC.9月 27 19:17:29 poweredge0-PowerEdge-R740 rshim[3043]: Probing pcie-0000:5e:00.29月 27 19:17:29 poweredge0-PowerEdge-R740 rshim[3043]: create rshim pcie-0000:5e:00.29月 27 19:17:29 poweredge0-PowerEdge-R740 rshim[3043]: another backend already attachedreturn of ifconfig:br-cd4cb1507b28: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255inet6 fe80::42:19ff:fe86:48ad prefixlen 64 scopeid 0x20ether 02:42:19:86:48:ad txqueuelen 0 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 4 bytes 386 (386.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0docker0: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255ether 02:42:09:de:d3:e9 txqueuelen 0 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 0 bytes 0 (0.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eno1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500inet 162.105.16.151 netmask 255.255.255.0 broadcast 162.105.16.255inet6 2001:da8:201:1016:2eea:7fff:feee:4208 prefixlen 64 scopeid 0x0inet6 fe80::2eea:7fff:feee:4208 prefixlen 64 scopeid 0x20ether 2c:ea:7f:ee:42:08 txqueuelen 1000 (Ethernet)RX packets 108289 bytes 9393882 (9.3 MB)RX errors 0 dropped 0 overruns 0 frame 0TX packets 451 bytes 87520 (87.5 KB)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0device interrupt 75eno2: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500ether 2c:ea:7f:ee:42:09 txqueuelen 1000 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 0 bytes 0 (0.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0device interrupt 77eno3: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500ether 2c:ea:7f:ee:42:0a txqueuelen 1000 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 0 bytes 0 (0.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0device interrupt 78eno4: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500ether 2c:ea:7f:ee:42:0b txqueuelen 1000 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 0 bytes 0 (0.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0device interrupt 80enp94s0f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500inet 192.168.1.1 netmask 255.255.0.0 broadcast 192.168.255.255inet6 fe80::ac0:ebff:fe2c:d78c prefixlen 64 scopeid 0x20ether 08:c0:eb:2c:d7:8c txqueuelen 1000 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 74 bytes 7997 (7.9 KB)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0enp94s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500inet 192.168.1.2 netmask 255.255.0.0 broadcast 192.168.255.255inet6 fe80::ac0:ebff:fe2c:d78d prefixlen 64 scopeid 0x20ether 08:c0:eb:2c:d7:8d txqueuelen 1000 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 60 bytes 7157 (7.1 KB)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536inet 127.0.0.1 netmask 255.0.0.0inet6 ::1 prefixlen 128 scopeid 0x10loop txqueuelen 1000 (Local Loopback)RX packets 164 bytes 12672 (12.6 KB)RX errors 0 dropped 0 overruns 0 frame 0TX packets 164 bytes 12672 (12.6 KB)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0veth951e6ee: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500ether 9e:ec:7c:45:b0:15 txqueuelen 0 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 0 bytes 0 (0.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0vethd0dd2f0: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500ether 72:be:4e:92:00:59 txqueuelen 0 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 0 bytes 0 (0.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0vethdd6b52a: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500ether c6:2d:ec:c5:69:85 txqueuelen 0 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 0 bytes 0 (0.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0shujunyi@poweredge0-PowerEdge-R740:~$ lspci | grep nox5e:00.0 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)5e:00.1 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)5e:00.2 DMA controller: Mellanox Technologies MT42822 BlueField-2 SoC Management Interface (rev 01)Hello Junyi Shu,Thanks for your question.It’s possible to access the console of DPU via PCIe interface, as well as via usb cable,which may be connected to another server. The rshim driver that was attached first,will allow you connecting the DPU. If you will start the second driver, either via usb,or via PCIe, you will see “another backend already attached” message in logs.This means the first driver that was attached to device will allow you to access the DPU.Best Regards,AnatolyI figured out how to reproduce it.When I reboot the server without power cycling it, it seems rshim does not terminate properly.So when the server is up again, rshim service cannot restart as it thinks there is another driver while there is actually none.At this moment, what I do is:whenever I try to reboot the server, I stop rshim firstIf someone reboots the server without doing so, I power cycle it againIt works, but it will be good to fix it in rshimPowered by Discourse, best viewed with JavaScript enabled"
267,does-connectx-6-dx-and-lx-support-ats-and-pri,"Hi,Would like to know ConnectX-6 Dx and Lx cards support PCIe extended capabilities ATS (Address translation Service) and PRI (Page Request Interface) features.
I see these features not listed in product feature list (Networking-Overal-DPU-Datasheet-ConnectX-6-Dx-SmartNIC-1991450.pdf)Thanks,
AnilHello anilreddyh22,Thank you for posting your query on our community. ConnectX-6 Dx and ConnectX-6 Lx support ATS and PRI capabilities.Please make sure to enable ATS in BIOS and then enable it on the card using mlxconfig as below:
mlxconfig -d /dev/mst/mt4123_pciconf0 set ATS_ENABLED=1For more details:
https://wikinox.mellanox.com/display/FW/pcie+ATS+feature?focusedCommentId=244745226&refresh=1611739334677#comment-244745226You can find it under “PCI Express Specifications” on the below links:ConnectX-6 Dx - NVIDIA ConnectX-6 Dx Ethernet Adapter Cards User Manual - ConnectX-6 Dx Ethernet - NVIDIA Networking Docs
ConnectX-6 Lx - NVIDIA ConnectX-6 Lx PCIe HHHL Ethernet Adapter Cards User Manual - ConnectX-6 Lx Ethernet - NVIDIA Networking DocsThanks & Regards,
Nvidia SupportThank you for the quick response and for the information.Hi,I am working with ConnectX-7 MT2910, I did what you recommended to enable the ATS capabilities as belowroot@localhost: # mlxconfig -d /dev/mst/mt4129_pciconf0 set ATS_ENABLED=1
Device #1:Device type:    ConnectX7
Name:           MCX713106AS-VEA_Ax
Description:    NVIDIA ConnectX-7 Ethernet adapter card; 200 GbE; Dual-port QSFP112; PCIe 5.0 x16; Secure Boot; No Crypto
Device:         /dev/mst/mt4129_pciconf0
Configurations:                                      Next Boot       New
ATS_ENABLED                                 True(1)         True(1)Apply new Configuration? (y/n) [n] : y
Applying… Done!
-I- Please reboot machine to load new configurations.After reboot, I didn’t see any difference in lspci and also linux PCI block indicating ATS feature not supported for this device. did I miss anything here.May I know what exactly need to enable in BIOS to support ATS.  I am not able to open the link mentioned, do you have an alternative link.
https://wikinox.mellanox.com/display/FW/pcie+ATS+feature?focusedCommentId=244745226&refresh=1611739334677#comment-244745226Powered by Discourse, best viewed with JavaScript enabled"
268,port-goes-down-when-added-to-mlag-portchannel-sn2010m,"Port configured for 1GB and 10GB speed has a 1GB Transceiver.When added to a MLAG PortChannel, the port goes down.Is  the SN2010M expecting 10GB links in an MLAG PortChannel?ThanksSN2010M is an HPE part number, so the best place to ask for assistance is through HP Enterprise support. SN2010 is the NVIDIA product that is almost identical.If this was a SN2010, the best way to start looking into this is to check if/why the interface is down as the physical port should be able to come up regardless of port-channel config when a supported SFP/cable is connected to a remote device configured to match. You can get details on the autonegotiation, speed, FEC, power, linkstate, and more with the following command (replace X with the front panel port number)Onyx Interface State:
For SN2xxx switches:
X is the port number
(config)# fae mlxlink -d /dev/mst/mt52100_pciconf0 -p X -m -e -cCumulus Linux/Sonic:
sudo mlxlink -d 03:00.0 -p X -m -e -cIf the physical port does not report any issues, then there is probably a misconfiguration between the two devices related to LACP. All the ports in a bond are required to be running at the same speed, and all links on both sides of the bond need to use the same LACP system MAC and be part of the same bond. If for example you have 2 bonds on the same switch cabled to one bond or across 2 different switches cabled to one bond, then links in only a single bond would be allowed to be up due to the mismatched LACP bond ID or system ID respectively. In this case all the interfaces on each side of the bond would need to be placed in the same bond/port-channel or the same mlag respectively.The best place to troubleshoot this would be a support case, so please contact the support for your switch after checking if this is a NVIDIA or HPE product.Powered by Discourse, best viewed with JavaScript enabled"
269,dell-r620-or-r630-compatibility-with-connectx-3-infiniband-mcx354a-fcbt,"My Dell r620/r630 machines will not recocognize MCX354A-FCBT before installation of operating system. This makes network boot/installation over card impossible. Is there a way to flash firmware so the cards will be recognized by the system/iDrac? Any show-stoppers with this card and these machines (ie. they’re not the Dell version of the hardware)? Is there a firmware update that can be installed with iDrac, or do they all require installation of an operating system as a pre-requisite?Hello Anthony,Thank you for posting your inquiry on the NVIDIA Networking Community.You can burn the following f/w on the HCA which includes Flexboot → http://www.mellanox.com/downloads/firmware/fw-ConnectX3-rel-2_42_5000-MCX354A-FCB_A2-A5-FlexBoot-3.4.752.bin.zipFlexboot enables PXE Boot. For more information, please review the Flexboot UM → https://docs.mellanox.com/display/PreBootDriversv12Also make sure, on your Dell, you only enable the adapters you want to use for PXEBoot in the BIOS of the system. Too many adapters enabled for PXEBoot, will use BIOS memory, which then results in the Mellanox adapter not showing up as PXEboot adapter. For more info on how-to change the BIOS, please review the Dell System information.Thank you and regards,~NVIDIA Networking Technical SupportI’m so grateful for your reply.Powered by Discourse, best viewed with JavaScript enabled"
270,mlx5-missing-registers-bar-error-code-19,"After endlessly troubleshooting I am resorting to the manufacturer in order to resolve an issue with Mellanox MT27800 Family [ConnectX-5] Drivers not performing properly.I have updated all firmware to the latest available especially BIOS:
A47 v2.72 (04/20/2023)BIOS is NOT in safe mode.I installed a fresh ISO from Ubuntu for LTE version 22.04.2 running kernel version 5.15.0-72-generic
 5.15.0-72-generic #79-Ubuntu SMP Wed Apr 19 08:22:18 UTC 2023 x86_64 x86_64 x86_64 GNU/LinuxI checked that the network adaptors are being seen:lshw -C networklspci -v | grep MellanoxInstallation succeeds for the driver software located here:
Linux Ethernet Drivers - MLNX_EN (nvidia.com)Yet I get this error when restarting the NIC driver…
image984×109 22.7 KB
mlx5_pci_init:1117: error requesting BARs, aborting
mlx5_pci_init failed with error code -19The NICs do not show up when using ‘ip a’I’m not sure what else could possibly be wrong. Please assist.Please set the pci=realloc=off kernel parameter in Linux, and let us know the status.This worked, in order to build intuition, can you explain how you knew to try this?The issue happens when installing 2 ConnectX adapters on 2 slots on the same server with Ubuntu OS.Ubuntu OS cannot allocate enough memory UAR BAR to the second interface (adapter). Therfore, The OS boots with only one adapter (from the first slot).This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
271,receive-side-scaling-rss-interrupts-missing,"I’m trying to set up RSS on the !00Gbe ConnectX-4 card running on Debian Linux and kernel 5.10.There are no interrupts (queues) being shown for the card. I.ecat /proc/interrupts | grep lan2results in nothing there. Intel NICs lan0 and lan1 show show up fine. This is occurring on three machines.Powered by Discourse, best viewed with JavaScript enabled"
272,unable-to-change-mode-to-legacy-mode-devlink-fails-on-one-port-hangs-on-the-other,"I am trying to test IPSec on Bluefield-2 and following steps mentioned in https://community.mellanox.com/s/article/ConnectX-6DX-Bluefield-2-IPsec-HW-Full-Offload-Configuration-Guide.When I am trying change the mode on p1 as:devlink dev eswitch set pci/0000:03:00.1 mode legacyError is thrown: “devlink answers: Device or resource busy”and If I try on p0 as“devlink dev eswitch set pci/0000:03:00.0 mode legacy” then it just get stuck there.Here are the BF’s details:ubuntu@linux:~$ sudo bfvcheckBeginning version check…-RECOMMENDED VERSIONS-ATF: v2.2(release):3.5.0-6-g1f6d422UEFI: 3.5.0-2-gc1b5d64FW: 24.29.1016-INSTALLED VERSIONS-ATF: v2.2(release):3.5.0-6-g1f6d422UEFI: 3.5.0-2-gc1b5d64FW: 24.29.1016Version check complete.No issues found.I wonder if I am missing anything!Hello Malik,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we would recommend:If you are still experiencing issues after this, please open a NVEX Networking Support ticket by sending an email to networking-support@nvidia.com. You have a valid support contract so our support engineers will be able to assist you further through the support ticket.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
273,connectx-increase-rx-buffer-miss-counter-with-max-buffer-size,"Hi there,I have a network consisting of Ryzen servers running ConnectX 4 Lx (MT27710 family) which run a fairly intense workload involving a lot of small packet websockets traffic. We’re noticing the rx_prio0_discards counter is continuing the climb even after we’ve replaced the NIC and increased the ring buffer to 8192Ring parameters for enp65s0f1np1:Pre-set maximums:RX: 8192RX Mini: 0RX Jumbo: 0TX: 8192Current hardware settings:RX: 8192RX Mini: 0RX Jumbo: 0TX: 8192Any ideas or suggestions here?Hello Alexander,Thank you for posting your question on the Mellanox Community.With discards increasing there is a chance that the adapter or node is not processing packets quickly enough. We would advise aligning your system in accordance with our tuning guide which can be found here:https://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersThe following tuning guide may also help as you are using AMD architecture. Please note that it may not be 100% applicable as it is more directed at the AMD EPYC datacenter processors rather than the Ryzen line.https://support.mellanox.com/s/article/how-to-tune-an-amd-server--eypc-cpu--for-maximum-performanceYou may also be able to improve performance by enabling iommu passthrough. We have seen improvement in the past when using this option with AMD EYPC processors.https://support.mellanox.com/s/article/understanding-the-iommu-linux-grub-file-configurationThe following page may also help to better understand the counters:https://community.mellanox.com/s/article/understanding-mlx5-ethtool-countersIf you need any further assistance, please open a case through the Mellanox support portal using a account with a valid support contracthttps://support.mellanox.com/s/Thanks and regards,~Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
274,whats-the-deal-with-infiniband-connected-mode,"I have a Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6] in servers, and running RHEL 7.9 x86-64.  In RHEL in the settings - network gui it offers the choice for Transport Protocol of either Datagram or Connected.   It seems only Datagram mode works, and Connected mode is not supported.Is Mellanox moving away from Connected Mode?  Redhat says they do not support it in RHEL 8 and that it “does not scale”.I have just 3 servers on a 100gbps infiniband network (this is not a 10,0000 node cluster) and I want to use connected mode.  I am under the impression that connected mode should be better than datagram in my case ?Hello ron7000cg,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Yes, the information provided by RH is correct. In case of using RH INBOX driver, they currently only support datagram mode.A few driver versions ago, we introduced Enhanced IPoIB, which will provide more better scalability and performance improvement than using CONNECTED_MODE.You can review the following direct link to the driver UM → https://docs.nvidia.com/networking/pages/viewpage.action?pageId=64306551#IPoverInfiniBand(IPoIB)-EnhancedIPoIB
to better understand the benefits when using Enhanced IPoIB mode (datagram).Snippet from this section:
"" Enhanced IPoIB feature enables offloading ULP basic capabilities to a lower vendor specific driver, in order to optimize IPoIB data path. This will allow IPoIB to support multiple stateless offloads, such as RSS/TSS, and better utilize the features supported, enabling IPoIB datagram to reach peak performance in both bandwidth and latency.Enhanced IPoIB supports/performs the following:** Stateless offloads (RSS, TSS)*
** Multi queues*
** Interrupt moderation*
** Multi partitions optimizations*
** Sharing send/receive Work Queues*
** Vendor specific optimizations*
** UD mode only""*Still it is based on your use case if CONNECTED_MODE will give you more performance increase. The performance increase you will see is only for ULP, legacy IP applications.The MLNX_OFED driver still supports CONNECTED_MODE and you can definitely run your setup with this setting → https://docs.nvidia.com/networking/pages/viewpage.action?pageId=64306551#IPoverInfiniBand(IPoIB)-ipoibmodesettingIPoIBModeSettingThank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
275,configuring-mellanox-hardware-for-vpi-operation-is-now-available-on-mellanox-com,"http://www.mellanox.com/related-docs/prod_gateway_systems/Configuring_Mellanox_Hardware%20_for_VPI_Operation_Application… http://www.mellanox.com/related-docs/prod_gateway_systems/Configuring_Mellanox_Hardware%20_for_VPI_Operation_Application_Note.pdfThis link does not appear to be working?Hi @hunter1Please see this page for more info.https://support.mellanox.com/s/Powered by Discourse, best viewed with JavaScript enabled"
276,vlan-sub-interfaces,"Hi, I found this in the Cumulus documentation:“A bridge cannot contain multiple subinterfaces of the same port. Attempting this configuration results in an error.”On a linux software bridge you can have multiple subinterfaces, with different VLANs, as members of a (traditional) bridge.I’m wondering why this is not possible with Cumulus - is this a hardware / asic limitation?I would like to configure a bridge as follows:auto br0
iface br0
bridge-ports swp2.10 swp2.11 swp2.12
bridge-vlan-aware noHi Swimgeek,It would be possible in hardware, but it is not implemented at the moment. By default frames are not allowed to be forwarded on the same port they were received on.However, could you explain how you are using this feature? Perhaps there are different solutions with the multiple vlan-aware bridge for example.Hi, thanks for the quick response.I’d like to use a Mellanox SN2010 to terminate a connection to a fibre to the home network - where the fibre network provides about 15 VLANs, one VLAN per geographic area. Ideally I want all the VLANs (and customer MACs) in one bridge where a DHCP server provides them with IPs from a single pool.I have this working with a linux software router at the moment, but we’d like to change to a hardware solution.I believe in Arista lingo this feature is called “Layer 2 Subinterfaces”:“A Layer 2 subinterface is a logical bridging endpoint associated with traffic on an interface distinguished by 802.1Q tags, where each interface, 802.1q tag tuple is treated as a first-class bridging interface.”frames are not allowed to be forwarded on the same port they were received onWe’re happy with this idea - currently we have nftables rules to isolate the vlans in the bridge - each vlan should only talk to the upstream router.If they are 15 different VLANs (in your case for separating geographic areas), wouldn’t you want to start routing at that aggregation point? I have to assume the gateway for all VLANs exist on the 2010. With your suggestion you would be bridging the different VLANs which is typically not such a good idea.If the sole purpose it to provide DHCP services, you can do that routed with a dhcp relay agent as well.Thanks, I tested some routing ideas using the Cumulus VX Air lab - seems to work as expected. We’ll look into using dhcp relays and adding /32 routes for each customer.Great, let us know if you need anything! AIR is a great resources to test this out.Powered by Discourse, best viewed with JavaScript enabled"
277,performance-of-ipoib-with-200gbps-adapter,"I’m new to the community and this is my first post. Here is my question.We got some 200Gbps IB card and 200Gbps switch.The IB adapters are installed in 2 servers, each has PCIeGen4 X16 and 1 amd CPU(NUMA not bothers)We test the bandwidth with ib_send_bw --run_infinitely and observe the traffic with collectl -sX, 22GB/s is reached.We setup the ipoib with connected mode and ipoib_enhanced is set to 0. We test the TCP performance with iperf3. The performance is about 25Gbps for single TCP connection. As we setup more TCP connections, the performance increased up to 80Gbps. And we find out ksoftirqd used up one CPU core at receive side server.Analysis and tried:So the questions is:What should I tune the system to make iperf3 get performance of 200Gbps?Hi,Please open a support case at support@mellanox.com for further assistance.Thanks,SamerWhy hide the solution behind closed doors?We’ve got Connect-X 3 adapter running IPOIB. Getting the same problem – ksoftirqd/0 is using 99% of CPU soft interrupts. Any way to parallelize that across several cores? I’ve tweaked some things according to the setup guide (power, stable freq, etc), but, again, we are basically hitting the ceiling with that ksoftirqd stuff on the receiving side. What’s the solution, guys? Getting just 30Gbps with iperf3. I know it can push 50, why not? Thanks!Powered by Discourse, best viewed with JavaScript enabled"
278,need-guidance-for-infrastructure-design-for-my-homelab,"Hello,I need guidance on how the best way to set up my lab.
Server 1 &2 are dell R630’s. They have dual port VPI connecct-x 5
Server 3 is a QCT storage server. It has a single port VPI Connect-x 5
I have a Dell 4112-ON-F switch that is dedicated for storageMy thought is to configure it so the two hosts are directly connected to each other with one of the 100g and then uplink the other and the storage server to the switch.The problem I am running into is it’s very confusing which technology I should be using for storage. Is iSER the latest and greatest? Should I be looking at SRP? Should I be using PFC and queuing? I have set ip iSER\iSERT before…I just am really struggling on figuring out the best way to move forward. Any guidance would be appreciated.Powered by Discourse, best viewed with JavaScript enabled"
279,mellanox-mcx516a-bdat-does-it-support-xdp-and-can-we-direct-connect-it-to-an-adapter-with-qsfp,"Hello,we’re thinking of arranging our 40G packet sniffers with Mellanox MCX516A-BDAT. But we have some considerations regarding:Thank you very much.Hello Alexander,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided:From ConnectX-4 and above we support XDP → https://docs.mellanox.com/display/MLNXENv531001/Release+Notes+Change+Log+History (Search for XDP). The following page provides an example on how-to run on ConnectX-5 → https://docs.mellanox.com/display/MLNXENv531001/Release+Notes+Change+Log+HistoryEOL announcements can be found through the following link → https://mellanox.com/support/eolYes, You can use a validated/supported cable/transceiver to connect back-2-back between to adapters, without the use of a switch. See the following link for the supported cable for the ConnectX-5 → https://docs.mellanox.com/display/ConnectX5Firmwarev16311014/Firmware+Compatible+Products#FirmwareCompatibleProducts-ValidatedandSupported40GbECablesYou can configure the required speed through ‘ethtool’ which is available for many Linux distro’sConnectX-4Thank you and regards,~NVIDIA Networking Technical SupportThanks Martijn for your detailed answer.Can I please be answered one more thing regarding XDP support.What level of XDP support there is?Is it Native (driver) XDP: The kernel executes the program from the earliest possible point during packet reception. At this moment, the kernel did not parse the packet and, therefore, no metadata provided by the kernel is available. This mode requires that the network interface driver supports XDP but not all drivers support this native mode.Or is it…Offloaded XDP: The kernel executes the XDP program on the network interface instead of on the host CPU. Note that this requires specific hardware, and only certain eBPF features are available in this mode.?(as described in - XDP section hereChapter 55. Understanding the eBPF networking features in RHEL Red Hat Enterprise Linux 8 | Red Hat Customer Portal)Hello,can you kindly advise. We’re waiting for the answer so we can make an informative decision here.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
280,mlxconfig-does-not-install,"Running Ubuntu 20.04 LTS
4 MCX4121A-ACAT CardsAttempting to install these cards with DPDK support. Following the steps listed here: https://docs.nvidia.com/networking/display/MLNXOFEDv551032/Installing+MLNX_OFEDI have tried reinstalling several times, using this command: ./mlnxofedinstall --without-dkms --add-kernel-support --kernel 5.13.0-39-generic --dpdk --without-fw-update --forceWhen I try to use the mlxconfig tool, it is not found. I have restarted etc. It does not appear to be installed in /usr/bin/ either.  These are the only packages installed:-rwxr-xr-x 1 root root 7208 Dec 31 2012 mlnx_conf_mgr.sh
-rwxr-xr-x 1 root root 8058 Apr 12 2021 mlnx_dump_parser
-rwxr-xr-x 1 root root 14268 Dec 31 2012 mlnx_interface_mgr.sh
-rwxr-xr-x 1 root root 4545 Apr 12 2021 mlnx_perf
-rwxr-xr-x 1 root root 19209 Apr 12 2021 mlnx_qos
-rwxr-xr-x 1 root root 16108 Apr 12 2021 mlx_fs_dumpOutput:Note: This program will create MLNX_OFED_LINUX TGZ for ubuntu20.04 under /tmp/MLNX_OFED_LINUX-5.5-1.0.3.2-5.13.0-39-generic directory.
See log file /tmp/MLNX_OFED_LINUX-5.5-1.0.3.2-5.13.0-39-generic/mlnx_iso.6787_logs/mlnx_ofed_iso.6787.logChecking if all needed packages are installed…
Building MLNX_OFED_LINUX DEBS . Please wait…
Creating metadata-rpms for 5.13.0-39-generic …
WARNING: If you are going to configure this package as a repository, then please note
WARNING: that it is not signed, therefore, you need to set ‘trusted=yes’ in the sources.list file.
WARNING: Example: deb [trusted=yes] file:/ ./
Created /tmp/MLNX_OFED_LINUX-5.5-1.0.3.2-5.13.0-39-generic/MLNX_OFED_LINUX-5.5-1.0.3.2-ubuntu20.04-ext.tgz
Removing old packages…
Uninstalling the previous version of MLNX_OFED_LINUX
Installing /tmp/MLNX_OFED_LINUX-5.5-1.0.3.2-5.13.0-39-generic/MLNX_OFED_LINUX-5.5-1.0.3.2-ubuntu20.04-ext
/tmp/MLNX_OFED_LINUX-5.5-1.0.3.2-5.13.0-39-generic/MLNX_OFED_LINUX-5.5-1.0.3.2-ubuntu20.04-ext/mlnxofedinstall --force --without-dkms --without-dkms --kernel 5.13.0-39-generic --dpdk --without-fw-update --force
Logs dir: /tmp/MLNX_OFED_LINUX.367183.logs
General log file: /tmp/MLNX_OFED_LINUX.367183.logs/general.logBelow is the list of MLNX_OFED_LINUX packages that you have chosen
(some may have been added by the installer due to package dependencies):ofed-scripts
mstflint
mlnx-tools
mlnx-ofed-kernel-utils
mlnx-ofed-kernel-modules
rdma-core
libibverbs1
ibverbs-utils
ibverbs-providers
libibverbs-dev
librdmacm1
rdmacm-utils
librdmacm-dev
libibumad3
ibacm
python3-pyverbsThis program will install the MLNX_OFED_LINUX package on your machine.
Note that all other Mellanox, OEM, OFED, RDMA or Distribution IB packages will be removed.
Those packages are removed due to conflicts with MLNX_OFED_LINUX, do not reinstall them.Checking SW Requirements…
Removing old packages…
Installing new packages
Installing ofed-scripts-5.5…
Installing mstflint-4.16.0…
Installing mlnx-tools-5.2.0…
Installing mlnx-ofed-kernel-utils-5.5…
Installing mlnx-ofed-kernel-modules-5.5…
Installing rdma-core-55mlnx37…
Installing libibverbs1-55mlnx37…
Installing ibverbs-utils-55mlnx37…
Installing ibverbs-providers-55mlnx37…
Installing libibverbs-dev-55mlnx37…
Installing librdmacm1-55mlnx37…
Installing rdmacm-utils-55mlnx37…
Installing librdmacm-dev-55mlnx37…
Installing libibumad3-55mlnx37…
Installing ibacm-55mlnx37…
Installing python3-pyverbs-55mlnx37…
Selecting previously unselected package mlnx-fw-updater.
(Reading database … 216252 files and directories currently installed.)
Preparing to unpack …/mlnx-fw-updater_5.5-1.0.3.2_amd64.deb …
Unpacking mlnx-fw-updater (5.5-1.0.3.2) …
Setting up mlnx-fw-updater (5.5-1.0.3.2) …Added 'RUN_FW_UPDATER_ONBOOT=no to /etc/infiniband/openib.confSkipping FW update.
Device (17:00.0):
17:00.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
Link Width: x8
PCI Link Speed: 8GT/sDevice (17:00.1):
17:00.1 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
Link Width: x8
PCI Link Speed: 8GT/sDevice (31:00.0):
31:00.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
Link Width: x8
PCI Link Speed: 8GT/sDevice (31:00.1):
31:00.1 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
Link Width: x8
PCI Link Speed: 8GT/sDevice (b1:00.0):
b1:00.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
Link Width: x8
PCI Link Speed: 8GT/sDevice (b1:00.1):
b1:00.1 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
Link Width: x8
PCI Link Speed: 8GT/sDevice (ca:00.0):
ca:00.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
Link Width: x8
PCI Link Speed: 8GT/sDevice (ca:00.1):
ca:00.1 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
Link Width: x8
PCI Link Speed: 8GT/sInstallation passed successfully
To load the new driver, run:
/etc/init.d/openibd restart
root@ts-Whitley:/mnt# etc/init.d/openibd restart
bash: etc/init.d/openibd: No such file or directory
root@ts-Whitley:/mnt# sudo /etc/init.d/openibd restart
Unloading HCA driver:                                      [  OK  ]
Loading HCA driver and Access Layer:                       [  OK  ]Hello,The Mellanox Firmware Tools (MFT) suite containing the mlxconfig utility, and can be downloaded and installed separately as needed.Download:The Mellanox Firmware Tools (MFT) package is a set of firmware management toolsInstallation Guide and Additional Documentation:
https://docs.nvidia.com/networking/display/MFTv4180/Compilation+and+InstallationThank you,
-Nvidia Network SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
281,error-during-burning-cc-image,"Hello,I’m working with ConnectX6-DX to activate PCC (programmable congestion control).I’ve done the prequirements according the PCC manual ver1.0
1. using RHEL 7.9
2. matching versions of firmware image and PCC image to 22.31.10
3. mlxconfig to use PCCBut during burning CC image, the error occurs as follows:tkhan@nxc-node0:~/mlnx_cc_user_app$ sudo flint -d /dev/mst/mt4125_pciconf0 -i cc_image -cc beta burn
FSMST_INITIALIZE -
OK
Writing CONGESTION_CONTROL component-
OK
-E- Burning CC image failed: MCC error, FW errorCan you give me any advice about this MCC and FW error?Best regards,
Taekyounghi,
could you try to first  set ROCE_CC_LEGACY_DCQCN to 0  and  USER_PROGRAMMABLE_CC to 1.the commands are:mlxconfig -d /dev/mst/mt4125_pciconf0 set ROCE_CC_LEGACY_DCQCN=0
mlxfwreset -d /dev/mst/mt4125_pciconf0 reset
mlxconfig -d /dev/mst/mt4125_pciconf0 set USER_PROGRAMMABLE_CC=1
mlxfwreset -d /dev/mst/mt4125_pciconf0 resetThen burn the image again.If still can’t.  try with the latest firmware. The firmware can be downloaded from:
https://network.nvidia.com/support/firmware/connectx6dx/Regards,
LeveiHello,First, thank you for your attention in this issue.I’ve actually done those prequirements (1. OS
setting (RHEL7), 2. matching submajor version and minor version of PCC and FW version, 3. mlxconfig to enable/disable USER_PROGRAMMABLE_CC /ROCE_CC_LEGACY_DCQCN )And we strongly guess that the cause of the MCC error in this burning process is the security attributes which is on secure-fw state.To avoid this, we tried to sign the cc_image by referring to the security firmware update in MFT4.17, but the same MCC error is repeated.
(Secure Firmware Update - MFT v4.17.0 - NVIDIA Networking Docs)Is there a way to change these security attributes or sign cc_image?Above all, I wonder if it is fundamentally possible to use PCC on a device in a secure-fw state.Any help would be appreciated.Best regards,
TaekyoungHi,Sign the image should no need anymore.
The sign action is removed from the newer version.
https://docs.nvidia.com/networking/display/MFTv422/Secure+Firmware+UpdateI think we need to investigate the issue more.
I see you’re using ConnnectX-6 Dx. Is it still under warranty?
If yes, I suggest sending the SN and the issue to our support:  networking-support@nvidia.com
So we can involve more resources to analyze the issue.Regards,
LeveiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
282,failed-to-run-ipsec-application-full-offload-failed,"Hi,I have a MBF2H332A-AENOT BlueField-2 P-Series DPU card which is Crypto Disabled. While tring to run IPsec application on the arm side, I got this error.Then, I try to configure it manually, however, it truns out there is no such directory ‘/sys/class/net/*/compat/devlink/ipsec_mode’.I am wondering whether I can run this application on my MBF2H332A-AENOT card or not.Thanks,
YiFuPowered by Discourse, best viewed with JavaScript enabled"
283,trouble-in-file-tcpdump-upload-in-mellanox-switch,"Hi,I am trying to capture traffic at the Mellanox SN2410 Ethernet Switch using tcpdump. Now I want to analyze the pcap file by importing it to a remote system. I tried several ways to access the file but I am unable to scp the file as I don’t know the location of the file and the file stored seems to not have permission to read. I also tried to upload the file from within the switch to scp to the remote system but that gives me public key authentication error. Can you please help me with this issue.Thank you,
ParidhikaHi Paridhika,If you are using Nvidia ONYX (which I believe you are based on the description of your problem), you may use the following syntax to accomplish what you are looking to do.If you continue to face issues, I suggest you open a support case to get assistance.Thank you!Hi,Thank you for your reply.
I am using this command
file tcpdump upload <filename> scp://<username>@<IP>/<filename>
But it gives me  Permission denied (publickey,password).
Can you please tell me what is vrf_name here?
I am trying to find out the location <path> where the tcpdump file is saved on the switch. Do you have any idea about that?Thanks!HI Paridhika,vrf_name is the vrf on the switch for which the target machine is reachable via.  If you are using the defualt vrf, then you can simply exclude the “vrf <vrf_name>” option from the command.Permission denied (publickey,password) error you are getting is from your target machine and has nothing to do with the ONYX switch.  It indicates you are using incorrect credentials (ssh keys or password) for the user that you are attempting to authenticate with.If you continue to face issues, you may install an sftp application like winscp and sftp to the switch itself and navigate to /var/opt/tms/tcpdumps/ in there you will find a “user” directory and a “wjh-pcaps” directory.  The “user” directory is where the tcpdumps you run manually and wrote to a file are saved.  The “wjh-pcaps” directory is where the “What-Just-Happened” feature writes its pcap files.  From there, you will be able to copy the files off the switch to your local machine.Hope this helps!Hi,That works, found my files.  I can scp from the file path /var/opt/tms/tcpdumps/.Thank you so much for the help.Best,
ParidhikaParidhika,Thanks for confirming this solution worked for you!  Glad we were able to sort it out.  Please don’t hesitate to reach out to NVIDIA for any other queries you may have in the future.Powered by Discourse, best viewed with JavaScript enabled"
284,bug-9-2-cm-image-create-swimage-a-x86-64-d-ubuntu2204-bootstrap-fails,"Currently, cm-image create swimage -a x86_64 -d ubuntu2204 --bootstrap fails on 9.2. It tries to install the linux-image-5.15.0-47 kernel, which has been deleted, see here: UbuntuUpdates - Package ""linux-modules-extra-5.15.0-47-generic"" (jammy 22.04)I can go in and install another kernel manually, but afterwards, creating the initramfs seems to fail and the image is not bootable. Can someone suggest a workaround?Powered by Discourse, best viewed with JavaScript enabled"
285,ovs-not-up-in-embedded-mode-of-bluefield-2,"Hi all, I recently tried to switch Bluefield-2 Infiniband DPU from Separated Host Mode to ECPF Mode and had difficulties with OVS startup. My steps are as follows.I changed INTERNAL_CPU_MODEL to 1 according to DPU OS 3.9.0(https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/Modes+of+Operation), and checked  /etc/ mellanox/mlnx-ovs.conf configuration:Everything looked fine, then I power cycled the server. Afterwards, I went up to the DPU and checked with sudo ovs-vsctl show and the result was as follows.It was empty. Unsuccessful bridging.Then I tried to reinstall the DPU OS with BFB. My package is DOCA_1.3.0_BSP_3.9.0_Ubuntu_20.04-6.signed. To coordinate with the version on the DPU, I also reinstalled the ofed (5.6.1.0.3) and Bluefield driver for the Host environment. I check /etc/mellanox/mlnx-ovs.conf before power cycling the server with the same result as above and subsequently power cycled the machine.
Nothing changed with ovs, still unsuccessful bridging.I went through the contents of BlueField DPU OS 3.9.0-Deploying DPU OS Using BFB from Host-Default Ports and OVS Configuration and checked the contents of /etc/modprobe.d/mlnx-bf.conf:
install ib_umad /sbin/modprobe --ignore-install ib_umad $CMDLINE_OPTS && (if [ -x /sbin/mlnx_bf_configure ]; then /sbin/mlnx_bf_configure; fi)This seems inconsistent with the description of The /sbin/mlnx_bf_configure script runs automatically with mlx5_ib kernel module loaded in the documentation, and I’m not sure if this is the cause of the ovs failure.I also tried running /sbin/mlnx_bf_configure directly and nothing happens. mlnx-sf -a show  print nothing but an empty line, ovs-vsctl show print results that was the same as before.In addition to the above, I checked the en3f1pf1sf0 port with the command ifconfig en3f1pf1sf0 and found the error:
en3f1pf1sf0: error fetching interface information: Device not foundOk, thank you very much for reading this, this is all I have tried to do for this problem, and now there is nothing I can do. Can somebody give me a hand with this? I would like to offer my sincere thanks.P.S. Our device works fine in Separated Host Mode, so I don’t think it’s a connection or hardware failure, but I welcome your criticism to point out my potential mistakes.Powered by Discourse, best viewed with JavaScript enabled"
286,any-virtualization-platform-for-infiniband-vmware-kvm-or,"I wanted to get started with infiniband network but that involves HBA (usually mellanox), IB switch and IB gateway/bridge to connect to normal ethernet network. Those are significant investments if i do at home network.Any virtualization O/S that let you create virtual IB switch + VM with IB HCA virtual interface? Then I can create may VMs.
So far in the past, I have worked with VMware workstation/sphere, HyperV, KVM but I dont remember any of them lets you create network other than ethernet type network.Hi guyen900,AFAIK there is  no Virtualization available for for Virtual IB switch + IB HCA.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
287,how-to-evaluate-the-eye-information-of-mellanox-connectx-4-network-card,"I have a server (OS:Centos7.6 aarch64) with a Mellanox ConnectX-4 network card.The CX4 network card is on a Riser card with an extended PCIe signal line. So I wanna to know if the signal of CX4 is reliable.Following is the information which maybe useful:2.Link grade and eye informationI have the information but I don’t know how to understand it. Could those numbers tell me whether my CX4 is reliable or not?From the result you provide, seems this is in very good condition.Please check following：​Effective Physical BER : 15E-255 ---------which mean no error foundFor EYE Opening value , this is also very good .ThaksIs there a standard for eye information?“For EYE Opening value , this is also very good .” If I have another new environment, how can I tell if it is good or not?Powered by Discourse, best viewed with JavaScript enabled"
288,oracle-linux-6-9-install-driver-happened-fault-failed-to-build-rshim1-8-rpm-how-can-i-resolve-it,"I failed when I installed the OL6.9 driver. Checking the installation log shows that rshim1.8 RPM RPM cannot be created, but I continue to check the installation log of  rshim1.8 RPM, and there is no error message in it. You can check the attached log file, but there is no relevant solution here.
sysinfo-snapshot-v3.6.5-localhost.localdomain-20230511-112227.tgz (560.2 KB)The latest version found that supports OL 6.9 is 4.7-3.2.9.0 (very very old combo), as a starter, I would validate that the kernel installed is supported (ref documentations Linux InfiniBand Drivers) and if supported and not default, that the driver is recompiled with --add-kernel-support flag. You can reference to our UM from the link I posted above.You can also install our driver with “–without-rshim” for the time being.At last, I would recommend a more recent OS/MLNX_OFED versions.you have the option as well to open a support case to Networking-support@nvidia.com should you want to further investigate this issue.hi,spruit:
thanks for you reply ,i resolve it use your solusion :–without-rshim parameter.thanks very much.Powered by Discourse, best viewed with JavaScript enabled"
289,innova-2-flex-connectx-5-ethernet-to-fpga-direct-communication,"I would like to enable communication between the XCKU15P FPGA and the 25GbE interfaces on the Innova-2 Flex SmartNIC.A team associated with Nvidia Networking has the FlexDriver Project which supposedly enables this.However, it requires that I “acquire FlexDriver IP (src/flc.dcp) from NVIDIA Networking”. Is this available online?Or, is there any online documentation for setting registers in the ConnectX-5 MT27808? The Innova-2 Documentation mentions you can use setpci -s 3c:08.0 0x70.w=0x50 to control the link between the ConnectX-5 PCIe switch and the FPGA. Is there a register to disable the ConnectX-5 to Host link and set the FPGA as the primary link?I want the same; i.e. to send/receive packets under the control of the FPGA, by creating custom logic in the fabric. How can one get this IP block?Powered by Discourse, best viewed with JavaScript enabled"
290,dmesg-flooded-with-ofed-related-warnings,"Hi everyone,We have a dozen Dell servers with CX3-series infiniband adapters. The M630 servers are connected to a Dell M4001T Infiniband switch, and the newly arrived R740XD are connected to a SX6025 switch. Both switches are connected with two QSFP cables.We use R740XD as glusterfs servers. However vdbench tests using M630 as clients constantly failed, complaining brick disconnection, which made us suspect the connection. dmesg output is flooded with warnings that may relate to MLNX OFED.Can somebody advise if something is indeed wrong with the adapters or is it a compatibility issue?Thanks,WadeM630:[7778551.705403] ------------[ cut here ]------------[7778551.705407] WARNING: CPU: 12 PID: 145278 at /var/tmp/OFED_topdir/BUILD/mlnx-ofa_kernel-4.6/obj/default/drivers/infiniband/core/cma.c:689 cma_acquire_dev_by_src_ip+0x21e/0x230 [rdma_cm][7778551.705409] Modules linked in: socwatch2_11(OE) sep5(OE) socperf3(OE) pax(OE) nls_utf8 isofs loop nfsv3 nfs_acl vtsspp(OE) sep4_1(OE) socperf2_0(OE) fuse rpcsec_gss_krb5 auth_rpcgss nfsv4 dns_resolver nfs lockd grace fscache rdma_ucm(OE) ib_ucm(OE) rdma_cm(OE) iw_cm(OE) ib_ipoib(OE) ib_cm(OE) ib_umad(OE) mlx5_fpga_tools(OE) mlx5_ib(OE) mlx5_core(OE) mlxfw(OE) mlx4_en(OE) mlx4_ib(OE) ib_uverbs(OE) ib_core(OE) sb_edac intel_powerclamp coretemp intel_rapl iosf_mbi kvm_intel kvm irqbypass crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd sg iTCO_wdt iTCO_vendor_support mxm_wmi dcdbas ipmi_si ipmi_devintf joydev pcspkr ipmi_msghandler acpi_power_meter wmi shpchp mei_me mei lpc_ich sunrpc knem(OE) ip_tables ext4 mbcache jbd2 sd_mod crc_t10dif crct10dif_generic mgag200[7778551.705450] i2c_algo_bit drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops ttm drm ahci mlx4_core(OE) libahci crct10dif_pclmul tg3 crct10dif_common crc32c_intel libata megaraid_sas i2c_core mlx_compat(OE) ptp devlink pps_core [last unloaded: pax][7778551.705466] CPU: 12 PID: 145278 Comm: glusterrdmaehan Tainted: G W OE ------------ 3.10.0-862.el7.x86_64 #1[7778551.705468] Hardware name: Dell Inc. PowerEdge M630/0R10KJ, BIOS 2.8.0 05/23/2018[7778551.705469] Call Trace:[7778551.705472] [] dump_stack+0x19/0x1b[7778551.705475] [] __warn+0xd8/0x100[7778551.705478] [] warn_slowpath_null+0x1d/0x20[7778551.705482] [] cma_acquire_dev_by_src_ip+0x21e/0x230 [rdma_cm][7778551.705486] [] rdma_bind_addr+0x91f/0x9e0 [rdma_cm][7778551.705489] [] ? path_openat+0x172/0x640[7778551.705492] [] ? mutex_lock+0x12/0x2f[7778551.705495] [] ucma_bind+0x93/0xe0 [rdma_ucm][7778551.705499] [] ucma_write+0xd8/0x160 [rdma_ucm][7778551.705502] [] vfs_write+0xc0/0x1f0[7778551.705505] [] SyS_write+0x7f/0xf0[7778551.705508] [] system_call_fastpath+0x1c/0x21[7778551.705510] —[ end trace f51c264d0e36b3f0 ]—[7778551.705513] ------------[ cut here ]------------​R740XD:OE ------------ 3.10.0-1160.el7.x86_64 #1[863438.434467] Hardware name: Dell Inc. PowerEdge R740xd/06WXJT, BIOS 2.12.2 07/09/2021[863438.434471] Call Trace:[863438.434473] [] dump_stack+0x19/0x1b[863438.434476] [] __warn+0xd8/0x100[863438.434479] [] warn_slowpath_null+0x1d/0x20[863438.434482] [] cma_acquire_dev_by_src_ip+0x215/0x220 [rdma_cm][863438.434485] [] rdma_bind_addr+0x8fa/0x990 [rdma_cm][863438.434489] [] ? mutex_lock+0x12/0x2f[863438.434492] [] ucma_bind+0xac/0x100 [rdma_ucm][863438.434494] [] ucma_write+0x101/0x180 [rdma_ucm][863438.434497] [] vfs_write+0xc0/0x1f0[863438.434501] [] SyS_write+0x7f/0xf0[863438.434503] [] system_call_fastpath+0x25/0x2a[863438.434505] —[ end trace 748a792a8e2e93f8 ]—[863438.434512] ------------[ cut here ]------------[863438.434515] WARNING: CPU: 12 PID: 210017 at /var/tmp/OFED_topdir/BUILD/mlnx-ofa_kernel-4.9/obj/default/drivers/infiniband/core/cma.c:709 cma_acquire_dev_by_src_ip+0x215/0x220 [rdma_cm][863438.434518] Modules linked in: binfmt_misc iptable_filter ip_tables iscsi_target_mod dm_thin_pool dm_persistent_data dm_bio_prison dm_bufio joydev target_core_user uio target_core_mod loop rpcsec_gss_krb5 auth_rpcgss nfsv4 dns_resolver nfs lockd grace fscache tun rdma_ucm(OE) ib_ucm(OE) rdma_cm(OE) iw_cm(OE) ib_ipoib(OE) ib_cm(OE) ib_umad(OE) mlx5_fpga_tools(OE) mlx5_ib(OE) ib_uverbs(OE) mlx5_core(OE) mlxfw(OE) bonding mlx4_en(OE) bridge stp llc ip_set nfnetlink sunrpc vfat fat iTCO_wdt dell_smbios iTCO_vendor_support dell_wmi_descriptor dcdbas skx_edac intel_powerclamp coretemp intel_rapl iosf_mbi kvm_intel kvm irqbypass crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd pcspkr sg ipmi_ssif i2c_i801 mei_me mei lpc_ich wmi ipmi_si ipmi_devintf ipmi_msghandler[863438.434557] acpi_power_meter acpi_pad knem(OE) xfs libcrc32c mlx4_ib(OE) ib_core(OE) sd_mod crc_t10dif crct10dif_generic mgag200 i2c_algo_bit drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops ttm crct10dif_pclmul crct10dif_common crc32c_intel ahci drm mlx4_core(OE) libahci megaraid_sas tg3 libata bnxt_en mlx_compat(OE) ptp devlink pps_core drm_panel_orientation_quirks nfit libnvdimm dm_mirror dm_region_hash dm_log dm_mod fuse [last unloaded: ip_tables][863438.434578] CPU: 12 PID: 210017 Comm: glusterrdmaehan Kdump: loaded Tainted: G W OE ------------ 3.10.0-1160.el7.x86_64 #1[863438.434579] Hardware name: Dell Inc. PowerEdge R740xd/06WXJT, BIOS 2.12.2 07/09/2021[863438.434580] Call Trace:[863438.434583] [] dump_stack+0x19/0x1b[863438.434588] [] __warn+0xd8/0x100[863438.434591] [] warn_slowpath_null+0x1d/0x20[863438.434595] [] cma_acquire_dev_by_src_ip+0x215/0x220 [rdma_cm][863438.434598] [] rdma_bind_addr+0x8fa/0x990 [rdma_cm][863438.434600] [] ? mutex_lock+0x12/0x2f[863438.434605] [] ucma_bind+0xac/0x100 [rdma_ucm][863438.434607] [] ucma_write+0x101/0x180 [rdma_ucm][863438.434610] [] vfs_write+0xc0/0x1f0[863438.434613] [] SyS_write+0x7f/0xf0[863438.434619] [] system_call_fastpath+0x25/0x2a[863438.434620] —[ end trace 748a792a8e2e93f9 ]—​Hello,Without more information on the specifics of the environment, such as the HCA model, the version of OFED installed, the firmware version installed on the adapter, and other details it will not be possible to determine if this is a compatibility issue.Please be advised that we do not directly provide support for vdbench issues, and recommend investigating the cause of any Mellanox adapters’ issues separately from the gluster environment first if possible.We recommend ensuring that you have the latest supported MLNX_OFED and Mellanox ConnectX-3 firmware versions installed for your model.For the latest Mellanox ConnectX-3 adapter firmware and release notes, please visit the following link:Mellanox offers two firmware tools to update and query adapter firmware: mlxup & MFTFor the latest MLNX_OFED download links and User Manual, please visit the following link:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)Once these are installed, please test connectivity with the utilities documented within the MLNX_OFED User Manual. There are also a number of benchmark utilities included with MLNX_OFED that may be useful in your testing for network and RDMA performance:https://community.mellanox.com/s/article/perftest-packageIf you find that you need further support in debugging a connectivity issue, please consider opening a case with our support team. If you do not have a current support contract, please email the team at Networking-contracts@nvidia.com to set a valid support contractThank you,-Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
291,can-the-msn2700-cs2f-switch-be-used-as-a-dhcp-server-please-ask-for-relevant-configurations,"I looked for the user manual and only saw the dhcp relay configuration of the switch, without any information about the dhcp server configuration. I ask the leaders to answer my questionsThe switch information is as follows
model: MSN2700-CS2F
version: 3.10.3004Onyx does not have native DHCP server support.However you can use Docker to deploy a DHCP server. Here is a configuration example:
https://enterprise-support.nvidia.com/s/article/how-to-deploy-docker-container-with-dhcp-service-over-mellanox-onyx-on-mellanox-spectrum-switchesCharles Stizza
Global Technical Support
NVIDIA, NVIDIA Enterprise ExperiencePowered by Discourse, best viewed with JavaScript enabled"
292,connectx6dx-rte-flow-rss-performance-drop-on-mixed-traffic,"Dear Support,card infos at end of email.There appears to be a huge performance issue on mixed UDP/TCP using symmetric load-balancing accross multiple workers. We are using the option “rxq_cqe_comp_en=4”
E.g. using default test-pmd on a DPDK v20.11 or newer :sudo ./dpdk-testpmd -n 8 -l 4,6,8,10,12,14,16,18,20  -a 0000:4b:00.0,rxq_cqe_comp_en=4  -a 0000:4b:00.1,rxq_cqe_comp_en=4  – --forward-mode=mac --rxq=8 --txq=8 --nb-cores=8 --numa -i -a --disable-rssand configuring:flow create 0 ingress pattern eth / ipv4 / tcp / end actions rss types ipv4-tcp end queues 0 1 2 3 4 5 6 7 end key 6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A / endflow create 0 ingress pattern eth / ipv4 / udp / end actions rss types ipv4-udp end queues 0 1 2 3 4 5 6 7 end key 6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A / endflow create 1 ingress pattern eth / ipv4 / tcp / end actions rss types ipv4-tcp end queues 0 1 2 3 4 5 6 7 end key 6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A / endflow create 1 ingress pattern eth / ipv4 / udp / end actions rss types ipv4-udp end queues 0 1 2 3 4 5 6 7 end key 6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A / endwill see significant packet drops at a load > 50 Gbps on any type of mixed UDP/TCP traffic. E.g.
Whenever those packet drops occur, I see those in the xstats as “rx_phy_discard_packets”On the other hand using a TCP-or UDP-only traffic profile perfectly scales up to 100Gbps w/o drops.Thanks for your help!TobiasConnectX6DXPowered by Discourse, best viewed with JavaScript enabled"
293,sr-iov-pps-limiting,"Hi,I am looking for an SR-IOV VF pps limiting solution. As described in the documents [bandwidth limiting], connectX series NICs provide bandwidth limiting.Although I have not found the formal pps limiting solution, I figured out sysfs pps directory for connectX-6 dx.: /sys/class/net/{PF_NAME}/device/sriov/{VF_NUMBER}/meters/tx/pps/rateThis sysfs file works as I intended. From this, I have two questions as following:Thanks.VF metering is supported on both CX6DX/LX, but only available in switchdev mode.VF in legacy mode,
VF rate limit provides min/max_tx_rate bps traffic shaping. It doesn’t support shaping for RX or pps.•             /sys/class/net/<ETH_IF_NAME> /device/sriov//max_tx_rate•             /sys/class/net/<ETH_IF_NAME> /device/sriov//min_tx_rateThank you for your answer!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
294,how-to-find-the-maximum-number-of-rx-queues-for-a-nic-connectx-5,"Hi,I am using DPDK with an MLX5 card (MCX515A-CCAT single port 100Gbe).I seem to be hitting a wall when trying to use more than 32 RX Queues. I get good performance with 32 RX Ques, but it drops very significantly when I go over to 36 Queues.Is there a limit of 32 RX Queues per port? I have not been able to find this in the documentation.Is this limit configurable or there is nothing I can do?Would I be able to go higher with ConnextX-6?Thanks!Hello Baptiste,Thank you for posting your question on the Mellanox Community.To help best answer your question, please answer the following questions:# mst start# mst statusThen use the outputted device in the command # flint -d <mst_device> qFor example:# flint -d /dev/mst/mt4119_pciconf0 qThanks and regards,~Mellanox Technical SupportHello Baptiste,Thank you for posting your question on the Mellanox Community.When using more than 32 queues on NIC Rx, the probability for WQE miss on the Rx buffer increases. In answer to your question this would also apply to the ConnectX-6To determine if the the performance decrease is due to hardware or software you should check the out_of_buffer counter.This counter counts the number of times the NIC wanted to scatter packet but there was no receive WQE. When it is ~0 it means the SW is not the bottleneck. You can find more information on counters here:https://community.mellanox.com/s/article/understanding-mlx5-ethtool-countersThis behavior can be seen with lesser amount of queues (up to 32) if the system is not tuned according to the benchmark reports which can be found on the DPDK website. Here is the report for DPDK 20.11:1436.03 KBFor best performance please test using the settings used in the report.Another thing to note is that as of DPDK 18.05 and Mellanox OFED 4.3 support has been added for stride RQ. Multi-Packet Rx Queue (MPRQ a.k.a Striding RQ) can further save PCIe bandwidth by posting a single large buffer for multiple packets. Instead of posting a buffers per a packet, one large buffer is posted in order to receive multiple packets on the buffer. A MPRQ buffer consists of multiple fixed-size strides and each stride receives one packet. MPRQ can improve throughput for small-packet traffic. You can test using this feature for better performance by setting the parameter mprq_en=1.For more information on this parameter please see section 32.5.3. on this page:https://dpdk-power-docs.readthedocs.io/en/latest/nics/mlx5.htmlYou can also potentially further improve performance by improving CQE compression ratio using the following commands:sudo mcra mlx5_0 0x815e0.0 0xcff0f3ffsudo mcra mlx5_0 0x81600.0 0xcff0f3ffsudo mcra mlx5_0 0x815e8.31 0sudo mcra mlx5_0 0x81608.31 0In the above commands mlx5_0 is used as an example you can get your actual RDMA ports by using the commands:mst startmst status -vThese settings are active unless the machine is rebooted, please make sure you have MFT installed (this is installed with the Mellanox OFED) and the CQE compression mode is set to AGGRESSIVE. You can set this with the command mlxconfig -d <PCIe_address> s CQE_COMPRESSION=1For general tuning recommendations with our adapters please see the following tuning guide:https://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersFurther analysis of this would require engineering investigation. If you wish to further pursue this please contact support with a valid support contract through the Mellanox support portal found here:https://support.mellanox.com/s/Thanks and regards,~Mellanox Technical SupportHI Abigail,Thanks a lot for your reply.Why would the probably for a miss increase after 32 RX Queues? Could that be a significant drop? Our performance is dropping by more than 20% from 32 Queues to 33 Queues.I have currently no access to the benchmark machine, but I will check the out_of_buffer stats next week.We have seen that MPRQ support was added. Unfortunately, we currently need the hash result and since we have compression enabled, it seems that the hash is not fully supported with MPRQ. So, we have not done any test with MPRQ currently. I will try to run a test with our benchmark code.I will try to tune the compression ratios. We already have aggressive compression enabled.I will go over the different documents and see if we can find something, but I have already been through them before and it seems like we have made proper tuning. I just can’t seem to go over 88MppsBest regardsBaptisteHi Abigail,Thanks for your answerI am using 19.11We are using in-tree drivers not OFEDThe firmware version is 16.20.1010Best regardsBaptistePowered by Discourse, best viewed with JavaScript enabled"
295,performance-drop-of-the-regex-accelerator-on-bluefield-2,"Hi, I am testing the performance of the regex accelerator on BlueField-2 using RXPBench. I find that the processing throughput shows a drop when increasing the # of cores (processes).The regex buffer len is set to 64B, batch size is 64, on BlueField ARM OS, fix bytes/core, vary # of cores for RXPBenchAs shown in the results, we can see that the regex accelerator performance drops by ~20%. Intuitively, it might not be due to the queue contention. I have no idea about the reason for the drop. Could you please do me a favor? Is there any hardware limitation on the accelerator? Or are there some other insights for this result? Thanks. :)Hello,What is this results posted came from? In better words what exactly are you running?
(Reference link: RXPBench :: NVIDIA DOCA SDK Documentation).As basis, this is based on latest DOCA/FW version?What guidelines have you followed using RXPBench?Should deeper investigation apply, a support case will need to be opened with Nvidia.Sophie.Hi Sophie,Thanks a lot for your response. :)FYI, the RXPBench guideline I used is from here.And I use the DOCA_v1.2.1_BlueField_OS_Ubuntu_20.04-5.4.0-1023-bluefield-5.5-2.1.7.0-3.8.5.12027-1.signed-aarch64.bfb which is provide by the DOCA_v1.2.1 to install the BlueField OS, therefore, the default DOCA version should be v1.2.1.For the FW, I installed MLNX_OFED_LINUX-5.5-1.0.3.2-ubuntu20.04-x86_64.tgz,Thanks again for your quick apply.Best regards.Hello,So you are on the latest and greatest version.
I am checking internally if there is a penalty/limitation using RXPBench and specific number of cores but to my knowledge, that would be odd.Sophie.Hello,I inquired about possible penalties/limitations on the number of cores used by RXPBench and the answer provided;
In general yes. Multicore application can’t scale linearly with cores.
I am not sure if you are running the benchmark from the host or the DPU however, my suggestion would be to open a support case in order to dissect and further investigate multiple factors are in play here.Sophie.Hi Sophie,Thanks a lot for your response. :)I run the RXPBench from the DPU. I am curious that if such a performance drop is due to some hardware design problems that are a blackbox for me. Besides, are there ways to mitigate this from proper software design. Hope to discuss this in details.BTW, could you please tell me how to open a support case?Best regards.Powered by Discourse, best viewed with JavaScript enabled"
296,can-we-use-dpdk-in-cumulus-linux,"Cumulus Linux doesn’t support DPDK because Cumulus Linux has to use the per-ASIC driver in order to rx/tx kernel traffic via the front panel ports.The whole idea of DPDK is to bypass the interface drivers, which would mean DPDK would have to have the proprietary knowledge of how to interact with the ASIC.Also it’s our understanding that DPDK requires a proprietary NIC.Cumulus Linux already gets hardware acceleration though switchd and DPDK would bypass this to do forwarding in software, which would actually slow things down.Powered by Discourse, best viewed with JavaScript enabled"
297,download-enterprise-mib-files,"From where do I download Mellanox Onyx enterprise MIB files?  i.e. the files I see here:  Network Management Interfaces - Onyx v3.8.2306 - NVIDIA Networking Docs–skGo to the download center at https://support.mellanox.com/s/downloads-center.Select Download → Switches and Gateways → Switch Software → Mellanox Onyx
Select More Files.The MIB files are available there.
image1646×1037 148 KB
Powered by Discourse, best viewed with JavaScript enabled"
298,the-raw-throughput-of-bluefield-1-cannot-reach-the-line-rate,"Hi, I am testing the raw throughput of BlueField DPU, the version is MBF1L516A-CSNAT. I find that the throughput of 64B-packet flow cannot reach the line rate 100 Gbps (or 148 Mpps). I don’t know whether my measurement strategy is correct.Thanks. :)Hi user52115,Thank you for posting your inquiry to the NVIDIA developer forums.In your results, you state that you can achieve line rate using larger payload size (1500b), as opposed to smaller packet sizes (64b). This is expected - using smaller packets will achieve shorter latency, whereas using larger packets will enable higher throughput (at the cost of latency).You may be able to realize better results via system tuning:
https://support.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersIf you require further assistance with system tuning or benchmarking, please open a support case at: https://support.mellanox.com/s/Best,
NVIDIA Networking SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
299,vf-lag-offload,"Hi,I am trying to offload LAG in the case of VFs. I read on an official online document available on the Nvidia website that Mellanox supports three bonding modes that can be offloaded; Active-Backup, Balance-Xor, and 802.3ad. The link to the document is as follows:
https://docs.nvidia.com/networking/pages/releaseview.action?pageId=25133702I tried to create this scenario on my setup. I used ConnectX-5 cards for this experimentation. I configured the bond in Xor mode and was trying to observe if the hash is calculated to select the interface in the bond when it is offloaded but it was observed that the hash was not calculated and a specific interface was selected for every packet.I used TC rules and wrote the following script to create the setup:All of the rules were offloaded as I checked it using “tc filter show dev  ingress” command.  Even the last rule in which the packet is directed from VF Representor to bond0 was offloaded; even though bond0 is not a Mellanox interface. So, I guess it was translated to another rule which was eligible to get offloaded. But regardless of the rule, I did not see an interface selected on the hash as it should be in the XOR mode, and always a specific interface was selected no matter the packet. And, until the wire was removed from that interface, it kept on sending packets in this case. I even downed the interface and checked but the packets were still being transmitted using that same specific interface. I checked this using “ethtool -S ” and monitored the physical counters of both the interfaces. In this case, I should also mention that I was sending packets from VF Rep so that offloaded rules would get hit.I also sent packets directly from the bond and in this case as it was up to the Linux bonding driver to select the interface and none of the offloading rules were hit in this case then the interface was selected based on the hash policy.I used the below-given command to check if the offloaded rule against the VF Rep is being hit:
""watch -d -n 1 -p “tc -s filter show dev enp129s0f0_0 ingress | grep ‘Sent hardware’”Can you tell if I am doing something wrong or, why the hash is not being calculated in the case of VF Lag Offload?By default bond mode 2 base on MAC address to hash. If you test from one same client then one port select is expection.Bond Mode 2 – Balance XOR
In a balance XOR bond mode the bond will evaluate the source and destination mac addresses to determine which interface to send the network packets out. This method will pick the same interface for a given mac address and as a result is capable of load balancing and fault tolerance.And another issue is Switch.connected to the same switch,you need to configure static aggregation on the switch.switch iss a Layer 2 device, it will records the mapping between MAC addresses and ports , one MAC address can only be mapped to one port at a time. mode 2, all NICs under bond0 share same MAC address.If your switch supports LACP, consider using mode 4.Or try xmit_hash_policy=layer3+4This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
300,spectrum1-and-recursive-route-lookups,"HiI was wondering if a recursive routing table lookup is in any way less efficient in the Spectrum1 ASIC?Basic routing use case, for some routes learnt via iBGP the switch needs to do an extra lookup to find the destination in routes learnt via OSPF - when for example the next-hop is outside our AS on a peering router.Is the extra lookup maybe slower?I think I worked out the answer, recursive next-hop resolution occurs in the RIB, prior to the FIB being programmed.Powered by Discourse, best viewed with JavaScript enabled"
301,the-speed-and-bandwidth-of-200g-mellanox-network-card-card-model-cx6141105a-connextx-6-200gbe-are-less-than-200g,"The follow is our equipment list:200g mellanox network card: CX6141105A ConnextX-6 200GbEtwo card.PC: Two computers are used to install 200g network card.CPU processor:AMD Ryzen 5 5600x 6-Core Processor 3.69GHz. PC Memory size: 8G200G QSFP58 CR4 DAC Cable: OneWe use iper3 to test the 200G Mellanox network card’s speed and bandwidth, But find the Transfer rate and Bandwidth less than 200G? The following is the screenshots of the process
Transfer rate and Bandwidth less than 200G1076×1162 216 KB

200G  QSFP28 CR4 DAC1088×1123 135 KB
Hi Li,I would recommend not to use -b option which limits the bandwidth to 100Gbs from the screenshot provided.We recommend using iperf and iperf2 and not iperf3. iperf3 lacks several features found in iperf2, for example multicast tests, bidirectional tests, multi-threading, and official Windows support. For testing our high throughput adapters (100GbE), we recommend to use iperf2 (2.0.x) in Linux and NTTTCP in Windows.Please refer to the documentation below on how to test performance with iperf and run the iperf with 8-16 threads on step #4:https://community.mellanox.com/s/article/howto-install-iperf-and-test-mellanox-adapters-performanceIn addition, please make sure your servers are tuned according to our recommendations, below please review the article on our community site :https://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersIf after reviewing the documentations above and you still have issue with the link speed/bandwidth issue, please open a support case and we will provide further assistance. If you do not have a current support contract, please email the Nvidia team at Networking-contracts@nvidia.com to set up a valid support contract.Thank you,Nvidia Networking Support1.We use iperf2 in Win10,The speed and bandwidth of 200g mellanox network card are less than 200G.2.We use NTTTCP in Win10,The speed and bandwidth of 200g mellanox network card are less than 200G.The following is the screenshots of the process
iperf2 in win10751×513 10.2 KB
﻿
NTTTCP in Win10748×668 10.2 KB
Powered by Discourse, best viewed with JavaScript enabled"
302,how-to-use-adaptive-routing-in-ib-subnet,"I’m trying to measure throughput performance in our IB HDR testbed when adaptive routing is enabled.I received the following message from our administrator which confirms AR is enabled.==========================================================================================================Master SM: Port=1 LID=1420 GUID=0x88e9a4ffff2332f6 devid=4123 Priority:15 Node_Type=CA Node_Description=ufm1 HCA-3Standby SM: Port=1 LID=1246 GUID=0x88e9a4ffff1ffba8 devid=4123 Priority:10 Node_Type=CA Node_Description=agpu1301 HCA-1Standby SM: Port=0 LID=197 GUID=0xb8cef6030076cbca devid=54000 Priority:8 Node_Type=SW Node_Description=MF0;IBGPUDR1:MCS8500/S03/U1Adaptive Routing is enabled on 192 switches==========================================================================================================I understand that, to benefit from AR, it is necessary to provide some UCX parameters when launching MPI programs as follows:UCX_IB_AR_ENABLE=yes, UCX_IB_SL=autoMy questions are …(1) Is that all I need to do to test AR? Or, do I miss something?(2) What will happen if UCX_IB_AR_ENABLE=no is given when the subnet is configured to use AR?Will the throughput be degraded because all the out-of-order packets are simply discarded at the destination nodes?Thank you for your reply in advance.Hi Jongwook Lee,Thank you for posting your inquiry to the Mellanox community.Please review the following article for additional testing/validation methods that can be used:https://community.mellanox.com/s/article/How-To-Configure-Adaptive-Routing-and-SHIELD-New[Section 6.1; ‘Adaptive Routing validation’]The UCX documentation has a table which describes behavior under all of these circumstances:https://docs.mellanox.com/display/HPCXv281/Unified+Communication+-+X+Framework+Library[Section ‘Adaptive Routing’, second yellow box]Thanks again, and best regards;Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
303,the-mlxup-4-22-1-binary-and-checksum-mismatching,"For mlxup utility version 4.22.1 (Linux x64), after utility binary file downloaded and checked that bianry sha256sum is not same as SHA256 value shown on webpage.
Please check this issue. Thanks.Hello liye.zhao,Thank you for reporting this issue. We checked the SHA256SUM as well, and you are correct. It is not the correct checksum. We will have our web content team correct this.For your information, the SHA256SUM for the latest version is correct.Thank you,
~NVEX Technical SupportHello,
Thanks for you reply.
Regards, LiyePowered by Discourse, best viewed with JavaScript enabled"
304,connectx-7-and-vsphere-8-0-beta,"Does anyone know if any of the new cx-7 ethernet nics can work with the beta of vsphere 8.0?So far, only support to CX6 ESXi7.x, you need wait for future WMware ESXi release.https://network.nvidia.com/products/ethernet-drivers/vmware/esxi-server/https://customerconnect.vmware.com/en/downloads/details?downloadGroup=DT-ESXI70U2-MELLANOX-NMLX5_CORE-42171101&productId=974I was told and there is reference to vsphere 8 in the nvidia documentation that the cx-7 nics are supported.
image887×695 33.8 KB
This section describes VMware Driver Installation.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
305,rtnetlink-error-on-loading-xdp-program-to-connectx-5,"Hi Mellanox team and community,I’m trying to load an XDP program from the Linux kernel samples fromkernel 5.4, ubuntu 20, on a Mellanox ConnectX-5 100G dual-port NIC.At a high level, I’m seeing an RTNETLINK invalid argument error uponloading the XDP program on to a Mellanox ConnectX-5 NIC, even thoughthe error does not occur when I load the program to the localhost (lo)interface. This is why I’m posting to the Mellanox community, just incase the error is device-specific. I would be grateful for anyassistance!Here is what I did:First I compiled the Linux’s kernel BPF samples by runningmake LLC=llc-9 CLANG=clang-9 M=samples/bpffrom the source tree v5.4 on a stock Ubuntu20 distribution on an AMDEPYC processor (x86_64). (This step requires installing clang-9 andllvm-9 tools.)If I then type the commandsudo ip link set dev ens3f0 xdp object samples/bpf/xdp_fwd_kern.o section xdp_fwd verboseto load the xdp_fwd sample, I see the following error:RTNETLINK answers: Invalid argumentHere, ens3f0 is one of the Mellanox ConnectX-5 interfaces. (I’veattached the full output of the command below.)If I load the program to the localhost interface instead, it loadswithout errors (I’ve attached full output of the command below).Similar things happen if I try to run the user-space loader from thekernel itself, i.e.,sudo ./samples/bpf/xdp_fwd loworks without error, whereassudo ./samples/bpf/xdp_fwd ens3f0fails withERROR: failed to attach program to ens3f0It is clear that the adapter and driver support XDP (I’m able to seeXDP counters upon running ethtool -S ens3f0 | grep xdpKindly let me know if I can provide any more details.Is there any chance the error is related to some kernel ordevice-driver configuration?I would be very grateful for any help. Thanks!SrinivasAs seen from the dmesg message in the comment above, mlx5_core 0000:41:00.0 ens3f0: XDP is not allowed with MTU(9000) > 3498 , changing the MTU of the interface allowed me to load the program successfully. I used ifconfig ens3f0 mtu 3498Hi Srinivas,Please note that xdp will support jumbo frame in kernel v5.11 or v5.12.Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
306,vpi-configuration-issue-on-sx6036g-proxy-arp-does-not-come-up,"I try to configure a factory-resetted Mellanox SX6036SX. I need the VPI functionality, therefore I try to setup Proxy-ARP. The software versions are the following:MLNX-OS 3.6.8010PPC_M460EX 3.6.8010 2018-08-20 18:04:16 ppcI configure the switch as described in the manual like the following:ibsw2 [standalone: master] (config) # system profile vpi-single-switchswitch-6314f4 [standalone: master] (config) # no ip routing% Unrecognized command “routing”.Type “no ip ?” for help.switch-6314f4 [standalone: master] (config) # no ip igmp snoopingswitch-6314f4 [standalone: master] (config) # no ib smswitch-6314f4 [standalone: master] (config) # vlan 100switch-6314f4 [standalone: master] (config vlan 100) # exitswitch-6314f4 [standalone: master] (config) # ip proxy-arpswitch-6314f4 [standalone: master] (config) # interface proxy-arp 1switch-6314f4 [standalone: master] (config interface proxy-arp 1) # ip address 10.0.53.199switch-6314f4 [standalone: master] (config interface proxy-arp 1) # ip netmask /24switch-6314f4 [standalone: master] (config interface proxy-arp 1) # ip vlan 100switch-6314f4 [standalone: master] (config interface proxy-arp 1) # ip pkey 0x7fffswitch-6314f4 [standalone: master] (config interface proxy-arp 1) # no shutdownswitch-6314f4 [standalone: master] (config interface proxy-arp 1) # show interfaces proxy-arp 1After the last command, the configuration of the proxy-arp should be displayed, however, it is not. When looking at the web frontend, the checkbox “IP Proxy-ARP” is checked, however, all other options are greyed out (see screenshot).I attached the resulting configuration of the system to this message.Even worse, if I reboot the switch with this configuration, parts of the web interface (System / Ports) do not work (see screenshot). Moreover, all Infiniband ports are dead. If I disable the checkbox “IP Proxy-ARP” and reboot, the system is functional again.I also had a look at the log (see attached file). In line 8180 one can see the command to enable the Proxy-ARP. Then in line 8258 something seems to go wrong:Nov 19 19:37:42 switch-6314f4 prad[5352]: TID 1230881936: [prad.NOTICE]: prad_update_virtual_port: Enable swid=2 dev=1 lcl=66 idx=130 gda=falseNov 19 19:37:42 switch-6314f4 prad[5352]: TID 1230881936: [prad.NOTICE]: prad_exec_enable, INNov 19 19:37:42 switch-6314f4 prad[5352]: TID 1230881936: [prad.NOTICE]: prad_get_port_guid not ready, try 0[…]Nov 19 19:29:40 switch-6314f4 prad[5352]: TID 1230881936: [prad.ERR]: prad_get_port_guid(), prad_logic.c:86, build 1: prad_get_port_guid timed out waiting for valid /smad/state/swid_guid_map/2/port_guidNov 19 19:29:40 switch-6314f4 prad[5352]: TID 1230881936: [prad.ERR]: prad_exec_enable(), prad_logic.c:1477, build 1: failed to read guid, error is 14031The log entry “Enable swid=2” seems strange to me: I have only one switch in this configuration?Does anyone have a clue what’s happening here? Is this a configuration issue, software bug or hardware issue?Best Regards,Hermannlog(1).txt (550 KB)config-6036G-211119.txt (1.2 KB)
pa1.png1036×431 71.3 KB

pa2.png1039×260 35.7 KB
This is an unusual condition. I see a generic “internal error”. It seems the software/config got corrupted. My recommendation cold boot by unplugging/replugging power cables. If that does not help, try the procedure below:Unplug port cables, save the VPI licence (copy and paste to a file for backup - seen with “show licence”), and reset the configuration with “reset factory keep-basic”. Then after it reboots, reinstall the licence, mgmt IP/mask, and reconfigure the switch entirely from the CLI (not web gui). I believe that will work.Use this link for configuration instructions:https://support.mellanox.com/s/article/howto-configure-infiniband-gateway-ha--proxy-arp-xFYI: Once the configuration looks normal and the “show” commands work again, keep in mind the proxy-arp interface, in order to go UP/Active, will need at least one active Eth link, and an IB link to a cluster that has an Subnet Manager running in it.Thank you for your quick answer - however, it did not help:Nothing changed the behaviour of the switch regarding the proxy.So I can only suspect that this is some kind of firmware issue?I’d therefore like to try another firmware. However, I found no way to download / get any new or different firmware for this device - does anyone know how to do that?Best Regards,HermannPowered by Discourse, best viewed with JavaScript enabled"
307,mellanox-nic-real-time-performance-monitoring,"Hi,We are using Mellanox NIC connectx5 for RDMA data transfer.We would like to know how to measure RDMA data transfer rate in real time. Is there any tool to analyse RDMA operation in real-time?Thanks and Regards,
AbhishekHi!Thanks for contacting us.
To test RDMA data transfer in real time you can use this commands:RDMA /write/read/send bandwidth testInformation regarding all tools to test RDMA can be found in this link:
https://docs.nvidia.com/networking/display/MLNXOFEDv571020/InfiniBand+Fabric+Utilities#InfiniBandFabricUtilities-PerformanceUtilitiesFor further information and options, please refer to the tool’s man page.Thanks,
Ilan.Powered by Discourse, best viewed with JavaScript enabled"
308,what-is-the-difference-between-vnf-and-switch-in-pipe-mode,"According to the Flow programming guide, it describes vnf as “the packet arrives from one side of application, processed, and sent from the other side. The miss packet by default goes to the RSS of all queues.”
Does this mean the “missed packet” is processed by the application, while other packet goes by defined rules by default? What does the go to “RSS of all queues” mean? I assume it means on cores of RSS queues of a dpdk port.And switch mode is defined as “used for internal switching, only representor supported. The missed packet is received on all RSS queues of the representor of the uplink.”Is the difference between vnf and switch mode, in the difference of received on the “representor of uplink” or not?
Or is the difference in the “only representor port supported”, like the vnf is “only” supporting the “not” representor ports, for example PF/SF/VF?The switch picture draws the packet goes to the DOCA_FLOW app, how is it different from “the packet arrives from one side of application, processed, and sent from the other side” in vnf?Also the switch pictures shows “HOST/ARM”, but I assume the E-Switch is on the NIC, how can it be on host?Do vnf and switch mode both involving hardware offloading? Do both involve E-switch on ConnectX adapter?I run the simple_forward_vnf and doca_switch example, do they represent such differences?So basically I still not understood the difference between vnf and switch mode. I would appreciate so much if someone could explain such long questions.Hi kylelsun,I have anderstood that vnf works with basic ports , (switchdev mode legacy) so there is no representor interface. In this mode hw acceleration is possible in basically way . if match criterions then forward to dev.
Switch mode is more elaborate. It works in conjunction with switchdev mode eswitch so it introduce representor devices and I guess it can do lookup into eswitch fdb (need to be confirmed)Hi auger,Thanks for your kindly explanation.
I am not sure if it is correct to understand as the switch mode only allows representor ports,
About the switchdev mode difference, is there any document that to read and in relation to the the flow api?Best wishes
KyleYou’re right, switchdev does more than just allows representor ports  , but flow seems to only use the representor ports for dataplane operations.kernel switchdev documenttion is a good starting pointSwitchdev internals : Ethernet switch device driver modelhttps://www.kernel.org/doc/html/latest/networking/switchdev.htmlMellanox ASAP2 : Accelerated Switching and Packet Processing https://network.nvidia.com/related-docs/products/SB_asap2.pdfSwitchdev documentationhttps://www.kernel.org/doc/Documentation/networking/switchdev.txtProof of concepthttps://community.mellanox.com/s/article/Configuring-VXLAN-Encap-Decap-Offload-Using-tcEvolution of mlxsw in linux kernel (ASIC underlying driver)Contribute to Mellanox/mlxsw development by creating an account on GitHub.Powered by Discourse, best viewed with JavaScript enabled"
309,network-operator-in-rke2-cluster-for-gpudirect-workloads,"NVIDIA I have a RKE2 based K8s cluster that im attempting to enable GPUDirectRDMA in. I’ve deployed the GPU operator and it seems to be up and running. But when I attempt to deploy the Network Operator I get the following from the NicClusterPolicy
Name:   state-OFED
State:  notReady
Name:   state-SRIOV-device-plugin
State:  notReady
Name:   state-RDMA-device-plugin
State:  notReadyand I don’t see any containers associated with those items deploy (or even attempt to deploy)
I am following the example in the K8s cloud Orchestration Network Operator application notes under the heading Network Operator Deployment for GPUDirect WorkloadsHardware:
1 Dell R440 – Ubuntu 22.04 5.15.0-75-generic
1 Microserver 1 Dell R440 – Ubuntu 22.04 5.15.0-75-generic
4x Tesla V100
1x Meallanox Connectx-5 100Gb Model CX516AThis is the ansible I use to deploy both the GPU and Network Operator:I’ve also attached files with my values.yaml for deploying the operator and the outputs of kubectl describe and logs from the gpu operator.Any assistance is welcome I cant figure out where my config is wrongThank youFranknet-values.yaml (8.5 KB)Kubectl_logs_pod_nvidia-net-network-operator.txt (4.3 MB)kubectl_describe_NicClusterPolicy.txt (5.1 KB)kubectl_describe_HostDeviceNetwork.txt (1.6 KB)Kubectl_describe_nodes.txt (19.0 KB)Sorry for all the extra posts, the system would only let me add one file per-post
Kubectl_get_all.txt (11.9 KB)Hello @francis.bethuy and welcome to the NVIDIA developer forums!I am afraid I will not be able to help here since I don’t know much about this topic. If you are ok with it, I can move your post to the dedicated GPU RDMA category. In that forum there are also discussions about GPU RDMA and Mellanox setups.Thanks!Markus,if that is the correct location for this then please move it!FrankThank you!I moved it, but I keep this on my watchlist in case I was wrong. But for now this is the best place to start with in my opinion.Powered by Discourse, best viewed with JavaScript enabled"
310,severe-kernel-memory-leak-in-rpcrdma-both-in-centos-and-mellanox-drivers,"While searching why files <= 700 bytes would be corrupted in our HPC environment, I discovered that they are not only “corrupted”, but contain parts of the memory.
In short, a user can create a very simple loop in a shell script and harvest whatever random data in memory.
The issue seems to be related to the rpcrdma module. Servers running with xprtrdma have not such security issue.
I can reproduce this bug in every server running CentOS >= 7.7.Linux 3.10.0-1160.53.1.el7.x86_64 x86_64
CentOS Linux release 7.9.2009 (Core)
ConnectX-3 cardPretty astonishing that nobody seems to care about a security issue where any user can read random parts of the memory.Here’s fix NFS/RDMA on CentOS 7, small files corruption - Unix & Linux Stack Exchange
You’re welcome.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
311,control-the-flow-of-roce-v2-by-flow-programming,"Hello all,
We’ve done so many  tests on “dns_filter” app and we really wish it can help us to control the flow of RoCE v2. After some tests on it,we find that only when turning off the offloading of OVS and changing the dst_port(4791) of the flow,can we use the dns_filter app to forward this kind of offloading traffic.However the speed of RoCE v2 fell down to 30MB/S from 2700MB/s after turning  off the offloading function of OVS. We really don’t want low speed of RoCE v2 ,so is it possible for us  to control the flow of RoCE v2 by “Flow Programming”  when offloading the OVS? Really need your help,please.Hi,the problem for the speed decrease from disabling offloading comes from the CPU handling the flow rules and/or the OVS decission making. Which means, the packets will come in the interface, sent to the DPU, processed by the CPU, sent back to the eSwitch and forwarded to the target (if the target is not the DPU).
(see also https://docs.nvidia.com/doca/archive/doca-v1.2/pdf/modes-of-operation.pdf Chapter 2, figure)For RoCEv2 packets as you noted you can directly accept port 4791 with an offloaded flow rule in the eSwitch which then forwards all packets to the RDMA engine.
However since (to my knowledge) the eSwitch in the current generations of Bluefields do not support selecting RoCEv2 headers and also cannot change them, you should not be able to modify those.
In any case, if you were to be able to e.g. forward RoCE packets to another destination, not only do you need to change the IP, but also identifiers for the QP, PSN etc which might be different from destination to destination.But I also would be interested in selectors and modifiers for RDMA fields coming in the future.Best,
image380×631 35.5 KB
So glad to see your reply!But I think I didn’t explain our problem clearly.Actually We take the RoCE v2 packets as the normal UDP packets when handling them since it is based on UDP protocal.So as you can see in this picture we found on the DOCA SDK Documents,we changed the dst_port of RoCE v2 flow from 4791 to 4790 in ovs-br1 and recover the dst_port of it from 4790 to 4791 in ovs-br2.And if we don’t do it like this,we cannot use flow programming to forward RDMA flow.The problem is that only when we turned off the offloading of ovs-br1 and ovs-br2,the test of ib_send_bw (perftest) could be finished completely with a low sending speed like 30MB/s.After we used ovs-tcpdump to capture the packets of it ,we found lots of packets were sended twice or more.               Anyway, thanks for your reply ,sincerely.And after a lot of tests on DPU,I think I have found the solution by myself.**We can use dpdk-l2fwd or dpdk-testpmd to forward the RDMA traffic from pf0hpf to p0.**And I also think these two applications are more simple and easier for us to learn.Finally I still wonder why we can’t get the RDMA traffic from SF and why we use SF and OvS in doca examples like doca_dns_filter or doca_url_filter while we can get the target traffic directly from p0 or pf0hpf.And is it possible for us to use p0 and pf0hpf directly when starting doca examples? If so ,how to do that?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
312,how-to-connect-a-qsfp56-nic-to-a-qsfp28-switch,"We are planning to get a ConnectX-6 200G (something like MCX613106A-VDAT). Before we get a 200G QSFP56 switch, can we connect this card to a switch with only QSFP28 ports?Can we simply plug a QSFP28 fiber from the switch to the NIC?Hello Baptiste,Thank you for posting your inquiry on the NVIDIA Networking Community.The following link will provide you the information regarding the current supported cables and link-operability based on the ConnectX-6 network adapter → https://docs.mellanox.com/display/ConnectX6Firmwarev20311014/Firmware+Compatible+ProductsFrom here, you will able to determine the supported cables and switch port speed.Thank you and regards,~NVIDIA Networking Technical SupportThanks, I will look into this document in detail, look very complete!Powered by Discourse, best viewed with JavaScript enabled"
313,rdma-is-not-activated-on-my-mellanox-connectx-2-card,"Hello, I cannot activate RDMA on my 2 Mellanox ConnectX- MNPA19-XTR Cards and I get low speeds. How can I activate this on Debian 11 and Windows side.My Mellanox ConnectX-2 Card Spec ;Image mellanox2 hosted in ImgBBHello techdocstr,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Based on the info provide, the MNPA19-XTR is a 10Gb/s adapter. Also this p/n is EOL and EOS for a long time and driver support is not always available in the newer OS versions. Also RDMA functionality between Linux and Windows is very limited.It is always between Linux ↔ Linux, and Windows <->. There is currently no such tool available to measure RDMA performance between Linux and Windows, nor is it on the roadmap.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
314,doca-sample-compile-error,"Hi,
I have tried to compile sample code on BF2, but face error below. And I have tried the method in document 1.1. Meson Complains About Missing Dependencies . It still didn’t work, How to fix the problem thx.ubuntu@localhost**:/opt/mellanox/doca/samples/doca_flow/flow_monitor_meter$ sudo meson buildThe Meson build systemVersion: 0.61.2Source dir: /opt/mellanox/doca/samples/doca_flow/flow_monitor_meterBuild dir: /opt/mellanox/doca/samples/doca_flow/flow_monitor_meter/buildBuild type: native buildProgram cat found: YES (/usr/bin/cat)Project name: DOCA_SAMPLEProject version: 1.5.0055C compiler for the host machine: cc (gcc 9.4.0 “cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0”)C linker for the host machine: cc ld.bfd 2.34C++ compiler for the host machine: c++ (gcc 9.4.0 “c++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0”)C++ linker for the host machine: c++ ld.bfd 2.34Host machine cpu family: aarch64Host machine cpu: aarch64Found pkg-config: /usr/bin/pkg-config (0.29.1)Found CMake: /usr/bin/cmake (3.16.3)Run-time dependency doca-common found: NO (tried pkgconfig and cmake)meson.build:29:0: ERROR: Dependency “doca-common” not found, tried pkgconfig and cmakeI also got the same error. The information provided in the trouble shooting document did not work. Please provide any suggestion.I have tried to unset PKG_CONFIG_PATH and tried to follow trouble shooting document again and it workedtry with command “sudo -E meson build” which passes user environmental variables to sudoHave you tried setting these environment variables?export PKG_CONFIG_PATH=${PKG_CONFIG_PATH}:/opt/mellanox/doca/lib/aarch64-linux-gnu/pkgconfig:/opt/mellanox/flexio/lib/pkgconfig
export PKG_CONFIG_PATH=${PKG_CONFIG_PATH}:/opt/mellanox/dpdk/lib/aarch64-linux-gnu/pkgconfig
export PATH=${PATH}:/opt/mellanox/doca/tools-JPowered by Discourse, best viewed with JavaScript enabled"
315,connectx-3-on-ubuntu-20-04,"I’ve tried installing driver 4.9-2.2.4.0 (LTS) and 5.0-2.1.8.0 and from source but always end up with:Failed to install mlnx-ofed-kernel-dkms DEBIn the log I find:Error! Bad return status for module build on kernel: 5.8.0-41-generic (x86_64)Consult /var/lib/dkms/mlnx-ofed-kernel/5.0/build/make.log for more information.dpkg: error processing package mlnx-ofed-kernel-dkms (–install):installed mlnx-ofed-kernel-dkms package post-installation script subprocess returned error exit status 10Errors were encountered while processing:mlnx-ofed-kernel-dkmsIn the referenced log I find:Error: CONFIG_MLX5_ESWITCH not support kernel version 5.6 or higher (current: 5.8.0-41-generic).When trying to run add_kernel_support it fails in the same way, the log shows;/tmp/mlnx_iso.131692/mlnx-ofed-kernel/mlnx-ofed-kernel-5.0/configure --kernel-version=5.8.0-41-generic --kernel-sources=/lib/modules/5.8.0-41-generic/build/ --with-core-mod --with-user_mad-mod --with-user_access-mod --with-addr_trans>Error: CONFIG_MLX5_ESWITCH not support kernel version 5.6 or higher (current: 5.8.0-41-generic).and further:checking for cross compilation… nochecking for external module build target… configure: error: kernel module make failed; check config.log for detailsFailed executing ./configuremake[1]: *** [debian/rules:62: override_dh_auto_configure] Error 1make[1]: Leaving directory ‘/tmp/mlnx_iso.131692/mlnx-ofed-kernel/mlnx-ofed-kernel-5.0’make: *** [debian/rules:50: build] Error 2dpkg-buildpackage: error: debian/rules build subprocess returned exit status 2I’m not sure what I can try to make the install work, any tips? Note that my Linux skills are green at best.Hello Ruben,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately, both driver versions do not support your kernel version. We recommend to to wait for the upcoming MLNX_OFED 4.9 LTS version which will be released later in the quarter which will contain support for higher kernel versions (expected kernel version 5.10)Thank you and regards,~NVIDIA Networking Technical SupportHallo Martijn,Goed om te horen/Great to hear that the Connectx-3 receives continued support.Best regards,RubenHello. When would the release of the new driver be ?Hello, some updates would be very welcome. Is it possible to have a more precise ETA?New version 4.9-3.1.5.0 appeared but unfortunately installation fails with the same error “not support kernel version 5.6 or higher”. When is the new kernel support coming?Can confirm. Clean install of ubuntu20.04. Same error as with previous versions.Good Day,Please can I get an update on an the updated LTS OFED package that does support a higher Kernel.Kind regards,TSAfter almost a year, I am still getting the same error on Ubuntu 20.04: “Error: CONFIG_MLX5_ESWITCH not support kernel version 5.6 or higher”. The LTS driver is supposed to support the Ubuntu 20.04. What should I do to make it work?Powered by Discourse, best viewed with JavaScript enabled"
316,my-bluefield-smartnic-does-not-work-in-separated-mode-to-the-best-of-my-knowledge-it-should-work-as-a-simple-nic-in-this-mode-but-no-packet-is-transmitted-received,"I am pretty new in using BlueField. So, my question may be much trivial.I have two hosts, and one BlueField is mounted on each of them, and they are connected to a switch for communication. The smartNICs are working in separated mode (default mode).The communication between host and the local smartNIC is fine, however I cannot communicate with the other host. In fact no packet is transmitted based on what I see in the switch. When I checked what is happening I got that ARP requests are not resolved.What should the problem. Another thing is that I have connected only one port of each smartNICs. Is that fine?Thank you in advance.Hi Hesam,I recommend to go over the BlueField documentation “Modes of Operation” section.https://docs.mellanox.com/display/BlueFieldSWv22011000/Modes+of+Operation#ModesofOperation-SeparatedHostIn addition according to our records your account have a valid support contract , for further investigation please send an email to networking-support@nvidia.com and we will be happy to assist.ThanksSamerPowered by Discourse, best viewed with JavaScript enabled"
317,question-about-virtual-rdma-support-in-containers,"Hi folks,I’m trying to understand if the virtual RDMA  can be supported in containers which share the single HCA; for that design, I assume in each container it cans till take the benefits bring by RDMA kernel/CPU-bybass  feature.  I did see SRIOV solution but the artical can be found only gives rough intro without details.The goal is actually to let containers running in their own vlan can talk to each other (could be located in different physical nodes) with RDMA, so, it can bring flexible deployment (containers) and with better network performance (RDMA) .Really appreciate for any suggestions.HuidePossibly this question need to be post in Mellanox OFED sub-category also?  please suggest.Hello Huide,Welcome, and thank you for posting your inquiry to the NVIDIA developer forums!Many end-to-end deployment examples can be found on our Solutions landing page:
https://docs.nvidia.com/networking/display/public/SOL/Solutions+HomeRelevant to your use case, we have 2 examples documented there that walk through configuration of both Docker and LXD containers with RDMA over Infiniband fabrics:https://docs.nvidia.com/networking/pages/releaseview.action?pageId=15049785https://docs.nvidia.com/networking/pages/releaseview.action?pageId=15049745Thanks,
NVIDIA Enterprise SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
318,connectx6-dpdk-dpdk-testpmd-receive-tcp-udp-mixed-flow-performance-is-very-low,"I use Ixia to construct two streamsTotal 20Gbps 29760000 ppsflow1 udp 64size small packet
image.png865×363 76.5 KB
Send per second 10G bps 14880000 ppsflow2 tcp 64size small packet
image.png865×542 87.3 KB
Send per second 10G bps 14880000 pps./dpdk-testpmd -l 4-22 -n 8 – -i --rxq 19 --txq 19 --nb-cores 18 --rxd 2048 --txd 2048 --portmask 0xffset fwd rxonlystartshow port stats alltestpmd> show port stats all######################## NIC statistics for port 0 ########################RX-packets: 103906391 RX-missed: 369790696 RX-bytes: 6234383466RX-errors: 0RX-nombuf: 0TX-packets: 0 TX-errors: 0 TX-bytes: 0Throughput (since last show)Rx-pps: 4205026 Rx-bps: 2018412608Tx-pps: 0 Tx-bps: 0############################################################################Recive per second2018412608 bps 2g bps4205026 pps 4 million ppsrx_discards_phy drop per second 10 million pps[root@localhost ~]# ethtool -S enp202s0f0 |grep disrx_discards_phy: 35892329864tx_discards_phy: 0rx_prio0_discards: 35892164419rx_prio1_discards: 0rx_prio2_discards: 0rx_prio3_discards: 0rx_prio4_discards: 0rx_prio5_discards: 0rx_prio6_discards: 0rx_prio7_discards: 0If both flow become TCP, rx_discards_phy will not be drop.flow1 tcp 64size small packetflow2 tcp 64size small packettestpmd> show port stats all######################## NIC statistics for port 0 ########################RX-packets: 7177423122 RX-missed: 369790696 RX-bytes: 430645390083RX-errors: 0RX-nombuf: 0TX-packets: 0 TX-errors: 0 TX-bytes: 0Throughput (since last show)Rx-pps: 29779180 Rx-bps: 14294006816Tx-pps: 0 Tx-bps: 0############################################################################29779180 pps 29 million ppsrx_discards_phy no drop[root@localhost ~]# ethtool -S enp202s0f0 |grep disrx_discards_phy: 0tx_discards_phy: 0rx_prio0_discards: 0rx_prio1_discards: 0rx_prio2_discards: 0rx_prio3_discards: 0rx_prio4_discards: 0rx_prio5_discards: 0rx_prio6_discards: 0rx_prio7_discards: 0serverdell poweredge r750[root@localhost proc]# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 64On-line CPU(s) list: 0-63Thread(s) per core: 1Core(s) per socket: 32Socket(s): 2NUMA node(s): 2Vendor ID: GenuineIntelCPU family: 6Model: 106Model name: Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHzStepping: 6CPU MHz: 2900.000BogoMIPS: 5800.00Virtualization: VT-xL1d cache: 48KL1i cache: 32KL2 cache: 1280KL3 cache: 55296KNUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 invpcid_single intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq md_clear pconfig spec_ctrl intel_stibp flush_l1d arch_capabilitiesMLX ConnectX6 100G PCIE4 x16[root@localhost ~]# ofed_info -sMLNX_OFED_LINUX-5.5-1.0.3.2:[root@localhost ~]# mlxfwmanagerQuerying Mellanox devices firmware …Device #1:Device Type: ConnectX6Part Number: MCX653106A-ECA_AxDescription: ConnectX-6 VPI adapter card; H100Gb/s (HDR100; EDR IB and 100GbE); dual-port QSFP56; PCIe3.0 x16; tall bracket; ROHS R6PSID: MT_0000000224PCI Device Name: 0000:ca:00.0Base MAC: 08c0eb204e5aVersions: Current AvailableFW 20.32.1010 20.32.1010PXE 3.6.0502 3.6.0502UEFI 14.25.0017 14.25.0017Status: Up to dateI changed a network card MCX614106A-CCA_Ax still has this problem!Device #1:Device Type: ConnectX6Part Number: MCX614106A-CCA_AxDescription: ConnectX-6 EN adapter card; 100GbE; dual-port QSFP56; Socket Direct 2x PCIe3.0 x16; tall bracket; ROHS R6PSID: MT_0000000220PCI Device Name: 0000:ca:00.0Base GUID: 0c42a103005f8570Base MAC: 0c42a15f8570Versions: Current AvailableFW 20.30.1004 N/APXE 3.6.0301 N/AUEFI 14.23.0017 N/AStatus: No matching image foundHi Li,I don’t think this is related as the issue exhibit when you are mixing flows.If CQE Compression is enabled then you need to use the value rxq_cqe_comp_en=4 for mixed traffic36. MLX5 Ethernet Poll Mode Driver — Data Plane Development Kit 22.07.0 documentation. ((See 36.5.3.2)).This is recommended for mixed UDP+TCP Traffic.Should this not resolve your issue, a support case with a valid support contract will need to be opened to further troubleshoot/debug.Sophie.Powered by Discourse, best viewed with JavaScript enabled"
319,connect-sx6036-with-another-sx6036-standalone-to-expand,"hello,We have two production switches  :Mellanox sx6036 IB switch running as a standalone running Version 3.6.3004 and partition the whole with 192.168.64.0/24 network.Mellanox sx6036 IB switch running as a standalone running Version 3.6.6106 and partition the whole with 192.168.128.0/24 networkWe’d like to connect these two switches together to expand the number of port since one has a limited left of free ports. We want to know how to go about making that setup work (process, documentation) with one running as a Master with SM and the other as HA (slave).
Do they have to run on the same Version (the highest of the two)?
Can that be done without having to reload the switches?Thanks
DanielHello Daniel.Prieto,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.The following MLNX-OS UM link will provide you the information on how-to configure your IB switches into HA → https://docs.nvidia.com/networking/display/MLNXOSv3102102/Subnet+Manager+High+AvailabilityFor this the MLNX-OS version on both switches needs to be aligned. We recommend to do this in a maintenance window/downtime as the switch OS upgrades requires a reload of the switch.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
320,how-to-fix-the-hca-self-test-fail-error-counter-check-on-ca-0-hca,"When I execute hca_self_test.ofed for testing configure of Infiniband, but I got the Error Counter Check on CA #0 (HCA) as following. I tried to reboot the machine, but this error was not removed.$ sudo /usr/bin/hca_self_test.ofedHCA I used is 88:00.0 Infiniband controller: Mellanox Technologies MT27700 Family [ConnectX-4]. The the detail information of my HCA is as follows.$ ibstatI am at a bit of a loss and any help would be appreciated.The errors themselves are showing the switch is sending traffic that is addressed to the wrong destination LID (subnet local ID)Have you tried clearing the counters on the device
(perfquery -R    in your case it will be perfquery -R 4 1)And rerunning the self test?Thank you for your replay. I try to perfquery -R 4 1, then pass the test of hca_self_test.ofed!Thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
321,obtaining-and-building-linux-kernel-source-for-doca-1-0,"Hi TeamI am very excited by the DOCA release. My project will require making Linux kernel modifications and then installing this modified kernel on the BlueField 2. To that end I have a couple of questions.Where can I obtain the source code for the DOCA 1.0 kernel. Ideally this is a git repository but a tarball works too.Does this source include a .config for the DOCA 1.0 kernel? If not can such a .config be provided?Are there instructions to build and install this kernel? My team is very comfortable building kernels (we do 10-100 per day) but I am not sure if there are special considerations for the BlueField-2. Also instructions on how to integrate this kernel image and modules into a BF2 disk image would be useful.Thanks!Stevie BatesThe official DOCA 1.0 BFB uses the Ubuntu distro, which is built and released by Canonical, and as such we are still working out the details on how to make the kernel source available.Be that as it may, DOCA also comes in other distros, such as CentOS (for the nonce), Debian, and Fedora.  These distros are built by Nvidia and we can supply the kernel source.   If you use any of these community distros, then the kernel source and the .config used to build it can be had by asking.  Please contact your friendly neighborhood SE/FAE to make a request for it.Note that the kernel source used in Ubuntu is slightly different from the kernel used in CentOS/Debian/Fedora.  While they all have the necessary BF2 patches, the Ubuntu kernel has additional patches provided by Canonical.Thanks @jtauI was not aware that all the driver code for BF-2 was upstreamed into the distro kernels. That is very useful. Does this include driver code for all the accelerator engines (e.g. compression, rege, nvme-of offload)?I will work with our SE/FAE to get access to the kernel source and .config.One final question for now. In DOCA is NVMe-oF target mode (i.e. on the storage server side) implemented? If so, is that implemented in kernel-space (via the NVMe-oF kernel driver) or via SPDK?StevieNo, the BF-2 specific patches are not yet upstreamed.  We did not mean that they are.  What we meant was that we take the CentOS/Debian/Fedora distros and install BF-2-patched kernel and roll it all up in the installation image.  For the CentOS/Debian/Fedora distros Nvidia builds the kernel and the image.  For the Ubuntu distro, Canonical does it.Nvidia can provide the kernel source used in the CentOS/Debian/Fedora installation images for the BF-2.As for NVMe-oF target mode, do you mean using the BF-2 as a controller or the (smart)NIC and the host as the controller?  For the former, it is nominally part of DOCA, but there are no new APIs for it.   The procedure to configure the BF2 as an NVMe-oF target controller under DOCA is the same as without DOCA.  To use the BF-2 as an NVMe-oF target requires that the BF-2 controller card be used.  This card cannot be plugged in an x86 server and be an NVMe-oF target controller.  The BF2 needs to be the root complex as it would be in a JBOF.But if the host is the NVMe-oF target controller and the BF-2 is just a (smart)NIC, then whether the BF2 is DOCA-enabled is irrelevant.@jtauThanks again for the great response!Nvidia can provide the kernel source used in the CentOS/Debian/Fedora installation images for the BF-2.Great! This is what we need access too. Who do we need to talk too to get approval for access to this code.and roll it all up in the installation imageDo you have a tool for the creation of these installation images? I see you have a docker hub container for this and if that is still up to date we would love access to the Dockerfile itself so we can do this generation in-house.To use the BF-2 as an NVMe-oF target requires that the BF-2 controller card be used.Yes this is what we are doing. We are using BF-2 as the NVMe-oF target at the front end of a Ethernet attached JBOF from an ODM we are working with. So yes, the firmware on the card will put the BF-2 into PCIe Root-Complex mode and we will be hanging NVMe SSDs and some other PCIe devices off the BF-2. There is no other CPU in the JBOF so the BF-2 will be used to terminate the NVMe-oF commands and then issue PCIe-based NVMe commands to the SSDs inside the JBOF and then NVMe-oF RDMA that data back to the initiators.StevieHi,I wonder if I can get the kernel source used in BF-2.
If so, who do I need to talk to get the source?And as Steive asks, how can we enable the storage-related accelerators?
I couldn’t find this information in the following link. Do you provide the specific libraries using SPDK?
https://docs.mellanox.com/display/BlueFieldSWv36011699/NVIDIA+BLUEFIELD+DPU+FAMILY+SOFTWARE+3.6.0.11699+DOCUMENTATIONThanks,How did you set the firmware to make BF-2 as a Root-Complex?
I’m trying to do this but still when I access the Arm core, I cannot find any PCIe devices plugged in other slots.
If you can provide more informations about this, it would be helpful!Thanks,The Ubuntu kernel source can be found here: git clone ~canonical-kernel/ubuntu/+source/linux-bluefield/+git/focal - [no description]By enabling storage-related offloads, given the context, you mean using the BF2 as a storage controller?  In that case, there’s no hardware acceleration with SPDK.  To set up hardware acceleration for storage target, the information found here (https://community.mellanox.com/s/article/howto-configure-nvme-over-fabrics--nvme-of--target-offload) still applies.*>> How did you set the firmware to make BF-2 as a Root-Complex? *This is device specific and set in the FW in the factory.  It’s not something a user can change at will.   So if your BF2 device is a controller card, then it should already be configured as a root complex.  No additional configuration is needed to configure it as a root-complex.  If your BF2 device is not a controller card, you have a DPU, in which case it can never be a root-complex.For this issue, please open a support case so that we can properly resolve it.Thanks for your response.I would check the link as you provided for the kernel source.For storage-related offloads, I mean the deduplication and compression not the storage target. Where can I find those acceleration related features? Those are integrated into the kernel as a user library?Regards,
WonsikCould you explain more about the hardware acceleration?
I am curious about the the compression/decompression and deduplication features in BlueField-2.
Do you provide some APIs for those features? If so, where can I find it?Are there any updates for this question?
I wonder whether I can enable the storage accelerator (deduplication, compression/decompression) in a form of API.Thanks in advance.Powered by Discourse, best viewed with JavaScript enabled"
322,does-bluefiled-2-support-ibv-atomic-glob,"I have written a small code to query the device’s capabilities and check rdma atomic capability. I have run the code in both the host and DPU. And from the result, it looks like both the host and DPU only support IBV_ATOMIC_HCA. Is this actually the case for BF2 DPU? or am I missing something? Is there any way to turn on GLOBAL atomic mode?I am running the DPU in Embedded CPU mode.ibv_devinfo for the device I am checking capability on the DPU side. 2 ports are 2 SFs:Update:I came across this pull request. From this commit, it looks like IBV_ATOMIC_HCA is OK to provide global atomicity as long as the device supports PCIe atomics. But when I am querying pcie atomic capabilities with ibv_query_attr_ex() I am getting attr_ex.pcie_atomic_caps.fetch_add = 0 and attr_ex.pcie_atomic_caps.compare_swap = 0.I have checked device configuration parameter PCI_ATOMIC_MODE. It was set to PCI_ATOMICS_DISABLED_EXT_ATOMICS_ENABLED(0). I have tried changing it to PCI_ATOMICS_ENABLED_EXT_ATOMICS_ENABLED(4) but it is giving same results. Does that mean BF2 doesn’t support pcie atomics?CPU I am using: Intel(R) Xeon(R) Silver 4310 CPU @ 2.10GHzhi AshBF2 should support pci atomic.How to check if PCIe atomic is capable or not.You need to check 3 components:1.mlxconfig -d ca:00.0 q | grep PCI_ATOMIC_MODEIf this is PCI_ATOMICS_ENABLED_EXT_ATOMICS_ENABLED_SERIALIZED(1), PCI atomic is capable for the device2.You need AtomicOp Requester enabled to be set on the NIC PCI devicefor example, this enabled: lspci -vvv -s ca:00.0AtomicOpsCtl: ReqEn+ : AtomicOp Requester Enabled3.If NIC connected via PCI bridge, on the bridge AtomicOp completer should be enabledYou need to check PCI tree, if connected via PCI bridge, and if yes, check PCI address of the bridge and in “lspci -vvvxxx -s ” - check “AtomicOp completer” configuration.If all 3 are enabled, PCI atomic capability enabled. If at least 1 is disabled, PCI atomic capability disabled.If no PCI bridge and NIC is connected directly to PCI slot on the server, 1 and 2 is enough.Thank you
Meng, ShiHi Meng,Thank you for the reply.Initially, the device configuration parameter PCI_ATOMIC_MODE was set to PCI_ATOMIC_DISABLED_EXT_ATOMIC_ENABLED(0). Then I changed it to PCI_ATOMICS_ENABLED_EXT_ATOMICS_ENABLED_SERIALIZED(1). Rebooted the machine and the DPU. Still I am facing same issue.Here is the PCI tree focused to DPU:So, it looks like BF2 is connected through pcie bridge 0000:16:02.0. Here is lspci -vvv -s 0000:16:02.0 [Printing out full output just to make sure I am not missing anything]I couldn’t find anything like “AtomicOp completer”. But following capabilities are there in the context of AtomicOp:Does that mean the PCIe bridge doesn’t support AtomicOp completer?Here is lspic output for the BF2 device after setting PCI_ATOMIC_MODE to PCI_ATOMICS_ENABLED_EXT_ATOMICS_ENABLED_SERIALIZED(1)lspci -vvv -s 0000:17:0.0:In the context of PCIe AtomicOp following capabilities can be noted:Let me know what you think. Thank you.hi ashyou bridge is support AtomicOp completer, but you didn’t enable it:
detail see:Search · AtomicOp · GitHubThank you
Meng, ShiHello Meng,I have looked at the code you have shared. I am Not sure what you mean by I have to enable it. Here is the lspci output of the bridge from my previous post:From this output it looks like on the bridge (1) AtomicOp Routing is enabled (2) 32, 64 and 128 bit AtomicOp completer is enabled.Do I need to enable any additional configuration on the bridge?And here the output from the BF2 device:AtomicOp Requester is enabled and AtomicOp completers are disabled. Do I need to enable atomic op completer on the BF2 device cap too?Thanks,
Ashfaqhi AshAs seems we need some time to sync the information.
I suggest you contact networking-support@nvidia.com for further debug.Thank you
Meng, ShiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
323,vlan-tagging-capture-using-wireshark,"Question ConnectX-4It depends on the cable you are using:
1.a. If copper SFP - 25g-CR CA-L/S/N it will use RS/FC/no FEC accordingly.
1.b. For optics we let the link partner choose (using parallel detect). If both ends are Mellanox products it will be RS FEC for 25G-SR optics.yes.Are you using windows or Linux? if Linux - can you try using tcpdump for capturing?Current we use Windows.Powered by Discourse, best viewed with JavaScript enabled"
324,connect-between-cx-6-and-infiniscale-switch,"Hello!Documentation for CX6 describe that this card support QDR speed.How connect CX6 card with IS5200 switch in QDR speed?https://docs.mellanox.com/display/ConnectX6Firmwarev20311014/Firmware+Compatible+Products#FirmwareCompatibleProducts-ValidatedandSupportedFDRCables - this document don’t give answerHello Ilya,Thank you for posting your inquiry on the NVIDIA Networking Community.The link you are referring to contains the correct information.Unfortunately, even though the protocol is supported, connectivity towards QDR switches is not supported.You can lower the speed to QDR on certain FDR and EDR switches. But as the matrix shows, no QDR switches are supported.To obtain QDR connectivity towards a ConnectX-6 you can add the following switches in between.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
325,significant-drop-in-vf-throughput,"Hello,I am in the process of using and testing BlueField 2 VF capabilities, and have noticed significant reduction in performance compared to PF.I have two nodes connected with 100Gb/s Ethernet, and when tested with iPerf using default PF, I obtain throughput results of around 98 Gb/s.However when I enable VF following VF tutorial
and run iPerf through newly functioning VFs, throughput decreases down to 50-60 Gb/s. Is that something to expect from SR-IOV VFs or not? Is there anything else that needs to be enabled in order to get performance similar to PFs? I tried increasing NUM_VF_MSIX to no success.I tested vf and was able to achieve 100G.Powered by Discourse, best viewed with JavaScript enabled"
326,is-there-a-mailing-list-to-get-software-security-updates,"Looking for a way to be notified of updates. Just inherited 6 switches through a customer and know nothing about them.Hello Joey,Thank you for posting your inquiry on the NVIDIA Networking Community.Yes, you can be notified on software updates. When you have a valid support contract for the switch, you can enable notifications for the software updates you need, through your Support Portal.The latest security notifications are done through the following link → NVIDIA Product Security | NVIDIAThis page provides an option to subscribe for receiving email notifications.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
327,how-to-save-settings-on-connectx-6-dx-en-pcie-mcx623106an-cdat,"I’ve used mlxlink to set a forced speed as shown in post
How to disable Smart AN on CX6 adapter.Now how do I save the setting so that it survives power cycling?Hi,It’s not possible by default , the only workaround is to try adding the disable under “rc.local” so it will disable it after the boot stage .Thanks,
SamerThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
328,release-notes-for-nvidia-bright-cluster-manager-9-1-15,"Release notes for Bright 9.1-15== General ==
=Improvements==Fixed Issues== Deprecated features=== CMDaemon ==
=Improvements==Fixed Issues=== Bright View ==
=Fixed Issues=== Head Node Installer ==
=Fixed Issues=== Machine Learning ==
=New Features==Improvements=== cm-clone-install ==
=New Features=== cm-scale ==
=Fixed Issues=== cmsh ==
=Improvements=== pbspro2022 ==
=Improvements=== slurm21.08 ==
=Fixed Issues=== slurm22.05 ==
=Improvements=Powered by Discourse, best viewed with JavaScript enabled"
329,issue-of-vma-snd-flags-dummy,"I am using Mellanox Technologies MT27800 Family [ConnectX-5] accelerated card for low latency network message transition.However, when using the “Dummy Send” function provided by VMA, the dummy message that I passed to the function: send(fd, buf, len, VMA_SND_FLAGS_DUMMY) would be occasionally ( around 1 time per 1000 billion call ) sent out by this function. But the documentation shows that the dummy message would be dropped before it was sent.This phenomenon happens rarely but every time it happens it brings us significant loss. The frequency that I call send(fd, buf, len, VMA_SND_FLAGS_DUMMY) is 20k/s to reduce the overhead of TCP Send.I used vma_stats –p  -v 3 to check whether the dummy send function works, the report shows that this function works properly but the phenomenon that the dummy message was send always happens once.I cannot reproduce the problem because sometimes it works fine but sometimes it just sends out the dummy message.May I know if anyone else encounter this problem? or there is any solution for this problem? or should I add VMA_TRIGGER_DUMMY_SEND_GETSOCKNAME when I use dummy send?May we know if there is any one who could help me ?ThanksHello,The scenario you described requires a deeper investigation and reproduction (checking the code you are running). In this situation we recommend to open a support case in Nvidia portal, and it will be handled according to the entitlement.Best Regards,
AnatolyHi,How do we open a support case for it ?Thanks & Regards,Michael from Alpha QuantPowered by Discourse, best viewed with JavaScript enabled"
330,a-doubt-about-mkpkc-tool,"./mkpkc -i 0x21 -f PKC -k rsa_priv.pem genkey，请问这个命令生成的hash值是公钥哈希还是私钥哈希，是否有私钥被破解的风险Are you using DPU? looks like this is for NVIDIA Jetson nano secure boot.But, fyi,MKPKC encryption algorithm uses several keys, some of which are public keys, while the remaining keys are private. Generalize RSA from one public key and one provite key to multi public key RSA.thanks, I’m using NVIDIA Jetson nano secure boot.
rsa_priv.pem is my private key. I’m worried that this command will generate my private key hash, so my private key will be at risk of disclosure.It should not. This thread is for BlueFiled DPU.And you can open thread on Jetson forum.Get support, news, and information about Jetson NanoThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
331,problem-with-dma-on-multicore,"Hello,I am writing an application on the bluefield and i have to use DOCA DMA to communicate with the host. The app runs correctly on 1 to 5 cores but when i keep increasing the number of core it sometimes crashed at the beginning. When using this with dpdk, multiple cores crash directly even if the code is the same except i add the usage of the function rte_eth_rx_burst. All mmap, ctx and every variable are local to each core, the buffer to exchange data with host are also separate (ie : the host has 7 malloced buffer and export each one to a different core).here is the crash error i got on DOCA :[08:27:46:005729][DOCA][ERR][DOCA_DMA:1442]: CQ received for failed job: status=2, vendor_error=104
[08:27:46:005834][DOCA][ERR][MAIN:169]: Failed to retrieve DMA job: Input/Output Operation FailedThe job is either a write or a read on the mmaped memory, it may occur in both case.does someone know what’s happening and/or how to correct it ?Thank youPowered by Discourse, best viewed with JavaScript enabled"
332,mellanox-connectx-5-physical-state-polling,"I have two ConnectX-5 direct connect each other but the physical state shows Polling. It seems the mlx related drivers are loaded correctly. I’m new to IB. What did I miss? Thanks.$ ibstatCA ‘mlx5_1’CA type: MT4119Number of ports: 1Firmware version: 16.27.1016Hardware version: 0Node GUID: 0x98039b0300862f75System image GUID: 0x98039b0300862f74Port 1:State: DownPhysical state: PollingRate: 10Base lid: 65535LMC: 0SM lid: 0Capability mask: 0x2651e848Port GUID: 0x98039b0300862f75Link layer: InfiniBand$ lsmod | grep mlxmlx5_ib 410355 0mlx5_core 1073216 1 mlx5_ibmlxfw 19917 1 mlx5_coreib_uverbs 141173 10 mlx5_ib,rdma_ucmib_core 377570 8 ib_cm,rdma_cm,ib_umad,ib_uverbs,ib_ipoib,iw_cm,mlx5_ib,rdma_ucmmlx_compat 45899 10 ib_cm,rdma_cm,ib_umad,ib_core,ib_uverbs,ib_ipoib,mlx5_core,iw_cm,mlx5_ib,rdma_ucmI was able to solve it by first upgrading the firmware and then loading opensm. The link state becomes “Active” and physical state “LinkUp”. My question is the card and cable should support 100Gbps but why “Rate” shows 10?CA ‘mlx5_1’CA type: MT4119Number of ports: 1Firmware version: 16.30.1004Hardware version: 0Node GUID: 0x98039b03008f93bdSystem image GUID: 0x98039b03008f93bcPort 1:State: ActivePhysical state: LinkUpRate: 10Base lid: 1LMC: 0SM lid: 2Capability mask: 0x2651e84aPort GUID: 0x98039b03008f93bdLink layer: InfiniBandSomehow I had to run mlxconfig to explicitly set link type to Ethernet then I’m able to get 100Gbps data rate. And if I set link type back to Infiniband it goes 10Gbps again. Is this what Mellanox card/software by design?Hello Colin,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the updates your were able to start the Subnet Manager, which is needed in every IB fabric.You need to run the SM on one node only if connected back2back. As you mentioned you were able to get a rate of 100GbE, suspicion is that you are using an Ethernet cable. For achieving 100Gb/s IB, you need to use the correct cable. You can find the list of supported IB cables through the RN of the f/w on the adapter → https://docs.mellanox.com/display/ConnectX5Firmwarev16301004/Firmware+Compatible+Products#FirmwareCompatibleProducts-ValidatedandSupportedEDR/100Gb/sCablesThank you and regards,~NVIDIA Networking Technical SupportThank you Martijn. I will get one of the IB cables and try again.Yes, you’re right. it is cable. Once I change to IB cable rate becomes 100. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
333,switch-hpe-3180-fm-with-cumulus-os,"Hi everyone,i have switch HPE Composable 3180FM. i just installed Cumulus OS.
But, i have the problem,  interface 100G not running well.
the problem is need license 100G ?please advices everyone
thanksHi @aka,I am moving this to the Cumulus category so the support team has visibility.That switch seems to be a rebranded SN2410, but not completely sure. Either way, Cumulus Linux doesn’t require specific feature licenses for anything.Could you be more specific on the problem you are experiencing?Powered by Discourse, best viewed with JavaScript enabled"
334,teaming-dual-25gbe-connectx-6-lx-cards-50gbps-speed,"We have two Mellanox dual 25Gbe Connectx-6 lx cards and two Win10 computers.We would like to aggregate them to have 50Gbe connection.We are able to make a team via mlx5muxtool, attach both of ports to team and Mellanox Miniport Virtual Driver was created on both of computers. We added manually IP adresses, both computers can ping each other and communicate via virtual driver. But speed is still 25Gbps and virtual driver also shows this. We suppose, that teaming will double the speed. Are we wrong?Thank you.RobertHi Robert,What driver version of WinOF2 have you installed?Using the mlx5muxtool.exe, what type of team did you create? (IE: Aggregate or Failover)If you query the team, do you see both members? (IE: mlx5muxtool queryteam )Can you elaborate on your statement “But speed is still 25Gbps” and “virtual driver also shows this”?(IE: tests performed/bandwidth measurement & virtual driver shows what).Where do these HCA’s connect to? (IE: switch vendor model).In aggregation mode, yes the teaming should double the speed, you might not get exactly 50Gbe but should pass 25Gbe.Do you have a service contract with us?A support case might be needed to further investigate and to reproduce in house.Sophie.Hi Sophie,Screenshot of the setting is attached.Best regardsRobertHi Robert,Where version 2.7.248 was pulled from? According to our website, our latest WinOF2 version is 2.70.51000 (Mellanox OFED for Windows - WinOF / WinOF-2).Are you aligned with the supported FW (26.31.1014)? (Firmware for ConnectX®-6 Lx | NVIDIA).There is no known issue documented from our driver/FW RN in regard to teaming.This particular issue will require an internal reproduction.A support case will be needed to further investigate and to reproduce in house.Regards,Sophie.Powered by Discourse, best viewed with JavaScript enabled"
335,connectx-6-nat-hardware-offloading-in-ubuntu-os,"We are planning to set up server using Ubuntu 22.04 LTS OS as our NAT device processing about 20Gbps traffic and 5M packets/second. NAT will be setup using standard Ubuntu iptables.As a NIC we are planning to use NVIDIA MCX623106AC-CDAT ConnectX-6 D because in datasheet it is writen that it supports NAT hardware offloading.We have several questions:When using NAT with iptables on Ubuntu 22.04 LTS do we need to somehow enable hardware NAT offloading in NIC or it will work automatically?Is there any limits (such as packets/second) for NAT hardware offloading?Powered by Discourse, best viewed with JavaScript enabled"
336,drivers-fail-pkcs-7-signature-not-signed-with-a-trusted-key,"We have to use UEFI, and Secure boot. When I install the latest MLNX drivers for our ConnectX-6 card, I get the following in the logFeb 03 17:04:51 management-node kernel: PKCS#7 signature not signed with a trusted keyFeb 03 17:04:51 management-node kernel: Lockdown: Loading of unsigned modules is restricted; see man kernel_lockdown.7and obviously the drivers will not work. Are the drivers not signed properly? Or is there something I can do to add a credential to the kernel’s trusted keys?I am not sure how to move forward. We have a significant number of HCA’s all in the same predicament.Is Ubuntu supposed to have a system_keyring? Because mine certainly does not; I only have something called “.secondary_trusted_keys”; see a couple of posts up.Apologies, here is my OS and driver version:Ubuntu 18.04.5 LTSMLNX_OFED_LINUX-5.2-1.0.4.0-ubuntu18.04-x86_64Apologies again, I believe this answers my own question. Sorry for the noise.UEFI Secure Boot - MLNX_OFED v5.2-1.0.4.0 - Mellanox DocsI spoke too soon. I followed that procedure, but it does not seem to make a difference. I downloaded the key from Mellanox, applied it via mokutil, rebooted, got the screen to enroll the key, enrolled the key.I noticed that my system does not havekeyctl list %:.system_keyringBut I do have the following keyrings, and Mellanox is in them:root@management-node:~# keyctl list %:.builtin_trusted_keys1 key in keyring:158064321: —lswrv 0 0 asymmetric: Build time autogenerated kernel key: xxxroot@management-node:~# keyctl list %:.secondary_trusted_keys8 keys in keyring:634468722: —lswrv 0 0 keyring: .builtin_trusted_keys163067684: —lswrv 0 0 asymmetric: Canonical Ltd. Master Certificate Authority: xxx366975476: —lswrv 0 0 asymmetric: SomeOrg: shim: xxx658563297: —lswrv 0 0 asymmetric: Mellanox Technologies signing key: xxx53066224: —lswrv 0 0 asymmetric: VMware, Inc.: VMware Secure Boot Signing: xxx1073723880: —lswrv 0 0 asymmetric: VMware, Inc.: xxx299092182: —lswrv 0 0 asymmetric: Microsoft Windows Production PCA 2011: xxx388698450: —lswrv 0 0 asymmetric: Microsoft Corporation UEFI CA 2011: xxxCan you advise?Hello Daniel,Thank you for posting your inquiry on the NVEX Networking Community.Please do in the following sequence:Based on above procedure we were not able to reproduce the issue in our lab and the driver installation was successful without the 'PKCS#7 signature not signed with a trusted key’ message.If the issue still exists after this installation, please open a NVEX Networking Technical support ticket by sending an email to networking-support@nvidia.comThank you and regards,~NVEX Networking Technical SupportUnfortunately, this did not work; same result. I have opened a case. I’m not sure if I’m supposed to do that through other means, because I have a support contract, or not.Hello Daniel,I see the case in the system and you have been updated what some of the same info you already tried.I will move the case to our L3 as we need to do a repro in the lab. The procedure needs to work for all supported OSses.We will support you through the support case.Thank you and regards,~NVEX Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
337,do-nvidia-have-any-connectx-6-cards-with-sma-for-pps-on-the-market,"I’m looking at NIC options and would prefer NICs with SMA adapters for time sync. Nvidia have (I think) 3 part numbers but only one appears on the store and it’s not available to ship and has a 30 week pre-order on it?Hello Barry,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately the ConnectX-6 Dx is the only adapter which has this ability. For the p/n, you can review the following link → https://docs.mellanox.com/display/ConnectX6DxEN/Introduction#Introduction-ProductOverviewIn the past there was also a ConnectX-5 adapter with this ability but that one became EOL last year.For the shipping, unfortunately, the current lead times are long, but please continue to re-visit the store for modified shipping dates. If needed, you can leave a message at the store, for them to contact you, when available.Thank you and regards,~NVIDIA Networking Technical SupportThanks for the confirmation. The extended lead time is clearly disappointing. Thanks for the confirmation.Powered by Discourse, best viewed with JavaScript enabled"
338,problem-loading-onie-after-resetting-sn2100,"Hi,
We are having a problem with a SN2100.  The switch was reset for a Sonic OS install and ONIE isn’t functioning properly.  The “ONIE: Install OS” option will not appear.

Screenshot_3903×881 15 KB
Any help is much appreciated.
JoshIf you still need assistance please open a support case, instructions are here -  https://support.mellanox.com/s/contact-support-page.Regards,JonPowered by Discourse, best viewed with JavaScript enabled"
339,failed-to-get-crypto-operational-regis,"Hi,I tried to use DPDK crypto driver, but I failed with this. I have seen an existing thread on this(Mellanox crypto drivers not working with DPDK), but I still have questions.My problem is that I can’t get CRYPTO_OPERATIONAL:I will be grateful for your answer, thank you.Hello,Try to change CRYPTO_POLICY via mlxconfig to “2” instead of “1”. You should be able to run the mlxreg after that.
E.G:
mlxconfig -d  s CRYPTO_POLICY=2
reboot or ""mlxfwreset -d  -l 3 -y resetBest Regards,
VikiThank you for your reply. Unfortunately, it didn’t help me. I still see this error.Maybe you have other ideas?Powered by Discourse, best viewed with JavaScript enabled"
340,did-anyone-observed-a-sudden-abrupt-jump-in-performance-while-working-with-dpdk-rxonly-mode-from-11-queues-to-12-queues-in-mellanox-connectx-5-100-gbe-nic,"Recently I observed a peculiar behaviour with Mellanox ConnectX-5 100 Gbps NIC. While working on 100 Gbps capture using DPDK Rxonly mode. It was observed that I was able to receive 142 Mpps using 12 queues. However with 11 queues, it was only 96 Mpps, with 10 queues 94 Mpps, 9 queues 92 Mpps. Can anyone explain why there is a sudden/abrupt jump in capture performance from 11 queues to 12 queues?Could you run two tests - one with 11 and one with 12, run ‘perf top’ for every test, capture the screen, and post it to the ticket? What are the names of the functions that consumes most of the CPUs?Powered by Discourse, best viewed with JavaScript enabled"
341,enable-web-gui-on-my-mellanox-sn2010-switches,"We have recently had a pair of Mellanox SN2010 switches installed and connected via MLAG, running Cumulus Linux 5.5.0.Can someone confirm how we enable the Web GUI on these switches?You can’t, CL doesn’t have a GUI.What are the options for having a GUI to manage the SN2010 switches?Perhaps if we change from CL to Onyx or add a 3rd party tool, such as NEO?What are the pros/cons of each?Both Onyx and Neo are EOL or soon will be. It’s not really typical to manage DC switches through a GUI. CL has an API that allows you to integrate with any other tools, but this is what we can provide you today.I was hoping for some kind of GUI to show performance trends of the switch and ports over a period of time and highlight config issues, firmware updates, amend config etc.Can you recomend a product to provide such insight via the switch API?You could have a look at NetQ and see if it meets your requirements, but that is typically meant for a larger Fabric: Network Operations and Cumulus NetQ | NVIDIACan you confirm costs for NetQ SaaS and Onprem?Yes, netq is a separately licensed product.Where do I find costs for NetQ?You should be able to get a quote from the reseller/distributor you purchased the switches from.Powered by Discourse, best viewed with JavaScript enabled"
342,mellanox-connectx-6-hdr100-qsfp56-ethernet-connection,"It says the ConnectX-6 HDR 100 is IB and Ethernet. How can I configure this adapter to use Ethernet?
I am currently running a Mellanox MQM8700 (version 3.9.3124) switch - do I need an IB to Ethernet gateway or do I just need a 100GB Ethernet switch?QM8700 is an IB switch.
What is the PSID of the card? If it is a VPI card it can be configured as ETH and connected to an ETH switch.If the HCA is VPI model then can use mlxconfig change firmware port type.eg,https://docs.nvidia.com/networking/display/MFTv4221LTS/Using+mlxconfigThanks for the response. I am very new to the Mellanox/InfiniBand world.
I should also clarify (respectfully), I know that the QM8700 is a switch, I guess I had 2 questions.I should also mention, that my HPC environment is working using InfiniBand and I don’t NEED to change to Ethernet, but I suspect that IB is only working when using MPI and not necessarilly with file sharing etc., and I think the environment would be so much faster if I could get that working properly (thus the questions about Ethernet).
I have 1 headnode and 15 compute nodes (Windows Server 2016), all running an Ethernet NIC and a Mellanox adapter with a 10Gb Ethernet switch and the Mellanox MQM8700 switch. The config is basically right out of the box using port splitting @ 100Gbs (for MPI functions etc.).  I have 2 issues:It says the ConnectX-6 HDR 100 is IB and Ethernet. How can I configure this adapter to use Ethernet?
I am currently running a Mellanox MQM8700 (version 3.9.3124) switch - do I need an IB to Ethernet gateway or do I just need a 100GB Ethernet switch?The Mellanox ConnectX-6 HDR 100 adapter supports both InfiniBand and Ethernet connectivity. To configure the adapter to use Ethernet, you can use the Mellanox drivers and software that are compatible with the adapter and the operating system you are running.Once you have installed the appropriate drivers and software, you can configure the adapter to use Ethernet by configuring the appropriate network settings (such as IP address, subnet mask, gateway, etc.) for the Ethernet interface on the adapter. You can also configure any additional Ethernet-specific settings, such as VLAN tagging, jumbo frames, etc.Regarding your question about the Mellanox MQM8700 switch, it is an InfiniBand switch and does not support Ethernet connectivity. If you want to use Ethernet with your ConnectX-6 HDR 100 adapter, you will need to connect it to a 100GbE Ethernet switch that supports the appropriate Ethernet standards (e.g., IEEE 802.3ba). Depending on your specific requirements, you may also need to consider other factors such as port density, throughput, and latency when selecting an Ethernet switch.Thank you (anawilliam850) - that information was very helpful.Thank you as well (dwaxman) - again, information was very helpful.
I think I understand now, and will take a look at setting up RDMA (RoCE) for storage with IB.Powered by Discourse, best viewed with JavaScript enabled"
343,how-to-configure-hierarchical-qos-offloading-in-connectx-6-nic,"As it is stated in the DPDK website, offloading TX scheduling to hardware is supported.https://doc.dpdk.org/guides/nics/mlx5.html#mlx5-offloads-supportI have a simple DPDK application for TX traffic scheduling (shaping) which for now works in software. I would like to use the hardware offloading to perform this function. Any help is appreciated.Could you shed more light on exactly what need to be offloaded/configured? Is this something that covered bo QoS testing - 120. QoS API — DPDK Test Plans documentation?Powered by Discourse, best viewed with JavaScript enabled"
344,fix-build-drivers-for-4-9-6-0-6-4-18-0-425-10-1-el8-7-x86-64-in-rhel-8-7-failing-because-of-brp-mangle-shebangs,"OS: RHEL/Rocky 8.7
Kernel: 4.18.0-425.10.1.el8_7.x86_64
RPM version: 4.14.3When trying to build kernel support for a more recent kernel than those supported by default, the installation/compilation script can fail because the default value for MLNX_PYTHON_EXECUTABLEis set to python when policy in RHEL-based distributions uses python2 or python3. This can easily be solved without touching sources by settingHowever, the phase executing rpmbuild with mlx-ofa-kernel fails withAs per https://fedoraproject.org/wiki/Changes/Make_ambiguous_python_shebangs_error , /usr/lib/rpm/redhat/brp-mangle-shebangs has changed behavior and now returns error on ambiguous shebangs that would’ve given a warning before. Since the error code returned is nonzero, compilation terminates early.I’m assuming ofed-scripts are used for all platforms and the ambiguity of the python shebang doesn’t happen on more recent distributions with deprecated support for python2 and other package managers.The obvious solution is to replace unanbiguous shebangs in the OFED sources, repackage it and run the installation script again. That is:From what I can gather from the logs, the installation script does some shebang manipulation similar to this but it seems that some files might not be corrected. Regarding the correct shebang to replace with, I checked the scripts and my best guess was that python2 was likely to work with all of them, so that’s what I used. This worked and drivers compiled correctly. Using normal installation without adding kernel support yielded a soft lockup in the kernel that stuck the boot process when starting openibd, I assume this can be expected for an unsupported kernel.I notify this because it seems likely to break for other OFED versions/other OSs. I’m not sure if this is the proper place to suggest a fix or if there is a specific way to interact with developers (such as a GitHub repository), if I’m in the wrong place I’d appreciate a nudge in the right direction.Regards,Joaquin Torres.
Comisión Nacional de Energía Atómica - Centro Atómico Constituyentes
HPC SysadminPS:
It would be nice if the documentation had more info for building the sources. In my experience, latest supported kernels in OFED releases are almost always behind latest kernels provided by the distribution and I’ve had much better results by recompiling than by using KMPs of older kernels… Except that compilation is a lot more likely to fail in unexpected ways.I understand that this might mean more development/packaging time but since an RPM build structure is already made what really would save an awful lot of time is to have a non-local repo with updated releases synced to RedHat package versions like the people at ELRepo | HomePageI realize that maintaining a cross-distribution set of packages is incredibly difficult and distributing stable RPMs can be a good compromise in that case (and probably there are a lot more issues that I’m not seeing). But, in the current state of affairs, my experience up to now has been:Good documentation on compilation would help to at least make this process a lot smoother. Currently the only ways I’ve found to diagnose a build issue are to follow the install.pl script or the multiple logs generated by the scripts. And, since the script chooses the defaults for the corresponding configure/make steps this can be tediously slow for each new try since -j or --with-ncpus default values provided can be extremely slow for multicore processors, and reproducibility of errors difficult because of the increasingly complex number of options.Hi torres2,Thank you for contacting Nvidia Support. I was able to successfully install the MLNX_OFED 4.9-6.0.6.0 drivers on my RHEL 8.7 machine. Please find the installation below:[bhargavi@localhost MLNX_OFED_LINUX-4.9-6.0.6.0-rhel8.7-x86_64]$ sudo ./mlnxofedinstall --add-kernel-support --upstream-libs --without-fw-update
Note: This program will create MLNX_OFED_LINUX TGZ for rhel8.7 under /tmp/MLNX_OFED_LINUX-4.9-6.0.6.0-4.18.0-425.10.1.el8_7.x86_64 directory.
See log file /tmp/MLNX_OFED_LINUX-4.9-6.0.6.0-4.18.0-425.10.1.el8_7.x86_64/mlnx_iso.567569_logs/mlnx_ofed_iso.567569.logChecking if all needed packages are installed…
Building MLNX_OFED_LINUX RPMS . Please wait…
Creating metadata-rpms for 4.18.0-425.10.1.el8_7.x86_64 …
Created /tmp/MLNX_OFED_LINUX-4.9-6.0.6.0-4.18.0-425.10.1.el8_7.x86_64/MLNX_OFED_LINUX-4.9-6.0.6.0-rhel8.7-ext.tgz
Installing /tmp/MLNX_OFED_LINUX-4.9-6.0.6.0-4.18.0-425.10.1.el8_7.x86_64/MLNX_OFED_LINUX-4.9-6.0.6.0-rhel8.7-ext
/tmp/MLNX_OFED_LINUX-4.9-6.0.6.0-4.18.0-425.10.1.el8_7.x86_64/MLNX_OFED_LINUX-4.9-6.0.6.0-rhel8.7-ext/mlnxofedinstall --force --upstream-libs
Logs dir: /tmp/MLNX_OFED_LINUX.1128256.logs
General log file: /tmp/MLNX_OFED_LINUX.1128256.logs/general.log
This program will install the MLNX_OFED_LINUX package on your machine.
Note that all other Mellanox, OEM, OFED, RDMA or Distribution IB packages will be removed.
Those packages are removed due to conflicts with MLNX_OFED_LINUX, do not reinstall them.rpm --nosignature -e --allmatches --nodeps libibverbs libibverbs libibverbs libibverbs librdmacm libibverbs librdmacmStarting MLNX_OFED_LINUX-4.9-6.0.6.0 installation …Install userspace package now and not after kernel packages: libibverbs
Installing libibverbs RPM
Verifying…                          ########################################
Preparing…                          ########################################
Updating / installing…
libibverbs-50mlnx1-1.49606            ########################################
Installing mlnx-ofa_kernel 4.9 RPM
Verifying…                          ########################################
Preparing…                          ########################################
Updating / installing…
mlnx-ofa_kernel-4.9-OFED.4.9.6.0.6.1.r########################################
Configured /etc/security/limits.conf
Installing mlnx-ofa_kernel-modules 4.9 RPM
Verifying…                          ########################################
Preparing…                          ########################################
Updating / installing…
…
…
…Installation finished successfully.Verifying…                          ################################# [100%]
Preparing…                          ################################# [100%]
Updating / installing…
1:mlnx-fw-updater-4.9-6.0.6.0      ################################# [100%]Added 'RUN_FW_UPDATER_ONBOOT=no to /etc/infiniband/openib.confTo load the new driver, run:
/etc/init.d/openibd restartbhargavi@localhost MLNX_OFED_LINUX-4.9-6.0.6.0-rhel8.7-x86_64]$ ofed_info -s
MLNX_OFED_LINUX-4.9-6.0.6.0:
[bhargavi@localhost MLNX_OFED_LINUX-4.9-6.0.6.0-rhel8.7-x86_64]$ uname -r
4.18.0-425.10.1.el8_7.x86_64For a list of supported OS, please refer to the Release notes - General Support in MLNX_OFED - MLNX_OFED v4.9-6.0.6.0 LTS - NVIDIA Networking DocsThanks,
Nvidia SupportPowered by Discourse, best viewed with JavaScript enabled"
345,sharp-error-in-sharp-connect-tree,"Hi,I am encountering SHARP-related error when testing gromacs/21.3
System:  AMD EPYC 7543 and 8 x A100-SXM-80GB
OS: CentOS 7.9.2009 with 3.10.0-1160 kernel
Env: gcc/10.2, cuda/11.4, openmpi  4.1.1 built against both UCX and HCOLL[Problem description]Error only appears when running gromacs on more than 3 nodes.Nevertheless, calculations proceeded to the end without crashing.The log message from sharp daemon is too cryptic. Suggestions to further diagnose the issue are much appreciated.Regards.What is the MOFED version?
how many HCA per server? is there any binding of the process to HCA? Can you check if you can run with 1 process per
server?Hi,MOFED version:
We are using MLNX_OFED_LINUX-5.4-3.1.0.0HCA per server:
For HGX-A100, there are 10 HCA per server, i.e. mlx5_{0…9}Process binding to HCA:
As I understand, the process binding to rail is automatically handled by UCX.
We do not use any specific binding options.Test with one process per server:
In case of GROMACS, we are using full 64 cores per node, i.e a CPU:GPU ratio of 8:1.
When sharp_connect_tree error appears, the total number of processes is 64x4 = 256.
In any case, we will reduce the number of process per node per your suggestion and see if the problem persists.Regards.Powered by Discourse, best viewed with JavaScript enabled"
346,hpcx-mpi-runtime-error,"I am trying to use the HPCX-OPENMPI on the new [ECMWF ATOS supercomputer]We use a Fortran based climate model which runs fine with the Intel-MPI. However, when I try to use the HPCX-OPENMPI, I get segfaults in the most benign part of the code.  The code simply does not run on multiple nodes, MPI_Bcast operation fails with a non-zero exit code. I have checked our code multiple times and different compilers, it works fine.
Only with HPCX-OPENMPI it has an issue.I saw on the forum that there are other things that could be done, for example : Mellanox Interconnect CommunityHowever, this option is not available in the HPCX-OPENMPI version 2.10.0 which is available on the supercomputer. The Intel compiler that HPCX is built with is 2021.4.0.I just want to know if there are any basic things that need to be used while compiling the code with HPCX-MPI? Or are there any runtime arguments that need to be passed with HPCX-MPI?To build your app with HPC-X. You need install full OFED of NVIDIA and install HPC-X. Then use mpicc of HPC-X.Please follow below,https://docs.nvidia.com/networking/display/hpcxv212/Installing+and+Loading+HPC-X#InstallingandLoadingHPCX-InstallingHPC-Xhttps://docs.nvidia.com/networking/display/hpcxv212/Installing+and+Loading+HPC-X#InstallingandLoadingHPCX-BuildingandRunningApplicationswithHPC-XThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
347,sn2010-dont-seem-to-be-able-to-link-at-25g,"We’re running into an issue with these new Mellanox units in that they don’t seem to be able to link at 25G . We have the ports enabled, and we’ve tried several ports with both Auto-speed and forcing the ports to 25G. We cannot get a link no matter what at 25G. Note that 10G links just fine.Also, we have proven that our same test units link at 25G on our HPE branded 25G switches without issue.Any help with this is appreciatedthose are 25G-SR transceivers?if they are 3rd party transceivers then this is expected with the Onyx OS.Powered by Discourse, best viewed with JavaScript enabled"
348,newer-dhcp-relay-rfc-rfc5107-not-isc-dhcp-relay,"HiAs far as I know, the ISC DHCP relay is end of life, but still part of Cumulus.I was wondering, which DHCP relay will Cumulus be using in the future?And is there one which supports RFC RFC5107?In Cumulus we use a fork of ISC DHCP relay. We are investigating if and how it should be replaced, but we don’t have an answer for you at this point.Powered by Discourse, best viewed with JavaScript enabled"
349,mounting-cifs-shares-with-mlnx-ofa-kernel,"I have installed the Mellanox drivers (5.4-1.0.3.1), including kmod-mlnx-ofa_kernel, on an Oracle Linux 8 system using the RedHat Compatible Kernel (4.18.0-305.el8.x86_64). When trying to mount any cifs share (these are regular CIFS shares, not over a Mellanox connection), I get the error:mount error: cifs filesystem not supported by the systemmount error(19): No such deviceThere is no cifs support advertised in /proc/filesystems. Doing a modprobe cifs works, but modinfo cifs shows that it’s the Mellanox dummy cifs kernel module, and trying to mount shares gives the same error as above. Looking deeper, I found that the cifs module is getting overridden by the Mellanox drivers. Why is that? How do I mount my CIFS shares?Hello Dainius,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, unfortunately you are running a non-supported combination with OL8 and the kernel version you are running.Based on the RN, we support the following combination:When we look in our lab on a system installed with MLNX_OFED 5.4, we see that the proper module is loaded, it does not show a dummy module. See below output from one of our lab systems.# modinfo cifsfilename: /lib/modules/3.10.0-1160.el7.x86_64/kernel/fs/cifs/cifs.ko.xzsoftdep: pre: ccmsoftdep: pre: aead2softdep: pre: sha256softdep: pre: cmacsoftdep: pre: aessoftdep: pre: nlssoftdep: pre: md5softdep: pre: md4softdep: pre: hmacsoftdep: pre: ecbsoftdep: pre: dessoftdep: pre: arc4version: 2.10description: VFS to access servers complying with the SNIA CIFS Specification e.g. Samba and Windowslicense: GPLauthor: Steve French sfrench@us.ibm.comalias: fs-cifsretpoline: Yrhelversion: 7.9srcversion: FC201ED8E7403B3F2D0D9F1depends: dns_resolverintree: Yvermagic: 3.10.0-1160.el7.x86_64 SMP mod_unload modversionssigner: CentOS Linux kernel signing keysig_key: E1:FD:B0:E2:A7:E8:61:A1:D1:CA:80:A2:3D:CF:0D:BA:3A:A4:AD:F5sig_hashalgo: sha256parm: CIFSMaxBufSize:Network buffer size (not including header). Default: 16384 Range: 8192 to 130048 (uint)parm: cifs_min_rcv:Network buffers in pool. Default: 4 Range: 1 to 64 (uint)parm: cifs_min_small:Small network buffers in pool. Default: 30 Range: 2 to 256 (uint)parm: cifs_max_pending:Simultaneous requests to server. Default: 32767 Range: 2 to 32767. (uint)parm: enable_oplocks:Enable or disable oplocks. Default: y/Y/1 (bool)Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
350,ubuntu-20-04-ip-over-ib-setup,"I’m trying to setup IP over IB on Ubuntu 20.04 – a little breeding edge.Yesterday, I tried downloading the drivers, adding the DEB distribution to my sources, and installing. It crashed QEMU because it was built against a shared library with an old version of IBVERBS and QEMU expected 1.8. It also generated conflicts with the InBox drivers. So I backed that out.With the InBox version , I only get the core driver module. mxflint finds the device but ibstat doesn’t, presumable because the kernel doesn’t load the driver module.Is there an InBox version of the IB driver? Can I still the IB driver module without also installing the old IBVERBS package?So the eventual solution was to:Hi Josh,As i understand there is no issue , and you were able to resolve the issue with above workaround.Currently our newest driver supports Ubuntu 20.04 at beta levelhttps://www.mellanox.com/products/infiniband-drivers/linux/mlnx_ofedFull support will be added in the next OFED version.Thanks,SamerHi, I tried to build on Ubuntu 20.04-1, kernel 5.8.x but the log says compat.h not found during “make”… wonder if anyone has any tips on how to get around…Or else I probably have to reinstall Ubuntu and go back to kernel 5.4.x the supported one.Powered by Discourse, best viewed with JavaScript enabled"
351,mellanox-crypto-drivers-not-working-with-dpdk,"Hello,
I’m trying to use Mellanox ConnectX-6 NICs along with DPDK. I’m unable to execute the sample applications as specified in this docs: [15. NVIDIA MLX5 Crypto Driver — Data Plane Development Kit 23.03.0-rc1 documentation (dpdk.org)]
(https://doc.dpdk.org/guides/cryptodevs/mlx5.html). DPDK is not able to detect the mellanox crypto driver at all.I’m using MLNX_OFED_LINUX-5.9-0.5.6.0 (OFED-5.9-0.5.6) driver along with ConnectX-6 Dx NIC.Moreover, I’m unable to get/set registers on the crypto device for wrapped/plaintext configuration. Here is a sample error:
node-0:~> sudo mlxreg -d /dev/mst/mt4119_pciconf0.1 --reg_name CRYPTO_OPERATIONAL --get
Sending access register…-E- Failed to send access register: ME_ICMD_OPERATIONAL_ERRORHere are the logs from execution of the DPDK application:
node-0:~>  sudo ./dpdk/build/app/test/dpdk-test -c 1 -n 1 -a ca:00.0,class=crypto cryptodev_mlx5_autotest – --driver-name mlx5EAL: PCI device 0000:ca:00.0 on NUMA socket 1
mlx5_pci
EAL: Probe PCI driver: mlx5_pci (15b3:101d) device: 0000:ca:00.0 (socket 1)EAL: Detected CPU lcores: 128
EAL: Detected NUMA nodes: 2
EAL: Detected static linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: VFIO support initialized
EAL: Probe PCI driver: mlx5_pci (15b3:101d) device: 0000:ca:00.0 (socket 1)
mlx5_crypto: Not enough capabilities to support crypto operations, maybe old FW/OFED version?
mlx5_common: Failed to load driver crypto_mlx5
EAL: Requested device 0000:ca:00.0 cannot be used
EAL: Bus (pci) probe failed.
TELEMETRY: No legacy callbacks, legacy socket not created
APP: HPET is not enabled, using TSC as default timer
APP: Invalid test requested: ‘–driver-name’
APP: Invalid test requested: ‘mlx5EAL:’
APP: Invalid test requested: ‘PCI’
APP: Invalid test requested: ‘device’
APP: Invalid test requested: ‘0000:ca:00.0’
APP: Invalid test requested: ‘on’
APP: Invalid test requested: ‘NUMA’
APP: Invalid test requested: ‘socket’
APP: Invalid test requested: ‘1’
RTE>>cryptodev_mlx5_autotestAny guidance is much appreciated.Hello ckeshavabs,Thank you for posting your query on our community. Based on the output provided, the mlxreg utility is unable to properly change the register value which could mean that the driver is not loaded properly. Please ensure that the MOFED drivers are loaded properly. Once verified, please reboot the server and give it another try.If the issue still persists, We would recommend opening a support case for further investigation of the issue. The support ticket can be opened by emailing "" Networking-support@nvidia.com ""Please note that an active support contract would be required for the same. If you do not have a current support contract, please reach out to our Contracts team at networking-contracts@nvidia.comThank you,
-Nvidia Network SupportHello,
Thank you for getting back to me. I’m not convinced that that mlxreg command is not working correctly. Other mlxreg commands are working as expected, except for the --get and --set commands.For example, here is an example for displaying all the registers on the device:
node-0:~/mft-4.22.1-11-x86_64-deb> sudo mlxreg -d /dev/mst/mt4125_pciconf0.1 --reg_name CRYPTO_OPERATIONAL --get
Sending access register…CHLMM
CHLTM
CHLTR
CNCT
CNMC
CPCS
CPID
CPQE
CREDENTIAL_HANDLE
CRYPTO_OPERATIONAL
CWCAM
CWGCR
CWPP
CWTP
CWTPM
DB_TRESHOLD
DCBX_APPLICATION
DCBX_PARAM
FORE
FP_SL_MAP
HCA_CMD_ENCAP
HOST_ENDIANNESS
IMPORT_KEK_HANDLE
Loopback_Control_Register
MBCT
MCAM
MCC
MCDA
MCDD
MCIA
MCION
MCPP
MCQI
MCQS
MDCR
MDDC
MDDQ
MDDT
MDFCR
MDIR
MDRCR
MDSR
MECCC
MERR
MFBA
MFBE
MFCR
MFM
MFMC
MFNR
MFPA
MFRL
MFSC
MFSL
MFSM
MFSV
MGCR
MGIR
MGNLE
MGPIR
MHMPR
MHSR
MINI
MIRC
MISC_COUNTERS
MJTAG
MKDC
MLCR
MMDIO
MMHI
MMIA
MNVDA
MNVDI
MNVGC
MNVGN
MNVIA
MNVQC
MPCIR
MPCNT
MPECS
MPEGC
MPEIN
MPEINJ
MPFM
MPGO
MPIR
MQDIK
MQIS
MRPR
MRSR
MRTC
MSECQ
MSEES
MSGI
MSPI
MSPMER
MSPS
MSSIR
MTBR
MTCAP
MTCQ
MTECR
MTEWE
MTMP
MTPPS
MTRC_CAP
MTRC_CONF
MTRC_CTRL
MTRC_STDB
MTUTC
MTWE
MVCAP
MVCR
NCFG
NVMF_TARGET_PACER_DEFAULTS
PAOS
PBMC
PBSR
PCAM
PCAP
PCMR
PCNR
PDDR
PFCC
PGMR
PGUID
PLTC
PMAOS
PMCR
PMLP
PMMP
PMPD
PMPT
PMTU
PORT_STATE_BEHAVIOR
POWER_SETTINGS
PPAD
PPAOS
PPCC
PPCNT
PPHCR
PPLM
PPLR
PPRT
PPTB
PPTT
PREI
PTER
PTYS
PVLS
QCAM
QEEC
QETCR
QHLL
QPDP
QPDPM
QPRT
QPTS
QSHR
QSPCP
QSPIP
QTCT
ROCE_ACCL
Resource_dump_registers
SBCAM
SBCM
SBCTC
SBCTR
SBCTS
SBDCC
SBDCM
SBDCR
SBGCR
SBHBR
SBHBR_V2
SBHPC
SBHRR
SBHRR_V2
SBIB
SBME
SBMM
SBPM
SBPR
SBSNS
SBSNT
SBSNTE
SBSR
SET_NODE
SLRED
SLRG
SLTP
UNIT_PERF_COUNTERS_
VHCA_TRUST_LEVEL
ZTT
node-0:~/mft-4.22.1-11-x86_64-deb>Can you please point me to resources for correctly configuring the mellanox crypto driver?Hello,
Here is some more debugging information from my machine. The kernel modules are loaded correctly. I’m not sure why the --get and --set operations are not working correctly.
image1331×762 23.4 KB
Hello @ckeshavabs,Have you solved this problem?Thank you in advance for your answer, thank you.Powered by Discourse, best viewed with JavaScript enabled"
352,ax-xdp-zero-copy-support-for-mlx4,"Hello, it’s unclear to me at the moment if the mlx4 driver in OFED supports AF_XDP zero-copy All of the documentation I can find only mentions mlx5.Can you confirm:ThanksDear customer,If mlx4 has support for AF_XDP zero-copy
No, the mlx4 doesn’t support AF_XDP.if not, is it planned, or was development of mlx4 drivers halted after OFED 5.1?
The mlx4 is no longer supported.    The development of mlx4 drivers was halted for a long time, it’s hard to find out “is it planned” or not.Thanks!
Longran Wei
Nvidia Networking SupportPowered by Discourse, best viewed with JavaScript enabled"
353,is-ha-subent-manager-needed-on-small-fabrics-40-host-ports,"Hello,we setup a small IB fabric with two QM8700 switches and about 40 host ports. I wonder if there is any reason to s setup a HA Subnet Manager configuration. I would prefer to just run a plain sm on each switch, with different priorities to get some redundancy. We won’t use any ‘fancy’ features, no QoS, adaptive. routing, I don’t expect to any  state to preserve to speedup failover? Of course we need to keep the configuration in sync manually. Beside this, is there any other reason to use a HA setup even in small fabrics with ~40 hostports and two switches ?thank you,HeinerA standby SM may be enough for your fairly simple use case.Your thought is OK, Better run active/standby openSM on server other than on switch.Powered by Discourse, best viewed with JavaScript enabled"
354,mellanox-mt4099-cannot-send-files-to-maximum-bandwidth-throughput-in-rdma-applications,"I am facing an issue with my device, I am unable to get maximum bandwidth throughput for file transfer or any other RDMA operations. I get buffer limit issue which I don’t understand because rdma are supposed to have the maximum bandwidth utility but I am only getting 8.13 mbit/sec while rdma transfer supposed to have 9.83gbit/sec or maximum? can you please confirm if there is any issue with the device or provide me with the documentation regarding this specific deviceHello baka_laowai,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.For the adapters to reach max. throughput for RDMA, please make sure all nodes are tuned to max. performance.Please review the following resources to understand Performance Tuning and how to apply it for your use case.Make sure your are running the latest f/w  and driver for your adapter → Linux InfiniBand DriversThank you and regards,
~NVIDIA Networking Technical SupportI have changed the settings in CPU frequency, I changed the scaling governor to “performance” instead of “power-safe” to utilize the maximum CPU frequency but still I am unable to see high throughput when transferring file. If there is some application to test the benchmark, can you share it ?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
355,mlx5-net-failed-to-allocate-tx-devx-uar-bf-nc,"Hey Team,We are getting mlx_net error with DPDK in Ubuntu VM ? We could not find any information on this issue online. Is this a known issue ? Are there any workaround that we can use ?ubuntu:~/project/dpdk/dpdk-21.08/build/examples$ sudo ./dpdk-helloworld -l 0-1 -n 2 -m 1
EAL: Detected 4 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Detected static linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: No available 1048576 kB hugepages reported
EAL: VFIO support initialized
EAL: Probe PCI driver: mlx5_pci (15b3:1016) device: 3840:00:02.0 (socket 0)
mlx5_net: Failed to allocate Tx DevX UAR (BF/NC)
mlx5_net: probe of PCI device 3840:00:02.0 aborted after encountering an error: Cannot allocate memory
mlx5_common: Failed to load driver mlx5_eth
EAL: Requested device 3840:00:02.0 cannot be used
EAL: Probe PCI driver: mlx5_pci (15b3:1016) device: 3c08:00:02.0 (socket 0)
mlx5_net: Failed to allocate Tx DevX UAR (BF/NC)
mlx5_net: probe of PCI device 3c08:00:02.0 aborted after encountering an error: Cannot allocate memory
mlx5_common: Failed to load driver mlx5_eth
EAL: Requested device 3c08:00:02.0 cannot be used
EAL: Probe PCI driver: mlx5_pci (15b3:1016) device: 477d:00:02.0 (socket 0)
mlx5_net: Failed to allocate Tx DevX UAR (BF/NC)
mlx5_net: probe of PCI device 477d:00:02.0 aborted after encountering an error: Cannot allocate memory
mlx5_common: Failed to load driver mlx5_eth
EAL: Requested device 477d:00:02.0 cannot be used
EAL: Bus (pci) probe failed.
TELEMETRY: No legacy callbacks, legacy socket not created
hello from core 1
hello from core 0usdn-admin@ubuntu:~/project/dpdk/dpdk-21.08/build/examples$ lspci
3840:00:02.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx Virtual Function] (rev 80)
3c08:00:02.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx Virtual Function] (rev 80)
477d:00:02.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx Virtual Function] (rev 80)Best Regards
Manish TiwariHey Team,I am new to DPDK, please help me understand if I am missing something.Best Regards
Manish TiwariPowered by Discourse, best viewed with JavaScript enabled"
356,sn2000-series-latency,"The SN2000 series spec sheet talks about: sub 300ns cut-through latency, at 100Gbps speeds.What would the latency be in store-and-forward mode at 10Gbps speeds?Hi Joe,For this request, can you please open a support ticket by sending an email to networking-support@nvidia.com.Thanks,Pratik PandePowered by Discourse, best viewed with JavaScript enabled"
357,flowtable-nat-hardware-offload-on-connectx-5-cards,"Hi,
I’m confused. I’ve seen multiple places on the internet that cards starting from ConnectX-4 have hardware flow offload functionality. Especially NAT44 offloading. I have ConnectX-5 MCX516A-CCAT where hardware NAT and offloading is even mentioned in datasheet but have trouble running it with nft flowtables.Can anyone give me a binding answer if it is supported and worth trying ? I can provide more information about my config but for now i have doubts if it is even supported by this card.Example link where this functionallity is menntioned:Best regards,
Wojciech WronaHi Wojciech,Thank you for posting your query on NVIDIA Community.Hardware Offload is supported on ConnectX-5 HCA. I would like to confirm if you are using MLNX OFED driver. If not, please install the driver by downloading the driver relevant to the OS in use —> Linux InfiniBand DriversFor Connection Tracking & NAT, you may refer section "" Connection Tracking""—> https://docs.nvidia.com/networking/pages/viewpage.action?pageId=111589098#OVSOffloadUsingASAP²Direct-BasicTCRulesConfigurationIf this requires additional debug, a support ticket will be needed. The support ticket can be opened by emailing "" Enterprise-support@nvidia.com ""Please note that an active support contract would be required for the same. For contracts information, please feel free to reach out to our contracts team at "" Networking-Contracts@nvidia.com ""Thanks,
Namrata.Hi again,Can You confirm which model exacly is this ? I dont see any “HCA” in this table:
[https:// docs. nvidia. com /networking/display/ConnectX5EN]
(Spaces added cuz of stupid limit about links)I have MCX516A-CCAT in our lab now.I’m using mlnx_en ver 5.8-2.0.3.0, is especially OFED needed for this functionality ?Yes there is a lot of stuff there but it is only related to TC functions. And i’m talking about NFT FLOWTABLE OFFLOAD functions. As mentioned here:[netfilter flowtable hardware offload [LWN.net]]Can You relate to this one ? Is it supported on MCX516A-CCAT card ? If not which model is needed to support it ?For now it is far from debugging. Now we have to tell that we still don’t know which model supports which functionality and we don’t know which one to buy :)Best regards,
Wojciech WronaPowered by Discourse, best viewed with JavaScript enabled"
358,mcx354a-wont-do-56gbps,"I’ve got a network with several beefy servers and MCX354A NICs on SX6036, all setup for 56GBE. (confirmed by switch link and NIC, cable are MC2207130-002).However I try though, I can’t get past ~40.5gbps.MTU is set to 9000 (9014 in switch).
switch970×488 156 KB

speed740×327 8.63 KB
I’ve tried iperf2 & multiple instances of iperf3, the CPUs hardly break a sweat but I can’t get past this speed barrier it seems. According to MLNX_TUNE all looks good, this profile was set;‘mlnx_tune -p HIGH_THROUGHPUT’CPU is EPYC 7402P, 4x 32GB 3200MHz.CPU is set to performance power mode;root@user:~# cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governorperformanceHello Ruben,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, on the adapter and switch side everything is set correctly.As this is AMD EPYC platform, you need to implement the recommendations from AMD to achieve full performance.You can use the following Guides based on your OS or environment you are running → https://developer.amd.com/resources/epyc-resources/epyc-tuning-guides/Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
359,how-to-enable-auto-negotiation-of-connectx-3-mt27500-family,"My ethernet device is ConnectX-3(MT27500 Family), which has 10GBASE-KR F/W.The device is connected to another 10GBASE-KR device(Not ethernet switch), which is the auto-negotiation is ‘on’. The connection is not linked because my device is the auto-negotiation is ‘off’.I would like to know how I can enable the auto-negotiation.The following is ethtool log of my device.[root@localhost ~]# ethtool enp4s0Settings for enp4s0:Supported ports: [ Backplane ]Supported link modes: 10000baseKR/FullSupported pause frame use: Symmetric Receive-onlySupports auto-negotiation: YesAdvertised link modes: 10000baseKR/FullAdvertised pause frame use: SymmetricAdvertised auto-negotiation: YesSpeed: Unknown!Duplex: Unknown! (255)Port: NonePHYAD: 0Transceiver: internalAuto-negotiation: offSupports Wake-on: dWake-on: dCurrent message level: 0x00000014 (20)link ifdownLink detected: noHello,To enable autonegotiation via ethtool:That said, there are a few considerations to check:Mellanox offers two firmware tools to update and query adapter firmware: mlxup & MFTThank you,Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
360,running-rxpbench-using-sfs,"Hi,I am testing RXPbench on Bluefield-2. My configuration of RXPbench is that it should receive traffic from a scalable function of p0. However, after trying several combinations of dpdk EAL options, it does not work. So my question is: how to set the dpdk EAL and other options to let RXPbench receive traffic from a sf?Supplementary:Hello wsf123,Thank you for posting your inquiry to the NVIDIA Developer Forums.Unfortunately, use of scalable functions is not supported with RXPBench.
Please review the following section of our DOCA SDK documentation for supported configuration steps: RXPBench :: NVIDIA DOCA SDK DocumentationBest regards,
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
361,connectx-6-lx-scheduled-sending-only-sending-25-packets,"We are trying to use send scheduling on a Connectx-6 LX. If we set no timestamps on the packet buffers and manually send each packet at approximately the right time everything works. However if we set timestamps in the buffers then the first 25 packets are sent and the packets are received at the expected times but all subsequent calls torte_eth_tx_burstreturn 0. If its relevant we are sending a single packet in each burst with timestamps 125 us apart.We’ve tried setting the timestamps to low values and the packets are transmitted correctly and as expected thetx_pp_timestamp_past_errorsvalue is incremented. We also set high values and this worked too withtx_pp_timestamp_future_errorsincrementing.Any ideas where to start debugging this? I couldn’t see any API which would give an error code forrte_eth_tx_burstfailing.We’re using DPDK 21.08 with the 5.4.3.1 Mellanox driver on Ubuntu 20.04.It looks like this was caused by not having enough TX descriptors, we were only specifying 64, increasing to 1024 fixes the problem.Powered by Discourse, best viewed with JavaScript enabled"
362,openibd-service-is-failed-on-a-particular-node,"“openibd.service” status:[compute-5 ~]$ sudo systemctl status openibd.service● openibd.service - openibd - configure Mellanox devicesLoaded: loaded (/usr/lib/systemd/system/openibd.service; enabled; vendor preset: disabled)Active: failed (Result: exit-code) since Mon 2021-02-22 02:42:23 CET; 1min 8s agoDocs: file:/etc/infiniband/openib.confProcess: 168237 ExecStart=/etc/init.d/openibd start bootid=%b (code=exited, status=3)Main PID: 168237 (code=exited, status=3)Feb 22 02:42:22 compute-5 openibd[168237]: Module ib_ipoib belong to kernel which is not a part of MLNX_OFED, skipping…[FAILED]Feb 22 02:42:22 compute-5 openibd[168237]: Loading HCA driver and Access Layer:[ OK ]Feb 22 02:42:22 compute-5 openibd[168237]: Module rdma_cm belong to kernel which is not a part of MLNX_OFED, skipping…[FAILED]Feb 22 02:42:22 compute-5 openibd[168237]: Module ib_ucm belong to kernel which is not a part of MLNX_OFED, skipping…[FAILED]Feb 22 02:42:22 compute-5 root[182861]: openibd: ERROR: Module ib_ucm belong to kernel which is not a part of MLNX_OFED, skipping…Feb 22 02:42:23 compute-5 openibd[168237]: Module rdma_ucm belong to kernel which is not a part of MLNX_OFED, skipping…[FAILED]Feb 22 02:42:23 compute-5 systemd[1]: openibd.service: main process exited, code=exited, status=3/NOTIMPLEMENTEDFeb 22 02:42:23 compute-5 systemd[1]: Failed to start openibd - configure Mellanox devices.Feb 22 02:42:23 compute-5 systemd[1]: Unit openibd.service entered failed state.Feb 22 02:42:23 compute-5 systemd[1]: openibd.service failed.We have tried to restart the “openibd.service” but it won’t help.[compute-5 ~]$ sudo systemctl restart openibd.serviceJob for openibd.service failed because the control process exited with error code. See “systemctl status openibd.service” and “journalctl -xe” for details.Hi Prekash,It seems that you are working with a non-supported kernel.You can try to re-install the MLNX_OFED driver with “–add-kernel-support” flag.Regards,ChenHi @Chen Hamami​ ,Thanks for your update.Will It impact any VM on the particular node because 4 VM’s are running on the node.Regards,Prakash Kumar KPowered by Discourse, best viewed with JavaScript enabled"
363,url-filtering-app-it-shows-the-failure-when-executing-commit-database-command,"Hi,I’m able to execute the URL filtering application. However, the application showed the failure and was aborted when executing ‘commit database’ command. I seek for the assistance about this failure. Detail is described as follows.Able to execute the URL filtering application$ sudo /opt/mellanox/doca/examples/url_filter/bin/doca_url_filter -a 0000:03:00.0,class=regex -a
auxiliary:mlx5_core.sf.3,sft_en=1 -a auxiliary:mlx5_core.sf.4,sft_en=1 -c3 – -p
EAL: Detected 8 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: No available hugepages reported in hugepages-32768kB
EAL: No available hugepages reported in hugepages-64kB
EAL: No available hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL:   Device is not NUMA-aware, defaulting socket to 0
EAL: Probe PCI driver: mlx5_pci (15b3:a2d6) device: 0000:03:00.0 (socket 0)
EAL: No legacy callbacks, legacy socket not created
Temporary WARN - Destination table level lower than Source
[01:49:52:893055][DOCA][I][DWRKR]: 1 cores are used as DPI workers
URL FILTER>>Create an empty database and the http filter signatureURL FILTER>> create database
URL FILTER>> filter http test www.slashdot.orgThe  command ‘filter http test www.slashdot.org’ will create the following signature in /tmp/signature.txt. (The following command is executed in another terminal)$ cat /tmp/signature.txt
drop tcp any any → any any (msg:“test”; flow:to_server; pcre:“/www.slashdot.org/I”; sid:1;)
drop tcp any any → any any (msg:“test”; flow:to_server; tls.sni; pcre:“/www.slashdot.org/”; sid:2;)Execute ‘commit database’ command. The application showed the failure and was aborted.URL FILTER>> commit database /tmp/signature.txt
/tmp/265053/signatures.rules
rules file is /tmp/265053/signatures.rules
Info: Setting target hardware version to v5.7…done
Info: Setting virtual prefix mode to 0…done
Info: Setting prefix capacity to 32K…done
Info: Setting compiler objective value to 5…done
Info: Setting number of threads for compilation to 1…done
Info: Reading ruleset…done
Info: Detected 2 rules
Info: Enabling global single-line mode…done
Info: Setting maximum TPE data width to 4…done
Info: Scanning rules…[==============================]…done
Info: Analising possible prefix usage…[==============================]…done
Info: Mapping prefixes, phase 1…[==============================]…done
Info: Mapping prefixes, phase 2…[==============================]…done
Info: Running rules analysis…[==============================]…done
Info: Optimizing memory map…[==============================]…done
Info: Analyzing memory map…[==============================]…done
Info: Calculating thread instructions…[==============================]…done
Info: Beginning to write memory map for ROF2…done
Info: PPE total 1-byte prefix usage: 0/256 (0%)
Info: PPE total 2-byte prefix usage: 0/2048 (0%)
Info: PPE total 3-byte prefix usage: 0/2048 (0%)
Info: PPE total 4-byte prefix usage: 1/32768 (0.00305176%)
Info: TPE instruction RAM TCM partition usage: 2048/2048 (100%)
Info: TPE instruction RAM external memory partition usage: 6218/13M (0.045615%)
Info: TPE class RAM usage: 2/256 (0.78125%)
Info: Estimated threads/byte: 5.183e-10
Info: Finalizing memory map for ROF2…done
Info: Storing ROF2 data…done
Info: Number of rules compiled = 2/2
Info: Writing ROF2 file to /tmp/265053/rof/signatures_compiled.rof2
Info: Writing binary ROF2 file to /tmp/265053/rof/signatures_compiled.rof2.binary…done
mlx5_regex: Rules program failed 22
mlx5_regex: Failed to program rxp rules.
[02:25:01:105139][DOCA][E][UFLTR::Core]: Loading DPI signature failed$[My configuration steps]Create a scalable function interface$ sudo /opt/mellanox/iproute2/sbin/mlxdevm port add pci/0000:03:00.0 flavour pcisf pfnum 0 sfnum 4
pci/0000:03:00.0/294928: type eth netdev eth0 flavour pcisf controller 0 pfnum 0 sfnum 4
function:
hw_addr 02:71:0e:09:0e:2c state inactive opstate detached roce true max_uc_macs 128 trust off
$ sudo /opt/mellanox/iproute2/sbin/mlxdevm port function set pci/0000:03:00.0/294928 hw_addr
02:25:f2:8d:a2:4c trust on state active
$ sudo sh -c ‘echo mlx5_core.sf.3 > /sys/bus/auxiliary/drivers/mlx5_core.sf_cfg/unbind’
$ sudo sh -c ‘echo mlx5_core.sf.3 > /sys/bus/auxiliary/drivers/mlx5_core.sf/bind’Create another scalable function interface$ sudo /opt/mellanox/iproute2/sbin/mlxdevm port add pci/0000:03:00.0 flavour pcisf pfnum 0 sfnum 5
pci/0000:03:00.0/294929: type eth netdev eth0 flavour pcisf controller 0 pfnum 0 sfnum 5
function:
hw_addr 00:00:00:00:00:00 state inactive opstate detached roce true max_uc_macs 128 trust off
$ sudo /opt/mellanox/iproute2/sbin/mlxdevm port function set pci/0000:03:00.0/294929 hw_addr 02:25:f2:8d:a2:5c trust on state active
$ sudo sh -c ‘echo mlx5_core.sf.4 > /sys/bus/auxiliary/drivers/mlx5_core.sf_cfg/unbind’
$ sudo sh -c ‘echo mlx5_core.sf.4 > /sys/bus/auxiliary/drivers/mlx5_core.sf/bind’List scalable function ports$ sudo mlnx-sf --action show
SF Index: pci/0000:03:00.0/294928
Parent PCI dev: 0000:03:00.0
Representor netdev: en3f0pf0sf4
Function HWADDR: 02:25:f2:8d:a2:4c
Auxiliary device: mlx5_core.sf.3
netdev: enp3s0f0s4
RDMA dev: mlx5_3SF Index: pci/0000:03:00.0/294929
Parent PCI dev: 0000:03:00.0
Representor netdev: en3f0pf0sf5
Function HWADDR: 02:25:f2:8d:a2:5c
Auxiliary device: mlx5_core.sf.4
netdev: enp3s0f0s5
RDMA dev: mlx5_4OVS configuration
$ sudo ovs-vsctl add-br ovsbr1
$ sudo ovs-vsctl add-br ovsbr2
$ sudo ovs-vsctl add-port ovsbr1 pf0hpf
$ sudo ovs-vsctl add-port ovsbr1 en3f0pf0sf4
$ sudo ovs-vsctl add-port ovsbr2 p0
$ sudo ovs-vsctl add-port ovsbr2 en3f0pf0sf5Show OVS info$ sudo ovs-vsctl show
cd52839a-d5f5-4987-bec0-b7ed3678502f
Bridge ovsbr1
Port pf0hpf
Interface pf0hpf
Port ovsbr1
Interface ovsbr1
type: internal
Port en3f0pf0sf4
Interface en3f0pf0sf4
Bridge ovsbr2
Port ovsbr2
Interface ovsbr2
type: internal
Port p0
Interface p0
Port en3f0pf0sf5
Interface en3f0pf0sf5
ovs_version: “2.14.1”Activate all interfaces (ovsbr1, pf0hpf, en3f0pf0sf4, ovsbr2, p0, en3f0pf0sf5) by executing “ifconfig network_interface up”.Start the ‘mlx-regex.service’ if it is not started.$ sudo sh -c ‘echo 2048 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages’Run URL filtering applicationThanks.I meet the similar problem today，and I just wanna test the regex
I config the regex followed by the doc, and the result shows belowsystemctl status mlx-regex
mlx-regex.service - Regex daemon for BlueField 2
Loaded: loaded (/etc/systemd/system/mlx-regex.service; enabled; vendor preset:enabled)
Active: active (running) since Sun 2022-04-03 08:59:49 UTC; 8s ago
Main PID: 60802 (mlx-regex)
Tasks: 1 (limit: 19077)
Memory: 528.0K
CGroup: /system.slice/mlx-regex.service
└─60802 /usr/bin/mlx-regexthen I create the datebase# Create a simple rules file, with a single rule “hello\s+world”
echo “1,/hello\s+world/” > test.rules
# Compile the rules file. All output files will be prefixed by “rof/synthetic”
rxpc -f test.rules -o hellofinally I start the dpdk regex test appopt/mellanox/dpdk/bin/dpdk-test-regex -a 03:00.1,class=regex -- --rules hello.rof2 --data test.datand it shows like belowIt return error code 22, I don’t konw what’s this meanI have the same problem and i solved , the regex engine was inactiveThe URL filtering app needs a ruleset that is compiled by the doca_dpi_compiler not rxpcThe doca_dpi_compiler accepts rules in suricata format, then invokes rxpc to compile the regular expression part of the rule, but the rules need to be in the json format the the doca_dpi_compiler outputs.Powered by Discourse, best viewed with JavaScript enabled"
364,is-burning-a-rom-image-required-for-connectx5-to-pxe-boot-https-www-mellanox-com-related-docs-prod-software-mellanox-preboot-drivers-user-manual-v4-0-pdf,"Hi Huy,Please review the following documents in order to ensure the PXE boot has been configured per the instructions provided.Kindly review the below User manual on prerequisites and burning the Expansion ROM image information:https://docs.mellanox.com/pages/viewpage.action?pageId=52008507To download the UEFI network driver, please see the link below:https://www.mellanox.com/products/adapter-ethernet-sw/UEFIFollowing documentation has the PreBoot Drivers User manual for your review:https://docs.mellanox.com/display/PreBootDriversv12UEFI Release Notes for supported FW for ConnectX-5:https://docs.mellanox.com/display/UEFIv142413Documentation link with information on FlexBoot (Legacy BIOS Mode) and UEFI (UEFI BIOS Mode) BIOS configuration:https://docs.mellanox.com/display/PreBootDriversv12/BIOS+ConfigurationThe document below contains information on PXE boot and UEFI PXE Boot:https://docs.mellanox.com/display/PreBootDriversv12/How+to+BootImportant notes for PXE UEFI booting :PXE/ UEFI ROM disabled by default. Flexboot is enabled by default , the desired PXE/UEFI value need to be set via mlxconfig:* mlxconfig -d set EXP_ROM_PXE_ENABLE=1* mlxconfig -d set EXP_ROM_UEFI_x86_ENABLE=1* mlxconfig -d set EXP_ROM_UEFI_ARM_ENABLE=1If after reviewing the documentations above and you still have the issue, please open a support case and we will provide further assistance. If you do not have a current support contract, please email the Nvidia team at Networking-contracts@nvidia.com to set up a valid support contract.Thank you,Nvidia Networking SupportDiscovered I had a defective card. It shows flex boot during POST and show mac address. After swapping out card the machine pxe booted.Powered by Discourse, best viewed with JavaScript enabled"
365,cannot-access-to-smartnic-after-modifying-ovs-bridge-configurations-on-bluefield,"Hi,I was trying to access Bluefield from a server and implement TLS hardware offloading based on it. When I configured the OVS bridge on Bluefield following this guide (TLS Offload :: NVIDIA DOCA SDK Documentation), I added the rshim interface using this command “ovs-vsctl add-port ovs-br0 tmfifo_net0” and this command somehow disables the rshim access from my server to Bluefield. I cannot access it again through ssh. The guide said I should use the interfaces facing the host in that command and probably I used the wrong interface for this command.To change the configuration, I need to access it first, but the current configuration makes me cannot access it from my server via rshim. I wonder what I should do in this case. Thanks in advance for any assistance or suggestions.Hi,
One solution is to use MBF25D kit or another one for your board and connect to console over usb/uart.hiyou can reburn the BFB image to the DPU through rshim interface
then all the config will be reset.Thank you
Meng, ShiHi,Thanks for your help! I have reinstalled the BFB following this link: NVIDIA Networking Docs. Now I am able to ping DPU (command: ping -I tmfifo_net0 192.168.100.2 -c2) but still fail to log in to it (command: ssh ubuntu@192.168.100.2). After I enter the password, it shows: Permission denied, please try again. I wonder if you know the reason. Thanks for your time and help again!hiafter reinstall the BFB, if you do not change the password cfg in BFB, the passoword should be reset to default, ubuntu/ubuntuThank you
Meng, ShiPowered by Discourse, best viewed with JavaScript enabled"
366,loopback-timestamping-errors,"Hello All,
I have a dual-port ConnectX-4 card, and I have a Loopback connection that is one port is a PTP master, and one port is a PTP slave. I am using the linuxptp daemon. Some observations I got are deviating from expected behavior and also from the behavior of other similar dual-port cards:The PTP offset reported in this setup is not constant. Usually (for other cards), since the same crystal oscillator drives both the ports, the PTP daemon reports a constant offset.

Mellanox924×616 144 KB
The PTP offset becomes much worse when additional background UDP traffic (iperf3) is running on the same ports.Is this expected behaviour of this card? If not, an explanation could be that HW timestamping is not being used. However, the card indeed supports it  and the PTP Daemon uses HW timestamping by default (this is also configured in the PTP Config file.)Hi Yash,Please confirm if the below requirements are met:a. Is the system running MLNX OFED driver? If not, please install the latest version 5.5-1.0.3.2 from —> Linux InfiniBand Driversb. Please confirm phc2sys is running based on our community article —> https://support.mellanox.com/s/article/Running-Linux-PTP-with-ConnectX-4-ConnectX-5-ConnectX-6Thanks,
Namrata.Dear Namrata,Thanks for your reply.a. Yes. I verified this once again. Please check the attached screenshot.
b. I think phc2sys doesn’t matter here, as I want to only look at the hardware timestamps of two PTP hardware clocks on the same NIC. Its a loopback setup.When looking over your community article mentioned by you. I see a different output of ethtool -T enpxxxx as compared to the article. In particular, I don’t see the following:software-transmit (SOF_TIMESTAMPING_TX_SOFTWARE)
software-receive (SOF_TIMESTAMPING_RX_SOFTWARE)
software-system-clock (SOF_TIMESTAMPING_SOFTWARE)Could this be the issue? I am looking only at the hardware timestamping capabilities and therefore software timestamping should not matter.

Ethtool1396×481 68.3 KB
Hi Yash,Based on the check on internal system, the output of ethtool -T , the output is same as seen on your system. In order to debug further, I would like to request opening a support ticket which requires an active contract level. The contracts team can be reached at networking-contracts@nvidia.com and support ticket can be opened by emailing Networking-support@nvidia.comThanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
367,ibv-device-open-fails-no-space-left-on-device,"Hi,
I have issues opening the mlx5_3 device in BF2.
Errno: “No space left on device”
I’m activating dpdk with “-a auxiliary:mlx5_core.sf.2,sft_en=1”
What can cause such an issue?Looks like you use SFT lib mentioned on DPDK summit right?This libs should not merge on dpdk mainline so far.764.36 KBYou need run “$ devlink dev show” see if sf  device “auxiliary/mlx5_core.sf.2” exist,
And if it configure “devlink port show”,then if add to OVS “ovs-vsctl show”If not, try setup and configure SF,NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
368,packet-generator-on-windows-for-connectx5-100gbe-connected-to-custom-hardware,"HelloI have a connextX5 mellanox adapter on a windows10 machineIts conected to a another custom hardware at 100GBEI want to test the max thougthut of this link/setupwhich is the best tool we can find that generate packets at max thoughput using this setup ?ThanksShalHiYou can use iperf for windows .ThanksYou can also use NTTCPthank you very muchI will try and return a feedbackShalHelloIperf3 and ntttcp use tcp for passing some control data , not the test dataIs there any tool that use only udp connection , without any tcp connection ?ThanksShaiperf and iperf3 can both be used in UDP mode with the -u flag, but it sounds like you may be looking for a tool capable of stateless traffic generation. Ostinato is a stateless traffic generator inspired by IXIA.https://ostinato.org/Powered by Discourse, best viewed with JavaScript enabled"
369,mellanox-soft-roce-for-windows-10,"How to find out if Mellanox Soft RoCE supports Windows operating systems or not? Looks like there is Linux support but no mention re Windows (Y or N).Hello user58403,Thank you for contacting Nvidia support.Mellanox soft-RoCE is a feature that was introduced for Open-source OS like Linux.For Windows OS, it’s not supported or introduced by Nvidia.Best regards,Nvidia supportPowered by Discourse, best viewed with JavaScript enabled"
370,when-the-port-is-down-then-up-the-ptp-clock-is-synchronized-by-the-system-clock,"I found that the ptp clock will be synchronized by the system clock every time I down then up the port or start ptp4l.  I wonder why there is such a behavior.I found that in addition to the above behavior, when the ioctl(fd, SIOCSHWTSTAMP, &ifreq) driver is called, the ptp clock is also synchronized by the system clock, maybe someone can tell me why.Hi Nico,Thank you for posting your query on NVIDIA Community.I would like to confirm if you are using MLNX OFED driver. THis can be verified by running ofed_info -sAs this requires debug, a support ticket will be needed. The support ticket can be opened by emailing "" Enterprise-support@nvidia.com ""Please note that an active support contract would be required for the same. For contracts information, please feel free to reach out to our contracts team at "" Networking-Contracts@nvidia.com ""Thanks,
Namrata.Hi Namrate,Thank you for your reply, the driver i use is as follows:Thanks,
Nico.Powered by Discourse, best viewed with JavaScript enabled"
371,vf-cant-set-trust-on,"I have same problem with [Bluefield2 DPU] Unable to set VFs trusted mode.
I try to delete all sf/vf and reboot, but the problem still on there

image1576×473 42.2 KB
I also set vf be trust manually, but the message show “Operation not permitted”
Powered by Discourse, best viewed with JavaScript enabled"
372,url-filter-system-design-ovs,"How should I configure ovs so that it respects the system design?
Should I add some flows?
Do I have to set the type of some port in “patch” and connect it to other ports?
How do I connect SF0 and SF1 to URL filter APP?
My current ovs configuration:Powered by Discourse, best viewed with JavaScript enabled"
373,can-i-still-use-a-output-mirror-port-as-a-management-connection,"Hey thereIs there a way to use a output-port of a mirror session as a input for management traffic?It seems that the activation of the mirror session maked the port unusuable for anything else. Is that true or I’m not able to configure it correctly?ThanksThomasHi Thomas,please describe the OS of the switch and the configuration command you ranPowered by Discourse, best viewed with JavaScript enabled"
374,cant-find-the-firmware-of-mt26428-mt-0d20110009,"Hi all, I currently manage an old server with an InfiniBand card. I want to look for the latest firmware for it, but I can’t find this board_id, I want to know which product it is. Do a google search and it used to appear in some Mellanox OFED Linux User’s Manual. I want to find a compatible one, and then force flash it.
lspci:
InfiniBand: Mellanox Technologies MT26428 [ConnectX VPI PCIe 2.0 5GT/s - IB QDR / 10GigE] (rev b0)
ibv_devinfo:
transport:			InfiniBand (0)
fw_ver:				2.7.700
vendor_id:			0x02c9
vendor_part_id:		26428
hw_ver:				0xB0
board_id:			MT_0D20110009
phys_port_cnt:			1
port:	1Powered by Discourse, best viewed with JavaScript enabled"
375,does-nvidia-msn2100-support-lossless-network-priority-based-flow-control-explicit-congestion-notification,"My goal is to build this sparkUCX environment with 3 servers. Could MSN2700 be replaced to MSN2100?https://docs.nvidia.com/networking/pages/releaseview.action?pageId=19819236#RDG:RoCEacceleratedApacheSparkUCX2.4/3.0clusterdeployment.-SparkUCXPluginPowered by Discourse, best viewed with JavaScript enabled"
376,nvdia-quadro-k2000-is-not-compatible-with-windows-10-and-windows-11,"Hello Nvdia Dev,
I am facing issues since long that Nvidia Quadro K2000 is Not Compatible with windows 10 and for 11 too.For Windows 11 :-The issue is that when we Launch any application its shows Error that ( application has been blocked from accessing graphics hardware ) and stop working and my screen Goes Black or Fill with checkes.For Windows 10 :-
Its Totally Hang my Pc or Same Issues like windows 11.also Driver software is removed by Windows Defender security.Kindly Fix Drivers or Anything which works perfectly.Screen shot Attached.Screenshot of My screen With checkeses .
Tried Old nd Latest Both Drivers.Thanks in Advance.

IMG202303250317091920×868 213 KB
Powered by Discourse, best viewed with JavaScript enabled"
377,installing-mlnx-ofed,"i am pretty noob on this. In https://docs.nvidia.com/networking/display/MLNXENv23041130/User+Manual
it states under section “Installing MLNX_EN” that by default, it wil install supporting ethernet only.Hello @g900nvda,Thank you for posting your query on our community.The MLNX_OFED drivers are designed to work with all Mellanox network adapters, supporting both Ethernet and InfiniBand modes. On the other hand, if your adapter card operates exclusively in Ethernet mode and you don’t require all the additional libraries that support InfiniBand, you can opt for the MLNX_EN drivers.MLNX_OFED drivers include MLNX_EN drivers supporting both InfiniBand and Ethernet network adapters, as well as includes the libraries and tools for networking and storage.The link to download the MLNX_OFED drivers can be found here - Linux InfiniBand DriversHope this answers your question.Thanks,
BhargaviNO, it did not answer my question at all, please do not WASTE my and your time if you do not read teh question and do not know the answer. If you look at my question ’ 1. What are the steps to install with infiniband mode?"" you talked about something else, fuzzy stuff or as if i asked ""whether mellanox supports or what mode it supports which I already know. I also did not ask about where to find the driver which I also know as well. I found myself about how to do that from mellanox isntruction and proceeded anyways which is use vma swich, you can add this to your knowledge.Powered by Discourse, best viewed with JavaScript enabled"
378,mlag-lacp-rate-mismatch-for-linux-host,"Hello Community,i have 2 SN2700 in MLAG and a CentOS8 host connected using a mlag-port-channel of 2 ConnectX5 Nics, using one port of each of them.NICs are in bond mode (LACP 4) on host end.Connectivity seems healthy, interfaces are all up, LACP works, pings indicate no packet loss.But: there is a substantial LACP rate mismatch:Mlag-port-channel 7:LACPDUs Marker Marker Marker Rsp Marker Rsp LACPDUs LACPDUs Illegal UnknownPort Sent Recv Sent Recv Sent Recv1/7 0 0 0 0 103482 3993 0 0SAN array connected in the exact same manner has no mismatch.Tested all combinations for LACP rate “0” and “1”, according to “https://community.mellanox.com/s/article/troubleshoot-lag-mlag-lacp-pdu-rate-issues”, but none of them seems to change that.All that might play into it is the fact that “spanning-tree bpdufilter enable” is configured on the switch ethernet ports?LACP mismatch is widely considered a misconfguration, and since this is an iSCSI network, i’d like to have that as clean as possible…Could anyone kindly give me a tip?Hi Frank,Thank you for reaching out to NVIDIA support.Based on the LACP counters output for Mpo7, we can see that the switch is sending out LACPUD’s at a fast rate as compared to its remote partner. This means that the host is configured for LACP rate fast ( bond-lacp-rate 1)The default LACP rate in Onyx is configured as slow (bond-lacp-rate 0) which sets the rate to ask the link partner to transmit LACP control packets every 30 seconds. You can check the lacp rate by using following commandswitch(config)# show lacp interfaces ethernet 1/7Can you please perform the following -Please configure the host for lacp-rate 0 to match the switch default LACP rate.clear the lacp counters from both switches using following commandswitch(config)# clear counters interface mlag-port-channel 7switch(config)# show lacp countersLet me know if any questions.Thanks,Pratik PandeHi Pratik,thank you very much for offering help!I actually did that already in the runup to this post:I tested to have both ends at slow and at fast pace. but the mismatch seems to persist.The iSCSI array connected to the MLAG does not exhibit this issue, just the LINUX server (CentOS8.3)Then doublechecked with “https://community.mellanox.com/s/article/troubleshoot-lag-mlag-lacp-pdu-rate-issues”.Now after having followed your reconmended steps, the situation looks like this:###########################switch1 [mlag-vip-domain1: master] # clear counters interface mlag-port-channel 7Mlag-port-channel 7:LACPDUs Marker Marker Marker Rsp Marker Rsp LACPDUs LACPDUs Illegal UnknownPort Sent Recv Sent Recv Sent Recv1/7 0 0 0 0 253705 148486 0 0###########################Edit: realised later that the mismatch does not apply for the other member conection:###########################switch2 [mlag-vip-domain1: standby] # clear counters interface mlag-port-channel 7switch2 [mlag-vip-domain1: standby] # show lacp countersMlag-port-channel 7:LACPDUs Marker Marker Marker Rsp Marker Rsp LACPDUs LACPDUs Illegal UnknownPort Sent Recv Sent Recv Sent Recv1/8 0 0 0 0 73607 70784 0 0######################################################switch1 [mlag-vip-domain1: master] # show lacp interfaces ethernet 1/7Port: 1/7Port State: BundleMLAG Channel Group: 7Pseudo mlag-port-channel: Mpo7LACP port-priority: 32768LACP Rate: FastLACP Activity: ActiveLACP Timeout: ShortAggregation State: Aggregation, Sync, Collecting, Distributing,LACP Port Admin Oper Port PortPort State Priority Key Key Number State1/7 Bundle 32768 29007 29007 0x7 0x3fswitch2 [mlag-vip-domain1: standby] # show lacp interfaces ethernet 1/8Port: 1/8Port State: BundleMLAG Channel Group: 7Pseudo mlag-port-channel: Mpo7LACP port-priority: 32768LACP Rate: FastLACP Activity: ActiveLACP Timeout: ShortAggregation State: Aggregation, Sync, Collecting, Distributing,LACP Port Admin Oper Port PortPort State Priority Key Key Number State1/8 Bundle 32768 29007 29007 0x8 0x3f###########################The port numbers differ because cabeling is symmetric, but host-ports of the port-channel are on different NICs.###########################switch1 [mlag-vip-domain1: master] # show running-config interface mlag-port-channel 7interface mlag-port-channel 7interface mlag-port-channel 7 mtu 7936 forceinterface mlag-port-channel 7 no shutdowninterface mlag-port-channel 7 switchport access vlan 20interface mlag-port-channel 7 spanning-tree bpdufilter enableinterface mlag-port-channel 7 spanning-tree port type edgeinterface mlag-port-channel 7 dcb priority-flow-control mode on forceswitch2 [mlag-vip-domain1: standby] # show running-config interface mlag-port-channel 7interface mlag-port-channel 7interface mlag-port-channel 7 mtu 7936 forceinterface mlag-port-channel 7 no shutdowninterface mlag-port-channel 7 switchport access vlan 20interface mlag-port-channel 7 spanning-tree bpdufilter enableinterface mlag-port-channel 7 spanning-tree port type edgeinterface mlag-port-channel 7 dcb priority-flow-control mode on force######################################################switch1 [mlag-vip-domain1: master] # show running-config interface eth 1/7interface ethernet 1/7 speed 40G forceinterface ethernet 1/7 mtu 7936 forceinterface ethernet 1/7 mlag-channel-group 7 mode activeinterface ethernet 1/7 lacp rate fastswitch2 [mlag-vip-domain1: standby] # show running-config interface eth 1/8interface ethernet 1/8 speed 40G forceinterface ethernet 1/8 mtu 7936 forceinterface ethernet 1/8 mlag-channel-group 7 mode activeinterface ethernet 1/8 lacp rate fast######################################################ifcfg-bond0:BONDING_OPTS=“mode=4 miimon=100 lacp_rate=1”###########################The only straw i am currently clutching at is that after creating the bond0, i just had opportunity to restart the entire network stack, which usually does the job (networking on L2/L3 works fine).I could not reboot the entire server, since it is in production use…Do you possibly have an idea?Thank you,HilmarHilmar,since we have no idea if changing the lacp reate on the server side does anything (not under Nvidia responsibility) - try to change the lacp rate on the switch to fasthttps://docs.mellanox.com/pages/viewpage.action?pageId=49160752#LinkAggregationGroup(LAG)-lacp(interface)On both switches:(config)# interface ether et 1/7 lacp rate fast(config)# interface ether et 1/8 lacp rate fastyou can check using tcpdump on the host side for the lacp packets (and thus it’s interval) with:Linux servertcpdump -i  ether proto 0x8809Hello Eddie,thank you very much for sticking with it, i really appreciate it!And sorry to be so late, i was required to take care about another problem…Actually i already tried all sensible combinations in the hope one of the both states or even just switching them would change the behaviour,but unfortunately, it didn’t.###########################ifcfg-bond0:DEVICE=“bond0”[ … ]BONDING_OPTS=“mode=4 miimon=100 lacp_rate=1”#BONDING_OPTS=“mode=4 miimon=100 lacp_rate=0”#BONDING_OPTS=“mode=802.3ad miimon=100 lacp_rate=fast xmit_hash_policy=layer2+3”######################################################switch1 [mlag-vip-domain1: master] # show lacp countersMlag-port-channel 4:LACPDUs Marker Marker Marker Rsp Marker Rsp LACPDUs LACPDUs Illegal UnknownPort Sent Recv Sent Recv Sent Recv1/4 0 0 0 0 14298 14304 0 0Mlag-port-channel 7:1/7 0 0 0 0 512988 397901 0 0###########################I then ran the tcpdump you recommended, and it confirmed that the update happens every other second:###########################[root@troubled-host $tcpdump -i enp65s0f0 ether proto 0x8809dropped privs to tcpdumptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on enp65s0f0, link-type EN10MB (Ethernet), capture size 262144 bytes11:06:48.208294 LACPv1, length 11011:06:49.214371 LACPv1, length 110[ … ]11:07:03.216880 LACPv1, length 11011:07:03.684935 LACPv1, length 11011:07:04.222151 LACPv1, length 110^C32 packets captured32 packets received by filter0 packets dropped by kernel###########################Because it looked like the mismatch narrows in over a prolonged time,i attempted to reset counters and compare to a short term count.But this time the reset of the counters for the specific interface simply did not work.Only a global reset worked.At the beginning it looked like the mismatch was resolved,but it eventually came back mounting up actually over time:After a couple of minutes:###########################switch1 [mlag-vip-domain1: master] # show lacp counters[ … ]Mlag-port-channel 3:LACPDUs Marker Marker Marker Rsp Marker Rsp LACPDUs LACPDUs Illegal UnknownPort Sent Recv Sent Recv Sent Recv1/3 0 0 0 0 12 12 0 0Mlag-port-channel 4:LACPDUs Marker Marker Marker Rsp Marker Rsp LACPDUs LACPDUs Illegal UnknownPort Sent Recv Sent Recv Sent Recv1/4 0 0 0 0 13 13 0 0Mlag-port-channel 7:LACPDUs Marker Marker Marker Rsp Marker Rsp LACPDUs LACPDUs Illegal UnknownPort Sent Recv Sent Recv Sent Recv1/7 0 0 0 0 379 365 0 0###########################This is with LACP rate = high at both ends.I have to split my post here technically in order to fit into the editor…Hello Eddie,in continuation of my last post just something i wanted to add:I compared both interfaces again,one of the troubled portchannel 7 and one of the healthy portchannel 4,and i can’t see any difference, except for that the interface 4 is still in default “slow”,and on the SAN side this will possible be as well.On the SAN end i just have to guess unfortunately, since there is no CLI interface available,and no explicite GUI item to control the LACP rate either.Currently configured with no adjustment applied so far in the meantime:###########################switch1 [mlag-vip-domain1: master] # show interfaces mlag-port-channel summaryMLAG Port-Channel Summary:Group Type Local PeerPort-Channel Ports Ports(D/U/P/S) (D/P/S/I) (D/P/S/I)1 Mpo3(U) LACP Eth1/3(P) Eth1/3(P)2 Mpo4(U) LACP Eth1/4(P) Eth1/4(P)3 Mpo7(U) LACP Eth1/7(P) Eth1/8(P)[ … ]######################################################switch1 [mlag-vip-domain1: master] # show running-config interface mlag-port-channel 7interface mlag-port-channel 7interface mlag-port-channel 7 mtu 7936 forceinterface mlag-port-channel 7 no shutdowninterface mlag-port-channel 7 switchport access vlan 211interface mlag-port-channel 7 spanning-tree bpdufilter enableinterface mlag-port-channel 7 spanning-tree port type edgeinterface mlag-port-channel 7 dcb priority-flow-control mode on forceswitch1 [mlag-vip-domain1: master] #######################################################switch1 [mlag-vip-domain1: master] # show running-config interface eth 1/7interface ethernet 1/7 speed 40G forceinterface ethernet 1/7 mtu 7936 forceinterface ethernet 1/7 mlag-channel-group 7 mode activeinterface ethernet 1/7 lacp rate fastswitch1 [mlag-vip-domain1: master] #switch1 [mlag-vip-domain1: master] # show interface eth 1/7Eth1/7:Admin state : EnabledOperational state : UpLast change in operational status: 5d and 23:47:27 ago (11 oper change)Boot delay time : 0 secMac address : b8:59:9f:7d:29:70MTU : 7936 bytes (Maximum packet size 7958 bytes)Fec : autoOperational Fec : no-fecFlow-control : receive off send offSupported speeds : 1G 10G 25G 40G 50G 56G 100GAdvertised speeds : 40GActual speed : 40GAuto-negotiation : EnabledWidth reduction mode : UnknownSwitchport mode : accessMAC learning mode : EnabledForwarding mode : inherited cut-throughTelemetry sampling: Disabled TCs: N/ATelemetry threshold: Disabled TCs: N/ATelemetry threshold level: N/ALast clearing of “show interface” counters: Never60 seconds ingress rate : 968 bits/sec, 121 bytes/sec, 1 packets/sec60 seconds egress rate : 1128 bits/sec, 141 bytes/sec, 2 packets/secRx:247550 packets0 unicast packets247550 multicast packets0 broadcast packets31682620 bytes0 discard packets0 error packets0 fcs errors0 undersize packets0 oversize packets0 pause packets0 unknown control opcode0 symbol errors0 discard packets by storm controlTx:273634 packets1179 unicast packets265847 multicast packets6608 broadcast packets35007140 bytes0 discard packets0 error packets0 hoq discard packetsswitch1 [mlag-vip-domain1: master] ############################From other dont-mismatched mlag-portchannel 4:###########################switch1 [mlag-vip-domain1: master] # show running-config interface eth 1/4interface ethernet 1/4 speed 40G forceinterface ethernet 1/4 mtu 7936 forceinterface ethernet 1/4 mlag-channel-group 4 mode activeswitch1 [mlag-vip-domain1: master] #switch1 [mlag-vip-domain1: master] # show interface eth 1/4Eth1/4:Admin state : EnabledOperational state : UpLast change in operational status: 4d and 23:26:52 ago (7 oper change)Boot delay time : 0 secMac address : b8:59:9f:7d:29:7aMTU : 7936 bytes (Maximum packet size 7958 bytes)Fec : autoOperational Fec : no-fecFlow-control : receive off send offSupported speeds : 1G 10G 25G 40G 50G 56G 100GAdvertised speeds : 40GActual speed : 40GAuto-negotiation : EnabledWidth reduction mode : UnknownSwitchport mode : accessMAC learning mode : EnabledForwarding mode : inherited cut-throughTelemetry sampling: Disabled TCs: N/ATelemetry threshold: Disabled TCs: N/ATelemetry threshold level: N/ALast clearing of “show interface” counters: Never60 seconds ingress rate : 40 bits/sec, 5 bytes/sec, 1 packets/sec60 seconds egress rate : 88 bits/sec, 11 bytes/sec, 1 packets/secRx:22128 packets12 unicast packets14315 multicast packets7801 broadcast packets2332694 bytes0 discard packets0 error packets0 fcs errors0 undersize packets0 oversize packets0 pause packets0 unknown control opcode0 symbol errors0 discard packets by storm controlTx:32428 packets1105 unicast packets28789 multicast packets2534 broadcast packets4782627 bytes0 discard packets0 error packets0 hoq discard packetsswitch1 [mlag-vip-domain1: master] ############################In the meantime i had opportunity to reboot the LINUX server, but that did not change the issue…At this point i wonder, where the “delta” frames actually remain?Since both interfaces don’t report any error or discarded packages.Or is it possible that different metrics are at play?Thank you again for your patience,BestHilmarPowered by Discourse, best viewed with JavaScript enabled"
379,hi-i-have-msx1012b-2bfs-and-it-has-current-version-of-mlnx-os-at-3-4-2008-when-i-go-to-upgrade-options-and-select-current-software-and-target-software-it-doesnt-show-any-files-to-upgrade-how-can-i-upgrade-this-managed-switch-to-latest-software,"Hi, I have MSX1012B-2BFS and it has current version of MLNX-OS at 3.4.2008. When I go to upgrade options and select current software and target software it doesn’t show any files to upgrade.How can I upgrade this managed switch to latest software ?And which latest software MSX1012B-2BFS can take?On ebay someone was selling this switch with Onyx software instead of MLNX-OS. Not sure what that means. Do you know?Hello JG,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, you can only obtain the MLNX-OS/ONYX software version with a valid support contract for the switch. This p/n is EOL for a while now so the latest version available for MLNX-OS/ONYX is version 3.6.8012Based on your current version your upgrade path is the following → 3.4.2008 → 3.5.1006 → 3.6.3004 → 3.6.4112 → 3.6.5000 → 3.6.8012MLNX-OS was the previous name. In the more recent versions of the code the name is changed to MLNX-ONYX. Capabilities stayed the same.Thank you and regards,~NVIDIA Networking Technical SupportThank you very much for your response.So just to clarify, only way to reach to the latest Onyx (3.9.XXXX) is to install each of the MLNX-OS softwares one after another till reaching last MLNX-OS software and then starting with Onyx software from beginning and reaching to latest lavel?So there is no direct path to skip some of the interim software version installations?Also as I have understood. There is some contract just to get the software. And there will be separate contract for support or maintenance or any further add-ons required on top if any?Also you mentioned about EOL, so even with software upgrade contract, I can not eventually upgrade all the way to latest ONYX version available now?With EOL, is there any discount for the software upgrade contract?And if I add more of Mellanox Switches can I use same contract related files for my other switches? How does this work?Powered by Discourse, best viewed with JavaScript enabled"
380,how-to-install-driver-in-coreos,"My host is CoreOS system, and MT27500 is inserted on the motherboard.lspci -v | grep -i mellanox 7e:00.0 Network controller: Mellanox Technologies MT27500 Family [ConnectX-3]I packaged all the deb packages and installed them through the mlnx_add_kernel_support.sh script, but I don’t know which ko files are necessary for the device.At the same time, I should create which devices to the /dev directory, and also provide technical assistance.I have another problem. When I execute the ib_write_bw command, I have the following output. What is wrong with this? How to solve it?Port number 1 state is Init Couldn't set the link layer Couldn't get context for the deviceHi,mlnxofedinstall script should do all the work and it will prompt to run kernel support if necessary by running mlnx_add_kernel_support. However, be sure that your OS is in the listed as supported in Mellanox OFED release notes.Once Mellanox OFED installed properly it will load the driver automatically.mlx4_en & mlx4_ib - are the drivers for ConnectX-2 and ConnectX-3 devicesmlx5_ib - for ConnectX-4 and newer devices.There are other dependencies, but when you do ‘modprobe mlx4_ib’ kernel will resolve them automatically. If you gonna load drivers manually, you might need to load another module one explicitly - ib_uverbs. But as I said, if Mellanox OFED installed in the right way, nothing need to be done manually.Do you know which deb package is installed for mlx4_en and mlx4_ib?Or, if I only install the driver, which deb file should be used?Hi,mlnxofedinstall script should do all the work and it will prompt to run kernel support if necessary by running mlnx_add_kernel_support. However, be sure that your OS is in the listed as supported in Mellanox OFED release notes.Once Mellanox OFED installed properly it will load the driver automatically.mlx4_en & mlx4_ib - are the drivers for ConnectX-2 and ConnectX-3 devicesmlx5_ib - for ConnectX-4 and newer devices.There are other dependencies, but when you do ‘modprobe mlx4_ib’ kernel will resolve them automatically. If you gonna load drivers manually, you might need to load another module one explicitly - ib_uverbs. But as I said, if Mellanox OFED installed in the right way, nothing need to be done manually.Hi @Hughen X​ @Aleksey Senin​Were you ever able to get Mellanox OFED working in CoreOS. I don’t see CoreOS listed in the supported operating systems within OFED, but I am curious if there is a workaround.Powered by Discourse, best viewed with JavaScript enabled"
381,how-do-a-loop-back-test-with-single-card-installed,"I have connect-6 modules installed however currently i am attempting to use loopback test only. Can anyone shed light on this on how to perform loopback test on single card with no cable attached? I am fairly well versed with ethernet but infinityband is my first foray into it by doing belows:So far I am looking at perf_test GitHub - linux-rdma/perftest: Infiniband Verbs Performance Tests and compiled successfully which produced several types of binary: ib_write/read_bw/ etc.Apparently one of them has loop back binary and produced following output:Hello g900nvda,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Unfortunately, this is not possible when running on a single port. You need to have a dual-port adapter for doing this as it needs a SM running on one of the ports and a physical layer between the two ports to have an active link up.Thank you and regards,
~NVIDIA Networking Technical SupportI actually see the card had two physical ports. But under Linux i see two devices one as Ethernet other one is ib. I am not sure two pcie devices had one to one mapping to logistical ports. I am not sure as well if it is possible to set either of them as either ib or Ethernet.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
382,with-cx-5-nic-setup-kernel-nvmeof-following-standard-steps-nvmet-report-error-when-we-try-run-nvme-connect,"dmesg log[Oct20 05:18] nvmet: creating controller 1 for subsystem hostnqn for NQN nqn.2014-08.org.nvmexpress:uuid:9cbac4ba24b8417d9281ae6a79b62673.
[  +0.002779] nvme nvme1: creating 63 I/O queues.
[ +10.156859] nvmet: ctrl 1 keep-alive timer (5 seconds) expired!
[  +0.000010] nvmet: ctrl 1 fatal error occurred!
[ +18.448703] nvme nvme1: mapped 63/0/0 default/read/poll queues.
[  +0.006434] nvmet: could not find controller 1 for subsys hostnqn / host nqn.2014-08.org.nvmexpress:uuid:9cbac4ba24b8417d9281ae6a79b62673
[  +0.000359] nvme nvme1: Connect Invalid Data Parameter, cntlid: 1
[  +0.003230] nvme nvme1: failed to connect queue: 1 ret=16770
any light shed up? do we need must install MOFED? before I remember kernel nvmeOF can very easily setup
our kernel is fedoraOS33[root@ch7-smc-2407 ~]# uname -a
Linux ch7-smc-2407 5.14.18-100.fc33.x86_64 #1 SMP Fri Nov 12 17:38:44 UTC 2021 x86_64 x86_64 x86_64 GNU/LinuxPowered by Discourse, best viewed with JavaScript enabled"
383,infiniband-nic-can-not-be-active-on-bluefied2,"Hi all,
I am experiencing the BlueField-2 device, and i am tring to configure it working as a common Infiniband NIC, but the  device port status is always ’DOWN‘.the cmd ibstat output:

image403×514 11.4 KB
port 1 connects to an IB switch by cable and the port status led in switch is ON, there is no cable for port 2.
Could someone help to figure out if I miss some configuration in my case？Did you figure this out?  Are you running an IB subnet manager somewhere? I think that getting LinkUp means the physical link is there, but the Down State usually means the card hasn’t been found by a subnet manager (which routes the network). Running the opensmd on a host or switch may fix this.As a heads up, the subnet managers built into our switches have never been able to enumerate our BlueField-1 ARM endpoints (though they find the host just fine). The only way I can get the BF1 ARMs up on InfiniBand is to run the subnet manager on a host that has a BF1. Hopefully that’s fixed in BF2.-CraigPowered by Discourse, best viewed with JavaScript enabled"
384,rdma-programming-on-windows,"Hello,
I am trying to do rdma programming on windows using devx sdk and ib_verbs
I was able to port my code developped on linux for the same purpose, however there are some functions which I cannot find in mlx5_verbs.h on windows.
For instance i am looking for queue pair creation, but the function ibv_create_qp does not exist.
Is there any tutorial on rdma programming on windows? Or any suggestions for this problem?HiThe windows rdma programing is diffierent as linux, we suggest you contact nvidia networking support to get more informationHi @shim1,Thanks for your reply!! Do you know any document to follow for windows rdma programming?hiThe detail doc should provided by Microsoft
you can reference this link:Powered by Discourse, best viewed with JavaScript enabled"
385,we-need-the-guide-on-how-to-accelerate-https-with-dpu-bluefield-2-on-host-server,"Bluefield 2 has the engine of Public Key Accelerator (PKA) and the documentation says PKA can be used by openssl.We want to know the guide on how to accelerate HTTPS with Bluefield 2 on HOST SERVER.Does any one know the methods or scheme?Thanks!Powered by Discourse, best viewed with JavaScript enabled"
386,mcx623106an-cdat-card-and-mcx623436an-cdab-card-i2c-registers-details,"Is there a document which briefly describes the i2c register details for the above OCP and NIC cards? I am trying to understand what registers of the above OCP and NIC cards are related to sleep mode operations.Hi,Thank you for contacting us!
Its a bit hard to understand the issue from the description you wrote.
What do you mean by sleep mode operations?
Can you please elaborate regarding what you are trying to accomplish and what is the expected result?Thanks,
Ilan.Is there a i2c register on nic,ocp dedicated to sleep mode. Also, do you know what are the different i2c registers for the molex optical transceiver? So the scenario is to put the nic/ocp to sleep/standby mode when the temperature is too highHi Hrishikesh,We have a dedicated protection mechanism in the chip for high-temperature scenarios.
Regarding i2c registers, for more details, it should go thru regular support ticket with the relevant support team. If you have a support contract with Nvidia, you can open a ticket by sending an email to enterprisesupport@nvidia.comThanks and have a great day!
Ilan.Powered by Discourse, best viewed with JavaScript enabled"
387,read-the-sonic-202012-user-guide,"We now have a user guide for the SONiC 202012 release. And if you see something that needs improving and you’re of a mind, feel free to submit a pull request to get your changes published!The link isn’t working for me as I get a ‘permission denied’ message. Is it just me? Thank you in advance.same for me.  Link is not working for me “Permission Debied”Powered by Discourse, best viewed with JavaScript enabled"
388,ibdump-with-connect-5-failed-to-set-port-sniffer1-command-interface-bad-param,"We are using centos7 and Mellanox ConnectX-5.We are trying to use ibdump from ibdump-6.0.0-1.50100.0.x86_64.rpmThis is the log…ibdumpInitiating resources …searching for IB devices in hostPort active_mtu=1024MR was registered with addr=0x1648010, lkey=0xafc0, rkey=0xafc0, flags=0x1Device : “mlx5_0”Physical port : 1Link layer : EthernetDump file : sniffer.pcapSniffer WQEs (max burst size) : 4096Failed to set port sniffer1: command interface bad paramethtool from mlnx-ethtool-5.4-1.50100.0.x86_64.rpmI then obtained ethtool from mellanox site andtried to run this commandethtool --set-priv-flags enp130s0f0 sniffer onI getethtool: bad command line argument(s)For more information run ethtool -hHow do I run ibdump with connectx-5 successfully?ThanksHello Sainath,Thank you for posting your inquiry to the Mellanox Community.We see that your organization currently has a valid support contract with Mellanox.As such, we’ve opened a Mellanox support ticket on your behalf.We will continue to track this issue via our support portal, and will provide further information to you directly.Thank you,Mellanox Technical SupportHI Sainath，Have you sloved this problem?​ I have the same problem with you. That is “Failed to set port sniffer1: command interface bad param”, i don’t know how to deal with it. If you know the answer, please share with me. Thank you a lot.😁Hello Sainath,I have the same problem with youInitiating resources …searching for IB devices in hostPort active_mtu=1024MR was registered with addr=0x22ca650, lkey=0x862f, rkey=0x862f, flags=0x1Device : “mlx5_1”Physical port : 1Link layer : EthernetDump file : sniffer.pcapSniffer WQEs (max burst size) : 4096Failed to set port sniffer1: command interface bad parami don’t know how to solve it.if you solve this problem,please share the answer with me,thank youPowered by Discourse, best viewed with JavaScript enabled"
389,no-link-w-eth-mode-in-w10-pro-on-connectx-2-dual-qsfp-vpi-ddr-20gbps-infiniband-ib-eth-mhrh2a-xsr,"I know CX-2’s aren’t supported anymore but I’ve acquired about a dozen of them for testing.I have myself two machines with dual QSFP+ CX2’s (MHRH2A-XSR) that can connect together via a 1M passive Arista QSFP+ DAC when running in IB mode and opensm/the subnet manager running. But when I switch them to ETH mode they stay disconnected with no LED link lights.Running the latest drivers “5.50.1474.0” with firmware 2.9.1810. But have also tried  5.35 5.10 4.2
and even tried firmware 2.10.720. On every restart, I get an “Error” msg in windows eventvwr for system: “Failed to initialize Mellanox ConnectX-2 Ethernet Adapter.” and then it leaves them disabled in device manager (but can re-enable them)but also this error “Native_7_0_0: Execution of FW command failed. op 0x9, status 0x8, errno -12, token 0x3d00, in_modifier 0x2, op_modifier 0, in_param 0.”Hello,The last WinOF version that supports ConnectX-2 cards is 4.80.
You can take it from here:Windows OS Host controller driver for Cloud, Storage and High-Performance computing applications utilizing Mellanox’ field-proven RDMA and Transport Offloads
Under Archive.
Also, how are you trying to switch it to Eth mode?
First, check that this is indeed a VPI card that can work also in Eth mode.Best Regards,
VikiPowered by Discourse, best viewed with JavaScript enabled"
390,dpdk-regex-problem-on-bluefield,"I was trying the dpdk-test-regex and it complains about no devices.
Do I need to manually connect to the Regex engine or something?

IMG_6229.HEIC4032×3024 2.89 MB
Make sure the RegEx engine is active:systemctl status mlx-regexIf the status is inactive (Active: failed), run:systemctl start mlx-regexenable RegEx offloading on the host, run:host> sudo /etc/init.d/openibd stop host> sudo echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepagesThen enable host access to the RegEx engine on the DPU:dpu> echo 1 > /sys/bus/pci/devices/0000:03:00.0/regex/pf/regex_enNVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.When I tried this procedure, however, the failed.

IMG_6232.HEIC4032×3024 3.02 MB
Output show app use VFIO driver which nvidia mlx device not build on, likely you not build DPDK correctly with NVIDIA OFED driver,Follow below build dpdk,https://doc.dpdk.org/guides/platform/mlx5.html#mlx5-common-compilationRun app with -a This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
391,how-to-make-incoming-traffic-go-outside-without-any-dma,"Hi.I want to make incoming traffic go outside without DMA in ConnectX-6 using eSwitch.Is there any example code or guide for this use case?What I found from DPDK document is only RSSing or dropping incoming packets.I checked that RSSing and dropping work correctly.
However, there is no guide related to steering incoming flow to outside without DMA.Hi @cerotyki ,Hairpin can match your request.
https://patches.dpdk.org/project/dpdk/patch/1565703468-55617-1-git-send-email-orika@mellanox.com/
https://doc.dpdk.org/api/structrte__eth__hairpin__conf.htmlIn general, the flow should be:In dpdk version 22.03 there is an option to use internal NIC memory for hairpin.Regards,
ChenPowered by Discourse, best viewed with JavaScript enabled"
392,dpdk-21-11-coexist-capability-of-pmd-with-kernel-network-interfaces,"I am targeting to capture only udp data over ipv4 using ConnectX5 device and DPDK dpdk-21.11.1.
Card Details:
Device Type:      ConnectX5
Part Number:      MCX512A-ACA_Ax_Bx
Description:      ConnectX-5 EN network interface card; 10/25GbE dual-port SFP28; PCIe3.0 x8; tall bracket; ROHS R6
PSID:             MT_0000000080
PCI Device Name:  0000:03:00.0
Base GUID:        043f720300b05496
Base MAC:         043f72b05496
Versions:         Current        Available
FW             16.31.1014     N/A
PXE            3.6.0403       N/A
UEFI           14.24.0013     N/AAs per my understanding, mlx5_core allows the PMD to coexist with kernel network interfaces which remain functional (https://doc.dpdk.org/guides/nics/mlx5.html).So, in our code as soon as we call rte_eth_dev_start(port_num), we are able to capture the udp and other data packets available over the interface.
But, now the problem is that the dpdk pmd is also capturing igmp, arp and icmp packets as well and the data is not available to the corresponding kernel network interface.
This in turn further leads to the problem that we are not able to ping this interface from the other host(s) connected in the network.
So, is there a way in dpdk mlx5_core pmd, we can skip all other types of data packets except UDP, so that the kernel network interface will take care of icmp, arp and igmp packets?Hi,Mellanox PMD allows for bifurcation of traffic between user and kernel space, so that specific types of packets can be processed either by DPDK applications or by the Linux kernel stack. By default, packets that do not match any flow rules will be processed by the kernel stack, unless explicitly specified otherwise in a flow rule.
You can read more about flow bifurcation in this link:
https://dpdk-power-docs.readthedocs.io/en/latest/howto/flow_bifurcation.html?highlight=kernel%20stackBest regards,
ChenThanks for the pointer, sir. Now, when I call rte_flow_isolate() before rte_eth_dev_configure(), a warning is thrown while enabling all multicast mode. I need to capture the high bitrate multicast UDP data using Mellanox PMD, I have added a corresponding flow. In this case, I am unable to capture any multicast data.If I remove rte_flow_isolate from my code and call enableallmulticast(), I am able to capture the multicast packets. But in this second approach, as I mentioned, the problem is that all igmp Membership query messages get captured by PMD, and now PMD, we do not send any response to this query, eventually, the switch drops the multicast stream. I need to pass this IGMP message to the Linux kernel.
A few more pointers from your side may help me a lot.
Thanks.Powered by Discourse, best viewed with JavaScript enabled"
393,why-does-the-device-name-of-connectx-6-dx-depend-on-the-os,"Hi,I have a question about the ConnectX-6 DX card installation.When I tried to install it into Ubuntu 22.04 server, lspci command output looks like following:On the other hand, the device name of lspci in the Ubuntu 18.04 server is different from the above:Please note that they have the same ConnectX-6 Dx NICs as shown in the PCIe vendor:device id, 15b3:101d.
The Mellanox driver installation does not mitigate this issue. (22.04 server includes MLNX_OFED_LINUX-5.8-2.0.3.0: and 18.04 server includes MLNX_OFED_LINUX-5.5-1.0.3.2:)Please share any thoughts.Best regards,Hi
THe result of lspci is come from pci utils, not related to OFED.
So you should upgrade PCI utils version, if you use the same version pci utils, the result should be the same.Thanks,
SuoThank you for your reply!Powered by Discourse, best viewed with JavaScript enabled"
394,unknown-psid-mt-0000000493,"Hi all,we’ve recently acquired two used Bluefield2 cards (no vendor in Germany would sell us new BF2 Ethernet cards), however, while trying to debug a kernel error I noticed that the PSID (MT_0000000493) of the card does not exist anywhere and the firmware manager is unable to update the firmware on the device:Is this some earlier silicon revision?RaphaelYes, this is very early product had been replaced by,MBF2H332A-AEEO	MT_0000000540	BlueField-2 P-Series DPU 25GbE Dual-Port SFP56; PCIe Gen4 x8; Crypto Enabled; 16GB on-board DDR; 1GbE OOB management; HHHLThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
395,smb-direct-support,"Does the inbox driver in CentOS 8.4 support NFSoRDMA?  When I first tried, it wasn’t working until I installed the OFED driver.  But now I’ve discovered this driver breaks CIFS support.  I would like to have NFS and CIFS both working with RDMA on the same machine.  Is this possible?  For CIFS, I compile the module with CONFIG_CIFS_SMB_DIRECT.Powered by Discourse, best viewed with JavaScript enabled"
396,how-is-the-lid-assigned,"Hi!How is the LID assigned?What is the flow chart when IB node assign the LID from Infiniband Fabric SM?For example, an ethernet network DHCP FLow chart.Thanks,​Hi JangwonYou can refer IB spec about how Opensm initializie the fabric .ThanksHi Suo​can i get the IB spec document???ThanksInfiniBand specification defines an architecture used to interconnect servers, communications infrastructure equipment, storage & embedded systems.
Est. reading time: 7 minutes
Powered by Discourse, best viewed with JavaScript enabled"
397,any-news-about-bf3,"Hello!
We are currently investigating a new product for which the BF3 DPU would seemingly be a perfect hit, but I was unable to find any information about the target schedule date, nor I could find many details about the specs (besides a 1-page datasheet)…
In an official presentation (beginning of October 2022) I was able to find that the release should be in Q2 FY23 — but the definition of fiscal year being a bit arbitrary, this doesn’t tell me much.
Could anyone point me to additional, up-to-date information, please?Thanks in advance,
RobPowered by Discourse, best viewed with JavaScript enabled"
398,how-to-disable-nic-multi-queue-in-linux,"Currently I am using Ubuntu 18.04 with Linux Kernel 4.15 and mlx5_core. I found my NIC queue is set to 60, now I need to do some experiment and disable multi-queue, I searched on the internet but very few info is present.If the mlx5 driver has a parameter that helps disable it, please tell me how to do that. Thank you !Hi Qing Dong,First, please kindly run the ethtool -l  command to check whether the primary NIC supports NIC multi-queue.[root@localhost ~]# ethtool -l eth0Channel parameters for eth0:Pre-set maximums:RX: 0TX: 0Other: 0Combined: 2 # This value indicates that a maximum of two queues can be configured.Current hardware settings:RX: 0TX: 0Other: 0Combined: 1 # This value indicates that one queue is in effect.You can run the following command on your hosts to disable multiple queues:ethtool -L eth0 combined 1Enjoy it!YuyingPowered by Discourse, best viewed with JavaScript enabled"
399,how-to-measure-rdma-performance-with-mlxndperf-exe-windows,"I have a Bluefield2 card set in an RDMA configuration; installed on a windows PC. I know the setup is correct because I can get rping and nd_rping to work on the ARM and windows host respectively.I’ve tried to run MlxNdPerf.exe but it is unclear what state the ARM needs to be in so that I can measure the performance.On the ARM I’ve tried usingNone of these have worked.I’ve attached the output. It seems like I need to put the ARM in some kind of ready state for the performance tool to work but i’m not sure what this is.Thanks!
MlxNdPerf.png849×602 46.6 KB
Hello Shane,Thank you for posting your inquiry on the NVIDIA Networking Community.We noticed you also opened a support case related to this subject with our Technical Support. We will assist you further through the support case.The issue you are facing is a common request though out the year from several different other customers, but currently we do not have such tool available.I reviewed the support ticket and in that ticket the support engineer (very knowledgeable on RDMA performance testing) provided the correct update.We are also checking further internally if there is a solution on the roadmap, which can be shared with our customer to overcome this issue.Currently the only solution I am aware of is ‘rperf’ from Starwind. Maybe you want to give this a try. We will be checking this tool as well on our lab to see the working of this.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
400,release-notes-for-nvidia-bright-cluster-manager-9-1-14,"== General ==
=  New Features= Improvements== CMDaemon ==
=  New Features=  Improvements=  Fixed Issues== Bright View ==
=  Fixed Issues== Node Installer ==
=  Improvements=  Fixed Issues== User Portal ==
=  Fixed Issues== cm-create-image ==
=  Fixed Issues== cm-kubernetes ==
=  Improvements== cm-kubernetes-setup ==
=  Improvements=  Fixed Issues== cm-setup ==
=  Fixed Issues== cm-uge ==
=  Improvements== cm-wlm-setup ==
=  Improvements=  Fixed Issues== cmsh ==
=  New Features=  Improvements=  Fixed Issues== hwloc ==
=  Improvements== ml ==
=  New Features== openpbs22.05 ==
=  Improvements== pythoncm ==
=  Improvements== slurm ==
=  Improvements=  Fixed Issues== slurm22.05 ==
=  ImprovementsPowered by Discourse, best viewed with JavaScript enabled"
401,ovn-ovs-gateway-offload-using-connectx-6,"Hi,I need some help offloading traffic on an OVN / OVS External GWThe diagram below provides a high level reference for what I am trying to achieve

image792×452 101 KB
The reference architecture for provider networks is outlined here:
https://docs.openstack.org/ocata/networking-guide/deploy-ovs-provider.htmlI’m not using Openstack at all here , just OVN , by configuring the logical topology in ovn-northd .Does anyone have an example configuration showing how the ConnectX-6 card should be configured on the gateway chassis to achieve full hardware offload?
If I use separate physical interfaces for the geneve tunnel and the provider interface , Nvidia tell me that cross port offload is not supported.  Has anyone ever achieved full offload on a gateway using these cards?Nvidia have no ready solution, you can check on openstack or OVS forum.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
402,release-notes-for-nvidia-bright-cluster-manager-8-2-30,"Release notes for Bright 8.2-30== General ==
=Improvements==Fixed Issues=== CMDaemon ==
=Fixed Issues=== Node Installer ==
=Fixed Issues=== Cluster Tools ==
=Improvements=== Machine Learning ==
=New Features==Changes=== User Portal ==
=Fixed Issues=Powered by Discourse, best viewed with JavaScript enabled"
403,inconsistent-udp-multicast-transmission-performance-on-windows,"I have a setup with a Linux server connected directly via a 40 Gb QSFP cable to a desktop PC running Windows 10 that has a Mellanox Connect-X 3 pro network card. I am sending multicast UDP packets on the Linux server and receiving them on the desktop PC.The problem is that the transmit throughput to the desktop is inconsistent across multiple tests. What I mean by this is if we define a test as send 13 Gbps from the Linux server and receive the packets on the desktop PC, that test will run perfectly around 80 to 95 % of the time. However, sometimes the desktop PC only gets something like 12.5 Gbps instead of 13 Gbps.If the transmission is running at full rate, it continues to run at full rate regardless of the test duration (e.g. whether the test is run for 5 seconds or a minute). However, if the test starts and is failing (only achieves the lower data rate of say 12.5 Gbps) then it achieves this rate for the duration of the test, regardless of how long it runs. That is, once the UDP transmission rate is 12.5 Gbps instead of 13 Gbps it does not recover or fluctuate, it is rock solid at the erroneous data rate and visa versa for when the test achieves the correct data rate. (At higher test data rates, the erroneous rate does fluctuate as shown below)I have used iperf, my own multicast benchmark app and the actual program for which I need this to work and they all exhibit this strange behavior.What I have tried:Here’s a screenshot of how it looks in Task Manager while running iperf. I probably ran the test about 30 times or so before getting the erroneous data rate. You can see from the CPU usage as well that at the erroneous data rate less CPU was used / allocated.Task manager view of the data rates across multiple iperf testsThis was the iperf invocation on the Linux server:And this on the Windows desktop, run in 4 consoles with ports 5001 through to 5004:In the erroneous condition there is packet loss across all 4 ports.If I run an app which only opens sockets, joins the multicast group and does nothing else I get something like this (while sending multicast data at around 34.5 Gbps). Again, note the CPU correlation, which makes it look like some kind of load balancing / thermal throttling issue. But why does it not recover from this condition?Task Manager when multicast group is joined but sockets are not read: AXoQRYg0vLx7.png Delete the space I can only put one link in the post (Same site as the first image)… .On Windows with iperf I get about 5 % loss at 21 Gbps and 0 % at 17 Gbps. So I would expect that 13 Gbps should work reliably. The maximum level is worse than Linux in any case as mentioned above (40 Gbps at practically 0 % loss).My questions are:It is nature you get dyamic result for testing TCP/IP, it effect by host OS stack and need tuning, pls check below tuning.https://support.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersPowered by Discourse, best viewed with JavaScript enabled"
404,on-what-switches-is-sonic-supported,"SONiC runs on over 100 switches at the time of this writing, on ASICs from both NVIDIA/Mellanox and Broadcom.Powered by Discourse, best viewed with JavaScript enabled"
405,how-to-reset-uefi-bootloader-password,"Powered by Discourse, best viewed with JavaScript enabled"
406,we-met-an-issue-create-qp-0x500-op-mod-0x0-failed-when-using-mlx5-poll-mode-driver,"by using linux command dmesg, get information, can you help to the root cause, thanks.[ 4.734223] mlx5_core 0000:d8:00.0: mlx5_cmd_check:769:(pid 853): CREATE_QP(0x500) op_mod(0x0) failed, status bad parameter(0x3), syndrome (0xd61c0b)[ 4.734265] infiniband mlx5_0: mlx5_ib_gsi_create_qp:189:(pid 853): unable to create hardware GSI QP. error -22[ 4.735481] infiniband mlx5_0: Couldn’t create ib_mad QP1[ 4.736840] infiniband mlx5_0: Couldn’t open port 1[ 4.738455] mlx5_core 0000:d8:00.0: mlx5_cmd_check:769:(pid 853): CREATE_QP(0x500) op_mod(0x0) failed, status bad parameter(0x3), syndrome (0xd61c0b)[ 4.739721] infiniband mlx5_0: Port 1 not found[ 4.739738] infiniband mlx5_0: Couldn’t close port 1 for agents[ 4.739754] infiniband mlx5_0: Port 1 not found[ 4.739766] infiniband mlx5_0: Couldn’t close port 1Hello,In previous cases exhibiting this error/syndrome, we found that the issue stemmed from the use of a kernel prior to version 5.12 that did not include support for the adapter’s real-time clock functionality.In kernels prior to 5.12, the kernel will try to create QPs with free running timestamp, which is not supported when REAL_TIME_CLOCK_ENABLE is set, thus causing the syndrome exhibited here.We would recommend either:With MFT installed, start the MST service and display your devices’ MST names:# mst start# mst status -vOnce you have the device name, you can set the REAL_TIME_CLOCK_ENABLE option to disabled with the following command:# mlxconfig -d /dev/mst/ set REAL_TIME_CLOCK_ENABLE=0For downloads and more information on the MFT package, please review the following links:MFT 4.18 User Manual:https://docs.mellanox.com/pages/viewpage.action?pageId=64302942MFT Downloads:The Mellanox Firmware Tools (MFT) package is a set of firmware management toolsIf you require further assistance with debugging, we would recommend opening a Support case regarding the issue. If you do not have a current Support contract, please reach out to the team at Networking-contracts@nvidia.com and they can assist you with obtaining one.Thank you,Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
407,ibv-reg-mr-hang,"our program hangs at start up, at ibv_reg_mr, and then I test use ib_send_bw, it also hang forever, see picture below

16756718263271716×397 134 KB
basic environment info list below:
OS:  openEuler 20.03Kernel: 4.19.90-2112.8.0.0131.oelofed: MLNX_OFED_LINUX-5.7-1.0.2.0I already post a thread to linux-rdma mailling list, they say it’s MLNX_OFED issueThis is the server side of the ib_send_bw.Are you running a client? Please attach its output.Is this a GPU server?A client can’t establish connection to server, I don’t have a snapshot.
the OS is a vmware client. we expose the two connectx-5 cards to it, the mlx5_0 is ok, only mlx5_1 hang.
here is the snapshot take from dmesg of our program,

kernel_hang1207×997 504 KB
we found our program hang at start up, and it hang at ibv_reg_mr, then we test use ib_send_bw it also hang.
after reboot the vmware host machine, the problem is solved.hi abbycinI suspect there’s a cqe lost in your startup.
Issue can caused by PCIE HW issue or wrong vm PCIE config.
Could you try test this without VM(same test in hypervisor) to confirm whether we have issue in PCIE HW part?Powered by Discourse, best viewed with JavaScript enabled"
408,how-can-i-get-the-statistics-of-dcqcn-in-cx5-cards,"Hello.I was configuring the DCQCN parameters on 100Gbps CX5 cards and ECN thresholds on the Mellanox SN2100 switch.After I ran some tests(randomly write data from one server to the other server), How could I know if the DCQCN was running during the tests? (equally, could I check the generated/received CNP packets on the server?)After reading the relevant document HowTo Configure DCQCN (RoCE CC) values for ConnectX-4 (Linux) https://community.mellanox.com/s/article/howto-configure-dcqcn--roce-cc--values-for-connectx-4--linux-x , I got that the default DSCP value is 48 and the default priority is 6 of CNP packets. (Check the path “/sys/kernel/debug/mlx5/cc_parameters/*”)So I can check the packets counters by using ethtool -S p4p1 | grep ""rx_prio6_packets"", the result was similar to the following figure:
360×716 98.7 KB
It showed that packets did occur on the 6th priority tx/rx queue. But how could I identify which packets are CNP packets?Are there another methods to monitor the CNP packets or other parameters of DCQCN?Thanks.Hello.I tried to capture all the packets issued and received during the test. But I could not find the packets with ECN marked. Couldn’t these control packets be captured by the administrator?However, I could find the CNP packets like this:
878×495 135 KB
Same format with the RoCEv2 congestion management standard:But this method for getting the statistics of DCQCN was too expensive. Could someone give some pieces of advice?Thank you.Finally, I found the relevant documents.HowTo Read CNP Counters on Mellanox adapters https://community.mellanox.com/s/article/howto-read-cnp-counters-on-mellanox-adaptersFinally, I found the relevant documents.HowTo Read CNP Counters on Mellanox adapters https://community.mellanox.com/s/article/howto-read-cnp-counters-on-mellanox-adaptersHello.I tried to capture all the packets issued and received during the test. But I could not find the packets with ECN marked. Couldn’t these control packets be captured by the administrator?However, I could find the CNP packets like this:
878×495 135 KB
Same format with the RoCEv2 congestion management standard:But this method for getting the statistics of DCQCN was too expensive. Could someone give some pieces of advice?Thank you.Is there a new location for this article ? The link leads to a landing page with no results.CNP packet format:
https://enterprise-support.nvidia.com/s/article/rocev2-cnp-packet-format-exampleCNP counters – I do not recommend using this old article (below updated link) as it is relying on some internal registers which might differ between different HW types. The same counters are available in this article: ESPCommunity
ESPCommunityPowered by Discourse, best viewed with JavaScript enabled"
409,cumulus-linux-os,"Hi Derrick here with University of Chicago, we purchased software license from Cumulus a few years ago. I understand that Cumulus is now Nvidia and the Cumulus portal no longer exists. Can someone please assist me in in retrieving the Cumulus Linux 2.5.2 OS so I can load on to my white box switches. Any relevant information you need from please let me know and I can be reached at derrickj@uchicago.edu thanks.Rather check in the cumulus forums:
https://forums.developer.nvidia.com/c/infrastructure/cumulus-linux/425
the mods there can surely help, this one here is only about the graphics driver.Thanks!I will move this topic over to the Cumulus category.Powered by Discourse, best viewed with JavaScript enabled"
410,unable-to-ssh-into-bluefield-after-separated-host-mode,"I have a BlueField2 and tried following  SDK documentation on enabling separated host mode. After verifying my configuration I tried to remove OVS bridges configuration from the Arm-side by running:However, I am unable to SSH into BlueField as I was before (during default mode). My connection just times out. Am I missing something that prevents SSH access from host to DPU in separated host mode?Thank you!Powered by Discourse, best viewed with JavaScript enabled"
411,gbswitch001-error-timeout-after-2500000-s-and-3-retries,"In /var/log/cmdaemon, constantly seeing errors like this many times in a row :where the <CE><BC> is some sort of control code showing up in reverse
color…  Tried restarting cmd on all nodes but didn’t help, any ideas?
ThanksPowered by Discourse, best viewed with JavaScript enabled"
412,how-to-do-remote-firmware-updates-on-windows,"I am looking to remotely update Firmware for the Mellanox MCX556-ECATAs far as I was able to find out, I have to compress the Firmware (.bin) file to a .mfa2 File, which I was able to do.Now, according to the documentation, (https://docs.mellanox.com/pages/viewpage.action?pageId=43717922) I need to use either ethtool or devlink to burn the firmware.How ever, it seems to me that ethtool and devlink are native commands on Linux, and not available on Windows.So, while the title of this question is a bit more general, and I’d appreciate additional guidance, right now I’m just trying to find out how to get access to the ethtool or devlink commandThanksHi CharlieYou use this card as InfiniBand or Ethernet ?ThanksHello Suo,I’m using this card as InfiniBandThanksHi CharlieYou can download a MFT from following linkhttps://support.mellanox.com/s/public-products-list?cid=a2z50000000CbJoAAKAnd then you can upgrade FW by use flintThanksSUOPowered by Discourse, best viewed with JavaScript enabled"
413,ipsec-encryption-across-dark-fibre-with-two-connectx-6-dx-en,"Hi,I am happy to join this forum! I have a dirk fibre route between two company buildings. As an aim for the future we want to upgrade our backbone/network to 100Gb. I want to keep the traffic over the dark fibre secured/encrypted.The ConnectX-6 Dx provides the feature of inline Hardware IPsec/TLS encryption and decryption.Can I use two SmartNIC to encrypt the dark fibre? Can the SmartNIC work on their own, like pluged in on an Linux device with low CPU and Memory just to power the SmartNIC over PCI?diagram:switch first building  =unencrypted=> 100Gb Port 1 of SmartNIC
=inline encryption on SmartNIC=> 100Gb Port 2 of SmartNIC
=encrypted over Dark Fibre=> 100Gb Port 1 of second SmartNIC
=inline decryption on SmartNIC=> 100Gb Port2 of second SmartNIC
=unencrypted=> switch second buildingHi,The feature you are interested in is described in OFED documentation.
Please take a look at the below link:
https://docs.nvidia.com/networking/display/MLNXOFEDv571020/IPsec+Crypto+OffloadBest Regards,
AnatolyPowered by Discourse, best viewed with JavaScript enabled"
414,nfs-rdma-on-centos-7-small-files-corruption-by-memory-leak,"In an HPC environment, we have nodes running CentOS 7.9.2009 / kernel 3.10.0-1160 mounting an NFS/RDMA server with the following, vendor (Mellanox) documented flags:10.0.0.1:/pool0/home on /mnt/rdma type nfs (rw,relatime,sync,vers=3,rsize=262144,wsize=262144,namlen=255,acregmin=0,acregmax=0,acdirmin=0,acdirmax=0,hard,nocto,noac,proto=rdma,port=20049,timeo=600,retrans=2,sec=sys,mountaddr=10.0.0.1,mountvers=3,mountproto=tcp,local_lock=none,addr=10.0.0.1)If I create an empty file with 701 bytes or more$ dd if=/dev/zero of=/mnt/rdma/test bs=1 count=701I would receive exactly the expected filehexdump /mnt/rdma/test0000000 0000 0000 0000 0000 0000 0000 0000 000000002b0 0000 0000 0000 0000 0000 0000 000000002bdNow if I do the same test with 700 bytes or less, the file is corrupted:$ dd if=/dev/zero of=/mnt/rdma/test bs=1 count=700$ hexdump /mnt/rdma/test | head -100000000 9dfe a757 0000 0100 0000 0000 0000 00000000010 0000 0000 0000 0000 0000 0000 0000 01000000020 0000 0100 0000 a401 0000 0100 0000 00000000030 0000 0000 0000 0000 0000 bc02 0000 00000000040 0000 0002 0000 0000 0000 0000 168c 00830000050 4a0f c612 0000 0000 b000 5a5b 1262 1e750000060 9d04 bc90 1262 1a75 c233 50e9 1262 1a750000070 c233 50e9 0000 bc02 0000 0100 0000 bc020000080 0000 0000 000a 5b5a 00b0 0000 0f9d 030c0000090 0000 2d00 0000 0000 0000 0000 0000 0000When trying the same commands with NFS over TCP instead of RDMA, the file is not corrupted.Tweaking /proc/sys/sunrpc/rdma_memreg_strategy produces very strange results, for example with rdma_memreg_strategy == 6 the noise looks like that:0000120 8898 123d 4506 c0f8 b0d6 3940 44b9 c0f80000130 bc92 ba64 4491 c0f8 165c 0ae4 448e c0f80000140 c82e 19e7 44a9 c0f8 2ddc 11f2 44dc c0f80000150 ad84 79aa 4520 c0f8 b4af 68ca 4571 c0f80000160 b752 b3ef 45cb c0f8 b04a 97b8 462d c0f80000170 7543 d694 4695 c0f8 d5dd 2cdc 4702 c0f80000180 a9bb f717 476e c0f8 4a78 13d9 47d7 c0f80000190 2a26 a9e4 4834 c0f8 71c8 3d91 4882 c0f800001a0 fcae 8d55 48bb c0f8 2b04 bcf8 48dd c0f800001b0 b04c a4be 48e8 c0f8 79d3 db6c 48dc c0f800001c0 9f84 c912 48bb c0f8 096c b285 4886 c0f800001d0 f0ff ee8e 483e c0f8 d171 e773 47e4 c0f800001e0 8e7b f3d1 4779 c0f8 4bbd d26f 46ff c0f8Notice the pattern?Now in the same environment, more ancient nodes running CentOS 7.1 with kernek 3.10.0-229 work just fine without any file corruption.Adding to the mystery, I tried replacing CentOS drivers with Mellanox’s (MLNX_OFED_LINUX-4.9-4.1.7.0-rhel7.9-x86_64) and the size where the corruption occurs reduces to 640 bytes.This is actually worse than I thought, your driver actually LEAKS memory to corrupted files. Here’s an xxd output from what’s supposed to be a 0 filled file:0000000: b449 1fc1 0000 0001 0000 0000 0000 0000 .I…0000010: 0000 0000 0000 0000 0000 0000 0000 0001 …0000020: 0000 0001 0000 01a4 0000 0001 0000 0000 …0000030: 0000 0000 0000 0000 0000 0258 0000 0000 …X…0000040: 0000 0200 0000 0000 0000 0000 8c16 8300 …0000050: 0f4a 12c6 0000 0000 00b0 5fd3 6214 a09b .J…_.b…0000060: 0c5f 0ffa 6214 a088 0f67 27e3 6214 a088 ._…b…g’.b…0000070: 0f67 27e3 0000 0258 0000 0001 0000 0258 .g’…X…X0000080: 0000 5c9a 0000 0000 0000 0000 0000 0020 ….…0000090: 0100 0601 c612 4a0f 0083 168c 0000 0000 …J…00000a0: 0000 0000 0a00 fd56 b000 0000 6d9c 0903 …V…m…00000b0: 0000 0000 3d33 8000 0000 011a 0000 0002 …=3…00000c0: 0000 011a 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d …------------00000d0: 0a0a 0a0a 0a2d 2d2d 2d2d 2d2d 2d2d 2d2d …-----------00000e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d ----------------00000f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2049 -------------- I0000100: 7465 7261 7469 6f6e 2038 3438 3928 2020 teration 8489(0000110: 2035 2920 202d 2d2d 2d2d 2d2d 2d2d 2d2d 5) -----------0000120: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d ----------------0000130: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 0a0a 0a20 ------------…0000140: 2020 2050 4f54 4c4f 4b3a 2020 6370 7520 POTLOK: cpu0000150: 7469 6d65 2020 2020 302e 3033 3532 3a20 time 0.0352:0000160: 7265 616c 2074 696d 6520 2020 2030 2e30 real time 0.00000170: 3334 390a 2020 2020 5345 5444 494a 3a20 349. SETDIJ:0000180: 2063 7075 2074 696d 6520 2020 2030 2e30 cpu time 0.00000190: 3332 383a 2072 6561 6c20 7469 6d65 2020 328: real time00001a0: 2020 302e 3033 3239 0a20 2020 2045 4444 0.0329. EDD00001b0: 4941 473a 2020 6370 7520 7469 6d65 2020 IAG: cpu time00001c0: 2020 332e 3139 3638 3a20 7265 616c 2074 3.1968: real t00001d0: 696d 6520 2020 2033 2e31 3938 310a 0000 ime 3.1981…00001e0: 3131 3238 2020 2020 0a48 4352 2020 2020 1128 .HCR00001f0: 2020 2020 2020 2020 2038 320a 2020 2020 82.0000200: 2031 322e 3934 3834 3732 3233 2020 2020 12.948472230000210: 2020 2020 2d34 2e31 3938 3830 3635 3138 -4.1988065180000220: 2020 2020 2020 2020 302e 3933 3536 3336 0.9356360000230: 3039 3935 2020 2020 0a20 2020 3133 2e33 0995 . 13.30000240: 3837 3634 3434 3334 3120 2020 2020 2020 8764443410000250: 322e 3732 3939 3339 2.729939Hi Emile,Thank you for using NVIDIA product and reporting memory leak about NFS over RDMA in OFED 4.9.As from Mellanox OFED version 3.4-x and above NFSoRDMA is not supported anymore.We’re confirming interanlly if OFED 4.9 support NFSoRDMA or not.At the same time, ​we suggest you switch to use the drivers supplied by the OS vendor (INBOX) to use NFSoRDMA.I’ll feedback here if there is update.Regards,LeveiHi Level,I also tried 4.9 and it is affected. Inbox kernels to this day, including current Linux kernel, are also affected by this bug. I’ve opened a bug report at RedHat’s bugzilla.I published a fix here NFS/RDMA on CentOS 7, small files corruption - Unix & Linux Stack Exchange TL;DR it’s the inline mode which is broken when reading data < 700B, forcing chunked mode for every file “fixes” the issue.Powered by Discourse, best viewed with JavaScript enabled"
415,ubuntu-20-04-2-mlnx-ofed-installation-troubles,"Hi,I am pretty new at this and trying to install mlnx_ofed for a small scale application involving:3 HPz840 workstations with Mellanox ConnectX-3 VPI MCX354A-FCBT cards (Firmware version: 2.42.5000) connected together with a Mellanox SX6005 unmanaged switch.I got 5.11.0-43-generic 20.04.2-Ubuntu installed on all 3 machines.I have downloaded MLNX_OFED_LINUX-4.9-4.1.7.0-ubuntu20.04-x86_64 which seems to be the correct driver for my ConnectX-3 cards.The trouble is this driver seem to be supporting kernel 5.4.0-26-generic.What are my options other than downgrading the kernel?I attached the installation log (mlnx-ofed-kernel-dkms.debinstall.log)I have also tried:sudo ./mlnxofedinstall --without-dkms --add-kernel-support --kernel 5.11.0-43-generic --without-fw-update --forceFailed to build MLNX_OFED_LINUX for 5.11.0-43-genericattached the log of this attempt as well: mlnx_ofed_iso.38990Any help would be appreciatedCheersmlnx_ofed_iso.38990.log (881 Bytes)mlnx-ofed-kernel-dkms.debinstall.log (122 KB)Hello Onur,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, the kernel version you are running is a HWE (Hardware Enabled) kernel, which unfortunately we do not support.The kernel which is supported with this release is 5.4.0-26-generic.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
416,nvme-tcp-via-mellanox-nic,"Do the Mellanox ConnectX NIC support NVME over TCP?
Is there a addtitional driver required?
OS:
VMware ESXi 7
Ubuntu 20.04 LTS
Windows 2019SilvioHi Silvio,We don’t support nvme-tcp with MLNX_OFED,if you want nvme-tcp you should use the inbox driver and contact the OS vendor for any issue.Best Regards,
VikiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
417,maximum-mtu-on-sn2410-running-sonic,"We are evaluating SONiC running in an SN2410 switch and currently experimenting what seems to be a strange behaviour regarding MTU. The version we are running is:We have a working LAG with 2x100G interfaces:By default MTU is set to 9100:We change the MTU to 9216 as this is our desired configuration:The LAG is a trunk with just a few VLANs. For one of these VLANs we have a L3 interface:We have a direcly connected L3 device with a VLAN interface on the same IPv6 network:The L3 device has MTU set accordingly and fragmentation isn’t seen when using the same L3 device with another switch not running SONiC and with MTU setup in the same way.So it seems that despite MTU is set to 9216, the default value of 9100 is being considered.Is the SN2410 limited to 9100 and perhaps SONiC isn’t rejecting a higher value when we set one?Any other suggestions that may explain this behaviour?Thank you in advance.Powered by Discourse, best viewed with JavaScript enabled"
418,vma-usage,"Dear all,I am new to this forum and to the topic of VMA.
I am trying to use the offload capability offered by VMA to improve network throughput performance using NVidia infiniband cards.
For my tests I started with a simple iperf3 test.Below are the preliminary commands I issued:(both on the server and on the client).
I then started the test by running the commands:The VMA library issued the following warning (both sides):Now, while it is clear to me what it means to set a capability on, say, an executable… I am a little confused about setting a capability on a resource such as the ib iface.
I downloaded the VMA PDF user manual, but the only mention of setcap is about adding the capability to the test tool, not to the above resource.Any suggestions would be appreciated.
Thanks in advance.Hi Alvise,Thank you for posting your query on NVIDIA Community.I would like to inform that, unfortunately, we have the following limitation mentioned in our VMA documentation realted to IPoIB —> https://docs.nvidia.com/networking/display/VMAv972LTS/Changes+and+New+Features"" * IPoIB is no longer supported with MLNX_OFED v5.1 and above""Thanks,
Namrata.Hi Namrata,
question: is there an offloading alternative for IPoIB and e.g. MOFED 5.1 + ConnectX5,6? We were using SDP before, that then got deprecated in favour of VMAthanks!
leoHi Leonardo,Apart from VMA, we offer XLIO, however, it is also suited for Ethernet network —> https://docs.nvidia.com/networking/display/XLIOv207LTS/NVIDIA+Accelerated+IO+(XLIO)+Documentation+Rev+2.0.7+LTSThanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
419,when-run-ib-write-bw-with-2g-msg-mlx5-reports-0x68-why,"Hi @511252461 ,How do you run the test? Do you use the “-s” flag?
Please share CLI and output.Thanks,
ChenPowered by Discourse, best viewed with JavaScript enabled"
420,release-notes-for-nvidia-bright-cluster-manager-8-2-29,"Release notes for Bright 8.2-29== General ==
=  Improvements== Fixed Issues=== CMDaemon ==
= New Features==Improvements==Fixed Issues=== Node Installer ==
=Improvements==Fixed Issues=== Cluster Tools ==
=Fixed Issues=== Machine Learning ==
=New Features=Powered by Discourse, best viewed with JavaScript enabled"
421,forward-to-sft-ipv4-udp-failed-error-no-pmd-support-for-sft,"hello i have this error here, do you know how i can solve it?
what does “no PMD support for SFT” mean?Hey there. Normally that’s due to the SF (and the representor) are not in trusted mode.https://docs.nvidia.com/doca/sdk/scalable-functions/index.html It’s in Step 2. Prerequisitesmlxreg -d /dev/mst/mt41686_pciconf0 --reg_id 0xc007 --reg_len 0x40 --indexes ""0x0.0:32=0x80000000"" --yes --set ""0x4.0:32=0x1""This will only affect SFs that are created after the command is entered. That means you have to either delete/recreate the SF, or create another SF and use that one.The way we set trusted/untrusted mode for the SFs will be changing in the upcoming DOCA 1.2 release.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
422,install-ufm-on-rocky-linux-8-5,"Dear All,
Has anyone succeeded and installed UFM on Rocky Linux 8.5 ?Can one skip or workaround this error ?
Do you want to install UFM server  [Y|n]? yUFM IB PREREQUISITE TESTInstalled distribution                                      [FAILED]
ERROR:
Installation package doesn’t match local machine distribution rocky8, please install suitable UFM packageHi,
UFM only support following OS.
Not include rocky OS.Thanks64-bit OS:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
423,nvme-driver-hang,"Software Environment
OS: oracle linux 8.7 with kernel 4.18
mlnx_ofed: 5.9Reproduce Steps:Results:attachment.txt (16.1 KB)Oracle8.7 not support by OFED5.9Pls select supported OS/Kernel versionhttps://docs.nvidia.com/networking/display/MLNXOFEDv590560/General+Support
图片1837×2030 255 KB
same issue on  Oracle Linux 8.5 with OFED 5.7

图片1145×392 35.8 KB
Powered by Discourse, best viewed with JavaScript enabled"
424,ipsec-full-offload-connectx-6-dx-upstream-linux,"Hi,I saw that the ipsec full offload code landed in the Upstream Kernel 6.2.
Is there any way to activate the full offload mode as well?
I can not see a way to change the mode in the upstream kernel.Another question, does the full offload mode only work in switch legacy or switchdev mode? I have read contradicting documentations about that and from my testing legacy mode should be correct.From my testing the full offload in the upstream kernel also does not work at the moment, offload is added from ip xfrm but there is no traffic being encrypted the ipsec counter in ethtool are not being increased.Thanks and best
SvenHello @sven.auhagen,Thank you for posting your query on our community. I would like to refer you to this documentation on Configuring IPSec Full Offload -https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/IPsec+Functionality, not sure if you had a chance to review it. Please note that this feature is supported only on BlueField-2 based platforms.You can configure IPSec to full offload after setting SR-IOV mode to ‘legacy’. This is also mentioned in the above provided link.
If you require further assistance on this, I would suggest you to open a support case for further investigation of the issue. The support ticket can be opened by emailing ""Networking-support@nvidia.com ""Please note that an active support contract would be required for the same. If you do not have a current support contract, please reach out to our Contracts team at networking-contracts@nvidia.comThank you,
-Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
425,can-not-run-test-regex-on-host,"I wanna run/home/notroot/regex# /opt/mellanox/dpdk/bin/dpdk-test-regex -c 1 -a 01:00.0,class=regex --file-prefix=rxpbench -- --rules ./rof/helloworld.rof2.binary --data helloworld.txt --nb_job 1on host to test the RXP,  but it failed and the log shows below.My env isBut when I run the same test-regex on BF, it works:I wanna know why I cannot run the test on the host?Hi Waterzhu,It would be great if you could confirm if you have referred to the following link —> RXPBench :: NVIDIA DOCA SDK DocumentationIn addition, the output displays the following message:mlx5_regex: Not enough capabilities to support RegEx, maybe old FW/OFED version?
mlx5_common: Failed to load driver regex_mlx5
EAL: Requested device 0000:01:00.0 cannot be usedPlease confirm if the OFED and firmware on the host are latest. This can be checked as follows:a. #ofed_info -s
b. #ibv_devinfo |grep fwAlso, to confirm you are using the correct device, please run the below command on the host:
#mst status -vThanks,
Namrata.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
426,how-push-full-100gbps-port,"Hello, we have Dual AMD EPYC 7742 / 512GB DDR4 / 12x8TB NVMe (Kioxia CD6 7.68TB NVMePCIe4x4) / ConnectX6 (MCX653106A-HDA_Ax) 100gbps.Ubuntu 22.04 64bit, Nginx v1.22. Content (hls video, many .ts chunks) stored on software raid0 for best read speed. Driver NIC -  MLNX_OFED_LINUX-5.6-1.0.3.3, ethernet version, but in system have openibd (infiniband?).Device Type:      ConnectX6
Part Number:      MCX653106A-HDA_Ax
Description:      ConnectX-6 VPI adapter card; HDR IB (200Gb/s) and 200GbE; dual-port QSFP56; PCIe4.0 x16; tall bracket; ROHS R6
PSID:             MT_0000000225
PCI Device Name:  /dev/mst/mt4123_pciconf1
Base MAC:         043f72a12dc6
Versions:         Current        Available
FW             20.33.1048     N/A
PXE            3.6.0502       N/A
UEFI           14.26.0017     N/AStatus:           No matching image foundHyperThreading is off, Node Per Socket = 1 (total 2 NUMA NODES, L3 cache is disable). We try with NPS0, NPS4 with L3 cache but it not give perfomance.have many tuning option in sysctl (bbr, fair queueing…), txqueuelen 20000, ring buffers rx/tx = 8192rx-usecs: 1024 / rx-frames: 2048
tx-usecs: 3072 / tx-frames: 6144
high number coalesce give better softirq% load CPU (almost 2 times lower CPU irq% load)because
/sys/class/net/enp161s0f0np0/device/numa_node = 1
then first nginx with start with 64 workers with affinity on core from numa node = 1 ( 64-123 core), we have native_queued_spin_lock_slowpath.part.0 < 5% in perf top, but maximum can push ~75gbps (~35k connection).
next, incrase wokers up to 128 - give ~80-82gbps and native_queued_spin_lock_slowpath.part.0 have is good, but in low traffic ( < 50-60gbps) 10-40%.
And now have workers is 256, worker_cpu_affinity auto, reuseport, sendfile on - 85gbps (one time have 92gbps maximum, but not stable). After more traffic we have crash, because non linear grow up proccess softirqd/nginx to 100% CPUs and after down traffic - 20-30% or more.Question: how push 100gbps port stable ? now we make limited bandiwdth to 80gbps and have stable work server. Better server hardware can not be found, we have the simplest task of distributing files ready on the file system. How else can you optimize performance? We thought to add a NIC to socket = 0, but in our case there is no such possibility, 3 slots pcie in CPU2 (numa node=1) and one pcie CPU1 half lenght not possible put NIC…on 80gbps we have load CPU: ~1100% system, 550% user, 2200% softirq, but after non linear grow ip IRQ%. Kernel TLS we tested, its not help us. Only decrease user%, but increase many system%.Hi z3rom1nd3I wonder if you went through the artiticle “Performance Tuning for Mellanox Adapters”
There are still several factors to consider except you did.Please refer to below articles.*reference
Performance Tuning for Mellanox Adapters (nvidia.com)Tunings:BIOS/iLO:Grub:mlxconfig tuning:Other tuningPCI_WR_ORDERINGHello, thanks u. This artiticle i read before…about BIOS:
iommu is disable, smt disable, c state disable, nps=1grub:
iommu=pt, numa_balancing its kernel options not grub! another options i put now
intel_pstate=disable and intel_idle.max_cstate=0 can put options if i have AMD EPYC ?mlxconfig:
ADVANCED_PCI_SETTINGS = 0, change to 1
MAX_ACC_OUT_READ - not found in list options
PCI_WR_ORDERING is default 1 (force_relax(1))other tuning:
PCI MaxReadRequest 4096, PCIe Max Payload 512, flow control disable, ring buffer 8k. Coalesce is high:rx-usecs: 1024
rx-frames: 2048
tx-usecs: 2048
tx-frames: 4096MTU is defailt 1500, for webserver nginx i can install 9000 ? all many client web use 1500 or i not understand this ? router can change to 9000, its not problem, but i afraid its will be problem to end clients (he not support 9k MTU)MAX_ACC_OUT_READFor MAX_ACC_OUT_READ you will need to upgrade the MFT package.now we set this options except numa_balancing, he is enable because we use all 128 core, on one numa node we can push only 70-75gbps. if we disable this option have instant grows softirq%, may be need in nginx bind affinity only core of numa node 1.after 1-2 day after restart  i found in statistic
rx_discards_phy: 189877
tx_discards_phy: 0
rx_corrected_bits_phy: 0
rx_err_lane_0_phy: 0
rx_err_lane_1_phy: 0
rx_err_lane_2_phy: 0
rx_err_lane_3_phy: 0rx_discards_phy its problem with buffer NIC ? ring buffer set maximun tx 8196 rx 8196
tcp/ip buffer also is high:
net.core.rmem_default = 2147483647
net.core.wmem_default = 2147483647
net.core.rmem_max = 2147483647
net.core.wmem_max = 2147483647
net.ipv4.tcp_rmem = 4096 87380 2147483647
net.ipv4.tcp_wmem = 4096 65536 2147483647
net.core.optmem_max = 25165824and i noticed often, almost every day after 80gbps non linear grows incomming traffic. not always, but in rush hour and after 80gbps. if normal on 80gbps ~ 3m packets and ~1.5-1.6gbit/s, we have sometimes to 4.5+m packets andn to 2.5-3gbpsi not know trust tcpdump info on 100gbps port, but he say many DUP ACK packets.UPDATE:
rx_discards_phy: 497999
its parametrs increases only have problem… today we push 89gbps OUT and have 1.6gbps IN ~3M input packets… proccessor ~sys 1000%, user 5000%, irq 2200, softirq# threads 5-12%, next we have fall down to ~60gbps and sys ~ 1500-2000 and sometimes 40000%… softirq ~ 40000%,  softirq# threads ~ 40%+
i cant understand why its hapenned, CPU have idle minimum 85000% on both CPU, numa node1 have 50-60% busy. may be its capped bottleneck infinity fabric between CPUs ? how check this problem ? i check if nginx binded only on numa1 and stay working sometimes ( 5 minutes) and after again turn cpu affinity both cpus its somotimes help resolve this freezy.




atop in 85gbps

atop1920×1080 211 KB
we have problem in RX traffic. TX not have error.i show ethtool priv flags and found options:rx_cqe_moder on
rx_striding_rq on
rx_no_csum_complete: offmaybe try switch they?UPDATE:
i try this
// Fix RX performance on mixed traffic flows
/usr/bin/mcra $pci_addr 0x815e0.0 0
/usr/bin/mcra $pci_addr 0x81640.0 0
but cant see  any changesI think you should open a CASE to get the optimal tunning suitable for your environment.Powered by Discourse, best viewed with JavaScript enabled"
427,i-am-using-hpcx-and-am-having-trouble-using-fortran-with-the-precompiled-fortran-mpi-mod-any-attempt-to-import-the-module-file-mpi-mod-produces-an-error-is-there-a-way-to-use-fortran-with-newer-or-different-i-e-ifort-compilers,"I am using HPCX 2.70-redhat7.7The mpi mod file showsGFORTRAN module version ‘10’ created from mpi.F90Is it possible to use this with newer versions of gfortran? How do we use Intel compilers?Is the src code for mpi.F90 available so that we can build a suitable mod file for whatever fortran compiler we have?I see online the following conversion table.GFORTRAN module version ‘10’ => gcc 4.8.3GFORTRAN module version ‘12’ => gcc 4.9.2GFORTRAN module version ‘14’ => gcc 5.1.0No, MPI.MOD is build combine with HPC-XPowered by Discourse, best viewed with JavaScript enabled"
428,issues-with-a-static-mac-on-vxlan-access-port,"Hi.There are some issues with static mac and vxlan in Cumulus with bcm lib.When adding a static mac into an access port with vxlan,
it is added into kernel, but not added into asic.But it is correctly added  into kernel and asic, if the vlan-id is trunk, or the port is bond (static or dynamic).Tested on Cumulus Linux 4.3.0 with bcm asic (Trident2 and Tomahawk)Test config with trunkauto vni13101
iface vni13101
bridge-access 3101
vxlan-id 13101auto swp1
iface swp1
bridge-vids 3101auto bridge
iface bridge
mtu 9100
bridge-ports swp1 swp3 vni13101
bridge-mcsnoop no
bridge-stp off
bridge-vids 3101
bridge-vlan-aware yes
post-up bridge fdb replace 00:11:12:13:14:15 dev swp1 master static vlan 3101 stickymac 00:11:12:13:14:15 presents in the kernel and in the asicAnd here is a test config with access vlanauto vni13101
iface vni13101
bridge-access 3101
vxlan-id 13101auto swp1
iface swp1
bridge-access 3101auto bridge
iface bridge
mtu 9100
bridge-ports swp1 vni13101
bridge-mcsnoop no
bridge-stp off
bridge-vids 3101
bridge-vlan-aware yes
post-up bridge fdb replace 00:11:12:13:14:15 dev swp1 master static vlan 3101 stickymac 00:11:12:13:14:15 presents only in kernel, but not installed into asic.As a solution, a single port static bond could be.auto vni13101
iface vni13101
bridge-access 3101
vxlan-id 13101auto swp1
iface swp1auto bond1
iface bond1
bridge-access 3101
bond-slaves swp1
bond-mode balance-xor
bridge-access 3101auto bridge
iface bridge
mtu 9100
bridge-ports bond1 vni13101
bridge-mcsnoop no
bridge-stp off
bridge-vids 3101
bridge-vlan-aware yes
post-up bridge fdb replace 00:11:12:13:14:15 dev bond1 master static vlan 3101 stickyPowered by Discourse, best viewed with JavaScript enabled"
429,automating-cpu-pinning-for-offload-worker-queue-in-hwol-conntrack-offload-environment,"Hello,I am currently testing for the introduction of HWOL and conntrack offload.
In this process, I am trying to apply CPU pinning to softirq and offload worker queue.For CPU pinning of the offload worker queue, I am planning to apply it by entering the CPU bitmask in the /sys/devices/virtual/workqueue/nf_ft_offload_*/cpumask file.
I have verified up to the part where the offload workqueue task is properly assigned with the CPU bitmask specified in this file.rubyCopy codeI tried to automate the above setting, but the /sys/devices/virtual/workqueue/nf_ft_offload_* type directory is only created when the conntrack offload task occurs, so it is difficult to automate before or after the VM is deployed.I am curious if there is another way for CPU pinning of the offload worker queue, or if there is a way to create this directory immediately after the VM is deployed.Thank you.hi kyoonAs far as I know, current kernel version do not have such API.Thank you
Meng, shiFrom our own tests, it appears that the mlx5_core Driver registers a Linux Worker Queue task when a VM is created.Considering that the Representation Port connected to the VM is activated when the VM is created, we conducted a test using the carrier event of networkd-dispatcher as follows:We confirmed that it works normally when a VM is created.Could you please let us know if there are any potential issues with this configuration or if there is a better approach?Powered by Discourse, best viewed with JavaScript enabled"
430,we-have-a-sn2100-switch-with-p-n-sn2100-bb2f-we-want-to-connect-with-switch-with-mellanox-25gbps-lan-card-25gbe-2-port-sfp28-network-adapter-mellanox-cx4,"Please let us know how do we connect SN 2100 P/N: SN2100-BB2F (40Gbps switch with QSFP28 ports) with dual port 25Gbps LAN card ? How do we stack 02 nos of SN2100 switches using DAC cables to configure it as single virtual switch for HA ?Hello Chintan,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, you can connect the SN2100 to a 25GbE adapter with a SFP28 cable suitable for your adapter → https://www.mellanox.com/related-docs/prod_cables/PB_MCP2M00-Axxx_25GbE_SFP28_DAC.pdfUnfortunately, our switches are not able to be stacked as one switch. However you can connect/configure them into a MLAG (Multi-Chassis LAG) which provides redundancy. Please review the following link regarding MLAG and how-to implement → https://community.mellanox.com/s/article/how-to-configure-mlag-on-mellanox-switchesThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
431,cifs-error-alma-linux-8-7,"After we install cifs-utils package on to the system and There is no cifs support advertised in /proc/filesystems. Doing a modprobe cifs works, but modinfo cifs shows that it’s the Mellanox dummy cifs kernel module, and trying to mount shares gives the same error as above. Looking deeper, I found that the cifs module is getting overridden by the Mellanox drivers.[root@almahpc ~]# modprobe cifs
[root@almahpc ~]# modinfo cifs
filename:       /lib/modules/4.18.0-425.3.1.el8.x86_64/extra/mlnx-ofa_kernel/fs/cifs/cifs.ko
version:        2.31
license:        Dual BSD/GPL
description:    cifs dummy kernel module
author:         Mohammad Kabat
rhelversion:    8.7
srcversion:     01E451882B55F354B7F130B
depends:        mlx_compat
name:           cifs
vermagic:       4.18.0-425.3.1.el8.x86_64 SMP mod_unload modversions
[root@ almahpc ~]# grep -c cifs /proc/filesystems
0
[root@almahpc ~]# grep cifs /proc/filesystems
[root@almahpc ~]#I removed the MLNX driver, and I see now the modinfo cifs is giving the correct version.[root@almahpc MLNX_OFED_LINUX-23.04-0.5.3.3-rhel8.7-x86_64]# ./uninstall.shThis program will uninstall all MLNX_OFED_LINUX-23.04-0.5.3.3 packages on your machine.Do you want to continue?[y/N]:yrpm -e --allmatches --nodeps  mstflint sharp mlnx-ethtool mlnx-ofa_kernel-modules iser infiniband-diags ibutils2 ucx-rdmacm ibarr mlnx-tools knem-modules rdma-core librdmacm-utils libibverbs mlnx-ofa_kernel-source opensm-static srp_daemon ucx ucx-ib hcoll rshim xpmem isert librdmacm libibverbs-utils opensm-libs opensm-devel dpcp ucx-cma ucx-xpmem mlnx-iproute2 mpitests_openmpi mlnx-ofa_kernel kernel-mft xpmem-modules libxpmem libibumad ibsim opensm ibdump ucx-knem mlnxofed-docs knem ibacm perftest ucx-devel openmpi mlnx-ofa_kernel-devel srp rdma-core-devel infiniband-diags rdma-core rshim mlnx-fw-updater mft kernel-mft ofed-scripts rdma-core-devel mlnx-ofa_kernel-23.04-OFED.23.04.0.5.3.1.rhel8u7.x86_64 sharp-3.3.0.MLNX20230417.ec919ce9-1.2304053.x86_64 hcoll-4.8.3221-1.2304053.x86_64 opensm-5.15.0.MLNX20230417.d84ecf64-0.1.2304053.x86_64 opensm-devel-5.15.0.MLNX20230417.d84ecf64-0.1.2304053.x86_64 opensm-static-5.15.0.MLNX20230417.d84ecf64-0.1.2304053.x86_64
Uninstall finished successfully[root@almahpc MLNX_OFED_LINUX-23.04-0.5.3.3-rhel8.7-x86_64]# modinfo cifs
filename:       /lib/modules/4.18.0-425.3.1.el8.x86_64/kernel/fs/cifs/cifs.ko.xz
softdep:        gcm
softdep:        ccm
softdep:        aead2
softdep:        sha512
softdep:        sha256
softdep:        cmac
softdep:        aes
softdep:        nls
softdep:        md5
softdep:        md4
softdep:        hmac
softdep:        ecb
softdep:        pre: des
version:        2.29
description:    VFS to access SMB3 servers e.g. Samba, Macs, Azure and Windows (and also older servers complying with the SNIA CIFS Specification)
license:        GPL
author:         Steve French
alias:          smb3
alias:          fs-smb3
alias:          fs-cifs
rhelversion:    8.7
srcversion:     368290631B75A5A1CA09E2C
depends:        ib_core,rdma_cm,libarc4,dns_resolver
intree:         Y
name:           cifs
vermagic:       4.18.0-425.3.1.el8.x86_64 SMP mod_unload modversions
sig_id:         PKCS#7
signer:         AlmaLinux kernel signing key
sig_key:        34:62:E5:6E:49:5A:36:CC:92:3A:DC:C9:8B:40:D9:2C:16:AB:77:F6
sig_hashalgo:   sha256
signature:      5E:4D:97:42:7B:80:B8:41:60:84:FC:AB:CE:57:76:22:B6:B3:7F:CA:
AF:73:72:E1:2B:74:08:2B:8B:0A:AF:A3:F6:02:36:3C:DC:3D:B6:46:
A9:22:17:00:26:02:C2:39:32:5A:D0:95:65:70:C3:21:9D:58:3A:AC:
EF:F8:0F:41:05:96:62:BD:AA:BA:86:E3:BB:A1:2B:FE:E5:F0:EF:EA:
5C:8E:7F:4B:B9:96:55:FD:12:00:F3:F4:43:7C:5F:06:53:4D:CE:24:
4F:B4:DA:FE:51:DD:2A:10:A5:66:5B:5A:FD:40:8D:33:CD:AE:69:F1:
9A:2A:F4:86:6D:DB:26:18:83:A4:33:68:57:55:9C:7E:A3:E4:7B:A8:
B9:1D:D3:74:78:1D:E4:FE:9F:E8:33:8A:51:C0:79:54:15:20:AC:95:
C6:54:CD:8F:71:5E:FA:01:FE:1B:08:76:76:0D:C4:AF:35:A8:96:D6:
C6:11:6C:E8:4D:62:B9:B5:D7:B3:BA:C5:8D:D7:8F:90:F8:5D:E1:62:
E0:85:22:6F:73:49:FE:74:D5:8C:48:8D:70:C2:98:4E:AB:8C:AB:FC:
76:55:2D:E6:08:23:99:A9:D4:60:07:1A:08:8F:5B:56:74:F6:1F:4C:
80:BA:48:BC:BE:9D:8C:A1:79:5E:92:34:CA:BA:E1:B3:93:12:23:33:
BB:12:9B:9E:D4:B1:AD:0C:10:04:2D:92:4C:B6:D6:1A:C6:1A:D9:0C:
21:F0:BA:2D:43:C5:F3:2E:B0:10:8D:6C:51:1A:81:A1:52:46:F9:8D:
88:48:57:63:BC:25:21:2A:06:76:F0:CE:F1:98:DF:04:6E:4E:DF:28:
8C:4B:90:6C:2F:52:CE:4A:6A:A7:12:66:4E:90:D9:29:F2:11:44:D7:
B3:49:C8:79:E3:DD:DC:47:00:04:44:6D:73:65:EC:40:95:7B:38:0A:
65:0C:88:C5:E7:B2:95:53:F1:83:91:D3:AA:1C:A5:80:09:6B:03:42:
5C:B7:5D:C6
parm:           smbd_logging_class:Logging class for SMBD transport 0x0 to 0x100 (uint)
parm:           smbd_logging_level:Logging level for SMBD transport, 0 (default): error, 1: info (uint)
parm:           CIFSMaxBufSize:Network buffer size (not including header) for CIFS requests. Default: 16384 Range: 8192 to 130048 (uint)
parm:           cifs_min_rcv:Network buffers in pool. Default: 4 Range: 1 to 64 (uint)
parm:           cifs_min_small:Small network buffers in pool. Default: 30 Range: 2 to 256 (uint)
parm:           cifs_max_pending:Simultaneous requests to server for CIFS/SMB1 dialect (N/A for SMB3) Default: 32767 Range: 2 to 32767. (uint)
parm:           enable_oplocks:Enable or disable oplocks. Default: y/Y/1 (bool)
parm:           enable_gcm_256:Enable requesting strongest (256 bit) GCM encryption. Default: n/N/0 (bool)
parm:           require_gcm_256:Require strongest (256 bit) GCM encryption. Default: n/N/0 (bool)
parm:           disable_legacy_dialects:To improve security it may be helpful to restrict the ability to override the default dialects (SMB2.1, SMB3 and SMB3.02) on mount with old dialects (CIFS/SMB1 and SMB2) since vers=1.0 (CIFS/SMB1) and vers=2.0 are weaker and less secure. Default: n/N/0 (bool)
[root@almahpc MLNX_OFED_LINUX-23.04-0.5.3.3-rhel8.7-x86_64]#This looks like a known issue with the Mellanox driver and still no solution is available.
https://docs.nvidia.com/networking/display/MLNXOFEDv23041130/Known+IssuesIf you remove the MLNX driver, you can mount CIFS as usual.
I found some information in the following link to fix it
https://mellanox.my.site.com/mellanoxcommunity/s/question/0D51T00008sHiFlSAK/mount-error-cifs-filesystem-not-supported-by-the-systemCould you please provide the procedure to recompile the CIFS module manually after installation of MLNX_OFED.?Hello and thank you for your question.When MLNX_OFED is installed, it replaces the INBOX IB driver implementation, rendering the original CIF kernel module dependent on it non-functional.Consequently, we provide a solution by substituting it with a dummy module, effectively removing CIF support.With the release of MLNX_OFED 5.5, a minor enhancement was introduced to prevent error displays during CIF loading. However, it’s important to note that CIFS functionality will still not be available.To ensure compatibility with CIF and our driver, you have two options:As CIFS is not part of the NVIDIA product portfolio, for further assistance and guidance on re-compiling the CIFS module after MLNX_OFED installation, we recommend reaching out to the CIFS vendor directly. They will be better equipped to provide the necessary support and instructions for this specific scenario.Thanks,
Ilan.Hello @ipavis ,What if we need to mount a SMB share on a machine which has MLNX driver installed?We are not using IB network for mounting SMB, its  via standard ethernet.Can you please provide the steps of recompile the CIFS module after installing MLNX_OFED ?Powered by Discourse, best viewed with JavaScript enabled"
432,number-of-svlans-and-cvlans-on-connect-x-5-connect-x-6,"Hello,I have a Mellanox Connect X-5 device.I was configuring SVLANs (QinQ) on a PF and want to inquire about the number of allowed SVLANs on a particular PF.Is it 4096? Can you please verify? Please also confirm the same about the number of CVLANs that can be configured.I am configuring SVLAN with the following command:ip link add link enp67s0f1np1 name vlan0 type vlan protocol 802.1ad id 4Regards and Thanks,
-Shahood.Hello @shahood,Thank you for posting your query on our community. The ConnectX-5 adapter card can support up to 4096 VLANs. However, information regarding the supported number of S-VLANs and C-VLANs on ConnectX-5 is not readily available and needs to be provided by our Engineering team. To obtain this information, please create a support ticket with us. The support ticket can be opened by emailing ""Networking-support@nvidia.com ""Please note that an active support contract would be required for the same. If you do not have a current support contract, please reach out to our Contracts team at networking-contracts@nvidia.comThank you,
BhargaviPowered by Discourse, best viewed with JavaScript enabled"
433,failed-to-run-applications-on-host-side-error-failed-to-start-port-failed-init-port-0,"Hi,I would like to run the Application Simple Forward VNF on host side, but get this error.I have already create two vfs through this guide NVIDIA DOCA Virtual Functions User Guide and able to run this application on Bulefield-2 DPU side. I am using DOCA 1.3.I would be really grateful if you cound give me some advice.Best Regards,
Zhaoyang.Hi Zhaoyang,This error message is usually caused by the VF or SF not being in trusted mode. Can you give that mlxreg command a try from the guide, here and see if that helps?NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.-JustinHi Justin,Thanks for reply. That is the solution.Now I am able to run applications on the host side. Sadly, when I tried to recreate sfs on the arm side to run applications, it turns out “No such device”I have tried to enable the SF function and cold reboot the device, it still didn’t help.I am wondering that can applications run on both host and arm side at the same time?Hi Zhaoyang,You should be able to run applications on both host and ARM at the same time.Can you attach the output of
devlink dev show   and
mlnx-sf -a showThanks!
-JustinHi Justin,I hava sloved my problem, thanks a lot for your help.Best Regards,
Zhaoyang.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
434,is-there-any-document-to-explain-how-to-understand-the-meaning-of-every-devx-message-field,"hi, teams
May I ask is there any document to explain how to understand the meaning of every devx message field. because we develop some feature by using devx command, but do not know the meaning of every devx message field. is there any document to explain this?  eg. [Mellanox Adapters Programmer’s Reference Manual (PRM)] it that free to download now?HI jpsun3000For specific PRM document to download, please open a CASE and provide 'a signed NDA ’ .
Then CASE owner can give it.
With regards to devx, you should contact NVIDIA regional Solution Architect engineer or sales Rep.
Then you should justify business opportunity to get an actual help from NVIDIA on it./HyungKwangPowered by Discourse, best viewed with JavaScript enabled"
435,opensm-failure-after-reboot-stuck-on-port-initialization,"Specs:Linux Kernel: 4.15.0-140-genericOS: Ubuntu 18.04MLNX_OFED_LINUX-4.9-2.2.4.0-ubuntu18.04-x86_64Everything was working until reboot. I tried several things with results provided below. Also the port GIUDs changed and I had to manually update /etc/opensm/opensm.conf. (NOTE: opensm.conf is default template, I only modified by specifying port IDs). (looking at the OFED manual now for further diagnostics).— Report —sudo modprobe ib_umad (worked?)sudo modprobe xprtrdmamodprobe: ERROR: could not insert ‘rpcrdma’: Unknown symbol in module, or unknown parameter (see dmesg)dmesg | tailrpcrdma: Unknown symbol ib_alloc_cq (err 0)rpcrdma: Unknown symbol ib_dereg_mr (err 0)rpcrdma: Unknown symbol rdma_create_id (err 0)rpcrdma: Unknown symbol ib_alloc_mr (err 0)rpcrdma: Unknown symbol ib_free_cq (err 0)rpcrdma: Unknown symbol rdma_accept (err 0)rpcrdma: Unknown symbol ib_destroy_qp (err 0)rpcrdma: Unknown symbol ib_dealloc_pd (err 0)sminfoibwarn: [18194] mad_rpc_open_port: can’t open UMAD port ((null):0)sminfo: iberror: failed: Failed to open ‘(null)’ port ‘0’NOTE: no rdma service installedsudo osmtest -f c (same output for -f a, except ‘validation’ instead of ‘inventory’)Command Line ArgumentsDone with argsFlow = Create InventoryApr 07 10:31:23 592167 [2110F740] 0x7f → Setting log level to: 0x03Apr 07 10:31:23 592367 [2110F740] 0x02 → osm_vendor_init: 1000 pending umads specifiedApr 07 10:31:23 661108 [2110F740] 0x02 → osm_vendor_bind: Mgmt class 0x03 binding to port GUID 0x2c903003fc582Apr 07 10:31:23 745819 [1F6B1700] 0x01 → __osmv_sa_mad_rcv_cb: ERR 5501: Remote error:0x000CApr 07 10:31:23 745869 [1F6B1700] 0x01 → osmtest_query_res_cb: ERR 0003: Error on query (IB_REMOTE_ERROR)Apr 07 10:31:23 745955 [2110F740] 0x01 → osmtest_validate_sa_class_port_info: ERR 0070: ib_query failed (IB_REMOTE_ERROR)Apr 07 10:31:23 745993 [2110F740] 0x01 → osmtest_validate_sa_class_port_info: Remote error = IB_MAD_STATUS_UNSUP_METHOD_ATTRApr 07 10:31:23 746013 [2110F740] 0x01 → osmtest_run: ERR 0138: Could not obtain SA ClassPortInfo (IB_REMOTE_ERROR)OSMTEST: TEST “Create Inventory” FAILsudo systemctl restart opensm, output of /var/log/opensm.logApr 07 10:24:02 117367 [8BF43740] 0x80 → Exiting SMApr 07 10:26:07 072815 [E3C3D740] 0x03 → OpenSM 5.7.2.MLNX20201014.9378048OpenSM 5.7.2.MLNX20201014.9378048Apr 07 10:26:07 072926 [E3C3D740] 0x80 → OpenSM 5.7.2.MLNX20201014.9378048Apr 07 10:26:07 077131 [E3C3D740] 0x02 → osm_vendor_init: 1000 pending umads specifiedApr 07 10:26:07 077241 [E3C3D740] 0x02 → osm_vendor_init: 1000 pending umads specifiedApr 07 10:26:07 077354 [E3C3D740] 0x02 → osm_vendor_init: 1000 pending umads specifiedEntering DISCOVERING stateApr 07 10:26:07 080343 [E3C3D740] 0x80 → Entering DISCOVERING stateApr 07 10:26:07 080556 [E3C3D740] 0x02 → osm_vendor_bind: Mgmt class 0x81 binding to port GUID 0x2c903003fc581Apr 07 10:26:07 171455 [E3C3D740] 0x02 → osm_vendor_bind: Mgmt class 0x81 binding to port GUID 0x2c903003fc582Apr 07 10:26:07 257881 [E3C3D740] 0x02 → osm_vendor_bind: Mgmt class 0x03 binding to port GUID 0x2c903003fc581Apr 07 10:26:07 344319 [E3C3D740] 0x02 → osm_vendor_bind: Mgmt class 0x04 binding to port GUID 0x2c903003fc581Apr 07 10:26:07 344394 [E3C3D740] 0x02 → osm_vendor_bind: Mgmt class 0x21 binding to port GUID 0x2c903003fc581Apr 07 10:26:07 344453 [E3C3D740] 0x02 → osm_opensm_bind: Setting IS_SM on port 0x0002c903003fc581SM port is downsudo hca_self_test.ofed---- Performing Adapter Device Self Test ----Number of CAs Detected … 1PCI Device Check … PASSKernel Arch … x86_64Host Driver Version … MLNX_OFED_LINUX-4.9-2.2.4.0 (OFED-4.9-2.2.4): 4.15.0-140-genericHost Driver RPM Check … PASSFirmware on CA #0 VPI … v2.42.5000Host Driver Initialization … PASSNumber of CA Ports Active … 0Error Counter Check on CA #0 (VPI)… PASSKernel Syslog Check … PASSNode GUID on CA #0 (VPI) … NA------------------ DONE ---------------------Hello Willy,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, your kernel was updated, after installing MLNX_OFED 4.9 GA.The default kernel version of Ubuntu 18.04 is 4.15.0.20.23. Your kernel version is -140. When the kernel is updated after the driver is installed, you need to reinstall the driver to have it rebuild against the new kernel.We did an install in our lab with kernel -140, and we were able to successfully and run the driver.Our recommendation is to reinstall the driver, which will resolve this issue.Thank you and regards,~NVIDIA Networking Technical Supportunfortunately still stuck.sudo sminfoibwarn: [20373] _do_madrpc: recv failed: Connection timed outibwarn: [20373] mad_rpc: _do_madrpc failed; dport (DR path slid 0; dlid 0; 0)sminfo: iberror: failed: querysudo osmtestDone with argsFlow = All ValidationsApr 08 11:12:02 903852 [3A15E740] 0x7f → Setting log level to: 0x03Apr 08 11:12:02 903970 [3A15E740] 0x02 → osm_vendor_init: 1000 pending umads specifiedApr 08 11:12:02 969024 [3A15E740] 0x02 → osm_vendor_bind: Mgmt class 0x03 binding to port GUID 0x2c903003fc582Apr 08 11:12:03 048683 [38700700] 0x01 → __osmv_sa_mad_rcv_cb: ERR 5501: Remote error:0x000CApr 08 11:12:03 048732 [38700700] 0x01 → osmtest_query_res_cb: ERR 0003: Error on query (IB_REMOTE_ERROR)Apr 08 11:12:03 048828 [3A15E740] 0x01 → osmtest_validate_sa_class_port_info: ERR 0070: ib_query failed (IB_REMOTE_ERROR)Apr 08 11:12:03 048879 [3A15E740] 0x01 → osmtest_validate_sa_class_port_info: Remote error = IB_MAD_STATUS_UNSUP_METHOD_ATTRApr 08 11:12:03 048898 [3A15E740] 0x01 → osmtest_run: ERR 0138: Could not obtain SA ClassPortInfo (IB_REMOTE_ERROR)OSMTEST: TEST “All Validations” FAILsudo ibquery errorsibwarn: [31162] sa_get_handle: No SM/SA found on port (null):0UPDATE:Working. It seems to be a finicky system. I rebooted, did some service restarts and plugged the cable into the alternative port on one card. For some reason the physical port was not responding on reboot.That was the second time the physical connection did not initiate without re-inserting the transceiver.Let’s see how it goes over the next while.Thank for your help, should I close these tickets in some way?Powered by Discourse, best viewed with JavaScript enabled"
436,does-connectx-6-dx-card-support-tls-offloading-with-aes256-and-tls-1-3,"Hello, I have a server with MCX623106AC-CDAT and I am trying to use the TLS hardware encryption support of the card. I’ve tested several ciphers with nginx ktls enabled web server and it successfully offloads the TLS encryption to the card when using the ciphers based on AES128 GCM SHA256 such as ECDHE-RSA-AES128-GCM-SHA256, AES128-GCM-SHA256, AES128-SHA256 and more, which are TLS v1.2. But trying to use cipher TLS_AES_128_GCM_SHA256, which is TLS 1.3 the packets are not encrypted in the card. This card MCX623106AC-CDAT is a Crypto card, which should support “Inline hardware TLS encryption and decryption > AES-GCM 128/256-bit key.” (taken from the connectX-6-dx-datasheet.pdf). First, how may I check which exactly ciphers the card supports? And does my card support TLS offloading using cipher TLS_AES_128_GCM_SHA256 (i.e. AES128 in Galois/Counter mode GCM with 256 SHA hash with TLS1.3)? And lastly, it is written “AES-GCM 128/256-bit key.” in the documentation, which means AES128 in GCM mode with 256 SHA key or AES256 in GCM mode with 384 SHA key - it is written really ambiguous.To check which ciphers the MCX623106AC-CDAT card supports, you can consult the product documentation or contact the manufacturer’s support team for more information.Regarding your question about whether the card supports TLS offloading using cipher TLS_AES_128_GCM_SHA256, it’s difficult to say for certain without knowing more about the specific implementation of the card’s hardware encryption engine. However, in general, support for TLS 1.3 ciphers like TLS_AES_128_GCM_SHA256 requires hardware engines with specific cryptographic primitives and key sizes.As for the ambiguity in the documentation, it’s possible that the statement “AES-GCM 128/256-bit key” is referring to support for both AES128 and AES256 in GCM mode, with SHA256 as the hash function. Without more context, it’s difficult to say for certain. Again, the best approach would be to consult the product documentation or contact the manufacturer’s support team for clarification.Powered by Discourse, best viewed with JavaScript enabled"
437,softroce-on-1-or-2-linux-ubuntu-workstations-without-mellanox-nic,"I have been doing RDMA on 2 Windows workstation with 2 Mellanox ConnectX-6 NIC. The NIC card were temporarily on a short term loan to my colleagues due to business priority. While I am waiting. I would like to know if:Hello @user58403,Thank you for posting your query on our portal. To answer your questions regarding SoftRoCE,Yes, you can continue your work using SoftRoCE without any physical RDMA-capable NICs. Soft-RoCE is a software implementation of RoCE that allows RoCE to run on any Ethernet network adapter whether it offers hardware acceleration or not. Soft-RoCE is released as part of upstream kernel 4.8 (or above). Mellanox OFED 4.0 or upstream driver could be used. If you install MLNX_OFED 4.0, you automatically get the Soft-RoCE kernel module and user space libraries.SoftRoCE is a feature that was primarily developed for Linux distributions. It is currently not supported on Windows.Hope this anwers your questions.Regards,
BhargaviThank you very much for the prompt, concise and very clear reply. Really appreciated.Powered by Discourse, best viewed with JavaScript enabled"
438,rdma-latency-stability-and-pcie-version-recognize,"We are testing the performance of RDMA using ConnectX-5 25G on windows 10 Enterprise.
we test the NetworkDirectSPI write operation latency with data length of 1024Byte.
ref: GitHub - microsoft/NetworkDirect: NetworkDirect Service Provider Interface
And both server and client are on the same PC.
the average latency is around 50 microseconds, but sometimes the latency get more than 100ms.
My question is:Why these high latency data happened, is there any settings of configs to avoid this.Durning the test we found that the PCIE version changes when running “mlx5cmd -stat” after the PC restarted. the PCIE hardware version is 2, but sometimes the command shows PCIE gen1, and the speed also affected. why the this happens and how to avoid?ref test data and PCIE version:

image1260×689 67.6 KB
Hello Ramadevi,Welcome, and thank you for posting your inquiry to the NVIDIA community!Given the fact that the PCIe link speed varies across reboots, the integrity of the PCIe link is called into question. When the PCIe link is unable to train at the optimal link speed/width, 3 scenarios are most likely:a) The adapter is not seated properly.
b) There’s a hardware issue with the adapter.
c) There’s a hardware issue with the slot (mainboard).As the integrity of the PCIe link itself is unknown, this needs to be rectified before performance tuning / troubleshooting can be performed.If reseating the adapter does not resolve the sporadic link speed degradation, a swap to another slot is recommended.If the same behavior is encountered in another slot, swap with a known good adapter is recommended.If the same behavior is encountered on this system with a known good adapter and/or in a different slot, then we recommend engaging your hardware vendor to assess next steps with regards to the mainboard hardware.Once the PCIe link is validated, we have several tuning recommendations in the ‘Troubleshooting’ section of the WinOF-2 User Manual >> https://docs.nvidia.com/networking/display/winof2v320/Troubleshooting . Relevant sections here would be ‘Ethernet Related Troubleshooting’ and ‘Performance Related Troubleshooting’.If you are unable to achieve stable performance after these steps have been followed, and you have valid support entitlement, we recommend opening a support ticket with our Enterprise Support team via the NVIDIA Enterprise Experience Support Portal: https://enterprise-support.nvidia.com/s/create-case . Our engineers will be able to assist you with determining the root cause of this degradation.Thanks, and best regards,
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
439,unable-to-create-send-cq-of-size-5080-on-mlx4-0-cannot-allocate-memory,"I recently installed Intel’s oneapi and want to run some benchmarks on an HP enclosure with a Mellanox SX1018HP Enet Switch .When I run this command I get the errors below.[hussaif1@lustwzb31 pt2pt]$ mpirun -v -np 1 -ppn 2 -genv I_MPI_DEBUG=5 -genv I_MPI_HYDRA_DEBUG=1 -hosts b31,b32 ./osu_latencylustwzb31:rank0.osu_latency: Unable to create send CQ of size 5080 on mlx4_0: Cannot allocate memorylustwzb31:rank0.osu_latency: Unable to initialize verbslustwzb31:rank0: PSM3 can’t open nic unit: 0 (err=23)Abort(1615503) on node 0 (rank 0 in comm 0): Fatal error in PMPI_Init: Other MPI error, error stack:MPIR_Init_thread(138)…:MPID_Init(1183)…:MPIDI_OFI_mpi_init_hook(1916):create_endpoint(2560)…: OFI endpoint open failed (ofi_init.c:2560:create_endpoint:Invalid argument)Hello Faraz,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, unfortunately we do not support Intel MPI. NVIDIA delivers HPC-X for which all components are optimized with our products.It is best to move this discussion to the Intel MPI One API community → Intel® oneAPI HPC Toolkit - Intel CommunitiesThank you and regards,~NVIDIA Networking Technical SupportHi Faraz,I recently upgraded my oneapi to 2021.04 and got the same error. It turned out I was using incorrect libfabric provider. Once I set it right, the code ran fine. Hope this helps.Powered by Discourse, best viewed with JavaScript enabled"
440,ets-features-are-not-supported-on-your-system,"I am able to compile successfully OFED driver on ubuntu 21.04 having CX5 card ,but when i ran mlnx_qos -i  I got error “ETS features are not supported on your system”Hi Ankit,Thanks for your question.We recommend to check the following:The latest firmware is installed.The OFED driver is up and running and all the relevant kernel modules are loaded.Best Regards,AnatolyPowered by Discourse, best viewed with JavaScript enabled"
441,ips-app-not-receiving-response-data-from-host,"Hello all,I have been working closely with the Bluefield-2 DPU recently with the goal to familiarize myself with the hardware as well as DOCA. While following the documentation for the IPS application I have found that I can send traffic through the physical port on the DPU as well as through the scalable functions I have defined to run the application but once my request reached the host I do not receive that data on the client side.For testing I have set up an Apache2 webserver on the host with the first goal being to be able to resolve that webpage from a browser and then create a Suricata rule for the DPI compiler to drop all packets going to that address at port 80. I still can’t determine how I would get the response back from the host as it is not clear to me how the second scalable function would route the traffic to the first ovs bridge which is where the first scalable function is located. I have set up my OVS to be exactly like the reference diagram in the documentation so I am unsure where I am messing up here. Any help would be greatly appreciated.Thanks,AustinHey Austin,I think the shortest answer is that all traffic goes through the IPS App. Even traffic that you don’t intend on blocking or applying a policy to.When you configure the SFs and OVS bridges in this way, traffic will not flow to the wire until the IPS App is running. Part of that DOCA application includes sending packets from SF0 <-> SF1. The other part of the DOCA application is the DPI library logic that will filter traffic based on the suricata style definition and regex.
ips-diagram626×1104 20.6 KB
From a networking perspective, the IPS app is a network bridge. It classic networking terms it would be like an inline transparent IPS appliance that bridges two VLANs together. You can only get traffic from SF0 to SF1 when the IPS application is running and processing packets.Thanks!
-JustinPowered by Discourse, best viewed with JavaScript enabled"
442,connectx-6-dx-crypto-and-secure-boot-xdp-hardware-offload-not-possible-because-of-ipsec,"Hey,I am trying to offload a simple XDP Program to hardware on my ConnectX-6 Dx Crypto and Secure boot device.e.g. the xdp1 kernel sample (linux/samples/bpf at master · torvalds/linux · GitHub):idp@shs01-muc5-fw:~/xdp/linux/samples/bpf$ sudo ./xdp1 -N enp99s0f1libbpf: elf: skipping unrecognized data section(16) .eh_framelibbpf: elf: skipping relo section(17) .rel.eh_frame for section(16) .eh_framelink set xdp fd failedOr directly with ip:idp@shs01-muc5-fw:~/xdp/t1$ sudo /opt/mellanox/iproute2/sbin/ip link set dev enp99s0f1 xdp obj simple.c.elf section simple verboseProg section ‘simple’ loaded (5)!Type: 6Instructions: 2 (0 over limit)License: WTFPLVerifier analysis:0: (b7) r0 = 21: (95) exitprocessed 2 insns (limit 1000000) max_states_per_insn 0 total_states 0 peak_states 0 mark_read 0RTNETLINK answers: Invalid argumentAccording to dmesg this is related to IPSec offloading:[ 3584.043640] mlx5_core 0000:63:00.1 enp99s0f1: can’t set XDP with IPSec offloadHow do I get XDP offloading to work, with or without IPSec?ethtool -k:idp@shs01-muc5-fw:~/xdp/linux/samples/bpf$ /opt/mellanox/ethtool/sbin/ethtool -k enp99s0f1Features for enp99s0f1:rx-checksumming: ontx-checksumming: ontx-checksum-ipv4: off [fixed]tx-checksum-ip-generic: ontx-checksum-ipv6: off [fixed]tx-checksum-fcoe-crc: off [fixed]tx-checksum-sctp: off [fixed]scatter-gather: ontx-scatter-gather: ontx-scatter-gather-fraglist: off [fixed]tcp-segmentation-offload: ontx-tcp-segmentation: ontx-tcp-ecn-segmentation: off [fixed]tx-tcp-mangleid-segmentation: offtx-tcp6-segmentation: ongeneric-segmentation-offload: ongeneric-receive-offload: onlarge-receive-offload: offrx-vlan-offload: ontx-vlan-offload: onntuple-filters: offreceive-hashing: onhighdma: on [fixed]rx-vlan-filter: onvlan-challenged: off [fixed]tx-lockless: off [fixed]netns-local: off [fixed]tx-gso-robust: off [fixed]tx-fcoe-segmentation: off [fixed]tx-gre-segmentation: ontx-gre-csum-segmentation: ontx-ipxip4-segmentation: ontx-ipxip6-segmentation: ontx-udp_tnl-segmentation: ontx-udp_tnl-csum-segmentation: ontx-gso-partial: ontx-sctp-segmentation: off [fixed]tx-esp-segmentation: ontx-udp-segmentation: onfcoe-mtu: off [fixed]tx-nocache-copy: offloopback: off [fixed]rx-fcs: offrx-all: ontx-vlan-stag-hw-insert: onrx-vlan-stag-hw-parse: off [fixed]rx-vlan-stag-filter: on [fixed]l2-fwd-offload: off [fixed]hw-tc-offload: offesp-hw-offload: on [fixed]esp-tx-csum-hw-offload: on [fixed]rx-udp_tunnel-port-offload: ontls-hw-tx-offload: ontls-hw-rx-offload: off [fixed]rx-gro-hw: off [fixed]tls-hw-record: off [fixed]Simple XDP:#include <linux/bpf.h>#define __section(NAME) attribute((section(NAME), used))__section(“license”)char __license = “WTFPL”;__section(“simple”) int xdp_simple(struct xdp_md *ctx) {return XDP_PASS;}Hi Herberth,XDP should work with IPsec. There is a bug fix that was posted recently to make XDP available also with IPsec, no need to disable IPsec.It will be available in kernel v5.12.Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
443,is-there-a-trial-version-of-netq-available,"Yes! Just create a simulation with NVIDIA Air. All of the available simulations (NVIDIA Cumulus in the Cloud, Cumulus Linux test drive, NVIDIA SONiC virtual test drive/demo/spines) incorporate NetQ.Powered by Discourse, best viewed with JavaScript enabled"
444,mcx4121a-acat-pnic-disappearing-affecting-multiple-systems-running-esxi-7-0u1,"We currently have an issue in our VMware environment which is affecting multiple (10+) systems with “MCX4121A-ACAT” network adapters.Our issue is a driver/firmware related issue with the Mellanox adapters that results in one of the physical nics disappearing randomly at boot time.The hosts are running the following driver and firmware combinations which are certified according to VMwares HCL:driver: 4.19.71.101 (some hosts are still running 4.19.71.1 but we see the issue regardless of driver version)firmware: 14.29.1016When we reboot a ESXi host with this configuration, the host often comes up with only 3 vmnics. The other vmnic is completely missing. We often need to reboot the host a few times for the missing vmnic to reappear.esxcli network nic list (vmnic5 missing):vmnic2 0000:3b:00.0 nmlx5_core Up Up 10000 Full 0c:42:a1:4a:2a:04 9000 Mellanox Technologies ConnectX-4 Lx EN NIC; 25GbE; dual-port SFP28; (MCX4121A-ACA)vmnic3 0000:3b:00.1 nmlx5_core Up Up 10000 Full 0c:42:a1:4a:2a:05 1600 Mellanox Technologies ConnectX-4 Lx EN NIC; 25GbE; dual-port SFP28; (MCX4121A-ACA)vmnic4 0000:af:00.0 nmlx5_core Up Up 10000 Full 0c:42:a1:4a:29:dc 9000 Mellanox Technologies ConnectX-4 Lx EN NIC; 25GbE; dual-port SFP28; (MCX4121A-ACA)lspci (vmnic5 displayed when running lspci):0000:3b:00.0 Network controller Ethernet controller: Mellanox Technologies ConnectX-4 Lx EN NIC; 25GbE; dual-port SFP28; (MCX4121A-ACA) [vmnic2]0000:3b:00.1 Network controller Ethernet controller: Mellanox Technologies ConnectX-4 Lx EN NIC; 25GbE; dual-port SFP28; (MCX4121A-ACA) [vmnic3]0000:af:00.0 Network controller Ethernet controller: Mellanox Technologies ConnectX-4 Lx EN NIC; 25GbE; dual-port SFP28; (MCX4121A-ACA) [vmnic4]0000:af:00.1 Network controller Ethernet controller: Mellanox Technologies ConnectX-4 Lx EN NIC; 25GbE; dual-port SFP28; (MCX4121A-ACA) [vmnic5]mellanox tool (unable to open device for missing vmnic):mlxfwmanager -d af:00.1 --query:Status: Failed to open device/opt/mellanox/bin/mst status -vvPCI devices:DEVICE_TYPE MST PCI RDMA NET NUMAConnectX4LX(rev:0) mt4117_pciconf2 3b:00.0ConnectX4LX(rev:0) mt4117_pciconf1.1 3b:00.1ConnectX4LX(rev:0) mt4117_pciconf3 af:00.0We’ve contacted VMware support and they’ve said we need to contact the vendor as this is a firmware/driver issue (VMware Knowledge Base)Troubleshooting already been carried out by us:updated drivers and firmwares using different combinations → did not helpreplaced mellanox nics with intel nics → fixed issuebooted a different OS distribution (ubuntu) → fixed issueDoes anyone here have any ideas how we can troubleshoot this issue? I feel we have exhausted all avenues available to us and now we need some assistance to identify the root cause of our issue.Hello James,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we recommend to open a NVIDIA Networking Support ticket (Valid support contract required) so our engineers can assist you with this issue, as it can require some extensive debugging. In most of the cases, it is a combination of which platform is being used, f/w and ESXi build.You can open a support ticket by sending an email to the following email address → networking-support@nvidia.comThank you and regards,~NVIDIA Networking Technical SupportHi,I’ve tried opening a support case (00946733) via https://support.mellanox.com/, but I received an email saying additional information is required, and nobody will be assigned to the ticket until this info is provided.The problem is, authorization is required to access the link in the email, and there is no way of logging in. So it is impossible for me to provide this information and my case is stuck in limbo.I’ve tried sending an email to support@mellanox.com a couple of weeks ago to no avail.I even tried chatting with support and got sent a password reset link, but this doesn’t exactly help if there is no way of entering that password in order to login.I will gladly work with you through the official support channel, but I need someone from NVIDIA Support to take ownership of my case (at the least help me to provide the additional info that is required).Getting back to the issue at hand, we’ve observed that if we disable the nmlx5_rdma driver, all our vmnics remain present. If we enable the nmlx5_rdma driver (default), then we see the random disappearance of vmnics at boot time.Regards,James.Powered by Discourse, best viewed with JavaScript enabled"
445,how-to-remove-a-memory-range-added-with-doca-mmap-populate,"When I call doca_mmap_populate() with a new <vaddr,size> I am assuming it stores that memory range in an internal structure.  How do I tell DOCA to remove that range from its mapping after I am done using it?
Currently the only way I see is via the callback function doca_mmap_memrange_free_cb_t() which will be called as part of doca_mmap_destroy().  If I do not want to wait until destroy to cleanup the mapping, which API should I use?Powered by Discourse, best viewed with JavaScript enabled"
446,driver-not-loading-on-host,"Hi, my setup was running just fine last week, then I came this morning to find that the drivers are not loading on the host side.
my other Realtek ethernet was named enp20s*, the name automatically changed to enp3s*.
I have done no configuration changes in between as this happened during the weekend, most probably due to some update.I tried uninstalling and reinstalling everything from scratch but that didn’t solve my problems.
any help or inspiration would be much appreciated.
thanksresetting the BIOS settings seems to have solved the problem.Seems you enable scure boot on BIOS. That will prevent unsign kernel module load.fyi,https://docs.nvidia.com/networking/display/OFEDv501000/UEFI+Secure+BootThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
447,ofed-5-8-on-rhel-rocky-8-8,"Hello, I am trying to install OFED (looking at latest 5.8-2.0.3.0 on Rocky 8.8 but even with --skip-distro-check I get an error that the operating system is not supported.  Is there a path-more-travelled way to install this?The OFED just can’t support Rocky 8.8 directly.
we can try to use ‘–add-kernel-support’ , ex:
#./mlnxofedinstall --force --skip-distro-check --distro rhel8.8 --add-kernel-support --skip-repoafter installation , execute:
#dracut -fmore infomation refer to:
https://docs.nvidia.com/networking/display/MLNXOFEDv582030LTS/Installing+MLNX_OFEDLeveiI tried this command on Rocky 8.8, but still got some errors when building the RPM packages.I ran ./mlnxofedinstall --force --skip-distro-check --distro rhel8.8 --add-kernel-support --skip-repo and got:Maybe the driver does not support the kernel version of Rocky 8.8 (4.18.0-477.10.1.el8_8.x86_64)?OFED 4.9 maybe too old to support Rockey OS. Could you try OFED 5.8Hi, so I triedbut it gave meBut this package is already installed…Did I miss anything?Powered by Discourse, best viewed with JavaScript enabled"
448,where-should-i-get-the-documentation-about-the-new-layer-api-on-connectx-6-dx,"The datasheet of ConnectX-6 Dx shows that ConnectX-6 Dx adds a new layer of innovation. I wonder how to use this new layer or how to program on this new layer? The datasheet of ConnectX-6 Dx also illustrates that there are APIs to build user-defined congestion control algorithms. Is there any documentation on these APIs? Where should I get them?
Thank you in advance.Hi,
ConnectX-6 Dx added a Programmable Congestion Control HW.
We do have an SDK for developing advanced congestion control algorithms using this HW.
The SDK is available to select customers.
Sharing this capability is related to the business opportunity we have with those selected customers.
Regards,
YanivHello Yaniv，
So I need to get it from the customer service or dealer who sells ConnectX-6 Dx to me, right? Is it a book or an instruction manual? If so, I’m sorry I may have lost it. Or other forms of documentation?
Thanks for your answer.You will have to open a ticket with customer service and we will see how we can help you.Powered by Discourse, best viewed with JavaScript enabled"
449,help-identifying-card,"Hello! I recently acquired some new-in-box OEM Dell R640 servers that came with Mellanox ConnectX-5 cards installed. I’m hoping someone may be able to identify these and tell me if they are OEM or retail, and what firmware they should use. According the the service tag, I’m not seeing these as installed by Dell, but then again as an OEM server I suppose it could have been custom built. Card info below.Model No: NV303212A
ConnectX-5 25GbE w/FPGA
P/N MNV303212A-ADLT
S/N MT1935X08342There is also another label with the following: DP/N 0NMD3RWindows Device Info: PCI\VEN_15B3&DEV_1017&SUBSYS_004615B3&REV_00On 2 of them, I attempted to install the latest drivers and update firmware for Windows 2022 following the instructions from the Nvidia support section: Windows Driver Installation - ConnectX-5 Ethernet - NVIDIA Networking Docs (Windows Server 2022).However, after performing the update and rebooting the server, the card is disabled with the following message: A PCIe link training failure is observed in Slot1 and device link is disabled mellanox R640I have follow Dell’s instructions to try and clear the error, but I’m afraid the firmware update may be the culprit. If I swap with the non-updated ones, I can still boot per normal, and the device info shows the following firmware version: 16.25.1020Is there a way I can reflash the updated ones?Also, my only real use for a 25GbE card would be for direct connection between servers for synchronization and iSCSI channels (Starwind VSAN). If these cards aren’t intended or suited for that, please let me know if possible.Thanks in advance for any info and help!Hello,What drivers and guidelines have you followed?Did you install our latest WinOF2 driver version 2.80?If you flashed the FW, I am assuming our MFT package has been installed.(Mellanox Firmware Tools (MFT))What is the PSID of the HCA card in order to identify if OEM or not?What FW did you upgraded from 16.25.1020?Did you upgrade to a FW that is supported according to our WinOF2 release note for driver version 2.80?mst status -vflint -d  qAfter upgrading and regardless the PCIe event generated, is the HCA card still visible under mst status -v?If yes, you can downgrade the FW the same way with our flint utility pointing to a different binary file.(> flint -d  -I fw.bin b followed by a reboot or mlxfwreset)What is the protocol on the HCA card?Where does it plug into?Is the card plugged into a PCI4.0/3.0 slot?Is it reproducible on a different server running Windows 2022?Is it reproducible on a supported Linux server? Same FW upgrade?Are you using supported cable(s) based on the FW release note?Sophie.Hello! Thank you for your reply. I’ll try and answer what I can.The cards are installed in PCIe 3.0 slots on a Dell R440 server running Windows Server 2022.
They had the following firmware installed to begin with: 16.25.1020
I followed this guide: Windows Driver Installation - ConnectX-5 Ethernet - NVIDIA Networking Docs
I selected the option to ‘Upgrade the HCA’s firmware version (Recommended)’ as described in the guide above.
I used WinOF-2 v2.80 for Windows Server 2022: https://www.mellanox.com/downloads/WinOF/MLNX_WinOF2-2_80_50000_All_x64.exe
After reboot, I get a message from server saying: UEFI0067: A PCIe link training failure is observed in Slot1 and device link is disabled.
Because the device link is disabled, I am unable to see it once Windows boots.I have an identical card that was not updated that still is visible in Windows.
PCI\VEN_15B3&DEV_1017&SUBSYS_004615B3&REV_00\6&15B57994&0&00800000These cards aren’t in use yet; but I’m trying to determine vendor/support in case I’m able to.
I have installed the MFT exe for Windows but have not used it yet.
Please let me know if I can provide anything else useful. Thank you!According to the documentation, I do not see Windows showing as a supported OS for this card.System Requirements
Operating
Systems/
Distributions
RHEL
Ubuntu
SLES
For version information, please refer to the Mellanox Innova-2 Flex Open Bundle
Release Notes.
Please refer to Vivado release notes for supported Operating Systems.https://docs.nvidia.com/networking/display/Innova2Flex/Recommended+Documents+and+ToolsThank you. So this is not a standard ConnectX-5 card then?

IMG_04731920×1440 209 KB
Not it is not😊Thank you; I think this solved my question of whether or not I have use for them. I also found this: https://docs.nvidia.com/networking/display/Innova2Flex.As for the 2 cards I updated - are those salvageable? I’ll likely want to sell these if not being used.Powered by Discourse, best viewed with JavaScript enabled"
450,find-the-mib-file-of-sonic-sn3700c,"Hi,
Where can i find the snmp mib file for sonic SN3700C?Best regards,Hi Iman,James here with NVIDIA GTS; NVIDIA does not provide software support for SONiC. I would suggest directing your questions to your internal team and/or the SONiC GitHub community.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
451,virtualized-development-environment-for-users-without-a-dpu,"Good Afternoon. We are working toward ordering our first DPU and in the meantime would like to learn/work with DOCA so we can be prepared once it arrives. Is there a recommended course of action for developers who want to use DOCA and do not have a DPU? I can’t seem to find any documentation for this particular situation. I would assume that users would not have access to the hardware accelerated functionality of the DPU but would be able to compile and run the reference applications but I could be wrong. Any assistance would be greatly appreciated.Powered by Discourse, best viewed with JavaScript enabled"
452,support-for-kernel-5-10,"Hi, as a lot of changes on the infiniband / rdma stuff were happening in 5.10 kernel the OFED 5.4-1.0.3.0 is incompatible with it, after installation I’m getting lots of errors in the foramt:Jul 6 13:34:36 at-host-46 kernel: ib_uverbs: Unknown symbol ib_unregister_client (err -22)Jul 6 13:34:36 at-host-46 kernel: ib_uverbs: disagrees about version of symbol ib_query_srqJul 6 13:34:36 at-host-46 kernel: ib_uverbs: Unknown symbol ib_query_srq (err -22)Jul 6 13:34:36 at-host-46 kernel: ib_uverbs: Unknown symbol ib_destroy_rwq_ind_table (err -2)Is there an eta for a newer version with support for kernel 5.10+ ?Where can I find the maximum supported kernel version for a specific OFED release?Hello Thomas,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, the errors you are facing appear when the kernel is update and the MLNX_OFED was already installed based on the previous kernel.To resolve this issue, we recommend to uninstall the current MLNX_OFED installation through the syntax → # ofed_uninstall.shand then reinstall MLNX_OFED with the following syntax → # ./mlnxofedinstall --add-kernel-support -vvv -yBe aware that MLNX_OFED 5.4 GA does not support kernels > 5.10+. Currently there is no information available when MLNX_OFED will support kernels > 5.10+Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
453,where-are-instructions-on-how-to-use-kubernetes-with-sr-iov,"Related files:https://www.mellanox.com/related-docs/prod_software/Mellanox_OFED_Linux_User_Manual_v4_4.pdfI read this user manual v4.4 in order to find more details on using Kubernetes with SR-IOV. On page 262, this user manual gives two instructions, which are Infrastructure & Networking - NVIDIA Developer Forums and Infrastructure & Networking - NVIDIA Developer Forums. However, these are all invalid page after clicking.Please ask where are new instructions on how to use Kubernetes with SR-IOV? Thank you very much!Hi mu du,Please review the following article: https://community.mellanox.com/s/article/kubernetes-ipoib-sriov-networking-with-connectx4-connectx5Regards,ChenThanks. But when I open this, it shows “invalid page” as well.
微信图片_202012031806181435×305 114 KB
Powered by Discourse, best viewed with JavaScript enabled"
454,no-rx-with-vlan-filter-offload,"Hello,DPDK: 19.11Driver: mlx5OS: CentOS Linux release 7.9.2009 (Kernel: 3.10.0-1160.66.1.el7.x86_64)Iface: Mellanox Technologies MT27710 Family [ConnectX-4 Lx Virtual Function]I am bringing-up our dpdk based L3 forwarding app in VM on Openshift redhat platform. It has both RX and TX HW offloads enabled, including VLAN filtering (DEV_RX_OFFLOAD_VLAN_FILTER). VLAN registers successfully in the VF - however no packets are getting received in with the registered VLAN. RX-packet counter doesn’t increment and there are no errors reported in stats, or even in logs with driver debug logging enabled via log-level=“mlx5,8”.When no VLAN is registered or vlan filtering offload is disabled, then IP unicast, broadcast and multicast packets are received, but no VLAN packet, which means VLAN filtering offload doesn’t work. Result from dpdk 17.11 testing is also same. Does anyone noticed or reported such problem? Any help or pointer will be appreciated.Thanks,
Rohithi ,
Could you describe how you test(commands, procedures).
Could you test with testpmd , then share the detail command?
It will help to understand the issue better.Regards,
LeveiHi @Levei_Luo,Thank you for your reply.I realized later that RX doesn’t work with mellanox CX4 SRIOV VF because the packet is double tagged with two .1q tags (802.1q in 802.1q). And there is a known issue  in the Red Hat Openshift “inbox” driver to handle. So, until this is not fixed in inbox driver, it won’t work with DPDK driver either.Regards,
RohitThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
455,obtaining-a-dpu-to-develop-and-test-doca-applications,"DOCA development can be performed in any pure software environment supporting cross compilation for x86 host DOCA applications and for Arm on the DPU.To test and benchmark hardware acceleration functions, a physical DPU is necessary. To purchase a DPU:Contact your NVIDIA Partner
Buy NVIDIA Networking Products | NVIDIAVisit the NVIDIA Networking store:
https://store.mellanox.com/poc/bluefield-dpuPowered by Discourse, best viewed with JavaScript enabled"
456,can-i-deploy-doca-on-a-arm-host-server,"Well,I try to download DOCA host repos, but find that all repos are for x86. Does that mean BlueField DPU is only supported on a x86 server?  If so, will there be any plan for supporting BlueField on Arm server?Powered by Discourse, best viewed with JavaScript enabled"
457,bluefield-not-reachable-from-the-host-after-installing-cuda-on-the-bluefield,"Installed CUDA on the Bluefield.  At the last stage of installation, the UEFI screen opened and asked for a password that would be called for after reboot.   However after reboot am no longer able to ssh into the Bluefield (and so have not seen any UEFI screen asking for that password).How can I reach the Bluefield from the Host again?The sequence I used was:
Install DOCA
Install CUDA (as per instructions at the end of the DOCA instructions)Have tried everything I can think of to do (remotely).  Is there any way to save this card?instructions were those from DOCA, the last instruction is the one that caused the problem with UEFI :hi brandt33I suggest you can reburn a BFB to recover the card:NVIDIA DOCA 软件框架 用于 NVIDIA BlueField DPU 的 加速应用开发 抢先体验 NVIDIA® DOCA™ 是释放 NVIDIA BlueField® DPU （数据处理器）卸载、加速和隔离数据中心工作负载潜力的关键。借助 DOCA，开发者通过创建软件定义、云原生、DPU 加速的服务来对未来的数据中心基础设施进行编程，并支持零信任保护，以满足现代数据中心日益增长的性能和安全需求。 抢先体验 如果您已经是 DOCA 抢先体验计划的成员，请直接前往此处开始。 DOCA...
or you can try minicom -o -D /dev/rshim0/console to login arm on BF card(if the cpu still can startup).If still have the issue after reburn the BFB, you can contact networking-support@nvidia.comThank you
M,SHi M,S,Thank you for your note and suggestions.The cpu is reachable and comes up normally.  But there is no minicom.  Is there an alternative to minicom that I could use ?   …Thank you,BrandtThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
458,bonding-teaming-over-multiple-adapters,"Is bonding possible over multiple cards? Every KB I find on this topic mentions a dual port adapter.We’d like to get over 63Gbps PCIe3 limit per card and thus would like to spread the traffic over multiple cards. (running connect-x 3). If not possible, how about connectx-4 LX cards?What if we need bonded RoCE?I’m amazed no one has asked this before.Thanks.Hello,RoCE LAG is a feature meant for mimicking Ethernet bonding for IB devices and is available for dual port cards only.Any change of bonding configuration that negates one of the above rules (i.e, bonding mode is not 1, 2 or 4, or both Ethernet interfaces that belong to the same card are not the only slaves of the bond interface), will result in exiting RoCE LAG mode and the return to normal IB device per port configuration.Please review the following for more information:https://docs.mellanox.com/pages/viewpage.action?pageId=58757626#RDMAoverConvergedEthernet(RoCE)-RoCELAG(ConnectX-3/ConnectX-3Pro)https://community.mellanox.com/s/article/How-to-Configure-RoCE-over-LAG-ConnectX-4-ConnectX-5-ConnectX-6Thank you,-HilaryWe’ve got two dual port cards. It’s unclear whether bonding is possible over them (over multiple cards). Or is it over ports of a single card only?The statement “both Ethernet interfaces that belong to the same card are not the only slaves of the bond” refers to two interfaces belonging to the same adapter; i.e., it is not possible to utilize multiple adapters’ ports within the LAG configuration.-HilaryPowered by Discourse, best viewed with JavaScript enabled"
459,which-cable-to-connect-connectx-7-with-broadcom-p2100g,"Hi Everyone,We’ve purchased a new storage server that comes with a MCX713106AS-VEAT 200Gbe ConnectX-7 card. We also have a system with a Broadcom P2100G 100Gbe card that has gone unused until now. The ConnectX-7 card has dual QSFP112 ports on it, while the P2100G has dual QSFP56 ports. Would we be able to connect the two using a single QSFP56 cable? Or would we need a QSFP112 to QSFP56 splitter to connect the two? We’d like to be able to connect the two so that we can utilize the full bandwidth.Regards,
– KurtHello UCBKurt,Thank you for posting your query on our community. The list of supported cables for ConnectX-7 cards is listed below:
https://docs.nvidia.com/networking/display/ConnectX7Firmwarev28332028/Firmware%20Compatible%20ProductsRegarding compatibility, QSFP112 is backwards compatible with QSFP56. Please refer to - http://qsfp112.com/Hope this helps.Thanks & Regards,
Nvidia SupportPowered by Discourse, best viewed with JavaScript enabled"
460,how-many-lines-of-code-does-physx-consist-of,"How many lines of code were compiled in order to produce physx? Approximately.
Also, how much manpower measured in ‘man hours’ was spent to produce physx? Approximately also.
ThanxHi - the correct place to ask it is at:NVIDIA PhysX SDK. Contribute to NVIDIAGameWorks/PhysX development by creating an account on GitHub.Powered by Discourse, best viewed with JavaScript enabled"
461,what-is-the-difference-between-dpi-compiler-and-rxp-compiler,"I wanna code a demo base on DOCA to do some regex match. After I read the Doca SDK doc NVIDIA DOCA SDK Documentation.
I just wanna know what is the difference between DPI compiler and RXP compiler?I see DOCA DPI API init and use the file compilered by DPI compiler, How can I use the RXP compiler to match regex?There’s a DOCA regex library without DPI coming in the April release that sounds like it will be what you are looking for.We don’t have DOCA APIs to work with directly with the compiled RXP format yet. You could use DPDK today, but DOCA APIs for this are right around the corner.In the meantime, maybe rxpbench would be a way to test regex until our April release? RXPBench :: NVIDIA DOCA SDK DocumentationPowered by Discourse, best viewed with JavaScript enabled"
462,sniff-roce-traffic-using-tcpdump,"I’m trying to sniff RoCE traffic using tcpdump with our ConnectX-5 adapter.However, running the following command returns the output:mariano@nslrack02:~$ sudo tcpdump -i mlx5_0tcpdump: mlx5_0: No such device exists(SIOCGIFHWADDR: No such device)I read in the docs that I must install libpcap >= 1.9, tcpdump >= 4.9.3 and OFED >= 5.1.This is my environment:mariano@nslrack02:~$ tcpdump --versiontcpdump version 4.9.3libpcap version 1.10.0 (with TPACKET_V3)OpenSSL 1.1.1 11 Sep 2018mariano@nslrack02:~$ ofed_infoMLNX_OFED_LINUX-5.3-1.0.0.1 (OFED-5.3-1.0.0):OS is Ubuntu 18.04 with Linux kernel: 5.4.0-74.So everything seems to the right version but it does not work. What am I missing? ThanksHI Mariano,We removed support for Offloaded Traffic Sniffer feature, but we can suggest using the docker-container solution to use RDMA devices, in order to capture and analyze RDMA packets using tcpdump.RequirementsCentOS/RHEL 7.x / 8.xUpstream Kernel must be higher than 4.9​- ConnectX-3/4/5MFT 4.9 and above (You can download it from Mellanox Firmware Tools (MFT) )perftest package to run ib_write_bw testInstallation instructions:Install OS that is compatible with kernel 4.9 and aboveInstall Upstream kernel starting from version 4.9 support sniffing RDMA(RoCE) trafficYum install dockerDocker pull mellanox/tcpdump-rdmaService docker startDocker run -it -v /dev/infiniband:/dev/infiniband -v /tmp/traces:/tmp/traces --net=host --privileged mellanox/tcpdump-rdma bashInstall MFT 4.9Install perftest package from MLNX_OFED RPMS directoryCapture RoCE packets with the following:tcpdump -i mlx5_0 -s 0 -w /tmp/traces/capture1.pcaportcpdump -i mlx4_0 -s 0 -w /tmp/traces/capture1.pcap​Server : ib_write_bw -d mlx5_0 -a -FClient: ib_write_bw -a -F <Server_ip>Thanks,SamerHi Samer,thanks for the answer. As far as I understood, this is a tcpdump limitation. However, I also want to sniff RoCE traffic with libpcap in a C application.Do I still need to use the Mellanox Docker container?Is there any chance to install the same libraries or packages that are inside the container into my host?Thanks.Mariano,I too have been running into the same error as you. While Samer’s statement about removing the Offload Traffic Sniffer holds true (meaning you can’t enable or disable the sniffer through ethtool anymore), RMDA sniffer support was introduced into libpcap version 1.9.0 or newer and shouldn’t rely on that. My question to you is are you using the Ubuntu distribution’s repo tcpdump package or have you installed them from somewhere else manually?In my scenario, I am using a similar config to you, except I am using Ubuntu 20.04 and the following tcpdump --version:tcpdump version 4.9.3libpcap version 1.9.1 (with TPACKET_V3)OpenSSL 1.1.1f 31 Mar 2020I get the same output:user@remote2:/mnt/nfsrdma$ sudo tcpdump -i mlx5_1tcpdump: mlx5_1: No such device exists(SIOCGIFHWADDR: No such device)What is making me scratch my head is when I download the exact same versions in source code form from the tcpdump.org site (tcpdump 4.9.3 and libpcap 1.9.1), compile them, then ‘make install’ them, it works just fine:user@remote2:/mnt/nfsrdma$ sudo tcpdump -i mlx5_1tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on mlx5_1, link-type EN10MB (Ethernet), capture size 262144 bytesNote: if you don’t ‘apt remove tcpdump libpcap0.8’ your path may still be pointing to the distro install. The compiled install path is /usr/local/sbin/.The only thing that I can think of is that the OFED install provided the Mellanox specific hw support information that was pulled into the compile process, which the distro package does not natively include. I would prefer to have a recommendation to my customers that doesn’t include compiling code, so if anyone else finds and shares an easier package manager alternative to compiling code I would appreciate it!Dear James,I also got this problem and thank you so much for sharing your experience.Powered by Discourse, best viewed with JavaScript enabled"
463,connectx-3-permission-denied-when-reducing-flexboot-menu-timeout,"Hello,I’ve been using a Mellanox ConnectX-3 on my desktop for some time now, and have recently been considering replacing it due to slow system boot times . I finally took a moment to read the message displayed, and saw that there was a message about entering setup for  it. I did, and saw that there is a 14-second timeout for that screen. I tried reducing this to 1 second, but when trying to save it, I receive the following message:
**Saving settings ... Setting flexboot_menu_to couldn't be saved - Permission denied (http://ipxe.org/021f203c)**I’ve googled this error, and nothing is coming up (I see one other reference of someone receiving the same message for another setting, but there was no resolution of that mentioned in the thread).Any ideas what could be preventing this setting from being changed?Thank youHi NateR,Please try to upgrade the NIC FW to the latest version 2.42.5000 first.
(NVIDIA Networking Firmaware Downloads)
Then if you don’t need to keep FW config, you can use this command to reset the NIC mlxconfig to default.mlxconfig -d mt4099_pciconf0 reset -y
Or
mstconfig -d mt4099_pciconf0 resetTry to reboot system, and see whether the steps above works.Longran Wei
Nvidia Support TeamPowered by Discourse, best viewed with JavaScript enabled"
464,sockets-and-packet-generation-with-doca,"Since DOCA abstracts DPDK, are there any sample packet generation or socket programs with DOCA?Powered by Discourse, best viewed with JavaScript enabled"
465,can-i-use-the-infiniband-switch-as-a-daisy-chain,"인피니밴드 스위치를 데이지 체인 방식으로 사용해도되나
성능적으로 불이익이 없나요?( ex 8 port + 8 port )Powered by Discourse, best viewed with JavaScript enabled"
466,how-to-simulate-a-dpu,"Is it possible to work with a software simulator to develop programs for the DPU environment using DOCA?Powered by Discourse, best viewed with JavaScript enabled"
467,how-can-i-obtain-the-real-rate-limit-of-qp,"Hi sirs，
I use two servers equipped with ConnectX-5 adapter cards connected back-to-back to achieve the Packet Pacing Coding example now. And the version of OFED is v5.8-1.0.1.1 LTS.
I try to modify the QP rate limit and use ibv_ query_ qp function to obtain the modified QP rate limit. Unfortunately, no matter how I modify the speed, I will get the same result in the end. All of them are 124.Why is that? What should I do to get a real qp rate limit?IBTA define rate limit on QP. You can use Verbs API below implement that.https://man7.org/linux/man-pages/man3/ibv_modify_qp.3.htmlhttps://man7.org/linux/man-pages/man3/ibv_modify_qp_rate_limit.3.htmlI’m sorry I didn’t express my question clearly.
I successfully limit the rate of QP by the API ibv_modify_qp_rate_limit which you pointed. In the next step, I hope to get the rate after QP is limited in a certain way. I tried this, as follows

Finally, I get the result is 0. But I set the rate_limit to 10000Kbps. Whatever I set the rate_limit to, the printed result always is 0. The other attributes of QP which are printed are correct.
I wonder why is that and how I can print the real rate_limit attribute of QP.modify QP and query QP API define different qp_attr and mask. On query QP attr rate_limit not supported.https://man7.org/linux/man-pages/man3/ibv_query_qp.3.htmlI think no API can “print the real rate_limit attribute of QP”.alternative you can set QP SL then print SLx xmit by MAD CMD. Then calculate。root@mtbc-r740-06:~# perfquery 7 1 -XPortSelect:…1
CounterSelect:…0x0000
XmtDataSL0:…528247
XmtDataSL1:…0
XmtDataSL2:…0
XmtDataSL3:…0
XmtDataSL4:…0
XmtDataSL5:…0
XmtDataSL6:…0
XmtDataSL7:…0
XmtDataSL8:…0
XmtDataSL9:…0
XmtDataSL10:…0
XmtDataSL11:…0
XmtDataSL12:…0
XmtDataSL13:…0
XmtDataSL14:…0
XmtDataSL15:…0Powered by Discourse, best viewed with JavaScript enabled"
468,high-availability-of-vf-based-roce-protocol,"Hi，I successfully configured roce over lag with ”https://enterprise-support.nvidia.com/s/article/How-to-Configure-RoCE-over-LAG-ConnectX-4-ConnectX-5-ConnectX-6“.
Is it only support PF? Is there any solution that supports for VF？
I want to support roce communication in k8s containers and need a VF-based high availability solution.Best regards,Hello,You might want to take a look at this link below, applicable to CX6 as well.
I believed this is what you are looking for.
https://enterprise-support.nvidia.com/s/article/kubernetes-ipoib-sriov-networking-with-connectx4-connectx5Kubernetes Using SR-IOVIn order to use RDMA in Kubernetes environment with SR-IOV networking mode, two main
components are required:
RDMA device plugin - this plugin allows for exposing RDMA devices in a Pod
SR-IOV CNI plugin - this plugin provisions VF net device in a PodSophie.Thanks a lot for your answer. I have already configured it use that way. But I need a solution that is high available for VF, likes NIC bond or Roce LAG.Maybe check ASAP² Direct solution, information can be retrieved from our MLNX_OFED driver UM or we have some articles on our community site.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
469,connectx4-on-freebsd11-4,"Do we support ConnectX4 on freebsd11.4? I have tried to bring up CX4 on a freeBSD VM on ESXI host but I don’t see NICs getting detected on freebsd. I wonder whether we have the required support or notHello Phani,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately we do no support FreeBSD 11.4. You can try to install the driver which was created for FreeBSD 11 (v289420) but it is still not a supported configuration. We recommend to upgrade to FreeBSD 12 or 13 in which the driver comes within the distro so there is no need to build the driver from source.Please review the following driver documentation RN & UM for the latest version → https://docs.mellanox.com/display/FREEBSDv370Thank you and regards,~NVIDIA Networking Technical SupportThanks for the reply. Can you please let me know if this Is a limitation with ESXI/FreeBSD combination or freeBSD 11.4 in general?Powered by Discourse, best viewed with JavaScript enabled"
470,cannot-create-vfs-on-one-pf-but-can-create-on-another,"On Host side ,I can create VFs on enp1s0f0 likeecho 2 > /sys/class/net/enp1s0f0/device/sriov_numvfsBut it doesn’t work on enp1s0f1echo 2 > /sys/class/net/enp1s0f1/device/sriov_numvfsIt returnsbash: echo: write error: Input/output errorI check the driver usingdpdk-devbind.py -sand mlxconfig tool ,nothing different between two ports except some SF configurationdmesg log shows belowmlx5_core 0000:01:00.1: mlx5_sriov_enable:186:(pid 6253): pci_enable_sriov failed : -5Hi Waterzhu,What OS/Kernel/MLNX_OFED/FW are running on the host?
What type of server?
SRIOV has been enabled on the second port via our mlxconfig utility, am I correct?
Is this server running the latest BIOS version?
Can you check if your BIOS ARI (Alternate Routing ID) setting is enabled?Sophie.Note:Summary of Hardware Considerations for SR-IOVIt’s Ubuntu 20.0，5.4.0-26-generic
super-micro x11
what’s I mean is One BF2 on the same Host , But One port can use SR-IOV on the host, but the other one cannot. I use the kernel echo 2 > /sys/class/net/enp1s0f0/device/sriov_numvfs, not via mlxconifg.
I just use the mlxconfig to check sriov Flag is set.
I supposed that the Hardware is OK，because One port use SR-IOV success.
Or you mean if BIOS has problem, it is expressed as one port can create success and the other cannotHello all,I’m experiencing exactly the same issue as @waterzhu :Note that /dev/mst/ has two subdirectories, mt4125_pciconf0 and mt4125_pciconf0.1, but mlxfwmanager only return the former. Would it be the problem?and get the following result:And on port 0 the VFs are setup:Since VFs can be set up on port 0, i.e. enp1s0f0, I guess the settings are alright.So why can’t enp1s0f1 set up its own VF?Powered by Discourse, best viewed with JavaScript enabled"
471,how-to-install-ofed-umd-in-docker-and-ofed-kmd-only-in-host-for-ofed-4-9,"here i faced a problem, host use docker + gpu driver for work, i cannot use “./mlnxofedinstall” in full packet install on physical host, because docker cannot see umd files, does anyone know how to install ofed umd in docker and ofed kmd only in host for ofed 4.9?Hi,Please find the below link to the documentation that may help you:
https://docs.nvidia.com/networking/label/SOL/k8s_and_containersBest Regards,
AnatolyThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
472,installed-mellnox-winof2-but-powershell-modukles-not-available-on-install,"Get-MlnxPCIDeviceSriovSettingGet-MlnxPCIDeviceSriovSetting: The term ‘Get-MlnxPCIDeviceSriovSetting’ is not recognized as a name of a cmdlet,Hello Sirshak,Thank you for posting your inquiry on the NVIDIA Networking Community.When using WinOF-2 versus WinOF, indeed some of the PowerShell modules are not included by design.The following link will provide you the differences between WinOF and WinOF-2 and the comparison tables for Features and Tools → https://community.mellanox.com/s/article/differences-between-winof-and-winof-2-windows-driversThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
473,how-to-find-and-set-the-maximum-some-of-parametrs-for-a-nic-connectx-5-with-exaple,"How to find and set the maximum some of parameters for a NIC (ConnectX-5) with example?
How to get maximum throughput from 100 Gb Ethernet adapter change driver?queues_rx
queues_tx
rx_max_pkts
tx_send_cntqueues_rx=20 (default 8)
Number of receive queues used by the network adapter for receiving network traffic.
queues_tx=12 (default 2)
Number of transmit queues used by the network adapter for transmit network traffic
rx_max_pkts=2048 (default 1024)
Receive queue maximum packet count
tx_send_cnt=16 (default 8)
Number of transmit packets chained for adapter processingSystem OS Win server 2019. Using Mellanox Firmware Tools mlxconfigThat is fw config suggest can’t change.For performance tuning pls refer below,https://docs.nvidia.com/networking/display/winof2v320/Performance+TuningHI Thx. This link have small info for perf. tune   Witch parameter i can change to performance tuning?C:\Program Files\Mellanox\WinMFT>mlxconfig -d mt4119_pciconf0 qDevice type:    ConnectX5
Name:           MCX556A-ECA_Ax
Description:    ConnectX-5 VPI adapter card; EDR IB (100Gb/s) and 100GbE; dual-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6
Device:         mt4119_pciconf0Configurations:                                      Next Boot
MEMIC_BAR_SIZE                              0
MEMIC_SIZE_LIMIT                            _256KB(1)
HOST_CHAINING_MODE                          DISABLED(0)
HOST_CHAINING_CACHE_DISABLE                 False(0)
HOST_CHAINING_DESCRIPTORS                   Array[0…7]
HOST_CHAINING_TOTAL_BUFFER_SIZE             Array[0…7]
FLEX_PARSER_PROFILE_ENABLE                  0
FLEX_IPV4_OVER_VXLAN_PORT                   0
ROCE_NEXT_PROTOCOL                          254
ESWITCH_HAIRPIN_DESCRIPTORS                 Array[0…7]
ESWITCH_HAIRPIN_TOT_BUFFER_SIZE             Array[0…7]
PF_BAR2_SIZE                                0
PF_NUM_OF_VF_VALID                          False(0)
NON_PREFETCHABLE_PF_BAR                     False(0)
VF_VPD_ENABLE                               False(0)
PF_NUM_PF_MSIX_VALID                        False(0)
PER_PF_NUM_SF                               False(0)
STRICT_VF_MSIX_NUM                          False(0)
VF_NODNIC_ENABLE                            False(0)
NUM_PF_MSIX_VALID                           True(1)
NUM_OF_VFS                                  0
NUM_OF_PF                                   2
PF_BAR2_ENABLE                              False(0)
SRIOV_EN                                    False(0)
PF_LOG_BAR_SIZE                             5
VF_LOG_BAR_SIZE                             1
NUM_PF_MSIX                                 63
NUM_VF_MSIX                                 11
INT_LOG_MAX_PAYLOAD_SIZE                    AUTOMATIC(0)
PCIE_CREDIT_TOKEN_TIMEOUT                   0
ACCURATE_TX_SCHEDULER                       False(0)
PARTIAL_RESET_EN                            False(0)
SW_RECOVERY_ON_ERRORS                       False(0)
RESET_WITH_HOST_ON_ERRORS                   False(0)
ADVANCED_POWER_SETTINGS                     False(0)
CQE_COMPRESSION                             BALANCED(0)
IP_OVER_VXLAN_EN                            False(0)
MKEY_BY_NAME                                False(0)
ESWITCH_IPV4_TTL_MODIFY_ENABLE              False(0)
PRIO_TAG_REQUIRED_EN                        False(0)
UCTX_EN                                     True(1)
PCI_ATOMIC_MODE                             PCI_ATOMIC_DISABLED_EXT_ATOMIC_ENABLED(0)
TUNNEL_ECN_COPY_DISABLE                     False(0)
LRO_LOG_TIMEOUT0                            6
LRO_LOG_TIMEOUT1                            7
LRO_LOG_TIMEOUT2                            8
LRO_LOG_TIMEOUT3                            13
LOG_TX_PSN_WINDOW                           7
LOG_MAX_OUTSTANDING_WQE                     7
TUNNEL_IP_PROTO_ENTROPY_DISABLE             False(0)
ICM_CACHE_MODE                              DEVICE_DEFAULT(0)
TX_SCHEDULER_BURST                          0
ZERO_TOUCH_TUNING_ENABLE                    False(0)
LOG_MAX_QUEUE                               17
LOG_DCR_HASH_TABLE_SIZE                     11
MAX_PACKET_LIFETIME                         0
DCR_LIFO_SIZE                               16384
LINK_TYPE_P1                                ETH(2)
LINK_TYPE_P2                                ETH(2)
ROCE_CC_PRIO_MASK_P1                        255
ROCE_CC_PRIO_MASK_P2                        255
CLAMP_TGT_RATE_AFTER_TIME_INC_P1            True(1)
CLAMP_TGT_RATE_P1                           False(0)
RPG_TIME_RESET_P1                           300
RPG_BYTE_RESET_P1                           32767
RPG_THRESHOLD_P1                            1
RPG_MAX_RATE_P1                             0
RPG_AI_RATE_P1                              5
RPG_HAI_RATE_P1                             50
RPG_GD_P1                                   11
RPG_MIN_DEC_FAC_P1                          50
RPG_MIN_RATE_P1                             1
RATE_TO_SET_ON_FIRST_CNP_P1                 0
DCE_TCP_G_P1                                1019
DCE_TCP_RTT_P1                              1
RATE_REDUCE_MONITOR_PERIOD_P1               4
INITIAL_ALPHA_VALUE_P1                      1023
MIN_TIME_BETWEEN_CNPS_P1                    4
CNP_802P_PRIO_P1                            6
CNP_DSCP_P1                                 48
CLAMP_TGT_RATE_AFTER_TIME_INC_P2            True(1)
CLAMP_TGT_RATE_P2                           False(0)
RPG_TIME_RESET_P2                           300
RPG_BYTE_RESET_P2                           32767
RPG_THRESHOLD_P2                            1
RPG_MAX_RATE_P2                             0
RPG_AI_RATE_P2                              5
RPG_HAI_RATE_P2                             50
RPG_GD_P2                                   11
RPG_MIN_DEC_FAC_P2                          50
RPG_MIN_RATE_P2                             1
RATE_TO_SET_ON_FIRST_CNP_P2                 0
DCE_TCP_G_P2                                1019
DCE_TCP_RTT_P2                              1
RATE_REDUCE_MONITOR_PERIOD_P2               4
INITIAL_ALPHA_VALUE_P2                      1023
MIN_TIME_BETWEEN_CNPS_P2                    4
CNP_802P_PRIO_P2                            6
CNP_DSCP_P2                                 48
LLDP_NB_DCBX_P1                             False(0)
LLDP_NB_RX_MODE_P1                          OFF(0)
LLDP_NB_TX_MODE_P1                          OFF(0)
LLDP_NB_DCBX_P2                             False(0)
LLDP_NB_RX_MODE_P2                          OFF(0)
LLDP_NB_TX_MODE_P2                          OFF(0)
ROCE_RTT_RESP_DSCP_P1                       0
ROCE_RTT_RESP_DSCP_MODE_P1                  DEVICE_DEFAULT(0)
ROCE_RTT_RESP_DSCP_P2                       0
ROCE_RTT_RESP_DSCP_MODE_P2                  DEVICE_DEFAULT(0)
DCBX_IEEE_P1                                True(1)
DCBX_CEE_P1                                 True(1)
DCBX_WILLING_P1                             True(1)
DCBX_IEEE_P2                                True(1)
DCBX_CEE_P2                                 True(1)
DCBX_WILLING_P2                             True(1)
KEEP_ETH_LINK_UP_P1                         True(1)
KEEP_IB_LINK_UP_P1                          False(0)
KEEP_LINK_UP_ON_BOOT_P1                     False(0)
KEEP_LINK_UP_ON_STANDBY_P1                  False(0)
DO_NOT_CLEAR_PORT_STATS_P1                  False(0)
AUTO_POWER_SAVE_LINK_DOWN_P1                False(0)
KEEP_ETH_LINK_UP_P2                         True(1)
KEEP_IB_LINK_UP_P2                          False(0)
KEEP_LINK_UP_ON_BOOT_P2                     False(0)
KEEP_LINK_UP_ON_STANDBY_P2                  False(0)
DO_NOT_CLEAR_PORT_STATS_P2                  False(0)
AUTO_POWER_SAVE_LINK_DOWN_P2                False(0)
NUM_OF_VL_P1                                _4_VLs(3)
NUM_OF_TC_P1                                _8_TCs(0)
NUM_OF_PFC_P1                               8
VL15_BUFFER_SIZE_P1                         0
NUM_OF_VL_P2                                _4_VLs(3)
NUM_OF_TC_P2                                _8_TCs(0)
NUM_OF_PFC_P2                               8
VL15_BUFFER_SIZE_P2                         0
DUP_MAC_ACTION_P1                           LAST_CFG(0)
MPFS_MC_LOOPBACK_DISABLE_P1                 False(0)
MPFS_UC_LOOPBACK_DISABLE_P1                 False(0)
UNKNOWN_UPLINK_MAC_FLOOD_P1                 False(0)
SRIOV_IB_ROUTING_MODE_P1                    LID(1)
IB_ROUTING_MODE_P1                          LID(1)
DUP_MAC_ACTION_P2                           LAST_CFG(0)
MPFS_MC_LOOPBACK_DISABLE_P2                 False(0)
MPFS_UC_LOOPBACK_DISABLE_P2                 False(0)
UNKNOWN_UPLINK_MAC_FLOOD_P2                 False(0)
SRIOV_IB_ROUTING_MODE_P2                    LID(1)
IB_ROUTING_MODE_P2                          LID(1)
PHY_FEC_OVERRIDE_P1                         DEVICE_DEFAULT(0)
PHY_FEC_OVERRIDE_P2                         DEVICE_DEFAULT(0)
PF_TOTAL_SF                                 0
PF_SF_BAR_SIZE                              0
PF_NUM_PF_MSIX                              63
ROCE_CONTROL                                ROCE_ENABLE(2)
PCI_WR_ORDERING                             per_mkey(0)
MULTI_PORT_VHCA_EN                          False(0)
PORT_OWNER                                  True(1)
ALLOW_RD_COUNTERS                           True(1)
RENEG_ON_CHANGE                             True(1)
TRACER_ENABLE                               True(1)
IP_VER                                      IPv4(0)
BOOT_UNDI_NETWORK_WAIT                      0
UEFI_HII_EN                                 True(1)
BOOT_DBG_LOG                                False(0)
UEFI_LOGS                                   DISABLED(0)
BOOT_VLAN                                   1
LEGACY_BOOT_PROTOCOL                        PXE(1)
BOOT_INTERRUPT_DIS                          False(0)
BOOT_LACP_DIS                               True(1)
BOOT_VLAN_EN                                False(0)
BOOT_PKEY                                   0
P2P_ORDERING_MODE                           DEVICE_DEFAULT(0)
ATS_ENABLED                                 False(0)
DYNAMIC_VF_MSIX_TABLE                       False(0)
EXP_ROM_UEFI_x86_ENABLE                     False(0)
EXP_ROM_PXE_ENABLE                          True(1)
IBM_TUNNELED_ATOMIC_EN                      False(0)
IBM_AS_NOTIFY_EN                            False(0)
ADVANCED_PCI_SETTINGS                       False(0)
SAFE_MODE_THRESHOLD                         10
SAFE_MODE_ENABLE                            True(1)hi @1welder1
In the above log one parameter we have could you please explain me what exactly it is ----LOG_MAX_OUTSTANDING_WQE 7Thanks
PrashanthThanks I found the details of that
image1641×53 4.41 KBThanks
prashanthThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
474,10-25-gbps-mellanox-nic-with-linux-nat-offload-support,"I’m looking for a 10 or 25 Gbps Mellanox NIC that has NAT44 offload support in Linux. This is for an ISP with a thousand users or so, and we have nftables rules that handle NAT mappings, including a map with ~100 public IPs, for snat and dnat. Peak loads are 5+ Gbps or so, and mean loads more like up to 2 Gbps. We want to add two NAT boxes to move NAT functionality off of our gateway.We actually think that general purpose hardware should be able to handle this load, without hardware acceleration. But, if there are Mellanox NICs that can do it, it may allow us to use lower-end CPUs, with increased performance and room for future growth.Would the ConnectX-3 or 4 do this for us?And, in case we also add shaping with tc on the same NIC, will the hardware offloads still work?Hello Pete,Thank you for posting your inquiry on the NVIDIA Networking Community.Our NVIDIA ConnectX-4/5/6 Ethernet adapters are capable of NAT44 offload, even if you shape (Packet Pacing) it with ‘tc’.We would not recommend the ConnectX-3 anymore, as it is reaching its EOL status for certain SKU’s.Thank you and regards,~NVIDIA Networking Technical SupportThanks. I think I see now how this works with nftables. You define a flowtable, and offload that flowtable to hardware, so that the initial routing decision is made in software when the flow starts, and further packets for that flow follow the hardware path.With the shaping, I see you’re referring to the hardware pacing feature in the card. We currently use fq_codel or straight Codel in software, but that doesn’t appear to be available in hardware. RED is possible, but more difficult to configure. My hope is that we can route some of these software-shaped flows separately from the hardware path to use tc qdiscs. Since they are lower rate 100 Mbps flows, it should not affect performance too much, while the high rate flows can keep using the hardware path.Do you think that will work out?Thanks,Pete HeistPowered by Discourse, best viewed with JavaScript enabled"
475,connectx-vpi-operating-system-compatibility,"I have a large cluster running RHEL 6.4 and RHEL 6.6. I want to configure Infiniband between the nodes. What ConnectX VPI hardware is natively supported or what ConnectX VPI hardware is supported with the addition of a driver?Hi ,The ConnectX is end of life product and end of support product .There is no driver in our archive that works with the mentioned OSs and ConnectX card.We suggest to contact your Sales Rep for further information as we recommend upgradingthe current adapters to new Adapters from ConnectX-5/6 series.Thanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
476,ats-and-pri-support-on-connectx-6-dx-en,"Hi,I have a ConnectX-6 Dx EN adapter card (MCX623106AC-CDAT) and was told it would support the PCIe extended capabilities ATS (Address Translation Service) and PRI (Page Request Interface) but I do not see these extended capabilities listed in the device’s config space.  The RC side in my machine doesn’t support this but I would still expect the endpoint capability to be listed.  Could someone confirm if these caps are support?Running Linux kernel 4.14.76Anything?Is this what you’re looking for?
https://docs.nvidia.com/ai-enterprise/multi-node/mn-getting-started.html#enable-ats-on-vmware-esxi-and-vmsThanks @liorp.  I’ll give those steps a try.
Those steps appear to describe enabling ATS, but it’s a little surprising that the PCIe config capability doesn’t even show up in the config space, regardless if it’s enabled or not.Thanks!Hi ,
In addition to what mentioned on this thread ,
ATS was implemented for NVIDIA and now supported in ConnectX-5 and ConnectX-6
Need to make sure ATS is enabled in Bios and enable it in the card using mlxconfig
mlxconfig -d /dev/mst/mt4123_pciconf0 set ATS_ENABLED=1For more details:
https://wikinox.mellanox.com/display/FW/pcie+ATS+feature?focusedCommentId=244745226&refresh=1611739334677#comment-244745226This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
477,nat-with-cumulus,"Hello GuysHave a switch Mellanox and need to know if is possible this configuration.The Datacenter assign a segment of six public IP Address a I need to connect directly to port swp1 and assign a IP public.In the switch create vlan 50 in L3 con segment 192.168.50.x/29 and from port swp2 to swp5 and 4 servers were connected and assigned IP of VLAN 50.Now need to know if is possible create a NAT where permit access to Internet (Specified ports) of the segment 192.168.50.0/29In the port I assign in access mode the vlan 50I need is to see if with a NAT rule indicate that everything that comes from the 192.168.50.0/24 segment goes through the public IP 200.200.200.1 assigned to port swp1If possible how should i configure a rule on the switch?Thanks for your comments.Hi Santunex,Please open a support case, this requires a detailed review.
Networking-support@nvidia.comThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
478,degraded-performance-with-drivers-version-5-vs-version-4,"Hello,We have a few Linux (CentOS 7) machines interconnected with ConnectX5 NICs (MCX515A-CCAT) on a 100GbE Ethernet network (Juniper switches) with support for RoCE. We have been using drivers version 4 for a while with no performance issues: ib_send_bw provides a very steady 97 Gb/s (MTU is 9000).The drivers are installed with kernel support from MLNX_OFED packages. The OS is up-to-date: CentOS 7.9 to this day, with Linux kernel version 3.10.0-1160.6.1.el7.x86_64We have tried each new release of the drivers version 5 with degraded performance:ib_send_bw -d mlx5_0 -F --report_gbits --run_infinitely -D 1 reports throughput oscillating between 50 and 70 Gb/s.Has anyone experienced such degradation?Has anyone any hint for a possible reason?Or a good way to investigate what could be the problem?Any help would be appreciated.Cheers,FabriceHi Fabrice,Please note that we are not familiar with such performance degradation in the new OFED version .According to our records your account has valid support contract therefore we suggest to open support ticket at Networking-support@nvidia.com in order to investigate this issue.Thanks,SamerHi Samer,Thank you, I will.Powered by Discourse, best viewed with JavaScript enabled"
479,cumulus-linux-bridge-vlan-configuration-question,"Hi experts,I have a question regarding Cumulus Linux bridge VLAN configuration. If this is my configuration related to VLANs:admin@SW-MLNX-01:mgmt:~$ nv config show -o commands
nv set bridge domain br_default vlan 99
nv set bridge domain br_default vlan 129
nv set bridge domain br_default vlan 130
nv set bridge domain br_default vlan 131
nv set bridge domain br_default vlan 132
nv set bridge domain br_default vlan 133
nv set bridge domain br_default vlan 134
nv set bridge domain br_default vlan 135
nv set bridge domain br_default vlan 136
nv set interface bond1 bond member swp1
nv set interface bond1 bond mlag id 1
nv set interface bond1-2,5-12,swp12-14 bridge domain br_default untagged 99
nv set interface bond1-4,11-12,swp1-2,9-12 link mtu 1500
nv set interface bond1-12 bond mlag enable on
nv set interface bond1-12 link state up
nv set interface bond1-12 type bond
nv set interface bond2 bond member swp2
nv set interface bond2 bond mlag id 2
nv set interface bond3 bond member swp9
nv set interface bond3 bond mlag id 3
nv set interface bond3-4 bridge domain br_default vlan 99
nv set interface bond4 bond member swp10
nv set interface bond4 bond mlag id 4
nv set interface bond5 bond member swp3
nv set interface bond5 bond mlag id 5
nv set interface bond5-10 bridge domain br_default vlan 1
nv set interface bond6 bond member swp4
nv set interface bond6 bond mlag id 6
nv set interface bond7 bond member swp5
nv set interface bond7 bond mlag id 7
nv set interface bond8 bond member swp6
nv set interface bond8 bond mlag id 8
nv set interface bond9 bond member swp7
nv set interface bond9 bond mlag id 9
nv set interface bond10 bond member swp8
nv set interface bond10 bond mlag id 10
nv set interface bond11 bond member swp51
nv set interface bond11 bond mlag id 11
nv set interface bond11 bridge domain br_default stp admin-edge on
nv set interface bond12 bond member swp11
nv set interface bond12 bond mlag id 12If this OK for bond1?Why bond1 allows VLANs 129-136 if it is not allowed in the configuration? These VLANs are defined in bridge br_default, but I override this configuration with the specific interface configuration “nv set interface bond1 bridge domain br_default untagged 99”., it should only allow VLAN 99 as untagged, as it shows the “net show configuration” command:net show configuration
…
interface bond1I don’t know why these two commands show different configurations.Regards,
Juliánnv set interface bond1-2,5-12,swp12-14 bridge domain br_default untagged 99It all comes down to this line of configuration. ‘Untagged’ is meant to be used to call-out the native/untagged/primary vlan on a trunk. Here the ‘untagged’ keyword is being used, so bond1 is being declared  a trunk, but only the native/untagged/primary VLAN has been specified and not the normal tagged VLANs. If normal tagged VLANs are not specified, they will all be allowed by default.If a configuration which consists of a single untagged VLAN is desired, it might be easier to declare the port as an access port like this:With this configuration line, both an untagged VLAN is defined while also disallowing all tagged VLANs too.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
480,cumulus-linux-lacp-modes,"Hi community,In Onyx, when I configure LACP, I have to set it to on, active or passive mode:interface ethernet x/x mlag-channel-group y mode [active, passive, on]While in Cumulus Linux, when you configure LACP, you cannot configure this setting. The documentation only says:""Cumulus Linux supports IEEE 802.3ad link aggregation mode (802.3ad) and balance-xor mode. The default mode is 802.3ad.""Any idea?Regards,
JuliánWhat would you like to configure? A dynamic (LACP) or static bond?Hi attilla,I am going to convert my Onyx switch into Cumulus Linux. My LACP configuration I have in Onyx is like this:interface ethernet 1/1 mlag-channel-group 1 mode activeSo I think it is dynamic bond.Regards,
JuliánPowered by Discourse, best viewed with JavaScript enabled"
481,mt25408a0-fcc-qi-connectx-failed-to-identify-the-device-can-not-create-signaturemanager,"After installationThe firmware for this device is not distributed inside Mellanox driver: 01:00.0                                                                                         (PSID: HP_0180000009)
To obtain firmware for this device, please contact your HW vendor.mlxfwmanager -d 01:00.0
Querying Mellanox devices firmware …
Failed to identify the device - Can not create SignatureManager!
Failed to identify the device - Can not create SignatureManager!Device Type:      ConnectX2
Part Number:      592520-B21
Description:      HP IB 4X QDR CX-2 PCI-e G2 Dual Port HCA
PSID:             HP_0180000009
PCI Device Name:  01:00.0
Port1 GUID:       0002c90300096bed
Port2 GUID:       0002c90300096bee
Versions:         Current        Available
FW             2.9.1000       2.9.1000Question: Where can I find a driver for my equipment?
It’s an old hardware but I don’t believe the drivers for my card are gone.This is an HP branded product – the firmware images are managed by HP.Need to approach HPE support for obtaining the firmware.This may be relevant:https://support.hpe.com/connect/s/softwaredetails?language=en_US&softwareId=MTX_d859112d72c14ec09df80c482ePowered by Discourse, best viewed with JavaScript enabled"
482,98gb-s-rdma-but-slow-speeds-in-win-11-with-100g-network,"While using rperf and mlxndPerf I am able to confirm 98 Gb/s read and write, but actual file transfers and read/write performance and iperf tests are only showing up to 50 Gb/s write and 10-14 Gb/s read. I have been working at this for over a month and have done consultations with IT tech, multiple tech support staff from different places and have spent many many many hours working on this and cannot figure this out.I have two workstations Im connecting to a NAS (TS-H1290FX) that has six 7.68TB seagate NVME drives in it configured with RAID 0 (each drive individually capable of 6700 MB/s read AND write). The workstations are NAS are all equipped with the same 100G NIC (QXG-100G2SF-CX6) which uses a Mellanox Dx6 controller. I am connecting all the NICs with Mellanox transceivers and MTP OM5 fiber optic cables from FS.com. I have installed Windows 11 Pro for Workstations on both PCs (to enable SMB 3.1.1, etc) I have confirmed in system BIOS and in Windows that the NICs are indeed running at 16x on the PCIe Gen 4 bus on both systems. Both systems are running AMD threadrippers, and the NAS has an AMD Epyc CPU. I have 128GB of RAM in one machine and 256GB RAM in the other. All HDs in these PCs are M.2 NVME drives capable of 3000MB/s in one machine and 6000 MB/s in the other (running newer Sabrent NVME drives).Things I have tried to increase speeds on both PCs, all to no avail:There seems to me to be no logical reason why either the hardware or software would be causing a bottleneck. I’ve tried everything! Been working on this for a month now. I have $14,000 in NAS storage I need to use but cannot because speeds are so bad. What am I missing?ndperf test rdma, iperf test tcp/ip. naturally, there are different.first need confirm wah SMB protocol your storage support, then follow below check windows SMB config.https://docs.nvidia.com/networking/display/winof2v320/Storage+ProtocolsIt says the RDMA is enabled, but not operational. It doesnt say on that link how to make it operational. Are there instructions how to do that?
not operational1272×160 10.7 KB
Powered by Discourse, best viewed with JavaScript enabled"
483,installing-mlnx-ofed-linux-5-6-2-0-9-0-rhel7-9-x86-64-on-centos-7-9-5-with-kernel-version-3-10-0-1160-66-1-el7-x86-64,"I am trying to install MLNX_OFED_LINUX-5.6-2.0.9.0-rhel7.9-x86_64 on centOS with kernel 3.10.0-1160.66.1.el7.x86_64. But it gives this msg as :
General log file: /tmp/MLNX_OFED_LINUX.30324.logs/general.log
Verifying KMP rpms compatibility with target kernel…
The kernel KMP rpms coming with MLNX_OFED_LINUX are not compatible with kernel: 3.10.0-1160.66.1.el7.x86_64
See log at /tmp/MLNX_OFED_LINUX.30324.logs/is_kmp_compat_check.logThe 3.10.0-1160.66.1.el7.x86_64 kernel is installed, MLNX_OFED_LINUX does not have drivers available for this kernel.
You can run mlnx_add_kernel_support.sh in order to to generate an MLNX_OFED_LINUX package with drivers for this kernel        .
Or, you can provide ‘–add-kernel-support’ flag to generate an MLNX_OFED_LINUX package and automatically start the installation.On checking the logs it states that :
warning: /opt/astellia/flex/melanox/MLNX_OFED_LINUX-5.6-2.0.9.0-rhel7.9-x86_64/RPMS/kmod-mlnx-ofa_kernel-5.6-OFED.5.6.2.0.9.1.rhel7u9.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID 6224c050: NOKEY
depmod: WARNING: /tmp/kmp_check.7gJULTZ/lib/modules/3.10.0-1160.66.1.el7.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_uverbs.ko needs unknown symbol fget_light
depmod: WARNING: /tmp/kmp_check.7gJULTZ/lib/modules/3.10.0-1160.66.1.el7.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/rdma_ucm.ko needs unknown symbol fget_light
image1531×250 25.1 KB
Can someone provide any inputs on this ? Any help would be appreciatedHere you go dude.it isn’t easy to findhttps://linux.mellanox.com/public/repo/mlnx_ofedDid you find a root cause and solution to this topic?  I see the same issue.Powered by Discourse, best viewed with JavaScript enabled"
484,installing-ofed-on-centos-stream-8-0,"Hey All,So I am having issues getting things installed on my server running a fresh copy of CentOS Stream 8. I am running a Mellanox Technologies MT27800 Family [ConnectX-5] Infiniband controller. Downloading the RHEL8.0 version of 5.2.2.0 results in a distro check failure unless I force it. Taking any suggestions on how to get this built (even via source which seems to have no documentation).~ Joe G.[root@Moleskine MLNX_OFED_LINUX-5.2-2.2.0.0-rhel8.0-x86_64]# ./mlnxofedinstall --distro rhel8.0 --add-kernel-support --without-fw-updateNote: This program will create MLNX_OFED_LINUX TGZ for rhel8.0 under /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.18.0-277.el8.x86_64 directory.See log file /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.18.0-277.el8.x86_64/mlnx_iso.1447830_logs/mlnx_ofed_iso.1447830.logChecking if all needed packages are installed…Building MLNX_OFED_LINUX RPMS . Please wait…ERROR: Failed executing “MLNX_OFED_SRC-5.2-2.2.0.0/install.pl --tmpdir /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.18.0-277.el8.x86_64/mlnx_iso.1447830_logs --kernel-only --kernel 4.18.0-277.el8.x86_64 --kernel-sources /lib/modules/4.18.0-277.el8.x86_64/build --builddir /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.18.0-277.el8.x86_64/mlnx_iso.1447830 --disable-kmp --build-only --distro rhel8.0”ERROR: See /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.18.0-277.el8.x86_64/mlnx_iso.1447830_logs/mlnx_ofed_iso.1447830.logFailed to build MLNX_OFED_LINUX for 4.18.0-277.el8.x86_64Hello Joe,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately, currently we do not support MLNX_OFED for CentOS 8 Stream. We are currently reviewing this internally, and hope to provide a resolution and statement soon.Thank you and regards,~NVIDIA Networking Technical SupportAny updates?Have there been any updates for this issue?Any updates on this ?Powered by Discourse, best viewed with JavaScript enabled"
485,im-getting-low-throughput-with-mpls-tagged-packets-is-there-something-that-can-be-tuned-to-improve-performance,"In this test setup I have a couple of VMs running iperf tests and the traffic between them is passed through another two VMs/virtual routers. All are running on seperate KVM hosts.The virtual routers have two SR-IOV VFs each. The hosts all have ConnectX-5 25Gbps adapters.VM1<–>Virtual router 1<–>Virtual router 2<–>VM2With no MPLS I’m getting decent throughput through this topology:[ ID] Interval Transfer Bandwidth[ 3] 0.0-10.0 sec 3.35 GBytes 2.88 Gbits/sec[ 8] 0.0-10.0 sec 3.76 GBytes 3.23 Gbits/sec[ 4] 0.0-10.0 sec 3.84 GBytes 3.30 Gbits/sec[ 7] 0.0-10.0 sec 3.94 GBytes 3.38 Gbits/sec[ 5] 0.0-10.0 sec 1.34 GBytes 1.15 Gbits/sec[ 6] 0.0-10.0 sec 3.94 GBytes 3.38 Gbits/sec[ 9] 0.0-10.0 sec 3.25 GBytes 2.79 Gbits/sec[ 10] 0.0-10.0 sec 3.82 GBytes 3.29 Gbits/sec[SUM] 0.0-10.0 sec 27.2 GBytes 23.4 Gbits/secBut if the the packets between the virtual routers are MPLS tagged, the throughput is only about 10-20%:[ ID] Interval Transfer Bandwidth[ 16] 0.0-10.0 sec 313 MBytes 263 Mbits/sec[ 4] 0.0-10.0 sec 433 MBytes 363 Mbits/sec[ 3] 0.0-10.0 sec 413 MBytes 346 Mbits/sec[ 9] 0.0-10.0 sec 341 MBytes 286 Mbits/sec[ 7] 0.0-10.0 sec 415 MBytes 348 Mbits/sec[ 6] 0.0-10.0 sec 418 MBytes 351 Mbits/sec[ 5] 0.0-10.0 sec 419 MBytes 351 Mbits/sec[ 10] 0.0-10.0 sec 431 MBytes 361 Mbits/sec[SUM] 0.0-10.0 sec 3.11 GBytes 2.67 Gbits/secI have tried running different software for the virtual routers. Both Ubuntu 20.04 and Nokia vSR. In both cases the performance seems to be the same.What Mellanox driver version are you using?If you are using Mellanox driver, what is the throughput with Inbox driver? (from native OS)What is the Kernel version?What is the current FW of the ConnectX5 25Gbs HCA card?What type of switch(s) these KVM servers connect to?Are you using TCP/UDP?What is your Packet layout?What is the MTU?What is the packet size(s)?Note: In our MLNX_EN version 5.x we added the following feature:https://docs.mellanox.com/display/MLNXEN501000/Changes+and+New+FeaturesMPLS TrafficAdded support for reporting TSO and CSUM offload capabilities for MPLS tagged traffic and, allowed the kernel stack to use these offloads.you can validate offloads parameters via the ethtool -k Sophie.We have tried this with the inbox driver that comes with Ubuntu. Version 5.0. We also tried a newer Mellanox driver (5.1-1.0.4)The Kernel version on the host is 5.4.73. On the Ubuntu guest it is 5.4.0.The firmware version is firmware version: 16.28.2006All the tests are run from machines with 1500 MTU using TCP. The packet size is 1470.I don’t see the offload parameters mentioned in ethtool. Here is the output of ethtool -k on one of the hosts:Features for ens10f0np0:rx-checksumming: ontx-checksumming: ontx-checksum-ipv4: off [fixed]tx-checksum-ip-generic: ontx-checksum-ipv6: off [fixed]tx-checksum-fcoe-crc: off [fixed]tx-checksum-sctp: off [fixed]scatter-gather: ontx-scatter-gather: ontx-scatter-gather-fraglist: off [fixed]tcp-segmentation-offload: ontx-tcp-segmentation: ontx-tcp-ecn-segmentation: off [fixed]tx-tcp-mangleid-segmentation: offtx-tcp6-segmentation: onudp-fragmentation-offload: offgeneric-segmentation-offload: ongeneric-receive-offload: onlarge-receive-offload: offrx-vlan-offload: ontx-vlan-offload: onntuple-filters: offreceive-hashing: onhighdma: on [fixed]rx-vlan-filter: onvlan-challenged: off [fixed]tx-lockless: off [fixed]netns-local: off [fixed]tx-gso-robust: off [fixed]tx-fcoe-segmentation: off [fixed]tx-gre-segmentation: ontx-gre-csum-segmentation: ontx-ipxip4-segmentation: ontx-ipxip6-segmentation: ontx-udp_tnl-segmentation: ontx-udp_tnl-csum-segmentation: ontx-gso-partial: ontx-sctp-segmentation: off [fixed]tx-esp-segmentation: off [fixed]tx-udp-segmentation: onfcoe-mtu: off [fixed]tx-nocache-copy: offloopback: off [fixed]rx-fcs: offrx-all: offtx-vlan-stag-hw-insert: onrx-vlan-stag-hw-parse: off [fixed]rx-vlan-stag-filter: on [fixed]l2-fwd-offload: off [fixed]hw-tc-offload: offesp-hw-offload: off [fixed]esp-tx-csum-hw-offload: off [fixed]rx-udp_tunnel-port-offload: ontls-hw-tx-offload: off [fixed]tls-hw-rx-offload: off [fixed]rx-gro-hw: off [fixed]tls-hw-record: off [fixed]I forgot to mention. The switches in this scenario are HPE (Mellanox) SN2100MPowered by Discourse, best viewed with JavaScript enabled"
486,what-is-the-theoretical-throughput-with-single-port-100gbe-mellanox-nic,"I’m using the MCX515CCAT Single Port 100GbE card on two VMs with passthrough configured, and the two NICs are connected via a cable back-to-back. When I use only one VM to send traffic to another, the throughput on the receiver side achieves approximately 100Gbps throughput. However, when I send traffic from both VMs, the received throughput decreases on both sides. And it seems to add up the two received throughputs is the final 100Gbps. Does 100Gbps mean that the uplink throughput plus the downlink throughput equals 100Gbps? If so, why called full-duplex?It should be full-duplex.
something in the configuration is probably not optimalHi @dwaxman,Thank you so much for your fast reply. Could you please recommend me some of the perf tools that can help me profile and identify the potential bottlenecks?Thanks,
LoganPowered by Discourse, best viewed with JavaScript enabled"
487,osm-sa-mad-ctrl-unbind-err-1a11-no-previous-bind-error-messages-are-logged-suddenly,"Below error messages are logged suddenly.​osm_sm_vendor_bind: ERR 5426: Unable to register class 129 version 1osm_sm_mad_ctrl_bind: ERR 3118: Vendor specific bind failedosm_sm_bind: ERR 2E10: SM MAD Controller bind failed (IB_ERROR)perfmgr_mad_unbind: ERR 5405: No previous bindosm_congestion_control_shutdown: ERR C108: No previous bindosm_sa_mad_ctrl_unbind: ERR 1A11: No previous bind​I searched about these messages and found that messages are logged when starting opensm service.​In my case, opensm service was already running and there is no action for stop/start opensm service.So, I do not know the reason of these messages.I am wondering if you can share about any information for my case.Thank you.​​​​OFED version is 4.6.1.0.1GUID in opensm.conf and HCA adapter are same.Hello Jae,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, it seems an entity (user or automated process) tried to start another OpenSM instance on the same node, or start the instance, while the original instance was not fully stopped/killed.Please check the audit logs from the node.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
488,how-to-disable-the-connectx-on-mnv303611a-edlt,"The ConnectX temperature on my MNV303611A-EDLT card is very hot (95 C). How can I disable it? I just need to use the FPGA.I am unaware of any way to do this. The ConnectX-5 IC includes a PCIe switch that hosts the FPGA and the two GbE interfaces so it cannot be fully disabled. From experience, it gets MUCH hotter if either of the ethernet interfaces are in use.Your best bet is to come up with a Cooling Solution. The Innova-2 is designed for use in servers with 300-800 LFM of airflow.
Innova2_Cooling_Solution665×536 82.7 KB
Consider exploring the ConnectX registers. Dump them all with the following command:POWER_SETTINGS and PORT_STATE_BEHAVIOR seem like they may be useful.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
489,we-want-to-install-mlnx-ofed-linux-4-7-3-2-9-0-ubuntu18-04-x86-64-tgz-on-ubuntu-18-04-5-but-cannot-install-ok-mlx5-is-there-anyone-know-this-issue-and-how-to-fix-it-we-want-to-use-verbs-exp-functions,"Installing mlnx-ofed-kernel-utils-4.7…Installing mlnx-ofed-kernel-dkms-4.7…Error: mlnx-ofed-kernel-dkms installation failed!Collecting debug info…See:/tmp/MLNX_OFED_LINUX.37031.logs/mlnx-ofed-kernel-dkms.debinstall.log/tmp/MLNX_OFED_LINUX.37031.logs/mlnx-ofed-kernel-dkms.make.logRemoving newly installed packages…in make.logchecking for Linux sources… /usr/src/linux-headers-5.4.0-42-genericchecking for /usr/src/linux-headers-5.4.0-42-generic… yeschecking for Linux objects dir… /usr/src/linux-headers-5.4.0-42-genericchecking for /boot/kernel.h… nochecking for /var/adm/running-kernel.h… nochecking for /usr/src/linux-headers-5.4.0-42-generic/.config… yeschecking for /usr/src/linux-headers-5.4.0-42-generic/include/generated/autoconf.h… yeschecking for /usr/src/linux-headers-5.4.0-42-generic/include/linux/kconfig.h… yeschecking for build ARCH… ARCH=, SRCARCH=x86checking for cross compilation… nochecking for external module build support… configure: error: unknown; check config.log for detailsFailed executing ./configureBuilding kernel modulesKernel version: 5.4.0-42-genericHello Hongbin,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, you are trying to install MLNX_OFED 4.7-3.2.9.0 on kernel version 5.4. The highest supported kernel version for this driver version is kernel version 5.3.We recommend to use MLNX_OFED version 4.9-3.1.5.0, which will support your kernel version and the verbs_exp functions as this is the LTS version for Experimental Verbs.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
490,how-to-use-connectx-6-dx-data-at-rest-feature,"The data sheet for the connectx6 dx card )https://network.nvidia.com/sites/default/files/doc-2020/pb-connectx-6-dx-en-card.pdf) references Data-at-rest AES-XTS encryption and decryption.
I am having problems finding information on how to use this feature.
I did not find any references to data at rest in the MLNX_EN documentation or the ConnectX-6 card user’s manual. Does anyone know where I can find more information on this feature and in what configuration it can be used under?I am using the connectx-6 DX cards in Supermicro server running Oracle Linux 8.5.We do not have the API as GA yet, but we can go in to POC in a few months.Powered by Discourse, best viewed with JavaScript enabled"
491,how-to-set-compiled-kernel-modules-as-persistant-like-lustre-client,"Hi FolksI have to connect my nodes to external lustre server , every time I compile lustre clients separately on multiple nodes and when nodes reboots , all configuration goes off .Hi,After installing the lustre client packages on the the test node, you’ll need to grab the image. Please refer to “5.6 Updating Running Nodes”, “Updating A Stored Image From A Running Node” in the admin manual.Once the changes are grabbed to the image, you can reboot the node to check that all the changes applied to the node are persistent.Kind regards,
adelcan we add lustre client to module load xxx ?Hi Adel , will look into it , thanks for sharing .
I wanted to know if we can add lustre client to → “module load xxx” ?Hi,Kernel modules should be loaded on boot time. The ‘module load’ command is mainly used to automatically edit env variables. You’ll need to add /etc/modprobe.d/lustre.conf in the software image of the nodes to let the lnet module use a particular interface to reach the MGS server. A line similar to the following should work if you’ll be using IB interfaces:options lnet networks=“o2ib2(ib0)”Kind regards,
adelHi AdelUpdated the image and then rebooted using section 5.6 , boot failed.
How to revert back ?
image1652×515 121 KB

image1687×867 107 KB
I mistakenly did this and ran image update .
now the option to delete is greyed out.
image1913×259 27.9 KB
Hi,There is no way to revert the changes that you have grabbed from a compute node to the software image unless you have made a clone of the image before grabbing the changes.You don’t need to add the lustre.ko in the list of kernel modules. The lustre kernel module will be added once the system attempts to mount the lustre filesystem.Also, the error says that the NFS kernel module is missing and probably has nothing to do with grabbing the image. My guess, from the screenshot, is that you have added kernel modules to the category of the nodes which has overridden the kernel modules from the image itself. You need to clear out the kernel modules from the category and just keep the kernel modules in the image.Kind regards,
adelkernel module is missing and probably has nothing to do with grabbing the image.Cleared the category and issue seems to be fixed but provisioning is taking lot of time
image1570×840 101 KB


image1590×886 102 KB
Hi Adel ,
Using Auto option and formatted all the storage devices on the node , still failing with this error . This node was working fine until i decommissioned it and starting to reinstall again . Please refer the attached screenshot
image1674×877 101 KB
I did Grab to image on running node , used a cloned image to grab the changes , made another category for the changed image and rebooted the node , I was not able to get lustre modules which i installed .
Please have a look at screenshots
image1542×815 74.3 KB

image865×127 2.82 KB
Hi,The screenshots don’t say much about what may have gone wrong. If you have installed the lustre modules in the kernel shown in the screen shot and the grabimage image went well, then you should be able to find the lustre.ko under the same location. My guess is that you may have either not installed the lustre modules or you may have installed them for a different kernel. If you have installed the modules for different kernel, then you’ll need to make sure that the kernel itself is installed and the kernelversion of the software is set to the kernel version for which you have installed the lustre modules.Kind regards,
adelPowered by Discourse, best viewed with JavaScript enabled"
492,connectx-3-in-windows-10-disable-jumbo-packets,"I am building a 10Gbe network and want to start with Jumbo Frames turned off and then try enabling it and see what happens as I have a lot of devices that can’t do jumbo frames. In Windows 10 it’s a freeform field that won’t let me set it to 0 which I thought would disable it. Do I have to set it to 1500 or something or is there a way to “disable” it?Hello John,Thank you for posting your inquiry on the NVIDIA Networking Community.Please review the UM provided for this driver through the following link → https://network.nvidia.com/related-docs/prod_software/MLNX_VPI_WinOF_User_Manual_v5.35.pdfSearch for ‘Jumbo’ and it will provide you the information you need.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
493,mellanox-connectx-4-lx-made-in-india-legit,"I recently purchased three “Mellanox ConnectX-4 Lx (MCX4121A-ACAT)” and was quite surprised all three “Made in India”. So far I was only aware of “Made in China” and “Made in Isreal” for Mellanox products. One PCB looks quite different then the others … although same origin, batch, time.Are those legit - any way to check that?
IMG_98031920×1440 362 KB
Have you purchased ConnectX-4 are purchased by NVIDIA(Mellanox) Distributors?NVIDIA networking solutions are sold worldwide through a network of authorized distributors.If yes, all products are legit.
If not, we have to check the warranty of those ConnectX with the serial number.How do I check warranty - just write the serial numbers to support?You can check the warranty from the NVIDIA Enterprise Support Portal site.
https://enterprise-support.nvidia.com/s/Please check it from
Enterprise Support Portal > Networking Support Bot > Check My CoverageOk says the following for all three serial numbers:Your product is covered by the #Factory Repair warranty until <printed date + 12mo>.So I guess it seems genuine.Thanks for the tip!Powered by Discourse, best viewed with JavaScript enabled"
494,adding-nodes-to-jobqueus-slurm,"I can’t find in the documentation how I add nodes to jobques.
I have created a job queu via wlm[slurm[->jobsqueues->program_001
I then
category use compute
roles
assign slurmclient
or use slurmclient (if exists)
append queues program_001
commitBut when I show program_001 under jobques, it shows nothing in the Nodes field, and when I do a sinfo or squeue, the nodelist is empty. Jobs sit in the pending state because they have no nodes to go to.
What did I miss?Powered by Discourse, best viewed with JavaScript enabled"
495,ibverbs-on-windows,"Hello,
I installed devx sdk and got the mlx5_verbs, however did not get the ibv_create_qp API.
Any suggestions?Please follow below to enable DevX,https://docs.nvidia.com/networking/display/winof2v30/DevX+InterfaceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
496,wake-on-lan-wol-mcx311a-xcat-not-supported,"I tried the solution posted here:However, it says that it’s not supported by the device. Here is info about the system and output from the attempt to enable WOL.

ethtool -I enp2s0
mlxup
mst status
mst status -vAttempt to enable WOL:The same thing happens if I try WOL_MAGIC_EN_P2, but to be clear, this is a single port board.
The board itself doesn’t have any other branding on it, but it was purchased on eBay, so I can’t 100% say where it came from. It doesn’t seem to be OEM, but is there a way I can tell via software?Also, power on by PCI-E is enabled in the BIOS.
Asus P8Z68-V LX motherboard (BIOS 4105).
Ubuntu 20.04.4, 5.4.0-110-generic x86_64
mst, mft 4.20.0-34, built on Apr 25 2022, 20:48:31. Git SHA Hash: 62bbc33Are there other tests I can run,  or versions I can try?Hello,Thanks for contacting us.
This feature is only supported on an old MFT version for ConnectX-3 (MFT 4.9)
That version is no longer supported and a case needs to be opened in the support portal to help in this issue.Thanks,
Ilan.Wow. That sounds horrible. Is there a reason that WOL isn’t included anymore? Or is there a thread I can read to catch up on the details?I’ll open a support case, to see what happens, but I’ll probably just find another brand of card to use if there isn’t going to be support for a common feature. Especially if it existed and then was removed (but maybe that’s just because I can’t understand why it happened).Hello, what do you mean by “That version is no longer supported” in the context of “an old MFT version for ConnectX-3 (MFT 4.9)”?According to a different thread, there is still a plan of releasing support on new kernel version:
The name ‘LTS’ should also suggest a long turn support commitment, isn’t it?However, I do wish somebody inside the company can update on a more clear release schedule.
The other thread promised a new release on June will get Debian 11.2 covered (5.10 based kernel).
And June is about to leave now :-)BTW, forgive my worry on the release schedule, because of the prior record on missing the schedule for a loooooong time:https://mymellanox.force.com/mellanoxcommunity/s/question/0D51T000088kry9SAA/connectx3-on-ubuntu-2004Should I expect a driver release soon? Give me a YES, please!Are you able to query the configuration from the NIC successfully?
It appears to be relatively common, some CX311s share the following issue:$ sudo mlxconfig -d /dev/mst/mt4099_pci_cr0  qDevice type:    ConnectX3
Device:         /dev/mst/mt4099_pci_cr0Configurations:                              Next Boot
-E- Failed to query device current configurationI can’t trust anything coming out of the mlxconfig tool if it cannot query the card in the first place…I get the same output:
And to clarify:
I’ve tried resetting the card, etc, but always the same results.I got a CX311 that shares the same problem. But another CX312 is fine.If you really need to change the config, there seems to be a workaround that may work:
https://mymellanox.force.com/mellanoxcommunity/s/question/0D51T00006Y8pY5SAJ/connectx3-en-failed-to-query-device-current-configuration
See the accepted answer in the thread.BTW, while I haven’t attempted the WOL config, if you really need the 4.9 driver version. This can be built on kernel 5.x with some manual fix… Most tricky part is to fix the configure script, which can be borrowed from newer release. I also see some other code change required for 5.10 kernel, but those seems to be relatively easy to work through.Powered by Discourse, best viewed with JavaScript enabled"
497,failed-to-run-ipsec-application-on-mbf2h332a-aenot-bluefield-2-p-series-dpu-card,"Hi,I have a MBF2H332A-AENOT BlueField-2 P-Series DPU card which is Crypto Disabled. While tring to run IPsec application on the arm side, I got this error.Then, I try to configure it manually, however, it truns out there is no such directory ‘/sys/class/net/*/compat/devlink/ipsec_mode’.I am wondering whether I can run this application on my MBF2H332A-AENOT card or not.Thanks,
YiFuPowered by Discourse, best viewed with JavaScript enabled"
498,what-is-the-thermal-design-power-tdp-for-the-mellanox-connectx-4-dual-port-vpi-100-gbps-infiniband-cards,"What is the Thermal Design Power (TDP) for the Mellanox ConnectX-4 dual port VPI 100 Gbps Infiniband cards?I am asking because I am possibly thinking about putting one into the HP T740 thin client (so that it can run OpenSM for my switch) but I need to know what the TDP is for that card (MCX456A-ECAT) so that I would be able to tell and plan on how to manage the system from a thermals perspective.Your help is greatly appreciated.Thank you.Hello Ewen,Thank you for posting your inquiry on the NVIDIA Networking Community.You can find all the specific power details for the ConnectX-4 VPI through the following link → https://docs.mellanox.com/display/ConnectX4IB/SpecificationsThank you and regards,~NVIDIA Networking Technical SupportThank you.Powered by Discourse, best viewed with JavaScript enabled"
499,mlx5-core-bad-op-in-xdpsq,"Can anyone explain this error log?[Tue May 30 05:55:25 2023] ------------[ cut here ]------------
[Tue May 30 05:55:25 2023] netdevice: enp129s0f1np1: Bad OP in XDPSQ CQE: 0xd
[Tue May 30 05:55:25 2023] WARNING: CPU: 2 PID: 0 at drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c:556 mlx5e_poll_xdpsq_cq+0x2c1/0x370 [mlx5_core]
[Tue May 30 05:55:25 2023] Modules linked in: tls ipmi_ssif binfmt_misc nls_ascii nls_cp437 vfat fat intel_rapl_msr intel_rapl_common amd64_edac edac_mce_amd kvm_amd kvm irqbypass ghash_clmulni_intel sha512_ssse3 sha512_generic aesni_intel crypto_simd cryptd rapl pcspkr ast drm_vram_helper drm_ttm_helper ttm drm_kms_helper i2c_algo_bit acpi_ipmi ccp sp5100_tco watchdog rng_core k10temp ipmi_si ipmi_devintf ipmi_msghandler joydev evdev acpi_cpufreq button sg drm fuse loop efi_pstore configfs efivarfs ip_tables x_tables autofs4 ext4 crc16 mbcache jbd2 crc32c_generic mlx5_ib ib_uverbs ib_core hid_generic usbhid hid sd_mod crc32_pclmul dm_mod crc32c_intel ahci xhci_pci libahci mlx5_core xhci_hcd libata nvme usbcore nvme_core t10_pi scsi_mod mlxfw psample crc64_rocksoft crc64 ptp crc_t10dif scsi_common crct10dif_generic i2c_piix4 pps_core usb_common crct10dif_pclmul crct10dif_common pci_hyperv_intf
[Tue May 30 05:55:25 2023] CPU: 2 PID: 0 Comm: swapper/2 Not tainted 6.1.0-9-amd64 #1  Debian 6.1.27-1
[Tue May 30 05:55:25 2023] Hardware name: Supermicro AS -2014TP-HTR-ZC010/H12SST-PS, BIOS 2.4.V1 11/22/2022
[Tue May 30 05:55:25 2023] RIP: 0010:mlx5e_poll_xdpsq_cq+0x2c1/0x370 [mlx5_core]
[Tue May 30 05:55:25 2023] Code: 54 24 24 48 85 c0 8b 4c 24 20 4c 8b 44 24 10 4c 0f 44 ef 4c 89 c2 4c 89 ee 44 89 54 24 08 48 c7 c7 10 21 85 c0 e8 6f 2e 0e d1 <0f> 0b 41 0f b6 44 24 3f 44 8b 54 24 08 c0 e8 04 0f b6 c8 e9 82 39
[Tue May 30 05:55:25 2023] RSP: 0018:ffffbc7c800fcd80 EFLAGS: 00010286
[Tue May 30 05:55:25 2023] RAX: 0000000000000000 RBX: 0000000000000002 RCX: 0000000000000000
[Tue May 30 05:55:25 2023] RDX: 0000000000000103 RSI: ffffffff92b3fa66 RDI: 00000000ffffffff
[Tue May 30 05:55:25 2023] RBP: 0000000000000000 R08: 0000000000000000 R09: ffffbc7c800fcbf0
[Tue May 30 05:55:25 2023] R10: 0000000000000003 R11: ffffa00a4f11ac28 R12: ffff9fcb4b1d3000
[Tue May 30 05:55:25 2023] R13: ffff9fcb33640000 R14: ffff9fcb1ce64240 R15: ffff9fcb55e3d000
[Tue May 30 05:55:25 2023] FS:  0000000000000000(0000) GS:ffffa0094e080000(0000) knlGS:0000000000000000
[Tue May 30 05:55:25 2023] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[Tue May 30 05:55:25 2023] CR2: 00007fbfd5368f38 CR3: 00000001e4240000 CR4: 0000000000350ee0
[Tue May 30 05:55:25 2023] Call Trace:
[Tue May 30 05:55:25 2023]  
[Tue May 30 05:55:25 2023]  mlx5e_napi_poll+0x3c5/0x710 [mlx5_core]
[Tue May 30 05:55:25 2023]  __napi_poll+0x2b/0x160
[Tue May 30 05:55:25 2023]  net_rx_action+0x2a5/0x360
[Tue May 30 05:55:25 2023]  __do_softirq+0xf0/0x2fe
[Tue May 30 05:55:25 2023]  ? handle_edge_irq+0x9f/0x240
[Tue May 30 05:55:25 2023]  __irq_exit_rcu+0xc7/0x130
[Tue May 30 05:55:25 2023]  common_interrupt+0xb9/0xd0
[Tue May 30 05:55:25 2023]  
[Tue May 30 05:55:25 2023]  
[Tue May 30 05:55:25 2023]  asm_common_interrupt+0x22/0x40
[Tue May 30 05:55:25 2023] RIP: 0010:cpuidle_enter_state+0xde/0x420
[Tue May 30 05:55:25 2023] Code: 00 00 31 ff e8 d3 a6 97 ff 45 84 ff 74 16 9c 58 0f 1f 40 00 f6 c4 02 0f 85 25 03 00 00 31 ff e8 c8 62 9e ff fb 0f 1f 44 00 00 <45> 85 f6 0f 88 85 01 00 00 49 63 d6 48 8d 04 52 48 8d 04 82 49 8d
[Tue May 30 05:55:25 2023] RSP: 0018:ffffbc7c801bfe90 EFLAGS: 00000246
[Tue May 30 05:55:25 2023] RAX: ffffa0094e080000 RBX: ffff9fcb31535400 RCX: 0000000000000000
[Tue May 30 05:55:25 2023] RDX: 0000000000000002 RSI: ffffffff92b3fa66 RDI: ffffffff92b18f95
[Tue May 30 05:55:25 2023] RBP: 0000000000000002 R08: 0000000000000002 R09: 000000003677d5f2
[Tue May 30 05:55:25 2023] R10: 0000000000000018 R11: 0000000000068637 R12: ffffffff933a9620
[Tue May 30 05:55:25 2023] R13: 00000a8a776498e0 R14: 0000000000000002 R15: 0000000000000000
[Tue May 30 05:55:25 2023]  cpuidle_enter+0x29/0x40
[Tue May 30 05:55:25 2023]  do_idle+0x20c/0x2b0
[Tue May 30 05:55:25 2023]  cpu_startup_entry+0x19/0x20
[Tue May 30 05:55:25 2023]  start_secondary+0x11a/0x140
[Tue May 30 05:55:25 2023]  secondary_startup_64_no_verify+0xe5/0xeb
[Tue May 30 05:55:25 2023]  
[Tue May 30 05:55:25 2023] —[ end trace 0000000000000000 ]—hi gopinathThis warning mean there’s a MLX5_CQE_REQ_ERR(0xd) return from the card.
Follow this log should have the detail cqe information to show what error happened.
Please contact networking-support@nvidia.com for further investigate.Thank you
Meng, ShiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
500,connectx6-socket-direct-and-vmware,"Hello,I have problems with a Mellanox ConnectX-6 Socket Direct card under VMware 7.0U2. I just can’t get any communication established.The card is connected via DAC cable to two SN2410b running Cumulus Linux. The port on the switch is configured as MLAG.Does anyone know about these cards under VMware and can help, Mellanox support doesn’t know either.GreetingsDanielHello Daniel,Thank you for posting your inquiry on the NVIDIA Networking Community.We noticed that you already have a valid support case open with NVIDIA Networking Technical Support.As we noticed some discrepancies within the documentation, we will continue to assist you through the official documentation.Based on the VMWARE HCL, the ConnectX-6 Socket Direct adapter is not mentioned in the list. We are validating this internal, and will continue to update you through the support case.Thank you and regards,~NVIDIA Networking Technical Support”Powered by Discourse, best viewed with JavaScript enabled"
501,release-notes-for-nvidia-bright-cluster-manager-9-2-9,"Release notes for Bright 9.2-9== General ==
=New Features==Fixed Issues=== CMDaemon ==
=New Features==Improvements==Fixed Issues=== Bright View ==
=Fixed Issues=== Node Installer ==
=Improvements==Fixed Issues=== Cluster Tools ==
=Fixed Issues=== Head Node Installer ==
=Fixed Issues=== cm-kubernetes-setup ==
=Improvements==Fixed Issues=== cm-scale ==
=Fixed Issues=== cm-wlm-setup ==
=Fixed Issues=== cmsh ==
=Fixed Issues=== pythoncm ==
=Improvements=== slurm ==
=Fixed Issues=== slurm22.05 ==
=Improvements=Powered by Discourse, best viewed with JavaScript enabled"
502,what-is-the-root-cause-of-packet-loss-in-the-processing-flow-of-the-function-mlx5-rx-err-handle-dpdk-mlx5-pmd,"When the mlx5 PMD driver receives a packet bigger the mbuf, the vector RX receive function reports an error , then triggers the error handle process,  the mlx5_rx_err_handle is called. In the meantime, many packets are found lost.I want to know what is the root cause of packet loss in the processing flow of the function mlx5_rx_err_handle?p.s.
The mlx5_rx_err_handle function is a new added to commit in June 2019[1].[1]: Added mlx5_rx_err_handle, see this commit extend Rx completion with error handlingHi Colin,Thank you for submitting your question on NVIDIA Community.Unfortunately, there isn’t sufficient information in the post to debug the packet loss. In addition, I would like to request opening a support ticket with additional information regarding the application test case and how you validate packet loss. You can open a support ticket by emailing Networking-support@nvidia.comIn order to debug via support ticket, you will need a valid support contract. Please reach out to contracts team at Networking-Contracts@nvidia.comThanks,
Namrata.Hi,
I think it have fixed, see this: https://git.dpdk.org/dpdk-stable/commit/drivers/net/mlx5?h=20.11&id=aad5672767479652695f199cdb26dcdae7e5f5e6 Powered by Discourse, best viewed with JavaScript enabled"
503,problem-with-ptp-in-a-port-channel-on-mellanox-sn2010-sn2100-switches,"We use 1 Mellanox SN2010 and 1 Mellanox SN2100 Switch (with Onyx X86_64 3.9.3220) and have them connected to each other with 2 100G Fiber Links.
Both switches are used in a enclosed broadcast environment with different VLANs (for Management, for SMPTE2110 Media streams etc.) and PTP on the Media VLAN (PTP generated by a sync generator connected to the SN2100). Now we wanted to create a port channel with both links between the switches. The physical interfaces as well as the port channel are configured as switchport mode trunk and all the vlans allowed.
We don’t use special IP routing on the network. LACP is enabled.The problem is, that once the port channel is activated (the physical interfaces are added to the port channel interface with “channel-group 1 mode active”), PTP doesn’t get through the port channel, and so our media network doesn’t work anymore (because the GM can’t be found by the other switch). Once the physical IF isn’t added to the port-channel anymore, PTP clocks right to the correct GM and everything is working again.It’s also weird, that the running-config shows, that ptp is enabled on the physical IFs as well as on the port channel, but the command "" show ptp interface port-channel 1"" replies, that there is no ptp enabled on the port-channel.Does anyone know, what the problem might be in our case?
LMK if more detailed information is needed.Greetings from Germany,
FlorianHey Florian,
This sounds like an order of operations issue with enabling PTP on a port-channel. Documented in issue 2306061.
image1146×258 26.4 KB
Can you blank out the interface config, add interface to port-channel and then enable PTP?CharlesIf this does not solve the issue I suggest upgrading to the latest code as there are quite a few fixes for PTP.Lastly if you still need help after these suggestions this is probably best suited for a support ticket to get a deeper level of analysis on it.Hi Charles,I am a colleague of Florian. He is on vacation at the moment and that’s why I am reporting here.
Thank you very much for your solution suggestion. We will try this and give a short feedback with the result.With best regards
MarkusHi Charles,
We reconfigured the port-channel and enabled the ptp after configuring the port-channel on the interfaces. You were right and everything is working perfectly!
Thank you for your answer and your knowledge!Greetings from Germany
FlorianHello Florian,Great news. I am glad to hear things are working.CharlesPowered by Discourse, best viewed with JavaScript enabled"
504,mlx5-pcie-event-detected-insufficient-power-on-the-pcie-slot-27w,"After I restarted the OFED driver using the command (sudo /etc/init.d/openibd restart ), the kernel log displayed the following information:Why does the error Detected insufficient power on the PCIe slot occur? Currently, I am able to run RDMA user programs normally, but does the error message indicate that the network card hardware is damaged?In the usual case this event means the PCIe slot hosting the ConnectX doesn’t provide enough power.
You need to review the server’s motherboard user manual and check the PCIe power limits for each of the slots.The card requires 27W for operating in all modes (including when using optical modules which may consume higher power than passive modules) - but as you can see it doesn’t necessarily mean the card won’t be operational.some more info can be seen here:
https://docs.nvidia.com/networking/display/ConnectX6DxEN/TroubleshootingThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
505,cannot-run-rxpbench-on-host,"I am trying to run RXPbench on a linux machine and compare the result with running RXPbench on a Bluefield-2 DPU with REgex acceleration. Due to some reasons, the linux machine does not have a Bluefield DPU and is only equipped with a mellanox NIC(shown below).
a1:00.0 Ethernet controller: Mellanox Technologies MT28908 Family [ConnectX-6]After installing the latest doca-tools and latest mlx5 driver on the linux machine, I tried RXPbench but the output shows failure:I am confused about two points:I have confirmed that it is due to DPDK initilization problem of my host machine.Powered by Discourse, best viewed with JavaScript enabled"
506,application-will-only-function-with-2-ports-num-of-ports-0,"some advice?Did you perform the step where you unbind from the sf config driver and bind to the mlx5_core.sf driver?ex)Also, you’ll need en3f0pf0sf4 to be trusted. en3f0pf0sf5 is already trusted.I hadn’t done the “unbind” and “bind” operations and I forgot to set “trust” en3f0pf0sf4.
Thank you.is it working correctly now?Yes it looks like it’s started up. The URL filter application is interactive like this. The docs show you how to create, compile and apply rules.Powered by Discourse, best viewed with JavaScript enabled"
507,install-doca,"hi I’m trying to install doca with the sdkmanager from the command line but despite the connection working correctly I get this message: “failed to get configuration file from the server. Please check your network connectivity”I have same problem…Same here :(Powered by Discourse, best viewed with JavaScript enabled"
508,msn3700c-bios-password-protected,"I installed OS Fedora on switch and lost ONIE installer on hard drive. And so I tried to enter BIOS to enable PXE boot, but it password protected. So i need it, how cat i get default password or and help?​Hello Ivan,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, can you please open a NVIDIA Networking Support ticket by sending an email to support@mellanox.com. We will assist you further through the support ticket.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
509,100gbase-sr2-supported-pcie-ethernet-adapter,"I have an application that requires 100GBASE-SR2 which uses 2x 50Gb/s PAM4.
I’ve narrowed down the transceiver to the MMA1T00-VS. I’m having trouble determining which PCIe HHHL adapter supports 100GBASE-SR2.
The ConnectX-6EN user manual does not say it supports 100GBASE-SR2, is that a typo?.Can I use one optical transceiver and break out 2 channels of 100GBASE-SR2 using a breakout cable or will I need 2 transceivers and only get only one 100Gbe port out of each qsfp56 cage.https://network.nvidia.com/related-docs/prod_cables/PB_MMA1T00-VS_200GbE_QSFP56_MMF_Transceiver.pdfHello jgee,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.The following link will provide you the information about which cables and transceiver is supported for the ConnectX-6 EN adapters → https://docs.nvidia.com/networking/display/ConnectX6Firmwarev20321010/Firmware+Compatible+ProductsThis f/w RN will also provide you the list with the p/n for the most suitable ConnectX-6 EN adapter for your application.Be aware break-out functionality is only supported on our Ethernet and InfiniBand switches. Our adapters do not have this functionality, limited to switch side only, adapter is the end-point.Thank you and regards,
~NVIDIA Networking Technical SupportThank you for replying and explaining the breakout functionality.Hello jgee,Just to reiterate, the ConnectX-6 adapter does not support break-out functionality. This is only supported on the switch side.So trying to breakout the adapter on single lines based on the modules is not working and the functionality does not exist.Thank you and regards,
~NVIDIA Networking Technical SupportHello MvB,
I thought breakout functionality meant to get more than one channel out of a single qsfp56 cage. For example, to get 2 independent 100GBASE-SR2 channels out of one 200GBASE-SR4 transceiver. That is not what I proposed in my previous reply. I wanted only one 100GBASE-SR2 channel per transceiver.I chose the MMA1T00-VS.transceiver because the datasheet says it does 50Gb/s PAM4 and UP to 200Gb/s.ConnectX-6 firmware release notes indicates it will support 100GbE in PAM4 mode which I assume is 100GBASE-SR2.These are the release notes for the NVIDIA® ConnectX®-6 adapters firmware Rev 20.32.1010. This firmware supports the following protocols:I guess what I am asking is will :Thank you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
510,release-notes-for-nvidia-bright-cluster-manager-9-2-6,"== General ==
=  New Features== Improvements == Known issues === CMDaemon ==
=Improvements== Fixed Issues=== Bright View ==
= Fixed Issues === Machine Learning ==
= New Features === cm-kubernetes-setup ==
= Improvements == Fixed Issues === cm-scale ==
= Fixed Issues === cm-setup ==
=Fixed Issues=== cm-wlm-setup ==
=Fixed Issues=== cmha-setup ==
=Fixed Issues=== cmsh ==
=New Features== Improvements==Fixed Issues=== pbspro2022 ==
=Improvements=Powered by Discourse, best viewed with JavaScript enabled"
511,connection-aborted-in-secure-channel-application,"Hello,i am trying to run the secure channel sample application, but receive an error when connecting the host to the server on the BlueField.CLI on the BlueField:CLI on the Host:The documentation mentions firmware version 24.33.0458 or later, which should be met according to ibv_devinfo:Are there any other prerequisites or ways i can get further output about why the connection was aborted?Thank you for your help.Powered by Discourse, best viewed with JavaScript enabled"
512,cumulus-and-debian-updates,"HiI noticed Cumulus ignores newer Debian packages that are available. Is there a way to change this?I’m guessing it does some kind of pinning / weighting to prefer packages from the cumulus repo.Hi ,Please review
https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-50/Installation-Management/Adding-and-Updating-Packages/#add-packages-from-another-repositoryHi, thanks - I had a look at that link.Seems pretty easy to add packages from the main Debian repo - that part works.I’m wondering how I upgrade packages which are in the Cumulus repo and in the Debian one. For example openssl recently had a security update.The Cumulus switch can show me the newer package available in Debian, but it does not want to install it. I think Cumulus does some special magic to keep packages stable which are in the Cumulus repo? Is there a way to force an upgrade in some cases?Powered by Discourse, best viewed with JavaScript enabled"
513,modifying-making-custom-ubuntu-image-backups,"I know the title is confusing, please bear with it…
This is about DOCA/Bluefield-2 -
I have a Dell host machine running Ubuntu 20.04 and this host has two Bluefield2 DPUs installed.
I’ve run the graphical installed and upgraded the firmware and gotten both cards loaded with Ubuntu 20 and the SDK.What I’m completely failing to grasp from the documentation is how I manage the changes I make to the OS on each card.  Ideally I’d like both to be identical with only networking IPs being different on the optical ports.  I’d also like to be able to ‘backup’ and modify these OS installs, perhaps as containers or something.But I can’t understand from the docs just how this is intended to work.  How can I not lose my work and configs with a re-flash, how can I send my list of OS and SW changes to other developers on my team?It would seem like I am supposed to have my own BFB image, but the docs just confuse me about this.Any help?Some examples of what I mean - I need to install certain packages on each card under Ubuntu.  For example, certain python libs.  If I reflash the card, I have to redo this each time from scratch.  Isn’t there a way to build a BFB that contains my custom changes?  If so, where is this documented?There are lots of ways to manage this. If you wanted to create your own BFB image, the process is documented here: GitHub - Mellanox/bfb-build: BFB (BlueField boot stream and OS installer) build environmentThe most popular way to deploy $things right now is by using containers. Being decoupled from the OS and its dependencies avoids a lot of the system administration concerns.Powered by Discourse, best viewed with JavaScript enabled"
514,connectx-7-ethernet-linux-support,"I’ve been trying to get a CX-7 NIC (MCX713106AC-VEA_Ax) to link up in Ethernet mode with RHEL 8.6 without any success. I’ve tried all link speeds from 25G NRZ to 200G PAM-4, with both optical and copper. The mlx5 driver shows all speeds supported, and the lasers in the optical transceivers are lighting up, but no link is occurring. mlxlink is showing that it’s polling, but it is also showing the error:Status Opcode                   : 1048
Group Opcode                    : MNG FW
Recommendation                  : Datapath failed to reach DPactivated after applyI have downloaded the latest versions of OFED and the tools (4.21.0-99), but they might not support CX-7, since it is not listed in the list of supported devices.
Is there a trick to getting the CX-7 to work? This was never a problem with the CX-5 or CX-6.The problem has been resolved with a F/W down-date. The F/W on the NIC was from the production line and needed to be updated (actually downgraded) to the current release. Once the new F/W was installed and the system power-cycled (not just a reboot), it is functioning correctly.can you give some details on which specific versions of fw that you downgraded from and to?  Also, do you know how to put the cx-7 into livefish mode?
Thanks for any help.The card was shipped with 28.98.2402. I downgraded it to 28.34.4000 and now it works. I’m not familiar with livefish mode.Thanks for the extremely fast response!  I have a cx-7 nic that has 28.33.0751 on it and I am trying to upgrade to 28.34.4000 (in the hopes that it might work with vsphere 8.x ).  The issue is that the nic is being recognized as non-encrypted and all of the new fw is signed.  I am getting the following error msg when trying to burn the new fw:-E- Burning encrypted image on non-encrypted device is not allowedThanks again for the quick reply.That sounds like you are trying to use the wrong image. Be sure to select the correct OPN and verify the PSID on the F/W download page. If that is all correct, we might need to wait for someone from nvidia to respond to this topic.Powered by Discourse, best viewed with JavaScript enabled"
515,getting-slow-speeds-on-connectx-4-lx,"I recently installed a Mellanox ConnectX-4 Lx 25GbE NIC in my desktop and also in my server. Because of performance issues, I’ve connected them directly with a DAC cable. The DAC has a 25gbps link.For some reason, I can not get more than 7.3gbps when testing. I’ve repeated the below test many times and end op with the same result.I noticed that the Cpu Util in the output “100%”. What is that figure from? My CPUs on both ends are at 10% max when I’m doing a transfer. I’m wondering if that’s the issue?I’ve written the following Powershell script to adjust the driver parameters:None of the above tweaks seem to affect anything, except for Flow Control, which drops performance when disabled.According to the driver Info, my cards are connected to a PCI-E 8.0GT/s x4 port which should be good for 3.938 GB/s.When copying and pasting using using SMB, the speed starts out at 1.75GB/s, fluctuates a little, and then it drops down to 0, waits for a few seconds, then jumps back up to 1.75. I’m not sure why it keeps disconnecting. Speeds are good, then it drops, so I end up with just 7gbps because it keeps dropping the connection in the middle of the transfers.Hi,
Which WinOF2 version are you using and which MFT version?
Make sure you are using the latest version from this page according to the Windows version:Windows OS Host controller driver for Cloud, Storage and High-Performance computing applications utilizing Mellanox’ field-proven RDMA and Transport Offloads
Also, make sure that latest MFT is installed and then check the link via the mlxlink.exe tool as described in the following user manual:
https://docs.nvidia.com/networking/display/MFTv4240/mlxlink+Utility
The following command line is helpful:
./mlxlink.exe -d  -e -m -cBest Regards,
VikiPowered by Discourse, best viewed with JavaScript enabled"
516,about-create-srq-by-devx-command,"hi, there
which ofed version can support to create srq by using gpu memory via devx command MLX5_CMD_OP_CREATE_SRQ, is there any document to explain how to edit an message field, thankshi jpsun3000SRQ is just for control path packet header process.
There’s no user data store in it, so it’s very small and no need to put it on GPU memory
So we do not have such interface for such operation.Thank you
Meng, Shiif SRQ control path is offloaded to GPU kernel,  could that buffer be GPU memory which is better? by the way, for devx command messages(eg. CREATE_QP), it there any document we can check out that the explanation of every field?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
517,mlx5-ib-problem-whilst-installing-mlnx-ofed-kernel-dkms,"I’m trying to run mlnxofedinstall, and get the following error output, with an error caused by mlx5_ib which I’m unsure of what the cause is. Can anyone help me diagnose the issue? I’ve attached the command line output along with: debinstall.log and make.log. Many thanks in advance and let me know if you need any more information/data. OS is Ubuntu 18.04.5 LTSmlnx-ofed-kernel-dkms.debinstall.log (379 KB)mlnx-ofed-kernel-dkms.make.log (562 KB)command-line-mlnxofedinstall.txt (1.66 KB)The problem seems to be that MLNX_OFED_LINUX-5.1-1.0.4.0 is incompatible with ubuntu kernel: 5.4.0-67-generic. The installation worked with 5.4.0-66-generic.Hello Will,Thank you for posting your inquiry on the NVIDIA Networking Community.This is a known issue, and we fixed it in the upcoming MLNX_OFED 5.3 GA release targeted for the end of March.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
518,where-are-the-mellanox-support-articles,"All articles are now available on the MyMellanox service portal.https://support.mellanox.com/s/Powered by Discourse, best viewed with JavaScript enabled"
519,hello-there-is-there-a-possibility-to-tag-a-mirror-port-with-a-vlan-id-like-to-combine-several-mirror-sessions-onto-one-port-but-would-liek-to-be-able-to-identify-the-traffic-from-which-port-it-came-thank-you-for-your-help-thomas,"I’m using a SN2410M. The ports I’d mirroring is Port1-3 and I’d like to bundle and tag them onto Port 49. The mirroring session works fine, but the outcoming traffic is a jumble of all three ports, and I’d love to identify them.With onyx you can a header with a vlan tag:https://docs.mellanox.com/display/Onyxv393202/Port+Mirroring#PortMirroring-header-formatheaderformatPowered by Discourse, best viewed with JavaScript enabled"
520,bad-mad-status,"Hi,I’m trying to understand why I am getting this message in the syslog file on my system.  I have an IB fabric with UFM running subnet manager.  Is there a set of commands that can help me understand why this message is happening and what the impact of it is.Jul 21 15:07:52 gmckee-vm1 srp_daemon[65975]: bad MAD status (110) from lid 0x53
Jul 21 15:07:53 gmckee-vm1 srp_daemon[65968]: No response to inform info registration
Jul 21 15:07:53 gmckee-vm1 srp_daemon[65968]: Fail to register to traps, maybe there is no SM running on fabric or IB port is down
Jul 21 15:07:53 gmckee-vm1 srp_daemon[65968]: bad MAD status (110) from lid 0x53
Jul 21 15:07:54 gmckee-vm1 srp_daemon[65983]: No response to inform info registration
Jul 21 15:07:54 gmckee-vm1 srp_daemon[65983]: Fail to register to traps, maybe there is no SM running on fabric or IB port is down
Jul 21 15:07:54 gmckee-vm1 srp_daemon[65983]: bad MAD status (110) from lid 0x53
Jul 21 15:07:55 gmckee-vm1 srp_daemon[65984]: No response to inform info registration
Jul 21 15:07:55 gmckee-vm1 srp_daemon[65984]: Fail to register to traps, maybe there is no SM running on fabric or IB port is down
Jul 21 15:07:55 gmckee-vm1 srp_daemon[65984]: bad MAD status (110) from lid 0x53
Jul 21 15:07:58 gmckee-vm1 srp_daemon[65980]: No response to inform info registration
Jul 21 15:07:58 gmckee-vm1 srp_daemon[65980]: Fail to register to traps, maybe there is no SM running on fabric or IB port is down
Jul 21 15:07:58 gmckee-vm1 srp_daemon[65980]: bad MAD status (110) from lid 0x53This is caused by the srp daemon running in the node not receiving response.This is a legacy module that is unused and can/should be disabled.Ok , just disabled the srp_daemon service on the node - and the errors have stopped .thanks for the adviceJul 21 15:23:26 gmckee-vm1 systemd[1]: Stopping SRP daemon that monitors port mlx5_0:1…
Jul 21 15:23:26 gmckee-vm1 srp_daemon[65968]: Got SIGTERM
Jul 21 15:23:26 gmckee-vm1 systemd[1]: Stopping SRP daemon that monitors port mlx5_1:1…
Jul 21 15:23:26 gmckee-vm1 srp_daemon[65971]: Got SIGTERM
Jul 21 15:23:26 gmckee-vm1 systemd[1]: Stopping SRP daemon that monitors port mlx5_2:1…
Jul 21 15:23:26 gmckee-vm1 srp_daemon[65975]: Got SIGTERM
Jul 21 15:23:26 gmckee-vm1 systemd[1]: Stopping SRP daemon that monitors port mlx5_3:1…
Jul 21 15:23:26 gmckee-vm1 srp_daemon[65976]: Got SIGTERM
Jul 21 15:23:26 gmckee-vm1 systemd[1]: Stopping SRP daemon that monitors port mlx5_5:1…
Jul 21 15:23:26 gmckee-vm1 srp_daemon[65980]: Got SIGTERM
Jul 21 15:23:26 gmckee-vm1 systemd[1]: Stopping SRP daemon that monitors port mlx5_6:1…
Jul 21 15:23:26 gmckee-vm1 srp_daemon[65983]: Got SIGTERM
Jul 21 15:23:26 gmckee-vm1 systemd[1]: Stopping SRP daemon that monitors port mlx5_7:1…
Jul 21 15:23:26 gmckee-vm1 srp_daemon[65984]: Got SIGTERM
Jul 21 15:23:26 gmckee-vm1 systemd[1]: Stopping SRP daemon that monitors port mlx5_8:1…
Jul 21 15:23:26 gmckee-vm1 srp_daemon[65985]: Got SIGTERM
Jul 21 15:23:26 gmckee-vm1 systemd[1]: srp_daemon_port@mlx5_8:1.service: Succeeded.
Jul 21 15:23:26 gmckee-vm1 systemd[1]: Stopped SRP daemon that monitors port mlx5_8:1.
Jul 21 15:23:26 gmckee-vm1 systemd[1]: srp_daemon_port@mlx5_0:1.service: Succeeded.
Jul 21 15:23:26 gmckee-vm1 systemd[1]: Stopped SRP daemon that monitors port mlx5_0:1.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: srp_daemon_port@mlx5_1:1.service: Succeeded.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: Stopped SRP daemon that monitors port mlx5_1:1.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: srp_daemon_port@mlx5_2:1.service: Succeeded.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: Stopped SRP daemon that monitors port mlx5_2:1.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: srp_daemon_port@mlx5_3:1.service: Succeeded.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: Stopped SRP daemon that monitors port mlx5_3:1.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: srp_daemon_port@mlx5_5:1.service: Succeeded.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: Stopped SRP daemon that monitors port mlx5_5:1.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: srp_daemon_port@mlx5_6:1.service: Succeeded.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: Stopped SRP daemon that monitors port mlx5_6:1.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: srp_daemon_port@mlx5_7:1.service: Succeeded.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: Stopped SRP daemon that monitors port mlx5_7:1.
Jul 21 15:23:27 gmckee-vm1 systemd[1]: srp_daemon.service: Succeeded.Powered by Discourse, best viewed with JavaScript enabled"
521,unable-to-enable-decompression-in-rx,"HiI wanted to enable decompression and compression of wqes , by enabling the flag rx_cqe_compress from ethtool using ethtool  --set-priv-flags enp5s0f1np1 rx_cqe_compress on
I have enabled rx_cqe_compress flag however, when I am sending packets the decompression is not performed.Can someone provide the detail how compression and decompression of the wqes work.
Thank YouHi Osama,We will need to know a bit more info to be able to understand and help with your question
What adapter are you using connectX-4,5,6,7?
What is the OS you are using?
What MOFED version do you have installed?Thanks,
Ilan.Hi llan,I am using connectX-5.
I am using Linux .
I am using MOFED 5.9.Thanks
OsamaHi Osama!The RX CQE Compression feature in NVIDIA network adapters allows the network adapter to compress the size of the received packet completion queue entry (CQE) and transmit it to the CPU. This reduces the amount of data that needs to be transferred from the adapter to the CPU, thereby reducing the amount of data that needs to be processed by the CPU.When a packet is received, the network adapter generates a CQE that contains information about the packet, such as the packet length, packet type, and the location of the data buffer. The RX CQE Compression feature compresses this CQE and sends it to the CPU.When the CPU receives the compressed CQE, it decompresses it and processes it in the same way as a normal CQE. The decompression is performed by the network driver, which uses the same compression algorithm as the network adapter.In order to use the RX CQE Compression feature, both the network adapter and the network driver must support it. Additionally, the ethtool command should be used to enable the RX CQE Compression feature on the network adapter.As you have already enabled the RX CQE Compression feature using the ethtool command, and the driver is the latest, and packets are still not being decompressed, there may be other issues that could be causing the problem. Here are a few things you can try:Verify that the RX CQE Compression feature is actually enabled on the network adapter by running the “ethtool -k ” command and checking if the “rx_cqe_compress” flag is listed as “on”.Check if there are any other conflicting settings that could be affecting RX CQE Compression. For example, some network drivers may have other settings that override the RX CQE Compression setting. You can try disabling these settings to see if it resolves the issue.Check if there are any network configuration issues that could be causing the problem. For example, there could be issues with the switch or router configuration that are preventing the packets from being properly transmitted and received.Finally, if none of the above steps work, please open a case with NVIDIA Support so this can be investigated.Thanks,
IlanHi llan,Thanks,
OsamaPowered by Discourse, best viewed with JavaScript enabled"
522,mellanox-ofed-drivers-for-ubuntu-20-04,"From what link can I download the drivers for Ubuntu 20.04?Hello nbalkanas,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.You can download all NVIDIA adapter drivers through the following link → https://developer.nvidia.com/networking/infiniband-softwareThank you and regards,
~NVIDIA Networking Technical SupportThx,I already visited that link, but thought it was just software description:(
Clicking on the name, downloads it:)BR
NikosHi,I have 1 more question about it.
The format of the driver package has changed a lot.
There are no install scripts or Readme files anymore:(
Is there a place i can read about Ubuntu 20.04 installation?TIA
NikosHello nbalkanas,You will find the UM (which includes the installation procedure) and the latest RN through the following URL → Site Home - NVIDIA Networking DocsClink on Adapter Software → MLNX_OFED InfiniBand/VPI and you will find the User Manual and RN.
image2729×1287 128 KB
Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
523,app-shield-doesnt-work,"I ran the app shield agent on DPU, it said can’t find the device/opt/mellanox/doca/applications/app_shield_agent/bin/doca_app_shield_agent -p 13577 -e hash.zip -m mem_regions.json -o symbols.json -f MT2125X03335MLNXS0D0F0VF1 -d mlx5_0 -t 3 -s linux
[23:56:58:155169][DOCA][ERR][COMMON:184]: Failed to create devinfo representor list. Representor devices are available only on DPU, do not run on Host.
[23:56:58:155788][DOCA][ERR][APSH_APP::Core:441]: Failed to open representor device
[23:56:58:155821][DOCA][ERR][APSH_APP:80]: Failed to init application: Invalid inputPowered by Discourse, best viewed with JavaScript enabled"
524,connectx5-rdma-cm-event-addr-error-error-110,"I face the problem of RDMA_CM_EVENT_ADDR_ERROR, error -110 when I use rping to test the connection of two servers.The error on the client is :There may be address resolve error but I don’t know how to configure it.  Can you tell me what’s wrong with my configuration? Thank you very much.I have configured the network interface as :When I use route -n it shows:I have installed MLNX_OFED_LINUX-5.6-2.0.9.0-ubuntu18.04-x86_64 on my servers.
I also configure MTU to 9000.Powered by Discourse, best viewed with JavaScript enabled"
525,rdma-host-device-performance-during-external-network-communications,"Hi all,
I am working on a BlueField 1 with 25GbE network (MBF1M332A) which is connected to an X86 host over PCIe Gen3 x8.
During the typical ib_write_bw test between host and device a bandwidth of 6.8GB/sec can be achieved, which is obviously limited by the host PCIe bus.
When executing an ib_write_bw test to communicate with another host over the network a bandwidth of around 2.7GB/s can be achieved, limited by the 25GbE interface.Now running both tests at the same time limits the bandwidth between host and device to 2.7GB/s per connection. So it is possible to run two ib_write_bw in parallel between from DPU to host and additionally 2.7GB/s from DPU to an external host.My question now is, why a single connection between DPU and Host is throttled to the 25GbE? My guess is, there is an engine (RDMA or the eSwitch) which is metering/throttling the bandwidth to have the same as the outgoing interface, but is only activated when traffic is flowing externally.  Or maybe some traffic management packets going back, but I haven’t found them yet.
Can someone shed any light on this?Best,
DanielPowered by Discourse, best viewed with JavaScript enabled"
526,connectx-4-5-6-configuring-disable-vlan-stripping-in-windows,"Hello,is it somehow possible to configure a ConnectX-4/5/6 Card to disable VLAN stripping using Windows?
Currently i only found a way using DPDK, but i would also want to disable VLAN stripping without using DPDK.I found a setting for ConnectX-3 Cards using linux/ethtool parameter rxvlan, but nothing for ConnectX-4/5/6 using windows.Kind Regards,
RobertSeem your requirement  can’t be implemented on windows. Other wise Microsoft ND provide API. On linux we have verbs libs API and DPDK can do that.Powered by Discourse, best viewed with JavaScript enabled"
527,im-experiencing-a-performance-issue-on-connectx5-ex-cards-device-id-0x1019-in-the-form-of-a-limit-of-the-packet-rate-to-around-6mpps-with-production-internet-traffic,"I’m using a very special setup that involves a home-grown driver (if you’reinterested, here it is: snabb/connectx.lua at master · snabbco/snabb · GitHub) and I’m not expecting anyone to help me specifically with that, of course. What I’m looking for is a hint what the issue could be based on a few well-defined observations that I’m trying to describe below. I’m a bit desperate by now and any kind of help would be greatly appreciated :)The setup is the following. I have a switch that aggregates production traffic from a set of optical taps in our network onto two 100G ports connected to a server with a ConnectX5-Ex card running my driver. The system is a Supermicro server with an AMD EPYC 7302P 16-core CPU and 16xPCIe4. I assume that PCI performance is not an issue here given the relatively low packet rate. The data rate is also well below 100Gbps. The NIC is only receiving traffic, not transmitting anything.I’m measuring the number of bytes and packets sent by the switch and compare them to the hardware counters on the server. The traffic has the following distribution in packet sizes8396463 : FramesTransmittedLength_eq_641234232229292 : FramesTransmittedLength_65_127683028646837 : FramesTransmittedLength_128_255279265696125 : FramesTransmittedLength_256_511390091671373 : FramesTransmittedLength_512_10232433908602171 : FramesTransmittedLength_1024_1518942049981351 : FramesTransmittedLength_1519_204796848 : FramesTransmittedLength_2048_40959 : FramesTransmittedLength_4096_81910 : FramesTransmittedLength_8192_92150 : FramesTransmittedLength_9216What I observe is that the NIC starts dropping packets when the packet rate is around 6Mpps. I’m using the RFC2863 PPCNT register to measure the bytes and packets, specifically the if_in_octets and if_in_ucast_pkts counter. The interesting thing is that if_in_octets exactly matches the counter of transmitted bytes on the switch but if_in_ucast_pkts is smaller than the corresponding counter on the switch. if_in_discards is always zero.Furthermore, there are no out-of-buffer drops on any of the RX queues (as per the Q_COUNTER facility). In fact, the effect does not depend on the number of RX queues at all. My conclusion is that the packets are all received correctly but some are getting dropped before they are placed on any receive queue (tested with direct and indirect TIRs). Unfortunately, I can’t find any information that would tell me why the packets are beingdropped. The only counter I found that matches the number of dropped packets is the ether_stats_drop_events counter from the RFC2819 PPCNT register but that is very unspecific. Also, I’m not receiving any events on the EQ that would signal any kind ofproblem.Another observation is that the pps limit is also independent of whether I use both ports of the NIC or just one.Some experiments in the lab suggest that the drops are related to packet size. There are no drops below ~100-200 byte packets (measured up to ~30Mpps) and also no drops for packets larger than ~800 bytes. For packets between 200 and 800 bytes I see a varying rate of drops.It looks like some kind of resource is exhausted on the NIC and the most likely reason is that my driver is missing something during initialization (I think I’m doing ALLOC_PAGES correctly and I also don’t get any PAGE_REQUEST events). So, my question to the experts is whether this observation on counters and packet drops rings any bells that could help me identify what’s going wrong or at least point me to the right direction.–AlexHello Alexander,Thank you for posting your inquiry on the NVIDIA Networking Community.As you mentioned, based on your setup, from our side, little support can be provided. But still want to give you some pointers.Even though the packet rate is not high, we still strongly recommend to apply all tuning recommendation based provided in the DPDK performance reports. Even with low packet rate, PCI performance can fluctuate, especially on an AMD platform.From the latest DPDK performance reports for Mellanox adapters, we recommend to apply the AMD and ConnectX-5 Ex tuning recommendations → DPDKMajority of the tuning recommendations for AMD are mentioned in the section for the ConnectX-6(Dx). For the ConnectX-5 Ex, just follow that section.When all is applied, run the same benchmarks as mentioned in the report to create a base line.Then run your own solution and compare the results.Good luck.Thank you and regards,~NVIDIA Networking Technical SupportHello MartijnThank you for taking your time and for the references. I will certainly check those options and also try to perform some PCI performance analysis on the system.One question you might be able to answer irrespective of my actual setup would also help me a lot to better narrow down the bottleneck. Do you know which events exactly contribute to the ether_stats_drop_events counter in the firmware? I imagine that there is only a small number of conditions under which a packet is dropped after reception but before the RX queue. I’d also like to point out again that those dropped packets are accounted for in the number of bytes received but not in the number of packets (in case of the RFC2863 counters), which strikes me as odd. That might also help to identify the place in the ingress processing on the NIC where the drop happens.Also, if you know of any other counters or feedback from the firmware that I could look at that would be very helpful.Thanks,AlexPowered by Discourse, best viewed with JavaScript enabled"
528,in-switch-multiple-entry-showing-for-lldp-output-for-single-interface-which-is-having-mellanox-mlx5-interface-also-mac-address-showing-instead-of-host-name-of-neighbor,"Switch output]>sh lldp neighbors Ethernet 5Last table change time : 0:00:09 agoNumber of table inserts : 80Number of table deletes : 40Number of table drops : 0Number of table age-outs : 1Port Neighbor Device ID Neighbor Port ID TTLEt5 0c42.a16c.734e 0c42.a16c.734c 48Et5 bruat em1 120Server outputUAT [root@bruat~]$ dmidecode | grep -i productProduct Name: PowerEdge R640~]$ modinfo mlx5_corefilename: /lib/modules/3.10.0-1127.19.1.el7.x86_64/kernel/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.ko.xzversion: 5.0-0license: Dual BSD/GPLdescription: Mellanox 5th generation network adapters (ConnectX series) core driverauthor: Eli Cohen eli@mellanox.comretpoline: Yrhelversion: 7.8srcversion: 347B715A7BFEF96508E57F2alias: pci:v000015B3d0000A2D3svsdbcsci*alias: pci:v000015B3d0000A2D2svsdbcsci*alias: pci:v000015B3d0000101Esvsdbcsci*alias: pci:v000015B3d0000101Dsvsdbcsci*alias: pci:v000015B3d0000101Csvsdbcsci*alias: pci:v000015B3d0000101Bsvsdbcsci*alias: pci:v000015B3d0000101Asvsdbcsci*alias: pci:v000015B3d00001019svsdbcsci*alias: pci:v000015B3d00001018svsdbcsci*alias: pci:v000015B3d00001017svsdbcsci*alias: pci:v000015B3d00001016svsdbcsci*alias: pci:v000015B3d00001015svsdbcsci*alias: pci:v000015B3d00001014svsdbcsci*alias: pci:v000015B3d00001013svsdbcsci*alias: pci:v000015B3d00001012svsdbcsci*alias: pci:v000015B3d00001011svsdbcsci*depends: devlink,ptp,mlxfwintree: Yvermagic: 3.10.0-1127.19.1.el7.x86_64 SMP mod_unload modversionssigner: CentOS Linux kernel signing keysig_key: B1:6A:91:CA:C9:D6:51:46:4A:CB:7A:D9:B8:DE:D5:57:CF:1A:CA:27sig_hashalgo: sha256parm: debug_mask:debug mask: 1 = dump cmd data, 2 = dump cmd exec time, 3 = both. Default=0 (uint)parm: prof_sel:profile selector. Valid range 0 - 2 (uint)when we disable the lldp service in the server in switch still the info is showing with mac address instead of host name.]>sh lldp neighbors Ethernet 5Last table change time : 0:00:28 agoNumber of table inserts : 80Number of table deletes : 41Number of table drops : 0Number of table age-outs : 1Port Neighbor Device ID Neighbor Port ID TTLEt5 0c42.a16c.734e 0c42.a16c.734c 48Is this a bug in the driver ?Hello Sandeep,Thank you for posting your inquiry to the Mellanox community.Unfortunately, we do not provide support for inbox (OS vendor provided) drivers.You will need to engage the CentOS community for further assistance.Thanks, and best regards;Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
529,mpi-only-using-1-port-on-dual-port-ib-nic,"I have a test setup with 2 nodes HGX A100.
Both nodes contain 4 cards; MCX653106A-ECAT-SP, they use splitter cables, 8 links to 4 ports on an MQM8700. All 8 ports on both nodes are active @ 100gbps and have been measured at ~linerate.When launching MPI like this:
./mpirun -np 2 --host 10.0.99.245,10.0.99.246 -x NCCL_P2P_LEVEL=PXB singularity exec /env/nvidia_pytorch_22.08.sif ../nccl-tests2/build/all_reduce_perf -g 8 -b 32M -e 2048M -t 1 -n 200 -w 10 -f 2It shows on the switch that only 1 port on the cards get used. I then tried
./mpirun -np 2 --host 10.0.99.245,10.0.99.246 -x NCCL_IB_HCA=mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_6,mlx5_7,mlx5_8,mlx5_9 singularity exec /env/nvidia_pytorch_22.08.sif ../nccl-tests2/build/all_reduce_perf -g 8 -b 32M -e 2048M -t 1 -n 200 -w 10 -f 2Which still results in only mlx5_0, mlx5_2, mlx5_6 and mlx5_8 getting used.
This command:
./mpirun -np 2 --host 10.0.99.245,10.0.99.246 -x NCCL_IB_HCA=mlx5_1,mlx5_3,mlx5_7,mlx5_9 singularity exec /env/nvidia_pytorch_22.08.sif ../all_reduce_perf -g 8 -b 32M -e 2048M -t 1 -n 200 -w 10 -f 2
Shows the same bandwidth and routes traffic over the other ports, indicating all ports work correctly.When running verbose, it shows how GPU0 and GPU1 both find mlx5_0 the best option, is there a way we can set some affinity to let GPU0 run over mlx5_0 and GPU1 over mlx5_1 etc?Hello Bryon,Have you attempted using a preceding ‘=’, as noted in the NCCL_IB_HCA section of the  Environment Variables — NCCL 2.15.5 documentation ? It may be that the match is being performed incorrectly.Also, would recommend experimenting with the port specifier argument as well.If this does not succeed, it may be best to open an issue on the NCCL Github, or engage our support team by creating a ticket via our portal at ESPCommunity for further assistance.Best,
NVIDIA Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
530,trying-to-put-a-bluefield-2-into-bluefield-x-mode-with-power-cycle,"Used sudo mlxconfig -d /dev/mst/mt41686_pciconf0 s PCI_DOWNSTREAM_PORT_OWNER[4]=0xF
to set up X mode on a BlueField 2.   Thenthe DOCA manual says "" Power cycle is required for configuration to take effect. To power cycle the host run:""
however when I run:
/usr/share/bash-completion/completions/ipmitool power cycle
or
sudo /usr/share/bash-completion/completions/ipmitool power cyclenothing seems to happen.   Can I just power cycle the entire server instead?using a separately controlled power cycle worked with the Bluefield going into X-mode.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
531,doca-1-0-is-available-now,"Sign up now for early access!NVIDIA DOCA SDK Early Access Early access to the DOCA software framework is available now. Please login or create an account using an active email address. For updates on DOCA, please opt-in to receive Enterprise and Developer emails from NVIDIA. We...Powered by Discourse, best viewed with JavaScript enabled"
532,building-openmpi-with-ucx-general-advice,"HiI am running into some issues with openmpi compilation and looking for general advice for the setup I described below.The fabric I am working with has 3 Xeon 28-core workstations housing Mellanox ConnectX-3 VPI MCX354A-FCBT NICs and connected through a Mellanox SX6005 switch.I am trying to build OpenMPI so I can compile codes that will essentially use the hardware resources (CPU,RAM) of all 3 workstations.All machines have:ConnectX-3s have latest fw 2.42.5000I want to use intel compilers (2022.01 Intel OneAPI) to compile openMPI and my own codes.I have “successfully” installed the MLNX_OFED_LINUX-4.9-4.1.7.0-ubuntu20.04-x86_64 drivers on all nodes (Linux 5.4.0-26-generic (ubuntu 20.04))I installed the MLNX_OFED_LINUX-4.9-4.1.7.0-ubuntu20.04-x86_64I used ./mlnxofedinstall --forceMaybe this is the root of all issues. I will try to install with apt-get with mlnx-ofed-all option.1-) The overview (https://docs.nvidia.com/networking/display/HPCXv281/HPC-X+Overview) of the HPCX 2.8.1 listsOFED / MLNX_OFED: OFED 1.5.3 and later and MLNX_OFED 4.7-x.x.x.x and later as requirements.In the HPC-X download center for HPCX-2.8.1 I see the options below.hpcx"" data-fileid=""0691T00000GSSRyQAPYou can see in the picture I attached that there is no MLNX_OFED 4.9 listed for HPC-X 2.8.1 as suggested in the HPC-X 2.8.1 overview.Is there such a HPC-X version? If not, which one of the available options should I use? I am currently using hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_642-) Options in OpenMPI compilation–with-ucx=: Build support for the UCX library.–with-mxm=: Build support for the Mellanox Messaging (MXM) library (starting with the v1.5 series).–with-verbs=: Build support for OpenFabrics verbs (previously known as “Open IB”, for Infiniband and iWARP networks).https://www.open-mpi.org/faq/?category=openfabrics says:“In the v4.0.x series, Mellanox InfiniBand devices default to the ucx PML. The use of InfiniBand over the openib BTL is officially deprecated in the v4.0.x series, and is scheduled to be removed in Open MPI v5.0.0.”Also:MXM support is currently deprecated and replaced by UCX.Do I use all the options above or can I configure OpenMPI compilation with just --with-ucx ? I compiled ucx1.10 that’s in the hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/sources/ucx-1.10.0.tar.gz - attached the config log for that build (ucx_1_10_config.log)3-) How about --with-fca?open-mpi.org says: “You can find more information about FCA on the product web page. FCA is available for download here: http://www.mellanox.com/products/fca” but I was not able to find anything on that link.In any case I have tried to configure and compile OpenMPI in hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/sources/openmpi-gitclone.tar.gz./configure CC=icc CXX=icpc F77=ifort FC=ifort --prefix=${HPCX_HOME}/ompi-icc \–with-ucx=/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ucx-1.10.0/build \–with-platform=contrib/platform/mellanox/optimized \2>&1 | tee config-icc-output.logmake allsudo make installI have attached my config output log (config-icc-output.log)I am receiving an error at make install as follows:/usr/bin/mkdir -p ‘/ompi-icc/share/openmpi’/usr/bin/install -c -m 644 help-mpi-common-sm.txt ‘/ompi-icc/share/openmpi’/usr/bin/mkdir -p ‘/ompi-icc/include/openmpi/opal/mca/common/sm’/usr/bin/install -c -m 644 common_sm.h common_sm_mpool.h ‘/ompi-icc/include/openmpi/opal/mca/common/sm’make[3]: Leaving directory ‘/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/opal/mca/common/sm’make[2]: Leaving directory ‘/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/opal/mca/common/sm’Making install in mca/common/ucxmake[2]: Entering directory ‘/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/opal/mca/common/ucx’LN_S libmca_common_ucx.lamake[3]: Entering directory ‘/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/opal/mca/common/ucx’/usr/bin/mkdir -p ‘/ompi-icc/lib’/bin/bash …/…/…/…/libtool --mode=install /usr/bin/install -c libmca_common_ucx.la ‘/ompi-icc/lib’libtool: warning: relinking ‘libmca_common_ucx.la’libtool: install: (cd /home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/opal/mca/common/ucx; /bin/bash “/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/libtool” --silent --tag CC --mode=relink icc -DNDEBUG -O3 -g -finline-functions -fno-strict-aliasing -restrict -Qoption,cpp,–extended_float_types -pthread -version-info 70:0:30 -L/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ucx-1.10.0/build/lib -o libmca_common_ucx.la -rpath /ompi-icc/lib libmca_common_ucx_la-common_ucx.lo -lucp -luct -lucm -lucs /home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/opal/libopen-pal.la /home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/opal/mca/memory/libmca_memory.la -lrt -lz )/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/libtool: line 10657: icc: command not foundlibtool: error: error: relink ‘libmca_common_ucx.la’ with the above command before installing itmake[3]: *** [Makefile:1875: install-libLTLIBRARIES] Error 1make[3]: Leaving directory ‘/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/opal/mca/common/ucx’make[2]: *** [Makefile:2103: install-am] Error 2make[2]: Leaving directory ‘/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/opal/mca/common/ucx’make[1]: *** [Makefile:2420: install-recursive] Error 1make[1]: Leaving directory ‘/home/baird/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/openmpi-gitclone/opal’make: *** [Makefile:1902: install-recursive] Error 1Any help would be greatly appreciated, especially any advice to set up this infiniband fabric I have described in the context!CheersOnurconfig-icc-output.log (207 KB)ucx_1_10_config.log (467 KB)I have solved this problem by adding a .conf file to /etc/ld.so.conf.d/Just added the library folder to the conf file as: “/opt/intel/oneapi/compiler/2022.0.1/linux/compiler/lib/intel64_lin”and then did ldconfigHello,If there are ConnectX-3 cards in the system and you would like to use HPC-X then you can use MLNX_OFED 5.0 branch (the latest MLNX_OFED that supports ConnectX-3) and then use can use the proper HPC-X 2.8.1 package for MLNX_OFED 5.0:https://developer.nvidia.com/networking/hpc-xunder ARCHIVE VERSIONS – > 2.8.1 → MLNX_OFED → 5.0-1.0.0.02.To use UCX you need only --with-ucx. MXM is deprecated and UCX replaces it.3.FCA is deprecated, HCOLL replaces FCA.Best Regards,VikiHi Viki,The reason I was using MLNX_OFED 4.9-4.1.7.0 LTS was because it is the latest released MLNX_OFED that supports Ubuntu 20.04 / mlnx4 and supports a newer ubuntu kernel that’s not in the MLNX_OFED 5.0 versions. Here are the release dates of each relevant release.5.0-1.0.0.0 March 3, 2020 supports mlx4 and ucx 1.8Ubuntu 20.04 (beta) x86_64 5.4.0-12-generic5.0-2.1.8.0 April 6, 2020 supports mlx4 and ucx 1.8Ubuntu 20.04 (beta) x86_64 5.4.0-18-generic4.9-4.1.7.0 December 2021 - mlx4 / ucx 1.8Ubuntu20.04 x86_64 5.4.0-26-genericSo in my situation I used the latest released that supports my mlx4 cards. MLNX_OFED 5.0-x supports an older kernel and in beta. So can you please confirm that I need to be using 5.0-1 ? Maybe I should at least use 5.0-2.1.8 which seems to be an update over 5.0-1.0.0?Hi Viki,update:I have now installed on all 3 of my nodes (with ubuntu 20.04 5.4.0-26-generic kernel):MLNX_OFED_LINUX-5.0-2.1.8.0-ubuntu20.04-x86_64andhpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.0-1.0.0.0-ubuntu18.04-x86_64MPI TESTS works fine on each individual node. My current issue is to be able to use mpirun on my main node (with opensm enabled on that main node) and use the cpu s in the other nodes.Example :I am on oak-rd0-linux (Main node), opensm is running, ibdiagnet does not report any warning or errors and I am trying to test using the cpu on oak-rd1-linux (host1) and oak-rd2-linux (host2) with:mpirun -x LD_LIBRARY_PATH -np 2 -H oak-rd1-linux,oak-rd2-linux $HPCX_MPI_TESTS_DIR/examples/hello_cNothing happens - it seems to hang and I am not sure where to go from here. What am I doing wrong at this step and what can I check to identify to problem?sudo ibnetdiscover output:vendid=0x2c9devid=0xc738sysimgguid=0xe41d2d0300b39ee0switchguid=0xe41d2d0300b39ee0(e41d2d0300b39ee0)Switch 12 “S-e41d2d0300b39ee0” # “SwitchX - Mellanox Technologies” base port 0 lid 3 lmc 0[1] ""H-0010e00001885688""2 # “oak-rd0-linux HCA-1” lid 1 4xQDR[2] ""H-0010e000018d08e0""1 # “oak-rd1-linux HCA-1” lid 4 4xQDR[3] ""H-0010e00001885908""1 # “oak-rd2-linux HCA-1” lid 2 4xQDRvendid=0x2c9devid=0x1003sysimgguid=0x10e0000188590bcaguid=0x10e00001885908Ca 2 “H-0010e00001885908” # “oak-rd2-linux HCA-1”1 “S-e41d2d0300b39ee0”[3] # lid 2 lmc 0 “SwitchX - Mellanox Technologies” lid 3 4xQDRvendid=0x2c9devid=0x1003sysimgguid=0x10e000018d08e3caguid=0x10e000018d08e0Ca 2 “H-0010e000018d08e0” # “oak-rd1-linux HCA-1”1 “S-e41d2d0300b39ee0”[2] # lid 4 lmc 0 “SwitchX - Mellanox Technologies” lid 3 4xQDRvendid=0x2c9devid=0x1003sysimgguid=0x10e0000188568bcaguid=0x10e00001885688Ca 2 “H-0010e00001885688” # “oak-rd0-linux HCA-1”2 “S-e41d2d0300b39ee0”[1] # lid 1 lmc 0 “SwitchX - Mellanox Technologies” lid 3 4xQDRPowered by Discourse, best viewed with JavaScript enabled"
533,rdma-performance-improvement-with-vma-offload,"I hope this post finds you well.I am able to see the performance improvement of sockperf through linking the VMA userspace library, however, when I try the VMA offload with an RDMA application like “ibv_uc_pingpong” with IBV_FORK_SAFE set to 1, I’m not able to see any performance improvements. In fact, for the ibv_uc_pingpong application, average latency and throughput with VMA are 3x worse than without VMA.My client and server both have a 100G ConnectX-6 Dx EN adapter card (MT4125 - MCX623106AN-CDAT).MLNX_OFED Version: 4.9-0.1.7.0VMA Version: 9.0.2-1The VMA User Manual mentions that code implemented with the native RDMA verbs API can be run with the VMA library to present the standard socket API to the application. However, there is no performance improvement with the ibv_uc_pingpong application.I’d really appreciate any clarifications, tips, or advice in this regard.Thank you, in advance!Hamedhi Hamed:Thank you for contacting NVIDIA Technologies Technical Support.For VMA performance test, we suggest you follow below guidance:https://community.mellanox.com/s/article/vma-performance-tuning-guideBest RegardsLeveiPowered by Discourse, best viewed with JavaScript enabled"
534,nvidia-gpudirect-on-bluefield2,"Hi all,
Is there a good document on how to enable GPUDirect on Bluefield2 to work with Nvidia A100 GPUs?
We tried doca sdk installation in this document but we failed in section 5. Is it only for the NVIDIA converged cards? or it should work on any Bluefield2 devices as well?NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.Here is the error we are getting when we run
apt-get -y install cudaCreating config file /etc/gdm3/greeter.dconf-defaults with new versiongdm.service is not active, cannot reload.invoke-rc.d: initscript gdm3, action “reload” failed.Setting up nvidia-settings (515.43.04-0ubuntu1) …Setting up network-manager-gnome (1.8.24-1ubuntu3) …Processing triggers for sgml-base (1.29.1) …Setting up sgml-data (2.0.11) …Processing triggers for sgml-base (1.29.1) …Setting up docbook-xml (4.5-9) …Processing triggers for initramfs-tools (0.136ubuntu6.7) …update-initramfs: Generating /boot/initrd.img-5.4.0-1035-bluefieldUnsupported platform on EFI system, doing nothing.Processing triggers for dbus (1.12.16-2ubuntu2.2) …Processing triggers for dictionaries-common (1.28.1) …aspell-autobuildhash: processing: en [en-common].aspell-autobuildhash: processing: en [en-variant_0].aspell-autobuildhash: processing: en [en-variant_1].aspell-autobuildhash: processing: en [en-variant_2].aspell-autobuildhash: processing: en [en-w_accents-only].aspell-autobuildhash: processing: en [en-wo_accents-only].aspell-autobuildhash: processing: en [en_AU-variant_0].aspell-autobuildhash: processing: en [en_AU-variant_1].aspell-autobuildhash: processing: en [en_AU-w_accents-only].aspell-autobuildhash: processing: en [en_AU-wo_accents-only].aspell-autobuildhash: processing: en [en_CA-variant_0].aspell-autobuildhash: processing: en [en_CA-variant_1].aspell-autobuildhash: processing: en [en_CA-w_accents-only].aspell-autobuildhash: processing: en [en_CA-wo_accents-only].aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].aspell-autobuildhash: processing: en [en_GB-variant_0].aspell-autobuildhash: processing: en [en_GB-variant_1].aspell-autobuildhash: processing: en [en_US-w_accents-only].aspell-autobuildhash: processing: en [en_US-wo_accents-only].Processing triggers for libgdk-pixbuf2.0-0:arm64 (2.40.0+dfsg-3ubuntu0.2) …Processing triggers for rygel (0.38.3-1ubuntu1) …Processing triggers for libc-bin (2.31-0ubuntu9.7) …Processing triggers for systemd (245.4-4ubuntu3.16) …Processing triggers for sgml-base (1.29.1) …Errors were encountered while processing:nvidia-dkms-515cuda-drivers-515cuda-driverscuda-runtime-11-7cuda-11-7cudaE: Sub-process /usr/bin/dpkg returned an error code (1)Powered by Discourse, best viewed with JavaScript enabled"
535,hi-how-to-recompile-openvswitch-and-mlnx-dpdk-with-debuginfo-in-mlnx-ofed-version-5-4-1-0-3-0-on-centos7-6,"I have tried some methods such as https://community.mellanox.com/s/article/howto-compile-mlnx-ofed-drivers--160---mlnx-ofa-kernel-example-x ,but cannot pass the test suit of openvswitch.Hi,What do you try to do :1.Recompile openvswitch with debug info ?Once your run boot.sh, you can run configure with the option : --with-debug2.Recompile dpdk with debug info (which version)If your build is based on meson/ninjaYou can choose the buildtypemeson configure -Dbuildtype=debug3.Recompile MOFED with debug info ?Under the source of mlnx-ofa-kernel, you can run configure with --with-debug-infoRegardsMarcPowered by Discourse, best viewed with JavaScript enabled"
536,wol-wake-on-lan-for-mellanox-connectx-4-lx-mcx4121a-acat,"Hi,I was browsing through the documentation and trying to get WOL working with a Mellanox ConnectX-4 Lx card.I just found some older material for ConnectX-3 with “…set WOL_MAGIC_EN_P1=1” (Wake On Lan (WOL) MCX311A-XCAT - not supported?).Is there a way to enable this for my card (MCX4121A-ACAT)? Any support / tutorial material available … ?I do not have a CX4 LX handy to check though, first validate if the “WOL_MAGIC_EN_P#” present via the configurable parameters with our mlxconfig utility. Then check if if enable or disable by default.
You can use the same utility to set the parameter if available.The NVIDIA Firmware Tools (MFT) package is a set of firmware management toolsmst start
mst status -v (list of mst devices)
mlxconfig -d  q | grep WOL_MAGIC_EN_P*According to our MLNX_OFED driver, you can use the ethtool utility:
Page 121, Wake-on-LAN (WoL)Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)Output of the query “mlxconfig -d mt4117_pciconf0 query” does not show any WOL parameters:Full output of query “mlxconfig -d mt4117_pciconf0 show_confs” see in file:
full-cmd-output.txt (163.1 KB)Line 1421 says something about it:Firmware installed on current card:… which is the latest according to "" ConnectX-4 Lx Ethernet Firmware Download Center"".If I try “mlxconfig -d mt4117_pciconf0 set WOL_MAGIC_EN_P1=1” it outputs:If I understand the support manual correctly the “mlxconfig” recognizes the feature but the firmware used in my system does not support WoL?Powered by Discourse, best viewed with JavaScript enabled"
537,hi-i-am-trying-to-download-updated-drivers-for-my-switches-and-adapters-but-the-web-page-does-not-work-correctly,"I tried with several browsers (firefox, chrome, microdoft edge) and the problem consists. For example, if I try to download drivers for my adapter, I cannot choose from the dropdownlists, the appropriate operating system, version etc.Can you help me?Best regards,GMSorry, I forget to say that I am trying to download from the support.mellanox.com Downloadcenter.GMHello Gero,We checked the Support Portal from our side and currently we are not experiencing any issues.We recommend to open a support case to check s/w entitlement.You can open a support case by sending an email to → networking-support@nvidia.comThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
538,connectx-5-cards-network-interface-disappeared-and-openibd-daemon-failed-on-several-nodes,"OS : Ubuntu 22.10
kernel : 5.19.0-45-generic
Server : ASUS ESC8000-G4
Device : MCX555A-ECAT ConnectX®-5 VPI adapter card, EDR IB (100Gb/s)
Driver : MLNX_OFED_LINUX-5.9-0.5.6.0-ubuntu22.10-x86_64.tgzmy customer operates approximately 8 nodes and 3 of them had an experience like this.Someday network card’s network interface, which comes out when using ‘ifconfig’ command, disappeared and openibd daemon was in failed status. Even that moment, the communication(ping 192.168.10.x) still worked. But after few days, the communication stopped finally.I resolved this deleting and reinstalling the driver with ‘–add-kernel-support’ option but still dont’ understand why this happened. Please help.IMG_15321920×1072 216 KB
IMG_1533.HEIC4032×3024 3.39 MBFrom the logs, likely driver kernel module not correctly load, or you install old version not uninstall. This kind of issue can resolve by reinstall.thanks for your advice.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
539,connectx-5-rss-maps-same-hash-to-different-cores,"Hi all! I am trying to setup a Mellanox card’s RSS and get seemingly illogical results.TLDR: I want to configure a NIC’s RSS so that it sends all UDP packets to a single CPU core. (needed for some research benchmarks / experiments I am running)Problem: Although I set the RSS hash function to use only IP destination address, packets from different client machines go to different cores. Packets from the same machine go to the same core.Steps:Any idea what might be going on?
Video for reference: Mellanox ConnectX-5 RSS not working as expected - YouTubeThe rx-flow-hash also calculate on L4 port,L4 bytes 0 & 1 [TCP/UDP src port]
L4 bytes 2 & 3 [TCP/UDP dst port]Also you can try set “n”,ethtool -N ens1f0np0 rx-flow-hash udp4 d nWhat OS/ethtool version?
Chcek the ring and hash rss enable by,ethtool -l eth_dev,
ethtool -x eth_devAnd could you check set rules like,ethtool -N eth_dev flow-type udp4 src-ip s-ip dst-ip d-ip action 1man page of ethtoolhttps://man7.org/linux/man-pages/man8/ethtool.8.htmlaction N will map to RXQ then RXQ map to CPU coreThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
540,connextx5-tc-flower-offload-between-ports-on-dual-port-nic,"We’re using dual port 25gbe NIC and have a need to pass traffic from the first port to the second port while making a small transformation of the packets (change vlan).  We will use linux tc (on RHEL8) to do this.Currently it works but it does not offload (does not indicate “in_hw”).Is offload supported when moving packets between the two PFs on the same NIC or is each PF completely isolated, requiring a trip up to the host and back?We are on MLNX_OFED_LINUX-5.8-1.0.1.1-rhel8.6-x86_64
Adapter is on F/W 16.35.1012Thanks!
-JNIC eSwitch device base on PCI bus, 2 PF can not share same eSwitch.You also can check by “devlink dev show” "" devlink dev eswitch show DEV""This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
541,doca-example,"When I was trying to run the url example using the json provided, there is an error indicating this way, does anyone know why is this happening.

image1177×305 13.1 KB

When I tried to run the url example interaction terminal, it succeeds.

image1140×340 12 KB
And after I used the mst configuring, it suggests this way

image1124×402 14.8 KB
Powered by Discourse, best viewed with JavaScript enabled"
542,nvidia-mellanox-mcx512a-acat-connectx-5-en-network-interface-card,"Hello,
I just installed NVIDIA Mellanox MCX512A-ACAT ConnectX®-5 EN Network Interface Card in Dell Percession 3930R on windows 10 pro and I am getting a:This device cannot start. (Code 10)
{Operation Failed}
The requested operation was unsuccessful.in device Manager.things I have Tried:
installed the driver WinOF-2Things I did not do:
update Firmware (I need to check current firmware version)any suggestions how to fix this issue?Hello t8z5h3,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.If you install the latest WinOF-2 driver, the f/w will be upgraded automatically during driver install. In case you adapter is an OEM adapter (HPE/Dell/SM/Lenono), this f/w upgrade will not happen and needs to be done manually as you need to download the correct f/w image through the OEM website.Instructions on how-to burn a new f/w manually on the adapter you can find through the following link → https://docs.nvidia.com/networking/display/MFTv4240/Burning+a+Firmware+ImageThank you and regards,
~NVIDIA Networking Technical SupportI did manually update the image to the last version.
Does the connectx-5 have to be in a pci-e 3.0 8x wired slot or can it run in a pci-e  3.0 4x wired slot?The adapter you have is a PCI 3.0 x8 adapter (https://network.nvidia.com/files/doc-2020/pb-connectx-5-en-card.pdf)It needs to be in a x8 slot. x4 does not work.the attached Document on your last post, Page 3 (top) at the PCI Express interface Section 4th point down:
Auto-Negotiates to X16,x8,x4,x2,x1 lane(s)based on this: https://www.dell.com/support/manuals/en-ca/precision-3930r-workstation/precision3930_rack_setup_and_specs/system-board-connectors?guid=guid-c04bd59c-1619-4c64-93f2-52308d13f5a3&lang=en-usI need to move the ConnectX-5 card from Riser 2 to Riser 1A. correct?You were right. It needs a pci-e x8 connectionThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
543,verbs-exp-h-no-such-file-or-directory,"my current environment is as below:ubuntu16.04MLNX_OFED_LINUX_5.1-2.3.7.1-ubuntu16.04-x86_64#ibstatusInfiniband device ‘mlx5_0’ port 1 status:default gid: fe80:0000:0000:0000:9a03:9bff:fe83:6562base lid: 0x0sm lid: 0x0state: 1: DOWNphys state: 3: Disabledrate: 40 Gb/sec (4X QDR)link_layer: Ethernet​Infiniband device ‘mlx5_1’ port 1 status:default gid: fe80:0000:0000:0000:9a03:9bff:fe83:6563base lid: 0x0sm lid: 0x0state: 4: ACTIVEphys state: 5: LinkUprate: 100 Gb/sec (4X EDR)link_layer: Ethernet​​but when i complie some code, there is some errors as below:​pingpong.h:37:34: fatal error: infiniband/verbs_exp.h: No such file or directory#include <infiniband/verbs_exp.h>^compilation terminated.In file included from pingpong.c:33:0:pingpong.h:37:34: fatal error: infiniband/verbs_exp.h: No such file or directory#include <infiniband/verbs_exp.h>​i find there is only no verbs_exp.h in /usr/include/infiniband, how can i resolve it ?Hello Liu,Thank you for posting your inquiry to the Mellanox Community.The ‘_exp’ suffix denotes an experimental library.Starting with MLNX_OFED v5.1, we removed support for experimental libraries - these are now supported in the 4.9LTS release chain.In order to successfully compile your code, either:Modify the code to work with the upstream libraries (/usr/include/infiniband/verbs.h)Install MLNX_OFED v4.9LTS in order to continue utilizing experimental libs.Best regards,Mellanox Technical Supporti also find when i use ​MLNX_OFED_LINUX_4.6-1.0.1.1-ubuntu16.04-x86_64, i can find verbs_exp.h，so where is verbs_exp.h in MLNX_OFED_LINUX_5.1?Hi Sam,I am trying to use the CORE-direct and send-with-calc feature using experimental libraries with connectX-4 card. Do we have a concrete document about how to use them? And is vector send-with-calc support now? How are we supposed to use them? My email is leepengqiu@gmail.com.You can send to my email if you want. Looking forward to your reply!Best,Pengqiu LiPowered by Discourse, best viewed with JavaScript enabled"
544,can-the-mcx515a-ccat-which-has-1-qsfp-port-be-setup-in-a-loopback-mode,"I have the MCX515A-CCAT card in my computer essential in a configuration where the TX  of the QSFP is conneceted through a system that loops back to the RX port on the QSFP, Is there a way to run Iperf or some Data rate test along this link?hi amillsFor your test, you need at least 2 cards.(connect 2 cards and each card in different VM or namespace)With 2 cards now i am running into the issue of getting iperf3 to run at full bandwith. 1 side runs it at 25Gbps and the other runs at about 15. Do you have any idea of what to do to solve this?Powered by Discourse, best viewed with JavaScript enabled"
545,mlx5-crypto-not-enough-capabilities-to-support-crypto-operations-maybe-old-fw-ofed-version,"Hi ,
I am testing mlx5 crypto driver/performance but the crypto device seems still disabled.
platform =>doca 1.5test on dpdk-22.11 (test with only doca 1.5 also have the same problem)
root@localhost:/home/ubuntu/bf-build/dpdk/dpdk-22.11# env LD_LIBRARY_PATH=./build/drivers/  ./build/app/test/dpdk-test -c 1 -n 1 -a 03:00.0,class=crypto
EAL: Detected CPU lcores: 8
EAL: Detected NUMA nodes: 1
EAL: Detected static linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: VFIO support initialized
EAL: Probe PCI driver: mlx5_pci (15b3:a2d6) device: 0000:03:00.0 (socket -1)
mlx5_crypto: Not enough capabilities to support crypto operations, maybe old FW/OFED version?
mlx5_common: Failed to load driver crypto_mlx5
EAL: Requested device 0000:03:00.0 cannot be used
EAL: Bus (pci) probe failed.
APP: HPET is not enabled, using TSC as default timerflint -d /dev/mst/mt41686_pciconf0 q full
Image type:            FS4
FW Version:            24.35.1012
FW Release Date:       28.10.2022
Part Number:           MBF2M345A-HECO_Ax
Description:           BlueField-2 E-Series DPU; 200GbE/HDR single-port QSFP56; PCIe Gen4 x16; Secure Boot Enabled; Crypto Enabled; 16GB on-board DDR; 1GbE OOB management; HHHL
Product Version:       24.35.1012
Rom Info:              type=UEFI Virtio net version=21.4.10 cpu=AMD64,AARCH64
type=UEFI Virtio blk version=22.4.10 cpu=AMD64,AARCH64
type=UEFI version=14.28.15 cpu=AMD64,AARCH64
type=PXE version=3.6.804 cpu=AMD64
Description:           UID                GuidsNumber
Base GUID:             1070fd03002a949c        8
Base MAC:              1070fd2a949c            8
Image VSD:             N/A
Device VSD:            N/A
PSID:                  MT_0000000716
Security Attributes:   secure-fw
Default Update Method: fw_ctrl
Life cycle:            GA SECURED
Secure Boot Capable:   Enabled
EFUSE Security Ver:    0
Image Security Ver:    0
Security Ver Program:  Manually ; DisabledAny response will be appreciated.  .
JackyHi Jacky,Thank you for posting your query on our community. I would like to confirm if you got an opportunity to review the following link which mentions the settings to be applied for crypto to take effect. Specifically ""section 15.2 Configuration "" —> https://doc.dpdk.org/guides/cryptodevs/mlx5.htmlFrom the above mentioned section, it explains the following along with steps to be executed:“When crypto engines are defined to work in wrapped import method, they come out of the factory in Commissioning mode, and thus, cannot be used for crypto operations yet. A dedicated tool is used for changing the mode from Commissioning to Operational”Please ensure the settings are correctly set. If the issue still persists, I would like to request submitting a support ticket for further troubleshooting. The support ticket can be opened by emailing "" Networking-support@nvidia.com ""Please note that an active support contract would be required for the same. For contracts information, please feel free to reach out to our contracts team at "" Networking-Contracts@nvidia.com ""Thanks,
Namrata.It seems  set nothing on plaintext mode.
How could I check the card in Wrapped/Plaintext mode?On the other hand, in case of plaintext mode, there is no need for all the above, DEK is passed in plaintext >>without keytag.root@localhost:~# mlxreg -d /dev/mst/mt41686_pciconf0 --reg_name CRYPTO_OPERATIONAL --get
Sending access register…Hi Jackylu33,The crypto device like it is shown with KEK and DEK means it is using wrapped keys. If you don’t use the crypto device, then it is plaintext.Thanks,
Namrata.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
546,cant-connect-to-new-sn2010-via-console-cable,"Hi,  I have a new SN2010 and trying to connect via console cable.  Have tried putty and tera using 115200 / 8 / 1 / none /none settings as per instructions in box.  I cannot get a connection.  I have tested the cabling/COM port on a ProCurve switch and everything works on that.Also tried DHCP connection.  I can see an ip is assigned (10.0.0.74)  but when I try and connect to https://10.0.0.74 I get an error saying connection refused.  Have tried Edge and Chrome.Any help would be greatly appreciated.BrettAre you using the same console cable that came with the switch?
How did you see that an IP was assigned to the switch mgmt0 port ? in the dhcp server logs?
ssh works?Brett,If you still are running into problems, please open a support case by following instructions at this link:https://support.mellanox.com/s/contact-support-pageRegards,JonPowered by Discourse, best viewed with JavaScript enabled"
547,is-it-possible-to-split-100g-sr4-fiber-in-4-25g-virtual-devices,"Can a ConnectX 6 VPI be configured to appear as 4 virtual functions that are 25G each, using the 4 SR channels of the 100G link under the hood?Hi Tom,Yes. it can work like that.We can use ‘ip link set dev vf 0’ to set speed.More Info about VF can refer to:https://docs.mellanox.com/pages/viewpage.action?pageId=52011200RegardsLeveiPowered by Discourse, best viewed with JavaScript enabled"
548,another-backend-already-attached,"I have a BlueField-2 DPU installed on my server, but when I try to start rshim, it says “another backend already attached”. I don’t even have other PCIe devices attached other than the DPU. What could go wrong?below are the logs of systemctl status rshim:shujunyi@poweredge0-PowerEdge-R740:~$ sudo systemctl status rshim● rshim.service - rshim driver for BlueField SoCLoaded: loaded (/lib/systemd/system/rshim.service; enabled; vendor preset: enabled)Active: active (running) since Mon 2021-09-27 19:17:28 CST; 2min 31s agoDocs: man:rshim(8)Process: 2979 ExecStart=/usr/sbin/rshim $OPTIONS (code=exited, status=0/SUCCESS)Main PID: 3043 (rshim)Tasks: 2 (limit: 6143)CGroup: /system.slice/rshim.service9月 27 19:17:28 poweredge0-PowerEdge-R740 systemd[1]: Starting rshim driver for BlueField SoC…9月 27 19:17:28 poweredge0-PowerEdge-R740 systemd[1]: Started rshim driver for BlueField SoC.9月 27 19:17:29 poweredge0-PowerEdge-R740 rshim[3043]: Probing pcie-0000:5e:00.29月 27 19:17:29 poweredge0-PowerEdge-R740 rshim[3043]: create rshim pcie-0000:5e:00.29月 27 19:17:29 poweredge0-PowerEdge-R740 rshim[3043]: another backend already attachedreturn of ifconfig:br-cd4cb1507b28: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500docker0: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500eno1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500eno2: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500eno3: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500eno4: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500enp94s0f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500enp94s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536veth951e6ee: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500vethd0dd2f0: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500vethdd6b52a: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500shujunyi@poweredge0-PowerEdge-R740:~$ lspci | grep nox5e:00.0 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)5e:00.1 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)5e:00.2 DMA controller: Mellanox Technologies MT42822 BlueField-2 SoC Management Interface (rev 01)Powered by Discourse, best viewed with JavaScript enabled"
549,ib-write-bw-does-not-go-beyond-20g-s-when-using-average-packet-size-512,"Dear community,We tried also some home made tools, with multithreading, and multiple work requests in parallel, and the maximum we could reach is 60G/s.Is this the maximum speed the adapter can reach for this size of packet ?#bytes #iterations BW peak[Gb/sec] BW average[Gb/sec] MsgRate[Mpps]512 10000000 0.00 20.58 5.023455Which card are you use ?we have 100G cards.Please find below more details:e2:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]e2:00.1 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]hca_id: mlx5_0transport: InfiniBand (0)fw_ver: 16.31.1014node_guid: b859:9f03:00c6:c552sys_image_guid: b859:9f03:00c6:c552vendor_id: 0x02c9vendor_part_id: 4119hw_ver: 0x0board_id: MT_0000000012phys_port_cnt: 1port: 1state: PORT_ACTIVE (4)max_mtu: 4096 (5)active_mtu: 1024 (3)sm_lid: 0port_lid: 0port_lmc: 0x00link_layer: Ethernethca_id: mlx5_1transport: InfiniBand (0)fw_ver: 16.31.1014node_guid: b859:9f03:00c6:c553sys_image_guid: b859:9f03:00c6:c552vendor_id: 0x02c9vendor_part_id: 4119hw_ver: 0x0board_id: MT_0000000012phys_port_cnt: 1port: 1state: PORT_ACTIVE (4)max_mtu: 4096 (5)active_mtu: 1024 (3)sm_lid: 0port_lid: 0port_lmc: 0x00link_layer: EthernetHi ,Please refer to the Performance tuning guide .https://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersThanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
550,what-is-the-difference-between-a-vf-and-sf,"I am looking at https://docs.nvidia.com/doca/sdk/scalable-functions/index.html
and it is not clear to me what the difference between an SF and VF is. I see SF provides VF-like functionality without needing SR-IOV enablement on the NIC. If someone could please point out any other difference, I’d appreciate it very much. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
551,mellanox-onyx-inter-vrf-routing-possible,"How do you route traffic to the management port from default and other VRFs?I am not able to ping the default gateway on the management port for some reason.Basically what I am asking is, can my data traffic be routed via the mgmt network?Is MGMT0 in default VRF or mgmt VRF?For example, MGMT0 has address 10.10.10.2, Gateway is 10.10.10.1. Can my traffic on VLAN 10 let say a workstation with IP address 10.10.10.3 which is on the default VRF be able to talk to gateway 10.10.10.1 via the MGMT interface?Is it possible to make this work?Hi Jimmy,The mgmt0 interface on Mellanox switches is a seperate NIC on the device.Traffic from mgmt0 interface cannot be forwarded out the QSFP/SFP data plane ports and the other way around.Regarding the VRF of mgmt0 interface, this depends on the version you are on.Version prior to 3.9.2006 mgmt0 is always on default VRF, 3.9.2006 and newer you have the option to configure vrf mgmt which is special for the OOB management interface mgmt0/1.https://docs.nvidia.com/networking/display/Onyxv393202/Management+Interfaces#ManagementInterfaces-ManagementVRFmanagementvrfThank you for the info, Hagai!Powered by Discourse, best viewed with JavaScript enabled"
552,can-the-interleaved-memory-regions-umr-provided-by-the-mlnx-ofed-mlx5dv-library-be-used-for-raw-packet-qps,"I have an application where I am using MLNX_OFED and the ibverbs API to receive packets into a queue pair with type IBV_QPT_RAW_PACKET. I would like to investigate using the User-Mode Registation feature in MLNX_OFED, as I’m interested to see if it can help with some of my packet processing needs (e.g. header/data split and potentially doing some reordering of the packet payload bytes). I could get header-data split already by using multiple SGEs for each packet, but I’m hoping that an interleaved region might give me better performance.The documentation on how to use this interface is kind of sparse, but I think I’ve pieced it together from various sources. It looks like I have to:I can provide a more detailed code example if needed, but I first wanted to make sure that what I’m doing is even on the right track.I’m finding that I never get a completion posted to the CQ after calling ibv_wr_complete() for the indirect mkey setup. Do I need to modify the RC QP’s state using ibv_modify_qp() in order to configure the indirect mkey? I am not using RC communication in my application, I’m just using raw Ethernet, so I don’t have all of the parameters required to put the RC QP into RTR or RTS states.If I am able to successfully configure the interleaved indirect mkey, can I then use that mkey with QPs of RAW_PACKET type to receive packets?Are there any full code examples of how to configure interleaved memory regions? All of the code snippets in the man pages are abbreviated and incomplete. Likewise, the only reference to it I can find in the rdma-core source code is in the pyverbs unit tests, which are difficult to follow as an example (they also use RC data transfers in the test, which I can’t use). It would be useful to have a full example somewhere.Powered by Discourse, best viewed with JavaScript enabled"
553,ipsec-openvpn-on-cumulus-linux,"Hello,Curious if it’s possible to setup IPSec or OpenVPN for point-to-point VPN on Cumulus Linux?​Thanks!​Hi Nick,it’s not supported (not tested or developed).Powered by Discourse, best viewed with JavaScript enabled"
554,connectx-3-cx311a-xcat-for-win11-crashed,"I have upgrade my OS to W11. There is a crash information show me SYSTEM_THREAD_EXCEPTION_NOT_HANDLED in mlx4eth63.sys. I already updated the newest WinOF, but it’s seem no use. Is there any solution? Thanks.Hello Hank,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, unfortunately we did not release a WinOF or WinOF-2 driver yet for Windows 11. Currently support for Windows clients does not exceed version 10.So installing the Windows 10 client driver version on a Windows 11 OS, is not supported.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
555,write-result-is-incorrect-with-nvmeof-target-offloading,"Hi,We found that when we issue large direct write to the target with NVMeoF offloading, the content won’t be written to the disk correctly. Some blocks will be replaced by other blocks in the result. For example, we constructed a 17MB buffer by filling every block with a formatted string (‘blk%05d’, block_index) and wrote the buffer all at once to the target. But when reading it, we found some out-of-order and repeated blocks. For example, in the attachment ‘output.txt’, if grepping ‘wrong’, we can see that blk27135 appears right after blk16382 (instead of blk16383) and again at its own position (following blk27134). Moreover, if we issue smaller writes, e.g., 16KB, this situation doesn’t happen (but the performance is much lower).For more information, we installed MLNX_OFED_LINUX-5.6-2.0.9.0-ubuntu20.04-x86_64 drivers on both host and target. The OS is Ubuntu 20.04 LTS, and we use Ext4 as the filesystem. The NIC is Dual-port Mellanox ConnectX-6 100 Gb NIC (PCIe v4.0). We also pasted our testing program after this post (I can only attach one link as a new user).The way we reproduce the bug is to run ‘./simulator /mnt/remote-sst/2/45 17825792 > output.txt’ on the host, where /mnt/remote-sst is an LVM partition on the target NVMe disk connect by NVMeoF.Does anyone know why this behavior would happen and how to fix/avoid it? Any thoughts are highly appreciated!output.txt (17.0 MB)Testing program:likely, your test involve filesystem and soft-raid LVM, you know both of that will re-allocate block base on device block size and fs config.https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout#BlocksIn theory, RDMA NVMEoF will not re-allocate block, since it “not seen” that, it just send/execute iscsi cmds.Hi Xiaofeng, Thanks for your reply! Yes, I realized that LVM might be reason. Previously we only considered extent mapping in ext4 (we fixed the extents by some workarounds). After changing to static disk partition (via fdisk) and recreating the filesystem, we haven’t seen the behavior for a long time.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
556,mellanox-connect-x-6-dx-virtio-and-vdpa-with-proxmox-kvm,"HiPlease point me at some documentation on how to get the virtIO acceleration features of the Mellanox Dx6 NIC to work with Proxmox.Hi Joe,Here is some guide:OVS Offload Using ASAP² Direct - MLNX_OFED v5.4-1.0.3.0 - Mellanox DocsHope it can give you some help.Regards,LeveiPowered by Discourse, best viewed with JavaScript enabled"
557,how-to-aggregate-cx5-vfs-bandwidth-in-kvm,"I have to two x86_64 servers and they have identical hardware configurations. Each of them have a 100Gb CX5 in PCIe slot1. Both servers and VMs are running OEL7U8, UEK5 kernel. I intend to aggregate the two ports of CX5 bandwidth in hosts and VMs. So I enabled SR-IOV and created VFs in both hosts:[rpmem@scaoda8m020 ~]$ lspci |grep Mellaaf:00.0 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex]af:00.1 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex]af:00.2 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function]af:00.3 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function]af:00.6 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function]af:00.7 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function]While I was creating the VFs of CX5 I noticed thataf:00.2 and af:00.3 are the VFs from PF at af:00.0af:00.6 and af:00.7 are the VFs from PF at af:00.1So I PCI passthrough af:00.2 and af:00.6 to VM in each hostand in VM they looked like:[root@scaoda8m020c2n1 network-scripts]# lspci |grep Mella00:08.0 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function]00:09.0 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function]So I believe ens8 is the af:00.2(port 1), and ens9 is the af:00.6 (port 2) in hosts.I have qspf28 cables connecting port1-port1, port2-port2 of CX5 in both physical servers.I want to aggregate the two VF ports bandwidth to get better throughput (~200Gb?) so I bond them together mode=4: BONDING_OPTS=“mode=4 miimon=100 lacp_rate=1”but somehow then the two VFs (in two VMs) can’t ping each other. In fact it also kill the bonding interface in the KVM hosts. But if I bond them with mode=1:BONDING_OPTS=“mode=active-backup miimon=100 primary=ens8”then they can ping each other successfully. Since mode=1 is active-backup so I believe only the ens8 is used normally. I also checked by ifup only ens8 or ens9 (without bonding) in both VMs and found they can ping each other successfully. So I think the individual connections are fine. That left the aggregation algorithm suspicious. Wonder is 802.3ad aggregation is supported in CX5? If this mode is not supported please suggest which mode I should use to aggregate CX5 two ports bandwidth. Please help me configure VFs (from two ports) in VM and get the max throughput.Thanks,KenHello Ken,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we want to continue to handle this issue through a regular support case as you have a valid support contract.We will reach out to you through the new support ticket for continuing this request.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
558,fw-upgrade-for-connectx4-lx-gigabyte-lom,"HiHow can I upgrade the firmware of the following Mellanox LOM:Device Type: ConnectX4LXPart Number: Gigabyte_MZ21-G210_CX4LX_2P_10G_LOM_A1Description: Gigabyte MZ21-G210 - LOM ConnectX-4 LX 10GigE dual-port SFP+ with NC-SI and WOL; PCIe3.0 x8 8GT/sPSID: GIG0000000005PCI Device Name: /dev/mst/mt4117_pciconf0Base MAC: N/AVersions: Current AvailableFW 14.26.1040 N/APXE 3.5.0803 N/AUEFI 14.19.0014 N/A Status: No matching image foundHello Lali,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, the adapter you are using is a Gigabyte ConnectX-4 Lx OEM adapter for which the OEM will provide the latest f/w.Please contact Gigabyte to obtain the latest f/w for this adapter.Thank you and regards,~NVIDIA Networking Technical SupportHi Martijn van Breugel,Thanks! I asked here because you have OEM FW, but not for Gigabyte. I will contact them.Best Regards!Powered by Discourse, best viewed with JavaScript enabled"
559,dpdk-start-app-mlx5-common-failed-to-create-tis-using-devx,"After installing the network card driver and DPDK environment, start the dpdk-helloword program, and load the mlx5 program to report an error（安装完网卡驱动和DPDK环境后启动dpdk-helloword程序，加载mlx5程序报错）。Q：​INFO:CA ‘mlx5_3’CA type: MT4123Number of ports: 1Firmware version: 20.32.1010Hardware version: 0Node GUID: 0x043f720300fea761System image GUID: 0x043f720300fea760Port 1:State: DownPhysical state: PollingRate: 10Base lid: 65535LMC: 0SM lid: 0Capability mask: 0x2651e848Port GUID: 0x043f720300fea761Link layer: InfiniBandERR INFO:EAL: Probe PCI driver: mlx5_pci (15b3:101b) device: 0000:5e:00.0 (socket 0)mlx5_common: probe device “0000:5e:00.0”.mlx5_common: RTE_MEM is selected.mlx5_common: Checking device “mlx5_3”…mlx5_common: Checking device “mlx5_2”…mlx5_common: Checking device “mlx5_1”…mlx5_common: Checking device “mlx5_0”…mlx5_common: Dev information matches for device “mlx5_0”.mlx5_common: DevX is supported.mlx5_common: initialized B-tree 0x1003e9dc1 with table 0x1003e7500EAL: Mem event callback ‘MLX5_MEM_EVENT_CB:(nil)’ registeredmlx5_common: Netlink socket send buffer: 212992mlx5_common: Netlink socket recv buffer: 212992mlx5_common: Netlink socket send buffer: 212992mlx5_common: Netlink socket recv buffer: 212992mlx5_net: Checking device “mlx5_3”mlx5_net: Checking device “mlx5_2”mlx5_net: Checking device “mlx5_1”mlx5_net: Checking device “mlx5_0”mlx5_net: PCI information matches for device “mlx5_0”mlx5_net: No E-Switch support detected.mlx5_net: naming Ethernet device “0000:5e:00.0”mlx5_common: Failed to create TIS using DevXmlx5_net: Failed to TIS 0 for bonding device mlx5_0.mlx5_net: TIS allocation failuremlx5_net: probe of PCI device 0000:5e:00.0 aborted after encountering an error: Cannot allocate memorymlx5_common: Failed to load driver mlx5_ethEAL: Mem event callback ‘MLX5_MEM_EVENT_CB:(nil)’ unregisteredmlx5_common: freeing B-tree 0x1003e9dc1 with table 0x1003e7500EAL: Requested device 0000:5e:00.0 cannot be usedEAL: PCI device 0000:5e:00.1 on NUMA socket 0EAL: probe driver: 15b3:101b mlx5_pciEAL: Probe PCI driver: mlx5_pci (15b3:101b) device: 0000:5e:00.1 (socket 0)mlx5_common: probe device “0000:5e:00.1”.mlx5_common: Checking device “mlx5_3”…mlx5_common: Checking device “mlx5_2”…mlx5_common: Checking device “mlx5_1”…mlx5_common: Dev information matches for device “mlx5_1”.mlx5_common: DevX is supported.mlx5_common: initialized B-tree 0x1003e9dc1 with table 0x1003e7500EAL: Mem event callback ‘MLX5_MEM_EVENT_CB:(nil)’ registeredmlx5_common: Netlink socket send buffer: 212992mlx5_common: Netlink socket recv buffer: 212992mlx5_common: Netlink socket send buffer: 212992mlx5_common: Netlink socket recv buffer: 212992mlx5_net: Checking device “mlx5_3”mlx5_net: Checking device “mlx5_2”mlx5_net: Checking device “mlx5_1”mlx5_net: PCI information matches for device “mlx5_1”mlx5_net: Checking device “mlx5_0”mlx5_net: No E-Switch support detected.mlx5_net: naming Ethernet device “0000:5e:00.1”mlx5_common: Failed to create TIS using DevXmlx5_net: Failed to TIS 0 for bonding device mlx5_1.mlx5_net: TIS allocation failuremlx5_net: probe of PCI device 0000:5e:00.1 aborted after encountering an error: Cannot allocate memorymlx5_common: Failed to load driver mlx5_ethEAL: Mem event callback ‘MLX5_MEM_EVENT_CB:(nil)’ unregisteredmlx5_common: freeing B-tree 0x1003e9dc1 with table 0x1003e7500EAL: Requested device 0000:5e:00.1 cannot be usedEAL: PCI device 0000:af:00.0 on NUMA socket 1EAL: probe driver: 15b3:101b mlx5_pciEAL: Probe PCI driver: mlx5_pci (15b3:101b) device: 0000:af:00.0 (socket 1)mlx5_common: probe device “0000:af:00.0”.​Hi,DEVX API is the modern way to utilize direct access to HCA. More info - GitHub - Mellanox/devx. Mellanox OFED version 5.x use it.Check your device status - it is down and in pulling state. Assuming you are working in InfiniBand mode, you need running Subnet Manager on the fabric.If the node connected to Ethernet fabric/switch, convert the port to Ethernet using mlxconfig command. Check Mellanox OFED user manual and MFT ( Mellanox Firmware Tools) manuals for additional info.#mlxconfig -d /dev/mst/mt4119_pciconf0 s LINK_TYPE_<1|2>=2If you are using bonding , undo the configuration and start with a single port.Another good think that you can do is to recompile DPDK with debugging information and trace failure to the line in the code and go from there. To add debug information to DPDK, use meson —buildtype=debug parameters. For additional info, check DPDK documentation.Thank you very much！I solved this problem by convert the port working mode to Ethernet.​Powered by Discourse, best viewed with JavaScript enabled"
560,doca-sample-app-error,"Hi ,I had installed the BF2 in my server , had flashed the binary using the GUI method , unfortunately facing the below error when application is ran , looks like doca-common package is missing , kindly do let me know , how to fix the sameroot@localhost:/opt/mellanox/doca/samples/doca_dma/dma_local_copy# meson build
The Meson build system
Version: 0.61.2
Source dir: /opt/mellanox/doca/samples/doca_dma/dma_local_copy
Build dir: /opt/mellanox/doca/samples/doca_dma/dma_local_copy/build
Build type: native build
Program cat found: YES (/usr/bin/cat)
Project name: DOCA_SAMPLE
Project version: 1.5.1007
C compiler for the host machine: cc (gcc 9.4.0 “cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0”)
C linker for the host machine: cc ld.bfd 2.34
C++ compiler for the host machine: c++ (gcc 9.4.0 “c++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0”)
C++ linker for the host machine: c++ ld.bfd 2.34
Host machine cpu family: aarch64
Host machine cpu: aarch64
Found pkg-config: /usr/bin/pkg-config (0.29.1)
Did not find CMake ‘cmake’
Found CMake: NO
Run-time dependency doca-common found: NO (tried pkgconfig)meson.build:29:0: ERROR: Dependency “doca-common” not found, tried pkgconfigA full log can be found at /opt/mellanox/doca/samples/doca_dma/dma_local_copy/build/meson-logs/meson-log.txtPlease refer to  1.1. Meson Complains About Missing Dependencies . I slove the problem with this method. You may try it.Thanks for suggestion , i had tried the same , but still then facing the same issue , the problem here is the meson is still taking the /usr/bin/pkg-config instead of /opt/mellanox/doca/lib/aarch64-linux-gnu/pkgconfig ,I had found the solution , cmakes was missing , so i had to install them and set the paths given in the troubleshooting guide as suggested in 1.1. Meson Complains About Missing Dependencies and also pkg config path was updated after running below commandecho $(pkg-config --variable pc_path pkg-config)${PKG_CONFIG_PATH:+:}${PKG_CONFIG_PATH}This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
561,how-to-develop-applications-with-nvidia-bluefield-dpu-and-nvidia-doca-libraries,"Is there any guide on how to develop applications with NVIDIA BlueField DPU and NVIDIA DOCA Libraries?
And also how to compile the source code?DOCA SDK: NVIDIA DOCA SDK Documentation
BlueField DPU guide: https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/BlueField+Software+Overview
you can use meson to build that doca’s doc use.Powered by Discourse, best viewed with JavaScript enabled"
562,my-problem-is-that-there-is-a-problem-when-i-compile-the-ibdump-source-file-and-run-the-ibdump-executable-file-the-ib-network-adapter-cannot-be-captured-after-modifying-the-ibdump-source-file-as-prompted-it-still-cannot-be-used-ibdump-of-the-driver-c,"![Y$02SHA8_0VG)(@G3DRLTGHello,Based on the output you have provided, the ibdump utility is active and listening. There is no indication of an application running alongside the ibdump utility.For more information on utilization of the ibdump utility, please see the following Community Article:https://community.mellanox.com/s/article/MLNX2-117-2032knPlease also ensure that ibdump is not run from the same port of the Subnet Manager as there is a known issue regarding this scenario:Contribute to Mellanox/ibdump development by creating an account on GitHub.ibdump may stop capturing packets when run on the same port of the Subnet Manager (E.G.: opensm). It is advised not to run the SM and ibdump on the same port.If these conditions are met and you require further debugging, please open a case with our Support team. If you do not have a current support contract for your adapter, please reach out to the team at Networking-contracts@nvidia.comThank you,-Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
563,mellanox-connectx-2-mnpa19-xtr-driver-for-windows-10,"I have attempted Windows install for Mellanox Connectx-2 MNPA19-XTR without success. (Have used the WinOF and WinOF2 branches without success.) Will be most grateful for assistance in locating such. ThanksHello samk,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.The ConnectX-2 is EOL and EOS for a while. Support for this adapter can only be found in older versions of the WinOF, for which no support is provided.We recommend to find a more recent adapter in the form of ConnectX-4 and up, which has full driver support.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
564,how-to-collect-device-proprietary-counters-in-linux,"I am interest about Device Proprietary Counters, e.g., PCI Back-pressure/sec, SQ Miss/sec & CQ Miss/sec, which may be useful for performance tuning.However, I cannot find available tools to collect these performance counters. I attempt to search for Mellanox Neo-hosts, but I am unable to find the available download link. I am also not sure that does it apply to Linux servers. Is there alternative ways? Thank you.Hi alogfans,Neo-hosts software is the correct software to use for the counters you mentioned.
Mellanox Neo-hosts software is Nvidia’s proprietary, and a support contract is needed to download it.
Alternatively, you can send an email to EnterpriseSupport@nvidia.com, create a support case according to your entitlement for receiving download instructions.Best regards,
YogevPowered by Discourse, best viewed with JavaScript enabled"
565,sn2010m-switches-high-pause-discards-packets,"Firstly, let me point out, i’m not a networking guy so please bare with me.We recently purchased a HPE HCI VMWare solution consisting of three HPE ProLiant servers, a HPE Nimble storage array and a pair of HPE/Mellanox SN2010m switches. The servers each have 4 x 25gb SFP nics, the Nimble also has 4 x 25gb SFP nics and the two switches are connected/stacked/mlag’ed via 100gb sfp dac.HPE engineers configured and installed the whole setup as part of the installation service. Each server has a bonded pair of 25GB nics (for management & lan purposes) and the other two 25GB nics are used for multi path communication with the Nimble storage (ie: iscsi-A and iscsi-B). The Nimble has two controllers, again, each with two 25gb nics (ie: iscan-A & iscsi-B). The servers and Nimble are connected to both switches for HA purposes.On the Mellanox switches, we’re seeing high levels of paused and discard packets. This only applies to the mlag ports and the server management ports. We don’t see any paused or discard packets against any of the ports used for iscsi.Jumbo frames is enabled on the switches, the Nimble and the servers. Flow control is set to ‘global’ for all switch ports with exception to the mlag ports, this is set to PFC.So far we have updated the servers and storage unit with the latest patches/firmware and replaced all the DAC cables, however, the issue still remains.We have tickets raised with HPE and VMWare but both are struggling to identify the root cause? Anyone have an suggestions or advice on what could be the issue.Hi Phil,Joey here from Nvidia support team, I will be assisting you with this case.I understand that you are connecting 3 HPE servers and a HPE storage array to a pair of HPE/Mellanox SN2010m switches which are running in mlag. I would suggest you provide us a detailed diagram about the connections and all devices and tell us which ports are showing  high levels of paused and discard packets, And could you collect sysdump or cl-support from both switches and upload them to the case for analysis? In addition. what’s the impact of this issue apart from high levels of paused and discard packets? thanksHi Joey,We have three HPE servers (VMWare/ESXi) which each have four 25gb SFP nics and two HPE servers (Windows) that have a single 10gb SFP nic. The HPE Storage array has four 25gb SFP nics. In addition to this, we also have a HP 2530-48G-PoEP Switch (J9772A) connected via 1gb. The SN2010M mlag’d switches is our core switch stack and only server hardware is connected to it. The HP 2530-48G is our edge switch and only client devices (workstations, phones, printers, etc) are connected to it. Hopefully, the table below gives a clear idea of how everything is connected.The ports showing heavy pause & discard packets are:Switch #1: 1, 3, 5, 13, 16, 21 & 22
Switch #1: 1, 3, 5, 21 & 22Hi Phil,Thanks for the detailed information and sysdump of one switch. I looked into the stats of all ports and here is a summary:
1/1, 1/3, 1/5 have discard packets for Rx and only have pause frames sent out, no received;
1/13, 1/16, 1/21, 1/22 don’t have discard packets for Rx, have pause frames sent out and received as well.After looking into the config, I think you are using PFC with priority 4 for traffic of ports 1/2, 1/4, 1/6, 1/7, and 1/8;
and also using flow-control (global pause) with default priority 0 for traffic of ports 1/1, 1/3, 1/5, 1/13, 1/16. Since we can see that the switch is sending pause frames out via 1/1, 1/3, and 1/5 but still showing discard for Rx, I would suggest you check the ProLiant #1/2/3 (ESXi) Mgmt Ports to see if it’s flow-control enabled. The ProLiant  servers side should stop sending traffic to the switch after receiving pause frames, and I can see that HP 2530 switch only has two 1G ports connected to SN2010 switches, which may cause congestion when the servers send high volume of traffic to any devices under HP 2530 switch.In addition, I also found in sysdump that there are some wjh packets showing the switch received some traffic from HP 2530 switch via port mpo25/Eth1/18 on 11/15 and 11/16, but the traffic should be forwarded out via this port, so it is not expected. Can you please provide us the output of the following commands on 2 mlag switches?Thanks Joey, that was very useful. I took a closer look at the ProLiant nics and found flow control (both tx & rx) was disabled on 2 out of the 4 nics on each ProLiant. Not sure why/how as i understand flow control should be enabled by default in VMWare ESXi7? Flow Control is now enabled on all ProLiant nics and i’ve cleared the switch counters so we can monitor over the next few days.I will send you the mlag switch command results via PM now.Hi Phil,Sorry for the wrong command, please use this one:I looked at the output of mlag, it looks good to me. Please monitor the stats of concerned ports and see how it works.Ok thanks. I’ll pm you the spanning-tree results now.
After 24 hours, we’re still seeing high levels of rx discards on the three VMWare Proliant mgmt ports and high tx pause packets on the mlag ports and the Windows Proliant server.Hi Phil,I think it’s reasonable to see TX pause on the ports when they receive high volume of traffic and can’t forward it out to other ports. Here we have two 1G ports connected to HP 2530 switch, so there will be congestion when the traffic flow originates from servers connected via 10G ports to any device under HP 2530 switch. The SN2010 switches will send pause frames to these servers in such scenario asking the servers to hold the traffic, and the servers should react to these pause frames by reducing the traffic rate until it stops receiving the pause frames.
In this way the SN2010 switches is supposed to not see any RX discard packets. So I would suggest you check the  flow control config on the server side. thanks.Hi Phil,Just want to follow up if you are able to check the flow control config on the server side. thanks.Hi JoeySince enable flow control on all esxi ports, we’re seeing much less pause/discard packets. Thanks for all your help.Hi Phil,Good to hear that. Feel free to reach out to us if you have any other questions. thanks.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
566,dpdk-rte-flow-is-degrading-performance-when-testing-on-connect-x5-100g-en-100g,"Hi,I am using DPDK 18.11 on Ubuntu 18.04, with Mellanox Connect X-5 100G EN (MLNX_OFED_LINUX-4.5-1.0.1.0-ubuntu18.04-x86_64).Packet generator: t-rex 2.49 running on another machine.I am able to achieve 100G line rate with l3fwd application (frame size 64B) using the parameters suggested in their performance report.(https://fast.dpdk.org/doc/perf/DPDK_18_11_Mellanox_NIC_performance_report.pdf)However, as soon as I install rte_flow rules to steer packets to different queues and/or use rte_flow’s mark action, the throughput reduces to ~41G. I also modified DPDK’s flow_filtering example application, and am getting the same reduced throughput of around 41G out of 100G. But without rte_flow, it goes to 100G.I didn’t change any OS/Kernel parameters to test l3fwd or the application that uses rte_flow. I also ensure the application is numa-aware and use 20 cores to handle 100G traffic.Upon further investigation (using Mellanox NIC counters), the drop in throughput is due to mbuf allocation errors.Is such performance degradation normal when performing hw-accelerationusing rte_flow on this NIC?Has anyone tested throughput performance using rte_flow @ 100G?Its surprising to see hardware offloading is degrading the performance, unless I am doing something wrong.Thanks,ArvindP.S. I have posted the same question on DPDK users community. If any Mellanox devs are on there, request them to respond to the same.Hi,Can you share the different tuning you applied ?Did you enable the CQE_COMPRESSION ?Flow control disable ?Did you try to increase the number of hugespages used ?How many buffer descriptors, rx/tx queues ?Can you provide the rte flow rules you added ?ThanksMarcSure. I mostly tried to use the same settings as your DPDK 18.11 Performance Test configurations.​Yes, CQE_COMPRESSION is enabled, Flow Control disabled.Lots of hugepages available, almost 56x1G free.​Kernel Parametersaum:~$ sudo cat /proc/cmdlineBOOT_IMAGE=/vmlinuz-4.15.0-43-generic ro quiet splash isolcpus=24-47 intel_idle.max_cstate=0 processor.max_cstate=0 intel_pstate=disable nohz_full=24-47 rcu_nocbs=24-47 rcu_nocb_poll default_hugepagesz=1G hugepagesz=1G hugepages=64 audit=0 nosoftlockup vt.handoff=1​Hugepagesaum:~$ grep -i huge /proc/meminfoAnonHugePages: 0 kBShmemHugePages: 0 kBHugePages_Total: 64HugePages_Free: 56HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 1048576 kB​​MLNX TUNE OUTPUTaum:~$ sudo mlnx_tune​Mellanox Technologies - System Report​Operation System StatusUBUNTU18.044.15.0-43-generic​CPU StatusGenuineIntel Intel(R) Xeon(R) Platinum 8168 CPU @ 2.70GHz SkylakeWarning: Frequency 3408.62MHz​Memory StatusTotal: 376.62 GBFree: 296.17 GB​Hugepages StatusOn NUMA 1:Transparent enabled: madviseTransparent defrag: madvise​Hyper Threading StatusINACTIVE​IRQ Balancer StatusINACTIVE​Firewall StatusNOT PRESENTIP table StatusNOT PRESENTIPv6 table StatusNOT PRESENT​Driver StatusOK: MLNX_OFED_LINUX-4.5-1.0.1.0 (OFED-4.5-1.0.1)​​ConnectX-5EX Device Status on PCI af:00.0FW version 16.24.1000OK: PCI Width x16Warning: PCI Speed 8GT/s >>> PCI width status is below PCI capabilities. Check PCI configuration in BIOS.PCI Max Payload Size 256PCI Max Read Request 1024Local CPUs list [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47]​enp175s0f0 (Port 1) StatusLink Type ethOK: Link status UpSpeed 100GbEMTU 1500OK: TX nocache copy ‘off’​ConnectX-5EX Device Status on PCI af:00.1FW version 16.24.1000OK: PCI Width x16Warning: PCI Speed 8GT/s >>> PCI width status is below PCI capabilities. Check PCI configuration in BIOS.PCI Max Payload Size 256PCI Max Read Request 1024Local CPUs list [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47]​enp175s0f1 (Port 1) StatusLink Type ethOK: Link status UpSpeed 100GbEMTU 1500OK: TX nocache copy ‘off’​​​RING PARAMETERSaum:~$ sudo ethtool -g enp175s0f0Ring parameters for enp175s0f0:Pre-set maximums:RX: 8192RX Mini: 0RX Jumbo: 0TX: 8192Current hardware settings:RX: 8192RX Mini: 0RX Jumbo: 0TX: 8192​aum:~$ sudo ethtool -g enp175s0f1Ring parameters for enp175s0f1:Pre-set maximums:RX: 8192RX Mini: 0RX Jumbo: 0TX: 8192Current hardware settings:RX: 8192RX Mini: 0RX Jumbo: 0TX: 8192​RX/TX Queues20 RX and 20 TX queues​Buffer Descriptions on Socket 150k+​rte_flowrte_flow rules were similar to DPDK’s flow_filtering example.I generate packets with dst_ip = 192.168.1.x - where x ranges from 1 to 80I call from generate_ipv4_flow() function in this file to install the rules.I steer every dst_ip (out of 80) to one of the 20 RX queue.Snippet below./* create flow for send packet with */uint32_t tempdst, tempsrc;struct rte_flow_error error;tempsrc = IPv4(192, 168, 0, 1);for (int i=1; i<81; i++) {tempdst = IPv4(192, 168, 1, i);flow = generate_ipv4_flow(0, i%20,tempsrc, 0,tempdst, 32, &error);if (!flow) {RTE_LOG(ERR, CALF, “Flow can’t be created %d message: %s\n”,error.type,error.message ? error.message : “(no stated reason)”);rte_exit(EXIT_FAILURE, “Error in creating flow”);}RTE_LOG(INFO, CALF, “Flow %d created.\n”, i);}RTE_LOG(INFO, CALF, “All flows created.\n”);If I do not install any flow rules, I am able to achieve line rate. :\Troubleshooting this was rather very painful, and not obvious.DEV_TX_OFFLOAD_VLAN_INSERTDEV_TX_OFFLOAD_TCP_TSOConfiguring ports with the above tx offload settings were causing this unusual drop.Both these settings are part of DPDK’s flow filtering example source code, making it less obvious.I am not sure if this drop in throughput was expected, if not there could be some bug in its implementation.Arvind or Marc, was this degradation due to any type of rte_flow action or those specific ones? Is there a solution if you’re using those?​What are the specs of machine?And have you also tried dpdk-pktgen to see what type of result you’ll get using that?cx5/cx6 with dpdkset dv_flow_en=1(default) will effect rx performanceset dv_flow_en=0 (verb flow) no this problem!Powered by Discourse, best viewed with JavaScript enabled"
567,mlx5-vdpa-disagrees-about-version-error-after-upgrading-mlnx-ofed-driver,"After upgrading the MLNX_OFED Driver to version 5.8-2.0.3, the following error occurs during boot:error logmlx5_vdap driver informationUpon checking the version of the mlx5_vdpa driver, it seems to be using the default driver of the 5.15 kernel, and it appears to be a version conflict with the mlx5_core driver.The mlx5_vdpa driver does not exist on the Nvidia driver download site, so I have set it to not load the mlx5_vdpa driver at boot using a blacklist.If there is a better solution or method, please let me know.Here are the details of my setup:
HardwareSoftware VersionsHello Kyoon,Thank you for writing us.
I would suggest that you install the 5.8-3.0.7.0-LTS and try again.Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)Thanks and have a great day!
Ilan.Hi ipavisThe ConnectX-6 LX currently in use is a Dell OEM product. The firmware versions supported by Dell are as follows:The 5.8-3.0.7.0 version you mentioned is only supported by Firmware versions 26.35.2000 ~ 26.35.3006, as confirmed in the documentation.Would there be any issues if the configuration is as follows?Powered by Discourse, best viewed with JavaScript enabled"
568,about-doca-flow,"For example, one rule matching the source ip address and the other matching destination address.If I want to match l4 ports only, I have to specify both out_l4_type and out_dst_ip.type, in addition to out_dst_port. Is this mandatory?Many thanks in advance.Best wishes,
KyleHi,1.No each flow can only match a packet with one single rule each time.2.Yes, that’s a mandatory.  Please refer to the “Setting Pipe Match” in this document link.
NVIDIA DOCA FlowThanks!
Best RegardsPowered by Discourse, best viewed with JavaScript enabled"
569,examine-sfp-qsfp-model-and-optical-quality,"I used to be able to do this with mst, but this is absent in the current mlx5_core driver.
mstflint and mstconfig don’t appear to have this abilityhere is an example of the old tools:is there a way?
thanxWhen you stated that ethtool does not tell you anything, what do ethtool -m $interface_name or ethtool -i $interface_name have to say?for instance, on the switch side, I can see the TX/RX optical levelsI see you are partly correct, I can identify the SFP with ethtool.  Thank You for that.is there a way to examine the optical level on the HBA side?Hi,
It might be that your driver/card/transceiver is not keen exposing this information - above ethtool -m gives me plenty of optical level information with Juniper transceivers for  example with connectx4/5  etc.I have absolutely no idea if this works with your combination and if you have the required bits installed but you could also consider trying to query this information with these cmds:
sudo mst start
sudo mst cable add
sudo mlxcables
sudo mlxcables --DDM -d $pci_device_idLike said, it is completely within the realm of possibility that your combination of transceiver and card is simply not willing to part with the DDM/DOM information.yes, mlxcables is how this used to be  done (see NOTE above: here is an example of the old tools:)
in the current mlx5_core driver, mst no longer installs as an application.  mstflint and mstconfig and the other apps don’t appear to have this ability.Powered by Discourse, best viewed with JavaScript enabled"
570,mofed-5-2-installer-fails-after-ubuntu-kernel-updated-from-5-4-0-62-to-5-4-0-65,"Anyone else have issues with the kernel patch or have a thought?Per subject, and after a day wasted, I removed MOFED 5.1 and tried to install the latest 5.2. It fails with “Failed to build MLNX_OFED_LINUX for 5.4.0-65-generic”. I have narrowed it down to the install option “–add-kernel-support”. If I omit that but keep all other options, the MOFED installs fine, but with that one option the installer throws the error noted earlier.Server seemed to be updated Friday night from Ubuntu 20.04.01 to .02, and the kernel advanced from 5.4.0-62 to -65. No other changes. It was working fine before the update. I was using it all week, rebooted at various times, and did not have any problems until it locked-up Friday night. Saturday morning it was dead and I had to wait until Monday to get someone in the data center to physically reboot it.After a day of trying fixes, now it only boots into the BIOS screen. Doh! The last action was running the MOFED 5.2 installer without the option “–add-kernel-support” noted earlier. It said it updated firmware on my ConnectX-6 card and a reboot was needed. So, I rebooted, but the server never came back up. Don’t know if it is due to the kernel issue or the firmware issue, but I cannot get past the BIOS so further troubleshooting is unlikely.My setup is a new Supermicro with dual AMD EPYC2 and 512GB RAM, a SATA boot drive and four NVMe data drives, a dual-port Mellanox ConnectX-6, and an NVIDIA A100 GPU. The server is dedicated to testing NVIDIA GPUDirect Storage (beta), so my MOFED install follows their instructions.UPDATEWe rebuilt the server from scratch: we wiped the boot drive and re-loaded Ubuntu. MOFED install fails with the same errors as before. We did not load any other software.The main log file:Installing new packagesBuilding DEB for ofed-scripts-5.2 (ofed-scripts)…Running /usr/bin/dpkg-buildpackage -us -ucBuilding DEB for mlnx-ofed-kernel-utils-5.2 (mlnx-ofed-kernel)…-W- --with-mlx5-ipsec is enabledRunning /usr/bin/dpkg-buildpackage -us -uc^[[31mFailed to build mlnx-ofed-kernel DEB^[[0mCollecting debug info…^[[31mSee /tmp/MLNX_OFED_LINUX-5.2-1.0.4.0-5.4.0-65-generic/mlnx_iso.4078_logs/OFED.4311.logs/mlnx-ofed-kernel.debbuild.log^[[0mThe debbuild.log refers to a debug log. That’s a bit too long to post here.Hello Mark,Thank you for posting your inquiry on the NVIDIA Networking Community.We released an update on MLNX_OFED 5.2 yesterday → https://content.mellanox.com/ofed/MLNX_OFED-5.2-2.2.0.0/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64.tgzThis version installs without any issues. See below output:Install syntax: # ./mlnxofedinstall --with-nvmf --with-nfsrdma --enable-gds -vvv# lsb_release -ra ; uname -r ; ofed_info -sNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 20.04.2 LTSRelease: 20.04Codename: focal5.4.0-65-genericMLNX_OFED_LINUX-5.2-2.2.0.0:Snippet from install log (general.log)…Installing libdapl-dev-2.1.10.1.mlnx…Running /usr/bin/dpkg -i --force-confmiss /var/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/DEBS/libdapl-dev_2.1.10.1.mlnx-OFED.4.9.0.1.4.52220_amd64.debInstalling dpcp-1.1.0…Running /usr/bin/dpkg -i --force-confmiss /var/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/DEBS/dpcp_1.1.0-1.52220_amd64.debInstalling srptools-52mlnx1…Running /usr/bin/dpkg -i --force-confmiss /var/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/DEBS/srptools_52mlnx1-1.52220_amd64.debInstalling mlnx-ethtool-5.8…Running /usr/bin/dpkg -i --force-confmiss /var/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/DEBS/mlnx-ethtool_5.8-1.52220_amd64.debInstalling mlnx-iproute2-5.8.0…Running /usr/bin/dpkg -i --force-confmiss /var/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/DEBS/mlnx-iproute2_5.8.0-1.52220_amd64.debRunning: FW_UPDATE_FLAGS=‘–log /tmp/MLNX_OFED_LINUX.10306.logs/fw_update.log -v --tmpdir /tmp’ RUN_FW_UPDATER=‘yes’ /usr/bin/dpkg -i /var/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/DEBS/mlnx-fw-updater_5.2-2.2.0.0_amd64.debRunning: /usr/bin/dpkg-deb -x /var/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/DEBS/mlnx-ofed-kernel-dkms_5.2-OFED.5.2.2.2.0.1_all.deb /var/tmp/mlnx-ofed-kernel_module-check 2>/dev/nullis_module_in_deb: ipoib is in /var/tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/DEBS/mlnx-ofed-kernel-dkms_5.2-OFED.5.2.2.2.0.1_all.debInstallation passed successfullyTo load the new driver, run:/etc/init.d/openibd restartNote: In order to load the new nvme-rdma and nvmet-rdma modules, the nvme module must be reloaded.If you are still experiencing installation issues with this driver release, please open a NVEX Technical Support ticket by sending an email to → networking-support@nvidia.comThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
571,connectx-4-lx-very-slow-on-windows-10-same-speed-as-an-intel-1gb-using-rj45,"HiI have installed a ConnectX-4 Lx 10GB Card on Windows 10, and an Intel PRO/1000 for comparison.In Network Connections the Mellanox is described as a 10GB connection, the Intel as a 1GB connection.I have used several benchmarking applications to test them with similar results, most recently ntttcp.The test uses a crossover cable between one port and the other on the same card, the test commands I use are below.As you can see in the attached text file, the Intel achieves a transfer rate higher than it’s 1GB spec at 12812.267 MB/s.The Mellanox underperforms at 13412.951 MB/s, running at just over 1/10th it’s 10GB capacity.I have tried:It is installed in a PCI-E 8.0 GT/s x8, so it should have a bandwidth of >60GB.Does anyone know what I need to do to get the speed of the Mellanox up to 10GB?See below for the full test results and details of my system and NICs.Many thanks!MellanoxTests.txt (7.33 KB)Hi,I suggest to review the below guides :Getting started with ConnectX-4 100Gb/s Adapter for Windowshttps://community.mellanox.com/s/article/getting-started-with-connectx-4-100gb-s-adapter-for-windowsGetting Started with ConnectX-5 100Gb/s Adapter for Windowshttps://community.mellanox.com/s/article/getting-started-with-connectx-5-100gb-s-adapter-for-windowsWindows Performance Tuninghttps://docs.mellanox.com/display/winof2v240/Performance+TuningAfter applying the same configuration and tuning steps , please share the results.Thanks,SamerHello,I have done everything I can do from that guide. The BIOS settings are completely different to my servers BIOS, and the mlxconfig command does not return LINK_TYPE_P1 or LINK_TYPE_P2.I did change the Jumbo packet setting, and doing so increased the speed of my tests to 14419 MB/s (the ntttcp settings recommended in the guide resulted in only 9334 MB/s). But this is still only 1.4GB and nowhere near 10GB.I am testing the card on a development machine. I need to give the go ahead for my hosting company to purchase x8 of these for my servers as per their recommendation, but I am concerned about doing this while they are underperforming so badly on my kit.Please advise.mlxconfigntttcp resultsHi Fergus,Please open a support case for further investigation atnetworking-support@nvidia.comThanks,SamerI contacted networking support as you suggested, but they will not deal with me without a contract. I object to paying for support when there is clearly an issue with the Mellanox card/drivers under Windows 10 that is causing this problem.Please advise.Thanks,FergusI just tried moving the Mellanox to a x16 PCI slot (it was in a x8 before), and removing the Intel Dual Port card and another PCI card I didn’t need.The results are still bad at 1.27GB.Powered by Discourse, best viewed with JavaScript enabled"
572,typeerror-servo-object-does-not-support-item-assignment,"I am testing following (test)program:I get error in line: pan.servo[0]=180TypeError: ‘_Servo’ object does not support item assignment
Exiting…
Cleaning up pinsI can’t figure out what is wrong here.please ignore my message: I found the error (after many hours)
pan.servo[0]=180   should have been pan.servo[0].angle=180Powered by Discourse, best viewed with JavaScript enabled"
573,fail-to-run-doca-flow-samples-on-dpu,"The samples built fine but won’t run. I tried flow_drop and flow_hairpin,they both failed
Here are the details and error message:root@localhost:/home/ubuntu/mellanox/doca/samples/doca_flow/flow_drop/build# ./doca_flow_drop -a auxiliary:mlx5_core.sf.4 -a auxiliary:mlx5_core.sf.5 – -l 60and the error is[12:27:51:390208][DOCA][ERR][dpdk_flow_hws_legacy:81]: failed to configure flow hws port 0 - rte flow configure, type 1 message: port configure with incorrect steering modeAnd the full run output is:root@localhost:/home/ubuntu/mellanox/doca/samples/doca_flow/flow_drop/build# ./doca_flow_drop -a auxiliary:mlx5_core.sf.4 -a auxiliary:mlx5_core.sf.5 – -l 60
EAL: Detected CPU lcores: 8
EAL: Detected NUMA nodes: 1
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
TELEMETRY: No legacy callbacks, legacy socket not created
[12:27:51:065275][DOCA][DBG][NUTILS:533]: Port 0 MAC: 00 00 00 00 04 00
[12:27:51:288932][DOCA][DBG][NUTILS:533]: Port 1 MAC: 00 00 00 00 04 01
[12:27:51:292929][DOCA][INF][engine_model:73]: engine model defined with mode=vnf
[12:27:51:292959][DOCA][INF][engine_model:75]: engine model defined with nr_pipe_queues=8
[12:27:51:292974][DOCA][INF][engine_model:76]: engine model defined with pipe_queue_depth=0
[12:27:51:296291][DOCA][INF][engine_field_mapping:329]: Engine field mapping initialized
[12:27:51:296364][DOCA][INF][engine_shared_resources:94]: Engine shared resources initialized successfully
[12:27:51:296401][DOCA][INF][dpdk_engine:326]: queue depth is zero, set it to default 128.
[12:27:51:296415][DOCA][INF][dpdk_engine:331]: ACL number of collisions is zero, set it to default 1
[12:27:51:296433][DOCA][INF][dpdk_pipe_items:175]: Initialized dpdk pipe items module
[12:27:51:296473][DOCA][DBG][dpdk_table_hws:907]: Initialized dpdk table work module to be HW steering
[12:27:51:296488][DOCA][INF][dpdk_table:70]: Initializing dpdk table successfully
[12:27:51:296501][DOCA][DBG][dpdk_flow_hws:36]: Initialized dpdk flow work module to be HW steering
[12:27:51:296514][DOCA][INF][dpdk_flow:56]: Initializing dpdk flow successfully
[12:27:51:296527][DOCA][INF][dpdk_resource_manager:184]: Dpdk resource manager register completed
[12:27:51:296711][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.meta.port, offset=0)
[12:27:51:296731][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.meta.mark, offset=0)
[12:27:51:296744][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.meta.nisp_syndrome, offset=0)
[12:27:51:296757][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.meta.ipsec_syndrome, offset=0)
[12:27:51:296772][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.eth.dst_mac, offset=0)
[12:27:51:296786][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.eth.src_mac, offset=6)
[12:27:51:296800][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.eth.type, offset=12)
[12:27:51:296814][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.eth.dst_mac, offset=0)
[12:27:51:296829][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.eth.src_mac, offset=6)
[12:27:51:296842][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.eth.type, offset=12)
[12:27:51:296855][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.eth_vlan0.tci, offset=0)
[12:27:51:296869][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.eth_vlan1.tci, offset=0)
[12:27:51:296882][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.eth_vlan0.tci, offset=0)
[12:27:51:296895][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.eth_vlan1.tci, offset=0)
[12:27:51:296908][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv4.src_ip, offset=12)
[12:27:51:296923][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv4.dst_ip, offset=16)
[12:27:51:296938][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv4.dscp_ecn, offset=1)
[12:27:51:296951][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv4.next_proto, offset=9)
[12:27:51:296967][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv4.ttl, offset=8)
[12:27:51:296980][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv4.version_ihl, offset=0)
[12:27:51:296995][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv4.src_ip, offset=12)
[12:27:51:297009][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv4.dst_ip, offset=16)
[12:27:51:297022][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv4.dscp_ecn, offset=1)
[12:27:51:297036][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv4.next_proto, offset=9)
[12:27:51:297048][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv4.ttl, offset=8)
[12:27:51:297060][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv4.version_ihl, offset=0)
[12:27:51:297073][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv6.src_ip, offset=8)
[12:27:51:297086][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv6.dst_ip, offset=24)
[12:27:51:297099][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv6.dscp_ecn, offset=0)
[12:27:51:297111][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv6.next_proto, offset=6)
[12:27:51:297124][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.ipv6.hop_limit, offset=7)
[12:27:51:297137][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv6.src_ip, offset=8)
[12:27:51:297151][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv6.dst_ip, offset=24)
[12:27:51:297165][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv6.dscp_ecn, offset=0)
[12:27:51:297177][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv6.next_proto, offset=6)
[12:27:51:297191][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.ipv6.hop_limit, offset=7)
[12:27:51:297206][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.udp.src_port, offset=0)
[12:27:51:297220][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.udp.dst_port, offset=2)
[12:27:51:297232][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.udp.src_port, offset=0)
[12:27:51:297245][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.udp.dst_port, offset=2)
[12:27:51:297258][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.tcp.src_port, offset=0)
[12:27:51:297272][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.tcp.dst_port, offset=2)
[12:27:51:297286][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.tcp.flags, offset=13)
[12:27:51:297299][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.tcp.src_port, offset=0)
[12:27:51:297313][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.tcp.dst_port, offset=2)
[12:27:51:297325][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.tcp.flags, offset=13)
[12:27:51:297338][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.icmp4.type, offset=0)
[12:27:51:297352][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.icmp4.code, offset=1)
[12:27:51:297365][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.icmp4.ident, offset=4)
[12:27:51:297377][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.icmp4.type, offset=0)
[12:27:51:297389][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.icmp4.code, offset=1)
[12:27:51:297453][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.icmp4.ident, offset=4)
[12:27:51:297467][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.icmp6.type, offset=0)
[12:27:51:297481][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.outer.icmp6.code, offset=1)
[12:27:51:297495][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.icmp6.type, offset=0)
[12:27:51:297508][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.inner.icmp6.code, offset=1)
[12:27:51:297523][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.vxlan.vni, offset=4)
[12:27:51:297537][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.gre_key.value, offset=0)
[12:27:51:297550][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.gre.protocol, offset=2)
[12:27:51:297566][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.gtp.teid, offset=4)
[12:27:51:297580][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.mpls.label, offset=0)
[12:27:51:297594][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.audp.hdr, offset=0)
[12:27:51:297608][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.nisp.hdr, offset=0)
[12:27:51:297621][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.geneve.ver_opt_len, offset=0)
[12:27:51:297635][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.geneve.o_c, offset=1)
[12:27:51:297649][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.geneve.next_proto, offset=2)
[12:27:51:297661][DOCA][DBG][dpdk_layer:81]: Registered dpdk field opcode=match.packet.tunnel.geneve.vni, offset=4)
[12:27:51:297673][DOCA][INF][dpdk_layer:404]: Dpdk layer register completed
[12:27:51:297687][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.meta.data, offset=4, len=20)
[12:27:51:297701][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.meta.port, offset=28, len=4)
[12:27:51:297713][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.meta.mark, offset=32, len=4)
[12:27:51:297743][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.meta.nisp_syndrome, offset=36, len=1)
[12:27:51:297783][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.meta.ipsec_syndrome, offset=37, len=1)
[12:27:51:297799][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.meta.flags, offset=0, len=4)
[12:27:51:297813][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.eth.dst_mac, offset=46, len=6)
[12:27:51:297827][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.eth.src_mac, offset=40, len=6)
[12:27:51:297841][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.eth.type, offset=52, len=2)
[12:27:51:297854][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.eth.dst_mac, offset=162, len=6)
[12:27:51:297867][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.eth.src_mac, offset=156, len=6)
[12:27:51:297881][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.eth.type, offset=168, len=2)
[12:27:51:297896][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.eth_vlan0.tci, offset=56, len=2)
[12:27:51:297909][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.eth_vlan1.tci, offset=58, len=2)
[12:27:51:297923][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.eth_vlan0.tci, offset=172, len=2)
[12:27:51:297936][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.eth_vlan1.tci, offset=174, len=2)
[12:27:51:297949][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv4.src_ip, offset=64, len=4)
[12:27:51:297964][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv4.dst_ip, offset=68, len=4)
[12:27:51:297977][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv4.dscp_ecn, offset=73, len=1)
[12:27:51:297991][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv4.next_proto, offset=74, len=1)
[12:27:51:298005][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv4.ttl, offset=75, len=1)
[12:27:51:298019][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv4.version_ihl, offset=72, len=1)
[12:27:51:298033][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv4.src_ip, offset=180, len=4)
[12:27:51:298046][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv4.dst_ip, offset=184, len=4)
[12:27:51:298060][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv4.dscp_ecn, offset=189, len=1)
[12:27:51:298073][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv4.next_proto, offset=190, len=1)
[12:27:51:298087][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv4.ttl, offset=191, len=1)
[12:27:51:298101][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv4.version_ihl, offset=188, len=1)
[12:27:51:298115][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv6.src_ip, offset=64, len=16)
[12:27:51:298129][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv6.dst_ip, offset=80, len=16)
[12:27:51:298142][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv6.dscp_ecn, offset=96, len=1)
[12:27:51:298154][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv6.next_proto, offset=97, len=1)
[12:27:51:298167][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.ipv6.hop_limit, offset=98, len=1)
[12:27:51:298207][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv6.src_ip, offset=180, len=16)
[12:27:51:298222][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv6.dst_ip, offset=196, len=16)
[12:27:51:298238][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv6.dscp_ecn, offset=212, len=1)
[12:27:51:298252][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv6.next_proto, offset=213, len=1)
[12:27:51:298266][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.ipv6.hop_limit, offset=214, len=1)
[12:27:51:298281][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.udp.src_port, offset=104, len=2)
[12:27:51:298294][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.udp.dst_port, offset=106, len=2)
[12:27:51:298308][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.udp.src_port, offset=220, len=2)
[12:27:51:298321][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.udp.dst_port, offset=222, len=2)
[12:27:51:298333][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.tcp.src_port, offset=104, len=2)
[12:27:51:298345][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.tcp.dst_port, offset=106, len=2)
[12:27:51:298359][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.tcp.flags, offset=108, len=1)
[12:27:51:298372][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.tcp.src_port, offset=220, len=2)
[12:27:51:298386][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.tcp.dst_port, offset=222, len=2)
[12:27:51:298400][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.tcp.flags, offset=224, len=1)
[12:27:51:298414][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.icmp4.type, offset=104, len=1)
[12:27:51:298428][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.icmp4.code, offset=105, len=1)
[12:27:51:298442][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.icmp4.ident, offset=106, len=2)
[12:27:51:298455][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.icmp4.type, offset=220, len=1)
[12:27:51:298469][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.icmp4.code, offset=221, len=1)
[12:27:51:298482][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.icmp4.ident, offset=222, len=2)
[12:27:51:298495][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.icmp6.type, offset=104, len=1)
[12:27:51:298509][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.outer.icmp6.code, offset=105, len=1)
[12:27:51:298522][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.icmp6.type, offset=220, len=1)
[12:27:51:298535][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.inner.icmp6.code, offset=221, len=1)
[12:27:51:298549][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.vxlan.vni, offset=116, len=3)
[12:27:51:298562][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.gre_key.value, offset=120, len=4)
[12:27:51:298575][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.gre.protocol, offset=118, len=2)
[12:27:51:298613][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.gtp.teid, offset=116, len=4)
[12:27:51:298628][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.nisp.hdr, offset=116, len=40)
[12:27:51:298641][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.audp.hdr, offset=116, len=24)
[12:27:51:298656][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.esp.spi, offset=116, len=4)
[12:27:51:298669][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.esp.sn, offset=120, len=4)
[12:27:51:298682][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.mpls.label.0, offset=116, len=4)
[12:27:51:298695][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.mpls.label.1, offset=120, len=4)
[12:27:51:298709][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.mpls.label.2, offset=124, len=4)
[12:27:51:298723][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.mpls.label.3, offset=128, len=4)
[12:27:51:298736][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.mpls.label.4, offset=132, len=4)
[12:27:51:298750][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.geneve.ver_opt_len, offset=116, len=1)
[12:27:51:298764][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.geneve.o_c, offset=117, len=1)
[12:27:51:298778][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.geneve.next_proto, offset=118, len=2)
[12:27:51:298791][DOCA][DBG][doca_flow_register:47]: Registered field opcode=match.packet.tunnel.geneve.vni, offset=120, len=3)
[12:27:51:298868][DOCA][INF][doca_flow_match:565]: Doca flow match UDS initialized
[12:27:51:298883][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.meta.data, offset=12, len=20)
[12:27:51:298896][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.meta.port, offset=36, len=4)
[12:27:51:298909][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.meta.mark, offset=40, len=4)
[12:27:51:298922][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.meta.nisp_syndrome, offset=44, len=1)
[12:27:51:298935][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.meta.ipsec_syndrome, offset=45, len=1)
[12:27:51:298948][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.eth.dst_mac, offset=54, len=6)
[12:27:51:298961][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.eth.src_mac, offset=48, len=6)
[12:27:51:298974][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.eth.type, offset=60, len=2)
[12:27:51:298988][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.eth_vlan0.tci, offset=64, len=2)
[12:27:51:299001][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.eth_vlan1.tci, offset=66, len=2)
[12:27:51:299014][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv4.src_ip, offset=72, len=4)
[12:27:51:299027][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv4.dst_ip, offset=76, len=4)
[12:27:51:299041][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv4.dscp_ecn, offset=81, len=1)
[12:27:51:299055][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv4.next_proto, offset=82, len=1)
[12:27:51:299070][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv4.ttl, offset=83, len=1)
[12:27:51:299084][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv4.version_ihl, offset=72, len=1)
[12:27:51:299098][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv6.src_ip, offset=72, len=16)
[12:27:51:299111][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv6.dst_ip, offset=88, len=16)
[12:27:51:299125][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv6.dscp_ecn, offset=104, len=1)
[12:27:51:299138][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv6.next_proto, offset=105, len=1)
[12:27:51:299153][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.ipv6.hop_limit, offset=106, len=1)
[12:27:51:299166][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.udp.src_port, offset=112, len=2)
[12:27:51:299179][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.udp.dst_port, offset=114, len=2)
[12:27:51:299193][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.tcp.src_port, offset=112, len=2)
[12:27:51:299271][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.tcp.dst_port, offset=114, len=2)
[12:27:51:299287][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.icmp4.type, offset=112, len=1)
[12:27:51:299301][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.icmp4.code, offset=113, len=1)
[12:27:51:299315][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.icmp4.ident, offset=114, len=2)
[12:27:51:299328][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.icmp6.type, offset=112, len=1)
[12:27:51:299342][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.outer.icmp6.code, offset=113, len=1)
[12:27:51:299356][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.vxlan.vni, offset=124, len=3)
[12:27:51:299372][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.gre_key.value, offset=128, len=4)
[12:27:51:299386][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.gre.protocol, offset=126, len=2)
[12:27:51:299399][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.gtp.teid, offset=124, len=4)
[12:27:51:299412][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.nisp.hdr, offset=124, len=40)
[12:27:51:299427][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.audp.hdr, offset=124, len=24)
[12:27:51:299442][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.esp.spi, offset=124, len=4)
[12:27:51:299455][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.esp.sn, offset=128, len=4)
[12:27:51:299469][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.mpls.label.0, offset=124, len=4)
[12:27:51:299482][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.mpls.label.1, offset=128, len=4)
[12:27:51:299497][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.mpls.label.2, offset=132, len=4)
[12:27:51:299509][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.mpls.label.3, offset=136, len=4)
[12:27:51:299523][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.mpls.label.4, offset=140, len=4)
[12:27:51:299558][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.geneve.ver_opt_len, offset=116, len=1)
[12:27:51:299579][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.geneve.o_c, offset=117, len=1)
[12:27:51:299595][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.geneve.next_proto, offset=118, len=2)
[12:27:51:299609][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.tunnel.geneve.vni, offset=120, len=3)
[12:27:51:299623][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.eth.dst_mac, offset=174, len=6)
[12:27:51:299636][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.eth.src_mac, offset=168, len=6)
[12:27:51:299650][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.eth.type, offset=180, len=2)
[12:27:51:299663][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.eth_vlan0.tci, offset=184, len=2)
[12:27:51:299677][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.eth_vlan1.tci, offset=186, len=2)
[12:27:51:299690][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv4.src_ip, offset=192, len=4)
[12:27:51:299703][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv4.dst_ip, offset=196, len=4)
[12:27:51:299717][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv4.dscp_ecn, offset=201, len=1)
[12:27:51:299730][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv4.next_proto, offset=202, len=1)
[12:27:51:299744][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv4.ttl, offset=203, len=1)
[12:27:51:299757][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv4.version_ihl, offset=200, len=1)
[12:27:51:299771][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv6.src_ip, offset=192, len=16)
[12:27:51:299785][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv6.dst_ip, offset=208, len=16)
[12:27:51:299798][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv6.dscp_ecn, offset=224, len=1)
[12:27:51:299811][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv6.next_proto, offset=225, len=1)
[12:27:51:299825][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.ipv6.hop_limit, offset=226, len=1)
[12:27:51:299839][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.udp.src_port, offset=232, len=2)
[12:27:51:299853][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.udp.dst_port, offset=234, len=2)
[12:27:51:299867][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.tcp.src_port, offset=232, len=2)
[12:27:51:299881][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.tcp.dst_port, offset=234, len=2)
[12:27:51:299894][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.icmp4.type, offset=232, len=1)
[12:27:51:299908][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.icmp4.code, offset=233, len=1)
[12:27:51:299921][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.icmp4.ident, offset=234, len=2)
[12:27:51:299935][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.icmp6.type, offset=232, len=1)
[12:27:51:299948][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.outer.icmp6.code, offset=233, len=1)
[12:27:51:300009][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.vxlan.vni, offset=244, len=3)
[12:27:51:300026][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.gre_key.value, offset=248, len=4)
[12:27:51:300040][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.gre.protocol, offset=246, len=2)
[12:27:51:300055][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.gtp.teid, offset=244, len=4)
[12:27:51:300068][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.nisp.hdr, offset=244, len=40)
[12:27:51:300080][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.audp.hdr, offset=244, len=24)
[12:27:51:300093][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.esp.spi, offset=244, len=4)
[12:27:51:300106][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.esp.sn, offset=248, len=4)
[12:27:51:300120][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.mpls.label.0, offset=244, len=4)
[12:27:51:300134][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.mpls.label.1, offset=248, len=4)
[12:27:51:300147][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.mpls.label.2, offset=252, len=4)
[12:27:51:300161][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.mpls.label.3, offset=256, len=4)
[12:27:51:300174][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.mpls.label.4, offset=260, len=4)
[12:27:51:300188][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.geneve.ver_opt_len, offset=116, len=1)
[12:27:51:300201][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.geneve.o_c, offset=117, len=1)
[12:27:51:300214][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.geneve.next_proto, offset=118, len=2)
[12:27:51:300227][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.encap.tunnel.geneve.vni, offset=120, len=3)
[12:27:51:300241][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.alter.crypto.nisp_id, offset=300, len=4)
[12:27:51:300255][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.alter.crypto.esp_id, offset=300, len=4)
[12:27:51:300268][DOCA][DBG][doca_flow_register:47]: Registered field opcode=actions.packet.alter.push.vlan_tci, offset=64, len=2)
[12:27:51:300405][DOCA][INF][doca_flow_actions:759]: Doca flow actions UDS initialized
[12:27:51:300424][DOCA][INF][doca_flow_layer:62]: Doca flow layer initialized
[12:27:51:300437][DOCA][INF][doca_flow:679]: Doca flow initialized successfully
[12:27:51:301408][DOCA][INF][utils_hash_table:123]: hash table a_tmplt_t port 0 created
[12:27:51:301476][DOCA][INF][utils_hash_table:123]: hash table p_tmplt_t port 0 created
[12:27:51:303115][DOCA][INF][utils_hash_table:123]: hash table dpdk_tbl_mgr port 0 created
[12:27:51:303258][DOCA][INF][utils_hash_table:123]: hash table grp_fwd port 0 created
[12:27:51:303285][DOCA][INF][dpdk_port:161]: match on IPsec ASO syndrome is not supported - port 0 error:Operation not supported
[12:27:51:303304][DOCA][INF][dpdk_port:303]: Dpdk port 0 initialized successfully with 9 queues
[12:27:51:390106][DOCA][DBG][dpdk_flow_hws_legacy:76]: Port configure: port_id=0 queue_depth=128 queues=9 meters=0 counters=2
[12:27:51:390208][DOCA][ERR][dpdk_flow_hws_legacy:81]: failed to configure flow hws port 0 - rte flow configure, type 1 message: port configure with incorrect steering mode
[12:27:51:390234][DOCA][ERR][dpdk_port_legacy:1167]: failed to create port 0 - configuration ret=-95
[12:27:51:390306][DOCA][INF][utils_hash_table:165]: hash table destroyed
[12:27:51:390836][DOCA][INF][utils_hash_table:165]: hash table destroyed
[12:27:51:390895][DOCA][INF][utils_hash_table:165]: hash table destroyed
[12:27:51:391552][DOCA][INF][utils_hash_table:165]: hash table destroyed
[12:27:51:391578][DOCA][INF][dpdk_port:463]: Dpdk port 0 destroyed successfully with 9 queues
[12:27:51:391595][DOCA][ERR][doca_flow:1405]: dpdk port creation failed
[12:27:51:391611][DOCA][ERR][flow_common:108]: Failed to start port: Unknown error
[12:27:51:391630][DOCA][ERR][FLOW_DROP:229]: Failed to init DOCA ports: Unknown error
[12:27:51:391667][DOCA][INF][doca_flow_match:577]: Doca flow match UDS destroyed
[12:27:51:391708][DOCA][INF][doca_flow_actions:771]: Doca flow actions UDS destroyed
[12:27:51:391723][DOCA][INF][doca_flow_layer:77]: Doca flow layer destroyed
[12:27:51:391737][DOCA][INF][dpdk_resource_manager:191]: Dpdk resource manager unregister completed
[12:27:51:391751][DOCA][INF][dpdk_flow:444]: Cleanup dpdk flow
[12:27:51:391764][DOCA][DBG][dpdk_flow_hws:72]: Cleanup dpdk flow HW steering module
[12:27:51:391776][DOCA][INF][dpdk_table:77]: Cleanup dpdk table
[12:27:51:391789][DOCA][DBG][dpdk_table_hws:914]: Cleanup dpdk table HW steering module
[12:27:51:391802][DOCA][INF][dpdk_layer:416]: Dpdk layer unregister completed
[12:27:51:391816][DOCA][INF][dpdk_pipe_items:187]: Destroyed dpdk pipe items module
[12:27:51:391830][DOCA][INF][engine_field_mapping:336]: Engine field mapping destroyed
[12:27:51:391851][DOCA][INF][engine_model:258]: engine model destroyed
[12:27:51:391864][DOCA][INF][doca_flow:694]: Doca flow destroyed
[12:27:51:391876][DOCA][ERR][FLOW_DROP::MAIN:77]: flow_drop() encountered errors
Tx port 0 is already stopped
[12:27:51:391968][DOCA][ERR][NUTILS:113]: Failed to bind hairpin queues (-16)
[12:27:51:391986][DOCA][ERR][NUTILS:200]: Disabling hairpin queues failed: err=21, port=0
Tx port 0 is already stopped
[12:27:51:392037][DOCA][ERR][NUTILS:126]: Failed to bind hairpin queues (-16)
[12:27:51:392052][DOCA][ERR][NUTILS:200]: Disabling hairpin queues failed: err=21, port=1
Device with port_id=0 already stopped
[12:27:52:692618][DOCA][DBG][NUTILS:1109]: DPDK fini is doneport configure with incorrect steering mode,how can I configyre the correct steering mode?
can anyone help me?
ThanksI have solved this problem by add dv_flow_en=2 into eal below./doca_flow_hairpin -a auxiliary:mlx5_core.sf.4,dv_flow_en=2 -a auxiliary:mlx5_core.sf.5,dv_flow_en=2 – -l 60cause the cfg is “vnf,hws”,now it seems work correctlyPowered by Discourse, best viewed with JavaScript enabled"
574,max-clock-info-update-nsec-field-in-mlx5dv-h-seems-in-us-rather-then-ns-unit,"I use v5.8-2.0.3.0 LTS driver and CX-5 and CX-6 Dx NIC card.I want to get the clock info from NIC.I have check themlx5dv_get_clock_info man doc:If the clock_info becomes too old then time conversion will return wrong conversion results. The user mustensure that mlx5dv_get_clock_info(3) is called at least once every max_clock_info_update_nsec as returned by the mlx5dv_query_device(3) function.So I use mlx5dv_query_device to get max_clock_info_update_nsec:mlx5dv_query_device return success, and I check the mlx5dv_attr.max_clock_info_update_nsec field.I found this field is about 1100000 on CX-5 card and 700000 on CX-6 Dx card. If the unit is nano second, it means I should update clock info every 1ms? It’s too short and unreasonable.The old dirver doc says we need to update clock info every 1s, not 1ms on CX-5 NIC. So I think the unit should be us rather than ns.I think the filed name should be fixed to max_clock_info_update_usec, or driver should filling real ns value into it.Related code in RDMA-core:Hello g199209,Welcome, and thank you for posting your inquiry to the NVIDIA Developer Forums!This request will require engineering input and effort, as such you will need to open a support ticket with NVIDIA Enterprise Experience so the appropriate teams can be formally engaged.If you have valid support entitlement, please open a ticket via our enterprise portal: https://enterprise-support.nvidia.com/s/create-case. Our support teams will be able to assist you with this request.Best regards,
NVIDIA Enterprise ExperiencePowered by Discourse, best viewed with JavaScript enabled"
575,sniffer-tool-that-captures-high-speed-roce-traffic-for-linux,"We are trying to capture RoCE traffic moving at high speed (target speed is 50+GiB) over a mirrored port for debugging purposes. On Windows the mlx5cmd Sniffer worked ok, but it started losing packets at higher speeds and iteration counts. Since we knew that interoperability might be a problem, we switched to Linux.The ibdump tool does not work (gives the error “command interface bad param”, and tcpdump only catches RoCE packets when we throttle the speed to 2.5GiB (and even then it’s losing packets). At any higher speed it only catches the TCP packets at the beginning and endIs there a tool similar to mlx5cmd -Sniffer for Linux? Alternatively, is there source code available for mlx5cmd somewhere that I haven’t looked yet?We are running on Red Hat using a Mellanox ConnectX-5 adapter, and we are currently using ib_send_bw to generate test traffic.I would recommend to check few thingsOn my setup, tcpdump doesn’t lose the packets and I’m running tcpdump on the same host as the ib_write_bw client. Speed is about 90 Gbps[root@XXXX ~]# mlnx_perf -i ens1f0 | egrep [r]x_bytesrx_bytes_phy: 11,361,121,028 Bps = 90,888.96 Mbpsrx_bytes_phy: 11,359,701,018 Bps = 90,877.60 Mbpsrx_bytes_phy: 10,054,267,618 Bps = 80,434.14 Mbpsrx_bytes_phy: 10,486,318,032 Bps = 83,890.54 Mbpsrx_bytes_phy: 11,351,984,490 Bps = 90,815.87 Mbpsrx_bytes_phy: 11,354,548,936 Bps = 90,836.39 Mbpsrx_bytes_phy: 11,352,971,872 Bps = 90,823.77 Mbps[root@XXXX ~]# tcpdump -i mlx5_0 -etn -c 1000000 -s 65535 | head -n 5tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on mlx5_0, link-type EN10MB (Ethernet), capture size 65535 bytes24:8a:07:9c:01:86 > 02:48:a0:79:c0:1a, ethertype IPv4 (0x0800), length 1082: 192.168.150.3.58171 > 192.168.150.4.roce: UDP, length 104024:8a:07:9c:01:86 > 02:48:a0:79:c0:1a, ethertype IPv4 (0x0800), length 1082: 192.168.150.3.58171 > 192.168.150.4.roce: UDP, length 104024:8a:07:9c:01:86 > 02:48:a0:79:c0:1a, ethertype IPv4 (0x0800), length 1082: 192.168.150.3.58171 > 192.168.150.4.roce: UDP, length 104024:8a:07:9c:01:86 > 02:48:a0:79:c0:1a, ethertype IPv4 (0x0800), length 1082: 192.168.150.3.58171 > 192.168.150.4.roce: UDP, length 104024:8a:07:9c:01:86 > 02:48:a0:79:c0:1a, ethertype IPv4 (0x0800), length 1082: 192.168.150.3.58171 > 192.168.150.4.roce: UDP, length 1040tcpdump: Unable to write output: Broken pipe[root@XXXX ~]# tcpdump -i mlx5_0 -etn -c 1000000 -s 65535 | tail -n 5tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on mlx5_0, link-type EN10MB (Ethernet), capture size 65535 bytes1000000 packets captured1000000 packets received by filter0 packets dropped by kernel24:8a:07:9c:01:86 > 02:48:a0:79:c0:1a, ethertype IPv4 (0x0800), length 1082: 192.168.150.3.58171 > 192.168.150.4.roce: UDP, length 104024:8a:07:9c:01:86 > 02:48:a0:79:c0:1a, ethertype IPv4 (0x0800), length 1082: 192.168.150.3.58171 > 192.168.150.4.roce: UDP, length 104024:8a:07:9c:01:86 > 02:48:a0:79:c0:1a, ethertype IPv4 (0x0800), length 1082: 192.168.150.3.58171 > 192.168.150.4.roce: UDP, length 104024:8a:07:9c:01:86 > 02:48:a0:79:c0:1a, ethertype IPv4 (0x0800), length 1082: 192.168.150.3.58171 > 192.168.150.4.roce: UDP, length 104024:8a:07:9c:01:86 > 02:48:a0:79:c0:1a, ethertype IPv4 (0x0800), length 1082: 192.168.150.3.58171 > 192.168.150.4.roce: UDP, length 1040Thank you for your reply,We tried running tcpdump with your flags on the machine running the ib_send_bw client, and still lost a significant number of packets.We’ve tried a couple of other things as well, including tuned (tune deamon) and setting the priority of tcpdump.EDIT: What was your ib_send_bw line for the client? Correct me if I’m wrong, but if you don’t add an iterations value wouldn’t that mean it would run until you kill it, which would allow tcpdump to collect the requested number of packets even if it drops a few?Powered by Discourse, best viewed with JavaScript enabled"
576,dpdk-testpmd-no-packets-are-exchanged,"Hello,
We have ARM server with Connectx-4 Nic. When we run testpmd application,  no packets are exchanged, all counters are zeros.Platform ARM Ampere Ultra
OS Ubuntu 22.04
Kernel  5.15.0-71-lowlatency
OFED Version: MLNX_OFED_LINUX-5.8-2.0.3.0 (OFED-5.8-2.0.3)
DPDK: 20.05
Mellanox NIC info-
MT27710 Family [ConnectX-4 Lx] 1015
Firmware: 14.32.1010
Vendor id: 0x02c9
board_id: MT_2420110034./dpdk-testpmd -l 1,2,3 -n 4 -w 0003:03:00.1 -w 0003:03:00.0 – --rxq=1 --txq=1 -i
…
…
…
testpmd> start
io packet forwarding - ports=2 - cores=1 - streams=2 - NUMA support enabled, MP allocation mode: native
Logical Core 2 (socket 0) forwards packets on 2 streams:
RX P=0/Q=0 (socket 0) → TX P=1/Q=0 (socket 0) peer=02:00:00:00:00:01
RX P=1/Q=0 (socket 0) → TX P=0/Q=0 (socket 0) peer=02:00:00:00:00:00io packet forwarding packets/burst=32
nb forwarding cores=1 - nb forwarding ports=2
port 0: RX queue number: 1 Tx queue number: 1
Rx offloads=0x0 Tx offloads=0x0
RX queue: 0
RX desc=256 - RX free threshold=0
RX threshold registers: pthresh=0 hthresh=0  wthresh=0
RX Offloads=0x0
TX queue: 0
TX desc=256 - TX free threshold=0
TX threshold registers: pthresh=0 hthresh=0  wthresh=0
TX offloads=0x0 - TX RS bit threshold=0
port 1: RX queue number: 1 Tx queue number: 1
Rx offloads=0x0 Tx offloads=0x0
RX queue: 0
RX desc=256 - RX free threshold=0
RX threshold registers: pthresh=0 hthresh=0  wthresh=0
RX Offloads=0x0
TX queue: 0
TX desc=256 - TX free threshold=0
TX threshold registers: pthresh=0 hthresh=0  wthresh=0
TX offloads=0x0 - TX RS bit threshold=0
testpmd> stop
Telling cores to stop…
Waiting for lcores to finish…+++++++++++++++ Accumulated forward statistics for all ports+++++++++++++++
RX-packets: 0              RX-dropped: 0             RX-total: 0
TX-packets: 0              TX-dropped: 0             TX-total: 0
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Any hints, highly appreciatedThank you,
RamanHello @raman,Thank you for posting your query on our community. I would like to refer you to the Tested Platforms section in the DPDK 20.05 Release Notes.
https://doc.dpdk.org/guides/rel_notes/release_20_05.html#tested-platformsAs mentioned here, ARM servers with Mellanox ConnectX-4 Lx NIC is not a tested and supported platform for DPDK 20.05. Please refer to the Release Notes to ensure that you are using a validated and supported platform.Thank you,
BhargaviPowered by Discourse, best viewed with JavaScript enabled"
577,can-i-create-flow-matching-rules-for-ethernet-layers-above-l4-using-mlnx-ofed,"I am using ibverbs to receive raw Ethernet frames from ConnectX-5 and ConnectX-6 NICs. I have a flow of packets that I would like to process in a particular way. The packets are part of a single UDP/IPv4 flow, but there are two kinds of packets in the stream: most of them are large data payload packets, with occasional (much smaller) metadata packets mixed in. They all have the same source/destination IP/MAC/port information, so if I create an IBV flow using that, it matches all of the packets in my flow (the large ones and the small ones).If possible, I would like to create separate flow matching rules for these two streams of packets so I can direct them to be written to different memory regions. In order to do this, I obviously would have to use some part of the packet to discriminate the two. The two kinds of packets that I want to separate, unfortunately, are identical from layers 2 through 4 (the UDP header). From what I can tell, the ibverbs API only provides matching functionality up through layer 4, so I get all of the packets in one flow.I can think of two criteria that I would try to use to differentiate the packets:Is there any functionality, perhaps at the mlx5dv layer, that would allow me to create matching rules that can go past the UDP header? I have dug through the rdma-core source code a bit and found some references to “flex rules”, “misc4 matchers”, and some other items that seem like they might be applicable to my situation, but the documentation there is pretty sparse, so it wasn’t clear.Unfortunately, this functionality is currently not supported, nor we do not have the information to share that this will be on future roadmaps.Powered by Discourse, best viewed with JavaScript enabled"
578,connect-linux-ptpd-to-connectx-4-port,"Trying to follow instructions for this as described here:
https://mymellanox.force.com/mellanoxcommunity/s/article/running-ptpd-with-connectx-4That includes steps to check the status of the interface including the capabilities.From the instructions:
2. Make sure that PTP is enabled on the desired interface (in this example, cx4p1):Time stamping parameters for cx4p2:Capabilities:
hardware-transmit (SOF_TIMESTAMPING_TX_HARDWARE)
software-transmit (SOF_TIMESTAMPING_TX_SOFTWARE)
hardware-receive (SOF_TIMESTAMPING_RX_HARDWARE)
software-receive (SOF_TIMESTAMPING_RX_SOFTWARE)
software-system-clock (SOF_TIMESTAMPING_SOFTWARE)
hardware-raw-clock (SOF_TIMESTAMPING_RAW_HARDWARE)
PTP Hardware Clock: 0
Hardware Transmit Timestamp Modes:
off (HWTSTAMP_TX_OFF)
on (HWTSTAMP_TX_ON)
Hardware Receive Filter Modes:
none (HWTSTAMP_FILTER_NONE)
all (HWTSTAMP_FILTER_ALL)***** On my system, the output is same except the Capabilities list is missing:
software-transmit (SOF_TIMESTAMPING_TX_SOFTWARE)
software-receive (SOF_TIMESTAMPING_RX_SOFTWARE)
software-system-clock (SOF_TIMESTAMPING_SOFTWARE)Here’s the result from my system:Time stamping parameters for enp216s0f1:
Capabilities:
hardware-transmit     (SOF_TIMESTAMPING_TX_HARDWARE)
hardware-receive      (SOF_TIMESTAMPING_RX_HARDWARE)
hardware-raw-clock    (SOF_TIMESTAMPING_RAW_HARDWARE)
PTP Hardware Clock: 5
Hardware Transmit Timestamp Modes:
off                   (HWTSTAMP_TX_OFF)
on                    (HWTSTAMP_TX_ON)
Hardware Receive Filter Modes:
none                  (HWTSTAMP_FILTER_NONE)
all                   (HWTSTAMP_FILTER_ALL)The next step of verification doesn’t report any PTP packets:************ Is there a way to enable the missing capabilities and is that all that’s missing to get this to work.
Thanks.You can refer below config PTP HW clock on NIC,https://support.mellanox.com/s/article/running-ptpd-with-connectx-4In the mean time, PTP need MLX switch configuration to support,https://docs.nvidia.com/networking/pages/viewpage.action?pageId=34261863This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
579,configure-connectx-with-the-mlxreg-mcra-utility,"Hello, I have two questions that are somewhat related.I was studying the script disable_wqe_checks.sh. It uses mcra to set a bunch of registers. Is there any resource available to understand what these registers are and what the values correspond to. Perhaps there is a way to translate the address given to mcra to registers reported by mlxreg.Secondly, I am trying to use the mlxreg utility to configure the endianness of the fetch-add verb (I want the output to be big-endian). I found the register HOST_ENDIANNESS that seemed like it would configure that. However, when I try to change it’s value, I get the error: ME_ICMD_OPERATIONAL_ERROR.I have tried it on ConnectX-5 and 6 as well. Does anyone know a solution to any of these two problems?Hello 5hubh4m,Thank you for posting your inquiry to the NVIDIA Developer Forums.Is there any resource available to understand what these registers are…Unfortunately, this is proprietary information, and cannot be shared.Secondly, I am trying to use the mlxreg utility to configure the endianness of the fetch-add verb (I want the output to be big-endian).Unfortunately, this is a non-standard configuration, and we do not support this.
Use of the mcra utility to modify adapter register values should only be done under direct instruction from NVIDIA engineering.If you require our products to operate in a capacity that they are not currently designed to operate, this is a customization consideration which we can only assist with through our solutions engineering team.You can reach out to our solutions engineering team via a request to our sales team. They will be able to assist you with developing a solution that fits your unique business needs.Thanks, and best regards,
NVIDIA Enterprise SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
580,switching-between-separated-and-embedded-mode,"Hi all,
When switching from separated mode to the embedded mode all pre-configured interfaces and ovs bridges are gone, and I can not access internet, do you know how to configure  the ovs bridges to access internet? Should I redo the doca installation?
Thanks for your timeNVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.Can you check that last step there?Note: If OVS bridges ovsbr1 and ovsbr2 are not created (ovs-vsctl show) make sure CREATE_OVS_BRIDGES=""yes"" in /etc/mellanox/mlnx-ovs.conf.Is that file there? If so can you set it to yes and power cycle the server?/etc/mellanox/mlnx-ovs.confYes, thank you the file is there and it is working now :)This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
581,lro-on-hairpin-queue,"Hi.Is there any way to make hairpin queue do LRO in connectX-6?Thank you in advance.Not sure what’s details of the requirement. there is hw support of LRO checked by ""ethtool -k ""
And many way make hairpin, link tc, ovs, dpdk etc.I want DPDK hairpin queue to do LRO without DMA.
I thought that LRO is not only done by HW.In mlx5 pmd code, mlx5_lro_update_hdr() updated the headers of LROed packets.
So I guessed hairpin queue cannot do LRO for incoming TCP traffic since the packets are not DMAed to host in case of hairpin queue.I turned on LRO feature on DPDK port, and setup a hairpin queue for that port, but I checked that the outgoing
traffic from hairpin TX queue is not LROed.So I wonder if there is any way for LRO without DMA.Powered by Discourse, best viewed with JavaScript enabled"
582,ovs-cannot-detect-the-bluefield-interface-but-the-bluefield-card-works-correctly,"Hi,I get two machines with each installed a bluefiled SmartNIC from Cloudlab. I changed one SmartNIC’s mode to ECPF mode. The other one is in Sperated host mode. I assigned two ips for the two SmartNIC interfaces on the host, two different ips for the other host. Then on each host ping the other host, ping success. However, when I login the ECPF mode SmarNIC and run sudo ovs-vsctl show, it shows:It means that the OVS cannot detect the SmartNIC interfaces, right? Why is that and how to solve this problem? Thanks for your help.Update: It was very strange, I just deleted the flow rule from ovsbr1, and ping from the x86 host to another x86 host and it still sccuess.  How that would be if all packets need go throught the OVS and then go to another host, how the OVS still works if I deleted all rules for ovsbr1.Hi Lyuxiaosu,Thank you for posting your query on our forum.I would like to check if you got an opportunity to review our following documentation in order to confirm configuration was done as per the document
a. https://docs.nvidia.com/networking/display/BlueFieldDPUOSv385/Modes+of+Operation#ModesofOperation-SmartNICmodeEmbeddedCPUFunctionOwnershipModeb. https://docs.nvidia.com/networking/display/BlueFieldDPUOSv385/Virtual+Switch+on+BlueField+DPUIf yes, and issue is still seen, I would like to request opening a support ticket since this might require in-depth debug and possible reproduction. In case you do not have an active contract with us, you may reach out to our contracts team at networking-contracts@nvidia.com . The support ticket can be opened by emailing Networking-support@nvidia.comThanks,
Namrata.Hi Namrata,Thanks for your reply. I read the documents and confirmed the mode is Embedded mode. The problem is PF(pf0hpf/pf1hpf) and SF(enp3s0f0s0/en3f1pf1sf0) ports cannot show up in BlueField OS.  Run ifconfig or lshw to show all interfaces, there were no such interfaces. That’s what OVS complains. I tried several ways to debug this problem, I reinstalled the OS on BlueField 2, reset the NIC with command mlxconfig -d /dev/mst/mt41686_pciconf0 -y reset, but not working, the issue is still there.I got two machines installed BlueField 2 SmartNIC from Cloudlab, one machine works correctly and can show all interfaces, the other one cannot. By comparing these two machines, I found the not working one does not have parameters  ECPF_ESWITCH_MANAGER and ECPF_PAGE_SUPPLIER. When switched to the Embedded mode and run command mlxconfig -d /dev/mst/mt41686_pciconf0 s ECPF_ESWITCH_MANAGER=1 ECPF_PAGE_SUPPLIER=1, it complains The Device doesn't support ECPF_ESWITCH_MANAGER parameter.I guess this relates to the problem.Should I email Networking-support@nvidia.com to describe the problem?Thanks,
Xiaosu.Hi Xiaosu,Please share the following outputs from both, working and non-working machine:a. #ibv_devinfo -v |egrep ‘board|fw’
b. From the ARM, #cat /etc/mlnx-releaseIf this requires extensive debug and in the case you submit a support ticket without an active support contract in place, unfortunately, debug would be restricted.Thanks,
Namrata.Hi Namrata,Here is the result:
Run ibv_devinfo -v |egrep ‘board|fw’ on the host machine of both the working one and not the working one, the result is the same:Run ibv_devinfo -v |egrep ‘board|fw’ on the ARM of the working one, the result is:Run ibv_devinfo -v |egrep ‘board|fw’ on the ARM of not the working one, the result is:Run cat /etc/mlnx-release on the ARM of both the working and not working machine, the result is the same, both are:Thanks for your reply.
Xiaosu.Hi Xiaosu,The PSID of card on both hosts is identical which means both are identical and on one machine it supports setting ECPF_ESWITCH_MANAGER=1 but not on other?I wouldn’t expect the cards being same to behave differently. I would recommend opening a support ticket, however, please note that in case we determine that you do not possess an active contract level, support would be restricted.Thanks,
Namrata.mlxconfig -d /dev/mst/mt41686_pciconf0 -y resetHi,I’m facing the same problem. Can you tell me if (and how) you solved it?Hi,Unfortunately I didn’t solve that issue and gave it up. It might be a hardware related problem but I don’t know.Sincerely,
Xiaosu.DEVICE_TYPE             MST                           PCI       RDMA            NET                       NUMA
BlueField2(rev:1)       /dev/mst/mt41686_pciconf0.1   03:00.1   mlx5_1          net-pf1hpf,net-en3f1pf1sf0,net-p1-1BlueField2(rev:1)       /dev/mst/mt41686_pciconf0     03:00.0   net-p0                    -1
`Powered by Discourse, best viewed with JavaScript enabled"
583,nvidia-native-esxi-drivers-management-tool-nmlxcli-v1-17-14-2-are-not-downloadable,"Hi Team,
Is there a workaround for the Mellanox Management Tool version NMLXCLI v1.17.14.2 download?
I’m trying to download this tool:https://network.nvidia.com/products/ethernet-drivers/vmware/esxi-server/downloads/Software/MLNX-NATIVE-NMLXCLI_1.17.14.2-10EM-670.0.0.8169922.zip
but then get the error message file not found?Any ideas how to fix that?Thank you in advance,RichardHi Richard,Thank you for posting your query on NVIDIA Community.We are working to get this fixed. Please check back in a couple of days.We appreciate your patience and cooperation.Thanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
584,how-to-connect-hdr-switch-to-older-adapter,"I have a QM8700 HDR switch and am having trouble connecting some older IB hardware to it. The older hardware is an MT26438 ConnectX VPI adapter with QDR Infiniband, built into an HP ProLiant SL390s G7 server node (HP NC543i adapter, part number MHQH29B-XTR).I tried connecting them with a MC2207128-003 cable. I can use this cable to successfully connect the QM8700 to an IB HDR adapter, and I can also connect the ProLiant QDR to an identical node. But I can’t connect the HDR switch to the ProLiant’s QDR.Is this connection possible? Is a different cable needed? Thanks.Hi Ben,The ConnectX HCA which is EOL for a few years wasn’t tested vs the HDR switches, so we can’t assue it will work (and we see it doesn’t). also the MC2207128-003 was never tested with MHQH29B-XTR HCA.For a connectivity matrix , I would follow:https://docs.mellanox.com/display/MellanoxQuantumFirmwarev2720082402/Firmware+Compatible+Products → Connectivity Matrixand Supported Link Speedaccording to the support link speed table, you should use a different cable PN:QDROptical3/10/15/100QDR speed is only supported when using the MC220731V-xxx FDR cablesCopperUp to 3Powered by Discourse, best viewed with JavaScript enabled"
585,sn2100-with-sonic,"I received two NVIDIA SN2100 switches. I tried to install Sonic from the Github page.
The installation went well, but after I noticed the vent runs at 100% and the status led turned red. Is there any known fix for this problem?It is not clear what version you’ve installed, but there is one known issue regarding with fans. Please see more details below:<!--
     Please make sure you've read and understood our contributing guidelin…es:
     https://github.com/Azure/SONiC/blob/gh-pages/CONTRIBUTING.md

     ** Make sure all your commits include a signature generated with `git commit -s` **

     If this is a bug fix, make sure your description includes ""fixes #xxxx"", or
     ""closes #xxxx"" or ""resolves #xxxx""

     Please provide the following information:
-->

#### Why I did it

Optimize thermal control policies to simplify the logic and add more protection code in policies to make sure it works even if kernel algorithm does not work.

#### How I did it

1. Reduce unused thermal policies
2. Add timely ASIC temperature check in thermal policy to make sure ASIC temperature and fan speed is coordinated
3. Minimum allowed fan speed now is calculated by max of the expected fan speed among all policies
4. Move some logic from fan.py to thermal.py to make it more readable

#### How to verify it

1. Manual test
2. Regression

#### Which release branch to backport (provide reason below if selected)

<!--
- Note we only backport fixes to a release branch, *not* features!
- Please also provide a reason for the backporting below.
- e.g.
- [x] 202006
-->

- [ ] 201811
- [ ] 201911
- [ ] 202006
- [ ] 202012
- [ ] 202106

#### Description for the changelog
<!--
Write a short (one line) summary that describes the changes in this
pull request for inclusion in the changelog:
-->


#### A picture of a cute animal (not mandatory but encouraged)If you already have versions mentioned in there, then please open a case with tech dump for further troubleshooting.Thanks, I installed cumulus as a solution, it is working as expected.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
586,how-to-do-l2-east-west-traffic-with-ipl-and-mlag,"Hi,We have multiple pairs of HPe SN2410M switches (they’re just Mellanox switches painted a different colour, hence why I 'm here!). We have them configured as MLAG pairs with a VIP, and they work well. We’re upgrading them to Onyx v3.9.1306 at the moment.We also have some Nexus 9K switches in exactly the same configuration - MLAG pairs (or vPC as Cisco call it). In all cases all the L3 routing is done by core switches upstream. These switches are all top-of-rack for servers, so everything is MLAGs down to the hosts. The HPe switches have mgmt0 connected back to back for the VIP, and the IPL goes over a port-channel of two 100G DACs.However, one major difference I notice between the HPe and the Nexus is that the switch-to-switch connectivity is very different. The Nexus will put traffic through the connection between the switches, so there’s East-West communication. The HPes don’t - the documentation clearly says the IPL will only transfer traffic in a failure scenario (i.e. the uplinks die).All I’m getting at is we have two 100G connections doing relatively nothing - I could’ve put in 10G and saved a whole load of money!Can we/should we do anything to enable the 100G DACs to be useful for traffic? Or is this just how it is with these switches? I’ve considered putting in a couple of 10G connections and moving the IPL over to them so I can then use the 100G DACs for a L2 MLAG, but then there’s all the fun of Spanning Tree loops to consider (something the Nexus figures out for itself and Just Works without any major fiddling).What’s considered the right thing to do here? We’re running Nutanix Hyperconvergence if that makes any difference, but there’s other servers also attached.Thanks in advance.Hi David,traffic will pass over the IPL in the following cases:@David Rickard​ , did you end up adding an L2 transit link between the switches? I am facing a similar dilemma, while I will be point-to-point routed northbound into a pair of Cisco Nexus switches, I need to peer OSPF with them as well.Normally, I would peer the two OSPF processes between the MLAG’d switches as well (as we would with Nexus vPC, Aruba VSX, et. al.), so the advertised topology remains consistent, but the published IPL behavior has me searching for answers. Do I let an MLAG port failure trigger an OSPF topology change (because the VLAN(s) will now transit the IPL, an OSPF DR should be elected, etc.), or let STP knock down the loop formed if I have an L2 transit in play (and would the IPL even participate in STP?).Eddie, sorry for not replying sooner. I thought I’d get a notification and never did. I just googled this again and found my own post!I had a look and I can see some MACs in the address table via the IPL, so I think I can now see what’s going on. For single-homed devices they definitely do traverse the IPL. For MLAGs they seem to show the same MAC on both switches via the MLAG, so I’m guessing the switch is smart enough to know when to keep it on the local switch, or send it across the IPL? It certainly seems that doesn’t happen often as there’s only around 7Mb/s going over it. Yet there’s two HCI clusters on it so all their traffic must be staying local to one switch (possibly the active). Either way I don’t think I’m seeing traffic hair-pinning via the core switch, which was really all I wanted to avoid.Powered by Discourse, best viewed with JavaScript enabled"
587,result-of-write-is-incorrect-with-connectx-6-nvmeof-target-offloading,"Hi,Previously I posted a similar topic in the OFED forum. In case it’s a hardware bug, I also repost it here. Sorry for any inconvinience.We found that when we issue large direct write to the target with NVMeoF offloading, the content won’t be written to the disk correctly. Some blocks will be replaced by other blocks in the result. For example, we constructed a 17MB buffer by filling every block with a formatted string (‘blk%05d’, block_index) and wrote the buffer all at once to the target. But when reading it, we found some out-of-order and repeated blocks. For example, in the attachment ‘output.txt’, if grepping ‘wrong’, we can see that blk27135 appears right after blk16382 (instead of blk16383) and again at its own position (following blk27134). Moreover, if we issue smaller writes, e.g., 16KB, this situation doesn’t happen (but the performance is much lower).For more information, we installed MLNX_OFED_LINUX-5.6-2.0.9.0-ubuntu20.04-x86_64 drivers on both host and target. The OS is Ubuntu 20.04 LTS, and we use Ext4 as the filesystem. The NIC is Dual-port Mellanox ConnectX-6 100 Gb NIC (PCIe v4.0). We also pasted our testing program after this post (I can only attach one link as a new user).The way we reproduce the bug is to run ‘./simulator /mnt/remote-sst/2/45 17825792 > output.txt’ on the host, where /mnt/remote-sst is an LVM partition on the target NVMe disk connect by NVMeoF.Does anyone know why this behavior would happen and how to fix/avoid it? Any thoughts are highly appreciated!Testing program:likely, your test involve filesystem and soft-raid LVM, you know both of that will re-allocate block base on  device block size and fs config.https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout#BlocksIn theory, RDMA NVMEoF will not re-allocate block, since it “not seen” that, it just send/execute iscsi cmds.Hi Xiaofeng, Thanks for your reply! Yes, I realized that LVM might be reason. Previously we only considered extent mapping in ext4 (we fixed the extents by some workarounds). After changing to static disk partition (via fdisk) and recreating the filesystem, we haven’t seen the behavior for a long time.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
588,ptp-on-sriov-vf-connect-x-6-dx,"I’m trying to synchronize my VM using ptp4l using Connect-x 6 DX VF
platform:  VMware  ESXi 7.0.3gwhen using PCI passthrough device to the VM, there is no issue and ptp4l is running normally, timestamping is done.When using SRIOV VF (interface name is splane0)~]$ ethtool -T splane0
Time stamping parameters for splane0:
Capabilities:
hardware-transmit     (SOF_TIMESTAMPING_TX_HARDWARE)
hardware-receive      (SOF_TIMESTAMPING_RX_HARDWARE)
hardware-raw-clock    (SOF_TIMESTAMPING_RAW_HARDWARE)
PTP Hardware Clock: 0
Hardware Transmit Timestamp Modes:
off                   (HWTSTAMP_TX_OFF)
on                    (HWTSTAMP_TX_ON)
Hardware Receive Filter Modes:
none                  (HWTSTAMP_FILTER_NONE)
all                   (HWTSTAMP_FILTER_ALL)~]$ cat /etc/ptp4l.conf
[global]
clock_type OC
time_stamping hardware
slaveOnly 1
priority1 128
priority2 128
domainNumber 24
logging_level 6
verbose 1
use_syslog      0
step_threshold  0.002
logAnnounceInterval -3
logSyncInterval -4
logMinDelayReqInterval -4
delay_mechanism E2E
network_transport L2
tsproc_mode raw
tx_timestamp_timeout 100~]$ sudo /usr/sbin/ptp4l -f /etc/ptp4l.conf -i splane0
ptp4l[8226.934]: selected /dev/ptp0 as PTP clock
ptp4l[8226.950]: port 1: INITIALIZING to LISTENING on INIT_COMPLETE
ptp4l[8226.950]: port 0: INITIALIZING to LISTENING on INIT_COMPLETE
ptp4l[8227.072]: port 1: new foreign master 8c47be.ffff.d34ac1-4
ptp4l[8227.396]: selected local clock 005056.fffe.33c210 as best master
ptp4l[8227.574]: selected best master clock 8c47be.ffff.d34ac1
ptp4l[8227.574]: port 1: LISTENING to UNCALIBRATED on RS_SLAVE
ptp4l[8228.262]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8228.510]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8228.574]: rms 36852582060 max 36852585277 freq -11737677 +/- 8933 delay   752 +/- 1329
ptp4l[8228.760]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8229.014]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8229.262]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8229.510]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8229.574]: rms 36852585199 max 36852586827 freq -11744817 +/- 3659 delay   386 +/-  27
ptp4l[8229.760]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8230.136]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8230.384]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8230.638]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8230.702]: rms 36852588999 max 36852590657 freq -11757884 +/- 4141 delay   388 +/-  36
ptp4l[8230.886]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8231.262]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8231.510]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8231.760]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8231.760]: rms 36852592758 max 36852594423 freq -11770817 +/- 3519 delay   386 +/-  35
ptp4l[8232.014]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8232.262]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8232.510]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8232.760]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8232.824]: rms 36852596257 max 36852598081 freq -11783603 +/- 4253 delay   371 +/-  29
ptp4l[8233.136]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8233.390]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8233.638]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8233.886]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8233.886]: rms 36852600094 max 36852601785 freq -11797722 +/- 4289 delay   366 +/-  30
ptp4l[8234.136]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8234.390]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8234.638]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8234.886]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8234.886]: rms 36852603596 max 36852605259 freq -11811406 +/- 4131 delay   370 +/-  27
ptp4l[8235.136]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8235.390]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8235.638]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8235.950]: rms 36852607141 max 36852608937 freq -11823798 +/- 3972 delay   375 +/-  20
ptp4l[8236.014]: clockcheck: clock jumped backward or running slower than expected!
ptp4l[8236.262]: clockcheck: clock jumped backward or running slower than expected!rms stays high there no actual change in time.will appreciate any feedback or ideas regarding this behavior.Hi Moshe,Thank you for posting your query on our community.Based on my internal check within the system, I see that you have already opened a support ticket (00552101: Connectx-6 DX HW time stamping not happening on VF) wherein you are working on getting the Enterprise Support Contract in place.I would like to share the following update regarding the issue described:For VMWare we are only using BlueField-2 HW solution - the PTP is running inside the DPU which allow all the VMs to get accurate time with no dependency on the ESXi.Unfortunately, we do not test PTP on VM’s . But, you need to ensure that each VM will need to run PTP client.If further troubleshooting is needed with current configuration you are using, I would like to request working via the support ticket once the Support contract is Active.Thanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
589,mlx5muxtool-is-lacp-possible-on-windows,"Hello team,
I have a connectx-6 lx adapter on a windows 11 pro machine. I created a team using mlx5muxtool.exe  for type i chose Aggregate. What exactly does aggregate mean in this context. Is this a static LAG?
I would love to get LACP working. My switch (Arista) was waiting for an LACP response on the ports involved, but never got one. Putting the port-channel on the Switch in a static LAG works fine.
Please let me know if LACP is possible with mlx5muxtool.best,
AndreasHi Andreas,Mlx5muxtool can be used to create teamed interfaces in Windows client / workstation.Aggregate mode means that you can create a static LAG which usually on network switches is called “port-channel mode on”.LACP is not supported in mlx5muxtool.Thank you and regards,YogevPowered by Discourse, best viewed with JavaScript enabled"
590,disable-roce-icrc-validation,"Hello,Is there a way to disable the RoCEv2 iCRC validation on ConnectX-6 Dx (MT4125 - MCX623106AN-CDAT) 100G NICs?I’d greatly appreciate your support!Thank you, in advance!
HamedHi HamedYes, it’s possible to disable ICRC on ConnectX-6 Dx.
For the details, please open a CASE. Because there are something to consider it./HyungKwangHi @hyungkwangcThank you very much for your guidance! I went ahead and opened a case.
The Case Number is 00554996.Thank you again!00554996Hi @hyungkwangc - my company has a RoCE implementation where we connect our custom hardware to a cluster with ConnectX-5 cards.  We have been able to disable ICRC on CX-5 cards, but not on CX-6 cards which is a problem as we would like to migrate our solution to CX-6.  Is it possible for you to post solution here on how to do this.
I also opened a case 00569035 for this.Thanks,
ToddPowered by Discourse, best viewed with JavaScript enabled"
591,developer-download-nvidia-com-connect-problem,"I used sdkmanager for installing doca in host OS and bluefield OS, so all worked well before 3hours.suddenly, Now I start the sdkmanager, sdkmanager doesn’t access developer.download.nvidia.com. My host OS connect the DNS8.8.8.8How can I solve this problem?Powered by Discourse, best viewed with JavaScript enabled"
592,unable-to-get-mellanox-connectx-5-to-connect-to-network,"Hello,I am having issues getting a Mellanox ConnectX-5 card to connect via ethernet to the network. I am running RHEL 9.0 and have tried reflashing the firmware, configuring the card to use ethernet, and manually installing the OFED drivers.Output of ifconfig. The IP is what it should be, but I can’t ping that address and I can’t ping anything from that addressI am getting the following message at boot from the mlx5_core driver:
modprobe -v mlx5_coreAny help would be appreciatedHi,try to use MTU 9000Regards,
OleUnfortunately, I’m still getting the same issues with MTU set to 9000Powered by Discourse, best viewed with JavaScript enabled"
593,resizable-bar-configuration,"We are on our way to starting using the GPUDirect feature of our NVIDIA graphics cards and in order to achieve that we have to configure Resizable Bar. How can we set it to 8GB on A100, A5000 and L40? This is a requirement that we have from our camera manufacturer.Asking because we are currently fighting with the configuration of Resizable Bar on A5000 and used dedicated scripts from NVIDIA to do so, after which our graphics card refused to be detected and we had to replace it with a new one. Also important part - we do not have that option of configuring Resizable Bar in our BIOS, that is why we had to resort to using the script.We used: nvidia-display-mode-selector-tool-home
And this document: developer.nvidia.com/sites/default/files/akamai/NVIDIA_Display_Mode_Selector_Tool_User_Guide.pdf
We used option: physical_display_enabled_8GB_bar1Powered by Discourse, best viewed with JavaScript enabled"
594,mlx-completion-with-error,"Good morning,First of all the error:Unfortunately, the interesting part (Vendor Syndrome) is not documented, as far as i could see. It would really help me, if I had the meaning of this…
The error appears to coincide with the usage of atomic operations.Now the scenario:
I am currently developing for my thesis and ran into an issue relating to Infiniband/Mellanox Hardware.
I am using a library (GPI2), that builds on verbs and allows to run via Ethernet or Infiniband. Running via Ethernet works fine and has no issues whatsoever even with greater node and thread counts.
Once i got to test on Infiniband, things hit the fan.The software uses a lot of threads per Node, issueing requests in parallel. The traffic consists of atomic operations, a lot of small messages and fewer, but still frequent big messages (~10KiB).Using a small amount of nodes, the software still runs fine via infiniband, but the issues escalate fast, leading to a roughly 50% error rate on 16Nodes with 40 threads each.The cluster i am testing on has the following specs:ibv_devinfo reports:If I can provide more information feel free to ask.
I unfortunately do not have a minimal example to reproduce the error.I am unfortunately at my wits end, as me and my supervisours experience with Infiniband is rather limited. Every help is greatly appreciated!Hi @uk077035 ,Please refer to IBV_WC_REM_ACCESS_ERR (10) in https://www.rdmamojo.com/2013/02/15/ibv_poll_cq/ :IBV_WC_REM_ACCESS_ERR (10) - Remote Access Error: a protection error occurred on a remote data buffer to be read by an RDMA Read, written by an RDMA Write or accessed by an atomic operation. This error is reported only on RDMA operations or atomic operations. Relevant for RC QPs.There are several potential causes for this issue:Regards,
ChenThank you for your response @chenh1 ,I have re-checked my buffers and offsets and found no issue there. I unfortunately do not have much control over the actual buffer creation etc. as I am using the aforementioned library.
I also double checked alignment where atomic operations would take place without results.The issue tends to occur if a lot of requests are send over the network - while working fine over Ethernet or with Infiniband and low counts of processes. Could burst traffic trigger the issue in any way, e.g. by network congestion or similar?If not, do I interpret your previous answer correctly, by then assuming that if my buffer offsets are correct, it must be an issue with the remote access keys or access flags?Thanks in advance!Edit: I have checked the libraries code:
The buffers are registered with
IBV_ACCESS_REMOTE_WRITE
| IBV_ACCESS_LOCAL_WRITE
| IBV_ACCESS_REMOTE_READ
| IBV_ACCESS_REMOTE_ATOMICThe rkey is at least set on operation, i can not check its validity though. I am assuming it is correct however, since the program is running fine for a few seconds before things go awry.Powered by Discourse, best viewed with JavaScript enabled"
595,bf2-dpu-shows-unclaimed,"I have the following message when checking for the status of the DPU. I have 2 other DPUs working fine but this one shows “unclaimed”. How can I fix this ?*-network:1 UNCLAIMED
description: Ethernet controller
product: MT42822 BlueField-2 integrated ConnectX-6 Dx network controller
vendor: Mellanox Technologies
physical id: 0.1
bus info: pci@0000:37:00.1
version: 01
width: 64 bits
clock: 33MHz
capabilities: cap_list
configuration: latency=0
resources: memory:e6000000-e7ffffff memory:e8800000-e8ffffff memory:cbffc000000-cbffdffffffI was able to fix this by re-setting the driversudo mlxfwreset -d /dev/mst/mt41686_pciconf0 -l 3 -y resetthen rebooted both the server and the NICThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
596,nvidia-doca-east-west-overlay-encryption-reference-application,"Hi all!Originally following:NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.
and
https://docs.mellanox.com/display/BlueFieldSWv35011563/IPsec+FunctionalityI have two /dev/mst/mt41686_pciconf0 cards connected back to backOutputs after commands in documentation run look like this on the DPUs:I am unsure if the commands under “Configuring IPsec Rules with iproute2” on
/display/BlueFieldSWv35011563/IPsec+Functionality
are relevant as they do not appear on
/doca/sdk/east-west-overlay-encryption/index.htmlCurrently there is no IPSec tunnel set up.For the time being I am using the default OVS config:I can currently ping between p1p1 on both hosts and see an output when running
ovs-appctl dpctl/dump-flows type=offloaded
on the DPUsWhich looks a bit like this (Modified MAC addresses before posting):I am unsure if the config under Setting IPSec Full Offload Using strongSwan should be on the Host or on the DPU
from: /display/BlueFieldSWv35011563/IPsec+FunctionalityAdditionally further down the document it mentionsYou may now send encrypted data over the HOST VF interface (192.168.70.[1|2]) configured for VXLAN.But 192.168.70.1 is not mentioned anywhere else in the documents.I am unsure if the commands under “Configuring IPsec Rules with iproute2” on
/display/BlueFieldSWv35011563/IPsec+Functionality
are relevant as they do not appear on
/doca/sdk/east-west-overlay-encryption/index.htmlAfter speaking with the support team they confirmed:""Configuring IPsec Rules with iproute2” is not required if using strongswan“Setting IPSec Full Offload Using strongSwan” should be done on the DPU“192.168. 70 .[1|2]” is a typoThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
597,failed-to-burn-an-image-onto-innova-2-flex-fpga,"I failed to burn an image onto Innova-2 Flex FPGA. Can anyone help?You are missing the BOPE and ConnectX devices.Try the following after a system reboot. Commands assume you have the Innova-2 Flex Open package extracted to ~ (home directory).Check out my notes on using the Innova-2 for FPGA development.The Innova-2 uses dual x4 FLASH ICs in parallel for faster loading/programming.innova2_flex_app requires _primary.bin and _secondary.bin files. Note the ,0 and ,1.So, I have to split my image img.bin into 2 separated images: img_primary.bin and img_secondary.bin to burn to the flash?To do that, I splitted my image and got an error with this command:Then I added parameter -b to the command and burned the image:When I booted to the User image, the red light on the board turned on.
What did I do wrong?Vivado generates the _primary.bin and _secondary.bin files during Memory Configuration File generation. I believe the data is interleaved.Confirm your project’s constraints (.xdc) file includes the following which is from the official Innova-2 constraints package.Run Generate Bitstream in Vivado then Generate Memory Configuration File. Select bin, mt25qu512_x1_x2_x4_x8, SPIx8, Load bitstream files, and a location and name for the output binary files. The bitstream will end up, for example, in the DESIGN_NAME/DESIGN_NAME.runs/impl_1 subdirectory as SOMETHING.bit.
Vivado_Write_Memory_Configuration_File600×567 69.7 KB
I do not see the file https://github.com/mwrnd/innova2_xcku15p_ddr4_bram_gpio/innova2_xcku15p_ddr4_bram_gpio.binIs it required to use specifically this file?
My board is MNV303611A-EDLT which does not have DDR4 onboard.That was an image I took for illustrative purposes from these notes. You can name your .bin output file whatever you want. Vivado will then create _primary.bin and _secondary.bin files from your project’s Bitstream .bit file. It will add _primary.bin and _secondary.bin to NAME.bin.You can use git to clone this demo project which contains a complete bitstream for the Innova-2.After loading the User Image using innova2_flex_app, reboot your system and try testing the design. The AXI BRAM and AXI GPIO LED tests should work with the MNV303611A-EDLT as long as the DDR4 block gets out of reset. If the design fails, you can open the project in Vivado, delete the DDR4 block, and re-implement the design.Here is the info after loading the User image. It looks like I cannot access the BRAM. The board is plugged into an enclosed chassis that I cannot see the leds on the board.Great debug logs. Very useful and well done.The good news is that you have all the drivers successfully installed and the card and XDMA block within the FPGA are recognized.The innova2_xcku15p_ddr4_bram_gpio demo project unfortunately uses the DDR4 clock for the AXI blocks. Source the design in Vivado then Open Block Design and delete the ddr4_0 and  rst_ddr4_0_300M blocks. Then connect the xdma_0 block’s axi_aclk and axi_aresetn signals to the s_axi_aclk and s_axi_resetn signals of the BRAM and GPIO blocks. Also, delete any unused signals. Generate Bitstream and an MNV303611A-EDLT compatible bitstream should be generated.You can also check out this tutorial for a step-by-step guide to creating an XDMA demo in the Vivado Block Designer.Here is a simple XDMA demo for the MNV303611A-EDLT. Just AXI BRAM and one LED under GPIO control. Please test it and let me know if it works.XDMA PCIe to BRAM and GPIO demo for the XCKU15P FPGA on the Innova-2 Flex Open VPI MNV303611A-EDLT - GitHub - mwrnd/innova2_mnv303611a_xcku15p_xdma: XDMA PCIe to BRAM and GPIO demo for the XCKU15P ...I think it works now. Thanks for your help.The XDMA Driver (Xilinx’s dma_ip_drivers) creates read-only and write-only character device files, /dev/xdma0_h2c_0 and /dev/xdma0_c2h_0, that allow direct access to the FPGA design’s AXI Bus. To read from an AXI Device at address 0x40010000 you would read from address 0x40010000 of the /dev/xdma0_c2h_0 (Card-to-Host) file. To write you would write to the appropriate address of /dev/xdma0_h2c_0 (Host-to-Card).For example, you can read or write to the AXI BRAM Memory using dd. Note seek and skip are the Base10 address of the BRAM Memory on the FPGA design’s AXI Bus, which is 0 on the  innova2_mnv303611a_xcku15p_xdma project. Try various read and write Block Sizes, bs, up to the maximum Range of the BRAM Memory as defined in the Vivado Address Editor. 128kb is the current maximum.dd_AXI_BRAM800×339 55.9 KBHere are more detailed notes including a simple C program that communicates with an XDMA design.I changed the C code to write {6,7,8,9} to the BRAM,
but it returned a number different from 0x06070809 :
…
uint8_t read_data[DATA_SIZE];
…
printf(""\nread_data: 0x%x\n"", read_data);
…
Output:
read_data: 0x18b24058read_data is the address of the array. There would have been an implicit type cast and gcc should have complained if run with -Wall. Add an index value for each byte you want to print.Is there a demo of XDMA Stream?Is there a demo of XDMA Stream?None that I am aware of. Search for xdma stream and look through the XMDA Guide. I would get something working with the basic XDMA driver before adding complexity.Powered by Discourse, best viewed with JavaScript enabled"
598,maximum-number-of-tc-flow-offloads-in-bluefield-ii,"Can anybody tell me the maximum number of Flows we can offload when configured through TC? It installs(shows) 10K flows easily but traffic does not match all of the flows. We also noticed that the latency increases many folds and throughput decreases when the installed flows reaches just above 200. And after around 450 flows, unknown/unmatched traffic stops coming to the ARM cores.Could you shed a light on how you configure the system (host + SmartNIC side)? Are you using SMFS or DMFS? Are you using IPSec?I am using the default settings currently. SMFS is configured.Following Mellanox OFED and SmartNIC user manuals you should be able to configure more flows. In general, limitations for amount of the flows come from OVS. You might check ovs-vswitchd man page for the details.In the case if everything is configured properly and you still seeing performance degradation you may open a support ticket with Nvidia in the case if your organization has a valid support contract.Changing the mode to dkms only increased the throughput from 6Gbps to ~13Gbps for 200+ TCP flows. We have tried to configure the DPU for performance tuning as mentioned in the user manuals but it did not have any affect on the number of flows. And, we are not using OVS. We are using TC utility to configure the flows.Powered by Discourse, best viewed with JavaScript enabled"
599,after-switch-migration-mlag-keeps-showing-peering-how-should-i-fix-it,"After the migration of two switches that normally run mlag, all the service interfaces of the standby switch are Down, and checking the mlag status shows peering. How can I fix it?master：
Admin status: Enabled
Operational status: Up
Reload-delay: 30 sec
Keepalive-interval: 1 sec
Upgrade-timeout: 60 min
System-mac: 00:00:5E:00:01:F7MLAG Ports Configuration Summary:
Configured: 48
Disabled:   0
Enabled:    48MLAG Ports Status Summary:
Inactive:       4
Active-partial: 0
Active-full:    441    Po1           4000       Up           10.0.0.2                                 10.0.0.1                                 6 days, 21:45:44     51C:34:DA:EE:FF:48   Up                           
1C:34:DA:EE:FD:C8   Peering                       sr102-F05-41standby：
Admin status: Enabled
Operational status: Up
Reload-delay: 30 sec
Keepalive-interval: 1 sec
Upgrade-timeout: 60 min
System-mac: 00:00:5E:00:01:F7MLAG Ports Configuration Summary:
Configured: 47
Disabled:   47
Enabled:    0MLAG Ports Status Summary:
Inactive:       47
Active-partial: 0
Active-full:    01    Po1           4000       Up           10.0.0.1                                 10.0.0.2                                 6 days, 21:44:39     51C:34:DA:EE:FD:C8   Peering                      
1C:34:DA:EE:FF:48   Peering                       sr102-F06-41Hello!Can you try to remove then re-add the IPL link and see if that helps:Then add the configuration for IPL and member interfaces back into the port-channel.
If this does not help, please open a case with support team for further investigation.Powered by Discourse, best viewed with JavaScript enabled"
600,cannot-upgrade-l4t,"Thank you for reading my topic.I’m using Jetson AGX Orin in my research.
I refer to "" Getting Started with Jetson AGX Orin Developer Kit(Getting Started with Jetson AGX Orin Developer Kit | NVIDIA Developer) to setup my device.
Before installing Jetpack, I have to upgrade L4T because my device’s version is R34 (release), REVISION: 0.1.I have revised the file""/etc/apt/sources.list.d/nvidia-l4t-apt-source.list"" from
deb https://repo.download.nvidia.com/jetson/common r34.0 main
deb https://repo.download.nvidia.com/jetson/t234 r34.0 main
to
deb https://repo.download.nvidia.com/jetson/common r32.1 main
deb https://repo.download.nvidia.com/jetson/t234 r32.1 main
using vi.
And I tried to execute “sudo apt update” but I failed.
The command prompt shows
Hit:1 blur blur blur
blur blur blur
Err:3 https://repo.download.nvidia.com/jetson/common r34.1 Release
Could not handshake: The TLS connection was non-properly terminated.[IP: 111.222.333.44 8080]
Err:4 https://repo.download.nvidia.com/jetson/t234 r34.1 Release
Could not handshake: The TLS connection was non-properly terminated.[IP: 111.222.333.44 8080]
Hit:5 blur blur blur
blur blur blur
Reading package lists… Done
E: The repository ‘https://repo.download.nvidia.com/jetson/common r34.1 Release’ does not have a Release file.
N: Updatig from such a repository can’t be done securely, abd is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration datails.
E: The repository ‘https://repo.download.nvidia.com/jetson/t234 r34.1 Release’ does not have a Release file.
N: Updatig from such a repository can’t be done securely, abd is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration datails.I look forward to your reply.
Thank you.I have exactly same issue in Jetson AGX Orin.
If anyone have an idea how to fix this issue:
E: The repository ‘https://repo.download.nvidia.com/jetson/common r34.1 Release’ does not have a Release file.
Thank you communityI tried to replace:
sudo bash -c ‘echo “deb https://repo.download.nvidia.com/jetson/common r34.1 main” >> /etc/apt/sources.list.d/nvidia-l4t-apt-source.list’
sudo bash -c ‘echo “deb https://repo.download.nvidia.com/jetson/t234 r34.1 main” >> /etc/apt/sources.list.d/nvidia-l4t-apt-source.list’By:sudo bash -c ‘echo “deb [trusted=yes] https://repo.download.nvidia.com/jetson/common r34.1 main” >> /etc/apt/sources.list.d/nvidia-l4t-apt-source.list’
sudo bash -c ‘echo “deb [trusted=yes] https://repo.download.nvidia.com/jetson/t234 r34.1 main” >> /etc/apt/sources.list.d/nvidia-l4t-apt-source.list’So I added  [trusted=yes] and the following errors disappears:
E: The repository ‘https://repo.download.nvidia.com/jetson/common r34.1 Release’ does not have a Release file.
E: The repository ‘https://repo.download.nvidia.com/jetson/t234 r34.1 Release’ does not have a Release file.but still have other errors, tell me if it this solution works for you?Thank you for kind reply.I tried your solution and might got the same result as you.
The part of error disappeared.
I got the following error messages.E: Failed to fetch https://repo.download.nvidia.com/jetson/common/dists/r34.1/main/binary-arm64/Packages  Could not handshake: The TLS connection was non-properly terminated. [IP: —.—.—.-- 8080]
E: Failed to fetch https://repo.download.nvidia.com/jetson/t234/dists/r34.1/main/binary-arm64/Packages  Could not handshake: The TLS connection was non-properly terminated. [IP: —.—.—.-- 8080]
E: Some index files failed to download. They have been ignored, or old ones used instead.Anyway I really appreciate your advice.I was able to let it work.If you have the same error as mine(refer to my previous reply), following solution might work.sudo vi /etc/apt/apt.conf
write configuration below.Acquire::http::Proxy “http://your proxy server:your port number/”;
Acquire::https::Proxy “http://your proxy server:your port number/”;if your proxy require the certification,write configuration below instead above.Acquire::http::Proxy “http://your ID:password@your proxy server:your port number/”;
Acquire::https::Proxy “http://your ID:password@your proxy server:your port number/”;I appreciate your kind advice.Thank you for your help,
I am not quite familiar with proxy,
Can you tell me how can I fill these lines according to my case?
Acquire::http::Proxy “http://your proxy server:your port number/”;
Acquire::https::Proxy “http://your proxy server:your port number/”;
Acquire::http::Proxy “http://your ID:password@your proxy server:your port number/”;
Acquire::https::Proxy “http://your ID:password@your proxy server:your port number/”;how to know my proxy server and port number and ID also?Thank you againIs there someone who is in charge of proxy management in your company or university?
You should ask him or her your ID, password, proxy server address and port number.your proxy might not require certification.
In that case, you don’t need ID and password.
So,
Acquire::http::Proxy “http://your proxy server:your port number/”;
Acquire::https::Proxy “http://your proxy server:your port number/”;
is OK.
Port number is generally 8080 but you should confirm.If you don’t have connection error, this solution might not work well.Hello,
yes indeed, I have asked the IT department.
Thank you for your solution, now it works for meThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
601,how-to-connect-infiniband-qsfp28-switch-with-rj45,"Hi!I have a bunch of servers having ConnectX-6 and connecting to a switch which only has QSFP28 ports (we bought this: https://www.qct.io/product/index/Switch/Ethernet-Switch/T7000-Series/T7032-IX1)Now I have another computer which only has RJ45 port. And there is no additional space in it to install a ConnectX. How can I do to connect it to my switch? Is there any adapters between RJ45 and QSFP28?Thanks!You can check CX6 ETH compatible cable below,https://docs.nvidia.com/networking/display/ConnectX6Firmwarev20352000LTS/Firmware+Compatible+Products#FirmwareCompatibleProducts-ValidatedandSupported1GbECablesSelect Validated and Supported 1GbE Cables|1GbE|MC3208011-SX|NVIDIA Optical module, ETH 1GbE, 1Gb/s, SFP, LC-LC, SX 850nm, up to 500m||1GbE|MC3208411-T|NVIDIA module, ETH 1GbE, 1Gb/s, SFP, Base-T, up to 100m|https://docs.nvidia.com/networking/display/MC3208411T
https://docs.nvidia.com/networking/display/MC3208011SXThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
602,mlnx-qos-enables-pfc-and-the-pfc-function-is-automatically-disabled-after-a-period-of-time,"Hi
mlnx_qos enables pfc, and the PFC function is automatically disabled after a period of time，root@smart03-MS-7D42:~# mlnx_qos -i enp1s0f0np0 --pfc 1,1,1,1,1,1,1,1 -p 0,1,2,3,4,5,7,6
DCBX mode: OS controlled
Priority trust state: dscp
dscp2prio mapping:
prio:0 dscp:07,06,05,04,03,02,01,00,
prio:1 dscp:15,14,13,12,11,10,09,08,
prio:2 dscp:23,22,21,20,19,18,17,16,
prio:3 dscp:31,30,29,28,27,26,25,24,
prio:4 dscp:39,38,37,36,35,34,33,32,
prio:5 dscp:47,46,45,44,43,42,41,40,
prio:6 dscp:55,54,53,52,51,50,49,48,
prio:7 dscp:63,62,61,60,59,58,57,56,
default priority:
Receive buffer size (bytes): 20016,156096,0,0,0,0,0,0,
Cable len: 7
PFC configuration:
priority    0   1   2   3   4   5   6   7
enabled     1   1   1   1   1   1   1   1
buffer      1   1   1   1   1   1   1   1
tc: 0 ratelimit: unlimited, tsa: vendor
priority:  0
tc: 1 ratelimit: unlimited, tsa: vendor
priority:  1
tc: 2 ratelimit: unlimited, tsa: vendor
priority:  2
tc: 3 ratelimit: unlimited, tsa: vendor
priority:  3
tc: 4 ratelimit: unlimited, tsa: vendor
priority:  4
tc: 5 ratelimit: unlimited, tsa: vendor
priority:  5
tc: 7 ratelimit: unlimited, tsa: vendor
priority:  6
tc: 6 ratelimit: unlimited, tsa: vendor===============================================
In less than a minute, use mlnx_qos -i enp1s0f0np0 to check the PFC situation and find that it is turned off
root@smart03-MS-7D42:~# mlnx_qos -i enp1s0f0np0
DCBX mode: OS controlled
Priority trust state: dscp
dscp2prio mapping:
prio:0 dscp:07,06,05,04,03,02,01,00,
prio:1 dscp:15,14,13,12,11,10,09,08,
prio:2 dscp:23,22,21,20,19,18,17,16,
prio:3 dscp:31,30,29,28,27,26,25,24,
prio:4 dscp:39,38,37,36,35,34,33,32,
prio:5 dscp:47,46,45,44,43,42,41,40,
prio:6 dscp:55,54,53,52,51,50,49,48,
prio:7 dscp:63,62,61,60,59,58,57,56,
default priority:
Receive buffer size (bytes): 20016,156096,0,0,0,0,0,0,
Cable len: 7
PFC configuration:
priority    0   1   2   3   4   5   6   7
enabled     0   0   0   0   0   0   0   0
buffer      0   0   0   0   0   0   0   0
tc: 0 ratelimit: unlimited, tsa: vendor
priority:  0
priority:  1
priority:  2
priority:  3
priority:  4
priority:  5
priority:  6
priority:  7The environment I use is as follows
root@smart03-MS-7D42:~# lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 22.04.1 LTS
Release:        22.04
Codename:       jammy
root@smart03-MS-7D42:~# uname -an
Linux smart03-MS-7D42 5.15.0-52-generic #58-Ubuntu SMP Thu Oct 13 08:03:55 UTC 2022 x86_64 x86_64 x86_64 GNU/LinuxYou may have LLDP enabled on the system. That will override your settings.Dear YanivserlinBefore any operation, lldp is turned off,root@smart03-MS-7D42:~# systemctl status lldp
Unit lldp.service could not be found.
root@smart03-MS-7D42:~#
root@smart03-MS-7D42:~# systemctl status lldpd
○ lldpd.service - LLDP daemon
Loaded: loaded (/lib/systemd/system/lldpd.service; disabled; vendor preset: enabled)
Active: inactive (dead)
Docs: man:lldpd(8)
root@smart03-MS-7D42:~# systemctl status lldpad
○ lldpad.service - Link Layer Discovery Protocol Agent Daemon.
Loaded: loaded (/lib/systemd/system/lldpad.service; disabled; vendor preset: enabled)
Active: inactive (dead)
TriggeredBy: ○ lldpad.socket
Docs: man:lldpad(8)
root@smart03-MS-7D42:~#
root@smart03-MS-7D42:~#​I suggest opening a case with Nvidia support as it requires more detailed debug.Hi,the same problem has occurred on our device(bluefield2-DPU),and I don’t know how to resolve it.Have you found the correct solution?Powered by Discourse, best viewed with JavaScript enabled"
603,how-to-reset-factory-defaults-switch-ib-sb7800,"I didn’t configured an IP Address and disabled the DHCP so I can’t have the CLI Interfase or the web GUI. Please help!!!I found the answer in the Mellanox OS Manual!!!use the command enable, then use config terminal and configuration jump-startThat’s it!!!Powered by Discourse, best viewed with JavaScript enabled"
604,how-can-i-calculate-rss-hash-value-from-c-code-to-reassemble-packet-and-send-them-to-the-same-process-packet-that-have-the-same-ip-port-arrive-to-the-same-queue-if-some-packet-are-ip-fragmented-packet-arrive-to-different-process-it-is-a-problem,"I am using Mellanox Technologies MT27800 Family [ConnectX-5], using dpdk multi rx queue with rss “ETH_RSS_IP | ETH_RSS_UDP | ETH_RSS_TCP”I analyzer traffic and need all packet of same session to arrive to the same process ( session for now can be ip+port)So Packet that have the same ip + port arrive to the same queue.But If some packet are ip fragmented, packet arrive to different process. It is a problem!How can i calculate the hash value in the c++ code, like it is done in the card, so i can reassemble packets and send them to the same process like the non fragmented packetsHi Yaron,In MLX5 poll mode driver, RSS hash for fragmented packets is not supported.The RSS offload types supported in DPDK & ConnectX family are the types mentioned in dpdk.org - MLX5 poll mode driver section.Which are:IPv4, IPv6, TCPv4, TCPv6, UDPv4 and UDPv6 RSS on any number of queues.RSS using different combinations of fields: L3 only, L4 only or both, and source only, destination only or both.Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
605,error-the-current-mlnx-ofed-linux-is-intended-for-rhel7-2,"Getting this error while installing Mellanox Driver for rhel7.2Error: The current MLNX_OFED_LINUX is intended for rhel7.2root@snx-pdx-blh16n052100010: /tmp → uname -sr
Linux 5.4.155-200.el7.x86_64Here is the link where I picked the driver from:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)Steps for installation:https://docs.mellanox.com/display/MLNXOFEDv543030/InstallationPlease let me know what mellanox package I can use for this kernel.Thank you,
AnishHi Anish,can you ask this question at:Mellanox OFED - NVIDIA Developer Forums ?I can move this to the OFED category for you.Best,
TomPlease doCan someone please comment on this since we are blocked on our tests due to absence of this driver.
Appreciate if someone could take this up with priority.Powered by Discourse, best viewed with JavaScript enabled"
606,nvidia-persistenced-failed-to-query-nvidia-devices,"I am trying to install the cuda-driver and cuda-toolkit on ubuntu 20.04. I am following the instructions from NVIDIA CUDA Installation Guide for Linux.When I run the following:  /usr/bin/nvidia-persistenced --verbose
nvidia-persistenced failed to initialize. Check syslog for more details.Then the syslog shows this:
Aug 2 07:25:41 srs-uav nvidia-persistenced: Verbose syslog connection openedAug 2 07:25:41 srs-uav nvidia-persistenced: Verbose syslog connection openedAug 2 07:25:41 srs-uav nvidia-persistenced: Started (33449)Aug 2 07:25:41 srs-uav nvidia-persistenced: Started (33449)Aug 2 07:25:41 srs-uav nvidia-persistenced: Failed to query NVIDIA devices. Please ensure that the NVIDIA device files (/dev/nvidia*) exist, and that user 0 has read and write permissions for those files.Aug 2 07:25:41 srs-uav nvidia-persistenced: Failed to query NVIDIA devices. Please ensure that the NVIDIA device files (/dev/nvidia*) exist, and that user 0 has read and write permissions for those files.Aug 2 07:25:41 srs-uav nvidia-persistenced: PID file unlocked.I did run a bug report and the output is attached.  I will appreciate any help with this.
Thank you
nvidia-bug-report.log.gz (154.6 KB)Please provide the command list you have committed to installing such packages into your system. Also, paste the output of nvidia-smi.I have checked your error log and suspended maybe you installed driver only supports older Nvidia products:Thank you please see below:$lspci | grep -i nvidia0000:65:00.0 VGA compatible controller: NVIDIA Corporation GA102GL [RTX A6000] (rev a1)0000:65:00.1 Audio device: NVIDIA Corporation GA102 High Definition Audio Controller (rev a1)$uname -m && cat /etc/*releasex86_64DISTRIB_ID=UbuntuDISTRIB_RELEASE=20.04DISTRIB_CODENAME=focalDISTRIB_DESCRIPTION=“Ubuntu 20.04.6 LTS”NAME=“Ubuntu”VERSION=“20.04.6 LTS (Focal Fossa)”ID=ubuntuID_LIKE=debianPRETTY_NAME=“Ubuntu 20.04.6 LTS”VERSION_ID=“20.04”HOME_URL=“https://www.ubuntu.com/”SUPPORT_URL=“https://help.ubuntu.com/”BUG_REPORT_URL=“https://bugs.launchpad.net/ubuntu/”PRIVACY_POLICY_URL=“https://www.ubuntu.com/legal/terms-and-policies/privacy-policy”VERSION_CODENAME=focalUBUNTU_CODENAME=focal$gcc --versiongcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0$uname -r5.15.0-78-genericsudo apt-get updatesudo apt-get install linux-headers-$(uname -r)sudo apt-key del 7fa2af80sudo dpkg -i cuda-keyring_1.1-1_all.debsudo apt install libnvidia-common-535Below, the dependencies will be added based on error messagesudo ubuntu-drivers autoinstallsudo apt install nvidia-driver-535sudo apt install nvidia-driver-535 nvidia-dkms-535 nvidia-kernel-source-535 nvidia-kernel-open-535 libnvidia-compute-535:i386 libnvidia-extra-535 nvidia-compute-utils-535 nvidia-compute-utils-535 libnvidia-decode-535:i386 libnvidia-encode-535:i386 libnvidia-fbc1-535:i386 nvidia-utils-535 xserver-xorg-video-nvidia-535The above didn’t install so I proceeded as follows:sudo apt-get -y install cuda-driverssudo apt-get -y install cuda-toolkit-12-2 cuda-drivers-535 --verbose-versionssudo apt-get -y install cuda-toolkit-12-2 cuda-drivers-535 nvidia-dkms-535 nvidia-driver-535 nvidia-kernel-common-535 --verbose-versionssudo apt-get -y install cuda-toolkit-12-2 cuda-drivers-535 nvidia-dkms-535 nvidia-driver-535 nvidia-kernel-common-535 libnvidia-extra-535 --verbose-versionssudo reboot$ nvidia-smiNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.Also below is the recommended driver$ sudo ubuntu-drivers devicesvendor : NVIDIA CorporationPowered by Discourse, best viewed with JavaScript enabled"
607,cable-or-opensm-problem,"I bought QSFP+ AOC 40G 850nm AQOA9N13ADLN0723 cable. According to the specification it is InfiniBand 4x DDR/QDR compliant. Unfortunately afteribportstate --Direct 0 1 enableI get: state: Down, and Physical state: PortConfigurationTraining or Disabled alternately. Afteribportstate --Direct 0 1 width 1(on both ends) i get Physical state: LinkUp but State: Initializing. OpenSM is running on one of the nodes (no switch). And in the OpenSM log:OpenSM 3.3.23osm_vendor_init: 1000 pending umads specifiedEntering DISCOVERING stateosm_vendor_bind: Mgmt class 0x81 binding to port GUID 0x117500006f916eosm_vendor_bind: Mgmt class 0x03 binding to port GUID 0x117500006f916eosm_vendor_bind: Mgmt class 0x04 binding to port GUID 0x117500006f916eosm_vendor_bind: Mgmt class 0x21 binding to port GUID 0x117500006f916eosm_opensm_bind: Setting IS_SM on port 0x00117500006f916eSM port is downSM port is uplog_send_error: ERR 5411: DR SMP Send completed with error (IB_TIMEOUT) – droppingMethod 0x1, Attr 0x11, TID 0x12a8Received SMP on a 1 hop path: Initial path = 0,1, Return path = 0,0sm_mad_ctrl_send_err_cb: ERR 3113: MAD completed in error (IB_TIMEOUT): SubnGet(NodeInfo), attr_mod 0x0, TID 0x12a8sm_mad_ctrl_send_err_cb: ERR 3120: Timeout while getting attribute 0x11 (NodeInfo); Possible mis-set mkey?Entering MASTER stateIs it cable compatibility or OpenSM problem?Hello Zbuntowany,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the provided cable information, we cannot determine if the cable is working. The cable you are using is not a supported NVIDIA InfiniBand cable, so functionality cannot be confirmed by NVIDIA Networking.Based on the OpenSM.log, the SM is running properly and it is in Master State so functioning properly.We recommend to use a supported NVIDIA networking cable. Several cables we have are backwards compatible with QDR.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
608,ndr-switch-support-for-cx-6-cards,"Does the ConnectX-6 card support to QM9700 switch? if Yes, Then what cables are requiredPlease check the followinghttps://docs.nvidia.com/networking/m/view-rendered-page.action?abstractPageId=68324159Powered by Discourse, best viewed with JavaScript enabled"
609,sx6018-please-help,"Hi everyone,I just purchased a SX6018 off of ebay… I know good/bad decision but either way I need help… The pull out label is marked P/N: 100-886-230-01 so I presume this unit was manufactured for DELL? Can anyone help me identify what model this is supposed to be? MSX6018F-1SFS, MSX6018F-1BRS, MSX6018T-1SFS, or MSX6018T-1BRS??Now on to the issues I am having with the unit… When I first power it on. The switch status LED on the front and back panel remains red for about 1 minute and 10 seconds then turns green. During that time this posts to my terminal screen:U-Boot 2009.01 SX_PPC_M460EX SX_3.2.0330-82-EMC ppc (Feb 27 2013 - 12:13:42)CPU: AMCC PowerPC 460EX Rev. B at 1000 MHz (PLB=166, OPB=83, EBC=83 MHz)Security/Kasumi supportBootstrap Option H - Boot ROM Location I2C (Addr 0x52)Internal PCI arbiter disabled32 kB I-Cache 32 kB D-CacheBoard: Mellanox PPC460EX BoardFDEF: NoI2C: readyDRAM: 2 GB (ECC enabled, 333 MHz, CL3)FLASH: 16 MBNAND: 1024 MiBPCI: Bus Dev VenId DevId Class IntPCIE0: link is not up.PCIE1: successfully set as root-complex01 00 15b3 c738 0c06 00Net: ppc_4xx_eth0, ppc_4xx_eth1Hit any key to stop autoboot: 0Waiting for PHY auto negotiation to complete… TIMEOUT !doneENET Speed is 10 Mbps - HALF duplex connection (EMAC0)Using ppc_4xx_eth0 deviceping failed; host 172.17.255.252 is not aliveLoading Kernel Image … OKABCDEFdrv_table_install: syslog at 0x00034080 with minor 0 (DRV_SETUP)(DRV_INIT)drv_table_install: isrlog at 0x00034080 with minor 1 (DRV_INIT)drv_table_install: userinterface at 0x00004db0 with minor 0 (DRV_SETUP) (DRV_INIT)drv_table_install: stty0 at 0x0006eb7c with minor 0 (DRV_SETUP) (DRV_INIT)drv_table_install: eth0 at 0x0006f50c with minor 0 (DRV_SETUP) (DRV_INIT)drv_table_install: i2c0 at 0x0007a114 with minor 0 (DRV_SETUP) (DRV_INIT)drv_table_install: i2c1 at 0x0007a114 with minor 1 (DRV_INIT)drv_table_install: itcpip at 0x0003c400 with minor 0tcpipInit: Startinginternal TCP/IP stack.(DRV_SETUP) (DRV_INIT)kernel_main: Drivers installed, installing INIT process with stack size = 8192.Bar [ 0 ] is the first memory barsk_init_main: Starting process based initialization - 8590.UDP socket 3 createdTCP socket 4 createdsk_init_main: Process based initialization complete - 8590.sk_init_main: Installing task table.task_table_install: console at 0x00015ca4 stack 0x00452410/26624 : 4task_table_install: inetd at 0x00035c9c stack 0x00472c10/8192 : 5task_table_install: poll_cqs at 0x000aa98c stack 0x00458c10/8192 : 6task_table_install: poll_ports at 0x000aaac4 stack 0x0045ac10/8192 : 7task_table_install: env_mon at 0x000832f4 stack 0x0045ec10/8192 : 8task_table_install: env_bin_api at 0x00014420 stack 0x00460c10/8192 : 9task_table_install: incoming_fw_files at 0x00013e84 stack 0x00462c10/8192 : 10task_table_install: incoming_fw_files at 0x00013e84 stack 0x00464c10/8192 : 11sk_init_main: Task table installed. Starting tasks and exiting.----------------------------- Board Info -----------------------------Chasis Type : STINGRAYNumber of Ports : 18U-Boot Revision : 3.2.330Firmware Revision : 9.9.1030INI file Revision : 0x21010009SwitchOS Revision : 1.297SwitchOS Build Date : 2013-08-02SwitchOS Build Time : 16:34:58SwitchOS Build Path : /emc/tdowning/ppc460_release/aug_02_201321:12:41 09/24/2014Switch-X(4)>I’m confused as to how do I login to the system to configure the unit? The user manual states “Login (from a serial terminal program) as admin and use admin as password. This starts the Mellanox configuration wizard.”When I type in admin at the prompt the response is:Switch-X(4)> adminadmin: Command not found.Switch-X(4)>So am I doing something wrong? Or is the SX6018 Defective? The person I purchased the unit from gave me a 30 day warranty which I have about 18 days to return the unit if it is defective… I should also note the LED indicators on the back panel for each of the two power supplies are off and so is the LED indicator on the fan unit. The LED indicators on the front panel for the fan, PS1, and PS2 are off as well… So that has me really confused as the fans are running and the power supplies has the unit powered up…Please help and Thank You in advance,RalphPeters100hido you have access to mellanox to get the MLNX-OS?if so, i can show you how to update the firmware of your switch back to mellanox, by doing a custom firmware build. i have done it sucessfully on 4 x SX6012 and 6005 Switches. But need the MLNX-OS software to install on the switch, and mellanox kindly wont give me access to download MLNX-OS!Angry!Hi Robert/Ralph,This an EMC switch - it’s their OS running on a Mellanox HW.I think you should be more cautious when buying from EBAY.hi bruceI’ve the same problems with a SX6018 Switch out of ebay, because it is an EMC. I do have an actual Image of MLNX-OS for that switch, but I dont know any way to bring it up. So if you have an idea, i would be able to share my image.full of hopeherbieHi Bruce. I am kind of desperate finding an OS for my SX6012. It has an EMC OS and it doesn’t go further than U-BOOT. I have installed TFTP on my PC but every firmware I tried so far to install from Nvidia website, I get Wrong Image Format for tftpboot command ERROR: can’t get kernel image! I can’t find out if U-BOOT can’t handle the .bin file or I am downloading the wrong firmware.Powered by Discourse, best viewed with JavaScript enabled"
610,security-updates-and-apt-mark-auto,"HiI noticed that Cumulus 5.0.1 (Debian 10 / Buster) is not running the latest opensslI can see the newer package is available if I do: apt-cache show opensslIf I do:
apt-mark auto openssl
apt-get install opensslI get: “openssl set to manually installed”Does Cumulus have a way to avoid security updates defined in /etc/apt/sources.list?deb Index of /debian-security buster/updates mainFuture versions of Cumulus Linux will address security vulnerabilities.  It’s not recommended / supported for end users to update the packages that comprise Cumulus Linux other than through Cumulus Linux updates.Powered by Discourse, best viewed with JavaScript enabled"
611,release-notes-for-nvidia-bright-cluster-manager-9-2-12,"Release notes for Bright 9.2-12== General ==
=New Features==Fixed Issues=== CMDaemon ==
=Improvements==Fixed Issues=== Machine Learning ==
=New Features=== cm-kubernetes-setup ==
=Improvements=== cm-scale ==
=New Features==Improvements==Fixed Issues=== cm-wlm-setup ==
=Fixed Issues=== cmsh ==
=Fixed Issues=== jupyter ==
=Fixed Issues=== slurm23.02 ==
=New Features=Powered by Discourse, best viewed with JavaScript enabled"
612,mcx545b-ccun-installed-lspci-ethtool-do-not-display-any-info-about-the-device,"Running on Debian 10.8 with proxmox-ve 6.4 install. I have two servers where the device MCX545B-CCUN is not visible in lspci output nor ethtool. When I run the install driver script , I get the following messageSetting up mlnx-fw-updater (5.5-1.0.3.2) …Initializing…Attempting to perform Firmware update…No devices found!Any ideas why lspci is not finding the device. What could be causing this.cheersmarkHello Mark,Thank you for posting your inquiry on the NVIDIA Networking Community.Several issues can be the cause on what you are experiencing.If you are still experiencing issues with the adapter after these check-points, please do open a NVIDIA Networking Support ticket (valid support contract needed), so we can assist you through the ticket → networking-support@nvidia.comThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
613,doca-examples,"Hi all,
I’m new to DOCA SDK. I’m trying to run the example applications but I’m getting the following error, can anyone help?./doca_url_filter -a 0000:03:00.0,class=regex -a auxiliary:mlx5_core.sf.4,sft_en=1 -a auxiliary:mlx5_core.sf.5,sft_en=1 -c3 –– -p./doca_url_filter: error while loading shared libraries: librte_regexdev.so.21: cannot open shared object file: No such file or directoryOk I solved the shared library problem by doing this:
export LD_LIBRARY_PATH=/opt/mellanox/dpdk/lib/aarch64-linux-gnu/But now I’m getting the following error:
/opt/mellanox/doca/examples/url_filter/bin# ./doca_url_filter -a 0000:03:00.0,class=regex -a auxiliary:mlx5_core.sf.4,sft_en=1 -a auxiliary:mlx5_core.sf.5,sft_en=1 -c3 – -pEAL: Detected 8 lcore(s)EAL: Detected 1 NUMA nodesEAL: failed to parse device “auxiliary:mlx5_core.sf.4”EAL: Unable to parse device ‘auxiliary:mlx5_core.sf.4,sft_en=1’EAL: Error - exiting with code: 1Cause: EAL initialization failedCan you try this set of arguments?/opt/mellanox/doca/examples/url_filter/bin/doca_url_filter -a 0000:03:00.0,class=regex:eth,representor=[65535],sft_en=0 –pI checked the docs today and it’s been updated to the string you are trying there. That might be a change to support updates/changes for DOCA 1.1 that just released.The missing LD_LIBRARY_PATH was fixed in the DOCA 1.1 BFB image, so I’m assuming your setup is installed on the previous release (DOCA 1.0).Anyway, the updated command line you’ve used applies to running the URL-Filter with SFs. I recommend going over the SF guide so to properly configure them on the DPU, as shown in the example figure in the application’s page.Thanks a lot for your response, Yes I had installed the previous version DOCA 1.0. Should I un-install and install the new version to get the newly released sdk 1.1?Can you try this and report back?/opt/mellanox/doca/examples/url_filter/bin/doca_url_filter -a 0000:03:00.0,class=regex:eth,representor=[65535],sft_en=0 –p/opt/mellanox/doca/examples/url_filter/bin/doca_url_filter -a 0000:03:00.0,class=regex:eth,representor=[65535],sft_en=0 –pshould I upgrade to version 1.1? I’m still using version 1.0after repeating the steps in the example, looks like it is working now! Thanks a lot./opt/mellanox/doca/examples/url_filter/bin/doca_url_filter -a 0000:03:00.0,class=regex:eth,representor=[65535],sft_en=0 –pEAL: Detected 8 lcore(s)EAL: Detected 1 NUMA nodesEAL: Multi-process socket /var/run/dpdk/rte/mp_socketEAL: Selected IOVA mode ‘VA’EAL: No available hugepages reported in hugepages-32768kBEAL: No available hugepages reported in hugepages-64kBEAL: No available hugepages reported in hugepages-1048576kBEAL: Probing VFIO support…EAL: VFIO support initializedEAL: Invalid NUMA socket, default to 0EAL: Probe PCI driver: mlx5_pci (15b3:a2d6) device: 0000:03:00.0 (socket 0)mlx5_pci: CT ASO is supported.mlx5_pci: CT ASO is supported.EAL: No legacy callbacks, legacy socket not createdcommon_mlx5: representor 2047 cannot set VF MAC address 1A:AF:E1:36:BF:10 : Invalid argumentINFO: 7 cores are used as DPI workersURL FILTER>>Happy to see that it works.On the BlueField OS image from March (DOCA 1.0) once LD_LIBRARY_PATH is configured to include “/opt/mellanox/dpdk/lib/aarch64-linux-gnu”, you will be able to run the applications using the 1.0 version of the command line (the “-a 0000:03:00.0,class=regex:eth,representor=[65535],sft_en=0” flags).Now that DOCA 1.1 was released, we recommend upgrading to it and to the latest BlueField OS version (3.7), as described in the updated installation guide. Once the new version is installed, the new command line (the “-a auxiliary:mlx5_core.sf.4,sft_en=1 -a auxiliary:mlx5_core.sf.5,sft_en=1”) should work as well.You can read more about the updated execution mode of the applications in the updated documentation here.Ok now that we have updated to doca 1.1
I’m getting a new error, any idea on this?
/opt/mellanox/doca/examples/url_filter/bin# ./doca_url_filter -a auxiliary:mlx5_core.sf.4,sft_en=1 -a auxiliary:mlx5_core.sf.5,sft_en=1EAL: Detected 8 lcore(s)EAL: Detected 1 NUMA nodesEAL: Detected shared linkage of DPDKEAL: Multi-process socket /var/run/dpdk/rte/mp_socketEAL: Selected IOVA mode ‘PA’EAL: No available hugepages reported in hugepages-32768kBEAL: No available hugepages reported in hugepages-64kBEAL: No available hugepages reported in hugepages-1048576kBEAL: Probing VFIO support…EAL: VFIO support initializedEAL: No legacy callbacks, legacy socket not created[03:06:48:938847][DOCA][E][FOFLD]: Application will only function with 2 ports, num_of_ports=0First, the example command line from the application’s documentation is the following:The error that you are seeing is probably an indicator that no Scalable Functions (SFs) were configured, or that the ones configured have different identifiers than “sf.4” and “sf.5”.The “-a” EAL flag adds a given device to the “allow list”, essentially meaning that the application will use 2 network devices (mlx5_core.sf.4 and mlx5_core.sf.5). In addition, as seen above in the example command line, the application also needs access to the RegEx hardware accelerator, and as such we also pass “-a 0000:03:00.0,class=regex”.The application’s documentation describes the recommended SF setup, and the Scalable Functions Guide explains how to configure the SFs themselves (and how to get their IDs so they could be used by the application).good informationHi, this error is returned to me, how can I fix it?Hi, is it possible that you are using DOCA 1.0 by any chance? Please see instructions below about upgrading to DOCA 1.1 which is the version that supports the above command line arguments.Now that DOCA 1.1 was released, we recommend upgrading to it and to the latest BlueField OS version (3.7), as described in the updated installation guide . Once the new version is installed, the new command line (the “-a auxiliary:mlx5_core.sf.4,sft_en=1 -a auxiliary:mlx5_core.sf.5,sft_en=1”) should work as well.You can read more about the updated execution mode of the applications in the updated documentation here .i think i have the latest version of doca, running these commands on the dpu i get these resultsThe DOCA packages are indeed those of DOCA 1.1, however I’m having trouble reproducing this error on my setup. Could you please add here the output of the following commands from your DPU?Powered by Discourse, best viewed with JavaScript enabled"
614,apt-get-update-fails-in-bf2,"root@localhost:/tmp# apt-get update
Get:1 file:/var/doca-dpu-repo-ubuntu2004-local ./ InRelease
Ign:1 file:/var/doca-dpu-repo-ubuntu2004-local ./ InRelease
Get:2 file:/var/hbn-repo-aarch64-ubuntu2004-local ./ InRelease
Ign:2 file:/var/hbn-repo-aarch64-ubuntu2004-local ./ InRelease
Get:3 file:/var/doca-dpu-repo-ubuntu2004-local ./ Release [551 B]
Get:4 file:/var/hbn-repo-aarch64-ubuntu2004-local ./ Release [469 B]
Get:3 file:/var/doca-dpu-repo-ubuntu2004-local ./ Release [551 B]
Get:4 file:/var/hbn-repo-aarch64-ubuntu2004-local ./ Release [469 B]
Get:5 file:/var/hbn-repo-aarch64-ubuntu2004-local ./ Release.gpg
Ign:5 file:/var/hbn-repo-aarch64-ubuntu2004-local ./ Release.gpg
Hit:8 Index of /ubuntu-ports focal InRelease
Hit:9 Index of /ubuntu-ports focal-updates InRelease
Hit:10 Index of /ubuntu-ports focal-backports InRelease
Hit:11 Index of /ubuntu-ports focal-security InRelease
Get:7 Index of /apt// kubernetes-xenial InRelease [8993 B]
Ign:12 Index of /apt// kubernetes-xenial/main arm64 Packages
Ign:12 Index of /apt// kubernetes-xenial/main arm64 Packages
Ign:12 Index of /apt// kubernetes-xenial/main arm64 Packages
Err:12 Index of /apt// kubernetes-xenial/main arm64 Packages
404  Not Found [IP: 172.217.160.142 443]
Fetched 8993 B in 2s (4177 B/s)
Reading package lists… Done
E: Failed to fetch https://packages.cloud.google.com/apt/dists/kubernetes-xenial/main/binary-arm64/by-hash/SHA256/9a285ef682715aa3b60d104849ed48d37384174f73cc66aaf600054a3ad51d2d  404  Not Found [IP: 172.217.160.142 443]
E: Some index files failed to download. They have been ignored, or old ones used instead.
root@localhost:/tmp# ./aptsources-cleanup.pyz
No duplicate entries were found.
root@localhost:/tmp# uname -a
Linux localhost.localdomain 5.4.0-1049-bluefield #55-Ubuntu SMP PREEMPT Mon Oct 17 20:09:22 UTC 2022 aarch64 aarch64 aarch64 GNU/Linuxapt-get update always fails on the DPU , i tried many workarounds but still didnt helphi thanujI have test your failed IP and address
It can be accessed, I think your issue caused by your network.
If you have issue with some source address.
You can modify /etc/apt/sources.list.d/ like normal UbuntuThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
615,latest-bluefield-2-os-with-doca,"Hi, in the other thread, it was mentioned that the latest BF2 image comes with DOCA install by default. However, it seems, that version is actually older than the current one installed on my Bluefield-2 (that does not have the DOCA installed). How can this be possible?
And, indeed there is no doca directory in /opt/mellanox, which means no DOCA support by default (?).My current Bluefield-2 OS is:The important part here is the 5.4.44-mlnx version number (or at least, I think it is the version number). Though, it is stated as mlnx not BlueOS/BluefieldThe one I can download:
https://content.mellanox.com/BlueField/BFBs/Ubuntu20.04/DOCA_v1.0_BlueField_OS_Ubuntu_20.04-5.3-1.0.0.0-3.6.0.11699-1-aarch64.bfb
has a version number 5.3 in its filename.After installing this “latest” version, and issuing the same commands, we can indeed see a lower/smaller version number:The version string here is not mlnx but Bluefield. Is it possible then these versionings are different that’s why I see lower version number?The 5.4.0 here in uname is the kernel version and will be the same for all Ubuntu 20.04 images.The 5.3.0 in the BF2 image name maps to the OFED mlx5 driver version. The DOCA BFB image also contains some additional DOCA software packages: Installation Guide :: NVIDIA DOCA SDK DocumentationThese two version systems don’t track or relate to each other.In theory it is possible to put together the software stack manually, but reimaging or reinstalling the BF2 using the DOCA BFB package is a nice clean install and one easy step.I see, I had the same feeling! Thanks for confirming @jubetzPowered by Discourse, best viewed with JavaScript enabled"
616,performance-degradation-when-using-connectx-6-dx-to-transfer-udp-multicast-jumbo-frames,"I am sending UDP multicast data between 2 PowerEdge servers that have ConnectX-6 Dx cards installed.  The operating system is RHEL 8.7 (4.18.0-425.3.1.el8.x86_64).  When running tests with iperf3 at high data rates (~27Gbits/sec), the packet loss increases from <1% (MTU 1500) to >35% (MTU 9000) when switching to jumbo frames (using ifconfig).  Are there any settings that need to change on the network card to support larger packet sizes sent at this rate?I would confirm that you have our latest MLNX_OFED driver/FW in place.There is/are no other setting(s) to set jumbo frame MTU to 9000 others than the ip link set or ifconfig.
The MTU should be set based on the message size sent.I would recommend using iperf/iperf2 and not iperf3 (We recommend using iperf and iperf2 and not iperf3. iperf3 lacks several features found in iperf2, for example multicast tests, bidirectional tests, multi-threading, and official Windows support).Some tuned up might be as well applicable here; we have different community articles and our driver UM you can consult.(If ethtool statistics report out of buffer, you can increase the RX/TX buffer up to 8192).At last, should you have a support contract with Nvidia, we can investigate your issue with further analysis.
Enterprise Support EnterpriseSupport@nvidia.comPowered by Discourse, best viewed with JavaScript enabled"
617,we-need-key-and-key-index-to-burn-fuse-for-secure-boot,"We want to enable secure boot.  We followed the instructions on NVIDIA DRIVE OS SDK Development Guide:
https://docs.nvidia.com/drive/drive_os_5.1.6.1L/nvvib_docs/index.html#page/DRIVE_OS_Linux_SDK_Development_Guide/Windows%20Systems/security_fskp.htmlThat’s what the article says：
Sharing an FSKP Key with the OEM
NVIDIA has a formal process to assign an FSKP key to an OEM. The participants in the process are:
• An FSKP owner who requests an FSKP key on behalf of the OEM.
• A key custodian who provides the key on behalf of NVIDIA.We need more information about specific operational processes.Powered by Discourse, best viewed with JavaScript enabled"
618,mellanox-connect-x5-rdma-debugging,"Hi,We are working on RDMA data transfer from custom ZynqMP-SOC platform to server PC(OS-CantOS-8 stream) with mellanox connect-x5 NIC card.
We are performing RDMA write operation from ZynqMP-SOC platform to server PC.We would like to know on debugging techniques available using Mellanox OFED-stack/ethtool applications on the server PC. I have the following questions on the same.Could you please help us to understand above questions?Thanks and Regards,
Abhishekhi1 NO, it’s impossible
2 In gerneral, ROCE will use different priority, you can check that priorit counter by ethtool -S
3 check hw-counter in /sys/class/infiniband/mlx5_0/ports/1/hw_counters/, if there’s no such counter, it means we do not have such one
4 same as 3Hi shim,Thanks for your input.
How ROCE priority is dependent on number of ROCE packets received? Is there any way on mellanox side to calculate total data transmitted using RDMA write operation?Best Regards,
Abhishekhiyou can use mlnx_qos set a single priority for roce,
https://docs.nvidia.com/networking/pages/viewpage.action?pageId=89154269This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
619,magp-number-of-switch,"Hello
What is the maximum number of routers participating in the MAGP protocol ?
RegardsHelloMAGP is intended to be used in conjunction with MLAG, so a maximum of 2 switches can participate (enable the feature)The user guide should provide additional insight about MLAG / MAGP
https://docs.nvidia.com/networking/display/Onyxv3104206/MAGPhttps://enterprise-support.nvidia.com/s/article/howto-configure-mlag-magp--running-config-exampleRegardsHello
Thank you
RegardsPowered by Discourse, best viewed with JavaScript enabled"
620,is-it-possible-to-use-sriov-on-bf2-not-on-the-host,"I am running multiple applications(dpdk-based) on one Bluefield SmartNIC. I want to separate the flow for this two dpdk applications based on ethernet frame mac address. I tried using scalable function and OVS on Bluefield 2 SmartNIC, it does separate traffic flow but the performance degradation is very high. So I want to try using SRIOV on BF2 to split one network interface to several interface.After some searching, I found that the host of BF2 can create VF and representors will show up on SmartNIC.Virtual Functions :: NVIDIA DOCA SDK Documentation But this method still requires OVS. I wonder if I can create VFs of the network interface on BF2, and directly use these VFs on BF2 w/o OVS, just like on any normal x86 servers?Hi @wsf123 ,You can use NIC mode - In this mode, the DPU behaves exactly like an adapter card from the perspective of the external host. The ECPFs on the Arm side are not functional in this mode but the user is still able to access the Arm system and update mlxconfig options.
This mode supports SR-IOV legacy, while the Smart NIC (embedded) mode only supports SR-IOV switchdev, which requires OVS.
https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/Modes+of+Operation#ModesofOperation-NICModeNICModeRegards,
ChenHi @chenh1 ,Thanks for your reply. I think I will try this setting. But maybe I did not deliver my question clearly, so please let me clarify my question.My desired setting is to run multiple apps on BF2. These apps need to share the Connectx 6 network interface on BF2. The question I would like to ask is that: can I let these applications(running on BF2) use the VFs of Connectx 6 network interface on BF2? This means I regard BF2 as a normal server with its own network interface and the applications on it share the NIC in a SRIOV style. Can NIC mode meet my requirements?Thanks!Powered by Discourse, best viewed with JavaScript enabled"
621,whats-the-proper-memory-region-access-flags-for-gpudirect-rdma,"Hi everyone,I’m trying GPUDirect RDMA technology to send some data in GPU memory to a remote host bypassing the GPU server’s CPU.When I register the GPU memory to the RDMA protection domain with empty access flag, the send/recv operations all succeed without reporting any error, but the data received in remote host are just a bunch of zeros. When I change the GPU side mr access flag to IBV_ACCESS_LOCAL_WRITE, the remote host can receive the correct data.From my perspective IBV_ACCESS_LOCAL_WRITE is not required on the GPU side because the RDMA HCA only reads the data in that region. What’s the problem here?System environment: I am using Nvidia A100 with CUDA version 12.1 on the GPU side and ConnectX-6 Infiniband cards on both sides.Hi chensy20,
This is not related to GPU or CPU memory. When you are using local buffer, without giving remote write access, we still set LOCAL_WRITE access flag to be able to write data locally before you are sending it to remote side.
This is what I can see in all RDMA samples not related to GPU.
Best regards,
Michael.Does that mean we still have to set LOCAL_WRITE flag even if that buffer is only modified by user but not by RDMA operations?YesI suggest to take some simple working sample for CPU memory and use it as referenceOk, thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
622,i-need-to-upgrade-the-firmware-in-a-mellanox-card-mt27800-they-are-installed-in-a-esxi-running-vpshere-6-7-u3-i-tried-to-install-the-mft-tool-but-i-have-this-error-l-could-not-find-a-trusted-signer-certificate-has-expired,"i have other hosts with 6.5 version i dont receive any error, anyone knows if the mfs and nmst tool have some compatibility issue with 6.7 U3? i downloaded the latest version , i tried to force the installation but same error, files :nmst-4.13.0.104-1OEM.650.0.0.4598673.x86_64.vib and mft-4.13.0.104-10EM-650.0.0.4598673.x86_64.vibHi,To workaround this issue apply the following:esxcli software acceptance set --level=CommunitySupportesxcli software vib install -d <vib_file> --no-sig-checkesxcli software acceptance set --level=PartnerSupportedThanks,Samerabove suggestion fails with secure-boot[root@esx:~] esxcli software acceptance set --level=CommunitySupportInvalid data constraint for parameter ‘level’. Expected a single value from the set [VMwareCertified, VMwareAccepted, PartnerSupported, CommunitySupported] got ‘CommunitySupport’[root@esx:~] esxcli software acceptance set --level=CommunitySupported[AcceptanceConfigError]Secure Boot enabled: Cannot change acceptance level to community.Please refer to the log file for more details.This post is a year ago, so this is probably not very helpful, but the issue was that it’s CommunitySupport but it should be CommunitySupported, like so:esxcli software acceptance set --level=CommunitySupportedThe full steps are available here:https://support.mellanox.com/s/article/ESXi-Packages-Installation-Fails-Could-not-find-a-trusted-signerPowered by Discourse, best viewed with JavaScript enabled"
623,performance-degradation-issue-when-applying-asap2-and-conntrack-offload-in-openstack-and-ovn-environment,"Sure, I can help you with the translation. Here’s the translation of the content you provided:1.Hardware
Server: Dell R7615
CPU: AMD Epyc 9654P
Memory: 384GB
NUMA: 1
NIC: Connect-X 6LX2.Software Versions
OS: Ubuntu 22.04.2 LTS
Kernel: 5.15
Openstack Version: Yoga
OVN: 22.03
OVS: 2.17.5
MLNX OFED Driver: 5.8-2.0.3
Firmware: 26.35.1012 (DEL0000000031)3.SmartNIC Configuration
3-1. Driver
vport_match_mode: metadata
steering_mode: smfs
ct_action_on_nat_conns: disable
ct_labels_mapping: disable3-2. Devlink Param
num_of_groups: 153-3. MSTConfig
PF_NUM_PF_MSIX_VALID: False(0)
STRICT_VF_MSIX_NUM: False(0)
NUM_PF_MSIX_VALID: True(1)
NUM_PF_MSIX: 127
NUM_VF_MSIX: 127
PF_NUM_PF_MSIX: 63
DYNAMIC_VF_MSIX_TABLE: False(0)4.OpenStack Configuration
Using OVN Mechanism Driver
Applying HWOL during host configuration
Applying OVN ACL and Conntrack Offload5.CPU Pinning and Multi Channel
Linux Scheduler, Systemd: 8 Cores Pinned ( isolcpus )
HWOL worker Queue: 6 Cores Pinned
Interrupt: 2 Cores Pinned
PF Channel: Combined 4
Representation Port: Combined 26.Guest VM
Flavor: 16 vCPU, 32GB
Driver: 5.8-2.0.3
OS: Ubuntu 20.037.Performance Testing
Packet Size: 64 bytes
Protocol: UDP
Source IP & Port: 100EA7.1 Provider Network Performance Testing
Load: 10 Mpps
Packet processing rate per interface
- OVS Internal Bridge: 10 Mpps
- OVN Patch Port: 7.71 Mpps
- Representation Port: 7.69 Mpps
- Guest VM performance: 7.72 Mpps7.2 Overlay Network + Floating IP
Load: 6 Mpps
Packet processing rate per interface
- OVS Internal Bridge: 6 Mpps
- OVN Patch Port: 5.11 Mpps
- Representation Port: 5.11 Mpps
- Guest VM performance: 5.10 MppsIn the above environment, performance testing was conducted by generating 64-byte UDP packets. Packet processing degradation was observed without any CPU bottlenecks or packet drops. HWOL Conntrack, NAT, and Geneve Offload are functioning properly. What other aspects would you like to verify?Powered by Discourse, best viewed with JavaScript enabled"
624,mlx5-ib-issue-while-installing-mlnx-ofed-linux-5-1-0-6-6-0-to-ubuntu-16-04-lts-with-hwe-kernel,"Hi, we encounter a mlx5_ib issue while we are installing MLNX_OFED_LINUX-5.1-0.6.6.0 to our Ubuntu 16.04 LTS OS with HWE kernel (4.15.0-140-generic).The MLNX_OFED_LINUX-5.1-0.6.6.0 driver seems to be the newest we could install according to the support matrix because we are using a Dell Mellanox ConnectX-5 NIC with firmware version 16.27.61.20. There is no newer firmware available from Dell at this time and we cannot use firmware from the Mellanox site because the Dell NIC has a “DE” PSID.I have attached the log files and the output.mlnx-ofed-kernel-dkms.debinstall.log (79.9 KB)mlnx-ofed-kernel-dkms.make.log (524 KB)ScreenOutput.txt (1.73 KB)Hello Jeroen,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, this is a known issue which is fixed in the upcoming MLNX_OGED 5.3 GA, targeted for the end of the month.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
625,mellanox-sn2410-issues,"I am unable to enter the interface config mode(Check attached screenshot)I figured I’d try to factory reset the switch. Unable to do that as well(screenshot)for the reset factory , the answer should be in capital letters YESfor the interface context issue:(config) # cli default prefix-modes enable(config) # configuration writePowered by Discourse, best viewed with JavaScript enabled"
626,delete-cl-support-files,"To delete the cl-support files that you generate when you need to file a support ticket, use the sudo rm <file> command to remove the file(s). Make sure to specify the full file path:Powered by Discourse, best viewed with JavaScript enabled"
627,doca-flow-inspector-service-cant-start-properly-on-bf-2,"I am trying to deploy doca-flow-inspector service, but the container consistently can’t start properly, I see the following error in the container log, how to solve it, is it because my hardware doesn’t support it?Many thanks in advance,tuningerhi goodtuningerYou miss some config for enable HWS.
please reference :
https://docs.nvidia.com/doca/sdk/flow-programming-guide/index.html#sample-prerequisitesCLI example for running the samples with “vnf,hws” mode:
./build/doca_<sample_name> -a auxiliary:mlx5_core.sf.2,dv_flow_en=2 -a auxiliary:mlx5_core.sf.3,dv_flow_en=2 – -l 60For hws(hardware steering) detail:
https://doc.dpdk.org/guides/nics/mlx5.htmldv_flow_en parameter [int]Value 2 enables the WQE based hardware steering. In this mode, only queue-based flow management is supported.Thank you
Meng, ShiPowered by Discourse, best viewed with JavaScript enabled"
628,roce-b-w-into-bf2-device-in-separated-mode,"Hi,
I have a 100GbE BF2 device (A) MBF2M516A-CENOT that is configured in separated mode.  I have another two hosts (B and C) with a ConnectX-5 100GbE. All connected to a Mellanox 100GbE switch.
Using perftest ib_send_bw, B-to-C I see the full ~90Gbps b/w (expected). When I run A-B or A-C I only see < 1 Gbps.I know this is not the usual bump in the wire mode of operation but I thought moving data in the ARM’s memory from the net interface would be more.Can someone indicate whether this is about right and expected or not?Not sure how you test, steps/logs etc.in separated mode, you need select real PF port, then run perftest on ARM.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
629,infiniband-partitioning-in-rocev2,"Is there a way to use Infiniband partitioning for Mellanox with Ethernet link layer(I am using RoCEv2)? 
In RoCEv2 spec Subnet Managment is defined not applicable and Methods for setting the P_Key table associated with a RoCEv2 port are not defined. But still The P_Key contained in the BTH is validated for inbound packets as required by the packet header validation protocols defined in Chapter 9 of the base specification.
C9-42: If the destination QP is QP1, the BTH:P_Key shall be compared to the set of P_Keys associated with the port on which the packet arrived. If the P_Key matches any of the keys associated with the port, it shall be considered valid. Hi,
Partition is a feature for InfiniBand, but RoCE is Ethernet, So it is not the same thing.
Thanks,This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
630,how-to-use-mellanox-connectx-4-lx,"Hello,I have a ​Mellanox ConnectX®-4 Lx and a Voltaire Grid Director 4036E infiniband switch. I also have a Mellanox cable that goes from 4 of the smaller ends to one of the bigger (regular sized) ends to work for this switch and card (Mellanox Technologies QSFP / 4 SFP+, 1m InfiniBand Cable 4 x SFP+).​Unfortunately, I’m starting to think that I have the wrong card (and that this only works for Ethernet), because I am unable to change the link type of this card to infiniband. I have followed all the instructions, but it says that the option (LINK_TYPE) isn’t found when I try via the command line.​​–This command just says that LINK_TYPE_P1, LINK_TYPE_P2 is not an existing parameter:mlxconfig -d /dev/mst/mt4117_pciconf0 set LINK_TYPE_P1=1 LINK_TYPE_P2=1​And this query here does not show this “LINK_TYPE” as an option at all:mlxconfig -d /dev/mst/mt4117_pciconf0 q–​Is it impossible to get this adapter to work just like a regular infiniband card… ? Just like how the ConnectX-3 works?​​​ThanksHello Shaun,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately the ConnectX-4 Lx is an Ethernet adapter only. You will not have the option to change the port-type as with our VPI adapter cards.Thank you and regards,~NVIDIA Networking Technical SupportHey,Thank you for officially confirming. I thought as much based on research I did, but I wanted to hear it from the horse’s mouth, so to speak. How come you make the cables for it then? https://highgate-it.co.uk/mc2609130-001-mellanox-technologies-qsfp-4-sfp-1m-infiniband-cable-4-x-sfp-black (Unless I guess you can use that older infiniband end as ethernet too… probably…)Powered by Discourse, best viewed with JavaScript enabled"
631,mellanox-sn2010-mlag-configure-issue,"Hi, I am new to the Mellanox Switch. I am going to configured MLAG for SN2010.I found that the port-channel for MLAG is up and “Operational Status : UP” from show mlag. But these two switch seems cannot form cluster ( no master and slave status). There is no result from show mlag-vip as well. Is there anything missing? Thanks
MLAG_3.png1664×1112 10.9 KB
mgmt0 port is up?No, mgmt0 is down. I haven’t configured anything on mgmt0 port!!Current physical connectionCore_SW01 eth1/21-1/22 <-> Core_SW02 eth1/21-1/22 (IPL)Core_SW01 eth1/1-eth1/2 & Core_SW02 eth1/1-eth1/2 (MLAG interface to Access-layer SW)Is it necessary to have connection at mgmt0 interface between Core_SW01 and Core_SW02?The cluster status is based on the vip connection that runs across the mgmt0 ports.As shown in this doc (https://docs.mellanox.com/display/Onyxv382306/MLAG), it is expected for the cluster to be unknown if this connection is down:“”""In a scenario where there is no IP communication between the MGMT ports of the MLAG switches (for example when one MGMT port is disconnected), the following CLI prompt is displayed: [:unknown]#. This does not reflect the MLAG state, but only the state of the cluster.“”""Powered by Discourse, best viewed with JavaScript enabled"
632,mlxconfig-for-switches-in-non-default-ib-subnet,"Hi, I looked around quite a while but my problem is obviously rare:
On a system attached to two IB fabrics (“subnets”) via mlx5_0 and mlx5_1, resp., how can I reach un-managed switches in the Fabric mlx5_1 is connected to by mlxconfig?I can get the switches and other devices of that secondary fabric byinto the /dev/mst directory,  but mlxconfig appears to not find these in the IB network:mlxconfig has no option/command-line switch to determine on which fabric it should look for the specified LID …
Acessing switches in the default (first) fabric behind mlx5_0 works ok.Uwemlxconfig has no option/command-line switch to determine on which fabric it should look for the specified LID …Hi Uwe,Correct. mlxconfig has no option to specify the out port.
Have you tried to run mlxconfig on a host within one fabric only?Thanks,
YuyingThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
633,dgx-a100-when-8-ib-network-cards-use-ib-write-bw-to-test-the-bandwidth-at-the-same-time-the-rate-decreases-which-is-not-expected,"I have a DGX A100 machine, and each machine has 8 200G single-port IB network cards. When I use the following command to test the bandwidth between the two at the same time, I found that I did not get what I expected. I thought it was Eight network cards can reach the rate of 200Gb/s at the same time, but the actual rate is greatly reduced.
They are connected to a QM8790 at the same time (use 200G AOC line)
Client：run perftest multi devices -d mlx5 0,mlx5 1,mlx5 2 -C 0,1,2 -cmd “ib write bw --report gbits” --remote 172.32.255.21
6332fff6a1a0ea9d9bac3b11aeecd831474×1372 98.7 KB

image657×687 86.9 KB

I think it may be a problem with the way I use the command, so I am eager to get some guidance, or the correct way to test the bandwidth of 16 IB network cards on two DGXs at the same timeplease help meHi,Have you tried pinning the HCA devices to the nearest cores?Please use ‘mst status -v’ to see each device’s local NUMA;Check the cores assigned to the NUMA using lscpu (or similar);Utilize the closest core/s for the HCA.-c, --cores Pin each device to a specific core using taskseti.e. --cores 0,1 - This will pin dev1 command to core 0 and dev2 command to core 1This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
634,release-notes-for-nvidia-bright-cluster-manager-9-0-19,"Release notes for Bright 9.0-19== General ==
=New Features==Improvements==Fixed Issues=== CMDaemon ==
=New Features==Improvements==Fixed Issues=== Bright View ==
=Fixed Issues=== Node Installer ==
=Improvements==Fixed Issues=== Cluster Tools ==
=Fixed Issues=== cmjob ==
=Fixed Issues=== Machine Learning ==
=New Features==Improvements=== cm-create-image ==
=Fixed Issues=== cm-kubernetes-setup ==
=Improvements==Fixed Issues=== cm-scale ==
=Fixed Issues=== cm-uge ==
=Improvements=== cm-wlm-setup ==
=Improvements==Fixed Issues=== cmsh ==
=Fixed Issues=== openpbs22.05 ==
=Improvements=== slurm ==
=Improvements=== slurm21.08 ==
=Improvements==Fixed IssuesPowered by Discourse, best viewed with JavaScript enabled"
635,whats-the-modulator-chip-of-the-mma1t00-hs-200g-ib-transceiver,"In the market, there are two kinds of modulator chip of 200G-SR4:the two chip could not communicate each other.SO, I want to know what’s the module chip of the MMA1T00-HS 200G-IB-Transceiver?ThanksNvidia product is DSP.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
636,firmware-for-sb7800-edr-switch-ib-2,"Hi, I’m a bit confused as for why there is nothing firmware-wise for the SB7800 in the Mellanox’s downloads. I do see SB7890 as the only one in the family. PSID for the device is MT_2630110032 , and it’s nowhere to be found either.Any ideas what’s this one about ? I’m on the support contract from DELL (that’s where the switches came from) and opened the SR, but thought that maybe would get to the bottom of this faster here…Thanks!Hello Karol,Thank you for posting your inquiry on the NVIDIA Networking Community.As you have a managed SB7800 switch, the latest MLNX-OS code for the switch is only available for contracted customer.As you mentioned that you have this switch under contract with Dell they need and will provide you with the MLNX-OS code you are requesting.Thank you and regards,~NVIDIA Networking Technical SupportThanks! However how do I differentiate in the future between what’s going to have FW available as it is with eg. SB7890 (as opposed to FW available only through MLNX-OS) ?Is it so that managed switches are limited to FW via MLNX-OS only ?Powered by Discourse, best viewed with JavaScript enabled"
637,mellanox-neo-performance-provider-fails-to-start-in-version-2-7-20-also-in-2-7-10-how-to-fix,"Just installed Mellanox NEO 2.7.20 and I see it has the same problem as the previous 2.7.10 release, namely the performance provider fails to start, here’s the output from “/opt/neo/neoservice start”:checking ha-mode : … [ OK ]Checking Localhost resolving … [ OK ]Checking Disk space usage … [ OK ]Starting neo-access-credentials: [ OK ]Starting neo-controller: [ OK ]Starting neo-device-manager: [ OK ]Starting neo-eth-discovery: [ OK ]Starting neo-host-manager: [ OK ]Starting neo-ib: [ OK ]Starting neo-ip-discovery: [ OK ]Starting neo-monitor: [ OK ]Starting neo-performance: [FAILED]Starting neo-provisioning: [ OK ]Starting neo-solution: [ OK ]Starting neo-telemetry: [ OK ]Starting neo-virtualization: [ OK ]Redirecting to /bin/systemctl start monit.serviceLooking at the console.log of the performance provider, located under /opt/neo/providers/performance/log/, I see this::0: UserWarning: You do not have a working installation of the service_identity module: ‘cannot import name ‘opentype’’. Please install it from https://pypi.python.org/pypi/service_identity and make sure all of its dependencies are satisfied. Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification. Many valid certificate/hostname mappings may be rejected.2021-08-12 16:11:09.408 netservice INIT Starting Provider…-E- Could not find version file.I guess the warning is just a warning, and the error is the “Could not find version file” part, does anyone know how to fix that?Hello Ivan,Thank you for posting your question on the Mellanox Community.Looking at the shared snippet of the logs, it looks like you are missing the python module, service_identityPlease try using the following commandpip3 install service_identityLet us know if this resolves your issue or if you need further assistanceThanks and regards,~Mellanox Technical SupportI had the same problem and in the end I managed to solve it.After some debug, “-E- Could not find version file.” refers to the “gperf” version.Downgrading from “3.0.4-8.el7” to official package version, all works:yum downgrade ./neo-2.7.20-4.el7/neo/packages/gperf-3.0.0-6.noarch.rpmPowered by Discourse, best viewed with JavaScript enabled"
638,uneven-load-softirq,"Hello, i have few servers 100gbps, on one ubuntu 22.04 i noticed first 4 softirqd/64, 65, 66, 67 have more load% cpu than another processes, why ? on debian 11 all queues (63) load evenly cpu% and another 1-10-20-40gbps server i check it ixgbe load evenly all core.check attached screenshot

irq21920×1080 186 KB


irq11920×1080 186 KB
Channel parameters for enp161s0f0np0:
Pre-set maximums:
RX:             n/a
TX:             n/a
Other:          512
Combined:       63
Current hardware settings:
RX:             n/a
TX:             n/a
Other:          0
Combined:       63maybe on first 4 proccesses work rx 4 queues, tx work on all 63 ? how it check ?Hi z3rom1nd3,Please confirm if you are using the latest MLNX OFED driver for the Nvidia(Mellanox) Adapters. You can run the command #ofed_info -sixgbe is the driver for Intel Adapter cards. Please confirm that the question posted on the community is relevant to Mellanox/Nvidia Adapter.Are both, the Ubuntu 22.04 and Debian 11 host 100% identical?Thanks,
Namrata.ofed_info -sThank you for sharing the details. As the systems are not 100% identical, unfortunately, we cannot determine why the systems have a difference in output unless we can debug it further.In order to perform a complete debug, a support ticket needs to be opened.  This would require a valid support contract. You may reach out to our contracts team at Networking-Contracts@nvidia.comThanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
639,what-is-the-procedure-to-update-the-driver-for-mlx5-core,"driver: mlx5_coreversion: 4.1-1.0.2 (27 Jun 2017) -->> Needs upgrade to 4.4-1.0.0 (HOW??)firmware-version: 14.20.1010 (MT_2470111034) → Upgraded to firmware-version: 14.24.1000 (MT_2420110034) . THis is done.Is there a separate procedure after FW is upgraded to upgrade Driver as well?Hello John,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, you managed to upgrade to f/w of the adapter successfully.For upgrading the driver, please download the driver version you want to install and follow the installation instructions mentioned in the UM → https://www.mellanox.com/related-docs/prod_software/Mellanox_OFED_Linux_User_Manual_v4_4.pdf , Chapter 2. Installation.We recommend to stop the driver first and them run the installation script. The installation script will remove the previous version of the driver and will install the new version.Example syntax:# /etc/init.d/openibd stop# <path to new MLNX_OFED driver s/w>/mlnxofedinstall -vvvThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
640,do-nic-virtual-functions-support-pxe-boot,"I have a XCP (xenserver) host with a ConnectX-4 card. I have enabled VF on the NIC, and can pass them through to my VM’s.
My VM needs to PXE boot using the attached VF NIC, but booting fails with ‘no bootable device’.Are VF’s supposed to support PXE booting or is this expected behavior?Hello Martin,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Unfortunately this is not possible as the VF does not have access to the option ROM on the physical adapter. Expected behavior.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
641,arm-cortex-a78ae-schedulling,"Does the ARM Cortex-A78AE  scheduller have feature to schedule packets based on a flow to schedule packets of a flow on one core and not in another core at the same time to help atomicity? This will help one packet of a flow to be processed by only one core at a time.Powered by Discourse, best viewed with JavaScript enabled"
642,mellanox-connectx-4-driver-update-on-synology,"Hello,I have a Synology DS1821+ and I’m having a weird issue with my Mellanox ConnectX-4 card.
(I am temporarily using a 1gbit ISP modem, network overhaul is planned in the near future)So, I’m using a SFP+ to RJ45 transceiver which does 1, 2.5, 5 and 10gbit on my NAS, and with that, I achieve the full 1gbit (WAN) connection and iperf3 also shows a 1gbit connection. But when I transfer a file over the SMB protocol to a desktop PC, i only get 30-35MB/s which i find very strange, and that is with the Mellanox card with SFP+ to RJ45 transceiver/module.Now, with just a cat6a cable directly into a 1gbit port on the NAS mobo itself, the SMB transfer speed is completely normal at the full 1gbit, the same for all the services and WAN connection.So I think there is something wrong with the transceiver or NIC.
Someone told me it’s okay that my ConnectX-4 card is using the mlx5_core drivers, eventhough I have a ConnectX-4 card, is this true?I think my firmware version is up to date, but I think the driver that was already on my Synology DS1821+ mlx5_core is outdated, might this be the issue or is would it be the transceiver not working as intended?
I think the transceiver SFP+ 10gbit to RJ45 needs to ‘trick’ the NIC into believing it’s a 10gbit connection and it has to autoneg that at the RJ45 end?Anyways, it’s probably wise to update the drivers but I don’t know which OS to select for my Synology DS1821+ all I know is this Linux-4.4.180+-x86_64-withI also tried binding the NIC to mlx4_core but I didn’t succeed in that.
And it also does not want to run at 1000mbps with ethtool -s eth5 speed 1000
Not even with autoneg off.I’ll wait patiently for your guys input.Thank you for your time.Hi jari_f1996,mlx5 is the low-level driver implementation for the Connect-IB and ConnectX-4 and above adapters designed by NVIDIA. mlx4 VPI driver only works with ConnectX-3 and ConnectX-3. Both mlx4 and mlx5 are included in the MLNX OFED.Please kindly found all the MLNX OFED drivers from the below link:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)Please note that MLNX OFED can support most of the common OS distributions. All the major distributions, RHEL, SLES, Ubuntu and more, also come with Inbox driver for Nvidia adapters.Thanks,
YuyingThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
643,mlag-ipl-link,"Good MorningI configured two 1410 switch with MLAG using the docMellanox Interconnect Community. I read that all VLANs are open on this port. There is no need to configure that, as once an interface is mapped as IPL all the VLANs are open on this port.in my config for PO1 (IPL) are used port 1/49 and 1/50sw-amb-40G-u39 [amb-vip: master] # show mlagAdmin status: EnabledOperational status: Up…MLAG IPLs Summary:ID Group Vlan Operational Local Peer Up Time Toggle CounterPort-Channel Interface State IP address IP address1 Po1 4001 Up 11.0.0.39 11.0.0.40 0 days, 12:08:19 3If I run command show vlan I don’t see any VLAN on ports that belongs to PO1 ???. It is normal ???sw-amb-40G-u39 [amb-vip: master] # show vlanVLAN Name Ports1 default Eth1/3, Eth1/4, Eth1/5, Eth1/6, Eth1/9,Eth1/10, Eth1/11, Eth1/12, Eth1/14, Eth1/15,Eth1/16, Eth1/17, Eth1/18, Eth1/21, Eth1/22,Eth1/23, Eth1/24, Eth1/25, Eth1/26, Eth1/28,Eth1/29, Eth1/30, Eth1/32, Eth1/33, Eth1/34,Eth1/35, Eth1/36, Eth1/40, Eth1/41, Eth1/42,Eth1/46, Eth1/47, Eth1/48, Eth1/51, Eth1/52,Eth1/53, Eth1/54, Eth1/55, Eth1/56, Eth1/57,Eth1/58, Eth1/59, Eth1/60, Mpo900102 Mpo531, Mpo900125 Mpo100, Mpo101, Mpo102, Mpo103, Mpo104,Mpo900540 Mpo303, Mpo304, Mpo531, Mpo800, Mpo9004001II see that PO1 in in trunk modesw-amb-40G-u39 [amb-vip: master] # show interfaces port-channel 1Po1:Admin state : EnabledOperational state : Up…Switchport mode : trunkit is normal that the ports that belongs to PO1 re in access mode ??w-amb-40G-u39 [amb-vip: master] # show interfaces ethernet 1/49Eth1/49:Admin state : EnabledOperational state : UpLast change in operational status : 12:11:46 ago (3 oper change)Boot delay time : 0 secDescription : mlag-iplMac address : 7c:fe:90:fa:3f:73MTU : 9216 bytes (Maximum packet size 9238 bytes)Fec : autoFlow-control : receive off send offActual speed : 40 GbpsAuto-negotiation : EnabledWidth reduction mode : UnknownSwitchport mode : accessMAC learning mode : Enabledsw-amb-40G-u39 [amb-vip: master] # show interfaces ethernet 1/50Eth1/50:Admin state : EnabledOperational state : UpLast change in operational status : 12:13:04 ago (3 oper change)Boot delay time : 0 secDescription : mlag-iplMac address : 7c:fe:90:fa:3f:53MTU : 9216 bytes (Maximum packet size 9238 bytes)Fec : autoFlow-control : receive off send offActual speed : 40 GbpsAuto-negotiation : EnabledWidth reduction mode : UnknownSwitchport mode : accessMAC learning mode : EnabledThanks in advanceHi Enrico,I see there is already a support case #00937026 opened for this question and I am working with Walter Bernocchi. To answer the question here, this is normal. If you have any further questions, you can reach out via support case and I will be happy to answer your questions.Thanks,Pratik PandePowered by Discourse, best viewed with JavaScript enabled"
644,problem-with-enabling-gtp-flex-parser,"Hi i want to use DPDK with mellanox NIC ConnectX4LX to loadbalance GTP traffic on queue of NIC base on inner IP layer.
i want to use rte flow for this purpose.
for using GTP in rte flow i must run this command:output:i dont know how to fix this.
i am using  MLNX_OFED_LINUX-5.4-3.1.0.0.my second question:
for my purpose that’s loadbalace GTP with RSS on NIC queue. is ConnectX4LX supports all the requirements?thanks.Hi @ali64mohammad6464 ,GTP is only supported starting from ConnectX-6 Dx.
ConnectX-4-Lx doesn’t support this flex profile.Regards,
ChenPowered by Discourse, best viewed with JavaScript enabled"
645,vpi-gateway-license-for-sx6036-details,"Is there documentation on what is provided by the Gateway license for the SX6036.  We have one in production running IB but only 8 ports and would like to use some of the remaining for 40GB Ethernet.Thanks,ToddHi ToddPlease refer to below article56GbE Etherent switch license is free of charge.In case you own the below Mellanox Ethernet Switches (or newer):It is also applicable for the VPI switches running in Gateway or Ethernet switch mode:/HyungKwangPowered by Discourse, best viewed with JavaScript enabled"
646,connection-between-two-infiniband-ports,"Hi All,I have a remote system in which ib0,ib1,ib2,ib3,ib4 and ib5 are the infiniband network interfaces.ib0 and ib1 are externally connected back to back with a cable[loopback connection].ib2 and ib3 are not connected.ib4 and ib5 are externally connected back to back with a cable[loopback connection].Is there any Linux command or mellanox utility through which one can detect that [ ib0 - ib1 ] and [ ib4 - ib5 ] are the pair of network devices connected back to back given all the network devices? Please provide some pointers on how to detect the connected pair.Thanks in Advance!Appreciate your reply. I went and tried running ibtracert and it works.Regarding subnet manager: For infiniband 2 port devices I believe only 1 subnet manager will be running. Does ibtracert work on infiniband 2 port devices as only 1 subnet manager will be running ?(Assuming you have 2 subnet managers running on these looped back ports) you could use /sbin/ibtracert to see the connection paths. Though just dumping the ibnetdiscover/ibdiagnet for each port should be pretty self explanatory.IBA architecture has (at least one) subnet manager per subnet. In the case of back to back CAs (without switches), each back to back connection is separate subnet. You can run multiple instances of SM on same machine but it needs proper configuration.Powered by Discourse, best viewed with JavaScript enabled"
647,is-mcx512a-acat-compatible-with-aspm,"I thought that ConnectX-3 and below were incompatible with ASPM, and after plugging a MCX512A-ACAT, It blocks my CPU package from going into a C8 state as it used to, and it now stays at C2 with nothing plugged into the SFP ports.01:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]
LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported
LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk+
01:00.1 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]
LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported
LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk+I’m pretty much unable to find a working example of a ConnectX-5 reporting either L0s or L1. No luck enabling ADVANCED_POWER_SETTINGS or ADVANCED_PCI_SETTINGS.So, does the card have any way to enable more power efficient modes? Or am I out of luck?Hi Barrenechea,Thank you for posting your question on NVIDIA community.ASPM as mentioned in lspci output, is not supported by the HCA. However, you may check if the feature can be enabled in BIOS/system drivers.In order to check system related details, it would be great if you can reach out to your server vendor.Thanks,
Namrata.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
648,dpi-sample-applications,"DOCA provides a DPI engine that leverages several individual components to provide a framework for examining the full content of data packets. Two sample DPI applications with their source code are included with Bluefield DOCA software:The files for these applications are found in /opt/mellanox/doca/examplesI tried to run and test the URL filtering application. I am so sorry to say this but the information found at DOCA SDK documentation is kinda useless as it is.
Can someone clarify these important aspects I tried to summarize below?
It’s a bit long, but I wanted to include everything that others might ask later.Very “meaningful” error.Furthermore, class=regex:eth is also not clear. DPDK documentation says it is used to access the DPI engine (i.e., RegEx HW accelerator on the Bluefield). It is okay then, but is there any meaning behind regex:? eth is just for ETHERNET, or it tries to identify an interface ID?sft_en=0 - I have not found any information about this at all.Besides the EAL options, using the app is also not straightforward. There is no pattern to follow how URLs should be defined. I found out by having a quick look at the source code that is used Suricata…so it creates Suricata definitions from the CLI commands.3.1. How to test the application? There is no information about how the “testing” environment should look like.
3.2 What interface we should send out the HTTP queries (for instance, by using SCAPY for brevity) to check the output of the URL filter?
3.3 The URL filters should not be quit, right? The output should be seen on its CLI?
3.4. Also not clear from which part of the system we should send the testing packets? Do they come from the Host? If yes, then there is no information about in which mode the SmartNIC should be? SEPERATED_HOST or EMBEDDED_CPU? I guess the latter, but still, then what application will connect the HOST’s logical interfaces to the Bluefield’s physical interfaces, i.e., the interfaces seen as ens5f0 (for instance in my case), connected through pf0hpf logical interface and the p0 physical interface on the Bluefield? Should there be an OvS running? If yes, again, can we run two DPDK applications on the same port? URL filter already consumes all the CPU cores by default…how will OvS run then?I tried to filter on slashdot.org via this filter command:it was eaten by the app but then did not know how to proceed. My Bluefield ports are, of course, not the internet-facing ports in my experiment, so I tried sending scapy-crafted packets on the physical ports, both from the host and from the Bluefield. In particular, from Host’s ens5f0, from Bluefield’s pf0hpf, and p0. Scapy said packet sent, but no message is observed in the URL filter app.Thanks, and sorry for the avalanche of questions :)Okay, it seems that after gaining access to all materials, the answers for some of the questions above can be found in the DOCA SDK documentation. Just not necessarily at the particular applications’ descriptions (like in the case of URL Filter) but at the programming and DPI/RegEx applications’ documentations. It would be nice to have cross-references :)Create DPI rules for the URL filter app: DPI Compiler :: NVIDIA DOCA SDK DocumentationRegarding the EAL flags, there is a note here: Application Recognition :: NVIDIA DOCA SDK DocumentationAbout representors: vSwitch and Representors Model :: NVIDIA DOCA SDK DocumentationThanksHi there again :)After finding the related material, I was playing around with the URL filter application, and I would like to ask whether it is normal that once it alerts on a specific packet it drops all consecutive ones WITHOUT even alerting?I have the following setup:
2 Host machines (H1,H2) connected back-to-back through Bluefield-2 DPUs.The Bluefield-2 DPU @ H2 is running the filter application. I have digged deeper a bit in Suricata (still very far from understanding much of it, though :)) and I created a DNS filter instead of the HTTP filter as I was not sure whether the TCP connection to a webserver should be established for the URL filter app to work.So, I created the following rule description /tmp/signature.txt:
@Bluefield@H2:
alert dns any any -> any 53 (msg:""Test dns.query option""; dns.query; content:""google""; nocase; sid:1;)Then, after starting the URL filter, I loaded this file as a database to let the URL filter to compile it to CDO.
@Bluefield@H2:
/opt/mellanox/doca/examples/url_filter/bin/doca_url_filter -a 0000:03:00.0,class=regex:eth,representor=[65535],sft_en=0 -- -pURL FILTER>> commit database /tmp/signature.txtIt is compiled without errors.Then, I craft a DNS packet on the other Bluefield@H1 (which is connected to the Bluefield@H2) and send it to the corresponding interface.@Bluefield@H1
>>> dns_packet=Ether(dst=""0c:42:a1:a4:8a:08"")/IP(dst=""8.8.8.8"")/UDP()/DNS(rd=1, qd=DNSQR(qname=""google.com""))>>> sendp(dns_packet,iface=""p0"") Without having any OvS(-DPDK) running on the Bluefield@H2, I encountered that packets are also sent up to the host itself, i.e., to H2. So, I ran a tcpdump on H2.
@H2: ifconfig ens5f0 up
@H2: tcpdump -ni ens5f0 udpSo, after sending a crafted DNS packet from @Bluefield@H1, I observe the following:
@Bluefield@H2:
INFO: SIG ID: 1, URL MSG: Test dns.query option, SFT_FID: 1
@H2 tcpdump:
04:27:41.284151 IP 192.168.100.2.53 > 8.8.8.8.53: 0+ A? google.com. (28)However, when I send the same DNS packet over and over again, I do not see any alert anymore, neither I see any packet received @H2 via tcpdump.
It’s like after the alert it is added to a banlist.I tried waiting for sometime to make any cached entry expire, but no results.
I can only achieve a new alert and packet received via tcpdump @H2, if I craft another DNS request, e.g., by changing the dst IP, or by restarting URL filter and reloading the database.As mentioned in the beginning, I am not a suricata expert, so this question might relate to suricata behavior not to the Bluefield’s directly.
Can someone enlighten me about this?You can see my terminals below. All of them has the “location” in their title/header.

image1919×1030 160 KB
ThanksHi cslev,
Thanks for trying out some of our reference applications.
The ‘print on match’ feature is only intended for the first time a flow is recognized, this means that traffic originating from the same FID is no longer inspected and is now filtered, therefore no tcpdump is seen.
You can try changing the FID’s values, or more precisely one of the 5-tuple which make this flow unique. If a different port is used, you should see the print again.I tried to add the same rule as you but with unsuccessful resultsalert dns any any -> any 53 (msg:""Test dns.query option""; dns.query; content:""google""; nocase; sid:1;)Powered by Discourse, best viewed with JavaScript enabled"
649,hello-everyone-i-have-one-ethernet-controller-0200-mellanox-technologies-mt27520-family-connectx-3-pro-15b3-1007-i-am-unable-to-bring-the-interface-up-i-have-four-other-machines-and-all-are-working-and-connecting-the-the-same-switch,"when I try to ifup eth4 I’ll get over 9k times Link Up/Link Down and finally stop.[4232442.841425] mlx4_en: eth4: Link Down[4232442.896407] mlx4_en: eth4: Link Up[4232450.013087] mlx4_en: eth4: Link Down[4232450.068127] mlx4_en: eth4: Link Up[4232454.701111] mlx4_en: eth4: Link Down[4232454.756094] mlx4_en: eth4: Link Upd8:00.0 Ethernet controller [0200]: Mellanox Technologies MT27520 Family [ConnectX-3 Pro] [15b3:1007I wonder if there is anything I can do to get more information on what may be wrong with the card of the Optics.Hello Suarezm,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provide, you can do the following to eliminate if it is the cable, switchport or adapter.If the issue stays with the adapter, we recommend to open a support ticket to assist you further, if there is a valid support contract on the adapter.You can open a support ticket, by sending an email to the following address → networking-support@nvidia.comThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
650,release-notes-for-nvidia-bright-cluster-manager-9-2-6a,"== cm-openssl ==
=New Features=Powered by Discourse, best viewed with JavaScript enabled"
651,rivermax-sdk-example-code-run-failed,"Hi expertI am using rivermax sdk example codes, but get  cuMemSetAccess failedCODE: media_sender.exe(Release-CUDA)
GPU:  NVIDIA RTX A4000
CUDA: 12.1
Driver: 531.14
wiindows: 10 rpo 21H2RUN CMD:   .\media_sender.exe -c 1 -a 2 -s .\sdps_samples\sdp_2110-20_narrow_gap_1080p50fps.txt -g 0 -r --max_gpu_freq/logs as follows*****************/##########################################################################################
Set env variable CUDA_DEVICE_ORDER=PCI_BUS_ID
gpu_device_id = 0
Writing log to default location: C:\Users\Administrator\AppData\Local\Temp\rivermax_0605_095514_5904.log
Created log file: C:\Users\Administrator\AppData\Local\Temp\rivermax_0605_095514_5904.log
[23-06-05 09:55:14.856135] Tid: 003972 info [InitLogger:92] Logger started
[23-06-05 09:55:14.856164] Tid: 003972 info [rmax_init:610] starting Rivermax: SDK version 1.30.16
[23-06-05 09:55:14.857025] Tid: 003972 debug [Clock:31]
[23-06-05 09:55:14.857044] Tid: 003972 debug [SysClock:41]
[23-06-05 09:55:14.857062] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_DISABLE_VIDEO_GROUPING to the value true
[23-06-05 09:55:14.857085] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_VIDEO_PACE_INTERVAL to the value 1000000
[23-06-05 09:55:14.857287] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_OUT_STREAM_SIZE_IN_PKTS to the value 32768
[23-06-05 09:55:14.857309] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_HEADER_STRIDE_SIZE to the value 64
[23-06-05 09:55:14.857329] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_DISABLE_FLOW_ID to the value false
[23-06-05 09:55:14.857348] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_SDP_PARSER_ENABLE_LOGGING to the value true
[23-06-05 09:55:14.857368] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_ENABLE_PTP_HW_RT_CLOCK to the value false
[23-06-05 09:55:14.857503] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_ENABLE_CUDA to the value true
[23-06-05 09:55:14.857526] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_ENABLE_STATISTICS to the value false
[23-06-05 09:55:14.857547] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_ENABLE_API_VERIFICATION to the value false
[23-06-05 09:55:14.857567] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_DISABLE_AUDIO_BUFFERING to the value false
[23-06-05 09:55:14.857785] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_SESSION_MAP_SIZE to the value 2000
[23-06-05 09:55:14.857819] Tid: 003972 debug [rivermax_get_user_env:151] parsed env RIVERMAX_SESSION_MAP_SIZE to the value 2000
[23-06-05 09:55:14.858010] Tid: 003972 debug [EventHandlerManager:125]
[23-06-05 09:55:14.858224] Tid: 003972 info [EventHandlerManager:132] will wakeup before frame begin event in 2000000 ns
[23-06-05 09:55:14.858244] Tid: 003972 debug [EventHandlerManagerHigh:259]
[23-06-05 09:55:14.858262] Tid: 003972 debug [start_thread:341] Starting internal thread
[23-06-05 09:55:14.858301] Tid: 003972 debug [rivermax_set_thread_affinity:719] successfully set thread affinity using cpu mask: 0x2, previous mask: 0xff
[23-06-05 09:55:14.858329] Tid: 003972 debug [start_thread:344] Started event handler thread
[23-06-05 09:55:14.858355] Tid: 003972 debug [init_globals:249] Time now is 1685930114858355500
[23-06-05 09:55:14.858947] Tid: 001968 info [print_thread_info:117] High priority internal thread: PID = 5904, thread ID = 1968
[23-06-05 09:55:14.859747] Tid: 003972 debug [load_provider:69] dpcp[0] = 6008005000000 ‘Mellanox ConnectX-6 Dx Adapter’
[23-06-05 09:55:14.859959] Tid: 003972 debug [load_provider:69] dpcp[1] = 6008006000000 ‘Mellanox ConnectX-6 Dx Adapter #2’
[23-06-05 09:55:14.860177] Tid: 003972 info [init:37] DPCP/DevX provider was loaded
[23-06-05 09:55:14.919350] Tid: 003972 debug [getAdapterInfo:473] Adapter 以太网 2 vlanId 0 len 6 MAC 04:3f:72:a4:99:94
[23-06-05 09:55:14.925612] Tid: 003972 debug [getAdapterInfo:511]   LUID 6008005000000 0x15b3/0x101d dpcp_adapter 0x1d173dc5cf0 opened true ret 0
[23-06-05 09:55:14.925961] Tid: 003972 debug [getAdapterInfo:528]   IP: 192.168.5.114 VLAN_ID: 0 Serial number: MT2035X03236
[23-06-05 09:55:14.926026] Tid: 003972 debug [getAdapterInfo:530]   MTU: 1500 TXlinkSpeed: 100 Gbps RXLinkSpeed:100 Gbps
[23-06-05 09:55:14.926326] Tid: 003972 info [getAdapterInfo:535] Device with IP addr: 192.168.5.114 was added to Device Collection [1]
[23-06-05 09:55:14.926563] Tid: 003972 warning [getAdapterInfo:430] Adapter 以太网 3 luidIdx 0x8006 is not Up
[23-06-05 09:55:14.929496] Tid: 003972 debug [getAdapterInfo:473] Adapter 以太网 3 vlanId 0 len 6 MAC 04:3f:72:a4:99:95
[23-06-05 09:55:14.933520] Tid: 003972 debug [getAdapterInfo:511]   LUID 6008006000000 0x15b3/0x101d dpcp_adapter 0x1d173e121e0 opened true ret 0
[23-06-05 09:55:14.934033] Tid: 003972 debug [getAdapterInfo:528]   IP: 169.254.42.137 VLAN_ID: 0 Serial number: MT2035X03236
[23-06-05 09:55:14.934059] Tid: 003972 debug [getAdapterInfo:530]   MTU: 1500 TXlinkSpeed: 18446744073 Gbps RXLinkSpeed:18446744073 Gbps
[23-06-05 09:55:14.934267] Tid: 003972 info [getAdapterInfo:535] Device with IP addr: 169.254.42.137 was added to Device Collection [2]
[23-06-05 09:55:14.936009] Tid: 003972 debug [getAdapterInfo:473] Adapter 以太网 vlanId 0 len 6 MAC 04:d4:c4:06:38:25
[23-06-05 09:55:14.936036] Tid: 003972 debug [getAdapterInfo:515] DPCP device with LUID 6008001000000 not found!
[23-06-05 09:55:14.936236] Tid: 003972 info [~winDevice:97] ~winDevice DTOR
[23-06-05 09:55:14.938938] Tid: 003972 debug [getAdapterInfo:473] Adapter Loopback Pseudo-Interface 1 vlanId 0 len 0 MAC 00:00:00:00:00:00
[23-06-05 09:55:14.943206] Tid: 003972 debug [GetPhysicalAdapterByMAC:358] No physical device found, bypassing
[23-06-05 09:55:14.943267] Tid: 003972 debug [getAdapterInfo:486] Physical adapter GUID wasn’t found, bypassing
[23-06-05 09:55:14.943536] Tid: 003972 info [~winDevice:97] ~winDevice DTOR
[23-06-05 09:55:14.969048] Tid: 003972 info [license_validate_v4:446] Licensed to: Beijing Enlightv Co., Ltd (N/A), evaluation period expires in 25 days
[23-06-05 09:55:14.969122] Tid: 003972 info [info_product:466] Rivermax license version: 4.1
[23-06-05 09:55:14.969834] Tid: 003972 info [license_validate:516] Rivermax license id 827d7712-80a4-1938-6474-902c070f7f24, revision 1
[23-06-05 09:55:14.970095] Tid: 003972 info [rmax_init:638] Statistics disabled
[23-06-05 09:55:14.970294] Tid: 003972 info [cuda_enable_etbl:362] Starting Cuda init
[23-06-05 09:55:14.970509] Tid: 003972 info [cuda_enable_etbl:396] Cuda init Done
List of supported devices:
Device with interface name: 以太网 3, IP addresses: [ 169.254.42.137 ], MAC address: 04:3f:72:a4:99:95, device_id: 4125, serial number: MT2035X03236
Device with interface name: 以太网 2, IP addresses: [ 192.168.5.114 ], MAC address: 04:3f:72:a4:99:94, device_id: 4125, serial number: MT2035X03236
[23-06-05 09:55:14.972290] Tid: 003972 debug [Clock:31]
[23-06-05 09:55:14.972649] Tid: 003972 debug [ExternalClock:66]
[23-06-05 09:55:14.972669] Tid: 003972 debug [~SysClock:46]
[23-06-05 09:55:14.972683] Tid: 003972 debug [~Clock:36]
[23-06-05 09:55:14.972871] Tid: 003972 debug [rmx_use_user_clock_v1:324] Using user time handler
TX Thread: 0 Mask: 0x4
CUDA memory allocation on GPU - cuMemCreate
RDMA is supported and enabled, status
cuMemSetAccess failed status = 999
CUDA cudaFreeMmap 304600000
CUDA memory free finished with status 1
Failed to allocate GPU memory.
Failed to allocate memory on GPU id 0
Failed to allocate memory with size: 62914560
thread 0 initialization failed
[23-06-05 09:55:15.060593] Tid: 003972 info [rmax_cleanup:689] Cleanup called
[23-06-05 09:55:15.060819] Tid: 003972 debug [~EventHandlerManagerHigh:265]
[23-06-05 09:55:15.061004] Tid: 003972 debug [~EventHandlerManager:139]
[23-06-05 09:55:15.061203] Tid: 003972 debug [free_evh_resources:148]
[23-06-05 09:55:15.061479] Tid: 003972 debug [stop_thread:163] event handler thread stopped
[23-06-05 09:55:15.061606] Tid: 003972 debug [free_evh_resources:152] Thread stopped
[23-06-05 09:55:15.061908] Tid: 003972 debug [~ExternalClock:71]
[23-06-05 09:55:15.062005] Tid: 003972 debug [~Clock:36]
[23-06-05 09:55:15.062205] Tid: 003972 debug [~DeviceCollection:28] ~DeviceCollection()
[23-06-05 09:55:15.062406] Tid: 003972 info [~winDevice:97] ~winDevice DTOR
[23-06-05 09:55:15.062918] Tid: 003972 info [~winDevice:97] ~winDevice DTOR
[23-06-05 09:55:15.063158] Tid: 003972 info [~RiverLogger:105] logger closingDear @wanglxOne of the possible cause might be a low BAR size. You can try to implement the following steps:https://developer.nvidia.com/nvidia-display-mode-selector-tool-home
https://apps.nvidia.com/pid/contentlibraries/detail?id=1066046Read the “NVIDIA Display Mode Selector Tool User Guide” for exact commands to increase the BAR size to 8G https://developer.nvidia.com/sites/default/files/akamai/NVIDIA_Display_Mode_Selector_Tool_User_Guide.pdf.\displaymodeselector.exe --gpumode
A warning message appears. Press the Y key to continue.
Select mode 2 (physical_display_enabled_8GB_bar1)If it doesn’t work, I advise you to open a new case in Enterprise Support for detailed inspection.Good luck,
VladislavDear vkhomyakovThanks for your reply!I got follows logs from  displaymodeselector.exe, it means RTX A4000 is not supported?Are you sure you want to continue?
Press ‘y’ to confirm (any other key to abort):
y
Select a number:
<0> physical_display_enabled_256MB_bar1
<1> physical_display_disabled
<2> physical_display_enabled_8GB_bar1Select a number (ESC to quit):
2
Specifed GPU Mode “physical_display_enabled_8GB_bar1”Update GPU Mode of all adapters to “physical_display_enabled_8GB_bar1”?
Press ‘y’ to confirm or ‘n’ to choose adapters or any other key to abort:
yUpdating GPU Mode of all eligible adapters to “physical_display_enabled_8GB_bar1”NVIDIA RTX A4000     (10DE,24B0,10DE,14AD) S:00,B:01,D:00,F:00Specified GPU mode not supported on this device 0x24B0.Dear @wanglxYes, I’ve double checked Display Mode Selector Tool docs and confirm the following:
The Display Mode Selector tool is a special tool for NVIDIA L40, NVIDIA RTX 6000 Ada, NVIDIA A40, NVIDIA RTX A5000, NVIDIA RTX A5500, and NVIDIA RTX A6000 only.  It should not be used with any other GPU.But has Windows GPUDirect supported on RTX A4000.
Could you please paste here nvidia-smi command output?Would you mind to open the case in Enterprise Support for further debugging with Rivermax Application Engineering?Regards,
VladislavDear vkhomyakovThe issue has been resolved.I have modified BIOS to support resizable BAR, it work fine for meThanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
652,monitoring-app-issues-writes-after-get-log-failure,"Hello, I would much appreciate you helping me to solve a mystery.I have a nvme device that does not support get smart log command.The app Issues “smartctl -H” command, and this command sends get log command to the device, which returns with an error since it is not supporting it.
Apparently I noticed that after a few get log commands that fail to the device, a write command is being sent to it to offset 0, size 4096 bytes.I would like to know if this behavior is defined by nvme spec or if it is unexpected (a bug in the app).Many thanks in advance,Matan.Powered by Discourse, best viewed with JavaScript enabled"
653,dpdk-pktgen-and-debian-11-with-connectx5-nic,"Hi. Trying to get Pktgen + DPDK to work with Connect X5 NIC on Debian 11.Pktgen fails to start and I suspect it’s because the Mellanox EN driver’s DPDK related parts do not support Debian 11 yet.root@lab-pc mlnx-en-5.4-1.0.3.0-debian10.8-x86_64 # ./install --upstream-libs --dpdkdpkg-query: no packages found matching nvidia-l4t-kernel-headersError: The current mlnx-en is intended for debian10.8Any advice or howto I can follow?Should I try using the DPKD built-in ConnectX5 support rather?Is there a way to build the Mellanox DPDK drivers for a later Debian version?Hello Joe,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, unfortunate at the moment there is no Debian 11 support for our MLNX_EN/OFED driver.You can use the rdma-core and the driver which comes with the OS, as long as it is recent. See the following link for the prerequisites → 36. MLX5 Ethernet Poll Mode Driver — Data Plane Development Kit 22.07.0 documentationThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
654,sn2100m-virtuozzo-virtuozzo-storage-sds,"Hello,we are currently in the process of planning a new Virtuozzo (+ Virtuozzo Storage SDS) Cluster. The complete storage traffic should run over 2 redundant SN2100M. (2 x Switch → each 100Gb → 4x25Gb breakout cable - to 4 servers each with 2 x HPE Ethernet 10 / 25Gb 2-port FLR-SFP28 NIC)Total 6-8 server nodes are currently being considered. Now the consideration (since the budget is unfortunately quite limited) is whether, in addition to the storage traffic, if the normal cluster traffic could also run via the SN2100M. In theory, only LAG / VLANs are required, no other special layer 2 or 3 features. In this case, we would have connected the servers with 25Gb too instead of operating them via dedicated 10Gb switches. Sure - is not the “cleanest” solution - but probably reasonable from a cost / benefit point of view, right?Am I missing something here that wouldn’t work that way? Or could the cluster be set up like this? (Never worked with SN2100M before - and our HPE Distributor said it’s not possible to use them for “normal” Networking … but after reading the Docs I don’t understand WHY it wouldn’t be possible …?)Thank you, bye from AustriaAndreas S.Hello Andreas,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we cannot go into the full details of the design, but there is no reason that our switches cannot be used for normal network operations. Our Spectrum switches our capable for all L2 and L3 functionalities available including OSPF, BGP and VXLAN.Based on your own design and requirements you can decide to segregate the traffic (storage and mgmt) by the use of VLANs or not.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
655,cant-install-mlnx-ofed-kernel-dkms-mlnx-en-5-6-1-0-3-5-debian11-2-x86-64-in-proxmox-7-1,"Unpacking mlnx-ofed-kernel-dkms (5.6-OFED.5.6.1.0.3.1) over (5.6-OFED.5.6.1.0.3.1) …
Setting up mlnx-ofed-kernel-dkms (5.6-OFED.5.6.1.0.3.1) …
Loading new mlnx-ofed-kernel-5.6 DKMS files…
First Installation: checking all kernels…
Building only for 5.13.19-2-pve
Building for architecture x86_64
Building initial module for 5.13.19-2-pve
Error! Bad return status for module build on kernel: 5.13.19-2-pve (x86_64)
Consult /var/lib/dkms/mlnx-ofed-kernel/5.6/build/make.log for more information.
dpkg: error processing package mlnx-ofed-kernel-dkms (–install):
installed mlnx-ofed-kernel-dkms package post-installation script subprocess returned error exit status 10
Errors were encountered while processing:
mlnx-ofed-kernel-dkms
make.log (69.9 KB)Hello buaapyj,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Unfortunately we do not provide support nor guidance for this OS.For the supported OS-list, please review the latest RN of the driver → https://docs.nvidia.com/networking/display/MLNXOFEDv561033/General+SupportThank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
656,how-to-set-traffic-class-using-ibverbs,"I have CX3 and CX5 adapters. And the same code of application using ibverbs api. When use CX3, field of TC in GRH has value 0x00. But when I use CX5 GRH TC has value 0x20. How can I control it. RDMA_CM set option I can not use, because use ibverbs api. May be via mlxconfig? Any help would be useful. Thanks.​​P.S. Any changes via linux utils in MLNX_OFED did nothing for me.​Hello Ivan,ibv_create_ah() holds this info and is used when calling ibv_post_send()Please refer tohttps://www.mellanox.com/related-docs/prod_software/RDMA_Aware_Programming_user_manual.pdfSection 3.4.15 ibv_create_ahibv_create_ah creates an AH. An AH contains all of the necessary data to reach a remote destination.In connected transport modes (RC, UC) the AH is associated with a queue pair (QP). Inthe datagram transport modes (UD), the AH is associated with a work request (WR).struct ibv_ah_attr is defined as follows:struct ibv_ah_attr{struct ibv_global_route grh;uint16_t dlid;uint8_t sl;uint8_t src_path_bits;uint8_t static_rate;uint8_t is_global;uint8_t port_num;};In the ibv_global_route struct the user should pass the TClassstruct ibv_global_route is defined as follows:struct ibv_global_route{union ibv_gid dgid;uint32_t flow_label;uint8_t sgid_index;uint8_t hop_limit;uint8_t traffic_class;};Best Regards,AnatolyPowered by Discourse, best viewed with JavaScript enabled"
657,ethernet-controller-mellanox-technologies-device-1019-not-working-with-kernel-version-4-19-97-00189-g5dd402a-it-was-working-fine-with-kernel-3-14-25,"Hello,We are using Mellanox NIC and it was working fine with the DPDK 20.08 and kernel version 3.14.25.Now, we have upgrade the our linux kernel version to 4.19.97 and but fail during probe of the PCI device from the DPDK. DPDK version is 20.08 only.This is the error we are getting with kernel 4.19.97.EAL: Detected 32 lcore(s)EAL: Detected 1 NUMA nodesMulti-process socket /tmp/dpdk/pio-0/mp_socketEAL: Selected IOVA mode ‘PA’EAL: No available hugepages reported in hugepages-2048kBEAL: Probing VFIO support…Invalid NUMA socket, default to 0Probe PCI driver: mlx5_pci (15b3:1019) device: 0000:00:07.0 (socket 0)Requested device 0000:00:07.0 cannot be usedBus (pci) probe failed.Detected 32 lcore(s)Detected 1 NUMA nodesMulti-process socket /tmp/dpdk/pio-0/mp_socketSelected IOVA mode ‘PA’No available hugepages reported in hugepages-2048kBProbing VFIO support…Please let me know if you need any more information.BRDhrupalThe issue is under debug and being handled via support case.Thanks,ChenPowered by Discourse, best viewed with JavaScript enabled"
658,mellanox-driver-reports-infiniband-mlx5-0-create-qp-pid-101966-create-qp-type-2-failed-message-at-higher-client-scale,"In our project we are launching numerous clients and servers over verbs;ofi_rxm using OFI/libfabrics.As we exceed certain number of clients, our attempts to connect to servers start failing with server dmesg containing errors of type:[507263.354558] infiniband mlx5_0: create_qp:2947:(pid 101966): Create QP type 2 failedIs there a way to determine what type of resource is mlx5_0 running out of? Are there any settings we could teak or additional debug info we could retrieve to figure out the reason for this problem?OFI version: v1.12.0Provider used: verbs;ofi_rxmMOFED version: 5.1.2System: Frontera@TACCHi,See ibv_create_qp documentation and in which case it fails.If you are out of resources, means that you reach max_qp (ibv_devinfo -v). you can try to implement a mechanism to count the number of QPS you are creating and to don’t create a new one if you already reach the max.MarcThanks for the response.Is there a way to increase max_qp size? If not, what is this value based on?Powered by Discourse, best viewed with JavaScript enabled"
659,nv-shield-pro-2019-connection-issues,"Nvidia Shield Pro, can connect with adb but shell cmd reports no device emulators found. Also won’t comm with an ftp server. Environment - Linux/Android.Hi @wadebob713,Welcome to the NVIDIA Developer forums. This is the Mellanox Networking category. Nvidia Shield is supported in the Geforce forums.https://www.nvidia.com/en-us/geforce/forums/shield-tv/9/Best,
TomThanks for the information. Fixed the problems anyway, but good to know for the future.Powered by Discourse, best viewed with JavaScript enabled"
660,i-cant-install-mlnx-ofed-linux-5-2-2-2-0-0-4-15-0-139-generic-and-mlnx-ofed-linux-5-2-2-2-0-0-4-15-0-137-generic,"OS：Ubuntu 18.04.5Kernel version：4.15.0-139-genericgcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)HCA：MT27800 Family [ConnectX-5]--------------------------------------------------------------------------------./mlnxofedinstall ，the source of the error is as follows：I think it was caused by -Werror=implicit-function-declaration.
QQ图片202103191044011298×406 22.1 KB
./mlnxofedinstall --force --add-kernel-support , Same as the above error.Everything works fine on 4.15.0-136-generic.mlnx-ofed-kernel-dkms.make.log (503 KB)Hello Zhe,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we can inform you that this issue is resolved in the upcoming version of MLNX_OFED 5.3 GA which is targeted for the end of the month.Thank you and regards,~NVIDIA Networking Technical SupportWill the same MOFED 5.3 address Ubuntu 20.04? I am having the same basic problem as Zhe Zhao while trying to install MOFED 5.2-2.2.0.0 on Ubuntu 20.04.02 with kernel 5.4.0-67. Several hundred errors about variable declarations. I don’t recall having problems when I installed this same version of MOFED on Ubuntu 20.04.02 with kernel 5.4.0-65 last month.Will the same MOFED 5.3 fix apply to Ubuntu 20.10?I have the same issues as others in this thread. The DKMS compilation fails with compilation errors; it seems like the build system was changed to treat warnings as errors, but the code was not updated to remove the warnings.As Mark points out, this problem has cropped up within the past few weeks. I’ve recently installed MOFED 5.2.1.0.4 and there were no issues during the installation several weeks ago.Can a workaround be provided in the meantime to address this issue?If not, can someone describe where the compiler flags are last set in the build system so that I can circumvent the problem myself. The build scripts are pretty complex.Powered by Discourse, best viewed with JavaScript enabled"
661,hpc-x-for-mlnx-ofed-4-9-lts,"Hello!I’m Using ConnectX-3 (MCX353A-FCCT) on Ubuntu 20.04 with mlnx_ofed 4.9 LTS and I would like to use HPC-X,
but there doesn’t seem to be a version especially for 4.9 LTS. Is there any recommend version for this setup?If not, would be upgrading to OFED-5.0-2.1.8 (last ofed version with CX3 support) and using the latest HPC-X package with my ConnectX-3 card a good idea?Any help would be appreciated!Powered by Discourse, best viewed with JavaScript enabled"
662,got-local-protection-error-when-doing-ibv-wr-send,"I’m trying to use RDMA to send data to or from a FPGA PCIe card.
The test platform is an Intel CPU PC with a cx4121a card and a FPGA PCIe card. OS is Ubuntu 22.04 x86_64, with MLNX_OFED_LINUX-5.8-2.0.3.0.
I wrote a kernel driver for the FPGA card to implement a peer_memory_client, which heavily referenced nv_peer_memory.
It worked fine on IBV_WR_RECV, cx4121a could successfully write the data it received to FPGA card’s BAR2 address (which was mapped to DDR on FPGA card). But when doing IBV_WR_SEND (using cx4121a to send data from FPGA card’s BAR2 address to remote), I got a local protection error. And there was no warning or error message in kernel log.
It was the same buffer address/length and same memory region used in both IBV_WR_SEND and IBV_WR_RECV, so I have no clue why IBV_WR_RECV worked but IBV_WR_SEND not.Any help would be appreciated, thanks!Here are some details:If I replaced FPGA card’s BAR2 with a kmalloc buffer in driver, every thing worked fine. So I assume the driver in general is good, just something is wrong with PCIe BAR address mapping in driver?SEND is different with regular RDMA read/write, it need mem copy to HCA buffer. If you are developing driver, better way is wotk with kernel community and get IB HCA PRM (program manual) from NIVDIA sales.Thanks for the infomation!I’ve tested four APIs:1 and 3 successed, 2 and 4 failed with local protection error. So it seems HCA can write to FPGA card’ PCIe BAR address, but cann’t read from it… Any thoughts?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
663,mlx5-ib-and-mlx5-core-cannot-load-in-mlnx-ofed-5-9-kernel-5-15,"Hi,I successfully installed mlnx_ofed 5.9 (even tried with 5.8 and 5.4). But when I try to run sudo /etc/init.d/openibd restart (or force-restart), I get the following error:Unloading HCA driver:                                      [  OK  ]
Loading Mellanox MLX5_IB HCA driver:                       [FAILED]
Loading Mellanox MLX5 HCA driver:                          [FAILED]
Loading HCA driver and Access Layer:                       [FAILED]Please run /usr/sbin/sysinfo-snapshot.py to collect the debug information
and open an issue in the http://support.mellanox.com/SupportWeb/service_center/SelfServiceNotes: my system: ubuntu 22.04, kernel 5.15.60 generic, NIC: Mellanox Connectx-4.Thanks a lot in advance.Hi nnaza008,Thank you for contacting Nvidia Support. When compiling MLNX OFED drivers for a different kernel version (non-default), please make sure to compile the drivers with the --add-kernel-support flag.The default kernel versions with which the drivers have been compiled and tested are listed in the Release notes - https://docs.nvidia.com/networking/display/MLNXOFEDv590560/General+SupportFor the list of installation options, please run: ./mlnxofedinstall --helpMake sure that the filename used is located under the “extra” directory as above.Thanks,
Nvidia SupportI had the same problem with ofed 5.9 on ubuntu 22.04.
Turns out that ubuntu secure boot was preventing the kernel module from loading.
Followed this link to disable secure boot. Now it works.Powered by Discourse, best viewed with JavaScript enabled"
664,sn2010-basic-qdisc-with-hw-offload,"HiRunning Kernel 6.1 + SwitchDev on SN2010.I’m wondering that’s the best basic qdisc config. It seems the default is pfifo_fast, but reading the wiki, it seems this is not hardware offloaded.I’m wondering, what’s the best / basic / simple config for qdisc, which is offloaded?Not planning to do any interesting QoS stuff, just basic routing.Looked at the docs again, best alternative seems to be to use PRIOtc qdisc replace dev swp1 root handle 1: prio bands 3 priomap 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Or, could just do pfifo as root qdisttc qdisc replace dev swp7 root handle 1: pfifoPowered by Discourse, best viewed with JavaScript enabled"
665,vl15-dropped-portrcvswitchrelayeroors,"HelloI have a HDR unmanged switch and some devices on and facing some errors i dont really know to interpret or solve.
Switch: MQM8790-HS2F
Cards in Device: ConnectX-6I installed everything fresh and still face the problems, servers are running ubuntu 22.04LTS, OFED 5.8-2.0.3.0-LTS.
IPoIB just configured like this for example:Now when i do ibqueryerrors i get following output, with the counter growing in big intervalls every second, i just cleared the errors and counters before cause it even wen to overflow.also rarly this error comes up for every port where a device is connected. Just werent in the first output cause of reseting. Its allways the GUID.Opensm is running on one of the servers without giving me any errors.Someone has an idea?Cheers
KilianHi,Port 41 is a logical port used in case SHARP is enabled on the fabric (unless you have splitted ports in which case it will be port 81)If you are not using SHARP – it may be necessary to review the SM configuration and turn it off.In the SM conf set:sharp_enabled = 1As for the errors and their meaning:VL15 dropped – those errors indicate SMPs are being dropped for some reason on the relevant ports. VL15 isn’t subjected to flow control – and isn’t expected to show this behavior. Though it may well be this relates to some misconfig in the SM (as mentioned above)PortRcvSwitchRelayErrors – those errors indicate a port received a packet with some DLID which isn’t registered in the switch forwarding tables – thus it doesn’t know what egress port to use for those packets. Numbers seem to be correlated on the ports – which means some entity tried sending a packet to some LID that doesn’t exist in the fabric.I have everything on default, basically just put the cables into the switch and servers, installed ubuntu 22.04 and ofed driver, configured netplan as mentioned, did just systemctl start opensm, then just found this error when checkig ib status.So i did no configuration on the sm.Hi,A more accurate fabric analysis would require a support case in order to be able to share more data.  If necessary, please open a case with Nvidia Technical Support.Thank youPowered by Discourse, best viewed with JavaScript enabled"
666,using-simple-forward-vnf-but-gtp-support-is-not-enabled,"I tried to use the Simple Forward VNF application but some errors came out below shows that GTP support is not enabled.I don’t know if the application has been successfully launched。root@localhost:/home/ubuntu# /opt/mellanox/doca/applications/simple_fwd_vnf/bin/doca_simple_fwd_vnf -a auxiliary:mlx5_core.sf.3 -a auxiliary:mlx5_core.sf.2 – -l 30
EAL: Detected 8 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL: No legacy callbacks, legacy socket not created
Temporary WARN - Destination table level lower than Source
[09:09:19:244862][DOCA][ERR][dpdk_flow_sws:114]: failed creating flow on port 0 - creation error type 13 message: GTP support is not enabled
[09:09:19:279626][DOCA][ERR][dpdk_flow_sws:114]: failed creating flow on port 1 - creation error type 13 message: GTP support is not enabledI already set FLEX profile number to 3 bysudo mlxconfig -d 03:00.0 s FLEX_PARSER_PROFILE_ENABLE=3
sudo mlxconfig -d 03:00.1 s FLEX_PARSER_PROFILE_ENABLE=3and encap mode is none;
Is there anyone who can help me?Likely you run fwd app on default sf.2/3, You can try create SF 4.5Note: The flag -a auxiliary:mlx5_core.sf.4 -a auxiliary:mlx5_core.sf.5 is mandatory for proper usage of the application. Modifying this flag results unexpected behavior as only 2 ports are supported. The SF number is arbitrary and configurable.Note: SFs must be enabled according to Scalable Function Setup Guide.On the other hand, have you power cycle server to reset firmware after set flex profile?Likely, flex profile 3 only support GRE/VXLAN not for GTP.Thank you very much!
it seems that i forgot power cycle the server and after reboot the server there’s no error came outroot@localhost:/home/ubuntu# /opt/mellanox/doca/applications/simple_fwd_vnf/bin/doca_simple_fwd_vnf -a auxiliary:mlx5_core.sf.2 -a auxiliary:mlx5_core.sf.3 – -l 50
EAL: Detected 8 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL: No legacy callbacks, legacy socket not created
[02:32:08:613470][DOCA][INF][engine_model:73]: engine model defined with mode=vnf
[02:32:08:613598][DOCA][INF][engine_model:75]: engine model defined with nr_pipe_queues=7
[02:32:08:613674][DOCA][INF][engine_model:76]: engine model defined with pipe_queue_depth=0
[02:32:08:613933][DOCA][INF][engine_field_mapping:96]: Engine field mapping initialized with 3 focus 12 protocols
[02:32:08:614011][DOCA][INF][engine_shared_resources:94]: Engine shared resources initialized successfully
[02:32:08:614077][DOCA][INF][dpdk_engine:437]: queue depth is zero, set it to default 128.
[02:32:08:614152][DOCA][INF][encap_table:119]: encap table created
[02:32:08:614231][DOCA][INF][dpdk_table:70]: Initializing dpdk table successfully
[02:32:08:614282][DOCA][INF][dpdk_flow:82]: Initializing dpdk flow successfully
[02:32:08:614323][DOCA][INF][engine_shared_resources:133]: Allocated 16 shared resources of type 2
[02:32:08:614363][DOCA][INF][dpdk_resource_manager:184]: Dpdk resource manager register completed
[02:32:08:614549][DOCA][INF][dpdk_layer:260]: Dpdk layer register completed
[02:32:08:614758][DOCA][INF][doca_flow_layer:466]: Doca flow layer initialized
[02:32:08:614814][DOCA][INF][doca_flow:526]: Doca flow initialized successfully
[02:32:08:614998][DOCA][INF][utils_hash_table:123]: hash table grp_fwd port 0 created
[02:32:08:615069][DOCA][INF][dpdk_port:167]: Dpdk port 0 initialized successfully with 8 queues
[02:32:08:812098][DOCA][INF][id_pool:68]: Initialized ID Pool meter_id_p0 with address 0xaaaadb112680 of size 8192, min index 1
[02:32:08:817933][DOCA][INF][id_pool:68]: Initialized ID Pool policy_id_p0 with address 0xaaaadb112f80 of size 8192, min index 1
[02:32:08:819391][DOCA][INF][id_pool:68]: Initialized ID Pool tm_1 with address 0xaaaadb242420 of size 1, min index 0
[02:32:08:826548][DOCA][INF][utils_hash_table:123]: hash table grp_fwd port 1 created
[02:32:08:826685][DOCA][INF][dpdk_port:167]: Dpdk port 1 initialized successfully with 8 queues
[02:32:09:023462][DOCA][INF][id_pool:68]: Initialized ID Pool meter_id_p1 with address 0xaaaadba1ff80 of size 8192, min index 1
[02:32:09:029329][DOCA][INF][id_pool:68]: Initialized ID Pool policy_id_p1 with address 0xaaaadba212b0 of size 8192, min index 1
[02:32:09:030865][DOCA][INF][id_pool:68]: Initialized ID Pool tm_2 with address 0xaaaadba28220 of size 1, min index 0
[02:32:09:039243][DOCA][INF][id_pool:68]: Initialized ID Pool tm_3 with address 0xaaaadba538a0 of size 1, min index 0
[02:32:09:045962][DOCA][INF][id_pool:68]: Initialized ID Pool tm_4 with address 0xaaaadbc7dd80 of size 1, min index 0
[02:32:09:053829][DOCA][INF][id_pool:68]: Initialized ID Pool tm_5 with address 0xaaaadbea8180 of size 1, min index 0
Temporary WARN - Destination table level lower than Source
[02:32:09:066990][DOCA][INF][id_pool:68]: Initialized ID Pool tm_6 with address 0xaaaadc0ccef0 of size 8192, min index 0
[02:32:09:075400][DOCA][INF][id_pool:68]: Initialized ID Pool tm_7 with address 0xaaaadc139bd0 of size 1, min index 0
[02:32:09:083098][DOCA][INF][id_pool:68]: Initialized ID Pool tm_8 with address 0xaaaadc36af00 of size 1, min index 0
[02:32:09:089925][DOCA][INF][id_pool:68]: Initialized ID Pool tm_9 with address 0xaaaadc5953a0 of size 1, min index 0
[02:32:09:103176][DOCA][INF][id_pool:68]: Initialized ID Pool tm_10 with address 0xaaaadc7ba030 of size 8192, min index 0Thanks again and I am wondering if it is working correctlyThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
667,storm-control-with-switchdev,"HiUsing SN2010 with debian and switchdev, do you have any advices for detecting and preventing loops and broadcast storms?The cumulus / switchd docs have a ‘storm control’ section, but I don’t see anything on the switchdev related wiki.Use case - planning on setting up some servers with bonds in active-backup mode (to more than one switch), and for paranoia reasons would like to see if the switches can disable or limit ports of there is a loop or broadcast storm.Hi,please refer to:Contribute to Mellanox/mlxsw development by creating an account on GitHub.andContribute to Mellanox/mlxsw development by creating an account on GitHub.Powered by Discourse, best viewed with JavaScript enabled"
668,vxlan-woes,"I’ve been having some major problems with VXLANs on  switches with Broadcom Trident II+ ASICS and could use some assistance.  I’m deploying a bunch of new hypervisors and want to be able to moves VMs from one to the other at will. Each hypervisor plugs into a pair of leaf switches operating as an MLAG pair.  Here’s an example of what that config might look like on one of my leaf switches.interface hv04
bond-slaves swp7
bridge-pvid 1200
bridge-vids 1718 1740
clag-id 7
mstpctl-bpduguard yes
mstpctl-portadminedge yesThen, my VLAN and VXLAN:interface vlan1740
address-virtual 00:00:5E:00:01:B0 10.1.1.1/28
vlan-id 1740
vlan-raw-device bridgeinterface vni1740
bridge-access 1740
bridge-learning off
mstpctl-bpduguard yes
mstpctl-portbpdufilter yes
vxlan-id 1740
vxlan-local-tunnelip 10.0.128.2 (switch’s loopback IP)I then advertise each of my VNIs via BGP:router bgp 4210000002
…
address-family ipv4 unicast
redistribute connectedaddress-family l2vpn evpn
neighbor FABRIC activate
advertise-all-vniRight off the bat, if I only have this configuration on a single leaf pair and deploy a VM on the hypervisor in VLAN 1740, I start to see some problems.  Theoretically, since I’m redistributing all of my connected routes via BGP, everything on my network should be able to get to VLAN 1740.  In fact, if I look at my spine switches, I see routes to 10.1.1.0/28 from this leaf pair.  In reality, my VM can only talk to very specific devices.  As it odd as this sounds, I find myself only being able to ping other VMs, either on this hypervisor or on others in other racks.  The VMs those VLANs belong to may or may not have a VXLAN attached to it.  If I try to ping other devices, I see the ICMP request hit them, but the reply gets lost before it hits my VM. Now, I imagine this doesn’t have anything to do with whether the device is a physical one or a VM, but, rather, whether traffic to/from it is VLAN tagged or not.NVIDIA has posted some information on some of the caveats of Broadcom Trident II+ switches here (Inter-subnet Routing | Cumulus Linux 4.1) and the workaround of basically turning every VLAN into a VXLAN sorta works.  When I tried this on a pair of leaf switches, I found VMs hanging off of other leaf switches, also in VXLANs, could ping all of the physical servers in the rack where I made the updates.  However, this seemed to have killed communication to the hypervisor in the rack where I made the update as well as the VMs on it. I rolled back my changes and suddenly had communication issues with most devices in the rack. I eventually solved those by ifdown’ing and ifup’ing every VLAN on the switches.  This didn’t fix the issue with the hypervisor, though, and I found that traffic leaving the hypervisor going to just one of the leaf switches was getting blackholed.  The only way to fix that issue was to delete the LACP bond to the hypervisor on the problematic switch and then re-creating it.  I actually had a similar issue at one point on another of my hypervisors and this also solved that issue.I’m really hesitant to move forward here.  Theoretically, I can turn each of my VLANs into a VXLAN on each of my switches and everything should work and I just might need to re-create the bond to my hypervisor. However, it worries me that I could suddenly have similar issues with just about any bonded devices on my network at some other time. My switches are on version 4.2 and I can only upgrade to 4.3 since they have Broadcom ASICs.  I’ve taken a look through the release notes, though, and I doubt the upgrade will make any difference.
I’m thinking about taking a pair of leaf switches and turning every VLAN into a VXLAN again.  I kind of hope this breaks my hypervisor so I can troubleshoot things and possibly come up with a solution. When things were broken, I ran some “net show bgp l2vpn evpn route” commands on my switches and the problematic switch was both sending and installing routes. What else could I look at?  What could I be missing here?Also, FWIW, I’ve tested my setup in GNS3 on version 4.2 and everything works great. It sure seems like all these issues come down to the fact that I’m using Broadcom switches.Powered by Discourse, best viewed with JavaScript enabled"
669,mlx5dv-devx-obj-create-fails,"I’m trying to create a completion queue using the DEVX API.I get errno 121:Remote I/O error, which the man page says to check the status and syndrome. Which I get:The status seems to correspond to “BAD_RESOURCE Attempt to access reserved or unallocated resource, or resource in inappropriate status. for example, not existing CQ when creating SQ/RQ”However I have been unable to find that syndrome.I only get these errors if I use MPI elsewhere in my application. Does anyone have any suggestion on the syndrome or how to interpret the status?Hello 12yht2,Thank you for posting your inquiry to the NVIDIA Developer Forums.It is likely that these details are not publicly available due to proprietary content.
Programming assistance is out of scope for Enterprise Support as well.For this type of query, we recommend reaching out to our Sales and Solutions team. They will be able to direct you to resources within NVIDIA who can assist you with this inquiry:https://www.nvidia.com/en-us/contact/sales/Thanks, and best regards,
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
670,ipl-both-switches-show-as-master,"FF-Core-A [FF-Core-1-Rack1: master] (config) # sh mlag-vip
MLAG-VIP:
MLAG group name: FF-Core-1-Rack1
MLAG VIP address: 10.255.12.2/23
Active nodes: 1FF-Core-A                                master               10.255.12.21FF-Core-A [FF-Core-1-Rack1: master] (config) # sh mlag
Admin status: Enabled
Operational status: Up
Reload-delay: 30 sec
Keepalive-interval: 1 sec
Upgrade-timeout: 60 min
System-mac: 02:00:00:00:00:01MLAG Ports Configuration Summary:
Configured: 1
Disabled:   0
Enabled:    1MLAG Ports Status Summary:
Inactive:       0
Active-partial: 0
Active-full:    11    Po4094        4000       Up           1.1.1.0                                  1.1.1.1                                  0 days, 00:15:24     390:0A:84:52:8E:C8   Up                           
90:0A:84:6D:21:08   Up                            -I can ping the IPl address of the remote and the reciprocal.mlag-vip FF-Core-1-Rack1 ip 10.255.12.2 /23 force
no mlag shutdown
mlag system-mac 02:00:00:00:00:01
interface port-channel 4094 ipl 1
interface vlan 4000 ipl 1 peer-address 1.1.1.1ANDmlag-vip FF-Core-1-Rack1 ip 10.255.12.2 /23 force
no mlag shutdown
mlag system-mac 02:00:00:00:00:01
interface port-channel 4094 ipl 1
interface vlan 4000 ipl 1 peer-address 1.1.1.0FF-Core-A [FF-Core-1-Rack1: master] (config) # sh interfaces port-channel 4094Po4094:
Admin state         : Enabled
Operational state   : Up
Description         : N/A
Mac address         : N/A
MTU                 : 9216 bytes (Maximum packet size 9238 bytes)
lacp-individual mode: Disabled
Flow-control        : receive off send off
Actual speed        : 2 X 100G
Width reduction mode: Not supported
Switchport mode     : trunk
MAC learning mode   : Enabled
Forwarding mode     : inherited cut-throughTelemetry sampling: Disabled   TCs: N/ACheck the system clock also.this should be part of a support case as extensive debug is needed.can you open such case?https://enterprise-support.nvidia.com/s/create-caseThanks. I factory reset and tried another run at it and it failed again even the the port-channel, L3 etc is all reporting properly and peer addressing is pingable.I have a support case open and will be looking at this Monday. Even the logs are showing the IPL transitioning properly from what I can tell.What is the case number?I have it with HPE Storage Networking.Powered by Discourse, best viewed with JavaScript enabled"
671,how-to-configure-a-redundant-ethernet-switch-setup-with-2-6036g-and-2-cisco-sg500x,"Hi,I have a simple two-switch setup with 2 Cisco SG500X switches (48*1G, (2+2)*10G), both are stacked together via a stacking cable. Every server is connected to each switch (LAG). In case of a switch failure, the LAG will use the remaining switch, so everything should keep going, the servers should still have network connectivity.Now I want to expand this scenario with 2* Mellanox 6036G switches: Basically I would connect each server via 40G ports to each switch, the same as for the Ciscos and do some kind of link aggregation.Furthermore I will have to connect the two switches with the Cisco devices. Basically I would use a breakout cable and connect each 6036G with both ciscos. Here I need to make sure that I don’t create a loop! So I can think of two variants:Variant 1: Create a LAG over 2 cables (1* 6036G → 2*SG500X) for each Mellanox switch. Traffic from one Mellanox to the other would then flow over the Ciscos. This has the drawback that the Mellanox switches are connected with 10G only, however, there should not be any loop.Variant 2: Stack the two 6036G together via MLAG and create a LAG over all connections ( 4 cables) between the 6036 “MLAG stack” and the Cisco stack.Attached to this Post you find a simple sketch depicting my setup.This opens the following questions:For Variant (2):Or is there perhaps an even better solution to my problem?Any help is appreciated!Best Regards,Hermann
sketch-ha-6036-sg500x.jpg1095×1128 101 KB
Hi HermannYou can referfollowing link for confirgure Gateway HA．https://community.mellanox.com/s/article/howto-configure-infiniband-gateway-ha--proxy-arp-x.Thanks,SuoHi Suo,Thank you for your reply, but I don’t get it: In which way is the linked article about a HA proxy arp (forwarding Infiniband/Ethernet via a 2*redundant switch setup) related to stacking two switches and connecting them to two other switches in an Ethernet-only setup?Best Regards,HermannPowered by Discourse, best viewed with JavaScript enabled"
672,what-switches-and-connectors-are-supported-in-cumulus-linux,"To see which switches and cables/connectors/pluggables that Cumulus Linux supports, check out our hardware compatibility list.Powered by Discourse, best viewed with JavaScript enabled"
673,mlx5-core-000000-0-poll-health-pid-0-devices-health-compromised,"Hi,
We see below error followed by watchdog soft lock up while running our tests[   11.376220] mlx5_core 0000:41:00.0: poll_health:853:(pid 0): device’s health compromised - reached miss count
[   11.376286] mlx5_core 0000:41:00.0: print_health_info:456:(pid 0): assert_var[0] 0x00004200
[   11.376321] mlx5_core 0000:41:00.0: print_health_info:456:(pid 0): assert_var[1] 0x0010ca5c
[   11.376359] mlx5_core 0000:41:00.0: print_health_info:456:(pid 0): assert_var[2] 0x00000000
[   11.376392] mlx5_core 0000:41:00.0: print_health_info:456:(pid 0): assert_var[3] 0x00000000
[   11.376424] mlx5_core 0000:41:00.0: print_health_info:456:(pid 0): assert_var[4] 0x00000000
[   11.376457] mlx5_core 0000:41:00.0: print_health_info:459:(pid 0): assert_exit_ptr 0x00806990
[   11.376491] mlx5_core 0000:41:00.0: print_health_info:461:(pid 0): assert_callra 0x00806c6c
[   11.376532] mlx5_core 0000:41:00.0: print_health_info:464:(pid 0): fw_ver 16.31.2006
[   11.376561] mlx5_core 0000:41:00.0: print_health_info:465:(pid 0): hw_id 0x0000020d
[   11.376594] mlx5_core 0000:41:00.0: print_health_info:466:(pid 0): irisc_index 10
[   11.376628] mlx5_core 0000:41:00.0: print_health_info:467:(pid 0): synd 0x8: unrecoverable hardware error
[   11.376665] mlx5_core 0000:41:00.0: print_health_info:469:(pid 0): ext_synd 0x0001
[   11.376697] mlx5_core 0000:41:00.0: print_health_info:471:(pid 0): raw fw_ver 0x101f07d6
dmesg_connectx5.log (250.2 KB)Any thoughts?JCThe issue is firmware stuck.The fw ver you used is very old, 16.32.xx.Please update to latest 16.35.xxhttps://network.nvidia.com/support/firmware/firmware-downloads/This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
674,bluefield-2-image,"Hi all,
Is it possible to do a full image backup of the BlueField 2 that I could restore later on?
I am developing a solution on a shared infrastructure (CloudLab) and when I finish my experiment I’d like to save everything and restore the state at once when I continue later on.
Thanks,
TomHave you tried to dd from device to a file?   dd if=/dev/rshim0/boot of=/cm/images/bluefield-2/PDU-2-%date.img bs=4k ?A bit related question;I’d like to PXE-boot the DPUs on a separate DPU management VLAN.   I’m using Bright CM, which I guess soon will be called Nvidia CM. ;)   Anyway, any pointers on how to set up Bluefield-2 to PXE over oob_net0 would be nice.  Possible?Thanks.Powered by Discourse, best viewed with JavaScript enabled"
675,does-the-mellanox-rnic-send-hardware-prefetch-instructions-to-the-memory-region,"When sending one-sided reads to remote nodes, I found the throughput count in by memory channel is greater than client application. So my question is that the read amplification is due to some hardware prefetch instructions from RNIC?Thanks!this problem is not from prefetch but unaligned access–end–Powered by Discourse, best viewed with JavaScript enabled"
676,what-is-the-difference-between-mfs1s00-hxxxe-and-mfs1s00-hxxe-ll,"Hi,MFS1S00-HxxxE vs MFS1S00-HxxxE-LL(low latency)
I can’t tell the exact difference even if I look at the specification documents of the two products.
The important thing in IB is low latency, so should I choose MFS1S00-HxxE-LL? I wonder why it is divided into two, and what is the difference and purpose between the two products?Powered by Discourse, best viewed with JavaScript enabled"
677,bad-input-length-0x50-when-creating-dci-qp,"Hello,To start my development, I’m trying to create a minimal example of Dynamically Connected Transport QPs. For that, one of the examples that I’m using is from NVIDIA itself: https://docs.nvidia.com/networking/display/rdmacore50/Dynamically+Connected+(DC)+QPsThe above example works on the DC Target side, but I’m facing an error on the Initiator side when creating the QP using the mlx5dv_create_qp function.The function fails with errno 5 (i/o error) on my Connect-X5 hardware and I’m seeing these lines on dmesg:[4494044.006304] mlx5_core 0000:af:00.0: mlx5_cmd_out_err:797:(pid 619708): CREATE_QP(0x500) op_mod(0x0) failed, status bad input length(0x50), syndrome (0x2f50ca), err(-5)
[4494044.007456] infiniband mlx5_0: create_qp:3192:(pid 619708): Create QP type 4098 failedThere is no further explanation on the logs that might suggest the cause error. Am I missing something on the initiator side that is not being covered by the example? Thank you.I have found there is little public documentation on syndromes/errors.However there is one page [1] that has useful information. The syndrome you have is documented there as:I’m not sure if this helps your situation but its the only info i could find[1] Mellanox error syndrome lists · GitHubmlx5_That’s certainly a step in the right direction, but I could only find PAS in the kernel driver code, so I wonder how this correlated with the user-land code.Hi @caianbene,It seems that the driver sent to the firmware buffer length parameter insufficient to establish the QP context.
This situation can arise if there is a compatibility issue between the driver and firmware versions. To verify this, please check the Release Notes of the driver, available at: Linux InfiniBand Drivers.
Additionally, improper parameter handling can also lead to this problem.Regards,
ChenThanks @chenh1, we had indeed a firmware issue on some of our nodes. We repeated the test on two Dell PowerEdge systems that are properly configured (driver 5.6-2.0.9 and firmware 16.32.1010) and now dmesg only shows:[4648076.498624] infiniband mlx5_0: create_qp:3192:(pid 4166615): Create QP type 4098 failedBut there is still a problem when creating the QP for the DCT. You mentioned improper parameter handling. Does this mean device/driver configuration or user code? Because the code was taken from NVIDIA documentation.Thank you,Powered by Discourse, best viewed with JavaScript enabled"
678,tls-retransmission-issue-with-hw-offloading,"Hi, I’m building a DPDK application that supports TLS encryption with NIC HW offloading. I modified MLX5 driver code in DPDK by referring to the kernel code for TLS offloading and successfully offload TLS encryption on HW using DPDK.But, I have a problem with TLS retransmission with HW offloading. The problem is that when the retransmission payload size is less than 13B, NIC HW does not encrypt data and it just sends plain-text data. I`m using TLS 1.2 and AES256-GCM-SHA384 as cipher suite. I properly resync TLS record for retransmission.
For example, let’s say TLS record size is 16408B (8 bytes for iv, 16 bytes for TAG, 16384 bytes for payload). When I try to retransmit the first 12 bytes payload (out of 16384 bytes payload), I prepared 5B record header and 8B iv, and 12 bytes payload for encryption. In this case, the payload is not encrypted. But, when the payload size is larger than 12B, the payload is successfully encrypted. Is there any requirements about minimum length for encryption?
If you need more information, please let me know.Hi @youngminchoi94 ,We don’t support TLS offloads using DPDK, but using DOCA.
Please review NVIDIA TLS Offload Guide in DOCA SDK documentation.Regards,
ChenPowered by Discourse, best viewed with JavaScript enabled"
679,rdma-without-switch-is-it-even-possible,"Hello,I just want to check if it is even possible to get RDMA running on two Widnows Datacenter 2019 (only for Scale-Out File Server)
RDMA1489×1505 526 KB
 hosts with two ConnectX-3 Pro RDMA adapters - connected directly (without switch).I’ve got them configured, IP addreses are “pingable”, however RDMA traffic is not going.Hi Piotr,Yes, RDMA communication between two Windows servers (without a switch) is possible.Please install WinOF, which is the Windows driver for ConnectX-3 and ConnectX-3 Pro network adapter cards. Downloadable from: http://www.mellanox.com/related-docs/prod_software/MLNX_VPI_WinOF_User_Manual_v5.35.pdfFor RDMA communication, the adapter performs virtual-to-physical memory address translation internally. This translation is defined by a Memory Translation Table (MTT) that is built internally by the WinOF device drivers.For proper installation & additional details, please follow WinOF user-manual:https://docs.mellanox.com/display/WINOFv550/Mellanox+WinOF+VPI+DocumentationRegards,Chenthanks @Chen Hamami​ , I have a question about RoCEv2 on ConnectX-5 Adapters. I’m using vSAN and in 7.0 U3 it supports RDMA / RoCEv2.Heres my szenario:Powered by Discourse, best viewed with JavaScript enabled"
680,how-to-create-a-n-w-interface-of-type-mlx5e-rep,"I use Ubuntu 22.04 for my development. How can I create a network interface of driver type “mlx5e_rep” on my “ConnectX-5” NIC?mlx5e build in mlx5_core, no standalone of that type.https://www.kernel.org/doc/html/v5.4/networking/device_drivers/mellanox/mlx5.htmlThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
681,mcx4121a-xcat-z590,"本人主板为MSIZ590 carbonWiFi ，购于21年10月4日于京东第三方卖家处购买，因为没有发票，无法注册本主板的信息。 本人 自有PCI-E x8的10G 光口网卡三块，型号MCX4121A-XCAT，隶属于CONNECTX-4 LX 10GBE,插在主板的第三条PCI-E X4 插槽上，主板PCIE 设置成自动或者X16+X0,每次电脑启动网卡都是时有时无，绝大部分时间是没有的，本人已经做过多次尝试，刷主板BIOS ，并重置，刷网卡最新固件 并安装最新的驱动,结果都是时好时坏，这块网卡我有两三块，替换后均是如此情况，而在我另外一台Z170主板和群晖NAS上，虽然都是运行在X4的情况下，但都能正常使用，唯独Z590 时好时坏。本人尝试过 网络提供的方法，比如屏蔽主板自带的INTEL网卡和无线网卡，关闭快速启动，开启CSM 诸如此类，发现一个情况就是，大部分情况下，如果发生硬件插拔，开启系统的当下，是能看到本地连接有这个网卡的，再次重启就大概率消失了，何时在会出现，就不得而知了。插在第二条PCIEX8 插槽能正常使用，但使用的显卡只能跑在X8 模式.
后期联系MSI 官方技术客服，也把网卡寄给他们测试，MSI 方面给我的答复是这个网卡可能和Z590 系列芯片组有兼容性问题，现贴上一些截图，这边论坛是英文的，不知道你们能否正常查看无法上传图片，我等会上传最后一张图片是MSI方面的技术人员发给我的测试图片，他们使用ASUS 的主板插这块主板也出现这个问题，希望这边给出我技术答复！谢谢为何图片无法上传？
111000×715 141 KB

22479×588 37.2 KB

33479×588 52.3 KB

44479×588 44.8 KB

552736×3648 140 KB

663648×2736 158 KB

773648×2736 200 KB

881184×1013 128 KB
Looks like this is PCIE bus issue, PCIE link not up. should hardware not compatible.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
682,opensm-and-rate-10,"Hi all,
is there some option to make opensm accept aka activate a link/a port that reports Rate: 10 ?
We have these in a mixed HDR/FDR fabric. It is a known firmware bug in the case of ConnectX-6, but it also happens to ConnectX-4 ports connected to HDR Quantum switches.
The physical links come up without probles, but the State never becomes Active.
My suspicion is that Opensm refuses to deal with such low band widths.But it would be useful to make it activate the link, so we could install the mft software and try to deal with the port with these tools.Regards,
ThomasHi,
Could you help to check which version FW are you use for both HCA and switch?Thanks,Well, the switch is a QM8790 with FW version:    27.2010.5042,
the ConnectX-4 HCA has FW version: 12.28.2006We have gone around this issue, though, by connecting the problematic old HCAs to an FDR switch.What worries me still is that a number of FDR-servers with similar or older firmware states (also ConnectX-3) do not show this behavior - under reboots and resets they have always come up with Rate: 56, although being connected to Quantum HDR switches.
So I expect this phenomenon to appear with these servers only when there is a crash, late in the night, on a weekend, when you would wish for everything to come up automatically instead of having to re-cable the machine…Cheers
ThomasCould you open a support ticket from our support portal, I think we need more informaiton for toubleshoot it.
Thanks,
SuoThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
683,lro-on-dpdk-hairpin-queue,"Hi.I want to make a DPDK hairpin queue (which transmits incoming packets without DMA) do LRO.I turned on LRO feature on DPDK port, and setup a hairpin queue for that port.
But I checked that the outgoing traffic from hairpin TX queue is not LROed.Is there any other requirement to enable LRO on DPDK hairpin queue without any DMA?Note)
In mlx5 pmd code, mlx5_lro_update_hdr() updates the headers of LROed packets, since HW only does checksum for concatenated payload.
So I guessed DPDK hairpin queue cannot currently do LRO for the no-DMAed traffic.
Is it right?hi CerotykiYou are right.
DPDK hairpin queue cannot do LRO.Thank you
Meng, ShiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
684,kernel-panic-seen-while-centos-8-3-linux-boot-with-the-adapter-mellanox-connectx-5-mcx516a-cdat,"When the card is in place and tried to boot the CentoOS8.3 and encountered with kernel panic with the inbox mlx5 drivers. However, it was working fine with CentOS 8.2.Below are the logs captured during boot:[ 106.843633] enic 0000:62:00.0: vNIC csum tx/rx yes/yes tso/lro yes/yes rss yes intr mode any type min timer 125 usec loopback tag 0x0000^M[ 106.872044] enic 0000:62:00.0: vNIC resources avail: wq 1 rq 1 cq 2 intr 4^M[ 106.888102] enic 0000:62:00.0: vNIC resources used: wq 1 rq 1 cq 2 intr 4 intr mode MSI-X^M[Apr 15 15:44:08.788] [ 107.819601] mlx5_core 0000:d8:00.0: enabling device (0140 → 0142)^M[ 107.834260] mlx5_core 0000:d8:00.0: firmware version: 16.28.4000^M[ 107.848257] mlx5_core 0000:d8:00.0: 126.016 Gb/s available PCIe bandwidth, limited by 8 GT/s x16 link at 0000:d7:00.0 (capable of 252.048 Gb/s with 16 GT/s x16 link)^M[Apr 15 15:44:09.119] [ 108.150522] mlx5_core 0000:d8:00.0: Rate limit: 127 rates are supported, range: 0Mbps to 97656Mbps^M[ 108.171066] mlx5_core 0000:d8:00.0: E-Switch: Total vports 10, per vport: max uc(1024) max mc(16384)^M[ 108.206017] mlx5_core 0000:d8:00.0: Port module event: module 0, Cable unplugged^M[ 108.224583] mlx5_core 0000:d8:00.0: mlx5_pcie_event:296:(pid 9): PCIe slot advertised sufficient power (27W).^M[ 108.230655] BUG: unable to handle kernel NULL pointer dereference at 0000000000000400^M[ 108.236391] mlx5_core 0000:d8:00.1: enabling device (0140 → 0142)^M[ 108.236626] mlx5_core 0000:d8:00.1: firmware version: 16.28.4000^M[ 108.236675] mlx5_core 0000:d8:00.1: 126.016 Gb/s available PCIe bandwidth, limited by 8 GT/s x16 link at 0000:d7:00.0 (capable of 252.048 Gb/s with 16 GT/s x16 link)^M[ 108.327923] PGD 0 P4D 0 ^M[ 108.334269] Oops: 0000 [#1] SMP PTI^M[ 108.342729] CPU: 9 PID: 1879 Comm: kworker/u32:2 Tainted: G ---------r-t - 4.18.0 #1^M[ 108.363668] Hardware name: Cisco Systems Inc UCSC-C220-M5SX/UCSC-C220-M5SX, BIOS C220M5.4.1.3e.0.1210201720 12/10/2020^M[ 108.388152] Workqueue: mlx5_hv_vhca mlx5_hv_vhca_invalidate_work [mlx5_core]^M[ 108.404537] RIP: 0010:hv_read_config_block+0xc4/0x150^M[ 108.416514] Code: 24 40 83 e2 1f c7 44 24 48 09 00 49 42 09 d0 ba 10 00 00 00 44 89 74 24 4c 89 44 24 50 48 8b 43 38 48 c7 44 24 38 c0 f3 ad b0 <48> 8b b8 00 04 00 00 44 89 64 24 54 e8 3b 6a cb 01 85 c0 74 1f 48^M[ 108.459739] RSP: 0018:ffffa60283e5bd80 EFLAGS: 00010246^M[ 108.472168] RAX: 0000000000000000 RBX: ffff8b02bffce038 RCX: ffffa60283e5bdb8^M[ 108.488829] RDX: 0000000000000010 RSI: ffffa60283e5bdc8 RDI: ffffa60283e5bd88^M[ 108.499392] mlx5_core 0000:d8:00.1: Rate limit: 127 rates are supported, range: 0Mbps to 97656Mbps^M[ 108.505478] RBP: ffffa60283e5be24 R08: 0000000000000006 R09: 0000000000000001^M[ 108.505479] R10: 8080808080808080 R11: 0000000000000010 R12: 0000000000000080^M[ 108.505479] R13: ffff8afedf86a000 R14: 0000000000000000 R15: ffff8b02de88a600^M[ 108.505481] FS: 0000000000000000(0000) GS:ffff8b02efa40000(0000) knlGS:0000000000000000^M[ 108.505482] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033^M[ 108.505483] CR2: 0000000000000400 CR3: 0000000507c28003 CR4: 00000000007606e0^M[ 108.505484] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000^M[ 108.505484] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400^M[ 108.505485] PKRU: 55555554^M[ 108.505486] Call Trace:^M[ 108.505496] ? __switch_to_asm+0x41/0x70^M[ 108.526217] mlx5_core 0000:d8:00.1: E-Switch: Total vports 10, per vport: max uc(1024) max mc(16384)^M[ 108.542881] ? _hv_pcifront_read_config+0x140/0x140^M[ 108.542941] mlx5_hv_config_common+0x53/0xf0 [mlx5_core]^M[ 108.542980] mlx5_hv_vhca_control_agent_invalidate+0x44/0x130 [mlx5_core]^M[ 108.563567] mlx5_core 0000:d8:00.1: Port module event: module 1, Cable unplugged^M[ 108.576370] mlx5_hv_vhca_invalidate_work+0x53/0x80 [mlx5_core]^M[ 108.595398] mlx5_core 0000:d8:00.1: mlx5_pcie_event:296:(pid 158): PCIe slot advertised sufficient power (27W).^M[ 108.608795] process_one_work+0x1a7/0x360^M[ 108.608797] worker_thread+0x30/0x390^M[ 108.608799] ? create_worker+0x1a0/0x1a0^M[ 108.608803] kthread+0x112/0x130^M[ 108.608806] ? kthread_flush_work_fn+0x10/0x10^M[ 108.845357] ret_from_fork+0x35/0x40^M[ 108.854147] Modules linked in: mlx5_core(+) enic^M[ 108.865233] Features: xt_u32 act_ct act_mpls^M[ 108.875529] CR2: 0000000000000400^M[ 108.883707] —[ end trace 73ba7b4f8ad0c9f9 ]—^M[ 108.900672] RIP: 0010:hv_read_config_block+0xc4/0x150^M[ 108.912681] Code: 24 40 83 e2 1f c7 44 24 48 09 00 49 42 09 d0 ba 10 00 00 00 44 89 74 24 4c 89 44 24 50 48 8b 43 38 48 c7 44 24 38 c0 f3 ad b0 <48> 8b b8 00 04 00 00 44 89 64 24 54 e8 3b 6a cb 01 85 c0 74 1f 48^M[ 108.955888] RSP: 0018:ffffa60283e5bd80 EFLAGS: 00010246^M[ 108.968309] RAX: 0000000000000000 RBX: ffff8b02bffce038 RCX: ffffa60283e5bdb8^M[ 108.984950] RDX: 0000000000000010 RSI: ffffa60283e5bdc8 RDI: ffffa60283e5bd88^M[ 109.001566] RBP: ffffa60283e5be24 R08: 0000000000000006 R09: 0000000000000001^M[ 109.018167] R10: 8080808080808080 R11: 00000000000000[Apr 15 15:44:10.064] 10 R12: 0000000000000080^M[ 109.034760] R13: ffff8afedf86a000 R14: 0000000000000000 R15: ffff8b02de88a600^M[ 109.051335] FS: 0000000000000000(0000) GS:ffff8b02efa40000(0000) knlGS:0000000000000000^M[ 109.070007] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033^M[ 109.083492] CR2: 0000000000000400 CR3: 0000000507c28003 CR4: 00000000007606e0^M[ 109.100050] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000^M[ 109.116591] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400^M[ 109.133130] PKRU: 55555554^M[ 109.139871] Kernel panic - not syncing: Fatal exception^M[Apr 15 15:44:11.257] [ 110.288463] Shutting down cpus with NMI^M[ 110.368474] Kernel Offset: 0x2f200000 from 0xffffffff81000000 (relocation range: 0xffffffff80000000-0xffffffffbfffffff)^M[ 110.398993] —[ end Kernel panic - not syncing: Fatal exception ]—^MHello Madhusudhanan,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we would recommend to upgrade the f/w of the adapter to the latest, which is version 16.30.1004. As your are using CentOS 8.3 which contains upstream kernel, it can be you have a very recent kernel which contains a compatibility issue with the f/w of the adapter.As you are running INBOX driver, support needs to be obtained through the OS vendor. The following link provides an explanation around this support model → Upstream Releases/Inbox DriversThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
685,e-burning-encrypted-image-on-non-encrypted-device-is-not-allowed,"Is there any workaround to this error msg?Hi，
It is not supported.
This is by design.
Thanks,thanks.
How can I encrypt the device so that I can upgrade the fw?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
686,innova2-flex-image-not-loading,"I have an Innova2 Flex Accelerator card plugged into an x86 Ubuntu system and I am trying to run the canned application downloaded from the Mellanox website (Innova2_Flex_Open_18_11.tar). I am following the instructions in the user manual “Innova-2TM Flex Open for Application Acceleration EN Adapter Card User Manual” v1.7I have successfully installed Mellanox OFED and am trying to run the application included in the Open Bundle (as described in Section 7.1 and 7.2). However, I am not able to get the Flex image to run (i.e. I never get the output as shown in 7.2.1 b, c, or d). I keep getting the menu for the Factory image (as shown in 7.2.1 (a) even after installing the driver as described in step 3 of section 7.2.I tried to uninstall and reinstall OFED and rerun the whole procedure a few times but still end up at the same point. Not sure what is going wrong – it seems like the driver is not getting installed for some reason.Any help would be much appreciated.Thanks.Is “Factory image” in red?If yes, then the FPGA Configuration Memory is faulty or (more likely) was incorrectly written. Reprogram the FPGA.Otherwise, run the following commands and post the output.Powered by Discourse, best viewed with JavaScript enabled"
687,updating-synchronizing-firmware-on-a-6036-and-6036g-switch,"Dear Mellanox support,I currently have two switches:SX6036G - PPC_M460EX 3.6.8010 2018-08-20 18:04:16 ppcSX6036 - PPC_M460EX 3.6.8012 2019-02-22 07:53:42 ppcIn order to get MLAG working properly, it is recommended to use the same firmware versions on both switches. Therefore I want to synchronize them.I simply thought about copying the (newer) firmware from the second switch to the first. Would that be possible and if yes, how? Is it a problem that I put a SX6036 Firmware on a SX6036G switch?Another solution would be to install the newest available firmware on both switches. However, where would I get that from? I had a look at the Mellanox website but I cannot find any firmwares for the 6036 switches?Any help is appreciated!Best Regards,HermannThe latest Software for the switchX series is 3.6.8012.you can get the software from myMellanoxhttps://support.mellanox.com/s/Powered by Discourse, best viewed with JavaScript enabled"
688,cumulus-vx-bgp-sessions-unstable-in-vrf-with-vlan-subinterfaces,"I’m having problems getting BGP sessions to stay up in Cumulus VX  on VMware ESXi when using VRFs and VLAN subinterfaces, the BGP session comes up periodically for a few seconds and then dies again - I see the following errors in the FRR log:2022-03-02T09:36:18.834354+00:00  bgpd[1991]: bind to interface RED failed, errno=1
2022-03-02T09:36:22.799796+00:00  bgpd[1991]: bind to interface BLUE failed, errno=1Is this a supported configuration?If I use BGP outside of a VRF context the BGP sessions are stable over VLAN subinterfaces.Hi Tim,Can you share the configuration you are using? To test if ESXi is the culprit, you can also spin up a topology in AIR (air.nvidia.com)Interestingly the problem does not exist in a second Cumulus VX VM I’m running on the same ESXi host, it is also peered with the NSX-T T0 but it does not use VRFs in the Cumulus config as I need EVPN and the documentation states that EVPN address family is not supported in VRFs. That config also uses VLAN subinterfaces and those BGP sessions are stable.Here is the current config for the VRF VX VM:The EVPN SAFI always runs in the default VRF indeed, that is how EVPN works.This configuration though looks quite straight forward with a BGP session on a subinterface, but I also see:On that subinterface. You shouldn’t need that, because it is already has the .1q tag. Can you also show the contents of /etc/network/interfaces?I’ve just spun up a simple topology in AIR to test this and it seems to work there, but I do see the same bind error in the logs on the instance running the non-default VRF:2022-03-02T10:13:22.821389+00:00 cumulus bgpd[7242]: bind to interface RED failed,errno=1The ‘vlan: 960’ config was automatically added when I created the interface using the ‘nv set interface swp1.960…’ commandHere is the contents of the file:Ah yes, the NVUE config still has new things for me as well. It looks as it should in e/n/iCould you also share the /etc/frr/frr.conf? Just to double check how that is being generated.Sure, here you go:Not sure if this is anything to do with the use of non-default VRF or subinterfaces, I’ve run some more tests using the default VRF with subinterfaces and also the default VRF with SVIs and I’m still seeing the same problems. It’s odd as all the ping tests I have done show the network connectivity to be reliable.I suspect you may be right about this being some issue with ESXi, do you know if the Cumulus VX OVA has been tested on ESXi 7.0?I am not seeing anything incorrect in the configuration that might cause this. Would you have a chance to create the same topology you have in ESXi with the AIR build tool (Create Your Topology). VX on ESXi isn’t used that much, so it could be something specific there, but you already said you are seeing the same issue in AIR as well. If you give me access to the topology, I can have a closer look.The VRF topology I created in AIR worked, the BGP session was stable but the error regarding interface binding was still apparent in the logfile - so it seems like the binding error message is not relevant to this problem?I can’t fully re-create the topology in AIR as I’m using a VMware NSX-T T0 router in my setup too, this issue is specific to one of the peerings to the T0. I’ll try peering two VX VMs directly on ESXi and see if that works, at least it would prove/eliminate the NSX-T T0 as the culprit.Thanks for reviewing the config, I’m new to Cumulus and its reassuring to know that it doesn’t look like a config issue.No problem.Do keep in mind that if you are using VX as a virtual router that this is an unsupported scenario. ;-)Powered by Discourse, best viewed with JavaScript enabled"
689,rsocket-rsendto-fail,"Hi,My card is ConnectX-5 EN and O/S is Ubuntu 22.04(64-bit). and installed “MLNX_OFED_LINUX-5.8-1.1.2.1-ubuntu22.04-x86_6”.I am trying to test rsocket functions included in “librdmacm”.
When I ran “rsendto”, a linux errno 2(“No such file or directory”) occurred. The “rsocket()” even succeeded and returned a file descriptor.
What I should do?Here is my code.===============================================
int ret, sock, svrAddrSize;
uint8_t *pSnd = NULL;ret = posix_memalign((void **)(&pSnd), 4096, 8192);
if (ret != 0) {	return -1; }svrAddr.sin_family = AF_INET;
svrAddr.sin_addr.s_addr = inet_addr(“192.168.103.20”);
svrAddr.sin_port = htons(10000);sock = rsocket(PF_INET, SOCK_DGRAM, 0);
if(sock < 0) {	return -1; }ret = rsendto(sock, pSnd, 8192, 0, (struct sockaddr *)&svrAddr, svrAddrSize);
if (ret < 0) {
printf(“Send FAIL, errno: %d\n”, errno); <=== error number 2 occurrs.
break;
}free(pSnd)
rclose(sock);hi Choirsocket is not part of our SW.
I suggest you test rdmacm with ib_write_bw(use -R option) first.
If there’s no issue in ib_write_bw, it means there’s no issue in rdma part.
detail please follow the reference:
https://enterprise-support.nvidia.com/s/article/ib-write-bw
https://enterprise-support.nvidia.com/s/article/howto-set-the-default-roce-mode-when-using-rdma-cmThank youPowered by Discourse, best viewed with JavaScript enabled"
690,how-to-solve-the-problem-that-buffers-commands-are-not-supported-on-your-system,"Hello, I met a problem when I use the tool mlnx_qos to set receive buffer size for Mellanox MCX516A-CCAT ConnectX-5 EN NIC. As picture 1 shows, when I input the command then it outputs an error message: “Buffers commands are not supported on your system”.How can I solve this problem?The version of OFED is MLNX_OFED_LINUX-5.1-2.3.7.1. The system information is as follows:
pic1.png1094×44 8.13 KB
Hi Chenbuffer size must be larger than the xoff_threshold.Example: 87296,87296,0,87296,0,0,0,0 sets receivebuffer size for buffer 0,1,2,3,4,5,6,7 respectivelyPlease check the xoff_threadThanks​Powered by Discourse, best viewed with JavaScript enabled"
691,release-notes-for-nvidia-bright-cluster-manager-8-1-26,"Release notes for Bright 8.1-26== General ==
=Improvements== Fixed Issues=== CMDaemon ==
=Improvements==Fixed Issues=== Machine Learning ==
=New Features=== cm-wlm-setup ==
=Improvements=Powered by Discourse, best viewed with JavaScript enabled"
692,sn3700-running-sonic-onie,"Hi !
I’m running SONIC on SN3700. I was just wondering if we could conduct SWITCHDEV side development by just disabling SONIC daemon ? Or should be install an other OS or side load an OS via live-CD ?
Thanks for your help
FredericPowered by Discourse, best viewed with JavaScript enabled"
693,disable-ddio-for-a-nic,"Is it possible to disable DDIO for a ConnectX NIC (rather than disable DDIO for all IO devices) ?Hi Qiang,Mellanox Socket Direct enables Intel® DDIO optimization on both sockets by creating a direct connection between the sockets and the adapter card.I don’t think NIC side can disable this. What’s the purpose you want to disable it?Maybe you can consult Intel® vendor if it can be disable for specific device.Regards,LeveiThank you for your replyPowered by Discourse, best viewed with JavaScript enabled"
694,unable-to-use-bluefields-pka-engine-on-bluefield-2-smartnic-p-n-mbf2h516a-ceeot,"(1)ecpu linux version:Linux localhost.localdomain 5.4.44-mlnx.9.gdffcb36f52b7 #1 SMP PREEMPT Tue Jul 7 02:43:36 IDT 2020 aarch64 aarch64 aarch64 GNU/Linux​(2)pka driver infomation:root@localhost:/usr/local/lib# lsmod | grep pkapka_mlxbf 77824 0(I’m not sure why vfio is missing, I have used “modprobe vfio”)​(3)pka lib/openssl engine libI compile from GitHub - Mellanox/pka: Mellanox BlueField PKA support mastergenerate libPKA.so and libbfengine.so(4)When I test with the openssl engineopenssl dgst -engine pka -sha256 -sign ./private.key -out 1.sign 1.txtthe output:engine “pka” set.PKA_USER: pka_get_rand_bytes: error: Error(-1) getting random number.PKA_USER: pka_get_rand_bytes: error: Error(-1) getting random number.PKA_USER: pka_get_rand_bytes: error: Error(-1) getting random number.PKA_USER: pka_get_rand_bytes: error: Error(-1) getting random number.PKA_USER: pka_get_rand_bytes: error: Error(-1) getting random number.PKA_USER: pka_get_rand_bytes: error: Error(-1) getting random number.PKA_USER: pka_get_rand_bytes: error: Error(-1) getting random number.PKA_USER: pka_get_rand_bytes: error: Error(-1) getting random number.Error Signing Data281473214142944:error:04088003:rsa routines:RSA_setup_blinding:BN lib:…/crypto/rsa/rsa_crpt.c:155:281473214142944:error:04066044:rsa routines:rsa_ossl_private_encrypt:internal error:…/crypto/rsa/rsa_ossl.c:297:please help me!​​DPU is commercial support, the pka driver is not public. Please contact sales to get it.Powered by Discourse, best viewed with JavaScript enabled"
695,mount-error-cifs-filesystem-not-supported-by-the-system,"Hello guysI am facing a problem trying to mount a cifs share and using the mellanox drivers because of the infiniband cards that I have. Then when I try to mount the cifs share with the command below I get that error shown below:mount -vvvvvvv -t cifs -o username=user,password=password,vers=2.0 ///user /media/tmpmount.cifs kernel mount options: ip=,unc=\\user,vers=2.0,user=soportemodemat,pass=**********mount error: cifs filesystem not supported by the systemmount error(19): No such deviceRefer to the mount.cifs(8) manual page (e.g. man mount.cifs) and kernel log messages (dmesg)I have tried with versions 1.0 and 3.0 too but I got the same error. I can provide the next information for you to give any idea to solve this issue please:cat /etc/redhat-releaseCentOS Linux release 8.4.2105uname -r4.18.0-305.17.1.el8_4.x86_64modinfo cifsfilename: /lib/modules/4.18.0-305.17.1.el8_4.x86_64/weak-updates/mlnx-ofa_kernel/fs/cifs/cifs.koversion: 2.31license: Dual BSD/GPLdescription: cifs dummy kernel moduleauthor: Mohammad Kabatrhelversion: 8.4srcversion: 01E451882B55F354B7F130Bdepends: mlx_compatname: cifsvermagic: 4.18.0-305.el8.x86_64 SMP mod_unload modversionssig_id: PKCS#7signer: Mellanox Technologies signing keysig_key: BA:B0:F5:CD:23:24:A0:lsmod | grep cifscifs 16384 0mlx_compat 16384 13 rdma_cm,ib_ipoib,cifs,mlxdevm,iw_cm,auxiliary,ib_umad,ib_core,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm,mlx5_coregrep -c cifs /proc/filesystems0Thank you in advance for your reply and any ideas are welcome.Hello,In RH8.4 it was added that CIF is depend on IB driver.When MLNX_OFED is installed, it replaces IB driver implementation, thus original CIF kernel module that depends on it cannot function.As a result, we replace it with dummy module, basically removing CIF support.In MLNX_OFED 5.5 we added a small fix to prevent from error display upon CIF loading, but still CIFS will not function.In order to work with CIF and our driver:You can either use inbox driver that comes with RH8.4 and thus no need in MLNX_OFED.Or, recompile CIF module manually after install of MLNX_OFED.Best Regards,VikiHi Viky.Thank you for your reply.I need the MLNX_OFED for the infiniband network card works properly. So could you tell me how can I recompile CIF module manually with the MLNX_OFED that is already installed.Powered by Discourse, best viewed with JavaScript enabled"
696,vgt-and-vlan-connectivity-on-dpdk19-11,"Noticed that using ConnectX-5 or ConnectX-6 LX on VGT+ and DPDK19.11 application, cannot communicate on VLANs that are allowed based on VGT+ configuration.
If disable VGT+ the VLAN communication works fine.Are you aware of any related issue?
Thanks.Hi Alex,Assuming that you are configuring VGT+ using the “ip” command according to the OFED guide:
ip link set dev  vf  vlan <vlan_id> [qos ]DPDK doesn’t know about this configuration.If you’d like to send/receive tagged traffic with DPDK, you can either use “vlan add” or rte_flow push/pop rules.For more info please take a look at testpmd examples:
https://doc.dpdk.org/guides/testpmd_app_ug/testpmd_funcs.htmlBest Regards,
AnatolyThank you very much.
We are going to follow the suggested solution.Thanks.
Br,
AlexPowered by Discourse, best viewed with JavaScript enabled"
697,connectx-3-one-way-traffic-to-switch-over-sfp-dac-10gbps-cx311a,"I bought a couple CX311A cards. I updated their firmware to latest 2.42.5000.I have never been able to ping across this NIC to the switch, much less any other traffic.When I connect to the switch via Aruba DAC cable SFP+ 10gbps, both sides show UP and 10gbps link speed. The switch shows traffic received, but almost zero “bytes TX” and regularly incrementing “Drops TX”. The server with the NIC shows Sent=0 and Received=64 with pretty much no regular incrementing - just a bit of Received incrementing on first connection. I ran Wireshark on the server, capturing this NIC and it does show the NIC generating some traffic such as ARPs looking for IP destinations and DHCP Discovers, but no received traffic.My Aruba DAC is in the supported 10gbps cables list in document “Mellanox ConnectX-3 Firmware
Release Notes Rev 2.42.5000” : Aruba HPE J9285B .I tried both Windows 10 and VMware ESXi 7.0.3 as the OS on the server, with same results.The server has motherboard Asus Prime X570-P BIOS version 4602, CPU AMD 3950X.The switch is a Aruba 5406 firmware KB.16.10.0014, and the DAC is connected to a SFP+ port on a J9990A card.Hello aglatt,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.To do the basic triage for this issue, is to eliminate the switch and connect two nodes back to back with the cable.If the issue resides, replace the cable for another known working cable. Step by step, you can determine if this is a bad cable issue or a faulty adapter.Thank you and regards,
~NVIDIA Networking Technical SupportHi MvB. Thanks for the advice.I stood up a second client PC with the second CX311A NIC installed and connected directly PC - PC via the two CX311A NICs and the same SFP+ Aruba DAC.  The two clients are able to ping each other, no problem, as well as transfer other traffic. So evidently this is some kind of Aruba switch problem.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
698,running-a-dhcp-server-on-leaf-switches-configured-with-mlag,"Q: Can I set up a DHCP server on an NVIDIA Mellanox SN2100 switch running Cumulus Linux and configured with MLAG?The server is configured with LACP, where port 1 of server is connected to switch1 and port 2 to switch 2. How can I set the DHCP server in such a way that if switch1 goes down and at that time the server rebooted switch should be able to handle the DHCP request and able to assign IP to the second port of server.A: It is possible to do this but we do not document it currently. Essentially you create two active/active DHCP servers that communicate with each other to keep in sync so if one dies the other has a complete picture.You can follow the guide here to see just how to do it → A Basic Guide to Configuring DHCP FailoverPowered by Discourse, best viewed with JavaScript enabled"
699,snmp-ifindex,"Hi,I have setup a couple of SN2010 switches with mlag configuration that I will supervise using SNMP.I found out that the SNMP ifindex are theses :ifDescr ifIndexlo 1mgmt0 3Eth1/1 109Eth1/2 110Eth1/3 105Eth1/4 106Eth1/5 107Eth1/6 108Eth1/7 101Eth1/8 102Eth1/9 103Eth1/10 104Eth1/11 97Eth1/12 98Eth1/13 99Eth1/14 100Eth1/15 65Eth1/16 66Eth1/17 67Eth1/18 68Eth1/19 73Eth1/20 69Eth1/21 81Eth1/22 77Po1 13826Mpo1 29001Mpo2 29002I have several question about that :On cisco product, you may have the ifindex persistent using “snmp-server ifindex persist” and “show snmp mib ifmib ifindex …”, for example :gaea3#show snmp mib ifmib ifindex g3/1/1Interface = GigabitEthernet3/1/1, Ifindex = 11301Alain RichardHello Alain,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we recommend to review the following two Community posts regarding SNMP, including the SNMP section in the UM.If you need further information after review, please open a NVIDIA Networking Technical Support ticket by sending an email to networking-support@nvidia.com. We will then continue to assist you through the ticket.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
700,listen-to-the-latest-kernel-of-truth-podcast-season-3-episode-2,"The latest episode discusses IT in the healthcare industry. Get it here or wherever you get your podcasts.Powered by Discourse, best viewed with JavaScript enabled"
701,install-doca-2-0-2-ubuntu2204-on-bluefield-2-failed,"Hi, I follow the guide how to install bfb image in Bluefield 2.I faced with a problem that the ubuntu installation did not start and ssh fell off. Did someone encounter this? Tried to use bfb-install several times, the result is the samebfb-install --bfb DOCA_2.0.2_BSP_4.0.3_Ubuntu_22.04-10.23-04.prod.bfb --config bf.cfg -rshim rshim0
Pushing bfb + cfg
1.09 GiB 0:01:14 [16.5MiB/s] [ <=> ]
Collecting BlueField booting status. Press Ctrl+C to stop…
INFO[PSC]: PSC BL1 START
INFO[BL2]: start
INFO[BL2]: DDR POST passed
INFO[BL2]: UEFI loaded
INFO[BL31]: start
INFO[BL31]: lifecycle Production
INFO[BL31]: runtime
INFO[UEFI]: eMMC init
INFO[UEFI]: eMMC probed
INFO[UEFI]: PCIe enum start
INFO[UEFI]: PCIe enum end
INFO[UEFI]: exit Boot ServiceWaiting couple hours did not help me.ThanksAlthough the doca_1.5_ubuntu2004 version is installed successfully.
Can someone suggest how to debug such kind of issue?Are you able to access the DPU console and gain access?
(It might give us a better idea of the current status of the DPU)
Was initially DOCA 1.5.1 successfully installed and you tried to push 2.0.2?
Are you able to successfully re-install 1.5.1?
Have you tried to disable secure boot, push 2.0.2 then re-enable secure boot?We have the same problem here. We just flashed the bfb image manually. DOCA 2.0.2 definitely has a problem on the install.yes, I have, I use rshim in order to restore  doca_1.5_ubuntu2004. I reinstall it from scratch and try to install doca2_ubuntu2204.bfb.Have you tried to disable secure boot, push 2.0.2 then re-enable secure boot?
I will check, thanks.Disabling secure boot helped to me. @spruitt thanks a lot.Yavtuk - what OS & specific version are you running on your host machine?Hi, Ubuntu 22.04This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
702,is-mlag-vip-required-to-form-an-sn2010-cluster,"New to MLAG and the SN2010MWant to MLAG two SN2010 switchesThis document states the MLAG VIP is optional.https://docs.nvidia.com/networking/display/Onyxv382306/MLAG#MLAG-MLAGVirtualSystem-MAC2 questions:Sorry I am brand new to this. Thank you for your help.MLAG needs to be able to determine the difference between two types of failures that appear very similar:
Peer switch fails
Peer-link has failedMLAG uses the management network communication to determine if the other switch is still working when the peer-link has failed. If the other switch is still responding when the peerlink fails, then the secondary switch will take down all mlag interfaces until the peerlink is restored or until the primary switch fails.If you do not have the VIP configured on the management network, then the switches will not properly handle peerlink failures and will go into the splitbrain state which relies on STP and LACP system MAC to handle failovers. If LACP on the clients does not notice that there is now two system MACs on different ports in the bond, or if the clients are using static port-channels instead of LACP, then a peerlink failure would result in an outage. If STP is not properly configured for this failure scenario, then an outage could happen as it is not a usual failure scenario where the management network and peerlink fail at the same time, and many networks are not designed to handle this double-failure splitbrain case using STP.Thank you so much for the answer.One more question, what is the VLAN requirement for the IPL port-channel?
This port-channel must be configured as “access”, correct? on any VLAN?
Can I use the same VLAN that I configured for the IPL VLAN interface (e.g., VLAN 4000) ?Thank you!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
703,mlnxofedinstall-with-option-add-kernel-support-generate-unsigned-module,"If we use mlnxofedinstal with option --add-kernel-support module will be unsigned and unusable on server with UEFI secure boot enabled.Hello Cyrille,Thank you for posting your question on the Mellanox Community.How to setup secure boot depends on which OS you are using. If you are using Redhat or SLES you can follow the instructions presented here:https://docs.mellanox.com/display/MLNXOFEDv531001/UEFI+Secure+BootIf you are using Ubuntu, unlike in Red Hat and SLES, the kernel modules installed by the Mellanox OFED are not pre-signed with the Mellanox signing keys. This is because DKMS on Ubuntu installs modules in such a way that it does not allow for modules to be pre-signed as they are in Red Hat and SLES. This means in order to work with the Mellanox kernel modules on Ubuntu with SecureBoot the modules must be manually signed a key that is user created.The following is the process for creating a private/public key pair and using that key pair to sign the Mellanox kernel modules:# This definition stops the following lines choking if HOME isn’t# defined.HOME = .RANDFILE = $ENV::HOME/.rnd[ req ]distinguished_name = req_distinguished_namex509_extensions = v3string_mask = utf8onlyprompt = no[ req_distinguished_name ]countryName = CAstateOrProvinceName = QuebeclocalityName = Montreal0.organizationName = cyphermoxcommonName = Secure Boot SigningemailAddress = example@example.com[ v3 ]subjectKeyIdentifier = hashauthorityKeyIdentifier = keyid:always,issuerbasicConstraints = critical,CA:FALSEextendedKeyUsage = codeSigning,1.3.6.1.4.1.311.10.3.6,1.3.6.1.4.1.2312.16.1.2nsComment = “OpenSSL Generated Certificate”Use the SSL certificate to create a private/public key pair. Using openssl.cnf as the example certificate, use the following command:# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/mlx5_core.ko# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/rdma_ucm.ko# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/rdma_cm.ko# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/ib_ipoib.ko# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/mlx5_ib.ko# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/ib_uverbs.ko# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/ib_cm.ko# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/ib_core.ko# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/mlxfw.ko# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/mlx_compat.ko# strip --strip-debug /lib/modules/5.4.0-66-generic/updates/dkms/iw_cm.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/mlx5_core.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/ib_core.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/rdma_ucm.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/rdma_cm.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/ib_ipoib.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/mlx5_ib.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/ib_uverbs.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/ib_umad.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/ib_cm.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/mlxfw.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/mlx_compat.ko# kmodsign sha512 MOK.priv MOK.der /lib/modules/5.4.0-66-generic/updates/dkms/iw_cm.koOnce these steps are followed, openibd will be allowed to load the signed modules. This procedure has been tested and proven to work on Ubuntu 18.04 and Ubuntu 20.04Thanks and regards,~Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
704,why-ibv-poll-cq-always-have-about-2-seconds-delay-before-available,"I made a simple test for using more then one sge which use Send/Recv and RC qp.sever side post two send work request immediately and then start poll cq.client side post one recv work request then start poll cq, when recv ready, it then post another recv work request.In my laptop (openSUSE Tumbleweed with rxe enabled) it runs as expected, but when I put the code run on server ( connect directly to mellanox device), after first work request was completed, there will be about 2 seconds delay, then the second work request complete. that’s weird.the following picture is the full actions I’ve made
ksnip_20210514-1456111541×1673 725 KB
The code is in attachment.How can I fix this problem ?Thank you!rdma.tar (60 KB)Hi,I would suggest you to ask this question on linux-rdma mailing list as it seems to be a generic RDMA programming matter.From brief review, it looks like there are mixed control/data patch. Application poll CQ and allocate resources in the same look that might be not optimal.Contents1 General tips1.1 Avoid using control operations in the data path1.2 When posting multiple WRs, post them in a list in one call1.3 When using Work Completion events, acknowledge several events in one call1.4 Avoid using many scatter/gather...
Est. reading time: 8 minutes
Try to continue with linux-rdma list, as this community is dedicated to specific Mellanox/Nvidia related questions.Powered by Discourse, best viewed with JavaScript enabled"
705,add-almalinux-os,"Is it possible to add AlmaLinux OS to build scripts? For example, if I attach simply patch?mlnx.alma.patch (580 Bytes)Hello Aleksey,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately, it is not possible to add this patch. We are still reviewing which CentOS based distro we need to support in the future, after the announcement from Red Hat to make CentOS Upstream.Best thing to do, is re-visit our website from time to time for newly support OS-es.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
706,pci-e-bus-errors-with-connectx-3-and-asus-x-99e-ws,"Hi,I am experiencing several problems when using a ConnectX-3 40GbE adapter (MCX313A-BCBT) in an Asus X99-E WS motherboard.First it makes system startup quite unstable. Approx. 2 out of 10 tries, the system halts before POST, and shows error code 94 on the 7-segment display of the mainboard (meaning PCI Enumeration Error).When it boots successfully, the latest Linux driver (mlnx-en-3.0-1.0.1.tgz), with the latest firmware, with Fedora 21 x86_64 (supported OS), fresh install, with a single NVidia GPU installed besides the HCA, it emits PCI bus errors during initialization. Sometimes it disables the card completely, sometimes it starts to work after a 1-1.5 minute wait during boot. When such errors occur, they look like:[ 10.743067] pcieport 0000:00:02.0: AER: Uncorrected (Non-Fatal) error received: id=0010[ 10.743077] pcieport 0000:00:02.0: PCIe Bus Error: severity=Uncorrected (Non-Fatal), type=Transaction Layer, id=0010(Requester ID)[ 10.743142] pcieport 0000:00:02.0: device [8086:2f04] error status/mask=00004000/00000000[ 10.743187] pcieport 0000:00:02.0: [14] Completion Timeout (First)[ 10.743225] pcieport 0000:00:02.0: broadcast error_detected message[ 16.852525] mlx4_core 0000:0a:00.0: command 0xff6 timed out (go bit not cleared)[ 16.852527] mlx4_core 0000:0a:00.0: RUN_FW command failed, aborting[ 16.855670] mlx4_core 0000:0a:00.0: mlx4_cmd_post:cmd_pending failed[ 16.855702] mlx4_core 0000:0a:00.0: Failed to start FW, aborting[ 17.858368] mlx4_core: probe of 0000:0a:00.0 failed with error -110[ 17.858638] pcieport 0000:00:02.0: AER: Device recovery failed[ 17.858643] pcieport 0000:00:02.0: AER: Uncorrected (Non-Fatal) error received: id=0010[ 17.858652] pcieport 0000:00:02.0: PCIe Bus Error: severity=Uncorrected (Non-Fatal), type=Transaction Layer, id=0010(Requester ID)[ 17.858735] pcieport 0000:00:02.0: device [8086:2f04] error status/mask=00004000/00000000[ 17.858787] pcieport 0000:00:02.0: [14] Completion Timeout (First)[ 17.858832] pcieport 0000:00:02.0: broadcast error_detected message[ 17.858836] pcieport 0000:00:02.0: AER: Device recovery failed…[ 61.820905] mlx4_core: device is working in RoCE mode: Roce V1[ 61.820907] mlx4_core: gid_type 1 for UD QPs is not supported by the devicegid_type 0 was chosen instead[ 61.820908] mlx4_core: UD QP Gid type is: V1[ 101.351233] mlx4_core 0000:0a:00.0: PCIe link speed is 8.0GT/s, device supports 8.0GT/s[ 101.351235] mlx4_core 0000:0a:00.0: PCIe link width is x8, device supports x8[ 101.354441] mlx4_core 0000:0a:00.0: irq 62 for MSI/MSI-X[ 101.354445] mlx4_core 0000:0a:00.0: irq 63 for MSI/MSI-X[ 101.354448] mlx4_core 0000:0a:00.0: irq 64 for MSI/MSI-X[ 101.354451] mlx4_core 0000:0a:00.0: irq 65 for MSI/MSI-X[ 101.354453] mlx4_core 0000:0a:00.0: irq 66 for MSI/MSI-X[ 101.354456] mlx4_core 0000:0a:00.0: irq 67 for MSI/MSI-X[ 101.354459] mlx4_core 0000:0a:00.0: irq 68 for MSI/MSI-X[ 101.354462] mlx4_core 0000:0a:00.0: irq 69 for MSI/MSI-X[ 101.354464] mlx4_core 0000:0a:00.0: irq 70 for MSI/MSI-X[ 101.354466] mlx4_core 0000:0a:00.0: irq 71 for MSI/MSI-X[ 101.354469] mlx4_core 0000:0a:00.0: irq 72 for MSI/MSI-X[ 101.354471] mlx4_core 0000:0a:00.0: irq 73 for MSI/MSI-X[ 101.354474] mlx4_core 0000:0a:00.0: irq 74 for MSI/MSI-X[ 102.097189] mlx4_core 0000:0a:00.0: mlx4_pci_err_detected was called[ 102.097198] mlx4_core 0000:0a:00.0: device is going to be reset[ 102.125455] mlx4_en: Mellanox ConnectX HCA Ethernet driver v3.0-1.0.1 (Feb 2014)[ 103.138702] mlx4_core 0000:0a:00.0: device was reset successfully[ 103.138717] mlx4_core 0000:0a:00.0: Could not post command 0xd: ret=-5, in_param=0x65ae56000, in_mod=0x100, op_mod=0x0[ 103.138721] mlx4_core 0000:0a:00.0: SW2HW_MPT failed (-5)[ 103.138724] mlx4_en 0000:0a:00.0: Failed enabling memory region[ 104.151519] pcieport 0000:00:02.0: AER: Device recovery failed[ 104.151526] pcieport 0000:00:02.0: AER: Uncorrected (Non-Fatal) error received: id=0010[ 104.151536] pcieport 0000:00:02.0: PCIe Bus Error: severity=Uncorrected (Non-Fatal), type=Transaction Layer, id=0010(Requester ID)[ 104.151540] pcieport 0000:00:02.0: device [8086:2f04] error status/mask=00004000/00000000[ 104.151543] pcieport 0000:00:02.0: [14] Completion Timeout (First)[ 104.151548] pcieport 0000:00:02.0: broadcast error_detected message[ 104.151553] mlx4_core 0000:0a:00.0: mlx4_pci_err_detected was called[ 104.151556] ------------[ cut here ]------------[ 104.151565] WARNING: CPU: 0 PID: 165 at drivers/pci/pci.c:1535 pci_disable_device+0x99/0xb0()[ 104.151567] mlx4_core 0000:0a:00.0: disabling already-disabled device[ 104.151569] Modules linked in:[ 104.151571] mlx5_core(OE) mlx4_ib(OE) mlx4_en(OE) vxlan udp_tunnel nf_conntrack_netbios_ns nf_conntrack_broadcast ip6t_rpfilter ip6t_REJECT xt_conntrack cfg80211 ebtable_nat ebtable_broute bridge stp llc ebtable_filter ebtables ip6table_nat nf_conntrack_ipv6 nf_defrag_ipv6 nf_nat_ipv6 ip6table_mangle ip6table_security ip6table_raw ip6table_filter ip6_tables iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack iptable_mangle iptable_security iptable_raw snd_hda_codec_hdmi vfat x86_pkg_temp_thermal fat coretemp kvm crct10dif_pclmul crc32_pclmul snd_hda_intel crc32c_intel eeepc_wmi asus_wmi snd_hda_controller sparse_keymap rfkill snd_hda_codec iTCO_wdt iTCO_vendor_support ghash_clmulni_intel snd_hwdep snd_seq snd_seq_device snd_pcm sb_edac snd_timer serio_raw edac_core snd soundcore[ 104.151619] mlx4_core(OE) mlx_compat(OE) mei_me i2c_i801 lpc_ich mei mfd_core shpchp tpm_infineon tpm_tis tpm nouveau video mxm_wmi igb drm_kms_helper ttm e1000e drm dca ata_generic ptp i2c_algo_bit pata_acpi pps_core wmi [last unloaded: mlx4_core][ 104.151642] CPU: 0 PID: 165 Comm: kworker/0:2 Tainted: G OE 3.17.4-301.fc21.x86_64 #1[ 104.151644] Hardware name: ASUS All Series/X99-E WS, BIOS 1102 04/28/2015[ 104.151650] Workqueue: events aer_isr[ 104.151653] 0000000000000000 0000000017f53b38 ffff880659a8bbe8 ffffffff8173f929[ 104.151657] ffff880659a8bc30 ffff880659a8bc20 ffffffff810970ad ffff88065ccbc000[ 104.151661] ffff88065cc60510 0000000000000001 ffff880658ecfb10 ffff88065cc85800[ 104.151665] Call Trace:[ 104.151671] [] dump_stack+0x45/0x56[ 104.151678] [] warn_slowpath_common+0x7d/0xa0[ 104.151683] [] warn_slowpath_fmt+0x5c/0x80[ 104.151696] [] ? mlx4_enter_error_state.part.7+0x188/0x350 [mlx4_core][ 104.151704] [] pci_disable_device+0x99/0xb0[ 104.151720] [] mlx4_pci_err_detected+0x77/0xa0 [mlx4_core][ 104.151725] [] report_error_detected+0x50/0x100[ 104.151730] [] ? find_source_device+0x80/0x80[ 104.151734] [] pci_walk_bus+0x79/0xa0[ 104.151738] [] ? find_source_device+0x80/0x80[ 104.151742] [] broadcast_error_message+0xdc/0x100[ 104.151746] [] do_recovery+0x43/0x280[ 104.151750] [] ? get_device_error_info+0xd9/0x1b0[ 104.151754] [] aer_isr+0x36a/0x450[ 104.151761] [] process_one_work+0x14d/0x400[ 104.151765] [] worker_thread+0x6b/0x4a0[ 104.151770] [] ? rescuer_thread+0x2a0/0x2a0[ 104.151773] [] kthread+0xea/0x100[ 104.151777] [] ? kthread_create_on_node+0x1a0/0x1a0[ 104.151783] [] ret_from_fork+0x7c/0xb0[ 104.151787] [] ? kthread_create_on_node+0x1a0/0x1a0[ 104.151789] —[ end trace 858d8c660747219b ]—[ 104.151793] pcieport 0000:00:02.0: AER: Device recovery failed[ 104.151796] pcieport 0000:00:02.0: AER: Uncorrected (Non-Fatal) error received: id=0010[ 104.151803] pcieport 0000:00:02.0: PCIe Bus Error: severity=Uncorrected (Non-Fatal), type=Transaction Layer, id=0010(Requester ID)[ 104.151807] pcieport 0000:00:02.0: device [8086:2f04] error status/mask=00004000/00000000[ 104.151810] pcieport 0000:00:02.0: [14] Completion Timeout (First)…When using the same Mellanox card in a different mainboard (for example, a Gigabyte GA-Z97X-UD3H), it boots and inits flawlessly, using the exact same OS.We have a cluster built up from these boards, and they all have the same issue randomly, so it’s not a unique error of a single mainboard, but looks like some incompatibility.Did anybody experience a similar issue?Please share any suggestions about how to stabilize this.Thanks,PeterUpdate: it looks like the problems persist, even after another BIOS update (to version 1301) and setting PCI-E gen 1.Some cards in the cluster still fail to initialize, while others do not even appear in lspci. After a reboot, they may or may not appear. Several reboots are necessary to correctly start everything up.Could you try the latest 3.1-1.0.3 from http://www.mellanox.com/page/products_dyn?product_family=26&mtag=linux_sw_drivers http://www.mellanox.com/page/products_dyn?product_family=26&mtag=linux_sw_drivers ?Update: forcing the mainboard to use PCI-E 1.0 speed seems to solve the issue.This can be done in the BIOS utility → Advanced → PCH Configuration → PCI Express Configuration → PCIe Speed: set from “Auto” to “Gen1”This has bene suggested by one of the posters here, in relation with Nvidia GPUs suffering from a similar error in X-99 chipset based mainboards:Question for X99 board owners with Nvidia cards: do you see PCIe bus errors, please respond to poll - Page 2 Question for X99 board owners with Nvidia cards: do you see PCIe bus errors, please respond to poll | Overclock.netWhile downgrading PCIe is a workaround, it’s still an issue waiting for a proper solution.I’m late to the party but I think we can confirm that this still occurs with CentOS 6.7 and driver version 3.1-1.0.3; I don’t have direct access to the systems but I am working to reproduce in a testing environment. If I can do anything to help accelerate resolution please let me know.Hi same problem with Mellanox Connect-X3 CX354A QBCT and an Asus X99 Deluxe II Mainboard. Any updates here? Set PCIe speed to 1.0 slows down the whole system.Powered by Discourse, best viewed with JavaScript enabled"
707,nvidia-gtc-21-access-technical-training-and-sessions-built-for-developers,"Access technical training and sessions built for developers.
Register for Free - GTC 2022: #1 AI ConferencePowered by Discourse, best viewed with JavaScript enabled"
708,bluefield-2-es-platform-software,"Mellanox BlueField-2 A1 BL1 V1.1
ERROR: Failed to load BL2R firmware.https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/Supported+Platforms+and+Interoperability
it says it have development keys.  but it don’t provide the firmware…
how to get.Powered by Discourse, best viewed with JavaScript enabled"
709,virtual-function-setup-failed,"$ sudo mlxreg -d /dev/mst/mt41686_pciconf0.1 --reg_id 0xc007 --reg_len 0x40 --indexes “0x0.0:32=0x80000000” --yes --set “0x4.0:32=0x1”-E- Failed send access register: ME_ICMD_OPERATIONAL_ERROR
When I am trying to set VF to be trust mode, this error occurs, any idea for solution?Do you want to continue ? (y/n) [n] : y
Sending access register…
-E- Failed send access register: ME_ICMD_OPERATIONAL_ERROR
This is a full view.I have a post with the same question. It has been answered here:Powered by Discourse, best viewed with JavaScript enabled"
710,using-libvma-with-gpudirect-rdma,"Hello, I am trying to get GPUDirect RDMA working with libVMA (distributed as part of MLNX_OFED) and am running into issues due to a lack of code examples.I’ve created a CUDA context and allocated device memory using cuMemAlloc() and want to put UDP packets directly into that device memory. However when I use libVMA’s extra APIs to register that memory with the socket’s rings (using ‘register_memory_on_ring’, and ‘vma_add_ring_profile’), ‘recv’ function calls stop working and I can see via ‘vma_stats’ that it’s timing out on receiving a packet.Am I missing a step here or fundamentally misunderstanding how to integrate CUDA memory into libVMA? If anyone has example code for specifically using libVMA with CUDA offload I’d be interested.According to this post (Mellanox Interconnect Community), what I’m trying to do should be possible, but I’m not sure where to go from here.Thank you for your time!I don’t think VMA has ready to use API let you register GPU memory.Instead you can use RAW THE QP work with socket packet, then use gpudirect register MR.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
711,sx6036-with-vlans-pfc-not-working,"I have followed the guide almost letter to letter, but somehow, I can’t establish a loseless link. It’s all fine and nice with “global flow control”, but PFC doesn’t seem to be working.I see the receiver-host sending pause frames ( tx_pause_prio_3: 49732) and those frames being received by switch ( 66498 pause packets), and then pause packets generated on switch’s sender-host interface (24309 pause packets), but when checking sender’s linux interface stats, the pause packets counter is zero (rx_pause_prio_3: 0). I’m also seeing ( 93970 discard packets) on RX part of sender-host switch interface counter output.I’m highly perplexed on what is wrong with all this setup… I’m using standard EL7 driver, using vlans with egress-qos-map configured. pfc*x params are set to 0x08 (and the counters output above was all fetched from prio 3 – thus the config was supposed to be correct).It works totally as expected when used with global flow control – I see pause frames being sent and received as expected on both linux hosts.Adapters are connectX-3 pro.Another thing – when executing tcp_wrap as per docs, I’m getting:Running stock EL7 kernel. Is this expected?Have tested with several adapters, it seems this is happening on connectX-3 Pro only, HP_1370110017 model. It works on regular connectX-3 (HP model, too). Have tried several firmwares, different kernels, installing mellanox OFED, yet still rx_pause_prio_* counters are zero, despite switch sending pause packets like crazy. Is this some kind of problem with hardware? How come entire 3Pro line is defective in this regard guys? Any workarounds? Much appreciated. Thanks.Hello!You’ve done some good testing/validation so far and have come across some behavior that is both reproducible and seems like it may be specific to a given hw/sw release of the product.At this point the recommendation would be to open a support case so that our support engineers can help you triage this further at a detailed level.Please open up a support ticket here so we can dig more deeply into this:https://support.mellanox.com/s/Thanks!What can be said… After spending several days troubleshooting this and after updating to latest available version (3.6.8010), I was able to get it going. But this thing is SO fragile, I can’t even stress enough how fragile it is. You have to execute config commands in specific sequence in order to get ports going on switch side.It’s not about adapters, it’s about the switch!Thanks everyone.Well, you also have to reboot the adapters after you had configured the PFC on corresponding switch ethernet interface. This is weird, but it worked for us.Powered by Discourse, best viewed with JavaScript enabled"
712,can-i-closed-icrc-computing,"Hi, I want to verify something, but I need to close icrc computing? Does it have some solutions to close icrc computing?Hello Liu,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on your request, unfortunately there is no method of disabling iCRC validation with RoCE v2, as this is an InfiniBand and RoCE Specification requirement, which we do not have control over.Thank you and regards,~NVIDIA Networking Technical SupportWould it be conceivable to add the “feature” in a next release? Likewise TCP checksum is part of the TCP standard but there is an option to not drop the TCP frames, or simply don’t compute checksum. Research wise this is an interesting feature.Also, note this is the third post asking for this :)NVIDIA Developer ForumsNVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
713,random-delays-on-two-back-to-back-connected-systems-with-connectx-5-2x100g-cards,"Dear Mellanox communitywe observe random delays on two back-to-back connected systems with ConnectX-5 2x100G cardsexample(long stretch of normal delay; excessive delay 10-100 ms, sometimes lasting very short, sometimes for minutes; again normal delay,no losses)[1630922510.721460] 64 bytes from 192.168.1.2: icmp_seq=36406 ttl=64 time=0.025 ms[1630922510.737460] 64 bytes from 192.168.1.2: icmp_seq=36407 ttl=64 time=0.024 ms[1630922510.783855] 64 bytes from 192.168.1.2: icmp_seq=36408 ttl=64 time=30.4 ms[1630922510.783861] 64 bytes from 192.168.1.2: icmp_seq=36409 ttl=64 time=14.4 ms[1630922510.783887] 64 bytes from 192.168.1.2: icmp_seq=36410 ttl=64 time=0.018 ms[1630922510.797453] 64 bytes from 192.168.1.2: icmp_seq=36411 ttl=64 time=0.021 mssystem: Supermicro AS-1113S-WN10RT, 256GB RAMCPU: AMD EPYC 7702P 64-CoreNICs MT27800 Family [ConnectX-5 2x100G], 2 cardscable: 100G-CR4 MLX (DAC)driver: mlx5_coreversion: 5.4-1.0.3 (newest as of writing this)firmware-version: 16.31.1014 (MT_0000000012) (newest as of writing this)kernel/OS: 5.4.0-81-generic #91~18.04.1-Ubuntu SMP (newest as of writing this, for ubuntu 18.04 LTS HWE)CPU no lower than C1 power statestorage 2x 8TB samsung:nvme:PM1733:2.5 + 2x 256G Micron_2200 (RAID1, boot)The systems host OSDs for a very lightly loaded test ceph cluster, (storage access + storage replication is of order of 50-100Mb/s currently), CPU 0-1%, 10VMs connecting to it from external hypervisior hosts, mostly idle)One NIC is connected back-to-back to the other host via plain L2/L3 (no vlan, no bonding, default parameters) for the purpose of this test and we see delays (also) there.The other ConnectX-5 card (actually used for ceph) is connected to a pair of dell S4048 in VLT/LACP mode (which then connects via eVLT to another rack with the same config). NICs neogtiate 40G which bacause of dellS4048-ON offers 40G only.We observe the excessive delay to happen on both cards (ceph and back to back), more or less at the same time, so we believe we can take the switches and the bonding out of the equation for now.kern.log, syslog, journalctl do not show anything around the time of the event (and no other errors during boot etc.)topology (fixed font)host nvme01 host nvme02enp129s0f0----enp129s0f0 (back to back connection, we also see excessive delay here)enp129s0f1----enp129s0f1host nvme01 host nvme02enp129s0f0 (bond0) — (vlt) dell4048-01 --evlt-- dell 4048-03 (vlt) – (bond0) enp129s0f0enp129s0f1 (bond0) — (vlt) dell4048-02 --evlt-- dell 4048-04 (vlt) – (bond0)enp129s0f0Thank you and regardsPiotrek ZHi Piotrek Z,There are lots of issue can cause this problem ,.You’d better collect a sysinfo-snapshot and send mail toNvidia Support Admin networking-support@nvidia.comThanks,Suo​Hi Suo,we spent quite some time on researching it, trying system power options, card power options, buffers, offload etc. Ultimately, one of the colleagues after several sessions with perf and stap narrowed it down to large time in IO virtual address alocation (alloc_iova, side effect was also slow write on NVMe). The suggestion was to disable iommu.GRUB_CMDLINE_LINUX_DEFAULT=“amd_iommu=off”After this change, the systems behave much better, after several days of pinging every 0.01s we found literally 1 with delay >1ms and rest of the statistics looking OK31502327 packets transmitted, 31502326 received, 0% packet loss, time 505214691msrtt min/avg/max/mdev = 0.021/0.054/3.989/0.018 msWe have also tested almost identical system but with Ubuntu 20.04.2 LTS and saw no problems.BestPiotrek ZHi Suo,For reference find the perf flamegraph below:
flame878×787 183 KB
as is evident from the famegraph, alloc_iova is eating a lot of CPU time spinlocking.A systemtap histogram reveals that some IO virtual address allocations would take up to 17ms(!).We have observed round trip delays of over 200ms.Pass 1: parsed user script and 480 library scripts using 128112virt/98420res/5960shr/92812data kb, in 240usr/20sys/250real ms.Pass 2: analyzed script: 4 probes, 3 functions, 1 embed, 3 globals using 198212virt/169484res/6848shr/162912data kb, in 810usr/100sys/910real ms.Pass 3: translated to C into “/tmp/stapU5aTxv/stap_8402ca207afb505112b5a6b5969166c8_2884_src.c” using 198212virt/169812res/7168shr/162912data kb, in 10usr/80sys/87real ms.Pass 4: compiled C into “stap_8402ca207afb505112b5a6b5969166c8_2884.ko” in 2030usr/280sys/2190real ms.Pass 5: starting run.Cpu 110 was already in that function?Cpu 112 was already in that function?Duration min:0us avg:57us max:17490us count:3920Duration (us):value |-------------------------------------------------- count0 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 23421 |@@@@@@@@@@ 4772 |@@@@@@@ 3594 |@@@@@@ 2868 |@@@@ 19516 |@@ 11932 |@ 8064 | 33128 | 10256 | 2512 | 01024 | 12048 | 14096 | 38192 | 816384 | 432768 | 065536 | 0WARNING: Number of errors: 0, skipped probes: 123Pass 5: run completed in 20usr/110sys/2646real ms.As mentioned by Piotr, disabling amd_iommu resolved this behaviour.In our other AMD Epyc setup, running kernel 5.11.0-25-generic #27~20.04.1-Ubuntu SMP we have not observed this issue.Regards,Borgert van der KluitPowered by Discourse, best viewed with JavaScript enabled"
714,computex-nvidia-and-arm-s-data-center-ambitions-enabled-by-taiwan,"https://buzzorange.com/techorange/en/2021/06/04/computex-nvidia-arm-taiwan/Hi all! I saw this commentary on Nvidia & Arm’s similar data center vision at Computex. Instead of posing a technical question, I’d like to know how others perceive the role of CPUs in future cloud computing and how Nvidia can expand into embedded high performance computing for the edge market. Thanks in advance for the input!Powered by Discourse, best viewed with JavaScript enabled"
715,python-udp-packets-not-showing-on-orin-board,"I have a very simple python script that sets up a udp socket and reads a single packet in. I have this script running in windows but when I try to test on my orin board no packets are shown in python. I connect to a udp packet generator through ethernet then set the static IP for the ethernet port. I am able to ping the IP and I can bind to the port in python but it shows that there are no packets coming through. I also have wireshark running and I can verify that UDP packets are being sent to that IP & Port. I’m not sure how to troubleshoot this.hi soph44leaThis a Orin issue, could you please kindly recreate a new topic to DRIVE AGX Orin - NVIDIA Developer ForumsThank you
Meng, ShiPowered by Discourse, best viewed with JavaScript enabled"
716,windows-failed-to-initialise-mellanox-mcx354a-fcbt,"Hello all,i own two HP 649281-B21/Mellanox MCX354A-FCBT and run it on two MSI Z490 Mainboards.Both boards supports PCIe3.0 4x slots via PCH intel chipset.One system run with Server 2019 Essentials and the other one is Windows 10 Pro.Both Machines working so far but i get sporadic problems at system starts and the cards show error code 43 in device manager.I would say 10 starts good and 1 fail. After reboot the problem is gone and the cards working very good again. The problem occur at both system in the same way.I am using the last mainboard bios, HPE firmware 2.42 and last mellanox drivers for Server 5.50.54000 / Client 5.50.53000. Have set both cards to HCE Port Typ “ETH”Windows show me this errors in event logging when the problem occurNative_3_0_0: Execution of FW command failed. op 0xfff, status 0x1, errno -5, token 0xffff, in_modifier 0x100, op_modifier 0, in_param 229ff000.Native_3_0_0: MAP_FA command failed with error -5. The adapter card is non-functional. Most likely a FW problem. Please burn the last FW and restart the mlx4_bus driver.Native_3_0_0: Driver startup failed because the hca could not be initialized.In fact the problem occur simply at bootup’s i suppose an issue with BIOS PCIe assignment or driver/chipset communication at OS level.I am not so skilled with FW tweaking at the Mellanox cards. Maybe there are several options to adapt the FW before flashing to the card.May it can help to disable some features which are not supported at the Z490 Boards (for example “SR-IOV”) !?!@Martijn van Breugel​I used the lastest HPE Firmware 2.42 ( link you posted) already when the problem occur.I did a swap to “fw-ConnectX3-rel-2_42_5000-MCX354A-FCB_A2-A5-FlexBoot-3.4.752.bin” to ensure there is no problem with the OEM stuff.Now with Mellanox firmware the behavior is 100% the same.It run well for a couple of reboots than the card not initialise again with error code 43 in device manager. Another reboot solve the issue again. This happen at Server & Client the same way. I using the lastest drivers for both machines Server/Client.It seams the SR-IOV is not the reason for that.After firmware burn i always reset the mlxconfig.Both FW having a little difference in defaults only ( yes, i set the linktype to ETH because no IB is used in this mesh):fw-ConnectX3-rel-2_42_5000-649281-B21_Bx-CLP-8025-FlexBoot-3.4.752_windowsDevice type: ConnectX3Device: mt4099_pciconf0Configurations: Next BootSRIOV_EN True(1)NUM_OF_VFS 16LINK_TYPE_P1 ETH(2)LINK_TYPE_P2 ETH(2)LOG_BAR_SIZE 5fw-ConnectX3-rel-2_42_5000-MCX354A-FCB_A2-A5-FlexBoot-3.4.752.binDevice type: ConnectX3Device: mt4099_pciconf0Configurations: Next BootSRIOV_EN False(0)NUM_OF_VFS 8LINK_TYPE_P1 ETH(2)LINK_TYPE_P2 ETH(2)LOG_BAR_SIZE 3Windows and Intel system drivers are all up to date. Setting of mellanox drivers are default… i have no more ideas to get this solved 😥Maybe it points out a bad incompatibility with the Z490 hardware.I wondering why the problem just appear in sporadic nature…Hello Max,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, you want to disable SR-IOV on the adapter as it is not supported on the systemboard.For disabling SR-IOV on the adapter, make sure you install the latest MFT Tools for Windows → https://www.mellanox.com/downloads/MFT/WinMFT_x64_4_16_1_9.exeThrough the command-line window you can disable SR-IOV through the following syntax:c:\Program Files\Mellanox\WinMFT> mst.exe statusc:\Program Files\Mellanox\WinMFT> mlxconfig.exe -d  set SRIOV_EN=0After this you need to reboot the node.If this does not resolve the issue, please reach out to HPE Support as the adapters you have are HPE Mellanox OEM adapters. HPE maintains the f/w for those adapters.Thank you and regards,~NVIDIA Networking Technical SupportHello Martijn, just looking for a solution to tweak the card firmware to get better compatibility with my Z490 hardware. The SR-IOV is just an example.Along the event log i found this reoccuring messages:Mellanox ConnectX-3 VPI (MT04099) Network Adapter (PCI bus 3, device 0, function 0): SR-IOV cannot be enabled because FW does not support SR-IOV. In order to resolve this issue please re-burn FW, having added parameters related to SR-IOV support.Native_3_0_0: EXT_QP_MAX_RETRY_LIMIT/EXT_QP_MAX_RETRY_PERIOD registry keys were requested by user but FW does not support this feature. Please upgrade your firmware to support it. For more details, please refer to WinOF User Manual.I am not sure if this can be the reason that the cards are sporadic not initialise…@Max Mayer​Changing the PSID on the adapter, will void any support and warranty. Why can you not burn the adapter with the HP f/w → https://downloads.hpe.com/pub/softlib2/software1/pubsw-linux/p1465926780/v147811/fw-ConnectX3-rel-2_42_5000-649281-B21_Bx-CLP-8025-FlexBoot-3.4.752.tgz. The f/w should by default have SR_IOV enabled. Else you have an option to go into the Flexboot Menu and enable SR-IOV (Change Virtualization Mode to on)But I would not change the PSID of the adapter. It will not benefit you.Thank you,~Martijn@Max Mayer​Unfortunately, we cannot test every system board which comes on the market. Based on all the tests you did and being on the latest driver and f/w, we need to assume that this is more a compatibility problem with the system board,. Any change you are able to test in a different system board manufacturer? Also I would recommend to flash the adapter back to the original OEM PSID as it voids warranty and support.Thank you,~MartijnHi Martijn, understand. Yes it seems to be a compatibility issue and I hope that MSI will release new UEFI BIOS and Intel chipset driver soon…In my case i have 2 MSI Z490 Boards with the same problems.The initializing issue (error code 43 in device manager) is not a special/unique thing to me.I found also users reported the same problems with MCX311A-XCAT cards and Windows OS.Let’s see what the Mainboard manufacturer with update in the future.But thanks for support Martijn…Powered by Discourse, best viewed with JavaScript enabled"
717,set-up-doca-apsh-system,"Hey,
I am using DOCA SDK on my Bluefield and trying to set up doca_apsh_system struct to read memory on the host.
I write the following code according to the apsh documentation:The code crashes at doca_apsh_sys_mem_region_set with the error:./lib/doca_apsh/libs/iSnap/src/mema.c:145:mema_open_device: i: 1, vport:0!!
…/lib/doca_apsh/libs/iSnap/src/mema.c:228:mema_get_introspection_mkey(): mema_introspection_mkey_access_allowed set to 1
introspection mkey 7936
mema_open_device: finished gracefully
Allocating HP
Huge page virtual address(0x4000000000000), size(0x200000)
AH: CQ handler = 4.
dma_key is 0x10e42…/lib/doca_apsh/libs/iSnap/src/mem_pool.c:37:create_machine_mem_regions(): create_machine_mem_regions: Errorr while opening the file, make sure that mem_regions resided in current working directory fp = (nil).What does the error mean? I think it is connected to the doca_apsh_sys_mem_region_set function. I read that I need to give the path to the file generated by the doca_system_mem_region tool, but I can’t find this tool anywhere on the host or on the Bluefield.
What should I do?We met the same issue, is there any progress about this issue?Powered by Discourse, best viewed with JavaScript enabled"
718,how-to-install-ofed-4-6-1-on-centos-7-8,"Dear Mellanox community.I’m attempting to set up the Cisco TRex traffic generator (https://trex-tgn.cisco.com/trex/doc/trex_faq.html) with ConnectX-4 and ConnectX-5 adapters.TRex has a specific dependency on Mellanox OFED version 4.6.1 (https://trex-tgn.cisco.com/trex/doc/trex_appendix_mellanox.html)I have a CentOS 7.8 box (x86_64). TRex recommends using CentOS 7.6, but I’m unable to downgrade to it from 7.8.Unfortunately, there isn’t a currently packaged version of OFED 4.6.1 (x86_64) for CentOS 7.8 at Linux InfiniBand DriversIs it even wise to try to install OFED 4.6.1 on CentOS 7.8? I have attempted something likesudo ./mlnxofedinstall --with-mft --with-mstflint --dpdk --upstream-libs --distro rhel7.6 --add-kernel-supportbut it failed. The error in the log file is the following:Running rpmbuild --rebuild --define ‘_topdir /tmp/MLNX_OFED_LINUX-4.6-1.0.1.1-3.10.0-1127.19.1.el7.x86_64/mlnx_iso.41279/OFED_topdir’ --define ‘_sourcedir %{_topdir}/SOURCES’ --define ‘_specdir %{_topdir}/SPECS’ --define ‘_srcrpmdir %{_topdir}/SRPMS’ --define ‘_rpmdir %{_topdir}/RPMS’ --nodeps --define ‘_dist .rhel7u6’ --define ‘configure_options --with-core-mod --with-user_mad-mod --with-user_access-mod --with-addr_trans-mod --with-mlxfw-mod --with-mlx4-mod --with-mlx4_en-mod --with-mlx5-mod --with-ipoib-mod --with-innova-flex --with-srp-mod --with-iser-mod --with-e_ipoib-mod --with-isert-mod’ --define ‘KVERSION 3.10.0-1127.19.1.el7.x86_64’ --define ‘K_SRC /lib/modules/3.10.0-1127.19.1.el7.x86_64/build’ --define ‘_prefix /usr’ /tmp/MLNX_OFED_LINUX-4.6-1.0.1.1-3.10.0-1127.19.1.el7.x86_64/mlnx_iso.41279/MLNX_OFED_SRC-4.6-1.0.1.1/SRPMS/mlnx-ofa_kernel-4.6-OFED.4.6.1.0.1.1.ga2cfe08.src.rpmFailed to build mlnx-ofa_kernel 4.6 RPMI can attach the more detailed debug log file if it might help.Please let me know how to proceed as the specific dependency of versions across OFED and CentOS and the traffic generator is blocking me from getting off the ground.Thank you in advance for any guidance!SrinivasHi Srinivas Narayana,MLNX_OFED 4.6 does not support Centos 7.8.You can use MLNX_OFED 5.0-2.1.8.0, which supports Centos 7.8It’s available to download from our website:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)I have a system with MLNX_OFED 5.0-2.1.8.0 & Trex, and it’s working well.Best Regards,ChenDear Chen,I was indeed able to run TRex v2.86 on CentOS 7.8 by installing the OFED version 5.0-2.1.8.0.sudo ./mlnxofedinstall --with-mft --with-mstflint --dpdk --upstream-libs --add-kernel-supportThanks so much for helping out with the number of a compatible OFED version! I’m much grateful.SrinivasPowered by Discourse, best viewed with JavaScript enabled"
719,mlnx-ofed-linux-5-7-1-mlx5-2-create-qp-pid-19774-create-qp-type-2-failed,"Hi,We are experiencing errors when trying to run large scale MPI application, the application is hanging while from the dmesg log, we cloud find:
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19774): Create QP type 2 failed
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19773): Create QP type 2 failed
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19774): Create QP type 2 failed
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19773): Create QP type 2 failed
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19774): Create QP type 2 failed
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19773): Create QP type 2 failed
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19774): Create QP type 2 failed
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19773): Create QP type 2 failed
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19774): Create QP type 2 failed
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19773): Create QP type 2 failed
[Sun Oct 30 14:28:32 2022] infiniband mlx5_2: create_qp:3206:(pid 19774): Create QP type 2 failed
[Sun Oct 30 14:29:32 2022] rcu: INFO: rcu_sched detected stalls on CPUs/tasks:
[Sun Oct 30 14:29:37 2022] smp: csd: Detected non-responsive CSD lock (#1) on CPU#64, waiting 5000000011 ns for CPU#13 do_kernel_range_flush+0x0/0x52(0xff136f217f92fb40).
[Sun Oct 30 14:29:37 2022] rcu:         13-…!: (1 GPs behind) idle=252/0/0x1 softirq=40883/40892 fqs=1016
[Sun Oct 30 14:29:37 2022]      (detected by 13, t=5011 jiffies, g=124653, q=4182926)
[Sun Oct 30 14:29:37 2022] smp:         csd: CSD lock (#1) unresponsive.
[Sun Oct 30 14:29:37 2022] NMI backtrace for cpu 13
[Sun Oct 30 14:29:37 2022] CPU: 13 PID: 0 Comm: swapper/13 Kdump: loaded Tainted: G           OE     5.4.17-2136.310.7.el7uek.x86_64 #2
[Sun Oct 30 14:29:37 2022] Hardware name: Oracle Corporation ORACLE SERVER X9-2c/TLA,MB TRAY,X9-2c, BIOS 66040600 07/23/2021
[Sun Oct 30 14:29:37 2022] Call Trace:
[Sun Oct 30 14:29:37 2022]  
[Sun Oct 30 14:29:37 2022] smp: csd: Re-sending CSD lock (#1) IPI from CPU#64 to CPU#13
[Sun Oct 30 14:29:37 2022]  dump_stack+0x6d/0x8d
[Sun Oct 30 14:29:37 2022]  nmi_cpu_backtrace+0x9f/0xa1
[Sun Oct 30 14:29:41 2022]  ? lapic_can_unplug_cpu+0xb0/0xa9
[Sun Oct 30 14:29:41 2022]  nmi_trigger_cpumask_backtrace+0x80/0x13d
[Sun Oct 30 14:29:41 2022]  arch_trigger_cpumask_backtrace+0x19/0x1f
[Sun Oct 30 14:29:41 2022]  rcu_dump_cpu_stacks+0x9a/0xce
[Sun Oct 30 14:29:41 2022]  rcu_sched_clock_irq+0x815/0x83a
[Sun Oct 30 14:29:41 2022]  ? tick_sched_do_timer+0x70/0x6b
[Sun Oct 30 14:29:41 2022]  update_process_times+0x28/0x4c
[Sun Oct 30 14:29:41 2022]  tick_sched_handle+0x2c/0x62
[Sun Oct 30 14:29:41 2022]  tick_sched_timer+0x3c/0x72
[Sun Oct 30 14:29:41 2022]  __hrtimer_run_queues+0x106/0x272
[Sun Oct 30 14:29:41 2022]  hrtimer_interrupt+0x116/0x244
[Sun Oct 30 14:29:41 2022]  smp_apic_timer_interrupt+0x6f/0x13f
[Sun Oct 30 14:29:41 2022]  apic_timer_interrupt+0xf/0x14
[Sun Oct 30 14:29:41 2022]  The site is configured with:The  ibv_devinfo -v is given below output:hca_id: mlx5_2
transport:                      InfiniBand (0)
fw_ver:                         16.29.1436
node_guid:                      043f:7203:00e2:f322
sys_image_guid:                 043f:7203:00e2:f322
vendor_id:                      0x02c9
vendor_part_id:                 4121
hw_ver:                         0x0
board_id:                       ORC0000000003
phys_port_cnt:                  1
max_mr_size:                    0xffffffffffffffff
page_size_cap:                  0xfffffffffffff000
max_qp:                         131072
max_qp_wr:                      32768
device_cap_flags:               0xed721c36
BAD_PKEY_CNTR
BAD_QKEY_CNTR
AUTO_PATH_MIG
CHANGE_PHY_PORT
PORT_ACTIVE_EVENT
SYS_IMAGE_GUID
RC_RNR_NAK_GEN
MEM_WINDOW
XRC
MEM_MGT_EXTENSIONS
MEM_WINDOW_TYPE_2B
RAW_IP_CSUM
MANAGED_FLOW_STEERING
Unknown flags: 0xC8400000
max_sge:                        30
max_sge_rd:                     30
max_cq:                         16777216
max_cqe:                        4194303
max_mr:                         16777216
max_pd:                         16777216
max_qp_rd_atom:                 16
max_ee_rd_atom:                 0
max_res_rd_atom:                2097152
max_qp_init_rd_atom:            16
max_ee_init_rd_atom:            0
atomic_cap:                     ATOMIC_HCA (1)
max_ee:                         0
max_rdd:                        0
max_mw:                         16777216
max_raw_ipv6_qp:                0
max_raw_ethy_qp:                0
max_mcast_grp:                  2097152
max_mcast_qp_attach:            240
max_total_mcast_qp_attach:      503316480
max_ah:                         2147483647
max_fmr:                        0
max_srq:                        8388608
max_srq_wr:                     32767
max_srq_sge:                    31
max_pkeys:                      128
local_ca_ack_delay:             16
general_odp_caps:
ODP_SUPPORT
ODP_SUPPORT_IMPLICIT
rc_odp_caps:
SUPPORT_SEND
SUPPORT_RECV
SUPPORT_WRITE
SUPPORT_READ
SUPPORT_SRQ
uc_odp_caps:
NO SUPPORT
ud_odp_caps:
SUPPORT_SEND
xrc_odp_caps:
SUPPORT_SEND
SUPPORT_WRITE
SUPPORT_READ
SUPPORT_SRQ
completion timestamp_mask:                      0x7fffffffffffffff
hca_core_clock:                 78125kHZ
raw packet caps:
C-VLAN stripping offload
Scatter FCS offload
IP csum offload
Delay drop
device_cap_flags_ex:            0x30000055ED721C36
RAW_SCATTER_FCS
PCI_WRITE_END_PADDING
Unknown flags: 0x3000004100000000
tso_caps:
max_tso:                        262144
supported_qp:
SUPPORT_RAW_PACKET
rss_caps:
max_rwq_indirection_tables:                     16777216
max_rwq_indirection_table_size:                 256
rx_hash_function:                               0x1
rx_hash_fields_mask:                            0x800000FF
supported_qp:
SUPPORT_RAW_PACKET
max_wq_type_rq:                 8388608
packet_pacing_caps:
qp_rate_limit_min:      1kbps
qp_rate_limit_max:      100000000kbps
supported_qp:
SUPPORT_RAW_PACKET
tag matching not supportedCan anyone help on it?Hi ,Please note that we had several QP bugs in older firmware versions , that were fixed in newer versions .
Therefore I recommend contact Oracle Support (since its OEM card) so they can provide you with
firmware 16.32.XXXX or newerThanks,
SamerPowered by Discourse, best viewed with JavaScript enabled"
720,rhel9-support,"Is there any eastimate for when the Mellanox OFED driver will support RedHat 9?  Also, will Rocky Linux and/or AlmaLinux be officially supported at that time, now that CentOS has been repurposed?Hi twkeayrq,RedHat 9 is already supported. You can find it here: Mellanox OFED (MLNX_OFED) Software: End-User Agreement | NVIDIA DeveloperRocky and Alma Linux are also supported. Find them under “Community OS”.Good luck,
Yogev, Nvidia SupportPowered by Discourse, best viewed with JavaScript enabled"
721,i-have-2-rdma-nics-installed-on-this-server-ideally-two-netdevs-per-port-for-mlx5-0-example-ens224-ens225-each-with-their-own-mac-addresses-but-they-both-show-up-under-a-single-ens224,"I have 2 RDMA nics installed on this server. Ideally two netdevs per port for mlx5_0, example: ens224, ens225 each with their own mac addresses, but they both show up under a single “ens224”.topology is :mlx5_0_port 1-----rdma(server) -----mlx5_0_port 2ibdev2netdevmlx5_0 port 1 ==> ens224 (Down)mlx5_0 port 2 ==> ens224 (Down)ibstatCA ‘mlx5_0’CA type: MT4119Number of ports: 2Firmware version: 16.28.2006Hardware version: 0Node GUID: 0x98039b030067c63aSystem image GUID: 0x98039b030067c63aPort 1:State: ActivePhysical state: LinkUpRate: 100Base lid: 0LMC: 0SM lid: 0Capability mask: 0x00010000Port GUID: 0x9a039bfffe67c63aLink layer: EthernetPort 2:State: DownPhysical state: DisabledRate: 100Base lid: 0LMC: 0SM lid: 0Capability mask: 0x00010000Port GUID: 0x0000000000000000Link layer: Ethernetibdev2netdev -vmlx5_0 (MT4119 - MCX516A-CCAT) CX516A - ConnectX-5 QSFP28 fw 16.28.2006 port 1 (ACTIVE) ==> ens224 (Up)0000:13:00.0 mlx5_0 (MT4119 - MCX516A-CCAT) CX516A - ConnectX-5 QSFP28 fw 16.28.2006 port 2 (DOWN ) ==> ens224 (Up)0000:13:00.0 mlx5_1 (MT4119 - MCX516A-CCAT) CX516A - ConnectX-5 QSFP28 fw 16.28.2006 port 1 (DOWN ) ==> ens256 (Down)0000:1b:00.0 mlx5_1 (MT4119 - MCX516A-CCAT) CX516A - ConnectX-5 QSFP28 fw 16.28.2006 port 2 (DOWN ) ==> ens256 (Down)lspci | grep -i mellanox13:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]1b:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]Hi,1.Try to restart the driver/etc/init.d/openibd restart2.Check the udev rules for interface naming or check this procedure to change interface’s nameshttps://community.mellanox.com/s/article/howto-change-network-interface-name-in-linux-permanentlyRegardsMarcPowered by Discourse, best viewed with JavaScript enabled"
722,doca-sdk-documentation,"Sign up for access to the release notes, programming guides, API references and more:NVIDIA DOCA SDK Early Access Early access to the DOCA software framework is available now. Please login or create an account using an active email address. For updates on DOCA, please opt-in to receive Enterprise and Developer emails from NVIDIA. We...Even at the very recent GTC’21, it was announced that DOCA is already available while NVIDIA has just confirmed the upcoming BF-3 DPUs.I already signed up for early access for DOCA, but still no response. Does this even exist? What is the average waiting time for getting approval done?@cslev Doca 1.0 is included in the latest BF2 image.So sorry for this approval delay. I checked this morning and it looks like you’ve been approved. Have fun!Hi @jubetz,
I am a research scientist at IIT Bombay and we have procured Bluefield-1 SmartNICs. We have been waiting for the DOCA SDK for quite some time. The latest DOCA SDK is for Bluefield-2 NICs.
Could you help us with the DOCA SDK for Bluefiled-1 NICs?Hey @rinku, BF1 will not support DOCA. BF1 is missing some hardware acceleration components (Regex, GPU, etc), that DOCA, as a whole, provides programming methods for. BF1 can make use of some of the components that DOCA consists of, such as DPDK and SPDK. It is still a highly programmable and flexible NIC, but does not have all of the same capabilities that DOCA, as a whole, aims to provide.Thanks for your response, @jubetz. If I understand this correctly, BF1 does not have P4 support too. Could you provide a link that states the features supported by BF1?I know it might be a cliché, but I don’t think BF1 supports P4. According to the brochure it only supports DPDK.
I am not even sure at the moment, whether BF2 supports P4 :) There is not much information available on how P4 is supported and could be used.Hello, I’m a graduate student at Institute of Computation Technology and I’m working with my graduation thesis. I have signed up for early access for DOCA and I also have bought one of your DPU products. I need the DOCA SDK for my graduation thesis experiment, so could you please approve my application soon ?Powered by Discourse, best viewed with JavaScript enabled"
723,bright-cluster-manager-simulator,"Hello, Is there a Bright cluster manager simulator available to download/use?To simulate a Bright cluster, you can just spin it up in one or more VMs. E.g. you could use VirtualBox.
It’s pretty straightforward but there’s a KB article that shows all the steps (note that in the most recent version of VirtualBox the screens may look slightly different).
You can also easily spin up a Bright cluster in a public cloud or VMware ESX by following the instructions in the Cluster on Demand (COD) section of this manual.
And lastly, here are some files that let you do a Vagrant driven Bright add-on installation.Cluster on DemandThanks.
Where can i get a copy of the ISO images? The vagrant link references "" bright9.2-rocky8u5.isoandbright9.2-rocky8u5.iso.md5` for an iso install.""Download Bright Cluster Manager
You’ll need a product key, which you may already have. If you don’t have one, you can purchase it, get an eval key by contact a Sales rep, or getting an Easy8 key from our Customer Portal. Easy8 keys are free and can be used for up to 8 nodes.Powered by Discourse, best viewed with JavaScript enabled"
724,does-pcie-prefetch-memory-impact-the-throughput,"We are using the ConnectX-6 NIC and the configuration is below,
Ethernet controller: Mellanox Technologies MT28908 Family [ConnectX-6]
Subsystem: Mellanox Technologies Device 0028
Flags: bus master, fast devsel, latency 0, IRQ 182, NUMA node 1
Memory at a0000000 (64-bit, prefetchable) [size=32M]
Expansion ROM at 9d000000 [disabled] [size=1M]
Capabilities: [60] Express Endpoint, MSI 00
Capabilities: [48] Vital Product Data
Capabilities: [9c] MSI-X: Enable+ Count=64 Masked-
Capabilities: [c0] Vendor Specific Information: Len=18 <?>
Capabilities: [40] Power Management version 3
Capabilities: [100] Advanced Error Reporting
Capabilities: [150] Alternative Routing-ID Interpretation (ARI)
Capabilities: [1c0] #19
Capabilities: [230] Access Control Services
Capabilities: [320] #27
Capabilities: [370] #26
Capabilities: [420] #25
Kernel driver in use: mlx5_core
Kernel modules: mlx5_coreWe have measured the RDMA throughput by varying the message size.
The results show that when the message size is more than 32 MB (i.e. 33, 34, 35 MB …), the throughput drops by ~50 Gbs.So would like to know Is the prefetchable memory 32 MB which is listed in the above configuration has any impact on RDMA throughput.
If yes, then Is there any way to modify or disable it to confirm the impact on throughput?Hello,There is no relation between the bar size and the message size or the RDMA performance/throughput.
The throughput drop is seen due to a different reason, and the fact that both memory and message size is set to 32MB is just a coincidence.
You can confirm that by modifying the PF_LOG_BAR_SIZE firmware parameter and see that it is not impacting performance.
You can modify it with the mlxconfig tool (part of the MFT driver). The default setting is 5 (which is 2^5=32MB).Best Regards,
VikiPowered by Discourse, best viewed with JavaScript enabled"
725,would-running-suse-linux-on-host-be-ok,"I am working with a server that has a BlueField-2 and was set up a while back.
The host runs SUSE linux and some BlueField -2 commands seem to work, but just wondering should I expect difficulties?Powered by Discourse, best viewed with JavaScript enabled"
726,how-do-i-run-performance-tests-between-a-windows-box-with-winof-2-and-a-linux-box-with-mlnx-ofed-linux-5-1-2,"TL;DR: How do I run a performance test between a Linux and a Windows using WinOF-2, without downloading extra non-Mellanox software?We are creating a setup with a Windows box, a Mellanox SN2700 Switch, and a Linux box, where everyone talks IPv6 and will eventually implement RoCE. We can ping between the machines using IPv6 pings (ping -6 and ping6). For this setup, transmission speed is the priority. Right now we are trying to run the performance tests between the two machines but we’ve hit a snag…In the “Getting Started with Connectx-5 … for Windows” article, the performance tests seem to assume that both of the machines will be Windows, and calls for use of the nd_send_bw command. The nd_send_bw command is a Windows-only command, and the nttcp tool has not okayed by management (hoping not to have to get random non-Mellanox tools). The Linux version of the page does not have anything for Performance testing.One option I’m seeing is to use ib_send_bw, but that is not on the Windows box. The Windows box has the WinOF-2 driver and the Mellanox Firmware Tools installed, but it appears that perftest is not installed with WinOF-2 like it is with WinOF, but instead we have mlx5_cmd.exe -PerfTuning.Can I mix mlx5cmd perftuning with ib_send_bw or nd_send_bw with ib_send_bw? Can I get the perftest tool set for WinOF-2? How else could I solve this?Hi Laura,Unfortunately, there are no ROCE/RDMA compatible tools between Windows & Linux.ib_write/read/send_bw from our driver is for Linux only, nd_write/read/send_bw from our driver is for Windows only.Also, as an FYI, we do not perform any performance QA between Linux and Windows.Sophie.Powered by Discourse, best viewed with JavaScript enabled"
727,no-conneciton-between-host-and-dpu-in-embedded-mode,"Hi all.
There’s no network connection between host and dpu after installation in embedded mode.
Here’s my conguration.On host:
pci@0000:3b:00.0  ens1f0        network        MT42822 BlueField-2 integrated ConnectX-6 Dx network controller
pci@0000:3b:00.1  ens1f1        network        MT42822 BlueField-2 integrated ConnectX-6 Dx network controller$ ifconfig ens1f0
ens1f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 172.16.10.1  netmask 255.255.255.0  broadcast 172.16.10.255
inet6 fe80::bace:f6ff:feaf:e842  prefixlen 64  scopeid 0x20
ether b8:ce:f6:af:e8:42  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 99  bytes 9950 (9.9 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0On dpu
pci@0000:03:00.0  enp3s0f0s0      network    MT42822 BlueField-2 integrated ConnectX-6 Dx network controller
pci@0000:03:00.1  pf1hpf          network    MT42822 BlueField-2 integrated ConnectX-6 Dx network controller
virtio@1          tmfifo_net0     network    Ethernet interface
ovsbr2          network    Ethernet interface
ovsbr1          network    Ethernet interface
pci@0000:03:00.1  p1              network    Ethernet interface
ovs-system      network    Ethernet interface
pci@0000:03:00.1  en3f1pf1sf0     network    Ethernet interface
pci@0000:03:00.0  pf0hpf          network    Ethernet interface
pci@0000:03:00.1  enp3s0f1s0      network    Ethernet interface
pci@0000:03:00.0  p0              network    Ethernet interface
oob_net0        network    Ethernet interface$ sudo ovs-vsctl show
3638c123-b7f2-4676-8dca-5b61edb5a59f
Bridge ovsbr2
Port p1
Interface p1
Port pf1hpf
Interface pf1hpf
Port en3f1pf1sf0
Interface en3f1pf1sf0
Port ovsbr2
Interface ovsbr2
type: internal
Bridge ovsbr1
Port p0
Interface p0
Port pf0hpf
Interface pf0hpf
Port ovsbr1
Interface ovsbr1
type: internal
Port en3f0pf0sf0
Interface en3f0pf0sf0
ovs_version: “2.15.1”
ubuntu@localhost:~$ubuntu@localhost:~$ ifconfig enp3s0f0s0
enp3s0f0s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 172.16.10.2  netmask 255.255.255.0  broadcast 172.16.10.255
ether 02:15:78:75:b8:c9  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 6  bytes 516 (516.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0ubuntu@localhost:~$Could someon help?
Thanks.Try assign ip to interfaces?
According to this note Configuring NVIDIA BlueField2 SmartNIC | Better Tomorrow with Computer ScienceThanks for reply.
IP has already been assigned to ens1f0(172.16.10.1, host side) and enp3s0f0s0(172.16.10.2 dpu side)Hey there are the physical interfaces up?That’s the solution.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
728,i-configured-vdpa-and-dpdk-bonding-active-backup-mode-on-connectx-6-dx-slave-0-is-normal-but-failed-to-create-flow-on-slave-1-mlnx-ofed-version-5-4-1-0-3-0,"
816646A9-8032-4f34-BA7D-D9EFD21E101D1752×529 58.6 KB
Hi,You can find information on vDPAhttps://docs.mellanox.com/pages/viewpage.action?pageId=39264792https://docs.mellanox.com/pages/viewpage.action?pageId=39264792#OVSOffloadUsingASAP²Direct-swvdpaVirtIOAccelerationthroughVFRelay(Software&HardwarevDPA)If you still need assistance , please open a support ticket.RegardsMarcPowered by Discourse, best viewed with JavaScript enabled"
729,lag-linux-bonding,"i am using dgx 1v and setup infiniband 4 card by ipoib.as i know infiniband only support active-backup. if i bond when does the rdma work only the active card? or works together with the slave cards?Hi Seungsuk Ryu,IPoIB bonding is exactly the same as Ethernet bonding and it should support all types of bonds.Tested bond types are:mode=0 (Balance Round Robin)mode=1 (Active backup)mode=2 (Balance XOR)mode=4 (802.3ad)and it can be dual port card or 2 single ports cards.In active-backup mode only the active slave’s physical port works (until the the active slave changes).Regards,Cheni have one more question.Is it possible to use RMDA with Infiniband card(Tpye Infiniband) bonded?Hi Chen,I can’t seem to have mode=0 working, only mode=1 at the moment.Can you explain a bit more how this is done? Besides, the following mellanox article states that "" the only meaningful bonding policy in IPoIB is High-Availability (bonding mode number 1, or “active-backup”)"", which get me even more confused.https://community.mellanox.com/s/article/howto-create-linux-bond--lag--interface-over-infiniband-networkPowered by Discourse, best viewed with JavaScript enabled"
730,cant-use-rdma-on-azure-nc24rs-v3,"I tried to use infiniband on Standard_NC24rs_v3 Ubuntu 16.04 , but failed. I installedhttps://www.mellanox.com/downloads/ofed/MLNX_OFED-4.7-3.2.9.0/MLNX_OFED_LINUX-4.7-3.2.9.0-ubuntu18.04-x86_64.tgz from https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/hpc/enable-infinibandWhen I test ib_write_bw, it reports errors.perfkit@pkb-61175e23-0:~$ ib_write_bwRDMA_Write BW TestDual-port : OFF Device : mlx4_0Number of qps : 1 Transport type : IBConnection type : RC Using SRQ : OFFCQ Moderation : 1Mtu : 2048[B]Link type : IBMax inline data : 0[B]rdma_cm QPs : OFFData ex. method : Ethernetlocal address: LID 0x57 QPN 0xb1603 PSN 0x4d16b6 RKey 0x78010800 VAddr 0x007f1b9e28a000remote address: LID 0x6c QPN 0xb943f PSN 0x618c07 RKey 0xa00108b9 VAddr 0x007f0a88ff8000#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]ethernet_read_keys: Couldn’t read remote addressUnable to read to socket/rdam_cmFailed to exchange data between server and clientsperfkit@pkb-61175e23-1:~$ ib_write_bw pkb-61175e23-0RDMA_Write BW TestDual-port : OFF Device : mlx4_0Number of qps : 1 Transport type : IBConnection type : RC Using SRQ : OFFTX depth : 128CQ Moderation : 1Mtu : 2048[B]Link type : IBMax inline data : 0[B]rdma_cm QPs : OFFData ex. method : Ethernetlocal address: LID 0x6c QPN 0xb943f PSN 0x618c07 RKey 0xa00108b9 VAddr 0x007f0a88ff8000remote address: LID 0x57 QPN 0xb1603 PSN 0x4d16b6 RKey 0x78010800 VAddr 0x007f1b9e28a000#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]Completion with error at clientFailed status 12: wr_id 0 syndrom 0x81scnt=128, ccnt=0Failed to complete run_iter_bw function successfullyHello Hao,Thank you for posting your question on the Mellanox Community. We also noticed that you opened a Mellanox Support ticket as you have a valid support contract.We will assist you further through the Mellanox Support ticket opened.Thank you,~Mellanox Technical SupportHello Hao, Martijn,I’d like to ask if you are able to please share the solution which you came up with. I’ve encountered the exactly the same error during this test.I’m using two ConnectX-4 Lx cards (installed on separate machines in the same LAN).Any tips, advice would be much appreciated!Hamedib_send_bw -x 4 192.168.1.2Send BW TestDual-port : OFF Device : mlx5_0Number of qps : 1 Transport type : IBConnection type : RC Using SRQ : OFFTX depth : 128CQ Moderation : 100Mtu : 1024[B]Link type : EthernetGid index : 4Max inline data : 0[B]rdma_cm QPs : OFFData ex. method : Ethernetlocal address: LID 0000 QPN 0x0702 PSN 0xca8817GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:01:03remote address: LID 0000 QPN 0x066a PSN 0xcb0f9cGID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:01:02#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]Completion with error at clientFailed status 12: wr_id 0 syndrom 0x81scnt=128, ccnt=0Powered by Discourse, best viewed with JavaScript enabled"
731,register-now-the-next-nvidia-dpu-virtual-hackathon,"Hurry Up and Register for the next DPU hackathon! It will begin on Wednesday December 8th at 9:00 a.m. PST, and will end on Thursday December 9th at 3:00 p.m. PST.Hurry Up!!The hackathon will begin on Wednesday December 8th at 9:00 a.m. PST, and will end on Thursday December 9th at 3:00 p.m. PST.Powered by Discourse, best viewed with JavaScript enabled"
732,mlx5-core-will-cause-a-leak-of-a-command-resource,"always got follow error.  sw/hw info attachedHi,Thank you for submitting your query on NVIDIA Developer Forum.I would like to request to check the output of "" #cat /proc/cmdline "" to check if the GRUB has the following kernel parameter: “iommu=pt”This parameter is important on systems with AMD CPU. If it doesn’t exist, please modify the GRUB and add the above mentioned parameter  and reboot the system to confirm if issue is resolved.Thanks,
Namrata.there is no iommu=pt. we will try. thanks.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
733,relationship-between-ovs-offloading-and-fdb,"Hello, I am using the ConnectX-5 & ConnectX-6 interface.
I am testing the OVS offloading function in many ways.Today’s inquiry is about the relationship between FDB and OVS offloading.If FDB learning is not possible, it seems that OVS offloading is not possible.
If I specify the output port directly in OVS, (ex. actions=output:“enp1_pf0vf0”) FDB learning is not performed, so it seems that OVS offloading is not performed.Could you check if the OVS offloading function is working normally only when FDB is being learned?What deployment, solution or use case are you referring to when it comes to our MLNX_OFED driver and/or our HCA/FW?
Your post is not clearly defined for a straight answer.
(IE: OVS Offload Using ASAP2 Direct/OVS-DPDK Offload).(Accelerated Switch and Packet Processing)
ASAP2  technology enables packet processing operations to be offloaded from OVS to the hardware of ConnectX eSwitch (OVS forwarding rules to the eSwitch.)
ASAP2 enables the option of adding rules and changing the source/destination IP – which allows the creation of a net/firewall.I am not sure if you had a chance to review our different community articles for these solutions above which might be useful for initial deployment, testing & debugging purposes.Hi, spruitt.I am using ASAP2 direct OVS offload.
Does the “OVS forwarding rule” you mentioned mean OVS FDB?What I want to know is if OVS offload is not possible in a situation where rules are not added to OVS FDB.Powered by Discourse, best viewed with JavaScript enabled"
734,eal-initialization-failed,"hello, i am getting the following message after running the example application. Any ideas ?ubuntu@dpu-h1:/opt/mellanox/doca/examples/application_recognition/src$ /opt/mellanox/doca/examples/application_recognition/bin/doca_application_recognition -a 0000:03:00.0,class=regex -a auxiliary:mlx5_core.sf.4,sft_en=1 -a auxiliary:mlx5_core.sf.5,sft_en=1 -l 0 - 64 – -c /tmp/ar.cdo -p
EAL: Detected 8 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /run/user/1000/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘VA’
EAL: No available hugepages reported in hugepages-32768kB
EAL: No available hugepages reported in hugepages-64kB
EAL: No available hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL: Couldn’t get fd on hugepage file
EAL: error allocating rte services array
EAL: FATAL: rte_service_init() failed
EAL: rte_service_init() failed
[17:01:18:072165][DOCA][E][ARGP]: EAL initialization failedThank you
Peterso the issue was due to permission, adding --log-level=eal,8 allowed me to see the cause.
However, I am running into another issue that /tmp/ar.cdo does not exist. Should I manually create this file ??so I had to create the ar.cdo file first by running:
doca_dpi_compiler -i /opt/mellanox/doca/examples/application_recognition/bin/ar_suricata_rules_example -o /tmp/ar.cdo -f suricatathen I had to change -l 0-64 to 0-7This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
735,use-roce-with-ovs-hardware-offload,"Hello all together,I have a little problem to configure my connectx-5 vdi cards correctly.The issue is the following:I would like to use RoCE and ovs with hardware offloads parallel on my card.
For this I have to change the devlink mode to switchdev according to the instructions. But after that RoCE no longer work.Is only enabled in the after stepMy host is a proxmox server, kernel 5.19 and i have installed with https://linux.mellanox.com/public/repo/mlnx_ofed/5.8-2.0.3.0/debian11.3/mellanox_mlnx_ofed.list the package mlnx-ofed-all.
In the end, a mesh network with 3 servers is planned. Therefore the explicit routes in the network part.I am following this link:https://docs.nvidia.com/networking/display/MLNXOFEDv582030LTS/Features+Overview+and+ConfigurationEthernet Network / RDMA over Converged Ethernet (RoCE)OVS Offload Using ASAP² DirectIf you still need info, please just say.Already many thanks for your help.Kind regardsDear kay5,Thank you for reaching out to Nvidia technical support.In relation to ConnectX-5 Ex, it is worth mentioning that switchdev mode and offloading are supported.Please consider the following points for review:Best regards,Nvidia supportDear ypetrov,thanks for your answer and please excuse my late reply.Regarding the points you noted:I hope this helps you.Best regards,
kay5Hello,Thank you for your prompt response and providing us with the additional information. We appreciate your cooperation.I understand that you are seeking further assistance, but at this stage, we would require additional logs and information to better understand and troubleshoot the issue you are facing. Regarding your main question about the compatibility of RoCE and Open vSwitch (OVS) with hardware offloads, I can confirm that they can work together in parallel as long as all the necessary conditions are met and the steps outlined in the documentation guide are followed accurately.To proceed with resolving the issue, I recommend referring to the documentation guide provided by us, as it contains detailed instructions and troubleshooting steps specific to your setup. Additionally, it may be beneficial to consult with the vendor of your system for further guidance and support.For any further questions or concerns, please feel free to reach out. We are here to assist you.Best regards,Yogev Petrov
Nvidia supportPowered by Discourse, best viewed with JavaScript enabled"
736,netdev-watchdog-eth0-mlx5-core-transmit-queue-0-timed-out-kernel-47075-368840-watchdog-bug-soft-lockup-cpu-64-stuck-for-22s-ksoftirqd-64-333,"I have a problem with 100G NICs. In the evening, when traffic peaks, mellanox NIC generate 100 IRQ on one core. At such time I’m watching network degradation. Please resolve that problem!Jun 12 22:12:22 138224 kernel: [45980.924388] ------------[ cut here ]------------Jun 12 22:12:22 138224 kernel: [45980.924390] NETDEV WATCHDOG: eth0 (mlx5_core): transmit queue 0 timed outJun 12 22:12:22 138224 kernel: [45980.924445] WARNING: CPU: 2 PID: 0 at net/sched/sch_generic.c:466 dev_watchdog+0x20d/0x220Jun 12 22:12:22 138224 kernel: [45980.924447] Modules linked in: binfmt_misc msr mst_pciconf(OE) amd64_edac_mod edac_mce_amd ipmi_ssif kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcspkr aufs(OE) ast joydev ttm drm_kms_helper drm evdev sg i2c_algo_bit ccp rng_core sp5100_tco ipmi_si ipmi_devintf ipmi_msghandler pcc_cpufreq acpi_cpufreq button tcp_bbr sch_fq bonding lp parport loop ip_tables x_tables autofs4 ext4 crc16 mbcache jbd2 fscrypto ecb raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid0 multipath linear hid_generic usbhid hid raid1 md_mod sd_mod crc32c_intel aesni_intel aes_x86_64 crypto_simd cryptd glue_helper ahci libahci libata nvme xhci_pci xhci_hcd nvme_core scsi_mod mlx5_core(OE) usbcore mlxfw(OE) mlx_compat(OE) devlink i2c_piix4 usb_commonJun 12 22:12:22 138224 kernel: [45980.924485] [last unloaded: mst_pci]Jun 12 22:12:22 138224 kernel: [45980.924488] CPU: 2 PID: 0 Comm: swapper/2 Tainted: G OE 4.19.0-16-amd64 #1 Debian 4.19.181-1Jun 12 22:12:22 138224 kernel: [45980.924489] Hardware name: Supermicro AS -2124BT-HNTR/H12DST-B, BIOS 1.1 01/10/2020Jun 12 22:12:22 138224 kernel: [45980.924491] RIP: 0010:dev_watchdog+0x20d/0x220Jun 12 22:12:22 138224 kernel: [45980.924493] Code: 00 49 63 4e e0 eb 92 4c 89 e7 c6 05 8f 09 b0 00 01 e8 97 bd fc ff 89 d9 4c 89 e6 48 c7 c7 a0 01 6e b8 48 89 c2 e8 5c 89 10 00 <0f> 0b eb c0 66 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 40 00 0f 1f 44Jun 12 22:12:22 138224 kernel: [45980.924494] RSP: 0018:ffff89b30dc83e90 EFLAGS: 00010286Jun 12 22:12:22 138224 kernel: [45980.924495] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000006Jun 12 22:12:22 138224 kernel: [45980.924495] RDX: 0000000000000007 RSI: 0000000000000086 RDI: ffff89b30dc966b0Jun 12 22:12:22 138224 kernel: [45980.924496] RBP: ffff89b2c5b8045c R08: 0000000000000943 R09: 0000000000000004Jun 12 22:12:22 138224 kernel: [45980.924497] R10: 0000000000000000 R11: 0000000000000001 R12: ffff89b2c5b80000Jun 12 22:12:22 138224 kernel: [45980.924498] R13: 0000000000000002 R14: ffff89b2c5b80480 R15: 0000000000000208Jun 12 22:12:22 138224 kernel: [45980.924499] FS: 0000000000000000(0000) GS:ffff89b30dc80000(0000) knlGS:0000000000000000Jun 12 22:12:22 138224 kernel: [45980.924500] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033Jun 12 22:12:22 138224 kernel: [45980.924500] CR2: 000055af702e6708 CR3: 0000011b1800a000 CR4: 0000000000340ee0Jun 12 22:12:22 138224 kernel: [45980.924501] Call Trace:Jun 12 22:12:22 138224 kernel: [45980.924504] Jun 12 22:12:22 138224 kernel: [45980.924508] ? pfifo_fast_enqueue+0x110/0x110Jun 12 22:12:22 138224 kernel: [45980.924513] call_timer_fn+0x2b/0x130Jun 12 22:12:22 138224 kernel: [45980.924515] run_timer_softirq+0x1c7/0x3e0Jun 12 22:12:22 138224 kernel: [45980.924517] ? ktime_get+0x3a/0xa0Jun 12 22:12:22 138224 kernel: [45980.924520] __do_softirq+0xde/0x2d8Jun 12 22:12:22 138224 kernel: [45980.924526] irq_exit+0xba/0xc0Jun 12 22:12:22 138224 kernel: [45980.924527] smp_apic_timer_interrupt+0x74/0x140Jun 12 22:12:22 138224 kernel: [45980.924530] apic_timer_interrupt+0xf/0x20Jun 12 22:12:22 138224 kernel: [45980.924531] Jun 12 22:12:22 138224 kernel: [45980.924533] RIP: 0010:native_safe_halt+0xe/0x10Jun 12 22:12:22 138224 kernel: [45980.924534] Code: ff ff 7f c3 65 48 8b 04 25 40 5c 01 00 f0 80 48 02 20 48 8b 00 a8 08 75 c4 eb 80 90 e9 07 00 00 00 0f 00 2d f6 2f 4d 00 fb f4  90 e9 07 00 00 00 0f 00 2d e6 2f 4d 00 f4 c3 90 90 0f 1f 44 00Jun 12 22:12:22 138224 kernel: [45980.924534] RSP: 0018:ffff9e9558a2bea8 EFLAGS: 00000246 ORIG_RAX: ffffffffffffff13Jun 12 22:12:22 138224 kernel: [45980.924535] RAX: ffffffffb7f34aa0 RBX: 0000000000000002 RCX: ffffffffb884f290Jun 12 22:12:22 138224 kernel: [45980.924536] RDX: 00000000a5bbe6da RSI: ffffffffb884aef8 RDI: 000029d1ff87e600Jun 12 22:12:22 138224 kernel: [45980.924537] RBP: 0000000000000002 R08: 0000000000000002 R09: 0000000000021a00Jun 12 22:12:22 138224 kernel: [45980.924537] R10: 00005e4ba43b061e R11: 0000000000000000 R12: 0000000000000000Jun 12 22:12:22 138224 kernel: [45980.924537] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000Jun 12 22:12:22 138224 kernel: [45980.924539] ? __sched_text_end+0x7/0x7Jun 12 22:12:22 138224 kernel: [45980.924540] default_idle+0x1c/0x140Jun 12 22:12:22 138224 kernel: [45980.924545] do_idle+0x1e3/0x270Jun 12 22:12:22 138224 kernel: [45980.924547] cpu_startup_entry+0x6f/0x80Jun 12 22:12:22 138224 kernel: [45980.924550] start_secondary+0x1a4/0x200Jun 12 22:12:22 138224 kernel: [45980.924554] secondary_startup_64+0xa4/0xb0Jun 12 22:12:22 138224 kernel: [45980.924556] —[ end trace 731a34cdffaba186 ]—Jun 12 22:12:20 138224 kernel: [45979.448511] NMI backtrace for cpu 64Jun 12 22:12:20 138224 kernel: [45979.448514] CPU: 64 PID: 333 Comm: ksoftirqd/64 Tainted: G OE 4.19.0-16-amd64 #1 Debian 4.19.181-1Jun 12 22:12:20 138224 kernel: [45979.448515] Hardware name: Supermicro AS -2124BT-HNTR/H12DST-B, BIOS 1.1 01/10/2020Jun 12 22:12:20 138224 kernel: [45979.448516] Call Trace:Jun 12 22:12:20 138224 kernel: [45979.448518] Jun 12 22:12:20 138224 kernel: [45979.448525] dump_stack+0x66/0x81Jun 12 22:12:20 138224 kernel: [45979.448527] nmi_cpu_backtrace.cold.4+0x13/0x50Jun 12 22:12:20 138224 kernel: [45979.448531] ? lapic_can_unplug_cpu+0x80/0x80Jun 12 22:12:20 138224 kernel: [45979.448534] nmi_trigger_cpumask_backtrace+0xf9/0x100Jun 12 22:12:20 138224 kernel: [45979.448536] rcu_dump_cpu_stacks+0x9b/0xcbJun 12 22:12:20 138224 kernel: [45979.448537] rcu_check_callbacks.cold.81+0x1db/0x335Jun 12 22:12:20 138224 kernel: [45979.448540] ? tick_sched_do_timer+0x60/0x60Jun 12 22:12:20 138224 kernel: [45979.448542] update_process_times+0x28/0x60Jun 12 22:12:20 138224 kernel: [45979.448543] tick_sched_handle+0x22/0x60Jun 12 22:12:20 138224 kernel: [45979.448544] tick_sched_timer+0x37/0x70Jun 12 22:12:20 138224 kernel: [45979.448546] __hrtimer_run_queues+0x100/0x280Jun 12 22:12:20 138224 kernel: [45979.448547] hrtimer_interrupt+0x100/0x210Jun 12 22:12:20 138224 kernel: [45979.448549] ? __perf_event_read+0xf5/0x230Jun 12 22:12:20 138224 kernel: [45979.448551] smp_apic_timer_interrupt+0x6a/0x140Jun 12 22:12:20 138224 kernel: [45979.448553] apic_timer_interrupt+0xf/0x20Jun 12 22:12:20 138224 kernel: [45979.448554] Jun 12 22:12:20 138224 kernel: [45979.448556] RIP: 0010:_raw_spin_unlock_irqrestore+0x11/0x20Jun 12 22:12:20 138224 kernel: [45979.448558] Code: d8 48 3d 90 d0 03 00 76 cc 80 4d 00 08 eb 98 90 90 90 90 90 90 90 90 90 90 0f 1f 44 00 00 c6 07 00 0f 1f 40 00 48 89 f7 57 9d <0f> 1f 44 00 00 c3 66 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 8b 07Jun 12 22:12:20 138224 kernel: [45979.448558] RSP: 0018:ffff9e955980bcb0 EFLAGS: 00000246 ORIG_RAX: ffffffffffffff13Jun 12 22:12:20 138224 kernel: [45979.448560] RAX: 0000000011c00040 RBX: ffff89a9420de178 RCX: ffff89af31168c78Jun 12 22:12:20 138224 kernel: [45979.448560] RDX: ffff89a9420de198 RSI: 0000000000000246 RDI: 0000000000000246Jun 12 22:12:20 138224 kernel: [45979.448561] RBP: 0000000100ae4230 R08: ffff8ab2be800000 R09: 000000000000a4f6Jun 12 22:12:20 138224 kernel: [45979.448561] R10: 000000000000527b R11: 0000000000000000 R12: ffff8ab2be81a7c0Jun 12 22:12:20 138224 kernel: [45979.448562] R13: 0000000000000000 R14: ffff8ab2be81a7c0 R15: 000000001c000040Jun 12 22:12:20 138224 kernel: [45979.448564] mod_timer+0x177/0x400Jun 12 22:12:20 138224 kernel: [45979.448567] sk_reset_timer+0x14/0x30Jun 12 22:12:20 138224 kernel: [45979.448570] tcp_retransmit_timer+0x530/0xa40Jun 12 22:12:20 138224 kernel: [45979.448572] tcp_write_timer_handler+0xb1/0x210Jun 12 22:12:20 138224 kernel: [45979.448573] tcp_write_timer+0x71/0x90Jun 12 22:12:20 138224 kernel: [45979.448574] ? tcp_write_timer_handler+0x210/0x210Jun 12 22:12:20 138224 kernel: [45979.448575] call_timer_fn+0x2b/0x130Jun 12 22:12:20 138224 kernel: [45979.448576] run_timer_softirq+0x1c7/0x3e0Jun 12 22:12:20 138224 kernel: [45979.448577] __do_softirq+0xde/0x2d8Jun 12 22:12:20 138224 kernel: [45979.448581] ? sort_range+0x20/0x20Jun 12 22:12:20 138224 kernel: [45979.448584] run_ksoftirqd+0x26/0x40Jun 12 22:12:20 138224 kernel: [45979.448585] smpboot_thread_fn+0xc5/0x160Jun 12 22:12:20 138224 kernel: [45979.448588] kthread+0x112/0x130Jun 12 22:12:20 138224 kernel: [45979.448589] ? kthread_bind+0x30/0x30Jun 12 22:12:20 138224 kernel: [45979.448590] ret_from_fork+0x22/0x40Powered by Discourse, best viewed with JavaScript enabled"
737,interconnecting-qsfp28-and-qsfp,"We’ve bought connectx-4 LX 50gbE card which apparently has QSFP28 port and we’d like to get it connected to our switch which has QSFP+ ports only. Is this possible at all? Thanks.What a miracle! QSFP+ can actually be plugged into QSFP28, but you have to be careful selecting the speed as mellanox’s proprietary 56GigE speed isn’t supported and you have to force 40GigE. What a pity you can’t have 50, but this is still better than having no link at all 😄Powered by Discourse, best viewed with JavaScript enabled"
738,kernel-build-for-sn2010-switchdev-failed-to-register-thermal-zone,"HiTrying to build a new kernel with Switchev support, but seeing an error when loading mlxsw_spectrum.Error:
[  418.684370] mlxsw_spectrum 0000:01:00.0: Failed to register thermal zone
[  418.774215] mlxsw_spectrum 0000:01:00.0: cannot register bus device
[  418.781275] mlxsw_spectrum: probe of 0000:01:00.0 failed with error -22Kernel 6.1.29 with config variables as in the wiki.‘Failed to register thermal zone’Issue seems to be in: /drivers/net/ethernet/mellanox/mlxsw/core_thermal.cAny suggestions for fixing this?I have a  6.1.13 kernel that works without problems. Something changed since then I’m guessing.This question will require more in-depth investigation than is possible through a forum. A support request would be the best way to resolve this issue.Powered by Discourse, best viewed with JavaScript enabled"
739,ufm-ha-reinstall-how-to-save-configuration,"What is the best way to preserve the UFM configuration after a complete reinstall ?  Is it sufficient to save and restore the gv.vfg file ?Hello glowell1,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Best to generate a UFMsnapshot as that will contain all configuration files needed to rebuild → https://docs.nvidia.com/networking/display/UFMEnterpriseUMv68/UFM+Snapshot+TabOr from the command-line → /opt/ufm/scripts/vsysinfo --section all --ext --output <output location>Bash-script:Thank you and regards,
~NVIDIA Networking Technical SupportThanks.  I may have missed something obvious, but what is the process for restoring from a snapshot ?  Looks like the directory names change in the snapshot tarball.Hello glowell1,The UFM Snapshot is used for support related issues, in which you need to provide NVIDIA Networking Support a snapshot from your UFM node.Restoring is only supported on UFM HAFor standalone, restore is manual based on certain files and directories for example “OpenSM”, gv.cfg and “licenses”. In most cases, a reinstall and copying the needed files  from the snapshot is faster.You have a NVIDIA Networking support case open with Sam. Request from him through the case, the internal document “How to Backup-Restore UFM”Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
740,dpdk-on-doca,"Hi all,
I’m trying to run the following hands-on, on bluefield2, where I have installed doca.
https://community.mellanox.com/s/article/Configuring-OVS-DPDK-Offload-with-BlueField-2
But looks like dpdk is not initialized, should I re-install it using ./configure –-with-dpdk? I don’t want to mess up the current doca installation:
ovs-vsctl get Open_vSwitch . dpdk_initialized
falsePowered by Discourse, best viewed with JavaScript enabled"
741,connectx-5-edr-connect-x6-hdr-with-sx6036-fdr-switch-incompatibility,"hi all. i would love your feedback on a weird problem where i try to mix HDR/EDR with an FDR switch. I tried all possible cables FDR, EDR and HDR, copper and glass, but i can only get the combination to really work when i force the ports on the switch to QDR. Any other setting results in extremely weak performance (merely Megabytes/second+huge latencies)From my understanding, IB is backwards compatible and i should be able to get a working FDR line speed right?
Am i missing something?Hello schonewille,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.To a certain extend you are right regarding the backwards compatibility of IB.In the f/w RN for the adapters and switches (HDR and NDR), we are providing a Connectivity Matrix what is supported and on which speed.If the correct combination used, you do not have to force any port speed.The connectivity matrix you can find through the following link → https://docs.nvidia.com/networking/display/NVIDIAQuantum2Firmwarev3120102110/Firmware+Compatible+ProductsOr through the main page → NVIDIA Documentation Center | NVIDIA Developer ,searching for the adapter or other switch RN.For all mixed fabrics to function with out any issues, all needs to be on the latest code (Switch and HCA f/w and driver as well)In any case you have a fully supported combination, and still experiencing issues, please do not hesitate to open a NVIDIA Networking Support ticket (Valid Support Entitlement needed) so we can assist you further.Thank you and regards,
~NVIDIA Networking Technical SupportHi MvBthanks for the heads up. Yes, all HCA-s and switch are at the latest available firmware level. The matrix more or less confirmed that it should work, and it sort of does, but not all the way:8k-32k is extremely poor. 64k-128k is not too pretty either.regards,
ABelieve it or not, but kernel 5.17.5-1 seem to have solved our problem. The Mellanox stack 5.4 or 5.5 did not as well as recent kernels, however the very latest kernel worked. Magic…unfortunately, only intelmpi/psm3 performed well. openmpi4/ucx is still way below par, even using ud or ud_x.am running out of options here except reducing the rates to QDR speed.Powered by Discourse, best viewed with JavaScript enabled"
742,connectx6-dpdk-dpdk-testpmd-receive-error-len-error-checksum-udp-packet-performance-is-very-low,"I use Ixia to construct two streams
mlx11355×221 26.3 KB
​flow1 udp 64size small packet
mlx21229×604 18.9 KB
​flow2 udp 64size small packet error len error checksum
mlx31268×672 20.5 KB
​Ixia Send per second 30G bps 57 million pps./dpdk-testpmd -l 4-22 -n 8 – -i --rxq 19 --txq 19 --nb-cores 18 --rxd 2048 --txd 2048 --portmask 0xffset fwd rxonlystart
mlx4834×216 4.96 KB
Recive per second 3G bps 5 million ppsOnly about one tenth was received!rx_discards_phy drop------------------------------------------------------------------------------------------------------------------------------------When I change the flow2 to no problem udp, It’s all normalRecive per second 29.4G bps 57 million pps
mlx6738×168 3.73 KB
rx_discards_phy no dropserverdell poweredge r750[root@localhost proc]# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 64On-line CPU(s) list: 0-63Thread(s) per core: 1Core(s) per socket: 32Socket(s): 2NUMA node(s): 2Vendor ID: GenuineIntelCPU family: 6Model: 106Model name: Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHzStepping: 6CPU MHz: 2900.000BogoMIPS: 5800.00Virtualization: VT-xL1d cache: 48KL1i cache: 32KL2 cache: 1280KL3 cache: 55296KNUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 invpcid_single intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq md_clear pconfig spec_ctrl intel_stibp flush_l1d arch_capabilitiesMLX ConnectX6 100G PCIE4 x16
mlx71116×362 9.38 KB
TCP also has this problem!Unable to close tcp udp checksum offloads​static struct rte_eth_conf port_conf = {.rxmode = {.mq_mode = RTE_ETH_MQ_RX_RSS,.split_hdr_size = 0,.offloads = 0,},​​​.offloads set 0​
企业微信截图_164290344642421050×242 14.9 KB
​After receiving the correct cksum TCP packet, the will be print RTE_MBUF_F_RX_L4_CKSUM_GOODThis indicates that TCP cksum offload is not closed!!​​Hi Li,Just a quick clarification should this issue needs in depth debugging, a support case with a valid support contract would be needed.I believe something is missing from your statement:“When I change the flow2 to no problem udp, It’s all normalReceive per second 29.4G bps 57 million pps”Can you elaborate what are you changing?I also noticed that FW “20.32.1010” was not flashed, can you flash it and re-test accordingly?Have you consulted the performance report from “http://core.dpdk.org/perf-reports/” to make sure your server is properly tunned?(Mellanox NIC Performance Report).Sophie.Are you as well using the latest DPDK version?Reference link: DPDK​no problem udpThe UDP package length and verification are correct
企业微信截图_164309080041021276×610 22.7 KB
​Ixia auto Calculated​FW 20.32.1010 ​20.30.1004 I’ve tested them all,All have this problem!​I tested 20.32.1010 first. After there was a problem, I downgraded to 20.30.1004.I saw it many times 36. MLX5 Ethernet Poll Mode Driver — Data Plane Development Kit 22.07.0 documentation, https://fast.dpdk.org/doc/perf/DPDK_21_08_Mellanox_NIC_performance_report.pdf.The test package of this report should send the correct checksum and the correct length package.I think this problem is the efficiency of the network card in the process of processing data packets.When an incorrect checksum or wrong length is received, the network card may have some processing, resulting in packet loss of the network card hardware.I use dpdk 21.11.0 and dpdk 20.11.3 (LTS) versions, which have this problem.It should not be the problem of dpdk setting.Please help me!Do you need any more data from me?I previously posted though I did not see an answer from it.You have 2 flows with same UDP size packet, what exactly are you changing that simulate this issue?This is not clear to me.I believe something is missing from your statement:“When I change the flow2 to no problem udp, It’s all normalReceive per second 29.4G bps 57 million pps”Can you elaborate what are you changing?Sophie.when there is a problem.udp head​udp-len: Fixed settings 123​(This is the wrong value)udp-checksum: Fixed settings 456 (This is the wrong value)​When I change the flow2 to no problem udp.​udp head​udp-len: ​Ixia auto Calculated correctudp-checksum: ​Ixia auto Calculated correct​I changed a network card MCX614106A-CCA_Ax still has this problem!Device #1:Device Type: ConnectX6Part Number: MCX614106A-CCA_AxDescription: ConnectX-6 EN adapter card; 100GbE; dual-port QSFP56; Socket Direct 2x PCIe3.0 x16; tall bracket; ROHS R6PSID: MT_0000000220PCI Device Name: 0000:ca:00.0Base GUID: 0c42a103005f8570Base MAC: 0c42a15f8570Versions: Current AvailableFW 20.30.1004 N/APXE 3.6.0301 N/AUEFI 14.23.0017 N/AStatus: No matching image foundPowered by Discourse, best viewed with JavaScript enabled"
743,is-possible-to-plug-qsfp28-100g-direct-cable-in-first-port-of-connext5-en-dual-ports-100g-and-qsfp-40g-direct-cable-in-second-port-of-same-nic,"For example :MCP1600-C002 in first portMCP1700-B002E in second portHello Jean-Philippe,Thank you for posting your inquiry on the NVIDIA Networking Community.Yes, you can insert a 100GbE cable in one port and a 40GbE cable in the other port. Both ports function independently from each other. Just make sure the cables you are using are in the validated/tested list of cables mentioned in the RN of the f/w of the adapter → https://docs.mellanox.com/display/ConnectX5Firmwarev16292002/Firmware+Compatible+ProductsThank you and regards,~NVIDIA Networking Technical SupportHello MartijnThank you for this answerRegardsJean-PhilippePowered by Discourse, best viewed with JavaScript enabled"
744,hi-everyone-is-there-is-a-way-to-get-the-model-number-in-the-cli-the-same-way-we-can-get-it-from-the-web-ui-e-g-mellanox-mlnx-os-sx1036-management-console-how-do-i-get-sx1036-from-cli-please,"The information can be found from the web UI, however due to the need to VPN etc. the ability to retrieve it from CLI would simplify the instructions significantly. I can get information from “show version” etc., but the model isn’t explicitly identified as far as I know.show inventoryThank you.Powered by Discourse, best viewed with JavaScript enabled"
745,how-to-use-mmap-dev-mem-with-gpio-port-jetson-xavier-nx,"I am in need of mmap to use GPIO01 in NVIDIA Jetson Xavier NX Developer Kit Carrier Board P3509_A01.
I check that GPIO01 is being controlled by GPIO_CTL2_GPIO3 (PQ.05)
#define GPIO_CTL2_GPIO3_BASE = 0x02233000
char *addr;
addr = mmap(NULL, 0x1000,PROT_READ | PROT_WRITE, MAP_SHARED, fd, GPIO_CTL2_GPIO3_BASE);=> But when I write the value to addr it doesn’t actually output the value to Pin.
Can anyone show me how to make it work?Powered by Discourse, best viewed with JavaScript enabled"
746,docker-pull-error,"docker pull nvcr.io/nvidia/pytorch:23.01-py3Error response from daemon: error parsing HTTP 406 response body: invalid character ‘<’ looking for beginning of value: “\r\n<html xmlns=""”>\r\n\r\n<meta http-equiv=""Content-Type"" content=""text/html; charset=iso-8859-1""/>\r\n406 - Client browser does not accept the MIME type of the requested page.\r\n<style type=""text/css"">\r\n\r\n\r\n\r\n\r\n<div id=""header"">Powered by Discourse, best viewed with JavaScript enabled"
747,rss-doesnt-work-for-mellanox-connectx-6-dx,"Hi,While testing dpdk(v19.11.10) application we noticed that nic card didn’t update mbuf.hash.rss value despite the fact that rxmode.mq_mode = ETH_MQ_RX_RSS_FLAG was set.Is there any other way to enable RSS on the card ?here is additional info :DPDK RSS offload enable code:static struct rte_eth_conf port_conf = {.rxmode = {.mq_mode = ETH_MQ_RX_RSS | ETH_MQ_RX_RSS_FLAG,.max_rx_pkt_len = RTE_ETHER_MAX_LEN,.offloads = DEV_RX_OFFLOAD_RSS_HASH,},.rx_adv_conf = {.rss_conf = {.rss_key = NULL.rss_hf = ETH_RSS_IP | ETH_RSS_UDP | ETH_RSS_TCP,},},.txmode = {.mq_mode = ETH_MQ_TX_NONE,.offloads = DEV_TX_OFFLOAD_IPV4_CKSUM | DEV_TX_OFFLOAD_TCP_CKSUM | DEV_TX_OFFLOAD_UDP_CKSUM,},};Card firmware:Image type: FS4FW Version: 22.32.1010FW Release Date: 1.12.2021Product Version: 22.32.1010Rom Info: type=UEFI version=14.25.17 cpu=AMD64,AARCH64type=PXE version=3.6.502 cpu=AMD64Description: UID GuidsNumberBase GUID: 08c0eb030059cd20 4Base MAC: 08c0eb59cd20 4Image VSD: N/ADevice VSD: N/APSID: MT_0000000359Security Attributes: N/Alspci output:07:00.0 Ethernet controller [0200]: Mellanox Technologies MT28841 [15b3:101d]Subsystem: Mellanox Technologies MT28841 [15b3:0016]Physical Slot: 2Flags: bus master, fast devsel, latency 0, IRQ 93, NUMA node 0Memory at f4000000 (64-bit, prefetchable) [size=32M][virtual] Expansion ROM at ec100000 [disabled] [size=1M]Capabilities: [60] Express Endpoint, MSI 00Capabilities: [48] Vital Product DataCapabilities: [9c] MSI-X: Enable+ Count=64 Masked-Capabilities: [c0] Vendor Specific Information: Len=18 <?>Capabilities: [40] Power Management version 3Capabilities: [100] Advanced Error ReportingCapabilities: [150] Alternative Routing-ID Interpretation (ARI)Capabilities: [180] Single Root I/O Virtualization (SR-IOV)Capabilities: [1c0] #19Capabilities: [230] Access Control ServicesCapabilities: [320] #27Capabilities: [370] #26Capabilities: [420] #25Kernel driver in use: mlx5_coreKernel modules: mlx5_core07:00.1 Ethernet controller [0200]: Mellanox Technologies MT28841 [15b3:101d]Subsystem: Mellanox Technologies MT28841 [15b3:0016]Physical Slot: 2Flags: bus master, fast devsel, latency 0, IRQ 112, NUMA node 0Memory at f2000000 (64-bit, prefetchable) [size=32M][virtual] Expansion ROM at eca00000 [disabled] [size=1M]Capabilities: [60] Express Endpoint, MSI 00Capabilities: [48] Vital Product DataCapabilities: [9c] MSI-X: Enable+ Count=64 Masked-Capabilities: [c0] Vendor Specific Information: Len=18 <?>Capabilities: [40] Power Management version 3Capabilities: [100] Advanced Error ReportingCapabilities: [150] Alternative Routing-ID Interpretation (ARI)Capabilities: [180] Single Root I/O Virtualization (SR-IOV)Capabilities: [230] Access Control ServicesCapabilities: [420] #25Kernel driver in use: mlx5_coreKernel modules: mlx5_coreethtool output:Features for ens2f0:rx-checksumming: ontx-checksumming: ontx-checksum-ipv4: off [fixed]tx-checksum-ip-generic: ontx-checksum-ipv6: off [fixed]tx-checksum-fcoe-crc: off [fixed]tx-checksum-sctp: off [fixed]scatter-gather: ontx-scatter-gather: ontx-scatter-gather-fraglist: off [fixed]tcp-segmentation-offload: ontx-tcp-segmentation: ontx-tcp-ecn-segmentation: off [fixed]tx-tcp-mangleid-segmentation: offtx-tcp6-segmentation: onudp-fragmentation-offload: offgeneric-segmentation-offload: ongeneric-receive-offload: onlarge-receive-offload: offrx-vlan-offload: ontx-vlan-offload: onntuple-filters: offreceive-hashing: onhighdma: on [fixed]rx-vlan-filter: onvlan-challenged: off [fixed]tx-lockless: off [fixed]netns-local: off [fixed]tx-gso-robust: off [fixed]tx-fcoe-segmentation: off [fixed]tx-gre-segmentation: ontx-gre-csum-segmentation: off [fixed]tx-ipxip4-segmentation: ontx-ipxip6-segmentation: ontx-udp_tnl-segmentation: ontx-udp_tnl-csum-segmentation: off [fixed]tx-gso-partial: ontx-sctp-segmentation: off [fixed]tx-esp-segmentation: off [fixed]fcoe-mtu: off [fixed]tx-nocache-copy: offloopback: off [fixed]rx-fcs: offrx-all: offtx-vlan-stag-hw-insert: onrx-vlan-stag-hw-parse: off [fixed]rx-vlan-stag-filter: on [fixed]l2-fwd-offload: off [fixed]hw-tc-offload: offesp-hw-offload: off [fixed]esp-tx-csum-hw-offload: off [fixed]rx-udp_tunnel-port-offload: onlinux kernel:4.15.0-109-genericThanks in advance,Tigran MartirosyanHi Tigran,ConnectX-6 Dx supports RSS. PMD supports RSS on standard 5tuple:(source IP address, source port, destination IP address, destination port, transport protocol)You can check that with testpmd, RSS over udp / tcp ports.In general, these macros are supported:#define ETH_RSS_IP ( \ETH_RSS_IPV4 | \ETH_RSS_FRAG_IPV4 | \ETH_RSS_NONFRAG_IPV4_OTHER | \ETH_RSS_IPV6 | \ETH_RSS_FRAG_IPV6 | \ETH_RSS_NONFRAG_IPV6_OTHER | \ETH_RSS_IPV6_EX)#define ETH_RSS_UDP ( \ETH_RSS_NONFRAG_IPV4_UDP | \ETH_RSS_NONFRAG_IPV6_UDP | \ETH_RSS_IPV6_UDP_EX)#define ETH_RSS_TCP ( \ETH_RSS_NONFRAG_IPV4_TCP | \ETH_RSS_NONFRAG_IPV6_TCP | \ETH_RSS_IPV6_TCP_EX)Another option is to move RSS to rte_flow, as described here:https://doc.dpdk.org/dts/test_plans/rss_to_rte_flow_test_plan.htmlThanks,ChenHi Chen,Do you have any example how to enable RSS in PMD ?It works in dpdk v21.11 , but doesn’t work under dpdk v19.11, dpdk v20.08Powered by Discourse, best viewed with JavaScript enabled"
748,i-need-to-know-how-to-make-rss-work-for-srv6,"I am developing on your product Connect5X EN 50GbE.Very good product!Specifically, we are using XDP to implement SRv6 functionality.So I have a question. Does this NIC allow RSS to work against SRv6 (i.e.IPv6 Exthdr)?Add support for inner header RSS on IP-in-IP and IPv6 tunneled packets.

Add rul…es to the steering table regarding outer IP header, with
IPv4/6->IP-in-IP. Tunneled packets with protocol numbers: 0x4 (IP-in-IP)
and 0x29 (IPv6) are RSS-ed on the inner IP header.
Separate FW dependencies between flow table inner IP capabilities and
GRE offload support. Allowing this feature even if GRE offload is not
supported.  Tested with multi stream TCP traffic tunneled with IPnIP.
Verified that:
Without this patch, only a single RX ring was processing the traffic.
With this patch, multiple RX rings were processing the traffic.
Verified with and without GRE offload support.

Signed-off-by: Aya Levin <ayal@mellanox.com>
Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>As far as I could tell, IPIP6 is supported, so I thought it might work.However, so far I have not been able to use multi-core.If you do support it, could you please tell me how to use it?Thank you.Hello Takeru,Thank you for posting your inquiry to the Mellanox community.We will need some further information in order to assist you further:What firmware revision is installed on the HCA?What OS and kernel version is installed on this system?Are you using MLNX_OFED, or inbox (upstream OS) drivers?You mention that you are not able to use multi-core - could you elaborate as to what you mean by that? What steps are you taking in order to configure RSS?Thank you, and we look forward to hearing from you soon.Best regards,Mellanox Technical SupportHello Takeru,I have opened a support case on your behalf, and will be providing further assistance through our support portal (https://support.mellanox.com).Best regards,Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
749,why-doesnt-my-mellanox-mcx311a-xcat-network-card-sit-correctly-on-my-supermicro-x10srh-cf-motherboard-pci-slot,"The high profile bracket disengages my Mellanox MCX311A-XCAT card from the PCI slot when I try to secure the bracket to my chassis with a screw. In short, the card does not sit correctly with the included high profile bracket. Do I need to buy a different high profile bracket?My SuperMicro X10SRH-CF Motherboard has the PCI slots listed below:Which PCI slot should my Mellanox MCX311A-XCAT card be installed to?
SuperMicro PCI Slots.JPG1001×1234 150 KB
Hi,Please refer the ConnectX-3 HW user manual1703.44 KBThanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
750,100g-inifiband-system-random-disconnect,"Hi guys, I have a small cluster, connected by ten 100G mellanox connectx-4 infiniband adaptors with sb7890 switch. Most of the time they run smoothly. But sometimes, one of the node would disconnects. This node is not fixed, random node.  Once disconnected, the light in the adaptor is yellow, which means its offline.
I dont know why, any hints? or how can I debug? The following is the opensm.log
opensm.log1920×1080 142 KB
yesterday another node is offline. Pls see the image

21920×1080 158 KB
Hello Kevin,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Based on the information provided, the issue can be related to various components in your fabric, e.g. unsupported cable, HCA f/w alignment with the switch f/w.For these kind of issue, we recommend to make sure all code in your fabric is aligned to the supported versions. You can find the latest RN of all s/w and f/w through the following url → Site Home - NVIDIA Networking DocsWhen you have all aligned and still experiencing node disconnects (logical/physical), please open a NVIDIA Networking Support case by sending an email to the following address →  networking-support@nvidia.comThank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
751,connectx-6-dx-missing-the-rdma,"Hello,
I have two identical servers out of which one is having problems with the RDMA.  The problem was already described well in the ConnectX-5 25GbE missing RDMA devices - Adapters and Cables / Ethernet Adapter Cards - NVIDIA Developer ForumsBut, the problem was not solved in that topic. I have Mellanox card with Part Number MCX623106PE-CDAT, so I am using the firmware 22.37.1014 (MT_0000000606) and driver 5.8-1.1.2. This should be according to the recommendation. But it still does not work.I even replaced it with the Mellanox ConnectX-6 with Part Number 0F6FXM (DELL) and the problem is the same.The strange thing is that it is working on the other server and it worked on the “faulty” server before with the same configuration. I have a fresh Ubuntu 22.04 (5.15.0-75-generic) installation, so there is nothing installed that could prevent it from working.
Any suggestions on how to fix this?Pls download and install OFED driver.Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)https://docs.nvidia.com/networking/display/MLNXOFEDv590560/InstallationHi @xiaofengl, as you can see in my description of the problem, I have the MLNX OFED installed. In fact, I believe I have a correct version installed.One thing that has mitigated the problem right now, is when I disabled the SRIOV for that particular Mellanox card. But it is really strange, that it has worked before. So, no idea what the problem might be, but at least after SRIOV is disabled it works.Powered by Discourse, best viewed with JavaScript enabled"
752,rdma-infiniband-cannot-open-hosts-iberror-discovery-failed-port-state-down,"I am facing an issue while configuring rdma and Infiniband on my two nodes. Both of these two nodes are connected and I have installed the recommended software libraries and packages required.
But my port status is down and physical state is Disabled. I tried to enable the state but I get the error of can’t open MAD PORTibwarn: [5630] mad_rpc_open_port: can’t open UMAD port ((null):0)src/ibnetdisc.c:784; can’t open MAD port ((null):0)/usr/sbin/ibnetdiscover: iberror: failed: discover failedI have also tried running commands as sudo but still I face the error. Can you guys guide me of what could be the issue ?Here is my ib_status output:Infiniband device ‘mlx5_0’ port 1 status:
default gid:	 fe80:0000:0000:0000:1270:fdff:fe6e:43e0
base lid:	 0x0
sm lid:		 0x0
state:		 1: DOWN
phys state:	 3: Disabled
rate:		 100 Gb/sec (4X EDR)
link_layer:	 EthernetI figured it out and I am sharing the answers for others to see so the issue was the network interface, you need to see which network interface the Infiniband and check the status.root@dtn0:~# /etc/init.d/openibd statusAfter that, I just assigned Ip and netmask on the interface and I was able to use the interface and reach the network.root@dtn0:~# ifconfig ens11np0 10.0.0.50/24hi,baka_laowai.I am sorry to trouble. I faced the same problem with you and I have tried to assigned Ip and netmask on the interface, but the port status is still down and physical state is disabled. What’s more, I also tried to enable the state but the MAD PORT can’t open. Could you help me please?
Following are more details:HCA driver loadedConfigured Mellanox EN devices:
enp33s0np0Currently active Mellanox devices:
enp33s0np0The following OFED modules are loaded:ib_ipoib
mlx5_core
mlx5_ib
ib_uverbs
ib_umad
ib_cm
ib_core
mlxfwInfiniband device ‘mlx5_0’ port 1 status:
default gid:	 fe80:0000:0000:0000:526b:4bff:fe28:4fd0
base lid:	 0x0
sm lid:		 0x0
state:		 4: ACTIVE
phys state:	 5: LinkUp
rate:		 100 Gb/sec (4X EDR)
link_layer:	 Ethernetwrong message:
ibwarn: [329771] mad_rpc_open_port: can’t open UMAD port ((null):1)
ibping: iberror: failed: Failed to open ‘(null)’ port ‘1’Powered by Discourse, best viewed with JavaScript enabled"
753,kvm-in-bluefield-linux,"1、As we know, KVM is not an independent module in bluefileld Linux(like kvm.ko in x86). For some reasons, I want to modify the module of KVM, so I have to recompile and replace image? Is there any better way?2、Meanwhile, what is the recommended profile tools(such as ftrace/perf/strace) on Bluefield arm linux that can help us analyze KVM virtual machines?3、In my experience, “perf kvm stat record"" and “ftrace” cannot track kvm:kvm_exit events on Bluefield Linux, why?Powered by Discourse, best viewed with JavaScript enabled"
754,how-many-udp-flooding-packets-will-be-scanned-and-blocked-by-dpu-ips-rule-in-bludfield2-per-core,"I encounter some limitations in DPU during scanning UDP packets by IPS rules. It onlyexaminess 15036 packets per core.I use the single core as shown in this command in DPU. I have used hping3 to send flooding traffic toward DPU host and enable IPS inside DPU.“/opt/mellanox/doca/applications/ips/bin/doca_ips -a 0000:03:00.0,class=regex -a auxiliary:mlx5_core.sf.3,sft_en=1 -a auxiliary:mlx5_core.sf.4,sft_en=1 -l 0-1 – --cdo /tmp/udp.cdo -p -n 1”
DPU IPS issue1326×436 98.1 KB
Screenshot attached.
No warning message appears
IPowered by Discourse, best viewed with JavaScript enabled"
755,about-the-bright-cluster-manager-user-forum,"Welcome to the NVIDIA Bright Cluster Manager User Forum. Use this forum to interact with fellow users of Bright Cluster Manager and other Bright experts.NOTE: THIS FORUM IS NOT A SUBSTITUTE FOR BRIGHT CLUSTER MANAGER ENTERPRISE SUPPORT - If you are experiencing an issue with your Bright managed system, please log into your Customer Portal account and enter a support request.Forum posts deemed covered via commercial Bright Support will be advised to submit a support request with the support team and the post will be closed by a moderator.Powered by Discourse, best viewed with JavaScript enabled"
756,tcpdump-for-roce,"Related to post Sniff RoCE traffic using tcpdump - #2 by samerkaI am able to collect packets, but it ends up collecting tcp, udp packets…
Using tcpdump options like “udp” or “udp port 4791” doesn’t collect any packets.
Any reason?  Is there a different way to restrict to collecting only RoCE packets?To capture RoCE offload packets, you need use latest libpcap and rebuild tcpdump.Follow tcpdump guide compile libpcap and tcpdump.Web site of Tcpdump and LibpcapThanks…Actual issue is that I’m not able to filter during collection (I can filter later in wireshark or by reading pcap file…)sudo tcpdump -i mlx5_0 -s 0 -w rdma.pcapsudo tcpdump -i mlx5_0 udp port 4791 -s 0 -w rdma.pcapsudo tcpdump -i mlx5_0 udp -s 0 -w rdma.pcapThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
757,mellanox-ofed-5-5-1-0-3-2-send-bandwidth-improves-when-registered-memory-is-aligned-to-system-page-size-4k-how,"OS - RHEL Centos 7.9 LatestOperation:
Sending 500MB chunks 21 times from one System to another connected via Mellanox Cables.
(Ethernet controller: Mellanox Technologies MT28908 Family [ConnectX-6])The registered memory region (500MB) is reused for all the 21 iterations.The gain in Message Send Bandwidth when using aligned_alloc() (with system page size 4096B) instead of malloc() for registered memory is around 35Gbps.with malloc() : ~86Gbps
with aligned_alloc() : ~121GbpsSince the CPU is not involved for these operations, how is this operation faster with aligned memory?
Please provide useful reference links if available that explains this.
What change does aligned memory bring to the read/write operations?Align buffers to cache line size can avoid read-modify-write.
It decreases the number of CPU cycles and memory accesses, and finally improves performance compared to unaligned memory buffers.Powered by Discourse, best viewed with JavaScript enabled"
758,about-ndi-interface-support-in-winof,"Hi, all.
I’m working on Windows RDMA programming(suppose supporting both roce and infiniband),
I see that from WinOF-2, only Microsoft’s Network Direct Interface is supported, I would like to ask
a few related questions as belowing:Hello and thank you for contacting us.Ibverbs was not supported in WinOF-2 from day 1, only Network direct.We support RDMA between Linux and windows, as far as I know it is tested in end-to-end.Microsoft wanted unified addressing for sockets, Infiniband and RoCE, hence they defined RDMA-CM (IP based) like interface.
Unlike Linux, in Windows, it is impossible to have RDMA stack without TCP/IP stack.Best Regards,Yogev PetrovNvidia Supportthanks, another fews questions are:Powered by Discourse, best viewed with JavaScript enabled"
759,my-connectx-6-dx-nics-cant-work-consistently,"I use two servers equipped with ConnectX-6 DX adapter cards connected back-to-back to do some congestion control programming in Ubuntu 18.0.4. Unfortunately, my network card can’t work for a long time, and both of them are. I only run the instance code provided by NVIDIA， Raw Ethernet Programming: Basic Introduction - Code Example (nvidia.com). The network card only worked for a minute or two and then went on strike. Then the server needs to be restarted，the adapter cards can run again.
Each time the server is restarted, it can run for one to two minutes, and then the network card is automatically disabled. At this time, the cable temperature is very high. Is the network card automatically disabled due to high temperature? If so, what should I do?Hi @binaryartist,Welcome to the NVIDIA Forums! Your topic belongs in the Networking category, I have moved it over for you.Hi binaryartistif you see think the server restarted caused by “At this time, the cable temperature is very high. Is the network card automatically disabled due to high temperature”, could you please open a ticket?Actually there are several commands to check cable & nic temperature. Those are needed to investigate by technical support guys./HyungKwangPowered by Discourse, best viewed with JavaScript enabled"
760,cannot-ping-from-routed-port-to-device-on-vlan,"SN2100 running Onyx 3.10.2002
I have a device connected to an ethernet port which is part of a vlan.
This vlan has an interface with an assigned ip address.
Locally on the switch I can ping the device without any issue.I also have an external switch (Arista) that is connected to the SN2100 via a L3 connection.
From that switch I can the vlan interface but not the device that is on that vlan.Looking at tcpdump I can see the ICMP echo request coming in but no reply is coming.
I have no idea why this doesn’t work.
Any help would be appreciated.Settings on SN2100 switch:
mlnx-sn2100 [standalone: master] (config) # show interfaces vlan 130Vlan 130:
Admin state      : Enabled
Operational state: Up
Autostate        : Enabled
Mac Address      : b8:59:9f:74:01:08
DHCP client      : Disabled
PBR route-map    : N/AIPv4 address:
10.54.31.91/24 [primary]Broadcast address:
10.54.31.255 [primary]MTU             : 2000 bytes
Arp timeout     : 1500 seconds
Arp responder   : Disabled
Arp cache-update: garp
Icmp redirect   : Enabled
Description     : N/A
VRF             : default
IP Enable       : Enabled
Counters        : DisabledPing to device works:
mlnx-sn2100 [standalone: master] (config) # ping 10.54.31.191
PING 10.54.31.191 (10.54.31.191) 56(84) bytes of data.
64 bytes from 10.54.31.191: icmp_seq=1 ttl=255 time=0.343 ms
64 bytes from 10.54.31.191: icmp_seq=2 ttl=255 time=0.189 ms
64 bytes from 10.54.31.191: icmp_seq=3 ttl=255 time=0.172 msRouting table:
mlnx-sn2100 [standalone: master] (config) # show ip routeFlags:
F: Failed to install in H/W
B: BFD protected (static route)
i: BFD session initializing (static route)
x: protecting BFD session failed (static route)
c: consistent hashing
p: partial programming in H/W192.168.1.0       255.255.255.0              0.0.0.0           mgmt0            direct     0/0
1.1.1.1           255.255.255.255            20.1.1.6          po2              bgp        200/0
3.3.3.3           255.255.255.255            20.1.1.8          po1              bgp        200/0
4.4.4.4           255.255.255.255            0.0.0.0           loopback1        direct     0/0
10.54.31.0        255.255.255.0              0.0.0.0           vlan130          direct     0/0
20.1.1.6          255.255.255.254            0.0.0.0           po2              direct     0/0
20.1.1.8          255.255.255.254            0.0.0.0           po1              direct     0/0
20.2.10.2         255.255.255.254            20.1.1.6          po2              bgp        200/0
20.2.10.4         255.255.255.254            20.1.1.6          po2              bgp        200/0On external switch ping to interface vlan 130 works:
Arista25G-1(config)#ping 10.54.31.91
PING 10.54.31.91 (10.54.31.91) 72(100) bytes of data.
80 bytes from 10.54.31.91: icmp_seq=1 ttl=64 time=0.133 ms
80 bytes from 10.54.31.91: icmp_seq=2 ttl=64 time=0.189 ms
80 bytes from 10.54.31.91: icmp_seq=3 ttl=64 time=0.065 ms
80 bytes from 10.54.31.91: icmp_seq=4 ttl=64 time=0.071 ms
80 bytes from 10.54.31.91: icmp_seq=5 ttl=64 time=0.122 msBut ping to device fails:
Arista25G-1(config)#ping 10.54.31.191
PING 10.54.31.191 (10.54.31.191) 72(100) bytes of data.— 10.54.31.191 ping statistics —
5 packets transmitted, 0 received, 100% packet loss, time 40msOn SN2100 tcpdump -i po1 shows:
09:52:56.947956 IP 20.1.1.8 > 10.54.31.191: ICMP echo request, id 20761, seq 1, length 80When the echo request is received by the SN2100 an entry in the ARP table is added, but no echo reply is send.Hey robertCould you add the running configuration of the Onyx switch and the arista device?Your TCPdump shows the ICMP source IP being 20.1.1.8. Have you tried sourcing the ping in arista to interface Vlan130?Hi Robert,Excellent work isolating the behavior so far! This looks to be a bit more involved for debugging so I’d recommend opening a support case in the event developer input is needed. You can open a support case by visiting this link: https://support.mellanox.com/s/contact-support-pageThank you!
RyanHi,
20.1.1.8 is the portchannel (po1) on which the arista is connected.
And yes, I could ping the interface vlan 130, this is shown above as “ping 10.54.31.91”.I will open a case.
Thanks.Powered by Discourse, best viewed with JavaScript enabled"
761,about-mellanox-driver-issue-ofed-and-en,"Hi,We ever installed OFED and EN of Connect X5 driver on the same serverWe noticed that after installing both OFED and EN driver, the lan-card seems not functional…
When using VMA mode ( we call it through API ), no VMA detail message is shown.
and it occasionally ( from 2 weeks to 6 months, sent around 20000 messages per day ) sent some corrupted message to another peer which caused serve production issue on our side.May we know whether the improper driver installation ( both OFED and EN installed ) could causes message corruption ?Many Thanks,
MichaelYou don’t need both. If you use CX5 ETH NIC, EN driver is enough.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
762,best-drivers-on-ubuntu-22-04-for-rtx-2070-mobile-max-q,"
Screenshot from 2022-12-11 11-31-191265×765 97.3 KB

I don’t know the best drivers on ubuntu for my graphic can any body tell me how can i chose the right one.Powered by Discourse, best viewed with JavaScript enabled"
763,performance-issue-with-inbox-drivers,"While evaluating the Mellanox ConnectX-5 network cards, we’ve encountered some network bandwidth issues when using the inbox drivers.To measure network performance, we equip two machines with ConnectX-5 cards and connect them using a QSFP28 100G copper cable. The client machine has an Intel Core i9 9900 CPU, the server has an Intel Xeon Silver 4208 CPU.The performance obtained when running 3 instances of the iperf3 program are as follow:~60Gbit/s with the version 5.13 of the Linux kernel that comes with Ubuntu 21.10~80Gbit/s with the version 5.16.10 of the Linux kernel~95-100Gbit/s with the drivers that come with the Mellanox OFED packageWe followed the recommendations found in the performance tuning guide (https://community.mellanox.com/s/article/performance-tuning-for-mellanox-adapters), but it did not result in any improvement for both inbox drivers. The iperf program gives the same result.We have multiple questions:Is there some configuration we missed that could explain this gap of performance?What exactly are the differences between the drivers found in the upstream Linux kernel and the ones in the Mellanox OFED packages?What are the plans for the upstream driver ?Hello Maxime,Thank you for posting your inquiry on the NVIDIA Networking Community.For performance recommendations when running INBOX drivers, support and recommendations are provided by Linux OS distro vendor.When we run benchmark tests for TCP, we only use iperf(2) as iperf3 lacks several features like multithreading , multicast and bi-directional tests. See the following link on how-to run iperf successfully → https://community.mellanox.com/s/article/howto-install-iperf-and-test-mellanox-adapters-performanceThe article you referring to, is the correct article to use when you want to further optimize the tuning.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
764,select-host-queues-from-the-nic,"Hi all,
Is there a way to define the queue to which packets should go on the host, but on the ARM cores of the bluefield 2. We’re using a DPDK-based application on the BF, but I guess the question is generic and the same with TC or OVS would be fine if the capability to attach this kind of metadata to rules or packets is something  possible.
Thanks,
TomHi @t.barbette,If you are using DPDK, you can use rte flows. You can read about the generic flow API in the link below.
https://doc.dpdk.org/guides/prog_guide/rte_flow.htmlRegards,
ChenHi Chen!Thank you very much. However the documentation does not cover this specific case. I would like to add a rule (using rte_flow indeed) from the ARM cores that targets a specific queue on the HOST. The goal is to enable load-balancing on the HOST from the ARM. So I can send traffic from a big flow to an underloaded CPU, for instance.Thanks,
TomPowered by Discourse, best viewed with JavaScript enabled"
765,is-oracle-linux-still-supported,"Hello,The MLNX_OFED LTS driver has support for Oracle Linux up to version 8.3. Will there be support for newer versions? Can we expect Rocky Linux 8 support now when CentOS 8 is EOL?Hello, Devin,Thank you for posting your inquiry on the NVIDIA Networking Community.Yes, Oracle Linux is very much supported. Please review the following for the supported versions and validated/tested kernels → https://docs.nvidia.com/networking/display/MLNXOFEDv551032/General+SupportAs the LTS version has a lesser release cadence, please keep an eye on the driver download site when the new version is released for support for newer version.Currently there is no support for Rocky Linux, we are evaluating this internally, and we will release an announcement about this in the next few months.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
766,performance-counters-for-accelerators,"I see performance counters here for BlueField-2. https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/Performance+Monitoring+Counters  Is it possible to access performance counters for accelerators (regex, compression, and PKA), like monitoring the utilization of accelerators?Thanks in advance!Hi HKylin,Thank you for posting your inquiry to the NVIDIA Developer Forums!Unfortunately, there are no exposed performance counters for the hardware offload functions on the Bluefield-2, the counters detailed in the DPU OS documentation you’ve linked are the only ones exposed (aside from traditional adapter performance counters).The adapter’s traditional performance counters can be read via NEO-Host, but this does not include any of the hardware offload/acceleration counters you are attempting to gather. For more information on NEO-Host installation, please review the following section (‘Installing NEO-Host Using mlnxofedinstall Script’) in the MLNX_OFED user manual:https://docs.nvidia.com/networking/display/MLNXOFEDv590560/Installing+MLNX_OFED#InstallingMLNX_OFED-neo-hostinstallationInstallingNEO-HostUsingmlnxofedinstallScriptOnce installed, these counters can be read with the included python script within the NEO-Host SDK, for example:# python3 /opt/neohost/sdk/get_device_performance_counters.py --mode=shell --dev-uid=0000:d8:00.0 --get-analysis --run-loopThanks,
NVIDIA Enterprise SupportThe method takes advantage of collecting hardware performance counters (also known as profiling counters) during empirical tuning. I see performance  spacebar counter   counters here for BlueField-2. … Is it possible to access performance counters for accelerators (regex, com… This paper investigates the use of CPU performance counters for estimating the quality of hybrid CPU-accelerator systems generated by HLS tools.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
767,connect-x4-lx-pxe-does-not-appear-in-dell-poweredge-uefi-boot-options,"I have a Dell R7525 server with a Connect-X4 Lx (MCX4131A-BCAT) PCIe card. We run the server in UEFI mode, and we PXE-boot it, but I don’t see the card as an option for PXE boot.Before posting, I made sure that firmwares etc. were up-to-date. We upgraded the R7525 to BIOS 2.1.6. We also upgraded the card to FlexBoot Family Firmware version 14.30.10.04 and EFI Firmware version 14.23.17.I also found that, although the EXP_ROM_PXE_ENABLE was on, EXP_ROM_UEFI_x86_ENABLE was disabled, so I fixed that; now both settings are on.At this point, I can see the card in the Dell’s list of configurable devices (along with the storage controller, the daughtercard NICs, etc. But I still don’t see that the card as an option for PXE boot (I only see the daughtercard ports).Hello Karl,Thank you for posting your question on the Mellanox Community.Please try setting UEFI_HII_EN to true on the adapter. This setting can be found in mlxconfig so the command would be:mlxconfig -d  s UEFI_HII_EN=1More information on mlxconfig can be found here:https://docs.mellanox.com/pages/viewpage.action?pageId=47026360After this try rebooting the node to see if the adapter is an available as an option for PXE boot.Thank you,Abigail.Hi Abigail, good evening,I just checked, and UEFI_HII_EN is already set to True(1).~ KarlP.S. I just realized it might help to give the full configuration. So, here is the output of mlxconfig -d … query: gist:c29a2abd4282d3551c5c2391b5971f72 · GitHubHello Karl,Thank you for the response.To further troubleshoot this issue we will open a support ticket to work on this issue as you have a valid support contract. We will close this case and we will update the support ticket once we have opened it.Thanks and regards,~Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
768,about-mcx75310aas-heat-transceivers,"MCX75310AAS-HEAT  200G speed but The interface is osfp  Which type of  transceivers   I need to usePls check below RN,https://docs.nvidia.com/networking/display/ConnectX7Firmwarev28371014/Firmware+Compatible+ProductsDo I need to use 400G OSFP or 200G OSFP transceiversIf you use 400GE/NDR then you need OSFP, 200GE/HDR is QSFP.The link list supported transceiver model, you can select as reference.The  list not found NDR 200 OSFP transceivers
So please make sure which transceivers the OSFP 200G adapter usesI checked with product team, NDR200 OSFP transceiver EOL.Now you can use NDR400 OSFP transceiver on HCA NDR200 port, then link to switch.EG,MMA4Z00-NS400, MMS4X00-NS/NL400This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
769,deep-convolutional-generative-adversarial-network-example-from-manual,"Hi- I’m trying to run the Deep Convolutional Generative Adversarial Network example from section 2.2 of the 9.2 machine-learning-manual.pdf. The Hello World from section 2.1 works fine, but I get some warnings and failure with DCGAN. The manual doesn’t mention needing a Google authentication bearer token, but I’m not sure if that’s it.I was able to resolve the Google warning with$ export NO_GCE_CHECK=‘true’But I’m still getting these errors:Thanks to Andrew in BCM support: Setting the TF_FORCE_GPU_ALLOW_GROWTH environment variable to “true” allowed the example to run perfectly.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
770,too-much-irq-on-only-one-queue-with-vma,"Hi,We are using VMA in multiple processes on a box with profile=latency and we can see that each time we write on the socket an IRQ is triggered and they are all on the same IRQ 241 (mlx5_comp0@pci:0000:41:00.0).Without VMA, we can see that IRQ are load balanced accross the different NIC queues and so different IRQ.We tried to specify different values for VMA_RING_ALLOCATION_LOGIC_TX but do not see any changes.How can me manage to have less IRQ and load balance them on different NIC queues/IRQ please?Regards,LaurentHello Danesi Laurent,Although VMA is an open source product, its support requires a valid contract. In the case if your organization have such contract, feel free to open an official support ticket by writing to Networking-support@nvidia.com with providing all details of the issue - application details, how to reproduce, log files, etc. Try to reproduce it using sockperf or iperf application, that can send/receive unicast and multicast traffic. In addition, please provide if this is a first time issue, if there is any degradation in performance.Without support contact, I would suggest to open a ticket on VMA page - https://github.com/Mellanox/libvma.In the case if you are interested in purchasing the support contract, please contact us by e-mail : Networking-contracts@nvidia.comPowered by Discourse, best viewed with JavaScript enabled"
771,rocev2-is-disabled-on-connectx-5-but-the-nic-still-captures-rocev2-packets,"Hi. I have a Mellanox ConnectX-5 with OFED 5.4 on Ubuntu 18.04 LTS . I want to parse some RoCEv2 packets with a custom program. So I need the ConnectX-5 not to capture RoCEv2 packets. I’ve followed the article How to Disable RoCE to disable the RoCE. If I get it right, now the RoCEv2 packets should be passed to kernel. But it seems the RoCE is still enabled:$ hping3 –udp -p 4791 No responding. This command will send some UDP packets to the port on the target host. If I use another port, the target will return an ICMP Unreachable which means the kernel receives the UDP packet. But sending UDP to port 4791 (the port RoCEv2 uses) gets no response so I assume the RoCEv2 is still enabled and the NIC drops the “malformed RoCEv2 packet”.Besides, I tried to remove the kernel module mlx5_ib to completely shutdown the functionality and it worked. But my program is actually a DPDK program that needs the mlx5_ib module, so this is not the solution.How can I gently disable the RoCEv2 and let the kernel gets the packets?I’m sorry if I misunderstand something. Thanks for your advice.Below are some info:$ ibv_devinfohca_id: mlx5_0transport: InfiniBand (0)fw_ver: 16.31.1014node_guid: ec0d:9a03:00bf:d92csys_image_guid: ec0d:9a03:00bf:d92cvendor_id: 0x02c9vendor_part_id: 4119hw_ver: 0x0board_id: MT_0000000012phys_port_cnt: 1port: 1state: PORT_ACTIVE (4)max_mtu: 4096 (5)active_mtu: 1024 (3)sm_lid: 0port_lid: 0port_lmc: 0x00link_layer: Ethernethca_id: mlx5_1transport: InfiniBand (0)fw_ver: 16.31.1014node_guid: ec0d:9a03:00bf:d92dsys_image_guid: ec0d:9a03:00bf:d92cvendor_id: 0x02c9vendor_part_id: 4119hw_ver: 0x0board_id: MT_0000000012phys_port_cnt: 1port: 1state: PORT_ACTIVE (4)max_mtu: 4096 (5)active_mtu: 1024 (3)sm_lid: 0port_lid: 0port_lmc: 0x00link_layer: Ethernet$ lspci | grep Mellanox82:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]82:00.1 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]$ cat /sys/bus/pci/devices/0000:82:00.*/roce_enable00$ cat /sys/class/infiniband/mlx5_*/ports/1/gid_attrs/types/3RoCE v2RoCE v2Hi,You might one of these methods:Be sure you have latest MOFED or latest MFT (Mellanox Firmware Tools) installedRun as root#mlxconfig -d 82:00.0 s ROCE_CONTROL=1#mlxconfig -d 82:00.1 s ROCE_CONTROL=1and reboot the host after that#which devlink#[/opt/mellanox/iproute2/sbin/devlink]#devlink dev param set pci/0000:82:00.0 name enable_roce value false cmode driverinit#[devlink ]dev reload pci/0000:82:00.0To restore#devlink dev param set pci/0000:82:00.0 name enable_roce value true cmode driverinit#devlink dev reload pci/0000:82:00.0Thank you very much!The second method won’t succeed with an error “kernel answers: Operation not supported” and I’m not sure what’s wrong. But anyway, the first method works well and it helps a lot.By the way, I’m wondering if it is possible to configure the RoCEv2 UDP destination port to a value other than 4791 so that I can bypass the NIC without disabling RoCEv2?Hi,Changing RoCEv2 port is not supportedThank you!Powered by Discourse, best viewed with JavaScript enabled"
772,when-gpu-is-installed-memory-pegs-at-100-on-excel-scrolling,"Problem: Over remote desktop connection, memory spikes on host computer to 100% & Excel freezesProblem does NOT occur when GPU is removed or disabled in hardware managerSame Problem on two systems (See Specs Below) using Excel Workbook over remote connection.Problem Description:Using the scroll wheel only scrolling up and down sheet 1 of the workbook for a few minutes:Remote Connection Information:Workbook Information (Sheet 1)Cures tried with no positive resultsSYSTEM 1Processor AMD Ryzen Threadripper PRO 5955WX 16-Cores 4.00 GHzInstalled RAM 32.0 GB (31.9 GB usable)System type 64-bit operating system, x64-based processorVideo Card NVIDIA T1000 8GBDisk 1TB SSDEdition Windows 11 ProVersion 21H2Installed on ‎5/‎17/‎2022OS build 22000.708Experience Windows Feature Experience Pack 1000.22000.708.0Microsoft® Excel® for Microsoft 365 MSO (Version 2205 Build 16.0.15225.20172) 64-bitWindows 11 & Office 365 – Set to Automatic UpdateSYSTEM 2Processor Intel(R) Core™ i7-7700K CPU @ 4.20GHz 4.20 GHzInstalled RAM 32.0 GB (31.9 GB usable)System type 64-bit operating system, x64-based processorVideo Card NVIDIA Quadro M4000 8GBEdition Windows 10 ProVersion 21H2Installed on ‎9/‎9/‎2020OS build 19044.1645Experience Windows Feature Experience Pack 120.2212.4170.0Microsoft® Excel® for Microsoft 365 MSO (Version 2205 Build 16.0.15225.20172) 64-bitWindows 10 & Office 365 – Set to Automatic UpdateHi,Did you manage to solve this?I have the same problem. I’ve tried searching the Microsoft forms for help, but I’m yet to see a solution.Many thanks,DamianNope, no resolution. I have just disabled the GPU - and am working without it.Waiting for Microsoft to release a secret patch to fix what the broke. I hope.Powered by Discourse, best viewed with JavaScript enabled"
773,welcome-mellanox-customers,"Hello Mellanox customers,Welcome to the NVIDIA Forums! If this is your first time here, we recommend that you create a new account. Users of the old Mellanox forums will need to create accounts to allow posting. Mellanox users should use the same email address that was used in the old forums, this allows us to link your past activity to your new profile.To register and create your account, please go to https://developer.nvidia.com/ and click “Join”.Once registered, you will have access to the forums. Please read this user guide to learn more about the new platform and features.We hope you like the new forums. Please post your comments or suggestions in the Forum Feedback category.Powered by Discourse, best viewed with JavaScript enabled"
774,does-bluefield-v1-smartnic-support-doca,"I am interested, does Bluefield v1 SmartNic support DOCA and all features of DOCA SDK? Did not find anything about itStability, improvement, features, bugs fixes, development etc started mainly on BF2 & on (BF3) DPU platforms.I posted a reference link below should you want to browse as applicable and as needed.NVIDIA DOCA Software Framework Accelerate application development for the NVIDIA BlueField DPU Access DOCA NVIDIA® DOCA™ is the key to unlocking the potential of the NVIDIA BlueField® data processing unit (DPU) to offload, accelerate, and isolate...This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
775,installing-ubuntu-18-04-on-bluefield-2-failed,"For some reasons, I have to install Ubuntu 18.04 instead of 20.04 on Bluefield-2.
I tried to use bfb images from https://developer.nvidia.com/zh-cn/networking/doca
However ALL Ubuntu 18.04 bfb images are for Bluefield, not Bluefield-2.
Pushing 18.04 bfb always shows ‘cat: connection timeout’ and fails.So, is there any way to install Ubuntu 18.04 on Bluefield-2? Thanks a lot.Hello,Ubuntu 20.04 is the default OS on the Bluefield-2 platform.For more information on supported DPU operating systems and versions, please see the following:https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/Supported+Platforms+and+Interoperability#SupportedPlatformsandInteroperability-EmbeddedSoftwareEmbeddedSoftwareThank you,
-Nvidia Network SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
776,dvorak-keyboard,"I just need to know how to set my keyboard to Dvorak.  Generally I do it through the GUI, which isn’t present.  Otherwise, I use setxkbmap, which is again, missing.  I see Dvorak as an option out in /usr/share/X11/xkb/symbols/us, and have set the /etc/default/keyboard, and run the dpkg-reconfigure console-setup-linux, but it does not change my keyboard.Is there any help for me?  I can muddle through on a QWERTY keyboard, but I haven’t really used one in 15-20 years. It will slow me up, and I’m afraid that it will introduce errors.Thank you for any help offered.  I appreciate it.Hello,Welcome to the NVIDIA Developer forums. You posted in the networking section of the community. Not sure how your issue relates to NVIDIA products, but it looks like the Linux category best fits your issue. I will move it over for you.Tom KSorry.  It is Cumulus Linux that I’m having a problem with.  I should have mentioned that.  It is used at work, and so I have installed it in a VM here at home and am trying to learn the ins and outs of it, but I’ve been stuck on this all weekend.I don’t know about cumulus linux but in general, the console keyboard layout should be set by systemd to the one set in /etc/vconsole.conf
Requires reboot/restart of the unit.
Temporarily, you can set the keyboard using
sudo loadkeys Addendum: the systemd keymaps can be queried/set with localectl
sudo localectl list-keymaps
sudo localectl set-keymap@jcwoodburn73, Sorry, I will move it to the Cumulus Linux category so the support team has visibility.Best,
Tom K/etc/vconsole.confCan you try to modify the layout with the following command?sudo dpkg-reconfigure keyboard-configurationI’ve run it a number of times.  I picked:My /etc/default/keyboard reads:/etc/vconsole.conf
  KEYMAP=dvSo, it looks like it is configured, it just isn’t being loaded.AND … VirtualBox just broke, so I can’t do anything with it until its fixed.  Not my day.Ok.  Virtual Box re-installed, dpkg-reconfigure re-run.  Looks just like it did, though there is no vconsole.conf file. My keyboard is still QWERTY.So I had to re-install VirtualBox, and I re-imported the Cumulus Linux .ova file.There is no /etc/vconsole.conf file.  When I tried the localectl list-keymaps, “Failed to read list of keymaps: No such file or directory”.Please install the keymaps
sudo apt install console-keymapsNo go.E: Unable to locate package console-keymapsHey!I’ve tried to setup a virtual box with Cumulus VX running 4.3.0 and I have not been able to change the layout. As you mentioned, some of the deb files required form other forums suggesting how to enable dvorak are not available in cumulus linux.I used a windows VirtualBox app. I I change the layout on the vbox host (windows) to dvorak, this is passed to the console window. I wonder how you are using vbox in your environment and if this is an optionAnother alternative is once you have your Cumulus Box you could SSH into the device rather than using the console. SSH should inherit the keystrokes passed transparently from your client to the switch.If that is still not the case, you can still push ansible playbook which you create in any environment that has support for your dvorak keymap .Powered by Discourse, best viewed with JavaScript enabled"
777,error-with-configuring-ovs-dpdk-on-bluefiled-2,"I am trying to configure ovs-dpdk on Bluefield-2 in embedded mode to offload the flows on it according the Configuring OVS-DPDK Offload with BlueField-2 document (Mellanox Interconnect Community).
after executing the commands and restarting the openvswitch, the openvswitch status shows this errors:|00018|dpdk|EMER|Unable to initialize DPDK: Invalid argument
ovs-vswitchd: Cannot init EAL (Invalid argument)
ovs|00002|daemon_unix|ERR|fork child died before signaling startup (killed (signal 6), core dumped)
ovs|00003|daemon_unix|EMER|could not initiate process monitoringAlso, it the ovs-vsctl show:Bridge ovs_dpdk_br0
datapath_type: netdev
Port ovs_dpdk_br0
Interface ovs_dpdk_br0
type: internal
Port dpdk1
Interface dpdk1
type: dpdk
options: {dpdk-devargs=“0000:03:00.0,representor=[0,65535]”}
error: “Error attaching device ‘0000:03:00.0,representor=[0,65535]’ to DPDK”
Port dpdk0
Interface dpdk0
type: dpdk
options: {dpdk-devargs=“0000:03:00.0”}
error: “could not add network device dpdk0 to ofproto (Resource temporarily unavailable)”
ovs_version: “2.17.7-e054917”The “systemctl restart openvswitch” command shows this error:
Failed to restart openvswitch.service: Unit openvswitch.service not found.Are these problems related together?I didn’t find any solution for this error. Would you please help me with this.Also, the document didn’t mention anything about binding the interfaces to DPDK drivers. Is it necessary or not?Thank you allThere is no straight answer(s)for these errors, further troubleshooting/validation needs to take place.
Though, it could be very well related to ovs-switchd having no or not enough memory allocated/reserved for vswitch to use (IE: hugepages).I would make sure latest DPU image/FW are in place
I would verified that configuration steps were properly followed (enabling ovs-dpdk hardware offload) https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest
I would make sure to configure hugepages (grep -sri HugePages_ /proc/meminfo)
Maybe test first DPDK via testpmd on the DPU to make sure that DPDK functions as expectedCheck the syslog, messages, ovs-vswitchd.log files for errors or pointers.Useful link:
https://enterprise-support.nvidia.com/s/article/Basic-Debug-utilities-with-OVS-DPDK-offload-ASAP-Directsystemctl status ovs-vswitchd -lLots to check and validate to narrow down and isolate the issue.I tried everything and check all the points that you mentioned but nothing worked. OVS still cannot initialize dpdk and cannot attach the ports to the bridge.Powered by Discourse, best viewed with JavaScript enabled"
778,doca,"Please HelpI was trying to run this doca application by:./doca_url_filter -a 0000:01:00.3,class=regex:eth,representor=[196609],sft             _en=0 -a 0000:01:00.4,class=regex:eth,representor=[196610],sft_en=0 -l 0-7 – -pIt suggests:

image1461×316 21.7 KB
By the way, I am trying to run this application on the host so I don’t understand if it is because the trust mode set for SF, though I tried the trust mode setting for SF by ’mlxdevm‘.I thought the sft_en=0 is mean to skip the usage of sft on the DPU, it seems that do not work?Powered by Discourse, best viewed with JavaScript enabled"
779,do-all-adapter-cards-specially-mcx515a-gcat-support-all-cables-and-sfp-listed-in-mellanox-connectx-5-firmware-release-notes,"Good morning. We have bought a MCX515A-GCAT with ConnectX-5 chip and I only find the “Product brief” sheet. Is there any other one specification sheet? That brief sheet says “interface type: QSFP28”. We wanted to know if the card is compatible with other QSPF modules, and in other answer is said that one can take a look to Mellanox ConnectX®-5 Firmware Release Notes. Is that correct?Hello Alberto,Thank you for posting your inquiry on the NVIDIA Networking Community.Yes, in the latest RN of the f/w you will have the full list of supported cables and transceivers for the full ConnectX-5 family of VPI and Ethernet adapters.You can find the previous supported cables list as well based on the archive RN’sBe aware that every ConnectX adapter has its own support cable list, mentioned in their respective f/w RN.Thank you and regards,~NVIDIA Networking Technical SupportThanks, Martijn,so those compatible transceiver with form factor SFP28 or SFP+ listed in RN of the f/w can be plugged in card’s QSFP28 ports using an adapter such as NVIDIA MAM1Q00A-QSA28 one, isn’t it? Without no further configuration…Besides, that card will be connected to a Open Ethernet Switch SN2100, so I was wondering of there is a similar list of cables and transceivers compatible, since in the manual I can’t find it. Anyway, I guess that SN2100’s compatibility will be higher to any Mellanox QSFP28, QSFP+ and QSFP pluggable, isn’t it?Thank you.Powered by Discourse, best viewed with JavaScript enabled"
780,application-examples,"Hi all,
I noticed that the DOCA documentations have been updated this week (July 6) But I had installed my doca SDK before and don’t see some of the example applications that is added to the new version.
I can only see the following applications:
ar common netflow url_filter
For example, I don’t see simple_fwd_vnf folder.
How can I get an update? Should redo the installation process?Yes, please make sure you’re updated to DOCA 1.1 | DOCA SDK 0.2. I think we’re working on some issues with that in another thread.This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
781,what-is-the-correct-bluefield-2-l3-cache-size,"Hello,according to the datasheet and the output of lscpu the BlueField-2 should have 6MB L3 cache.
When looking at the output of dmidecode it reports 12MB L3 cache.Now when looking at the BlueField-1, there are 2 banks with 6MB L3 cache each according to the datasheet, which I suspect will be similar to the BlueField-2.What is the correct size of the L3 for the BlueField-2?Best,
DanielPowered by Discourse, best viewed with JavaScript enabled"
782,dual-port-connectx-5-with-network-bandwidth-issue,"Currently, we are using ConnectX-5 NICs slotted in gen 3 PCIe x16 slots. They are dual ported, although we are attempting to utilize only 1 of the channels to use all 16 lanes available in the PCIe.The expected network bandwidth that we should be getting is ~80 Gb/s to 100 Gb/s. Instead we are reaching ~60 Gb/s or less. This is due to the PCIe still thinking we have 2 ports plugged in rather than only 1, so we are only utilizing 8 lanes instead of 16.This is referenced in the article: PCIe configuration for max performance, where the max PCIe bandwidth is calculated to beMaximum PCIe Bandwidth = 8G * 8 * (1 - 2/130) - 1G = 64G * 0.985 - 1G = ~62Gb/s.Which explains why we are only getting ~60Gb/s. Again, the goal is to try to utilize all 16 lanes to increase the bandwidth.For reference we also have a single port ConnectX-5 that is also connected to a PCIe x16 on a 6 core Intel(r) Xeon(R) W-2133 CPU @ 3.60Ghz w/ 64 GB of RAM. This ConnectX-5 produces ~80Gb/s.The dual port ConnectX-5 are on similar machines with the same specs.I already sent in a case ticket, but we figured that making a thread would also help.Hi, were you able to find a solution to increasing your bandwidth? Im stuck at 55 Gb/s max and cannot break it no matter what I do. Been trying to solve this slow speed on my 100G network for a month now.Powered by Discourse, best viewed with JavaScript enabled"
783,connect-x6-dx-best-firmware-for-vmware-sriov,"Hi could you please tell me what is the best firmware for ConnectX-6 DX for Vmware 7.0U3 to support  SRIOV/DPDK?
I have a Dell mellanox PSID: DEL0000000027According to this page, I should get 22.34ConnectX® Ethernet Driver for VMware® ESXi ServerBut as my psid is not listed on nvidia downloads for connectx-6 I cannot get it.I found a 22.32 in the download section dedicated to Dell OEM devicesOn Dell site, I can only get 22.28.Can you please adivse if there is 22.34 that I could use on my cards?Thanks,Hi a.piaszIt’s simple. As your HCA is DELL OEM, you should download HCA FW at Dell./HyungKwangPowered by Discourse, best viewed with JavaScript enabled"
784,opensm-was-dead,"Hello jyh,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.To answer your questions:For more information, please review the following section of the MLNX_OFED driver UM → https://docs.nvidia.com/networking/display/MLNXOFEDv561033/OpenSMThank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
785,how-to-memcpy-in-linux-driver-directly-to-gpu,"Hello,I have a 3rd party device that is filling a linux kernel buffer with data.  I would like to (in the driver) copy that data directly to GPU.I think is sort of like gdrcpy except that I want to do the memcpy in the driver.My first attempt was to:This memcpy fails spectacularly.  (entire server requires a hard power cycle.)What step did I miss?  I can add the mmap from gdrcopy but is it only for user space?  Or is there some pointers created in the mmap call that I can also use to do the memcopy in the driver?hi BrandyYou can referernce with such doc:The API reference guide for enabling GPUDirect RDMA connections to NVIDIA GPUs.Hello, thank you.  Yes, of course, I have referenced that document and it works great from user space.  But I wanted to keep the interactions in the driver.  So what I needed to do was memremap the nvidia page table physical addresses to a kernel virtual address.  Then I could keep all the interactions in the kernel.here is the snippet of codehi brandySuch detail requirement I suggest you contact Nvidia support.Thank you
Meng, ShiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
786,mt28908-family-connectx-6-wqe-dump-wq-size-1024-wq-cur-size-0-wqe-index-0x63-len-128,"Hello! There is a problem. Today my server with mellanox 100G card stop working.In messages many such entries:Jun 2 06:29:39 138224 kernel: [934519.424191] 00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.424195] 00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.424198] 00000020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.424199] 00000030: 00 00 00 00 04 00 51 04 0a 00 02 83 b8 63 dc d2Jun 2 06:29:39 138224 kernel: [934519.424202] WQE DUMP: WQ size 1024 WQ cur size 0, WQE index 0x63, len: 128Jun 2 06:29:39 138224 kernel: [934519.424203] 00000000: 00 b8 63 0a 00 02 83 05 00 00 00 08 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.424205] 00000010: 00 00 00 00 c0 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.424206] 00000020: 00 00 00 42 00 00 22 00 00 00 00 00 ee 84 50 feJun 2 06:29:39 138224 kernel: [934519.424207] 00000030: 00 00 02 f8 00 00 22 00 00 00 00 00 f4 41 2d 08Jun 2 06:29:39 138224 kernel: [934519.424208] 00000040: 00 00 02 74 00 00 22 00 00 00 00 00 ef 67 10 00Jun 2 06:29:39 138224 kernel: [934519.424211] 00000050: e7 d4 00 00 01 01 08 0a 66 55 ef f7 22 a1 a2 2bJun 2 06:29:39 138224 kernel: [934519.424214] 00000060: 00 00 0b 50 00 00 22 00 00 00 00 00 f3 4a d0 b5Jun 2 06:29:39 138224 kernel: [934519.424217] 00000070: ea 5f 40 00 40 06 15 d3 32 07 ee 1a 05 3b 04 25Jun 2 06:29:39 138224 kernel: [934519.506293] 00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.506295] 00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.506300] 00000020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.506302] 00000030: 00 00 00 00 30 10 68 02 29 00 02 83 00 6d 2b d2Jun 2 06:29:39 138224 kernel: [934519.506306] WQE DUMP: WQ size 1024 WQ cur size 0, WQE index 0x6d, len: 64Jun 2 06:29:39 138224 kernel: [934519.506307] 00000000: 00 00 6d 29 00 02 83 02 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.506308] 00000010: 00 00 00 00 c0 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.506311] 00000020: c7 7d d9 96 0c 42 a1 0a 30 92 08 00 45 00 0a bcJun 2 06:29:39 138224 kernel: [934519.506314] 00000030: 28 a9 40 00 40 06 a1 50 32 07 ee 1a b0 3b 95 e5Jun 2 06:29:39 138224 kernel: [934519.562513] 00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.562515] 00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.562517] 00000020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.562524] 00000030: 00 00 00 00 30 10 68 02 29 00 02 83 00 00 5d d2Jun 2 06:29:39 138224 kernel: [934519.562528] WQE DUMP: WQ size 1024 WQ cur size 0, WQE index 0x0, len: 64Jun 2 06:29:39 138224 kernel: [934519.562532] 00000000: 00 00 00 29 00 02 83 02 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.562533] 00000010: 00 00 00 00 c0 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.562535] 00000020: c7 7d d9 96 0c 42 a1 0a 30 92 08 00 45 00 0b 84Jun 2 06:29:39 138224 kernel: [934519.562536] 00000030: d1 d5 40 00 40 06 8b b3 32 07 ee 1a 4d de 63 ebJun 2 06:29:39 138224 kernel: [934519.630932] 00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.630934] 00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.630935] 00000020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.630939] 00000030: 00 00 00 00 30 10 68 02 29 00 02 83 00 00 54 d2Jun 2 06:29:39 138224 kernel: [934519.630945] WQE DUMP: WQ size 1024 WQ cur size 0, WQE index 0x0, len: 64Jun 2 06:29:39 138224 kernel: [934519.630951] 00000000: 00 00 00 29 00 02 83 02 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.630956] 00000010: 00 00 00 00 c0 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.630957] 00000020: c7 7d d9 96 0c 42 a1 0a 30 92 08 00 45 00 0a bcJun 2 06:29:39 138224 kernel: [934519.630958] 00000030: dd 29 40 00 40 06 ea 4a 32 07 ee 1a 55 73 f3 32Jun 2 06:29:39 138224 kernel: [934519.659629] 00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.659631] 00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.659633] 00000020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00Jun 2 06:29:39 138224 kernel: [934519.659635] 00000030: 00 00 00 00 30 10 68 02 29 00 02 83 00 00 ae d2Jun 2 06:29:39 138224 kernel: [934519.659637] WQE DUMP: WQ size 1024 WQ cur size 0, WQE index 0x0, len: 64after thatJun 2 06:29:40 138224 kernel: [934519.921473] ------------[ cut here ]------------Jun 2 06:29:40 138224 kernel: [934519.921495] WARNING: CPU: 84 PID: 0 at drivers/iommu/iova.c:817 iova_magazine_free_pfns.part.13.cold.23+0x8/0xfJun 2 06:29:40 138224 kernel: [934519.921496] Modules linked in: fuse btrfs zstd_compress zstd_decompress xxhash ufs qnx4 hfsplus hfs minix vfat msdos fat jfs xfs dm_mod binfmt_misc msr mst_pciconf(OE) amd64_edac_mod edac_mce_amd kvm_amd kvm ipmi_ssif irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcspkr aufs(OE) ast ttm drm_kms_helper drm joydev i2c_algo_bit evdev sg ccp rng_core sp5100_tco ipmi_si ipmi_devintf ipmi_msghandler pcc_cpufreq acpi_cpufreq button tcp_bbr sch_fq bonding lp parport loop ip_tables x_tables autofs4 ext4 crc16 mbcache jbd2 fscrypto ecb raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid0 multipath linear hid_generic usbhid hid raid1 md_mod sd_mod crc32c_intel aesni_intel aes_x86_64 crypto_simd cryptd glue_helper ahci xhci_pci libahciJun 2 06:29:40 138224 kernel: [934519.921547] xhci_hcd libata mlx5_core(OE) nvme mlxfw(OE) nvme_core scsi_mod usbcore mlx_compat(OE) devlink i2c_piix4 usb_common [last unloaded: mst_pci]Jun 2 06:29:40 138224 kernel: [934519.921557] CPU: 84 PID: 0 Comm: swapper/84 Tainted: G OE 4.19.0-16-amd64 #1 Debian 4.19.181-1Jun 2 06:29:40 138224 kernel: [934519.921558] Hardware name: Supermicro AS -2124BT-HNTR/H12DST-B, BIOS 1.1 01/10/2020Jun 2 06:29:40 138224 kernel: [934519.921560] RIP: 0010:iova_magazine_free_pfns.part.13.cold.23+0x8/0xfWhat to do?It is difficult to conclude that failure happens in ConnectX-6 code. WQDUMP are informational messages. Check the log/dmesg and see if there are any errors related to mlx5 driver.The error itself comes from “drivers/iommu/iova.c:817” code, that is not a Mellanox areaBe sure you are using latest Mellanox OFED GA v5.3 and the firmware. In the case of AMD platform, if the issue is reproducible, check the tuning guide including grub configuration - https://www.amd.com/system/files/TechDocs/56224.pdfАлексей, спасибо за Ваш ответ! Прочитал рекомендации, немного дополнил свой стартапскрипт, сейчас он выглядит так:#!/bin/bashset_irq_affinity.sh eth0mlnx_tune -p HIGH_THROUGHPUTtuned-adm profile throughput-performanceethtool -C eth0 adaptive-rx off adaptive-tx offethtool -K eth0 lro onethtool -G eth0 tx 8192 rx 8192ethtool -C eth0 rx-usecs 0 rx-frames 10 tx-usecs 16 tx-frames 100echo “mq-deadline” > /sys/block/nvme1n1/queue/schedulerecho “mq-deadline” > /sys/block/nvme0n1/queue/schedulerecho “mq-deadline” > /sys/block/nvme2n1/queue/schedulerecho “mq-deadline” > /sys/block/nvme3n1/queue/schedulerecho “mq-deadline” > /sys/block/nvme4n1/queue/schedulerecho “mq-deadline” > /sys/block/nvme5n1/queue/schedulerРасскидывание интераптов по NUMA делает хуже, потому что ядер 4, а очередей 64. Все 4 ядра NUMA node4 сразужестают загруженными IRQ 100% истановится только хуже.n# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianAddress sizes: 43 bits physical, 48 bits virtualCPU(s): 128On-line CPU(s) list: 0-127Thread(s) per core: 1Core(s) per socket: 64Socket(s): 2NUMA node(s): 32Vendor ID: AuthenticAMDCPU family: 23Model: 49Model name: AMD EPYC 7742 64-Core ProcessorStepping: 0CPU MHz: 3249.788CPU max MHz: 2250,0000CPU min MHz: 1500,0000BogoMIPS: 4500.17Virtualization: AMD-VL1d cache: 32KL1i cache: 32KL2 cache: 512KL3 cache: 16384KNUMA node0 CPU(s): 0-3NUMA node1 CPU(s): 4-7NUMA node2 CPU(s): 8-11NUMA node3 CPU(s): 12-15NUMA node4 CPU(s): 16-19NUMA node5 CPU(s): 20-23NUMA node6 CPU(s): 24-27NUMA node7 CPU(s): 28-31NUMA node8 CPU(s): 32-35NUMA node9 CPU(s): 36-39NUMA node10 CPU(s): 40-43NUMA node11 CPU(s): 44-47NUMA node12 CPU(s): 48-51NUMA node13 CPU(s): 52-55NUMA node14 CPU(s): 56-59NUMA node15 CPU(s): 60-63NUMA node16 CPU(s): 64-67NUMA node17 CPU(s): 68-71NUMA node18 CPU(s): 72-75NUMA node19 CPU(s): 76-79NUMA node20 CPU(s): 80-83NUMA node21 CPU(s): 84-87NUMA node22 CPU(s): 88-91NUMA node23 CPU(s): 92-95NUMA node24 CPU(s): 96-99NUMA node25 CPU(s): 100-103NUMA node26 CPU(s): 104-107NUMA node27 CPU(s): 108-111NUMA node28 CPU(s): 112-115NUMA node29 CPU(s): 116-119NUMA node30 CPU(s): 120-123NUMA node31 CPU(s): 124-127Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd sev ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca4Если просто выполнять set_irq_affinity.sh eth0IRQ 100% вылазитна CPU000Если выполнить set_irq_affinity_cpulist.sh 64-127 eth0IRQ 100% вылазит на CPU068 (смотрите вложение)Если выполнить set_irq_affinity_cpulist.sh 8-71 eth0IRQ 100% вылазит на CPU018Firmware и driver последней версии:driver: mlx5_coreversion: 5.3-1.0.0firmware-version: 20.30.1004 (MT_0000000225)expansion-rom-version:bus-info: 0000:41:00.0supports-statistics: yessupports-test: yessupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: yesСо всеми оптимизациями никуда не уходит проблема, что в один прекрасный момент накаком-то ядре становится загрузка 100% и трафик уходит вниз.Почему такое неравномерное распредлеление irq по ядрам?Powered by Discourse, best viewed with JavaScript enabled"
787,counter-to-measure-pcie-limitations,"Hi,We are using a ConnectX-5 100Gb/s
adapter with the Linux kernel driver
running a program using express data
path (XDP) on ubuntu 20. These are the
driver details printed by ethtool -i.driver: mlx5_core
version: 5.19.0-38-generic
firmware-version: 16.34.1002 (MT_0000000011)
expansion-rom-version:
bus-info: 0000:ca:00.0
supports-statistics: yes
supports-test: yes
supports-eeprom-access: no
supports-register-dump: no
supports-priv-flags: yesWe are running some packet processing
throughput benchmarks. We’re able to use
a traffic generator (t-rex) connected
through a single cable with the CX-5
interface on the device under test
(DUT).A simple packet-forwarding XDP program
on the DUT forwards 86 million packets
per second (Mpps) for 64 byte packets
with 14 cores. However, the true limit
on packets/sec to hit 100 Gbit/s is 148
Mpps. Our traffic generator runs on an
identical machine configuration, and
using DPDK can indeed hit this 148 mpps
limit.We suspect that the limitation in
throughput arises from the combination
of PCIe and driver.We came to know that the rx_discards_phy
counter printed by ethtool
indicates when there are drops from the
physical layer due to backpressure from
PCIe during receive operations on the
NIC. The problem is that this counter
increments both when the bottleneck is at
the PCIe and when we are CPU
bottlenecked (e.g. when we use a small
number of cores on our XDP device).Is there any counter that directly
indicates a bottleneck in the PCIe (but
not CPU or some third component)?Thanks for any help in advance,Srinivas Narayana5.19.0-38-genericHello,I can see that you are running the OS Inbox driver (5.19.0-38-generic) versus MLNX_OFED driver (Nvidia driver).Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)I would check that the server has been properly tuned (BIOS/OS) according to our best practices.We have various community/knowledge posts addressing tuning based on different deployments.You can also follow some tuning recommendations (Nvidia Mellanox NICs performance) from http://core.dpdk.org/perf-reports/.Check the maximum PCie Gen/width the HCA supports (per FW release notes) in comparison to which slot the HCA is installed.Powered by Discourse, best viewed with JavaScript enabled"
788,offload-infiniband-application-to-bluefield2-dpu,"Hello,I am new to Bluefield and DOCA. Please bear with me for the potentially naive questions.My previous experience mainly includes programming Infiniband applications with libibverbs. Recently, I got access to some Bluefield2 chips. I want to see whether I could offload some message-processing tasks of my applications to the Bluefield DPU while the host still runs the major tasks. For example, host A sends a message to host B but some handshake communication for this message is handled by DPUs.  However, it seems that the DOCA SDK document is mainly about DPDK processing ethernet packets. Is it possible for the current Bluefield2 to offload tasks of Infiniband applications? If it is, could anyone point me some related materials to start with? How should the process on host collaborate with the process on DPU?Thanks in advance!Hi PapaDi,It is also my question. Did you find the answer of you question?Thanks,Hi,
I also want to know how to offload infiniband application from host to DPU?
Did you know the solution regarding the issue? If so, could you share your solution?Thanks in advance.Thanks for everyone’s interest. My latest conclusion is it is not possible to offload Infiniband application from host to DPU. We have to use DPDK to do the offloading.Could you talk more regarding how to offloading the data/application to DPU via DPDK?
Thanks a lot in advance.Powered by Discourse, best viewed with JavaScript enabled"
789,how-to-deal-with-dma-on-the-host-to-access-dpus-memory,"I place DMA JOB operation on the host from Bluefield-2 DPU in order to reduce the load of DPU’s arm core. I follow DOCA DMA Development Document, and the steps are here:But when I create memory map to the remote buffer (in DPU’s memory), the function doca_mmap_create_from_export  failed. The error code is  DOCA_ERROR_NOT_SUPPORTED .export object json:
{
“addr”:187650922414160,
“len”:4096,
“lkey”:395842,
“vhca_id”:0,
“access_key”:“eShVkYp3s9vdy$B&E)O@MeQfTjWnZq4”,
“access_key_sz”:32
}Now, I don’t know where the problem is. And I’m not sure if the DOCA supports DMA operation is on the HOST to access DPU’s memory?Hey @hlnan21 , any luck with this? Did you come to know if DOCA supports DMA from host to DPU memory?I haven’t got success. It seems you must operate them in dpu.Powered by Discourse, best viewed with JavaScript enabled"
790,connectx-4-roce-speed-less-than-expected,"Hey,
I’m currently working on a simple setup of two systems connected via two ConnectX-4 Lx NICs and a fiber-optics cable with SFP28 modules on both ends. We are aiming to utilize GPUDirect RDMA, whereas only one system has a GDR supported GPU (Here: RTX A5000). Currently I’m looking for advices on how to improve the RoCE throughput, because we expect to get ~ 25 GBit/s but instead we were only able to achieve an average of 14.35 GBit/s (reported by ib_send_bw).The setup looks as follows:Used fiber-optics cable: https://www.fs.com/de/products/40233.html?attribute=803&id=18479All NICs are connected to PCIe 3.0 x16 (each Port has x8 which equals 64 GBit/s per Port), so I would expect that is not the limiting factor. Following commands have been run by us:This is the report by ib_send_bw:Other configuration informations:Is there anything else I should take care of? I appreciate any help or advice on this topic.I was wrong - the mainboard of system #2 is limiting the PCIe width to x2 for PCIe 3.0 which results in max. 16 GBit/s theoretical throughput and is close to the reported bandwidth of ib_send_bw.See the output of lspci -vv:I will switch the NIC to another slot and let you know, whether this has improved the RoCE performance.1.SEND is not real RDMA, it just copy data to NIC buff. You need use ib_write_bw.3.To use GDR you need locate HCA nd GPU on same pcie bridge, and disable pcie ACS4.To use GDR you need use nv_peer_mem kernel driver5.To test RDMA perf you need bind perf test to locale NUMA core by taskset/numactl etc.First: Switching the CX4 to another slot allows now to utilize the full bandwidth of the SFP28 module (25 GBit/s).Thanks, you are right. I was lucky the perftest tools used the correct GID (here: 5 for RoCEv2).ACS is disabled, because we have tested GPUDirect Storage (GDS) before and used gdscheck -p to see, if we have disabled IOMMU and ACS (all done in BIOS) as required by GDS. This is the output of nvidia-smi topo -m for system #1:nv_peermem is loaded on system #1. Since system #2 doesn’t has any supported GPU, there is no nv_peermem available. Checked via lsmod | grep peer.I haven’t heard anything about this so far. Do you have any references/manuals explaining this in detail?Two general question:When running ib_write_bw I see one core on each system is fully utilized (100%). I’m not sure but I expected that the CPU is less utilized thus experiencing the big benefit of using RDMA by being almost independent of CPU performance.Since I’m now able to utilize the specified bandwidth - I would like to know whether it is possible to send gpu memory data from system #1 via NIC utilizing GDR without having a supported GPU on system #2? We just want to send data directly (bypassing CPU and system memory) from the RTX A5000 via RoCE. The receiver is an FPGA connected via Ethernet which just fast-forwards the data with very low latency in an appropriate format to another system.3.PHB not good, since it cross host pcie bridge, you need relocate, make it as PXB better.5.NO manual mentioned. You need check “mst status -v” see where is the HCA PCIE locate on NUMA, then check lscpu identify cpu core to use. Then run test prefix by “taskset -c <core nu#>” etc.gq,1.it is work as design, rmda verbs polling on kernel will utilized 1 core 100%. It is huge benefit already, if you look at TCP/IP cpu handled stack etc, it at lease use serval core 100% to reach line rate.2.it is possible GPU mem ↔ CPU mem RDMA.
serval test method,perftest with -use-cude, need rebuild code,Infiniband Verbs Performance Tests. Contribute to linux-rdma/perftest development by creating an account on GitHub.OSU BW test,osu_bw D Hhttp://mvapich.cse.ohio-state.edu/static/media/mvapich/README-OMB.txtThank you very much so far, @xiaofengl!I agree, but the used mainboard does not allow a better physical connection. We will certainly look for another mainboard which is more suitable for RDMA.I guess this ain’t required for our setup, since our mainboard provides only one socket/cpu whereas NUMA is from my understanding a concept for virtually managing multiple cpu’s connected via one system bus. However, it makes somehow sense to set the CPU affinity for the benchmark application.Based on this I started an investigation to understand respectively experience the real benefit of RoCE in our minimal P2P setup (no switches). I hope using qperf is a reliable way to compare these two communication protocols. First, qperf was started on one system as a server. Next, we executed following commands and their corresponding results on the other system:From my understaing, the cost for using RDMA with QP RC is a lot less than compared to TCP. In the following a short comparison:send cost: TCP costs by a factor of 36.3 more than RDMA
recv cost: TCP costs by a factor of 63.5 more than RDMALet’s check the latency:It seems TCP is 24.7 % slower than RDMA. I expected an even bigger difference based on all readings so far, but it’s likely that I have to optimize my setup first. One way to reduce the latency for RoCE is by enabling the pooling mode in qperf which reduces the latency even more:Pooling introduces an even bigger gap between TCP and RDMA. As a result, TCP is now 71 % slower than RDMA.@xiaofengl Can you please confirm my findings? Am I doing everything right while investigating RDMA performance? I’m fairly sure there are potential improvements, but we first need to understand whether our setup works in an expected way as a base for further investigations and tests.Yes, I think it is. Your perf result base on your setup no issue.For compare RDMA with TCP, latency is not make sense. If you look into TCP stack you can see it can’t guarantee latency, it base on sliding window and dynamic response time. RDMA (RoCE Infiniband) base on hardware control ACK. In the mean time, TCP/IP not design for High performance network, drop/retransmit is also problem. You may get good performance result on idle system, but if network/server high load it is another thing.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
791,east-west-overlay-encryption-with-ipsec-example,"VXLAN can be used to provide direct layer 2 connectivity between entities over a routed path. L3 (BGP based) datacenter fabric designs make use of overlay and virtualized networks to more flexibly meet application needs and requirements.IPsec can be used to encrypt overlay encapsulations such as VXLAN. Whether crossing a shared/public line or implementing hybrid cloud connectivity solutions, IPsec encryption on a DPU can be entirely transparent to the application and impose no performance penalty with hardware acceleration.In this example, Strongswan provides the IPsec control plane and key management for encrypting VXLAN tunnels between two hosts:NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.Powered by Discourse, best viewed with JavaScript enabled"
792,nvidia-container-cli-initialization-error-load-library-failed-libnvidia-ml-so-1,"Hi everyone,
I’m trying to run this image ultralytics/yolov5:v7.0 that use an NVIDIA image. I want to run it without sudo permission, but with my user permission.So when I try the commande using sudo, it works :Without sudo permission
BUT when I run it without the sudo permission, I get the following error:This is probably due to the fact that sudo privileges were used during the installation.
I was installing nvidia drivers and nvidia container toolkit with sudo and when I tried to run the docker container without sudo, I got the exact same error.Powered by Discourse, best viewed with JavaScript enabled"
793,negotiation-failure-connecting-to-sonic-switch,"We are trying to connect our ConnectX-6 and ConnectX-7 cards to our SONiC switch, but we are getting a negotiation failure error when they are connectedAssuming you have NVIDIA support entitlement for the adapter cards, please open a support case on our portal and we can then help troubleshoot the issue.Details about enterprise support and how to engage with support are detailed at https://docs.nvidia.com/ai-enterprise-support-and-services-user-guide.pdf.Powered by Discourse, best viewed with JavaScript enabled"
794,how-to-drop-packets-in-the-switch,"I would like to test custom software reliability, is there any way to make the switch to drop X packets every second?Hello Morad,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, unfortunately this option is not available on our switches.Thank you and regards,~NVIDIA Networking Technical SupportHi Morad,I think you can use the acl option of policer to rate limit number of packets on the ingress of a interfacehttps://docs.mellanox.com/display/Onyxv392006/ACL+CommandsPowered by Discourse, best viewed with JavaScript enabled"
795,connectx-6-dx-sriov-esxi-trust-mode,"Hi,I am setting-up a fortigate VM on esxi 7.0u3 on top of a dell R650.
I have enable SRIOV in firmware, drivers, server bios…
I see all the VF available, dpdk running fine. I need to use 1 VF to carry multiple VLANs and this does not work. When I use 1 VF per vlan it is working.  I have seen some documentation (including from Fortinet) talking about the necessity to enable “Trust mode” and to disable “spoof check” on the VF.  All examples are with intel nics though, I cannot find similar commands for mellanox connectx-6.Is thre such options for Mellanox?I have dell Mellanox Connectx6-DX with driver 4.19.71.1  and FW  22.32.2004.ThanksNV HCA have setting for VGT/VGT+ which allow vlan tag from VF or Switch. But it is for linux. ESXi should has it’s own setting for guest/switch tag. That not control by NV. You can check with VMWARE.https://docs.nvidia.com/networking/pages/viewpage.action?pageId=111589059#SingleRootIOVirtualization(SRIOV)-VLANGuestTagging(VGT)andVLANSwitchTagging(VST)Thnaks for your answer, Yes I saw that guide but it is only for Linux. I cannot find such commands for ESX.
In the fortinet guide they instruct how to do it with the intel driver from vmware cli:esxcli intnet sriovnic vf set -s false -t true -v  -n So there might also be something to do with the mellanox driverThere is no vlan limit on NV driver parameters,https://docs.nvidia.com/networking/display/VMwareUMv419711/Module+ParametersHi,Thnaks, I got it working by removing all vnic from the vm and recreating them…This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
796,vrrp-mac-received-packet-with-own-address-as-source-address,"HiTwo SN2010 switches with VRRP configured (FFR), seeing the following in the backup VRRP  device syslog:The VRRP MAC is on both switches, so I can understand the log message. Is this normal? Is there a better way of doing things?Please open a support case so we can collect more details and look into this for you.Depending on your specific scenario VRR might be required instead of VRRP.Powered by Discourse, best viewed with JavaScript enabled"
797,flow-management-features-on-windows-dpdk,"Test-setup:Are Flow Management features (flow rules, distribution, steering etc.) available on Windows DPDK ?Hello Burto,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.The following link provides the information regarding the limitations for Windows DPDK when using the ConnectX-6 Dx → 36. MLX5 Ethernet Poll Mode Driver — Data Plane Development Kit 22.07.0 documentationThe following link provides the information on how-to build the Windows PMD → 5. MLX5 Common Driver — Data Plane Development Kit 22.07.0 documentationHope this will get you further.Thank you and regards,
~NVIDIA Networking Technical SupportHi MvB,From limitation, it seems Flow Management is available but supporting packages are not getting compiled.How to make them compilable?Thanks,
BurtoPowered by Discourse, best viewed with JavaScript enabled"
798,is-there-a-command-that-will-tell-us-why-one-of-the-ports-on-our-516-ccat-adapter-will-not-link-to-a-switch-or-how-to-detect-if-there-is-an-error-on-the-port-or-card,"I have a dual port adapter, and one of the ports link light never turns on when connected to a Mellanox 100G switch using an approved Mellanox copper 100G cable.I tried “mlxlink -d ” and State says “Polling” and never changes, the other port goes immediately to “Active”.Thanks for any feedback/ideas/…EricHello,To check if the configuration of the adapter, cabling, and switch represent a supported configuration, please check the firmware release notes for the adapter/firmware you are using. The latest firmware for the MCX516A-CCAT is v16.31.1014:https://docs.mellanox.com/display/ConnectX5Firmwarev16311014/Firmware+Compatible+ProductsFor basic physical troubleshooting guidance, please review the ConnectX-5 User Manual:https://docs.mellanox.com/display/ConnectX5EN/TroubleshootingFor more in-depth troubleshooting, please review the MFT User Manual:https://docs.mellanox.com/display/MFT4170/NVIDIA+Firmware+Tools+(MFT)+v4.17.0+DocumentationIn particular, the sections regarding “mlxlink” should provide guidance on checking link parameters on the adapter and cabling.The MLNX_OFED User Manual also contains many troubleshooting measures as well:https://docs.mellanox.com/display/MLNXOFEDv541030/Troubleshooting-Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
799,mcx4121a-acat-safe-mode-enable,"I found through mlxconfig -d device query SAFE_MODE_ENABLE True(1)What does this parameter do?Hi,Please open a support ticket through support channel in order to get answer to this question.Networking-support@nvidia.comThanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
800,factory-reset-problem-on-sx1036-switch,"Hello@allWe here have a Mellanox SX1036 Switch, which was found on eBay. Becaurse of the fact that we have one of those switches in our infrastructure, we bought this device as a cold spare.The problem now is, that we have no password for this device - and also the factory reset does not work. If we press and hold the reset button for 10, 15, 30 or even 60 secs, the switch boots up and there is still the old host name and password. We`ve also booted another image via console but also this has no effect on login or resetting this switch.What can we do now? The seller also does not have the admin- password.Here is our startup-log:U-Boot 2009.01 SX_PPC_M460EX SX_3.2.0330-82 ppc (Dec 20 2012 - 17:53:54)CPU: AMCC PowerPC 460EX Rev. B at 1000 MHz (PLB=166, OPB=83, EBC=83 MHz)Security/Kasumi supportBootstrap Option H - Boot ROM Location I2C (Addr 0x52)Internal PCI arbiter disabled32 kB I-Cache 32 kB D-CacheBoard: Mellanox PPC460EX BoardFDEF: NoI2C: readyDRAM: 2 GB (ECC enabled, 333 MHz, CL3)FLASH: 16 MBNAND: 1024 MiBPCI: Bus Dev VenId DevId Class IntPCIE0: link is not up.PCIE1: successfully set as root-complex01 00 15b3 c738 0c06 00Net: ppc_4xx_eth0, ppc_4xx_eth1Reading image settings from EEPROMMellanox MLNX-OSDefault image: ‘PPC_M460EX 3.5.1006 2016-03-09 11:29:49 ppc’Press Enter to boot this image, or ‘Ctrl B’ for boot menuBooting default image in: 0Booting location 2: ‘PPC_M460EX 3.5.1006 2016-03-09 11:29:49 ppc’INIT: version 2.86 bootingStarting: PPC_M460EX 3.5.1006 2016-03-09 11:29:49 ppcStarting udev: [ OK ]Setting clock (utc): Sat Jan 1 01:11:08 UTC 2000 [ OK ]Setting hostname localhost: [ OK ]Checking filesystemsChecking all file systems.[ OK ]Remounting root filesystem in read-write mode: [ OK ]Mounting local filesystems: [ OK ]Running vpart script: [ OK ]Applying file system skeletons: base_var base_config .Enabling /etc/fstab swaps: [ OK ]INIT: Entering runlevel: 3Starting system servicesStarting sx_low_level_if: Starting sx_low_level_if:Loading i2c_mux_pca954x - SuccessLoading sx_glue_if - SuccessLoading watchdog - SuccessLoading cpld_handler - SuccessLoading mellaggra_mod - SuccessLoading switchx - Success[ OK ]Starting openibd: IPoIB configuration for embedded systemLoading SX driver:[ OK ]Loading HCA driver and Access Layer:[ OK ]Setting up InfiniBand network interfaces:Setting up service network . . .[ done ]Reloading udev:[ OK ]Starting system logger: [ OK ]Starting kernel logger: [ OK ]Starting fips_post: [ OK ]Running renaming interfacesRenaming: MAC: 00:02:C9:64:17:F4 ifindex: 2 name: mgmt0Renaming: MAC: 00:02:C9:64:17:F5 ifindex: 3 name: mgmt1Checking for unexpected shutdownDetected unexpected shutdown!Probing for HRNG moduleStarting rngd: [ OK ]Running system image: PPC_M460EX 3.5.1006 2016-03-09 11:29:49 ppcApplying initial configuration: Jan 01 01:12:18 INFO LOG: Initializing SX log with STDOUT as output file.Starting internal_startup: [ OK ]Starting tc_ingress_policy: mDNS policing rate=4000kbit burst=400kIngress policing enable on interface mgmt0 rate=9000kbit burst=900kIngress policing enable on interface mgmt1 rate=9000kbit burst=900k[ OK ]Starting clean_issnvram: Deleting issnvram.txt[ OK ]Starting intr_hndl: Starting :Loading int handler module - Success[ OK ]Starting iss-nvram-mac: [ OK ]Starting copy_rh_files_to_vtmp: [ OK ]Starting secure_mode_check: [ OK ]Starting sx_pra: Starting proxy arp management:Loading proxy arp management module - Success[ OK ]Starting udevd: Reloading udev…[ OK ]Starting pm: [ OK ]Starting oops_dump_reg: Starting kernel reg dump:Loading kernel reg dump module - Success[ OK ]Starting lnpuppetvar.sh: [ OK ]Starting mst: Starting MST (Mellanox Software Tools) driver setLoading MST PCI module - SuccessLoading MST PCI configuration module - SuccessCreate devices[ OK ]Mellanox MLNX-OS Switch Managementuwt-sw-6L02 login:We hope that anyone is able to help us.Thank you in advanceAljoschaDo you find a way to reset the password? I’m stuck at the same place with a SX6036… Thank!I do find a way. use the xmladmin/xmladmin user/password to login and change the admin password.Powered by Discourse, best viewed with JavaScript enabled"
801,missing-commands-in-nclu,"Hi,I’m really new to Mellanox and Cumulus. I am trying to set up some basic layer 2 multiple vlan switching on a Mellanox MSN2410.I am trying to use the “net” commands to configure the switch but I don’t seem to have the available commands that all the documentation says I should. For example when I issue the “net” command and press the TAB key I only get the options for clear help and show.Do I need to do something else to make all the net commands available?cumulus@cumulus:mgmt:~$ net
clear  :  clear counters, BGP neighbors, etc
help   :  context sensitive information; see section below
show   :  show command outputHi Many,Based on your question description - I believe you are using Cumulus Linux version 5.x where the default Configuration cli is NVUE and nclu (net xxx) is used only for show commands.you can check your Cumulus Linux version with:cat /etc/lsb-release
or
net show versionsome more details can be found at:https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-53/System-Configuration/NVIDIA-User-Experience-NVUE/NVUE-CLI/https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-53/System-Configuration/NVIDIA-User-Experience-NVUE/NVUE-CLI/#net-show-commandsPowered by Discourse, best viewed with JavaScript enabled"
802,how-to-convert-a-cumulus-os-switch-to-run-mlnx-os,"If a switch is already running with Cumulus OS, would a reboot (and break of the boot system) provide the option to switch OS? if so, what happens to the existing configuration file that was there for Cumulus?ThanksHi Hamid,Please see the steps below to change from Cumulus OS to Mellanox OSIf you have a valid support contract with us, please open a case and we will assist you with your request.Thanks,Pratik PandeIs there any instruction of how to do that?Powered by Discourse, best viewed with JavaScript enabled"
803,compute-nodes-provisioning-failed-installer-unreachable,"Hi FolksMy cluster got powered off due to lab outage. After powering on Head node , I was trying to get the compute nodes up , they all the stuck at the screenshot attached , please let me know if you need more information.I tried the below link , its still not workingCan't provision compute nodes: ""Failed to load ldlinux.c32""

This ""cluster"" is just bog-standard desktop hardware, super vanilla. The network cards are ConnectX-2 10G cards. They get through...please refer the output below as wellHi,From the screenshot, I see that the node has already booted and mounted the /cm/node-installer from the headnode. At the stage “Running the node-installer” the node will be running “/scripts/node-installer”. “INSTALLER_UNREACHABLE” may indicate that the head node has lost connection to the compute node. Did you try to ssh into the node and check the /var/log/node-installer? I also see that the MTU is set to 9000, perhaps the MTU needs to be adjusted to a lower value?Perhaps you can open a support ticket at “Bright Computing Support Form”? Then it will be easier to submit logs and screenshots.Kind regards,
adelHi AdelCompute nodes are pinging but SSH not going through , please refer belowAlso I have Easy 8 license key , so no commercial support . I opened a support request but they directed me to this forum .
Please let me know if you need more informationHi,Perhaps you could connect to the console of the node and check in a separate emergency shell (alt+2->12) why the node is stuck at “Running the node-installer”?At the stage “Running the node-installer” the node-installer will run “chroot /installer_root /linuxrc”. The /installer_root is an NFS mount on :, so I would start by checking that the mount point is ok and is readable. Perhaps you can also use a lower MTU.If you run “ping -s 9000 -M do ” then you’ll get an indication whether the MTU value is good or needs adjustment.Connecting a compute node back-to-back with the head node can be useful to test if the the switch in between is causing any unexpected issues.Kind regards,
adelIt was MTU Adel !!! :)…but how come it was working before with MTU 9000 , i used this cluster for 40 days and rebooted multiple times compute machinesAll devices, end-to-end, need to have the same MTU setting.  Now that you know the MTU is the issue, you can investigate the settings on the switch, perhaps?Glad that it’s working, though!BR,
kwd-to-end, need to have the same MTU seHey K , network stack is all 100G > mellanox switches enabled jumbo frames + connect X5 adaptersVery odd.  Something in the mix doesn’t have jumbo frames enabled.   I would offer that you should check everything, even if you “know” that it’s set to 9000.  Make 100% sure.Kind regards,Ken Woods
Manager, Nvidia Bright Cluster Manager SupportPowered by Discourse, best viewed with JavaScript enabled"
804,dpdk-on-cx7-with-mlx5-pmd,"Hi, I’m trying to compile and run the dpdk-test-flow_perf on a CX 7 card running mlx5 driver. I’m running upstream 5.4 kernel and the DPDK application is built with rdma_core v41. I can run testpmd just fine but running flow_perf gives:Running strace seems to suggest some unsupported ibverbs op, but I’m not sure how to pinpoint what exactly that is:Any help is appreciated!!hi cuiweiI suspect there’s may be some SW mismatch in your system.
Can you test with last OFED(version 23.04,  it include the match rdma-core) with DPDK 22.11 or 23.03.Here is the link for OFED:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)Thank you
Meng, ShiHi Meng, thanks for the suggestion,  I have some limitations on the platform I’m running to install OFED, is there a combination of upstream kernel, rdma-core and DPDK that we know works fine with mlx5 driver?hi WeilongWe do not test DPDK without OFED.
you can try to install ofed with --dpdk optionThank you
Meng, ShiPowered by Discourse, best viewed with JavaScript enabled"
805,mellanox-sb7800-and-linkx-200gbe-cables-200gb-s-to-2x100gb-s,"RHEL 8.4
MLNX_OFED_LINUX-5.7-1.0.2.0-rhel8.4lspci | grep Mellanox
a1:00.0 Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6]Cable part number:
MCP7H50-H002R26ibstatus
Infiniband device ‘mlx5_0’ port 1 status:
default gid:     fe80:0000:0000:0000:e8eb:d303:00ef:5d22
base lid:        0xe
sm lid:          0x1
state:           4: ACTIVE
phys state:      5: LinkUp
rate:            25 Gb/sec (1X EDR)
link_layer:      InfiniBandmlxlink -d mlx5_0Operational Info
…
State                           : Active
Physical state                  : LinkUp
Speed                           : IB-EDR
Width                           : 1x
FEC                             : Standard LL RS-FEC - RS(271,257)
Loopback Mode                   : No Loopback
Auto Negotiation                : ONSupported Info
…
Enabled Link Speed              : 0x00000075 (HDR,EDR,FDR,QDR,SDR)
Supported Cable Speed           : 0x0000007f (HDR,EDR,FDR,FDR10,QDR,DDR,SDR)Troubleshooting Info
…
Status Opcode                   : 0
Group Opcode                    : N/A
Recommendation                  : No issue was observed.Tool Information
…
Firmware Version                : 20.32.2004
amBER Version                   : 2.02
MFT Version                     : mft 4.21.0-99How to set the speed to 100GbE with these cables? Currently they run only in 25GbE (1XEDR).mlxlink -d mlx5_0 -p 1 --speeds edr
mlxlink -d mlx5_0 -p 1 --speeds hdrBoth gives the same results 25GbE (1xEDR)Apparently the Mellanox SB7800 can only handle 25Gb/s lanes so 2 x 50Gb for split cables doesn’t work because of this.EDR switch not support split cable, you can only get 1x, 4x link.
HDR can get 1x,2x,4x linkThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
806,mlag-proper-active-active-iscsi-cabeling,"Hello community,i have a fairly simple situation actually, but apparently surprisingly many options.I have the following deployment:Each array controller features two “channels” (ethernet interfaces), which apparently correspond to the other controller:The manufacturer of the array seems to recommend to link both channels per controller to the same switch, but without taking into account, how these switches actually are configured, i.o.w.: whether and how they “cooperate”.But the MLAG documentation by MELLANOX for gerneral downstream devices recommends to connect both links of each LAG of each “downstream host” to different switches, which i find understandable.My understanding is that the following connection-schema would therefore conform with the MELLANOX recommendation for connect downstream devices to MLAG interfaces:CTLA:ch3 => SW1:p3CTLA:ch4 => SW2:p3CTLB:ch3 => SW1:p4CTLB:ch4 => SW2:p4While the manufacturer of the array seems to recommend:CTLA:ch3 => SW1:p3CTLA:ch4 => SW1:p4CTLB:ch3 => SW2:p3CTLB:ch4 => SW2:p4albeit with explicitely ignoring any switch-configuration (MLAG, Stack, …).If all there can be known about the inner workings of the “symmetric active/active” mode of the array is,what would you think which is the better/correct approach of wiring up that rig?I’d highly appreciate any of your thoughts on this,best,HilmarHello Hilmar,Depending on your design goals either proposed solution could work. In order to best assist you and your specific use case you can explore upgrading your support contract level or procure a design review from our professional services team by emailing networking-support@nvidia.com.Below are some general considerations which I gathered from the information you’ve provided which may be able to help you make a more informed decision.When using the manufactures recommendation of connecting the storage controllers to only one switch respectively you would gain a predictable traffic pattern. If you connect each storage controller to separate clustered MLAG switches you would gain more efficient link utilization through load balancing and increased availability through physical link redundancy.Load balancing between the cluster can be manipulated via the global configuration mode, see below for syntax example.sn2100-02 [mlag-vip: master] (config) # port-channel load-balance ethernet ?destination-ip Destination IP addressdestination-mac Destination MAC addressdestination-port Destination UDP/TCP portflow-label IPv6 flow-label fieldingress-port Ingress portl2-protocol Ethertype fieldl3-protocol IP protocol fieldsource-destination-ip Source and destination IP addressessource-destination-mac Source and destination MAC addressessource-destination-port Source and destination UDP/TCP portssource-ip Source IP addresssource-mac Source MAC addresssource-port Source UDP/TCP portsymmetric Symmetric hashing; bidirectional flows follow same pathA validated MLAG configuration for Mellanox switches can be found here at this link. And if you wish to combine this setup with a L3 first hop redundancy protocol it can be coupled with this MAGP configuration guide.I hope this information has been helpful.Best,Brian T.Global Technical SupportNVIDIA NetworkingHello Brian,thank you very much for helping so fast and looking into my question!The MLAG Guide you recommended is really good and ultimately helped me to come to the conclusion that the general recommendation of the array manufacturer would possibly be not the best solution in our specific environment.It definitely helps to hear from you that fundamentally both options are valid nevertheless, just fitting different scenarios.Fortunately the upstream L3 situation is nothing i have to deal with, which helps keeping the complexity at bay ;-)Since i have a mixed usecase (virtualisation, HPC), i’ll have to test and measure anyways and hope it will help to decide upon the best option.Thank you very much again so far,bestHilmarPowered by Discourse, best viewed with JavaScript enabled"
807,which-connect-x-5-cards-works-with-pcie-bifurcation-and-socketdirect,"Greetings,We want to use a Supermicro motherboard that supports PCIe4 with bifurcation options in the BIOS, with 2x AMD EPYC CPUs from the 7003 generation, which allows both sockets to use the same PCIe4 slot. At least in accordance to the manual “MNL-2363.pdf”, Figure 1-4, for the motherboard H12DSi-N6.We know/believe that we should be able to use the bifurcation feature in Socket Direct, as stated here: NVIDIA Mellanox Socket Direct Adapters | NVIDIAThrough either a connection harness that splits the PCIe lanes between two cards or by bifurcating a PCIe slot for a single card.This is nice and all, but it’s not entirely clear which cards we can buy that provide us with this feature, namely: having a single PCIe card with which we can use the PCIe x8x8 bifurcation option in the BIOS, so that each CPU socket can use half of a single Infiniband card, via SocketDirect in said card.Furthermore, if we can use a single card, then can we also only use a single QSFP28 port to properly use Connect-X 5 100Gbps with the Socket Direct feature, or if must use two ports.We were thinking of purchasing the MCX555A-ECAT model, which is a PCIe3 single port card, but given the confusion of which card we can use, we don’t know if we must use the MCX556A-EDAT (PCIe4 dual port) or even the MCX556M-ECAT-S25 (without using the secondary card?).The firmware release notes seem to imply that bifurcation is supported after a firmware update since version 16.24.1000, but the message is too vague to be certain if this refers to the same feature or not.Many thanks! Best regards,
Bruno SantosOK, I finally figured out where the bifurcation information in the commercial blurb was coming from: Introduction - ConnectX-6 InfiniBand/VPI - NVIDIA Networking DocsSo in other words, in Connect-X 6 there is such a card, but not in Connect-X 5.Powered by Discourse, best viewed with JavaScript enabled"
808,mellanox-connectx-2-mnpa19-xtr-10gb-card-with-unstable-iperf3-values-in-window,"I am trying many possibilities, but the Mellanox ConnectX-2 MNPA19-XTR Ethernet Card is definitely not giving stable values in Windows 11 Device. From Debian Server to Windows 11 iper3 -c ip address I always get 9.86Gbit stable values while from Windows 11 Pro Workstation operating system to Debian Server I never get stable iperf -c values. Sometimes 96Megabit Sometimes 2gbit sometimes 2gbit sometimes 7 gbit sometimes 3gbit Why is this card so problematic under Windows 11? Which settings should I make in the Ethernet settings so that I can get stable values as in Linux.Hello,Thanks for your question.CX2 adapter is not supported anymore.
Please consider using one of supported products:Full suite of end-to-end solutions supporting InfiniBand and Ethernet networking technologies. NVIDIA Mellanox’s solutions include adapters, transceivers, DPUs, cables and transceivers and software.Best Regards,
AnatolyPowered by Discourse, best viewed with JavaScript enabled"
809,accessing-the-pcie-rshim-interfaces-with-dpdk-on-the-bluefield,"Hi all,I installed DPDK 21.08 on the BlueField.  I can successfully exchange packets with the outside world as the CX6 is discovered by the DPDK EAL. However I’m not sure how to send packets to the host.
Here are the BF PCIe devices:And the DPDK app bootup:DPDK only sees the CX6. How can I reroute packets to the host? I guess 01:00.0 should appear somehow…Powered by Discourse, best viewed with JavaScript enabled"
810,does-the-mlnx-ofed-driver-support-linux-kernel-6-2,"I have confirmed that the highest possible kernel version supported with the release of Ubuntu 22.04.3 is 6.2.I have found that there is no driver supporting “ASAP2 OVS-Kernel SR-IOV” in Kernel version 6.2.I am curious if there is a plan to support Kernel 6.2 in the future, and if so, when it is scheduled to be released.Hi
Please check the latest version MLNX_OFED:
https://docs.nvidia.com/networking/display/MLNXOFEDv23041130/General+Support
Thanks,
SuoThank you for your insightful response.
It was very helpful and much appreciated.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
811,gpudirect-rdma-difference-between-ibv-reg-mr-and-ibv-reg-dmabuf-mr,"Hey there,I’m still trying to understand what is the actual difference between these two calls?
We have eitherBut how does it impact the data flow between the GPU and NIC? Can you please explain it based on the PCI topology? From my current understanding, the GPU and NIC share a pinned memory block on host memory filled with virtual addresses through which they communicate with each other. Afaik this is the regular implementation for ibv_reg_mr when using GPU memory. But what exactly happens when using ibv_reg_dmabuf_mr?dmabuf however is not supported by my GPU (RTX A5000) as noted by perftest > ib_write_bw via the following code:cuDeviceGetAttribute(&is_supported, CU_DEVICE_ATTRIBUTE_DMA_BUF_SUPPORTED, cuDevice)Powered by Discourse, best viewed with JavaScript enabled"
812,how-can-i-install-ofed-driver-for-connect-x6-under-opensuse-leap-15-3,"I got error message during the installation
The current MLNX_OFED_LINUX is intended for sles15sp3Hi heinz_zhang,Thank you for posting your query on our community.Based on the Release Notes of latest OFED , OpenSUSE 15.3 is listed under Community OS based on the support Matrix —> https://docs.nvidia.com/networking/display/MLNXOFEDv581121LTS/General+SupportPlease confirm you are Installing the OFED based on procedure described for Community OS → https://docs.nvidia.com/networking/display/MLNXOFEDv581121LTS/Installing+MLNX_OFED#InstallingMLNX_OFED-InstallationonCommunityOperatingSystemscommunityThanks,
Namrata.Hi Namrata,many thanks for your help!
After reading the docs, I have aonther questions.Thanks again for your time during holidays.Hi heinz_zhang,For question 1, based on the Release Notes link shared previously, it states the following:Overall, the following should be noted when running OFED on the community-supported operating systems:For question 2, as listed , for OpenSUSE 15.3 and using MLNX OFED, NFSoRDMA is not supported. If you decide to use Inbox driver that comes with OS, questions regarding support of specific functionality with Inbox driver should be addressed with OS vendor.Thanks,
Namrata.Hi Namrata,I need you help again. I tried to download the community version
and run ./install.pl --all
installation failed during  mlnx-ofa_kernel 5.7 RPM
I tried to parse installation logs but only found following errors:cp: target ‘/var/tmp/OFED_topdir/BUILDROOT/mlnx-ofa_kernel-5.7-OFED.5.7.1.0.2.1.opensuse15sp3.x86_64//usr/src/ofa_kernel/x86_64/5.3.18-150300.59.68-default’ is not a directory
error: Bad exit status from /var/tmp/rpm-tmp.f3TDTd (%install)RPM build errors:
user builder does not exist - using root
group dip does not exist - using root
user builder does not exist - using root
group dip does not exist - using root
Bad exit status from /var/tmp/rpm-tmp.f3TDTd (%install)Do you have any idea to procceed?many thanks!Powered by Discourse, best viewed with JavaScript enabled"
813,error-one-or-more-required-packages-for-installing-ofed-internal-are-missing-mlnx-ofed-linux-4-9-5-1-0-0-ol7-9-x86-64,"Hi mellanox MLNX_OFED_LINUX-4.9-5.1.0.0-ol7.9-x86_64, cannot be installed in a pod-container because some packages are missing… although the packages are already installed.[root@oralinux-28-ens7-30908-586fd58f95-vdf66 /]# uname -r
5.4.17-2136.308.9.el7uek.x86_64[root@oralinux-28-ens7-30908-586fd58f95-vdf66 MLNX_OFED_LINUX-4.9-5.1.0.0-ol7.9-x86_64]# yum install createrepo
Loaded plugins: ovl, ulninfo
Package createrepo-0.9.9-28.el7.noarch already installed and latest version
Nothing to dowget https://yum.oracle.com/repo/OracleLinux/OL7/UEKR6/x86_64/getPackage/kernel-uek-devel-5.4.17-2136.308.9.el7uek.x86_64.rpm
rpm -i kernel-uek-devel-5.4.17-2136.308.9.el7uek.x86_64.rpm[root@oralinux-28-ens7-30908-586fd58f95-vdf66 MLNX_OFED_LINUX-4.9-5.1.0.0-ol7.9-x86_64]# rpm -i kernel-uek-devel-5.4.17-2136.308.9.el7uek.x86_64.rpm
package kernel-uek-devel-5.4.17-2136.308.9.el7uek.x86_64 is already installed[root@oralinux-28-ens7-30908-586fd58f95-vdf66 MLNX_OFED_LINUX-4.9-5.1.0.0-ol7.9-x86_64]# ./mlnxofedinstall --upstream-libs --dpdk --add-kernel-support
Note: This program will create MLNX_OFED_LINUX TGZ for ol7.9 under /tmp/MLNX_OFED_LINUX-4.9-5.1.0.0-5.4.17-2136.308.9.el7uek.x86_64 directory.
See log file /tmp/MLNX_OFED_LINUX-4.9-5.1.0.0-5.4.17-2136.308.9.el7uek.x86_64/mlnx_iso.34862_logs/mlnx_ofed_iso.34862.log
Checking if all needed packages are installed…
ERROR: ‘createrepo’ is not installed!
‘createrepo’ package is needed for creating a repository from MLNX_OFED_LINUX RPMs.
Use ‘–skip-repo’ flag if you are not going to set MLNX_OFED_LINUX as repository for
installation using yum/zypper tools./lib/modules/5.4.17-2136.308.9.el7uek.x86_64/build/scripts is required to build mlnx-ofa_kernel RPM.
Please install the corresponding kernel-source or kernel-devel RPM.Error: One or more required packages for installing OFED-internal are missing.
Please install the missing packages using your Linux distribution Package Management tool.
Run:
yum install kernel-uek-devel-5.4.17-2136.308.9.el7uek.x86_64
Failed to build MLNX_OFED_LINUX for 5.4.17-2136.308.9.el7uek.x86_64
[root@oralinux-28-ens7-30908-586fd58f95-vdf66 MLNX_OFED_LINUX-4.9-5.1.0.0-ol7.9-x86_64]#hi,
which oracle linux version are you using?
suggest :use higer OFED version. ex OFED 5.4 , the newest is 5.7.  They can support the kernel 5.4.17
download OFED from: https://network.nvidia.com/products/infiniband-drivers/linux/mlnx_ofed/then don’t need to use ’ --add-kernel-support’Regards,
LeveiPowered by Discourse, best viewed with JavaScript enabled"
814,compile-mlnx-ofed-5-1-2-3-7-1-against-kernel-5-8-12,"Hej,is there a quick fix to compile MLNX_OFED_LINUX-5.1-2.3.7.1-rhel8.2-x86_64 against Kernel 5.8.12?Don’t get fooled by the Kernel name, it is CentOS 8.2 with a Fedora Kernel :)CC [M] /tmp/MLNX_OFED_LINUX-5.1-2.3.7.1-5.8.12-100.fc31.x86_64/mlnx_iso.2710/OFED_topdir/BUILD/mlnx-ofa_kernel-5.1/obj/default/drivers/net/ethernet/mellanox/mlx5/core/en_sysfs.o/tmp/MLNX_OFED_LINUX-5.1-2.3.7.1-5.8.12-100.fc31.x86_64/mlnx_iso.2710/OFED_topdir/BUILD/mlnx-ofa_kernel-5.1/obj/default/drivers/infiniband/core/umem_odp.c: In function ‘ib_umem_odp_map_dma_pages’:/tmp/MLNX_OFED_LINUX-5.1-2.3.7.1-5.8.12-100.fc31.x86_64/mlnx_iso.2710/OFED_topdir/BUILD/mlnx-ofa_kernel-5.1/obj/default/drivers/infiniband/core/umem_odp.c:1132:25: error: ‘struct mm_struct’ has no memberdown_read(&owning_mm->mmap_sem);^~~~~~~~mmap_base/tmp/MLNX_OFED_LINUX-5.1-2.3.7.1-5.8.12-100.fc31.x86_64/mlnx_iso.2710/OFED_topdir/BUILD/mlnx-ofa_kernel-5.1/obj/default/drivers/infiniband/core/umem_odp.c:1163:23: error: ‘struct mm_struct’ has no memberup_read(&owning_mm->mmap_sem);^~~~~~~~mmap_base/tmp/MLNX_OFED_LINUX-5.1-2.3.7.1-5.8.12-100.fc31.x86_64/mlnx_iso.2710/OFED_topdir/BUILD/mlnx-ofa_kernel-5.1/obj/default/drivers/infiniband/core/umem_odp.c: At top level:cc1: warning: unrecognized command line option ‘-Wno-address-of-packed-member’make[4]: *** [scripts/Makefile.build:281: /tmp/MLNX_OFED_LINUX-5.1-2.3.7.1-5.8.12-100.fc31.x86_64/mlnx_iso.2710/OFED_topdir/BUILD/mlnx-ofa_kernel-5.1/obj/default/drivers/infiniband/core/umem_odp.o] Errormake[4]: *** Waiting for unfinished jobs…Greetings,Bjoernmlnx-ofa_kernel-5.1.rpmbuild.log (424 KB)Well… I am aware that this Kernel is not supported but I was not asking if the Kernel is supported or not.I was asking if there is an easy fix to get it compiled on this not supported KernelNot supported doesn’t mean it can’t be fixed. It would still be unsupported, but might work ;)Hello Bjoern,Thank you for posting your question on the Mellanox Community.Unfortunately the kernel you are building the Mellanox OFED for is not a supported kernel.You can find a list of supported OS and kernels as part of the release notes for the Mellanox OFED which can be found here:https://docs.mellanox.com/display/OFED510660/General+Support+in+MLNX_OFEDAgain thank you for your question and have a nice week.Thank you,Mellanox Technical Support.Did you manage to get it working? Struggling with a similar issue on Ubuntu 20.04 / kernel 5.8Powered by Discourse, best viewed with JavaScript enabled"
815,memory-error-when-passing-connectx-4-nic-to-vm,"I’m attempting to passthrough a Mellanox ConnectX-4 NIC to a VM and getting memory errors in dmesg:These errors repeat hundreds or thousands of times while the VM is starting. Eventually the VM boots properly, lspci in the guest shows the NIC, but doesn’t load the driver for it so it’s unusable.I’m able to passthrough other PCI devices like NVMe SSDs and it works fine with no dmesg errors, it’s specifically the Mellanox NICs that have problems. I’ve tried both ConnectX-4 Lx and a ConnectX-6 card. The passthrough works without errors in other machines, it’s specifically with this AM5 platform that I’m having issues. I’ve tried using SR-IOV and passing through just one virtual function and that also causes the same errors. I’ve also tried the NIC in different PCIe slots and the same thing happens. Each port of the NIC is in it’s own IOMMU group, and I’ve tried passing in each individual port, as well as both ports together, each time getting the same errorsThis is with a new MSI X670E ACE motherboard with a 7950X CPU. I’m seeing the issue with kernel versions from 5.15 to 6.2. I tried installing MLNX_EN on Ubuntu and that driver resolved the issue, so it seems like the error is in the mlx5_core driver. Unfortunately I’m unable to install MLNX_EN on my primary OS, Fedora.Hello nvidia.fxzgc,Thank you for contacting Nvidia support.For installing MLNX_EN driver on Fedora, use the following driver:https://www.mellanox.com/downloads/ofed/MLNX_EN-5.4-3.6.8.1/mlnx-en-5.4-3.6.8.1-fc32-x86_64.tgzThe errors are not surely caused by our driver. Furthermore, the installation of the driver and not seeing the errors, may be a coincidence. To be sure, further investigation is needed.For further investigation, is needed create a support ticket and a support entitlement for your products.Customers with support entitlement, can create a support ticket via sending an email to EnterpriseSupport@nvidia.comBest regards,Nvidia supportPowered by Discourse, best viewed with JavaScript enabled"
816,connectx4-in-pcie4-ice-lake-server,"Would plugging in a Connectx4 NIC (PCIe3) into a PCIe4 slot cause any issues with performance of the CPU or data integrity?  Any other issues?  THANKS!CX4 not support PCIE4.0, can’t sure Your hardware compatible with NIC.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
817,howto-increase-memory-size-used-by-mellanox-adapters-mellanox-connect-x5,"How to increase Memory Size used by Mellanox Adapters i.e. for Mellanox connect-x5 adaptors. I am able to register memory of 4kB, 8kB, 16kB. ibv_reg_mr fails for registering memory of size 64kB.How the memory size limit can be changed for mellanox connect x5 devices?Thanks.…!!!Powered by Discourse, best viewed with JavaScript enabled"
818,what-is-the-expected-behavior-when-set-vf-tx-rate-limit-to-1-gb-and-set-vf-queue-in-vm-tx-maxrate-to-5-gb,"I have configured 1 Gb per VF maximum rate and it works. But when I configured 5 Gb rate limit on tx-0 or VF, I was able to get 5 Gb bitrate. I am curious that how a queue limit should exceed the limit of the VF port. The same behavior I observed with OVS police action.Hi Ullah:Could you show your config commands and the test result.So we can check and understand your question better.RegardsLeveiPowered by Discourse, best viewed with JavaScript enabled"
819,example-projects-for-rdma-communicate-application-on-windows-10,"I’m trying to create an RDMA communication app on windows 10 with connectx-5.
I’ve confirmed that I use MlxNdPerf.exe to measure some performance,
but I couldn’t find a project for this application.
Is this app or a similar example project published ?Thanks!Hello,MlxNdPerf.exe is itself an executable for measuring performance.For more information on its usage, please review the relevant WinOF-2 documentation:https://docs.nvidia.com/networking/display/winof2v290/Fabric+Performance+Utilities#FabricPerformanceUtilities-MlxNdPerfUtilityMlxNdPerfUtilityIf you need more information or example code regarding NetworkDirect, please see the following:master/src/examplesNetworkDirect Service Provider Interface. Contribute to microsoft/NetworkDirect development by creating an account on GitHub.Thank you,
-Nvidia Networking SupportHi hilaryn. These example code was provided by Microsoft. Thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
820,connectx-5-hierarchical-qos-hardware-offload-htb,"I’ve been testing HTB offload with a ConnectX-5 MCX512A (2x25G) NIC and getting good performance results up to the limit of 256 leaf nodes.I saw there where previous commits of the mlx5_core driver where the MLX5E_QOS_MAX_LEAF_NODES constant was defined as 1024 so I recompiled a recent kernel with that higher value but still hit the 256 node limit in the card firmware instead of at the driver.Looking in the release notes for the MLNX_EN driver I see it says under the Packet Pacing section “Up to 512 send queues are supported”.Is there a definitive number of queues supported by the hardware and is this a firmware limitation and could it be increased?Are there different limits on the ConnectX-6 or ConnectX-7 cards?Hi,Have you found an answer to your question? ConnectX-6 or 7 allows for more than 256 leaf nodes?I want to test  HTB offload with a bluefield2. Can you give me some help.
thanks.Powered by Discourse, best viewed with JavaScript enabled"
821,failed-to-build-mlnx-rdma-rxe-4-7-rpm,"I tried to recompile the mellanox infiniband driver but I get the next error when I run the command:LogsBuild mlnx-rdma-rxe 4.7 RPMRunning rpmbuild --rebuild --define ‘_topdir /tmp/MLNX_OFED_LINUX-4.5-1.0.1.0-4.14.0-115.29.1.el7a.ppc64le/mlnx_iso.126164/OFED_topdir’ --define ‘_sourcedir %{_topdir}/SOURCES’ --define ‘_specdir %{_topdir}/SPECS’ --define ‘_srcrpmdir %{_topdir}/SRPMS’ --define ‘_rpmdir %{_topdir}/RPMS’ --define ‘src_release OFED.4.5.1.0.1.1.gb4fdfac.kver.4.14.0_115.29.1.el7a.ppc64le’ --define ‘KVERSION 4.14.0-115.29.1.el7a.ppc64le’ --define ‘K_SRC /lib/modules/4.14.0-115.29.1.el7a.ppc64le/build’ --define ‘_prefix /usr’ /tmp/MLNX_OFED_LINUX-4.5-1.0.1.0-4.14.0-115.29.1.el7a.ppc64le/mlnx_iso.126164/MLNX_OFED_SRC-4.5-1.0.1.0/SRPMS/mlnx-rdma-rxe-4.5-OFED.4.5.1.0.1.1.gb4fdfac.src.rpmFailed to build mlnx-rdma-rxe 4.7 RPMInformation about my Server:IBM Power Systems AC922Red Hat Enterprise Linux Server release 7.6 (Maipo)Linux SinergiaAC922 4.14.0-115.29.1.el7a.ppc64le #1 SMP Tue Jul 28 22:41:18 UTC 2020 ppc64le ppc64le ppc64le GNU/LinuxMLNX_OFED_LINUX-4.7-3.2.9.1-rhel7.6alternate-ppc64le.isoThanks.HI Americo,Which image are you using ?RHEL/CentOS 7.6 or RHEL/CentOS 7.6 alternate ?Did you try to use the latest MLNX_OFED 5.2-1.0.4.0Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)Thanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
822,cant-access-gpu-from-bluefield2,"Hi, I follow the guide to access GPU from bluefield2.I configured the DPU to BlueField-X mode.But after rebooting, the A30 and A5000 cards are still visible from the host and invisible from the DPU.Did I miss any configurations? Or are there any other ways to access GPU from DPU? Thanks.I guess, it seems like only the GPU model with ‘x’ suffix can be configured with bluefield-x mode.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
823,all-my-mlx-cx5-2-100g-cards-unable-to-line-rate-speed,"Hi all,​I have two mellanox cx5 2*100G nic. I following the dpdk 20.11 mlx performance report Test#3 Mellanox ConnectX-5 Ex 100GbE Throughput at Zero PacketLoss (1x 100GbE)。Spirent TestCenter send 300B packet 39Mpps，server run dpdk testpmd in fwd io mode，but throughput just 85% of line ratre. How to check what problem cause the low perfmance ?More infomation please see the attachement file.Mellanox CX5 unable to line rate speed.docx (484 KB)Hello Eason,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, performance difference can occur, when not the same test setup is being used for these performance tests. Also from your documentation, it is clear that you are using a ConnectX-5 EN adapter and not the ConnectX-5 Ex adapter (which is a low-latency version of the ConnectX-5 and is PCIe Gen4 X16, see PB → https://www.mellanox.com/files/doc-2020/pb-connectx-5-en-card.pdf).Also we recommend to follow the performance reports from the latest DPDK version available.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
824,we-have-a-mellanox-technologies-mt27630-family-and-mellanox-technologies-mt27710-family-cards-that-has-pre-set-maximums-rx-64-and-tx-8192-is-it-possible-to-have-the-rx-value-increased,"[me~]$ ethtool -g eth4Ring parameters for eth4:Pre-set maximums:RX: 64RX Mini: 0RX Jumbo: 0TX: 8192Current hardware settings:RX: 64RX Mini: 01RX Jumbo: 0TX: 8192Hello Anthony,Thank you for posting your inquiry on the NVIDIA Networking Community.Yes, you can change both TX and RX ring buffers to a max of 8192. Be aware that increasing the ring size buffer will allocated more memory. Make sure you have enough memory in your system to do this.Command syntax:# ethtool -G eth tx 8192 rx 8192Thank you and regards,~NVIDIA Networking Technical Support@Martijn van Breugel​ Thank you for you response, yes I have already increased the buffer values but for clarification I wanted to know if I can change the pre-set RX 64 to higher value and then change the Current hardware settings from 64 to the new value if I can able to increase the pre-set max value of 64.Please let me know if you need further details.@Martijn van Breugel​ What I want to do is increase the buffer size for network interface card. With ethtool I have set the RX buffer size to 64 KB. But I want to increase to more than 64 KB. Is that possible, and what should I do?Powered by Discourse, best viewed with JavaScript enabled"
825,qsfp56-ports-accept-qsfp28-devices,"Hi Mellanox,according to note 5 in this table:https://docs.mellanox.com/display/CABLEMANAGFAQ/Cable+and+Connector+DefinitionsI assume it is possible to connect the MCX653106A-ECAT-SP ConnectX-6 VPI Adapter Card featuring QSFP56 connector to a Xilinx FPGA with 4x SFP28 connectors, using a QSFP28 to SFP28 breakout cable.The FPGA can reach 4x25Gb/s using the four SFP28. What is the maximum bandwidth in the connectx 6 in this setup (100 or 50Gb/s) ?best regardsmy concern is related to modulation protocol in use:does a QSFP56 port switches to NRZ modulation when a QSFP28 cable is plugged ?Hello Raphael,Thank you for posting your inquiry on the NVIDIA Networking Community.The ConnectX-6 VPI adapter supported PAM4 and NRZ. See the following link for all Features available for the ConnectX-6 adapter → https://docs.mellanox.com/display/ConnectX6Firmwarev20292002/Changes+and+New+Feature+HistoryTo connect the cable to the ConnectX-6, you need to use the MAM1Q00A-QSA28 module as mentioned in the f/w RN (Search for MAM1Q00A-QSA28) → https://docs.mellanox.com/display/ConnectX6Firmwarev20292002/Firmware+Compatible+ProductsThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
826,sshd-14556-error-kex-exchange-identification-in-sn2010,"Hello!After updating Onix version from 3.8.2306 to 3.9.3202 the configuration stopped being collected on both backup systems cBackup and rConfig.Please help me figure out the problem probably the problem is in the SSH configuration in SN2010.Thank you in advance for your help!Log cBackup:can’t establish SSH connection.Message: Algorithm negotiation failLog rConfig:CONN Error: Authentication Failed or unable to connect to 172.17.106.38 on port 22CONN Failure: Unable to connect via SSH to mellanox-sw-sn2010-1-tst - 172.17.106.38 for command (show configuration) when running Router ID 3 - in ErrorLog SN2010:Sep 16 13:03:40 mellanox-sw-sn2010-1-tst sshd[14556]: error: kex_exchange_identification: Connection closed by remote hostConfiguration SSH SN2010:mellanox-sw-sn2010-1-tst [standalone: master] (config) # show ssh serverSSH server configuration:SSH server enabled: yesServer security strict mode: yesMinimum protocol version: 2TCP forwarding enabled: noX11 forwarding enabled: noSSH login timeout: 120SSH login max attempts: 6SSH server login record-period: 1SSH server ports: 22Interface listen enabled: yesListen Interfaces:No interface configured.Host Key Finger Prints and Key Lengths:RSA v2 host key: SHA256:FLDjgmPok/oBf9NWuU+tqg2uhPmMkwiuBymp3TBHXn4 (2048)DSA v2 host key: SHA256:GYpxCAFnzGI4HNiR/FLFOjdmK70DovN+2A+yK1ZfcUM (1024)
log_SN20101067×664 187 KB

log_cBackup1029×740 97.5 KB
Hi Roman,As of 3.9.32xx SSH cipher diffie-hellman-group14-sha1is deprecated and was removed from strict SSH ciphers to non-strict ciphers.So Please make sure that your ssh client in the configuration management system supports other ciphers.Thank you for helping Eddie.Powered by Discourse, best viewed with JavaScript enabled"
827,can-i-use-the-same-ip-address-vlan-mpo-for-the-ipl-in-2-different-mlag-pair-but-connected-to-same-uplink-switch,"I have an MLAG Pair using the following for the IPL:Can I have another MLAG Pair, with the same VLAN, Mpo number and IP address for its IPL?Both “pairs” would be connected to an uplink switch (core switch)Would this work? Or do I need to make them unique across MLAG pairs?ThanksHi Shadowplay,I assume you are referring to the IPL Port-channel (PO) and not MPO (Mlag Port-channel).
The answer is yes - ipl vlan address can be reused ( it’s not a best practice)as once the IPL vlan interface is configured, that VLan is removed / can’t be configured on any other switchport. it’s just used in the IPL PO.Powered by Discourse, best viewed with JavaScript enabled"
828,problems-when-i-try-to-install-ofed,"Hello!When I tried to install OFED, at the end he ask me to load the new driver using:"" To load the new driver, run:/etc/init.d/openibd restart""But when I tried I got it:
$: sudo /etc/init.d/openibd restart
Unloading mlx_compat                                       [FAILED]
rmmod: ERROR: Module mlx_compat is in use by: rpcrdma svcrdmaI’m trying everything I know to try to stop this module but I’m at a dead end, Can someone help me?I’m using Rocky Linux 9 and the Version of OFED is 23.04-1.1.3.0rpcrdma svcrdmaHello @ppinheiro,Thank you for posting your query on our community. Please try the following options to help resolve the issue,Hope this helps.Thank & Regards,
BhargaviPowered by Discourse, best viewed with JavaScript enabled"
829,is-there-a-programmers-guide-available-for-the-connextx-5-series-of-cards,"I have found “Mellanox Adapters Programmer’s Reference Manual (PRM)” but is Revison 0.40 from 2016 and specifically mentions “Supporting ConnectX®-4 and ConnectX®-4 Lx”. I’m looking to see if there is a newer version of this document that includes any changes for the ConnectX-5 series of cards. If not, will this document also apply to the newer 5 (and 6) series?Hello Leonard,Thank you for posting your inquiry on the NVIDIA Networking Community.Yes, there is a recent version which describes all the adapters including the ConnectX-5/6/Bluefield.This document is only available for customers with a valid support contract. In case you have a valid support contract, please send an email to networking-support@nvidia.com and it will be provided through the ticket.Thank you and regards,~NVIDIA Networking Technical SupportStill only available for customers with a valid support contract now?
and where can we buy the support contract?Powered by Discourse, best viewed with JavaScript enabled"
830,mlnx-en-5-7-1-0-2-0-rhel9-0-error-mnt-rpms-read-failed-is-a-directory-21,"Hello Forum,i am trying to install the NVIDIA/MELLANOX drivers fromCore error message:Applied procedure:I drilled down to where the procedure breaks:Attempting to reproduce it:I doublechecked for the presence of ACLs, SELinux labels or filesystem permissions.
Also tried directories outside of “/tmp” and “/mnt”.
But nothing helped the problem.
I just want to make sure the evaluation skript would not indicate incompatibility for the wrong reasons…
Has anyone an idea what actually is the problem?Any idea would be highly appreciated.BestHello again,i just wanted to update, that apparently the error message can be read verbatim:
it simply attempts to read the directory.
Pointing it directly to the files would make it operable:But that format again is not what “is_kmp_compat.sh” expects:which also would contradict the name of the option (path)…Do i possibly use the wrong syntax for the options?BestHi,
Thanks for contacting us.
From looking at the issue and the details you have added these versions of Rocky9.0 or Centos 9.0 have not been tests and are supported yet.
Please visit this link to see which OSs have been tested and supported with the last versions of MOFED 5.7-1.0.2.0
https://network.nvidia.com/products/infiniband-drivers/linux/mlnx_ofed/Rocky 9 will be supported in future releases.
Thanks and have a great day!
Ilan.Hi ipavis,please excuse my late replay.
Thank you very much for your answer.
This will be great, if Rocky will be officially supported in the future, since we haved switched to it completely.For the time being, the way to go about it seems to be to look up the sources under
“Linux InfiniBand Drivers / Community / rpm”
and compile them.BestThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
831,error-installing-mellanox-ofed-on-centos-7-9,"I’m trying to install the Mellanox OFED on a CentOS 7.9 that I have on a Singularity container with the following command:./mlnxofedinstall --add-kernel-support --force --skip-repoHowever, I get the following error:/lib/modules/4.17.0-rc2/build/scripts is required to build mlnx-ofa_kernel RPM.Please install the corresponding kernel-source or kernel-devel RPM.Error: One or more required packages for installing OFED-internal are missing.Please install the missing packages using your Linux distribution Package Management tool.Run:yum install kernel-devel-4.17.0-rc2Failed to build MLNX_OFED_LINUX for 4.17.0-rc2When I try yum install kernel-dev-4.17.0-rc2, I get the following error:No package kernel-devel-4.17.0-rc2 available.Error: Nothing to doAnd when I try yum install kernel-dev, an older version 3.something is installed that does not fix the issue.This is output of uname -s -r:Linux 4.17.0-rc2And this is the output of ls -l /usr/src/kernels/:3.10.0-1160.53.1.el7.x86_643.10.0-1160.53.1.el7.x86_64.debug4.17.0-0.rc2.git3.1.fc29.x86_64Could you please let me know how I can fix this?Hello Mehran,Thank you for posting your inquiry on the NVIDIA Networking Community.As you want to install MLNX_OFED on a non-default kernel on CentOS 7.6, please make sure you have the following RPMS installed as mentioned in the RN under section → https://docs.nvidia.com/networking/display/MLNXOFEDv494170/General+Support+in+MLNX_OFED#GeneralSupportinMLNX_OFED-HardwareandSoftwareRequirementsBe aware that in some cases, it is not possible to rebuild the driver against the installed non-default kernel, if the kernel is much more recent than the MLNX_OFED version you want to install.Also make sure you use the same GCC version on which your kernel is build.Another method, is to run the ‘mlnx_add_kernel_support.sh’ script manually, by pointing towards the correct kernel source. Use the ‘help’ option for the available options.Thank you and regards,~NVIDIA Networking Technical SupportHi Martijn,Thanks a lot for your response. I’m wondering if this is the correct approach I’m pursuing for enabling Infiniband on a Singularity container. I have not been able to find suitable instructions. I would be so grateful if you could kindly help with that.(I need to compile MVAPICH (MPI for Infiniband) on my Singularity container which requires enabling IB on my container.)Powered by Discourse, best viewed with JavaScript enabled"
832,mlnx-ofed-all-rpm-in-el8-libfabric-obsoletes-issue,"I have a customer wanting to use libfabric along with mlnx-ofed-all rpm package in Rocky 8.4. We did not have issues with both libfabric and mlnx-ofed-all being installed together on CentOS 7.x variants. We have found that the new mlnx-ofed-all EL8 rpm contains a obsoletes flag for libfabric, which is why were can’t install both libfabric and mlnx-ofed-all rpm’s along side each other in EL8. We can’t seem to find any documentation on why this flag has been set. Any explanation would be greatly appreciated here. I also don’t see that the mlnx-ofed-all EL8 RPM contains any of the libfabric commands.Let me know if I can provide any more details or information. Appreciate the time!Hi Kevin,Thanks for posting your inquiry to the NVIDIA Developer Community.Support for community operating systems (Rocky Linux, Alma, PhotonOS, and others) is not available in MLNX_OFED 5.4-1.0.3.0. Support for these OSes is only tested and supported as of MLNX_OFED 5.6-1.0.3.0.As of this release, we only test and validate Rocky Linux v8.5 and later:
https://docs.nvidia.com/networking/display/MLNXOFEDv561033/General+Support
[§ ‘Supported Community Operating Systems’]For community operating systems, installation instructions can be found here:
https://docs.nvidia.com/networking/display/MLNXOFEDv561033/Installing+MLNX_OFED
[§ ‘Installing OFED on Community Operating Systems’]Regardless, the fact that libfabric is obsoleted is expected here. libfabric relies on inbox driver packages - MLNX_OFED packages are not compatible with it. To install libfabric would replace the MLNX_OFED-provided libibverbs, libibverbs-utils, rdma-core-devel, ibacm, libibumad, librdmacm, librdmacm-utils, infiniband-diags, srp_daemon, and some others with the versions in the OS repository. As such, we obsolete these packages as part of MLNX_OFED installation.IB packages coming from the OS vendor repositories are not built against MLNX_OFED libraries, they are built against the OS vendor-provided driver/libraries. If using libfabric is a requirement, you will need to uninstall MLNX_OFED, and use the OS vendor-provided upstream driver/libraries.I hope that clears things up. If you require further assistance on this issue, please open a support case at https://support.mellanox.com/s (or email networking-support@nvidia.com).Best,
Sam Simcoe
NVIDIA Networking SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
833,mellanox-connectx-6dx-support-for-symmetric-rss-hash-inner-ip,"Test-setup:Does the adapter have support for Symmetric RSS Hash & Inner IP (IP inside tunnels like GTP, GRE etc) based RSS distribution on dpdk?
If yes, please guide us which options we should use to configure adapter & dpdk.Hello Burto,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.We provided this info already in another thread, but we will repeat it again. Windows DPDK Support is very limited. See the link regarding the limitations.The following link provides the information regarding the limitations for Windows DPDK when using the ConnectX-6 Dx → 36. MLX5 Ethernet Poll Mode Driver — Data Plane Development Kit 22.07.0 documentationThe following link provides the information on how-to build the Windows PMD → 5. MLX5 Common Driver — Data Plane Development Kit 22.07.0 documentationHope this will get you further.Thank you and regards,
~NVIDIA Networking Technical SupportHi MvB,In the link, RSS is mentioned as supported. But there is no explicit information for RSS Hash on Inner IP (IP inside tunnels like GTP, GRE etc).The flag (ETH_RSS_LEVEL_INNERMOST) for inner IP is not working. How to enable RSS Hash on Inner IP?Thanks,
BurtoPowered by Discourse, best viewed with JavaScript enabled"
834,release-notes-for-nvidia-bright-cluster-manager-9-2-11,"Release notes for Bright 9.2-11== General ==
=Improvements==Fixed Issues==Changes=== CMDaemon ==
=New Features==Improvements==Fixed Issues=== Node Installer ==
=Fixed Issues=== Cluster Tools ==
=Fixed Issues=== Head Node Installer ==
=New Features==Fixed Issues=== Machine Learning ==
=New Features=== cm-kubernetes-setup ==
=Fixed Issues=== cm-scale ==
=New Features==Fixed Issues=== cm-wlm-setup ==
=Improvements=== cmsh ==
=Improvements==Fixed Issues=== slurm23.02 ==
=Improvements=Powered by Discourse, best viewed with JavaScript enabled"
835,how-can-i-program-and-deploy-custom-congestion-control-algorithms-in-connectx-6-dx,"ConnectX-6 Dx’s datasheet describes programmable congestion control APIs.The user manual describes how to tune the parameters of the default congestion control DCQCN.Additionally, I found that ZTR RTTCC can be applied through firmware update.However, the way that I am looking for is how to program and deploy the new Congestion Control algorithm.If you know about this, could you share your experience or help me?Best.
TaekyoungPowered by Discourse, best viewed with JavaScript enabled"
836,connectx-5-en-2x100g-with-finisar-100g-sr4-ftlc9555repm,"Hi allDoes anybody knows if FINISAR Transceiver 100G SR4 FTLC95555REPM is compatible with Connectx-5 EN 2x1000G?Thanks for your help.Hi ,Please refer to the supported transceivers page on the release notes of the firmware :https://docs.mellanox.com/display/ConnectX5Firmwarev16301004/Firmware+Compatible+ProductsThanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
837,rdma-connectx-3-smb-bsod-lm-server-internal-error,"Hello,We have next env:Windows Server 2016mellanox Connectx-3 (fw:2.42.5000; winof: 5.50.53)Created hyper-v team switch, then vNICs, qos, etc…When RDMA enabled on one vnics and disabled on other vnics and copying big file ~20TB - smb server is crashed with error LM_SERVER_INTERNAL_ERROR:PS C:\Users\pnd> get-netadapterrdmaName InterfaceDescription EnabledvEthernet (v23-cluster) Hyper-V Virtual Ethernet Adapter #3 FalsevEthernet (v22-lm) Hyper-V Virtual Ethernet Adapter #2 TruevEthernet (v21) Hyper-V Virtual Ethernet Adapter Falseib2 Mellanox ConnectX-3 Ethernet Adapter Trueib1 Mellanox ConnectX-3 Ethernet Adapter #2 TrueWhen copying small file ~1TB - smb server not crashedWhen rdma is disabled and no matter for file size - smb server not crached.Please help :)Thanks.Hello Leonid,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, unfortunately the error you are receiving is a Microsoft error. And should be further investigated by Microsoft support. We recommend to open a support ticket with Microsoft directly for this. If Microsoft sees an issue with our driver, they will open a case with us directly.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
838,cant-seem-to-set-lro-to-on-using-connectx-4-lx-and-ubuntu-20-04,"Sorry for the very basic question, but I am not having any success with this. I’d like to run some tests with large-receive-offload enabled but I am unable to turn the feature on. Everything I’ve read about this adapter implies that I should be able to use this feature.$ uname -aLinux ts-dc1-ph-j1csk93 5.4.0-84-generic #94-Ubuntu SMP Thu Aug 26 20:27:37 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux$ ethtool -i ens1f0driver: mlx5_coreversion: 4.9-3.1.5$ sudo ethtool --show-offload ens1f0 | grep large-receive-offloadlarge-receive-offload: off$ sudo ethtool --offload ens1f0 lro onCould not change any device features$ sudo ethtool --show-offload ens1f0 | grep large-receive-offloadlarge-receive-offload: off [requested on]Any pointers would be greatly apprciated.Forgot to mention, I can successfully modify most other offload features, but not LRO.Software-based LRO was deprecated in kernel 4.10: ipv4: Remove inet_lro library · torvalds/linux@7bbf3ca · GitHubRegarding Hardware LRO, Ethernet-based NICs that do not support the new API (which allows merging packets before they enter the kernel, i.e., hardware) can only use GRO.For most intents and purposes LRO functionality has been subsumed by GRO functionality in later kernels and this was kept in mind when the drivers for these devices were written. In some cases, older kernels prior to the 4.10 kernel can have the legacy LRO functions enabled through ethtool, and in those circumstances, more information on the ethtool syntax/usage can be found here:https://docs.mellanox.com/display/MLNXOFEDv493150/EthtoolI suspect that this is the case for your installation based on the kernel version you mentioned. If in doubt for future implementations, you can also confirm whether the LRO value is [fixed] by checking the output of “ethtool -k ”, in which case the option can’t be toggled:For example, results on a 3.10 kernel system:[root@LABSYSTEM0 ~]# ethtool -k enp3s0f0 | grep receivegeneric-receive-offload: onlarge-receive-offload: offreceive-hashing: on[root@LABSYSTEM0 ~]# ethtool -K enp3s0f0 lro on[root@LABSYSTEM0 ~]# ethtool -k enp3s0f0 | grep receivegeneric-receive-offload: onlarge-receive-offload: onreceive-hashing: onResults on a 5.11 kernel system:root@LABSYSTEM2:~# ethtool -k enp5s0f0 | grep largelarge-receive-offload: off [fixed]root@LABSYSTEM2:~#root@LABSYSTEM2:~# ethtool -K enp5s0f0 lro onActual changes:rx-lro: off [requested on]Could not change any device features-Nvidia Network SupportThanks very much for your detailed reply. That all makes sense. Based on everything you said I do have one followup question…Since we are running kernel 4.9 (so, < 4.10) and since running …ethtool --show-offload ens1f0 | grep large-receive-offload… returns …large-receive-offload: off… and does NOT return …large-receive-offload: off [fixed]… it seems that I should be able to enable LRO, but I cannot. All attempts at running …ethtool --show-offload ens1f0 | grep large-receive-offload… return …large-receive-offload: off [requested on]It is acting as if the feature is “fixed” but it is not showing the feature as “fixed”.Dang, cut-n-paster error in the last part of the reply above. Meant to say …All attempts at running …ethtool --offload ens1f0 lro on… return …large-receive-offload: off [requested on]It is acting as if the feature is “fixed” but it is not showing the feature as “fixed” in the original “ethtool --show-offload ens1f0” query.Powered by Discourse, best viewed with JavaScript enabled"
839,lts-for-centos-8-4,"Hi,will there be support for LTS drivers on CentOS 8.4? When?Hello Michael,Thank you for posting your inquiry on the NVIDIA Networking Community.Yes, there will be support for RHEL/CentOS 8.4 in the upcoming MLNX_OFED 4.9 LTS version. This version is targeted somewhere around Q4 of this year. Exact date, unfortunately we cannot provide at this time.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
840,slurm-fails-with-multiple-processes-mpi-init-errors-pml-add-procs-failed,"Hi Folks , used slurm workload manager in Bright Cluster manager 9.2 and just edited the slurm.conf file for GRES configurations  , MPI_init errors showing up when running srun or sbatch with multiple processes , single process passes . Below is the trace
FYI , using enroot + pyxis for running containers with slurm .UCX and PMIX errors can be ignored by adding the flags in srun or sbatchBelow is the trace from simply running < ls > command on node001 , it runs fineHi.While we appreciate the questions, I must stress that this forum is NOT a replacement for professional support.
We will answer your questions when we have spare cycles to do so, but you should strongly consider purchasing a commercial license to gain access to the support team.kwThanks Ken , I understand but I have no choice as of now . I am trying to build a HPC cluster POC around Bright so that I can put up across my management for buying licenses etc . Till then I just have to figure myself or hope for the help :)Ah! That’s good to know.In that case, the correct path forward is to obtain an evaluation license from the sales team. An eval license will give you access to the support team.Would you like to send me your contact information via email, (kwoods@nvidia.com), and I can coordinate contact with sales?Thanks,
kwI have easy8 license that’s being used right now , I tried getting the evaluation license but nobody contacted me . I will send the email to you . Appreciate itkwoods@nvidia.comThanks Ken , got the requested support. Is bright website down today , I am trying to login but page getting timed out .Bright Computing - Customer LoginPowered by Discourse, best viewed with JavaScript enabled"
841,connectx-5-25gbe-missing-rdma-devices,"On a Ubuntu 20.04 system we have a ConnectX-5 dual port 25GbE adapter (MCX512F-ACAT). tcp Communication over a port of this card is possible, but we see no rdma device.system: Ubuntu 20.04.4
kernel 5.4.0-100-generic (5.4.0-126-generic also tested)
MOFED 5.4-3.5.8.0-ubuntu20.04
adapter FW: 16.34.1002on boot we see some messages, which we do not have on other nodes:thanks for any idea, RalfHi Ralf,Thank you for contacting us.
After looking at the details you added in the forum I can see that your MOFD version is not supported by the FW you are using.
Please review this link:
https://docs.nvidia.com/networking/display/ConnectX5Firmwarev16341002/Firmware+Compatible+Products#FirmwareCompatibleProducts-DriverSoftware,ToolsandSwitchFirmwareInstalling the supported version of MOFED should fix your issue.Thanks and have a great day!
Ilan.Hi Ilan,thanks for your answer. I looked up the docs at the provided link and installed MOFED 5.7-1.0.2.0 corresponding to the fw version 16.34.1002.  I still get the ibv_devinfo output “no devices” …
I also tried with  downgraded firmware  16.31.2006 and MOFED 5.4-3.5.8.0 which also did not show rdma devices.
What else can I check? Besides ordering a Dell OEM mlx adapter for this machine…
Kind regards, RalfWe now installed a Dell OEM  ConnectX-5 card, P/N 04TRD3 and now we see the RDMA devices. This card shows PSID DEL0000000016,
while the other card shows MT_0000000183best regards, RalfThe MCX512F-ACAT (MT_0000000183) is now running in an older
Primergy RX200 S7 server and ibv_devices shows the devices there.
best regards, RalfPowered by Discourse, best viewed with JavaScript enabled"
842,bluefield-2-host-connectivity-problem,"Hello,I have some issues to connect host to BlueField-2. I will appreciate if you can help on this to me.My HOST configuration is as follows:DEVICE_TYPE             MST                           PCI       RDMA            NET                       NUMA
BlueField2(rev:1)       /dev/mst/mt41686_pciconf0.1   c1:00.1   mlx5_3          net-enp193s0f1            1BlueField2(rev:1)       /dev/mst/mt41686_pciconf0     c1:00.0   mlx5_2          net-ib0                   1ConnectX6(rev:0)        /dev/mst/mt4123_pciconf0.1    a1:00.1   mlx5_5          net-ib2                   1ConnectX6(rev:0)        /dev/mst/mt4123_pciconf0      a1:00.0   mlx5_4          net-ib1                   1ConnectX4LX(rev:0)      /dev/mst/mt4117_pciconf0.1    21:00.1   mlx5_1          net-enp33s0f1             0ConnectX4LX(rev:0)      /dev/mst/mt4117_pciconf0      21:00.0   mlx5_0          net-enp33s0f0             0mt41686_pciconf0.1_cable_1
mt41686_pciconf0_cable_0
mt4123_pciconf0_cable_0
mt4117_pciconf0_cable_0mashemat@n017:~$ sudo mlxconfig -d /dev/mst/mt41686_pciconf0 q | grep -i internal_cpu_model
INTERNAL_CPU_MODEL                  EMBEDDED_CPU(1)mashemat@n017:~$ ifconfig
enp193s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 192.168.200.10  netmask 255.255.255.0  broadcast 192.168.200.255
ether b8:ce:f6:e6:ec:4d  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 7470  bytes 448200 (448.2 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0enp33s0f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 10.128.1.17  netmask 255.255.254.0  broadcast 10.128.1.255
ether b8:ce:f6:ce:e2:20  txqueuelen 1000  (Ethernet)
RX packets 3358949  bytes 3126947094 (3.1 GB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 3589130  bytes 3621327041 (3.6 GB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0enp33s0f1: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
inet 192.168.200.20  netmask 255.255.255.0  broadcast 192.168.200.255
ether b8:ce:f6:ce:e2:21  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0enp68s0f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 192.168.200.30  netmask 255.255.255.0  broadcast 192.168.200.255
ether 3c:ec:ef:5e:8b:d6  txqueuelen 1000  (Ethernet)
RX packets 23517  bytes 1716930 (1.7 MB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0enp68s0f1: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
inet 192.168.200.40  netmask 255.255.255.0  broadcast 192.168.200.255
ether 3c:ec:ef:5e:8b:d7  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0ib1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 2044
inet 10.128.3.17  netmask 255.255.254.0  broadcast 10.128.3.255
unspec 20-00-10-49-FE-80-00-00-00-00-00-00-00-00-00-00  txqueuelen 256  (UNSPEC)
RX packets 39467  bytes 6391446 (6.3 MB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 63690  bytes 5477612 (5.4 MB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
inet 127.0.0.1  netmask 255.0.0.0
loop  txqueuelen 1000  (Local Loopback)
RX packets 345006  bytes 28018594 (28.0 MB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 345006  bytes 28018594 (28.0 MB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0tmfifo_net0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 192.168.100.1  netmask 255.255.255.0  broadcast 192.168.100.255
ether 00:1a:ca:ff:ff:02  txqueuelen 1000  (Ethernet)
RX packets 5641  bytes 900292 (900.2 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 9332  bytes 784838 (784.8 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0my DPU configuration is as follows:ubuntu@n17-dpu:~$ ifconfig
en3f1pf1sf0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 192.168.200.1  netmask 255.255.255.0  broadcast 192.168.200.255
ether 22:1b:62:d8:04:8c  txqueuelen 1000  (Ethernet)
RX packets 4980  bytes 1587026 (1.5 MB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0enp3s0f1s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 192.168.200.50  netmask 255.255.255.0  broadcast 192.168.200.255
inet6 fe80::87:abff:fe54:8917  prefixlen 64  scopeid 0x20
ether 02:87:ab:54:89:17  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 4980  bytes 1587026 (1.5 MB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
inet 127.0.0.1  netmask 255.0.0.0
inet6 ::1  prefixlen 128  scopeid 0x10
loop  txqueuelen 1000  (Local Loopback)
RX packets 75  bytes 8106 (8.1 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 75  bytes 8106 (8.1 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0oob_net0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 10.128.1.21  netmask 255.255.254.0  broadcast 10.128.1.255
inet6 fe80::bace:f6ff:fee6:ec56  prefixlen 64  scopeid 0x20
ether b8:ce:f6:e6:ec:56  txqueuelen 1000  (Ethernet)
RX packets 619415  bytes 60136794 (60.1 MB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 87347  bytes 7813013 (7.8 MB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0p1: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
ether b8:ce:f6:e6:ec:51  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0pf1hpf: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet6 fe80::a8b5:7dff:fe6f:a6b4  prefixlen 64  scopeid 0x20
ether aa:b5:7d:6f:a6:b4  txqueuelen 1000  (Ethernet)
RX packets 7470  bytes 448200 (448.2 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0tmfifo_net0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 192.168.100.2  netmask 255.255.255.252  broadcast 192.168.100.3
inet6 fe80::21a:caff:feff:ff01  prefixlen 64  scopeid 0x20
ether 00:1a:ca:ff:ff:01  txqueuelen 1000  (Ethernet)
RX packets 9435  bytes 793448 (793.4 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 5699  bytes 910404 (910.4 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0ubuntu@n17-dpu:~$ sudo systemctl status openvswitch-switch
● openvswitch-switch.service - LSB: Open vSwitch switch
Loaded: loaded (/etc/init.d/openvswitch-switch; generated)
Active: active (running) since Mon 2022-03-07 13:46:22 UTC; 50min ago
Docs: man:systemd-sysv-generator(8)
Process: 636766 ExecStart=/etc/init.d/openvswitch-switch start (code=exited, status=0/SUCCESS)
Tasks: 14 (limit: 19077)
Memory: 196.9M
CGroup: /system.slice/openvswitch-switch.service
├─636822 ovsdb-server: monitoring pid 636823 (healthy)
├─636823 ovsdb-server /etc/openvswitch/conf.db -vconsole:emer -vsyslog:err -vfile:info --remote=punix:/var/run/openvswitch/db.sock --private-key=db:Open_vSwitch,SSL,private_key --certificate=db:O>
├─636839 ovs-vswitchd: monitoring pid 636840 (healthy)
└─636840 ovs-vswitchd unix:/var/run/openvswitch/db.sock -vconsole:emer -vsyslog:err -vfile:info --mlockall --no-chdir --log-file=/var/log/openvswitch/ovs-vswitchd.log --pidfile=/var/run/openvswit>Mar 07 13:46:21 n17-dpu systemd[1]: Starting LSB: Open vSwitch switch…
Mar 07 13:46:21 n17-dpu openvswitch-switch[636802]:  * Starting ovsdb-server
Mar 07 13:46:21 n17-dpu ovs-vsctl[636824]: ovs|00001|vsctl|INFO|Called as ovs-vsctl --no-wait – init – set Open_vSwitch . db-version=8.3.0
Mar 07 13:46:21 n17-dpu ovs-vsctl[636829]: ovs|00001|vsctl|INFO|Called as ovs-vsctl --no-wait set Open_vSwitch . ovs-version=2.15.1 “external-ids:system-id=""0f4d895c-2f66-4994-bb5f-37a7adc04a38""” ""external->
Mar 07 13:46:21 n17-dpu openvswitch-switch[636802]:  * Configuring Open vSwitch system IDs
Mar 07 13:46:22 n17-dpu openvswitch-switch[636802]:  * Starting ovs-vswitchd
Mar 07 13:46:22 n17-dpu ovs-vsctl[636857]: ovs|00001|vsctl|INFO|Called as ovs-vsctl --no-wait add Open_vSwitch . external-ids hostname=localhost.localdomain
Mar 07 13:46:22 n17-dpu openvswitch-switch[636802]:  * Enabling remote OVSDB managers
Mar 07 13:46:22 n17-dpu systemd[1]: Started LSB: Open vSwitch switch.
ubuntu@n17-dpu:~$ sudo ovs-vsctl show
57d70fe0-b54e-4a77-8720-eb96d26d5744
Bridge ovsbr1
Port pf1hpf
Interface pf1hpf
Port ovsbr1
Interface ovsbr1
type: internal
Port en3f1pf1sf0
Interface en3f1pf1sf0
Port p1
Interface p1
Port enp3s0f1s0
Interface enp3s0f1s0
ovs_version: “2.15.1”ubuntu@n17-dpu $ sudo ovs-ofctl dump-flows ovsbr1
cookie=0x0, duration=3083.972s, table=0, n_packets=0, n_bytes=0, priority=0 actions=NORMALWhen I try to ping from host to DPU:
mashemat@n017:~$ ping 192.168.200.1PING 192.168.200.1 (192.168.200.1) 56(84) bytes of data.From 192.168.200.10 icmp_seq=1 Destination Host Unreachable
From 192.168.200.10 icmp_seq=2 Destination Host Unreachable
From 192.168.200.10 icmp_seq=3 Destination Host UnreachableIt does not connect. But the packet arrives up to pf1hpfpf1hpf: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet6 fe80::a8b5:7dff:fe6f:a6b4  prefixlen 64  scopeid 0x20
ether aa:b5:7d:6f:a6:b4  txqueuelen 1000  (Ethernet)
RX packets 7479  bytes 448740 (448.7 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0But it does not reach to any of interfaces or even outside.Could you please let em know where would be possible issue?Thanks,Same issue here…My issue was solved. The personality of the ports were not match with the switch. Try to check the link type as follows:test@n018:~$ sudo mlxconfig query |grep -e LINK_TYPE -e “Device.*mst”
Device:         /dev/mst/mt41686_pciconf0
LINK_TYPE_P1                        ETH(2)
LINK_TYPE_P2                        IB(1)Device:         /dev/mst/mt4123_pciconf0
LINK_TYPE_P1                        IB(1)
LINK_TYPE_P2                        IB(1)
Device:         /dev/mst/mt4117_pciconf0They should be connected correctly to the proper switch (IB/ETH).Thanks,enp3s0f1s0I see enp3s0f1s0 is in Bridge ovsbr1.
Is it a must?
It’s absent in Bridge ovsbr1 on my testbed.
And there’s no output “$ sudo mlxconfig query |grep -e LINK_TYPE” on my test bed.
Thanks.Hi wangyw,No actually, when I reboot the DPU it back to the default config:ubuntu@n19-dpu :~$ sudo ovs-dpctl show
system@ovs-system:
lookups: hit:108569 missed:17973 lost:3
flows: 1
masks: hit:126532 total:1 hit/pkt:1.00
port 0: ovs-system (internal)
port 1: ovsbr1 (internal)
port 2: en3f0pf0sf0
port 3: p0
port 4: pf0hpfHere is my query:mashemat@n019:~$ sudo mlxconfig query |grep -e LINK_TYPE
LINK_TYPE_P1                        ETH(2)
LINK_TYPE_P2                        IB(1)Seems you dont see your links.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
843,mellanox-mcx654105a-hcat-and-dell-r7525,"Hi all, attempting to achieve 200Gb interconnect between a few R7525s.Tl;dr: can these cards (dual PCIe) be made compatible with this machine?After installation, following the hardware maintenance guide on proper riser distribution, I am presented with “The system detected an exception during the UEFI pre-boot environment.” during POST, where the system hangs.BIOS, Networking FW, iDRAC and LCC, are at latest versions.Any and all suggestions greatly appreciated.After testing the card in other systems at my disposal, I found one that would POST with it.Once booted, I decided to update the card’s firmware to latest using mstflint. Swapped it back into the non-booting R7525, and behold, it POSTed.With the secondary PCIe board installed, the system correctly reads an available bandwidth of 200Gb/s.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
844,unable-to-set-sniffer-on-nic-card-when-using-v5-8-driver,"NIC: CX-5I use v4.5-1.0.1.0 driver before, and can set sniffer on by using: ethtool --set-priv-flags xxxxx sniffer on.But when I upgrade to v5.8-2.0.3.0 LTS driver, I’m unable to do that, same command will return error:I have compared the private flags in different driver version:v4.5:v5.8:The sniffer is disapperaed in v5.8 driver. I wonder why and how to solve this problem?Note: I want to set sniffer on due to I want to use tcpdump to capture offload network traffic. And the doc says I need to use ethtool --set-priv-flags xxxxx sniffer on for both v5.8 & v4.5 driver.Sorry for my careless, settting sniffer on is not needed on new driver.I have captured the packets successfully using RDMA device.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
845,cumulus-vx-and-linux-headers-sources,"Good day everyone,I’m trying to get acquainted with Cumulus Linux VX 4.4 (use QEMU and Hyper-V for that) and I have a question about kernel modules development: is it possible to install a proper linux-headers package there?I usually install it with
apt install linux-headers-$(uname-r)but it doesn’t seem to work on this environment:
uname -r returns 4.19.0-cl-1-amd64
and aptitude search linux-headers doesn’t show this particular version (but I can install 4.19.0-20-amd64, for example).Considering I need to populate this path:/lib/modules/4.19.0-cl-1-amd64/sourcethere is also another way, it could be done with make headers_install, but this requires the kernel sources which don’t seem to be available.Could you advise me if the goal can be achieved in this environment/OS?Thank youHi,Could you please paste here the content of your /etc/apt/sources.list file.The linux-headers-$(uname -r) package should be available for download as well as the linux-source package.Also please paste the content of /etc/image-release and /etc/lsb-release.And make sure to run apt update before trying to install the linux-headers package.Powered by Discourse, best viewed with JavaScript enabled"
846,in-nccl-how-to-understand-the-relationship-between-message-channel-chunk-and-qp,"For example, when calling ncclAllReduce in the framework layer, the data passed in is a message, while ncclIbIsend is called in the underlying layer to send the data, which is associated with QP. The intermediate layer also involves the concepts of Channel and Chunk. So what is the relationship between message, channel, chunk, and QP? How do they affect communication performance respectively?if anything wrong, please correct me, thanksIt is internal design concept, we will not explain.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
847,hpe-sn2010m-cumulus-version,"HiI was wondering why the HPE SN2010M does not yet have Cumulus v5 support?Where can I read about differences between the Mellanox SN2010 and the HPE SN2010M?Hi,They are exactly the same models, except it is from HPE. So it does have support for v5 as well.Powered by Discourse, best viewed with JavaScript enabled"
848,neo-management-software,"Hi,
Can someone tell me from where can I download NVIDIA/Mellanox NEO software?Have you got it? Please also share it with me if you find it.I am interested as well. However for last 5 days nvidia support can’t help. I would appreciate any advice.Powered by Discourse, best viewed with JavaScript enabled"
849,the-performance-of-event-apis-could-be-bounded-by-softirqs,"I’m trying to use RDMA event mode for handling many connections. I found that the performance degraded when there were many send requests under event mode. This issue can be reproduced by using perf-test.What I observed is that only 10 clients can easily make ksoftirqd thread busy and there is only 1 CPU core handling interrupts, which could be the performance bottleneck.This issue can be reproduced with GitHub - linux-rdma/perftest: Infiniband Verbs Performance TestsN_ITER=100000000
CLIENT_IP=10.3.1.1
pkill ib_send_latTo launch server:for i in $(seq 1 10); do
port=$((12345+$i))
./ib_send_lat -e -n $N_ITER -p $port &
doneTo launch client:for i in $(seq 1 10); do
port=$((12345+$i))
./ib_send_lat $CLIENT_IP -e -n $N_ITER -p $port &
doneAnd then, use htop to monitor server-side:
Screen Shot 2022-05-19 at 20.29.233430×926 364 KB
Linux Distribution:LSB Version:    :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch
Distributor ID: CentOS
Description:    CentOS Linux release 7.8.2003 (Core)
Release:        7.8.2003
Codename:       Core
Linux Kernel and Version:
Linux gpu01.cluster 3.10.0-1127.19.1.el7.x86_64 #1 SMP Tue Aug 25 17:23:54 UTC 2020 x86_64 x86_64 x86_64 GNU/LinuxInfiniBand hardware and firmware version:driver: mlx5_core[ib_ipoib]
version: 5.0-2.1.8
firmware-version: 16.21.2010 (MT_0000000010)
expansion-rom-version:
bus-info: 0000:02:00.0
supports-statistics: yes
supports-test: yes
supports-eeprom-access: no
supports-register-dump: no
supports-priv-flags: yesNIC: Mellanox Technologies MT27800 Family [ConnectX-5]Hello and thank you for contacting us.Looking at the information you shared it looks like a deeper debug than what we can provide in the community is needed here.
I would advice on opening a case VIA the support portal so our support engineers can look at this issue.
Support Email:Networking-support@nvidia.comThanks.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
850,is5030-humble-request,"Hello community!I am a programmer who is looking to understand and play with Infiniband technology. I bought an IS5030 (I know, it’s ancient by today’s standards) to use in my home lab. No management stack license or a license to make the switch run at it’s rated capacity on the pull out tab.I’ve contacted Mellanox and as expected they wouldn’t give out any licenses for such old equipment. I would be grateful for all eternity if someone could fulfill my request.Does anyone have a copy of a license for the IS5030 switch they are willing to share?Thank you in advance.Hello Antreas,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, for the switch to function, you do not need any licenses installed on the switch. The license you are referring to as for the FabricIT add-on only, See the following link for more information → https://www.mellanox.com/related-docs/prod_ib_switch_systems/MIS5030_Switch_User_Manual.pdfThis add-on is obsolete and not maintained as well due to EOL/EOS.For proper switch functionality no license is needed.Thank you and regards,~NVIDIA Networking Technical SupportHi Martijn, thank you for the prompt reply.Going on the web management interface for it shows that the bottom 18 ports are locked, and pointing at them with my mouse yields a small dialog box that says “Not licensed”. Furthermore, it won’t let me run the lanes at 40Gbit/s. It seems to be locked at 20Gbit/s. Trying to set each port’s settings through the interface to make it run at 40Gbit/s doesn’t seem to have any effect.My IB HCA’s are MHQH29B-XSR, rated for 40Gbit/s. They are VPI, dual port cards.Running on Finisar Quadwire AOC’s rated for 40Gbit/s. On the mgmt interface, the switch lists QDR as a mode of operation for that cable.I’m new to this technology - am I missing something?It would certainly be nice to get the switch to run at it’s rated speed.I should have been more specific in my original question, sorry for that.Hi Atreas,As just confirmed based on your message, that your IS50xx is a DDR switch, so max. throughput is 20Gb/s.As the switch is EOL and EOS for a long time, we are also not able to create any new licenses for it anymore. If possible we would recommend to get a SX60xx switch (SX6036 for example). This is FDR but still you will be able to connect your HCA’s to it. And you do not need any licenses for these switches for IB funcionality.Your IB HCA’s are indeed QDR (40Gb/s throughput) so the link up to 20Gb/s is correct.Thank you, stay safe and healthy…Happy New Year.~MartijnPowered by Discourse, best viewed with JavaScript enabled"
851,how-to-offload-htb-on-bf2,"I’m doing network configuration on bf2 now. I updated my kernel version (5.15) and installed the NIC driver（mlnx_ofed） as required by the documentation. when i want to use the tc qdisc add dev XX root htb offload command to offload to bf2(this parameter is mentioned in the man htb), but the actual application of the tc command does not support the offload parameter(RTNETLINK answers: Operation not supported).Powered by Discourse, best viewed with JavaScript enabled"
852,doca-support-for-p4-release,"Any update on release date on DOCA support for P4 compiler and P4 tools ?Powered by Discourse, best viewed with JavaScript enabled"
853,mlxfwmanager-update-description,"Himlxfwmanager --queryReturns a description, something like ’ Description: Spectrum™ based 10GbE/100GbE 1U Open Ethernet switch with MLNX-OS; 18 SFP28 ports; 4 QSFP28 ports; 2 power supplies (AC); x86 dual core; Short depth; P2C airflow; Rail Kit; RoHS6’If you swap the fan direction and change the OS, can you update this somehow?Hi,Changing the fan direction is not supported in the field. If you require changing the fan direction you’ll need to speak with whoever sold you the switch to exchange it.For the description change part of your question, this may require a more in-depth investigation and is better suited for a support case.Hi. It looks like a simple text field. Can it be updated?I already swapped the fan direction and changed the OS. Not a big deal, just cosmetic, but would be nice.Hi,I looked into this and didn’t see an easy way to change it. This will require a more in-depth investigation and is better suited for a support case.Starting to feel like the solution to all things is not create a support case.Hi - just an FYI. changing the text description of the Switch FW ini file will not change any thing related to FAN direction.
the fan direction is determined by the PSU and FAn units - it has different PNs for read and forward air flowHi, thanks - yeah, I gave up on this idea. Seems there are ways to change the PSID of the device, but this will probably not give me the description text I have in mind after my changes to the device.Powered by Discourse, best viewed with JavaScript enabled"
854,set-dv-flow-en-2-but-cannot-create-hws-action-since-hws-is-not-supported,"Hi.
I’m trying to use rte_flow rules with asynchronous API in DPDK 22.11.
When I configure rte flow set using rte_flow_configure(), it returns like below and fail to configure.mlx5_net: [mlx5dr_action_create_generic]: Cannot create HWS action since HWS is not supporte
dI found that dv_flow_en is set as 2, but when querying HCA capability, mlx5dr_cmd_query_caps() returns caps->wqe_based_update of 0.I guess there is something more that I should setup with firmware.
Is there anyone has tried asynchronous rte_flow APIs?
How can I use rte_flow_configure()?ThanksAre you familiar with the following links:
This is basis though might provide you some answers to your inquiries.
You might want to check that the HCA is supported and flashed with our latest FW respectively.https://doc.dpdk.org/guides/nics/mlx5.html
Section HW steering (NIC ConnectX-5 and before are not supported) + other validations worst checkinghttps://doc.dpdk.org/guides-22.11/howto/rte_flow.html#
https://doc.dpdk.org/guides-22.11/prog_guide/rte_flow.html#generic-flow-apiAfter installing latest FW, it worked. Thank you.Powered by Discourse, best viewed with JavaScript enabled"
855,sn2010-firmware-upgrade,"Hello!I need upgrade firmware SN2010 from 3.9.0300 to 3.9.2400 version.
sn20101052×550 69.9 KB
I can’t get any files from link:https://support.mellanox.com/s/downloads-center?fid=a2y50000000Gps7&cid=a9U1T000000oNBDUA2&pid=a2v50000000XcP9AAK
3.9.24001061×586 52.5 KB
Tell me please, where can I find a new image file: image-X86_64-3.9.2400?I would strongly recommend using 3.9.3202 instead of 3.9.2400.You need to choose the architecture (X86_64).If still an issue after that - please open a case with networking-support@nvidia.com so they can help with the download of the file.I opened the case but I didn’t get an answer.Сan you check probably on your site problems with the selection button Architecture?It does not work in any browser.Technical Support Nvidia replied to my case 00956316 that download permission only for contract users.I logged in with a corporate account and downloaded the latest image.
3.9.32021006×749 66.1 KB
Powered by Discourse, best viewed with JavaScript enabled"
856,cx5-25g-card-total-bandwidth-problem,"Hi, I’ve a CX5-512A-ACAT card.
I notice this card share 25G bandwidth to dual port. This looks like a hardware limitation.
I did the following test.Server A port 1 <-25G-> server B port 1
Server A port 2 <-25G-> server C port 1Server A open two iperf server.
Server B open iperf client and connect to server A
Server C open iperf client and connect to Server AIn Server A console, showing 12 - 13G throughput in each iperf server console.Powered by Discourse, best viewed with JavaScript enabled"
857,configuring-connectx-5-pfc-without-using-vlan,"Hello, I am trying to enable PFC between two ConnectX-5 adapters without using VLAN.I followed the [MLNX_OFED document].(https://docs.nvidia.com/networking/display/MLNXOFEDv23040533/Flow+Control#FlowControl-PFCConfigurationonHosts)When I check prior[n]_rx_frames with ethtool -S <ehtX> only prio0 frames are transmitted. I set ToS using iperf -S 0x8 option.It works well with VLAN configured, as written in the same documentation. Here mentions "" 1. If the underlying device is not a VLAN device, the mapping is done in the driver"". It seems not to working properly. Is there any more setting I should have done to map Socket Priority to UP?This is my hostnamectl result.This is my ofed_info result.Hello @jounghoolee,Thank you for posting your query on our community. Please verify that PFC has been enabled on the correct priority using this command → # mlnx_qos -i If not, use the below command to enable priority.
For example, to enable priority 3 → # mlnx_qos -i  -f 0,0,0,1,0,0,0,0More information on this command can be found here → https://nvid.nvidia.com/espContent/index.html?type=article&id=mlnx-qosIf the issue still persists, I would like to request you to submit a support ticket and provide a snapshot for further troubleshooting.The support ticket can be opened by emailing "" Networking-support@nvidia.com "". Please note that an active support contract would be required for the same. For contracts information, please feel free to reach out to our contracts team at "" Networking-Contracts@nvidia.com ""Thanks,
BhargaviApologies, the commands have been cropped in my previous update. The commands would be -# mlnx_qos -i interface_name
# mlnx_qos -i interface_name -f 0,0,0,1,0,0,0,0Thanks,
BhargaviThank you Sribhargavid,I have followed all the instructions carefully, including mlnx_qos -i *interface_name* -f 0,0,0,1,0,0,0,0. It still did not work without VLAN.However, I found a workaround by configuring VLAN with ID 0.
sudo ip link add link <ethX> name <ethX>.0 type vlan id 0Anyway you are saying ConnectX officially supports PFC without VLAN? (i.e., no need for such thing I have mentioned)P.S. Thank you for support ticket information. mlnx-qos URL you have posted is not accessible.Powered by Discourse, best viewed with JavaScript enabled"
858,link-flapping-after-firmware-update,"We have HPE DL380Gen10+ servers used as VMware ESXi hosts. This week I started update of one cluster. First 5 hosts without firmware updates, all went well. Then I included the latest HPE firmware packages which includes a firmware 26.34.1002 update for the MCX adapter in PCI slot, the firmware for OCP was already on latest HPE version 26.34.1002. Driver is latest from HPE web page, I also tried the latest from VMware.Two Dual Port adapters are used, only one port of each connected to a Cisco ACI switch.Mellanox MCX631102AS-ADAT Ethernet 10/25Gb 2-port SFP28 (PCI)
Mellanox MCX631432AS-ADAI Ethernet 10/25Gb 2-port SFP28 OCP3After the update link flapping started on both hosts. I contacted VMware and HPE, as well as our network team. There is no clear response, all point to firmware update and/or the adapters.  Problem is that HPE as vendor is not very helpful when it comes to issues other than completely failing hardware.What I tried:I’m out of ideas here.Sometimes I see an Status Opcode 14 in mlxlink, but not always. I first thought it would only be the port of the PCI adapter that has the issue, but after hours the flapping suddenly changes to the port of the OCP adapter. There were never both ports affected at the same time!One thing that is still a mystery to me is FEC mode. There is no way to configure it directly in ESXi. Only with mlxlink tool. I see in output that it is set to Firecode FEC, network team told me that its set on switch side to “inherit” witch is kind of auto mode. I read a while ago that it should be RS-FEC depending on SFP. But whatever I try to set with mlxlink, I don’t see an difference in mlxlink output. This can be totally unrelated but FEC mode is something that I feel nobody in normal operations really takes care of (and there is no obvious way to do in ESXi).Hello,Please share the outputs of:
/opt/mellanox/bin/mlxlink -d mt4127_pciconf0 -e -m -c
/opt/mellanox/bin/mlxlink -d mt4127_pciconf1 -e -m -cThank you,
VikiThanks for you reply. The situation is more stable since Saturday, no more flapping. I removed the interface that had issues at that time from the VMware vSwitch config and waited ~2h. Then I added it back and no more flapping occurred.But I still have no idea what triggered this and if it is really fixed. Network team did not find anything unusual.Since one year we have all kind of strange issues. At this time network team deployed Cisco ACI and we started using HPE servers with 25G Mellanox adapters.We have those flapping issues, often after reboots or other changes. I also have some servers where an interface is completely down after a reboot of server or switch. Only powering down and removing all power cables from server solves this problem (or sth like ‘mlxfwreset -d 37:00.1 -l 4 reset’, this kills the server but at least nobody has to go on-site). So we are currently not very happy with the situation but don’t know if it is more an Cisco ACI or Mellanox issue.server #1server #1This is the port configuration on Cisco side. FEC is set to cl74-fc-fec.We use “845398-B21 HPE 25Gb SFP28 SR 100m” Transceiver ( HPE 25G and 100G Transceivers), according to mlxlink info this is a Amphenol SFP.I don’t see much info about FEC mode in HPE or Amphenol specs.Hi @vikiz does my provided information help? I got some feedback from my network team that Cisco knows about similar issues with Mellanox adapters in the past.Hi @ralf.gross1Please be aware that the cable is not supported. Additionally, the CDR in the modules is currently disabled when it should be enabled. To investigate the issue further, more information and logs are needed. The best course of action is to initiate a support case for thorough debugging.Thanks,
ChenThanks for your feedback. First time I hear about CDR, is this a setting that has to be done static on adapter/module and switch port or should this be negotiated? I don’t see an option for mlxlink command. How would I enable CDR?Regarding opening a case, can I do this for an OEM adapter from HPE? I created a case at HPE weeks ago but received 0 feedback.We also received feedback from Cisco, they believe the issue is related to this:3rd Party Switches Link Is Down Due to Auto-Negotiation (nvidia.com)Hi @ralf.gross1I can’t confirm with certainty that this is the same issue, further debugging is necessary.
Please create a support case based on your entitlement by sending an email to EnterpriseSupport@nvidia.com and mention the card PSID.Thanks,
ChenCan someone please tell me where/how I can enable CDR? Our network team does not see anything regarding this on the switches and I  don’t see na option in mlxlink. I see that some interfaces have ít enabled, others not. And I find 0 information about this.Powered by Discourse, best viewed with JavaScript enabled"
859,cannot-find-doca-examples-after-using-sdkmanager-cli-to-install-doca,"my command of installing doca:  sdkmanager --cli install --logintype devzone --product DOCA --host --targetos Linux --version 1.0 --flash allI hava installed these packages into my virtual ubuntu,but I can`t find the directory “opt/mellanox/doca” .This is the  screenshot.Powered by Discourse, best viewed with JavaScript enabled"
860,determining-the-maximum-value-of-combined-channel-based-on-sriov-numvfs-in-guest-os,"Hello,I am currently working in the following environment:HardwareSoftware VersionsSmartNIC ConfigurationIt appears that the maximum value of the Combined Channel in the Guest OS is determined by the sriov_numvfs. Is there a specific formula for determining the Combined Channel in the Guest OS?Here are the test results of the Combined Channel value depending on the sriov_numvfs value:Thank you in advance for your help.Best regards,Hi Kyoon,
What do you mean the ‘combined Channel’ here?
Could you list the query command?Regards,
LeveiHi Levei,The ‘combined channel’ I mentioned refers to the combined setting of the transmit (TX) and receive (RX) queues in a network interface card (NIC). This setting is often referred to as ‘Receive Side Scaling (RSS)’ or ‘Multiqueue’ in the context of NIC configurations.The command to check the combined channel setting varies depending on the operating system and the network driver in use. On a Linux system with an Ethernet interface (for example, eth0), you can check the combined channel setting with the following command:bashCopy codeThis command will display the current and maximum settings for the TX, RX, and combined channels.Please let me know if you need further clarification.Best, KyoonThat’s funny.
The value of combined shouldn’t be changed by sriov_numvfs.
But let me check more first.Do you change the sriov_numvfs like below, then see the combined value is changed?echo 48 > /sys/class/net/eth0/device/sriov_numvfs
ethtool -l eth0echo 0 > /sys/class/net/eth0/device/sriov_numvfs
echo 32 > /sys/class/net/eth0/device/sriov_numvfs
ethtool -l eth0LeveiHi Levei,Thank you for your response. I apologize if there was any confusion, but I’m not referring to the combined channel (Multi Queue) of the PF.
I’m referring to the Multi Queue within the VM that is using the VF created through the specification of sriov_numvfs.As you mentioned, it wouldn’t make sense for the Multi Queue to change when adjusting the VF settings.
From what I understand, the total limit of Multi Queue that can be used by the VF is set by NUM_VF_MSIX.
Currently, NUM_VF_MSIX is set to its maximum value of 127.
However, as shown below, the limit of Multi Queue varies depending on the number of VFs.here was a missing detail in the environment I previously described. It’s not just an environment that uses SR-IOV, but also one that utilizes HWOL and VF-LAG. When changing the number of VFs, a reboot is required due to the bond configuration, hence not all test cases could be conducted.When setting a different number of VFs and creating a VM using the same 32 cores, the maximum Multi Queue that can be used in the VM changes depending on the number of VFs created.I hope this clarifies my question. I look forward to your further insights.Best regards,
kyoonI’m sharing the information you requested.
VF Interface Information** PF multi queue**** representation port multi queue**** Guest port multi queue**Guest lscpuHi Levei,I have been trying to understand the relationship between NUM_VF_MSIX and NUM_OF_VFS through the following document:
https://docs.nvidia.com/networking/display/ConnectX6LxFirmwarev26311014/Changes+and+New+FeaturesThe document explains the correlation between the number of MSIX per VF (NUM_VF_MSIX) and the number of VFs (NUM_OF_VFS). However, it does not provide information on how these values are set according to any specific formula. Here’s the relevant excerpt:“Note that increasing the number of MSIX per VF (NUM_VF_MSIX) affects the configured number of VFs (NUM_OF_VFS). The firmware may reduce the configured number of MSIX per VF and/or the number of VFs with respect to maximum number of MSIX vectors supported by the device (MAX_TOTAL_MSIX).”If anyone could provide insight into this, it would be greatly appreciated.Powered by Discourse, best viewed with JavaScript enabled"
861,cards-port-type-is-unintentionally-changed,"hello. I am using various mellanox connect-x6 vpi cards.
If you reboot the device that has Ethernet mode set, some devices will change to InfiniBand mode.
any resources that can help with this issue?Hello sspitare,Thank you for posting your inquiry on the NVIDIA Networking Community.For configuring the port types to Ethernet mode on a device, use the mlxconfig utility which is part of the MFT package, available at Mellanox Firmware Tools (MFT)Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
862,psio-msf0000000012-ver-16-31-1014-or-later,"Hello,In purpose to enable WinDbg over net with multiple PF support I need to update FW on my Mellanox ConnectX-5 Adapter (Part Number MCX515-CCAT) to minimal version that supports multiple PFs.Unfortunately download page only provides MT_0000000011 PSIO.Please assist,AlexPowered by Discourse, best viewed with JavaScript enabled"
863,execute-doca-url-filter,"hi i have problems running the url filter application.I also followed the advice in this post(https://forums.developer.nvidia.com/t/doca-examples-url-filter/186476/9) but to no avail.ovs configurationscalable function configurationFor this one, I think its permissions with access to the sysfs for the hugepages. Can you try to sudo this or change to root and run it (with the trusted SF config in place too).Thanks!exactly here I have not put “sudo”now I close this post because it is equal to this oneThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
864,sn2100-inaccurate-time,"I am trying to sync my server to the SN2100 with ideally the server being the grandmaster. When I sync them together, the server selects the switch as the grandmaster.The oddest part is that the time the switch provides to the server is way off from 2001. The time on the SN2100 from “show clock” is some time in 2010 (I set it that as a test).Is there a bug of glitch that is causing the PTP time to send out 2001?
image1462×581 122 KB
Hi Benjamin,What version of Onyx are you running?CharlesHey Benjamin,Pending your reply.My recommendation on this is to upgrade to the latest version of Onyx since there are a number of problems with PTP on older releases of Onyx. Without more details it is hard to say which one it is hitting.  They are documented in the release notes. (https://networkingdownloads.nvidia.com/custhelp/Non_Monetized_Products/SwitchesandGateways/MellanoxOnyx/Onyx_ETH_v3_10_4206_RN.pdf)If you are sure that your configuration is correct and still have issues after the upgrade I recommend opening a support case so that one of our support engineers can have a further look.CharlesI believe I am running version 3.9.2006 (On the web gui page, it says X86_64 3.9.2006 2020-12-02 16:59:03 x86_64 for software version).Hey Benjamin,Assuming the configuration is correct, there are many fixes for PTP in the latest release of Onyx. Have a look at the release notes.Best option here is to upgrade to latest version and see if the problem persists.CharlesPowered by Discourse, best viewed with JavaScript enabled"
865,the-mlx5-core-driver-supports-hardware-timestamps-for-receiving-and-sending-packets-based-on-the-dpdk,"Can the following requirements be met?Implements the timesync_enable, timesync_disable, timesync_read_rx_timestamp, and timesync_read_tx_timestamp functions of the eth_dev_ops mlx5_os_dev_ops structure, and supports obtaining nanosecond-level hardware timestamps of received and sent packets synchronously based on any UDP port.Hi Cloud,As this question looks like a Feature Request based on following link —> http://code.dpdk.org/dpdk/v21.02/source/drivers/net/mlx5/mlx5.c, I would like to request you to open a support ticket by emailing "" Networking-support@nvidia.com ""Thanks,Namrata.Powered by Discourse, best viewed with JavaScript enabled"
866,can-regex-accelerator-be-used-in-separated-host-mode,"I changed my BlueField-2 DPU from Embedded CPU Function Ownership Mode to Separated Host Mode. In Separated Host Mode, I cannot find /sys/class/net/p0/smart_nic/pf/regex_en, which is needed for enabling Regex Accelerator. (According to https://docs.mellanox.com/display/BlueFieldDPUOSv370/RegEx+Acceleration)In this case, does it mean that we cannot use Regex Accelerator in Separated Host Mode?Thanks in advance!Powered by Discourse, best viewed with JavaScript enabled"
867,can-edr-cable-be-used-for-sx6036,"Hi~​I am trying to connect SX6036 to SB7800 infiniband switch.I only have EDR cables. (MFA1A00-E010 / Mellanox® active fiber cable, IB EDR, up to 100Gb/s, QSFP, LSZH, 10m)SX6036 supports up to FDR. Is this cable compatible with SX6036?Thank U for your help.Yes.Starting From SwitchX FW (9.3.1200) / MLNX-OS 3.4.1008 - there is support for for this PN:MFA1A00-Exxxthis is documented in the SwitchX Fw release notes:To update firmware on a single switch system in a Linux or Windows environment380.57 KBChanges in Rev 9.3.1200• Added GA support for MFA1A00-Exxx EDR 100Gb/s AOCs over SwitchX® based systemsThank you sir😄Powered by Discourse, best viewed with JavaScript enabled"
868,debian-11-6,"I can see that there is OFED drivers available for Debian 11.3, but nothing for Debian 11.6?Debian 11.6 is 6 months old now. When can we expect to see a release?Hello ops9,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Currently there are no plans to provide support for Debian 11.6 in the upcoming future releases.Thank you and regards,
~NVIDIA Networking Technical SupportWhat will the next supported Debian release be?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
869,connectx6-dx-hardware-vdpa-offloading-through-ovs-kernel,"Hi, I’m trying to use ConnectX6-DX hardware vDPA offloading with ovs-kernel following the provided document: https://docs.nvidia.com/networking/pages/viewpage.action?pageId=39279792#OVSOffloadUsingASAP²Direct-hwvdpaIn the steps, it requires installing DPDK even though I would like to follow ovs-kernel. Is it a necessary step? If so, please explain the role of DPDK.Thanks.Hi bk-2,Thank you for posting your inquiry to the NVIDIA Developer Forums.The application in the User Guide is a part of DPDK, and the underlying mechanism to access this functionality is also part of DPDK. You will need to install DPDK in order to use vDPA.Please refer to the following RedHat and DPDK documentation for additional information.RedHat vDPA documentation:
https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework
DPDK vDPA testing and configuration:
https://doc.dpdk.org/guides/sample_app_ug/vdpa.htmlThanks, and best regards,
NVIDIA Enterprise ExperienceThank you for your detailed answer!Hi ssimcoejr,I am still wondering why the DPDK is necessary.According to Redhat reference you provide, there is also kernel-based vDPA framework that does not involve DPDK to overcome DPDK’s limitations.
image1132×1442 108 KB

image1120×628 43.3 KB
However, the ASAP H/W vDPA offloading documentation addresses not kernel-based vDPA but only DPDK-based vDPA.
Is it a future work? or Is it theoretically impossible to offload vDPA without DPDK to NIC?Please correct me if I misunderstood.Thanks.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
870,what-10gbe-link-modes-are-supported-by-the-mcx4121a-acat-connectx-4-lx-en-nic-25gbe-dual-port-sfp28,"I’ve just installed a MCX4121A-ACAT ConnectX-4 Lx EN NIC in a Linux PC (x86_64, Linux Mint 19.3 (based on Ubuntu 18.04), kernel 5.4.0-72).The NIC is connected to an external device which supports only 10Gbase-R (e.g. 10Gbase-SR, 10Gbase-CR etc. but not 10Gbase-KR). A link appears to be established, but no data appears to be transmitted or received.‘ethtool’ reports the following when using a copper cable / transceiver in the 1st SFP28 cage on the MCX4121A-ACAT :Settings for enp1s0f0:Supported ports: [ Backplane ]Supported link modes: 1000baseKX/Full10000baseKR/Full25000baseCR/Full25000baseKR/Full25000baseSR/FullSupported pause frame use: SymmetricSupports auto-negotiation: YesSupported FEC modes: None BaseRAdvertised link modes: 10000baseKR/FullAdvertised pause frame use: SymmetricAdvertised auto-negotiation: NoAdvertised FEC modes: NoneSpeed: 10000Mb/sDuplex: FullPort: Direct Attach CopperPHYAD: 0Transceiver: internalAuto-negotiation: offLink detected: yesSimilarly, when using an optical cable / transceiver in the 2nd SFP28 cage :Settings for enp1s0f1:Supported ports: [ FIBRE ]Supported link modes: 1000baseKX/Full10000baseKR/Full25000baseCR/Full25000baseKR/Full25000baseSR/FullSupported pause frame use: SymmetricSupports auto-negotiation: YesSupported FEC modes: None BaseRAdvertised link modes: 10000baseKR/FullAdvertised pause frame use: SymmetricAdvertised auto-negotiation: NoAdvertised FEC modes: NoneSpeed: 10000Mb/sDuplex: FullPort: FIBREPHYAD: 0Transceiver: internalAuto-negotiation: offLink detected: yesThus, it appears that the MCX4121A-ACAT supports base-SR, base-CR and base-KR at 25G, but only base-KR at 10G. Is this correct ?Further information which may be useful :01:00.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]Subsystem: Mellanox Technologies Stand-up ConnectX-4 Lx EN, 25GbE dual-port SFP28, PCIe3.0 x8, MCX4121A-ACATFlags: bus master, fast devsel, latency 0, IRQ 16Memory at a6000000 (64-bit, prefetchable) [size=32M]Expansion ROM at a5400000 [disabled] [size=1M]Capabilities: [60] Express Endpoint, MSI 00Capabilities: [48] Vital Product DataCapabilities: [9c] MSI-X: Enable+ Count=64 Masked-Capabilities: [c0] Vendor Specific Information: Len=18 <?>Capabilities: [40] Power Management version 3Capabilities: [100] Advanced Error ReportingCapabilities: [150] Alternative Routing-ID Interpretation (ARI)Capabilities: [180] Single Root I/O Virtualization (SR-IOV)Capabilities: [1c0] #19Capabilities: [230] Access Control ServicesKernel driver in use: mlx5_coreKernel modules: mlx5_core01:00.1 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]Subsystem: Mellanox Technologies Stand-up ConnectX-4 Lx EN, 25GbE dual-port SFP28, PCIe3.0 x8, MCX4121A-ACATFlags: bus master, fast devsel, latency 0, IRQ 17Memory at a8000000 (64-bit, prefetchable) [size=32M]Expansion ROM at a5300000 [disabled] [size=1M]Capabilities: [60] Express Endpoint, MSI 00Capabilities: [48] Vital Product DataCapabilities: [9c] MSI-X: Enable+ Count=64 Masked-Capabilities: [c0] Vendor Specific Information: Len=18 <?>Capabilities: [40] Power Management version 3Capabilities: [100] Advanced Error ReportingCapabilities: [150] Alternative Routing-ID Interpretation (ARI)Capabilities: [180] Single Root I/O Virtualization (SR-IOV)Capabilities: [230] Access Control ServicesKernel driver in use: mlx5_coreKernel modules: mlx5_coreThank you,Andrew.Thank you Aleksey for your quick response.I’ve upgraded the firmware to 14.30.1004, as per dmesg output below :[ 1.186634] mlx5_core 0000:01:00.0: firmware version: 14.30.1004[ 1.186664] mlx5_core 0000:01:00.0: 63.008 Gb/s available PCIe bandwidth (8 GT/s x8 link)[ 1.403056] mlx5_core 0000:01:00.0: E-Switch: Total vports 10, per vport: max uc(1024) max mc(16384)[ 1.405656] mlx5_core 0000:01:00.0: Port module event: module 0, Cable unplugged[ 1.417804] mlx5_core 0000:01:00.1: firmware version: 14.30.1004[ 1.417847] mlx5_core 0000:01:00.1: 63.008 Gb/s available PCIe bandwidth (8 GT/s x8 link)[ 1.643971] mlx5_core 0000:01:00.1: E-Switch: Total vports 10, per vport: max uc(1024) max mc(16384)[ 1.647506] mlx5_core 0000:01:00.1: Port module event: module 1, Cable pluggedHowever, ‘ethtool’ still indicates that only base-KR is supported for 10G. ‘ethtool’ output below is for one of the SFP28 ports with no SFP module or cable connected, so I’d expect that the supported link modes are not affected by any attached module or cable :Settings for enp1s0f0:Supported ports: [ FIBRE Backplane ]Supported link modes: 1000baseKX/Full10000baseKR/Full25000baseCR/Full25000baseKR/Full25000baseSR/FullSupported pause frame use: SymmetricSupports auto-negotiation: YesSupported FEC modes: Not reportedAdvertised link modes: 1000baseKX/Full10000baseKR/Full25000baseCR/Full25000baseKR/Full25000baseSR/FullAdvertised pause frame use: SymmetricAdvertised auto-negotiation: YesAdvertised FEC modes: Not reportedSpeed: Unknown!Duplex: Unknown! (255)Port: FIBREPHYAD: 0Transceiver: internalAuto-negotiation: onLink detected: noCan you please confirm whether or not the MCX4121A-ACAT supports 10G link modes other than 10GbaseKR (even if using Mellanox validated modules / cables) ?The modules / cables I am using are not Mellanox ones, but they are a well-known brand, and I don’t think that could be preventing the MCX4121A-ACAT from showing any other supported 10G link modes - particularly in the case shown above with no module / cable connected.Thanks,Andrew.It is always a good idea to use latest firmware. Could be that you need to use supported cables/module. Please, take a look herehttps://docs.mellanox.com/display/ConnectX4LxFirmwarev14301004/Firmware+Compatible+ProductsA further update to my previous reply : I’ve tried re-connecting and checking the link to the external device mentioned in my first post, and data now appears to be received and transmitted.‘ethtool’ still does not show any supported 10G link mode other than 10GbaseKR, which I know for certain is not supported by the external device I’m using. However, it seems that the link is now working (using the same optical module / cable as before, so the link must be up using 10GbaseSR). Presumably the firmware update has fixed something,Thanks again Aleksey for your response.Andrew.Powered by Discourse, best viewed with JavaScript enabled"
871,azure-mellanox-dpdk-19-11-11-21-11-0-22-03-0-loosing-packets-after-minutes-to-hours-of-fine-operation,"Issue:
I have a VM in microsoft azure.There we run a DPDK application which reads traffic and duplicates it to a bunch of hosts using dpdk. If the server didn’t do anything for a longer time and we start the application, it runs fine for 1 to 5 hours, it successfully receives an average of about 25’000 packets per second. and sends them out  ca. 42’000  packets per second.After that  we see a sudden increase in time used for the function call “rte_eth_rx_burst”, it increases from avg 300 ns to 150 us. At the same time we start loosing/not receiving also of packets while the interface shows no increase in imissed, ierrors or rx_nombuf.Note  that after this degradation, the  lost packet  count  should be far above 1 million. See timely graph below.
The different lines are just different streams in this product set. Occational paket drops are expected as it receives UDP from around the globe. (Y-Axis lost packets in log-scale)

packet_miss1801×521 88.8 KB
The same application having the same input sources runs fine for weeks on Amazon AWS and Alibaba Cloud services.Environment:
The environment is a 16 core server with 2 accelerated network interfaces and Ubuntu 18.04 on it.Also activating  all the logs --log-level=‘.*,8’ does not give any insights about the time it degrades.
We tested dpdk 19.11.11, 21.11.0 and 22.03.0 on it using the following parameters.On top, the test was  run with 1 to 6 rx/tx queues, but it didn’t seem to make a hughe difference.Good to know
If dpdk/mellanox,… is in this broken state, the  application can be  stopped and started again.
But it  seems to be immediatelly in the broken state again. Waiting for some minutes lets the application run fine again for 5 to 15 minutes till it breaks again. If the machine is restarted, it usually works for 1 to  6 hours again.
As if the network driver/interface is in some  weird broken state.From the description, it’s hard to know what causes the issue. To isolate the problem cause, maybe could try to :Regards,
LeveiPowered by Discourse, best viewed with JavaScript enabled"
872,lost-second-bluefield2,"Hi all,
We have two Bluefield2 cards in one server. I tried to change the IP address of tmfifo_net1to something other than 192.168.100.2.
Now when I run “mst start” the second card does not show in /dev/mst/
Looks like we lost the second card
But it shows on lspci:lspci | grep -i mellanox
27:00.0 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)
27:00.1 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)27:00.2 DMA controller: Mellanox Technologies MT42822 BlueField-2 SoC Management Interface (rev 01)Any idea on that?have you received answer? if not please open support ticket, we have dedicated engineers for HPE.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
873,ssh-hangs-at-login-to-jetson-nano,"Because I relocated my Jetson Nano to another room in the house, I needed to implement wi-fi networking.  I’m using a TP-Link TL-WN725N.  I have a half dozen other computers, most running Ubuntu Linux, plus one Mac, that should all access each other via ssh.  When the Nano had ethernet, this was not a problem, but now I the ssh login/connection process hangs either before or after entering the password from each of the other machines.  If I run the Nano from a console (normally headless), I often cannot ssh out.  On the other hand, I can ping into and out of the Nano.  I have tried reinstalling ssh, deleting the .ssh folder and setting up the keys again - all with no effect.I’m running Ubuntu-MATE 18.If I start ssh with -vvv, I get the following at the end:Is there some ssh setting that I’ve missed?Problem apparently solved.  In the Network Connections interface there was a Docker entry that I don’t use.  When I deleted that, the wi-fi SSH interface started working properly.Powered by Discourse, best viewed with JavaScript enabled"
874,disable-roce-and-handle-roce-packets-as-normal-udp-packets,"I’d like to disable RoCE (v2) on the adapter, in a way that packets are not captured by the NIC.The command “/sys/bus/pci/devices/0000:41:00.0/roce_enable” disables RoCE but packets are still captured. I can verify this using the docker image to capture packets (mentioned in a related thread).I found the following thread that proposes 2 methods :
However the first one "" sudo mlxconfig -d 41:00.0 s ROCE_CONTROL=1 ""
Gives me :                                                                                                                                                                                                                                                         Device #1:                                                                                                                                                  ----------                                                                                                                                                                                                                                                                                                              Device type:    ConnectX5                                                                                                                                   Name:           09FTMY_071C1T_Ax                                                                                                                            Description:    Mellanox ConnectX-5 Ex Dual Port 100 GbE QSFP Network Adapter                                                                               Device:         41:00.0                                                                                                                                                                                                                                                                                                 Configurations:                              Next Boot       New                                                                                            -E- The Device doesn’t support ROCE_CONTROL parameterAnd the second one use a version of devlink that has a “param” argument that mine does not support:
sudo devlink dev param set pci/0000:82:00.0 name enable_roce value false cmode driverinit                                                                                                                                                                                                            Command “param” not foundAny solution?Hello,Unfortunately, the information provided in this thread will not be sufficient to determine solution.If you have a current support entitlement contract, please open a case with our support teams for further assistance. If you do not have a current support contract, please reach out to our Contracts team at networking-contracts@nvidia.com to discuss your options.Thank you,
-Nvidia Network SupportI finally found a way to use the devlink solution !I did this on Cloudlab that has Ubuntu 18.04 by default, for who it may help :)In short, one needs an updated Kernel, and an updated devlink to be able to disable RoCEIn long, this is the solution on Ubuntu Server 18.04 :This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
875,rivermax-sdk-example-code-run-cqe-error,"Hi expertI am using rivermax sdk example codes, but get “CQE error” failedCODE: media_sender.exe(Release-CUDA)
GPU: NVIDIA RTX A4000
CUDA: 12.1
Driver: 531.14
wiindows: 10 rpo 21H2RUN CMD: .\media_sender.exe -c 1 -a 2 -s .\sdps_samples\sdp_2110-20_narrow_gap_1080p50fps.txt -g 0 -r --max_gpu_freq/logs as follows*****************/PS D:\wlx\Rivermax\tests> .\media_sender.exe -c 1 -a 2 -s .\sdps_samples\sdp_2110-20_narrow_gap_1080p50fps.txt -g 0 -r --max_gpu_freq
#############################################Rivermax SDK version: 1.30.16
Media sender version: 1.30.16
#############################################
Set env variable CUDA_DEVICE_ORDER=PCI_BUS_ID
gpu_device_id = 0
Writing log to default location: C:\Users\enlightv\AppData\Local\Temp\rivermax_0606_181623_3736.log
Created log file: C:\Users\enlightv\AppData\Local\Temp\rivermax_0606_181623_3736.log
[23-06-06 18:16:23.411369] Tid: 001016 info [InitLogger:92] Logger started
[23-06-06 18:16:23.411436] Tid: 001016 info [rmax_init:610] starting Rivermax: SDK version 1.30.16
[23-06-06 18:16:23.412164] Tid: 001016 debug [Clock:31]
[23-06-06 18:16:23.412218] Tid: 001016 debug [SysClock:41]
[23-06-06 18:16:23.412272] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_DISABLE_VIDEO_GROUPING to the value true
[23-06-06 18:16:23.412327] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_VIDEO_PACE_INTERVAL to the value 1000000
[23-06-06 18:16:23.412402] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_OUT_STREAM_SIZE_IN_PKTS to the value 32768
[23-06-06 18:16:23.412478] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_HEADER_STRIDE_SIZE to the value 64
[23-06-06 18:16:23.412542] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_DISABLE_FLOW_ID to the value false
[23-06-06 18:16:23.412615] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_SDP_PARSER_ENABLE_LOGGING to the value true
[23-06-06 18:16:23.412672] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_ENABLE_PTP_HW_RT_CLOCK to the value false
[23-06-06 18:16:23.412740] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_ENABLE_CUDA to the value true
[23-06-06 18:16:23.412796] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_ENABLE_STATISTICS to the value false
[23-06-06 18:16:23.412854] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_ENABLE_API_VERIFICATION to the value false
[23-06-06 18:16:23.412909] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_DISABLE_AUDIO_BUFFERING to the value false
[23-06-06 18:16:23.412968] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_SESSION_MAP_SIZE to the value 2000
[23-06-06 18:16:23.413027] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_SESSION_MAP_SIZE to the value 2000
[23-06-06 18:16:23.413266] Tid: 001016 debug [EventHandlerManager:125]
[23-06-06 18:16:23.413293] Tid: 001016 info [EventHandlerManager:132] will wakeup before frame begin event in 2000000 ns
[23-06-06 18:16:23.413332] Tid: 001016 debug [EventHandlerManagerHigh:259]
[23-06-06 18:16:23.413381] Tid: 001016 debug [start_thread:341] Starting internal thread
[23-06-06 18:16:23.413462] Tid: 001016 debug [rivermax_set_thread_affinity:719] successfully set thread affinity using cpu mask: 0x2, previous mask: 0xff
[23-06-06 18:16:23.413499] Tid: 001016 debug [start_thread:344] Started event handler thread
[23-06-06 18:16:23.413564] Tid: 001016 debug [init_globals:249] Time now is 1686046583413563900
[23-06-06 18:16:23.413732] Tid: 006880 info [print_thread_info:117] High priority internal thread: PID = 3736, thread ID = 6880
[23-06-06 18:16:23.414238] Tid: 001016 debug [load_provider:69] dpcp[0] = 6008006000000 ‘Mellanox ConnectX-6 Dx Adapter’
[23-06-06 18:16:23.414264] Tid: 001016 debug [load_provider:69] dpcp[1] = 6008007000000 ‘Mellanox ConnectX-6 Dx Adapter #2’
[23-06-06 18:16:23.414289] Tid: 001016 info [init:37] DPCP/DevX provider was loaded
[23-06-06 18:16:23.461965] Tid: 001016 debug [getAdapterInfo:473] Adapter 以太网 3 vlanId 0 len 6 MAC 04:3f:72:a4:99:90
[23-06-06 18:16:23.465444] Tid: 001016 debug [getAdapterInfo:511] LUID 6008006000000 0x15b3/0x101d dpcp_adapter 0x2051b5b1be0 opened true ret 0
[23-06-06 18:16:23.465551] Tid: 001016 debug [getAdapterInfo:528] IP: 192.168.5.44 VLAN_ID: 0 Serial number: MT2035X03235
[23-06-06 18:16:23.465608] Tid: 001016 debug [getAdapterInfo:530] MTU: 1500 TXlinkSpeed: 100 Gbps RXLinkSpeed:100 Gbps
[23-06-06 18:16:23.465669] Tid: 001016 info [getAdapterInfo:535] Device with IP addr: 192.168.5.44 was added to Device Collection [1]
[23-06-06 18:16:23.465724] Tid: 001016 warning [getAdapterInfo:430] Adapter 以太网 4 luidIdx 0x8007 is not Up
[23-06-06 18:16:23.468125] Tid: 001016 debug [getAdapterInfo:473] Adapter 以太网 4 vlanId 0 len 6 MAC 04:3f:72:a4:99:91
[23-06-06 18:16:23.471336] Tid: 001016 debug [getAdapterInfo:511] LUID 6008007000000 0x15b3/0x101d dpcp_adapter 0x2051b5b1cc0 opened true ret 0
[23-06-06 18:16:23.471519] Tid: 001016 debug [getAdapterInfo:528] IP: 169.254.90.23 VLAN_ID: 0 Serial number: MT2035X03235
[23-06-06 18:16:23.471575] Tid: 001016 debug [getAdapterInfo:530] MTU: 1500 TXlinkSpeed: 18446744073 Gbps RXLinkSpeed:18446744073 Gbps
[23-06-06 18:16:23.471633] Tid: 001016 info [getAdapterInfo:535] Device with IP addr: 169.254.90.23 was added to Device Collection [2]
[23-06-06 18:16:23.474342] Tid: 001016 debug [getAdapterInfo:473] Adapter 以太网 2 vlanId 0 len 6 MAC d4:5d:64:d2:c1:49
[23-06-06 18:16:23.474400] Tid: 001016 debug [getAdapterInfo:515] DPCP device with LUID 6008005000000 not found!
[23-06-06 18:16:23.474454] Tid: 001016 info [~winDevice:97] ~winDevice DTOR
[23-06-06 18:16:23.477058] Tid: 001016 debug [getAdapterInfo:473] Adapter Loopback Pseudo-Interface 1 vlanId 0 len 0 MAC 00:00:00:00:00:00
[23-06-06 18:16:23.481534] Tid: 001016 debug [GetPhysicalAdapterByMAC:358] No physical device found, bypassing
[23-06-06 18:16:23.481591] Tid: 001016 debug [getAdapterInfo:486] Physical adapter GUID wasn’t found, bypassing
[23-06-06 18:16:23.481648] Tid: 001016 info [~winDevice:97] ~winDevice DTOR
[23-06-06 18:16:23.505503] Tid: 001016 info [license_validate_v4:446] Licensed to: Beijing Enlightv Co., Ltd (N/A), evaluation period expires in 24 days
[23-06-06 18:16:23.505603] Tid: 001016 info [info_product:466] Rivermax license version: 4.1
[23-06-06 18:16:23.506273] Tid: 001016 info [license_validate:516] Rivermax license id 827d7712-80a4-1938-6474-902c070f7f24, revision 1
[23-06-06 18:16:23.506337] Tid: 001016 info [rmax_init:638] Statistics disabled
[23-06-06 18:16:23.506408] Tid: 001016 info [cuda_enable_etbl:362] Starting Cuda init
[23-06-06 18:16:23.506499] Tid: 001016 info [cuda_enable_etbl:396] Cuda init Done
List of supported devices:
Device with interface name: 以太网 3, IP addresses: [ 192.168.5.44 ], MAC address: 04:3f:72:a4:99:90, device_id: 4125, serial number: MT2035X03235
Device with interface name: 以太网 4, IP addresses: [ 169.254.90.23 ], MAC address: 04:3f:72:a4:99:91, device_id: 4125, serial number: MT2035X03235
[23-06-06 18:16:23.508224] Tid: 001016 debug [Clock:31]
[23-06-06 18:16:23.508254] Tid: 001016 debug [ExternalClock:66]
[23-06-06 18:16:23.508275] Tid: 001016 debug [~SysClock:46]
[23-06-06 18:16:23.508298] Tid: 001016 debug [~Clock:36]
[23-06-06 18:16:23.508321] Tid: 001016 debug [rmx_use_user_clock_v1:324] Using user time handler
TX Thread: 0 Mask: 0x4
@@@cudaAllocateMmap:0 size:25165824 align:0
CUDA memory allocation on GPU - cuMemCreate
RDMA is supported and enabled, status
CUDA memory allocation on GPU - cuMemCreate Done
GPU allocation succeeded, GPU id = 0 ,size = 25165824
Note: Allocation using huge pages size requested 1105920 is smaller then one page size: 2097152
Allocated 2097152 bytes using Large Pages
sdp for stream 0 is:
v=0
o=- 1443716955 1443716955 IN IP4 192.168.5.44
s=SMPTE ST2110-20 narrow gap 1080p50
t=0 0
m=video 2000 RTP/AVP 96
c=IN IP4 224.1.1.1/64
a=source-filter: incl IN IP4 224.1.1.1 192.168.5.44
a=rtpmap:96 raw/90000
a=fmtp:96 sampling=YCbCr-4:2:2; width=1920; height=1080; exactframerate=50; depth=10; TCS=SDR; colorimetry=BT709; PM=2110GPM; SSN=ST2110-20:2017; TP=2110TPN; TSMODE=SAMP; TSDELAY=0
a=mediaclk:direct=0
a=ts-refclk:localmac=40-a3-6b-a0-2b-d2[23-06-06 18:16:23.581104] Tid: 001016 info [init_large_pages:35] huge pages are supported with page size 2097152
[23-06-06 18:16:23.581143] Tid: 001016 debug [hugePageAlloc:67] allocted 2097152 memory at 0x20532a00000 factor 1 allocSize 2097152
[23-06-06 18:16:23.581172] Tid: 001016 debug [rivermax_get_user_env:151] parsed env RIVERMAX_ENABLE_MP_WQE to the value false
[23-06-06 18:16:23.581196] Tid: 001016 debug [SessionTX:86] MP_WQE disabled for session
[23-06-06 18:16:23.581230] Tid: 001016 info [sdp_parse:594] trying to parse using smpte2110…
[23-06-06 18:16:23.581280] Tid: 001016 info [sdp_parse:610] sdp parsed successfully
[23-06-06 18:16:23.581305] Tid: 001016 info [license_assert_device:719] Validating Rivermax license for device with local ip 192.168.5.44
[23-06-06 18:16:23.581328] Tid: 001016 info [is_sn_matched:144] No serial number restriction
[23-06-06 18:16:23.581349] Tid: 001016 debug [session_tx_initialization:1001] got 4 blocks, 16 stride in chunk, 4320 packets per frame, network_len 46
[23-06-06 18:16:23.581373] Tid: 001016 debug [session_tx_initialization:1024] processing block 0 with 4320 packets
[23-06-06 18:16:23.581393] Tid: 001016 debug [session_tx_initialization:1026] processing application header
[23-06-06 18:16:23.581420] Tid: 001016 debug [session_tx_initialization:1024] processing block 1 with 4320 packets
[23-06-06 18:16:23.581440] Tid: 001016 debug [session_tx_initialization:1026] processing application header
[23-06-06 18:16:23.581467] Tid: 001016 debug [session_tx_initialization:1024] processing block 2 with 4320 packets
[23-06-06 18:16:23.581486] Tid: 001016 debug [session_tx_initialization:1026] processing application header
[23-06-06 18:16:23.581514] Tid: 001016 debug [session_tx_initialization:1024] processing block 3 with 4320 packets
[23-06-06 18:16:23.581535] Tid: 001016 debug [session_tx_initialization:1026] processing application header
[23-06-06 18:16:23.581629] Tid: 001016 debug [session_tx_initialization:1088] fix intv is every 50 frames
[23-06-06 18:16:23.581653] Tid: 001016 info [session_tx_initialization:1166] Detected ST2110-20 video stream
[23-06-06 18:16:23.581678] Tid: 001016 debug [init:85] MP_WQE disabled for ring
[23-06-06 18:16:23.581699] Tid: 001016 info [init:20] do open: true
[23-06-06 18:16:23.581950] Tid: 001016 debug [init:344] cpu_vec 0x0 eqn 7
[23-06-06 18:16:23.581981] Tid: 001016 debug [init:354] Reserved MKey created lkey=0x700 addr=0x2051b5c9e28
[23-06-06 18:16:23.582002] Tid: 001016 debug [init:362] Adapter frequency (khz) 1000000
[23-06-06 18:16:23.582021] Tid: 001016 debug [init:366] DPP supported is enabled
[23-06-06 18:16:23.582040] Tid: 001016 debug [calculate:259] rate 2271600 pps 216000, DI 0.000000, burst size 1262 inter burst gap 4444.44 accurate ibg 4444.44 active_time 0.96 , inter packet_gap 4444.44
[23-06-06 18:16:23.582100] Tid: 001016 debug [create_comp_channel:163] created completion channel: 0x205280e6150 with handle 0x32c
[23-06-06 18:16:23.583416] Tid: 001016 debug [create_cq:207] created CQ sz 32768 cqn 0x434
[23-06-06 18:16:23.583438] Tid: 001016 debug [get_dv_cq:305] CQ id 0x434
[23-06-06 18:16:23.622416] Tid: 001016 debug [create_pp_sq:1056] created packet pacing SQ 0x20519802d50 state SQ_RDY status 0 wqe 0x20532e16000 stride num/sz 32768/64 sq 4350
[23-06-06 18:16:23.622522] Tid: 001016 debug [create_cq_sq:316] got prm cq buf 0x20532c06000 sq buf 0x20532e16000
[23-06-06 18:16:23.623257] Tid: 001016 debug [SenderSG:53] SQ num 0x10fe buf 0x20532e16000 stride 64 cnt 32768 dummyInt 0 extra_dummy 0
[23-06-06 18:16:23.623337] Tid: 001016 debug [Mlx5Poll:34] cq num 0x434 cqe size 64 cq size 32768 cqn 1076 dbrec 0x20527f8ff40
[23-06-06 18:16:23.623914] Tid: 001016 debug [bind:180] called bind to ip 192.168.5.44 port 52894
[23-06-06 18:16:23.624036] Tid: 001016 debug [fill_net_header:103] Final DstMAC=01:00:5e:01:01:01 vlan_id=0
[23-06-06 18:16:23.624096] Tid: 001016 debug [fill_net_header:120] DSCP=0
[23-06-06 18:16:23.624157] Tid: 001016 debug [fill_net_header:122] ECN=0
[23-06-06 18:16:23.624213] Tid: 001016 debug [fill_headers:188] Resolved Src: 192.168.5.44 to SrcMAC=04:3f:72:a4:99:90 Dst: 224.1.1.1 to DstMAC=01:00:5e:01:01:01 VLANId=0
[23-06-06 18:16:23.624294] Tid: 001016 debug [prepare_headers:119] network len 42 max_usr_hdr 64 stride length is 128
[23-06-06 18:16:23.624370] Tid: 001016 debug [hugePageAlloc:67] allocted 4194304 memory at 0x20535000000 factor 2 allocSize 4194304
[23-06-06 18:16:23.624917] Tid: 001016 debug [create_direct_mkey:487] map sz = 2 lkey 0x4948
[23-06-06 18:16:23.624975] Tid: 001016 debug [prepare_headers:140] done preparing raw network header in address 0x20535000000 with size 2211840 lkey 0x4948 total header allocated 17280
[23-06-06 18:16:23.625052] Tid: 001016 debug [session_tx_initialization:1415] calculated 180 DI in gap, one extra dummy every 1.7053e-13 frames
[23-06-06 18:16:23.625124] Tid: 001016 debug [ChunkMgr:44] creating chunkmgr mem_block_array_len: 4 m_chunk_size_in_stride: 16 data_stride_size: 1280, app header 64
[23-06-06 18:16:23.625479] Tid: 001016 debug [create_direct_mkey:487] map sz = 3 lkey 0x4a49
[23-06-06 18:16:23.626055] Tid: 001016 debug [create_direct_mkey:487] map sz = 4 lkey 0x4b4a
[23-06-06 18:16:23.626749] Tid: 001016 debug [create_direct_mkey:487] map sz = 5 lkey 0x4c4b
[23-06-06 18:16:23.627514] Tid: 001016 debug [create_direct_mkey:487] map sz = 6 lkey 0x4d4c
[23-06-06 18:16:23.627799] Tid: 001016 info [disable_mp_wqe:104] MP WQE disabled for ring 0x2052836b2b0
[23-06-06 18:16:23.627864] Tid: 001016 debug [ChunkMgr:254] MP_WQE disabled for ring 0
[23-06-06 18:16:23.627892] Tid: 001016 debug [add_tx_session_to_map:36] created new TX session with id 0
[23-06-06 18:16:23.627967] Tid: 001016 debug [add_tx_session_to_map:40] adding session 0 to map with period 2e+07
Stream ID: 0
Source: 192.168.5.44:52894
Destination: 224.1.1.1:2000
Successfully set thread affinity using cpu mask: 0x4, previous mask: 0xff
running 1 streams, each mem_block using 270 chunks, each frame has 270 chunks, each chunk has 16 strides, sending 4320 packets per frame, 50 frames per second, frame/field duration: 20000 [us]
running scenario with: chunks in frame: 270 chunks in mem_block: 270 strides in chunk : 16 first commit in ms: 1010.941440
[23-06-06 18:16:24.653137] Tid: 009628 error [poll:55] idx 1 wqe id 0 CQE error, vendor syndrome=0x51, HW syndrome=0x2, HW syndrome type=0x0 syndrome=0x4
[23-06-06 18:16:24.653264] Tid: 009628 error [poll:57] send_code 0xfe wqe_cnt 0 user_idx 0x434Save Edit
CloseDear @wanglxThanks for your post.It looks like a GPU problem. Unfortunately I can’t provide you more details right now, but I’ll do it later once I have updates.As an alternative option you can open the case in Enterprise Support and we’ll do our best to track and debug the issue.Regards,
VladislavPowered by Discourse, best viewed with JavaScript enabled"
876,doca-sdk-installation,"Hi,I have a machine connected to a Bluefield-2 DPU. However, when I try to setup the SDK using the SDK manager, the SDK doesn’t detect the hardware and does not offer me the chance to install DOCA.
I’m wondering if this is because the SDK cannot find the NIC (this would be surprising because the rshim is up and running), or if its because my developer account does not have access to DOCA.Can you tell me which is true? If its the latter, how do I get access? The DOCA early access page is just a circular loop and leads nowhere.Powered by Discourse, best viewed with JavaScript enabled"
877,wrong-credentials-cannot-access-bluefield-via-rshim,"Hi, I have been using Bluefield for a while, but at some point, I cannot access it anymore through
ssh ubuntu@192.168.100.2I got a permission denied error. I checked all my other interfaces and IP addresses and routing tables; there is no chance that I accidentally try to connect to some other machine on the LAN - I see the explicit entry in the output of route -n for interface tmfifo_net0.I decided to reinstall ubuntu on the SmartNIC by explicitly setting a new password to be used. I followed this documentation: https://docs.mellanox.com/display/BlueFieldSWv36011699/Upgrading+NVIDIA+BlueField+DPU+Software#heading-SoftwarePrerequisites, more precisely the section Image installation.Still, I cannot log in; I have the same permission denied error.I also tried reinstalling the OS  without the --config parameter as the documentation said; if it is missed, the system will ask me to change the password after the first login.
I ended up in the same situation.As a last resort, I tried communicating to the Bluefield via the rshim0 console.
screen /dev/rshim0/console.
Credential is not working either: Login incorrect.Can someone help me with this?I believe this requirement to set a password at BFB install with --config changed in 5.3/3.6. It is now compulsory.NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.-jbOkay, I got this working. It turned out that the system I was using (@Cloudlab) was also configured ifb interfaces for QoS purposes. I thought they might redirect my connection request to somewhere else, so I removed all of them by removing the kernel module itself:
rmmod ifbAfter this, I was able to access Bluefield again, and as promised, it was asking for password change.
So, remove ifb devices if you have any, and I can confirm that --config is NOT compulsory; it works as intended.I have written a post about the whole reinstallation process. If someone wants to do the same or has the same issue as I did, feel free to give it a read!Install the Latest Bluefield OS with DPDK and DOCA. In this episode, we will install the latest Bluefield OS on the Bluefield-2 DPU
Reading time: 7 min read
Powered by Discourse, best viewed with JavaScript enabled"
878,p4-programming,"Hi all,
I’m new to Doca SDK, I have followed the instructions and looks like everything is working and I tried the sample applications, and it is working.
My question is: Is there any P4 programmability support for Bluefield 2? and is there any sample codes that we could try out with Doca SDK?
ThanksCurrently, there isn’t any official support for p4 with Bluefield-2 and DOCA. We’re currently targeting May 2022 for the first phase of p4 support including a p4 compiler and some associated development tools.This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
879,how-to-adjust-the-cq-parameters-to-reduce-network-latency-with-mlxndperf-exe,"Hello.
I’m working on Windows RDMA performance with MlxNdPerf.exe.
The purpose of this measurement was to reduce the network Latency ,
and I tried modify the parameters of CQ Parameter(ex.CqMod Count/CqMod Int),
but Latency did not improve.
[MlxNdPerf:Estat Average Latency: 548664 usec]I have attached the output. I would like advice on how to adjust the CQ parameters.
thanks.
powershell841×659 12.7 KB
Powered by Discourse, best viewed with JavaScript enabled"
880,persistent-established-state-connections-in-conntrack-table-in-hwol-environment-post-load-testing,"Hello,I’ve been conducting load tests in a Hardware Offload (HWOL) Conntrack environment and have encountered an issue where, post-testing, all connections on the VM have terminated but the conntrack table still retains flows in the ESTABLISHED state.Here’s a snippet of the conntrack table:rubyCopy codeThe current netfilter conntrack table size is set to 1048576 and during the load test, the conntrack table size did not exceed 800K.Here are the details of my setup:HardwareSoftware VersionsAny insights or suggestions on how to resolve this issue would be greatly appreciated.Thank you.Hi @kyoon,There was a kernel bug that the connections are not aged. Fix was done in v5.18.Regards,
ChenHi @Chen,Thank you for your response and for shedding light on the issue. I appreciate your help.I’ve looked into the patch history for kernel 5.18 regarding conntrack, but I couldn’t find the specific bugfix commit you mentioned. Could you possibly share the commit details?I am currently using Ubuntu 22.04 (Jammy) and I’m planning to inquire about whether this fix is scheduled to be applied to this distribution. Having this information would be extremely helpful.Thanks again for your assistance.Best regards,
kyoonPowered by Discourse, best viewed with JavaScript enabled"
881,number-of-hairpin-queues,"Hi,
I am using bluefield-2 and I’d like to use hairpin queues between 3 representors, smtng like that:en3f0pf0sf3
|
en3f0pf0sf2  - en3f1pf1sf4Depends on some rule I’d like to offload packets from sf2->sf4 or from sf3->sf2.I found that rte_eth_dev_hairpin_capability_get contains hard coded valuesIs it possible to use 2 or more hairpin queues?
Or otherwise capability to offload traffic between 3 representors with arm cores in one direction and without arm cores in another?Thanks in advancePowered by Discourse, best viewed with JavaScript enabled"
882,must-mellanox-infiniband-nics-work-with-mellanox-infiniband-switch,"Hi, I am new to RDMA. My lab would like to deploy an RDMA system for experiments. I am wondering, to work with InfiniBand, should I buy an InfiniBand switch in addition to some InfiniBand NICs? Or if I can connect these InfiniBand NICs via some other ordinary switches? Will this deployment have some different effects on RDMA performance?Many thanks for the help.Hi,To work with InfiniBand, you can either:Regards,
ChenPowered by Discourse, best viewed with JavaScript enabled"
883,mellanox-connectx-2-card-windows-11-problem-hk,"Hello I bought 2 Mellanox MNPA19-XTR cards excitedly to upgrade my home network to 10Gb. My client computer is Windows 11 on the other side I will install openmediavault or truenas. Windows 11 automatically recognized the Mellanox Connectx-2 card. However, when I want to check the firmware and properties of the card with some commands, it gives the following error message. Although it shows as properly installed in Windows 11 device manager, something seems to be wrong. How can I verify that the card is working fine and how can I update the firmware and properties. Has anyone recently recognized the mellanox connectx-2 card in Windows 11 and updated the firmware?What could be the cause of the following errors?PS C:\Windows\system32> mst statusMST devices:mt26448_pci_cr0mt26448_pciconf0PS C:\Windows\system32>PS C:\Windows\system32> mlxupQuerying Mellanox devices firmware …FATAL - Can’t find device id.FATAL - Can’t find device id.FATAL - Can’t find device id.Device #1:Device Type: N/APart Number: –Description:PSID:PCI Device Name: mt26448_pci_cr0Port1 MAC: N/APort1 GUID: N/APort2 MAC: N/APort2 GUID: N/AVersions: Current AvailableFW –Status: Failed to open device-E- Failed to query mt26448_pci_cr0 device, error : MFE_UNSUPPORTED_DEVICEPS C:\Windows\system32> flint -d mt26448_pciconf0 queryFATAL - Can’t find device id.-E- Cannot open Device: mt26448_pciconf0. MFE_UNSUPPORTED_DEVICEHi,The first WinOF-2 driver version tested/supported Windows11 is v2.80.
Tested Operating System - WinOF-2 v2.80 - NVIDIA Networking DocsBut, this v2.80 version only support ConnectX-4/ConnectX-5/ConnectX-6/BlueField-2.  Your ConnectX-2 NIC is not supported in our WinOF-2 v2.80 driver, and it’s EOL already.
Supported Network Adapter Cards and MFT Tools - WinOF-2 v2.80 - NVIDIA Networking DocsWe can’t provide further support regarding how to use this ConnectX-2 NIC on Win11 and update firmware.Powered by Discourse, best viewed with JavaScript enabled"
884,connectx-5-how-to-disable-use-of-vports,"I have a MCX515A-CCA_Ax_Bx ConnectX-5 EN network interface card 100GbE single-port QSFP28
I want to avoid using vports for incoming packets e.g. so that I can see network traffic using
tcpdump -i enp25s0
I have disabled SRIOV in the BIOS and using mstconfig. Incoming packets go to rx_vport_packets and not rx_packets.
Please can you help?? Thank you!!I include some relevant data below:roddy@CTSPC4:~$ sudo mlxburn -d /dev/mst/mt4119_pciconf0 -vpd
VPD-KEYWORD    DESCRIPTION             VALUERead Only Section:
PN             Part Number             MCX515A-CCAT
EC             Revision                AB
V2             N/A                     MCX515A-CCAT
SN             Serial Number           MT1952J09189
V3             N/A                     0c0619441c29ea1180001c34da76420c
VA             N/A                     MLX:MODL=CX515A:MN=MLNX:CSKU=V2:UUID=V3:PCI=V0
V0             Misc Info               PCIeGen3 x16
RV             Checksum Complement     0x19
IDTAG          Board Id                CX515A - ConnectX-5 QSFP28roddy@CTSPC4:~$ sudo flint -d /dev/mst/mt4119_pciconf0 q
Image type:            FS4
FW Version:            16.32.1010
FW Release Date:       1.12.2021
Product Version:       16.32.1010
Rom Info:              type=UEFI version=14.25.17 cpu=AMD64
type=PXE version=3.6.502 cpu=AMD64
Description:           UID                GuidsNumber
Base GUID:             1c34da030076420c        4
Base MAC:              1c34da76420c            4
Image VSD:             N/A
Device VSD:            N/A
PSID:                  MT_0000000011
Security Attributes:   N/Aroddy@CTSPC4:~$ sudo dmesg | grep -i mlx | grep 19
[    2.919328] mlx5_core 0000:19:00.0: firmware version: 16.32.1010
[    2.919361] mlx5_core 0000:19:00.0: 126.016 Gb/s available PCIe bandwidth (8 GT/s x16 link)
[    2.930016] mlx5_core 0000:19:00.0: handle_hca_cap:697:(pid 436): log_max_qp value in current profile is 18, changing it to HCA capability limit (17)
[    3.134742] mlx5_core 0000:19:00.0: Rate limit: 127 rates are supported, range: 0Mbps to 97656Mbps
[    3.134894] mlx5_core 0000:19:00.0: E-Switch: Total vports 2, per vport: max uc(128) max mc(2048)
[    3.138086] mlx5_core 0000:19:00.0: Port module event: module 0, Cable plugged
[    3.138513] mlx5_core 0000:19:00.0: mlx5_pcie_event:302:(pid 5): PCIe slot advertised sufficient power (27W).
[    3.151795] mlx5_core 0000:19:00.0: mlx5_fw_tracer_start:815:(pid 436): FWTracer: Ownership granted and active
[    3.361970] mlx5_core 0000:65:00.0: E-Switch: Total vports 2, per vport: max uc(128) max mc(2048)
[    3.384518] mlx5_core 0000:19:00.0: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)
[    3.657404] mlx5_core 0000:19:00.0: Supported tc offload range - chains: 4294967294, prios: 4294967295
[    3.917725] mlx5_core 0000:19:00.0 enp25s0: renamed from eth0
[    6.865241] mlx5_core 0000:19:00.0 enp25s0: Link down
[  705.635900] mlx5_core 0000:19:00.0 enp25s0: Link up
(There was delay getting the other end of the connection ready.)roddy@CTSPC4:~$ sudo mstconfig -d 0000:19:00.0 q | grep -i sriov
SRIOV_EN                            False(0)
SRIOV_IB_ROUTING_MODE_P1            LID(1)
roddy@CTSPC4:~$ ethtool -S enp25s0 | grep rx_packets
rx_packets: 0
rx_packets_phy: 50000
roddy@CTSPC4$ ethtool -S enp25s0 | grep -v “: 0” | grep rx | grep -v cache
rx_vport_multicast_packets: 50000
rx_vport_multicast_bytes: 145600000
rx_packets_phy: 50000
rx_bytes_phy: 145800000
rx_multicast_phy: 50000
rx_2048_to_4095_bytes_phy: 50000
rx_prio0_bytes: 145800000
rx_prio0_packets: 50000
roddy@CTSPC4$ lspci | grep Mellanox
0000:19:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]Powered by Discourse, best viewed with JavaScript enabled"
885,dns-filter-failed-on-bluefield-2,"Running dns filter application with two SF ports failed with “Cannot create HWS action since HWS is not suported”.The command to run is “/opt/mellanox/doca/applications/dns_filter/bin/doca_dns_filter -a auxiliary:mlx5_core.sf.2,dv_flow_en=2 -a auxiliary:mlx5_core.sf.3,dv_flow_en=2 – -l 60 -p 03:00.0 --rules /tmp/regex_rules.rof2.binary --type allow”Hi,You can try the below:For more information about dns filter app you can try the below link (requires a registration):
https://docs.nvidia.com/doca/sdk/dns-filter/index.htmlBest Regards,
AnatolyThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
886,sn2100-procedure-to-delete-and-recreate-mlag-group,"We have a SN2100 switches cluster (MLAG Configuration) and several 25G MLAG-groups connected to our hypervisors. However, we have changed one of them and the card we will use is a 10GB quad port card. The problem we have is that the ports on the card will not get up because the MLAG speed is at 25G and it cannot be changed as the physical ports do not have the speed at 10GB set/forced.Reading the documentation, it reports that it is necessary to set the speed to 10G prior the creation of the corresponding MLAG.I understand that it would be necessary to unassign each of the MLAG-group member ports, delete the MLAG, set the physical ports on each switch to 10GB, create again the MLAG on both switches and assign the MLAG member ports on each switch. am I right?My problem is that the last time I tried to do this, it rendered a service disruption (maybe caused because I didn’t shutdown the member ports before deleting the MLAG port) and I would like to ask you if you can help me to confirm the MLAG deletion/creation procedure without impact and safely.Can you please help us to find the best way to delete and recreate a MLAG group?Hello Alberto,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provide, we recommend to open a NVIDIA Networking Support ticket as you have a valid support contract, by sending an email to networking-support@nvidia.com.We will assist you further through the support ticket to make sure your configuration is validated and to provide you the correct procedure.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
887,what-is-the-effect-of-having-leafs-that-bond-to-2-borders-each-in-a-different-as,"We have an MSN2700 based network. One site has 2 border switches (West & East) each with a unique ASN. The leafs are bonded to the borders and are not running BGP at all.VLANs are created on the leafs are bonded to both borders and connect to storage. This means that VLAN traffic can appear on either border & therefore different ASNs.What would be the effects of packets from the storage crossing the VLAN & appearing in either or both ASN routing domains?Thank you for any help or thoughts on this configuration.Are you stretching these VLANs over the BGP sessions using VXLAN or EVPN? Generally the border leafs being in different ASNs(eBGP) isn’t an issue but I would need a lot more detail about the environment/traffic and a topology to give you a more useful answer.We are using VXLAN over BGP. There are a number of services running on the leafs with a teamed connection into each leaf. The topology is a ring of 3 data centres using an East & west border on each site. The major site has spines & many leafs. The smaller sites have bonded leafs connecting to the bordersThis should work provided that the border leafs are running MLAG and are connected via a MLAG bond to the downstream leafs. That way no matter which border leaf a packet hashes to, the anycast IP would be advertised across the VXLAN fabric. However, typically in this setup both border leafs in the MLAG pair would be in the same ASN but I have seen this work fine in different ASNs as well.Powered by Discourse, best viewed with JavaScript enabled"
888,how-does-the-symmetric-rss-hashing-for-mellanox-connectx-4-on-agx-orin-mlx5-core-work,"Hi,We are using Mellanox ConnectX-4 on AGX-Orin. In its kernel module mlx5-core, the symmetric RSS hashing is by default on:Without the symmetric RSS hashing (by modifying the kernel module source), the hash value that we calculate using the hash-key from ethtool is the same as the one from the NIC, which is expected. But, with the symmetric RSS hashing on, the NIC returns a different hash value than the one we calculate. So far we didn’t find any useful information online about how the symmetric RSS hashing is implemented by the Mellanox card, so we are not able to work out the same hash value. So, could you please tell us how does Mellanox ConnectX-4 calculate the hash value in the symmetric hashing situation? Modifying the kernel source is not an option for our customers.Thanks!
Cheershi zhudaThis is a reserverd bit, The function of this bit was also reserved.
And we can not tell you the detial HW hash mechanism.
for you requirement, i think you can set the hash func by ethtool.
https://docs.nvidia.com/networking/display/MLNXOFEDv571020/RSS+SupportPowered by Discourse, best viewed with JavaScript enabled"
889,opensm-start-failed,"# systemctl start opensm
Job for opensmd.service failed because the control process exited with error code.
See “systemctl status opensmd.service” and “journalctl -xe” for details.# journalctl -xeApr 11 14:22:43 ss16 OpenSM[4759]: /var/log/opensm.log log file opened
Apr 11 14:22:43 ss16 OpenSM[4759]: OpenSM 5.10.0.MLNX20211115.e645cc83
Apr 11 14:22:49 ss16 opensmd[4750]: Starting IB Subnet Manager…[FAILED]
Apr 11 14:22:49 ss16 systemd[1]: opensmd.service: Control process exited, code=exited status=1
Apr 11 14:22:49 ss16 systemd[1]: opensmd.service: Failed with result ‘exit-code’.
Apr 11 14:22:49 ss16 systemd[1]: Failed to start LSB: Activates/Deactivates InfiniBand Subnet Manager.
– Subject: Unit opensmd.service has failed
– Defined-By: systemd
– Support: Red Hat Customer Experience & Engagement - Red Hat Customer Portal
– Unit opensmd.service has failed.The result is failed.# cat  /var/log/opensm.log
Apr 11 11:38:10 408527 [58783740] 0x03 → OpenSM 5.10.0.MLNX20211115.e645cc83
OpenSM 5.10.0.MLNX20211115.e645cc83Apr 11 11:38:10 408553 [58783740] 0x80 → OpenSM 5.10.0.MLNX20211115.e645cc83
Apr 11 11:38:10 410916 [58783740] 0x02 → osm_vendor_init: 1000 pending umads specified
Apr 11 11:38:10 410948 [58783740] 0x02 → osm_vendor_init: 1000 pending umads specifiedNo local ports detected!# ibstat
CA ‘mlx5_0’
CA type: MT4127
Number of ports: 1
Firmware version: 26.32.1010
Hardware version: 0
Node GUID: 0xb8cef6030083c020
System image GUID: 0xb8cef6030083c020
Port 1:
State: Active
Physical state: LinkUp
Rate: 25
Base lid: 0
LMC: 0
SM lid: 0
Capability mask: 0x00010000
Port GUID: 0xbacef6fffe83c020
Link layer: Ethernet
CA ‘mlx5_1’
CA type: MT4127
Number of ports: 1
Firmware version: 26.32.1010
Hardware version: 0
Node GUID: 0xb8cef6030083c021
System image GUID: 0xb8cef6030083c020
Port 1:
State: Active
Physical state: LinkUp
Rate: 25
Base lid: 0
LMC: 0
SM lid: 0
Capability mask: 0x00010000
Port GUID: 0xbacef6fffe83c021
Link layer: EthernetLink layer: Ethernet.
I think it needs to be changed to ib mode.Powered by Discourse, best viewed with JavaScript enabled"
890,connectx-3-en-driver-for-windows-7,"I am trying to use the ConnectX-3 to a Windows 7 computer and having some issues getting any drivers to work. Coming up asThis device cannot start. (Code 10)Anyone know the latest driver that will install right on Windows 7? Even an inf over an installer would be welcome.Only one oddly even though some newer ones say they are for Windows 7 was the 4.13.3.6 driver which is probably super old.Hello me. So I figured out how to get a newer one that said Windows 7 installed. There is a KB in Windows for a patch with SHA2 that it fails to detect and is installed so followed the install commands to override it. So that’s great. Now my intel nic no longer works. If I click on the IPV4 on it and says the device is not installed. Try setting the IP info for the ConnectX-3 and it sets but then when i go back in they are set to DHCP but have a static IP.Hello John,Thank you for posting your inquiry on the NVIDIA Networking Community.The last driver version which supports ConnectX-3 and Windows Client 7 is version 5.35.You can download this version through the following direct download link → http://www.mellanox.com/downloads/WinOF/FUR/MLNX_VPI_WinOF-5_35_All_Win7_x64.exeBe aware the ConnectX-3 EN is EOL and EOS for awhile now, support is very limited to none.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
891,how-to-poweroff-an-idle-connectx-6-to-prevent-overheating,"Hi all,everything in title…I tried pci remove, rmmod mlx_core KEEP_ETH_LINK_UP_P1=0 and AUTO_POWER_SAVE_LINK_DOWN_P1=1 without successregardsHello Raphael,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately, it is not possible to shutdown our adapter when plugged into a PCI bus. That feature is not available on our adapters.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
892,im-trying-to-enable-sriov-with-the-adapter-mcx512a-acat-in-a-esxi-7-0-but-no-luck-so-far-i-have-followed-the-instructions-of-this-link-https-docs-nvidia-com-networking-pages-releaseview-action-pageid-15053024-what-am-i-missing,"The blade is a Supermicro Super Server and sriov is enabled in the BIOS.In the kernel logs I see the following:ESC[7m2022-02-18T01:53:51.528Z cpu2:1049177)WARNING: PCI: 870: 0000:17:00.0: unable to allocate 0x800000 bytes in prefetch mmio VF-BAR[0]ESC[0m2022-02-18T01:53:51.528Z cpu2:1049177)PCI: 344: 0000:16:00.0 P2P bridge resources:2022-02-18T01:53:51.528Z cpu2:1049177)PCI: 345: IO: 0x0 - 0x02022-02-18T01:53:51.528Z cpu2:1049177)PCI: 346: Mem: 0xc5d00000 - 0xc5efffff2022-02-18T01:53:51.528Z cpu2:1049177)PCI: 347: PrefetchMem: 0xc0000000 - 0xc3ffffff2022-02-18T01:53:51.528Z cpu2:1049177)PCI: 955: Failed to allocate and program VF BARs on 0000:17:00.0ESC[7m2022-02-18T01:53:51.528Z cpu2:1049177)WARNING: PCI: 1558: Enable VFs failed for device @0000:17:00.0, Please make sure the system has proper BIOS installed and enabled for SRIOV.ESC[0m2022-02-18T01:53:51.528Z cpu2:1049177)<NMLX_ERR> nmlx5_core: 0000:17:00.0: nmlx5_CoreQueryMaxVfs - (nmlx5_core_main.c:719) vmk_PCIEnableVFs failed - Failure2022-02-18T01:53:51.528Z cpu2:1049177)<NMLX_ERR> nmlx5_core: 0000:17:00.0: nmlx5_core_EnableVFs - (nmlx5_core_main.c:848) done, status: Failure2022-02-18T01:53:51.528Z cpu2:1049177)<NMLX_INF> nmlx5_core: nmlx5_en_Attach - (nmlx5_core_en_main.c:4976) called2022-02-18T01:53:51.528Z cpu2:1049177)<NMLX_INF> nmlx5_core: nmlx5_en_CheckHCACaps - (nmlx5_core_en_main.c:2169) wqeInlineMode = 22022-02-18T01:53:51.528Z cpu2:1049177)<NMLX_INF> nmlx5_core: nmlx5_en_CheckHCACaps - (nmlx5_core_en_main.c:2177) etsSupported = 1 pfcSupported = 12022-02-18T01:53:51.528Z cpu2:1049177)<NMLX_INF> nmlx5_core: nmlx5_en_CheckHCACaps - (nmlx5_core_en_main.c:2183) HCA_CAP.dcbx = 02022-02-18T01:53:51.528Z cpu2:1049177)<NMLX_INF> nmlx5_core: nmlx5_en_CheckHCACaps - (nmlx5_core_en_main.c:2189) vxlanSupported = 1 swpSupported = 12022-02-18T01:53:51.528Z cpu2:1049177)<NMLX_INF> nmlx5_core: nmlx5_en_CheckHCACaps - (nmlx5_core_en_main.c:2195) geneveSupported = 1 geneveSwLimitOptLenEn = 02022-02-18T01:53:51.529Z cpu2:1049177)<NMLX_INF> nmlx5_core: 0000:17:00.0: nmlx5_en_Attach - (nmlx5_core_en_main.c:5059) Device 0x15fa4305a5a071b5 is not configured as ENS uplink, mode: ENS disabled2022-02-18T01:53:51.529Z cpu2:1049177)<NMLX_DBG> nmlx5_core: 0000:17:00.0: nmlx5_en_Attach - (nmlx5_core_en_main.c:5079) vmk_EnsIsDeviceConfigured query failed for device 0x15fa4305a5a071b52022-02-18T01:53:51.529Z cpu2:1049177)<NMLX_INF> nmlx5_core: 0000:17:00.0: nmlx5_en_Attach - (nmlx5_core_en_main.c:5088) Device 0x15fa4305a5a071b5 is not configured as ENS uplink2022-02-18T01:53:51.529Z cpu2:1049177)<NMLX_INF> nmlx5_core: nmlx5_en_DCBInitCurrMode - (nmlx5_core_en_dcb.c:812) No HW DCBX Capability!It worked after reducing NUM_OF_VFS from 8 to 2. VF_LOG_BAR_SIZE is already set to 0, so I don’t think I can lower it further. I suppose the limitation comes from the chipset I’m using. Is there a way to decrease the BAR size of VFs in the BIOS?Hello Marcelo,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, and several similar tickets we had in the past for this particular message, it is system platform related. We recommend to update the BIOS of your platform to the latest version available and reach out to your system vendor and inquire regarding to the limitation for SR-IOV on your platform.Be aware, that when configuring SR-IOV in ESXi, you need to configure the VF you want to use on the OS + 1 on the f/w of the adapter.So if you want to use 16 VFs in ESXi, the NUM_OF_VFS should be 17.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
893,error-using-gpudirect-and-connect-x5,"I am running libfabric 1.12.1, CUDA 11.2, and using the latest nv_peer_mem driver and MOFED 5.2-2.2.0.0. The underlying calls libfabric is using is to libibverbs.When trying to send using a buffer from GPU memory, I get the following error:mlx5: host_unknown: got completion with error:00000000 00000000 00000000 0000000000000000 00000000 00000000 0000000000000000 00000000 00000000 0000000000000000 04005104 0a0002fe 00028ad2Libfabric reports that it is a protection error, however the memory is being properly registered in nv_peer_mem and I have checked that I am using the correct memory address and local key. Is there any way to interpret this dump and get to the bottom of this error?In general, this seems to be a software issue and not the hardware and if the issue is reproduced when using Mellanox/Nvidia components requires opening a ticket with support team if there is valid support contract exists. However, there are few recommendations:-Use latest MOFED v5.3-If using AMD, validate that iommu=pt parameter used in kernel/GRUB configuration ( cat /proc/cmdline).-If using MPI, try reproduce the issue with using HPC-X. libfabric is out of support scope.-Try reproduce using perftest package -GitHub - linux-rdma/perftest: Infiniband Verbs Performance Tests - it has to be recompiled with CUDA. Check perftest documentationThank youPowered by Discourse, best viewed with JavaScript enabled"
894,connectx-4-rx-performance-issues-on-dpdk,"Are there any RX-side pps performance tips for ConnectX-4/PMD mlx5 family?Our usecase requires optimising RX pps, I don’t care about TX. Adding more receiving lcores actually decreases RX performance.After applying performance tips I am able to achieve 107M pps on TX side (no RX) using one 5-tuple or around 92M pps using 16 5-tuples for better RSS hashing.However, I am not able to exceed 60Mpps on RX side in very specific case and around 18-37Mpps in more typical cases. (Performance is heavily affected by increasing number of queues above 4).Running our DPDK application on 2x10G and 4x10G cards on different PMDs we have much more predictable performance scaling. I would rather expect that with 8 RX lcores I would be close to 100M RX pps.Test setup details:I’m not expecting 148Mpps here, but according to performance results from http://fast.dpdk.org/doc/perf/DPDK_17_11_Mellanox_NIC_performance_report.pdf http://fast.dpdk.org/doc/perf/DPDK_17_11_Mellanox_NIC_performance_report.pdf , card should be able to do >90Mpps full duplex using single port.I use two ports, one port for RX, one for TX, though.Example commands:./testpmd --file-prefix=820 --socket-mem=8192,8192 -l 12-23 -n 2 -w 0000:82:00.0,txq_inline=256 – --port-topology=chained --forward-mode=rxonly --rss-udp --rxq=2 --txq=2 --nb-cores=8 --socket-num=1 --stats-period=1 --burst=128 --rxd=2048 --txd=512./testpmd --file-prefix=820 --socket-mem=8192,8192 -l 12-23 -n 2 -w 0000:82:00.0,txq_inline=256 – --port-topology=chained --forward-mode=rxonly --rss-udp --rxq=8 --txq=8 --nb-cores=8 --socket-num=1 --stats-period=1 --burst=128 --rxd=2048 --txd=512./pktgen --file-prefix=both --socket-mem=28672,28672 -w 0000:82:00.0,txq_inline=256,txqs_min_inline=4 -w 0000:82:00.1,txq_inline=256,txqs_min_inline=4 -l 0-11,12-23 -n 4 – -P -N -T -m “[1:12-15].0, [16-23:1].1”…I will respond to myself, hope somebody will find this useful.I am facing similar issue with ConnectX-5 (dual port 100G, pcie 4)I am running pktgen in another server, connected to RX server via 100G DAC (only one 100G port is used for testing). pktgen is generating 25 Mpps.But at RX server, it is receiving at a rate of 12-14 Mpps. I tried RSS and spread it to 4 RX Queues and dedicated lcore for reading from each RX Queue. But the collective RX capability still remains 12-14 Mpps. No matter how many more Queues i inrease, total RX rate remains the same as 12-14 Mpps.Any help would be highly appreciated. RSS conf I used at receiver side is given below.rx_adv_conf = {.rss_conf = {.rss_hf = ETH_RSS_IP | ETH_RSS_UDP |ETH_RSS_TCP | ETH_RSS_SCTP,}},Powered by Discourse, best viewed with JavaScript enabled"
895,how-to-disable-smart-an-on-cx6-adapter,"How to disable Smart AN on CX6 adapterHi Ankit,To disable Auto Negotiation, you need to install the MFT package from the link below.The Mellanox Firmware Tools (MFT) package is a set of firmware management toolsNote: MFT is part of MLNX_OFED, so if you have the driver installed on the system, you should already have MFT.Then, you can use the mlxlink tool.mlxlink UM:https://docs.nvidia.com/networking/display/MFTv4180/mlxlink+UtilitySteps:To get the MST device:To query current configuration:To Set Auto Negotiation to OFF and force the speed:E.G:Then you must toggle the link so the above changes will take place:Then query again:You can also change FEC type to be the same as the switch, as FEC type should be identical on all end points:Change to RS FEC:Or - change to FC FEC:mlxlink -d  -fec FC -fec_speed Then toggle:Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
896,doca-application-meson-build-complain,"Hi, I have tried the Trouble shooting here Troubleshooting Guide :: NVIDIA DOCA SDK Documentation
to solve it. However, after adding the two export path command, this problem seems to be present still, what could be the reason. Also, I was trying to install cmake, but it failed. Here I add the specific information.
meson build problem:

IMG_62234032×3024 1.89 MB

cmake installation problem:

IMG_6227.HEIC4032×3024 2.86 MB
Please follow below install guide,NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.Thanks for the reply, however, the document you provide seems to be irrelevant for my question.
When I compile the doca application and samples they all complain about missing package.

Screenshot 2023-02-11 1606411300×760 56.9 KB


Screenshot 2023-02-11 1607021521×658 53.1 KB
Powered by Discourse, best viewed with JavaScript enabled"
897,hi-there-what-are-the-main-differences-between-connect-ib-and-connectx-3,"I’m considering using NFS over InfiniBand (and IPoIB). Since prices are more-less similar I wandering which of the two should I buy?Hello Zbuntowany,Thank you for posting your inquiry on the NVIDIA Networking Community.Below links will provide you the product capabilities for both adapters:The Connect-IB is IB only. The ConnectX-3 VPI has the option to change the port type to IB or Ethernet.Both adapters in general are EOL and majority of the ConnectX-3 models are EOS.Our latest GA drivers, will not support Connect-IB anymore.Recommendation would be to try to purchase a more recent adapter like the ConnectX-4 or 5 VPI, or even the ConnectX-6 VPI. All these VPI adapters will give you the opportunity to change the port type to IB or Ethernet so they will be more versatile to use.Thank you and regards,~NVIDIA Networking Technical SupportThanks for a quick answer,As I understand both are enough to run NFS over RDMA. So I don’t think I’ll spend money on anything better than Connect-IB. This is not for production but a kind of academic purpose.Well, if you would like to donate some for our university…Powered by Discourse, best viewed with JavaScript enabled"
898,about-window-ndi-interface-support-in-winof,"Hi, all.
I’m working on Windows RDMA programming(suppose supporting both roce and infiniband),
I see that from WinOF-2, only Microsoft’s Network Direct Interface is supported, I would like to ask
a few related questions as belowing:Hello and thank you for contacting us.Ibverbs was not supported in WinOF-2 from day 1, only Network direct.We support RDMA between Linux and windows, as far as I know it is tested in end-to-end.Microsoft wanted unified addressing for sockets, Infiniband and RoCE, hence they defined RDMA-CM (IP based) like interface.
Unlike Linux, in Windows, it is impossible to have RDMA stack without TCP/IP stack.Best Regards,Yogev PetrovNvidia Support TeamPowered by Discourse, best viewed with JavaScript enabled"
899,using-rdma-raw-ethernet-with-srq,"Dear support team,I tried to use SRQ (shared receive queue) with RDMA raw ethernet, while initial test shows failed using the linux tool, such as example in https://community.mellanox.com/s/article/raw-ethernet-lat as following, remove --use-srq works as expected.My question is does my connectX-5 card support SRQ + raw ethernet?update: I also test enable SRQ at server side, doesn’t have luck.Hi,Please note the below flag is not supported with this combination.Thanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
900,hardware-time-stamp-mismatch-connectx-5,"Hello everyone,I have created a DPDK (21.11) capture application in linux that reads the hardware timestamp of received packets but, for instance, if I do a capture of 1 minute and I calculate the difference of timestamps from the last received packet and the first packet, I get a duration around 10 seconds. I transmit traffic all time so I expected that the timestamp difference should be of 1 minute. Why could it be happening?RegardsAlbertThe problem is that the returned timestamp is in raw format.  I used the function use mlx5dv_ts_to_ns to convert the timestamp to nsPowered by Discourse, best viewed with JavaScript enabled"
901,nvme-snap-sdk,"Hell everyone,DOCA newbie here.  I came across the NVMe SNAP framework on https://www.mellanox.com/files/doc-2020/sb-mellanox-nvme-snap.pdf and https://docs.mellanox.com/display/BlueFieldDPUOSv370.  I was wondering if I could get access to the SNAP SDK documentation.  Thanks in advance!-TristanHi,I also want to have the access to SNAP. How can we apply for it?Thanks!Hi,I also want to have the access to SNAP. How can we apply for it?Thanks a lot!Powered by Discourse, best viewed with JavaScript enabled"
902,does-mellanox-technologies-mt27800-family-connectx-5-support-af-xdp-zerocopy-mode-on-linux,"Hi,I am trying to use AF_XDP on Ubuntu 20.04 with Mellanox Technologies MT27800 Family [ConnectX-5] . The problem I am facing is that when I try to bind the AF_XDP socket in zerocopy mode it fails. There is no problem connecting it in copy mode. Since the zerocopy performance is much higher I wanted to know if this feature is supported by Mellanox drivers on Linux and how I can get it to work.Thank youHi,Which kernel do you have ?Did you try a latest one ?RegardsMarcI am using a fresh installed Ubuntu 20.04. It has kernel version 5.4.0.Hi,Yes the non -zero copy mode must work as expected, but please open a support case if you have any issue,MarcYes, Non-ZeroCopy mode works fine. So I guess ZeroCopy is not supported.Thank youIf any one else wonders why AF_XDP zerocopy is not working with Mellanox (for example why xdpsock in Linux BPF sample programs is not able to connect) then please have a look at here https://www.spinics.net/lists/xdp-newbies/msg01347.html . As said in this link Connect X4 and above will support AF_XDP zerocopy mode.Thank youPowered by Discourse, best viewed with JavaScript enabled"
903,looking-for-mlx-file-for-connectx-4-lx,"Hello,
Recently we have received two network adapters to upgrade ones in our servers. Adapters are shipped from Huawei, but they are based on the Mellanox ConnectX 4 Lx. After installing one of the adapters the OS (Windows Server 2019) started to log the warnings:Mellanox ConnectX-4 Lx Ethernet Adapter #7 Firmware version 14.20.1010 is below the minimum FW version recommended for this driver.
Minimum recommended Firmware version for this driver: 14.31.1014.Also PS commandlets do not return information about available\allocated VMQs, and guest OSes inside the VM cannot setup RSS.
I have tried to update the firmware from Nvidia Download Center, both OEM and common images of different versions - no success. The result was “no cable connected” on both ports.
As the last resort, I want to build a custom firmware using the config file grabbed from the original firmware. Is it possible to get a .mlx image file of version 14.31.1014?
photo_2022-11-29_12-15-061280×960 131 KB
Powered by Discourse, best viewed with JavaScript enabled"
904,trouble-running-dpdk-testpmd-on-a-kvm-machine-using-connectx-5-interfaces,"Host configuration:OS: Debian GNU/Linux 10 (buster)Kernel: 4.19.0-17-amd64Architecture: x86_64mlnx_oefd version: MLNX_OFED_LINUX-5.4-1.0.3.0-debian10.8-x86_64Guest configuration:OS: Ubuntu 16.04.1 LTSKernel: 4.4.0-47-genericmlnx_ofed version: MLNX_OFED_LINUX-5.4-1.0.3.0-ubuntu16.04-x86_64Dpdk version: 18.11Please see the attachment for more detailed configuration.This is what I get when I try to run testpmd on the guest.EAL: PCI device 0000:00:03.0 on NUMA socket -1EAL: Invalid NUMA socket, default to 0EAL: probe driver: 15b3:101a net_mlx5net_mlx5: mlx5_flow.c:364: mlx5_flow_discover_priorities(): port 0 verbs maximum priority: 8 expected 8/16testpmd: /home/sbandi/dpdk/drivers/net/mlx5/mlx5.c:1259: mlx5_dev_spawn: Assertion `(mlx5_glue->dealloc_pd(pd)) == 0’ failed.Please let me know what is going wrong here.mlnx.txt (25.9 KB)mlx5_flow_discover_priorities is a verbs based function and Mellanox OFED v5.4 more is rdma-core based. There were changes in this portion of the codeCheck DPDK v18.11 release notes to find tested MOFED versionhttp://doc.dpdk.org/guides-18.11/rel_notes/release_18_11.htmlI would suggest to switch to latest LTS DPDK and see if the issue is still there. It is also worth trying latest DPDK version.In addition, firmware version running on the host is not supported by this version of the MOFED. Please, check MOFED release notes for supported version. As you are running OEM (DELL) card, you might contact OEM to get relevant version. If you have a Mellanox branded PSID, you should be able to get latest version from Mellanox web site.You might also try MOFED v4.9 LTS, that still using verbs. Looks like your OS’es are supported - https://docs.mellanox.com/display/MLNXOFEDv494080/General+Support+in+MLNX_OFEDThank you for the response.I did try downgrading to MOFED v4.9 LTS, but now I’m facing an issue with loading libibverbs library while running testpmd.root@cep:~# ./testpmd -w 0000:00:03.0net_mlx5: mlx5.c:1712: mlx5_glue_init(): cannot load glue library: /usr/lib/libibverbs.so.1: version `IBVERBS_1.8’ not found (required by /usr/lib/x86_64-linux-gnu/librte_pmd_mlx5_glue.so.18.11.0)net_mlx5: mlx5.c:1730: mlx5_glue_init(): cannot initialize PMD due to missing run-time dependency on rdma-core libraries (libibverbs, libmlx5)The libibverbds is part of the MOFED installation, so I’m not sure what dpdk is complaining about. Would you have any idea?Seems like MOFED installation issue. MOFED does contain libibverbs.I would highly recommend to use latest 5.4 and 20.11 DPDK version first. Install MOFED with ‘mlnxofedinstall --all’ and the compile DPDKDownload, extract and runmeson --prefix= buildUsing prefix is very recommended until you finish our development cycle.ninja -C build installIt could be that you have to add meson/ninja packages to your OSPowered by Discourse, best viewed with JavaScript enabled"
905,sniff-roce-traffic-on-connectx-6,"I’m trying to sniff RoCE traffic on a ConnectX-6 adapter.As for ConnectX-5, Offloaded Traffic Sniffer feature is removed. So I used the mellanox/tcpdump-rdma Docker container. However, even the container does not work:[root@nslrack05 /]# tcpdump -i mlx5_1tcpdump: mlx5_1: No such device exists(SIOCGIFHWADDR: No such device)What am I missing? Thanks in advance.Hello Mariano,To sniff RoCE traffic with tcpdump, please review the information (for ConnectX-4 and above) provided below:https://community.mellanox.com/s/article/how-to-dump-rdma-traffic-using-the-inbox-tcpdump-tool–connectx-4-xInformation on installing and build tcpdump and libpcap below :http://books.gigatux.nl/mirror/snortids/0596006616/snortids-CHP-2-SECT-4.htmlDownload latest libpcap 1.10 and tcpdump 4.99 from https://www.tcpdump.org/Extract, Configure, compile and install:configure --enable-rdma --prefix=make -jmake installset LD_LIBRARY_PATH to use new pcap libraryexport LD_LIBRARY_PATH=/libRun command below to check if current tcpdump reports RDMA capable interfacestcpdump --list-interfaces |grep RDMARun RoCE traffic and tcpdumptcpdump -i mlx5_0If after reviewing the documentations above and you still have issue with issue with sniff RoCE traffic issue, please open a support case and we will provide further assistance. If you do not have a current support contract, please email the Nvidia team at Networking-contracts@nvidia.com to set up a valid support contract.Thank you,Nvidia Networking SupportPowered by Discourse, best viewed with JavaScript enabled"
906,mlx4-core-missing-uar-aborting,"OS: Ubuntu 16.04.3 LTS$ lspci | grep Mellanox01:00.0 Ethernet controller: Mellanox Technologies MT27520 Family [ConnectX-3 Pro]$ mstflint -d 01:00.0 qImage type: FS2FW Version: 2.42.5000FW Release Date: 5.9.2017Product Version: 02.42.50.00Rom Info: type=PXE version=3.4.752 devid=4103Device ID: 4103PSID: MT_2340111023I have installed all drivers, ran ‘/etc/init.d/mlnx-en.d retstart’ , got :Unloading NIC driver: [ OK ]Loading NIC driver: [ OK ]BUT can’t see mellanox interface (ifconfig -a)when I type now ‘dmesg | grep mlx’ , I get:[0.991278] mlx_compat: loading out-of-tree module taints kernel.[0.991286] mlx_compat: module verification failed: signature and/or required key missing - tainting kernel[0.992456] mlx4_core: Mellanox ConnectX core driver v4.1-1.0.2 (27 Jun 2017)[0.992479] mlx4_core: Initializing 0000:01:00.0[0.992621] mlx4_core 0000:01:00.0: Missing UAR, abortingWhat is the problem here?P.S. I had no problem with this NIC on my computer, when used Ubuntu 14.04 LTSIs this issue still actual to you?I had the same issue on ConnectX-3 Pro, caused by large number of VFs I put into the HCA configuration file.Symptoms (dmesg output):Mar 31 17:36:17 macpro kernel: pci 0000:06:00.0: [15b3:1007] type 00 class 0x028000Mar 31 17:36:17 macpro kernel: pci 0000:06:00.0: reg 0x10: [mem 0xf7900000-0xf79fffff 64bit]Mar 31 17:36:17 macpro kernel: pci 0000:06:00.0: reg 0x18: [mem 0xf4000000-0xf47fffff 64bit pref]Mar 31 17:36:17 macpro kernel: pci 0000:06:00.0: reg 0x30: [mem 0xf7800000-0xf78fffff pref]Mar 31 17:36:17 macpro kernel: pci 0000:06:00.0: reg 0x134: [mem 0x00000000-0x007fffff 64bit pref]Mar 31 17:36:17 macpro kernel: pci 0000:06:00.0: VF(n) BAR2 space: [mem 0x00000000-0x03ffffff 64bit pref] (contains BAR2 for 8 VFs)Mar 31 17:36:17 macpro kernel: pci 0000:06:00.0: BAR 2: no space for [mem size 0x00800000 64bit pref]Mar 31 17:36:17 macpro kernel: pci 0000:06:00.0: BAR 2: failed to assign [mem size 0x00800000 64bit pref]…Mar 31 21:09:48 macpro kernel: mlx4_core: Mellanox ConnectX core driver v4.3-1.0.1Mar 31 21:09:48 macpro kernel: mlx4_core: Initializing 0000:06:00.0Mar 31 21:09:48 macpro kernel: mlx4_core 0000:06:00.0: enabling device (0100 → 0102)Mar 31 21:09:48 macpro kernel: mlx4_core 0000:06:00.0: Missing UAR, abortingSolution:# mstflint -d 06:00.0 q fullImage type: FS2FW Version: 2.42.8016FW Release Date: 21.3.2018MIC Version: 2.0.0Config Sectors: 2PRS Name: cx3pro_MCX354A_fdr_09v.prsRom Info: type=PXE version=3.4.752Device ID: 4103…PSID: MT_1090111019In my case it is ConnectX3Pro-rel-2_42_6000-web.tgzthe archive contains set of *.ini files for different HCAs based on the same chip.find the configuration file for your PSID:# grep MT_1090111019 *.iniMCX354A-FCC_Ax.ini:PSID = MT_1090111019# mstflint -d 06:00.0 dc current.ini# diff -u MCX354A-FCC_Ax.ini current.ini[HCA]hca_header_subsystem_id = 0x0003hca_header_device_id = 0x1007dpdp_en = trueeth_xfi_en = truemdio_en_port1 = 0num_pfs = 1total_vfs = 4sriov_en = false*# mlxburn -fw ./fw-ConnectX3Pro-rel.mlx -c current-sriov.ini *-wrimage ./fw-ConnectX3Pro-rel-2_42_8016-MCX354A-FCC_Ax-FlexBoot-3.4.752.binmlxfwmanager -d /dev/mst/mt4103_pciconf0 -i fw-ConnectX3Pro-rel-2_42_8016-MCX354A-FCC_Ax-FlexBoot-3.4.752.bin -uHi,The issue is related to PCI allocation in Ubuntu 16.04.3 (Ubuntu kernel cannot allocate enough memory to bring up network interfaces during boot)Please add the following line to grub2 :edit /etc/default/grubadd GRUB_CMDLINE_LINUX_DEFAULT="" pci=realloc=off""update-grubRebootThanks,SamerThis didn’t work for me, but enabling SR-IOV in the motherboard BIOS did in case anyone stumbles across this same issue.  Under chipset features on an AMD motherboard.Powered by Discourse, best viewed with JavaScript enabled"
907,is-rdma-roce-with-winof-and-windows-10-possible-how-about-from-a-hyper-v-linux-box-on-that-same-system,"Is RDMA/RoCE with WinOF and Windows 10 possible?How about from a Hyper-V Linux box on that same system?I surmise that it would have to be EN, rather than IB.There is older documentation out on the web that says no, but there are other indications that improvements have been made.So is this possible yet? Or should I abandon the thought for now?ConnectX-5 card.ThanksHello Andrew,Many thanks for posting your question on the Mellanox Community.Currently RDMA is only supported on Windows 10 Pro for Workstations. The regular Windows 10 Pro does not have that capability and will also not inherit this even when running on a Hyper-V hypervisor.Many thanks,~Mellanox Technical Support​Looks like the windows 10 enterprise is also included on that list (per microsoft) but I haven’t attempted it. Should be an interesting experiment.Thanks much!what is the client application programming APIs ? Is it Windows Network Direct SPI as following link:NetworkDirect/docs at master · microsoft/NetworkDirect · GitHubPowered by Discourse, best viewed with JavaScript enabled"
908,do-we-need-enhanced-host-management,"We have several “ConnectX-5 MT27800” 100GB cards. We need to buy a spare one. Sales told us, that these cards are “end of sale”. They gives us this to choose from “ConnectX-5 EN Cards”. There we have two 100GB options: MCX516A-CCAT and MCX516A-CCHT (with Enhanced Host Management). I am not sure what is “Enhanced Host Management” in adapters and do we need it ?Thank youHello Sergey,Thank you for posting your inquiry on the NVIDIA Networking Community.Enhanced host management gives you the opportunity to control the adapter through a BMC interface. In normal circumstances, it is not needed and the MCX516A-CCAT will be more than sufficient.Thank you and regards,~NVIDIA Networking Technical SupportThank you very much, Martijn !Powered by Discourse, best viewed with JavaScript enabled"
909,why-do-lossy-roce-accelerations-change-rdma-write-first-middle-last-to-rdma-write-only,"Hello,I have a 100G Mellanox RDMA NIC (MT27800 Family [ConnectX-5]) installed for RoCE and setup lossy RoCE Accelerations according to https://community.mellanox.com/s/article/How-to-Enable-Disable-Lossy-RoCE-Accelerations.However after enabling these feature RDMA Writes larger than the MTU which have previously been fragmented into RDMA Write {First, Middle, Last} are only sent as multiple RDMA Write Only.Is this intended behaviour? (see attached packet dump)Another thing I observed is that now If sending multiple packets, a few RDMA Write packets have the request-ACK bit set even though they are not the last packet of a fragmented write. This was not the case without Lossy RoCE Accel. enabled.Best regards,Matthias
2021-11-29-175514_1917x1208_scrot.png1917×1208 225 KB
Hi MatthiasIt is expected behavior which for better performancePowered by Discourse, best viewed with JavaScript enabled"
910,af-xdp-zero-copy-support-for-mlx5-core-driver,"Hello, Will mlx5_core drive support AF_XDP ZERO copy?NIC: Mellanox Technologies MT27800 Family [ConnectX-5]
Processor: AMD EPYC 7452 32-core Processor
kernel version: 5.10.0-22-amd64
Operating System: Debian 11Regards,
Gopinath KHello Gopinath K.,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.AF_XDP Zero Copy support resides in our driver for several years now.See the following link → [bpf-next,v4,00/17] AF_XDP infrastructure improvements and mlx5e support - PatchworkThank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
911,public-key-accelerator-performance-on-bluefield-2,"Hi there, I’m new to Bluefield 2, currently using the card’s hardware Public Key Accelerator to perform ECDSA signing for certain network packets. I’m using the libPKA(GitHub), and have read its architectural documentation.However, I would like to seek for further clarification on these following topics regarding the Accelerator’s performance:Thanks a lot.I’m also working on offloading TLS encryption and decryption to the DPU，Can you share how you used libpka to complete encryption and decryption? Thanks a lot!I used the libPKA directly with the APIs in pka.h of libPKA/lib. I used it for, and only for ECDSA operations.
You can find my adapter code here: lyftfc/ecdsa-libpka: ECDSA wrapper utility for Mellanox/pka library running on the Bluefield SmartNICs (github.com)
In particular, the content in ecdsa_utils.c is the actual implementation.Some basic benchmark results are available here. Could be inaccurate, for reference only.Thank you very much for your sharing sincerely！ I’ll read it carefully.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
912,4-9-ofed-driver-not-working,"Kernel:  Linux bi-headnode01 5.4.0-137-generic
OS: Ubuntu 20.04.5 LTS
Version: MLNX_OFED_LINUX-4.9-6.0.6.0
IB Card: ConnectX-3I’m having trouble getting our headnode working after reinstalling it. It appears that traffic isn’t crossing the card, but it can see the switch and other nodes.This is doing an ibping from a lustre node to the headnode:
[root@bi-ddn2 log]# ibping -G 0x506b4b030073a571
ibwarn: [23083] _do_madrpc: recv failed: Connection timed out
ibwarn: [23083] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 4)
ibwarn: [23083] _do_madrpc: recv failed: Connection timed out
ibwarn: [23083] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 4)
ibwarn: [23083] _do_madrpc: recv failed: Connection timed out
ibwarn: [23083] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 4)
ibwarn: [23083] _do_madrpc: recv failed: Connection timed out
ibwarn: [23083] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 4)
ibwarn: [23083] _do_madrpc: recv failed: Connection timed out
ibwarn: [23083] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 4)
ibwarn: [23083] _do_madrpc: recv failed: Connection timed out
ibwarn: [23083] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 4)
ibwarn: [23083] _do_madrpc: recv failed: Connection timed out
ibwarn: [23083] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 4)It can talk to the switch with no problem.
root@bi-headnode01:~# ibnodes
Ca      : 0xec0d9a0300144560 ports 2 “DataDirect HCA-1”
Ca      : 0x506b4b03006d6ef0 ports 1 “bi-node002 HCA-1”
Ca      : 0xec0d9a0300143ac0 ports 2 “DataDirect HCA-1”
Ca      : 0x506b4b030073a570 ports 1 “bi-headnode01 HCA-1”
Switch  : 0x1070fd0300b9d5a2 ports 41 “MF0;bi-clusterswitch1:MQM8700/U1” enhanced port 0 lid 1 lmc 0The modules seem to be loading:
root@bi-headnode01:~# lsmod |grep -i ib
ko2iblnd              237568  1
lnet                  573440  7 osc,ko2iblnd,obdclass,ptlrpc,mgc,lmv,lustre
libcfs                475136  12 fld,lnet,osc,fid,ko2iblnd,obdclass,ptlrpc,mgc,lov,mdc,lmv,lustre
ib_ucm                 20480  0
ib_umad                24576  0
mlx5_ib               401408  0
ib_uverbs             135168  3 rdma_ucm,mlx5_ib,ib_ucm
ib_ipoib              180224  0
ib_iser                53248  0
rdma_cm                61440  3 ko2iblnd,ib_iser,rdma_ucm
ib_cm                  57344  3 rdma_cm,ib_ipoib,ib_ucm
libiscsi               61440  1 ib_iser
mlx5_core            1216512  2 mlx5_fpga_tools,mlx5_ib
scsi_transport_iscsi   110592  2 ib_iser,libiscsi
mlx4_ib               229376  0
ib_core               335872  12 rdma_cm,ib_ipoib,ko2iblnd,mlx4_ib,iw_cm,ib_iser,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm,ib_ucm
mlx4_core             352256  2 mlx4_ib,mlx4_en
mlx_compat             65536  17 rdma_cm,ib_ipoib,mlx4_core,ko2iblnd,mlx4_ib,iw_cm,mlx5_fpga_tools,ib_iser,ib_umad,mlx4_en,ib_core,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm,mlx5_core,ib_ucmAre you running ibping server on the destination (0x506b4b030073a571)?Yes I am running ibping -S on the head node (0x506b4b030073a571).When I run it in reverse I get nothing back.Could you please try the below?a. Gather the Base LID of the headnode by running the following command on the headnode → ibstat
b. Once you get the LID of the headnode, run this command on the destination (in our case, the headnode) → ibping -S
c. Run this command from the source → ibping -c 5 Please let us know if that works.root@bi-headnode01:~# ibstat
CA ‘mlx4_0’
CA type: MT4099
Number of ports: 1
Firmware version: 2.42.5000
Hardware version: 1
Node GUID: 0x506b4b030073a570
System image GUID: 0x506b4b030073a573
Port 1:
State: Active
Physical state: LinkUp
Rate: 56
Base lid: 4
LMC: 0
SM lid: 1
Capability mask: 0x02514868
Port GUID: 0x506b4b030073a571
Link layer: InfiniBand
root@bi-headnode01:~# ibping -S[root@bi-ddn2 network-scripts]# ibping -L 4 -c 5 -f
ibwarn: [4967] _do_madrpc: recv failed: Connection timed out
ibwarn: [4967] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 4)
ibwarn: [4967] _do_madrpc: recv failed: Connection timed out
ibwarn: [4967] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 4)
ibwarn: [4967] _do_madrpc: recv failed: Connection timed out
ibwarn: [4967] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 4)
^C
—  (Lid 4) ibping statistics —
4 packets transmitted, 0 received, 75% packet loss, time 4030 ms
rtt min/avg/max = 0.000/0.000/0.000 msThank you for providing the output. Could you please verify if opensm is running → systemctl status opensm
If it is not running, please run → systemctl start opensmAlso, use this command to restart openibd → /etc/init.d/openibd restartCan you confirm if this issue is occuring across all nodes in your fabric?
If using virtualization, GRH (global routing header) must be present in the packet. For ibping, --dgid parameter needs to be used (see man ibping).
To get GIDs, on the server run ‘show_gids’ and use the output on the client side.Please note that if the system OS on the headnode was upgraded, thereby causing a kernel upgrade; OFED will need to be recompiled with new kernel.The subnet manager is running on the Infiniband switch, so I don’t think that’s the issue.I have tried it on another node that was freshly installed and it didn’t work either.On orders from my boss, I went back to the previous OS, which is Centos7.8 with the same issue.Please ensure that you are using the recommended firmware version on the other node as well. This can be found in the Release notes of the driver.
For MLNX_OFED 4.9-6.0.6.0 LTS -https://docs.nvidia.com/networking/display/MLNXOFEDv496060LTS/General+Support+in+MLNX_OFED#GeneralSupportinMLNX_OFED-SupportedNICsFirmwareVersionsHello ,Hope the above information was helpful. For further investigation, you will need to create a support ticket and a support entitlement for your products. Please reach out to Networking-Contracts@nvidia.com who can help you with the support entitlement. Once you have an active contract, you can create a support ticket via sending an email to EnterpriseSupport@nvidia.com.Best regards,
Nvidia supportAfter talking to DDN about it we attempted to install a newer version of the Lustre client. Even though ibping doesn’t work the new Lustre client can talk to the DDN appliance.I have no idea how Lustre can work when ibping does not, but for some reason it does.Powered by Discourse, best viewed with JavaScript enabled"
913,netq-agent-isnt-running-on-my-host,"On Ubuntu, CentOS and RHEL 7 hosts, a few other packages must be installed before you install the NetQ Agent.On Ubuntu hosts, you need the following packages and versions:On RHEL 7 or CentOS hosts, you need the following packages and versions:In addition, you need to make sure NTP is configured. For more information on all this, read the NetQ user guide.Powered by Discourse, best viewed with JavaScript enabled"
914,ofed-driver-cant-load-probe-as-module-auxiliary-is-conflicted-with-intels-net-diver,"I can’t properly install OFED 5.7 or 5.4 dirver on my system after I had installed intel i40e/ice driver.
I got the error message when I run [/etc/init.d/openibd restart] to load driver:
[root@localhost MLNX_OFED_LINUX-5.4-1.0.3.0-rhel8.4-x86_64]# /etc/init.d/openibd restart
Unloading auxiliary                                        [FAILED]
rmmod: ERROR: Module auxiliary is in use by: iceI found the reason is that the OFED dirver and i40e driver all have a module auxiliary installed, and the two “auxiliary.ko” is conflict.
/lib/modules/4.18.0-305.el8.x86_64/updates/drivers/net/ethernet/intel/auxiliary/auxiliary.ko
/lib/modules/4.18.0-305.el8.x86_64/extra/mlnx-ofa_kernel/drivers/base/auxiliary.koI tried the latest dirver and got the some result. Is there any fix or any proper way to use both driver/device ?Hi,
the restart include stop and start.
if you don’t need stop, then just need to start as
/etc/init.d/openibd startif have to stop, could you stop ice related module first?
As you know, the error is raised by kernel  protection mechanism.
“rmmod: ERROR: Module auxiliary is in use by: ice”Regards,
LeveiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
915,doca-flow-library-now-available-with-doca-1-1,"DOCA FLOW is the most fundamental API for building generic execution pipes in HW.The library provides an API for building a set of pipes, where each pipe consists of match criteria, monitoring, and a set of actions. Pipes can be connected where after pipe-defined actions are executed, the packet may proceed to another pipe.Using DOCA FLOW API, it is easy to develop HW-accelerated applications that have a match on up to two layers of packets (tunneled).Sign up for DOCA Early Access: https://developer.nvidia.com/nvidia-doca-sdk-early-accessView the DOCA FLOW documentation here: Flow Programming Guide :: NVIDIA DOCA SDK DocumentationPowered by Discourse, best viewed with JavaScript enabled"
916,no-physical-link-established-between-mcx341a-xcen-and-mcx342a-xccn,"I have MCX341A-XCEN and MCX342A-XCCN ( both Mellanox Connectx-3 NICs ) connected directly through SFP+ cable but both I/O LED doesn’t lit up.However, if I connect between two MCX341A-XCEN using the same cable, both I/O LED do lit up.Same goes for connect between two MCX342A-XCCN.I notice the firmware version for MCX342A-XCCN is 2.42.5000, while MCX341A-XCEN version is 2.31.1598. I have tried to upgrade both cards to the same firmware version ( 2.42.5000 ) from the firmware download site. But I can’t find the same matching PSID firmware for MCX341A in version 2.42.5000Hi Ray,First upgrade to latest firmware in both Nicsi think there is a misstypo since there is no such P/N =MCX341A-XCEN , it should be MCX341A-XCGNhttp://www.mellanox.com/downloads/firmware/fw-ConnectX3-rel-2_42_5000-MCX341A-XCG_Ax-UEFI-14.11.45-FlexBoot-3.4.752.bin.zipYou can check by running the below command:flint -d <device_id> qnote - in order to get device_id , run “mst start” and then “mst status”In addition, refer to the supported cables and modules899.35 KBThanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
917,advanced-rfs-configuration-suggestion-differs-between-nvidia-and-rhel,"NVIDIA documentation for configuration of (Advanced) RFS is way different from all other guides on Internet (Red Hat Enterprise Linux for one).In the NVIDIA doc, it’s suggested that the number of entries in the per-queue flow should be the same as the number for the global flow table:[https://docs.nvidia.com/networking/display/MLNXOFEDv581011/Flow+Steering](Flow Steering doc)In all other source on the Internet (RHEL is one) it says to set the value of per-queue flow to the value of rps_sock_flow_entries divided by N, where N is the number of receive queues on a device.Does anyone know why is such difference, and which one should be followed?Powered by Discourse, best viewed with JavaScript enabled"
918,how-many-rss-queues-do-the-connectx-6-chip-support-for-each-port,"Hi Nvidia team,
Could you help to confirm below question?“How many RSS queues do the connectx-6 chip support for each port?”Thanks,
JeremyDoes any NVIDIA member to reply my question?Thanks,
JeremyMaybe you can reference at RSS Support - MLNX_OFED v5.9-0.5.6.0 - NVIDIA Networking Docs
and  Ethtool - MLNX_OFED v5.9-0.5.6.0 - NVIDIA Networking Docs
ethtool -l eth|Shows the number of channels.
ethtool -L eth [rx ] [tx ]|Sets the number of channels.Hi NVIDIA member,
I didn’t find out the answer in RSS support chapter from your shared  website.
Please help to confirm ""How many RSS queues do the connectx-6 chip support for each port?” again?Thanks,
JeremyHi Jeremysung1002,Unfortunately, we cannot share internal information related to addressed question. If you hold an existing Account with us, you may reach out to the Sales Account Representative and they can determine/validate further if this information is permitted to be shared.In addition, RSS is a concept about scaling to specific queue/s. As mentioned in above article, you can check the number of queues available. Our implementation of queues is generally based on the number of CPU’s present on the system.Thanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
919,building-and-running-applications-with-hpc-x,"Edit:I fixed the issue below by switching to hpcx-2.6 which support ucx v1.8. So now I can run the hello_c example.mpicc $HPCX_MPI_TESTS_DIR/examples/hello_c.c -o $HPCX_MPI_TESTS_DIR/examples/hello_cmpirun -np 2 $HPCX_MPI_TESTS_DIR/examples/hello_cHello, world, I am 1 of 2, (Open MPI v4.0.3rc4, package: Open MPI root@0e5a40994726 Distribution, ident: 4.0.3rc4, repo rev: v4.0.3rc4-6-g8b4a8cd34c, Unreleased developer copy, 148)Hello, world, I am 0 of 2, (Open MPI v4.0.3rc4, package: Open MPI root@0e5a40994726 Distribution, ident: 4.0.3rc4, repo rev: v4.0.3rc4-6-g8b4a8cd34c, Unreleased developer copy, 148)oshcc $HPCX_MPI_TESTS_DIR/examples/hello_oshmem_c.c -o $HPCX_MPI_TESTS_DIR/examples/hello_oshmem_coshrun -np 2 $HPCX_MPI_TESTS_DIR/examples/hello_oshmem_cHello, world, I am 0 of 2: http://www.open-mpi.org/ (version: 1.4)Hello, world, I am 1 of 2: http://www.open-mpi.org/ (version: 1.4)Original post:Hi,I am following the example provided in the hpcx documentationI am receiving the following error. I compiled ucx-1.10 provided in the sources that came with hpcx and still get the same error.ibdiagnet report summary:Summary-I- Stage Warnings Errors Comment-I- Discovery 0 0-I- Lids Check 0 0-I- Links Check 0 0-I- Subnet Manager 0 0-I- Port Counters 0 0-I- Nodes Information 0 0-I- Speed / Width checks 0 0-I- Alias GUIDs 0 0-I- Virtualization 0 0-I- Partition Keys 0 0-I- Temperature Sensing 0 0-I- You can find detailed errors/warnings in: /var/tmp/ibdiagnet2/ibdiagnet2.log-I- ibdiagnet database file : /var/tmp/ibdiagnet2/ibdiagnet2.db_csv-I- LST file : /var/tmp/ibdiagnet2/ibdiagnet2.lst-I- Network dump file : /var/tmp/ibdiagnet2/ibdiagnet2.net_dump-I- Subnet Manager file : /var/tmp/ibdiagnet2/ibdiagnet2.sm-I- Ports Counters file : /var/tmp/ibdiagnet2/ibdiagnet2.pm-I- Nodes Information file : /var/tmp/ibdiagnet2/ibdiagnet2.nodes_info-I- Alias guids file : /var/tmp/ibdiagnet2/ibdiagnet2.aguid-I- VPorts file : /var/tmp/ibdiagnet2/ibdiagnet2.vports-I- VPorts Pkey file : /var/tmp/ibdiagnet2/ibdiagnet2.vports_pkey-I- Partition keys file : /var/tmp/ibdiagnet2/ibdiagnet2.pkeyAny help would be appreciated to get me started on being able to run mpirun on all my nodes would be greatly appreciated.Cheers(base) user@oak-rd0-linux:~/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64$ source hpcx-init.sh(base) user@oak-rd0-linux:~/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64$ hpcx_load(base) user@oak-rd0-linux:~/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64$ env | grep HPCXHPCX_HCOLL_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/hcollHPCX_CLUSTERKIT_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/clusterkitHPCX_OSU_CUDA_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompi/tests/osu-micro-benchmarks-5.6.2-cudaHPCX_OSU_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompi/tests/osu-micro-benchmarks-5.6.2HPCX_MPI_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompiHPCX_OSHMEM_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompiHPCX_HOME=/home/user/infiniband/hpcxHPCX_UCX_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ucxHPCX_IPM_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompi/tests/ipm-2.0.6HPCX_SHARP_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/sharpHPCX_IPM_LIB=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompi/tests/ipm-2.0.6/lib/libipm.soHPCX_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/nccl_rdma_sharp_pluginHPCX_MPI_TESTS_DIR=/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompi/tests(base) user@oak-rd0-linux:~/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64$ mpicc $HPCX_MPI_TESTS_DIR/examples/hello_c.c -o $HPCX_MPI_TESTS_DIR/examples/hello_c(base) user@oak-rd0-linux:~/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64$ mpirun -np 2 $HPCX_MPI_TESTS_DIR/examples/hello_c[1642388680.640057] [oak-rd0-linux:2429918:0] ucp_context.c:1467 UCX WARN UCP version is incompatible, required: 1.10, actual: 1.8 (release 0 /lib/libucp.so.0)[1642388680.640358] [oak-rd0-linux:2429919:0] ucp_context.c:1467 UCX WARN UCP version is incompatible, required: 1.10, actual: 1.8 (release 0 /lib/libucp.so.0)[1642388680.680581] [oak-rd0-linux:2429918:0] ucp_context.c:1467 UCX WARN UCP version is incompatible, required: 1.10, actual: 1.8 (release 0 /lib/libucp.so.0)[1642388680.681228] [oak-rd0-linux:2429919:0] ucp_context.c:1467 UCX WARN UCP version is incompatible, required: 1.10, actual: 1.8 (release 0 /lib/libucp.so.0)/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompi/tests/examples/hello_c: symbol lookup error: /home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompi/lib/openmpi/mca_pml_ucx.so: undefined symbol: ucp_tag_recv_nbx/home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompi/tests/examples/hello_c: symbol lookup error: /home/user/infiniband/hpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu20.04-x86_64/ompi/lib/openmpi/mca_pml_ucx.so: undefined symbol: ucp_tag_recv_nbxPrimary job terminated normally, but 1 process returneda non-zero exit code. Per user-direction, the job has been aborted.mpirun detected that one or more processes exited with non-zero status, thus causingthe job to be terminated. The first process to do so was:Process name: [[6258,1],0]Exit code: 127Hello,When application compiles against specific version of MPI, same version of MPI must be present on the compute node to run the application. For your case, UCX must be there.Looking on the output, it seems that host is not finding required UCX version - looking for v1.10, that seems to be part of HPC-X, but finding v1.8 one that installed under /lib64. Try to follow steps from HPCX user manual to compile and run application and if possible not use user root.One note, when running mpirun, could you double check that HPC-X environment initiated correctly ( export HPCX_HOME; source $HPCX_HOME/hpcx-init.sh; hpcx_load ) and check if adding ‘-x LD_LIBRARY_PATH’ help.Best Regards,VikiHi Viki,update:I have now installed on all 3 of my nodes (with ubuntu 20.04 5.4.0-26-generic kernel):MLNX_OFED_LINUX-5.0-2.1.8.0-ubuntu20.04-x86_64andhpcx-v2.8.1-gcc-MLNX_OFED_LINUX-5.0-1.0.0.0-ubuntu18.04-x86_64MPI TESTS works fine on each individual node. My current issue is to be able to use mpirun on my main node (with opensm enabled on that main node) and use the cpu s in the other nodes.Example :I am on oak-rd0-linux (Main node), opensm is running, ibdiagnet does not report any warning or errors and I am trying to test using the cpu on oak-rd1-linux (host1) and oak-rd2-linux (host2) with:mpirun -x LD_LIBRARY_PATH -np 2 -H oak-rd1-linux,oak-rd2-linux $HPCX_MPI_TESTS_DIR/examples/hello_cNothing happens - it seems to hang and I am not sure where to go from here. What am I doing wrong at this step and what can I check to identify to problem?sudo ibnetdiscover output:vendid=0x2c9devid=0xc738sysimgguid=0xe41d2d0300b39ee0switchguid=0xe41d2d0300b39ee0(e41d2d0300b39ee0)Switch 12 “S-e41d2d0300b39ee0” # “SwitchX - Mellanox Technologies” base port 0 lid 3 lmc 0[1] ""H-0010e00001885688""2 # “oak-rd0-linux HCA-1” lid 1 4xQDR[2] ""H-0010e000018d08e0""1 # “oak-rd1-linux HCA-1” lid 4 4xQDR[3] ""H-0010e00001885908""1 # “oak-rd2-linux HCA-1” lid 2 4xQDRvendid=0x2c9devid=0x1003sysimgguid=0x10e0000188590bcaguid=0x10e00001885908Ca 2 “H-0010e00001885908” # “oak-rd2-linux HCA-1”1 “S-e41d2d0300b39ee0”[3] # lid 2 lmc 0 “SwitchX - Mellanox Technologies” lid 3 4xQDRvendid=0x2c9devid=0x1003sysimgguid=0x10e000018d08e3caguid=0x10e000018d08e0Ca 2 “H-0010e000018d08e0” # “oak-rd1-linux HCA-1”1 “S-e41d2d0300b39ee0”[2] # lid 4 lmc 0 “SwitchX - Mellanox Technologies” lid 3 4xQDRvendid=0x2c9devid=0x1003sysimgguid=0x10e0000188568bcaguid=0x10e00001885688Ca 2 “H-0010e00001885688” # “oak-rd0-linux HCA-1”2 “S-e41d2d0300b39ee0”[1] # lid 1 lmc 0 “SwitchX - Mellanox Technologies” lid 3 4xQDRPowered by Discourse, best viewed with JavaScript enabled"
920,updated-centos-7-9-2009-kernel-using-elrepo-kernel-mlnxofedinstall-add-kernel-support-failed,"I updated CentOS 7.9.2009 kernel using elrepo-kernel.And then when I tried to run mlnxofedinstall --add-kernel-support, it failed.I installed both the mainline kernel and also the mainline-devel kernel from elrepo-kernel.uname -r5.15.11-1.el7.elrepo.x86_64ofed_info -n5.5-1.0.3.2./mlnxofedinstall --add-kernel-supportNote: This program will create MLNX_OFED_LINUX TGZ for rhel7.9 under /tmp/MLNX_OFED_LINUX-5.5-1.0.3.2-5.15.11-1.el7.elrepo.x86_64directory.See log file /tmp/MLNX_OFED_LINUX-5.5-1.0.3.2-5.15.11-1.el7.elrepo.x86_64/mlnx_iso.24347_logs/mlnx_ofed_iso.24347.logChecking if all needed packages are installed…Building MLNX_OFED_LINUX RPMS . Please wait…ERROR: Failed executing ""MLNX_OFED_SRC-5.5-1.0.3.2/install.pl --tmpdir /tmp/MLNX_OFED_LINUX-5.5-1.0.3.2-5.15.11-1.el7.elrepo.x86_64/mlnx_iso.24347_logs --kernel-only --kernel 5.15.11-1.el7.elrepo.x86_64 --kernel-sources /lib/modules/5.15.11-1.el7.elrepo.x86_64/build --builddir /tmp/MLNX_OFED_LINUX-5.5-1.0.3.2-5.15.11-1.el7.elrepo.x86_64/mlnx_iso.24347 --disable-kmp --build-only --distrorhel7.9""ERROR: See /tmp/MLNX_OFED_LINUX-5.5-1.0.3.2-5.15.11-1.el7.elrepo.x86_64/mlnx_iso.24347_logs/mlnx_ofed_iso.24347.logFailed to build MLNX_OFED_LINUX for 5.15.11-1.el7.elrepo.x86_64systemctl status openibd? openibd.service - openibd - configure Mellanox devicesLoaded: loaded (/usr/lib/systemd/system/openibd.service; enabled; vendor preset: disabled)Active: failed (Result: exit-code) since Fri 2021-12-24 01:40:37 EST; 34min agoDocs: file:/etc/infiniband/openib.confProcess: 1076 ExecStart=/etc/init.d/openibd start bootid=%b (code=exited, status=3)Main PID: 1076 (code=exited, status=3)Dec 24 01:40:36 aes18 openibd[1076]: [93B blob data]Dec 24 01:40:36 aes18 openibd[1076]: [95B blob data]Dec 24 01:40:36 aes18 openibd[1076]: [94B blob data]Dec 24 01:40:36 aes18 openibd[1076]: [49B blob data]Dec 24 01:40:36 aes18 openibd[1076]: [93B blob data]Dec 24 01:40:37 aes18 openibd[1076]: [94B blob data]Dec 24 01:40:37 aes18 systemd[1]: openibd.service: main process exited, code=exited, status=3/NOTIMPLEMENTEDDec 24 01:40:37 aes18 systemd[1]: Failed to start openibd - configure Mellanox devices.Dec 24 01:40:37 aes18 systemd[1]: Unit openibd.service entered failed state.Dec 24 01:40:37 aes18 systemd[1]: openibd.service failed.I’ve attached the log files and hopefully that will be of use/help.Your help is greatly appreciated.Thank you.mlnx_ofed_iso.24347.log (3.07 KB)mlnx-ofa_kernel-5.5.rpmbuild.log (322 KB)Hi ChanCentOS 7.9.2009#uname -r5.15.11-1.el7.elrepo.x86_64#ofed_info -n5.5-1.0.3.2Unfortunately, there are kernel compatibility issues when using Linux kernel 5.15.x.NVIDIA is only supporting GA kernel version (5.15). Any other kernel version (5.15.x, x>0) is not officially supported.According to OFED release notes: https://docs.mellanox.com/display/MLNXOFEDv551032/General+SupportNVIDIA only support either CentOS 7.9 - 3.10.0-1160.el7.x86_64 or kernel 5.15 GA.The changes in Linux kernel 5.15.x are going to be supported in a future OFED release which will support kernel 5.16 GA.ThanksPowered by Discourse, best viewed with JavaScript enabled"
921,how-do-i-achieve-full-bandwidth-from-both-100gb-ports-on-the-mcx516a-cdat-adapter,"We have an MCX516A-CDAT adapter installed into a PCIe x16 GEN4 slot (AMD CPU/Windows 10 Pro), and each port is connected via a 3-meter QSFP28 compatible DAC to a 100Gb port on a Mellanox MSN2010 switch (ONYX). When our data sources are connected to the switch and running, we cannot seem to achieve the full 100Gb throughput from each port. We run data at 65.4Gb/sec from the switch through each 100Gb port. No matter how we configure the data sources (1 100Gb port or 2 100Gb ports), we can never exceed 100Gb throughput on the board. The specifications for this board clearly state that if installed in a PCIe x16 GEN4 slot, each port can run up to 100Gb. We are not trying to LAG ports at all. We have not been able to find anything in the documentation or device settings that allow us to maximize this board’s throughput. We are currently using 2 NICs to work around this problem, which is not optimal for our application.Many factors could impact performance degradation or not reaching rate line.
Have you validated that the following components were deployed for maximum performance:Server BIOS performance settingsOS performance settings (MTU, RSS,RX/TX buffer, etc…)Latest WinOF2/FW (ref UM performance tuning section “https://docs.nvidia.com/networking/display/winof2v30”The best tool to measure TCP/UDP for Windows is the ntttcp tool.For ROCE traffic, our driver embed our “MlxNdPerf Utility” for basic RDMA test, previously provided via “nd_write/read/send_bw” in older driver.At last, once the server is optimized for maximum performance, should the performance issue remains, I would suggest opening a support case with Nvidia with a valid support contract.Sophie.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
922,for-performance-considerations-how-to-get-the-number-of-queue-pairs-that-have-been-established-on-the-roce-nic,"As mentioned in the title, is there an API interface or a tool to get this information?In order to prevent the establishment of too many QPs, are there any plans for the future?Hi Vivienne,Thank you for posting your inquiry to the Mellanox community.If you are utilizing RDMA_CM, you can use the /opt/mellanox/iproute2/sbin/rdma binary to see this information:[root@node ~]# /opt/mellanox/iproute2/sbin/rdma resource show qplink mlx5_0/- lqpn 0 type SMI state RTS sq-psn 180935 comm [ib_core]link mlx5_0/- lqpn 1 type GSI state RTS sq-psn 0 comm [ib_core]link mlx5_0/1 lqpn 77 type UD state RTS sq-psn 561450 comm [ib_core]link mlx5_0/1 lqpn 207 rqpn 457 type RC state RTS rq-psn 8319677 sq-psn 16058286 path-mig-state MIGRATED pdn 43 pid 14177 comm ib_send_bwlink mlx5_0/1 lqpn 208 rqpn 458 type RC state RTS rq-psn 12268335 sq-psn 12898667 path-mig-state MIGRATED pdn 44 pid 14177 comm ib_send_bwlink mlx5_1/- lqpn 0 type SMI state RTS sq-psn 7374 comm [ib_core]link mlx5_1/- lqpn 1 type GSI state RTS sq-psn 0 comm [ib_core]link mlx5_1/1 lqpn 327 type UD state RTS sq-psn 1224 comm [ib_core]Best regards,Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
923,does-connectx-5-support-receiver-congestion-handling,"In this post https://community.mellanox.com/s/article/Congestion-Handling-Modes-for-Multi-Host-in-ConnectX-4-Lx-WRED, I find that ConnectX-4 support this function, I tried to configure it with ConnectX-5, but mstcongestion return “-E- Failed to perform the operation: Mark action is not supported!”So does connectX-5 support this function ?
image.png1451×356 14.2 KB
Hello Hexiang,As this article is only applicable for the Multi-host ConnectX-4 Lx configuration, for the ConnectX-5 adapter, please review the following UM section related to ECN configuration → https://docs.nvidia.com/networking/pages/viewpage.action?pageId=64306472The ‘mstcongestion’ tool is only applicable for multi-host devices. See the following link for more information about the tool → https://docs.nvidia.com/networking/pages/viewpage.action?pageId=71013598Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
924,how-to-factory-reset-cumulus-linux,"I have no clue what I am doing and fear I have messed up my new Cumulus Linux.I would like to return it to out of the box.I am also open to someone connecting to it and assist with fixing it and setting it up for use.Hi Jeff,Please open a support case to have an engineer assist with this. They are going to need to know what state the device is currently in and what was done in order to have the device end up in this state before they can assist. Please provide outputs from the switch when opening the case for reference.Thank youPowered by Discourse, best viewed with JavaScript enabled"
925,upgrading-to-doca-1-1,"Hi all,
I had installed doca 1.0 and trying to upgrade it to the new version 1.1
But I’m getting the following error when trying to install doca-tools:sudo apt install doca-toolsReading package lists… DoneBuilding dependency treeReading state information… DoneSome packages could not be installed. This may mean that you haverequested an impossible situation or if you are using the unstabledistribution that some required packages have not yet been createdor been moved out of Incoming.The following information may help to resolve the situation:The following packages have unmet dependencies:doca-tools : Depends: doca-dpi-tools (= 1.1.024-1) but 21.03.038-1 is to be installedE: Unable to correct problems, you have held broken packages.Also, when following the documents, it requires to restart networking, I’m using ubuntu 20.04 but the only two files in /etc/init.d are network-manager and  ntpHow should I run:  * /etc/init.d/networking restart ?I was able to reproduce this issue. You can work past this by doing an apt purge for doca-dpi-tools then trying the install again.So do this:
root@bluefield2:~# sudo apt purge doca-dpi-toolsThen, retry your sudo apt install doca-toolsFor your second question there about restarting networking the ubuntu instructions are incorrect thanks for raising this.The step says:7. Execute network restart for implemented tmfifo_net0 static configuration.For Ubuntu, the ip address is added using netplan. So to apply this configuration, the command should be:sudo netplan applyThanks a lot,
One more question, I have put two Bluefield2 in one server, what would be the ssh for the second card?
I know 192.168.100.2 is preconfigured for the first card, what happens to the second card? and should I add something similar for tmfifo_net1 in the /etc/netplan/01-netcfg.yaml file ?tmfifo_net0:
addresses: [192.168.100.1/24]
dhcp4: noYou’ll have to manage the IP addressing manually. Keep the x86 host as 192.168.100.1/24, but as part of your DPU configuration, change the address from 192.168.100.2/24 to another address in that subnet. That way, as more DPUs are added and setup, they will be hard coded with .2 and you can obtain access to the new card.Powered by Discourse, best viewed with JavaScript enabled"
926,slurm-cluster-not-working,"Hi teamI used slurm cluster on this flavour of Nvidia BCM and it worked but now its showing below mentioned error when running some srun commands . please refer below and let me know if more info is required .Hi Team , anything on above issue with slurm + pyxis + enrootPowered by Discourse, best viewed with JavaScript enabled"
927,mellanox-configuration-steps-after-initial-setup,"Hi All,Need to know after initial configuration of 2*Mellanox 8700 switches, what next to be done for configuring the IB ports for DGX nodes communication?
do we need to configure MLAG or leave the ports as default?
Also i heard there is no need of stacking across Mellanox switches?
Any update would be really grateful.thanks in advanceHi there,The DGX systems come preconfigured as IB ports.
IB switches are plug and play
You just need to start opensm.
You can do that by running “ibsm” on one of the switchesThanks.Thanks for the info
subnet manager is already running in the switches
we had done stacking between 2 switches too, is it really needed?
Also there is a UFM appliance, where we had connected both the switches to UFM IB ports. Any config to be done in UFM?Powered by Discourse, best viewed with JavaScript enabled"
928,hello-everyone-i-have-a-question-about-shutting-down-the-interface-when-i-try-to-make-the-port-down-by-issuing-mstlink-d-pciadd-a-dn-command-i-get-the-errors-below,"Errors------Sending PAOS raised the following exception: port is not Down, for some reasons:1- The link is configured to be up, run this command if KEEP_ETH_LINK_UP_Px is True:mlxconfig -d set KEEP_ETH_LINK_UP_P<port_number>=02- Port management is enabled (management protocol requiring the link to remain up, PAOS won’t manage to disable the port)3- In case of multi-host please verify all hosts are not holding the port upcasual1st:~# mstconfig -d  q KEEP_ETH_LINK_UP_P1Device #1:Device type: ConnectX5Name: MCX516A-CCA_AxDescription: ConnectX-5 EN network interface card; 100GbE dual-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6Device: Configurations: Next BootKEEP_ETH_LINK_UP_P1 False(0)I don’t have an answer for 2 and 3. I don’t know how to check those.Thanks in advance.Hello Ramazan,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we cannot reproduce this issue in our labs.Please make sure that the following is in place:If all this does not resolve the issue, we recommend to open a NVIDIA Networking Support ticket, by sending an email to networking-support@nvidia.com (Valid support contract required).Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
929,restore-sn2100-running-cumulus-5-0-0-to-factory-default,"Can anyone here help with a quick question? We’re testing Cumulus 5.0.0 in our lab and want to restore the spare switch back to default configuration from the factory. What’s the command to do so?It was suggested to issue “onie-select -k” but that wipes out all user data AND the operating system. We do not want to delete the OS, just the configuration we were testing.Use:onie-install -i /tmp/cumulus-linux-5.0.0-mlx-amd64.bin -arebootthe cumulus-linux-5.0.0-mlx-amd64.bin file can be downloaded from the myMellanox portal and then copied to the switch /tmp/ directory.
the command will reinstall cumulus 5.0.0 in the next reboot and the default configuration will be applied.Powered by Discourse, best viewed with JavaScript enabled"
930,connectx-5-ex-no-longer-negotiates-100gbe-after-centos-upgrade,"I updated several Dell C6420 clients with a Mellanox Technologies MT28800 Family [ConnectX-5 Ex] card from centos 7.4.1708 to centos 7.9.2009 and a lot of them no longer negotiate 100GbE connections and the link doesn’t come up.some do … with the same cable, connected to the same switch … more confusingly, booting back into the older kernel, or even booting the older CentOS 7 installer image also does not bring up the link…5e:00.0 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex]
5e:00.1 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex]on a happy system, with centos 7.8 / 3.10.0-1127.el7.x86_64State                           : Active
Physical state                  : LinkUp
Speed                           : 100GbE
Width                           : 4x
FEC                             : Standard RS-FEC - RS(528,514)
Loopback Mode                   : No Loopback
Auto Negotiation                : ONEnabled Link Speed              : 0xf8f1f0d3 (100G,50G,40G,25G,10G,1G)
Supported Cable Speed           : 0x2024a101 (100G,56G,50G,40G,25G,10G,1G)Status Opcode                   : 0
Group Opcode                    : N/A
Recommendation                  : No issue was observedFirmware Version                : 16.32.2004
MFT Version                     : mft 4.22.1-11Querying Cables …Cable name    : mt4121_pciconf0_cable_0No FW data to show
-------- Cable EEPROM --------
Identifier                     : QSFP28 (11h)
Technology                     : 850 nm VCSEL (00h)
Compliance                     : 100GBASE-SR4 or 25GBASE-SR
Wavelength                     : 850 nm
OUI                            : 0xac4afe
Vendor                         : DELL EMC
Serial number                  : CN04HG0017E4063
Part number                    : 14NV5
Revision                       : A1
Temperature [c]                : 46 [-10…80]
Digital Diagnostic Monitoring  : YES
Length [m]                     : 50 mon another, identical system that was upgraded to centos 7.8 …Enabled Link Speed              : 0x0801f0d3 (40G,25G,10G,1G)
Supported Cable Speed           : 0x2024a101 (100G,56G,50G,40G,25G,10G,1G)Physical state: ETH_AN_FSM_ABILITY_DETECT,
State: Polling
Troubleshooting info:
Status Opcode: 2
Group Opcode: PHY FW
Recommendation: Negotiation failure …same mst cable info,
both connected to a Z9264F-ON OS Version: 10.5.0.6C1mlxconfig reset did not resolve the issue …so far 7 systems have failed after the upgrade and I have many more left to upgrade so any tips would be very much appreciated!Have you tried toggling the link on the failed servers?Ive toggled the link, moved cables, swapped phy’s, mlxconfig reset, mlxfwreset, powercycled everything … booted back into older OS versions …I went through the full list of packages that would be included in the CentOS 7.8 update and applied the most relevant ones individually. I found that the NetworkManager-1.18.4-3.el7.x86_64 broke 100GbE.
simply removing the package restores functionality but I still have no idea why it would change the negotiated / available speeds on these cards …Great, thank you for the update and for sharing the solution for the issue.
Best regards,
Nvidia supportPowered by Discourse, best viewed with JavaScript enabled"
931,vlans-configuration-in-mellanox-qm8700,"Hello Im new in infiniband and i have a mellanox qm8700 swicth . I need to configure some ports in some vlans that i have . Is this posible ??  Would you send me the correct commands to do thisThanksHello. In Infiniband there is no configuration for VLAN as in Ethernet. If it is required to isolate traffic in infiniband PKEY is used. Please find document link below that might help to explainhttps://support.mellanox.com/s/article/in-between-ethernet-vlans-and-infiniband-pkeysThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
932,trouble-with-communicate-bewteen-server-and-client,"I try to run a server-client communication example. The server sends a string first. After the client received it, the client sent another string back.
I modify the code from GitHub, which only the server sends a string to the client. The codes I run are below:
rmda_socket.hpp:server.cpp:client.cpp:I run the code. The client’s output shows below:the server is stuck and the output shows below:How can I fix it?And I want to debug these codes, but I can stick them with gdb. How can I debug?You can check rping.c as reference,Regarding the description of completion error:
mlx5: g14r2n15: got completion with error:
00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000
00000000 00008716 0a005df8 000013d2We can see ""8716”
Syndrome 0x16 - RNR Retry Counter Exceeded, Vendor error 0x87 - RNR_NAK_CTR_EXD
This means, that the sender started to send “send” WQEs when receiver still did not post receive WQEs, so sender got error CQE RNR (receiver not ready) counter exceeded.
You need to implement posting receive WQEs on receiver side before the sender is starting to send traffic.
For each polling receive CQE on the receiver side, you need to post Receive WQE immediately to keep receive queue full all time. Receive queue size should be not less that send queue size.@michaelbe Thanks for your response.
I follow your advice to change my code. On the server side, I post a recv WQE. And on the client side, I add some output after client.connect().
The server code:But it still doesn’t work.
The server’s output:The client is stuck. And the client’s output:Do I miss something? When I use rdma_get_send_comp(_id, &wc);, should I also need to use ibv_poll_cq() to check the complete queue?Hi @eric503630 ,
The function rdma_get_send_comp is already calling ibv_poll_cq(), so you can’t call both.
The implementation of this function you can see in librdmacm/rdma_verbs.h (rdma-core sources). You can see this functionstatic inline int
rdma_get_send_comp(struct rdma_cm_id *id, struct ibv_wc *wc)
{
struct ibv_cq *cq;
void *context;
int ret;}static inline int
rdma_get_recv_comp(struct rdma_cm_id *id, struct ibv_wc *wc)
{
struct ibv_cq *cq;
void *context;
int ret;}If you are using rdma_get_send_comp and rdma_get_recv_comp in parallel, id->recv_cq and id->send_cq should be different CQ-s. For sequential use there is no problem. If you are getting segmentation fault in the function rdma_get_recv_comp, you can put its implementation debug prints to see there this happens.To understand better what is wrong in your application, I suggest to take some basic working example, add trace prints to the working and your application code and compare what is wrong in calls sequence in your application.
Best regars,
Michael.Powered by Discourse, best viewed with JavaScript enabled"
933,opensm-discovering-same-port-over-and-over,"OpenSM 5.10.0 message state_mgr_report_new_ports: Discovered new port with GUID...
is repeated every few minutes.The host behind this port has not changed at all in that intervall (no reboot, no activity, no errors).This is a ConnectX-3 HCA connected to an HDR switch. Of this combination we have many, all of them working (so is this one), but not showing up in the OpenSM log.What is going wrong here?Hi,
Can you past more opensm log?
At the same time, I think it is not recommend for connect FDR card to HDR switch.
It is not tested.Thanks,
SuoWell, I think Mellanox must have tested it - otherwise, where does the compatibility matrix from? ;-)
Also, we have extensively tested it - works.In this case,  the relevant log lines readSo, lid 1 is the switch, also named hdrleaf-gc5a-28. Lid 162 aka lxmds21 allegedly dropped out, but the machine did not feel it.If I just look for occurences,there is another machine, lxmds23, that is also completely idle and not rebooting.If this is perhals connected to the HCA hardware, it would’nt matter so much, could change it.If this rather points to an issue with the switch, it is more urgent, because there are more important machines connected to that switch.Thanks,
ThomasHi,
Please check the firmware release note:—This is what we tested.
Thanks,
Suohttps://docs.nvidia.com/networking/display/NVIDIAQuantumFirmwarev2720106102/Firmware+Compatible+ProductsThanks, Suo.
The point about ports #27-34, I faintly remembered this but did not care enough when setting up our current fabric.We do have ConnectX3 ↔ Quantum connections on theses “wrong” ports - seeing no problems so far. But this can be recabled, of course.However, the two machines I worried about here are conected to “allowed” ports on the Quantum switch.
According to your firmware compatibility matrix, these should work.Anyhow I will have the problematic machines relocated, which would at least work around this issue.Regards,
ThomasPowered by Discourse, best viewed with JavaScript enabled"
934,firmware-security-on-ib-fabrics,"I have a fabric with some untrusted hosts on it - some hosts where non-admins have root access.
Is there a way I can prevent these hosts from being able to flash or change the configuration of other devices on the fabric? Both other HBAs and switches?
Thanks.Hello,While managed switches and UFM would still require a password, other IB components may be vulnerable via MADs if the untrusted user has root access.These types of security/access measures would be better handled in Linux user permissions than in the IB fabric.If you would like further assistance with helping to find ways to better secure and optimize your fabric, please reach out to your Nvidia account team to request solution engineering assistance.Thank youPowered by Discourse, best viewed with JavaScript enabled"
935,dell-pe-r730xd-fans-running-too-fast-because-of-connectx-3-pcie-card,"Hello,Recently we bought a DELL PowerEdge R730xd with a Mellanox ConnectX-3 PCI card inside (MCX353A-FCBT). Without any load, each of the 6 server fans runs at about 17000 RPM !!By removing the ConnectX-3 card, the fans speed drops to ~5000 RPM, which seems more normal according to what we have always observed in other computing nodes (even with ConnectX-3 cards).I contacted Dell support which finally told me it’s normal because they are not able to fine tune cooling response for such cards, or something like that. I think that the main argument I found is given here Dell PowerEdge Server R7XX Series Fan Speed with GPU with this comment from Dell people : “[…] Dell PowerEdge servers have the cooling capacity to support a broad array of PCI adapter cards. For PCI cards that are designed or qualified by Dell, the response is optimized while for third-party installed cards the response cautions on the side of more cooling. Many of these third-party cards do not have active thermal sensor monitoring or standard sensor reading topologies hence limiting our ability to fine tune cooling response for such cards.”As far as I understood, connectX-3 firmware is providing temperature sensors since version 2.40.5030, am I right ? I’ve got version 2.40.5048, so I should have those temperature sensors available, right ?My first questions are :how can I get values of those sensors ? IPMI ?Can we get the current temperature by this way ?I also installed MFT to get the current temperature of the ConnectX card, the value returned by mget_temp is 45 (Fans at 17000 RPM). I guess this is in °C, right ?What is the normal value I can expect ? The maximum value I should not overpass ?Thanks a lot for any helpBest regardsPierreHmm, I have not used the Dell provided cards, but its a shame to hear they have more compatibility issues with Dell servers then the standard Mellanox cards. I assume you have already verified the FW is up to date. Unfortunately I do not have any other advise for you.Hi Grant,Back to the office, I’ve just made some tuning tests (that is, all the different system profiles). Nothing works… Each time, I felt like a jet was taking off in my office.The only way I found to make the server silent (fans at 5000 RPM) is to get the IB card out of the server. This Mellanox card is Dell provided.I also checked with ol ConnectX IB card taken into a Dell C6100, I’ve got the same problem in the R730xd (not in the C6100).PierreHi all,As recommended, I contacted again Dell support. For Dell, there is no trouble, for this kind of PCIe card, the fans must run at 100%… As my consolation prize, it was proposed to add a warning message on their configurator when such a card can cause excessive ventilation.Anyway, thank all of you for the help.PierreHi Grant, I will double-check the bios settings as soon as I’m back to the office. Thanks for the suggestion. PierreAre those cards Dell provided? I ran Connectx-3 and now Connectx-4 cards in a Dell r730xd. They were the standard Mellanox ones, non Dell provided. I would assume if they worked the Dell provided ones should also work. The only time i saw the fans run 100% all of the time was when I set the system performance profile to max. Setting it to OS or System controlled worked. Also disabling some of the cstates may have caused the issue as well. I would double check your bios settings, I believe if you follow the Mellanox tuning guide to a T you will end up with the fans at full blast.Hi,mget_temp is the utility to obtain device temperature. Typical range is 0-55C, for specific card you might check out website for thee documentation. Here is the example for ConnectX-3 http://www.mellanox.com/related-docs/user_manuals/ConnectX-3_Ethernet_Single_and_Dual_QSFP+_Port_Adapter_Card_User_Manua… http://www.mellanox.com/related-docs/user_manuals/ConnectX-3_Ethernet_Single_and_Dual_QSFP+_Port_Adapter_Card_User_Manual.pdfThanks Alkx for your answer, as you suggested I checked the range on the documentation of my card (Mellanox Products: ConnectX®-3 Single/Dual-Port Adapter with VPI http://www.mellanox.com/page/products_dyn?product_family=119&mtag=connectx_3_vpi ), you’re right the (operational) range is 0-55°C.If the utility “mget_temp” returns the device tempreature, it means that there is sensor for that, right ? In such a case, I would be surprised that Dell did’nt make use of this sensor to regulate the cooling of the server.With the 6 server fans at 17000 RPM (which means at 100%), the temperature returned by mget_temp is 45°C. I’m curious to know what would be the device tempreature if the server fans speed was less (ex.: 5000 RPM). But,unfortunately, I’m not able to deactivate the over-cooling triggered by this additional PCI card on the server, it’s always ON. If i was able to activate/deactivate this over-cooling, I could regulate it by myself according to the device temperature.Anyway, thanks again Alkx for you answer.Best regardsPierreHi Grant,Yes, I checked the FW also. Anyway, thanks for your help/advise. I will contact again Dell support to escalate the issue.Pierre please open case and escalate via Dell support. AFAIK if the card is indeed DELL branded it should have integration for fan control with the Dell server, as well as many other things. That’s one of the value add of Dell (and other server OEM) branded solutions.I have the same issue with my r730xd, where I put in a Mellanox ConnectX-3 card and the r730xd fans go to 98%.  Did Dell support give you anything to alleviate this?Have you taken it with Dell as suggested earlier on this thread?
They did reply the user who raised this case to the forum.From the following link:https://mymellanox.force.com/mellanoxcommunity/s/question/0D51T00006RVuctSAD/dell-pe-r730xd-fans-running-too-fast-because-of-connectx3-pcie-cardYou will see the resolution of this issue with the customer reporting Dell see this fan operation as normal with PCIe cards in the likes of ConnectX-3 on the rx730 platform.
Here’s the relevant excerpt (last post is the relevant one)[//cdck-file-uploads-global.s3.dualstack.us-west-2.amazonaws.com/nvidia/original/3X/2/b/2b9425e704e3616888f6cf3bc21a89b08fd2d5ec.png]Hi TomPIndeed, this is not considered a problem by Dell, and the server has
been running like this for over 5 years.Best regardsPierrePowered by Discourse, best viewed with JavaScript enabled"
936,innova-2-flex-for-ai,"Can anyone let me know if it is possible to implement any AI project (like Vitis AI or any other DPU) on Innova-2 Flex?Xilinx’s DPUs require an external memory interface (DDR or HBM) and are only available as Encrypted RTL so they cannot be altered.
DPUCZDX8G_Requires_External_DDR_Memory542×578 13.2 KB
I have not had the time to finish it but I started adding support for the Innova-2 to the vivado-risc-v project. It includes the ability to generate RISC-V cores with the Gemmini Accelerator. It should be possible to fit a very small model and bare-metal RISC-V code within the XCKU15P’s URAM. The XCKU15P has 70.6Mbit~=8Mbytes of URAM but you will at most be able to cascade one of the four columns. With some effort you could create a proof-of-concept.For the limited resources of the MNV303611A-EDLT I recommend you explore small embedded projects and older Neural Network to FPGA mapping research: 1, 2, 3, 4.Can Pytorch and Transformers run on Gemmini?
Is it possible to run Gemmini on MNV303611A-EDLT with CPU and RAM on the localhost instead of the RISC-V?Can Pytorch and Transformers run on Gemmini?Yes for inference. See also 1, 2.run Gemmini … instead of the RISC-V?Gemmini is an accelerator for RISC-V. It gets implemented as part of your RISC-V core and relies on it.Is it possible to run Gemmini on MNV303611A-EDLT with CPU and RAM on the localhostGood question. Gemmini relies on its processor’s internals through the RoCC interface, see Pg40. If throughput and latency are not an issue then implementing a RoCC-to-AXI bridge and a customized XDMA driver and software should allow testing RoCC accelerators using the MNV303611A-EDLT. I believe this does not exist as implementing a full system in an FPGA with memory allows for better performance estimates.Machine learning relies heavily on memory performance so all the latest accelerators depend on High-Bandwidth Memory (HBM).If you were thinking of a simpler system that connects host system memory to a RISC-V with Gemmini, note that PCIe is host-centric. Host software would need to manage the RISC-V’s accesses to memory. On the other hand, here is a post that suggests user logic can initiate transfers.Your best bet for a proof-of-concept project on the MNV303611A-EDLT is a small model that would also work on a microcontroller. Tensorflow-Lite for Microcontrollers supports RISC-V:Infrastructure to enable deployment of ML models to low-power resource-constrained embedded targets (including microcontrollers and digital signal processors). - GitHub - tensorflow/tflite-micro: I...The XCKU15P has 128 blocks of (4Kx72-Bit = 288Kbit) UltraRAM for 36Mbit total:Opening up Implementation View for a project, it turns out all of it is in a single column, X0:
XCKU15P_Implementation_View268×746 10.2 KB
That means it should be possible to cascade all of it for a total of 36Mbit=4.5Mbytes.Some example projects I came across: 1, 2, 3If using ucb-bar/chipyard, there is an error with ./build-setup.sh riscv-tools for 1.9.x. Try v1.8.1,I have successfully built Gemmini hardware. Now I want to put it in Vivado to see the block design. Can you guide me do that?Your question is too vague. Be specific about what you have done.Gemmini is an accelerator add-on for processors. Did you build a RISC-V core with a Gemmini Accelerator? Which framework did you use?If I were to attempt this I would first edit an existing example design for the MNV303611A-EDLT. My first goal would be the ability to communicate via JTAG or some other debugger interface with a RISC-V core. chipyard has a demo for the Arty. My next goal would be to run a bare-metal demo on the RISC-V core implemented in the FPGA. Only then would I attempt to rebuild the design to include a Gemmini Accelerator.I have a partially working RISC-V design for the MNV303212A-ADLT you could attempt to re-target. source the design in Vivado and replace the DDR4 with a Block Memory Generator. 2MB will work. If you succeed, you will later be able to generate a RISC-V+Gemmini system using vivado-risc-v.put it in Vivado to see the block designLook into creating custom IP blocks and creating custom AXI peripherals.I want to build an FPGA project that accelerates a large language model which requires more than 64GB of RAM. Since no FPGA board has that much RAM, I think I should use the RAM and CPU on the host and interact with Gemmini on FPGA through the RoCC interface. I have successfully built Gemmini hardware using this command:and failed to create a Vivado IP with the following steps:Vivado > Tools > Create and Package New IP > Package a specified directory > chipyard/generators/gemmini/generated-src/verilator/chipyard.TestHarness.CustomGemminiSoCConfig/gen-collateralrequires more than 64GB of RAM … no FPGA boardThe Alveo U200 has 64GB of DDR4 and DPUCADF8H supports it. Also, AWS F1 Instances.I think I should use the RAM and CPU on the host and
interact with Gemmini on FPGA through the RoCC interfaceWhen you tested the demo project you got complete transfer times on the order of 100,000 nanoseconds.This is due to the latency of PCIe and software/driver overhead. PCIe bandwidth is high but latencies are not great. DDR4 has complete transfer times on the order of 100ns. Your system will be very slow.I have successfully built Gemmini hardware using this command:You built a Gemmini system for the Verilator simulator. This is a much better idea than trying to use the MNV303611A-EDLT. Simulate your software running on RISC-V+Gemmini. Simulation should be the first step in hardware design.I got some information when simulating the software:This is a ucb-bar/chipyard Issue. Try running their Docker Image.I want to build an FPGA project that accelerates a large
language model which requires more than 64GB of RAM.Attempting to run an LLM faster than on a CPU while using the MNV303611A-EDLT would be a massive project and the likelihood of success is low.Your board shows up as PCIe device 17:00 and you mention 64GB so you are probably using a server with lots of memory. Consider quantizing the model and figuring out how to speed it up using CPU Optimizations.The MNV303611A-EDLT can help you design a proof-of-concept or prototype but I do not see how you can accelerate an LLM with it. If CPU performance is not good enough you should explore GPUs first before trying to use FPGAs.Can you give me an example of C code that waits for a result of computing on the FPGA after sending it input data?Can you give me an example of C code that waits for a
result of computing on the FPGA after sending it input data?How this should be achieved will depend heavily on the design of your system. Eventually you should use XDMA Interrupts but as a first step to test your ideas, try busy-waiting.Please take a step back and try running a simple test. The innova2_mnv303611a_xcku15p_xdma demo project creates 4 sets of files to access the design’s AXI Bus.Create two copies of the xdma_test.c demo and run each in a separate Terminal with a different set of /dev/xdma0_... files. Modify each to simulate a system where one thread writes to the AXI BRAM and the other waits for it to finish and then sends new data to the BRAM. Use a simple flag and look into locks, 1.Once you have that working, source the design in Vivado and modify it by adding a MicroBlaze soft processor without external memory. Connect it to the AXI BRAM and then write a simple bare-metal C program for it to test the concept. References: 1, 2, 3, 4.This may seem like a step back from your eventual goal but it will be a lot easier to find help for Vivado+MicroBlaze and all the concepts will apply later to getting RISC-V+Gemmini working.I am not building RISC-V+Gemmini. I am just building a floating-point computing module on FPGA.Modify each to simulate a system where one thread writes to the AXI BRAM and the other waits for it to finish and then sends new data to the BRAM. Use a simple flag and look into locks, 1.How can I access a flag of a C program from a separate Terminal?How can I access a flag of a C program from a separate Terminal?A flag is some location in memory you use to store the state of something. You arbitrarily decide on its location and value.Create two copies of xdma_test.c and come up with a simple communication protocol between them. Treat them as separate threads. For example, replace the AXI GPIO code sections with:xdma_test1.c:xdma_test2.c:Then run each program in a separate Terminal:Run_Multiple_XDMA_Test727×781 87.1 KBThe program running in one terminal busy-waits for the program running in the second to write a specific value to a specific location in the FPGA’s AXI BRAM.I am just building a floating-point computing module on FPGAHow you communicate with it will depend on how it is implemented.Here is a tutorial on adding FFT to a Vivado Block design. It is for an older version of Vivado so everything looks different but the key concepts are there and the code to communicate with the FFT Block is in Python.Powered by Discourse, best viewed with JavaScript enabled"
937,bluefield-2-network-performance-degradation,"We have 2 servers with bluefield2 installed and connected by a 100G switch through bf2’s p1 interface. The bluefield2 is set to work in Embedded CPU Function Ownership Mode. We tested the bandwidth of the link and thing goes well at the beginning as we could get a result of about 93Gbps.Then we tried VF QoS function mentioned in this tutorial on one of two Bluefield 2 (I’ll called it BF2-A in the following) and limited max egress rate of p1 and the other Bluefield 2 was untouched.  QoS works as we expected and the egress rate of p1 on BF2-A is limited by our setting. Then we removed all QoS related settings on BF2-A, and the problem happened: we tested bandwidth using iperf2 again but the maximum egress rate of p1 on BF2-A degraded to only about 50Gbps and the ingress rate is still about 93Gbps.We repeated the iperf test many times and the results were same. We thought that some QoS settings may not be removed completely so we reinstalled the BF2-A’s OS using BFB-install tool, but it didn’t help.Then we wanted to switch BF2-A to separated host mode to see if this helps and another weird thing happened: we can’t actually switch the mode. We followed the instruction from nvidia’s tutorial ‘Modes of Operation’ and after rebooting both the host server and BF2-A we can see BF2-A is in separated mode according to the output of mlxconfig q. But the behavior of BF2-A shows it’s still working in ECPF mode. For example, the statistic info (n_packets and n_bytes) in the output of ‘ovs-ofctl dump-flows’ is still growing if there is network traffic between two host servers.To make a summary:Is there any suggestion on this situation?Here is the output of ‘mlxconfig -d /dev/mst/mt41686_pciconf0.1 q’ (before switching to separate host mode):Device type:    BlueField2
Name:           MBF2M516A-CEEO_Ax_Bx
Description:    BlueField-2 E-Series DPU 100GbE Dual-Port QSFP56; PCIe Gen4 x16; Crypto Enabled; 16GB on-board DDR; 1GbE OOB management; FHHL
Device:         /dev/mst/mt41686_pciconf0.1Configurations:                              Next Boot
MEMIC_BAR_SIZE                      0
MEMIC_SIZE_LIMIT                    _256KB(1)
HOST_CHAINING_MODE                  DISABLED(0)
HOST_CHAINING_CACHE_DISABLE         False(0)
HOST_CHAINING_DESCRIPTORS           Array[0…7]
HOST_CHAINING_TOTAL_BUFFER_SIZE     Array[0…7]
INTERNAL_CPU_MODEL                  EMBEDDED_CPU(1)
FLEX_PARSER_PROFILE_ENABLE          0
PROG_PARSE_GRAPH                    False(0)
FLEX_IPV4_OVER_VXLAN_PORT           0
ROCE_NEXT_PROTOCOL                  254
ESWITCH_HAIRPIN_DESCRIPTORS         Array[0…7]
ESWITCH_HAIRPIN_TOT_BUFFER_SIZE     Array[0…7]
PF_BAR2_SIZE                        3
PF_NUM_OF_VF_VALID                  False(0)
NON_PREFETCHABLE_PF_BAR             False(0)
VF_VPD_ENABLE                       False(0)
PF_NUM_PF_MSIX_VALID                False(0)
PER_PF_NUM_SF                       True(1)
STRICT_VF_MSIX_NUM                  False(0)
VF_NODNIC_ENABLE                    False(0)
NUM_PF_MSIX_VALID                   True(1)
NUM_OF_VFS                          125
NUM_OF_PF                           2
PF_BAR2_ENABLE                      False(0)
HIDE_PORT2_PF                       False(0)
SRIOV_EN                            True(1)
PF_LOG_BAR_SIZE                     5
VF_LOG_BAR_SIZE                     1
NUM_PF_MSIX                         63
NUM_VF_MSIX                         11
INT_LOG_MAX_PAYLOAD_SIZE            AUTOMATIC(0)
PCIE_CREDIT_TOKEN_TIMEOUT           0
LAG_RESOURCE_ALLOCATION             DEVICE_DEFAULT(0)
PHY_COUNT_LINK_UP_DELAY             DELAY_NONE(0)
ACCURATE_TX_SCHEDULER               False(0)
PARTIAL_RESET_EN                    False(0)
RESET_WITH_HOST_ON_ERRORS           False(0)
NVME_EMULATION_ENABLE               False(0)
NVME_EMULATION_NUM_VF               0
NVME_EMULATION_NUM_PF               1
NVME_EMULATION_VENDOR_ID            5555
NVME_EMULATION_DEVICE_ID            24577
NVME_EMULATION_CLASS_CODE           67586
NVME_EMULATION_REVISION_ID          0
NVME_EMULATION_SUBSYSTEM_VENDOR_ID  0
NVME_EMULATION_SUBSYSTEM_ID         0
NVME_EMULATION_NUM_MSIX             0
PCI_SWITCH_EMULATION_NUM_PORT       0
PCI_SWITCH_EMULATION_ENABLE         False(0)
VIRTIO_NET_EMULATION_ENABLE         False(0)
VIRTIO_NET_EMULATION_NUM_VF         0
VIRTIO_NET_EMULATION_NUM_PF         0
VIRTIO_NET_EMU_SUBSYSTEM_VENDOR_ID  6900
VIRTIO_NET_EMULATION_SUBSYSTEM_ID   1
VIRTIO_NET_EMULATION_NUM_MSIX       2
VIRTIO_BLK_EMULATION_ENABLE         False(0)
VIRTIO_BLK_EMULATION_NUM_VF         0
VIRTIO_BLK_EMULATION_NUM_PF         0
VIRTIO_BLK_EMU_SUBSYSTEM_VENDOR_ID  6900
VIRTIO_BLK_EMULATION_SUBSYSTEM_ID   2
VIRTIO_BLK_EMULATION_NUM_MSIX       2
PCI_DOWNSTREAM_PORT_OWNER           Array[0…15]
CQE_COMPRESSION                     BALANCED(0)
IP_OVER_VXLAN_EN                    False(0)
MKEY_BY_NAME                        False(0)
PRIO_TAG_REQUIRED_EN                False(0)
UCTX_EN                             True(1)
REAL_TIME_CLOCK_ENABLE              False(0)
RDMA_SELECTIVE_REPEAT_EN            False(0)
PCI_ATOMIC_MODE                     PCI_ATOMIC_DISABLED_EXT_ATOMIC_ENABLED(0)
TUNNEL_ECN_COPY_DISABLE             False(0)
LRO_LOG_TIMEOUT0                    6
LRO_LOG_TIMEOUT1                    7
LRO_LOG_TIMEOUT2                    8
LRO_LOG_TIMEOUT3                    13
LOG_TX_PSN_WINDOW                   7
LOG_MAX_OUTSTANDING_WQE             7
TUNNEL_IP_PROTO_ENTROPY_DISABLE     False(0)
ICM_CACHE_MODE                      DEVICE_DEFAULT(0)
TLS_OPTIMIZE                        False(0)
TX_SCHEDULER_BURST                  0
ZERO_TOUCH_TUNING_ENABLE            False(0)
ROCE_CC_LEGACY_DCQCN                True(1)
LOG_MAX_QUEUE                       17
LOG_DCR_HASH_TABLE_SIZE             11
DCR_LIFO_SIZE                       16384
ROCE_CC_PRIO_MASK_P1                255
ROCE_CC_PRIO_MASK_P2                255
CLAMP_TGT_RATE_AFTER_TIME_INC_P1    True(1)
CLAMP_TGT_RATE_P1                   False(0)
RPG_TIME_RESET_P1                   300
RPG_BYTE_RESET_P1                   32767
RPG_THRESHOLD_P1                    1
RPG_MAX_RATE_P1                     0
RPG_AI_RATE_P1                      5
RPG_HAI_RATE_P1                     50
RPG_GD_P1                           11
RPG_MIN_DEC_FAC_P1                  50
RPG_MIN_RATE_P1                     1
RATE_TO_SET_ON_FIRST_CNP_P1         0
DCE_TCP_G_P1                        1019
DCE_TCP_RTT_P1                      1
RATE_REDUCE_MONITOR_PERIOD_P1       4
INITIAL_ALPHA_VALUE_P1              1023
MIN_TIME_BETWEEN_CNPS_P1            4
CNP_802P_PRIO_P1                    6
CNP_DSCP_P1                         48
CLAMP_TGT_RATE_AFTER_TIME_INC_P2    True(1)
CLAMP_TGT_RATE_P2                   False(0)
RPG_TIME_RESET_P2                   300
RPG_BYTE_RESET_P2                   32767
RPG_THRESHOLD_P2                    1
RPG_MAX_RATE_P2                     0
RPG_AI_RATE_P2                      5
RPG_HAI_RATE_P2                     50
RPG_GD_P2                           11
RPG_MIN_DEC_FAC_P2                  50
RPG_MIN_RATE_P2                     1
RATE_TO_SET_ON_FIRST_CNP_P2         0
DCE_TCP_G_P2                        1019
DCE_TCP_RTT_P2                      1
RATE_REDUCE_MONITOR_PERIOD_P2       4
INITIAL_ALPHA_VALUE_P2              1023
MIN_TIME_BETWEEN_CNPS_P2            4
CNP_802P_PRIO_P2                    6
CNP_DSCP_P2                         48
LLDP_NB_DCBX_P1                     False(0)
LLDP_NB_RX_MODE_P1                  OFF(0)
LLDP_NB_TX_MODE_P1                  OFF(0)
LLDP_NB_DCBX_P2                     False(0)
LLDP_NB_RX_MODE_P2                  OFF(0)
LLDP_NB_TX_MODE_P2                  OFF(0)
DCBX_IEEE_P1                        True(1)
DCBX_CEE_P1                         True(1)
DCBX_WILLING_P1                     True(1)
DCBX_IEEE_P2                        True(1)
DCBX_CEE_P2                         True(1)
DCBX_WILLING_P2                     True(1)
KEEP_ETH_LINK_UP_P1                 True(1)
KEEP_IB_LINK_UP_P1                  False(0)
KEEP_LINK_UP_ON_BOOT_P1             False(0)
KEEP_LINK_UP_ON_STANDBY_P1          False(0)
DO_NOT_CLEAR_PORT_STATS_P1          False(0)
AUTO_POWER_SAVE_LINK_DOWN_P1        False(0)
KEEP_ETH_LINK_UP_P2                 True(1)
KEEP_IB_LINK_UP_P2                  False(0)
KEEP_LINK_UP_ON_BOOT_P2             False(0)
KEEP_LINK_UP_ON_STANDBY_P2          False(0)
DO_NOT_CLEAR_PORT_STATS_P2          False(0)
AUTO_POWER_SAVE_LINK_DOWN_P2        False(0)
NUM_OF_VL_P1                        _4_VLs(3)
NUM_OF_TC_P1                        _8_TCs(0)
NUM_OF_PFC_P1                       8
VL15_BUFFER_SIZE_P1                 0
NUM_OF_VL_P2                        _4_VLs(3)
NUM_OF_TC_P2                        _8_TCs(0)
NUM_OF_PFC_P2                       8
VL15_BUFFER_SIZE_P2                 0
DUP_MAC_ACTION_P1                   LAST_CFG(0)
MPFS_MC_LOOPBACK_DISABLE_P1         False(0)
MPFS_UC_LOOPBACK_DISABLE_P1         False(0)
UNKNOWN_UPLINK_MAC_FLOOD_P1         False(0)
SRIOV_IB_ROUTING_MODE_P1            LID(1)
IB_ROUTING_MODE_P1                  LID(1)
DUP_MAC_ACTION_P2                   LAST_CFG(0)
MPFS_MC_LOOPBACK_DISABLE_P2         False(0)
MPFS_UC_LOOPBACK_DISABLE_P2         False(0)
UNKNOWN_UPLINK_MAC_FLOOD_P2         False(0)
SRIOV_IB_ROUTING_MODE_P2            LID(1)
IB_ROUTING_MODE_P2                  LID(1)
PF_TOTAL_SF                         200
PF_SF_BAR_SIZE                      10
PF_NUM_PF_MSIX                      63
ROCE_CONTROL                        ROCE_ENABLE(2)
PCI_WR_ORDERING                     per_mkey(0)
MULTI_PORT_VHCA_EN                  False(0)
ECPF_ESWITCH_MANAGER                ECPF(1)
ECPF_PAGE_SUPPLIER                  ECPF(1)
PORT_OWNER                          True(1)
ALLOW_RD_COUNTERS                   True(1)
RENEG_ON_CHANGE                     True(1)
TRACER_ENABLE                       True(1)
IP_VER                              IPv4(0)
BOOT_UNDI_NETWORK_WAIT              0
UEFI_HII_EN                         True(1)
BOOT_DBG_LOG                        False(0)
UEFI_LOGS                           DISABLED(0)
BOOT_VLAN                           1
LEGACY_BOOT_PROTOCOL                PXE(1)
BOOT_RETRY_CNT                      NONE(0)
BOOT_INTERRUPT_DIS                  False(0)
BOOT_LACP_DIS                       True(1)
BOOT_VLAN_EN                        False(0)
BOOT_PKEY                           0
P2P_ORDERING_MODE                   DEVICE_DEFAULT(0)
EXP_ROM_VIRTIO_NET_PXE_ENABLE       False(0)
EXP_ROM_VIRTIO_NET_UEFI_x86_ENABLE  False(0)
EXP_ROM_VIRTIO_BLK_UEFI_x86_ENABLE  False(0)
EXP_ROM_NVME_UEFI_x86_ENABLE        True(1)
ATS_ENABLED                         False(0)
DYNAMIC_VF_MSIX_TABLE               False(0)
EXP_ROM_UEFI_ARM_ENABLE             True(1)
EXP_ROM_UEFI_x86_ENABLE             True(1)
EXP_ROM_PXE_ENABLE                  True(1)
ADVANCED_PCI_SETTINGS               False(0)
SAFE_MODE_THRESHOLD                 10
SAFE_MODE_ENABLE                    True(1)Hi,Regarding the QoS settings. Actually it requires a more detailed investigation what exactly was configured, and how you reverted the configuration. In this situation we usually ask to open a support case in Nvidia portal.
In any case, all the sysfs configurations are only in runtime, and don’t survive a reboot, so to make sure that all of them reverted to the default, you can just reboot the DPU system.Regarding changing from EMBEDDED to SEPARATED mode. After such change you may need to perform a cold boot of the server in order to make it working.i.e.
This is the server where SEPARATED mode was configured, but will take effect only after the cold boot of the server:[root@l-csi-rivermax-04 ~]# mlxconfig -d 33:00.0 -e q | grep -e ^Configurations -e INTERNAL_CPU_MODEL
Configurations:                                                          Default                 Current                    Next Boot
INTERNAL_CPU_MODEL                  EMBEDDED_CPU(1) EMBEDDED_CPU(1) EMBEDDED_CPU(1)
[root@l-csi-rivermax-04 ~]# mlxconfig -d 33:00.0 s INTERNAL_CPU_MODEL=0Device type:    BlueField2
Name:           MBF2H516A-CENO_Ax_Bx
Description:    BlueField-2 DPU 100GbE Dual-Port QSFP56; PCIe Gen4 x16; Crypto Disabled; 16GB on-board DDR; 1GbE OOB management; FHHL
Device:         33:00.0Configurations:                              Next Boot       New
INTERNAL_CPU_MODEL                  EMBEDDED_CPU(1) SEPARATED_HOST(0)Apply new Configuration? (y/n) [n] : y
Applying… Done!
-I- Please reboot machine to load new configurations.
[root@l-csi-rivermax-04 ~]# mlxconfig -d 33:00.0 -e q | grep -e ^Configurations -e INTERNAL_CPU_MODEL
Configurations:                                               Default                         Current                                  Next Boot[root@l-csi-rivermax-04 ~]#Best Regards,
AnatolyPowered by Discourse, best viewed with JavaScript enabled"
938,hi-there-im-new-in-cumulus-and-need-to-configure-access-with-user-the-switch-has-access-via-private-key-and-work-nice-but-we-want-to-have-an-alternative-way-to-access-the-switch-i-seek-docs-but-dont-see-anything-concrete,"I also want to “activate” if possible, the “web access” but without make it.Anyone helps me?ThanksHello IsraelIf you are running cumulus OS, once you login with a private key, you should be able to create a user and add it to the sudoers group and depending on NCLU permission level, to the netshow (no changes) and netedit (changed permitted) groups. This is the standard debian (Linux) method to add usersPlease see the following documentationhttps://docs.nvidia.com/networking-ethernet-software/cumulus-linux-42/System-Configuration/Authentication-Authorization-and-Accounting/User-Accounts/https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-42/System-Configuration/Authentication-Authorization-and-Accounting/Using-sudo-to-Delegate-Privileges/https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-42/System-Configuration/Authentication-Authorization-and-Accounting/LDAP-Authentication-and-Authorization/#ncluHere is a sample from google on creating a user and adding it to sudoers and/or other groupsOur easy guide will show you three simple ways to create a new user with sudo access on Debian. Learn add user, create user, create group & more!
Est. reading time: 4 minutes

Screenshot_11111×637 79.3 KB
Thanks for the answer, Adolfo,I think there is the user “cumulus”, but when login the switch tells me that the “method” of access is with the private key. Maybe has not enabled both authentications methods.In the config I see : cumulus no password or anything elsePowered by Discourse, best viewed with JavaScript enabled"
939,when-should-i-use-the-admin-ui-to-upgrade-netq,"You can use the NetQ admin UI to upgrade NetQ only from versions 3.1.1 and later. To upgrade from older versions, you need to use the CLI.Powered by Discourse, best viewed with JavaScript enabled"
940,416-port-nonblocking-architecture,"I am suggesting 52 node of GPU Server to the customer.(1ea GPU have 8 HDR port)so i decide to use 24 leaf switch but have question about spine switchIs it necessary use 20 spine switch?
To many ports are not using…
Cant i design it with less spine switch?? if i use less spine switch would it have a problem with performance??
(using 12 spine switch)Hello,It is ok to use less spine switches without any issues.
12 spine switches should do the jobPowered by Discourse, best viewed with JavaScript enabled"
941,priority-trust-mode-is-not-supported-on-your-system,"Hello, I met a problem when I set the trust-mode for the ConnectX3-Pro 40GbE NIC.The system information follows:LSB Version: :core-4.1-amd64:core-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core)Release: 7.3.1611Codename: CoreThe ConnectX3-Pro NIC information follows:hca_id: mlx4_1transport: InfiniBand (0)fw_ver: 2.40.7000node_guid: f452:1403:0095:2280sys_image_guid: f452:1403:0095:2280vendor_id: 0x02c9vendor_part_id: 4103hw_ver: 0x0board_id: MT_1090111023phys_port_cnt: 2Device ports:port: 1state: PORT_ACTIVE (4)max_mtu: 4096 (5)active_mtu: 1024 (3)sm_lid: 0port_lid: 0port_lmc: 0x00link_layer: Ethernetport: 2state: PORT_DOWN (1)max_mtu: 4096 (5)active_mtu: 1024 (3)sm_lid: 0port_lid: 0port_lmc: 0x00link_layer: EthernetIt’s the first time that I have met this problem. So I don’t know what to do.What does this tip mean? Is the system version the main cause?Waiting for your help.Thanks.Hi,Can you give more details on what you tried and what did you use ?ThanksMarcHi,What is your current system ? Distribution / KernelConnectX-3 ProFW version ?PSID ?Can you try with the latest Mellanox OFED 4.4 ?Maybe the p4p1 is not the mellanox interface ?Maybe it is not configured as ethernet interface ?Please checkMarcHi, Marc.The system information is as following:The RNIC information is as following:
590×537 103 KB
And I upgrade the OFED to 4.4, the result is the same.I have checked the interface parameter and it’s right.Thanks.Hi,After a first check on my card ConnectX-3, I got the same behaviorIt seems to be supported only from ConnectX-4 and above.If you want me to investigate it more, please open a ticket.Buffers commands are not supported on your systemMarcHiCan you show me ibdev2netdev outputCan u also trymlnx_qos -i ThanksMarcHi, Marc.I installed MLNX_OFED_LINUX-4.1-1.0.2.0 on my server and used the provided tool “mlnx_qos” to set the trust mode for Connect-X 3 Pro.The command is “mlnx_qos -i p4p1 --trust=dscp”.Then the result is “Priority trust mode is not supported on your system”.ThanksThanks for your help!Hi, MarcAh…Sorry, after upgrading OFED to 4.4, the trust mode can be set on RNIC.But there is another error message: “Buffers commands are not supported on your system”.
520×616 58.1 KB
Thanks.Hi,Can you make the try to modify the buffer size and send me the output.ibdev2devnet also , please.MarcHi, MarcThe result is as following:It seems that the buffer commands is a new feature for PFC in OFED 4.4.I checked the OFED 4.3 and didn’t see this option for mlnx_qos.Thanks.Hello, I encountered the same problem as the picture show, what should I do to fix it? I have read this post, but I don’t find a certain solution😥Powered by Discourse, best viewed with JavaScript enabled"
942,lts-ofed-support-for-nfsordma-on-lts-ubuntu,"Hello,I manage a small compute cluster that uses Mellanox ConnectX 3 InfiniBand Cards and runs Ubuntu.Mellanox OFED driver support for ConnectX 3/ 3 Pro cards is now limited to the LTS drivers. While NFSoRDMA is supported in the Mellanox OFED LTS driver for Ubuntu, it is limited to the 18.04 LTS version.My question is can support for NFSoRDMA be added to the Mellanox OFED LTS driver for the most recent LTS release of Ubuntu vice the 2 year old version. I would like to update our cluster for security reasons but support for NFSoRDMA and the OFED driver is the limitation preventing this. We would use the inbox driver but performance benchmarks show the Mellanox driver is superior for our HPC workloads.Thank you!Hi Aaron,NFSoRDMA is officially supported in MLNX_OFED LTS 4.9-2.2.4.0 only on Ubuntu 18.04.3.Currently there’s no plan to add NFSoRDMA support on additional Ubuntu releases.I’ll keep you informed of any changes.Regards,ChenHello Chen,Thank you for the response.I do not understand Mellanox reasoning here.The 5.1x OFED drivers support NFSoRDMA on Ubuntu 20.04. Had the choice to move support for ConnectX 3 hardware to the LTS driver come two releases later (some number of months), your ConnectX 3 customers would have support for Ubuntu 20.04.Ubuntu is the only nonEnterprise distribution for which NFSoRDMA support is provided. It seems reasonable for Mellanox to support major performance features, such as NFSoRDMA, for major releases of at least one OS distribution on Mellanox hardware until the EOS milestone is reached. This would allow for your customers to not have to compromise between performance and cyber security while using your hardware within its support timeline. In this case we are only talking a single additional version as the next LTS release, Ubuntu 22.04, would seem to be beyond the EOS milestone for ConnectX 3 hardware.I do not understand Mellanox reasoning here and wish you would reconsider. I will soon be forced to upgrade to 20.04 for security reasons (IT policy outside my control) and my users will lose performance compared to what they already have today. This is not for corner-case use either, performance of file transfers across the cluster affects many aspects of everyday use of the cluster for my users.Thank you,AaronHi Aaron,I understand. The next OFED LTS version should be released in a few months. I will forward your request to the relevant team to check if support for Ubuntu 20.4 can be added to this upcoming release.Regardless the list of supported OS versions that is mentioned in the release-notes, have you tried to use NFSoRDMA on Ubuntu 20.04 and saw any issue?Regards,ChenHello Chen,Thank you for the response.I think that the NFSoRDMA module would likely work fine for the most part. However, I cannot test this as the mlnxofedinstall script responds with the following when the installer is run with the --with-nfsrdma option, “WARNING: NFSoRDMA is not supported over kernel 5.4.0-51-generic, will continue installation without it”.I do not know how to force the installer to install the NFSoRDMA module.Thanks,AaronPowered by Discourse, best viewed with JavaScript enabled"
943,can-edr-cable-be-used-for-mt4103-connectx-3,"I have HP servers, and their HCA card is MT4103 - ConnectX-3, Firmware version: 2.33.5220.​Is it possible to connect to SX6036 IB Switch (9.3.1260) using EDR cable?​In ConnectX3-FW-2_33_5000-release_notes, there was no entry for EDR cables.​Thanks for letting me know.😉Hi Yongho Kim,You can refer following release note .: NIC-to-Switch Connectivity Matrixhttps://docs.mellanox.com/display/ConnectX6Firmwarev20311014/Firmware+Compatible+ProductsThanks,SuoHi Yongho Kim,​EDR cables are not able to be used to connect between ConnectX-3 and SX6036. FDR DAC or AOC cables are recommended to connect ConnectX-3 and SX6036. Please refer to the NIC-to-Switch Connectivity Matrix section of release notes.https://docs.mellanox.com/display/ConnectX6Firmwarev20311014/Firmware+Compatible+Products#FirmwareCompatibleProducts-NIC-to-SwitchConnectivityMatrixThanks,Yuying​Thank you for your answer!Thank you for your helpPowered by Discourse, best viewed with JavaScript enabled"
944,are-there-any-secrets-to-using-a-qsfp-sfp-adapter-setting-link-speed-is-not-enough,"I’m trying to use a qsfp → sfp adapter with a SN2100 switch and I can’t get the link to come up. Are there any secrets to this? According to the documentation I need to set the link speed on the switch, but that isn’t enough, the link doesn’t come up. I’m using the adapter to convert from 25G at the device, to in the 100G switch.Hello Simon,Thank you for posting your inquiry on the NVIDIA Networking Community.For converting from 100GbE to 25GbE, we only support the following QSA adapter in the SN2100 switch → MAM1Q00A-QSA28 → https://www.mellanox.com/related-docs/prod_cables/PB_MAM1Q00A-QSA28_QSFP28_to_SFP28_Adapter.pdfWe also recommend to upgrade the switch code to the latest version 3.9.2302.If you still experiencing issues with the us of the Mellanox QSA adapter, we recommend to open a NVIDIA Networking support ticket (valid support contract required) so we can better assist you. To open a ticket, please send an email to networking-support@nvidia.comThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
945,spectrum1-buffer-monitoring,"HiWhen using SN2010 with debian and switchdev, please suggest a few ideas for buffer monitoring.The mlxsw github wiki talks about ethtool statistics: tc_no_buffer_discard_uc_tc_X : The number of unicast packets with traffic class X dropped due to lack of shared buffer resources.I also found the ‘Devlink exporter’ section which exports packet drop events to Prometheus.But I can’t find a way to graph / query % of shared buffers in use.Advice and pointers please.You can check the pool occupancy of the shared buffer usage as described here:Contribute to Mellanox/mlxsw development by creating an account on GitHub.Hi, thanks.I gave that a read. Do you have suggestions for creating some aggregate numbers to record and graph for monitoring reasons?It seems there is a byte pool and a descriptor pool involved.Can I query the ASIC for something like:Of the byte pool 60% is used.
Of the descriptor pool 20% is used.Doing this per port and per pool seems complicated and not needed. This page seems to be about QoS - we’re not really interested in QoS, just buffers used while doing standard routing.Powered by Discourse, best viewed with JavaScript enabled"
946,are-there-any-compile-time-macros-that-show-the-ofed-version,"MLNX_OFED has many of its interfaces changed since v5.0 (namely, migrated to RDMA-Core). Some RDMA code written on OFED v4.x that uses ibv_exp_* interfaces will not compile on newer versions of OFED. I think it would be good if we can distinguish OFED versions with compile-time macros defined in some header files, however I can’t find any.
So are there any such macros? If not, is it possible (or unrecommended) to detect OFED version at compile time?Powered by Discourse, best viewed with JavaScript enabled"
947,cant-build-drivers-for-ofed-4-9-in-rhel-centos-8-6-with-4-18-0-372-19-1-el8-6-x86-64-kernel,"Hi,I’m trying to install Mellanox OFED 4.9-5.1.0.0 LTS on the latest kernel of RHEL/Centos 8.6 (more strictly, Rocky Linux). I see in the release notes that the last supported version is 4.18.0-372.9.1.el8.x86_64 but my latest update (8.5 → 8.6) seems to have skipped 4.18.0-372.9.1.el8_6.x86_64 entirely in favor of 4.18.0-372.19.1.el8_6.x86_64. Mellanox OFED was not previously installed.From what I can gather, the problem seems to be in some changes made to the kernel interface that conflict with some (now repeated) definitions in the drivers.Then in /tmp/MLNX_OFED_LINUX-4.9-5.1.0.0-4.18.0-372.19.1.el8_6.x86_64/mlnx_iso.66001_logs/mlnx_ofed_iso.66001.logIn the rpmbuild.log some relevant errorsmlnx-ofa_kernel-4.9.rpmbuild.log (880.1 KB)(I’ve uploaded the full log since it’s too long to properly abbreviate)
In the kernel-devel file /usr/src/kernels/4.18.0-372.19.1.el8_6.x86_64/include/linux/slab.h the following definitions seem to be new with respect to other kernels (I took a look at the same file in a machine with an older kernel):Finally, $build_dir/OFED_topdir/BUILD/mlnx-ofa_kernel-4.9/source/include/linux/mm.h contains a conflicting definition:Looking at the full rpmbuild.log the same thing seems to be happening with  kvcalloc, kvmalloc_array, kvmalloc_node , kvmalloc_array, etc. always in default/include/linux/slab.h and default/include/linux/mm.h.
I’m guessing it might be possible to adjust the compilation options to ignore local definitions and use the kernel’s, but:I would really appreciate if someone could point me in the right direction. I’m guessing this will be addressed in the next release, but it would be cleaner to be able to compile instead of trying to downgrade everything to 8.5 while waiting for the release.Regards,Joaquín Torres.HPC System Administrator.
Centro Atómico Constituyentes.
Comisión Nacional de Energía Atómica.
Villa Maipú. Buenos Aires, Argentina.Dear Mellanox support,holds also for OFED 5.4-3.4.0.0 from the LTS branch of MOFED.Cheers, PeterHello @torres2,I’ve reproduced the issue and tested different building parameters and I have no luck with solution.
Looks like the best direction now is to wait for a new MLNX OFED release, which should include support for the new kernels.If it works for you, you may temporary go back to 4.18.0-372.9.1.el8.x86_64 which is installed with Rocky 8.6 by default.
There is no issues with compiling driver for this kernel version.Btw, have you tried to install OFED via YUM from mounted ISO image as a local repository?
MLNX_OFED supports KMP within the same OS release and it should work without recompile.Regards,
VladislavUsing the yum local repo appears to have worked for now. Thanks!  Since the documentation specified that “unsupported kernels require rebuilding the drivers” I had assumed that both install methods were interchangeable, I just now noticed the comment noting KMP support. Is the script method preferred for some reason or is it just given more visibility for portability?I did, however, notice some erratic behavior trying this out: The installation process seemingly added kernel-core 4.18.0-372.9.1.el8_6 as a dependency (It wasn’t previously installed, my latest upgrade went from rev 8.5 with a 4.18.0-348.20.1.el8_5 kernel straight to 4.18.0-372.19.1.el8_6). This is listed under dnf/yum list kernel-core but not under dnf/yum list kernel, so I’m guessing not all kernel components were installed. This seemed OK until I rebooted the server and the booting process just crashed with a lot of “warning dracut-initqueue timeout”, multiple problems recognizing partitions and finally a dracut root shell. Rebooting was working fine previously.Apparently, the yum installation of kernel-core 4.18.0-372.9.1.el8_6 added a new boot option to grub and set it as default. When changing manually to 4.18.0-372.19.1.el8_6 the system booted correctly and Infiniband worked properly. I changed the default to  4.18.0-372.19.1.el8_6 withMy guess is, since only the kernel-core package was installed, the older kernel didn’t have all necessary components to properly boot the system and shouldn’t have been set to default. I don’t know much about KMP but I’m guessing the OFED packages still work using the old kernel-core as a dependency even when the currently loaded kernel is 4.18.0-372.19.1.el8_6.It worries me a bit that the installation broke the booting process initially, but for now it seems to be working. I’ll keep testing to see if everything is in order.Regards,
Joaquín.Hi @vkhomyakovDo you have an estimate for a new release for the 4.9.x LTS containing this fix? We recently needed to upgrade our Rocky 8 kernel to 4.18.0-372.26.1.el8_6.x86_64 in order to fix a kernel bug that prevented Intel MPI from workingI appreciate the workaround above, but given the clash in kernel function/data structures I’m hesitant to install a source code incompatible version of MLNX OFED on a production system.Hello @m.paceyIf you need to be sure about adding particular fix to the release I suggest you to open Support Case.
Without it we can’t guarantee any related improvements in LTS or non-LTS release.Regards,
VladislavHi @vkhomyakovThanks for the update. Can you advise on the best route to opening up a Support Case for this? I’ve tried the Support link on the driver page, but https://support.mellanox.com is currently unreachable as it’s serving an untrusted certificate@m.paceyPlease check do you have an access to https://nvid.nvidia.com
Also you may reach out to support by email: enterprisesupport@nvidia.comRegards,
VladislavThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
948,failed-to-create-qp-when-add-a-workq-to-a-ctx,"Hey guys. I try to build a benchmark on DOCA SDK and implement it mostly based on the provided samples dma_copy_host and dma_copy_dpu. However, I found a problem that when I try to use more than 4 threads in benchmark, the function doca_ctx_workq_add  will throw an error showing DMA work queue context unable to create QP. err=DOCA_ERROR_NO_MEMORY. However, when I use configuration like 4 clients and 2 threads each, the benchmark will not throw the error. It’s quite strange, so I’m writing for a help.
You can find my benchmark on github and I have checked that the DPU can create more than 4 QPs under a normal RDMA benchmark so I guess that the DPU works well.Also, here’s the output of ulimit -a on my DPU.Powered by Discourse, best viewed with JavaScript enabled"
949,release-notes-for-nvidia-bright-cluster-manager-9-2-10,"Release notes for Bright 9.2-10== General ==
=New Features==Improvements==Fixed Issues=== CMDaemon ==
=Improvements==Fixed Issues=== Bright View ==
=Fixed Issues=== Cluster Tools ==
=Improvements==Fixed Issues=== Machine Learning ==
=Fixed Issues=== cm-kubernetes-setup ==
=New Features==Improvements==Fixed Issues=== cm-scale ==
=Fixed Issues=== cmburn ==
=Improvements=== licensing ==
=Fixed Issues=== openpbs20 ==
=Fixed Issues=== openpbs22.05 ==
=Fixed Issues=== pbspro2021 ==
=Fixed Issues=== pbspro2022 ==
=Fixed Issues=== slurm ==
=New Features=== slurm22.05 ==
=Improvements=Powered by Discourse, best viewed with JavaScript enabled"
950,keep-getting-no-afi-safi-activated-for-peer-when-trying-to-test-cumulus-on-2-vm-instances,"No mater what i try I keep getting this issue.Current Setup…any advise would be appreciated.Powered by Discourse, best viewed with JavaScript enabled"
951,failure-while-installing-mlnx-ofed-linux-5-3-1-0-0-1-ubuntu18-04-x86-64-on-ubuntu-18-04-w-5-4-0-71-generic-hwe-perl-issue,"error running install::~/MLNX_OFED_LINUX-5.3-1.0.0.1-ubuntu18.04-x86_64$ sudo ./mlnxofedinstallfileparse(): need a valid pathname at ./mlnxofedinstall line 46.any help please? i have never run into this issue previously.Hi,I guess it is a perl issue, because I cannot see any problematic code at line 46 in the install script.Which version of perl do you use ?RegardsMarcPowered by Discourse, best viewed with JavaScript enabled"
952,bluefield2-dpu-unable-to-set-vfs-trusted-mode,"Hi,I would like to run the Application Recognition example from host side, but unable to set the VFs to be trusted per the  NVIDIA DOCA Virtual Functions User Guide.
The step#3 use the command to set VF trusted modemlxreg -d /dev/mst/mt41686_pciconf0 --reg_id 0xc007 --reg_len 0x40 --indexes “0x0.0:32=0x80000000” --yes --set “0x4.0:32=0x1”But I always got the error ME_ICMD_OPERATIONAL_ERROR.Also, I tried to se the trusted mode from host side after VFs created, per the
Single Root IO Virtualization (SR-IOV)
but also failed:My setup:Host OS: Ubuntu 20.04
DPU: Bluefield-2
DOCA: v1.2.1It would be very appreciated if  you can have the advice.Regards,
CalebHello,Based on the output provided, the mlxreg utility is unable to properly change the register value. Unfortunately, it will not be possible to determine the cause of the error returned with the information provided. Further investigation will be required.We would recommend opening a support case for further investigation of the issue. If you do not have a current support contract, please reach out to our Contracts team at networking-contracts@nvidia.comThank you,
-Nvidia Network SupportHi Hilaryn,Thanks a lot for pay attention on this ticket.Before I posted this issue on this forum, I had my colleague’s help to create the support ticket. But the Nvidia supporter insisted this issue is DOCA relative and I should move to this forum. The support ticket has been closed:
00984115 [DOCA] Unable to enable VF trusted modeSo, I would like close this ticket due to there is no answers.Regards,
CalebThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
953,windows-dpdk-bifurcation-support,"We have been looking into adding kernel bypass support for Mellanox NICs on Windows. Could you tell me the current state of DPDK support for Mellanox NICs on Windows, in particular is bifurcation supported? If this is supported could you let me know which NICs?Hello and thank you for contacting us.Here you can see which OS are supported under paragraph 1.6 Tested Platforms for the latest version 22.03:
http://doc.dpdk.org/guides/rel_notes/release_22_03.htmlUnfortunately, Windows OS isn’t in the list. On the other hand, there is a threat with install information for Windows. It means that you can install Windows but it is not supported because it hasn’t been fully tested. Probably in the future. You can monitor dpdk.org for future OS support.Here is the link how to install on Windows:
http://doc.dpdk.org/guides/windows_gsg/intro.htmlBest Regards,Yogev Petrov,
from Nvidia Support TeamPowered by Discourse, best viewed with JavaScript enabled"
954,a-question-about-the-design-of-doca-buf,"I am unclear about the purpose of the head and head_len members in struct doca_buf.  While the user can control the buffer size with data and data_len, it seems that head and head_len may be redundant. And I am curious as to why the user only needs to set the source buffer without setting a destination buffer. If head and head_len are used to control the range of doca_buf, and data and data_len control the data to be transferred, it is unclear to me why the user would not need to specify a destination buffer. And if I do set the destination buffer, the program might throw an error: “[DOCA] [ERR][DOCA DMA:1442]: CQ received for failed job: status=2, vendor error=104.”Powered by Discourse, best viewed with JavaScript enabled"
955,sriov-performance-vs-number-of-vfs-configured,"Hello,Recently I got information that number of configure VFs impacts performance.Specifically the situation was as follows.For simplicity reasons we are usually configuring max available VFs (63). But recently I was told (by people concerned with VNFs deployment) that due to performance reasons we should configure only the number of VFs that are actually use by VNF. Unfortunately I was not presented with any data (or documentation) to support their claim.I would be grateful for any data or pointing my to any documentation that supports (or disproves) the claim about performance impact.Thanks in advance,Regards,LukaszSimilar question here, if I configure NUM_OF_VFS with mstconfig to the maximum of the card, and create the right number of VFs I need with echo X > sriov_numvfs, any performance impact ?Powered by Discourse, best viewed with JavaScript enabled"
956,peer-ip-mismatch-between-clag-peers,"I am experiencing something odd on 5.4 with MLAG.  We have other switches in MLAG on 4.3 and even 3.7 ( don’t judge ) and in 5.4 I am seeing the message in the subject on the peer-ip when checking for consistency issues.  I DO NOT see this error when checking issues on leaf3-1 only on leaf3-2 but as you can see the config matches (see second image).
image911×275 29.5 KB

image698×274 10.5 KB
Interesting issue. Let me ping an MLAG developer to see if they might have any insights.Can you please share clagctl output. Do you see the bonds in protodown state?Powered by Discourse, best viewed with JavaScript enabled"
957,poor-performance-with-connectx5,"HiI have two SuperMicro servers running RHEL each with one of these cards:Device type:    ConnectX5
Name:           MCX516A-CCA_Ax
Description:    ConnectX-5 EN network interface card; 100GbE dual-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6Configured as LACP bond on the hosts with 40G optics:""Settings for bond0:
Supported ports: [  ]
Supported link modes:   Not reported
Supported pause frame use: No
Supports auto-negotiation: No
Supported FEC modes: Not reported
Advertised link modes:  Not reported
Advertised pause frame use: No
Advertised auto-negotiation: No
Advertised FEC modes: Not reported
Speed: 80000Mb/s
Duplex: Full
Auto-negotiation: off
Port: Other
PHYAD: 0
Transceiver: internal
Link detected: yes
""iperf3 -i 5 -s
iperf3 -i 5 -t 60 -c beast.drcmr""
[root@beauty ~]# iperf3 -i 5 -t 60 -c beast.drcmr
Connecting to host beast.drcmr, port 5201
[  5] local 172.21.15.51 port 36664 connected to 172.21.15.72 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-5.00   sec  2.04 GBytes  3.50 Gbits/sec    0   2.34 MBytes
[  5]   5.00-10.00  sec  2.06 GBytes  3.55 Gbits/sec    0   3.11 MBytes
[  5]  10.00-15.00  sec  2.06 GBytes  3.53 Gbits/sec    0   3.11 MBytes
[  5]  15.00-20.00  sec  1.96 GBytes  3.37 Gbits/sec    0   3.11 MBytes
[  5]  20.00-25.00  sec  1.96 GBytes  3.38 Gbits/sec    0   3.11 MBytes
[  5]  25.00-30.00  sec  1.96 GBytes  3.37 Gbits/sec    0   3.11 MBytes
[  5]  30.00-35.00  sec  1.96 GBytes  3.37 Gbits/sec    0   3.11 MBytes
[  5]  35.00-40.00  sec  1.93 GBytes  3.32 Gbits/sec    0   3.11 MBytes
[  5]  40.00-45.00  sec  2.01 GBytes  3.45 Gbits/sec    0   3.11 MBytes
[  5]  45.00-50.00  sec  2.00 GBytes  3.44 Gbits/sec    0   3.11 MBytes
[  5]  50.00-55.00  sec  1.98 GBytes  3.40 Gbits/sec    0   3.11 MBytes
[  5]  55.00-60.00  sec  1.96 GBytes  3.37 Gbits/sec    0   3.11 MBytes[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-60.00  sec  23.9 GBytes  3.42 Gbits/sec    0             sender
[  5]   0.00-60.04  sec  23.9 GBytes  3.42 Gbits/sec                  receiver
""Any suggestions as to why? Those numbers seem way off.Thanks.Mvh.TorkilHello Torkil,Thank you for posting your inquiry to the NVIDIA Developer Forums.We do not recommend using iperf3 for TCP benchmarking on Linux hosts.
iperf3 lacks several features that iperf2 contains, such as multithreading (and multicast test capabilities).
Multithreaded (parallel) testing (using multiple cores) are a much more realistic example of what you should expect for real-world throughput than single-stream, single-thread performance.A quick example of iperf2 testing, using 8 cores, can be found here:
https://enterprise-support.nvidia.com/s/article/howto-install-iperf-and-test-mellanox-adapters-performanceIf you are still experiencing lower-than-expected throughput while using iperf2, we would recommend reviewing our comprehensive host tuning guide, available here:
https://enterprise-support.nvidia.com/s/article/performance-tuning-for-mellanox-adaptersGeneral OS tuning guidelines can be found here, as well as Mellanox-specific tuning guidelines.
We also discuss the importance of NUMA-locality and provide instructions for pinning your applications to local CPU cores (https://enterprise-support.nvidia.com/s/article/understanding-numa-node-for-performance-benchmarks).If after following these guidelines you are still not able to reach line rate (or near line rate), and you have valid Enterprise support entitlement, we would recommend engaging our Enterprise support team via the NVIDIA Enterprise support portal (https://enterprise-support.nvidia.com/s/create-case).Thanks, and have a great day;
NVIDIA Enterprise SupportHiThanks for the links. Iperf2 does indeed show resonable numbers for all but 2 hosts.Can you also provide a link to the mlnx_tune script? I find it referenced a lot but with broken links.Mvh.TorkilHi Torkil,The mlnx_tune script is bundled with MLNX_OFED. This is our proprietary driver stack.You can also find mlnx_tune on the Mellanox userland tools and scripts GitHub:Mellanox userland tools and scripts. Contribute to Mellanox/mlnx-tools development by creating an account on GitHub.
(It’s within the Python directory)HTH,
NVIDIA Enterprise Supportwhat’s the windows version of mlx_tune?
i’m seeing 56gb/s on a 100gbe adapter with iperf2This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
958,how-to-generate-pfc-pause-packets-with-mlnx-cx-5-and-cx-6-dx-cards,"Hi,
I am using Mlnx CX-5 and CX-6 Dx cards in my lab. I have enabled PFC with 8 TCs and configured QoS with DSCP mode. I am using DPDK pktgen to generate packets with 100 Gbps (CX-6 Dx) per PHY port. I can see the receiver card is dropping the packets by seeing rx_discard packets counter.
I am assuming the PFC should be lossless for UDP data and whenever the Rx buffer fills, it should generate PFC pause frames to stop or pause data on the sender side but I can’t see this effect.
Can someone guide me how to setup Mlnx cards to work as lossless traffic device for UDP packets.ThanksHello,Welcome to the NVIDIA Developer forums. I am going to move your topic to the Networking category so the support team has visibility.Hi, Thanks for moving to the appropriate place. I would be highly thankful for the kind reply.RegardsSomeone from the Networking team will jump in here to help ASAP.Can you please provide the output of the following command from the interfaces used in the test:
mlnx_qos -i ethernet_interface
Do you have a switch in between the nodes or they are connected in back to back?
With what priority are you marking the outgoing traffic (DSCP value in the IP header)? You can use tcpdump to review if your are not sure.
Regards,
YanivThanks Yaniv,Here are the details:mlnx_qos -i 

MicrosoftTeams-image (4)1387×643 97.8 KB
No switch in between. Two ConnectX-6 Dx cards are connected back to back.Different DSCP values but one at a time. For example 0, 8, 16, 24, 32Additional info:
OS: CentOS 8.5
Latest Ofed versionSome more information:
After reboot, when QoS configurations are done, we see the PFC pause packets. But if we make any change to QoS configurations like UP-TC mapping, DSCP to UP or change buffer sharing, then we can’t receive any PFC pause packets.But still, if we analyze the data at the time of PFC pause packets, we are unable to stop completely the sending data. Rather I can see the sender server contineosly sends the data. When it receives the PFC pause packets, it reduces the bandwidth from 100Gbps to 24Gbps but can’t reach to 0.
As I have made changes to the Rx driver to do empty the Rx Queue, that’s why I am assuming that the data should not be lost. But I can see the Rx buffer or Queue drops the data which is not a lossless effect.Hope this will provide you enough information to analyze what happens.Hi,
In general we do not recommend touching the receive buffer on ConnectX-6 Dx devices. I can see how it might break the PFC.
I suggest you start simple with standard mlnx_qos config (in trust DSCP mdoe) and enabling one or more (NOT all) priorities with PFC. Do not change any buffer sizes and do not remap buffer and priorities. The (shared) buffer architecture in ConnectX-6 Dx should be able to handle that.
Your UP-TC mapping should be ok.
Regards,
YanivThanks Yaniv,I can see the same behaviour with default QoS settings. Here are the QoS configurationsHere are the ethtool stats on both servers

MicrosoftTeams-image (6)1326×819 98.1 KB
See ethtool statistics in below attached image.By seeing the PFC packets, it generates PFC packets with timeout 65355 and after some time generate the same packet with timeout 0. This sequence continues.The question I have is, why the Rx buffer drops data in PFC mode? Is there a fix timeout after which the Rx buffer become empty in the PFC mode or the data remain in the Rx buffer untill it is read by the Rx Queue?
MicrosoftTeams-image (6)1326×819 98.1 KB
In the data you provided I do not see any drops on the RX buffer.
In general, the way it works is that when we reach some threshold we will issue an xoff pause and on a different threshold we will issue an xon pause
I suggest that you approach technical support to review further (as more debug data is required).
Regards,
YanivThanks Yaniv,Can you tag the technical team here or assign them this ticket or guide me how to contact them please.Regards,
FarhatPlease send an email to enterprisesupport@nvidia.com with the details and they will guide you through the process.
Thanks,
YanivPowered by Discourse, best viewed with JavaScript enabled"
959,connectx-6-testpmd-no-probed-ethernet-devices,"Hi Team,
I am using DPDK 18.11 and Bluefield 2 indeed built with CONFIG_RTE_LIBRTE_MLX5_PMDwith DPDK 18.11  testpmd is unable to probe the devices but testpmd with DPDK 21.11.2 is working fine.
Below is the working version with DPDK 21.11.2is DPDK 18.11 is supported version for  ConnectX-6 ? If so can you provide some pointers to debug this issue further ?Hi ,
We can get which NIC supported from DPDK release notes.
https://doc.dpdk.org/guides-20.11/rel_notes/release_20_11.htmlTo support ConnectX-6 Dx, suggest at least using Dpdk19.11.
It’s better to use DPDK 20.11/above. Then make better use of NIC features.It’s a good start to tune NVIDIA Mellanox NIC with DPDK from “DPDK_20_11_Mellanox_NIC_performance_report”1436.03 KBwe can get BIOS/BOOT setting and other optimization suggestions from here.Regards,
Leveithank you Levei,  i also figured the same.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
960,logging-to-remote-syslog-server,"My syslog server isn’t seeing log messages.  I notice that source-interface isn’t set:aind-vast1-sw02 [ND1: master] (config) # sh logging source-interfaceVRF name: defaultSource IP for syslogd client:
Configured: none
Current   : none
IPv4-addr : none
IPv6-addr : noneaind-vast1-sw02 [ALLENINSTND1: master] (config)When I try to set source-interface, I see the following:
aind-vast1-sw02 [ND1: master] (config) # logging source-interface mgmt0% Only loopback interfaces supported (not mgmt0)
aind-vast1-sw02 [ND1: master] (config) #I am ssh’ed into the switch via interface mgmt0 … I would like syslog packets to exist the switch via mgmt0.  But if such packets must be sourced from a loopback address, how do I go about picking the IP address of the loopback interface?  Picking an address from the same subnet which mgmt0 is on isn’t working.  Do I pick some other subnet for the loopback IP address … but then how do I configure routing inside the Mellanox to permit the loopback address to talk across the mgmt0 interface?  Seems to me that I’m missing something;  this smells like a lot of work to get logging functioning.aind-vast1-sw02 [ND1: master] (config) # interface loopback 0 ip address a.b.c.d/32How might I go about picking values for ‘a.b.c.d’?  And what else do I need to configure for loopback 0 to then be able to forward syslog packets across mgmt0?–skOK, I got itlogging vrf mgmt {IP address of syslog server}No need for loopback source-interface at this point–skHello StuartYou can also find the user manual in this link. Page 343 onwards has logging documentation
https://support.mellanox.com/sfc/servlet.shepherd/version/download/0681T00000HRq7LQATThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
961,ibm-branded-connectx3-40-gbit-qsfp-mt26428-on-esxi-6-7-cu3,"Hi, I have a question about a Mellanox ConnectX3 Infiniband card.It appears that this card has the same hardware than a Mellanox ConnectX3 QSFP single port.ConnectX-3 Pro EN NIC; 40GigE; single-port QSFP (MCX313A-BCC)But VMware doesn’t load it, it only shows a PCI device.After lot of searching I found that the “VMware Inbox driver” seems not to load because IBM or Mellanox decided to give it a device ID which is not on the HCL of VMware.https://www.vmware.com/resources/compatibility/detail.php?deviceCategory=io&productid=44553&deviceCategory=io&details=1&partner=55&releases=485,428,427,369&keyword=connectx-3&deviceTypes=6&page=1&display_interval=10&sortColumn=Partner&sortOrder=Asc0x1007 would be appropriate for the original Mellanox product, but the card sold as IBM FRU 46M2205 shows the device ID 0x673c (also as sub device ID) but the vendor is 0x15b3 - correct.actually I don’t know the reason why the driver doesn’t load and the /var/syslog.log has tenthousands of entries during a boot… so my first question isSome days in the past I hacked inf files under Windows to forece a device driver to accept a different device ID, and that worked fine when it’s a scanner or printer who’se capabilites are identical but the manufacturer wants it’s customers to buy new hardware by not supporting the compatible device in it’s newest Windows 7 / 10 driver.And my second question:Apparently there is a custom build Lenovn ISO for ESXI 6.7 but I don’t really want to reinsatll my main Lab box… also it appears that Lenovo hasn’t considered ESXI 6.7 as OS for the IBM connectX3Abount my background… I do sort of tech support for performance troubleshooting in an CAE application that often runs against some bottlenecks in virtualized or physical environmnets, and want to rebuild some customer scenarios in a lab with sort of limited budget.ESXi I do for 20 years, more on top level but I am also aware on the command line and have Linux experience too (Redhat, Suse which shares lot of tools and structures with ESX)I studied lot of pages about Mellanox Infiniband in ESX and some people said OMG, pls don’t use the “foreign branded” cards for HP or IBM, take the original ones. Something I didn’t know before I got some IBM branded ConnectX3 (obviously the pro) in my hands.Hello Andreas,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately, it is not possible to change the device id of the adapter. As this is an IBM branded Mellanox OEM adapter, all the support needs to be provided by IBM.Thank you and regards,~NVIDIA Networking Technical Supportthanks for your answer… will try my best with Lenovo / IBM or find a way to make the inbox driver accept the OEM device ID.Powered by Discourse, best viewed with JavaScript enabled"
962,do-i-need-to-install-swagger-ui-to-use-it-with-the-netq-api,"If you are using NetQ in an on-premises deployment, the Swagger UI is already built in. Just enter /swagger after the hostname or IP address you use to access the NetQ UI. For example, https://10.0.1.2/swaggerHowever if you are using a cloud deployment, you need to install the Swagger UI. See the NetQ userr guide for details.Powered by Discourse, best viewed with JavaScript enabled"
963,asking-maximum-performance-of-mcx653106a-hdat-sp,"In case of connecting dual port of MCX653106A-HDAT-SP, I am wondering that 200Gb/s can be guaranteed for each port always.Since there is explanation on the webpage that up to 200Gb/s can be supported for each node, I am wondering that 400Gb/s can be supported with this card.Somehow I believe that 200Gb/s can only be supported, considering the maximum bandwidth(200Gb/s), but I need a clarification for this issue.Features(https://store.nvidia.com/en-us/networking/store/product/MCX653106A-HDAT-SP/nvidiamcx653106a-hdat-spconnectx-6vpiadaptercardhdr200gbe/)Thanks for your help.Hi Yeongmin Seo,This because this card use 16x pcie 4.0 , so it can’t support 400Gb/s performance .Thanks,Thanks for clarification.If so, I have another question for detail.If one port uses 10Gb/s, the others can only utilize 190Gb/s as maximum performance. Am I understanding in right way?​YesHi again,Is there any way that I can get an official letter or document that I can check with?I tried to persuade my customers about limitation of maximum performance, but they asked an official letter about the fact."" If one port uses 10Gb/s, the others can only utilize 190Gb/s as maximum performance. ""If there is any way, please let me know.Powered by Discourse, best viewed with JavaScript enabled"
964,connectx-5-vpi-adapter-cards,"Hi, I am new to RDMA and Mellanox equipment. Can anyone help me to confirm this thing, a connectX-5 VPI adapter card can be configured to work with InfiniBand and RoCE ? I currently  don’t have enough budget, so I am a bit worried about buying the wrong adapter card. Thanks.Hello Huangwentaochina,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Yes, you can change the port_type of the ConnectX-5 VPI to IB or Ethernet based on your evironment.You can use the following link as a reference on how-to change the port_type → Changing Mellanox ConnectX VPI Ports to Ethernet or InfiniBand in LinuxThe following link will give you a real life example → Changing Mellanox ConnectX VPI Ports to Ethernet or InfiniBand in LinuxThank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
965,use-cuda-dmabuf-is-not-supported-on-this-gpu,"Hey there,we are currently testing the perftools samples and trying to run GPUDirect RDMA. Besides the CLI option “–use_cuda” which works great, we tried additionally the CLI option “–use_cuda_dmabuf” but the following is returned after starting ib_send_bw:Two systems are P2P connected via ConnectX-4 NICs using RoCE. My questions are:We saw in perftools that the following line is checking a required attribute:
cuDeviceGetAttribute(&is_supported, CU_DEVICE_ATTRIBUTE_DMA_BUF_SUPPORTED, cuDevice)Can you please help and explain this CUDA attribute? I couldn’t find any more specific informations online about this. ThanksI appreciate any help or advice on this topic.So far I think I was able to explain to myself what DMA_BUF actually is.DMA_BUF allows to register memory regions directly on peer devices from the HCA perspective, e.g. for GPUDirect RDMA on VRAM of a GPU, instead of using the system memory. GPUDirect RDMA is a “closed” source solution, so one has access to this feature if all prerequisities are fulfilled (GPU, NIC, Software).From my understanding, this is identical to Device Memory Programming which should be supported for ConnectX-5 and above. If one wants to utilize GPUDirect RDMA (which is somehow based on DMA_BUF), one can even use lower types of ConnectX, e.g. ConnectX-4.What I still don’t understand is the line
cuDeviceGetAttribute(&is_supported, CU_DEVICE_ATTRIBUTE_DMA_BUF_SUPPORTED, cuDevice)
which checks the attribute of the GPU. We are using ConnectX-4, so I’m fine with this, that DMA_BUF may not work. But why does CUDA tell me, that my GPU is not supported, though it is a RTX A5000 which seems appropriate for all these features from my point of view?use_cuda_dmabufhi,
GPUDirect Over DMA-BUF is a new feature from OFED 5.8-1.0.1.1some information about this:
Added support for GPUDirect support over dma-buf. As such, using the new mechanism nv_peer_mem is no longer required.The following is required for dma-buf support:
Linux kernel version 5.12 or later
OpenRM version 515 or laterPerftest support was added as well:
Default option in perftest is without dmabuf. To run with this option, add --use_cuda_dmabuf in addition to use_cuda flag.Alright, I know 90% of all the informations you have shared with me, though the software requirement of OpenRM were new to me.Is there any way to check whether all requirements are fulfilled? The kernel version is okay, but I’m not sure about OpenRM. Is it included automatically in the CUDA driver?PS: It seems like OpenRM is part of the CUDA Toolkit.Powered by Discourse, best viewed with JavaScript enabled"
966,connectx-6-vpi-vs-connectx-6-de-product-differences,"Hi,I understand that the difference between the two products is VPI support.
I would like to know the features of the DE(MCX683105AN-HDAT) product, not the difference in the number of physical ports.Thanks.Hi Jayce,Thank you for posting your inquiry to the NVIDIA Developer Forums.The ConnectX-6 DE, aside from the absence of VPI (and consequently, Ethernet) functionality, has the same feature set and limitations as the ConnectX-6 VPI adapter.Please refer to the ConnectX-6 DE firmware release notes. The following section:
https://docs.nvidia.com/networking/display/ConnectX6DEFirmwarev22322306/Changes+and+New+Features
states that ""ConnectX-6 DE has the same feature set as ConnectX-6 adapter card. ""The ‘Known Issues’ section of these release notes contains the same statement, alongside descriptions of some ConnectX-6 DE specific issues: https://docs.nvidia.com/networking/display/ConnectX6DEFirmwarev22322306/Known+IssuesAdditional specifications can be found here:
https://docs.nvidia.com/networking/display/ConnectX6VPI/Introduction#Introduction-ConnectX-6DEPCIex16CardAnd here:
https://docs.nvidia.com/networking/display/ConnectX6VPI/Specifications#heading-MCX683105AN-HDATSpecificationsI do hope this is the information you were looking for.Please do reach out to our Sales team if you require more specific information, or need some assistance with matching our products to your needs - Our product catalog is extensive, but our Sales team has the resources and expertise behind them to help you navigate:Get your questions answered. Search for products or services by category and get routed to the correct customer support person.Thanks, and have a great weekend;
NVIDIA Enterprise SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
967,how-to-connect-mt25408a0-to-an-mqm8700-switch,"Hello,we have a bunch of old hardware, which still needs running (common scenario ;)
Last week the HPC cluster was rebuild - aka all machines out of the rack, some old ones to garbage, some old ones reused in the rack.Just the main facts in a few words, I’m eager to hear your tips and hints:Before:computenode with MT25408A0 was connected to an IB switch SX6012. worked fine.AfterWe now have a 40port HDR switch MQM8700 and the MT25408A0  device remains in state ‘Polling’Technical Details:[root@w6 ~]# lsb_release -d
Description:    CentOS Linux release 7.9.2009 (Core)[root@w6 ~]# rpm -qf /usr/sbin/ibstatus
infiniband-diags-2.1.0-1.el7.x86_64[root@w6 ~]# lspci -v
…
06:00.0 InfiniBand: Mellanox Technologies MT25408A0-FCC-QI ConnectX, Dual Port 40Gb/s InfiniBand / 10GigE Adapter IC with PCIe 2.0 x8 5.0GT/s In… (rev b0)
Subsystem: Mellanox Technologies MT25408A0-FCC-QI ConnectX, Dual Port 40Gb/s InfiniBand / 10GigE Adapter IC with PCIe 2.0 x8 5.0GT/s Interface
Flags: bus master, fast devsel, latency 0, IRQ 16, NUMA node 0
Memory at cf300000 (64-bit, non-prefetchable) [size=1M]
Memory at c2800000 (64-bit, prefetchable) [size=8M]
Capabilities: [40] Power Management version 3
Capabilities: [48] Vital Product Data
Capabilities: [9c] MSI-X: Enable+ Count=128 Masked-
Capabilities: [60] Express Endpoint, MSI 00
Capabilities: [100] Alternative Routing-ID Interpretation (ARI)
Capabilities: [148] Device Serial Number 00-02-c9-03-00-28-86-f6
Kernel driver in use: mlx4_core
Kernel modules: mlx4_core
…[root@w6 ~]# ibstatus| egrep ‘device|state|rate’; ibstat|egrep ‘CA|Firmware|Hardware|State|Rate’
Infiniband device ‘mlx4_0’ port 1 status:
state:           1: DOWN
phys state:      2: Polling
rate:            10 Gb/sec (4X)
CA ‘mlx4_0’
CA type: MT26428
Firmware version: 2.9.1000
Hardware version: b0
State: Down
Rate: 10If we connect the node to the old SX6012 switch, which is connected also with the new switch, it works. But we want to get rid of the old switch.So how could we proceed?Best regards
JoeThe devices are not tested together as the CX3 is very old.What’s is the cable used for this?
Can you please run mlxlink on the switch port with -m -e -c flags?It can be run inband with the switch device addressed by its lid.E.g.Mlxlink -d lid-23 -p 14 -m -e -cDid this from another machine connected:45:00.0/dev/mst/CA_MT26428_taede147_mlx4_0_lid-0x0001
/dev/mst/CA_MT26428_w1_mlx4_0_lid-0x0012
/dev/mst/CA_MT26428_w3_mlx4_0_lid-0x0013
/dev/mst/CA_MT4099_c25-ib0_mlx4_0_lid-0x0043
/dev/mst/CA_MT4099_c26-ib0_mlx4_0_lid-0x0009
/dev/mst/CA_MT4099_c27-ib0_mlx4_0_lid-0x0008
/dev/mst/CA_MT4099_c28-ib0_mlx4_0_lid-0x0002
/dev/mst/CA_MT4099_c29-ib0_mlx4_0_lid-0x0006
/dev/mst/CA_MT4099_c30-ib0_mlx4_0_lid-0x0004
/dev/mst/CA_MT4099_c31-ib0_mlx4_0_lid-0x0007
/dev/mst/CA_MT4099_c32-ib0_mlx4_0_lid-0x0005
/dev/mst/CA_MT4099_w10_mlx4_0_lid-0x0045
/dev/mst/CA_MT4099_w8_mlx4_0_lid-0x0014
/dev/mst/CA_MT4099_w9_mlx4_0_lid-0x0016
/dev/mst/CA_MT4123_c34_mlx5_0_lid-0x0017
/dev/mst/CA_MT4123_MT4123_ConnectX6___Mellanox_Technologies_lid-0x0011
/dev/mst/CA_MT53001_Mellanox_Technologies_Aggregation_Node_lid-0x0010
/dev/mst/SW_MT51000_switch-9c1bc6""_lid-0x0003
/dev/mst/SW_MT54000_switch-742018""_lid-0x0015CA_MT26428_taede147_mlx4_0_lid-0x0001,mlx4_0,1_cable
CA_MT26428_w1_mlx4_0_lid-0x0012,mlx4_0,1_cable
CA_MT4099_c26-ib0_mlx4_0_lid-0x0009,mlx4_0,1_cable
CA_MT4099_c27-ib0_mlx4_0_lid-0x0008,mlx4_0,1_cable
CA_MT4099_c28-ib0_mlx4_0_lid-0x0002,mlx4_0,1_cable
CA_MT4099_c29-ib0_mlx4_0_lid-0x0006,mlx4_0,1_cable
CA_MT4099_c30-ib0_mlx4_0_lid-0x0004,mlx4_0,1_cable
CA_MT4099_c31-ib0_mlx4_0_lid-0x0007,mlx4_0,1_cable
CA_MT4099_c32-ib0_mlx4_0_lid-0x0005,mlx4_0,1_cable
CA_MT4123_MT4123_ConnectX6___Mellanox_Technologies_lid-0x0011,mlx4_0,1_cable
SW_MT51000_switch-9c1bc6""_lid-0x0003,mlx4_0,1_cable_1
SW_MT51000_switch-9c1bc6""_lid-0x0003,mlx4_0,1_cable_11
SW_MT51000_switch-9c1bc6""_lid-0x0003,mlx4_0,1_cable_2
SW_MT51000_switch-9c1bc6""_lid-0x0003,mlx4_0,1_cable_4
SW_MT51000_switch-9c1bc6""_lid-0x0003,mlx4_0,1_cable_7That’s the MQM8700:
…
/dev/mst/SW_MT54000_switch-742018""_lid-0x0015
…The machine with MT25408A0 which isn’t working is on port 28:[root@w10 ~]# mlxlink -d lid-0x0015 -p 28 -m -e -c
ibwarn: [179208] _do_madrpc: recv failed: Connection timed out
ibwarn: [179208] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 21)
ibwarn: [179208] _do_madrpc: recv failed: Connection timed out
ibwarn: [179208] mad_rpc: _do_madrpc failed; dport (Lid 21)
ibwarn: [179208] _do_madrpc: recv failed: Connection timed out
ibwarn: [179208] mad_rpc_rmpp: _do_madrpc failed; dport (Lid 21)
-E- ibvsmad : cr access read to Lid 21 failed
FATAL - crspace read (0xf0014) failed: Invalid argument-E- Failed to read device IDLid requires decimal value… in your case it will be 21(0x15)Try lid-21.And if that isn’t working please try without any flags (no -m/e/c)ALSOIf this is a managed switch you can enter the CLI and run this from the fae menu (en – conf term – fae mlxlink…)Actually I’ve got the wrong lid.[root@w10 ~]# ibswitches
Switch  : 0x98039b0300ff9a80 ports 12 “MF0;switch-9c1bc6:SX6012/U1” enhanced port 0 lid 3 lmc 0
Switch  : 0x1070fd03008a0f9e ports 41 “MF0;switch-742018:MQM8700/U1” enhanced port 0 lid 15 lmc 0[root@w10 ~]# mlxlink -d lid-15 -p 28 -m -e -cState                           : Polling
Physical state            : ETH_AN_FSM_ENABLE
Speed                          : N/A
Width                          :N/A
FEC                              : N/A
Loopback Mode        : No Loopback
Auto Negotiation      : ONEnabled Link Speed              : 0x00000005 (QDR,SDR)
Supported Cable Speed           : 0x00000007 (QDR,DDR,SDR)Status Opcode                   : 2
Group Opcode                    : PHY FW
Recommendation              : Auto-negotiation no partner detected.Firmware Version                : 27.2010.1202
amBER Version                   : 1.64
MFT Version                     : mft 4.18.0-106Identifier                      : QSFP+
Compliance                      : N/A
Cable Technology                : Copper cable unequalized
Cable Type                      : Passive copper cable
OUI                             : Mellanox
Vendor Name                     : Mellanox
Vendor Part Number              : MC2206130-003
Vendor Serial Number            : MT1529VS05052
Rev                             : A3
Wavelength [nm]                 : N/A
Transfer Distance [m]           : 3
Attenuation (5g,7g,12g) [dB]    : 11,0,0
FW Version                      : N/A
Digital Diagnostic Monitoring   : No
Power Class                     : N/A
CDR RX                          : N/A
CDR TX                          : N/A
LOS Alarm                       : N/A
Temperature [C]                 : N/A
Voltage [mV]                    : N/A
Bias Current [mA]               : N/A
Rx Power Current [dBm]          : N/A
Tx Power Current [dBm]          : N/A
IB Cable Width                  : 1x,2x,4x
Memory Map Revision             : 0
Linear Direct Drive             : 0
Cable Breakout                  : Channels implemented [1,2,3,4]/Far end is unspecified
SMF Length                      : N/A
MAX Power                       : 0
Cable Rx AMP                    : N/A
Cable Rx Emphasis               : N/A
Cable Rx Post Emphasis          : N/A
Cable Tx Equalization           : N/A
Wavelength Tolerance            : N/A
Module State                    : N/A
DataPath state [per lane]       : N/A,N/A,N/A,N/A
Rx Output Valid                 : 0,0,0,0
Rx Input Valid                  : 0,0,0,0
Nominal bit rate                : 0.000Gb/s
Rx Power Type                   : OMA
Manufacturing Date              : 16_07_15
Active Set Host Compliance Code : N/A
Active Set Media Compliance Code: N/A
Error Code Response             : N/A
Module FW Fault                 : N/A
DataPath FW Fault               : N/A
Tx Fault [per lane]             : N/A
Tx LOS [per lane]               : N/A
Tx CDR LOL [per lane]           : N/A
Rx LOS [per lane]               : N/A
Rx CDR LOL [per lane]           : N/A
Tx Adaptive EQ Fault [per lane] : N/APhysical Grade                  :      0,     0,     0,     0
Height Eye Opening [mV]         :    N/A,   N/A,   N/A,   N/A
Phase  Eye Opening [psec]       :    N/A,   N/A,   N/A,   N/ATime Since Last Clear [Min]     : N/A
Symbol Errors                   : N/A
Symbol BER                      : N/A
Effective Physical Errors       : N/A
Effective Physical BER          : N/A
Raw Physical Errors Per Lane    : N/A
Raw Physical BER                : N/A
Link Down Counter               : N/A
Link Error Recovery Counter     : N/AAre you using the same cable to connect the SX6012 to the QM switch?
Can you please share the same output mlxlink output when you connect it to the SX6012?I couldn’t guarantee for the cable and at the moment I have no colleague at the cluster, but I have another of the old ones with the same IB CA on SX6012:[root@w10 ~]# ibnetdiscover
…
vendid=0x2c9
devid=0xc738
sysimgguid=0x98039b0300ff9a80
switchguid=0x98039b0300ff9a80(98039b0300ff9a80)
Switch  12 “S-98039b0300ff9a80”         # “MF0;switch-9c1bc6:SX6012/U1” enhanced port 0 lid 3 lmc 0
[2]     ""H-0002c903000e2c0e""1          # “taede147 mlx4_0” lid 1 4xQDR
[4]     ""H-0002c90300289e38""1          # “w3 mlx4_0” lid 13 4xQDR
[7]     ""H-0002c903000cf350""1          # “w1 mlx4_0” lid 12 4xQDR
[11]    “S-1070fd03008a0f9e”[40]                # “MF0;switch-742018:MQM8700/U1” lid 15 4xSDR[root@w10 ~]# ibswitches
Switch  : 0x98039b0300ff9a80 ports 12 “MF0;switch-9c1bc6:SX6012/U1” enhanced port 0 lid 3 lmc 0
Switch  : 0x1070fd03008a0f9e ports 41 “MF0;switch-742018:MQM8700/U1” enhanced port 0 lid 15 lmc 0Unfortunately I get this:
[root@w10 ~]# mlxlink -d lid-3 -p 4 -m -e -c-E- Device is not supported[root@w10 ~]# rpm -qf /usr/bin/mlxlink
mft-4.18.0-106.x86_64[root@w10 ~]# rpm -qi mft
Name        : mft
Version     : 4.18.0
Release     : 106
Architecture: x86_64
Install Date: Fr 18 Feb 2022 04:41:36 CET
Group       : System Environment/Base
Size        : 187144723
License     : Proprietary
Signature   : DSA/SHA1, So 28 Nov 2021 16:54:56 CET, Key ID c5ed83e26224c050
Source RPM  : mft-4.18.0-106.src.rpm
Build Date  : So 28 Nov 2021 09:24:39 CET
Build Host  : appsbuild-03-03.mtl.labs.mlnx
Relocations : /usr /etc
Packager    : Omer Dagan omerd@mellanox.com
Vendor      : Mellanox Technologies Ltd.
Summary     : Mellanox firmware tools
Description :
Mellanox firmware toolsNext Tuesday (we have holiday on monday)  I have the customer in place to check the cable.To summarize –The Quantum switch doesn’t support QDR rates (was never interop tested vs. the EOL ConnectX-2 devices)To continue using those legacy devices – I would keep connecting them with the SX6012 and in turn, connect the SX6012 to the Quantum switch using FDR cables.Kinds Regards,DanHi Dan,
first of all thank you very much.
Some new questions in this context appeared.Exemplary three machines: c27, c28, w1############ c27 ###########
3b:00.0 Network controller: Mellanox Technologies MT27500 Family [ConnectX-3]
Infiniband device ‘mlx4_0’ port 1 status:
state:		 4: ACTIVE
phys state:	 5: LinkUp
rate:		 56 Gb/sec (4X FDR)
CA ‘mlx4_0’
CA type: MT4099
Firmware version: 2.42.5000
Hardware version: 1
State: Active
Rate: 56
############ c28 ###########
3b:00.0 Network controller: Mellanox Technologies MT27500 Family [ConnectX-3]
Infiniband device ‘mlx4_0’ port 1 status:
state:		 4: ACTIVE
phys state:	 5: LinkUp
rate:		 56 Gb/sec (4X FDR)
CA ‘mlx4_0’
CA type: MT4099
Firmware version: 2.42.5000
Hardware version: 1
State: Active
Rate: 56
############ w1 ###########
01:00.0 InfiniBand: Mellanox Technologies MT25408A0-FCC-QI ConnectX, Dual Port 40Gb/s InfiniBand / 10GigE Adapter IC with PCIe 2.0 x8 5.0GT/s In… (rev b0)
Infiniband device ‘mlx4_0’ port 1 status:
state:		 4: ACTIVE
phys state:	 5: LinkUp
rate:		 40 Gb/sec (4X QDR)
CA ‘mlx4_0’
CA type: MT26428
Firmware version: 2.9.1000
Hardware version: b0
State: Active
Rate: 40Switch	: 0x1070fd03008a0f9e ports 41 “MF0;switch-742018:MQM8700/U1” enhanced port 0 lid 15 lmc 0
Switch	: 0x98039b0300ff9a80 ports 12 “MF0;switch-9c1bc6:SX6012/U1” enhanced port 0 lid 3 lmc 0vendid=0x2c9
devid=0xc738
sysimgguid=0x98039b0300ff9a80
switchguid=0x98039b0300ff9a80(98039b0300ff9a80)
Switch	12 “S-98039b0300ff9a80”		# “MF0;switch-9c1bc6:SX6012/U1” enhanced port 0 lid 3 lmc 0
[2]	""H-0002c903000e2c0e""1 		# “taede147 mlx4_0” lid 1 4xQDR
[4]	""H-0002c90300289e38""1 		# “w3 mlx4_0” lid 13 4xQDR
[7]	""H-0002c903000cf350""1 		# “w1 mlx4_0” lid 18 4xQDR
[11]	“S-1070fd03008a0f9e”[40]		# “MF0;switch-742018:MQM8700/U1” lid 15 4xQDRWhen we start a StarCCM job from w1 choosing c27 as computenode it’s “fast”.
When we start a StarCCM job from w1 choosing c28 as computenode it’s “fast”.
When we start a StarCCM job from w1 choosing  c27 and c28 as computenodes it’s ""it’s slow, almost stalled.In the past this worked fast.
Without more details depending StarCCM setup, the question is:
Do we need some special configuration on the Quantum switch in reference to the differrent adapters and cables connected?Do we need to configure a new / an extra SM on the Quantum switch?
We still have a SM running on one of the old nodes:[root@c27-ib0 ~]# rpm -qf /usr/sbin/opensm
opensm-3.3.21-3.el7_8.x86_64[root@c27-ib0 ~]# ps auxww | grep opensm
root       4066  0.0  0.0 115404   576 ?        S    Mai01   0:00 /bin/bash /usr/libexec/opensm-launch
root       4067  0.0  0.0 2101160 1952 ?        Sl   Mai01   0:08 /usr/sbin/opensmBest regards
JoeWe could fix the described StarCCM  Problem by adding  this options:… -mpidriver openmpi  -mpiflags “–mca routed direct” -fabric ucx …SOLVEDWith some “plug and pray” we made it with the right cables and putting the old Connect X-2 to the old switch to a running system:endid=0x2c9
devid=0xd2f0
sysimgguid=0x1070fd03008a0f9e
switchguid=0x1070fd03008a0f9e(1070fd03008a0f9e)
Switch	41 “S-1070fd03008a0f9e”		# “MF0;switch-742018:MQM8700/U1” enhanced port 0 lid 15 lmc 0
[7]	""H-f4521403003e36d0""1 		# “w9 mlx4_0” lid 16 4xQDR
[10]	""H-98039b0300e25ba0""1 		# “c29-ib0 mlx4_0” lid 6 4xFDR
[11]	""H-98039b0300e263c0""1 		# “c31-ib0 mlx4_0” lid 7 4xFDR
[12]	""H-98039b0300e26590""1 		# “c32-ib0 mlx4_0” lid 5 4xFDR
[13]	""H-98039b0300e25f90""1 		# “c30-ib0 mlx4_0” lid 4 4xFDR
[14]	""H-98039b0300d599b0""1 		# “w8 mlx4_0” lid 12 4xFDR
[15]	""H-98039b0300d5a350""1 		# “w10 mlx4_0” lid 45 4xFDR
[16]	""H-98039b0300e26020""1 		# “c28-ib0 mlx4_0” lid 2 4xFDR
[17]	""H-98039b0300e26580""1 		# “c25-ib0 mlx4_0” lid 14 4xFDR
[18]	""H-98039b0300e269f0""1 		# “c27-ib0 mlx4_0” lid 8 4xFDR
[19]	""H-98039b0300e269b0""1 		# “c26-ib0 mlx4_0” lid 9 4xFDR
[25]	""H-b8cef60300a7fb34""1 		# “c34 mlx5_0” lid 17 4xHDR
[27]	""H-b8cef60300a7ebcc""1 		# “MT4123 ConnectX6   Mellanox Technologies” lid 11 4xHDR
[40]	“S-98039b0300ff9a80”[11]		# “MF0;switch-9c1bc6:SX6012/U1” lid 3 4xQDR
[41]	""H-1070fd03008a0fa6""1 		# “Mellanox Technologies Aggregation Node” lid 10 4xHDRvendid=0x2c9
devid=0xc738
sysimgguid=0x98039b0300ff9a80
switchguid=0x98039b0300ff9a80(98039b0300ff9a80)
Switch	12 “S-98039b0300ff9a80”		# “MF0;switch-9c1bc6:SX6012/U1” enhanced port 0 lid 3 lmc 0
[1]	""H-0002c903000e298a""1 		# “c22 mlx4_0” lid 20 4xQDR
[2]	""H-0002c903000e2c0e""1 		# “taede147 mlx4_0” lid 1 4xQDR
[3]	""H-0002c903000e297e""1 		# “c23 mlx4_0” lid 22 4xQDR
[4]	""H-0002c90300289e38""1 		# “w3 mlx4_0” lid 13 4xQDR
[5]	""H-0002c903000e27fe""1 		# “c24 mlx4_0” lid 23 4xQDR
[6]	""H-0002c903000e2992""1 		# “c21 mlx4_0” lid 21 4xQDR
[7]	""H-0002c903000cf350""1 		# “w1 mlx4_0” lid 18 4xQDR
[8]	""H-0002c903000e2bee""1 		# “taede146 mlx4_0” lid 19 4xQDR
[9]	""H-0002c903002886f6""1 		# “w6 mlx4_0” lid 24 4xQDR
[11]	“S-1070fd03008a0f9e”[40]		# “MF0;switch-742018:MQM8700/U1” lid 15 4xQDRThanks again Dan!CU
JoeHappy to hear all is fixed.Best regards,DanThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
968,deploy-k8s-cluster-on-bcm-using-local-docker-repository-and-local-yum-repos,"We dont have internet in our labs , how can we deploy K8s seamlessly within private network .I have private docker registry and local yum repos in private networkSee https://kb.brightcomputing.com/knowledge-base/installing-kubernetes-on-air-gapped-systems/
You may have to tweak it a bit for your situation and for Bright 9.2.Powered by Discourse, best viewed with JavaScript enabled"
969,issues-while-configuring-nvme-over-fabrics-nvme-of-target-offload,"I am trying to configure NVMe over Fabrics (NVMe-oF) Target Offload. I am running into issues when I try to connect.I am using Ubuntu 20.04 with IOMMU disabled on both hosts and MLNX_OFED_LINUX-5.6-2.0.9.0-ubuntu20.04-x86_64.isoI have followed this link https://support.mellanox.com/s/article/howto-configure-nvme-over-fabrics--nvme-of--target-offload for installation.
However, when I try to run
nvme connect -t rdma -n testsubsystem -a 10.10.1.1 -s 4420
I get the below error
Failed to write to /dev/nvme-fabrics: Input/output errorThe hose dmesg | tail -50 shows[  451.386608] nvmet: adding nsid 1 to subsystem testsubsystem
[  531.859367] nvmet_rdma: enabling port 1 (10.10.1.1:4420)
[  572.842137] nvmet: creating controller 1 for subsystem nqn.2014-08.org.nvmexpress.discovery for NQN nqn.2014-08.org.nvmexpress:uuid:b21f00f6-7ca5-44ae-afd6-3968c43eed23.
[  600.639453] nvmet: creating controller 1 for subsystem testsubsystem for NQN nqn.2014-08.org.nvmexpress:uuid:b21f00f6-7ca5-44ae-afd6-3968c43eed23.
[  602.408514] nvmet_rdma: using dynamic staging buffer 00000000441388f3
[  602.445333] nvme 0000:c5:00.0: Failed to get peer resource xrq=000000004d1fc490 be_ctrl=00000000e3fde924
[  602.454821] nvmet_rdma: failed to get XRQ for queue (1)
[  602.460047] nvmet: failed to install queue 1 cntlid 1 ret 4006Please let me know if anything else is required. Any help is appreciated!Powered by Discourse, best viewed with JavaScript enabled"
970,do-i-need-to-buy-a-ethernet-100gbe-cable,"Hi, I am new to RDMA or Mellanox related things and my question may sound quite silly. My group is considering buying some new machines and may equip them with Mellanox connectX-5 VPI adapter NICs. The switch is Edgecore Wedge 100BF-32X programmable switch. If we want to work with RoCE, is it necessary to connect these adapters to the switch with an Ethernet 100GbE cable (e.g., NVIDIA MCP1600-C00AE30N DAC Cable Ethernet 100GbE QSFP28)?
Thanks.Hello Huangwentaochina,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Yes, if you want to connect the ConnectX-5 VPI adapter to your Ethernet switch, you need to use a supported Ethernet cable for the ConnectX-5 VPI.
The ConnectX-5 VPI has the ability to change the port type from IB to Ethernet and vice-versa. You can review the following link on how-to change the port_type → https://docs.nvidia.com/networking/display/MLNXOFEDv561033/Ethernet+Interface#EthernetInterface-porttypemanagementPortTypeManagement/VPICardsConfigurationThank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
971,how-to-test-flow-matching-performance-properly,"Hi everyone,I am wondering if there is a way to test flow matching performance properly. Should I create 1k flow matching rules, or generate 1k traffic to test performance? How to measure properly? Is there any application or tool to use?Many thanks in advance,
KyleHi Kyle,Could you please clarify what is flow matching rules?Thanks
SamerRules like how many millions of ip-tuples could be matched on accurately on a BlueField-2.
I would like to measure:I have not found any official specifications respecting to the doca flow performance about above measurements.Thanks for the reply.Best wishes,
KyleHi Kyle,Thank you for your reply.Please note that the number of rules is > 100000. there is 100% match accuracy on any speed.
If you have further questions , we suggest to open support ticket by sending email to
Enterprise Support EnterpriseSupport@nvidia.com
And our experts will be happy to assist.Thanks,
SamerHi SamerThanks a lot for the clarification. What about accuracy of more than 1million rules? Do I need to contact to find out?Best wishes,
KyleHi Kyle,Yes .Thanks,
SamerThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
972,we-have-a-few-servers-with-mcx623106as-cdat-ethernet-100gb-2-port-qsfp56-cards-is-there-any-published-performance-baseline-for-these-cards-what-am-i-supposed-to-see-if-i-run-a-raw-ethernet-bw-test-between-2-of-these,"This is what I see, testing on 2 new HP dl385G10Plus v servers, with latest generation Epyc CPUs. One Arista 7800 switch between them.Ubuntu 20.04.1 , kernel 5.4.0-81-genericserver: raw_ethernet_bw --server -d mlx5_0 -B 88:e9:a4:33:48:b1 -F --duration 20client: raw_ethernet_bw --client -d mlx5_0 -B 88:e9:a4:20:20:d3 -E 88:e9:a4:33:48:b1 -F --duration 20results:Max msg size in RawEth is MTU 1518Changing msg size to this MTUSend BW TestDual-port : OFF Device : mlx5_0Number of qps : 1 Transport type : IBConnection type : RawEth Using SRQ : OFFPCIe relax order: ONibv_wr* API : OFFTX depth : 128CQ Moderation : 1Mtu : 1518[B]Link type : EthernetGID index : 0Max inline data : 0[B]rdma_cm QPs : OFFData ex. method : Ethernetraw ethernet header**************************************| Dest MAC | Src MAC | Packet Type ||------------------------------------------------------------|| 88:E9:A4:33:48:B1| 88:E9:A4:20:20:D3|DEFAULT ||------------------------------------------------------------|#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]1518 42973376 0.00 6221.15 4.297334Is this result as expected?any suggestion?A typical Iperf (iperf -c 192.168.1.1 -w 2m -P 32) between 2 nodes shows bandwidth fluctuating between 40 and 60 Gbps, way below what we would expect.Thanks,Hi Paolo,Yes we suggest reviewing the below performance guides :https://community.mellanox.com/s/article/getting-started-with-performance-tuning-of-mellanox-adaptersandhttps://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersOnce you tune the system run the tests again , if the results are not as expected we suggest opening a new support ticket for further investigation by sending email to Networking-support@nvidia.comThanks,SamerThanks. We did, together with a long list of tests, as per yours and AMD’s tuning whitepapers, cross-checking them with RedHat and SUSE’s tuning whitepapers.At the moment the only way in which we can get consistent good performances is by setting the IRQs as per your guide, pinning the card to the right numa node (using your configuration script) AND pinning the iperf server process to the same numa node where the card is pinned to. With that, I get a consistent 95 Gbps.If the iperf (server) process is not pinned to anything, performances vary quite a bit, 50 Gbps in average. if I force it to numa node 7 (the card is on 2), it goes down to 35 Gbps.Pinning the process I need to a specific set of CPUs is not a valid solution for the production environment. We need to be able to use all the cores we have.Things tested:assorted kernels (5.4.xx, 5.11.xx), various drivers (your latest, inbox driver), assorted NumaPerSocket settings, all sorts of OS network stack optimizations, all sort of power governor settings, BIOS parameters around, Hardware config changes, etc etc.What really bugs me is that if I move one of these cards to an old spare server, I immediately get excellent performance. No tuning whatsoever.I tried opening a support ticket, it was closed right away as I do not have a direct support contract with Mellanox. All hardware was bought trough HPE (I have a case open with them). We also have a case open with AMD.I’m really just looking for ideas on how to triage further.Thanks,PPPowered by Discourse, best viewed with JavaScript enabled"
973,ibv-poll-cq-always-gets-cqe,"In my example, I use RDMA CM to connect nodes and deal with the connection in RDMA_CM_EVENT_ESTABLISHED.
here is my server’s code:In my client’s code, I only call ibv_post_send() once.
I think when I call ibv_poll_cq() one CQE will pop out. In my case, there will be no CQE in CQ. But I can always get one actuality. Is there some wrong when I use the API?Hi @eric503630,Regards,
ChenThanks for your answers.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
974,mbf2h332a-aeeot-bundled-firmware-manager-does-not-include-firmware-for-this-board,"Using SDKManager to try to install DOCA 2.0.2 on a MBF2H332A-AEEOT Bluefield2 the install failed at “Burn Target Firmware” with this error:WARNING: The bundled firmware manager does not include firmware for this board. Recommended firmware version cannot be determined.The board PSID is MT_0000000540 which, as far as I know, is current and fully supported.I need a pointer or some assistance to resolve this in order to complete the DOCA install.Thanks
-JHere is some diagnostic info that maybe helpful:root@localhost:~# bfvcheck
Beginning version check…-RECOMMENDED VERSIONS-
ATF: v2.2(release):4.0.3-25-g4e9af27
UEFI: 4.0.3-10-gb8abccc
FW: N/A-INSTALLED VERSIONS-
ATF: v2.2(release):4.0.3-25-g4e9af27
UEFI: 4.0.3-10-gb8abccc
FW: 24.37.1300WARNING: The bundled firmware manager does not include firmware for
this board. Recommended firmware version cannot be determined.Firmware Manager Path: /opt/mellanox/mlnx-fw-updater/firmware/mlxfwmanager_sriov_dis_aarch64_41686
Part Number: MBF2H332A-AEEOTVersion check complete.
root@localhost:~#root@localhost:~# ./mlxup
Querying Mellanox devices firmware …Device Type:      BlueField2
Part Number:      MBF2H332A-AEEO_Ax_Bx
Description:      BlueField-2 P-Series DPU 25GbE Dual-Port SFP56; PCIe Gen4 x8; Crypto Enabled; 16GB on-board DDR; 1GbE OOB management; HHHL
PSID:             MT_0000000540
PCI Device Name:  /dev/mst/mt41686_pciconf0
Base GUID:        b8cef60300677860
Base MAC:         b8cef6677860
Versions:         Current        Available
FW             24.37.1300     24.37.1300
NVMe           N/A            20.4.0001
PXE            3.7.0102       3.7.0102
UEFI           14.30.0013     14.30.0013
UEFI Virtio blk   22.4.0010      22.4.0010
UEFI Virtio net   21.4.0010      21.4.0010Status:           Up to dateroot@localhost:~#This is a bug in the sdkmanager - the firmware is not included in the bundle.Workaround is to manually install firmware.  Then patch the sdkmanager to not attempt to check/install firmware, then install completes fine and works.We have enterprise support, they have a ticket on this along with my workaround patch.  Hopefully they can produce a proper fix that can help others.Powered by Discourse, best viewed with JavaScript enabled"
975,connect-x-3-unable-to-change-virtual-mac,"Hi,I recently deployed some hp servers with hp 546flr-sfp+ adapters. During the deployment the proccess created bond interfaces for the two ports on the card and set the virtual mac address on both ports to the same address. The deployment failed. How do i delete this setting? How do I revert to the physical mac addresses?
MicrosoftTeams-image (16)1376×916 131 KB
Hi,Thank you for contacting us!
To revert back you would need to use the physical Mac Address.
The Mac Address can be found on a sticker, on the adapter itself.Thanks,
Ilan.Hi,
thank you very much.
At what point do I have to set this. In the Bios everything is grayed out and in the OS the virtual Mac is taken for the permanent address and I am unable to change it.Kind RegardsHow do i delete this setting? How do I revert to the physical macHi,Please review this document of how to enable SR-IOV on ConnectX-3
https://support.mellanox.com/s/article/howto-configure-sr-iov-for-connectx-3-with-kvm--ethernet-xYou will just need to backtrack on this procedure and you should be able to delete the setting.Thanks,
Ilan.Hi,I am very very thankfull for your help. I installed and configured everything like the guide said, but it seems that i miss the step where I am able to configure the mac address.eno49d1 is the second port and it still has the wrong mac address.I am very sorry but would it be possible to give me another hint.Thank you very much.Hi,after reboot it looks like this but still has the original interface the wrong mac address.Kind RegardsWhat was the deployment package/utility, that set identical Virtual MACs for You ?
I am facing the same problem - got card with two identical Virtual MACsPowered by Discourse, best viewed with JavaScript enabled"
976,sx6036g-proxy-arp-setup-telnet-yes-ping-no,"I’m kinda new here, so feel free to teach me. Mastersso here’s the situation
infiniband_question998×745 23.3 KB
I’ve got a Network Setup with 10.10.100.0 / 16 with gateway of 10.10.255.1. Just like the picture above.And also have configured Proxy-ARP following the documentaion of “Configuring Mellanox Hardware for VPI Operation Application Note Rev 1.2”. So I could use IPOIB for the servers using Infiniband cables (QSPF copper cable for connecting 6036 with 6036E or Servers and MAM1Q00A-QSA-SPwith SFP-10GB-SR with LC-SR Multi Mode cables to connect between catalyst 4948 and 6036)When those were done, I went on and set-up the servers (Server01, Server02) with opensm and IP addresses, so it could do the job. But for some reason, the servers can’t access the network 10.10.100.0 / 16 AND more strange thing is,that I could telnet 6036 from a computer connected to Catalyst 4948 or the network above, but SX6036G can’t ping out to the 10.10.100.0 / 16 network nodes or the gateway (10.10.255.1).In a sentense, “I could telnet into 6036-01 but in the 6036-01 console, I can’t ping to anywhere”For the Infiniband Networks, Nothing seems to be working. (No pings, no telnet anything or anywhere). Still, I get a green light on the port of 6036G and 4036E though.I’m thinking about three cases which isThat’s about it. I’ve been hashing through this problem since… somewhere around March now. Appreciate all the help from you guys. Thank you in Advance and, for reading this whole question6036-01 question.txt (3.15 KB)Hi,The topology is unusual. Typically the two 6036 VPI gateways would be configured in “HA” to load balance traffic. Also there would be at least 1 IB switch between these VPI gateways and the IB servers.Then, you’d have server01 and server02 in the same cluster, managed by a single master SM. You’d then also have traffic load-balanced between the two VPI gateways.Also unusual is a 4036E in the mix, which is a very olf “Voltaire” version of a VPI gateway. If you are ONLY using the IB ports on it (using it like an IB switch), its fine as long as the cables on it link UP.This is the most common, simplistic topology:https://community.mellanox.com/s/article/howto-configure-infiniband-gateway-ha–proxy-arp-xI recommend reviewing that document.I recommend disabling ib0, and configuring the management port Mgmt0 instead, as that port is isolated from the production traffic. It should be in a different management IP subnet.Your IB server IPoIB interfaces should also be in the 10.10.x.x /16 subnet. If the router interface is also in the 10.10.x.x ip subnet, there is no need for a default gateway in the VPI gateway config to point to it.Make sure you have physical link with show int eth 1/39 and show int ib 1/x.If you are expecting server01 and server02 to be in the same IB cluster, you need the IB switch between them and the6036VPI gateways for full redundancy and communication and load balancing through the VPI gateways.For HA mode on the gateways, you’ll need the mgmt0 interfaces UP, in an out-of-band management network, so that each VPI gateway can PONG each other’s mgmt0 interface (required for HA).Eventually, you want to see both gateway PRA interfaces UP and Active, seen with “show interface proxy-arp 1 ha” when SSH’d to the proxy-arp “VIP” address. After that, you want to be able to PING between the IB servers and the Eth servers to confirm proper operation.Powered by Discourse, best viewed with JavaScript enabled"
977,bluefield-2-supported-interfaces,"Hi All,
I have a MBF2H516A-CENOT DPU. Its physical connector is, described in the Specification chapter of its User Guide, “Dual QSFP56 (copper and optical)”.
Specifications
And the default card firmware setting described in the “Supported Interfaces” chapter is QSFP28 mode.
Supported InterfacesSo how should I upgrade the firmware so that it can work in QSFP56 mode?
image1822×955 233 KB

How to understand this description? Does this mean that MBF2H516A-CENOT supports QSFP56 mode. We only need to upgrade the firmware to make it work in QSFP56 mode?Powered by Discourse, best viewed with JavaScript enabled"
978,setting-service-level-in-ib-send-bw-doesnt-increase-expected-priority-counters-in-ethtool,"HI,I’m trying to get RoCE going in a small cluster of 10 nodes, each with 2x Connect-X5 NICs running at 100 GbE though a CISCO switch (sorry I don’t make the purchasing decisions).My main problem is that I get rx_discard_phy increasing when I run ib_send_bw on 2 hosts funneling into one host. I think this is because PFC isn’t working.We’ve configured (we think) TC=2 and TC=3 to be PFC / no drop on the switch. We’ve run mlx_qos as below.While debugging the main problem, I think I’ve found a smaller one: if I set the service level with ib_send_bw to 3 on the transmitter, the tx_prio0_bytes increase in ethtool -S as shown below. I expected prio3 bytes to increase. They stay stubbornly at zero.What have I missed?Receiver command:Transmitter:QoS RX it looks like this:QoS the sender it looks like this:TC wrap looks like this:Hello,Have you validated that PFC has been configured point to point? I am referring to PFC properly configured on the switch(s) ports as PFC and not GP (Global pause). Have you checked the switch(s) counters from the switch ports to check RX/TX which priority the traffic is being sent/received? Verify that PFC is properly enabled on the switch and validate the traffic priority via config & counters.Sophie.Powered by Discourse, best viewed with JavaScript enabled"
979,sn2100-sn2700-100g-10g-rate-conversion,"We have four SN2100 and SN2700 switches (all ONYX 3.10.4100) happily interconnecting themselves, CX5/6 NICs, and dozens of FPGA boards. The links are ALL 100 GbE, either over DAC or AOC. Fine. Everything works.We’ve been frustrated and unsuccessful at getting any port on any SN2100/2700 to make link at 10G. We’re sure the switch and media are good, because the 10G switch makes 10G link fine with the CX5/6 or FPGA.Since the switches “see” the cable I2c, but dont make 10G link, we suspect that for some reason they are not allowed. We are using a MLNX adapter to plug a SFP in into the QSFP on the switch. And with AOC or DAC, no link! But pull it out of the switch and plug to a CX5/CX6, 10G link as expected.Any ideas on what to look at or try here so these fine100G switches can have some 10G love?is the port explicitly configured to 10g on the switch?Let’s see the connection status when failing to connect. Please run the following on the switch CLI:There’s an mlxlink command (might require fae shell (I.e. #fae mlxlink -d <mst_dev> -p <#port> -m)For getting the mst device use
#fae mst statusSure! Here’s the status of port 15…ar-sn2100-03 [standalone: master] (config fae) # mlxlink -d /dev/mst/mt52100_pci_cr0 -p 15 -mI went into the web gui to enable the 10G and disable the 100G bit and now from mlxlink I see…Link is up and we are moving packets.
Thanks so much for this help @dwaxman !
Much appreciated! -ShepNice! Glad this was resolved.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
980,ofed-installation,"I have installed OFED ""mlnx-en-5.2-1.0.4.0-rhel8.3-x86_64.iso in Cent OS (8.3 kernel version) in Azure VM. I have ConnectX-4 NIC in my VM. Installation was successful but when I tried to run my third-party TRex traffic generator I see the errorEAL: /lib64/libmlx5.so.1: version `MLX5_1.15’ not found (required by so/x86_64/libmlx5-64.so)EAL: FATAL: Cannot init pluginsEAL: Cannot init pluginsAny further installation missing here?I installed using./install --add-kernel-supportHello Vijayram,Thank you for posting your inquiry on the NVIDIA Networking Community.Depending on the VM you use, the drivers are already integrated in the Azure VM, see the following link → Configuration and Optimization of InfiniBand enabled H-series and N-series Azure Virtual Machines - Azure Virtual Machines | Microsoft DocsFor further support regarding this issue, we recommend to contact Microsoft Azure support, as they can provide the information if driver installation is possible in the Azure VM image.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
981,rte-flow-create-async-pushes-flows-immediately,"Hi.
I’m trying to use rte_flow_create_async() to batching rule insertion with connectX-6.I expected that rte_flow_create_async() does not push the rule when postpone is marked as 1.
But it does push even though I does not call rte_flow_push().What’s the reason for this issue? I just want to postpone pushing.P.S.
I checked the code in the PMD, and saw below.Here, I understood that the root means table with group 0.
I guess mlx5dr_rule_create_hws() does not push immediately, but mlx5dr_rule_create_root() pushes immediately. Am I right?Is there any example code for testing rte_flow_create_async() and checking whether it pushes the rule immediately?Thank youPowered by Discourse, best viewed with JavaScript enabled"
982,is-there-any-api-or-tool-that-can-be-used-to-get-the-power-usage-of-bluefield2,"I’d like to get the power consumption and TDP of Bluefield2. Is there any API or tool that can be used to get the power information? Thanks in advance.hi JamesHere is the SpecificationsThank you
Meng, ShiHow do I get the realtime power consumption?hi jamesWe do not have API to get realtime power consumption.
You need to check is server side have some way to get realtime power consumption of pcie slotThank you
Meng, ShiDo you also mean that there are no Mallenox Tool or hardware registers of Bluefield2 that I can use to get the real-time power consumption?And more, the BMC document of Bluefield2 shows that we can use the IPMI commands to get real-time power information. But I don’t know if it is doable and which command I can use.
URL: BlueField BMC Software Overview - BlueField BMC v23.04-3 - NVIDIA Networking Docs

image1053×627 39.7 KB
Powered by Discourse, best viewed with JavaScript enabled"
983,low-nvme-of-target-offload-performance-when-size-is-small,"Hello,​I ran some NVMe-oF Target Offload tests using ConnectX-5 and compared with NVMe-oF results using the same environment.Very low CPU utilization showed that target offload was done correctly, but target offload showed higher latency compared to NVMe-oF when the value size was smaller than 32K.For the tests, I ran fio and the my connectX-5 is Infiniband controller: Mellanox Technologies MT27800 Family [ConnectX-5]ThanksYou haven’t provided enough details about the setup and I would assume that you’ve used FIO with libio, that goes thought kernel, thus latency fluctuation is expected. Using different library that is based on the polling and not events is recommended. For best results use SPDK and run perf tool, that is part of SPDK.Powered by Discourse, best viewed with JavaScript enabled"
984,how-do-i-persist-the-lossy-roce-accelerations-over-reboots,"How do I persist the Lossy RoCE Accelerations over reboots? They are described here: https://community.mellanox.com/s/article/How-to-Enable-Disable-Lossy-RoCE-Accelerations, but seem temporary on boot.Hi ,Please note that mlxconfig configurations ate persistent .In case you would like add more scripts to be applied after reboot you can use the /etc/rc.locali.eAn example to add this make it persistent after reboot:#! /bin/bashmst startThanks,SamerUnfortunately, the command to enable them is mlxreg, and it does not seem to be persistent. Perhaps I have just missed how to invoke it from mlxconfig? I have not seen any of the parameters that seem to be linked, either a bitmask or official ones.rc.local is OK, but a bit more fragile than I like as it depends on a bunch of things including mellanox’s MFT install being correct etc. I will still mark your answer as best if there is no simple way, as it will work mostly.Powered by Discourse, best viewed with JavaScript enabled"
985,enabling-ieee-mode-for-vsan-rdma,"We’re trying to enable RDMA on vSAN 8 (vCenter: 8.0.1 build: 21860503, VMware ESXi, 8.0.1, 21813344) with our ConnextX5 NICs but running into challenges getting everything configured.No matter what we configure, we can’t get our NIC to show “Mode: 3 - IEEE mode”. It remains in Mode: 0 - Unknown.I’ve configured the NIC and ESXi like this:We followed this guide for the switches:
https://enterprise-support.nvidia.com/s/article/qos-configuration-examples-for-cisco-nexus-5600We also gathered the information from:
https://www.reddit.com/r/vmware/comments/ozhq6j/vsan_rdma_with_mellanox_nic/Any idea what we may be missing or what else we can try? Everything looks like we should be able to use RDMA, yet here we are.Powered by Discourse, best viewed with JavaScript enabled"
986,packet-stuck-in-buffer-in-connect-4-lx,"Sending UDP packets through a server and 1 packet gets stuck in the buffer.
Eg send 1, it gets stuck, send 100, 99 gets transmitted with 1 stuck.
We can see the packet in TCPdump but the switch on the remote side does not receive it until either a new packet comes in or we bring down the interface…
Server1 →  em2 UUT p2p1 [Connectx-4-Lx] → Switch → Server 2If we change to another interface using a ConnectX-5 card no problems.ethtool -S between the two cards doesn’t show anything different.CentOS Linux release 7.9.2009 (Core)
Kernel 3.10.0-1160.2.2.el7.x86_64*-network:0
description: Ethernet interface
product: MT27710 Family [ConnectX-4 Lx]
vendor: Mellanox Technologies
logical name: p2p1
version: 00
serial: 0c:42:a1:e9:9F:10
size: 1Gbit/s
width: 64 bits
clock: 33MHz
capabilities: bus_master cap_list rom ethernet physical tp autonegotiation
configuration: autonegotiation=on broadcast=yes driver=mlx5_core driverversion=5.0-0 duplex=full firmware=14.27.1016 (MT_2420110004) ip=192.168.18.1 latency=0 link=yes multicast=yes port=twisted pair speed=1Gbit/s*-network:1
description: Ethernet interface
product: MT27800 Family [ConnectX-5]
vendor: Mellanox Technologies
logical name: em4
version: 00
serial: 1c:34:da:6f:d2:11
size: 1Gbit/s
width: 64 bits
clock: 33MHz
capabilities: bus_master cap_list rom ethernet physical tp autonegotiation
configuration: autonegotiation=on broadcast=yes driver=mlx5_core driverversion=5.0-0 duplex=full firmware=16.27.6106 (DEL0000000016) ip=192.168.20.1 latency=0 link=yes multicast=yes port=twisted pair speed=1Gbit/sexample of udp packet
tcpdump: listening on p3p2, link-type EN10MB (Ethernet), capture size 262144 bytes
19:56:09.866719 IP (tos 0x0, ttl 254, id 2640, offset 0, flags [DF], proto UDP (17), length 80)
192.168.167.101.mmcc > 192.168.200.102.mmcc: [udp sum ok] UDP, length 52Any thoughts on what I am missing , some setting?Powered by Discourse, best viewed with JavaScript enabled"
987,mellanox-infiniband-connectx-5,"Currently I am trying to get the Mellanox connected to a KVM-Qemu VM as the application that I need to run in a virtual machine needs to have access to RDMA.I have followed the following guide:
https://enterprise-support.nvidia.com/s/article/HowTo-Configure-SR-IOV-for-ConnectX-4-ConnectX-5-ConnectX-6-with-KVM-Ethernet#jive_content_id_OverviewI managed to get 16 VF’s (4 for each port) and everything looks fine until I tried to add to the KVM Qemu:After looking further I noticed that each card is fully in their own group, which I suspect is the problem:Card 1:
IOMMU Group 96 80:01.0 Host bridge [0600]: Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge [1022:1482]
IOMMU Group 96 80:01.1 PCI bridge [0604]: Advanced Micro Devices, Inc. [AMD] Starship/Matisse GPP Bridge [1022:1483]
IOMMU Group 96 81:00.0 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] [15b3:1019]
IOMMU Group 96 81:00.1 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] [15b3:1019]
IOMMU Group 96 81:00.2 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 96 81:00.3 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 96 81:00.4 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 96 81:00.5 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 96 81:00.6 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 96 81:00.7 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 96 81:01.0 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 96 81:01.1 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]Card 2:
IOMMU Group 79 c0:01.0 Host bridge [0600]: Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge [1022:1482]
IOMMU Group 79 c0:01.1 PCI bridge [0604]: Advanced Micro Devices, Inc. [AMD] Starship/Matisse GPP Bridge [1022:1483]
IOMMU Group 79 c1:00.0 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] [15b3:1019]
IOMMU Group 79 c1:00.1 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] [15b3:1019]
IOMMU Group 79 c1:00.2 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 79 c1:00.3 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 79 c1:00.4 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 79 c1:00.5 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 79 c1:00.6 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 79 c1:00.7 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 79 c1:01.0 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]
IOMMU Group 79 c1:01.1 Ethernet controller [0200]: Mellanox Technologies MT28800 Family [ConnectX-5 Ex Virtual Function] [15b3:101a]even trying to unbind the card unfortunately does not help me any further. Do I need to get these all in their own group and if so how should I do it? Also, is there a different option to get it passed through to the VM with the RDMA function?The host is running Ubuntu 20.04;
uname -a
Linux hostserver1 5.4.0-135-generic #152-Ubuntu SMP Wed Nov 23 20:19:22 UTC 2022 x86_64 x86_64 x86_64 GNU/LinuxThank youMake sure baremetal (Hypervisor) and Vm’s are running our MLNX_OFED driver.Our driver embed as well the supported FW.The error you are getting (posted below) points to an issue with the vfio driver and iommu group and not related to SRIOV/Nvidia (non Nvidia developed).error: Failed to start domain Ubuntu_testerror: internal error : qemu unexpectedly closed the monitor: 2022-12-16T14:28:08.442593Z qemu-system-x86_64: -device vfio-pci,host=0000:81:00.2,id=hostdev0,bus=pci.7,addr=0x0: vfio 0000:81:00.2: group 96 is not viable.Some pointers below:Please ensure all devices within the iommu_group are bound to their vfio bus driver.Does the dmesg/syslog file report 0000:81:00.2 being added to group 96?Do you see this virtual function (0000:81:00.2) under /sys/kernel/iommu_groups/96/devicesDoes your /proc/cmdline has iommu=pt & _iommu=on?Is IOMMU at the BIOS set to ON, AUTO or DISABLED?Are you getting the following warning “Warning: Your system has booted with the PCIe ACS Override setting enabled. The below list doesn’t not reflect the way IOMMU would naturally group devices.
To see natural IOMMU groups for your hardware, go to the VM Settings page and set the PCIe ACS Override setting to No”.Sophie.You can also configure SRIOV or PCIe passthrough as options.Sophie.Hi! Sorry for later reply, had a long vacation :) Best wishes for 2023!Make sure baremetal (Hypervisor) and Vm’s are running our MLNX_OFED driver.Our driver embed as well the supported FW.Not sure where I can get the version information, however using mlxconfig I am able to set the SRIOV_EN and NUM_OF_VFS, followed by:to activate the VFsThe error you are getting (posted below) points to an issue with the vfio driver and iommu group and not related to SRIOV/Nvidia (non Nvidia developed).error: Failed to start domain Ubuntu_testerror: internal error : qemu unexpectedly closed the monitor: 2022-12-16T14:28:08.442593Z >qemu-system-x86_64: -device vfio-pci,host=0000:81:00.2,id=hostdev0,bus=pci.7,addr=0x0: vfio >0000:81:00.2: group 96 is not viable.Some pointers below:Please ensure all devices within the iommu_group are bound to their vfio bus driver.I will have to take a look into this.Does the dmesg/syslog file report 0000:81:00.2 being added to group 96?
It does:These are all the VF’s, spread over 2 ports and 2 network adapters (so total of 16 VFs)Do you see this virtual function (0000:81:00.2) under /sys/kernel/iommu_groups/96/devices
I do see them indeed:Does your /proc/cmdline has iommu=pt & _iommu=on?
I also do see these enabled:Is IOMMU at the BIOS set to ON, AUTO or DISABLED?Both SRIOV and IOMMU are set to a hard ENABLED.Are you getting the following warning “Warning: Your system has booted with the PCIe ACS Override setting enabled. The below list doesn’t not reflect the way IOMMU would naturally group devices.
To see natural IOMMU groups for your hardware, go to the VM Settings page and set the PCIe ACS Override setting to No”.I was unable to find anything that would indicate this warning message in DMESG/SyslogA thing to add, I am currently assigning 1 VF to the VM. - I am aware that things being in the same IOMMU group could cause issues in some cases and I am wondering if that would be the case here?Anything that could guide me into the right direction to get it to work is appreciated a lot!Since other post is still under review, it maybe? out of order. Anyway:I got it working, I got the whole card under the VM but this is wrong.Since all VF’s and the whole card is within the same IOMMU group, it was so far the only way to do so. However I only need 1 VF port onto the VM. I assume the VF functionally supposed to be able to do this?Additional information: The server uses an AMD EPIC Processor, and I am aware with AMD it may not always do IOMMU groups nicely, but it could also be that I am just missing the biggest flag to make them go into separate IOMMU’s.Hope you can advice on this, thank you!Please ensure all devices within the iommu_group are bound to their vfio bus driver.You can try this solution:
https://enterprise-support.nvidia.com/s/article/PCIe-AER-Advanced-Error-Reporting-and-ACS-Access-Control-Services-BIOS-Settings-for-vGPUs-that-Support-SR-IOVHi michaelsav,Thank you, this looks very promising, I done some tests, was able to assign 2 VF’s separately now too to the VM, and assigned IP addresses for testing if I was able to reach the other side.So far with these options (mainly AER and ACS enabled as others where already enabled) this looks very promising.I have not yet been able to test RDMA yet, as I can do that on Friday.So far, it looks very promising. I will mark this as a solution towards the main question (aka: how to get them in their own IOMMU).Unfortunately I do hit a different issue as RDMA does not work.[  188.565871] rping[2090]: segfault at 0 ip 00007f8300347490 sp 00007f82fffdcbe0 error 4 in librdmacm.so.1.3.43.0[7f8300341000+15000]
[  188.565883] Code: 00 4c 89 ff 44 8b 18 45 85 db 0f 84 8a 00 00 00 e8 d5 ca ff ff 89 45 14 85 c0 0f 85 8a 00 00 00 48 8b bd 50 01 00 00 48 8b 07 <48> 8b 00 44 8b 50 14 45 85 d2 0f 85 60 fa ff ff e8 8b be ff ff 89I found a post related to this at I am trying to setup CX5 RDMA in between two KVM guests (one in each physical node) but failed with rping segfault ... in librdmacm.so.1.1.20.2. Is there any articles here I can follow to resolve the issue?Which is basically what I am also trying to do. However, I am running firmware 16.35.2000 (latest) yet I cannot set GUID.As in this post, I also do get an operation not permitted MFE_CR_ERROR.Do you have an alternative guide on setting the GUID? as links within this forum post are no longer working.Thank you!Hi All,Managed to solve it. Documentation may not be fully up to date?All I had to do was:
echo 00:11:22:33:44:55:1:0 > /sys/class/net/enp193s0f0np0/device/sriov/0/nodefor each VF, in the documentation it was /sys/class/infiniband/mlx5_x - but here I could not find anything related to VF that was having infiniband/device/sriov/0/node (only the master/physical ports had this)Thank you again for your support!Can you please point the relevant section in the doc?
We will make sure to fix this.Hi,https://docs.nvidia.com/networking/pages/viewpage.action?pageId=80581196I see there is no documentation here for the 5.8, but up to 5.6 it states the following:echo 00:11:22:33:44:55:1:0 > /sys/class/infiniband/mlx5_0/device/sriov/0/nodeecho 00:11:22:33:44:55:2:0 > /sys/class/infiniband/mlx5_0/device/sriov/0/portUnfortunately, there are no ports at mine, both mlx5_0 and 1 contains the main physical interface but not the VF ports.However after poking around I did found it was on the interface itself for this system:
echo 00:11:22:33:44:55:1:0 > /sys/class/net/enp193s0f0np0/device/sriov/0/nodeThat worked and changed the guid :).This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
988,connectx-6-dx-adaptor-clock-ppm-configuration,"Hi,We are looking to tune the Mellanox NIC’s clock PPM value for ConnectX-6 Dx Adaptors. I found a reference to similar configuration for older adaptors as belowmlxconfig -d /dev/mst/mt4117_pciconf0 set PPM_P1=-200But when I try that for ConnectX-6 Dx Adaptor, I get an error as below.  Would you know if the parameter is changed for new Adaptors?  I tried “mlxconfig -d  query” which shows all supported parameters, PPM_P1 is not listed and nothing seems to be close to what PPM_P1 would be doing. Appreciate any suggestions. Thank you.mlxconfig -d /dev/mst/mt4125_pciconf0 set PPM_P1=-200Device type:    ConnectX6DX
Name:           MCX623105AC-VDA_Ax
Description:    ConnectX-6 Dx EN adapter card; 200GbE; Single-port QSFP56; PCIe 4.0 x16; Crypto and Secure Boot
Device:         /dev/mst/mt4125_pciconf0Configurations:                                      Next Boot       New
-E- Unknown Parameter: PPMWhere did you see the reference to the older devices config?Thank you for the response. Correct. It was on older devices.  Just an update, I just got roped into another thread with Nvidia Support and learned that this parameter has been deprecated.Powered by Discourse, best viewed with JavaScript enabled"
989,cumulus-5-3-on-spectrum-configuration-docs-errors,"I am working on putting a pair of SN2010 switches running Cumulus into a MLAG pair but before even getting that far I’ve been trying to get used to NVUE for all configuration changes.According to the docs (and reinforced by NVUE command errors) on the Spectrum chipset the link speed cannot be set to anything other than auto, and I have ran into repeatable instances where by unless I manually edit the network interfaces file to contain “link-speed 1000”, I cannot get the link state to show as up.I have coming out of a 25G port a 1000G SFP to Ethernet adapter module that says it’s supported but even if I use 1G fiber to 1G fiber, I cannot get a link to come up unless I manually specify the link-speed.As you can imagine, this is a problem because when I run NVUE commands that edit interfaces file, it strips away the manual configurations.All in all, I have to say the frustration with 5.3 is making my head spin lolI’m really sorry to hear about your experience.
Can you show me the errors you’re seeing?
I’m working virtually in Cumulus VX but the result should be the same in real hardware and if it’s not I want to make sure a bug is filed capturing exactly this. For what it’s worth this is what I see in VX v5.3.1.I am not having any issues with port state provided I have speed set and that’s working fine now provided I use “1G” instead of “1000”.Powered by Discourse, best viewed with JavaScript enabled"
990,is-there-a-way-to-show-what-sfp-adapters-are-installed-without-pulling-them-from-the-card,"I currently have ConnectX-2 cards installed on my servers with SFP adapters installed. I would like to query the SFPs and find out what they are without pulling them from the machines.The servers are running Ubuntu and ethtool does not report any information about the SFPs. Is there a tool or command that I can use to obtain this information?Hello,There isn’t a tool for that as far as I know if the NIC runs on ethernet. if it is running on IB you can use the latest ibdiagnet tool that comes from ibutils2 and use the flag --get_cable_infoat one time, you could use mst for this
start mstthenHowever this appears to hav e been deprecated in the mlx5_core driver kit.
I cannot figure out how to see my QSFPs manufacturer/model or examine optical quality with the current toolset.Powered by Discourse, best viewed with JavaScript enabled"
991,configuring-nic-with-0-rx-queue-tx-queues-only-on-dpdk-21-11,"Hi,I am using DPDK on a Connectx-5 to generate packets at very high bandwidth.Because I only need to send packets, I am using one or more TX queues, but no RX queues. This worked fine using up to DPDK 21.08, but does not work anymore after updating to DPDK 21.11. I am getting the error mlx5_net: port 0 cannot allocate rxq private data when calling rte_eth_dev_configure.After debugging, I found that the commit 4cda06c3c35e004a333e98f83786ef3a16260ad4 of DPDK introduced my issue (drivers/net/mlx5/mlx5_ethdev.c:108), by trying to allocate private memory for RX queues even if rxqs_n (the number of RX queues requested) equals 0. Allocating 0 bytes of memory obviously returns a NULL pointer, triggering the error.Q: Should it be allowed to allocate only TX queues on a Mellanox NIC ? If yes, I could provide a simple code path for the case where rxqs_n==0. But I fear breaking more code…I want to know if this is a bug or expected behaviour, and what is the best way to fix / best workaround.I know that I can allocate an unused RX queue, but this would cost the mempool, and the flow API, so a bigger footprint and complexity to this simple application.I attached a MWE triggering the error on DPDK 21.11 but working fine with DPDK 21.08.I run it with # ./no_rx_queue -a 84:00.0. Here is my PCI configuration:Thanks for any help,Julienno_rx_queue.c (1 KB)I changed code from mlx5_ethdev.c at line 108 from:toThis works fine for my problem, and I can send packets without any issue. I can write a pull request, but could I have confirmation that this is not supposed to break anything ? I am not familiar with the whole mlx5_net driver, and do not have time to practice in-depth checks for this.Thanks,Julienhi Julien,Thank you for pointing out unexcepted result with ‘rxq=0’.Compared to v20.11, it seems v21.11 adding more strictercheck for ‘priv->rxq_privs’, while bring unexpected result for rxq=0.I’m confirming internally and will feedback here when there is an update.​Levei​​Hi Julien,Could you write an email about this issue and send it to networking-support@nvidia.com through your working email?In the email, you can include message ""assign this case to Levei Luo’ . So, I can get this case to continue.Regards,LeveiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
992,url-filter-loading-dpi-signature-failed,"Hi I’m trying to run the URL Filter app, specifically I’m trying to add some suricata rules that I got from
/opt/mellanox/doca/examples/application_recognition/bin/doca_application_recognition/ar_suricata_rules_exampleBut the “commit” command fails for some reason.signature.txtHello,Based on the output provided, we would recommend opening a support case for further investigation of the issue. If you do not have a current support contract, please reach out to our Contracts team at networking-contracts@nvidia.comThank you and regards,
~NVIDIA Networking Technical SupportI solved , the regex engine was inactiveThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
993,im-looking-for-mlnx-os-interoperability-matrix,"Hello, we have a Infiniband Network with 6 SX6036 36port FDR switches and running MLNX-OS 3.6 and 3.4 and want to migrate to two QM8700 HDR Switches with MLNX-OS 3.10… What is the best practice for migration, enable the connection, move the SM to the new switches and plug the cables one by one to new switches, are there knows problems with different os versions. i can’t find any information about OS  Interoperability on the support pagesFirst make sure you have the correct compatibility between the QM8700 FW versions (those are listed in the MLNX-OS relesae notes) or with the output of “show asic-version”Updating Firmware for Quantum™ Based InfiniBand Switch PlatformsFor example:https://docs.nvidia.com/networking/display/NVIDIAQuantumFirmwarev2720106102/Firmware+Compatible+ProductsHello Eddies, thanks for your answer. looks like that the firmware on the old SX Switches
9.3.5080  not compatible is with the new QM switches  27.2010.4210,
Problem is that we need to run the SM on the new switches to disconnect the old ones,
is there a possibility to move the SM to new switches without HA?Hi Udo,You can enable the SM on the new switches with a higher priority then the old SM on the SwitchX switches.Powered by Discourse, best viewed with JavaScript enabled"
994,rhel9-and-full-sriov-support,"re: https://docs.nvidia.com/networking/display/RHEL9/General+SupportIs this document to say there is not IB support over SR-IOV?My organization is looking to upgrade to RHEL9 bypassing RHEL8 as we are currently on CentOS7. We currently have libvirt VMs using SR-IOV to communicate with our infiniband network so I am hoping I read that wrong.Appreciate the help!Hi @JS1234 ,Please check the SR-IOV Support section in the document that you shared:""
Running InfiniBand (IB) SR-IOV requires IB Virtualization support on the OpenSM (Session Manager).
This capability is supported only on OpenSM provided by NVIDIA, that is not available Inbox.
This support can be achieved by running the highest-priority OpenSM on a NVIDIA switch in an IB fabric.
The switch SM can support this feature by enabling the virt flag (# ib sm virt enable).Please note that this capability is not tested over the Inbox environment and is considered a technical preview.
""To summarize, this functionality is supported with MLNX_OFED driver, which can be downloaded from this link: Linux InfiniBand DriversFor more information and configuration steps, please review the driver’s User Manual:https://docs.nvidia.com/networking/pages/viewpage.action?pageId=127764834#SingleRootIOVirtualization(SRIOV)-ConfiguringSR-IOV(InfiniBand)Regards,
ChenThank you for the update. Ill review the documentation. Appreciate the help!Powered by Discourse, best viewed with JavaScript enabled"
995,cpu-overhead-is-being-utilized-at-maximum-capacity-over-infiniband-on-file-transfer-applications,"I am running a file transfer over Infiniband Ethernet and using Reliable Communication (RC) for data transfer. I am noticing that Infiniband CPU overhead is at maximum regarding large file transfers or any file transfer with large buffer size. Infiniband CPU overhead reaches 100% over a single thread and even distributing it on multiple threads creates the same scenario. Is it a usual behavior of InfiniBand applications?Hello,Many factors can be related to performance related issues and CPU overhead (IE OS/Kernel/Nvidia driver/FW/- BIOS/OS/HCA tuning, tool(s) utilized for testing (ib_write/read/send_bw), server architecture, PCIe (Gen/Width) etc…You can refer to our community site for articles about these corresponding tuning to start with as basics.
I would also recommend using our latest IB Nvidia driver/FW.Are you checking the CPU utilization via top/htop? What tool(s) are you using to measure bandwidth and CPU utilization?What type of ConnectX card?Are you exhibiting the same issue with Datagram versus RC?Your issue might need to be dissected if needed which will require to open a case with a valid support contract to further analyzed & debug as necessary.Powered by Discourse, best viewed with JavaScript enabled"
996,multiple-ibv-contexts-in-a-single-process,"I am using RDMA verbs in Mellanox OFED 5.x. I am wondering that whether it is safe to allocate multiple ibv_contexts for a Infiniband device by calling ibv_open_device() in a single process [i.e., a ibv_context per thread]. Is there any potential race/undefined behavior? Thank you.Hello alogfans,Thank you for posting your inquiry to the NVIDIA Developer Forums!Using multiple contexts is possible within a single process (single threaded), or a multi-threaded application; however, it is not necessary. libibverbs is thread-safe. The below blog post goes into more detail on thread-safe functions/features, locking mechanisms, and internal implementation:Let's start with the bottom line: the verbs API is fully thread safe and verbs can be called from every thread in the process. Part of the thread safe is implemented at the libibverbs level and part of it is implemented at the low-level driver...
Est. reading time: 2 minutes
If you’re in need of further development assistance, or require assistance in building out a deployment, I would highly recommend engaging your account team. They can help put you in touch with resources within our Solutions and Engineering team, and provide you with a support bundle to suit your needs.Have a great day,
NVIDIA Networking SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
997,example-projects-for-innova2-flex-smartnic,"I have an Innova2 Flex SmartNIC plugged into an x86 Ubuntu server and I am new to it. So I wonder if there is any example project I can start with and run it on my testbed. Or any example to show how to develop the SmartNIC?Any help would be much appreciated.Thank you!Hello Heng Yu,Many thanks for posting this question on the Mellanox Community.Based on your request, please consult the following Innova-2 Flex SmartNIC documentation → https://docs.mellanox.com/display/Innova2FlexThe documentation also contains a section on how to use the Innova-2 Flex SmartNIC → https://docs.mellanox.com/display/Innova2Flex/Using+the+FPGA+on+the+Adapter+CardIf you have any further question, please do not hesitate to open a Mellanox Support case for you inquiries.Many thanks,~Mellanox Technical SupportHello Martijn,I’m seeking an HLS development environment from Mellanox which contains the AXI-Stream interfaces which would allow for network processing on the FPGA as a bump in the wire.I believe there should be a top level wrapper provided by Mellanox which developers can use to wrap their HLS IP logic in before performing synthesis, place & route, and bitstream generation.However, I was not able to find these items in the provided Mellanox DDR4 example.Could you please provide a reference or point me to an example of an HLS development environment which can be used?Thanks, in advance!HamedI have a basic demo project for the Innova-2 on GitHub.Powered by Discourse, best viewed with JavaScript enabled"
998,hdr-in-edr-fat-tree,"I have an existing all EDR Fat tree topology and am planning to upgrade the spine swithes to HDR. In the meantime, I have a need to temporarily add an additional leaf switch. Is it possible to add an HDR leaf switch to an EDR spine? Understanding that I would take the performance hit, would there be compatibility issues?Powered by Discourse, best viewed with JavaScript enabled"
999,opensm-pkey-table-is-full,"opensm saysWe are not using partitions.Although I note that opensm.conf has the lineThat file does not exist.Anyhow, why this particular client? There are ~900 nodes in this fabric.Then there is the  error, probably relatedThe ‘src’ does not show up with ERR 0512, though.What might be behind this?Regards
ThomasThis host was crazy, quite simply. Reboot has helped.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1000,sn2010-switchdev-and-asic-profile-large-chunks,"HiDo I need ‘lange_chunks’ in my ASIC profile when doing basic peering routing, no more than 32 members of a nexthop group.Any suggested sample configs? Mostly optimising for v4 and v6 peering routes.Thinking about using this config for SN2010.devlink resource set pci/0000:01:00.0 path /kvd/linear size 48128devlink resource set pci/0000:01:00.0 path /kvd/linear/singles size 16128devlink resource set pci/0000:01:00.0 path /kvd/linear/chunks size 32000devlink resource set pci/0000:01:00.0 path /kvd/linear/large_chunks size 0devlink resource set pci/0000:01:00.0 path /kvd/hash_single size 160768devlink resource set pci/0000:01:00.0 path /kvd/hash_double size 49152devlink dev reload pci/0000:01:00.0From: Resource Management · Mellanox/mlxsw Wiki · GitHubLinear-Large-Chunks:	Multiple nexthops for multipath routes requiring up to 512 entries eachSeems there is no ‘Issues’ section on Github for this project though.Does anybody have config examples?Please open a support request so we can better look into this question for you.Please point me at that process.Use the file a support case link under enterprise support on the following page:Find help for gaming, graphics, or enterprise products.We also have phone numbers listed on this page:Support for developers, forums, solutions and licensing information for NVIDIA AI Enterprise, vGPU, Omniverse, DGX and more.Seems to ask for a login.Someone should have received a link to create your account in the welcome email when your support contract was first created. I DM’ed you a link that should work if you do not have that welcome email.Does support require a paid account of sorts?I’m just hoping to run Debian on a switch. All open source.Cumulus Linux is the closest thing to Debian running on a switch. It takes a linux networking config and programs it into the ASIC and it functions similar to a server with dozens of ports that can switch/route traffic at line rate.With ONIE you can install a NOS, but to take advantage of the switch hardware you would need to write/acquire a NOS that can do this.With just Debian installed on the switch and only open source software, you might be able to use it as a low powered linux server, but hardware designed to work as a server instead of a switch would perform much better.Switchdev support would be through a support contract or other agreement with NVIDIA. Sales would be the best place to find out more details. Switchdev support is not something you can simply order on our website, it is different from the way you can easily order a Spectrum based switch with Cumulus Linux and X years of Y level of support.How would I go about contacting the people who wrote this?
https://github.com/Mellanox/mlxsw/wiki/Resource-ManagementThe following page of the wiki has contact information at the bottom:Contribute to Mellanox/mlxsw development by creating an account on GitHub.Powered by Discourse, best viewed with JavaScript enabled"
1001,slurm-configuration-issue-with-pmix,"Hi FolksBelow is the bash file I am trying to run inside a enroot container using pyxis plugin with srun , one python command is executing but other fails , please refer the file below and trace .Is it possible to simplify the jobscript in such a way that we will be able reproduce the issue without your src data?I guess my issue is similar to this one , runnning any 2 executables python or exe in a bash script.
Can we achieve this by configuring 2 different ranks for 2 different processes ?version of Open MPI: 4.0.1

installation from source (.tar.gz)

Operating sy…stem/version: Ubuntu 18.04

-----------------------------

I have a mpi-aware executable which I run by a command like 
``` shell
mpirun -np 1 ./exe
```

It's the same, If I open a file named `run.sh` and in it I write:
``` shell
./exe
```
and, I execute by
``` shell
mpirun -np 1 bash run.sh
```

But when I want  to execute `./exe` two consecutive times by modifying `run.sh` as
``` shell
./exe
./exe
```
and invoking by the same `mpirun -np 1 bash run.sh` command, the second `./exe` fails with the following error:
``` shell
[amir-MS-7A37:11554] OPAL ERROR: Unreachable in file ../../../../../openmpi-4.0.1/opal/mca/pmix/pmix3x/pmix3x_client.c at line 112
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
```Powered by Discourse, best viewed with JavaScript enabled"
1002,when-using-the-connectx-lx-en-in-vmware-with-sr-iov-are-there-any-settings-on-the-host-related-to-pause-frames-and-or-ring-buffer-size-that-effects-overrides-the-guest,"Typically when the NIC is used by the host using vmxnet3, you will get improved performance if you change settings on the NIC using the host exscli. I am using a VF for SR-IOV and I am not sure if I need to change the vmnic settings on the host or if the guest completely controls this behavior and there would be no performance improvement changing the vmnic settings.Hello Klassen,Thank you for posting your inquiry on the NVIDIA Networking Community.OOB, our adapters are already configured for the highest performance, you still are able to change certain settings based on the network traffic pattern in the fabric. You can use the following syntax to retrieve the module parameters, based on the ESXi version in use.Based on the retrieved list and the purpose for the parameters from the UM, you can adapt to your own satisfactionThank you and regards,~NVIDIA Networking Technical SupportThanks for the reply. I think I have seen that command before but I didn’t see anything related to ring buffer settings. Please correct me if I missed something. From your response it would appear that no device settings are needed the performance is already optimal.Thanks again.Powered by Discourse, best viewed with JavaScript enabled"
1003,performance-test-finding-bottleneck-and-optimization,"Hello,
I want to make a single-core performance test for Mellanox ConnectX-5 with dpdk-testpmd.Here are the installed versions of drivers and firmware:
DPDK 21.11
MLNX_OFED_LINUX-5.5-1.0.3.2-rhel8.0-x86_64 (I am using Centos 8 Stream)
Mellanox Firmware 16.32.20.04Hardware Info:
CPU : 2x AMD 7H12 (2.6GHz up to 3.3) with 256GB (DDR4) MemoryTest Setup:
I am using the T-rex traffic generator that uses the below profile :Generated Traffic info is below:
PPS    :      11.74 Mpps
CPS    :     437.65 Kcps
BPS    :      45.08 GbpsI found that 60K of 1.2 B packets are discarded (shown as rx_phy_discard_packets). When I decrease the amount of traffic, I can get 0 rx_phy_discard_packets for 20Gbps, but there are various reports showing 0 discarded packets for higher throughput (in terms of pps and bps).How can I find the bottleneck point to optimize performance?
This is the running command:
./dpdk-testpmd -l 20-24 -n 4 -a 41:00.0 -a 41:00.1 --socket-mem=8192 – --socket-num=0 --burst=64 --txd=8192 --rxd=8192 --mbcache=512 --rxq=1  --txq=1 --nb-cores=1 -i -a --rss-udp --disable-crc-strip --record-core-cycles --record-burst-statsThank youI found my problem, it is about the BIOS settings. Changing to full performance is solved.
Thanks all.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1004,setpci-not-working-on-a-passthrough-mellanox-nic-in-an-esxi-vm,"I setup the connect-x 5 NIC dedicating to one VM via the passthrough mode in VMware ESXi, but setpci is not working on all registers of the NIC. I have tried setting up PCI register on another PCIe device, and it works. Could any one can help with it?Incidentally,  the platform information is listed as follows.
NIC model: MCX515A-CCAT
vmware version: ESXi 6.0
OS version: Ubuntu 20.04 LTS
MLXN OFED version: MLNX_OFED_LINUX-5.8-2.0.3.0-ubuntu20.04-x86_64
Firmware version: fw-ConnectX5-rel-16_35_2000-MCX515A-CCA_Ax_Bx-UEFI-14.28.16-FlexBoot-3.6.805.binHello,Which registers are you trying to change via setpci?
Did you confirm with VMware that these registers are allowed by ESXi?
Some registers will simply be forwarded to real NIC while some will be blocked, that’s hypervisor’s policy.Best Regards,
VikiHi Viki @vikiz,Thanks for replying. I was trying to setup the MaxReadReq register. Currently, the value is 128B, and I want to change it to 512B. However, the value simply does not change. I have doubled checked the configurations and VMware ESXi Manual, it seems everything is just fine.Is there any more hints for solutions that I can try?Thanks,
WendiPowered by Discourse, best viewed with JavaScript enabled"
1005,how-to-set-power-management-mode-programically,"Hi all, following this post - I’m trying to use nvapi.dll to set the power management mode, but when I’m looking on the export table of this DLL file I don’t see any function except nvapi_QueryInterface.More specifically - I’m looking for the NvAPI_DRS_SetSetting function, that should be there according to this documentationCan someone help me to find a way to change the power management mode from software?Thankshi lioryou can download nvapi SDK at https://developer.nvidia.com/rtx/path-tracing/nvapi/get-started
Should have such interface.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1006,how-to-connect-nvidia-infiniband-adapter-with-nvidia-gpu-card-directly,"I have a idea about build a docking station, to separate GPU from SFF-size workstation, which the power supply and heat exchange in air cooling is limited. the main function of this docking station is the PCI-E socket connection board to build a communication between eGPU and infiniband adapter(SFF PC–Adapter–Adapter–GPU) as simple as possible. Is that will be availble to connect Nvidia infiniband adapter and Nvidia RTX GPU card DIRECTLY via PCI-E socket connect board(contain power supply and PCI-E lane retimer)   in technical? Will the chips in Nvidia infiniband adapter can be the controller like PCI-E controller and DMA controller, what there function are in regular mother board construction by specific change of configuration? Or to make it work, what external circuit should it have in PCI-E socket connection board? Or another solution to make it?Powered by Discourse, best viewed with JavaScript enabled"
1007,error-running-the-innova-2-flex-open-application,"The application cannot find the ConnectX device. I am not familiar with setting up Mallanox nic card. This Linux system we using already has a mlx5_core driver loaded upon power-up.Any suggestion would be very helpful.Here is more detailApplication Errormaster001:~/9.MI2/Innova_2_Flex_Open_18_12/app # ./innova2_flex_app -v===============================================Verbosity: 1BOPE device: NoneConnectX device: NoneCannot find appropriate ConnectX devicelspci Commandmaster001:~/9.MI2/Innova_2_Flex_Open_18_12/app # lspci -v | grep Mellanox03:00.0 PCI bridge: Mellanox Technologies Device 1974 (prog-if 00 [Normal decode])04:08.0 PCI bridge: Mellanox Technologies Device 1974 (prog-if 00 [Normal decode])04:10.0 PCI bridge: Mellanox Technologies Device 1974 (prog-if 00 [Normal decode])05:00.0 Class 2000: Mellanox Technologies Device 0264Subsystem: Mellanox Technologies Device 026406:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5, PCIe 3.0]Subsystem: Mellanox Technologies Device 004606:00.1 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5, PCIe 3.0]Subsystem: Mellanox Technologies Device 004608:00.0 Network controller: Mellanox Technologies MT27500 Family [ConnectX-3]Subsystem: Mellanox Technologies Device 016cMore Detailsmaster001:~/9.MI2/Innova_2_Flex_Open_18_12 # lspci -vvv -s 06:00.006:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5, PCIe 3.0]Subsystem: Mellanox Technologies Device 0046Control: I/O- Mem+ BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx-Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- SERR- <PERR- INTx-Interrupt: pin A routed to IRQ 26Region 0: Memory at c2000000 (64-bit, prefetchable) [size=32M]Expansion ROM at c7400000 [disabled] [size=1M]Capabilities: [60] Express (v2) Endpoint, MSI 00DevCap: MaxPayload 512 bytes, PhantFunc 0, Latency L0s unlimited, L1 unlimitedExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset+DevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported-RlxdOrd- ExtTag- PhantFunc- AuxPwr- NoSnoop+ FLReset-MaxPayload 256 bytes, MaxReadReq 512 bytesDevSta: CorrErr+ UncorrErr- FatalErr- UnsuppReq+ AuxPwr- TransPend-LnkCap: Port #0, Speed unknown, Width x16, ASPM not supported, Exit Latency L0s unlimited, L1 <4usClockPM- Surprise- LLActRep- BwNot-LnkCtl: ASPM Disabled; RCB 64 bytes Disabled- CommClk+ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-LnkSta: Speed unknown, Width x16, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-DevCap2: Completion Timeout: Range ABC, TimeoutDis+, LTR-, OBFF Not SupportedDevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis-, LTR-, OBFF DisabledLnkCtl2: Target Link Speed: Unknown, EnterCompliance- SpeedDis-Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-Compliance De-emphasis: -6dBLnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete+, EqualizationPhase1+EqualizationPhase2+, EqualizationPhase3+, LinkEqualizationRequest-Capabilities: [48] Vital Product DataProduct Name: Innova-2 Flex Open for Application Acceleration, dual-port SFP28, 25GbE, KU15P, No Crypto, PCI4.0 x8, HHHL, active heat sink, tall bracket, ROHS R6Read-only fields:[PN] Part number: MNV303212A-ADLT[EC] Engineering changes: AA[V2] Vendor specific: MNV303212A-ADLT[SN] Serial number: MT2045X09746[V3] Vendor specific: b8d4ac17c321eb118000043f72e6e896[VA] Vendor specific: MLX:MODL=NV303212A:MN=MLNX:CSKU=V2:UUID=V3:PCI=V0[V0] Vendor specific: PCIeGen4 x8[RV] Reserved: checksum good, 1 byte(s) reservedEndCapabilities: [9c] MSI-X: Enable- Count=64 Masked-Vector table: BAR=0 offset=00002000PBA: BAR=0 offset=00003000Capabilities: [c0] Vendor Specific Information: Len=18 <?>Capabilities: [40] Power Management version 3Flags: PMEClk- DSI- D1- D2- AuxCurrent=375mA PME(D0-,D1-,D2-,D3hot-,D3cold+)Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=0 PME-Capabilities: [100 v1] Advanced Error ReportingUESta: DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-UEMsk: DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-UESvrt: DLP+ SDES- TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-CESta: RxErr- BadTLP- BadDLLP- Rollover- Timeout- NonFatalErr-CEMsk: RxErr- BadTLP- BadDLLP- Rollover- Timeout- NonFatalErr+AERCap: First Error Pointer: 04, GenCap+ CGenEn- ChkCap+ ChkEn-Capabilities: [150 v1] Alternative Routing-ID Interpretation (ARI)ARICap: MFVC- ACS-, Next Function: 1ARICtl: MFVC- ACS-, Function Group: 0Capabilities: [180 v1] Single Root I/O Virtualization (SR-IOV)IOVCap: Migration-, Interrupt Message Number: 000IOVCtl: Enable- Migration- Interrupt- MSE- ARIHierarchy+IOVSta: Migration-Initial VFs: 8, Total VFs: 8, Number of VFs: 0, Function Dependency Link: 00VF offset: 2, stride: 1, Device ID: 1018Supported Page Size: 000007ff, System Page Size: 00000001Region 0: Memory at 0000000000000000 (64-bit, prefetchable)VF Migration: offset: 00000000, BIR: 0Capabilities: [1c0 v1] #19Capabilities: [230 v1] Access Control ServicesACSCap: SrcValid- TransBlk- ReqRedir- CmpltRedir- UpstreamFwd- EgressCtrl- DirectTrans-ACSCtl: SrcValid- TransBlk- ReqRedir- CmpltRedir- UpstreamFwd- EgressCtrl- DirectTrans-Capabilities: [320 v1] #27Capabilities: [370 v1] #26Capabilities: [420 v1] #25Kernel modules: mlx5_coreHello Amrish,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, please open a support ticket by sending an email to networking-support@nvidia.comWe will assist you further through the support ticket.Thank you and regards,~NVIDIA Networking Technical SupportThe ConnectX and BOPE devices are not found. Good news is the driver looks to be installed: Kernel modules: mlx5_coreThe ConnectX device (mlx5_fpga_tools) is created by running sudo insmod /usr/lib/modules/uname -r/updates/dkms/mlx5_fpga_tools.koThe BOPE device is created by running sudo ~/Innova_2_Flex_Open_18_12/driver/make_deviceTry the following:Powered by Discourse, best viewed with JavaScript enabled"
1008,cannot-send-masked-atomic-compare-and-swap-rdma-requests,"Hello,I’m having trouble sending a masked atomic compare-and-swap request with the RDMA verbs API. I’ve included the header file “verbs_exp.h” which has added support for these masked atomic requests.My client is facing a segmentation fault upon calling “ibv_exp_post_send” to send this request type. Using the same RDMA verb, I am able to send one sided READ and WRITE and regular atomic compare-and-swap requests, however, I’m facing issues with sending masked atomic requests.gdb points to the following, which hasn’t been particularly helpful:Program received signal SIGSEGV, Segmentation fault.
0x00007ffff78441a8 in perror_internal (fp=fp@entry=0x607260,
s=s@entry=0xfffffffff797d156 <error: Cannot access memory at address 0xfffffffff797d156>, errnum=errnum@entry=22)
at perror.c:32
32	perror.c: No such file or directory.I initially thought this may be a hardware limitation. However, according the the RDMA Programming User Manual (v1.7), these extended atomic operations are supported by ConnectX®-2 and subsequent hardware.My testbed uses ConnectX-4 Lx 40G NICs and MLNX OFED 4.9-7.1.0.0-LTS run Ubuntu 18.04 and kernel 5.4.0-126-generic.I’ve attached my source code and Makefile, in case it assists with debugging. I’ve also included instructions on how to run the program at the top of the source code file.I’d greatly appreciate any advice anyone could provide!Thank you for your time.HamedMakefile (99 Bytes)
rdma_rw-RC.c (37.1 KB)Powered by Discourse, best viewed with JavaScript enabled"
1009,rshim-17469-another-backend-already-attached,"Hi,I’m trying to re-image the DPU using bfp-install utility which comes by default as part of rshim driver package on Centos. But I’m seeing rshim driver reporting an error “another backend already attached”.Apr 26 04:33:07 5a9s9-node4 rshim[17469]: Probing pcie-0000:81:00.2Apr 26 04:33:07  rshim[17469]: create rshim pcie-0000:81:00.2Apr 26 04:33:08  rshim[17469]: another backend already attachedApr 26 04:33:08  rshim[17469]: USB device detectedThis causes no rshim device entry(/dev/rshim0) and network interface(tmfifo_net0) for DPU at host machine. So, without these we cannot access DPU from host.One important observation occurred during re-imaging is that re-image using bfb-install got stuck for 5+ hours and to come out of the stuck bfb-install command, I had issued reboot of host machine.I have tried to solve this issue by referring to NVIDIA troubleshoot guide and work-around suggested in this forum for other users. But the fixes suggested at above links didn’t solve the issue.So, I started looking into rshim driver source code to understand when driver reports this error.From rshim driver code here rshim/rshim.c at 6dc1c010e809ab744dc6f387e6804ad61498d9c9 · Mellanox/rshim · GitHub , the reason for this error is one of the rshim register named ‘RSH_SCRATCHPAD1’ is written a some magic value indicator of some other rshim backend(probably, usb or pcie_live_fish) is holding the access to DPU.I suspected, usb backend could be that other backend which competed with pcie to get access to DPU and overwriting SCRATCHPAD1 register on DPU . So, I explicitly disabled usb backend by adding rshim configuration at ‘/etc/modprobe.d/rshim.conf’ to disable usb & pcie_live_fish backends). But disabling other backends also does not reset register and rshim still reports the same error & not proceeding for device register.Few other things I have tried without success are below:[root]# rshim -b pcie -d pcie-0000:81:00.2 -i 0 -l 4 -fProbing pcie-0000:81:00.2create rshim pcie-0000:81:00.2another backend already attachedI see the same error here also.Any help appreciated.Thanks,
GaneshHello,Can you post “systemctl status rshim”.
Any change if you do a cold reboot? (power cycled the server)
Are you connected via USB or PCIe?
Was it working initially as expected?
Did this issue occurred upon initiating a bfb-install?Sophie.root@5a9s9-node4:~# systemctl status rshim
● rshim.service - rshim driver for BlueField SoC
Loaded: loaded (/lib/systemd/system/rshim.service; enabled; vendor preset: enabled)
Active: active (running) since Fri 2022-05-13 16:17:18 PDT; 1 weeks 2 days ago
Docs: man:rshim(8)
Process: 2184 ExecStart=/usr/sbin/rshim $OPTIONS (code=exited, status=0/SUCCESS)
Main PID: 2201 (rshim)
Tasks: 7 (limit: 309093)
Memory: 3.2M
CGroup: /system.slice/rshim.service
└─2201 /usr/sbin/rshimMay 13 16:17:18 5a9s9-node4 systemd[1]: Started rshim driver for BlueField SoC.
May 13 16:17:18 5a9s9-node4 rshim[2201]: Probing pcie-0000:81:00.2(uio)
May 13 16:17:18 5a9s9-node4 rshim[2201]: Create rshim pcie-0000:81:00.2
May 13 16:17:18 5a9s9-node4 rshim[2201]: rshim pcie-0000:81:00.2 enable
May 13 16:17:19 5a9s9-node4 rshim[2201]: rshim0 attached
May 13 16:17:19 5a9s9-node4 rshim[2201]: USB device detected
May 13 16:17:19 5a9s9-node4 rshim[2201]: Probing usb-3.b
May 13 16:17:19 5a9s9-node4 rshim[2201]: create rshim usb-3.b
May 13 16:17:19 5a9s9-node4 rshim[2201]: another backend already attached
May 13 16:17:19 5a9s9-node4 rshim[2201]: rshim usb-3.b deletedAny change if you do a cold reboot? (power cycled the server)
==> No change.Are you connected via USB or PCIe?
==> PCIeWas it working initially as expected?
==> YesDid this issue occurred upon initiating a bfb-install?
==> Yes. The bfb-install command to re-image DPU got stuck for 5+ hours w/o any progress, so to come out of this state, I issued reboot of host machine. Post reboot, I started to see this issue.Thanks,
GaneshPowered by Discourse, best viewed with JavaScript enabled"
1010,some-nics-not-reaching-its-available-bandwidth-mt28908-connectx-6,"Hey!So we’re facing an annoying problem.We have multiple servers with identical components and identical drivers/software/settings.
However, some NICs are not achieving its required bandwidth while everything is exactly the same.
Even the BIOS settings are identical.Server 1 for example:
[   14.962184] mlx5_core 0000:c4:00.0: firmware version: 20.32.1010
[   14.962244] mlx5_core 0000:c4:00.0: 252.048 Gb/s available PCIe bandwidth (16 GT/s x16 link)
[   14.970402] mlx5_core 0000:c4:00.0: handle_hca_cap:692:(pid 787): log_max_qp value in current profile is 18, changing it to HCA capability limit (17)
[   15.126171] mlx5_core 0000:c4:00.0: Rate limit: 127 rates are supported, range: 0Mbps to 97656Mbps
[   15.144644] mlx5_core 0000:c4:00.0: E-Switch: Total vports 2, per vport: max uc(128) max mc(2048)While server 2
[   14.309102] mlx5_core 0000:c4:00.0: firmware version: 20.32.1010
[   14.309133] mlx5_core 0000:c4:00.0: 63.012 Gb/s available PCIe bandwidth, limited by 16 GT/s x4 link at 0000:c0:03.1 (capable of 252.048 Gb/s with 16 GT/s x16 link)
[   14.316864] mlx5_core 0000:c4:00.0: handle_hca_cap:692:(pid 995): log_max_qp value in current profile is 18, changing it to HCA capability limit (17)
[   14.529035] mlx5_core 0000:c4:00.0: Rate limit: 127 rates are supported, range: 0Mbps to 97656Mbps
[   14.529944] mlx5_core 0000:c4:00.0: E-Switch: Total vports 2, per vport: max uc(128) max mc(2048)As you can see, on server 2, clearly not reaching its bandwidth.The only thing that I can think of is that “maybe” some of the bios settings that have “auto” might be configured differently.
I’ve tried numerous of settings and always end up with the same result.Could use some pointers where to look :)Thank you in advance!Hello elio.vp,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Based on your information provided, please make sure you do not set any BIOS settings to auto. Auto settings do not always guarantee the correct outcome. Make sure all BIOS versions and settings are completely identical.If you are certain that this is all accomplished, then swapping the adapters around would be the next step, before submitting an RMA of the adapter.In the node, which is working as expected, swap out the adapter for one of the known bad performance one and check if the issue follows the adapter. If the issue follows the adapter, we recommend to issue a RMA. If the issue stays with the node, we recommend to contact the system vendor. Please make sure the system vendor, certified the adapter.Thank you and regards,
~NVIDIA Networking Technical SupportHey MvB and thank you for your reply.It’s not the NIC that needs to be RMAd.It’s 100% sure a BIOS setting (or multiple in combination)
This is what I was trying to ask.But it’s fine, we will figure it out.GreetingsElioHello elio.vp,Still there is a possibility that the adapter is at fault. The provided triage steps will determine if it is the system node or the adapter.If it is the adapter, and you still have warranty or a valid support contract on it, please do not hesitate to open a RMA request → https://support.mellanox.com/s/public-rmaThank you and regards,
~NVIDIA Networking Technical SupportNICs were not seated properlyFor anyone else that might run into the same issue.
Check that the NIC is seated firmly enough into the pcie slot.We had several servers where this was not the case and the system downgraded it to either x8, x4 or even x2.CheersThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1011,does-this-ethernet-adapter-card-support-base-r-fec-types,"I had a few questions regarding the Nvidia ConnectX6 smartNIC.Does this ethernet adapter card support base-r fec types? I have been observing blanket failures with fec_type as kr/base-r for 25GBASE_CR, 40GBASE_CR4, 50GBASE_CR2Hello,ConnectX-6 supports all types of FEC that are SPEC compliant.
such as: Standard RS-FEC - RS(528,514), Firecode FEC and No FEC.
If there is specific connection that is not working as expected then it depends as well on the other edge and it needs to be reviewed.
For specific issue please open a support case with Nvidia by sending an email to Networking-support@nvidia.com.Best Regards,
VikiPowered by Discourse, best viewed with JavaScript enabled"
1012,native-xdp-on-pf-vf-representors-on-bluefield-2-arm,"We have a use case for a native XDP app attached to PF/VF representors (pf0hpf, pf0vf0) on bluefield-2 ARM subsystem. Only skb mode works today. Same is true for representors on ConnectX-5 and newer NICs, but there one can revert to simply attaching XDP to the actual PF and VFs.Are there plans to add those hooks to the mellanox net drivers or are there better ways to add XDP, short of just using split mode or attaching to p0 and p0m0 (basically not leveraging representors)?Powered by Discourse, best viewed with JavaScript enabled"
1013,direct-connection-of-two-infiniband-ports-without-ib-switch,"I want to install two ConnectX-6(VPI) cards in each PC and then connect two infiniband ports directly.
When I googling, I’ve not seen a configuration without an IB switch, so I’d like to know if the switch is always required.You can do it.It will limit you to a 2-port subnet comprised of the 2 connected ports.Powered by Discourse, best viewed with JavaScript enabled"
1014,traffic-filtration-using-nic-capabilities,"Like Intel cards have ethtool
Zrzut ekranu 2021-10-8 o 12.36.251862×790 163 KB
So… do mellanox do that too? - if it is possible?How to do it?where to download the tool?I’m using now MCX516A-CCAT CX-5Hello,Ethtool can be used with Mellanox adapters and in most cases, the version of ethtool provided by various distributions is sufficient. However, there is a version customized for certain options/features unique to Mellanox adapters included with MLNX_OFED. Once MLNX_OFED is installed, it can be found in the /opt/mellanox/ethtool/ directory.In cases where usage of the utility diverges from common usage in ways that are unique to Nvidia/Mellanox adapters or their drivers, the “Ethtool” section of the MLNX_EN and MLNX_OFED User Manual will have more information. There are also examples of the flow-steering capabilities in the “Flow Steering” section.The download package and User Manual for the latest versions of the MLNX_OFED driver can be found at the following link:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)Thank you,-Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
1015,cx4121-acat-firmware-14-28-2006-and-higher-break-sfp-transceivers,"Hey,I’ve got a couple of CX4121-ACAT - I upgraded the firmware from 14.23 to 14.32 and the SFP+ transceivers (10GBase-LR transceivers from Infinera) stopped working.I could bisect the firmware versions, and upgrading from 14.28.1300 to 14.28.2006 breaks the connection.
14.28.1300 works fine, 14.28.2006 is the first affected version that does not work.Any idea whether this can be fixed with a software setting?Hello robin.christ,Welcome, and thank you for posting your inquiry to the NVIDIA developer forums!Unfortunately, Infinera transceivers have never been in our test matrix. This is true for many transceivers/cables, but some may ‘just work’ regardless.A lot of changes are made between adapter firmware revisions, especially when it comes to signalling. Something must have changed between 14.28.1300 → 14.28.2006 that the Infinera transceivers didn’t like.Reviewing the release notes, the only change that I can see which might be related is new support for Rate-Select for Avago SFP+ modules in 14.28.2006:
https://docs.nvidia.com/networking/display/ConnectX4LxFirmwarev14282006/Changes+and+New+FeaturesPerhaps the Infinera transceiver is using some non-standard Rate-Select configuration / pin mapping? But again, we don’t really know, because we don’t test Infinera transceivers.To ensure continued functionality across firmware releases, our recommendation is always going to be to use transceivers/cables that are in our test matrix, and fully supported.These can be found in the firmware release notes in the ‘Firmware Compatible Products’ section → https://docs.nvidia.com/networking/display/ConnectX4LxFirmwarev14321010/Firmware+Compatible+ProductsThanks,
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1016,nvue-isis-and-fabric-configuration,"@epulvino this might be a better question for you but we do quite a bit with IS-IS on Extreme.With FRR supporting the basic RFC we’re looking at a few ways we want to test but ultimately implement it on Cumulus vs VXLANs over EVPN.Are snippets the best options since NVUE doesn’t support it yet?  Is there a safer way to configure FRR for these non NVUE configs?Lastly, does Cumulus/NVIDIA have future NVUE plans for configuring OpenFabric (fabricd) in FRR?A traditional snippet for the FRR would be the method to configure this.
https://docs.nvidia.com/networking-ethernet-software/cumulus-linux-54/System-Configuration/NVIDIA-User-Experience-NVUE/NVUE-Snippets/#etcfrrfrrconf-snippetsNote that there can be other challenges running ISIS in CL. The controlplane ISIS protocol messages must be punted up to FRR from the data plane. In the past this has required the addition of specific SwitchD rules to enable these punts to controlplane. I’m not sure we’ve enabled them by default.You should be able to talk to your Solution Architect to get an official answer on how you might be able to test with ISIS in your environment but until it is officially supported, your mileage may vary. If you need a hand figuring out who that might be let me know and I can look them up for you.OpenFabric has not been on any roadmap I am aware of at this time.Powered by Discourse, best viewed with JavaScript enabled"
1017,how-to-fix-mellanox-bluefield-2-subnet-interface-keeping-in-port-init,"One of my BlueField DPU’s port in is PORT_ACTIVE, which is normal. However, the other is always in PORT_INIT state, even after I ran /etc/init.d/opensmd restart. When I check opensmd status, it says ‘active(running)’, sounds normal, so how can I fix this problem? Please help me😢My DPU OS is ubuntu 20.04, ofed version is MLNX_OFED_LINUX-5.2-1.0.4Here is my ibv_devinfo result:
截屏2021-11-05 上午10.00.411670×2106 301 KB
Hi Rongxin,Let’s start with a few questions.What is the part number of the DPU?What is the part number for the cables in used?Are both ports connected to the same IB switch?What type of IB switch?Where does the SM is running (SM lid 29, smpquery nd 29)?Sophie.Make sure both ports of the HCA are connected to the same IB Fabric in order for SM to assign a lid and bring the logical port state to active/linkup.Hi SophieThx for your reply !For some reason, I can’t physically touch the machine myself right now. But I’ll keep your suggestion in mind and check the HCA connection when I get a chance.Powered by Discourse, best viewed with JavaScript enabled"
1018,where-can-i-get-wqe-format-in-mlnx,"I am curious about the format of WQE during my work. I found an entry named “WQE Format in MLNX_OFED” in OFED 49224 doc (https://docs.mellanox.com/display/MLNXOFEDv492240/WQE+Format+in+MLNX_OFED), which is close to my requirements. However, when clicking the link, it says “Please refer to “Wake-on-LAN (WoL)” section.” But there is nothing I need except two comands in WoL Section. I also notice that the entry “WQE Format in MLNX_OFED” is deleted in some higher version of the OFED DOC(e.g. OFED ver 54103).Does someone knows what was going on?Where can I find the detail format of WQE in Mellanox ?pls helpHello Rongxin,Thank you for notifying us of this documentation issue.This documentation was incorrect, and has been removed to avoid confusion.Work requests are submitted to the HCA through Verbs abstraction.Work Request Elements themselves are abstract, and are created by transformation of a prepared Work Request using software verbs.Please review Infiniband Spec: InfiniBand Specification Frequently Asked Questions(especially sections related to Work Request processing [10.8, 11.4] ) for further information.Best regards,Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1019,infiniband-bridging-in-bluefield-2,"I have a simple question.Can I use BlueField-2 to bridge InfiniBand traffic between Host and ARM when operating in Embedded Mode?If not, what is the main benefit of using BF-2 in InfiniBand with Embedded Mode?Hi Jongwook,​Do you mean you want to set BlueField-2 as ECPF mode (Embedded CPU Function Ownership Mode (SmartNIC Mode)) ?Simple answer for your question is Yes.​Here is more information below:​Currently, BlueField-2 DPU supports configuring network ports as either Ethernet only or InfiniBand only.​In ECPF mode, the NIC resources and functionality are owned and controlled by the embedded Arm subsystem. A network function is still exposed to the host.​There are two ways to pass traffic to the host interface:1)Either using representors to forward traffic to the host (every packet to/from the host would be handled also by the network interface on the embedded Arm side),2)or push rules to the embedded switch which allows and offloads this traffic.​Regards,​LeveiPowered by Discourse, best viewed with JavaScript enabled"
1020,need-advise-on-four-switch-mlag-configuration-onyx-mellanox,"Hello,
I need some advise to get a four switch (two sites) Mellanox (HPE OEM) SN2010M (ONYX) in a supported configuration.There are two sites with two switches- each pair in a own MLAG domain (2x100Gbs ISL LACP Channel-Port-Group).S1 =MLAG1 ISL= S2       site1S3 =MLAG2 ISL= S4       site2on Switch 1+3 there are only VLAN3901 access ports
on Switch 2+4 there are only VLAN3902 access ports  (is a iSCSI configuration with two separte VLAN/subnets)There are 4 connections possible between the two sites, and should be fully redundant or only redundant on each VLAN/Subnet.
How to configure that without any loops, no success with MLAG-Port-Channel or normal port-channels with 4 connections from site to site, straight and crossover, or straight only .Thanking for any help,MartinHi Martin,I would start with the documentation to understand the capabilities and limitations:
https://docs.nvidia.com/networking/display/Onyxv31040100/MLAG
https://enterprise-support.nvidia.com/s/article/how-to-configure-mlag-on-mellanox-switches
Not all of this documentation is relevant for what you need but you can have a look.I am not entirely clear on the design constraints and a forum like this is a bit challenging to get design guidance.Can you create a diagram of the design that closest meets the needs with the switches + interfaces listed +VLAN/TRUNK, along with site description of something that did not work?Let’s see if we can hash out where it is wrong and what would be a supported configuration.CharlesHallo CharlesThank you for your response. I am aware of the documentation how to get MLAG working and how to create channel groups with and without mlag. But in this case I got no clear view if it is supported or not. Please see the picture.

image2048×1536 183 KB
The hand drawn lines are the four connections between the sites and switches.One opinion  is that this configuration is not supported only a spine leaf configuration with more switches.
2nd opinion was the only supported config is to get the MLAG not between the switches on one site but on each switch to the other site.In the MLAG documentation there is such a configuration as drawn mentioned but no more details to avoid loops over the MLAG isl.Thank you for any input.Hi Martin,Thanks for the additional clarification on this.IMO, MLAG is not the correct solution here due the localized VLANs and redundancy requirements.  For MLAG to function correctly we need all VLAN’s on both peers. Your MPO would not meet those requirement based on where the VLAN’s reside.Have you considered VXLAN? This would allow the inter site & inter switch links, which would be L3,  to be all active and each switch could have any (or no) vlans.CharlesMLAG with too much distance can cause issues.  I agree, I would look at using VXLAN/EVPN.Powered by Discourse, best viewed with JavaScript enabled"
1021,dpdk-devx-register-failed,"Hello,i am having trouble running DPDK on Windows. I am trying to use the example programs and testpmd, but they fail with some errors (see outputs below).
I am using Windows Server 2022 with a Mellanox ConnectX-4 LX Card using Win-OF 2 2.90.50010 / SDK 2.90.25518.
I am using the current build (DPDK Version 22.07-rc2)i followed to DPDK Windows guide, but currently i always get the following error:
mlx5_common: DevX read access NIC register=0X9055 failed errno=0 status=0 syndrome=0does anybody have a idea how to resolve this problem, or at least get some more information why it failed?Hello,
this issue is solved for me. It was necessary to set the regparam “DevxFsRules” to 0xFFFFFF instead of 0x000000 which i initially configured.The following error might still appear, but has no effect and can be ignored depending on the used NIC:
mlx5_common: DevX read access NIC register=0X9055 failed errno=0 status=0 syndrome=0See also: Windows examples failed to start using mellanox cardPowered by Discourse, best viewed with JavaScript enabled"
1022,mellanox-ofed-5-5-kernel-panics-on-rhel-7-9-when-running-sysctl-a,"Running latest RHEL 7.9 with latest MLNX OFED 5.5 install via yum repo Index of /public/repo/mlnx_ofed/latestHardware: ConnectX-5 VPIProblem: When running sysctl -a the kernel panics and machine reboots.Workaround: Downgrading to MLNX OFED 5.4 fixed the issue and sysctl -a works fine.Any way to get the driver developers something useful to help fix this problem?Is there any more information needed to help debug this?Thanks,
NickDid some extra digging and found that on the servers this was happening on, NFS RDMA was enabled and had loaded kernel modules rpcrdma and svcrdmaWhen i disabled NFS RDMA by editing /etc/sysconfig/nfs and removing RPCNFSDARGS=""--rdma=20049""Then also editing /etc/nfs.conf and commenting outBADGOODRebooting the server and lsmod no longer showed rpcrdma and svcrdma  and sysctl -a was able to work again.Hopefully this helps you all debug the issue.Hello,Thank you for sharing this information with us. We are glad to hear that you were able to resolve the issue.If you require further support with this issue and have a current support entitlement, please submit a new support case through the customer support portal.-Nvidia Network SupportHilaryn,While i was able to create a workaround, the OFED driver still has a bug in it that causes the system to kernel panic. Might want to tell your engineers about it.Thanks,
NickHilaryn,Also on the topic of support, i’ve tried twice now to get an active support contract for our NICs, but the sales folks said they weren’t able to sell me a contract for an existing NIC, i would need to purchase a new NIC in order to get a contract.Thanks,
NickCould you share panic log from the console?
As additional check, does adding ‘nosmap’ parameter to grub.conf and restarting the servers resolves the issue?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1023,do-connectx-3-series-card-compatible-to-each-other-between-each-subversion,"For example can I connect a MCX342A-XCCN directly to MCX342A-XCEN ？ Based on mellanox card change history, it only mention changes between each revision but do not mention compatibility between revision change​​Hi Ray,Yes, the cards are compatible to each other.Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
1024,ompi-comm-world-local-size-problem-between-pbs-and-mlnx-ofed,"I’ve installed MLNX_OFED_LINUX-5.5-1.0.3.2, including the user space programs and libraries, on two nodes of my cluster, in order to validate the performance before updating whole cluster to Mellanox OFED. The cluster runs PBS 18.1.4. If I allocate nodes for interactive use, and run tests like ib_read_bw or ucx_perftest manually, then these tests work fine and bandwidth is as expected. However, when I try to use OpenMPI delivered along with the MLNX_OFED distribution, through using /usr/mpi/gcc/openmpi-4.1.2rc2/bin/mpirun from a PBS job submission script setup to allocate two nodes, and run a MPI rank on each node, the problem is that PBS sets OMPI_COMM_WORLD_LOCAL_SIZE variable to 2 for each of my ranks, instead of 1. The $PBS_NODEFILE content turns out OK, namely it indeed lists both nodes requested, and the ranks do get run on different nodes, but this variable is just wrong (and also OMPI_COMM_WORLD_LOCAL_RANK is set 0 and 1 for the two ranks, respectively, instead of 0 for both). So, is there any known incompatibility between mentioned versions of PBS and MLNX_OFED, or - any other hint how to fix this issue?Thanks.OK, got in the meantime that OpenMPI delivered along with MLNX_OFED is not built with tm support, so the MPI ranks in my case actually get all assigned to the single node, and OMPI_COMM_WORLD_LOCAL_SIZE and OMPI_COMM_WORLD_LOCAL_RANK are correct. Namely, the problem is that nodes are read from PBS generated file that lists one node per line, but since OpenMPI is built without tm support the mpirun doesn’t check info about node allocations, and considers that the first node in the list from the given hostfile has as many slots as it has cores, and thus both ranks ends up on this node.So I guess my question becomes: is it supposed that this OpenMPI version should not be used along with Torque/PBS, or are there some workarounds?In the meantime, I’ve built my own OpenMPI version, using “–with-tm” flag to activate PBS support, and it works fine; I kept using UCX delivered along with MLNX_OFED.It doesn’t seem Mellanox is listening much to this community forum, but nevertheless I’d like to conclude with an appeal to have OpenMPI that is included in MLNX_OFED built with “–with-tm” flag in the future. If I understood it correctly, there is no harm in building with Torque/PBS support, and most of the Linux distributions build with this flag anyway, so it would be good to have it activated for MLNX_OFED build of OpenMPI too.Hello,The openMPI does not come compiled with the --with-tm flag for Torque/PBS support because we don’t compile with proprietary deps, but only open public source but you can rebuild openmpi with PBS support as you did, the sources are part of our HPC-X package.Best Regards,VikiPowered by Discourse, best viewed with JavaScript enabled"
1025,doca-1-3-is-available-now,"https://developer.nvidia.com/blog/bolster-network-storage-and-security-infrastructure-services-with-nvidia-doca-1-3/Powered by Discourse, best viewed with JavaScript enabled"
1026,connectx-5-ex-vpi-drops-packets-while-performing-rdma-write,"Hello,I am working on RoCE IP core for FPGA and I have some problems regarding dropped packet. Adapter used in this project is ConnectX-5 Ex VPI.I’m trying to perform RDMA write operation and it works for transfers of size below 256 kB. When I try to transfer more, e.g. 512 kB, adapter receive only 66 packets (this number is constant accross multiple tries) and the rest is (I assume) dropped. This number is taken from available hardware counters which also shows that no error occured. This happens if I’m sending one packet after another without pause between them.If I insert pause between subsequent packets it gets better and eventually (with increasing duration of the pause) results in success. Unfortunately this results in throughput about 5 % of the bandwidth. I have few ideas about what could cause this kind of behaviour:I would really appretiate help in this matter as I know nothing about internal architecture of the adapter.Thank you very much.Hi Jakub,I have reviewed your questions and for further investigation i suggest to open a support case atnetworking-support@nvidia.comAccording to our records your account have support contract that is expiredIn order to renew the support contract , please contact Networking-contracts@nvidia.comThanks,SamerHi Samer,I already found the problem. It was the absence of any kind of control flow mechanism. It works fine now.Thank you,JakubPowered by Discourse, best viewed with JavaScript enabled"
1027,dpu-bluefield-2-hiwire-shift-aec-cable-to-convert-qsfp56-to-qsfp28,"Hello,
we would like to connect in lab our server with a Bluefield-2 DPU Two port QSFP56 card ( Ref: MBF2H536C-CECOT P-Series ) to QFX5120-48T Juniper Switch with QSFP28 interface.
Can we use Hiwire’s Shift AEC active electrical Cables PAM4 to NRZ  (Ref
Credo_HiWire_SHIFT_200G_4x56G_QSFP56_PAM4_to_Two_100G_4x28G_QSFP28_NRZ_06282021.pdf (319.2 KB)
f: CAC23X301Q2P-A0-HW ) for this purpose ( see the cable datasheet )This cable allows to convert 200G (4x56G) QSFP56 PAM4 to Two 100G (4x28G) QSFP28 NRZ
Best Rregards.Hello pyxisRegarding your question, could you please open a CASE?
That kind of question should be reviewed under CASE/HyungKwangPowered by Discourse, best viewed with JavaScript enabled"
1028,cannot-install-the-connectx-3-driver-for-debian10-the-downloaded-driver-is-mlnx-ofed-linux-4-9-2-2-4-0-debian10-0-x86-64-iso-the-used-command-is-mlnxofedinstall-vvv-distro-debian10-0-the-output-is-attached-in-details,"When trying to install the linux-headers-4.19.0-11-amd64i takeUnable to locate package linux-headers-4.19.0-11-amd64E: Couldn’t find any package by glob ‘linux-headers-4.19.0-11-amd64’E: Couldn’t find any package by regex ‘linux-headers-4.19.0-11-amd64’anduname -r gives 4.19.0-11-amd64what is going here?output (861 Bytes)Hi Antonis,MLNX_OFED 4.9-2.2.4.0 was tested and certified with kernel 4.19.0-5-amd64.Please try to compile the driver’s modules against the new kernel using the ‘–add-kernel-support’ flag.Regards,ChenThanks Chen for your reply.Even with --add-kernel-support we end up to the same error. It tries to install linux-headers-4.19.0-11-amd64 but cannot find it in the official repo. It seems that when the driver was made, the above header was active, but now it has been removed from the deb 10 repo.One idea would be to install this header from snapshot repo but i dont know if it is a good ideaPowered by Discourse, best viewed with JavaScript enabled"
1029,netq-free-trial-no-email-received-titled-netq-access-link,"I signed up for the trial of NetQ SaaS via Real-time Network Fabric Visibility | NVIDIAI received my login to the NVIDIA Licensing Portal and the entitlement certificate for “EVAL-NetQ”. I was able to download the NetQ software (Cloud Single Server VMware) from the licensing portal and install using the Install Guide.However, I am stuck on step 8 of the following setup as I haven’t received the NVIDIA email titled “NetQ Access Link”
https://docs.nvidia.com/networking-ethernet-software/cumulus-netq-46/Installation-Management/Install-NetQ/VMware-Setup-sngl-cld/#I therefore cannot register the NetQ Appliance I’ve installed as I don’t have the config key or login credentials for https://netq.nvidia.com/.Is this still awaiting approval or should I obtain it from somewhere else?Powered by Discourse, best viewed with JavaScript enabled"
1030,get-200mhz-arm-frequency-when-using-dpdk-on-dpu,"Do Arm cores have low frequecy?We get the 2.75GHz arm frequency from dpu documents(https://docs.nvidia.com/networking/display/BlueField2DPUENUG)

2022-03-29 10-33-21屏幕截图1384×81 17 KB

But I got 200MHz when using dpdk-l2fwd as these pictures have showed.

2022-03-28 22-41-32屏幕截图764×187 9.57 KB


2022-03-28 22-43-52屏幕截图769×872 31.1 KB
Hi Mark. Interesting finding. What does the output of dmidecode |grep - speed look like on your setup?What DPDK version you used? rte_get_tsc_hz() is to read X86 TSC register,  measured frequency of the RDTSC counter. There is no TSC to use on AARCH64 need ASM porting. looks like this DPDK API not working well, just read system counter CNTFRQ_EL0 or wrong PMU counter. anyway, it is DPDK API issue.Currently 20.11.3.Thanks for your reply.That’s very helpful!!!There is no much change of implement for rte_tsc_hz(), from DPDK20.xx to current latest 22.xx, AARCH64 need hardware support PMU, and define marco to use ARM64 PMU get procise clock. The RTE_ARM_EAL_RDTSC_USE_PMU is not enable default, so just read system counter “asm volatile(“mrs %0, cntfrq_el0” : “=r” (freq));”, not like the TSC procise on X86.dpdk source/config/arm/meson.builddodk source/lib/librte_eal/arm/rte_cycles.cget_tsc_freq_arch(void)
{
#if defined RTE_ARCH_ARM64 && !defined RTE_ARM_EAL_RDTSC_USE_PMU
return __rte_arm64_cntfrq();
#elif defined RTE_ARCH_ARM64 && defined RTE_ARM_EAL_RDTSC_USE_PMU
#define CYC_PER_1MHZ 1E6
/* Use the generic counter ticks to calculate the PMU
* cycle frequency.
*/
uint64_t ticks;
uint64_t start_ticks, cur_ticks;
uint64_t start_pmu_cycles, end_pmu_cycles;/** Read generic counter frequency */
static __rte_always_inline uint64_t
__rte_arm64_cntfrq(void)
{
uint64_t freq;}X86 implement very simple, just read MSR and calculate TSC counter base on CPU model.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1031,need-help-decoding-messages-from-mlx5-ib-qp-err-syndrome,"I’m getting these messages and need help decoding vendor_error_syndrome and hw_error_syndrome values:587496.823345] mlx5_0/1: QP 65526 error: unrecognized status (0x22 0x0 0x95)
[587496.833350] mlx5_0/1: QP 63410 error: unrecognized status (0x22 0x0 0x95)
[587496.843191] mlx5_0/1: QP 66547 error: unrecognized status (0x22 0x0 0x95)Thanks,ChienHello,
It seems that completion error was returned with wrong index. It might be due to Error CQE buffer is corrupted.
I suggest you to add more debug prints, for example to dump error CQE.
For further debug, it would require a support case.Best Regards,
VikiCan you be specific?  what index are you referring to? wr_id?  Adapter is writing CQE so how can it be corrupted?  Can you give me the exact meaning for 0x22 and 0x95?In addition to 0x22 and 0x95, there is also this one.[587500.402815] mlx5_1/1: QP 72315 error: unrecognized status (0x23 0x0 0x9d)What’s 0x23 and 0x9d?Powered by Discourse, best viewed with JavaScript enabled"
1032,statistics-of-bluefield-2s-eswitch,"Hi all!I am very interested in the performance data of BlueField’s embedded switch, which is also refered to as PCIe switch or eSwich. However, I cannot found these information from the documents:Hope someone can help, thanks!Hello,Technical, based on the type of DPU you have (25/100/200) You can check the datasheet (https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/documents/datasheet-nvidia-bluefield-2-dpu.pdf), you should be reaching rate line (results may varies based on deployment(s)
We do not release official performance (white papers) however, if that helps, you can review performance reports we have posted for DPDK/Bluefieldfromhttp://fast.dpdk.org/doc/perf/DPDK_22_03_NVIDIA_Mellanox_NIC_performance_report.pdfYou will need to check the FW RN according to the part# of the DPU for PCIe Gen/Width support.
https://network.nvidia.com/support/firmware/bluefield2/Powered by Discourse, best viewed with JavaScript enabled"
1033,cannot-add-team-member-trunk-port,"Computer: Windows 11 22H2 with March 2023 updates (22621.1413)
Product: Connect-X 5 (P/N 0F7V1F)
Driver: 3.20.25915.0
Firmware: 16.35.1012Problem: cannot add a member to team - CLI tool complains about missing parameterQuestion:Documentation for WinOFv2 3.20 (https://docs.nvidia.com/networking/display/winof2v320/Ethernet+Network#EthernetNetwork-TeamingandVLAN)
says that one can set single VLAN ID only.Is it still applicable?
That documentation says „WinOF-2 v2.30 supports configuring only a single VLAN to a team interface“ - it explicitly says v2.30 but documentation is for WinOFv2 3.2.Can be a trunk port configured on ConnectX-5 on Windows client at all?
Trunk port = multiple VLAN IDs on single port (or teamed device).
WindowsTerminal_XuHQkXmWfI1920×1050 94.9 KB
Problem solved by using CMD.exe instead of Powershell.Question(s) is still waiting for response…Correct cmd is,add adapter to the team use:
mlx5muxtool attach team  {} [primary] [SetTeamMacAddress]eg,mlx5muxtool.exe attach team MyTeam {90F5F52D-4384-4263-BD12-4588CA5CE80A} primaryadd adapter to the team use:
mlx5muxtool attach team {} [primary] [SetTeamMacAddress]eg,mlx5muxtool.exe attach team MyTeam {90F5F52D-4384-4263-BD12-4588CA5CE80A} primaryWhy are you citing documentation with no added value in your post?From my screenshot it is apparent that I followed syntax and semantics exactly, only problem was Powershell which probably was expanding/evaluating string in {}.Regarding the trunk port or multiple VLANs on interface, seems that only feasible way is to create team via mlx5muxtool and assign VLANs to it via Hyper-V switch on client Windows SKUs.Powered by Discourse, best viewed with JavaScript enabled"
1034,unable-to-set-up-ebgp-peers,"Hi community,My BGP configuration is very simple, but I cannot set up eBGP peers, they remain in Active state. The BGP configuration is like that:nv set vrf default router bgp autonomous-system 64514
nv set vrf default router bgp router-id 10.10.20.1
nv set vrf default router bgp neighbor 10.10.20.2 remote-as 64515
nv set vrf default router bgp neighbor 10.10.30.1 remote-as 64512
nv set vrf default router bgp neighbor 10.10.30.1 multihop-ttl 100
nv set vrf default router bgp neighbor 10.10.30.2 remote-as 64513
nv set vrf default router bgp neighbor 10.10.30.2 multihop-ttl 100The configuration in the peers is the opposite.BGP router identifier 10.10.20.1, local AS number 64514 vrf-id 0
BGP table version 0
RIB entries 0, using 0 bytes of memory
Peers 3, using 68 KiB of memoryNeighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt
10.10.20.2      4      64515         0         0        0    0    0    never       Active        0
10.10.30.1      4      64512         0         0        0    0    0    never       Active        0
10.10.30.2      4      64513         0         0        0    0    0    never       Active        0Total number of neighbors 3But I can ping the peers successfully:cumulus@SW-MLNX-01-AVZ1:mgmt:~$ ping 10.10.20e[Ke[K30.1
vrf-wrapper.sh: switching to vrf “default”; use ‘–no-vrf-switch’ to disable
PING 10.10.30.1 (10.10.30.1) 56(84) bytes of data.
64 bytes from 10.10.30.1: icmp_seq=1 ttl=61 time=7.78 ms
64 bytes from 10.10.30.1: icmp_seq=2 ttl=61 time=5.03 ms
64 bytes from 10.10.30.1: icmp_seq=3 ttl=61 time=5.64 ms
64 bytes from 10.10.30.1: icmp_seq=4 ttl=61 time=4.38 ms
64 bytes from 10.10.30.1: icmp_seq=5 ttl=61 time=7.10 ms
^C
— 10.10.30.1 ping statistics —
5 packets transmitted, 5 received, 0% packet loss, time 11ms
rtt min/avg/max/mdev = 4.380/5.985/7.783/1.276 msI cannot see any interesting in frr.log, only this:2023-02-06T11:48:01.162971+00:00 SW-MLNX-01-AVZ1 watchfrr[1913]: zebra state → up : connect succeeded
2023-02-06T11:48:01.165817+00:00 SW-MLNX-01-AVZ1 watchfrr[1913]: bgpd state → up : connect succeeded
2023-02-06T11:48:01.167261+00:00 SW-MLNX-01-AVZ1 watchfrr[1913]: all daemons up, doing startup-complete notify
2023-02-06T11:48:01.931755+00:00 SW-MLNX-01-AVZ1 zebra[1955]: Configuration Read in Took: 00:00:00
2023-02-06T11:48:02.789740+00:00 SW-MLNX-01-AVZ1 bgpd[1998]: Configuration Read in Took: 00:00:01
2023-02-06T11:48:02.839266+00:00 SW-MLNX-01-AVZ1 watchfrr[1913]: Daemon: zebra: is in Up state but expected it to be in DAEMON_DOWN state
2023-02-06T11:48:02.842824+00:00 SW-MLNX-01-AVZ1 watchfrr[1913]: Daemon: bgpd: is in Up state but expected it to be in DAEMON_DOWN state
2023-02-06T11:48:02.843310+00:00 SW-MLNX-01-AVZ1 watchfrr[1913]: Daemon: staticd: is in Up state but expected it to be in DAEMON_DOWN stateI don’t know if those logs are fine or have to do with the BGP session establishment.
Any idea?Regards,
JuliánDo you have a diagram and a complete config that you can share?Sure, here you are the diagram and the configuration files. I am using IPs 172.x.x.x for BGP router-ids and establishing the BGP sessions. The upper routers only have static routes and the gateways for reachability between the BGP router-ids, and the simulate a MPLS network, they do not have BGP configuration. After doing a little change, the BGP sessions are only established between the directly connected switches, but not between the switches of the different sites, although they can ping each other.
image1275×398 42.2 KB
BGP router identifier 172.16.0.1, local AS number 64514 vrf-id 0
BGP table version 0
RIB entries 0, using 0 bytes of memory
Peers 3, using 68 KiB of memoryNeighbor                    V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt
SW-MLNX-02-AVZ1(172.16.0.2) 4      64515       123       123        0    0    0 00:05:20            0        0
172.17.0.1                  4      64512         0         0        0    0    0    never       Active        0
172.17.0.2                  4      64513         0         0        0    0    0    never       Active        0Total number of neighbors 3If a do a tcpdump on a switch of site AVZ1, I see there is BGP messages sent to the directly connected switch of site AVZ1, but no BGP messages sent to switches of site AVZ2. But when I add a static route to 172.17.x.x (site AVZ2) on a switch of site AVZ1, the switch starts to sent BGP messages to switches of site AVZ2 and they establish the BGP session, even they already have a default route with the same gateway.Adding “nv set vrf default router static 172.17.0.0/24 via 172.16.0.100” to a switch of site AVZ1:BGP router identifier 172.16.0.1, local AS number 64514 vrf-id 0
BGP table version 0
RIB entries 0, using 0 bytes of memory
Peers 3, using 68 KiB of memoryNeighbor                    V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt
SW-MLNX-02-AVZ1(172.16.0.2) 4      64515       284       284        0    0    0 00:13:24            0        0
SW-MLNX-01-AVZ2(172.17.0.1) 4      64512         9         9        0    0    0 00:00:20            0        0
SW-MLNX-02-AVZ2(172.17.0.2) 4      64513         9         9        0    0    0 00:00:20            0        0Total number of neighbors 3What am I missing?Regards,
Juliánconfig.rar (1.9 KB)The config that you provided are the nv set commands. It is hard to troubleshoot from that. However, given the diagram, I assume that this is a running simulation in AIR.Perhaps you can add me to the simulation (attilla (at) nvidia (dot) com), so I can look at the simulation directly.Hi attilla,Sorry but I am new on Cumulus/Linux. Do you mean the configuration you have when you run the command “nv config show”? I was simulating in AIR, but I switched to GNS3 VM because I thought the BGP issue was an AIR limitation. I will check but I think my AIR simulation expired. Thanks for your interest.Regards,
JuliánPowered by Discourse, best viewed with JavaScript enabled"
1035,intermittent-connection-timed-out-while-setting-rc-qp-to-rtr,"Hi All,I am using Connectx5-EX on 5.19.0-45-generic. One side creates an RC qp and sends it’s qp info over TCP and the other side upon receiving this message creates it’s own RC QP and sets it to RTR followed by setting to RTS, and it responds back over same TCP connection, to the initiator which then sets it’s own QP to RTR then RTS. This works most of the times but sometimes I get connection time out from ibv_modify_qp() while setting to RTR. I have two dual function Connectx5-Ex so total four interfaces mlx5_0 to mlx5_3, so far I have observed this on mlx5_2 on a particular node which could indicate an adapter or cable issue, but I was looking for some more info to prove that. So I tried to set the /sys/module/mlx5_core/parameters/debug_mask to 3 and found no mlx5_core messages in syslog also I was looking to run some of mst tools and it says running mlxtrace would need a config file which I don’t have. I also tried running wqdump by referring the mft doc and that also didn’t help much. So any suggestion or pointers would be highly appreciated.Thanks,
ArkaPowered by Discourse, best viewed with JavaScript enabled"
1036,the-permission-issue-of-setting-dpdk-hugepages,"I am new to the DPU&DOCA, I encounter a problem while executing the DOCA example applications.
I have installed the OS image download from https://www.mellanox.com/eula/blue-os?mtag=bluefield_sw_drivers&mrequest=downloads&mtype=BlueField&mver=BFBs&mname=Ubuntu20.04&mfile=DOCA_v1.1_BlueField_OS_Ubuntu_20.04-5.4.0-1013-bluefield-5.4-1.0.3.0-3.7.0.11805-1.signed-aarch64.bfb (DOCA_v1.1_BlueField_OS_Ubuntu_20.04-5.4.0-1013-bluefield-5.4-1.0.3.0-3.7.0.11805-1.signed-aarch64.bfb) and i can access the device by ssh now with account ‘ubuntu’.when I test the example ‘/opt/mellanox/doca/examples/ar/bin/doca_app_rec’, the log remind that there is no DPDK hugepage，I try to set the huagepsage by ‘ sudo echo 600 >  /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages’， but the ‘Permission denied’ error happened.  when i try to su ‘root’ account, the password is not correct.Would you please help to check how to fix this issue or tell me the default root account’s password?
Thank you.Using sudo with echo works different than you would expect. Can you tryecho '600' | sudo tee -a  /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepagesPowered by Discourse, best viewed with JavaScript enabled"
1037,xilinx-ultrascale-rtx-4000-rdma-sample-code,"Hi, Is there any sample code (git, open source etc.) available for enabling RDMA from Xilinx Ultrascale FPGA (e.g. XCKU085) to RTX 4000? Initially, we are transferring 2.5 GB (GigaBytes) / second and will eventually need to get to 10 GB/s. Has anybody tested such large transfers, and what are some of the issues one need to watch out for? Our current implementation transfers the data to a circular buffer on server and reads it back into the GPU. The obvious issues we are facing are CPU interrupts and packet drops. This was done purely to get the application working. But, the next step is to optimize the flow.  Thanks.Powered by Discourse, best viewed with JavaScript enabled"
1038,rivermax-initialize-error,"I’ve got the below error at initialization[21-03-31 21:52:21.337200] Tid: 058504 debug [init_globals:168] Invalid time handler flag: init_config->time_handler.time_handler_type 0[2021-03-31 21:52:21.337208] [0x00007f879a70d780] [error] failed initializing Rivermax[21-03-31 21:52:21.337277] Tid: 058504 debug [~SysClock:44][21-03-31 21:52:21.337282] Tid: 058504 debug [~Clock:34][21-03-31 21:52:21.337286] Tid: 058504 debug [~DeviceCollection:28] ~DeviceCollection()Hi Takahiro,I would like to bring to your notice that Rivermax is a product which requires customers to have a valid support contract. In case you have a valid support contract, it would be great if you can open a ticket by emailing "" Networking-support@nvidia.com ""Thanks,Namrata.Powered by Discourse, best viewed with JavaScript enabled"
1039,ipoib-enhanced-on-sr-iov-vfs-not-working-i-have-trouble-using-an-ipoib-interface-on-a-vf-of-a-connectx-5-device-when-ipoib-enhanced-1-it-works-nicely-when-switching-to-connected-mode-with-ipoib-enhanced-0-its-on-sl7-9-with-mlnxofed-5-0-and-5-3,"Is this unsupported ?Hi Norbert,SR-IOV & enhanced IPoIB is supported, and the default mode from MLNX_OFED 4.X and above is enhanced IPoIB.Are you referring to OL 7.9? In that case, please note that this OS is supported only in MLNX_OFED 5.2-1.0.4.0 / 5.2-2.2.0.0 / 5.2-2.2.3.0.Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
1040,trouble-configuring-gateway-proxy-arp-on-sx6036g,"I am following the user manual satisfying the prerequisites - no ip routing, no igmp snooping and no subnet manager. Ports 33 to 36 are Ethernet, rest of them are IB. After enabling Proxy ARP in gateway, the rest of the configuration tabs are greyed out. While using CLI, show interfaces proxy-arp 1 returns empty. Anyone encountered this before? Thanks, SureshHi, I recommend using this community post. It is much better than the documentation in the user manual. Please read though it and follow it carefully and you should not have any problems.https://community.mellanox.com/s/article/howto-configure-infiniband-gateway-ha--proxy-arp-xIt should be configured using the CLI, as explained in this knowledge article.no ip routing, no igmp snooping and no subnet managerNone are enabled by default, so these won’t be a problem.Unfortunately I have no solution, I experience the very same problem. Even worse, when enabling IP Proxy-ARP and rebooting the device, it is not fully operational, parts of the web configuration do not work, the IB ports do not come up etc.I tried to configure Proxy-ARP as described here (without HA): https://community.mellanox.com/s/article/howto-configure-infiniband-gateway-ha–proxy-arp-xHowever, it still does not work. Any clues?Powered by Discourse, best viewed with JavaScript enabled"
1041,rivermax-gpudirect,"Hello,We are adding direct support for GPUDirect on our Rivermax-based SMPTE ST 2110 sender on x86.
It was failing to create a flow whenever we allocated memory using the GPU, so we decided to test the generic_receiver demo:sudo ./generic_receiver -i 10.10.1.10 -m 239.1.1.1 -p 2000 -s 10.10.1.10 -g 0And it also fails with the following error:(…)
##################################################################################
(…)
CUDA memory allocation on GPU - cuMemCreate
RDMA is not supported or not enabled, status = 0 val = 0
Error: Fail to Allocate GPU Payload memory
(…)It seems like we need to enable RDMA but we cannot find documentation that clearly explains how to do it.
The module should already be installed, according to the 5th warning here: https://docs.nvidia.com/networking/display/GPUDirectRDMAv18/Installation:""GPUDirect RDMA kernel mode support is now provided in the form of a fully open source nvidia-peermem kernel module, that is installed as part of the NVIDIA driver. The nvidia_peermem module is a drop-in replacement for nv_peer_mem.This simplifies the installation workflow for our customers, so that there is no longer a need to retrieve and build code from a separate site. Now, simply installing the driver will suffice.Please refer to nvidia_peermem documentation for more information.""We are using Driver Version: 515.65.01, CUDA Version: 11.7 and, in fact, the module is there:bisect@dolores /o/m/r/1/apps> lsmod | grep nvidia_peermem
nvidia_peermem         16384  0
nvidia              40816640  1068 nvidia_uvm,nvidia_peermem,nvidia_modeset
ib_core               397312  9 rdma_cm,ib_ipoib,nvidia_peermem,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cmAnyway, we tried installing  nvidia-peer-memory_1.1, but it fails:DKMS: install completed.
Building initial module for 5.15.0-48-generic
Secure Boot not enabled on this system.
Done.nv_peer_mem.ko:
Running module version sanity check.depmod…DKMS: install completed.
modprobe: ERROR: could not insert ‘nv_peer_mem’: Invalid argument
dpkg: error processing package nvidia-peer-memory-dkms (–install):
installed nvidia-peer-memory-dkms package post-installation script subprocess returned error exit status 1
Errors were encountered while processing:
nvidia-peer-memory-dkms=====
dmesg:
[1646355.231123] nv_peer_mem: module uses symbols from proprietary module nvidia, inheriting taint.
[1646355.231188] nv_peer_mem: disagrees about version of symbol ib_register_peer_memory_client
[1646355.231191] nv_peer_mem: Unknown symbol ib_register_peer_memory_client (err -22)System info:bisect@dolores /o/m/r/1/apps> lsb_release -a
No LSB modules are available.
Distributor ID:    Ubuntu
Description:    Ubuntu 20.04.5 LTS
Release:    20.04
Codename:    focalbisect@dolores /o/n/nvidia-peer-memory-1.1> apt search ofed
Sorting… Done
Full Text Search… Done
hping3/focal 3.a2.ds2-9 amd64
Active Network Smashing Toolmlnx-ofed-kernel-dkms/now 5.5-OFED.5.5.1.0.3.1 all [installed,local]
DKMS support for mlnx-ofed kernel modulesmlnx-ofed-kernel-utils/now 5.5-OFED.5.5.1.0.3.1 amd64 [installed,local]
Userspace tools to restart and tune mlnx-ofed kernel modulesmlnx-tools/now 5.2.0-0.55103 amd64 [installed,local]
Userspace tools to restart and tune MLNX_OFED kernel modulesofed-scripts/now 5.5-OFED.5.5.1.0.3 amd64 [installed,local]
MLNX_OFED utilitiesbisect@dolores /o/m/r/1/apps> lspci | egrep ‘Mell|NV’
25:00.0 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex]
25:00.1 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex]
26:00.0 VGA compatible controller: NVIDIA Corporation Device 2507 (rev a1)
26:00.1 Audio device: NVIDIA Corporation Device 228e (rev a1)##################################################################################VMA INFO: ---------------------------------------------------------------------------
VMA INFO: VMA_VERSION: 9.4.0-1 Release built on Oct  5 2021 11:18:30
VMA INFO: Cmd Line: ./generic_receiver -i 10.10.1.10 -m 239.1.1.1 -p 2000 -s 10.10.1.10 -g 0
VMA INFO: OFED Version: mlnx-en-5.5-1.0.3.2:
VMA INFO: ---------------------------------------------------------------------------
VMA INFO: Log Level                      INFO                       [VMA_TRACELEVEL]
VMA INFO: Tx Mem Segs TCP                4                          [VMA_TX_SEGS_TCP]
VMA INFO: Tx Mem Bufs                    256                        [VMA_TX_BUFS]
VMA INFO: Tx QP WRE                      128                        [VMA_TX_WRE]
VMA INFO: Tx Prefetch Bytes              32                         [VMA_TX_PREFETCH_BYTES]
VMA INFO: Rx Mem Bufs                    256                        [VMA_RX_BUFS]
VMA INFO: Rx QP WRE                      128                        [VMA_RX_WRE]
VMA INFO: Rx Prefetch Bytes              32                         [VMA_RX_PREFETCH_BYTES]
VMA INFO: Force Flowtag for MC           Enabled                    [VMA_MC_FORCE_FLOWTAG]
VMA INFO: CQ AIM Max Count               64                         [VMA_CQ_AIM_MAX_COUNT]
VMA INFO: ---------------------------------------------------------------------------bisect@dolores /o/m/r/1/apps> nvidia-smi
Wed Sep 21 11:52:53 2022
±----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------±---------------------±---------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce …  On   | 00000000:26:00.0  On |                  N/A |
|  0%   60C    P2    38W / 130W |    346MiB /  8192MiB |      2%      Default |
|                               |                      |                  N/A |
±------------------------------±---------------------±---------------------+bisect@dolores /o/m/r/1/apps> lsmod | grep nvidia_peermem
nvidia_peermem         16384  0
nvidia              40816640  1068 nvidia_uvm,nvidia_peermem,nvidia_modeset
ib_core               397312  9 rdma_cm,ib_ipoib,nvidia_peermem,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cmAny help is much appreciated.From the nvidia-smi output, it looks as if the GPU is a GeForce.GPU Direct RDMA is not supported on the GeForce family of GPUs.-tomThanks, Tom. Do you know where I can find a list of supported GPUs? Is this still accurate: https://docs.nvidia.com/networking/display/GPUDirectRDMAv18/System+Requirements+and+Recommendations?
Does it mean that only the Kepler and Pascal series support it?Unfortunately, the webpage that you list is significantly out of date.All current RTX Professional GPUs support GPU Direct RDMA, this includes:RTX A6000, A5000, A4500, A4000, etc.Please accept our apologies for the confusion.Many thanks for the update.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1042,sn2010m-tryng-to-ping-failed-to-load-bpf-prog-prohibit,"hi guys,I have sn2010M running 3.9.1… when i trying to ping I get this error:Failed to load BPF prog:any idea to solve it?regardsHi Daniel,This is a known issue and has been published in the release notes:"" BPF Cannot use BPF filters due to default Linux restrictions for maximum amount of locked memory per process.Affected utilities include those which run in a VRF context such as tcpdump, ping, etc. ""Please kindly upgrade to the latest release as soon as possible.Thanks,YuyingPowered by Discourse, best viewed with JavaScript enabled"
1043,how-to-troubleshoot-diagnose-ib-completion-errors,"Setup is a ConnectX 5 Ex (MCX516A-CDAT) NIC with firmware v16.30.1004 and MellonxOFED 5.3-.1.0.0.1 basic installation, on Ubuntu 20.04.2 LTS, 5.4.0-74-generic, x86_64.The primary application catches work completion ibv_wc->status errors after the [ibv_poll_cq](https://www.rdmamojo.com/2013/02/15/ibv_poll_cq/) call - the QPs are setup with IBV_QPT_RAW_PACKET. There is also a printout seemingly from the driver level itself, all posted below. This primary application receives UDP packets.In an exemplary ping-pong application (from the examples) no such errors occur. I’ve seen a post that attributed a similar error to the form of the packets themselves.Error printout below: An initial completion error code 0x4, then two 0x10, then 0x5 (this last one repeats indefinitely). The vendor error too jumps around; 0x32, then 0x99 twice then 0xf9 indefinitely. The last 8 hex chars of the mlx5 completion error changes each time.mlx5: seti-node4: got completion with error:00000000 00000000 00000000 0000000000000000 000067ba 07000000 0000000000000000 20009232 00000000 0000203a000006c1 920c3204 00000000 000030e00: got completion error 0x4 vendor error 0x32 (wr_id 0 qp_num 0)mlx5: seti-node4: got completion with error:00000000 00000000 00000000 0000000000000000 000067ba 07000000 0000000000000000 20000099 00000000 0000203a000006c1 000c9922 00000000 000116e00: got completion error 0x10 vendor error 0x99 (wr_id 1 qp_num 0)mlx5: seti-node4: got completion with error:00000000 00000000 00000000 0000000000000000 000067ba 07000000 0000000000000000 20000099 00000000 0000203a000006c1 000c9922 00000000 000216e00: got completion error 0x10 vendor error 0x99 (wr_id 2 qp_num 0)0: got completion error 0x5 vendor error 0xf9 (wr_id 3 qp_num 4665)When I use the sender executable of the ping-pong example as the source for the primary application’s packets, the errors are hardly different (this time the packets are not UDP):mlx5: seti-node4: got completion with error:00000000 00000000 00000000 0000000000000000 000067ba 07000000 0000000000000000 20009232 00000000 0000006200001ed3 920b3204 00000000 000045e00: got completion error 0x4 vendor error 0x32 (wr_id 0 qp_num 0)mlx5: seti-node4: got completion with error:00000000 00000000 00000000 0000000000000000 000067ba 07000000 0000000000000000 20000099 00000000 0000006200001ed3 000c9922 00000000 000164e00: got completion error 0x10 vendor error 0x99 (wr_id 1 qp_num 0)mlx5: seti-node4: got completion with error:00000000 00000000 00000000 0000000000000000 000067ba 07000000 0000000000000000 20000099 00000000 0000006200001ed3 000d9922 00000000 000265e0The initial completion error of 0x4 indicates the important issue (from RDMAmojo):This was rectified by correctly linking the lkey of the sge_buffers to that of the registered memory region.Powered by Discourse, best viewed with JavaScript enabled"
1044,i-failed-to-build-mlnx-ofed-linux-for-5-4-0-70-generic,"I tried to install MLNX_OFED on two machines, one with ubuntu 20.04.01 and Kernel 5.4.0-66-generic x86_64, the other with ubuntu 20.04.01 and Kernel 5.4.0-70-generic x86_64. I succeeded on the first one but failed the second.Details:$ sudo ./mlnxofedinstall --without-dkms --add-kernel-support --with-nvmf --force --without-fw-updateNote: This program will create MLNX_OFED_LINUX TGZ for ubuntu20.04 under /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-5.4.0-70-generic directory.See log file /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-5.4.0-70-generic/mlnx_iso.55884_logs/mlnx_ofed_iso.55884.logChecking if all needed packages are installed…Building MLNX_OFED_LINUX DEBS . Please wait…ERROR: Failed executing “MLNX_OFED_SRC-5.3-1.0.0.1/install.pl --tmpdir /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-5.4.0-70-generic/mlnx_iso.55884_logs --kernel-only --kernel 5.4.0-70-generic --kernel-sources /lib/modules/5.4.0-70-generic/build --builddir /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-5.4.0-70-generic/mlnx_iso.55884 --without-dkms --force --without-debug-symbols --build-only --distro ubuntu20.04”ERROR: See /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-5.4.0-70-generic/mlnx_iso.55884_logs/mlnx_ofed_iso.55884.logFailed to build MLNX_OFED_LINUX for 5.4.0-70-genericAnd the log:Logs dir: /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-5.4.0-70-generic/mlnx_iso.51899_logs/OFED.52096.logsGeneral log file: /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-5.4.0-70-generic/mlnx_iso.51899_logs/OFED.52096.logs/general.logBelow is the list of OFED packages that you have chosen(some may have been added by the installer due to package dependencies):ofed-scriptsmlnx-ofed-kernel-utilsmlnx-ofed-kernel-modulesiser-modulesisert-modulessrp-modulesmlnx-nfsrdma-modulesmlnx-nvme-moduleskernel-mft-modulesknem-modulesChecking SW Requirements…One or more required packages for installing OFED-internal are missing.Attempting to install the following missing packages:build-essential python3-distutils make pkg-config debhelper dh-autoreconf bzip2 quilt gccThis program will install the OFED package on your machine.Note that all other Mellanox, OEM, OFED, RDMA or Distribution IB packages will be removed.Those packages are removed due to conflicts with OFED, do not reinstall them.Installing new packagesBuilding DEB for ofed-scripts-5.3 (ofed-scripts)…Running /usr/bin/dpkg-buildpackage -us -ucBuilding DEB for mlnx-ofed-kernel-utils-5.3 (mlnx-ofed-kernel)…-W- --with-mlx5-ipsec is enabledRunning /usr/bin/dpkg-buildpackage -us -ucFailed to build mlnx-ofed-kernel DEBCollecting debug info…See /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-5.4.0-70-generic/mlnx_iso.51899_logs/OFED.52096.logs/mlnx-ofed-kernel.debbuild.logIs there any possibility that MLNX_OFED_LINUX-5.3-1.0.0.1 is not compatible with kernel 5.4.0-70-generic?Please help!Hello Jin,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we are not able to reproduce the issue in our lab.MLNX_OFED driver 5.3 install successfully on kernel 5.4.0-70As you are running Ubuntu 20.04, recommended is to make use of DKMS which is enabled by default in the OS.Our install log with the syntax used to install the driver:# ./mlnxofedinstall -vvv --with-nvmf --force --without-fw-updateDistro was not provided, trying to auto-detect the current distro…Auto-detected ubuntu20.04 distro.set_cfg: name: ar-mgr, version: 1.0, debpath: /var/tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-ubuntu20.04-x86_64/DEBS/ar-mgr_1.0-5.8.2.MLNX20210321.g58d33bf.53100_amd64.deb…Logs dir: /tmp/MLNX_OFED_LINUX.10284.logsGeneral log file: /tmp/MLNX_OFED_LINUX.10284.logs/general.logBelow is the list of MLNX_OFED_LINUX packages that you have chosen(some may have been added by the installer due to package dependencies):ofed-scriptsmlnx-ofed-kernel-utilsmlnx-ofed-kernel-dkmsiser-dkmsisert-dkmssrp-dkmsmlnx-nvme-dkmsrdma-corelibibverbs1ibverbs-utilsibverbs-providerslibibverbs-devlibibverbs1-dbglibibumad3libibumad-devibacmlibrdmacm1rdmacm-utilslibrdmacm-devmstflintibdumplibibmad5libibmad-devlibopensmopensmopensm-doclibopensm-devellibibnetdisc5infiniband-diagsmftkernel-mft-dkmsperftestibutils2ar-mgrdump-pribsimibsim-docucxsharphcollknem-dkmsknemopenmpimpitestslibdapl2dapl2-utilslibdapl-devdpcpsrptoolsmlnx-ethtoolmlnx-iproute2rshimThis program will install the MLNX_OFED_LINUX package on your machine.Note that all other Mellanox, OEM, OFED, RDMA or Distribution IB packages will be removed.Those packages are removed due to conflicts with MLNX_OFED_LINUX, do not reinstall them.Checking SW Requirements……Running /usr/bin/dpkg -i --force-confmiss /var/tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-ubuntu20.04-x86_64/DEBS/rshim_2.0.5-10.g0ae03b4.53100_amd64.debRunning: FW_UPDATE_FLAGS=‘–log /tmp/MLNX_OFED_LINUX.10284.logs/fw_update.log -v --tmpdir /tmp’ RUN_FW_UPDATER=‘no’ /usr/bin/dpkg -i /var/tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-ubuntu20.04-x86_64/DEBS/mlnx-fw-updater_5.3-1.0.0.1_amd64.debSelecting previously unselected package mlnx-fw-updater.(Reading database … 118653 files and directories currently installed.)Preparing to unpack …/mlnx-fw-updater_5.3-1.0.0.1_amd64.deb …Unpacking mlnx-fw-updater (5.3-1.0.0.1) …Setting up mlnx-fw-updater (5.3-1.0.0.1) …Added 'RUN_FW_UPDATER_ONBOOT=no to /etc/infiniband/openib.confSkipping FW update.Running: /usr/bin/dpkg-deb -x /var/tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-ubuntu20.04-x86_64/DEBS/mlnx-ofed-kernel-dkms_5.3-OFED.5.3.1.0.0.1_all.deb /var/tmp/mlnx-ofed-kernel_module-check 2>/dev/nullis_module_in_deb: ipoib is in /var/tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-ubuntu20.04-x86_64/DEBS/mlnx-ofed-kernel-dkms_5.3-OFED.5.3.1.0.0.1_all.debInstallation passed successfullyTo load the new driver, run:/etc/init.d/openibd restartNote: In order to load the new nvme-rdma and nvmet-rdma modules, the nvme module must be reloaded.root@lx-u200401:/var/tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-ubuntu20.04-x86_64#Thank you and regards,~NVIDIA Networking Technical SupportHi Martijn! Thank you so much for your reply! I tried again and still failed and below is my detailed log. Please help me.And here is log of make.I’ve installed the lsb-core as the above file pointed, but now the mlnx-ofed-kernel-dkms still cannot be installed.Thank you. It turns out that someone in my group changed gcc to another edition, after changing it back to the original one, OFED is now installed. ​Powered by Discourse, best viewed with JavaScript enabled"
1045,regex-only-works-on-one-pcie-address,"When running doca_file_scan on 03:00:0, the regex in working correctly. But when run with “doca_file_scan —pci-address 03:00:1 —rules xxx —data yyy”, it failed with “Regex not supported on the device”.
What could be the reason?
Many thanks in advance.Hello kylelsun,Thank you for posting your query on our community. The RegEx accelerator must explicitly be initialized on interface p0 corresponding to the PCI address of 03:00.0. Please refer to this link for more information on running this application: File Scan :: NVIDIA DOCA SDK Documentation.Thanks & Regards,
~ Nvidia SupportThank you for clearing my confusion.Best regards,
KyleThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1046,is-a-mcb193a-fcat-single-fdr-56gb-s-3-0-x16-able-to-work-up-to-40-56-gb-s-inside-a-pcie-2-0-x8-is-a-mcb193a-fcat-single-fdr-56gb-s-3-0-x16-able-to-negotiate-to-40gb-s-as-to-work-with-is5023,"I am new to infiniband and recently such wonderful devices came into my possession, refurbished, IS5023 switches and MCB193A adapters. I am trying to find out if they are compatible with eachother (the 56GBe adapters being able to negotiate to 40GBe to match the IS5023 switches and if the adapter being PCIe 3.0 x16, if they are able to work up to 40GBe (to match the IS5023 switch) into PCIe 2.0 x8 (our risers only use 2.0 x8 PCIe). Thank you and all the best!Hello Ionut,Hello ,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the provided information, yes you will be able to use the Connect-IB adapter in a PCI 2.0 slot, as it is backwards compatible. See following PB → https://www.mellanox.com/related-docs/prod_adapter_cards/PB_Connect-IB.pdf. The Connect-IB adapter will auto-negotiate at 40Gb/sThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1047,stride-size-in-multi-packet-and-how-to-enable-shampo-in-connectx-6,"Hi,Is it possible to change stride size for Multi-Packet Rx Queue (MPRQ a.k.a Striding RQ). If yes, then can you guide me how to do it.?Also, I want to enable Multi Packet SHAMPO in ConnectX-6 but the “rx-gro-hw” feature in the ethtool features is off and fixed. How can I enable it?Thank you for your help.Striding receive WQ/Striding RQ functions/values etc… Can not be changed via CLI (IE: ethtool, mlxconfig, echo etc…) Code modification is at play here.
You do have the option from ethtool to enable or disable rx_striding_rq on/off via the --set-priv-flags.SHAMPO is a ConnectX-7 feature for implementing HW_GRO feature, HW_GRO is exposed via ethtool and is off by default.
So not relevant as it is a non feature for ConnectX-6Powered by Discourse, best viewed with JavaScript enabled"
1048,rocev2-with-sr-iov-on-azure,"HiIs it possible to enable RoCEv2 on a VM accelerated networking enabled on Azure?
It seems that “ConnectX-4 Lx Virtual Function” is provided by the accelerated network.$ lspci
29aa:00:02.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx Virtual Function] (rev 80)OS is RHEL 8.
Let me know if I can give you any other information.Regards,The VF should support RoCEv2.
use below command to current RoCE mode
#cma_roce_mode -d  -p modify RoCE mode
#cma_roce_mode -d  -p  -m <1|2>LeveiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1049,question-about-transceiver-for-connectx-4-lx-en-mcx4121a-xcat,"I’m trying to solve a connectivity problem in the most economical way. I’ll provide some details first, but my question is listed near the bottom of this post in bold font if you want to skip ahead.I have three Mellanox ConnectX-4 Lx EN (P/N MCX4121A-XCAT) which is 10G NIC with dual SFP28 ports. I don’t have enough equipment to justify getting a good 10G switch, so I’m connecting directly between servers using Mellanox MC3309130-003 cable (passive copper cable with connectors compatible with SFP28 ports).Everything’s working fine with one exception. I need to connect only a single port of one of my ConnectX-4 Lx EN NICs to a switch so that my virtual machines can access the rest of my network and the internet. I have a HPE 2920-48G (J9728A) 48-port gigabit switch that has an optional 10G module installed (J9732A). This module has dual port 10GBase-T with RJ-45 connectors.So the problem is that I need to connect from SFP28 port on my ConnectX-4 NIC to 10GBase-T RJ-45 port on my HPE switch. I want to connect between them using a CAT6a network cable. That CAT6a cable will fit the RJ-45 port on the switch, but it won’t fit on the SFP28 port on the NIC.QUESTION:Is it possible (and safe) to plug a transceiver directly into one of my SFP28 ports on my ConnectX-4 NIC?Unfortunately, I’ve not found any “officially supported” product for this scenario. However, I found a 10G transceiver on FS.com that converts from SFP+ form factor to RJ-45. SFP+ has the same form factor as SFP28 but only supports speeds up to 10G (whereas SFP28 supports 25G). FS.com tech support claims it’s compatible with my ConnectX-4 NIC because only 10G speed is needed in my case, so I’m willing to give it a shot.FS for Mellanox MFM1T02A-T Compatible, 10GBASE-T SFP+ Copper RJ-45 30m Transceiver Module (Standard) #89584Has anyone done something this before using ConnectX-4 Lx EN NIC? If yes, would you mind telling me which transceiver worked successfully? As mentioned above, I’m trying to connect using copper network cable (not optical) with RJ-45 connectors.Hello Ashok,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we noticed that you also opened an official NVIDIA Networking Support ticket which is being handled by one of our engineers.He will continue to support you through the official ticket.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1050,libmlx4-so-missing-in-mellanox-4-4-2-dpdk-driver-packages,"libmlx4.so missing in Mellanox 4.4.2 (–dpdk) driver packages?Background:RHEL: 7.5 kernel 3.10.0-862.11.6.el7.x86_64Mellanox driver version we are trying to install: 4.4.2When we install mellanox 4.4.2 drivers (dpdk only) on RHEL 7.5 using yum/usr/bin/yum -d 0 -e 0 -y install mlnx-en-dpdkIt says libibverbsUpdated By: libibverbs-41mlnx1-OFED.4.4.2.0.1.44207.x86_64 (sriov_mlnx)And we had to remove system package “libibverbs-15-6.el7.x86_64” to have “mlnx-en-dpdk” installed.However even after installing all packages (with optionmlnx-en-dpdk ) we don’t see libmlx4.so file installed at desired location (although we see libmlx5.so)Is that file missing to include in rpms of 4.4.2 version?lrt /lib64/ | grep mlx-rwxr-xr-x 1 root root 150232 Aug 9 2018 libmlx4-rdmav2.so-rwxr-xr-x 1 root root 375736 Aug 9 2018 libmlx5.so.1.0.0-rw-r–r-- 1 root root 500680 Aug 9 2018 libmlx5.alrwxrwxrwx 1 root root 16 Feb 4 12:22 libmlx5.so.1 → libmlx5.so.1.0.0lrwxrwxrwx 1 root root 16 Feb 4 12:22 libmlx5-rdmav2.so → libmlx5.so.1.0.0lrwxrwxrwx 1 root root 16 Feb 4 12:22 libmlx5.so → libmlx5.so.1.0.0mlx_fe-fe-0$ ls -lrt /usr/lib64/ | grep mlx-rwxr-xr-x 1 root root 150232 Aug 9 2018 libmlx4-rdmav2.so-rwxr-xr-x 1 root root 375736 Aug 9 2018 libmlx5.so.1.0.0-rw-r–r-- 1 root root 500680 Aug 9 2018 libmlx5.alrwxrwxrwx 1 root root 16 Feb 4 12:22 libmlx5.so.1 → libmlx5.so.1.0.0lrwxrwxrwx 1 root root 16 Feb 4 12:22 libmlx5-rdmav2.so → libmlx5.so.1.0.0lrwxrwxrwx 1 root root 16 Feb 4 12:22 libmlx5.so → libmlx5.so.1.0.0Packages Installed:rpm -qa | grep -i mlnxlibrdmacm-41mlnx1-OFED.4.2.0.1.3.44207.x86_64librdmacm-utils-41mlnx1-OFED.4.2.0.1.3.44207.x86_64libibverbs-devel-41mlnx1-OFED.4.4.2.0.1.44207.x86_64libibverbs-devel-static-41mlnx1-OFED.4.4.2.0.1.44207.x86_64libmlx5-41mlnx1-OFED.4.4.2.0.1.44207.x86_64mlnx-ofa_kernel-devel-4.4-OFED.4.4.2.0.7.1.gee7aa0e.rhel7u5.x86_64libibverbs-utils-41mlnx1-OFED.4.4.2.0.1.44207.x86_64mlnx-ofa_kernel-4.4-OFED.4.4.2.0.7.1.gee7aa0e.rhel7u5.x86_64libmlx5-devel-41mlnx1-OFED.4.4.2.0.1.44207.x86_64libmlx4-41mlnx1-OFED.4.4.2.0.0.44207.x86_64librdmacm-devel-41mlnx1-OFED.4.2.0.1.3.44207.x86_64mlnx-en-dpdk-4.4-2.0.7.0.noarchlibibverbs-41mlnx1-OFED.4.4.2.0.1.44207.x86_64kmod-mlnx-ofa_kernel-4.4-OFED.4.4.2.0.7.1.gee7aa0e.rhel7u5.x86_64mlx_fe-fe-0$ rpm -qa | grep -i mlxlibmlx5-41mlnx1-OFED.4.4.2.0.1.44207.x86_64libmlx5-devel-41mlnx1-OFED.4.4.2.0.1.44207.x86_64libmlx4-41mlnx1-OFED.4.4.2.0.0.44207.x86_64Regards,KiranSo essentially two problems here.libibverbs-15-6.el7.x86_64rdma-core-15-6.el7.x86_64It fails with below errors:Requires: libmlx5.so.1(MLX5_1.0)(64bit)Removing: libibverbs-15-6.el7.x86_64 (@base)libmlx5.so.1(MLX5_1.0)(64bit)Updated By: libibverbs-41mlnx1-OFED.4.4.2.0.1.44207.x86_64 (sriov_mlnx)Not foundError: Package: ibacm-15-6.el7.x86_64 (@base)Requires: rdma-core(x86-64) = 15-6.el7Removing: rdma-core-15-6.el7.x86_64 (@base)rdma-core(x86-64) = 15-6.el7Obsoleted By: mlnx-ofa_kernel-4.4-OFED.4.4.2.0.7.1.gee7aa0e.rhel7u5.x86_64 (sriov_mlnx)Not foundError: Package: libibcm-15-6.el7.x86_64 (@base)Requires: libibverbs(x86-64) = 15-6.el7Removing: libibverbs-15-6.el7.x86_64 (@base)libibverbs(x86-64) = 15-6.el7Updated By: libibverbs-41mlnx1-OFED.4.4.2.0.1.44207.x86_64 (sriov_mlnx)libibverbs(x86-64) = 41mlnx1-OFED.4.4.2.0.1.44207Error: Package: libibumad-15-6.el7.x86_64 (@base)Requires: rdma-core(x86-64) = 15-6.el7Removing: rdma-core-15-6.el7.x86_64 (@base)rdma-core(x86-64) = 15-6.el7Obsoleted By: mlnx-ofa_kernel-4.4-OFED.4.4.2.0.7.1.gee7aa0e.rhel7u5.x86_64 (sriov_mlnx)Not foundError: Package: ibacm-15-6.el7.x86_64 (@base)Requires: libibverbs(x86-64) = 15-6.el7Removing: libibverbs-15-6.el7.x86_64 (@base)libibverbs(x86-64) = 15-6.el7Updated By: libibverbs-41mlnx1-OFED.4.4.2.0.1.44207.x86_64 (sriov_mlnx)libibverbs(x86-64) = 41mlnx1-OFED.4.4.2.0.1.44207Error: Package: libibcm-15-6.el7.x86_64 (@base)Requires: rdma-core(x86-64) = 15-6.el7Removing: rdma-core-15-6.el7.x86_64 (@base)rdma-core(x86-64) = 15-6.el7Obsoleted By: mlnx-ofa_kernel-4.4-OFED.4.4.2.0.7.1.gee7aa0e.rhel7u5.x86_64 (sriov_mlnx)Not foundYou could try using --skip-broken to work around the problem2.libmlx4.so library missing after installing dpdk specific packages.Hi Kiran,MLNX_OFED 4.4-2.0.7.0 driver is an old driver from 3 years ago that we no longer maintain or debug, as well as the mlnx-en-dpdk driver. Please try to install driver version 5.0, which is the latest version that supports ConnectX-3 NIC.Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
1051,get-implemented-infiniband-specification-version-of-nic,"Hello,we would like to know which IB specification is implemented for our ConnectX-4 NIC.
I couldn’t find any references. I do know that IB 1.7 was released this year, but which specification is used by our hardware/software? Is there any utility I can use to find out?What is the question that you have with regards to the spec?Powered by Discourse, best viewed with JavaScript enabled"
1052,wrn-nue47-user-requested-maximum-vls-is-larger-than-supported-vls,"Dear all,I have been trying to connect six switches using octahedron network topology. I modified the opensm.conf file while starting opensm and check if there is any error.The opensm.conf file I entered modifies the default opensm.conf file for the following lines:However, there are errors like the get_max_num_vls: WRN NUE47: user requested maximum #VLs is larger than supported #VLs exist in the log.Am I not suppose to set max_op_vls too large?Many thanks!!The error is issued when the nue_max_num_vls is larger than any of the portinfo opVL across the fabric.
ibdiagnet (db_csv file) would hold the opVLs configured on the fabric ports – need to ensure those are all <=8Have you tried reducing the number of VLs to see if it is not reproducing?Regardless – the Nue protocol isn’t maintained by NVDA. For issues with the protocol, it is required to contact the dev.Thanks to your response!!Yes by reducing max_op_vls to 4 it works. We have tried that both max_op_vls 8 and max_op_vls 7 do not work.We are running an application where the message size distribution isThe main objective for us is to know how to tune the QoS to get a better performance.Powered by Discourse, best viewed with JavaScript enabled"
1053,how-to-enable-out-of-order-data-placement-with-non-mellanox-switches,"Greetings,We are using CX-5, and want to use OoO data placement for adaptive routing, but using non-mellanox switches (for example, not Quantum/Switch-IB but using Intel Tofino).In the configuration webpage of Adaptive Routing, it is stated as following:Adaptive Routing (AR) and Self-Healing Networking (interconnect enhancement for intelligent datacenters) mechanisms in an InfiniBand fabric that utilizes Mellanox switch systems (Quantum ™/Switch-IB® 2 and above) and HCAs (ConnectX®-5 and above).Here are our questions:(1) Can we enable the out-of-order receive (e.g., reordering bufferings) for RDMA packets w/o MLX switches? If yes, is it supported by default or how can we enable it?(2) In the configuration of AR (adaptive routing), we believe we need to specify the topology to “routing_engine” flag (e.g., Dragonfly or fat-tree). Do we need to specify more details of topology, e.g., number of levels in fat-tree?Thank you in advance to your answers!
Best regards,
MasonPowered by Discourse, best viewed with JavaScript enabled"
1054,cant-start-fresh-ufm-install,"Hey there!
I followed the manual as best I could on a fresh CentOS 7.9 machine, and I’m running into a weird error:I have a case number open with Mellanox support, but I’d like to see if anyone here knows the answer and to document it for the public once we figure it out.  Google has zero results for me.Thanks!
-DerekHello Derek,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Based on the information provided, this is a common configuration issue. You need to assign a valid IP address (IPoIB) on the IB interface connected to the IB fabric.You can do this manually with the standard Linux commands ‘ifconfig’ and ‘ip addr add’  or to make it persistent through reboot, create an interface configuration file.When a valid IP address is assigned to ib0, you will be able to start UFM properly.In the following link → https://docs.nvidia.com/networking/display/UFMEnterpriseUMv68/UFM+Software+Installation+Prerequisites
it mentions this as a prerequisite.

image2351×595 76.8 KB
Thank you and regards,
~NVIDIA Networking Technical SupportHey MvB!
Thank you for the reply!Do we need to define an IP address for ib0 even if we’re not using IPoIB?
The line right above made me think that you could use either ib0 or eth0, my mistake.How do I know what a valid IP address is for ib0?  I checked some of our other servers, but I’m not seeing an IP address assigned for ib0 on them.Thank you!I tried applying a garbage IP to ib0 (10.254.254.7), tried to start it again, and it worked!So looks like you were correct and I should’ve applied “some” IP address to ib0 (which I configured in /etc/sysconfig/network-scripts/ifcfg-ib0 as datagram mode, AKA non-IP…).On the IP configuration note, I used section 13.8.7 here: 13.8. Configuring IPoIB Red Hat Enterprise Linux 7 | Red Hat Customer PortalThe file looks like:Launched fine after that and I can access it via IP/ufm_web, and it’s seeing a lot of IB switches and nodes! Not sure if anything is missing yet, but looking good so far!Thank you again for the help!Hi Derek,Good to see that is working as designed. Yes, even if you do not use IPoIB, UFM needs to have this configured, else it would not start.As you mentioned you have a support case open with us, I found it in the system but it was closed due to the fact that no support entitlement was found. Support for UFM can only be provided through a valid support entitlement. As your customer has a valid license, next time mention when opening a ticket the end-customers name so we can locate the support entitlement automatically, and we will support you through the support ticket.Cheers,.
~MartijnHey Martijn,
Likewise, I’m glad to see it working!
Though I would really like to see an update to the documentation to explain what to do, ideally with an example configuration file.  It’s very hard conceptually to understand exactly what to do when it doesn’t offer any details nor example for this step.
In my mind and in the gv.cfg file, I kept going back over, “Why does it need an IP for native/datagram mode?  That makes no sense.  Even if it does require an IP, I have no idea what subnet it should be in or where it should be defined…”But to double check, it doesn’t require any specific IP, just “something” defined in the system network interface file for the InfiniBand interface?Ok will do!  Thank you for the tip!
One note on that subject, the only replies I ever got from submitting that ticket were “Additional Information required” emails.  I clicked the contained link and filled out the information three times but the system never seemed to acknowledge it that way, looked like a bug in that system because after submitting it would complain that I wasn’t logged into Force.com (even if I were logged into https://support.mellanox.com ).This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1055,bluefield-2-dpu-connect-issue,"Hi all,ubuntu@localhost:~$ sudo mlnx-sf -a showSF Index: pci/0000:03:00.0/491520
Parent PCI dev: 0000:03:00.0
Representor netdev: en3f0pf0sf0
Function HWADDR: 02:cc:d4:9d:59:c8
Auxiliary device: mlx5_core.sf.2
netdev: enp3s0f0s0
RDMA dev: mlx5_2ubuntu@localhost:~$ ifconfig enp3s0f0s0
enp3s0f0s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 10.0.10.2  netmask 255.255.255.0  broadcast 10.0.10.255
inet6 fe80::cc:d4ff:fe9d:59c8  prefixlen 64  scopeid 0x20
ether 02:cc:d4:9d:59:c8  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 316  bytes 39238 (39.2 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0ubuntu@localhost:~$When I ping the host from enp3s0f0s0, and do tcpdump on en3f0pf0sf0.
No package could be caputred.Is there any other configuration to disable this?
Thanks.=====
I want to add that the tx of enp3s0f0s0 and  rx of en3f0pf0sf0 grows when ping.ubuntu@localhost:~$ ifconfig en3f0pf0sf0
en3f0pf0sf0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
ether 36:eb:43:4b:23:4a  txqueuelen 1000  (Ethernet)
RX packets 346  bytes 41038 (41.0 KB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 0  bytes 0 (0.0 B)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0ubuntu@localhost:~$ ifconfig enp3s0f0s0
enp3s0f0s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet 10.0.10.2  netmask 255.255.255.0  broadcast 10.0.10.255
inet6 fe80::cc:d4ff:fe9d:59c8  prefixlen 64  scopeid 0x20
ether 02:cc:d4:9d:59:c8  txqueuelen 1000  (Ethernet)
RX packets 0  bytes 0 (0.0 B)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 346  bytes 41038 (41.0 KB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0ubuntu@localhost:~$Hi wangyw,Thank you for posting your inquiry to the NVIDIA Networking Forums.You should be able to capture packets directly from the physical interface representor (IE: ARM-side p0, p1 interface).The data path will change depending on your configuration, and which mode of operation you have configured.
Please review the following document (‘Modes of Operation’) for more details about the data path in each mode of operation: https://docs.nvidia.com/networking/display/BlueFieldDPUOSv385/Modes+of+OperationIf you need further assistance, please open a support case at: https://support.mellanox.com/s/Best regards,
NVIDIA Networking SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1056,can-t-insert-module-libnvm-ko-unknown-symbol,"Hi everyone, I’m trying to run an open source project(BaM) on the A100 server. Unfortunately,  when inserting the libnvm.ko module(insmod libnvm.ko), the following error occurs on the system[ 2128.621501] libnvm: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 2128.621518] libnvm: Unknown symbol nvidia_p2p_dma_unmap_pages (err -2)
[ 2128.621571] libnvm: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 2128.621576] libnvm: Unknown symbol nvidia_p2p_get_pages (err -2)
[ 2128.621593] libnvm: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 2128.621597] libnvm: Unknown symbol nvidia_p2p_put_pages (err -2)
[ 2128.621610] libnvm: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 2128.621613] libnvm: Unknown symbol nvidia_p2p_dma_map_pages (err -2)
[ 2128.621627] libnvm: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 2128.621631] libnvm: Unknown symbol nvidia_p2p_free_page_table (err -2)By searching I feel this is due to the nvidia kernel version. I have tried the version 470.161.03、470.141.03、470.103.01, but the above problems still exist. I can’t search for any more help so I hope you can help me out, thank you very much!Hi,This issue relate to GPL.If you have A100 support service, please contact the corresponding technical support team for the help.Thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1057,error-checking-lossy-roce-acceleration-state,"Hello,I am having some trouble running jobs in my RoCE mini cluster and was suggested to enable lossy RoCE acceleration as described at https://community.mellanox.com/s/article/How-to-Enable-Disable-Lossy-RoCE-Accelerations. When I try to follow the instructions, I can see the registers, which include ROCE_ACCL, but everything else goes downhill from there. Either of the 4 commands:sudo mlxreg -d 5e:00.0 --reg_name ROCE_ACCL --get[sudo mlxreg -d 5e:00.0 --reg_name ROCE_ACCL --get][sudo mlxreg -d /dev/mst/mt4121_pciconf0 --reg_name ROCE_ACCL --get[sudo mlxreg -d /dev/mst/mt4121_pciconf0.1 --reg_name ROCE_ACCL --get]returns the error:Sending access register…-E- Failed to send access register: ME_ICMD_OPERATIONAL_ERRORI cannot do much besides listing registers with mlxreg and got other error messages. However, I think that the first step is to understand what ME_ICMD_OPERATIONAL_ERROR means or why it’s happening.Thanks.Hello Arturo,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, this error message you are getting when the driver is not loaded/ or properly loaded.See below example output based on a node running FreeBSD w/ ConnectX-6 configured in Ethernet mode.With driver loaded:Sending access register…Field Name | Data====================================================roce_adp_retrans_field_select | 0x00000001roce_tx_window_field_select | 0x00000001roce_slow_restart_field_select | 0x00000001roce_slow_restart_idle_field_select | 0x00000001roce_adp_retrans_en | 0x00000000roce_tx_window_en | 0x00000000roce_slow_restart_en | 0x00000000roce_slow_restart_idle_en | 0x00000000====================================================Example output with driver unloaded:Id Refs Address Size Name1 28 0xffffffff80200000 227b0a0 kernel3 1 0xffffffff824b7000 63fd0 mlx5.ko4 2 0xffffffff8251b000 39a8 mlxfw.ko5 2 0xffffffff8251f000 4790 xz.ko6 4 0xffffffff82524000 2da38 linuxkpi.ko7 1 0xffffffff82552000 3b98 dcons.ko9 1 0xffffffff8258b000 a3538 ibcore.ko10 1 0xffffffff82912000 1a20 fdescfs.ko11 1 0xffffffff82914000 2698 intpm.ko12 1 0xffffffff82917000 b40 smbus.ko13 1 0xffffffff82918000 1860 uhid.ko14 1 0xffffffff8291a000 2908 ums.ko15 1 0xffffffff8291d000 46f0 autofs.koSending access register…-E- Failed to send access register: ME_ICMD_OPERATIONAL_ERRORWith driver loaded, and Enabling Lossy RoCE accelerations:You are about to send access register: ROCE_ACCL with the following data:Field Name | Data====================================================roce_adp_retrans_field_select | 0x00000001roce_tx_window_field_select | 0x00000001roce_slow_restart_field_select | 0x00000001roce_slow_restart_idle_field_select | 0x00000001roce_adp_retrans_en | 0x00000001roce_tx_window_en | 0x00000001roce_slow_restart_en | 0x00000001roce_slow_restart_idle_en | 0x00000000====================================================Do you want to continue ? (y/n) [n] :Also make sure the adapter is configured for Ethernet.Instructions work as well when running Linux, just make sure that the driver is loaded properly.Thank you and regards,~NVIDIA Networking Technical SupportHi Martijn,I have installed & reinstalled the drivers several times and it always causes the same problems. I found an earlier post https://community.mellanox.com/s/article/understanding-rocev2-congestion-management, which is creating some doubts (in my mind) whether you still need to install a congestion manager or if the congestion manager could be responsible for the error.Thanks,ArturoPowered by Discourse, best viewed with JavaScript enabled"
1058,bluefield-1-device-with-part-number-mbf1m636a-csnat-supports-infiband-edr-mode,"When configuring a network interface on a BlueField device, there is no Infiniband mode.Hello karc,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Unfortunately the p/n you have is not a VPI adapter. You have Ethernet only.Please review the following link with the p/n information and capabilities → https://docs.nvidia.com/networking/display/MBF1600VPI/Introduction
image1793×353 59.3 KB
Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1059,what-is-the-longest-distance-that-two-msb7800-can-connect,"What is the longest distance that two MSB7800 can connectHi stevemontg93,NVIDIA provides long-haul systems, such as MetroX-2 Systems, to connect IB switches over long distances. NVIDIA MetroX-2 systems can extend InfiniBand to data centers for distances of 10 and 40 kilometers. You can find more detail in the below link:NVIDIA long-haul systems enable seamless connectivity of remote InfiniBand data centers, and storage.Thanks,
YuyingThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1060,mlnx-ofed-kernel-installation-failed,"Hi All, We are trying to install Mellanox and getting “mlnx-ofed-kernel installation failed!” issue. Need your support to resolve the issue.Followed below steps:Install the libverbs library using the below command. apt-get install ibverbs-utilsDownload the latest Mellanox OFED/EN as below. The version in the example may vary. This can be found from the Mellanox siteEx: wget http://content.mellanox.com/ofed/MLNX_OFED-5.0-1.0.0.0/MLNX_OFED_LINUX-5.0-1.0.0.0-ubuntu18.04-x86_64.tgz./mlnxofedinstall --upstream-libs --dpdkError: mlnx-ofed-kernel-dkms installation failed!Collecting debug info…See:/tmp/MLNX_OFED_LINUX.26945.logs/mlnx-ofed-kernel-dkms.debinstall.log/tmp/MLNX_OFED_LINUX.26945.logs/mlnx-ofed-kernel-dkms.make.logRemoving newly installed packages…Note: Please find the hardware details:Model no: CX516AHCA---->ConnectX®-5/ConnectX®-5 Ex (50GbE)mlnx-ofed-kernel-dkms.make.log (82.3 KB)mlnx-ofed-kernel-dkms.debinstall.log (82 KB)Hello Ravi,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, this is a known issue when trying to install this version of MLNX_OFED 5.0 on the newer kernel versions provided by the Linux distro (>-66).This issue is resolved in the latest MLNX_OFED GA version, which is version 5.3You can download the latest version through the following link → Linux InfiniBand DriversThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1061,is-rnic-read-write-data-in-increasing-address-order,"Lots of paper about one-sided RDMA [e.g. Sherman@SIGMOD22, Flock@SOSP21] claim RNIC read/write data in increasing address order, so the version-based consistency check is effective.Data layout: [version | … payload … | version]One thread updates the data record, while other threads read the data record. if left version == right version, the payload is expected consistent. However in my platform (CX-6, OFED5.0, RDMA RC one-sided operation), if the payload is large than 2 cachelines, middle of the payload may remain unchanged but other parts haved been updated. (mixture of old data and new data, but versions are matched)I am wondering about the following questions:Thank you!Powered by Discourse, best viewed with JavaScript enabled"
1062,mlag-and-redundancy,"Referring to the doc, Data traffic is not routed via the IPL link, in the event that the LINK 2 is down as shown in the image, will the traffic be forwarded on the IPL link, so the redundancy will be ensured ?
Yes.
Traffic will be redirected over the IPL per  the scenario in the diagram and thusThank youPowered by Discourse, best viewed with JavaScript enabled"
1063,how-much-device-memory-can-i-allocate-in-connectx-6,"Hi.
I’m trying to use device memory in ConnectX-6.
When I query MEMIC_SIZE_LIMIT, it shows 256KB.
Is it the maximum device memory size I can allocate?
Also, what does MEMIC_BAR_SIZE mean?I want to allocate more than 256KB on ConnectX-6 memory for partial DMA.
It is known to have 4MiB for its SRAM, but I do not know why MEMIC_SIZE_LIMIT cannot be set larger than 256KB.
Can you tell me how to allocate large device memory in ConnectX-6?Thank you.Hi cerotki,Thank you for posting your inquiry to the NVIDIA Developer Forums.Description for these firmware configuration elements can be found via mlxconfig (included with Mellanox Firmware Tools - MFT).For example, using the latest MFT (available at this link), and assuming your device is mlx5_0:mlxconfig -d mlx5_0 show_confsThe MEMIC CONF section displays the following details:If you have any additional questions about this, and have valid support entitlement, please open a support ticket with Enterprise Support.Thanks,
NVIDIA Enterprise ExperienceThank you for your response.
I have one more simple question.
What do BAR and MEMIC mean?Also, when I tried to set MEMIC_BAR_SIZE as larger than 0, or MEMIC_SIZE_LIMIT as larger than 1 (256KB), mlxconfig returned “Applying … Failed!”.Parameter MEMIC_BAR_SIZE’ value is larger than maximum allowed 0
Parameter MEMIC_SIZE_LIMIT’ value is larger than maximum allowed 1Is there any way to set the maximum allowed values?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1064,using-qinq-on-openstack-train-sriov-vfs,"Hi all,our setup: RedHat OpenStack OSP16.1 (Train), HPE-branded CX-4 ethernet-only NICs.Trying to allow VMs to do VLAN tagging, along the lines of https://docs.mellanox.com/pages/viewpage.action?pageId=12013554 , the Neutron network is created as a regular SRIOV network, eg. the VF has a VLAN tag attached to it.Observations:The “seen on the network” is with a span-session on the port where the SRIOV NIC is connectedWondering if anyone has got this working specifically on CX4? I’ve already taken this up with RH and they tested this with CX5 and they have no issues…Thanks for any feedback/ideas/…JanHello Jan,Thank you for posting your inquiry to the Mellanox Community.For HPE adapter support, you will need to engage HPE.Best regards,Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1065,mft-mxlreg-mlxconfig,"从此文档(https://docs.mellanox.com/display/MFT4170/mlxreg+Utility)中看到，mlxreg和mlxconfig工具的使用方法，具体使用过程中有些疑问：1、在设置和获取寄存器fields值得时候，–indexes参数是怎么获取的，是从查询的寄存器信息中计算的吗？
20211207-171420(WeLinkPC)1039×503 36.6 KB
2、mlxreg工具使用在文档目录的debug使用部分，用户是否能够通过设置寄存器，获取更佳的性能？会不会有性能稳定性能方面问题？如果可行的话，请问有没有关于列出寄存器功能等更加详细的文档？3、使用工具mlxconfig 参数为query的时候，可以获取当前设备的设置，show_confs可以获取所有参数列表。请问query查询的当前设备的参数就是当前设备所支持的全量可配置参数吗？是否在show_confs中仍在存在可以配置的参数选项？两者是子集的关系？4、mlxconfig的参数设置是不是mxlreg寄存器设置的参数组合？两者之间是有联系的吗？感谢。😀 ​你好；谢谢​感谢回复。3中query获取的设备参数值，针对不同的网络应用，理论上讲是不是存在不同的参数模板，能够有更佳的网络性能表现？​基本不会， 你可以仔细看看mlxconfig可以改的参数， 主要是功能，不是性能​你好,感谢回复。我们这边修改了CQE_COMPRESSION和INT_LOG_MAX_PAYLOAD_SIZE参数，并且使用OSU benchmark进行性能测试，发现在某些场景下能够提升性能。现在想请问下，除了reboot重启或者使用mlxfwreset 是配置生效，有没有其它不影响网络的方法使配置生效的？因为我们后期想针对应用特征对设置进行动态设置，所以需要保证网络的稳定行。Powered by Discourse, best viewed with JavaScript enabled"
1066,ssd-failure-across-at-least-7-sb7700s-need-firmware-download-please,"Yes the cheap Innodisk 16gb mSATA has corruption in one or multiple disk parts.   since they are out of warranty i am fixing them myself since no support options are available.
As somebody else stated… the DOWNLOAD button is just a graphic with no hyperlink to any file…Do we have a location to get the most recent firmware from??I just need the firmware or any relevant flashing tools.   thanksHello djones3,Unfortunately, to obtain new versions of MLNX-OS code and/or f/w for NVIDIA Infiniband switches a support contract is needed.You can contact our contracts department for the available entitlement options available. This can be done by opening a ticket through networking-contracts@nvidia.comThank you,
~NVEX Global SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1067,which-cables-to-use-100gbs,"Hello, so I need a suggestion which cables to use for direct 1:1 100Gbs connection between Mellanox SwitchIB-2 7800 and MCX653106A-HDAT.Based on connectivity matrix
Switch and HCAs Cable Connectivity Matrix: June21 Release - Included Content - NVIDIA Networking Docsshould be EDR cables MFA1A00-E0xx compatible. Am I right?Powered by Discourse, best viewed with JavaScript enabled"
1068,windows-10-teaming-with-mlx5muxtool,"I set up a teaming on Windows 10 with mlx5muxtool containing 2 ConnectX-4 Lx adapters.
When I disable or disconnect the primary NIC the Virtual Miniport gets disabled/disconnected as well, but I would expect to see a failover to the second NIC instead. It looks like the virtual team is just a clone of the primary NIC instead of a team of both NIC’s.The team I created:
Found 1 team(s)Name               : MyTeam
GUID               : {9410BA34-319B-49F5-A02F-9F9EAF9F3B35}
PortType           : Ethernet
TeamType           : Failover
MemberCount        : 2
Member[0]          : {80F37596-D050-4788-9928-1E9AA92C8E38} (Ethernet 8)
Member[1]          : {E3DDC041-A649-4D87-BFCF-D757DF4013A3} (Ethernet 9)Any advice why the interfaces doesn’t failover?hiyou need add a primary member.
please following this guide:
https://docs.nvidia.com/networking/display/winof2v320/Ethernet+Network#EthernetNetwork-TeamingandVLANThanks for the reply.
I added a VLAN id for both NIC’s but doesn’t seem to make a difference. The virtual team still gets disabled when I disable the primary NIC. I also don’t see why the VLAN ID could impact this behavior?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1069,no-compress-devices-found-when-measuring-the-compression-accelerator-on-bluefield-2,"Hi,I am using dpdk-test-compress-perf to measure the performance of the onboard compression accelerator on BlueFeild-2 (MBF2H516A-CENOT). The Mellanox OFED driver is MLNX_OFED_LINUX-5.5-1.0.3.2-ubuntu20.04-x86_64 and the bfb file I used is DOCA_v1.2.1_BlueField_OS_Ubuntu_20.04-5.4.0-1023-bluefield-5.5-2.1.7.0-3.8.5.12027-1.signed-aarch64.bfb. The measure command is
sudo /opt/mellanox/dpdk/bin/dpdk-test-compress-perf -l 4 -a 03:00.0,class=compress -- --driver-name mlx5_compress --input-file file.txt --seg-sz 8192 --compress-level 1:1:9 --num-iter 10 --extended-input-sz 1048576 --max-num-sgl-segs 16 --huffman-enc fixedThe logs show that “No compress devices type mlx5_compress available”. Could you please check whether my command is correct? Or are there other ways or tools to benchmark the compression accelerator? The full log is shown below.Any response will be appreciated. :)Best regards.Hello,Try to run:
./dpdk-test-compress-perf -l0-1 -n 1 -a 03:00.0,class=compress – –driver-name mlx5 --input-file [file to compress]Best Regards,
VikiHi,you can try something like this:root@l-csi-bf2-25g-24:/home/ubuntu# du -sh file.txt
1001M	file.txt
root@l-csi-bf2-25g-24:/home/ubuntu# sysctl -w “vm.nr_hugepages=4096”
vm.nr_hugepages = 4096
root@l-csi-bf2-25g-24:/home/ubuntu# /opt/mellanox/dpdk/bin/dpdk-test-compress-perf -l0-1 -n 1 -a 03:00.0,class=compress – --driver-name mlx5 --input-file file.txt --compress-level 1:1:9 --num-iter 10 --extended-input-sz 1048576  --max-num-sgl-segs 16 --huffman-enc fixed
EAL: Detected 8 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Detected shared linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Selected IOVA mode ‘PA’
EAL: No available hugepages reported in hugepages-32768kB
EAL: No available hugepages reported in hugepages-64kB
EAL: No available hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support…
EAL: VFIO support initialized
EAL:   Device is not NUMA-aware, defaulting socket to 0
EAL: Probe PCI driver: mlx5_pci (15b3:a2d6) device: 0000:03:00.0 (socket 0)
EAL: No legacy callbacks, legacy socket not created
USER1: Compress device does not support chained mbufs. Max SGL segments set to 1USER1: 1048576 bytes read from file file.txtApp uses socket: 0
Burst size = 32
Input data size = 1048576Warning: for the current input parameters, number of ops is higher than one, which may result in sub-optimal performance.
To improve the performance (for the current input data) following parameters are suggested:
* Segment size: 59460
* Number of segments: 18For the current input parameters (segment size = 2048, maximum segments per SGL = 1):
* Total number of buffers: 512
* 511 buffer(s) 2048 bytes long, last buffer 2048 byte(s) long
* Number of ops: 512
* Total memory allocation: 1048576
* 511 ops: 1 segment(s) in each, segment size 2048
* 1 op (the last one): 1 segment 2048 byte(s) longroot@l-csi-bf2-25g-24:/home/ubuntu#Best Regards,
AnatolyPowered by Discourse, best viewed with JavaScript enabled"
1070,mellanox-connectx-5-tx-performance-issue,"NIC: MCX516A-CCAT
DPDK 19.11, not configure any devargs
Enable 16 RxQ and 48 TxQ per port, both RxQ and TxQ size are 16384.When we try to forward 100G 1500 byte size packets,  about 12% packets cannot send out.
However, it can reach higher tx packet rate if forward 512 byte size packets.hi jiantao,How much pps when using 1500 frame size?
As you know, when using a smaller 512 frame size, the pps should get much higher compared to 1500.
The specific data can be found here:
https://fast.dpdk.org/doc/perf/DPDK_19_11_Mellanox_NIC_performance_report.pdfAlso, the above performance report lists test settings.  It’s a good start to tune performance according to the suggestion.
Make sure all optimizations have been done before to performance test.Regards,
LeveiPowered by Discourse, best viewed with JavaScript enabled"
1071,mlx5-connectx6-dpdk-flow-create-integrity-not-supported-inner-l4-ok-l3-ok,"Red Hat Enterprise Linux Server release 7.9 (Maipo)ofed_info -sMLNX_OFED_LINUX-5.5-1.0.3.2:mlxfwmanagerQuerying Mellanox devices firmware …Device #1:Device Type: ConnectX6Part Number: MCX653106A-ECA_AxDescription: ConnectX-6 VPI adapter card; H100Gb/s (HDR100; EDR IB and 100GbE); dual-port QSFP56; PCIe3.0 x16; tall bracket; ROHS R6PSID: MT_0000000224PCI Device Name: 0000:98:00.0Base MAC: b8cef6f974eeVersions: Current AvailableFW 20.32.1010 N/APXE 3.6.0502 N/AUEFI 14.25.0017 N/AStatus: No matching image found./dpdk-testpmd -l 5,7,9,11,13,15,17 -n 4 – -i --rxq 6 --txq 6 --nb-cores 6 --rxd 2048 --txd 2048 --portmask 0xff --enable-rx-cksumflow create 0 ingress pattern eth / ipv4 / tcp / integrity value mask 8 value spec 8 / end actions count / queue index 0 / endreturn error msg: integrity_item not supported: Operation not supportedgdb dpdk-testpmd finddpdk-21.11\drivers\common\mlx5 ->mlx5_devx_cmds.c->mlx5_devx_query_pkt_integrity_match()printf(""inner_l3_ok =%d\n "",MLX5_GET(flow_table_nic_cap, hcattr,ft_field_support_2_nic_receive.inner_l3_ok));printf(""inner_l4_ok =%d\n "",MLX5_GET(flow_table_nic_cap, hcattr,ft_field_support_2_nic_receive.inner_l4_ok));printf(""outer_l3_ok =%d\n "",MLX5_GET(flow_table_nic_cap, hcattr,ft_field_support_2_nic_receive.outer_l3_ok));printf(""outer_l4_ok =%d\n "",MLX5_GET(flow_table_nic_cap, hcattr,ft_field_support_2_nic_receive.outer_l4_ok));printf(""inner_ipv4_checksum_ok =%d\n "",MLX5_GET(flow_table_nic_cap, hcattr,ft_field_support_2_nic_receive.inner_ipv4_checksum_ok));printf(""outer_ipv4_checksum_ok =%d\n "",MLX5_GET(flow_table_nic_cap, hcattr,ft_field_support_2_nic_receive.outer_ipv4_checksum_ok));inner_l3_ok =0inner_l4_ok =0outer_l3_ok =0outer_l4_ok =0inner_ipv4_checksum_ok =0outer_ipv4_checksum_ok =0Powered by Discourse, best viewed with JavaScript enabled"
1072,enable-vf-lag-on-multiple-nics,"I have confirmed that the MLNX OFED Driver version 23.04-0.5.3.3 supports “Multiport E-Switch”.If Multiport E-Switch is supported, it seems possible to bundle one port from each of the two NICs into VF-LAG.Is that correct?No, e-switch on same NIC only, if multi NIC need SW switch in theory.The multiport eSwitch mode allows to add rules on a VF representor with an action forwarding the packet to the physical port of the physical function.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1073,one-core-100-irq-some-times,"Hello!I have Mellanox Technologies MT28800 Family [ConnectX-5 Ex] card on server with AMD EPYC 7742 64-Core Processor. I used all recommendations about tuning this NIC, but some time I see one core with 100% irq. Ususaly it CPU0 or CPU68, if I try set_irq_affinity_bynode.sh 1 eth0.What to do?driver: mlx5_coreversion: 5.0-0firmware-version: 16.29.2002 (MT_0000000013)expansion-rom-version:bus-info: 0000:41:00.0supports-statistics: yessupports-test: yessupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: yes
fdc11352×991 48.7 KB
Now NIC stops working:Hi Андрй,In AMD based servers, please assign the affinities to cores belong to the numa nodes with the shortest distance to the closes numa node connected to the NIC.HowTo Find the Numa node connected to the network adapterHowTo find the numa nodes with the shortest distance:^Please choose the nodes with distance 10 and 11.HowTo find the relevant cores:HowTo assign the cores:For example:Notes:Regards,ChenThanks for your answer!0available: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63node 0 size: 515906 MBnode 0 free: 3090 MBnode 1 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127node 1 size: 516066 MBnode 1 free: 939 MBnode distances:node 0 10: 10 321: 32 10I made set_irq_affinity_bynode.sh 0 eth0Discovered irqs for eth0: 563 569 575 581 587 593 599 605 611 617 623 629 635 641 647 653 658 663 668 673 678 683 688 693 698 703 708 713 718 723 728 733 738 743 748 753 758 763 768 773 778 783 788 793 798 803 808 813 818 823 828 833 838 843 847 850 853 855 857 858 859 860 861 862Optimizing IRQs for Single port trafficAssign irq 563 core_id 0Assign irq 569 core_id 1Assign irq 575 core_id 2Assign irq 581 core_id 3Assign irq 587 core_id 4Assign irq 593 core_id 5Assign irq 599 core_id 6Assign irq 605 core_id 7Assign irq 611 core_id 8Assign irq 617 core_id 9Assign irq 623 core_id 10Assign irq 629 core_id 11Assign irq 635 core_id 12Assign irq 641 core_id 13Assign irq 647 core_id 14Assign irq 653 core_id 15Assign irq 658 core_id 16Assign irq 663 core_id 17Assign irq 668 core_id 18Assign irq 673 core_id 19Assign irq 678 core_id 20Assign irq 683 core_id 21Assign irq 688 core_id 22Assign irq 693 core_id 23Assign irq 698 core_id 24Assign irq 703 core_id 25Assign irq 708 core_id 26Assign irq 713 core_id 27Assign irq 718 core_id 28Assign irq 723 core_id 29Assign irq 728 core_id 30Assign irq 733 core_id 31Assign irq 738 core_id 32Assign irq 743 core_id 33Assign irq 748 core_id 34Assign irq 753 core_id 35Assign irq 758 core_id 36Assign irq 763 core_id 37Assign irq 768 core_id 38Assign irq 773 core_id 39Assign irq 778 core_id 40Assign irq 783 core_id 41Assign irq 788 core_id 42Assign irq 793 core_id 43Assign irq 798 core_id 44Assign irq 803 core_id 45Assign irq 808 core_id 46Assign irq 813 core_id 47Assign irq 818 core_id 48Assign irq 823 core_id 49Assign irq 828 core_id 50Assign irq 833 core_id 51Assign irq 838 core_id 52Assign irq 843 core_id 53Assign irq 847 core_id 54Assign irq 850 core_id 55Assign irq 853 core_id 56Assign irq 855 core_id 57Assign irq 857 core_id 58Assign irq 858 core_id 59Assign irq 859 core_id 60Assign irq 860 core_id 61Assign irq 861 core_id 62Assign irq 862 core_id 63done.Now all fine, most loaded core 64% irq. But sometimes one core loaded 100% irq, and trafic goes down from 45 Gbps to 30 Gbps
mlnx12134×1362 205 KB
2 weeks all was fine. Now every evening the same picture: today 100% load irq CPU004. Helps to change the node for a few minutes set_irq_affinity_bynode.sh 1 eth0 after that 2 CPU load 100% CPU068 and CPU072, after that I change it again to 0 node set_irq_affinity_bynode.sh 0 eth0 and almost a day all fine until the evening peaks. What wrong?
mlnx42133×1366 205 KB

mlnx31000×561 146 KB

mlnx22041×1176 179 KB
The same picture with Mellanox Technologies MT28908 Family [ConnectX-6]driver: mlx5_coreversion: 5.3-1.0.0firmware-version: 20.30.1004 (MT_0000000225)expansion-rom-version:bus-info: 0000:41:00.0supports-statistics: yessupports-test: yessupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: yes
mlnx11587×505 24.1 KB
Every day such picture:
mlnx-traffic797×468 16.9 KB

mlnx-cpu797×316 24.7 KB
Hello, now have better result? we can push 85gbps on same server spec.
on u graphs i see few nginx workers… total 8 workers per 128core ? try with 64 core (one socket) and bind affinity to CPU near socket NIC (cat /sys/class/net//device/numa_node)We can push even 96gbps on this server.

зображення2160×1380 330 KB
with 18 nginx workers ?
i see u have 5781% irq, if total 6400% one socket, thats mean nginx affinity bind on another socket or nginx affinity not binded ?
u delivery mp4 or hls video ?nginx not binded.  Number of workers plays no role. If there is no work, like today, they are sleepingu have a lower cpu usage system% and user%. we on 85gbps have 1200%system and 500-600%user.
and nginx wokers have balanced busy, because we use reuseportmp4 content ?u have different busy workers nginx, its no good… u use multi_accept on, right ? why ? i test with this options, not have different in load CPU, but only 8-10 workers is work with 80-90%, another is sleeping. without this options all workers ~ 15%Андрей, спасибо за reuseport, давно не читал ченджлог нгинкс, эта опция прошла мимо меня. Добавил, нагрузка на процессор снизилась.

Screenshot 2022-09-17 1851451071×1397 393 KB

Относительно описанной мной проблемы, когда прерывания сетевой карты грузят одно ядро. У меня сейчас 5 идентичных серверов с 100Г картой. Конфиги фирмавари, ядро итд - идентичное. На 4 сейчас все нормально, на одном все равно есть проблема с загрузкой одного ядра. Это при том, что нигде нет нагрузки даже 50 гбит/сУ меня не было таких проблем, прерывания сетевой карты всегда раскидываются по ядрам по дефолту, может у вас дистрибутив старый? я тестил на дебиане 11 и убунту 22.04. У нас сейчас 2 сервера как раз на этих ОС. примерно на 80-85 гигабит хватает. На дебиане кстате постабильнее, падения очень легкие - 1-3% трафика, в отличии от убунты - 20-30% ).
Еще я заметил у вас высокий уровень нагрузки irq, можно же сетевые прерывания оттюнить. У нас в 2 раза меньше irq)) но тем не менее падаем.А вообще на 96 гигабит стабилен был сервер? или это так уже с фризами? )Собственно видно, как около 12 ночи отпускает это ядро. При чем, если перекинуть интерапты на другую нума ноду set_irq_affinity_bynode.sh 0 enp161s0f0np0, все равно остается загрузка одного ядра на 100%, cpu064 меняется на cpu000. К сожалению инженеры меланокс не проявляют интерес к этой проблеме.

Screenshot 2022-09-17 1907431035×1392 404 KB
Где-то примерно после 70 Гбит/с проблема пропадает. А до этой нагрузки бывают вот такие залипания. На северах сейчас везде дебиан с ядром 4.19.0-20-amd64интерапты нельзя переносить на другой сокет, это ведь на уровне железа, если сетевая в сокете CPU2, то на CPU1 ты не сможешь ее поставить.
lspci -s 81:00.0 -v | grep NUMA
Flags: bus master, fast devsel, latency 0, IRQ 2403, NUMA node 1, IOMMU group 115если сделать так
echo 0 > /sys/class/net/enp129s0f0np0/device/numa_node
то драйвер сразу ругается:
[Firmware Bug]: Overriding NUMA node to 0.  Contact your vendor for updates.
Соответственно, если перенести интерапты на CPU1 ядра, наверно нагрузка будет и там и там ) ну или ни чего хорошего не будет))А вторую сетевую поставить в CPU1 тоже не получится :). А это бы решило проблему.ну у нас похожие проблемы в час пике, только на трафике > 80гигабит… 84-85 гигабит.

а это на дебиане, падение реально слабое.
с падениями мы решили, мы искуственно после 80 гигабит сбрасываем качество, и сразу ровная линяя канала идет ~80гигабит
и кстате там драйвер недавно обновился, у нас 5.6 везде, там 5.7 появился.Почему это нельзя переносить? Все можно, скрипт set_irq_affinity_bynode.sh нормально отрабаывает, переносит все интерапты на другой CPU.  С точки зрения оптимизации - это не оптимально,  потому что будет еще использована межпроцессорная шина.Powered by Discourse, best viewed with JavaScript enabled"
1074,gpudirect-rdma-for-udp-packets,"I have an FPGA card that sends UDP packets over a 100G Ethernet link. On the receiving side is a ConnectX-6 VPI card an NVIDIA GPU.Is it possible to use GPUDirect RDMA to capture the UDP packets directly to GPU memory? How do I get started? All of the examples/documentation I’ve seen relate to RoCE or InfiniBand. Can I use GPUDirect RDMA to receive plain UDP packets?Hi Bill,GPUDirect RDMA is an API between IB CORE and peer memory clients.It provides access to the NIC’s read/write peer memory data buffers, as a result it allows RMDA-based applications to use the peer device without the need to copy data to host memory.RDMA is based on OS memory management, as it requires pin memory of the OS.If you want use NIC direct access GPU on the same node to offload CPU on MPI, it requires openmpi and cuda.https://developer.nvidia.com/gpudirecthttps://community.mellanox.com/s/article/gpudirecthttps://docs.mellanox.com/category/gpudirectRegards,ChenPowered by Discourse, best viewed with JavaScript enabled"
1075,can-eth0-or-eth1-be-used-as-access-or-uplink-ports,"Hello, apologies for at least 3 things.We have a 10/25G switch with SFP connections running cumulus 4.2. It is mostly to be used as an isolated network but we do need to bridge the physical servers on it to a 1G ethernet network. Most of the servers don’t have 1G ethernet interfaces, otherwise we could use some of those to bridge. It’s not possible to connect the 25G cumulus network to our 1G ethernet network via the 25G ports. (We could look to see if there is an adapter that could do it if necessary).Is it possible to use either eth0 or eht1 (1G management interfaces) to act basically as a wan interface on the switch (apologies for the poor description), so that traffic can route to and from the switch via one of those interfaces to the 25G switch ports?The research I have done suggests that isn’t possible but I want to double check that please.Many thanks in advance.Technically, yes. Practically, no.The management interfaces are not connected to the ASIC and meant for oob management purposes. By default they are configured in the management VRF that prevents traffic passing from the front panel ports to the management ports (for security and management reasons). If you wouldn’t configure the oob ports in the management vrf, you can pass traffic between them, but that means traffic would pass over the CPU. Depending on the switch model, this would result in low speeds and affect the control plane as well.As for the “adapter”, that would be a better solution, although that has affects as well (depending on the use-case). You can look at one of these: Universal TransceiverThe best solution though would be to look at the Nvidia SN2201 switch that provides you the necessary 1G connectivity.Thanks very much for your responses attilla, I appreciate your time and effort. I think we will have to configure one of the 2 management ports to not be in the management VFT initially and live with passing traffic over the cpu. Then look at a universal transceiver or similar.I assume removing a management port from the mngt VRF and adding it to the relevant vlan the swp’s are in should be enough or does the switch need to be configured to route via the management port too?I agree with Attilla; I definitely recommend the adapter. They are very inexpensive and are a much better option. The use case of connecting Eth0 with the frontpanel ports is not supported officially these days and I would be hesistant to recommend it anywhere.You would need to modify switchd.conf to set:
“# Ignore routes that point to non-swp interfaces
#ignore_non_swps = TRUE
ignore_non_swps = FALSE”And restart the switchd service.
It may not work though as it’s not a recommended/tested configuration any longer.Update to close out, went with the universal transceiver in one of the servers so not using a switch management port to route or bridge networks. Thanks for all your help, especially the transceiver pointer.Powered by Discourse, best viewed with JavaScript enabled"
1076,enable-promiscuous-mode-for-mellanox-connectx-4-lx-in-windows-server-2019,"Hi,I was looking for information about how to Enable promiscuous mode for Mellanox ConnectX-4 LX in Windows server 2019 core.I have a server with Windows Server 2019 core running hyper-v equipped with the ConnectX-4 LX network card that I want to switch to promiscuous mode.NIC firmware version: 14.30.1004Windows driver version: 2.60.5100SR-IOV is enabled in BIOSI set “Enable MAC address spoofing” in Hyper-v.In the NIC documentation I had access to, I did not find anything concrete except a reference to VF Spoof Protection and some keys in the registers: VFAllowedTxEtherTypeListEnable.I tried to enable and disable FAllowedTxEtherTypeListEnable but the NIC did not switch to promiscuous mode.The other network cards on the server motherboard can be switched to promiscuous mode, but I was unable to switch to promiscuous Mellanox ConnectX-4 LX NIC.
PromiscuousMode873×234 43.9 KB
Any advice on activating the promiscuous mode in Windows Server 2019 would be very helpful!Thanks a lot!Best regards,Paulhere are the steps on how to enable Promiscuous Mode on CoonectX-4 & higher Mellanox adapterPS C:> Get-NetAdapterSLOT 2 Port 2 Mellanox ConnectX-4 Adapter 6 Disconnected E4-1D-2D-E7-0F-77 …s+++++++++++++++++++++++++++++++++++++# $(Get-NetAdapter -Name “SLOT 2 Port 2”).PromiscuousModeFalse+++++++++++++++++++++++++++++++++++++++++++Pick up the adapter “Slot 2 Port 2” , “enable Promiscuous mode” on all or the relevant adapter4.press Strat5**.# $(Get-NetAdapter -Name “SLOT 2 Port 2”).PromiscuousMode**TrueHope this helpThank you very much for the answer.Unfortunately I can’t install Wireshark or any other application that uses GUI because the version of Windows Server I use is CORE, without GUI.The network card is installed on a server running Windows Server 2019 Core with hyper-v virtualization.One of the virtual machines uses the CARP protocol and for this it absolutely needs the promiscuous mode.I’m looking for a method by which when booting the network card to be automatically switched to promiscuous mode.Powered by Discourse, best viewed with JavaScript enabled"
1077,mcx512a-acat-problems-with-link-status-and-ethtool-on-centos-7,"Hello,I have been trying to configure my Mellanox NIC (MCX512A-ACAT) in order to have a 10G-BASE SR connection using SFP and fiber optic cables, to another board (a custom made board that supports the same type of connections). I have the following problems:The following outputs are produced:ethtool -s enp6s0f1 autoneg offCannot set new settings: Operation not supportednot setting autonegethtool -s enp6s0f1 duplex fullCannot advertise duplex fullethtool -s enp6s0f1 speed 10000Cannot advertise speed 10000Here are some outputs and some things I have found while trying to resolve this, that might be useful:A) ifconfigenp6s0f1: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500inet 192.168.2.50 netmask 255.255.255.248 broadcast 192.168.2.55inet6 fe80::a75d:a26b:de27:4c78 prefixlen 64 scopeid 0x20ether 98:03:9b:cc:82:a9 txqueuelen 1000 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 0 bytes 0 (0.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0B) ethtoolSettings for enp6s0f1:Supported ports: [ FIBRE ]Supported link modes: 1000baseKX/Full10000baseKR/Full25000baseCR/Full25000baseKR/Full25000baseSR/FullSupported pause frame use: SymmetricSupports auto-negotiation: YesSupported FEC modes: Not reportedAdvertised link modes: 1000baseKX/Full10000baseKR/Full25000baseCR/Full25000baseKR/Full25000baseSR/FullAdvertised pause frame use: SymmetricAdvertised auto-negotiation: YesAdvertised FEC modes: Not reportedSpeed: Unknown!Duplex: Unknown! (255)Port: FIBREPHYAD: 0Transceiver: internalAuto-negotiation: onSupports Wake-on: dWake-on: dCurrent message level: 0x00000004 (4)linkLink detected: noC) ip addr show enp6s0f15: enp6s0f1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000link/ether 98:03:9b:cc:82:a9 brd ff:ff:ff:ff:ff:ffinet 192.168.2.50/29 brd 192.168.2.55 scope global noprefixroute enp6s0f1valid_lft forever preferred_lft foreverinet6 fe80::a75d:a26b:de27:4c78/64 scope link tentative noprefixroutevalid_lft forever preferred_lft foreverD) This occurs on both slots of the NIC, using different SFPs and cables (which however produce light, have been tested for being functional and reliable). Currently, I am attempting an external loopback on the fibers, which fails, I presume due to the link status.E) I am able to use nmtui to configure and activate my connections.F) dmesg | grep -e mlx5_core -e enp6s0f1[ 1.365269] mlx5_core 0000:06:00.0: firmware version: 16.24.1000[ 1.365306] mlx5_core 0000:06:00.0: 63.008 Gb/s available PCIe bandwidth (8 GT/s x8 link)[ 1.580318] mlx5_core 0000:06:00.0: irq 74 for MSI/MSI-X[ 1.580331] mlx5_core 0000:06:00.0: irq 75 for MSI/MSI-X[ 1.580336] mlx5_core 0000:06:00.0: irq 76 for MSI/MSI-X[ 1.580342] mlx5_core 0000:06:00.0: irq 77 for MSI/MSI-X[ 1.580347] mlx5_core 0000:06:00.0: irq 78 for MSI/MSI-X[ 1.580352] mlx5_core 0000:06:00.0: irq 79 for MSI/MSI-X[ 1.580356] mlx5_core 0000:06:00.0: irq 80 for MSI/MSI-X[ 1.580360] mlx5_core 0000:06:00.0: irq 81 for MSI/MSI-X[ 1.580365] mlx5_core 0000:06:00.0: irq 82 for MSI/MSI-X[ 1.580369] mlx5_core 0000:06:00.0: irq 83 for MSI/MSI-X[ 1.580374] mlx5_core 0000:06:00.0: irq 84 for MSI/MSI-X[ 1.580378] mlx5_core 0000:06:00.0: irq 85 for MSI/MSI-X[ 1.580385] mlx5_core 0000:06:00.0: irq 86 for MSI/MSI-X[ 1.580389] mlx5_core 0000:06:00.0: irq 87 for MSI/MSI-X[ 1.580394] mlx5_core 0000:06:00.0: irq 88 for MSI/MSI-X[ 1.580398] mlx5_core 0000:06:00.0: irq 89 for MSI/MSI-X[ 1.581401] mlx5_core 0000:06:00.0: Port module event: module 0, Cable unplugged[ 1.588381] mlx5_core 0000:06:00.1: firmware version: 16.24.1000[ 1.588437] mlx5_core 0000:06:00.1: 63.008 Gb/s available PCIe bandwidth (8 GT/s x8 link)[ 1.808030] mlx5_core 0000:06:00.1: irq 91 for MSI/MSI-X[ 1.808036] mlx5_core 0000:06:00.1: irq 92 for MSI/MSI-X[ 1.808041] mlx5_core 0000:06:00.1: irq 93 for MSI/MSI-X[ 1.808046] mlx5_core 0000:06:00.1: irq 94 for MSI/MSI-X[ 1.808051] mlx5_core 0000:06:00.1: irq 95 for MSI/MSI-X[ 1.808056] mlx5_core 0000:06:00.1: irq 96 for MSI/MSI-X[ 1.808061] mlx5_core 0000:06:00.1: irq 97 for MSI/MSI-X[ 1.808065] mlx5_core 0000:06:00.1: irq 98 for MSI/MSI-X[ 1.808069] mlx5_core 0000:06:00.1: irq 99 for MSI/MSI-X[ 1.808074] mlx5_core 0000:06:00.1: irq 100 for MSI/MSI-X[ 1.808078] mlx5_core 0000:06:00.1: irq 101 for MSI/MSI-X[ 1.808082] mlx5_core 0000:06:00.1: irq 102 for MSI/MSI-X[ 1.808087] mlx5_core 0000:06:00.1: irq 103 for MSI/MSI-X[ 1.808091] mlx5_core 0000:06:00.1: irq 104 for MSI/MSI-X[ 1.808096] mlx5_core 0000:06:00.1: irq 105 for MSI/MSI-X[ 1.808100] mlx5_core 0000:06:00.1: irq 106 for MSI/MSI-X[ 1.809457] mlx5_core 0000:06:00.1: Port module event: module 1, Cable plugged[ 1.816221] mlx5_core 0000:06:00.0: slow_pci_heuristic:4521:(pid 321): Max link speed = 25000, PCI BW = 63008[ 1.816289] mlx5_core 0000:06:00.0: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)[ 1.945731] mlx5_core 0000:06:00.1: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)[ 6.913876] mlx5_core 0000:06:00.0 enp6s0f0: Link down[ 6.919262] IPv6: ADDRCONF(NETDEV_UP): enp6s0f1: link is not ready[ 6.985507] mlx5_core 0000:06:00.1 enp6s0f1: Link down[ 6.988709] IPv6: ADDRCONF(NETDEV_UP): enp6s0f1: link is not ready[ 17.441260] IPv6: ADDRCONF(NETDEV_UP): enp6s0f1: link is not ready[ 17.443918] IPv6: ADDRCONF(NETDEV_UP): enp6s0f1: link is not ready[69629.698456] mlx5_core 0000:06:00.1: Port module event: module 1, Cable unplugged[69639.097465] mlx5_core 0000:06:00.1: Port module event: module 1, Cable plugged[69720.194832] mlx5_core 0000:06:00.0: Port module event: module 0, Cable plugged[146454.650697] IPv6: ADDRCONF(NETDEV_UP): enp6s0f1: link is not ready[146529.819726] IPv6: ADDRCONF(NETDEV_UP): enp6s0f1: link is not readyG) cat /etc/centos-release: CentOS Linux release 7.9.2009 (Core)I apologize for the long post, and can provide any additional information. I have checked to the best of my ability all user guides and online resolutions, but if you have any suggestions they could be very helpful!MaryHello,You had mentioned a custom board being utilized on the peer side, and it may be a significant part of the link problem.Please be advised that if the optical modules, cabling, and peer device on the other end of the link are not within the Firmware Compatible Product listings in the firmware release notes, it is considered an untested configuration. To confirm if the device and the interconnect devices between them are validated for use with the adapter, please check the Firmware Compatible Products matrix in the latest Firmware Release Notes:https://docs.mellanox.com/display/ConnectX5Firmwarev16311014/Firmware+Compatible+ProductsI do see the mlx5_core module loaded, but to be sure, we recommend ensuring you have the latest MLNX_OFED version supported for the adapter installed:MLNX_OFED Download:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)MLNX_OFED v5.4-3.0.3.0 User Manual:https://docs.mellanox.com/display/MLNXOFEDv543030/NVIDIA+MLNX_OFED+Documentation+Rev+5.4-3.0.3.0Based on the output you provided, we also noticed that you are utilizing firmware version 16.24.1000 which was released in December of 2018 and is not compatible with our latest MLNX_OFED driver; if at all possible, please consider updating to the latest firmware version for the MCX512A-ACAT, v16.31.1014 which can be found at the following link:Updating Firmware for ConnectX®-5 EN PCI Express Network Interface Cards (NICs)To review the MLNX_OFED:Firmware compatibility matrix, please visit the following link:Below is a list of the recommend MLNX_OFED driver / firmware sets for Mellanox products.Thank you,Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
1078,connectx-5-infiniband-mc2210511-lr4-transceivers-10gbit-s-sdr,"Hey,I have a Single Mode LC link which needs to transport 40G Infiniband to an SX6036. That means 40GBase-LR transceivers are my only option and I cannot use 56G MTP SR or 40G SR transceivers.But now for the issue:I am using ConnectX-5 cards (Dual Port) with the latest firmware, 16.32.1010 and I just can’t get 40G IB to work.I have tried different vendors and different programmings (fs.com, Finisar) and even bought genuine Mellanox LR4 transceivers (MC2210511-LR4) out of pure despair… But no chance.MC2210511-LR4:Infiniband device ‘mlx5_0’ port 1 status:default gid: fe80:0000:0000:0000:0c42:a103:006c:0170base lid: 0xffffsm lid: 0x0state: 2: INITphys state: 5: LinkUprate: 10 Gb/sec (4X SDR)link_layer: InfiniBandInfiniband device ‘mlx5_1’ port 1 status:default gid: fe80:0000:0000:0000:0c42:a103:006c:0171base lid: 0xffffsm lid: 0x0state: 2: INITphys state: 5: LinkUprate: 10 Gb/sec (4X SDR)link_layer: InfiniBand10G in IB Mode, 40G in Eth modeHowever, this is not a general limitation.If I plug in a DACInfiniband device ‘mlx5_0’ port 1 status:default gid: fe80:0000:0000:0000:0c42:a103:006c:0170base lid: 0xffffsm lid: 0x0state: 2: INITphys state: 5: LinkUprate: 100 Gb/sec (4X EDR)link_layer: InfiniBandInfiniband device ‘mlx5_1’ port 1 status:default gid: fe80:0000:0000:0000:0c42:a103:006c:0171base lid: 0xffffsm lid: 0x0state: 2: INITphys state: 5: LinkUprate: 100 Gb/sec (4X EDR)link_layer: InfiniBandI am seeing the same limitations with some 100G transceiversKaiam XQX5170 (100GBase-CWDM4) → No connection at all in IB mode, 100G in EthernetInfiniband device ‘mlx5_0’ port 1 status:default gid: fe80:0000:0000:0000:0c42:a103:006c:0170base lid: 0xffffsm lid: 0x0state: 1: DOWNphys state: 2: Pollingrate: 10 Gb/sec (4X SDR)link_layer: InfiniBandInfiniband device ‘mlx5_1’ port 1 status:default gid: fe80:0000:0000:0000:0c42:a103:006c:0171base lid: 0xffffsm lid: 0x0state: 1: DOWNphys state: 2: Pollingrate: 10 Gb/sec (4X SDR)link_layer: InfiniBandfs.com 100GBase-CWDM4 → 10G in IB Mode, 100G in EthernetInfiniband device ‘mlx5_0’ port 1 status:default gid: fe80:0000:0000:0000:0c42:a103:006c:0170base lid: 0xffffsm lid: 0x0state: 2: INITphys state: 5: LinkUprate: 10 Gb/sec (4X SDR)link_layer: InfiniBandInfiniband device ‘mlx5_1’ port 1 status:default gid: fe80:0000:0000:0000:0c42:a103:006c:0171base lid: 0xffffsm lid: 0x0state: 2: INITphys state: 5: LinkUprate: 10 Gb/sec (4X SDR)link_layer: InfiniBandAny ideas?At least the genuine MC2210511-LR4 transceivers should work?I have checked and the MC2210511-LR4 does not have the Infiniband Support bit set - Is this intended?Is there any way to get a 40G-LR4 Link with Infiniband on ConnectX-5?Also, according to NVIDIA Mellanox LinkX InfiniBand Optical Transceivers | NVIDIA, the MC2210511-LR4 should even be able to do FDR10…I will double check on older CX-3 cards.Powered by Discourse, best viewed with JavaScript enabled"
1079,nvidia-connectx-6-lx-drivers-not-compatible-with-esx-6-0,"Are there any drivers available for the Nvidia ConnectX-6 LX NIC which are compatible with vmware ESXi 6.0? I see there are ESXi 6.0 drivers are for the ConnectX-3 but not for the ConnectX-6 LX.Hi,
There is no ConnectX-6 LX driver for ESXi 6.0.
The OS is too old.
Thanks,This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1080,mlx5-eq-async-int-pid-13335-cq-error-on-cqn-0x45-syndrome-0x1,"HelloCFD _Solver runs intelmpi over Infiniband Mellanox -on 2 nodes ( hp apollo 230) with both rhel 79Some times ( same computation ) crashes with messagesmlx5_core 0000:5c:00.0: mlx5_eq_async_int:390:(pid 13335): CQ error on CQN 0x45, syndrome 0x1any hints ?ibstatibstatCA ‘mlx5_0’CA type: MT4115Number of ports: 1Firmware version: 12.27.4000Hardware version: 0Node GUID: 0xb88303ffff79d104System image GUID: 0xb88303ffff79d104Port 1:State: ActivePhysical state: LinkUpRate: 100Base lid: 61LMC: 0SM lid: 1Capability mask: 0x2659e84aPort GUID: 0xb88303ffff79d104Link layer: InfiniBandCA ‘mlx5_1’CA type: MT4115Number of ports: 1Firmware version: 12.27.4000Hardware version: 0Node GUID: 0xb88303ffff79d105System image GUID: 0xb88303ffff79d104Port 1:State: DownPhysical state: PollingRate: 10Base lid: 65535LMC: 0SM lid: 0Capability mask: 0x2659e848Port GUID: 0xb88303ffff79d105Link layer: InfiniBand[root@fnx628 plugins]#Hi Mrkus,Based on the information provided, syndrome 0x1 points to IBV_WC_LOC_LEN_ERR (1) - “Local Length Error”.Can you please provide the vendor syndrome?Regards,ChenSorry Chen for the late response,what exactly is the “vendor syndrome” ?ThanksRegardsMarkusPowered by Discourse, best viewed with JavaScript enabled"
1081,doca-simple-fwd-vnf-undefined-symbol-rte-mtr-meter-policy-add-version-experimental,"Environment:root@localhost:/home/ubuntu/src# /tmp/build/doca_simple_fwd_vnf -h
/tmp/build/doca_simple_fwd_vnf: symbol lookup error: /lib/aarch64-linux-gnu/libdoca_flow.so.1: undefined symbol: rte_mtr_meter_policy_add, version EXPERIMENTALHello,Based on the output provided, we would recommend opening a support case for further investigation of the issue. If you do not have a current support contract, please reach out to our Contracts team at networking-contracts@nvidia.comThank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1082,where-can-i-find-the-windows-libraries-sdk-files-to-develop-a-simple-ib-verb-example,"It doesn’t look like the current windows OFED2 SDK has examples that run in windows. Are there examples somewhere else I can find?thanks,~ShaneHello Shane,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately, with WinOF-2, the SDK is not supported anymore, as it only applies for adapters up to ConnectX-3.Instead of the IBAL API, all development needs to be done against Microsoft’s Network Direct Interface (NDI) API.The Network Direct Interface (NDI) architecture provides application developers with a networking interface that enables zero-copy data transfers between applications, kernel-bypass I/O generation and completion processing, and one-sided data transfer operations.NDI is supported by Microsoft and is the recommended method to write RDMA application. NDI exposes the advanced capabilities of the Mellanox networking devices and allows applications to leverage advances of RDMA.Both RoCE and InfiniBand (IB) can implement NDI.For further information, please refer to: http://msdn.microsoft.com/en-us/library/cc904397(v=vs.85).aspxFor code examples using NDI, you may refer to: https://msdn.microsoft.com/library/cc853440(v=vs.85).aspxThank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1083,infiniband-network-device-ib0-doesnt-exist,"Hi,  I have installed mlnx_ofed v5.5 (and previously v4.9) on a new rocky8.5 host using “dnf install mlnx-ofed-all”, with a connect-6 ib card. This has installed just fine and I can run ibstat to display the device, but the associated ib0 network device doesn’t exist.[root@wmc010 net]# ibstat
CA ‘mlx5_0’
CA type: MT4123
Number of ports: 1
Firmware version: 20.29.1016
Hardware version: 0
Node GUID: 0x88e9a4ffff1acbac
System image GUID: 0x88e9a4ffff1acbac
Port 1:
State: Active
Physical state: LinkUp
Rate: 100
Base lid: 462
LMC: 0
SM lid: 220
Capability mask: 0x2650e848
Port GUID: 0x88e9a4ffff1acbac
Link layer: InfiniBandBut there is no ib0 in /sys/class/net.  Am I missing something simple?thanks, GregHello g.hall,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Currently, there is no support for Rocky Linux in MLNX_OFED 5.x GA. Support for Rocky Linux will be available in the upcoming MLNX_OFED 5.6 GA version, available in a few weeks from now.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1084,dpdk-mlx5-pmd-rx-queue-deferred-start,"Hi,I’m using DPDK with the MLX5 PMD to receive UDP packets. In order to receive UDP packets only, I use DPDK’s Flow API, which requires starting the port before configuration (rte_eth_dev_start).The thing is that I don’t actually want to start receiving packets as soon as the port is started, but only do this because it is advised before using the Flow API.In order to start receiving packets at the right time for my application, I want to use rte_eth_rxconf::rx_deferred_start together with rte_eth_dev_rx_queue_start. On DPDK 22.07, this does not work as expected:RX queue 0 still starts on rte_eth_dev_start, and packets are received even if do not call rte_eth_dev_rx_queue_start. No commit seems to change this behaviour up to the latest DPDK version at the time of writing (23.03).
After a quick look in the code, rx_deferred_start seems to be mentionned only by the MLX5 PMD for configuration getter, not setters.Is it intended ? (limitations in mlx5 ?)
Could it be fixed in a future version ?Thanks,
JulienHi Julien,Thank you for posting your query on NVIDIA Forum.Based on internal check, unfortunately, yes, this is a limitation and currently this is not on the roadmap.We regret the inconvenience caused due to the same.Thanks,
Namrata.Hi Namrata,Thank you for the information. Could you at least document this as a limitation of the MLX5 PMD ? I may be wrong but I found no mention of this.Can you share any details about the reason of this limitation ? I’m not familiar with MNLX_OFED programming, but does the limitation stems from the driver, or only that it has not been wrapped in DPDK ? I could provide an implementation in the latter situation.Best regards,
JulienHi, Julien
Deferred start is imposed to provide late queue setup, after device start.
mlx5 PMD does not support deferred start - all queues must be configured before device start.I would recommend to consider the following scenarios:
Scenario A - using queue stop/startScenario B - using isolated modeScenario C - do not call rx_burst till flows configuredHi Viecheslavo,Thanks for all of these details, this is very informative. I didn’t know that deferred start provided late queue setup, I understood it as “do not start the queue on device start <=> stop it right at device start”I was actually using your scenario C as a workaround currently, but scenario A looks promising ! For some reason, I thought that if deferred start didn’t work, rte_eth_rx_queue_stop would not work either.Thanks again for the complete answer,
JulienThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1085,a-low-throughput-enviornment-with-connect-x3-40g-vpi-ethernet-card,"Server: Windows server 2019, Dell R7x0, Mellanox connect-x3 40Gb
Client: I7 13xxx CPU, X520 10Gb, disable interrupt mediation
Test with iperf:
64K tcp window, 400Mb
1M tcp window, 10Gb
seems normal, but compare with another server with same networkcard, the iperf result with 64K window can reach 10Gb, what probably wrong?I would check that both servers are aligned with the same WinOF driver version/FW(Mellanox OFED for Windows - WinOF / WinOF-2 >>> WinOF download)Note that CX3 is end of life and end of service accordingly.Are the OS between servers the same versions? Patched with the same KB’s?I would suggest for comparison purpose to validate that both servers are properly tunned.We have community articles that address that topic (performance) accordingly.Some pointers:BIOS settings set to max performance profile.Disable power saving.MTU setting default 1500 or Jumbo 9000.MTU setting on switch(s) port(s) default or 9000.Cables used and length.Device manager - > Network Adapters → Mellanox HCA Properties → Advanced → RX/TX buffers settingsRSS settings, the cores from the closest NUMA node should be used.Avoid using core 0 which is utilized for OS tasks.The preferred tool for performance testing (for throughput measurement) is NTttcp for comparison purpose.iperf can be used, make sure the version used is the same and parallelism is supported and used with the tool.For additional information, consult our WinOF UM on the performance section.thanks, we’ll have a try. The client OS for testing is windows 10 and Intel x520 net card, server OS is windows server 2019Powered by Discourse, best viewed with JavaScript enabled"
1086,bcm-9-2-node-provision-problem-installer-unreachable,"Recently my nodes booted up and are saying:I can SSH to them no problem and shares from the head node are mounted:From the booted regular node I can SSH back to the head node fine. How do I troubleshoot? ThanksMy regular node:Hi,Assuming that the node has booted up then the “INSTALLER_UNREACHABLE” in this case means that the cmd.service on the compute node can’t talk to the cmd.service on the headnode.Perhaps you can check the following:if there is a firewall filtering the traffic on the internal management network, then make sure that port 2 is open:
telnet  2 #  here has to be replaced with the real hostname of the compute nodecheck the /var/log/cmdaemon on the compute node to see if cmd.service is failing to start for any reason.Kind regards,
adelThanks it turns out my default-image was messed up and was missing cmdaemon.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1087,i-am-using-connectx5-mt27710-2port-100g-nic-for-our-benchmarking-exercise-we-are-trying-to-measure-throughput-for-vsrx-using-100g-card-but-we-are-seeing-traffic-entering-into-vsrx-vm-image-from-2nd-port-but-not-from-1st-port,"OS - Ubuntu 20.04.1 LTSKernel - 5.4.0-88-genericCPU - AMD EPYC 7763 64-Core ProcessorMLXOFED - MLNX_OFED_LINUX-5.4-1.0.3.0-ubuntu20.04-x86_64mft - mft-4.17.0-106-x86_64-deb.tgzI have missed to add in description.I have created 1 virtual function on each port using SR-IOV. In Port 1 virtual function traffic is not entering from IXIA traffic generator whereas same thing is working in Port 2 virtual function.Even it is not working in Ubuntu VM :- focal-server-cloudimg-amd64.img.Hi Muruli,We worked on this issue via the support case that you opened.Summary:The issue is invalid IXIA configuration. You used source MAC of multicast - the LSB of the first byte of the MAC address, for example in address xx:yy:zz:bb:cc:aa, the xx can’t have the LSB as ‘1’.We are treating these packets as invalid and dropping them in HW.When running promisc on the PF, the HW doesn’t discard these packets.Regards,ChenPowered by Discourse, best viewed with JavaScript enabled"
1088,mellanox-cx5-isnt-showing-pasid-capability-on-lspci,"Hi,I’m working with Mellonox CX-5 CDAT, I don’t see the PASID capability ID 0x001B on the extended capability address space when I get the hexdump using the lspci.I’m using Intel processor and enabled the VT-d on BIOS as well. Kernel Version is 5.15.0-69-genericHello bgodi,Thank you for posting your query on our community.Please make sure to enable ATS in BIOS and then enable it on the card using mlxconfig as below:
mlxconfig -d /dev/mst/mt4123_pciconf0 set ATS_ENABLED=1For more details:
https://wikinox.mellanox.com/display/FW/pcie+ATS+feature?focusedCommentId=244745226&refresh=1611739334677#comment-244745226 2Thanks & Regards,
Nvidia SupportHi Sribhargavid,Thank you for the response, I did what you recommended and I didn’t see any difference. I enabled the VT-d (ATS, I didn’t find anything exclusively that says ATS on Aptiov) and on the drivers I enabled the ATS_ENABLED flag. I see the ATS capability structure on lspci but not the PASID capability structure.I used other graphics card with PASID capability that is showing up on lspci in the same environment. So, I’m striking out the kernel modules/BIOS enablement/and any other environment settings.Thank you,
Bharathou for the response, I did what you recommended and I didn’t see any difference. I enabled the VT-d (ATS, I didn’t find aHello Bharath,Thank you for your quick response. PASID is a feature that can be enabled in the context of ATS.
ConnectX-5 don’t enable it and don’t support it in the FW.Thanks & Regards,
Nvidia SupportPowered by Discourse, best viewed with JavaScript enabled"
1089,cant-turn-on-infiniband-mode-connectx-5-solved,"Hi there!
I cant turn on my card into InfiniBand mode
Card is found by lspci as
a1:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]
Subsystem: Mellanox Technologies Device 0007
Flags: bus master, fast devsel, latency 0, IRQ 93, NUMA node 1
Memory at 71c7e000000 (64-bit, prefetchable) [size=32M]
Expansion ROM at 9e300000 [disabled] [size=1M]
Capabilities: [60] Express Endpoint, MSI 00
Capabilities: [48] Vital Product Data
Capabilities: [9c] MSI-X: Enable+ Count=64 Masked-
Capabilities: [c0] Vendor Specific Information: Len=18 <?>
Capabilities: [40] Power Management version 3
Capabilities: [100] Advanced Error Reporting
Capabilities: [150] Alternative Routing-ID Interpretation (ARI)
Capabilities: [1c0] #19
Capabilities: [230] Access Control Services
Kernel driver in use: mlx5_core
Kernel modules: mlx5_coreibv_devinfo says:
$ ibv_devinfo
hca_id:	mlx5_0
transport:			InfiniBand (0)
fw_ver:				16.35.2000
node_guid:			1070:fd03:005e:42c0
sys_image_guid:			1070:fd03:005e:42c0
vendor_id:			0x02c9
vendor_part_id:			4119
hw_ver:				0x0
board_id:			MT_0000000012
phys_port_cnt:			1
Device ports:
port:	1
state:			PORT_ACTIVE (4)
max_mtu:		4096 (5)
active_mtu:		1024 (3)
sm_lid:			0
port_lid:		0
port_lmc:		0x00
link_layer:		EthernetWhen I try to turn on InfiniBand mode, nothing happend:
$ mlxconfig -d  /dev/mst/mt4119_pciconf0 set LINK_TYPE_P1=1Device type:    ConnectX5
Name:           MCX516A-CCA_Ax
Description:    ConnectX-5 EN network interface card; 100GbE dual-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6
Device:         /dev/mst/mt4119_pciconf0Configurations:                              Next Boot       New
-E- The Device doesn’t support LINK_TYPE_P1 parameterHi,The card you have is an ETH only card and not a VPI card (IB/ETH)Thank for so sad answer.So, I can’t connect it with InfiniBand switch? Only with other such card? And could you say what is its bandwidth?It is a 100Gb Ethernet card.It cannot be used in an infiniband setup.Powered by Discourse, best viewed with JavaScript enabled"
1090,mellanox-connectx-3-pro-eth-driver-mlnx-ofed-linux-5-0-2-1-8-0-ubuntu20-04-x86-64-does-not-allow-openstack-victoria-vm-dhcp-after-install-the-driver,"Hi,We have 4 hypervisors and I am trying to configure SRIOV in a specific hypervisor, the only compatible driver that has worked is the MLNX_OFED_LINUX-5.0-2.1.8.0-ubuntu20.04-x86_64 for the Openstack Victoria version and the network card “ConnectX-3 Pro”. But after installing the driver in one of the hypervisors, the VMs do not get IP and it seems that there is no communication through the Mellanox network card.Any suggestions or ideas?Thanks a lot and regards.Solved, my bond miss configuration when driver was installed. regards.Powered by Discourse, best viewed with JavaScript enabled"
1091,dpdk-testpmd-fails-with-common-mlx5-failed-to-create-sq-using-devx,"[root@cu-testpmd build]# ./app/dpdk-testpmd -l $LCORES --main-lcore=$(echo $LCORES | sed -e ‘s/(.).*/\1/g’) -a $MLX_PCIEADDR,txq_inline_max=512 – --port-numa-config=0,0 --socket-num=0 --burst=64 --txd=256 --rxd=256 --mbcache=512 --rxq=32 --txq=32 --forward-mode=txonly -i --nb-cores=1 --txonly-multi-flow --mbuf-size=2176EAL: Detected 24 lcore(s)EAL: Detected 1 NUMA nodesEAL: Multi-process socket /var/run/dpdk/rte/mp_socketEAL: Selected IOVA mode ‘VA’EAL: No free hugepages reported in hugepages-2048kBEAL: No available hugepages reported in hugepages-2048kBEAL: Probing VFIO support…EAL: VFIO support initializedEAL: Probe PCI driver: mlx5_pci (15b3:101d) device: 0000:b5:00.0 (socket 0)common_mlx5: Failed to create SQ using DevXmlx5_pci: Can’t create sq object.mlx5_pci: FUNCIONS: mlx5_dev_spawn:1236EAL: mem_free Error: Invalid memorymlx5_pci: probe of PCI device 0000:b5:00.0 aborted after encountering an error: Operation not permittedcommon_mlx5: Failed to load driver = mlx5_pci.…[root@cu-testpmd build]# mstvpd b5:00.0ID: ConnectX-6 Dx EN adapter card, 100GbE, Dual-port QSFP56, with PPS In/Out, PCIe 4.0 x16, Crypto and Secure BootPN: MCX623106PC-CDATEC: A2V2: MCX623106PC-CDATSN: MT2022X19516V3: 8ae07ad3b8abea1180000c42a198c870VA: MLX:MN=MLNX:CSKU=V2:UUID=V3:PCI=V0:MODL=CX623106PV0: PCIeGen4 x16root@cu-testpmd build]# mstflint -d b5:00.0 qImage type: FS4FW Version: 22.31.1014FW Release Date: 30.6.2021Product Version: 22.31.1014Rom Info: type=UEFI version=14.24.13 cpu=AMD64,AARCH64type=PXE version=3.6.403 cpu=AMD64Description: UID GuidsNumberBase GUID: 0c42a1030098c870 4Base MAC: 0c42a198c870 4Image VSD: N/ADevice VSD: N/APSID: MT_0000000500Security Attributes: secure-fwHi,Unfortunately, question has no info about what is the DPDK version running.Be sure you are using lates 20.11.3 (LTS) and if the issue is still happens, test if it present in 21.05 and 21.08If you were able to run testpmd before, what is different between now and then?Hi, Thanks for the response. dpdk-testpmd works with 20.11.3 from dpdk-stable. Previous offending version was 20.11 and worked until firmware upgrade. One additional question on l2fwd-nv. The program fails with message with the message not being able to parse the device:./l2fwdnv -l $LCORES --main-lcore=$(echo $LCORES | sed -e ‘s/(.).*/\1/g’) -a $MLX_PCIEADDR,txq_inline_max=0 #-- -m 1 -g0 -w 3 -P 0 -b 64 -B 1 -c 1 -d 9216************ L2FWD-NV ************EAL: Detected 24 lcore(s)EAL: Detected 1 NUMA nodesEAL: Detected static linkage of DPDKEAL: failed to parse device “0000:b5:00.1”EAL: Unable to parse device ‘0000:b5:00.1,txq_inline_max=0’EAL: Error - exiting with code: 1Cause: Invalid EAL argumentsLet me know to submit a new case if need be.Glad to hear original issue solved.Regarding second question, create a new post with adding extended command line in such way that instead of “echo XXX| sed YYY” and MLX_PCIEADDR there will be a real content.Why there is an “#” on the command line?Thanks will open a new case.Powered by Discourse, best viewed with JavaScript enabled"
1092,hpe-branded-mellanox-hw-with-mellanox-passive-dac-cables,"I’ve got some Proliant servers with Mellanox NICs (P42044-B21 - Mellanox MCX631102AS-ADAT Ethernet 10/25Gb 2-port SFP28 Adapter for HPE) which I will be connecting to a (Q9E63A - SN2010M 25GbE 18SFP28 4QSFP28 Power to Connector Airflow Half Width Switch).  Both are Mellanox HW but HPE branded.Question: Can I use Mellanox DAC cables like the ones in MCP2M00-A series for servers to switch connection?Will these DAC cables be considered as third party or something worst like shutting down NICs or switch ports?Thanks!!!Powered by Discourse, best viewed with JavaScript enabled"
1093,newbie-just-want-to-confirm-parts-before-buying,"Hi, small SOHO so on a tight budget. This is the plan:Using for:Does this sound right? Specifically is the cable the right one?Next question will be howto configure properly once it all arrives.ThanksHi Willy,MCX354A-FCBT is ConnectX-3 VPI NIC, means it can work with Infiniband & Ethernet.Therefore, IPoIB is supported.For validated and supported cables, you can check the release notes of the latest firmware:899.35 KBPlease note that Cisco QSFP-H40G-ACU10M 40G QSFP was not tested this NIC+firmware.For supported OS versions and platforms, please refer to MLNX_OFED (driver) user manual:https://docs.mellanox.com/display/MLNXOFEDv492240/General+Support+in+MLNX_OFEDRegards,ChenThanks.How sensitive is the compatibility issue? The compatibility list has only 2 AOC cables (1), however there is a better priced option that is one of your own cables:Mellanox MC2206310-015 AOCAm I safe in ordering this?All these cables conform to standards so I would expect there to be very little if any incompatibility. Is there something unique about higher speed networking that makes standardization difficult?(1)Mellanox MC2210310-XXXQSFP-H40G-AOC15MPowered by Discourse, best viewed with JavaScript enabled"
1094,p4-programming-how-to-program-and-load,"Hi,
Where can i find information on P4 program loading and running ?  I could not find the same in DOCA manual. Am i missing anything ?Thanks,
RatheeshBlueFields do not support P4. You could use software P4 implementation on the ARM cores, probably.Is the P4 programming support available now ?
In another thread there was a mention that P4 support is targeted by May 2022 ( P4 programming).Powered by Discourse, best viewed with JavaScript enabled"
1095,installation-help-with-doca-1-1,"What is the appropriate setup to finish the mgmt port install?Have new Bluefield2 cards in a Dell R7525 running Ubuntu 20.04. Downloaded the DOCA util and installed everything till it got to the OOB management port part. I plugged in the ethernet port mgmt of the Bluefield 2 card into a network switch but no link.I ssh’ed to 192.168.100.2 as ubuntu and saw it had a static route to 192.168.100.1 yet I dont think the host machine is setup to NAT 192.168.100.1 through the box out to the internet.I have students who want to participate in the Hack-A-Thon and want to get them access to this hardware ASAP.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
1096,why-mellanox-sharp-didnt-improve-the-performance-when-running-tests-with-openmpi-and-deep-learning-distributed-training-benchmark,"By following the instruction from Mellanox documents, I have successfully install MOFED and Mellanox SHARP on 4 GPU servers. However, when I used OpenMPI to run osu_allreduce with and without Mellanox SHARP, the performance is the same. The below snippets is my command:​Without SHARP:$ mpirun -np 4 -H 192.168.67.228:1,192.168.67.229:1,192.168.67.230:1,192.168.67.231:1 --allow-run-as-root --bind-to core --map-by node -x LD_LIBRARY_PATH -mca coll_hcoll_enable 1 -x UCX_NET_DEVICES=mlx5_0:1 -mca pml ob1 -mca btl ^openib -mca btl_tcp_if_include ib0 -x HCOLL_MAIN_IB=mlx5_0:1 ~/hpcx-v2.8.0-gcc-MLNX_OFED_LINUX-5.2-1.0.4.0-redhat8.3-x86_64/ompi/tests/osu-micro-benchmarks-5.6.2/osu_allreduce -m 0:134217728​ # OSU MPI Allreduce Latency Test v5.6.2…1048576 1441.462097152 2478.804194304 4136.408388608 9056.9116777216 19946.7233554432 46106.7067108864 92746.40134217728 185052.29​With SHARP ​$ mpirun -np 4 -H 192.168.67.228:1,192.168.67.229:1,192.168.67.230:1,192.168.67.231:1 --allow-run-as-root -bind-to core --map-by node -x LD_LIBRARY_PATH -mca coll_hcoll_enable 1 -x UCX_NET_DEVICES=mlx5_0:1 -mca pml ob1 -mca btl ^openib -mca btl_tcp_if_include ib0 -x NCCL_DEBUG=INFO -x NCCL_SOCKET_IFNAME=ib0 -x HCOLL_MAIN_IB=mlx5_0:1 -x SHARP_COLL_ENABLE_SAT=1 -x SHARP_COLL_LOG_LEVEL=3 -x HCOLL_ENABLE_SHARP=3 ~/hpcx-v2.8.0-gcc-MLNX_OFED_LINUX-5.2-1.0.4.0-redhat8.3-x86_64/ompi/tests/osu-micro-benchmarks-5.6.2/osu_allreduce -m 0:134217728…1048576 1339.992097152 2293.314194304 4241.368388608 9110.4216777216 19284.4333554432 45360.2967108864 90132.80134217728 179329.17​Moreover, when I run distributed training over 4 GPU on RestNet50 Benchmark with and without SHARP. The similar situation happened:Without SHARP$ mpirun --allow-run-as-root --tag-output -np 4 -H 192.168.67.228:1,192.168.67.229:1,192.168.67.230:1,192.168.67.231:1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca btl_tcp_if_include ib0 -x NCCL_SOCKET_IFNAME=ib0 -x HPCX_SHARP_DIR -x LD_LIBRARY_PATH ~/miniconda3/envs/tuantd/bin/python ~/horovod_repo/examples/pytorch/pytorch_synthetic_benchmark.py[1,0]:Model: resnet50[1,0]:Batch size: 32[1,0]:Number of GPUs: 4[1,0]:Running warmup…[1,0]:Running benchmark…[1,0]:Img/sec per GPU: 242.9 ±1.7[1,0]:Total img/sec on 4 GPU(s): 971.7 ±6.7​​With SHARP​**$** mpirun --allow-run-as-root --tag-output -np 4 -H 192.168.67.228:1,192.168.67.229:1,192.168.67.230:1,192.168.67.231:1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca btl_tcp_if_include ib0 -x NCCL_SOCKET_IFNAME=ib0 -x ENABLE_SHARP_COLL=3 -x SHARP_COLL_ENABLE_SAT=3 -x NCCL_COLLNET_ENABLE=3 -x NCCL_ALGO=CollNet -x HPCX_SHARP_DIR -x LD_LIBRARY_PATH ~/miniconda3/envs/tuantd/bin/python ~/horovod_repo/examples/pytorch/pytorch_synthetic_benchmark.py[1,0]:Model: resnet50[1,0]:Batch size: 32[1,0]:Number of GPUs: 4[1,0]:Running warmup…[1,0]:Running benchmark…[1,0]:Img/sec per GPU: 237.0 ±21.2[1,0]:Total img/sec on 4 GPU(s): 948.1 ±84.8​Can you guys help me?​Best regards,Tuan​Hi Tuan ,First of all please install HPC-X 2.7 instead of 2.8 (we have known issue with this version)https://www.mellanox.com/products/hpc-x-toolkitSince you are looking for SAT (sharp for high BW) there is need to add specific flags to the CLIadd -x HCOLL_ALLREDUCE_HYBRID_LB=1 -x HCOLL_SHARP_NP=0 -x --SHARP_COLL_JOB_QUOTA_PAYLOAD_PER_OST=1024 -x SHARP_COLL_JOB_QUOTA_OSTS=32 -x SHARP_COLL_JOB_QUOTA_MAX_GROUPS=4 -x SHARP_COLL_LOG_LEVEL=3 -x HCOLL_BCOL_P2P_ALLREDUCE_SHARP_MAX=4100change -x HCOLL_ENABLE_SHARP=3 to -x HCOLL_ENABLE_SHARP=43 .remove -mca btl_tcp_if_include ib0Thanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
1097,how-to-ssh-from-cli,"hi team.​how to ssh from CLI promt… For example:​<> Ssh admin@10.10.10.10​​from promt just is enable Telnet, but ssh doesn’t appear…​Onyx 3.8.6​​Any suggestion??​Hello Daniel,Thank you for posting your inquiry on the NVIDIA Networking Community.You can use the following CLI command from the switch → ‘slogin’Example output:$ ssh -ladmin Mellanox MLNX-OS Switch ManagementPassword:Last login: Tue Feb 9 03:16:47 UTC 2021 from 172.27.6.37 on pts/0Number of total successful connections since last 1 days: 22Mellanox Switchswitch [standalone: master] > slogin -lroot root@host’s password:Last login: Tue Feb 9 01:13:37 2021 from 10.252.64.234[root@host ~]#Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1098,impact-of-the-network-on-storage,"Hi everyone, I wanted to get your thoughts (best practices, experiences, war stories) about the impact of the network (focusing on Ethernet) on your storage performance. What sort of storage array do you use? Block, file, or object? What are your experiences with deep buffer switches?Powered by Discourse, best viewed with JavaScript enabled"
1099,putting-ufm-on-a-compute-node-rather-than-on-a-standalone-server,"Hello there,Is there any prerequisite to put UFM on standalone servers?
I mean, is it possible to put UFM on compute nodes? Any pitfalls?Thanks!Hi,Please refer to UFM system requirements:https://docs.nvidia.com/networking/display/ufmenterprise69/Installation+Notes#InstallationNotes-SystemRequirementsGenerally it is advised to install it on a designated server and not a participant compute node.Thanks,DanThanks @dwaxman !Powered by Discourse, best viewed with JavaScript enabled"
1100,mlxndperf-exe-successfully-ran-on-one-computer-but-failed-on-the-second-computer,"2 computers are connected via fiber optics and can ping itself and other.
MlxNdPerf.exe cannot started as Server on one computer:  MlxNdPerf.exe error message below:ERROR: ndStartUp failed, Could not load provider, hr = 0xc00000bb
Any idea where I can look to resolve this problem?Looks like you not install ND provider.Run “ndinstall -l” to see a list of installed ND Providers:Current providers:
0000001001 - Hyper-V RAW
0000001006 - MSAFD Tcpip [TCP/IP]
0000001007 - MSAFD Tcpip [UDP/IP]
0000001008 - MSAFD Tcpip [RAW/IP]
0000001009 - MSAFD Tcpip [TCP/IPv6]
0000001010 - MSAFD Tcpip [UDP/IPv6]
0000001011 - MSAFD Tcpip [RAW/IPv6]
0000001016 - RSVP TCPv6 Service Provider
0000001017 - RSVP TCP Service Provider
0000001018 - RSVP UDPv6 Service Provider
0000001019 - RSVP UDP Service Provider
0000001055 - NDv1 Provider for Mellanox WinOF-2
0000001056 - NDv2 Provider for Mellanox WinOF-2Run “ndinstall -i” to install all available NVIDIA® ND Providers.Installing mlx5nd provider: successful
Installing mlx5nd2 provider: successfulCurrent providers:
0000001001 - Hyper-V RAW
0000001006 - MSAFD Tcpip [TCP/IP]
0000001007 - MSAFD Tcpip [UDP/IP]
0000001008 - MSAFD Tcpip [RAW/IP]
0000001009 - MSAFD Tcpip [TCP/IPv6]
0000001010 - MSAFD Tcpip [UDP/IPv6]
0000001011 - MSAFD Tcpip [RAW/IPv6]
0000001016 - RSVP TCPv6 Service Provider
0000001017 - RSVP TCP Service Provider
0000001018 - RSVP UDPv6 Service Provider
0000001019 - RSVP UDP Service Provider
0000001055 - NDv1 Provider for Mellanox WinOF-2
0000001056 - NDv2 Provider for Mellanox WinOF-2Thank you very much for your reply. I am using the Mellanox NIC on a different project with another system disk that does not have the same problem. I will reboot from the problem system disk to check and confirm your suggestion. My strong impression is that your suggestion will solve the problem based on the error code which I looked up from the NetDirect winDDK source code.Hi, I reboot from the ‘problem’ system disk. This is what I got:ndinstall -lCurrent providers:
0000001001 - MSAFD Tcpip [TCP/IP]
0000001002 - MSAFD Tcpip [UDP/IP]
0000001003 - MSAFD Tcpip [RAW/IP]
0000001004 - MSAFD Tcpip [TCP/IPv6]
0000001005 - MSAFD Tcpip [UDP/IPv6]
0000001006 - MSAFD Tcpip [RAW/IPv6]
0000001007 - AF_UNIX
0000001008 - RSVP TCPv6 Service Provider
0000001009 - RSVP TCP Service Provider
0000001010 - RSVP UDPv6 Service Provider
0000001011 - RSVP UDP Service Provider
0000001012 - Hyper-V RAW
0000001013 - MSAFD L2CAP [Bluetooth]
0000001014 - MSAFD RfComm [Bluetooth]
0000001015 - NDv1 Provider for Mellanox WinOF-2
0000001016 - NDv2 Provider for Mellanox WinOF-2I have 2 other Mellanox OFED working system disks that I can use right now. I can wait to solve this mystery later on.  Thanks for your assistance.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1101,connectx-3-debian-11-mlnx-ofed-4-9,"Hi,The following thread states that the Debian 11 MLNX_OFED 4.9 package would be ready in June, 2022.However, I’m unable to find it on LTS:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)Any idea of where the Debian 11 package can be found, or if not yet released, when it will be available?Thanks!Summoning @MvB , Can we get a status about Bullseye support for MLNX_OFED 4.9 LTS?@ypetrov Can You please give us an update in this topic?Unfortunately we have no updates about adding Debian 11 to the range of supported OS for MLNX_OFED 4.9. LTS.Now Bullseye is supported only in mofed 5.7 which doesn’t work with ConnectX-3.Thanks for replying @vkhomyakov. Could You please elaborate a bit more, is the MLNX_OFED_LTS support for Bullseye still planned as @MvB wrote in May, just delayed?Thanks for the update.As @laszlo.kondas mentioned, it would be extremely useful if you could let us know if there are still plans to make an MLNX_OFED v4.9 LTS for Bullseye (Debian 11).We have a cluster upgrade that can’t proceed due to this at the moment :(Waiting for the proper version of these aswell, been backporting by hand each update til recently where it stopped working…Hi,As others have mentioned, would it be possible to update us on whether Nvidia intends to release a v4.9 MLNX_OFED for Debian 11?
(i.e. is it just delayed or permanently off the development roadmap)We have infrastructure that requires updating to Debian 11 & cannot proceed until we have a definitive answer :(Thanks!also interested in the MLNX_OFED for debian 11 package…As Nvidia seems to no longer be supporting ConnectX-3 on a modern OS, does anybody have an idea how to switch from MLFX_OFED to inbox drivers, or possibly compile the drivers ourselves on Debian 11?I have to say that the situation Nvidia has left us in will heavily influence our future enterprise hardware choices.
Not happy.Thanks!I was able to compile with kernel 5.15 on my own distribution that I’m testing. I believe it will also work with Debian 11 and kenel 5.10.@siliba100 That’s good to hear.
Could you please let me know which documentation you followed?
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
1102,mlnx-ofed-kernel-modules-installation-for-additional-kernels-and-re-installation-fails-on-ubuntu-20-04-3,"First installation of MLNX_OFED_LINUX-5.5-1.0.3.2-ubuntu20.04-x86_64.iso on Ubuntu server 20.04.03 with GA kernel 5.4.0-91-generic succeeded (log in file: mlnxofedinstall_ok.log).Install for kernel 5.4.0-40 failed on the same system and then installation for kernel -94 failed again. Finally, reinstall for 5.4.0-91 failed in the exact manner. Now I cannot install driver, and can’t revert kernel with a working driver.What can be done to mitigate the problem (aside of system reinstall)?mlnxofedinstall.log (2.89 KB)MLNX_OFED_LINUX.471488.logs_mlnx-ofed-kernel-modules.debinstall.log (122 KB)MLNX_OFED_LINUX.471488.logs_general.log (46.6 KB)mlnxofedinstall_ok.log (5.92 KB)Hello,With the MLNX_OFED_LINUX-5.5-1.0.3.2-ubuntu20.04-x86_64.iso mounted, we were able to successfully install against Ubuntu 20.04.03 with all three of the mentioned kernels using the install options from your attached text files. Prior to attempting each install, we rebooted the system into the install target kernel and ran the uninstall.sh script contained within the iso image. The results for each:Installation passed successfullyTo load the new driver, run:/etc/init.d/openibd restartroot@ubuntu20:/mnt# uname -aLinux ubuntu20 5.4.0-91-generic #102-Ubuntu SMP Fri Nov 5 16:31:28 UTC 2021 x86_64 x86_64 x86_64 GNU/Linuxroot@ubuntu20:/mnt# /etc/init.d/openibd restartUnloading HCA driver: [ OK ]Loading HCA driver and Access Layer: [ OK ]root@ubuntu20:/mnt#root@ubuntu20:/mnt# ofed_info -sMLNX_OFED_LINUX-5.5-1.0.3.2:Installation passed successfullyTo load the new driver, run:/etc/init.d/openibd restartroot@ubuntu20:/mnt#root@ubuntu20:/mnt# uname -aLinux ubuntu20 5.4.0-40-generic #44-Ubuntu SMP Tue Jun 23 00:01:04 UTC 2020 x86_64 x86_64 x86_64 GNU/Linuxroot@ubuntu20:/mnt#root@ubuntu20:/mnt# /etc/init.d/openibd restartUnloading HCA driver: [ OK ]Loading HCA driver and Access Layer: [ OK ]root@ubuntu20:/mnt#root@ubuntu20:/mnt# ofed_info -sMLNX_OFED_LINUX-5.5-1.0.3.2:Installation passed successfullyTo load the new driver, run:/etc/init.d/openibd restartroot@ubuntu20:/mnt#root@ubuntu20:/mnt# uname -aLinux ubuntu20 5.4.0-94-generic #106-Ubuntu SMP Thu Jan 6 23:58:14 UTC 2022 x86_64 x86_64 x86_64 GNU/Linuxroot@ubuntu20:/mnt#root@ubuntu20:/mnt# /etc/init.d/openibd restartUnloading HCA driver: [ OK ]Loading HCA driver and Access Layer: [ OK ]root@ubuntu20:/mnt#root@ubuntu20:/mnt# ofed_info -sMLNX_OFED_LINUX-5.5-1.0.3.2:When attempting to run the installation script without running the uninstall.sh script against the previous installation first, the installation is similarly successful against the 5.4.0-91-generic kernel:Installation passed successfullyTo load the new driver, run:/etc/init.d/openibd restartroot@ubuntu20:/mnt#root@ubuntu20:/mnt# uname -aLinux ubuntu20 5.4.0-91-generic #102-Ubuntu SMP Fri Nov 5 16:31:28 UTC 2021 x86_64 x86_64 x86_64 GNU/Linuxroot@ubuntu20:/mnt#root@ubuntu20:/mnt# /etc/init.d/openibd restartUnloading HCA driver: [ OK ]Loading HCA driver and Access Layer: [ OK ]root@ubuntu20:/mnt#root@ubuntu20:/mnt# ofed_info -sMLNX_OFED_LINUX-5.5-1.0.3.2:Please attempt to run the included uninstall.sh script, reboot into the target kernel, and attempt the installation again. For more information on the installation process, please review the MLNX_OFED User Manual:https://docs.nvidia.com/networking/display/MLNXOFEDv551032/InstallationIf you are still running into errors with the installation or need assistance in debugging the issue and have a valid support contract, please open a support case. If you do not have a current/valid support contract, please reach out to the team at Networking-contracts@nvidia.com for assistance in obtaining a contract.Thank you,Nvidia Networking SupportPowered by Discourse, best viewed with JavaScript enabled"
1103,explanation-of-chroot-environment-for-ofed-on-debian-ubuntu-distros,"I was reading the install doc of OFED and noticed this for Debian: https://docs.mellanox.com/display/MLNXOFEDv531001/Installing+Mellanox+OFEDMLNX_OFED for Ubuntu should be installed with the following flags in chroot environment:./mlnxofedinstall --without-dkms --add-kernel-support --kernel  --without-fw-update --forceFor example:./mlnxofedinstall --without-dkms --add-kernel-support --kernel 3.13.0-85-generic --without-fw-update --forceWhy is there a need to chroot for OFED install?Could you provide detailed steps on chrooting for this install process to me, and update the documentation as well? Would I have to load a live ubuntu image, chroot on the installed root filesystem just to get OFED installed? That’s a lot of manual steps and makes automated bring up a difficult taskDetails:Running Ubuntu 20.04.1 with a 100GB Mellanox Technologies MT27700 Family [ConnectX-4]Hi Sam,That  DOESN’T mean need to do much.In the link which you send. There is an example of command:./mlnxofedinstall --without-dkms --add-kernel-support --kernel 3.13.0-85-generic --without-fw-update --forcewe can use ‘uname -a’ to get the kernel version number.Regards,LeveiThanks for clarifying that, Levei. I guess I got thrown off by the word chroot!Powered by Discourse, best viewed with JavaScript enabled"
1104,dpdk-programming-resources,"Please refer to DPDK’s official programmer’s guide for programming guidance as well as relevant BlueField platform and DPDK driver information on using DPDK with your DOCA application on BlueField-2I have had the chance to get my hands dirty with BF-2 DPUs deployed at Cloudlab. However, since everyone is now crazy about leasing resources at the Clemson site, I did not finish everything I wanted to do.
If you don’t want to jump from pointers to pointers, I am making a series of blog posts about the first impressions.https://cslev.medium.com/getting-your-hands-dirty-with-mellanox-bluefield-2-dpus-deployed-in-cloudlabs-clemson-facility-bcb4e689c7e6Before talking about DPDK programming resources, it would be great if we could run standard DPDK applications on the Bluefield without any error.
Just as usual, I was following an “official” blogpost about how to get OvS-DPDK working on the Bluefield, but of course it is not working out-of-the-box :)  sorry, sarcasm :DOnce the ports are added to OvS-DPDK, I got the error of “address family not supported by protocol”. I remember the “good”-old days when playing around with OvS-DPDK on servers with dumber Mellanox ConnectX-5, and I had similar problem due to the mlx5 driver. After compiling everything on my own, it was working back then…should I do the same here again?
I was so happy after upgrading the OS to the freshest one and saw all things are supplied by default, like ovs, doca, dpdk…
Anyone encountered the same issue before?The blog I mentioned above was lacking of the RDMA-related setting found in another official blog that suggested to enable RDMA after switching the mode of operation from SEPARATED_HOST mode to EMBEDDED/SMARTNIC mode.ThanksWhat software and versions are running on the Bluefield-2? The installation guide in the DOCA SDK documentation is the best resource to make sure dependencies are installed correctly on both the host and BF2 sides:NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.When using the DOCA BFB image (DOCA_v1.0_BlueField_OS_Ubuntu_20.04-5.3-1.0.0.0-3.6.0.11699-1-aarch64.bfb), the BF2 gets setup and preconfigured in Embedded mode with all of the representer ports created and added to ovs bridges.NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.Thanks, @jubetz , I used the official guide and I am running the latest firmware with DOCA and every other package (e.g., dpdk, ovs, spdk) pre-configured and installed.
Still, I have the issue.I tried running dpdk-testpmd app, which did not find a shared library.So, I rather removed all OvS and DPDK packages and started to reinstall DPDK from scratch on the Bluefield.
The new dpdk-testpmd at least runs now…let’s see if I send some packets to it, it works properly.On the other hand, I have been a heavy DPDK user for ages (mostly x86 and OvS-DPDK) but I have never seen and used this representor thing in the EAL options like this:
ovs-vsctl set Open_vSwitch . other_config:dpdk-extra=“-w <pcie_device_id>,representor=[0,65535]”
I only used the pcie_device_id to identify a port. Can you let me know what this representor means in this environment and whether it is necessary to be used?Can you try to add this environment variable, then try to run? See if that works?NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.export LD_LIBRARY_PATH=/opt/mellanox/dpdk/lib/aarch64-linux-gnu/Thanks! I tried. Now, testpmd works and can find the libraries.
I tried, however, to compile dpdk-pktgen from source.
It complains about not finding libdpdk and libmlx4, which is strange. I have installed the dev libraries for libdpdk, but of course I did not install any mlx4 driver.Have you encountered such an error so far?thanksPowered by Discourse, best viewed with JavaScript enabled"
1105,ib-write-bw-fails-at-sending-65536-bytes-when-connection-type-is-set-to-rc-by-default,"Hi all,
I am testing my cluster’s RDMA, but it seems that the""ib_write_bw -a"" command always fails when it tries to send 65536 or more bytes. I find that ib_write_bw can succeed if I set the connection type to UC. I have tried to use cable to connect my machine directly, but the problem remains the same. Can anyone help me trouble shoot?the log When I am using RC:#bytes #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]
Conflicting CPU frequency values detected: 1499.933000 != 2452.957000. CPU Frequency is not max.
2          5000             4.27               4.26   		   2.230961
Conflicting CPU frequency values detected: 1499.550000 != 2952.146000. CPU Frequency is not max.
4          5000             9.61               9.46   		   2.480504
Conflicting CPU frequency values detected: 1499.656000 != 2997.671000. CPU Frequency is not max.
8          5000             20.04              19.67  		   2.578615
Conflicting CPU frequency values detected: 1499.770000 != 1463.734000. CPU Frequency is not max.
16         5000             40.12              39.24  		   2.571379
Conflicting CPU frequency values detected: 1499.778000 != 2998.152000. CPU Frequency is not max.
32         5000             80.03              78.54  		   2.573588
Conflicting CPU frequency values detected: 1499.958000 != 2998.873000. CPU Frequency is not max.
64         5000             160.62             157.78 		   2.585021
Conflicting CPU frequency values detected: 1499.806000 != 2999.000000. CPU Frequency is not max.
128        5000             320.68             308.64 		   2.528346
Conflicting CPU frequency values detected: 1499.747000 != 2999.080000. CPU Frequency is not max.
256        5000             637.45             600.40 		   2.459247
Conflicting CPU frequency values detected: 1499.620000 != 2999.557000. CPU Frequency is not max.
512        5000             1262.81            1193.57		   2.444440
Conflicting CPU frequency values detected: 1499.639000 != 2999.262000. CPU Frequency is not max.
1024       5000             2516.95            2370.48		   2.427374
Conflicting CPU frequency values detected: 1499.607000 != 2999.490000. CPU Frequency is not max.
2048       5000             5126.39            4789.65		   2.452302
Conflicting CPU frequency values detected: 1499.728000 != 2999.404000. CPU Frequency is not max.
4096       5000             6460.28            6175.59		   1.580951
Conflicting CPU frequency values detected: 1499.713000 != 2999.890000. CPU Frequency is not max.
8192       5000             6528.70            117.22 		   0.015004
Conflicting CPU frequency values detected: 1499.861000 != 2999.505000. CPU Frequency is not max.
16384      5000             6573.54            6411.19		   0.410316
Conflicting CPU frequency values detected: 1499.650000 != 2999.793000. CPU Frequency is not max.
32768      5000             6605.99            6311.28		   0.201961
Conflicting CPU frequency values detected: 1499.738000 != 2999.748000. CPU Frequency is not max.
65536      5000             6610.41            4672.71		   0.074763
mlx5: ubuntu: got completion with error:
00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000
00000000 00008813 0800011e 38803bd3
Completion with error at client
Failed status 10: wr_id 0 syndrom 0x88
scnt=128, ccnt=0
Failed to complete run_iter_bw function successfullythe log When I am using UC:Hi user46,Thank you for contacting Nvidia community support.Could you please share the following information about your setup:Looking forward for your reply.Thank you and regards,Nvidia supportThanks for your reply,
Driver version is 525.60.11, CUDA version is 12.0. And my GPUs are A6000. The  network card is Mellanox Technologies MT27800 Family [ConnectX-5].
Perftest version is perftest-4.5-0.17, which I downloaded from this github link.
I am not sure what you are referring to by saying “firmware”, could you tell me how to check that?Hi User46,Thank you for the information provided.Firmware version can be pulled out with this command:
ethtool -i 
Get interface name  - by using the command “ifconfig” or “ip a”Looking forward for your reply.Nvidia supportHi ypetrov,
I use the ethtool command and get the following output:driver: mlx5_core
version: 5.0-0
firmware-version: 16.27.2008 (MT_0000000010)
expansion-rom-version:
bus-info: 0000:23:00.0
supports-statistics: yes
supports-test: yes
supports-eeprom-access: no
supports-register-dump: no
supports-priv-flags: yesIt shows the firmware version is 16.27.2008Hi user46,Based on the information you provided, you are using inbox driver.
Please update to the latest MOFED driver and latest firmware available for this adapter, according to the OS you are using, and then re-test your setup. You can download it from here:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)In case you still encounter any issues, you can contact enterprisesupport@nvidia.com and create a support case according to your entitlement.Thank you and regards,Nvidia supportHi ypetrov,
Thanks for your reply. I tried to follow your instruction and install the latest driver(the driver’s version is MLNX_OFED_LINUX-5.9-0.5.6.0-ubuntu18.04-x86_64). But the IB network interface goes down after the driver is installed. I cannot find the network interface through ifconfig command. This issue remains even I try to reboot for a couple of times.
Here is some ib-relevant command output I get after is driver is updated:command: ibstat
output:
CA ‘mlx5_0’
CA type: MT4119
Number of ports: 1
Firmware version: 16.27.2008
Hardware version: 0
Node GUID: 0x043f720300cbb392
System image GUID: 0x043f720300cbb392
Port 1:
State: Down
Physical state: Disabled
Rate: 100
Base lid: 0
LMC: 0
SM lid: 0
Capability mask: 0x00010000
Port GUID: 0x063f72fffecbb392
Link layer: Ethernetcommand: ibdev2netdev
output:
mlx5_0 port 1 ==> enp35s0np0 (Down)command: ibv_devinfo
output:
hca_id:	mlx5_0
transport:			InfiniBand (0)
fw_ver:				16.27.2008
node_guid:			043f:7203:00cb:b392
sys_image_guid:			043f:7203:00cb:b392
vendor_id:			0x02c9
vendor_part_id:			4119
hw_ver:				0x0
board_id:			MT_0000000010
phys_port_cnt:			1
port:	1
state:			PORT_DOWN (1)
max_mtu:		4096 (5)
active_mtu:		1024 (3)
sm_lid:			0
port_lid:		0
port_lmc:		0x00
link_layer:		EthernetIt looks like the ib port is down, I am wondering how I can make the port up.Powered by Discourse, best viewed with JavaScript enabled"
1106,mt27710-connectx-4-lx-connection-troubleshoot,"Background:MB: Supermicro X10DRiSW: 2x Supermicro SSE-F3548R (MLAG - UP and running)Cables: 1x Mellanox MCP2M00-A003E30L and 1x FS S28-PC025OS: Ubuntu 18.04Problem:For my perspective everything seems to be fine and all interfaces are UP but no connection.Host have also other separate 1G port (connected on separate SW) which is alright…after netplan apply bond and everything seems to be UP…pinging 25GB network card but it seems somehow throught 1G port becasue i can’t ping from 25GB SW.After reboot still everything is UP but no connection either to separate 1G port :DWhat could be wrong ?/////////////////////////////////////lspci | grep -i Mellanox81:00.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]81:00.1 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]/////////////////////////////////////mst status:MST modules:MST PCI module is not loadedMST PCI configuration module loadedMST devices:/dev/mst/mt4117_pciconf0 - PCI configuration cycles access.domain:bus:dev.fn=0000:81:00.0 addr.reg=88 data.reg=92 cr_bar.gw_offset=-1Chip revision is: 00/////////////////////////////////////ibstat:CA ‘mlx5_bond_0’CA type: MT4117Number of ports: 1Firmware version: 14.29.2002Hardware version: 0Node GUID: 0x0c42a10300ba8d6eSystem image GUID: 0x0c42a10300ba8d6ePort 1:State: ActivePhysical state: LinkUpRate: 25Base lid: 0LMC: 0SM lid: 0Capability mask: 0x04010000Port GUID: 0xf06504fffe689d2dLink layer: Ethernet/////////////////////////////////////lshw -class network*-network:0description: Ethernet interfaceproduct: MT27710 Family [ConnectX-4 Lx]vendor: Mellanox Technologiesphysical id: 0bus info: pci@0000:81:00.0logical name: ens5f0version: 00serial: f2:65:04:68:9d:2dwidth: 64 bitsclock: 33MHzcapabilities: pciexpress vpd msix pm bus_master cap_list rom ethernet physical autonegotiationconfiguration: autonegotiation=on broadcast=yes driver=mlx5_core driverversion=5.0-0 duplex=full firmware=14.29.2002 (MT_2420110034) latency=0 link=yes multicast=yes slave=yesresources: irq:24 memory:c8000000-c9ffffff memory:fbe00000-fbefffff memory:cc000000-cc7fffff*-network:1description: Ethernet interfaceproduct: MT27710 Family [ConnectX-4 Lx]vendor: Mellanox Technologiesphysical id: 0.1bus info: pci@0000:81:00.1logical name: ens5f1version: 00serial: f2:65:04:68:9d:2dwidth: 64 bitsclock: 33MHzcapabilities: pciexpress vpd msix pm bus_master cap_list rom ethernet physical autonegotiationconfiguration: autonegotiation=on broadcast=yes driver=mlx5_core driverversion=5.0-0 duplex=full firmware=14.29.2002 (MT_2420110034) latency=0 link=yes multicast=yes slave=yesresources: irq:111 memory:ca000000-cbffffff memory:fbd00000-fbdfffff memory:cc800000-ccffffff*-networkdescription: Ethernet interfacephysical id: 3logical name: bond-lanserial: f2:65:04:68:9d:2dcapabilities: ethernet physicalconfiguration: autonegotiation=off broadcast=yes driver=bonding driverversion=3.7.1 duplex=full firmware=2 ip=10.20.32.199 link=yes master=yes multicast=yes/////////////////////////////////////ip a:4: ens5f0: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond-lan state UP group default qlen 1000link/ether f2:65:04:68:9d:2d brd ff:ff:ff:ff:ff:ff5: ens5f1: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond-lan state UP group default qlen 1000link/ether f2:65:04:68:9d:2d brd ff:ff:ff:ff:ff:ff6: bond-lan: <BROADCAST,MULTICAST,MASTER,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000link/ether f2:65:04:68:9d:2d brd ff:ff:ff:ff:ff:ffinet 10.20.32.199/24 brd 10.20.32.255 scope global bond-lanvalid_lft forever preferred_lft foreverinet6 fe80::f065:4ff:fe68:9d2d/64 scope link tentativevalid_lft forever preferred_lft forever/////////////////////////////////////SW ports are in 25GB, full-duplexSMIS# sh interface port-channel 3po3 up, line protocol is up (connected)Bridge Port Type: Customer Bridge PortHardware Address is 0c:c4:7a:2c:d0:c5MTU 1500 bytes,LACP Port Admin Oper Port PortPort State Priority Key Key Number StateFx0/3 Bundle 128 3 3 0x3 0x3dReception CountersOctets : 890Unicast Packets : 0Unicast Packets Rate : 0/SecBroadcast Packets : 0Broadcast Packets Rate : 0/SecMulticast Packets : 9Multicast Packets Rate : 0/SecPause Frames : 0Undersize Frames : 0Oversize Frames : 0CRC Error Frames : 0Discarded Packets : 0Error Packets : 0Unknown Protocol : 0Received Rate : 0 bpsTransmission CountersOctets : 635Unicast Packets : 0Unicast Packets Rate : 0/SecBroadcast Packets : 0Broadcast Packets Rate : 0/SecMulticast Packets : 5Multicast Packets Rate : 0/SecPause Frames : 0Discarded Packets : 0Error Packets : 0Transmit Rate : 492 bpsHello Simon,Thank you for posting your inquiry to the Mellanox community.Unfortunately, the FS S28-PC025 is not a supported cable for the ConnectX-4 LX:https://docs.mellanox.com/display/ConnectX4LxFirmwarev14292002/Firmware+Compatible+ProductsBest regards,Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1107,database-maintenance-and-backup,"I am using bacula to get backups of our head node, and the nightly incremental backups are quite large, over 30 GB, so I was looking into what comprises so much changed data everyday.I found mysql-bin files in /var/lib/mysql going back to the beginning about 2.5 years ago… actually that directory only comprises 1.5 GB so it’s not that significant but I didn’t even realize the system was using MySQL.  Either way, I just wanted some feedback on proper maintenance and backup of the databases… I don’t know much in that area, but I do know that taking a straight backup of /var/lib/mysql is improper.In fact, bacula comes with a mysqldump script that is supposed to be run before the backup starts, then the dumped data files are backed up and I believe the binary data gets pruned or flushed as well.  I’ve included it below.There looks to be additional databases in various formats in /var/lib/mysql/cmdaemon/, /var/lib/mysql/mysql/, /var/lib/mysql/performance_schema/, and /var/lib/mysql/slurm_acct_db/ as well.Would using this script still be recommended?  I think some of the options may need to be adjusted and maybe add a few dump commands for the differing databases.  I checked the admin manual and the database setup seems rather complex but it didn’t really mention anything about maintenance/pruning and backups, except for the daily cmd backups that are put in /var/spool/cmd/backup/ which I’m of course already backing up.ThanksPowered by Discourse, best viewed with JavaScript enabled"
1108,will-connectx-5-and-above-support-psm4-or-psm8-transcievers,"Trying to do some research on this topic wondering if there is any roadmap or design plans for Long Range SMF support.Thanks.HiYes, CX5 support PSM4.
Please refer to Knoweldge Ariticle.
https://enterprise-support.nvidia.com/s/article/introduction-to-100g-psm4-transceiverFor PSM8, please contact your regional NVIDIA Sales person or Solution Architect engineer./HyungKwangPowered by Discourse, best viewed with JavaScript enabled"
1109,can-doca-work-fine-with-suse-linux,"Inherited a system that has SUSE Linux as the CPU OS and am trying to set up DOCA for a Bluefield that sits in the same platform.Would we expect to have any difficulties in using the BLuefield in either standard or BLuefield-x mode with such an OS on the host?admin@localhost:~> cat /etc/os-release
NAME=“SLES”
VERSION=“15-SP3”
VERSION_ID=“15.3”
PRETTY_NAME=“SUSE Linux Enterprise Server 15 SP3”
ID=“sles”
ID_LIKE=“suse”
ANSI_COLOR=“0;32”
CPE_NAME=“cpe:/o:suse:sles:15:sp3”
DOCUMENTATION_URL=“https://documentation.suse.com/”admin@localhost:~> uname -r
5.3.18-57-defaultThe operating system supported on the BlueField DPU is Ubuntu 20.04.The following operating systems are supported on the host machine:
Ubuntu 18.04/20.04/22.04
CentOS/RHEL 7.6/8.0/8.2
Rocky 8.6
Debian 10.8FYI,NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1110,40ge-sr4-channelized-to-4x10,"Hi,Is it possible to channelize a ConnectX-4 40GE SR4 into a 4x10. We would like the card to be used as an aggregation point for individual 10GE connections using a break out cable.Thanks!JoeHello Joseph,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately, the split functionality is only applicable for our switches. Our adapters do not have that functionality due to the design.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1111,bluefield-2-dpus-rdma-performance-is-weaker-than-connectx4,"Our network device is a InfiniBand BlueField-2 DPU, equipped with 2 100Gbps network ports and 1 ConnectX6 NIC. And our DPUs are all in Seperated Host Mode.Recently I found that our DPUs are not fully performing as well as CX6 NICs for RDMA networks, and are even weaker than CX4 NICs. To show this, I use perftest to gather statistic. Notice our host lock CPU frequency to a fixed value. There is no other difference except network device.My command is ib_read_bw -s 64 --duration 5 --run_infinitely -q 6 -l 512 -t 512. I choose a CX4 pair, a CX6 pair, and a DPU pair to show the gap:Pair | Throughput
cx4 | 2180M/sec
cx6 | 2450M/sec
dpu | 1904M/secThe results are shocking to me, I am wondering why.
My personal guess is that CX6 on DPU forcibly pre-allocate some resources to the embedded switch or the arm subsystem, making host cannot use full power of CX6. But I didn’t find evidence to confirm.Expecting someone to discuss with me, thanks.Powered by Discourse, best viewed with JavaScript enabled"
1112,i-have-some-problem-when-i-install-k8s-on-bcm,"I can’t install Kubernetes via bright view or cm-Kubernetes-setup
it have some error,error message :after I skip process it can install until progress : 80 #### stage: kubernetes: Wait Until Ingress Controller Readyroot@bright92:~# kubectl get pods --all-namespaces
NAMESPACE       NAME                                         READY   STATUS              RESTARTS   AGE
ingress-nginx   ingress-nginx-admission-create-1-5-1-lc6kp   0/1     Pending             0          17m
ingress-nginx   ingress-nginx-admission-patch-8sqzj          0/1     Pending             0          17m
ingress-nginx   ingress-nginx-controller-6db97f6465-qmh9b    0/1     ContainerCreating   0          17m
kube-system     calico-kube-controllers-7bdbfc669-4gf6s      0/1     Pending             0          17m
kube-system     calico-node-5rzdw                            1/1     Running             0          17m
kube-system     coredns-7958c64b9d-czhhk                     0/1     Pending             0          17m
kube-system     coredns-7958c64b9d-qbm29                     1/1     Running             0          17mHow to fix this problem ?Powered by Discourse, best viewed with JavaScript enabled"
1113,port-mirroring-in-sn2700,"I need to duplicate data stream (RDMA) out of selected ports on SN2700 and route to a destination server or storage device. The destination server is connected to a different network built with SN2700 switch.
I want to confirm if the “port mirroring” feature of the SN2700 switch can accomplish this task.The documentation says “port mirroring” is intended to monitor data traffic for analysis, etc. I am trying to use it to do multicast. It implies data coming out from the mirrored port is not same as the original data.Thanks.openflow can more suitable here.
But I suggest you can also advice with your Nvidia Networking SE to better understand the design and needs.Thanks. I am not familiar with openflow, so wanted some clarifications.
In case of RDMA unicast or single destination it establishes queue pair and ack/nak protocols.
Now consider the case of multicast when a source send data to multiple destinations (traversing through multiple switches). Does the source receive ack/nak from all destinations? Does it need to know or care if there are multiple destinations (using openflow feature)?
It would be very helpful if you could point me to an example.Powered by Discourse, best viewed with JavaScript enabled"
1114,linux-rdma-perftest-ib-read-bw-failure-with-use-cuda-option,"I’d like to measure GPU-GPU ib_read / write bandwidth with varying combination of our PCIe topology within a single node.But, --use_cuda option doesn’t work properly.Could you give me help for going further step to measure bandwidth?Software Infoib_read_bwServer$ ./ib_read_bw -d mlx5_1 --use_cuda=0 -p 50001initializing CUDAListing all CUDA devices in system:RDMA_Read BW TestDual-port : OFF Device : mlx5_1Number of qps : 1 Transport type : IBConnection type : RC Using SRQ : OFFPCIe relax order: ONibv_wr* API : ONCQ Moderation : 1Mtu : 4096[B]Link type : IBOutstand reads : 16rdma_cm QPs : OFFData ex. method : Ethernetlocal address: LID 0x04 QPN 0x1854 PSN 0xd2b3ee OUT 0x10 RKey 0x00277e VAddr 0x007faec3210000remote address: LID 0x03 QPN 0x1000 PSN 0xdfe56e OUT 0x10 RKey 0x002467 VAddr 0x007f5643210000#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]ethernet_read_keys: Couldn’t read remote addressUnable to read to socket/rdma_cmFailed to exchange data between server and clientsClient$ ./ib_read_bw 127.0.0.1 -d mlx5_2 --use_cuda=7 -p 50001initializing CUDAListing all CUDA devices in system:RDMA_Read BW TestDual-port : OFF Device : mlx5_2Number of qps : 1 Transport type : IBConnection type : RC Using SRQ : OFFPCIe relax order: ONibv_wr* API : ONTX depth : 128CQ Moderation : 1Mtu : 4096[B]Link type : IBOutstand reads : 16rdma_cm QPs : OFFData ex. method : Ethernetlocal address: LID 0x03 QPN 0x1000 PSN 0xdfe56e OUT 0x10 RKey 0x002467 VAddr 0x007f5643210000remote address: LID 0x04 QPN 0x1854 PSN 0xd2b3ee OUT 0x10 RKey 0x00277e VAddr 0x007faec3210000#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]mlx5: ai004: got completion with error:00000000 00000000 00000000 0000000000000000 00000000 00000000 0000000000000000 00000000 00000000 0000000000000000 00008914 10001000 0000b0d2Completion with error at clientFailed status 11: wr_id 0 syndrom 0x89scnt=128, ccnt=0Failed to complete run_iter_bw function successfully$ nvidia-smi topo -mGPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 mlx5_0 mlx5_1 mlx5_2 mlx5_3 mlx5_4 mlx5_5 CPU Affinity NUMA AffinityGPU0 X NV12 NV12 NV12 NV12 NV12 NV12 NV12 SYS PXB SYS SYS SYS SYS 48-63,176-191 3GPU1 NV12 X NV12 NV12 NV12 NV12 NV12 NV12 SYS PXB SYS SYS SYS SYS 48-63,176-191 3GPU2 NV12 NV12 X NV12 NV12 NV12 NV12 NV12 PXB SYS SYS SYS SYS SYS 16-31,144-159 1GPU3 NV12 NV12 NV12 X NV12 NV12 NV12 NV12 PXB SYS SYS SYS SYS SYS 16-31,144-159 1GPU4 NV12 NV12 NV12 NV12 X NV12 NV12 NV12 SYS SYS SYS SYS SYS PXB 112-127,240-255 7GPU5 NV12 NV12 NV12 NV12 NV12 X NV12 NV12 SYS SYS SYS SYS SYS PXB 112-127,240-255 7GPU6 NV12 NV12 NV12 NV12 NV12 NV12 X NV12 SYS SYS PXB SYS SYS SYS 80-95,208-223 5GPU7 NV12 NV12 NV12 NV12 NV12 NV12 NV12 X SYS SYS PXB SYS SYS SYS 80-95,208-223 5mlx5_0 SYS SYS PXB PXB SYS SYS SYS SYS X SYS SYS SYS SYS SYSmlx5_1 PXB PXB SYS SYS SYS SYS SYS SYS SYS X SYS SYS SYS SYSmlx5_2 SYS SYS SYS SYS SYS SYS PXB PXB SYS SYS X SYS SYS SYSmlx5_3 SYS SYS SYS SYS SYS SYS SYS SYS SYS SYS SYS X PIX SYSmlx5_4 SYS SYS SYS SYS SYS SYS SYS SYS SYS SYS SYS PIX X SYSmlx5_5 SYS SYS SYS SYS PXB PXB SYS SYS SYS SYS SYS SYS SYS XPowered by Discourse, best viewed with JavaScript enabled"
1115,sb7800-ssd-failure-steps-to-recover,"Good day,
We run four SB7800 switches, and have recently had a drive failure.  Does anyone know the procedure to recover?  I already have a replacement drive, and I’ve currently entered the console via serial port.Need to open a support ticket for handling.
It may require an RMA.This unit is beyond its warranty, hence my post instead of contacting supportCan you please post the SN of the unit?SN is MT164100078This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1116,af-xdp-with-xdp-pass-results-in-garbed-frames,"System: ubuntu 22.04 LTS
Driver: MLNX_EN 5.9-5.9.0.5.6
FW: 16.35.2000I have a chain of XDP programs, which tail call each other, and the last one in chain can either return with XDP_PASS or redirect via xskmap. When XDP_PASS return is used, this works just fine: frames are processed by BPF, they are valid and contain valid frame data - this is confirmed, and in the end are passed to OS and can be examined with tcpdump on the interface. The problem arises when AF_XDP socket is bound. If XDP program redirects to the socket, AF_XDP program receives frame, and the frame is valid and it does contain expected data - this is also confirmed. But if XDP program returns XDP_PASS, the frame comes out from netdev, and its size matches original frame, but the contents are garbed - there is no trace of original data in these frames, it is either all zeroes, or some random data. AF_XDP processor shows zero processed frames as expected. If the AF_XDP processor is terminated, frames coming out of netdev become valid as expected. This is both in zerocopy and copy AF_XDP bind modes if XDP program is attached to interface in DRV mode. If XDP program is attached to interface in SKB mode, the problem goes away in both AF_XDP bind modes. The problem is also absent when whole setup is replicated on virtio interfacesHi dairininXDP is a pure software solution which provided by your OS vendor.
From your description I didn’t see any issue related to Nvidia products.
Please contact with your OS vendor for further help.Thank you
Meng, ShiHello,Sorry for bringing up the topic, but I have additional details.
I have switched to the in-kernel mlx5_core module and rerun same tests. No garbed packets. Once I come with this to my OS vendor, they will instantly send me back here, because the only difference between working and non-working cases is MLNX_EN, which is not part of OS distribution. The kernel is 5.15.85. Also forgot to mention in the original message, this is mellanox connectix 5:BTW, I have noticed, that garbed frames do not always contain zeroes. There where couple of frames carrying parts of a system log from systemd’s journal (or maybe an application which originally supplied this log). The UMEM buffer comes from mmap, so it is supposed to be zeroed and there is no way it should be able to get such a data even if I do something wrong with AF_XDP part.Hello,Sorry for bringing up closed topic, but I have additional details.
This is mellanox connectix 5 card:I have switched to in_kernel mlx5_core module and rerun same tests. No garbed packets. Once I come with this to my OS vendor, they will instantly send me back here, because the only difference between working and non-working cases in MLNX_EN, which is not part of OS distribution. Kernel is 5.15.85Powered by Discourse, best viewed with JavaScript enabled"
1117,connectx6-dx-100g-configuration-required-to-be-used-as-4x25g,"Hello community!
Hopefully someone here can help me with my question. I am basically looking for the configuration required on the  ConnectX6-DX NIC to change it from 1x100G Ethernet to 4x25G Ethernet. I have found information on how to do this on the switch here, but I was wondering how to do this on the server side NIC instead (if possible!).Thanks!NIC port cannot be physically splitted.
.Thanks for your reply @dwaxman . Is there another NIC model from Nvidia/Mellanox that supports this feature? I know for a fact that Emulex/Broadcom have this kind of feature (break-out cable support) in some of their NICs because I used in the past.Can you confirm @dwaxman, that the ConnectX cards do not support (eg, 4x25G) breakout cables?Hi,The NICs do not support splitting physical lanes (unlike switch ports)I’m not familiar with a usage scenario that requires a NIC port physically splitted to 4 ports.Just for the thread completeness…
As advised in this thread, Mellanox NICs do not support it so we ended up using an Intel one that supports this:

Source: Ethernet Port Configuration Tool Infographic – IntelPowered by Discourse, best viewed with JavaScript enabled"
1118,set-vlan-prio-on-rocev1-on-connectx-3-pro-on-el7-inbox-driver,"I’ve got vlans and prios working with egress-qos-map, but what I can’t achieve is telling mlx4 driver to set a vlan prio for RoCE traffic. I have configured a mirror port on the switch and I’m seeing roce traffic being sent over proper vlan, but it has prio 0, not 3 like in egress-qos-map. Is there ANY way to get it working on inbox driver? I just don’t want to install whole MLNX_OFED thing only to get 1 tiny (but important) option.05:57:31.971188 00:02:c9:3e:48:b0 > 9c:dc:71:58:5c:21, ethertype 802.1Q (0x8100), length 338: vlan 300, p 0, ethertype 0x89151 tiny thing till RoCE is working over here!Thanks.Actually… I got it working by… doing nothing. It’s very important NOT to follow mellanox guides and NOT to use tc_wrap.py script on inbox/bare install. It should be used only with MLNX_OFED install.Hello,As you’d deduced, the instructions/functions discussed in the MLNX_OFED user manual are designed with the official MLNX_OFED product in mind, and will likely not be interoperable with the inbox driver as they are separate offerings from different entities.If you need further assistance with the inbox driver’s functions, support and documentation will be provided directly by the OS vendor as we do not maintain or control the code.-Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
1119,mellanox-connect-4x-only-using-one-queue,"Hi,I’m currently having a issue with my connect 4x card. It seems to only be using one queue, I tried to change the queue number with ethtool but it already is at 12​ This is the model :​ConnectX-4 EDR +100GbEPart Number: MCX456A ECAT​Do anyone have an idea on why is it only using one queue ? I can only reach something like 10gbps on my 100G link and only 1MPPS​​Yours thankfully​​Hello,Please ensure that you have installed the latest firmware for the adapter, the latest MLNX_OFED driver compatible with the firmware.The number of channels/queues available to configure will depend on the number of CPU cores in the system and the capabilities of the adapter itself.When working with the ConnectX-4 adapters, there is also a specific syntax for enabling channels that will need to be used. For more information, please see “ethtool” section of the MLNX_OFED user manual, in particular:https://docs.mellanox.com/display/MLNXOFEDv543030/Ethtoolethtool -L eth [rx ] [tx ]Sets the number of channels.Notes:For ConnectX-4 cards, use ethtool -L eth combined  to set both RX and TX channels.ConnectX-4 Firmware Download:Updating Firmware for ConnectX®-4 VPI PCI Express Adapter Cards (InfiniBand, Ethernet, VPI)ConnectX-4 Firmware v12.28.2006 Release Notes:https://docs.mellanox.com/display/ConnectX4Firmwarev12282006MLNX_OFED Download:Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)MLNX_OFED v5.4-3.0.3.0 User Manual:https://docs.mellanox.com/display/MLNXOFEDv543030/NVIDIA+MLNX_OFED+Documentation+Rev+5.4-3.0.3.0For performance concerns, please review the performance tuning guide and our community article regarding the mlnx_tune tool.Performange Tuning For Mellanox Adapters:https://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersHow to Tune Your Linux Server for Best Performance Using the mlnx_tune Tool:https://community.mellanox.com/s/article/How-to-Tune-Your-Linux-Server-for-Best-Performance-Using-the-mlnx-tune-ToolIf after reviewing the documentation above you still need assistance in debugging the issue, please open a support case on the matter. If you do not have a current support contract, please email the team at Networking-contracts@nvidia.com to set a valid support contractThank you,-Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
1120,self-paced-doca-flow-dli-course-available-now,"Build your own DOCA Flow Application!
https://courses.nvidia.com/courses/course-v1:DLI+S-NP-02+V1/NVIDIA® DOCA™ is the key to unlocking the potential of the NVIDIA BlueField® data processing unit (DPU) to offload, accelerate, and isolate data center workloads. With DOCA, developers can program the data center infrastructure of tomorrow by creating software-defined, cloud-native, DPU-accelerated services with zero-trust protection to address the increasing performance and security demands of modern data centers.DOCA Flow is the most fundamental API for building generic execution pipes in hardware. The library provides an API for building a set of pipes, where each pipe consists of match criteria, monitoring, and a set of actions. Pipes can be chained so that after a pipe-defined action is executed, the packet may proceed to another pipe.In this course you will be introduced to DOCA Flow programming by building an “ARP Storm Control” application which prevents network failures caused by broadcast storms through the creation of a DOCA Flow pipeline that can dampen malicious broadcast network activity without impacting well-behaved traffic.Powered by Discourse, best viewed with JavaScript enabled"
1121,need-help-with-xdma-stream-for-nvidia-innova-2-flex,"I successfully wrote data to XDMA Stream on an NVIDIA Innova-2 Flex card, but it took very long and return a -1 error. Can anyone help. This is the info in the dmesg:This is not enough information. Your issue is likely the result of the interplay between FPGA IP, driver, and software. sudo dmesg | grep -i xdma, the core code in software you are using for communication, and a picture of your Block Design would make this easier.dma_ip_drivers uses the same dma_to/from_device tools for both Memory-Mapped and Stream IP. I suggest you make the minimal changes to the working demo project to enable AXI-Stream and attempt reading and writing to BRAM Memory using dma_streaming_test.sh.There is a default 10-second timeout for transactions. Something is blocking them. Either you are attempting to read/write to an incorrect address or the Block you are communicating with is still in Reset. Are you reading or writing to the correct read or write address using the correct read or write device? What are you attempting to communicate with and what is its reset connected to?Check out Answer Records  65062 - PCI Express Address Mapping and  71435 - XDMA Driver and IP Debug Guide. A very good resource for PCIe which is focused on 7-Series FPGAs is fpgaemu.I have just fixed the block. There is no error now, but it always returns 0 when I read the xdma0_c2h_0. Here is the block design. It is just a multiply function for float numbers:
Screenshot from 2023-07-25 06-35-20858×331 24.9 KBThis is not the best forum for general FPGA questions. Let’s take this to Xilinx Support.Powered by Discourse, best viewed with JavaScript enabled"
1122,sn2000-series-bios-update-and-mpfa,"Hi. I found two articles talking about BIOS upgrades for the SN2010, but I can’t seem to find any files to do the update.The SONiC documentation talks about: Mellanox Platform Firmware Archive.Where can I find this?https://docs.nvidia.com/networking-ethernet-software/sonic-202012/Getting-Started/Platform-Firmware-Components/#check-for-available-firmware-updatesHello,Please share the output of the following commands to see if an update is even required:fwutil show updates --image=nextshow platform firmware updatesshow platform firmware statusThanks,Christian MatlockHi, from a fairly new ONIE image I get:fwutil show updates --image=next/bin/sh: fwutil: not foundshow platform firmware updates/bin/sh: show: not foundONIE VERSION=“2020.11-5.3.0005-115200”Where would you like me to run these commands?Powered by Discourse, best viewed with JavaScript enabled"
1123,how-to-configure-reflective-relay-for-layer2-switching-for-mellanox-switch-sn2700,"Hi,I was looking for information about how to configure reflective relayfor layer2 switching for Mellanox switch SN2700.Using this feature we can enable reflective relay to turn back trafficout of the same port it came in on. We can enable reflective relay ona Layer 2 physical portThis is to support the VEPA feature.To clarify, this is the feature I was looking for:https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus9000/sw/7-x/layer2/configuration/guide/b_Cisco_Nexus_9000_Series_NX-OS_Layer_2_Switching_Configuration_Guide_7x/b_Cisco_Nexus_9000_Series_NX-OS_Layer_2_Switching_Configuration_Guide_7x_chapter_01011.pdfSo wanted to check if you can please clarify the following:If this feature is supported in the switch SN2700 ?If not, then which switch supports this featureIf it is supported, it would be great if you can share the commands to use.Thanks a lot!Best RegardsMarc SpencerHi Marc,Thank you for contacting NVIDIA Technical Global Support.Currently Onyx does not support Reflective Relay feature for layer 2 switching. If you need this feature please reach out to the NVIDIA account/sales team and they can help you with raising this Feature Request.Thanks,Nvidia Global Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1124,connectx-5-support-for-inner-outer-rss,"Hello,I am using a Mellanox ConnectX-5 Ex adapter with DPDK. My application uses GRE for tunneling and the GRE header encapsulates an inner IP and TCP header. Because of the encapsulation I was having an issue with the RSS functionality: different flows are being sent to the same receive queue because the NIC is only using the outer IP headers for hashing. I looked over some of the DPDK and mlx5 docs and from my understanding it should support tunnel hw offloads for inner/outer RSS.E.g. https://doc.dpdk.org/guides-21.11/nics/mlx5.htmlTunnel HW offloads: packet type, inner/outer RSS, IP and UDP checksum verification.When I try to run my application and set the DPDK rss_hf in rte_eth_conf to RTE_ETH_RSS_LEVEL_INNERMOST the device doesn’t seem to support it. The hash function is not set properly and when I run my application I receive everything in queue 0. I also tried to use test-pmd to test if things are working and when I doThe return message says: Port 0 modified RSS hash function based on hardware support,requested:0x8000000000000 configured:0 which indicates that the requested rss hash function was not configured. How can I get the inner TCP header rss offload to work? Documentation seems to suggest this is possible, but I can’t get it to work. This question has been asked a couple of other times in this forum, but it was never really answered.DPDK version: 21.11
mlx5_core version: 5.0-0
firmware: 16.25.4062
linux kernel version: 5.10Hi usermuser,Thank you for posting your query on NVIDIA Community.Based on the software stack details shared, you are using Inbox driver(that comes with OS) and not MLNX OFED driver. In such situations, OS vendor needs to be involved.mlx5_core version: 5.0-0In order for us to debug the issue, the issue needs to be reproduced when using MLNX OFED driver. The latest version can be downloaded from —> Linux InfiniBand DriversIn addition, if issue is reproduced with MLNX OFED, a support ticket needs to be opened. This will also require a valid support contract.
Support ticket can be opened by emailing Networking-support@nvidia.comFor contracts, please feel free to reach out to Networking-contracts@nvidia.comThanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
1125,help-ive-posted-and-send-request-and-it-wasnt-completed-with-a-corresponding-work-completion-what-happened,"hello ,I had a programming problem. The server side of the program will execute ibv_ post_ send, return success. Every time I run for about an hour, there will always be a post after waiting long enough, I still haven’t received the post’s send_ complete event. I would like to ask, is there any good way to debug, or possible causes of this problem?server side:dirver：MLNX_OFED_LINUX-4.3-1.0.1.0hardware: Mellanox Technologies MT27640 FamilyVersions: Current AvailableFW 16.26.1040 N/APXE 3.5.0803 N/AUEFI 14Regards,longfeiHi,Do you run ibv_post_send and ibv_poll_cq serially or in differents threads ?Did you check if you didn’t receive IBV_EVENT_CQ_ERR that it causes by a CQ overrun, in case your CQ cannot be used anymore .RegardsMarcThank you very much for your response。ibv_post_send and ibv_poll_cq in differents threads and not received IBV_EVENT_CQ_ERR or any other err events from CM and any async events​ from ibv_get_async_event。Regards,longfei​Hi,Marc, very thanks.​Can you provide a reference process for handling disconnect? To avoid mem-leak send post, my process is like this:call rdma_disconnect–>wait CM DISCONNEC_ EVENT–>send last_ Post identifies the last send_ post–>poll_cq threads get last_ post— >safely destroy qp/cm_ idI’m using SRQ, but I don’t deal with recv CQE at present. Any suggests?Powered by Discourse, best viewed with JavaScript enabled"
1126,quadro-rtx-6000-bar-1-size-for-rdma,"I want to do RDMA transfers on Nvidia Quadro RTX 6000 cards and I notice that the BAR 1 is only 256 MiB which is too little for our application case. So, I have looked around about this limitation. I found that we can potentially resize (or more technically speaking ‘rebar’) this BAR 1 for doing larger RDMA transfers. Our system is a Linux Ubuntu. For checking that the resizeable BAR is well enabled on the system, I patched nvidia.ko for printing the output of pci_rebar_get_possible_sizes, recompiled/reinstalled the nvidia.ko and it works. The function returns 0x1c0 which meems that the resizable bar feature is well enabled !.. but that the possible sizes are only 64, 128 and 256 MiB…Is there any way to get a larger BAR 1 on Quadro RTX 6000 for doing RDMA ?It seems that other GPUs (GeForce RTX) have larger BAR 1.Looking at p.8 of the datasheet NVIDIA-Quadro-RTX-6000-PCIe-Server-Card-PB-FINAL-1219.pdf, it seems BAR 1 can do 32GiB…Should enable/disable SR-IOV/ARI support in the bios ? Here is my configuration : bios configuration supermicroThe host is a SuperMicro SuperServer 4029GP-TRT+1 would like to know how to achieve bigger BAR sizes.Powered by Discourse, best viewed with JavaScript enabled"
1127,failed-to-install-mlnx-ofed-kernel-dkms-deb-with-version-4-9-4-1-7-0,"I am trying to install MLNX_OFED_LINUX-4.9-4.1.7.0-ubuntu20.04-x86_64 because I have a Connectx-3 SPF+ card however when it gets to mlnx-ofed-kernel-dkms it fails. Attached are the log files. I could use some help.make.log (9.91 KB)mlnx-ofed-kernel-dkms.debinstall.log (9.96 KB)Hello Trent,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, you are running kernel version 5.13 on Ubuntu 20.04.3 LTS, the issue is related due to the linux-header for this kernel not properly installed.I was able to reproduce this issue in our lab. It is a Ubuntu distro related issue.For installing the linux-headers, a more recent version is needed on the system. See output below.# dpkg -i linux-headers-5.13.0-28_5.13.0-28.31_all.deb linux-headers-5.13.0-28-generic_5.13.0-28.31_amd64.deb(Reading database … 131877 files and directories currently installed.)Preparing to unpack linux-headers-5.13.0-28_5.13.0-28.31_all.deb …Unpacking linux-headers-5.13.0-28 (5.13.0-28.31) over (5.13.0-28.31) …Preparing to unpack linux-headers-5.13.0-28-generic_5.13.0-28.31_amd64.deb …Unpacking linux-headers-5.13.0-28-generic (5.13.0-28.31) over (5.13.0-28.31~20.04.1) …Setting up linux-headers-5.13.0-28 (5.13.0-28.31) …dpkg: dependency problems prevent configuration of linux-headers-5.13.0-28-generic:linux-headers-5.13.0-28-generic depends on libc6 (>= 2.34); however:Version of libc6:amd64 on system is 2.31-0ubuntu9.2.dpkg: error processing package linux-headers-5.13.0-28-generic (–install):dependency problems - leaving unconfiguredErrors were encountered while processing:linux-headers-5.13.0-28-genericThis package is needed to rebuild the driver against DKMS.Currently, there is a bug open for this issue → https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1926938Our only recommendation is to run an older version of the kernel which does not have this dependency or update the Distro to 21.xx or higher.The OS-es and their kernel versions mentioned in the MLNX_OFED 4.9 RN are validated and tested → https://docs.nvidia.com/networking/display/MLNXOFEDv494170/General+Support+in+MLNX_OFED#GeneralSupportinMLNX_OFED-MLNX_OFEDSupportedOperatingSystemsUnfortunately Kernel 5.13 is not in this list.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1128,mellanox-cx5-cx6-dx-with-dpdk-rte-flow-dv-flow-en-1-performance-problem,"Hardware:DPDK version: 18.11/19.11/20.5/20.8/20.11OFED & mlx5 drv version: 4.6/4.7-3.2.9.0/4.7-3.2.9.0/5.0-2.1.8FW version: 16.27.2008Memory: 16G*8KERNEL verision: 4.14.13/5.4.56CPU: INTEL broadwell microarch 2696v4 core*22 NUMA * 2Test config:IXIA 100G, IXIA-> switch → DPDK testpmd;DPDK testpmd:app_opt=""-i --burst=32 --txd=1024 --rxd=1024 --mbcache=512 ""app_opt+="" --no-numa --enable-rx-cksum --mlockall ""app_opt+="" --rxq=16 --txq=16 --nb-cores=16 --coremask=0xffff0 ""app_opt+="" --pkt-filter-mode=perfect ""app_opt+="" --forward-mode icmpecho -a""dev_opt="" -w 54:00.0,rx_vec_en=1,dv_flow_en=1 ""./testpmd --log-level=8 -c 0xffffffff --socket-mem=5120,5120 -n 2 -r 2 $dev_opt – $app_opttestpmd> startDPDK rte_flowflow create 0 ingress pattern eth / ipv4 / udp / end actions count / drop / endIXIA send 99G 64bytes traffic, but flow only count 14G+ trafficmlx_fs_dump :FT: 0x40009 (level: 0x1, type: NIC_RX)±- FG: 0x20 (OUT_HDR: |ip_ver|cvlan_tag|ip_prot|)±- FTE: 0x0 (FWD FLOW_COUNT) to (TIR:0x57) ip_ver:0x4 ip_prot:UDPFT: 0x40008 (level: 0x20, type: NIC_RX)|-- FG: 0x1a (OUT_HDR: |dmac_47_16|dmac_15_0|ip_ver|cvlan_tag|)| |-- FTE: 0x0 (FWD) to (TIR:0x54) dmac:0xffffffffffff ip_ver:0x6| ±- FTE: 0x1 (FWD) to (TIR:0x55) dmac:0xffffffffffff ip_ver:0x4|-- FG: 0x1b (OUT_HDR: |dmac_47_16|dmac_15_0|)| ±- FTE: 0x2492 (FWD) to (TIR:0x56) dmac:0xffffffffffff|-- FG: 0x1c (OUT_HDR: |dmac_47_16|ip_ver|cvlan_tag|)| |-- FTE: 0x4924 (FWD) to (TIR:0x54) dmac:0x33330000 ip_ver:0x6| ±- FTE: 0x4925 (FWD) to (TIR:0x55) dmac:0x33330000 ip_ver:0x4±- FG: 0x1d (OUT_HDR: |dmac_47_16|)±- FTE: 0x6db6 (FWD) to (TIR:0x56) dmac:0x33330000FT: 0x40007 (level: 0x1d, type: NIC_RX)|-- FG: 0x19 (NO_MATCH)| ±- FTE: 0x0 (FWD) to (TIR:0x56)±- FG: 0x1f (OUT_HDR: |dmac_47_16|dmac_15_0|)±- FTE: 0x2492 (FWD) to (TIR:0x56) dmac:0x98039b975076FT: 0x40006 (level: 0x1b, type: NIC_RX)|-- FG: 0x18 (OUT_HDR: |ip_ver|cvlan_tag|)| |-- FTE: 0x0 (FWD) to (TIR:0x54) ip_ver:0x6| ±- FTE: 0x1 (FWD) to (TIR:0x55) ip_ver:0x4±- FG: 0x1e (OUT_HDR: |dmac_47_16|dmac_15_0|ip_ver|cvlan_tag|)|-- FTE: 0x2492 (FWD) to (TIR:0x54) dmac:0x98039b975076 ip_ver:0x6±- FTE: 0x2493 (FWD) to (TIR:0x55) dmac:0x98039b975076 ip_ver:0x4but we set dv_flow_en=0 use mlx5 verbs flow API with rte_flowIXIA send 99G 64bytes traffic, flow can count 99G trafficMaybe DV flow API has BUG ?Hi Arthas,Which traffic is generated via IXIA? TCP?What happens if you’re deleting the rule and keeping dv flow enabled?Thanks,ChenPowered by Discourse, best viewed with JavaScript enabled"
1129,mellanox-8700-switch-cabling,"Need mellanox 8700 cabling with UFM appliance, DGX nodes
we have 28700, 1UFM appliance, 2*DGX nodes
need to know how to connect the QSFP cabling and does stacking is needed across mellanox switches? how to connect the switches to UFM appliance?Hello psatish69Rough draw with “|”, is not working at below.
It’s simple. Spread cables to each swtich the sam as UFM and 2 DGX servers for redundancy.For further details, please contact NVIDIA regional Sales for the proper implementation.QM8700------QM8700
|                      |
|                 |
|          |
|    |
UFM (redundancy)QM8700------QM8700
|       |           |        |
|         |      |           |
|    |                |      |
DGX1              DGX2Powered by Discourse, best viewed with JavaScript enabled"
1130,when-the-version-comes-out-mlnx-en-lts-for-rhel-8-3,"Are there only 8 versions for CentOS 8.x now? CentOS 8.1 and 8.2 are already End-of-life, and there is still no version for 8.3. In CentOS 8.3, the netdev structure was changed and the version for 8.2 does not work now./var/tmp/OFED_topdir/BUILD/mlnx-ofa_kernel-4.9/obj/default/drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib.c:50:29: error: initialization of 'void (*)(struct net_device *, unsigntible-pointer-types].ndo_tx_timeout = mlx5i_tx_timeout,^~~~~~~~~~~~~~~~/var/tmp/OFED_topdir/BUILD/mlnx-ofa_kernel-4.9/obj/default/drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib.c:50:29: note: (near initialization for 'mlx5i_netdev_ops..nCC [M] /var/tmp/OFED_topdir/BUILD/mlnx-ofa_kernel-4.9/obj/default/drivers/net/ethernet/mellanox/mlx5/core/fpga/conn.occ1: some warnings being treated as errorsmake[3]: *** [scripts/Makefile.build:315: /var/tmp/OFED_topdir/BUILD/mlnx-ofa_kernel-4.9/obj/default/drivers/net/ethernet/mellanox/mlx5/core/en_main.o] Error 1make[3]: *** Waiting for unfinished jobs…/var/tmp/OFED_topdir/BUILD/mlnx-ofa_kernel-4.9/obj/default/drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib_vlan.c:160:29: error: initialization of 'void (*)(struct net_device *, ncompatible-pointer-types].ndo_tx_timeout = mlx5i_tx_timeout,^~~~~~~~~~~~~~~~/var/tmp/OFED_topdir/BUILD/mlnx-ofa_kernel-4.9/obj/default/drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib_vlan.c:160:29: note: (near initialization for 'mlx5i_pkey_netdev_ops.<acc1: some warnings being treated as errorscc1: some warnings being treated as errorsHi Evgenii,The next release of Mellanox OFED LTS is currently scheduled to be released by end of April 2021, and it is planned to support CentOS/RHEL 8.3Please note that changes in the schedule and supported platforms might occur as the driver is under development.Regards,ChenDoes this also mean that OL8.3 support will also be forthcoming?Powered by Discourse, best viewed with JavaScript enabled"
1131,psu-led-status-clarification,"Hi, I’ve got an AS4610-55T switch with two PSU connected to PDU strips.One PSU displays a Green LED and the other display a RED LED.I’ve had a look through the Mellanox docs and cannot find any information regarding the status of these LED.I apologies for asking a simple question, but is someone able to confirm what these Green and RED status indicate?I assume green is active - (currently in use) and red is passive (hot spare)Or is it a case that both should be green and that the red status indicates that there is a problem?Hello Rakesh,On most switching platforms, a solid red LED on the PSU typically indicates some sort of fault was found on the PSU. This is the case on Mellanox hardware based switches. However, I would recommend reaching out to the manufacturer, Edgecore, for a confirmation of what this means on their AS4610-55T switch.Regards,Christian MatlockPowered by Discourse, best viewed with JavaScript enabled"
1132,kubernetes-on-bluefield-2,"I have been working on setting up workers nodes (through K8s and K3s) on the bluefield 2 cards (Ubuntu 20.04) and so far I have had had no success. Major issues include faulty kubelet status (loaded, but not active), bus error while downloading packages, etc. Anyone here who has successfully brought a heterogeneous cluster (x86/aarch64) using bluefield 2? If so, please let me know. Thanks.Hey Vivek, could you share with us some more details about these errors? Are there any logs or errors or failures you can share about that faulty kubelet status?Can you also share the console/terminal output of the bus error that you see while downloading packages?Thanks!Hi Justin, we have trouble setting up a K8 worker node on bluefield2.
The main problem is that curl and wget commands do not work on Bluefield 2 as explained hereI wonder if you have any solution for that?Hi,
Regarding the wget/curl issue, if IPSEC isn’t used/needed, you can change the default OpenSSL behavior for the entire DPU, and it will workaround the current OpenSSL bug.The BFB comes with two config files,Based on the need, either of the above two config files can be copied to /etc/ssl/openssl.cnf to be used by OpenSSL package.Ex: To disable PKA, execute # cp /etc/ssl/openssl.cnf.orig /etc/ssl/openssl.cnfHowever, this step will need to be repeated on every reboot (due to restrictions from the IPSEC daemon).Hi Justin,Would you mind to post the output of “systemctl status kubelet” and as well as “cat /etc/cni/net.d/99-loopback.conf”?Also do you have k8s version (kubeadm,kubelet, kubectl) and BF-2 BFB image version?Thanks eitkin. That did not solve the curl/wget issue either. (w/o the need for IPSEC).So I got the cluster working with bluefield2 as one of the worker nodes. However, none of the images appear to be loading on the ARM cores. Attached below are the outputs of:$ kubectl get pods -ANAMESPACE     NAME                             READY   STATUS             RESTARTS              AGE    IP              NODE        NOMINATED NODE   READINESS GATES
kube-system   coredns-78fcd69978-sjlj5         1/1     Running            0                     2d8h   10.xx.xx.xx     themis                 
kube-system   coredns-78fcd69978-z7v99         1/1     Running            0                     2d8h   10.xx.xx.xx     themis                 
kube-system   etcd-themis                      1/1     Running            3                     2d8h   10.xx.xx.xx    themis                 
kube-system   kube-apiserver-themis            1/1     Running            0                     2d8h   10.xx.xx.xx    themis                 
kube-system   kube-controller-manager-themis   1/1     Running            0                     2d8h   10.xx.xx.xx    themis                 
kube-system   kube-flannel-ds-4smmg            1/1     Running            0                     2d8h   10.xx.xx.xx    themis                 
kube-system   kube-flannel-ds-k2l7j            0/1     CrashLoopBackOff   662 ( ago)   2d8h   10.xx.xx.yy   bluefield              
kube-system   kube-proxy-k92bg                 1/1     Running            0                     2d8h   10.xx.xx.xx    themis                 
kube-system   kube-proxy-tkk5x                 1/1     Running            0                     2d8h   10.xx.xx.yy   bluefield              
kube-system   kube-scheduler-themis            1/1     Running            0                     2d8h   10.xx.xx.xx    themis                 $ kubectl describe pods kube-flannel-ds-k2l7j -n kube-system
Screen Shot 2021-08-23 at 1.09.40 AM1920×1605 128 KB
Hi, you mentioned that it did not solve the issue? I just tested it again on my setup and wget worked as a charm. The command I used is the one noted above:
# cp /etc/ssl/openssl.cnf.orig /etc/ssl/openssl.cnfCould you maybe post the contents of your current “/etc/ssl/openssl.cnf” file, as well as the wget command you are trying to use?Thanks,
Eyal.Hi,Can you please create a file /etc/cni/net.d/99-loopback.conf on the BF2 and add this following:
{
“cniVersion”: “0.3.1”,
“type”: “loopback”
}Then restart kubelet.On the kmaster node, I would suggest you using node selector/affinity to avoid flannel to currently run on the bluefield.Please post me the output of:
kubectl get nodes -o wideroot@bluefield:/home/ubuntu# curl -sLS https://get.arkade.dev
curl: (35) error:14094419:SSL routines:ssl3_read_bytes:tlsv1 alert access deniedroot@bluefield:/home/ubuntu# ls /etc/ssl/openssl.cnf
/etc/ssl/openssl.cnfHi Eitkin, here is the output of /etc/ssl/openssl.cnf file:
root@bluefield:~# cat /etc/ssl/openssl.cnf#.include filenameHOME = .#oid_file = $ENV::HOME/.oidoid_section = new_oids[ new_oids ]tsa_policy1 = 1.2.3.4.1tsa_policy2 = 1.2.3.4.5.6tsa_policy3 = 1.2.3.4.5.7####################################################################[ ca ]default_ca = CA_default # The default ca section####################################################################[ CA_default ]dir = ./demoCA # Where everything is keptcerts = $dir/certs # Where the issued certs are keptcrl_dir = $dir/crl # Where the issued crl are keptdatabase = $dir/index.txt # database index file.#unique_subject = no # Set to ‘no’ to allow creation ofnew_certs_dir = $dir/newcerts # default place for new certs.certificate = $dir/cacert.pem # The CA certificateserial = $dir/serial # The current serial numbercrlnumber = $dir/crlnumber # the current crl numbercrl = $dir/crl.pem # The current CRLprivate_key = $dir/private/cakey.pem# The private keyx509_extensions = usr_cert # The extensions to add to the certname_opt = ca_default # Subject Name optionscert_opt = ca_default # Certificate field optionsdefault_days = 365 # how long to certify fordefault_crl_days= 30 # how long before next CRLdefault_md = default # use public key default MDpreserve = no # keep passed DN orderingpolicy = policy_match[ policy_match ]countryName = matchstateOrProvinceName = matchorganizationName = matchorganizationalUnitName = optionalcommonName = suppliedemailAddress = optional[ policy_anything ]countryName = optionalstateOrProvinceName = optionallocalityName = optionalorganizationName = optionalorganizationalUnitName = optionalcommonName = suppliedemailAddress = optional####################################################################[ req ]default_bits = 2048default_keyfile = privkey.pemdistinguished_name = req_distinguished_nameattributes = req_attributesx509_extensions = v3_ca # The extensions to add to the self signed certstring_mask = utf8only[ req_distinguished_name ]countryName = Country Name (2 letter code)countryName_default = AUcountryName_min = 2countryName_max = 2stateOrProvinceName = State or Province Name (full name)stateOrProvinceName_default = Some-StatelocalityName = Locality Name (eg, city)0.organizationName = Organization Name (eg, company)0.organizationName_default = Internet Widgits Pty Ltd#1.organizationName = Second Organization Name (eg, company)#1.organizationName_default = World Wide Web Pty LtdorganizationalUnitName = Organizational Unit Name (eg, section)#organizationalUnitName_default =commonName = Common Name (e.g. server FQDN or YOUR name)commonName_max = 64emailAddress = Email AddressemailAddress_max = 64[ req_attributes ]challengePassword = A challenge passwordchallengePassword_min = 4challengePassword_max = 20unstructuredName = An optional company name[ usr_cert ]basicConstraints=CA:FALSEnsComment = “OpenSSL Generated Certificate”subjectKeyIdentifier=hashauthorityKeyIdentifier=keyid,issuer#nsCaRevocationUrl = http://www.domain.dom/ca-crl.pem#nsBaseUrl#nsRevocationUrl#nsRenewalUrl#nsCaPolicyUrl#nsSslServerName[ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEncipherment[ v3_ca ]subjectKeyIdentifier=hashauthorityKeyIdentifier=keyid:always,issuerbasicConstraints = critical,CA:true[ crl_ext ]authorityKeyIdentifier=keyid:always[ proxy_cert_ext ]basicConstraints=CA:FALSEnsComment = “OpenSSL Generated Certificate”subjectKeyIdentifier=hashauthorityKeyIdentifier=keyid,issuer#nsCaRevocationUrl = http://www.domain.dom/ca-crl.pem#nsBaseUrl#nsRevocationUrl#nsRenewalUrl#nsCaPolicyUrl#nsSslServerNameproxyCertInfo=critical,language:id-ppl-anyLanguage,pathlen:3,policy:foo####################################################################[ tsa ]default_tsa = tsa_config1 # the default TSA section[ tsa_config1 ]dir = ./demoCA # TSA root directoryserial = $dir/tsaserial # The current serial number (mandatory)crypto_device = builtin # OpenSSL engine to use for signingsigner_cert = $dir/tsacert.pem # The TSA signing certificatecerts = $dir/cacert.pem # Certificate chain to include in replysigner_key = $dir/private/tsakey.pem # The TSA private key (optional)signer_digest = sha256 # Signing digest to use. (Optional)default_policy = tsa_policy1 # Policy if request did not specify itother_policies = tsa_policy2, tsa_policy3 # acceptable policies (optional)digests = sha1, sha256, sha384, sha512 # Acceptable message digests (mandatory)accuracy = secs:1, millisecs:500, microsecs:100 # (optional)clock_precision_digits = 0 # number of digits after dot. (optional)ordering = yes # Is ordering defined for timestamps?tsa_name = yes # Must the TSA name be included in the reply?ess_cert_id_chain = no # Must the ESS cert id chain be included?ess_cert_id_alg = sha1 # algorithm to compute certificateThanks,
Are you suggesting not to allow flannel to run on bluefield2?
We think the network fabric need to be hosted on all the nodes for sending the queries.
here is the output
kubectl get nodes -o wide
NAME        STATUS   ROLES                  AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION         CONTAINER-RUNTIME
bluefield   Ready                     2d22h   v1.22.0   10.xx.xx.xx           Ubuntu 20.04.2 LTS   5.4.0-1013-bluefield   docker://20.10.8themis      Ready    control-plane,master   2d22h   v1.22.0   10.xx.xx.xx            Ubuntu 20.04.2 LTS   5.4.0-81-generic       docker://20.10.8And this is the output of pods:
kubectl get pods -A -o wide
NAMESPACE     NAME                             READY   STATUS             RESTARTS              AGE     IP              NODE        NOMINATED NODE   READINESS GATES
kube-system   coredns-78fcd69978-sjlj5         1/1     Running            0                     2d22h   10.93.120.2     themis                 
kube-system   coredns-78fcd69978-z7v99         1/1     Running            0                     2d22h   10.93.120.3     themis                 
kube-system   etcd-themis                      1/1     Running            3                     2d22h   10.xx.xx.xx    themis                 
kube-system   kube-apiserver-themis            1/1     Running            0                     2d22h   10.xx.xx.xx    themis                 
kube-system   kube-controller-manager-themis   1/1     Running            0                     2d22h   10.93.226.77    themis                 
kube-system   kube-flannel-ds-4smmg            1/1     Running            0                     2d22h   10.xx.xx.xx    themis                 
kube-system   kube-flannel-ds-k2l7j            0/1     CrashLoopBackOff   823 ( ago)   2d22h   10.93.231.112   bluefield              
kube-system   kube-proxy-k92bg                 1/1     Running            0                     2d22h   10.xx.xx.xx    themis                 
kube-system   kube-proxy-tkk5x                 1/1     Running            0                     2d22h   10.xx.xx.xx   bluefield              
kube-system   kube-scheduler-themis            1/1     Running            0                     2d22h   10.xx.xx.xx    themis                 Thanks. The content you posted matches that of “/etc/ssl/openssl.cnf.orig” and specifically the file doesn’t define the PKA engine as is defined in “/etc/ssl/openssl.cnf.mlnx”, meaning that the OpenSSL bug shouldn’t occur now.Did you test curl/wget after you performed the “cp /etc/ssl/openssl.cnf.orig /etc/ssl/openssl.cnf” command? I would expect them both to work without errors.We did – still similar errors. Rebooted the machine for a sanity check (and changed the conf yet again), yet no success. Any other point of failure that comes to your mind?Could you please post the error you get from both “curl” and “wget”? From the curl error you posted above this seems like an https error and not a crash (segfault) like the initial error.While I tried your above command and it worked on my setup, it would be great if you could try accessing a different server so we could check what is the cause for this issue:wget https://linux.mellanox.com/public/repo/doca/1.1/ubuntu20.04/doca.listcurl -sLS https://linux.mellanox.com/public/repo/doca/1.1/ubuntu20.04/doca.listThis is correct. We’ll provide a CNI in future but for now we only support “host network” PODs.
The cni loopback configuration is just to make kubelet running.
You should be able to get a micro app pod deployed and running.I’m currently running myself a NGINX pod in my setup:root@kmaster:/# kubectl get pod -o wide
NAME         READY   STATUS    RESTARTS   AGE   IP             NODE                          NOMINATED NODE   READINESS GATES
static-web   1/1     Running   0          13d   10.110.169.9        bf-02.internal.nvidia.com root@kmaster:/# kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE    SELECTOR
kubernetes   ClusterIP   10.233.0.1             443/TCP          118d   
nginx        NodePort    10.233.44.43           8080:31822/TCP   20d    app=nginxroot@bluefield:/home/ubuntu# wget https://linux.mellanox.com/public/repo/doca/1.1/ubuntu20.04/doca.list–2021-08-25 03:35:11--  https://linux.mellanox.com/public/repo/doca/1.1/ubuntu20.04/doca.list
Resolving linux.mellanox.com (linux.mellanox.com)… 168.62.212.37
Connecting to linux.mellanox.com (linux.mellanox.com)|168.62.212.37|:443… connected.
OpenSSL: error:14094419:SSL routines:ssl3_read_bytes:tlsv1 alert access denied
Unable to establish SSL connection.root@bluefield:/home/ubuntu# curl -sLS https://linux.mellanox.com/public/repo/doca/1.1/ubuntu20.04/doca.listcurl: (35) error:14094419:SSL routines:ssl3_read_bytes:tlsv1 alert access deniedThanks for the output traces. The config file update indeed solved the original (segfault) issue, but it seems you encounter some certificate-based issue, as it happens for both curl/wget and it isn’t site-specific.Could you please supply the following so we could try to reproduce it on our end?If you didn’t have the /etc/cni/net.d/99-loopback.conf it is likely that you aren’t using the latest BlueField OS version, in which case an update will most probably resolve this issue.We are using Bluefield OS 3.7 version. I could see the /etc/cni/net.d/99-loopback.conf file configured correctly.cat /etc/cni/net.d/99-loopback.conf
{
“cniVersion”: “0.3.1”,
“type”: “loopback”
}root@bluefield:/home/ubuntu# ofed_info -s
MLNX_OFED_LINUX-5.4-1.0.3.0:root@bluefield:/home/ubuntu# openssl version
OpenSSL 1.1.1f  31 Mar 2020Powered by Discourse, best viewed with JavaScript enabled"
1133,trouble-using-connectx-5-ex-cards-using-host-chaining-mode-some-connections-work-but-not-others,"I have 3 servers, each with a dual port ConnectX-5 Ex card. I have them connected in a chain like this: S1 → S2 → S3. Server 2 (S2) uses both ports and has (by default) host chaining mode set to BASIC(1). I assigned IP addresses to all the ports and try to ping on each server the other two and I find some combinations that don’t work.S1 can ping S2 and S3 correctly. (which means host chaining seems to be working)S2 can ping S1, but pings to S3 failS3 can ping S1, but pings to S2 failI tried a different topology. Now I have S1 → S3 → S2S1 can ping S2 but pings to S3 failS2 can ping S1 and S3S3 can ping S2 but pings to S1 failAny thoughts as to what may be happening? Or how to get more information to fix this issue? It seems weird that in the first experiment S3 responds to pings from S1 but not from S2 on the same port.I appreciate the help.Hello Rafael,Thank you for posting your inquiry on the NVIDIA/Mellanox Community.Based on your information, we noticed you have a valid support contract, therefor it is more appropriate to assist you further through a support ticket.You will receive a notification from your new support ticket shortly.Thank you,~NVIDIA/Mellanox Technical Support.Hey did you ever get this resolved?  I believe I’m having the exact same issue as I have three connectx-5 En cards, each one in a separate computer, where I can communicate as you’ve described between them, but not directly between two of the nodes.For anyone who might be having a similar issue, I was able to avoid this by connecting all three nodes into a ring topology.  So setting host chaining to true for each card via mstconfig as mentioned above, and then connecting all three in a ring: S1 → S2 (both right port), S2 → S3 (left port on S2, right port on S3) and S3 → S1 (both left port).  The port selection may not matter, but I figured I’d include it just in case.  Not sure why they can’t ping directly, but having them chain works fine without any noticeable lag in my applications.  For reference, I’m using 3 ConnectX-5 En cards, MCX516A-CCAT, with three ubuntu 20.04 machines with varying ages of mobo and cpus.  For the En cards, using mlnx-en-5.9-0.5.6.0-ubuntu20.04-x86_64.iso.  You can find instructions here https://docs.nvidia.com/networking/display/MLNXOFEDv461000/Installing+Mellanox+OFEDOnce mounted, I used “install -vvv --with-nvmf --force” to install everything properly for ubuntu 20.04.  I’m including all this extra info because I had such a hard time collating everything I needed, so hopefully this helps someone else.Powered by Discourse, best viewed with JavaScript enabled"
1134,mlx5-core-local-protection-error-vend-err-0x51,"Hi,I’m trying to diagnose the following error, is anyone able to shed light on the vend_err of 51 here?Jun 30 08:32:30 sys9client1 kernel: mlx5_core 0000:21:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x002c address=0x180ff000 flags=0x0000]
Jun 30 08:32:30 sys9client1 kernel: mlx5_core 0000:21:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x002c address=0x180ff000 flags=0x0000]
Jun 30 08:32:30 sys9client1 kernel: infiniband mlx5_0: mlx5_handle_error_cqe:333:(pid 0): WC error: 4, Message: local protection error
Jun 30 08:32:30 sys9client1 kernel: infiniband mlx5_0: dump_cqe:273:(pid 0): dump error cqe
Jun 30 08:32:30 sys9client1 kernel: 00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Jun 30 08:32:30 sys9client1 kernel: 00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Jun 30 08:32:30 sys9client1 kernel: 00000020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Jun 30 08:32:30 sys9client1 kernel: 00000030: 00 00 00 00 04 00 51 04 00 00 48 63 fc 7d ef e2
Jun 30 08:32:30 sys9client1 kernel: iser: iser_err_comp: task_rsp failure: local protection error (4) vend_err 0x51Hello @PAB01,Thank you for posting your query on our community. In order to be able to assist you further with this error, we would need more information. We would recommend opening a support case for further investigation of the issue. The support ticket can be opened by emailing "" Networking-support@nvidia.com "". Please provide us with a snapshot from your server. The sysinso-snapshot tool and instructions can be found at - GitHub - Mellanox/linux-sysinfo-snapshot: Linux Sysinfo SnapshotPlease note that an active support contract would be required for the same. If you do not have a current support contract, please reach out to our Contracts team at networking-contracts@nvidia.comThank you,
-Nvidia Network SupportAdditionally, please also provide detailed information when the issue is seen.Thanks,
BhargaviPowered by Discourse, best viewed with JavaScript enabled"
1135,implement-rdma-congestion-control-on-bluefield2,"Can I implement RDMA congestion control（like DCQCN or HPCC) on DPU? If yes, how can I do itHi @czq2,Our firmware/HW implemented this algorithm, so you don’t need to implement it. We have config parameters in the firmware, but we don’t recommend to change them. This should work in default configuration.Regards,
Chenthanks you @chenh1
I want to kown does the DPU support programmable congestion control? Can I implement my own cc algorithm on bluefiled2?Best WishesHi @czq2,Basically, it can be done in the application level. It is possible to develop applications that implement congestion control (but if other application that doesn’t support CC runs in parallel, this can cause CC to not work).Regards,
ChenThanks again @chenh1
Can the DPU support PCC like the CX6 NIC? I checked the firmware information of the DPU and found a similar information of cx6 allow user cc
Best RegardsHi @czq2 - yesRegards,
Chen@chenh1  Thank you for taking the time answer all my questions , and wish you a wonderful day！Powered by Discourse, best viewed with JavaScript enabled"
1136,mellanox-oynx-ptp-stuck-in-uncalibrated,"I have two identical systems of Grandmaster → PC → Mellanox SN2100.For testing, I am trying to get both switches to sync to their respective grandmaster. One of them successfully makes the switch port a slave while the other is stuck in uncalibrated. The PCs have the same ptp4l config setup and files.From looking at documentation, uncalibrated means its in a transient state (to allow synchronization), but it has been in that state for a while. Only that port has ptp enabled so the PC should be the master (and thus that port the slave)Is there any idea what can put the switch state to slave? I see the PTP messages being sent via wireshark.Hello Benjamin, I cannot tell what OS you are using and not know how PC is configured as. But please refer to below URL if this is Onyx.
https://docs.nvidia.com/networking/display/ONYXv381174/Precision+Time+ProtocolYour lPC needs to be boundary clock. If you need further troubleshooting, please open a case.Thank you @jsl2 . I managed to resolve the issue. I wasn’t aware that I needed to update the BC to be close to the master clock. Once I restarted phc2sys, it went from uncalibrated to slave and solved the issue.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1137,cannot-deregister-memory-region-ibv-dereg-mr-if-mr-size-reach-2gb,"MLNX_OFED version:
MLNX_OFED_LINUX-5.4-3.0.3.0-ubuntu20.04-x86_64System:
Ubuntu 20.04When mr size reaches about 2GB:However, mr = ibv_reg_mr() still works well, I can even do rdma operations when ibv_dereg_mr not called.
When mr size not reaching 2GB, about 2045MB, it also works well.Code Example:// mem_ptr points to mmaped 2M hugepages, dereg problem occurs when mem_sz reaches 2GB.
auto mr = ibv_reg_mr(pd, mem_ptr, mem_sz, IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_READ | IBV_ACCESS_REMOTE_WRITE | IBV_ACCESS_REMOTE_ATOMIC);// When sleeping, everything works well, ibv_devinfo also shows the device
int count = 30;
while (count > 0) {
count–;
sleep(1);
LOG(2) << ""sleeping… mr size: "" << mr->length;
}{
auto rc = ibv_dereg_mr(mr);   // When problem occurs, ibv_devinfo prints “failed to open device”
LOG(2) << “dereg mr”;              // When problem occurs, process stucks and this line is not printed
LOG_IF(2, rc != 0) << ""dereg mr error: "" << strerror(errno);
}Powered by Discourse, best viewed with JavaScript enabled"
1138,dpdk-mlx5-without-root-permissions,"Hi,I am trying to use DPDK on a Connectx-5 using the mlx5 driver without root permissions.I followed the documentation on how to use DPDK without root permissions, but the guide information only concerns the VFIO driver.Following this guide only, I get the error “Failed to create TIS using DevX”, for example with testpmd:By launching the same command as root, no error occurs.I already tried tuning the permissions of the mst devices, without success:(I am in group rdma)I suppose that some other permissions have to be tuned with the mlx5 driver, but I can’t figure which ones.Q1: Is it actually possible to use the mlx5 driver without root permissions ?Q2: If yes, what is the procedure to use mlx5 driver backend for DPDK without root permissions ?Thanks for any help,JulienHi Julien,You can try running the test with debug level, by adding ‘–log-level=eal,8’, and look for additional useful prints.I.E:In addition, for testing purposes you can try disabling DV flow and use the Verbs flow engine instead, by adding devargs of dv_flow_en=0. Do you see different results?I.E:Best Regards,ChenHi Chen,Thanks for your answer.I attached to this message the logs with maximum verbosity for the EAL, both with and without root permissions. The error message is essentially the same (lines 219-223 of the non-root log).I noticed a few differences between the two logs, especially regarding the NUMA nodes, but I am not sure they have anything to do with my problem.By disabling DV flow, I obtained the exact same result.By gathering information about your dv_flow_en parameter, I came across the CONFIG_RTE_LIBRTE_MLX5_DEBUG compilation option, that I did not know about. I’ll try and see if this brings me more information…Please let me know if you have any other idea,Best regards,JulienHi again,I just figured that it is possible to increase verbosity of mlx5-related messages. Please find attached the logs produced this way, as a standard user. I also tried disabling DevX (UCTX_EN=0), which gives a different error, but still not much information.I’ll let you know of any progress on my side.Best regards,JulienJust as an update, I managed to run testpmd without root permissions by tuning file capabilities, which is a viable option for me.For now, I simply use sudo setcap all=eip dpdk-testpmd, but I’ll try and narrow down the exact capabilities required.So it looks like the only file capability needed is cap_net_raw, which makes sense.Everything seem to run fine once I run sudo setcap cap_net_raw=eip dpdk-testpmd before launching testpmd.EDIT: To avoid tweaking file permissions on hugepages, I now set the cap_dac_override capability at the same time, with sudo setcap cap_net_raw,cap_dac_override=eip dpdk-testpmdPowered by Discourse, best viewed with JavaScript enabled"
1139,diagnosing-switch-that-does-not-post,"Hi,I have a broken switch here to play around with and I am wondering if you can help me revive it.It is an SN2700 and it has the following symptoms:Outside LEDS:Port side board:Main board:I think the problem lies with these last few leds which I beleve are indicating that the CPU and memory voltages are not working or are not active.If I remove the DIMM the result is the same. I’ve already tried replacing the DIMM but that makes no difference it seems.I would appreciate any suggestions, I am hoping to use this one for testing in the lab here. Thanks!I had a similar issue.  I unscrewed and pulled out the mSATA drive and put it back.  That fixed it for me!Approx. 30 screws to get inside the case though!Thanks, I’ve tried that a few times already. Even tried a blank mSATA. No luck though.Still hoping someone has another idea, hate to see this go to waste. :)Powered by Discourse, best viewed with JavaScript enabled"
1140,mellanox-connectx-4-is-incompatible-with-amd-ryzen-9-5950x-and-centos,"It would appear that no matter which way I slice and dice it, the Mellanox ConnectX-4 card (MCX456A-ECAT) is incompatible with an AMD Ryzen 9 5950X and CentOS (I’ve tried both 7.7.1908 and 7.9.2009).My MCX456A-ECAT has already been updated to the latest firmware and I am using MLNX_OFED_LINUX-5.5.1.0.3.2-rhel7.9-x86_64 (which is the latest drivers available for it) and it still results in problems.If I use the “inbox” drivers, it results in a kernel panic (and I’ve already tried reaching out to the CentOS team about it).If I try to use the MLNX OFED drivers, openibd fails to restart after the installation of the driver or fails to start properly when the system is rebooted.Hi ChanWith limited info, it’s hard to give a right answer.i’d like you to open a CASE and contact a supporter.Meanwhile, could you please double check if there are some errorscould you please re-install OFED and collect below and open a CASE with thoes?ThanksPowered by Discourse, best viewed with JavaScript enabled"
1141,enabling-scalable-functions-sf-in-docker-containers,"Hello,I am interested in creating a Scalable Function and passing it to container namespace. I am following this NVIDIA tutorial: Upstream step by step guide · Mellanox/scalablefunctions Wiki · GitHub on how to enable SF.I manage to successfully enable SF on host machine and ping from it, however when I pass that SF interface to container namespace and bring it up there it stops working. I can’t seem to reach outside of container using that new SF interface even though I bring it UP and assign it static IP address, both using ifconfig.Any ideas on why SF interface doesn’t seem to work once passed to container namespace?Also to note, once I tested SF interface on host system, RX/TX counters did not get updated for SF interface which I was using but rather the Physical Interface (PF) which it originated from? What could that be?Best regards!Powered by Discourse, best viewed with JavaScript enabled"
1142,is-it-possible-to-run-rdma-roce-on-windows-7,"There seems no available adapter driver for window 7, as the min support OS of WinOF2 is Windows 8.1 and the older WinOF doesn’t support Connext-CX4.We need to run RoCE on Windows 7 as many of our customers still deploy their app client on Windows 7. Is there any possible solution?Hello Derek,Thank you for posting your inquiry on the NVIDIA Networking Community.Unfortunately we do not provide a WinOF-2 driver for Windows 7. As Windows 7 is longer supported (Jan. 2020), they only resolution is to upgrade to a supported Windows OS version.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1143,missing-sources-in-mlnx-ofed,"Are there supposed to be missing sources in the MLNX_OFED source downloads (or the src directories of the packaged downloads)?  Because of this, what you get from ‘apt install mlnx-ofed-basic’ is not the same as ‘./install.pl --basic’For example, attempting to include infiniband-diags leads to:Indeed, there is no source for opensm in SOURCES.  The file BUILD_ID says:but that is not included anywhere as far as I can tell.Am I missing something?FWIW, I’m poking around here since the LTS MLNX_OFED doesn’t support dkms builds on Debian 11(trying to avoid building kernel modules but still get the rest of the goodies).opensm is not opensource so far, instead we provide pre-build binary. For debian 11 we support |Debian11.2| |x86_64|5.10.0-10-amd64|https://docs.nvidia.com/networking/display/MLNXOFEDv571020/General+SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1144,gpudirect-storage-issue-on-nvidia-dgx-a100-system,"Hello,I am trying to test out GPUDirect storage capabilities on an NVIDIA DGX A100 40GB system. I have compiled the sample CUFile application shown here:The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context,...When I run this app, I get the following error:This error code corresponds to CU_FILE_INTERNAL_ERROR.We are using the CUDA 12.0 driver and toolkit.Please advise on how I can eliminate this error. Thank you!Can you share the cufile.log file to check the reason for the error? It should be available in the directory where you ran your application.Below is the output from cufile.log:error creating udev_device for block device dev_no: 0:52From the logs it looks like we are not able to get the udev attributes of the block device. Can you tell me what kind of block device and which file system is being used for the IO operation?I am not sure how to find the kind of block device and file system. We are trying to write to the NVME SSDs available on the NVIDIA DGX A100.Do you know any commands that I could run to find the information needed?Can you provide following?Here is the output of those commands:/home/sashok6/test.datFrom the df -Th log
/home/sashok6/ has zfs file system which is not supported by GDS. Only ext4 and xfs are the local file systems supported by GDS currently. Please refer to this documentation for the same.
https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-local-fsAhh, that looks like the problem. Thanks for the help! I’ll look into getting an ext4 partition set up for further testing.Powered by Discourse, best viewed with JavaScript enabled"
1145,bluefield-dpu2-bringup-failied,"Hi Folks,i’m unable to upgarde the dpu bf image to latest.I downloaded the image from Linux Installation Guide :: NVIDIA DOCA SDK Documentation .root@UCSC-C240-M6S:/home/ubuntu# bfb-install --bfb DOCA_2.0.2_BSP_4.0.3_Ubuntu_22.04-10.23-04.prod.bfb --config bf.cfg --rshim rshim0
Pushing bfb + cfg
cat: write error: Connection timed out                                                                                                                                                             ]
320KiB 0:02:35 [2.06KiB/s] [             <=>                                                                                                                                                      ]
Failed to push BFB
root@UCSC-C240-M6S:/home/ubuntu#Console log:Welcome to minicom 2.8OPTIONS: I18n
Port /dev/rshim0/console, 11:50:59Press CTRL-A Z for help on special keysMellanox BlueField-2 A1 BL1 V1.1
ERROR:   Failed to load BL2R firmware.root@UCSC-C240-M6S:# lspci -s $(sudo lspci -d 15b3: | head -1 | awk ‘{print $1}’) -vvv | grep “Product Name”
Product Name: BlueField-2 P-Series DPU 100GbE Dual-Port QSFP56, integrated BMC, Secure Boot Enabled, Crypto Disabled, 16GB on-board DDR, 1GbE OOB management, FHHLroot@localhost:/home/ubuntu# bfrecprimary: /dev/mmcblk0boot0
backup: /dev/mmcblk0boot1
boot-bus-width: x8
reset to x1 after reboot: FALSE
watchdog-swap: disabled
lifecycle state: Secured (development)
secure boot key free slots: 3What could be the issue?Hi,Have you tried to disable secure boot, push 2.0.2 then re-enable secure boot?Powered by Discourse, best viewed with JavaScript enabled"
1146,ufm-node-license,"Hi,
I understand that node license is calculated per HCA, not per HCA port.
I know that it is necessary to purchase a 2 node license when installing 2 HCAs on a bare metal server and uploading UFM Enterprise s/w.
Then, the UFM Enterprise Appliance has two dual port HCAs by default, do I need to purchase a 2 node license for this as well?Hi Jayce,Thank you for posting your inquiry to the NVIDIA Developer Forums!This type of inquiry should be directed to our sales team - they’ll be able to answer all of your questions about licensing, and assist you with procuring coverage that suits your needs.https://www.nvidia.com/en-us/contact/sales/Best regards,
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1147,mlxup-exe-issues,"Hi all,I have a weird one.
I have been using mlxup.exe to upgrade Firmware on BF2 cards and I have issue with few servers.
When I run it I getmlxup.exe -d mt41686_pciconf0
‘C:\Users\USERNA~1\AppData\Local\Temp\sfxter_USERNAME’ is not recognized as an internal or external command,
operable program or batch file.Anyone had that before?Generally you should not update DPU fw.If you need,1.verify by dpu#> sudo bfvcheck
2.update by below cmds,dpu# sudo /opt/mellanox/mlnx-fw-updater/mlnx_fw_updater.pl --force-fw-updatedpu# sudo mlxfwreset -d /dev/mst/mt41686_pciconf0 --sync 1 -y resetThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1148,asking-official-documents-for-maximum-performance-of-mcx653106a-hdat-sp,"I got an answer that MCX653106A-HDAT-SP can only provide the maximum 200Gb/s speed including both 2 ports. This means if one port uses 10Gb/s, the others can only utilize 190Gb/s as maximum performance.I am wondering if there is any official letter or document so I can provide this to the customers. Customer wants an official certification of performance limitation of 2 port to choose different model.Please help!In case that you need to find, I would like to leave the case number that I asked before.Case No : 00978859Hi, Yeongmin,We already answer your question in case.ThanksPowered by Discourse, best viewed with JavaScript enabled"
1149,infiniband-connection-failure,"I have an hp cluster using InfiniBand running on Centos7.  Previous there was no issue with the connection.  I had a failed cable, and replaced it I now get a light again (good).  My issue is I once had a connection ib0 that is no longer there.  I tried to recreate it and I cannot start the connection seems there is some sort of naming issues.[root@server ~]# lspci -Qvv | grep Mellanox
Product Name: Mellanox ConnectX-6 Single Port VPI HDR100 QSFP Adapter
[VE] Vendor specific: NMVMellanox Technologies, Inc.
[root@server ~]# lspci -Qvv | grep d8:00.0
d8:00.0 Class 0207: Device 15b3:101b
[root@server ~]# msflint -d d8:00.0 q
bash: msflint: command not found…
[root@server ~]# mstflint -d d8:00.0 q
Image type:            FS4
FW Version:            20.35.1012
FW Release Date:       28.10.2022
Product Version:       20.35.1012
Rom Info:              type=UEFI version=14.28.15 cpu=AMD64
type=PXE version=3.6.804 cpu=AMD64
Description:           UID                GuidsNumber
Base GUID:             1c34da0300734862        4
Base MAC:              1c34da734862            4
Image VSD:             N/A
Device VSD:            N/A
PSID:                  DEL0000000013
Security Attributes:   secure-fwIb0 is the ipoib interface name.what is the output of ‘ifconfig -a’ or ‘ip link show’More on this:
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-configuring_ipoibHello,thanks for the response, it does not show up here.  I can see it in the idrac, see lights on there when its plugged in results are below:1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
2: em1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
link/ether 34:48:ed:f4:12:7c brd ff:ff:ff:ff:ff:ff
inet 10.141.250.10/16 brd 10.141.255.255 scope global noprefixroute em1
valid_lft forever preferred_lft forever
inet6 fe80::5bcd:a3a8:ebde:90a/64 scope link noprefixroute
valid_lft forever preferred_lft forever
3: em2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
link/ether 34:48:ed:f4:12:7d brd ff:ff:ff:ff:ff:ff
inet 172.16.16.15/20 brd 172.16.31.255 scope global noprefixroute em2
valid_lft forever preferred_lft forever
inet6 fe80::1540:fe0c:6114:b0c0/64 scope link noprefixroute
valid_lft forever preferred_lft forever
4: em3: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000
link/ether 34:48:ed:f4:12:7e brd ff:ff:ff:ff:ff:ff
5: em4: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000
link/ether 34:48:ed:f4:12:7f brd ff:ff:ff:ff:ff:ff
6: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
link/ether 52:54:00:47:5b:59 brd ff:ff:ff:ff:ff:ff
inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
valid_lft forever preferred_lft forever
7: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000
link/ether 52:54:00:47:5b:59 brd ff:ff:ff:ff:ff:ff1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: em1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 34:48:ed:f4:12:7c brd ff:ff:ff:ff:ff:ff
3: em2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 34:48:ed:f4:12:7d brd ff:ff:ff:ff:ff:ff
4: em3: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000
link/ether 34:48:ed:f4:12:7e brd ff:ff:ff:ff:ff:ff
5: em4: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000
link/ether 34:48:ed:f4:12:7f brd ff:ff:ff:ff:ff:ff
6: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default qlen 1000
link/ether 52:54:00:47:5b:59 brd ff:ff:ff:ff:ff:ff
7: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN mode DEFAULT group default qlen 1000
link/ether 52:54:00:47:5b:59 brd ff:ff:ff:ff:ff:ffCan you please run‘mst status -v’if mst command is not found on the server you will need to install the MFT tools.DEVICE_TYPE             MST                           PCI       RDMA            NET                       NUMA
ConnectX6(rev:0)        /dev/mst/mt4123_pciconf0      d8:00.0ok, so where do I go from here?  swapped the card still doesnt show in the network devices.  Should I try to reload the drivers, not sure why this would have happened here or where to go.You are not seeing those because you haven’t run ‘mst start’.DEVICE_TYPE             MST                           PCI       RDMA            NET                       NUMA
ConnectX6(rev:0)        /dev/mst/mt4123_pciconf0      d8:00.0                                             1[root@server1 ~]# nmcli d
DEVICE      TYPE      STATE        CONNECTION
em2         ethernet  connected    em2
em1         ethernet  connected    em1
virbr0      bridge    connected    virbr0
em3         ethernet  unavailable  –
em4         ethernet  unavailable  –
lo          loopback  unmanaged    –
virbr0-nic  tun       unmanaged    –
[root@server ~]# nmcli c
NAME      UUID                                  TYPE        DEVICE
em2       8f4a9a9e-91ae-4abc-9249-08d5c919e9e9  ethernet    em2
em1       259a73b9-2364-4bb0-a151-3e0b57538298  ethernet    em1
virbr0    b079822c-5f8f-4466-a05c-ce887467b12f  bridge      virbr0
em3       8d79a661-5786-4590-8051-c38bee32a3ca  ethernet    –
em4       1a232114-a4fa-44a4-8513-0706fbef1f80  ethernet    –
MLX1_ib0  cad27371-bdf3-4dd6-938c-0232550a9f5f  infiniband  –
[root@server ~]#Hi nathan.backing,Please kindly try to remove IPoIB module and add it back again.
rmmode ib_ipoib
modprobe ib_ipoibIf it doesn’t work, please use below Link to register to Enterprise Support Portal with a valid entitlement:https://enterpriseproductregistration.nvidia.com/?LicType=COMMERCIAL&ProductFamily=Networking-HWSupportAfter you complete the registration process with email, you will be able to login and access the portalWe are looking forward to hearing from you.Thanks,
YuyingI removed and re-added that IPoIB.  That did not work, but I had one other comment.  When I changed the device to ethernet mode I can get it to display in the devices without any issue.  I don’t know, if that would shed any more light on anything.  I am registered and tried to start up an case I was told that the card was sold through dell so contact dell.  Dell, just wanted to send me a new card and it did not work so I was look at the forums for any additional Ideas that may help.If you lost your IB interface, first thing might be to check the output of ibstat - that should show you whether your card is online at all -Once you see the port with State: Active and Physical state: LinkUp, you can proceed with the IPoIB stuff - module loading and probably restarting the networkd.Hi nathan.backing,Thanks for the update.
You’re recommended to upgrade the MLNX OFED driver to the latest release and test it again if it works.
If not, please kindly replace the care as Dell advised.Good luck.Thanks,
YuyingThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1150,disabling-icrc-check-on-mt27700-connectx-4-nic,"When the NIC receives a packet with an invalid ICRC checksum, the packet is discarded. I know that it is possible to configure the NIC to ignore the ICRC checksum and I know how to configure it for a ConnectX-5. However, I would like to know how to do it for a ConnectX-4 NIC.Many thanksConnectX-4 cards should be the same way with ConnectX-5.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1151,loopback-traffic-can-preempt-the-bandwidth,"When i use three hosts in 100 Gb,cx5. 1 loopback traffic and 2 rdma send, i find loopback traffic is 46G, but 2 rdma send traffic is 23Gb. Is there a mechanism to guarantee loopback traffic?if i start a new rdma send, 3 rdma send share 46G bandwidth, and each of them is about 16Gb.You use two card on the same server or you use dule port HCA ?ThanksPowered by Discourse, best viewed with JavaScript enabled"
1152,dhcpd-config-error-using-nvue,"In a demo environment I have setup a DHCP service (IPv4 only right now) and specified the correct interfaces I want it to run on, I can see in the run command being generated it’s including those, but when I plugin to one of those interfaces I am presented with the following error:You can see my config here that’s auto generated by NVUE:I am unsure why DHCPd is asking for interface specific declarations.  Based on the docs, by specifying the interfaces via NVUE it would be my hope that NVUE would edit the configs so that those powers have any required declarations needed to work.Ultimately DHCPd delivers DHCP requests from L3 interfaces.
So if swp2 is configured as a L3 interface, and it is not attached to a bridge this could be an expected output.What does the output show for:If using 5.x → ifquery br_default; ifquery swp2If using 4.x → ifquery swp2; ifquery bridgePowered by Discourse, best viewed with JavaScript enabled"
1153,comm-channel-samples-cannot-run-cc-server-connection-aborted,"Hi,I’m trying to run Comm Channel Samples. But when try to start listen in server, doca_comm_channel_ep_listen returns “Connection aborted”.The document saids “Connection aborted” means “registration of service failed”, but what is the service meaning here?What’s more, is there any configuration about network need to configure previously?The comm channel document didn’t mention it.Here is some key parameters of my program,I think these parameters should satisfy the limitation in the documentation:struct doca_comm_channel_init_attr attr = {256,DOCA_CC_INIT_FLAG_NONBLOCK,32,1024};
server_name=“hellodoca”Comm Channel need you run server/client on DPU/host.Your issue look like no backend app on DPU.Please refer connection flow,NVIDIA Documentation Center Welcome to the NVIDIA Documentation Center where you can explore the latest technical information and product documentation.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1154,storage-spaces-direct-switch-config,"I am setting up an S2D Cluster and looking for info on switch configuration.Using Windows Server 2022, Supermicro custom servers,  Melanox SwitchX, specifically SX6036 with the Eth enabled.I am following the Lenovo Storage Spaces Direct Deployment Guide.  It uses RoCE, PFC with DCB.I have the Nodes configured but am struggling with the switch settings.Is there a Guide or KB on SwitchX config with Server 2019/2022.  Most of what I find is for Server 2016.Searching I think i needswitch (config) # dcb priority-flow-control enable
switch (config) # dcb priority-flow-control priority 3 enable
switch (config) # interface ethernet 1/1-1/4 dcb priority-flow-control mode on force
switch (config) # dcb ets tc bandwidth 49 50 0 1Thanks,ToddThere is no specific guide specific to Windows Server for switch ROCE config.That switch is end of life already so support and documentation will be limited.This does not apply to your switch, but on Onyx there is a new command to enable ROCE for the whole switch with the default settings for ROCE:
roce losslessVerify with the following command:
show rocehttps://docs.nvidia.com/networking/display/Onyxv391908/RoCE+CommandsPowered by Discourse, best viewed with JavaScript enabled"
1155,mellanox-qsfp-40gbase-cr4-mc2210126-005-passive-copper-program-code,"Hi,we use the following moduleAs we are investigating the compatibility to a Sophos Firewall we would like to know which program code this transceiver is running?We couldnt find any information in the datasheet. Is there an answer?Thanks,
MNot sure I follow your q.This is a Mellanox branded Passive copper cable. It doesn’t have any active logic running within the module.Can you please explain what is ‘program code’? an example would be best.Hi dwaxman,we reference to Sophos` compatible module listing which states they do not support Cisco or generic program code. Only Intel is supported.
Service and Support (in the chapter 40G Transceivers).Honestly we dont exactly know what they mean by “program code”.Thanks,
MUnderstood.
As mentioned, those are passive cables – they hold an EEPROM in accordance to the SFF spec.They should be working fine unless Sophos firmware is actively blocking vendors other than Intel in their product.
This should be queried with Sophos.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1156,need-help-to-find-appropriate-windows-driver-for-mt27600,"Hello. I’m newbie in the networking. I have got following card which want to use with Windows 10:Infiniband controller [0207]: Mellanox Technologies MT27600 [Connect-IB] [15b3:1011]Subsystem: Mellanox Technologies Device [15b3:0062]I tried to install WinOF (MLNX_VPI_WinOF-5_50_53000_All_Win2019_x64.exe) and WinOF-2 (MLNX_WinOF2-2_70_53000_All_x64.exe ) software but they are not contain drivers for my card. Also it is not clear which family the card belong to (I suppose that is ConnectX-3 ?)And the second question. I’m planing to use the card to test my own hardware. Does it support for 10G/40G Ethernet or it limited to Infiniband only?Thank you.My fault. I read strings above several times and found that the card belongs to separate family “Connect-IB” which is not supported under Windows and do not support any Ethernet.Yes, that is correct.Powered by Discourse, best viewed with JavaScript enabled"
1157,cumulus-netq-4-0-has-been-released,"We released Cumulus NetQ 4.0.0 (over the long weekend here in the US). You can read the release notes and see what’s new, then download the software when you’re ready.Powered by Discourse, best viewed with JavaScript enabled"
1158,sn2010-firmware-13-2010-4026,"HiI’m wondering why this firmware version is not in the regular Linux Firmware archive?SN2010
13.2010.4026‘Index of /firmware’ → 3146 latestHi,That firmware appears to be very new, where are you seeing references to it?mlxfwmanager --query
Querying Mellanox devices firmware …Device Type:      Spectrum
Part Number:      Q9E63-63001_Ax
Description:      HPE StoreFabric SN2010M 25GbE 18SFP28 4QSFP28 Half Width Switch
PSID:             HPE0000000025
PCI Device Name:  0000:01:00.0
Base MAC:         98039bfde580
Versions:         Current        Available
FW             13.2010.4026   N/A…mlxfwmanager --query
Querying Mellanox devices firmware …Device Type:      Spectrum
Part Number:      MSN2010-Cxxx_Ax
Description:      Spectrum™ based 10GbE/100GbE 1U Open Ethernet switch with MLNX-OS; 18 SFP28 ports; 4 QSFP28 ports; 2 power supplies (AC); x86 dual core; Short depth; P2C airflow; Rail Kit; RoHS6
PSID:             MT_0000000108
PCI Device Name:  0000:01:00.0
Base MAC:         1c34da964a40
Versions:         Current        Available
FW             13.2010.4026   N/AHi,The firmware you’re looking at should be bundled with the OS that was installed on the switch and is not expected to be independently downloadable. If you are running SwitchDev, you should run the firmware noted in the release notes for your kernel version:Contribute to Mellanox/mlxsw development by creating an account on GitHub.The wiki release notes seem to mention very old firmware versions. Oh well, everything seems fine with the newer firmware.Powered by Discourse, best viewed with JavaScript enabled"
1159,mellanox-cumulus-linux-mtu,"I recently bought Mellanox SN2010 Cumulus Linux. Is there any recommendation from mellanox on how i must configure MTU on the ports for my 9000 bytes VMWare’s jumbo frames?Or i should just go with 9216 bytes as it the max MTU that supported by mellanox.ThanksHello Firman,We would recommend to set the MTU on the switch to 9216 to allow COS and VLAN tags.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1160,whats-the-eta-for-the-debian-11-bullseye-version-of-the-ofed-drivers,"Now that Thinkparq has released their BeeGFS version to support Debian 11 (released on August 14th, 2021), I am now wondering when I can use the corresponding OFED suite to utilize RDMA with BeeGFS.Cheers, JanHello jbehrend,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Our upcoming MLNX_OFED 5.6 GA release will provide support for Debian 11.2 (Bullseye). The release is targeted for sometime next week (1st week of May)Thank you and regards,
~NVIDIA Networking Technical SupportHello Martijn,thanks for your prompt answer!On Wed, 2022-04-27 at 00:28 +0000, Martijn van Breugel via NVIDIA
Developer Forums wrote:Our upcoming MLNX_OFED 5.6 GA release will provide support for Debian
11.2 (Bullseye). The release is targeted for sometime next week (1st
week of May)This is fantastic, very much looking forward to this release!Cheers, JanYou’re welcome…This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1161,how-do-i-monitor-when-configuring-ovs-kernel-hardware-on-connectx-6-lx,"In the case of DPU, the telemetry function is available to collect metrics.When configuring HWOL through ConnectX-6 LX and OVS I want to monitor via ovs eporter and TC Exporter.However, I can’t see the thresholds for NICs, so if there is an API or solution to get them, please share.Powered by Discourse, best viewed with JavaScript enabled"
1162,bluefield-network-latency-vs-host-network-latency,"Hi all,
I have a question about the data path,
I have setup Bluefield 2 in the embedded mode,
In another server, I tried to ping the host and Bluefield2.
I was expecting getting lower delay on Bluefield2, because the packets should go to Bluefield 2 first then Host os. but looks like the network latency of Bluefield is 2 times moreAny idea why this is the case? Or ping is not a good metric to measure the network latency ?— Bluefield2 ping statistics —
33 packets transmitted, 33 received, 0% packet loss, time 32732ms
rtt min/avg/max/mdev = 0.232/0.296/1.318/0.182 ms— Host os ping statistics —
34 packets transmitted, 34 received, 0% packet loss, time 33781ms
rtt min/avg/max/mdev = 0.113/0.125/0.300/0.035 msI was expecting getting lower delay on Bluefield2, because the packets should go to Bluefield 2 first then Host os. but looks like the network latency of Bluefield is 2 times moreUpdate:
In my previous experiment I was pinging ens2f0 on the host which is the intel nic of the host, when I ping the physical interface ens2f0 instead, the delay on bluefield2 is 50 ms lower.ens10f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet xx.xx.226.xx  netmask 255.255.254.0  broadcast
inet6 fe80::6a05:caff:fee1:da1c  prefixlen 64  scopeid 0x20
ether 68:05:ca:e1:da:1c  txqueuelen 1000  (Ethernet)
RX packets 4146194  bytes 353873668 (353.8 MB)
RX errors 0  dropped 178  overruns 0  frame 0
TX packets 6721556  bytes 10118944545 (10.1 GB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
device memory 0xa3a00000-a3afffffens2f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet xx.xx.231.xx  netmask 255.255.255.0  broadcast
inet6 fe80::bace:f6ff:fea8:82fa  prefixlen 64  scopeid 0x20
ether b8:ce:f6:a8:82:fa  txqueuelen 1000  (Ethernet)
RX packets 1632295  bytes 1448764119 (1.4 GB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 2312763  bytes 1786897169 (1.7 GB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0ens2f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
inet xx.xx.231.xx  netmask 255.255.255.0  broadcast
inet6 fe80::bace:f6ff:fea8:82fb  prefixlen 64  scopeid 0x20
ether b8:ce:f6:a8:82:fb  txqueuelen 1000  (Ethernet)
RX packets 1036823  bytes 1010709940 (1.0 GB)
RX errors 0  dropped 0  overruns 0  frame 0
TX packets 60222  bytes 7743265 (7.7 MB)
TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0Powered by Discourse, best viewed with JavaScript enabled"
1163,mellanox-connectx-6dx-receiving-packets-out-of-order,"Test-setup:Sending packets at speed 200,000 pps.
Receiving Packets at Mellanox Management Tool “Mlx5Cmd.exe”.
Receiving all packets but with random out of order packets (some packets are jumbled) on some test runs. (in 1/5 runs)Hello Burot,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Make sure that you following all performance tuning guidelines when using the ConnectX-6 Dx on Windows OSses → https://docs.nvidia.com/networking/display/winof2v280/Performance+TuningFor resolving out-of-order packets, please review the following section in the WinOF-2 UM → https://docs.nvidia.com/networking/display/winof2v280/Configuring+the+Driver+Registry+Keys#ConfiguringtheDriverRegistryKeys-BasicRegistryKeysBasicRegistryKeys

image1053×162 9.92 KB
Be aware, Windows DPDK is not mature and using it with our ConnectX-6 Dx has its limitations.Thank you and regards,
~NVIDIA Networking Technical SupportHi MvB,Followed both documents & set necessary parameters including performance but still receiving out-of-order packets.Ran with “ReceiveBuffers” value - 512(default), 1024 and 4096. Received out-of-order packets in all cases.Thanks,
BurtoHello Nvidia Team,Any updates on this thread ?We switched our setup to Linux (Ubuntu 20.04), still see same issue with adapter.
Please suggest.Linux Env details:Hi,Does this also exist with CX-6 when used with Linux ?Thanks
SekarI am experiencing the same thing on ubuntu 22.04 with MLNX_OFED_LINUX-5.9-0.5.6.0-ubuntu22.04-x86_64Any workaround?Powered by Discourse, best viewed with JavaScript enabled"
1164,vlan-aware-linux-bridging-is-not-functional-on-connectx4lx-card-unless-manually-put-in-promiscuous-mode,"When an adapter is configure for vlan-aware linux bridging, all traffic stops flowing on the bridge. Both, untagged and tagged traffic is affected. The same configuration works as it should with non mellanox cards.To restore traffic flow on vlan-aware bridge, Mellanox card needs to be manually put in promiscuous mode by issuing: “ip link set dev ens6f0np0 promisc on”The second interface on the same card on NON vlan-aware bridge enters promisc mode automatically once added to the bridge with no user interaction.Any ideas will be appreciated.Configuration details:OS: Debian 10 Buster + ProxMox (PVE 6.4), latest 5.12.12 Linux kernelNIC/FW:root@pve-bfs-1:~# mlxfwmanagerQuerying Mellanox devices firmware …Device #1:Device Type: ConnectX4LXPart Number: MCX4121A-ACU_AxDescription: ConnectX-4 Lx EN network interface card; 25GbE dual-port SFP28; PCIe3.0 x8; UEFI Enabled; tall bracketPSID: MT_0000000266PCI Device Name: /dev/mst/mt4117_pciconf0Base MAC: Versions: Current AvailableFW 14.30.1004 14.30.1004PXE 3.6.0301 3.6.0301UEFI 14.23.0017 14.23.0017Status: Up to dateDriver:root@pve-bfs-1:~# modinfo mlx5_corefilename: /lib/modules/5.12.12-1-edge/kernel/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.kolicense: Dual BSD/GPLdescription: Mellanox 5th generation network adapters (ConnectX series) core driverauthor: Eli Cohen eli@mellanox.comsrcversion: E69AFAD4870C439C8F80D4Calias: auxiliary:mlx5_core.ethalias: pci:v000015B3d0000A2DCsvsdbcsci*alias: pci:v000015B3d0000A2D6svsdbcsci*alias: pci:v000015B3d0000A2D3svsdbcsci*alias: pci:v000015B3d0000A2D2svsdbcsci*alias: pci:v000015B3d00001021svsdbcsci*alias: pci:v000015B3d0000101Fsvsdbcsci*alias: pci:v000015B3d0000101Esvsdbcsci*alias: pci:v000015B3d0000101Dsvsdbcsci*alias: pci:v000015B3d0000101Csvsdbcsci*alias: pci:v000015B3d0000101Bsvsdbcsci*alias: pci:v000015B3d0000101Asvsdbcsci*alias: pci:v000015B3d00001019svsdbcsci*alias: pci:v000015B3d00001018svsdbcsci*alias: pci:v000015B3d00001017svsdbcsci*alias: pci:v000015B3d00001016svsdbcsci*alias: pci:v000015B3d00001015svsdbcsci*alias: pci:v000015B3d00001014svsdbcsci*alias: pci:v000015B3d00001013svsdbcsci*alias: pci:v000015B3d00001012svsdbcsci*alias: pci:v000015B3d00001011svsdbcsci*alias: auxiliary:mlx5_core.eth-repalias: auxiliary:mlx5_core.sfdepends: tls,pci-hyperv-intf,mlxfwretpoline: Yintree: Yname: mlx5_corevermagic: 5.12.12-1-edge SMP mod_unload modversionsparm: debug_mask:debug mask: 1 = dump cmd data, 2 = dump cmd exec time, 3 = both. Default=0 (uint)parm: prof_sel:profile selector. Valid range 0 - 2 (uint)network/interfaces:auto loiface lo inet loopbackiface ens6f0np0 inet manualiface ens6f1np1 inet manualmtu 9000auto vmbr0iface vmbr0 inet staticaddress 10.0.0.11/24gateway 10.0.0.1bridge-ports ens6f0np0bridge-stp offbridge-fd 0bridge-vlan-aware yesbridge-vids 2-4094auto vmbr1iface vmbr1 inet staticaddress 10.0.1.11/24bridge-ports ens6f1np1bridge-stp offbridge-fd 0mtu 9000Hi Andrew,Thank you for posting your question on our community.Based on the information shared, you are running Inbox driver(one that comes default with the OS). In that case, it would be great if you could reach out to OS vendor.To test with MLNX OFED driver we provide, please install a supported OS and kernel version. Currently, MLNX OFED doesn’t support OS: Debian 10 Buster + ProxMox (PVE 6.4).In order to install the OFED driver for one of the supported OS, please visit → Linux InfiniBand DriversTo check the list of Supported OS, please visit → https://docs.mellanox.com/display/MLNXOFEDv541030/General+Support#GeneralSupport-SupportedOperatingSystemsApart from the kernel versions listed in the above link, we also support upstream vanilla kernel without any customizations to it → Upstream Releases/Inbox DriversIt would be great if you can validate your results with MLNX OFED.Thanks,Namrata.Powered by Discourse, best viewed with JavaScript enabled"
1165,cannot-collect-traffic-on-grafana,"Q: I added to the datasource as an InfluxDB and created a graph according to the documentation but there is no data on graph.A: Check to see the influxdb database itself is actually populating. Your NetQ agents may not be streaming data to the influxdb instance correctly.Next, enable the statsd service:Then configure the service for the NetQ Agent:You can check the influxdb entries by using the following command within influxdb:Powered by Discourse, best viewed with JavaScript enabled"
1166,high-on-multiple-gpio-pins,"hi
Iam trying to send out signals from jeson to aurdino , but not able to make 2 gpio pins high at the same time . THe moment i make one up the other goes downTried with rpi.gpio and sudo echo 0 > /sys/class/gpio/gpio232/valueon pins 23, 24, 7, and 8 and 18 same problemIs this a Jetson Nano issue? If so, I can move this to the proper forum for you.Its sorted out nowHappy to hear.Powered by Discourse, best viewed with JavaScript enabled"
1167,when-will-ofed-installer-be-available-for-debian-10-6,"We run Proxmox, and just before I installed the 12 (!) 100g Mellanox cards I just purchased I ran standard proxmox update…which upgraded OS from Debian 10.3 to 10.6 so now OFED will not install because “Error: The current MLNX_OFED_LINUX is intended for Debian 10.3”so…major upgrade project dead in the water until we can install drivers.Hello John,Thank you for posting your question on the Mellanox Community.Unfortunately we do not currently have a version of the Mellanox OFED driver available for Debian 10.6 we apologize for any inconvenience.For any further questions please contact Mellanox support with a valid support contract and we will be happy to help answer them.Thank you,Mellanox Technical Support.I want to re-open this discussion.We use Proxmox, and we have purchased about 20 Mellanox COnnextx-4 family 100G cards.The issue we are having is that proxmox is based on Debian “even” versions.10.2, 10.4, latest is 10.6Mellanox OFED drivers support the “odd” versions. 10.0, 10.3, 10.6Is there any way around this? Does anyone know is I drop back to previous proxmox version (so on debian 10.4) will the latest Mellanox ofed (for 10.5) work?Powered by Discourse, best viewed with JavaScript enabled"
1168,the-role-of-action-skbedit-ptype-host,"Hi Mlnx support team,I am testing OVS with Multiple VMs using Mlnx CX-5 Card. During ping when it sends ARP request with broadcast MAC address, i saw and action is offloaded to change the ptype to host. Your card also does support ‘action skbedit ptype host’ using TC rule only.My question is what this action does actually with the packet? Because i didn’t see any change in the packet after receiving in VM 2 using tcpdump. Please provide some details what is mean by ptype host, what is change in the packet.Secordly, your card doesn’t support skbedit ptype broadcast and multicast. Can you provide the reason?RegardsFarhat UllahHi,All supported features are explained in the below pagehttps://docs.mellanox.com/pages/viewpage.action?pageId=39285091Thanks,SamerThanks Samer Kayouf,This link is very helpful but it doesn’t cover the action skbedit ptype. Can you share some details on this too?RegardsFarhat UllahPowered by Discourse, best viewed with JavaScript enabled"
1169,bluefield-2-not-powering-up,"Hi,
We have 2 “MBF2H516A-CENOT” Bluefield cards, when connected with or without auxiliary PCIe power cable as prescribed by External Power Supply Connector pin description here: https://docs.mellanox.com/display/BlueField2DPUENUG/Pin+DescriptionWe see neither  LED activity once the server comes up nor the lspci shows any entry for this card.
How do we debug this ?Thanks.Hi do you see anything relevant in syslog or in the output of dmesg? What host OS are you using?Powered by Discourse, best viewed with JavaScript enabled"
1170,mellanox-sn2010-no-connectivity-between-two-hosts-on-same-subnet-through-l3-gateway-on-mellanox-interface-with-vlan,"Checked configuration on hosts, all is working. But no responses from Mellanox gateway interface for ARP or who-has _gateway from source host IP.Hosts are SAN system and ubuntu server connecting via Ethernet 1/18 on mellanox switch. Have L2 and L3 configurations set on mellanox but hosts cannot ping gateway or get arp response for their requests from the mellanox.Please refer this documentation as reference of your case:https://community.mellanox.com/s/article/howto-configure-inter-vlan-routing-on-mellanox-switchesTo enable L3 function, you need to enable “ip routing” globally.Powered by Discourse, best viewed with JavaScript enabled"
1171,sx6036-proxy-arp-mtu-and-roce-ib-pass-through,"Shalom!I’ve got this funny 6036G switch and I’m trying to setup a gateway between ipoib and ethernet hosts. I’m a bit perplexed on what mtu should be picked. When going through KB articles and MLNX-OS guide, I’m seeing contradicting info. First of all, proxy-arp switch interface can have max 4092 mtu. That’s totally fine with me (have decided to use 4090), but here comes the hard part. First of all, manual mentions the following command: interface ib 1/10 mtu 4000, which can’t be executed on the switch because IB interfaces can only have fixed mtus - 1k, 2k, 4k. Secondly, ethernet interface on the host won’t select active_mtu of 4096 unless you set a very high mtu on the interface – it’s currently set to 4200 over here. But on the switch side mtu for ethernet port is set to 4090. When I set mtu on the host with **ip li set enp5s0 mtu 4090**, it downgrades **ibv_devinfo |grep active_mtu** to 2048.As you see, I’m completely confused. What mtu should I actually set so that the entire thing works as it should? Obviously, I’m aiming at highest possible mtu with this switch as I’d like to push large I/O over it.My current mtu mess is as following:4200 (linux) - 4096 (driver/active_mtu) HCA ~~~ 4090 (EN port) switch - 4090 (switch proxy-arp interface) - 4096 (IB port) switch ~~~ HCA 4096 (driver/active_mtu) - ipoib 4090 (linux)Second question is about the gateway functionality that I’m not sure I completely understand. We’ve got some stuff running under Infiniband over here which I’m trying to gradually move over ROCE, but I’m a bit lost as nowhere it says switch gateway is able to “commutate” RDMA traffic over gateway/proxy interface. Do I understand correctly that RC won’t be passed through proxy-arp and that only plain dumb IP traffic is able to traverse it?Currently, ib_write_bw , rping tools are outputting funny messages when run between ipoib and roce hosts.Thanks!Hi,The below article has examples of how to change the MTU to 4K on the switch.https://community.mellanox.com/s/article/howto-configure-infiniband-gateway-ha--proxy-arp-xTo change the mtu on the host use “ifconfig” instead of ip li setWhat is funny messages when running ib_write_bw?the SX6036G doesn’t support transporting RDMA from IB<->Eth.only IPoIB <-> IPoEth is supportedThis is incorrect. They’re using the same kernel calls and ifconfig has been deprecated for ages.23: eth5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9200 qdisc mqprio state UP mode DEFAULT group default qlen 1000max_mtu: 4096 (5)active_mtu: 4096 (5)max_mtu: 4096 (5)active_mtu: 4096 (5)max_mtu: 4096 (5)active_mtu: 2048 (4)max_mtu: 4096 (5)active_mtu: 4096 (5)23: eth5: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 4096 qdisc mqprio state DOWN mode DEFAULT group default qlen 1000As you see, active_mtu has been lowered even when though ifconfig was used.Thanks. Exactly what I presumed.Powered by Discourse, best viewed with JavaScript enabled"
1172,rshim-is-failing-to-enable-intx-and-wont-attach-to-bluefield-2-dpu,"I am getting an error stating that RShim is failing to enable INTx interrupts on the bluefield 2 device and so it won’t attach. I am looking for a way to fix thisHello wk10, and welcome to the NVIDIA Developer Forums!This could be happening if power or airflow requirements are not being met.
Please review the following link for power requirements for the Bluefield-2 DPU:
https://docs.nvidia.com/networking/display/BlueField2DPUVPI/SpecificationsPlease do also note that this device is not intended for use in a desktop/tower system. These systems typically cannot meet the airflow requirements for this device. The device requires sufficient airflow to operate, or the chips will shut down (and possibly become damaged). Please do reach out to your sales representative for further details on airflow requirements.Here is a partial list of systems in which Bluefield-2 DPUs have been tested and validated:
https://docs.nvidia.com/networking/display/BlueField2DPUVPI/Supported+Servers+and+Power+CordsAgain, please do reach out to your sales representative if you have questions regarding compatibility with other systems.If:And you are still unable to get the device to function as expected, we recommend creating a support case with our Enterprise Support team so we can assist you further.Thanks,
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1173,sn2100-ptp-sync-grandmaster,"I have two servers that I am trying to sync through the MSN2100 switch. I am doing the instructions in the manual as well as here:
https://mellanox.my.site.com/mellanoxcommunity/s/article/ieee-1588-ptp-on-spectrum-switches-running-onyxI am able to sync to one PTP server, but am unable to sync to the other. I believe it is because of the “interface ethernet 1/# ip .” part. Both servers have a similar ip 192.168.1.XXX. I can only set one IP that will satisfy the IPs. When I try to set the other port’s IP, I get subnet overlap or already configured. If I set a separate IP, itll send the PTP to the port, but the server wont sync to it.Both PTP servers are using linuxptp/ptp4l to sync to the switch.Any help would be appreciated.PTP can also be configured on a VLAN with trunks instead of layer-3 interfaces. You would need to configure the VLAN with an IP, configure the switchports as hybrid/trunk ports, add the VLAN to the truinks, enable PTP on the VLAN, and finally enable PTP on the interfaces.So I started doing that, and it started working, but for some reason, the server connected to the grandmaster syncs to the switch instead of the grandmaster. The grandmaster and server (32 and 64 respectively) have a lower priority number than the PTP (128). Any idea why the ptp keeps getting selected as grandmaster? Assuming 'show ptp’s GMC identity field is the overall grandmasteralso, when i do “show ptp,” it shows that both ports are master. Shouldn’t one be master and one be slave?Yes, the switchport attached to the GM clock should be master, and the other ports should be slave.The PTP communication happens between each pair of devices, so on each side of each link, one will be master and one will be slave.Yes, the GMC identity will be the address of the clock that time is originating from, not the neighboring clock.A support case would be the best way to get help when deeper troubleshooting is required.I dont have the GM clock directly connected to the switch due to constraints. With that, the server connected to the GM Clock/PTP time source should be the master while the switch and other server should be a slave but “show ptp” shows otherwise as both ports being master (thus one should be master and the other should be slave).I am trying to understand why the GMC identity isn’t the GM Clock and is instead the clock identity in the switchI followed the instructions for PTP switch interface here
https://docs.nvidia.com/networking/pages/viewpage.action?pageId=19807036Update, it was a firewall issue on the server. Now the PTP server keeps toggling between grandmaster clock between itself and server 1.  @chada1 is there any reason why? I tried using force-master as well as amt, but those seem to block the desired master instead.I am hoping to get 00:21:32:FF:FE:01:B9:90 to be the master clock. It only identifies the server as GMC when its un-calibrated. I have the GMC have a lower clock class, accuracy, priority1 and 2, but it still isn’t syncing.Powered by Discourse, best viewed with JavaScript enabled"
1174,cross-channel-communication-capability-for-connectx-4,"Hi, I would like to consult a question regarding to cross channel communication. I am currently using the connectx-4 card. According to the user manual, cards newer than connectx-3 should support cross channel capability for creating managed QP. However, the result of query device(ibv_exp_query_device) returns that the exp_device_cap_flags to be (IBV_EXP_START_FLAG << 30), which doesn’t equal to IBV_EXP_DEVICE_CROSS_CHANNEL as mentioned in the manual. Besides, the ibv_exp_create_cq for creating CQ with flags for cross channel communication cannot work.Can anyone give me some clues how to enable and use such capability? Is this a reason caused by the new version mallanox OFED driver?Thanks in advance!Cross Channel feature supported by v4.9 LTS version of the MOFED. Newer version of MOFED based on rdma-core package and doesn’t use experimental verbshttps://docs.mellanox.com/display/rdmacore50/Cross+ChannelPlease, check RDMA programming manual how to use ithttps://www.mellanox.com/related-docs/prod_software/RDMA_Aware_Programming_user_manual.pdfPowered by Discourse, best viewed with JavaScript enabled"
1175,mcx556a-edat-direct-connection-via-ethernet-unable-to-reach-more-than-73gbit-s,"Hello,​I’m trying to reach 100G over 2 directly connected MCX556A cards. I am using OFED 5.4.1 on Linux Centos 7.9 with stock kernel (3.10.0-1160).​I have executed mlnx_tune and set additional parameters:​sysctl net.core.rmem_max=2147483647sysctl net.core.wmem_max=2147483647sysctl net.ipv4.tcp_rmem=“4096 87380 2147483647”sysctl net.ipv4.tcp_wmem=“4096 65536 2147483647”sysctl net.core.netdev_max_backlog=250000sysctl net.ipv4.tcp_no_metrics_save=1sysctl net.ipv4.tcp_congestion_control=htcpsysctl net.ipv4.tcp_mtu_probing=1sysctl net.core.default_qdisc=fq​The hosts are 2x AMD EPYC 7542 with 1 TB Memory, htop and top show utiliziation during tests of 1-2%. The CPU is configured for 4 NUMA nodes, and the adapter is bound to the corresponding one. The Adapter is connected via PCIex4 x16. RPS and XPS cpus are pinned.The eth interfaces are set to mtu 9000.​I’m testing with iperf, iperf3 and raw_ethernet_bw. The maximum I was able to achieve was 73 Gbit/s. iperf and iperf3 are run as separate processes, I have tried from 2 to 8 processes and everytime the same result. I do not see some retries from iperf3, but they are around 200 - 500 for a 30 second test.​I did the same test with a switch in between (Dell S5232F-ON) there I had much high retries, around 50k.​I have tested by reducing the link to 50G and 25G and both times I can reach the maximum speeds (46.3 Gbit and 23.2 Gbits) - so I would expect 4x 23.2 Gbits, so around 92.8 Gbits.​Locally (lo interface) I can easily reach 190 Gbits Send/Receive.I have followed the tuning guidelines:​https://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptershttps://community.mellanox.com/s/article/how-to-tune-an-amd-server--eypc-cpu--for-maximum-performance​I will test still a different cable, but mlxlink doesn’t report any issues.What else can be checked? How can I find out WHAT is limiting the performance here?How can I test a loopback configuration?​Hi Rosenstein,Thank you for posting your question on our community.As you mentioned these are AMD CPU based hosts, can you please confirm the below two requirements are met as these help improve performance on AMD based CPU’s:a. GRUB command line used have “iommu=pt” . Please share output of #cat /proc/cmdlineb. Are all DIMM’s populated?In addition, as you are using OFED 5.4, I believe you have the latest firmware installed unless you installed the driver using ""–without-fw-update "" flag.In case you have the above parameters in place and still see reduced performance, we will open a support ticket as I see your account holds a valid support contract.Thanks,Namrata.Thanks,Namrata.Edit: it seems to actually have worked, I can now reach 92.4 GBit/s via ethernet, same as via RDMA@Namrata Motihar​Hi, very sorry I have not responded earlier, I actually did not see your post!I have added iommu=pt, but it did not change anything - we are not using SRVIO - plain bare metal hardwarecmdline:BOOT_IMAGE=/vmlinuz-5.10.37 root=/dev/mapper/cl-root ro crashkernel=896M rd.lvm.lv=cl/root net.ifnames=0 biosdevname=0 scsi_mod.use_blk_mq=1 dm_mod.use_blk_mq=y mitigations=off console=tty0 console=ttyS1,115200 iommu=ptPlease disregard the 5.10.37 kernel here, I have rebooted into the up2date kernel, cmdline is the sameb) 8 Dimms are populated per CPU:description: DIMM DDR4 Synchronous Registered (Buffered) 3200 MHz (0.3 ns)product: HMAA8GR7MJR4N-XNvendor: Hynix Semiconductor (Hyundai Electronics)physical id: 17serial: 933237DFslot: B8size: 64GiBwidth: 64 bitsclock: 3200MHz (0.3ns)I can reach 99 Gbit/s via Infiniband (ib0) and 91 Gbit/s via Ethernet (eth2) when using ib_read_bw / ib_write_bwUsing iperf3 or iperf I max out around 60 - 70 Gbits (mtu 9000)We do have an active support contract, currently until next week.Powered by Discourse, best viewed with JavaScript enabled"
1176,slow-or-failing-link-negotiation-on-connectx-6-dx-sfp28-card,"My ConnectX-6 DX takes anywhere from 2 to 15 mins to negotiate the link on FS S5860-20SQ switch and completely fails to establish the link on Ubiquiti USW Enterprise XG 24 switch. Once the link is established, the card works fine with no reliability or performance issues.The card has the latest FW and was installed on Gigabyte E251-U70 server and then moved it to a PC to rule out host issues. I also tried to use disable Auto negotiation and configure link mode force (–link_mode_force with mlxlink) and set the same duplex, speed, FEC parameters on both sides of the connections, it didn’t help much. Link establishment in Force mode is a bit faster but still takes minutes.We also have ConnectX-4 LX card that works fine with either of the switches and link establishments takes a few seconds.It would be great to know what’s different in link negotiation context between 6DX and 4LX and what can be done about it.Driver Version : 2.80.25134.0Firmware Version : 22.32.1010Port Number : 1Bus Type : PCI-E 8.0 GT/s x4Link Speed : 25.0 Gbps/Full DuplexPart Number : MCX621102AC-ADATSerial Number : MT2144X07371Device Id : 4125Revision Id : 0Current MAC Address : 08-C0-EB-8B-DC-58Permanent MAC Address : 08-C0-EB-8B-DC-58Network Status : ConnectedAdapter Friendly Name : Ethernet 6Port Type : ETHIPv4 Address #1 : 10.0.50.99IPv6 Address #1 : fe80::3964:ec49:84c4:1718%15Cable I tried (The same cables work perfectly for ConnectX-4LX card):Mellanox MCP2M00-A00AE30N (25GbE DAC)Mellanox MFA2P10-A005 (25GbE AOC)Hello,Unfortunately, neither of the switches that you are attempting to link with the adapter appear within the Firmware Compatible Products listings of the ConnectX-4 LX or the ConnectX-6 DX’s Release Notes. As such, they have not been validated for use with these adapters. Please contact the switch vendors for support.For a complete listing of tested and validated switches, cables, and modules, please see the following links:Firmware Compatible Products section of the ConnectX-6 DX Firmware Release Notes:https://docs.nvidia.com/networking/display/ConnectX6DxFirmwarev22321010/Firmware+Compatible+ProductsFirmware Compatible Products section of the ConnectX-4 LX Firmware Release Notes:https://docs.nvidia.com/networking/display/ConnectX4LxFirmwarev14321010/Firmware+Compatible+ProductsThank you,-Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
1177,tcpdump-commands-not-working-on-sn2410,"Hi,I am following the following reference for using tcpdump on the SN2410 switch.
“Mellanox Interconnect Community”The following command doesn’t work on the switch :tcpdump -i  eth1.0Can you please tell me if this document is targeted for a particular switch or is it something wrong in my configuration?Here are the details of my ethernet port:
Eth1/1:
Admin state                      : Enabled
Operational state                : Up
Last change in operational status: 1d and 15:33:40 ago (7 oper change)
Boot delay time                  : 0 sec
Description                      : N\A
Mac address                      : ec:0d:9a:5f:d8:3c
MTU                              : 1500 bytes (Maximum packet size 1522 bytes)
Fec                              : auto
Flow-control                     : receive off send off
Actual speed                     : 10 Gbps
Auto-negotiation                 : Disabled
Width reduction mode             : Unknown
Switchport mode                  : hybrid
MAC learning mode                : Enabled
Forwarding mode                  : inherited cut-throughTelemetry sampling: Disabled   TCs: N\A
Telemetry threshold: Disabled  TCs: N\A
Telemetry threshold level: N\AThank you,
ParidhikaHi
The article was for L3 ports only.
In this case - you have a L2 port (in switchport mode)
Please try the belowtcpdump -i ethsl1p1
In any case the traffic you will see - will be only cpu ingress/egress traffic, not traffic forwarded by HWand if it’s not working  - please provide the switch software version:show version conciseHi,It doesn’t work. Here are the details:ualloc-mlnx2 [standalone: master] (config) # tcpdump -i ethsl1p1
tcpdump: ethsl1p1: No such device exists
(SIOCGIFHWADDR: No such device)
ualloc-mlnx2 [standalone: master] (config) # show version concise
X86_64 3.6.8190 2018-11-05 19:20:10 x86_64Thank you,
ParidhikaThis version is tool old.
Plrase try using 3.9.x or 3.10.xHi,Can you please provide me a link to the instructions to upgrade the version?Thank you,
ParidhikaPlease open a case with the support team by sending an email to:Networking-support@nvidia.comPowered by Discourse, best viewed with JavaScript enabled"
1178,slow-transfer-between-mellanox-sn3700c-and-connectx-6-dx-adapter-on-windows-10,"Howdy all.
We have a business need to sync different sets of 16TB of High res media daily.
These are SuperMicro Chasis devices running Windows 10 Enterprise with ConnectX-6 Dx Adapters.
When transferring a file(s) from one Win box to another over the 100Gbs connection, we are only getting 1.2GBs or 9.8Gbs.
The switch is essentially flat, and the ConnectX-6 are also running default settings.It would appear I need to set up RDMA Over Converged Ethernet (RoCE), but all of the pages I find on how to do this are either not working or use a Linux box.Does anyone have a set of instruction on how to set up RDMA Over Converged Ethernet (RoCE) on WIN10 OR any other configuration changes I can make to increase throughput?If you still need assistance, please open a support case at https://support.mellanox.com/s/contact-support-pageRegards,JonPowered by Discourse, best viewed with JavaScript enabled"
1179,ib-routing-engine-for-small-fabric-with-just-two-switches-is-minhop-sufficient,"Hello,
we setup a small Infiniband fabric  using two switches QM8700. We’ll connect the switches on ports 1-6 and use 7-40 to connect host ports. Can we just use the default  minhop routig engine or do we need to use something more advanced like LASH to get this working? I want to make sure that we get routes passing between switches, i.e. that all hosts on switch A can reach hosts on switch B. ;-)  We’ll run the subnet manager on the QM8700 switch itself, with just about 40 host ports this should be fine.thank you,HeinerHi Heiner,It’s fine to run Minhop routing engine in the network you mentioned because there is no possibility of  a credit loop.
You can find more information from man help:Thanks,
YuyingThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1180,release-notes-for-nvidia-bright-cluster-manager-9-2-7,"Release notes for Bright 9.2-7== General==
=New Features==Known issues==Deprecated features=== CMDaemon==
=Improvements==Fixed Issues===Bright View==
=Fixed Issues===Machine Learning==
=New Features===cm-clone-install==
=New Features==Fixed Issues===cm-docker-registry-setup==
=Fixed Issues===cm-kubernetes-setup==
=Improvements===cm-scale==
=Fixed Issues===cm-wlm-setup==
=New Features==Fixed Issues==cmsh==
=Improvements===slurm22.05==
=Improvements=Powered by Discourse, best viewed with JavaScript enabled"
1181,installation-error-cannot-create-dev-rshim0-boot,"Hi new at this but following the sdkmanager cli guide it gives an error:info: sudo bfb-install -bfb <file.bfb> --rshim rshim0 --config ./bf.cfig
error: sh: 1: cannot create /dev/rshim0/boot: Invalid Argument
info: failed to push BFB
info: sudo ifconfig tmfifo_net0 192.168.100.1 netmask 255.255.255.0 up
info: waiting for response…Timing out ofcourse, it can’t send the boot streamUbuntu 20.04 Host (fresh install), Bluefield BF2H322AHi TinkerFrank,Thank you for posting your query on NVIDIA community.I would like to inform that the command line used to run the command is incorrect. You will have to add the actual path of the BFB image that has been installed on the Host rather than using “<file.bfb>”. The < > are just to explain that needs to be added as a parameter for the -bfb flag.You can download the appropriate BFB image from NVIDIA DOCA SDK | NVIDIA Developer
Please scroll down the page to the section "" BlueField DPU Runtime and Driver Downloads"" and click the “Bluefield Software” Tab.Thanks,
Namrata.bluefield_install1200×543 81 KB
I ommitted it because I had to type it over, tried again on 22.04 but same error :(Some more info:which according to: Mellanox Interconnect Community, indicates: “If the lspci output of the HCA shows “Flash Recovery”, this means that the flash is empty or the FW in the flash does not work.”
But the whole point is that I am trying to flash using rshim0 but doesnt work, is there another way?Powered by Discourse, best viewed with JavaScript enabled"
1182,driver-bug-in-linux-5-10-after-upgrade-from-4-19-lots-of-illegal-loopback-warnings-in-dmesg,"I just upgraded a host with a MT27710 Family [ConnectX-4 Lx] adapter from Linux 4.19 to 5.10, and now dmesg is filling up with:bond1: (slave enp179s0f0np0): An illegal loopback occurred on slaveCheck the configuration to verify that all adapters are connected to 802.3ad compliant switch portsThese messages do not occur on Linux 4.19, switch seems correctly configured and connection appears to work. So is this just a driver bug in the 5.10 kernel?Hello Eli,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, the issue looks related to a firmware issue of the adapter. On the RedHat site, there is KB article related to this → Bonding driver reports illegal loopback occurred on adapter when using Mellanox ConnectX-4 (mlx5) NICs - Red Hat Customer PortalWe recommend to upgrade the f/w of the adapter to the latest available. You can download the latest f/w through the following link → Firmware for ConnectX®-4 Lx ENThank you and regards,~NVIDIA Networking Technical SupportI can confirm after upgrading tofirmware-version: 14.31.1014 (MT_2420110004)fromfirmware-version: 14.26.1040 (MT_2420110004)the issue appears to have resolved, thanks.Powered by Discourse, best viewed with JavaScript enabled"
1183,failed-send-access-register-me-icmd-operational-error,"Hi,
I am just getting started with BlueField-2 and DOCA and trying to run the example applications.
I am trying to create a Virtual Function by following the documentation on:
NVIDIA DOCA Virtual Functions User Guide
However, I am stuck at step #3 of chatper 2: Prerequisites which says to set the VFs to trusted mode by running this command on the DPU:
$ mlxreg -d /dev/mst/mt41686_pciconf0 --reg_id 0xc007 --reg_len 0x40 --indexes ""0x0.0:32=0x80000000"" --yes --set ""0x4.0:32=0x1""
as the previous command fails with the following output:any idea how to solve this problem ?Check VFs/Sfs on device bymst status -v
/opt/mellanox/iproute2/sbin/mlxdevm port showDelete VFs/SFsPower cycle server/DPURepeat  below,
Virtual Functions :: NVIDIA DOCA SDK DocumentationThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1184,bad-p-key-how-to-run-down-this-issue,"Hi ,I have these log messages in UFM log.What do they mean ? And how can I fix this issue ?Jul 21 15:24:46 334009 [FF6CC700] 0x01 → log_trap_info: Bad P_Key:0xcc2f on SL:0 from LID1:80 (GUID 0x946dae030057f07a) QP1:0x104c to LID2:83 (GUID 0x946dae030093e282) QP2:0x1
Jul 21 15:24:46 334024 [FF6CC700] 0x02 → log_notice: Reporting Generic Notice type:2 num:259 (Bad P_Key (switch external port)) from LID:110 GID:fe80::900a:8403:7b:91c0
Jul 21 15:24:47 437691 [AEE3700] 0x01 → log_trap_info: Received Generic Notice type:2 num:259 (Bad P_Key (switch external port)) Producer:2 (Switch) from LID:110 TID:0x00006dc600000103
Jul 21 15:24:47 437706 [AEE3700] 0x01 → log_trap_info: Bad P_Key:0xcc2f on SL:0 from LID1:125 (GUID 0x946dae030070dee2) QP1:0x104c to LID2:83 (GUID 0x946dae030093e282) QP2:0x1
Jul 21 15:24:47 437721 [AEE3700] 0x02 → log_notice: Reporting Generic Notice type:2 num:259 (Bad P_Key (switch external port)) from LID:110 GID:fe80::900a:8403:7b:91c0
Jul 21 15:24:48 370384 [FCEC7700] 0x01 → mcmr_rcv_join_mgrp: ERR 1B11: method = SubnAdmSet, scope_state = 0x1, component mask = 0x0000000000010083, expected comp mask = 0x00000000000130c7, MGID: ff12:401b:e394::ffff:ffff from port 0x946dae030070ed4a (gmckee-vm1 HCA-1)
Jul 21 15:24:48 541407 [24F17700] 0x01 → log_trap_info: Received Generic Notice type:2 num:259 (Bad P_Key (switch external port)) Producer:2 (Switch) from LID:110 TID:0x00006dd200000103
Jul 21 15:24:48 541428 [24F17700] 0x01 → log_trap_info: Bad P_Key:0xcc2f on SL:0 from LID1:10 (GUID 0x946dae030070dd0a) QP1:0x1 to LID2:83 (GUID 0x946dae030093e282) QP2:0x1Hello,Thanks for contacting us.
The error you are seeing in UFM indicates that there is some configuration issue your partitioning settings on the fabric.
Partition Key (P_Key): Enforces membership. Administered through the subnet manager by the partition manager.
P_Key configuration can be found under /opt/ufm/files/conf/opensm/partitions.confIncase you would like us to debug the issue. I would suggest to open a case on the Enterprise Support website so this could be fixed.Thanks,
Ilan.Powered by Discourse, best viewed with JavaScript enabled"
1185,eol-connectx-v1-mt25408a0-fcc-qi-working-stack,"Hello, need to bring some old ConnectX (Version1?) back to life.Has someone information about the latest working linux kernel / MLN_OFED stack?I tried with no success on Ubuntu 20.04:with kernels: 5.8.0-43 / 4.14.0-20BestMikeHello Mike,Thank you for posting your inquiry on the NVIDIA Networking Community.The ConnectX adapters is EOL and EOS for a while now. The MLNX_OFED version you are mentioning, unfortunately do not provide support the adapter anymore.Recommendation is to upgrade to a more recent ConnectX family adapter, for example ConnectX-4/5/6.Thank you and regards,~NVIDIA Networking Technical SupportHello Martijn,thanks for the quick response.Which Ubuntu / Debian / Kernel is the last working one?So the ConnectX 2 was announced 2009 so I assume its something around Ubuntu 8.4 ?BestMikeYou might check this thread as wellhttps://bugs.centos.org/view.php?id=16726Powered by Discourse, best viewed with JavaScript enabled"
1186,having-trouble-installing-ofed-on-ol,"Getting error when I ran ./mlnxofedinstall --fw-image-dir /tmp/my_fw_bin_files --add-kernel-support to install OFED for OL. Error details :ERROR: Failed executing “MLNX_OFED_SRC-5.2-2.2.0.0/install.pl --tmpdir /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.14.35-2047.501.2.el7uek.x86_64/mlnx_iso.61842_logs --kernel-only --kernel 4.14.35-2047.501.2.el7uek.x86_64 --kernel-sources /lib/modules/4.14.35-2047.501.2.el7uek.x86_64/build --builddir /tmp/MLNX_OFED_LINUX-5.2-2.2.0.0-4.14.35-2047.501.2.el7uek.x86_64/mlnx_iso.61842 --disable-kmp --build-only --distro ol7.8”How can I fix this ?Hi Chandra,Please note that according to the release notes the current kernel is not supportedhttps://docs.mellanox.com/display/OFEDv522200/General+Support+in+MLNX_OFEDlatest kernel supported for OL 7.8 is 4.14.35-1902.300.11.el7uek.x86_64The current kernel 4.14.35-2047.501.2.el7uek.x86_64 will be supported on the next MLNX_OFED 5.3 that will be released end of this month (March 2021) .Thanks,SamerStill seeing the issue with new release.ERROR: Failed executing “MLNX_OFED_SRC-5.3-1.0.0.1/install.pl --tmpdir /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-4.14.35-2047.501.2.el7uek.x86_64/mlnx_iso.113956_logs --kernel-only --kernel 4.14.35-2047.501.2.el7uek.x86_64 --kernel-sources /lib/modules/4.14.35-2047.501.2.el7uek.x86_64/build --builddir /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-4.14.35-2047.501.2.el7uek.x86_64/mlnx_iso.113956 --disable-kmp --build-only --distro ol7.9”ERROR: See /tmp/MLNX_OFED_LINUX-5.3-1.0.0.1-4.14.35-2047.501.2.el7uek.x86_64/mlnx_iso.113956_logs/mlnx_ofed_iso.113956.logFailed to build MLNX_OFED_LINUX for 4.14.35-2047.501.2.el7uek.x86_64Powered by Discourse, best viewed with JavaScript enabled"
1187,mst-folder-does-not-show-devices,"Hi all,
Has anyone experience losing one of the devices in the /dev/mst folder after  a reboot?
How can we bring it up?
We have two bluefield2 cards, and when trying to bring up the second card after running “sudo mlxfwreset -d /dev/mst/ -l 3 -y reset” I rebooted the system, and now it doesn’t show the second card in the list of devices in /dev/mst/ folder.
Any ideas on that?
Thanks,
DimanPowered by Discourse, best viewed with JavaScript enabled"
1188,after-mode-switch-still-unable-to-expose-gpu-to-the-dpu-side,"Hi,I have a Bluefield-2 DPU card and a GTX 1070 Ti GPU card. I set DPU to ‘BlueField-X mode – the GPU is exposed to the DPU and is no longer visible on the host’. However, GPU is still exposed to the host.FOR HOST SIDEFOR DPU SIDEI have already install the recommand driver and cuda on Installation Guide: installing-cuda-on-converged-accelerator. Sadly, I still could not open nvidia-smi successfully.I would be really grateful if you cound give me some advice.Best Regards,
Zhaoyang.Powered by Discourse, best viewed with JavaScript enabled"
1189,header-data-split-with-connectx-5-on-linux,"From the product data sheets and WinOF documentation I know ConnectX 5 supports header-data split (HDS) on Windows. I was wondering whether it was possible to enable it to work with Linux as well.I was looking at the programmers reference manual (https://www.mellanox.com/related-docs/user_manuals/Ethernet_Adapters_Programming_Manual.pdf) and there is a mention of HDS in the features summary but it’s not mentioned anywhere else in the document.Hi Shubham,Thank you for posting your question on our community.I would like to inform you that ConnectX-5 uses winOF-2 driver rather than WinOF driver. Please refer to our following WinOF-2 latest version User Manual ----> https://docs.mellanox.com/display/winof2v26051000/Ethernet+NetworkThe section “Header Data Split” which is listed in WinOF driver User Manual mainly used for ConnectX-3 and lower cards no longer exists in the WinOF-2 User Manual.In addition, it does not exist in our Linux driver MLNX OFED User Manual as well.I am consulting internally to get a confirmation on this feature and it’s support in Linux environment. The link to the PRM shared is not latest and the one applicable for ConnectX-5 is —> SalesforceThanks,Namrata.Hi Shubham,I have an internal confirmation that in the past this was supported only in Windows .Thanks,Namrata.Powered by Discourse, best viewed with JavaScript enabled"
1190,preventing-sd-card-corruption-in-the-event-of-power-failure-forced-restart,"Hello,I am building a personal project for image classification, deployed on Jetson nano developer kit. I am testing several steps while developing. I am using headless OS. I am facing the issue that SD card gets corrupted after several restart via power disconnect. Several restart might be any number from 2 times to 50 times. How can I avoid and protect the SD card corruption after several restart?Thank youplease post your Q on Jetson forum. Here it’s DOCA for Bluefield DPU forum…Powered by Discourse, best viewed with JavaScript enabled"
1191,we-upgraded-mellanox-drivers-to-mlnx-en-4-4-2-0-7-0-rhel7-5-x86-64-but-after-that-ibv-get-device-list-function-is-not-working,"System: RHEL 7.5 x86_64 with Mellanox driverEarlier we had 4.0.2 version driver which worked fine with dpdk 17.11.4. Now when we upgraded Mellanox driver version to 4.4.2 (used mlnx-en-4.4-2.0.7.0-rhel7.5-x86_64.iso), it doesn’t give expected results.I can see new version of modules get loaded correctly through lsmod command.mlx_fe-fe-0$ lsmod | grep mlx4_enmlx4_en 142833 0ptp 19231 2 mlx4_en,mlx5_coremlx4_core 352500 1 mlx4_enmlx_compat 28081 4 mlx4_en,mlx4_ib,mlx4_core,mlx5_coredevlink 42368 3 mlx4_en,mlx4_core,mlx5_coremlx_fe-fe-0$ lsmod | grep mlx4_coremlx4_core 352500 1 mlx4_enmlx_compat 28081 4 mlx4_en,mlx4_ib,mlx4_core,mlx5_coredevlink 42368 3 mlx4_en,mlx4_core,mlx5_coreHowever with this driver version OFED function ibv_get_device_list doesn’t give correct list of devices.This function is supposed to return an array of RDMA devices currently available. However it doesn’t give desired output. We have below four devices on our system but this function doesn’t return anything after upgrading the Mellanox drivers to version to mlnx-en-4.4-2.0.7.0-rhel7.5-x86_64$ lspci | grep Mell00:06.0 Ethernet controller: Mellanox Technologies MT27500/MT27520 Family [ConnectX-3/ConnectX-3 Pro Virtual Function]00:07.0 Ethernet controller: Mellanox Technologies MT27500/MT27520 Family [ConnectX-3/ConnectX-3 Pro Virtual Function]00:08.0 Ethernet controller: Mellanox Technologies MT27500/MT27520 Family [ConnectX-3/ConnectX-3 Pro Virtual Function]00:09.0 Ethernet controller: Mellanox Technologies MT27500/MT27520 Family [ConnectX-3/ConnectX-3 Pro Virtual Function]I have few questions with this regardTIAKiranHello Kiran,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, please make sure you install the new driver version with the option ‘–dpdk’Example:This will install all the needed packages for DPDK. Maybe the earlier version you were running was MLNX_OFED, which by default installs all packages.Thank you and regards,~NVIDIA Networking Technical SupportThanks Martijn for your response.installing with --dpdk didn’t solve the problem - I have more information to share with you which probably will help.Before upgrading the drivers we were able to see below infiniband devices on our system.ls -lrt /sys/class/infiniband/total 0lrwxrwxrwx 1 root root 0 Jan 29 13:06 mlx4_3 → …/…/devices/pci0000:00/0000:00:09.0/infiniband/mlx4_3lrwxrwxrwx 1 root root 0 Jan 29 13:06 mlx4_2 → …/…/devices/pci0000:00/0000:00:08.0/infiniband/mlx4_2lrwxrwxrwx 1 root root 0 Jan 29 13:06 mlx4_1 → …/…/devices/pci0000:00/0000:00:07.0/infiniband/mlx4_1lrwxrwxrwx 1 root root 0 Jan 29 13:06 mlx4_0 → …/…/devices/pci0000:00/0000:00:06.0/infiniband/mlx4_0However after upgrading and restarting /etc/init.d/mlnx-en.d - we see all these devices got removed, possibly that is the reason ibv_get_device_list function doesn’t find any device.Does any of rpm installation removes these infiniband devices?Any hint around that will help.Thanks,KiranPowered by Discourse, best viewed with JavaScript enabled"
1192,win-of2-driver-3-10-says-connectx-4-cards-unsupported,"The Win-OF/Win-OF2 landing page directs users with ConnectX-4 cards to download the Win-OF2 driver. I tried downloading and installing the most current version 3.10 for Win11 Client and Win Server 2022. The install fails on both attempts claiming ConnectX-4 cards unsupported.Please advise.Version 3.10 supports ConnectX-4 Lx and not ConnectX-4.https://docs.nvidia.com/networking/display/winof2v310/Supported+Network+Adapter+Cards+and+MFT+ToolsYou need to download version 3.0https://docs.nvidia.com/networking/display/winof2v30/Supported+Network+Adapter+Cards+and+MFT+ToolsPowered by Discourse, best viewed with JavaScript enabled"
1193,onie-onys-partition-issues,"I’ve replaced the SSD in an SN2410 switch. Embedded ONIE and now trying to install Onyx. But it fails with this error:=== Writing partition table to DISK1Error: You requested a partition from 256MiB to 448MiB (sectors 524288…917503).The closest location we can manage is 385MiB to 448MiB (sectors 788480…917503).Error: Partition doesn’t exist.== Creating ext3 filesystem on /dev/sda10 for VARmke2fs: No such device or address while trying to determine filesystem size*** Could not make filesystem on /dev/sda10== Cleanup====== Ending image install with error at 20210827-124209Error: writeimage failed, exiting.====== Ending image manufacture with error at 20210827-124209Error: failed running /etc/init.d/rcS.d/S10tms_dhcpcRunning /etc/init.d/rcS.d/S30tms_autostartAny idea what this might be? Looks to me it’s trying to create partitions too large that it won’t fit on the disk?Full output:==================================================Manufacture script starting==================================================== Using model: x86onie== Using kernel type: smp== System disk size: 60 GB== Using layout: X86ONIE_32GB== Using partition name-size list:== Using device list: /dev/sda== Using interface list: mgmts0 mgmts1== Using interface naming: ifindex-sorted== Smartd disabled== Cluster enable: no== Cluster ID: (none)== Cluster description: (none)== Cluster interface: (none)== Cluster master virtual IP address: 0.0.0.0== Cluster master virtual IP masklen: 0== Cluster shared secret: (none)== Cluster expected number of nodes: 0– Mapping MAC: ec:0d:9a:7d:b6:86 from: eth0 to: mgmts0– Mapping MAC: ec:0d:9a:7d:b6:87 from: eth1 to: mgmts1== Using image from file: /hd-media/image-X86_64-3.8.2204.img== Calling writeimage to image system— Executing: /sbin/writeimage.sh -m -I -f /hd-media/image-X86_64-3.8.2204.img -v -k smp -L X86ONIE_32GB -d /dev/sda====== Starting image install at 20210827-124005==== Manufacturing image==== Using layout: X86ONIE_32GB==== Layout targets: DISK1==== Using devices: /dev/sda==== Using bootmgr: grub2==== Hardware name: X86_64==== Verifying no needed partitions currently mounted==== Verifying targets can be repartitioned[ 16.608646] sda: sda1 sda2==== Verifying MTD partitions are the correct size==== Verifying targets are big enough==== Target change list: DISK1== Free RAM: 7132MB.== Ramdisk 3072MB for image extraction will be used.==== Mounting tmpfs for working area==== Tmpfs size used: 3072M==== Copying image file from /hd-media/image-X86_64-3.8.2204.img==== Verifying image integrity for image-X86_64-3.8.2204.imgUnpacked directory is /tmp/mnt_image_wi/tmpfs/unzipVerifying hashesDone== Running version: X86_64 3.8.2204 2019-12-29 16:11:11 x86_64== Image version: X86_64 3.8.2204 2019-12-29 16:11:11 x86_64== Image size: 659 MB / 1755 MB uncompressed==== Uncompressing source image file: /tmp/mnt_image_wi/tmpfs/unzip/image-X86_64-x86_64-x86_64-20191229-161111.tbz to /tmp/mnt_image_wi/tmpfs/unzip/image-X86_64-3.8.2204.tar==== Disk partitioning=== Calculating gpt partition table for DISK1== Device size: 57241 M== Allocated fixed size: 30104 M== Unallocated by fixed: 27073 M== Unallocated after growth: 27137 MModel: ATA SFSA060GU4AA2TO- (scsi)Disk /dev/sda: 57242MiBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags:Number Start End Size File system Name Flags1 1.00MiB 257MiB 256MiB fat16 EFI System boot, hidden2 257MiB 385MiB 128MiB ext4 ONIE-BOOT hidden=== Writing partition table to DISK1Error: You requested a partition from 256MiB to 448MiB (sectors 524288…917503).The closest location we can manage is 385MiB to 448MiB (sectors 788480…917503).Error: Partition doesn’t exist.Model: ATA SFSA060GU4AA2TO- (scsi)Disk /dev/sda: 57242MiBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags:Number Start End Size File system Name Flags1 1.00MiB 257MiB 256MiB fat16 EFI System boot, hidden2 257MiB 385MiB 128MiB ext4 ONIE-BOOT hidden3 448MiB 640MiB 192MiB ext3 primary4 640MiB 832MiB 192MiB ext3 primary5 832MiB 3904MiB 3072MiB ext3 primary6 3904MiB 6976MiB 3072MiB ext3 primary7 6976MiB 7168MiB 192MiB ext3 primary8 7168MiB 7360MiB 192MiB ext3 primary9 7360MiB 57242MiB 49882MiB ext3 primary==== Making filesystemsPartprobe/dev/sda: gpt partitions 1 2 3 4 5 6 7 8 9== Creating ext3 filesystem on /dev/sda4 for BOOT1== Creating ext3 filesystem on /dev/sda5 for BOOT2== Creating ext3 filesystem on /dev/sda3 for BOOTMGR== Creating ext3 filesystem on /dev/sda8 for CONFIG== Creating ext3 filesystem on /dev/sda9 for OUTPUT[ 132.443719] random: crng init done== Creating ext3 filesystem on /dev/sda6 for ROOT1== Creating ext3 filesystem on /dev/sda7 for ROOT2== Creating ext3 filesystem on /dev/sda10 for VARmke2fs: No such device or address while trying to determine filesystem size*** Could not make filesystem on /dev/sda10== Cleanup====== Ending image install with error at 20210827-124209Error: writeimage failed, exiting.====== Ending image manufacture with error at 20210827-124209Error: failed running /etc/init.d/rcS.d/S10tms_dhcpcRunning /etc/init.d/rcS.d/S30tms_autostart(none) login:Hi - please open a case at neworking-support@nvidia.com.Please specify the switch serial number.Powered by Discourse, best viewed with JavaScript enabled"
1194,hi-mellanox-team-please-help-identify-if-the-below-points-to-a-mellanox-bug-observed-on-cisco-csr1000v-mlx5-core-b7dd02-0-eth0-error-cqe-on-cqn-0x41c-ci-0x0-sqn-0x50d-syndrome-0x1-vendor-syndrome-0x68,"CSR1000v running on 17.2.1r deployed on MS Azure VM.lost access to it suddenly, tunnel re down. NO CLI into the device.Was inaccessible from the serial console as wellWe saw several “gibberish” logs on the screen. Apart from that, we had nothing.reloaded the VM instance and the CSR came back up.The only relevant logs that are pointing us to Mellanox is below :mlx5_core b7dd:00:02.0 eth0: Error cqe on cqn 0x41c, ci 0x0, sqn 0x50d, syndrome 0x1, vendor syndrome 0x68.If it’s a bug, what needs to be done by Microsoft/Cisco to mitigate the issue.Hello Chitttaranjan,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, we recommend to open a support ticket with Microsoft Azure Support. If needed, they will contact NVIDIA Networking through the internal channels.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1195,driver-support-for-rocky-linux-8-4,"Hi all,I searched for “rocky” “rockylinux” “rocky linux” with zero results, so if there’s something out there already, I apologize for missing it.Trying to install OFED combo driver (5.4-1.0.3.0) for a ConnectX-4 EN NIC in Rocky Linux 8.4 (Obsidian) and installer errors out with “current operation system is not supported!”. I tried the rhel/centos 8.4 driver package. Understandable as Rocky’s a clone, but thought I’d ask here anyways.A: is Mellanox going to officially support Rocky in the future?B: anyone have suggestion on method to get drivers installed on Rocky in the meantime?Cheers,AndrewHello Andrew,Thank you for posting your inquiry on the NVIDIA Networking Community.At the moment, unfortunately we do not provide a driver version for Rocky Linux and no timeline can be provided when we do.In the meantime, we recommend to us the provided INBOX drivers and utilities which come with the OS-distro.Thank you and regards,~NVIDIA Networking Technical SupportHi Andrew,I used the following steps to install OFED drivers for ROCKY-8.5. After Download and untar Mellanox ofed driver file -Unloading rdma_cm [FAILED]rmmod: ERROR: Module rdma_cm is in use by: rpcrdma ib_srpt ib_isertUnloading HCA driver: [ OK ]Loading HCA driver and Access Layer: [ OK ]wait for some time and check the statusBest Wishes,Rahul AkolkarPowered by Discourse, best viewed with JavaScript enabled"
1196,how-to-remove-flow-steering-rules-on-mellanox-connectx-5-en-nic,"Hi Community,I am using a Mellanox ConnectX-5 EN 100G NIC (driver mlx5_core 5.0-1.0.0.0, firmware 16.28.4000 (MT_0000000012)). However, I find many (seems) preset flow steering rules in the NIC (I am using mlx_fs_dump tool to dump fs rules). Is there any way to remove these flow steering rules? I also attached my dumped rules.I want to remove these fs rules because I suspect these rules cause my NIC to perform much worst than other NICs I used before (which do not have any preset fs rules). And I indeed observed around 5x message rate drop compared to a ConnectX-5 EX 100G NIC without preset fs rules (although EX is more powerful than EN, I do not believe this would lead to 5x gap).Any suggestions or comments are appreciated!Best,
Yangfs_dump.txt (5.3 KB)Any suggestions or comments are appreciated!This post also seems to indicate my guess: How to improve the performance of Flow Steering on ConnectX-5?Hi Yangzhou1,Please reset the configuration to its factory state and upgrade the firmware to the latest release.If it doesn’t work, please use the below Link to register to Enterprise Support Portal with a valid entitlement:https://enterpriseproductregistration.nvidia.com/?LicType=COMMERCIAL&ProductFamily=Networking-HWSupportAfter you complete the registration process with email, you will be able to login and access the portalWe are looking forward to hearing from you.Thanks,
YuyingThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1197,dpu-bluefield-regex-does-not-work,"
IMG_62044030×860 1.02 MB

When I was trying to run the Regex sample from File Scan :: NVIDIA DOCA SDK Documentation, this error came and I did not find anything useful on the webpage. Can anyone help me out?Hi,Can you please check if:Regards,
ChenHi, thanks for your reply.I have checked the regex engine is active and the huge pages are also allocated.
What else could be the reason?Hi,Can you please share the full output of “systemctl status mlx-regex” on the DPU?Thanks,
Chen
image4032×3024 2.95 MB

Here it is.I would suggest checking the permissions based on the warning at the bottom and restarting mlx-regex or a power cycle.Regards,
ChenI somehow managed to eliminate the warning and then restart the regex engine, however, the new error comes, and I do not know what to do with it.

IMG_6233.HEIC4032×3024 3.38 MB
Powered by Discourse, best viewed with JavaScript enabled"
1198,sn2100-switch-becomes-unresponsive-after-several-min,"We have an SN2100 switch that has worked flawlessly for 3 years, and then the other day it started to become unresponsive. There are no known infrastructure changes.The switch has all green indicator lights, but will not forward traffic. The management interface also goes down.A hard reboot will bring the switch up for a few min, but then it will go down again.Any idea what may be happing here? and or any troubleshooting steps we should take to diagnose the problem.Hi,Please open a ticket to networking-support@nvidia.com - we will be able to see if it’s software or a HW issue.Powered by Discourse, best viewed with JavaScript enabled"
1199,how-can-i-erase-the-configuration-in-cumulus-linux,"Hi experts,I am new on Cumulus Linux, so I am doing some testing in Nvidia Air. I wanted to erase the configuration of my switch and leave it as out of the box, so I erased the  /etc/nvue.d/startup.yaml file and did a switch reboot. But surprise, when the switch rebooted, it remained with the same configuration. Then how can I erase the configuration in Cumulus Linux?Thanks in advance,
JuliánA simple method to fully wipe the switch via NVUE is to use the following command:nv config apply emptyHere we are asking NVUE to apply the named revision called “empty” this is a well-known revision name for the system’s configuration when it is initially booted.Note: this will wipe the Eth0/management port configuration which could lock you out of a production switch if not using DHCP and logged-in via SSH.Hi epulvino,I have just tested and it worked. Thank you very much, also for the note which is very important :)Regards,
JuliánThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1200,about-udp-performance,"NIC: ConnectX5 MCX516A-CCAT
NIC FW: v16.35.1012
NIC Driver: v5.7-1.0.2
OS: Ubuntu 20.04 LTSWe use Dual CPU (Intel Xeon Silver 4314). Two Connect X-5s were installed in one PC to configure 4 channels.
We want to achieve performance of around 3.5GByte/s per channel by using only a single thread per channel.
However, when measuring the performance of 4 channels by writing UDP samples using recvfrom() and sendto(), only about 2Gbyte/s per channel comes out. (Send byte and receive byte are set to 8k)
Is there anything I can look into for better performance?In the sendto() and recvfrom() functions, there is a problem that communication is cut off when the size is set higher than 8k. (example: 16k, 32k, 62k, …)
Is there a setting that I have to set up to make the size bigger than 8k?Powered by Discourse, best viewed with JavaScript enabled"
1201,feature-request-mlx5-dpdk-flow-item-type-raw-support,"Current DPDK 21.11 flow API does not support RTE_FLOW_ITEM_TYPE_RAW for MLX5.I need support of RTE_FLOW_ITEM_TYPE_RAW in DPDK flow API to enqueue some ingress packets by content to GPU with support of GPUDirect RDMA and other to CPU via distinct HW queues (RTE_FLOW_ACTION_TYPE_QUEUE).For now RTE_FLOW_ITEM_TYPE_UDP and RTE_FLOW_ITEM_TYPE_IPV4 filtering and enqueueing with address and ports are supported.Are there any plans to support RTE_FLOW_ITEM_TYPE_RAW for MLX5?Agreed, this feature would be very helpful in many cases where more custom inspection is called for
(as well as more options for flow flex input/output links)
adding this comment to try to attract an answer :)Powered by Discourse, best viewed with JavaScript enabled"
1202,how-to-enable-disable-nic-rx,"Hi,My computer has two Ethernet ports(ConnectX 5 EN) for receiving data, and each port has an open socket. The sender also has two Ethernet ports, and they are synchronizing and transmitting the same data.Based on my understanding, when a UDP socket is open, incoming data is accumulated in the socket buffer. However, I want to be able to receive the same data from the sender at my convenience.How can I disable and enable the receiving (Rx) of the network interface card (NIC) in my application S/W?My O/S is Rocky Linux 9(x86_64) and NIC device driver is ‘MLNX_OFED_LINUX-5.8-2.0.3.0’.Thank you.Not sure your requirement.There is no way control NIC HW TX/RX.But there are many way control TX/RX flow on software side.EG,iptables, tc, ethtool -N/-Uhttps://netfilter.org/documentation/https://man7.org/linux/man-pages/man8/tc.8.htmlhttps://www.man7.org/linux/man-pages/man8/ethtool.8.htmlThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1203,i-cant-type-acl-for-vlan,"Dear Friends and Engineers.I am using Mellanox and Cumulus linux. I’m writing an ACL but I can’t add it into the VLAN and I get the following error. Im using 4.2 cumulus version.What’s the problem?I need to write for VLAN. How can I fix this problem?Thank you for your help.Regardscumulus@YPAYCore-1:mgmt:~$ net add interface vlan1050 acl ipv4 SNMPAccess inbound
ERROR: “net (add|del) vlan …” must be used to edit a vlan interface.Hi,can you try:net add vlan1050 acl ipv4 SNMPAccess inboundThank you very much My friend. This command is worked.Powered by Discourse, best viewed with JavaScript enabled"
1204,i-am-looking-for-a-7-meter-dac-or-aoc-cable-for-the-mnpa19-xtr-10gb-mellanox-connectx-2-card,"I am looking for a 7 meter dac or aoc cable for the MNPA19-XTR 10GB MELLANOX ConnectX®-2 card. Is there a model that you can recommend me and that works for sure? I will make the connection between 2 MNPA19-XTR. I will not put a switch in between.Hello Techdocstr,Thank you for posting your inquiry on the Developers Forum.As the ConnectX-2 is EOL/EOS for a long time already, compatible cables will be limited in availability.The only 10Gb/S SFP+ 7m Eth. cable which we had available is the MC3309124-007 (https://network.nvidia.com/pdf/prod_cables/PB_MC33091xx-xxx_MCP210x-XxxxB_10GbE_SFP+_DAC.pdf)Recommend to do a search on eBay or FS.com for a compatible cable.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1205,how-to-change-width-x8-lanes-to-width-x16-lanes-for-mellanox-connectx-4,"Hello,We have installed Mellanox ConnectX-4 Network adapters.
As per the specs, width should be x16. However we see that the width is only x8.Can you please suggest if there is any command to change this settings to X16(16 lanes)LnkCap: Port #0, Speed 8GT/s, Width x16, ASPM not supported
LnkCtl: ASPM Disabled; RCB 64 bytes Disabled- CommClk+
LnkSta: Speed 8GT/s, Width x8, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-
LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete-, EqualizationPhase1-What is the card OPN?Powered by Discourse, best viewed with JavaScript enabled"
1206,how-to-i-what-is-the-easiest-way-to-add-a-second-sx6012-switch-to-my-setup-i-have-another-sx6012-and-3-nodes,"Hello. How to I/what is the easiest way to add a second Sx6012 switch to my setup? I have another Sx6012 and 3 nodes.I have 3 server nodes each having 1 dual port ConnectX-4 adapter. I’d like to use the second port on each of these adapters and connect them to a second Sx6012 to add HA to this setup.Can someone give me the cliff notes version on what to expect with this setup? I installed the first setup, so I should be fine doing this as well. I have all components ready to go. Thanks to anyone willing to help. ThanksHello Steven,Thank you for posting your inquiry on the NVIDIA Networking Community.Based on the information provided, you can use the instructions provided in from the following link to setup HA between the two switches (Make sure the switches are on the same code level, and we recommended the latest MLNX-OS code available for this p/n ; MLX-OIS 3.6.8012)Instructions on how to configure → https://community.mellanox.com/s/article/understanding-subnet-manager--sm--high-availability--ha--on-mellanox-infiniband-switches#jive_content_id_Mellanox_SM_HA_Solution_Mellanox_InfiniBand_SwitchesLater UM description of SM HA including diagram → https://docs.mellanox.com/display/MLNXOSv390606/Subnet+Manager+High+AvailabilityConnect the two switches together and connect the 2nd port of the HCA to switch new switch, and you are good to go. (See diagram below)For creating the LAG on the adapter, please follow the instructions from the following link → https://community.mellanox.com/s/article/howto-create-linux-bond--lag--interface-over-infiniband-networkWhen using this setup with IPoIB, you will have an active/standby configuration. If using RDMA traffic, you can run traffic on both ports of the adapter.Thank you and regards,~NVIDIA Networking Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1207,doca-flow,"How do I match ARP packets？doca_flow_match structure has nothing to do with arp.Powered by Discourse, best viewed with JavaScript enabled"
1208,error-attaching-device-000000-0-representor-0-65535-to-dpdk,"Hi,
I have a problem with using ovs-dpdk on Bluefield-2. after using this Nvidia document(Mellanox Interconnect Community), these errors occured:ovs-vsctl show
5365ebe0-f264-4e3a-86f3-0031c4fc8078
Bridge ovs_dpdk_br0
datapath_type: netdev
Port dpdk0
Interface dpdk0
type: dpdk
options: {dpdk-devargs=“0000:03:00.0”}
Port dpdk1
Interface dpdk1
type: dpdk
options: {dpdk-devargs=“0000:03:00.0,representor=[0,65535]”}
error: “Error attaching device ‘0000:03:00.0,representor=[0,65535]’ to DPDK”
Port ovs_dpdk_br0
Interface ovs_dpdk_br0
type: internal
ovs_version: “2.15.1”systemctl status openvswitch-switch.serviceovs|00057|dpdk|ERR|Invalid port_id=1024Any idea how should I solve these errors?Hi @rtaheri,Please check if “dpdk_initialized : true” under:
“ovs-vsctl --no-wait list Open_vSwitch .”
If it’s “false” it means you didn’t compile or point to DPDK tree, check the OVS compile log where it checks for DPDK folder.Best Regards,
ChenThanks for your responseI checked dpdk_initialized and it’s true.Is there any reason that makes this error?Powered by Discourse, best viewed with JavaScript enabled"
1209,whats-the-difference-between-a-check-a-show-and-a-trace,"These concepts tend to apply more to the NetQ CLI than the UI, but the terms are used throughout the documentation and product.A check is an on-demand validation of various elements in your network fabric at the current time or a time in the past. Such elements include the NetQ Agent, various protocols such as BGP or OSPF, and configurations like interfaces or EVPN.A trace verifies network connectivity on-demand between two devices at either layer 2 or layer 3.A show command is used for monitoring both hardware and software.Powered by Discourse, best viewed with JavaScript enabled"
1210,is-there-any-configuration-that-is-limiting-the-virtual-function-bandwidth-on-connectx-5-ex-adapter-weve-working-with-this-adapter-and-noticed-that-we-are-limited-to-50gb-when-using-the-virtual-function-while-the-same-limit-does-not-occur-with-the-pf,"IB_SEND_BW result:PF:#bytes #iterations BW peak[Gb/sec] BW average[Gb/sec] MsgRate[Mpps]65536 344452 0.00 60.19 0.11480765536 344471 0.00 60.20 0.114814Single VF:#bytes #iterations BW peak[Gb/sec] BW average[Gb/sec] MsgRate[Mpps]65536 275444 0.00 48.14 0.09181665536 276957 0.00 47.92 0.091394Both VFs:#bytes #iterations BW peak[Gb/sec] BW average[Gb/sec] MsgRate[Mpps]65536 126892 0.00 22.06 0.04208365536 126602 0.00 22.13 0.042201#bytes #iterations BW peak[Gb/sec] BW average[Gb/sec] MsgRate[Mpps]65536 128045 0.00 22.38 0.04268265536 126065 0.00 22.03 0.042022Hi Efi ,First of all we recommend tune the server and the adapter following the below guide so you can reach line rate on the PF .https://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersRegarding the VFs performance as i can see you running the performance test from the baremetal and not from VM (as pure SRIOV solution) , the performance should be a little bit lower than the PF and it also depends to how many VFs you create per PF as the PF resources are splitted between all VFs related to the same port(PF) .Thanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
1211,rdma-issues-connectx-5-ex-40gbe-dual-port-qsfp28,"Hey Everyone!I’m facing a strange issue here that I can’t seem to solve :(Long story short, I have around 20 servers to setup.After installing the first 9, I realized that some of them are acting up.The weird thing that is happening is that when setting up RDMA, most of them seem to be able to communicate nicely (receive/send) but some just won’t receive or send…I have 2 types of servers where the second one is the most troublesome one.The install is identical, they all have the same identical Ubuntu 20.04 setup and the first 6 are completely identical in terms of hardware components and the last 3 are identical as well.They all however have the same identical NIC “ConnectX-5 Ex 40GbE Dual-Port QSFP28”.Here’s an overview i made of which ones that are able and not able to ping (using rping).so the MA server can connect to w1,w2,… and vice versa.Same for the W* ones.Then the weirdness starts.C1, C2 and C3 can rping to MA, W1, W3, W4 but NOT to W2Also C1 is able to rping C3 but not the other way around.I hope someone can point me into the right direction where to troubleshoot because i’m out of ideas.Already tried to replace transceiver modules, swap cables around…Nothing seemed to have worked.Edit: iperf3 works (both ways), regular ping works, just now installed the latest driver, nothing changed…Any advice would be extremely appreciated!!Thank you in advance!What is the error you are getting?Are you able to run ib_write_bw? ib_write_bw -R?Hey @Aleksey Senin​ and thank you for your response.The first 7 servers can ib_write_bw to and from each other without any problems whatsoever.The C series servers can ib_write to and from 3 of those 7 servers but not the other 4 (while they are identical and have an identical setup)Example of a finished test.---------------------------------------------------------------------------------------RDMA_Write BW TestDual-port : OFF Device : mlx5_0Number of qps : 1 Transport type : IBConnection type : RC Using SRQ : OFFPCIe relax order: ONibv_wr* API : ONTX depth : 128CQ Moderation : 1Mtu : 4096[B]Link type : EthernetGID index : 3Max inline data : 0[B]rdma_cm QPs : OFFData ex. method : Ethernet---------------------------------------------------------------------------------------local address: LID 0000 QPN 0x00e5 PSN 0x72f5e5 RKey 0x1c4b6a VAddr 0x007ffff7aea000GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:00:10remote address: LID 0000 QPN 0x00b6 PSN 0xef4205 RKey 0x183fee VAddr 0x007ffff7ae9000GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:00:25---------------------------------------------------------------------------------------#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]Conflicting CPU frequency values detected: 2973.833000 != 3299.583000. CPU Frequency is not max.65536 5000 4667.63 4663.27 0.074612---------------------------------------------------------------------------------------Failed one (sending)---------------------------------------------------------------------------------------RDMA_Write BW TestDual-port : OFF Device : mlx5_0Number of qps : 1 Transport type : IBConnection type : RC Using SRQ : OFFPCIe relax order: ONibv_wr* API : ONTX depth : 128CQ Moderation : 1Mtu : 4096[B]Link type : EthernetGID index : 3Max inline data : 0[B]rdma_cm QPs : OFFData ex. method : Ethernet---------------------------------------------------------------------------------------local address: LID 0000 QPN 0x00d1 PSN 0x52f93f RKey 0x1b482d VAddr 0x007ffff7aea000GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:00:12remote address: LID 0000 QPN 0x00b7 PSN 0xe84afa RKey 0x183fef VAddr 0x007ffff7ae9000GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:00:25---------------------------------------------------------------------------------------#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]Completion with error at clientFailed status 12: wr_id 0 syndrom 0x81scnt=128, ccnt=0Failed to complete run_iter_bw function successfullyI’ve been trying to figure out why, how, but currently no solution yet :(The only difference here is the servers themselves.However, they all have the same exact NICs…Just checked lsmod and same modules are loaded as well…Hi Elio,What type of topology is this?What switch/switches are in between?Is PFC/ECN configured? Any other special configuration on the switch?Is there any major difference between the way these servers are configured (Bonding, etc)?Different VLANs?Do we see the same behavior when utilizing RDMA_CM (ib_write_bw -R)?Can you provide the output of:as well?Thanks,Mellanox Technical SupportBe sure you are using latest MOFED v5.3Try exclude switch and connect hosts directly one to anotherValidate that there is no firewall that can block the trafficBe sure you are using correct IP configuration and not using both ports with IP from same subnet. Test with only one port up and other down/disconnectedConcentrate on C1/C3 communication. You might use tcpdump with latest libpcap traffic to capture RoCE traffic and see if packets transferred between the hosts and communication brakes in the middleSwap the cards and see if the issue follows the cards or stays with the hostSwap ports on the switch and see if the issue stays with server or follow the switch portSwap the cables to see if the issue follow the cable or stays with host/switch port.Further troubleshooting most likely will require collecting logs and analyzing them that requires valid support contract and if you have one, you may open a support ticket with networking-support@nvidia.comPowered by Discourse, best viewed with JavaScript enabled"
1212,completion-errors,"During scaled up runs of DAOS stack on the Frontera/TACC cluster, we occasionally experience completion errors coming out of mlnx driver that look like:c121-063.frontera.tacc.utexas.edu ERROR 2021/04/05 17:38:12 daos_engine:0 mlx5: c121-063.frontera.tacc.utexas.edu: got completion with error:00000000 00000000 00000000 0000000000000000 00000000 00000000 000000000000222f 00000000 00000000 0000000000000000 00008813 080096cd 0351fad2Is there a way to decode those errors to understand/debug what went wrong?MLNX_OFED_LINUX-5.3-1.0.0.1:Provider: verbs;ofi_rxmOFI: v1.12.0Few other completion errors from a different cluster running the same reproducer:wolf-118: Jun 19 00:12:33 wolf-118. srv[14520]: ERROR: daos_engine:1 mlx5: wolf-118.: got completion with error:wolf-118: Jun 19 00:12:33 wolf-118. srv[14520]: 00000000 00000000 00000000 00000000wolf-118: Jun 19 00:12:33 wolf-118. srv[14520]: 00000000 00000000 00000000 00000000wolf-118: Jun 19 00:12:33 wolf-118. srv[14520]: 0000014c 00000000 00000000 00000000wolf-118: Jun 19 00:12:33 wolf-118. srv[14520]: 00000000 00008813 10002cce 00449dd3wolf-119: – Logs begin at Fri 2021-06-18 21:07:29 UTC, end at Mon 2021-06-21 14:12:30 UTC. –wolf-119: Jun 19 00:12:33 wolf-119. srv[14655]: ERROR: daos_engine:1 mlx5: wolf-119.: got completion with error:wolf-119: Jun 19 00:12:33 wolf-119. srv[14655]: 00000000 00000000 00000000 00000000wolf-119: Jun 19 00:12:33 wolf-119. srv[14655]: 00000000 00000000 00000000 00000000wolf-119: Jun 19 00:12:33 wolf-119. srv[14655]: 0000016e 00000000 00000000 00000000wolf-119: Jun 19 00:12:33 wolf-119. srv[14655]: 00000000 00008813 100029a0 002851d2wolf-120: – Logs begin at Fri 2021-06-18 21:07:00 UTC, end at Mon 2021-06-21 14:12:30 UTC. –wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: ERROR: daos_engine:1 mlx5: wolf-120.: got completion with error:wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000000 00000000 00000000 00000000wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000000 00000000 00000000 00000000wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000289 00000000 00000000 00000000wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000000 00008813 10002a9b 00589bd2wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: mlx5: wolf-120.: got completion with error:wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000000 00000000 00000000 00000000wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000000 00000000 00000000 00000000wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000299 00000000 00000000 00000000wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000000 00008813 10002aab 005da1d2wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: mlx5: wolf-120.: got completion with error:wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000000 00000000 00000000 00000000wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000000 00000000 00000000 00000000wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 000002a4 00000000 00000000 00000000wolf-120: Jun 19 00:12:33 wolf-120. srv[14654]: 00000000 00008813 10002ab6 004994d2wolf-121: – Logs begin at Fri 2021-06-18 21:07:29 UTC, end at Mon 2021-06-21 14:12:31 UTC. –wolf-121: Jun 19 00:12:33 wolf-121. srv[14622]: ERROR: daos_engine:0 mlx5: wolf-121.: got completion with error:wolf-121: Jun 19 00:12:33 wolf-121. srv[14622]: 00000000 00000000 00000000 00000000wolf-121: Jun 19 00:12:33 wolf-121. srv[14622]: 00000000 00000000 00000000 00000000wolf-121: Jun 19 00:12:33 wolf-121. srv[14622]: 00000162 00000000 00000000 00000000wolf-121: Jun 19 00:12:33 wolf-121. srv[14622]: 00000000 00008813 10002b7f 003520d2In our experience, it is application/software error and not an Nvidia issuePlease, check “7.12.7 Completion With Error” section in Ethernet Adapters Programming Manual https://www.mellanox.com/related-docs/user_manuals/Ethernet_Adapters_Programming_Manual.pdf0x13 syndrome is “Remote Access Error” that might be caused by invalid R_Key, for example.Please, add more debug information to your application to verify all data. Most likey, it is some memory corruption.Similar issue can be observed by running ib_read_bw application and using different message sizes on both sidesOn one side:ib_read_bw -d mlx5_0 -s 128On the other:ib_read_bw -x mlx5_0 -s 65535Result:One side:#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]ethernet_read_keys: Couldn’t read remote addressUnable to read to socket/rdma_cmFailed to exchange data between server and clientsOther side:#bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps]mlx5: b-csi-0527s: got completion with error:00000000 00000000 00000000 0000000000000000 00000000 00000000 0000000000000000 00000000 00000000 0000000000000000 00008813 10000123 000084d2Completion with error at clientFailed status 10: wr_id 0 syndrom 0x88scnt=128, ccnt=0If after debugging, you’ll discover the issue with Nvidia component, don’t hesitate to open an standard support ticket as your organization has a support contract with us.Powered by Discourse, best viewed with JavaScript enabled"
1213,connectx-3-aspm-support,"I find my CX311/CX312 refuses to enable ASPM even though the link-capability claims ASPM support:I suspect this prevents the CPU package from entering deep power saving mode ( was ~92% PC8 without the NIC, and ~93% PC3 with the NIC) and contributes to much higher measured power consumption (~7.5W) than the rated 3.8W typical.Question: is there any way (driver / firmware config) to get the NIC to run in ASPM mode?Question2: does SR-IOV play any role here? (I also played around with SR-IOV enabled and disabled and find no direct impact on the power. But I do noticed that the VF is listed as ASPM not supported, which makes me worry…)I tried some dirty trick from this post to manually force enable the ASPM L0S mode.
Unfortunately this lead to floods of error message in the kernel log:[ 597.337196] pcieport 0000:00:01.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Transmitter ID)
[ 597.337197] pcieport 0000:00:01.0: device [8086:1901] error status/mask=00001000/00002000
[ 597.337198] pcieport 0000:00:01.0: [12] Timeout
[ 597.339789] pcieport 0000:00:01.0: AER: Multiple Corrected error received: 0000:00:01.0
[ 597.339825] pcieport 0000:00:01.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Transmitter ID)
[ 597.339826] pcieport 0000:00:01.0: device [8086:1901] error status/mask=00001100/00002000
[ 597.339826] pcieport 0000:00:01.0: [ 8] Rollover
[ 597.339827] pcieport 0000:00:01.0: [12] Timeout
[ 597.339829] pcieport 0000:00:01.0: AER: Multiple Corrected error received: 0000:00:01.0
[ 597.339854] pcieport 0000:00:01.0: AER: can’t find device of ID0008Does this indicate that the ASPM support is not working at all?Does this indicate that the ASPM support is not working at all?did you ever figure this out?I am also trying to find out why this ConnectX-3 dual SFP+ card uses 12 watts with nothing plugged onto the system. I thought I read it should be 3 watt per portHi,
I am also very interested to figure this out. Maybe somebody from support team @Mellanox_Support can chime in.Regards!Powered by Discourse, best viewed with JavaScript enabled"
1214,how-to-use-softwareimages-as-created-on-another-cluster,"Hi FolksI have to use my softwareimages on another cluster.
I copied /cm/images/<my softwareimage> but it is not registered in BCM on another clusterIf the image directory is already present on the new cluster, you can register it in the Bright configuration using cm-create-image as follows:# cm-create-image -d /cm/images/<my softwareimage> -n <image name>Others in the community can answer with how they perform backups, but Bright has no official mechanism for backing up software images.Powered by Discourse, best viewed with JavaScript enabled"
1215,mcx516a-cdat-compatibility-with-qsfp28-line-switches,"I can’t seem to find a compatibility matrix for the MCX516A-CDAT ConnectX-5 cards, showing what third-party QSFP28 switches they are usable with. Does anyone have some links? We are trying to pick up various switches to test solutions with but there doesn’t seem to be a lot of information around. For example, “Edge-Core” AS7512-32X QSFP28 100GBE switches would seem to be compatible from their datasheets, but it would be great to have something official in hand from Nvidia or Mellanox to work from.Hello MakerOfGames,Thank you for contacting Nvidia support.For ConnectX5 adapters, on the page for firmware downloads, you can find for each firmware version the release notes documentation. In this document, there is a compatibility table of tested and supported equipment for the adapter and specific firmware version. As well as for 3rd party vendors (last topic in the page).To navigate to the release notes document, start from the firmware downloads page:https://network.nvidia.com/support/firmware/connectx5en/Choose the firmware version,then choose the adapter model, then PSID and below the Download/Documentation column, there is a link for the release notes documentation.Direct link for the release notes section:https://docs.nvidia.com/networking/display/ConnectX5Firmwarev16352000LTS/Firmware+Compatible+ProductsBest regards,Nvidia supportThanks, that’s exactly what I was looking for. Was kind of hoping for a more exhaustive list of hardware to be on that chart (you’d think every 100 GbE switch maker would have been happy to send Nvidia samples for testing), but it is what it is. I’ll make it work!Powered by Discourse, best viewed with JavaScript enabled"
1216,imageupdate-failed-to-request-image-update,"Restarting cmd on head node and all compute nodes hasn’t helped… any ideas what I could try?
ThanksPowered by Discourse, best viewed with JavaScript enabled"
1217,hpe-dac-cables-with-msn2010-cb2f,"I want to substitute a broken HPE FlexFabric 10G switch for 2 two MSN2010-CB2F.
Can I still use these HPE passive DAC cables with this switch?:
J9281D: Aruba 10G SFP+ to SFP+ 1m Direct Attach Copper Cable
J9283D: Aruba 10G SFP+ to SFP+ 3m Direct Attach Copper Cable
JG330A: FlexNetwork X240 40G QSFP+ to 4x10G SFP+ 3m Direct Attach Copper Splitter Cable
The last one is a 4 to 1 splitter DAC cable.Thank you…Hi,There shouldn’t be any issue with handling DAC cables in the MSN2010 (including the splitter cable).thanks,DanThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1218,installing-doca-3-7-on-ubuntu-no-dpu-interfaces,"I followed the DPU Installation Guide for v3.7 using Ubuntu 20.0.4 on the server with BF2 and installed SDKManager and used it to update the host and DPU.  When it completed, I can see that the DPU has Port1 and OOB connectivity - but the host has lost all connectivity.  And an OS error message pops up that netplan script is corrupt…I think I have run into a known SDK release note issue:
27301
57
Description: Kernel upgrade is not currently supported on BlueField as there are out of tree kernel modules (e.g., ConnectX drivers that will stop working after kernel upgrade).Workaround: Kernel can be upgraded if there is a matching DOCA repository that includes all the drivers compiled with the new kernel or as a part of the new BFB package.Keywords: Kernel; upgrade
Discovered in version: 3.7.0I fixed the host network connectivity by having netplan generate a new config file - but that file only has one embedded host NIC and the tmfifo NIC.I don’t see any instructions to follow the release note workaround.  It sounds like the BF2 host drivers need to be rebuilt with the current kernel.How do I do that?Hi Monty, have you made any progress on this? I’ve never heard of issues with an SDKManager install that produces this problem, but I’d recommend trying a manual install and ensure that you perform the steps to reset the nvconfig params: sudo mlxconfig -d /dev/mst/<device> -y reset and then also that the firmware is up to date (on the DPU you run sudo /opt/mellanox/mlnx-fw-updater/mlnx_fw_updater.pl)The complete manual installation steps are here in our guide: Installation Guide :: NVIDIA DOCA SDK DocumentationJustin, yes I reinstalled following the installation guide and problem was resolved.  Thanks!Powered by Discourse, best viewed with JavaScript enabled"
1219,is-debian-11-bulleyes-not-supported-yet-on-the-latest-mlnx-ofed-4-9-lts-driver,"Can’t find Debian 11.2 in OS section on download page.The change required to get the code build on Debian 11 kernel is manageable.
It wasted me quite some time to figure it out, but here is my change to get it work.
It’s based on the 4.9-4.1.7.0 version though. Didn’t bother to upgrade the driver yet…Pastebin.com is the number one paste tool since 2002. Pastebin is a website where you can store text online for a set period of time.Powered by Discourse, best viewed with JavaScript enabled"
1220,gpudirect-rdma-access-through-directx,"I am switching from AMD gpu to Nvidia gpu and have some queries to use DirectX in Nvidia gpus.Please provide appropriate response.Where you get info opengl/driectx related with GDR(GPU Driect RDMA)?OpenGL/DriectX is graph libs. GDR is for communication between node(through RDMA NIC HCA), We support GDR through CUDA/infiniband Verbs, base on Tesla GPU and NVIDIA infiniband/RoCE.I found the info from this official document - https://developer.download.nvidia.com/devzone/devcenter/cuda/docs/GPUDirect_Technology_Overview.pdf
Slide 2
Please confirm if my understanding is correct or not.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1221,pcie-3-0-x16-card-in-x16-physical-x8-electrical-slot,"I’m new to Mellanox/NVIDIA and am interested in MCX515A-CCAT. As a 100 GbE card, I realize this card would need PCIe 3.0 x16 to reach its full 100 GbE speed. What happens if I put this card in a PCIe 3.0 x16 physical / x8 electrical slot? Since x8 slots max out at 64 Gb/s in one direction, will this card try to reach 100 GbE speeds or will it shift down into 50 GbE mode?Thank you!Hello,The MCX515A-CCAT was designed for a full PCIe Gen.3 x16 connection as listed in its User Manual specifications. Utilizing it with a narrower PCIe width would be an unsupported configuration.In cases where the PCIe width is narrowed, the adapter’s link speed will not be limited and will still link at the 100GbE speed, but you will not achieve 100GbE performance as the performance will ultimately be bottlenecked by the PCIe throughput.I would recommend reviewing the following community article for more information on determining your systems’ PCIe capabilities, as well as how to calculate their limitations at the PCIe level:https://community.mellanox.com/s/article/understanding-pcie-configuration-for-maximum-performanceMore information on the ConnectX-5 can also be found here:https://www.mellanox.com/files/doc-2020/pb-connectx-5-en-card.pdfThanks,-Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
1222,unable-to-uninstall-mlnx-winof2-1-45,"I´m unable to uninstall MLNX_WinOF2 1.45. After running uninstall in Windows “Add or remove programs” it reapperas after a few seconds.What do I have to do to uninstall it?Hi Christoph,To uninstall MLNX_WinOF2 you need to have elevated administrator privileges.You can uninstall it in one of the following ways:Click Start-> Control Panel-> Programs and Features-> MLNX_WinOF2 → Uninstall.Open a CMD console-> Click Start-> Task Manager-> File-> Run new task-> and enter CMD.Run:Regards,ChenI´ve tried both ways with elevated administrator privileges and it didn´t work. It still reappears in Programs and Features after a few seconds.In the MLNX_WinOF2.log I get this error:CheckIfPckgInstalled: ERROR !!! ,Mellanox WinOF package for is not installedPowered by Discourse, best viewed with JavaScript enabled"
1223,connectx-5-is-sometimes-recognized-by-windows-10,"Hi,
I have just installed a MCX516A-CCAT card in my Windows 10 workstation. I installed the WinOF-2 Driver and the MFT as suggested in ESPCommunity.
Running “mst status” usually returns “mt4119_pciconf0”, but, after some times (from seconds to minutes), the card stops working and the previous command returns “No MST devices found”. Then I have to reboot the machine to make it again detected by Windows. In Windows device manager, a warning symbol appears over the icon of the first adapter when the card starts to be not detected. Moreover, after refreshing the list of detected hardware, the network adapters are no more shown.
Please consider that the the card was previously installed in a Linux workstation and worked correctly.
Could you help me, please?
Thank you in advance,AlessandroHi,Base on your description, I have the following suggestions:Move this NIC from this Windows 10 workstation to a Linux server, then upgrade the FW version to the latest version 16.35.2000 vesrion.
https://www.mellanox.com/downloads/firmware/fw-ConnectX5-rel-16_35_2000-MCX516A-CCA_Ax-UEFI-14.28.16-FlexBoot-3.6.805.bin.zipMove this NIC back to this Windows 10 workstation, if you workstation has another PCI slot, try to plug into other PCI slot.Boot up your Windows 10 workstation, enter the server BIOS configuration. If there is any performance/tuning config for that specific PCI slot, please make sure the BIOS configs are all good.Boot into Windows system, try again in device manager.Sembra che mi abbiano considerato sul forum NVIDIA/Mellanox.A quanto capisco si deve rimettere la scheda 100 Gbit/s su Linux, fare l’upgrade del firmware e poi installarla su Windows.Sempre più complicato…AlessandroPowered by Discourse, best viewed with JavaScript enabled"
1224,can-an-oracle-connectx-3-be-flashed-with-stock-firmware,"I have four ConnectX-3 dual port 40G cards, currently configured as IB ports. The firmware is identified as v 2.11.1280 but the PSID is ORC1090120019 … can these be flashed with a newer stock firmware that will allow mlxconfig to switch them to Ethernet ports, without having to struggle with Oracle support?No. For OEM card you have to contact OEM vendor.However, you might try to use port_type_array parameter for mlx4_core and see if it helpsSome examples, here - https://support.mellanox.com/s/article/howto-configure-sr-iov-vfs-on-different-connectx-3-portsAlso check older Mellanox OFED user manuals, version 4.9 or newer that still support ConnectX-3 cardPowered by Discourse, best viewed with JavaScript enabled"
1225,are-the-connectx-5-6-cards-able-to-initialize-and-operate-as-virtual-switches-with-no-installed-transceivers-sfp-modules,"Like say have few VFs on a card with PF down and still be able to switch packets among them. Is the switching done in card’s hardware - I’ve read that the eSwitch is done in silicon not in the driver’s software. This is meaningful to us because it will allow us to test and troubleshoot our solution installed on CPE equipment without customer’s network equipment(switches/routers) interfering. Thus better isolate potential problems.Hello Simon,To answer your question, “Is the switching done in card’s hardware?”Yes, the eSwitches are integrated in the NIC ASIC.Hope this helps.Thank you and regards,~NVIDIA Networking Technical SupportThe question was: Can VF(virtual functions) operate without SFP installed into the card?The question was: Can VF(virtual functions) operate without SFP installed into the card?@Lonie Lin​ just mentioning you to let you know that what I was interested in the first place is if the Connectx-5/6 cards can operate as “local” switches without SFP modules installed. Say switching from one VM to another one, each with its own virtual function.May that be answered?Hello Simon,Thank you for your reply.Yes, it is possible VM’s can communicate with each other while PF is down and a physical link is not needed between the ConnectX-5/6 and a switch.Following community article describes the process of configuring Mellanox Adapter this way.https://community.mellanox.com/s/article/howto-set-virtual-network-attributes-on-a-virtual-function–sr-iov-xThe option of configuring the Mellanox adapter/s port to the link-state “enable” will let the VF to communicate with other VFs on the host even if the PF link state is down.Hope this helps.Thank you,LoniePowered by Discourse, best viewed with JavaScript enabled"
1226,ztp-dhcp-configuration-option-43,"Hi Community,I currently implement ZTP for ONYX mellanox switches. It is well working for matching DHCP Option 60 and pushing configuration and firmware.But, to distinguish between firmware Versions and providing the correct upgrade path, the switches need to tell the DHCP server its current version. From the documentation, this is done via DHCP Option 43:https://docs.mellanox.com/display/MLNXOSv381000/Getting+Started#GettingStarted-ZTPBut there is no real-life example how to configure this on a (Linux-ISC-) DHCP server (-> match model and firmware version). What I read was that Option 43 is used to push vendor specific configuration TO the device but not to receive Information FROM it.Any help is appreciated!Thank you, :-)SebastianHi Minh,thank you, I’ve attached the files as requested.Best Regards,SebastianThe thing I don’t get is: The Mellanox documentation says that the DHCP server identifies the client based on option 60 and option 43. But based on the RFC for DHCP, only 60 is used for identity and 43 is used to push information back to the client:“Configuring the DHCP option 60 helps in identifying the incoming DHCP client. If the vendor class identifier (VCI) advertised by the DHCP client matches with the DHCP server, the server makes a decision to exchange the vendor-specific information (VSI) configured as part of DHCP option 43.”could you please provide an example on how to read from Option 43 information with an isc-dhcp-server? The documentation does not seem to be very clear. It only states that there are “corresponding subtypes defined” but not how to use or match them.Thanks,SebastianHi Sebastian,I found KB about it, please let me know it works for you:ZTP:Zero Touch ProvisioningZero Touch Provisioning (ZTP) automates initial configuration of Mellanox switches at boot time. It helps minimize manual operation and reduce the customer’s initial deployment cost. ZTP allows you to automatically upgrade the switch with a specified OS image, set up an initial configuration database, and load and run a container image file.The initial configuration is applied by using a regular text file containing valid cli configuration commands.Running DHCP-ZTP:There is no explicit command to enable ZTP. It is enabled by default. Disabling it is performed by a user-initiated configuration save(using the command “configuration write”). The only way to re-enable ZTP would be to run a “reset factory” command, clearing theconfiguration of the switch and rebooting the system.ZTP is based on DHCP. For ZTP to work, the software enables DHCP by default on all its management interfaces. The switch OSrequests option 66 (tftp-server-name) and 67 (bootfile-name) from the DHCPv4 server or option 58 (bootfile-url) from the DHCPv6server, and waits for the DHCP responses containing file URLs. The DHCP server must be configured to send back the URLs for thesoftware image, configuration file, and docker container image via these two options. Option 66 would contain the URL prefix to thelocation of the files, option 67 would contain the name of files, and option 58 would contain the complete URLs of files. The formatof these two options is a string list separated by commas. The list items are placed in a fixed order:, , The item value can be empty, but the comma shall not be omitted.To have the DHCP server discern the proper files based on switch-specific information, the OS must provide some sort of identityinformation for the server to classify the switches. Besides the aforementioned options, the OS attaches option 43 (vendor specificinformation) and option 60 (vendor class identifier) in DHCPv4 requests, and option 17 (vendor-opts) in DHCPv6. Option 60 is set asstring “Mellanox” and options 17 and 43 contain the following Mellanox-specific sub-options:• System Model• Chassis Part Number• Chassis Serial Number• Management MAC• System Profile• Mellanox Onyx™ Release VersionThe corresponding subtypes respectively are defined as:DHCP_VENDOR_ENCAPSULATED_SUBOPTION_TLV_TYPE_MODEL 1DHCP_VENDOR_ENCAPSULATED_SUBOPTION_TLV_TYPE_PARTNUM 2DHCP_VENDOR_ENCAPSULATED_SUBOPTION_TLV_TYPE_SERIAL 3DHCP_VENDOR_ENCAPSULATED_SUBOPTION_TLV_TYPE_MAC 4DHCP_VENDOR_ENCAPSULATED_SUBOPTION_TLV_TYPE_PROFILE 5DHCP_VENDOR_ENCAPSULATED_SUBOPTION_TLV_TYPE_RELEASE 6Upon receiving such DHCP requests from a client, the server should be able to map the switch-specific information to the target fileURLs according to predefined rules.Once the OS receives the URLs from the DHCP server, it executes ZTP as follows:a. Perform disk space cleanup if necessary and fetch the image if it does not exist locallyb. Resolve the image version:i. If it is already installed on an active partition, proceed to step 2ii. If it is installed on a standby partition, switch partition and rebootiii. If it is not installed locally, install it and switch to the new image and then rebootiv. If a reboot occurs, ZTP performs step 1 again and no image upgrade will occura. Fetch the configuration fileb. Apply the configuration filea. Fetch the docker image fileb. Load the docker imagec. Clean up the docker images with the same name and different tag.d. Start the container based on the imagee. Remove the downloaded docker image fileIf some sort of failure occurs, the switch waits a random number of seconds between 1 and 20 and reattempts the operation. Theswitch attempts this up to 10 times.ZTP progress is printed to terminals including console and active SSH sessionsDHCPv4 Configuration Example (Linux dhcpd)The below dhcpd.conf configuration has 3 items in the tftp-server-name option (option 66) and 3 items in the bootfile-name option (option 67) and it’s only sent when the dhcp client has “Mellanox” in it’s Vendor-Class identified (Option 60)class “Vendor-Class” {match option vendor-class-identifier;}class “Model” {match option onyx_vendor_specific.model;}subnet 192.168.100.0 netmask 255.255.255.0 {range 192.168.100.100 192.168.100.200;subclass “Vendor-Class” “Mellanox” {option tftp-server-name “tftp://192.168.100.22/, tftp://192.168.100.22/, tftp://192.168.100.22/”;option bootfile-name ""image-X86_64-3.8.1208.img, basic_mgmt_config.txt, init_Docker.tgz,"";option host-name “3700” ;}}See the switch console output when 3 items where provisioned below:Software image, text configuration file and a docker image[2019-08-07 12:56:38.345] Mellanox Onyx Switch Management[2019-08-07 12:56:38.350] switch-0b42d4 login:[2019-08-07 12:56:51.727] Initializing zero-touch[2019-08-07 12:56:57.245][2019-08-07 12:56:57.245] Waiting for zero-touch start[2019-08-07 12:57:27.375][2019-08-07 12:57:27.375] Fetching configuration basic_mgmt_config.txt[2019-08-07 12:57:27.401][2019-08-07 12:57:27.401] Applying configuration basic_mgmt_config.txt[2019-08-07 12:57:31.181][2019-08-07 12:57:31.181] Fetching docker image mft_4.12.3-14_Docker.tgz[2019-08-07 12:58:24.219][2019-08-07 12:58:24.219] Loading docker image from mft_4.12.3-14_Docker.tgz[2019-08-07 12:58:37.287][2019-08-07 12:58:37.287] Docker image mft:4.12.3-14 is loaded[2019-08-07 12:58:37.291][2019-08-07 12:58:37.291] Starting docker container ‘ztp_containter’ from image mft:4.12.3-14[2019-08-07 12:58:37.640][2019-08-07 12:58:37.640] Zero-touch is finished3700 [standalone: master] # show zero-touchZero-Touch status:11.106][2019-08-07 13:01:11.108] Active: yes[2019-08-07 13:01:11.112] Status: Zero-touch is finished[2019-08-07 13:01:11.119] Suppress-write: yes[2019-08-07 13:01:11.124] Configured by zero-touch: yes[2019-08-07 13:01:11.128] Configuration changed after zero-touch: noDHCP offer packet from the switch:DHCP Offer from the DHCP server:Hi,I know this article, but there is not described how to read from Option 43 information with an isc-dhcp-server. The documentation (this KB article) does not seem to be very clear. It only states that there are “corresponding subtypes defined” but not how to use or match them.Hi Sebastian,I also needed to push specific images and configs via ztp and needed to differentiate the devices beyond the vendor-class-identifier.  Below is a snippet from my isc-dhcp server config.
I’m specifying the ztp deployed image and config based on the switch model (in my case MQM8700).  You should be able to do the same for the current switch firmware level with suboption 6 (release).  I have an example of that below that is commented out.# Create option space for switch info
option space NVSWZTP;
option NVSWZTP.model       code 1 = text;
option NVSWZTP.partnum   code 2 = text;
option NVSWZTP.serial        code 3 = text;
option NVSWZTP.mac          code 4 = text;
option NVSWZTP.profile      code 5 = text;
option NVSWZTP.release    code 6 = text;
option NVSWZTP-43      code 43 = encapsulate NVSWZTP;
class “NVZTP” {
#match if option NVSWZTP.release = “3.10.1000”;
match if option NVSWZTP.model = “MQM8700”;
option tftp-server-name = “http://1.1.1.1/install/mlnx-os/,http://1.1.1.1/install/mlnx-os/”;
option bootfile-name “image-X86_64-3.1.1.img, switch.conf,”;
}Thank you, this worked pretty good and was exactly what I was looking for!Powered by Discourse, best viewed with JavaScript enabled"
1227,poor-performance-rdma,"hello, I’m opening a ticket to try to improve RDMA performance on our new Hyper-v Cluster.
we have 4 Hyper-V Windows2019DTC nodes, HPE DL360Gen10, with 2 Connectx-5 100GB / s cards, configured in Team-Set connected to two SN2100M switches. on it we have configured all the windows side PFC policies as per the WINof 2.80 manual. also on the switch we have enabled the roce and PFC with priority 3 and enabled on the ports.
running tests with diskpd the maximum speed in RDMA is about 2426 MB / S. I guess the speed must be considerably higher.
I have checked and double checked several times but I can’t find a solution.Hello,Have you used our RDMA performance tool that comes embedded with our driver? (nd_write/read/send_bw)
We do not test performance with the diskpd utility.Did you test b2b omitting the switch(s)?Are you monitoring the RDMA-traffic via perfmon?Did you validate recommended BIOS Settings for maximum performance have been set on both Windows servers?Have you consulted our WinOF2 UM section “Tunable Performance Parameters”? (IE: MTU, rcv/send buffers,RSS, etc)Note: Make sure MTU is matching on switch(s)/ports.
RSS configuration very important to distribute traffic across cores belonging to the NUMA closest to the HCA.Did you test omitting the Team-set? (IE: single HCA from both baremetal servers)?Was the FW aligned when version 2.80 was installed?Is or are the HCA’s installed on PCIe 3/4 x8/x16? 3x16 or 4x16 will provide better performance.Are you seeing drop counters from the switch(s) and/or servers?Sophie.Hello Sophie, thanks for the reply. I’ll answer you point by point with the tests I will perform.
yes I have run the tests with specific tools of the WINof 2.80 drivers.
this is a test with send_bw. the others are analogues to this result.
#qp #bytes #iterations MR [Mmps] Gb / s CPU Util.
0 65536 100000 0.186 97.73 100.00yes I’m monitoring the results also through windows perfmon …
in the bios settings I set the “Virtualization - Max Performance” workload. I think it is the most recommended since they are hyper-v hosts. By default this profile configures down all parameters in the WINof driver specification as suggested by mellanox.I looked at the performance tuning part. I don’t know whether to increase the ReciveBuffer and SendBuffer entries. or rather, I tried but found no benefit. as for the registry keys, it is not well documented, and I don’t understand if they also refer to Win2019.MTU is correct and is set to 9000 across the whole network, ports, switches.
and if it is disabled you will notice a lot the difference.Rss, I checked and everything seems ok. do you have any particular advice to check it?yes I also tried to bypass the team, using single ports.
but the results are identical.yes the firmware of the cards are updated to the latest version and match the driver 2.80. (also verified with HPE support).the cards are connected under PCI x16.
Bus Type: PCI-E 8.0 GT / s x16I don’t see anything dropped from the switch counter, nor from the windows performance.since you don’t find much in the forums or on the web, I’d like to know how much such a configuration should perform.
therefore I cannot understand if the infrastructure is really going to the maximum or not.Hello,You are getting 97.73Gbs so you are reaching rate line with our tool which imply our HCA is working as expected.
ROCE only supports active-standby so the bandwidth measurement is from 100Gbs only.Have you consulted with the vendor that blessed, support and tested such deployment?Sophie.yes of course, all the hardware configuration has been certified by our reseller during the purchase, and also certified by HP.the problem is that I can’t get a comparison with another configuration. I could even do a thousand tests, but without a comparison I wouldn’t know.
for example: the mellanox tools have calculated that the network travels at 97.73 gbs.a trivial test: the copy of 1 large file, such as a 50gb VHDX from the c: \ of one server to the c: \ of another server, the copy is about 800mbs …
possible? is this a wrong measurement? or am I doing something wrong?
server storage is on NVME (mixed use).I would recommend in parallel to engage the certified vendor(s) for QA inquiries, how do they test and what is the overall bandwidth expected. They should have some benchmarking numbers and applicable tool(s) they certified for this deployment. From our HCA perspective, the rate line is reached.Sophie.Powered by Discourse, best viewed with JavaScript enabled"
1228,packet-drop-if-ip-tos-contains-priority-bits,"Hi.I have ConnectX-6 LX card, and i’m trying to push some IP traffic through it using DPDK app. In most cases, this works as expected, but, if input traffic have random IP TOS bits set, then card starts to drop traffic.More precisely, if all incoming traffic have the same value for IP TOS, everything works as expected. But if incoming traffic have different values for IP TOS, and for each IP TOS value packet rate is >100k pps, then i’m unable to push more than ~3M pps of total traffic through single VF. Application is using SRIOV, and running inside of VM on hypervisor. Hypervisor is Openstack (KVM).For example, if IP TOS is set to 5, we can push >3M pps on single VF. If incoming traffic have ~1M5 pps of packets with IP TOS set to 5 and ~2M pps of packets with IP TOS set to 0, then maximum traffic that is coming to DPDK app is capped to ~3M pps.Does anyone else have this issue?Please note that i’m experiencing the same issue on both ConnnectX-4 LX and ConnectX-5 cards. In case of ConnecX-4 LX, the following commands make this problem disappear:The same “commands” does not work for other cards. Also, i do not have a clue what those commands do - i just found them somewhere.If anyone know what those commands do, please can provide them for other cards also?Regards,H.Hi,The mcra commands you mentioned are disabling priority check on the firmware level.
It’s not recommended to modify it unless you exactly know what it does.The described issue requires a deeper investigation, and more detailed information is required in order to do so (i.e. OS/driver/firmware/DPDK versions)In order to perform such investigation, we recommend to open a support case in Nvidia portal, and we will be happy to assist you.Best Regards,
AnatolyPowered by Discourse, best viewed with JavaScript enabled"
1229,multiple-connectx-3-cards-no-fiber-light-no-link-light,"I have an issue with my Mellanox ConnectX-3 cards where any transceivers don’t output any light.
The card is detected in Windows 11 and Ubuntu 22.10. Both ports are visible on both operating systems, but I can never get the transceivers to work, and the link lights never turn on.I’ve updated the firmware and drivers to the latest versions. I’ve tested different firmwares. I’ve reinstalled the firmwares and drivers multiple times.
I’ve even tried two different ConnectX-3 cards, of the same model.I’ve verified all the transceivers work and the fiber is good.
I can’t get any light from the transceivers. My light meter doesn’t detect anything.Card Models: Mellanox MCX314A-BCBT
Driver Version: 5.50.14740.1
Firmware Version: 2.42.5000Tested Transceivers: Cisco QSFP-40G-SR-BD, Cisco SFP-10G-SR
Fiber Type: LC-LC OM3 MMFOS: Windows 11 Pro for Workstation 23493, Ubuntu 22.10
Tested Multiple PCIe slots as well.I have no idea where to troubleshoot at this point. The cards seem to be detected fine, and even updated fine, so I don’t think they’re broken. The firmware release notes also specify it’s compatible with my 40Gig transceivers.image1194×485 50.6 KBConnectX-3 cards are EOL.You can try opening a support ticket and get the FW dumps extracted from the cards to try and explain why the modules aren’t being powered on (as it seems from your description).I got 2 known-working Mellanox MFM1T02A-SR modules and tested them in both cards on both servers running Windows and Ubuntu, and they’re doing the same thing; appearing to not power on at all.You can try opening a support ticket and get the FW dumps extractedHow would I go about doing this? Thank you for the reply!I think you need a support contract for opening a case with support.what happens with copper cables? Do you get a link up?
if that is the case – it may be that the PCIe slots to which the cards are connected aren’t providing sufficient power.What is the platform (motherboard etc.) you are connecting the card/s to?I think you need a support contract for opening a case with support.That’s unfortunate, but I understand. Thank you for helping me this far!what happens with copper cables? Do you get a link up?DAC cables come in Saturday, so I’ll be able to test those then.What is the platform (motherboard etc.) you are connecting the card/s to?The three platforms I’ve tried have been AMD X399, AMD B550, and Intel Z790
I’ve messed with the PCIe power settings, tried different slots, tried both PCIe 2.0 and 3.0. Manually set v3.0 on the slot. Toggled SR-IOV. And none of those factors appear to have any effect on the modules not turning on.The only reason for them not turning on will be the power or compatibility.do you happen to have the linux messages/dmesg output?I’m now testing an Intel Z170 machine running TrueNAS-22.12.2 (Debian 11)Here’s some potentially helpful command outputs for the ConnectX-3 card’s status.
This card is running the 314A-BCB (ETH Only) firmware. I’ve tested it an MCX354A-FCB VPI firmware on the cards.This is the dmesg output. (hit a character limit)
root@truenas[~]# dmesg[ 0.000000] microcode: microcode updated early to rev - Pastebin.comEdit: The two lines at the bottom, starting with  MFT device name created: id: 4099, are after I ran the mst start command.FYI - mlxlink won’t work on it (only CX4 onwards)Can you post the messages & dmesg files anywhere accessible when cables are connected?Not sure if this NIC generation has it – but it should report if it fails to activate the cables.you can also attach mstdump output from the device – I’ll try to review and see why the modules aren’t being turned on.mstdump -full /dev/mst/mt4099_pci_cr0Also ibstat and ethtool output from the machine may help.ibstatethtool -m <interface_name>ethtool <interface_name>thanks,DanFYI - mlxlink won’t work on it (only CX4 onwards)Thanks! Someone suggested I run it on the L1Techs forum post for this.Can you post the messages & dmesg files anywhere accessible when cables are connected?The previous dmesg was with two SFP+ 10gig mellanox modules plugged in with an sc-mmf cable connecting the two together.you can also attach mstdump output from the devicemt4099.dmp (764.2 KB)I’m unable to install the infiniband-diags package for the ibstat command, as TrueNAS doesn’t support adding packages, but I’m sure I can work around that if needed.ethtool output for each interface: (The 10Gig mellanox SFP+ modules appear to be detected)As always, thank you so much for helping!Good news! Maybe…My SFP+ 10gig DAC cables just arrived (10GTek CAB-10GSFP-P3M). I plugged them into my Mellanox ConnectX-3 QSFP+ 40Gig card using the QSFP to SFP adapters and link came right up.So…
The card works. It can recognize and print out any transceiver I put in it, but it never puts light out of the transceiver. They stay off. That happens with BOTH Cisco and Mellanox modules.I’m at a complete loss as to what would be causing this and what the next troubleshooting step would be.
This is happening to BOTH of my Mellanox 314A 40Gig cards on any motherboard and any operating system.Can you please share the motherboard details?Can you please share the motherboard details?Currently using an NZXT N7 B550.
It’s currently in the PCI-E 16x slot, with the version manually set to 3.0. It should be outputting the full 75 watts capable.
I’ve tried its 3.0 4x slot as well with the same results.I’ve also tried an ASUS Z170 ProI don’t have my ASUS ROG STRIX X399-E GAMING motherboard available anymore, but it was doing the same thing as well in the top PCIe 16x slot.you can also attach mstdump output from the device – I’ll try to review and see why the modules aren’t being turned on.Did you get a chance to do this? I really appreciate all the time you’ve spent helping me thus far.I’m looking at purchasing ConnectX-3 354A cards to replace these 314A cards to see if it makes a difference.The ConnectX-3 354A cards came in.
They’re doing the EXACT same thing as the 314A cards.
Works totally fine with DAC cables, but in any system at all, any fiber module plugged in never turns on its led/laser, and there’s never link light.I’m at a complete loss on what to do from here anymore.I’m currently at the point where I think these cards only support 1.5 watts per port, and I think all these optics have been 2.5 watts and 3.5 watts.I don’t know if there any way to override the max power per port?
I have very active cooling on the card.Tried the CX314 and CX354 cards in a Supermicro MBDH12SSLNTO Motherboard with an EPYC 7763 and still had the same exact issue.Went ahead and returned all the CX3 cards and bought some CX4 cards. Going to see if they make a difference, especially since I’m using BIDI transceivers.Powered by Discourse, best viewed with JavaScript enabled"
1230,connectx-6-dx-sr-iov-libvirt-error,"hi,I’m trying to provide a VF to my instance via libvirt, but I’m getting the following error when I entered the command virsh create instance.xml:The MLNX_OFED version I have installed is MLNX_OFED_LINUX-5.5-1.0.3.2.Please let me know how to mitigate the above error.Hi bk-2,Please verify if the  libvirt version is 7.1.0 and above. Based on the error string shared, it looks like you are experiencing issue identified as a bug in libvirt that was fixed in following commit —> util: Add phys_port_name support on virPCIGetNetName · libvirt/libvirt@5b1c525 · GitHubIf correct version of libvirt is used on your setup and if this involves further debug, I would like to request opening a support ticket by emailing Networking-support@nvidia.comPlease note a valid support contract is needed for the same. For details on contracts, please feel free to contact our contracts team at Networking-contracts@nvidia.comThanks,
Namrata.Thanks for your reply.I’ll check it out and bring it here if there are any errors.Powered by Discourse, best viewed with JavaScript enabled"
1231,connectx-6-lx-teaming-mlx5muxtool-doesnt-find-adapters,"Hello there,i recently installed a connectx-6 lx card in a workstation running windows 10 21H2. i downloaded and installed the latest WINOF2-driver version 2.70.53000_all with all features.in my device manager i can get the following information:Driver version: 2.70.24739.0Firmware version: 26.31.1014Part number: MCX6311102AC-ADATI then ran the mlx5up.exe to ensure the latest firmware version is installed.running “mlx5muxtool showlist” shows no results.i checked the registry. the value for installed path unter HKEY_LOCAL_MACHINE_\SOFTWARE\MELLANOX\MLNX_WINOF2 is C:\Program Files\Mellanox\MLNX_WINOF2so in theory I assume the muxtool should be able to find the mux drivers. What am i missing here? Can anybody help me in that case?best regards,Andreas WolfDear Andreas,I had the same issue, but I managed to find the IDs for the adapters using PowerShell:Get-NetAdapter | Select InstanceID, NameRegards,
PeterDear Peter,until now I didn’t find a solution. There is a new driver version Win-OF-2 2.90 released. I might give that a try together with checking for the latest firmware on the cards.
It seems though that Mellanox isn’t updating the mlx5muxtool. But still worth a try. Other people seemed to have had success with Connectx-6 cards, as far as I remember.Regards,
AndreasHi Peter,
with the updated Win-OF2 3.0 release mlx5muxtool.exe finds adapters and you can create a team normally.
best regards,
AndreasPowered by Discourse, best viewed with JavaScript enabled"
1232,mlnx-ofed-undre-rhel7-9,"Hi all,
did anyone succedded in installing mlnx-ofed-all from repository?
I am stuck at his point:Error: Package: rdma-core-devel-55mlnx37-1.55103.x86_64 (mlnx_ofed)
Requires: pkgconfig(libnl-3.0)
Error: Package: rdma-core-devel-55mlnx37-1.55103.x86_64 (mlnx_ofed)
Requires: pkgconfig(libnl-route-3.0)
You could try using --skip-broken to work around the problemIt seems that there is some issue with libnl.
Before trying with --skip-broken option, i’d like to be sure that libnl3-devel is not available for rdhat EL 7.9.Thank youI have… Are you using this repo?Index of /public/repo/mlnx_ofed (mellanox.com)Hi, i had meet the same problem when i installed mlnx-ofed 5.8.1.1.2.  Have you found a good solution?
Thank you[root@centos-7.5 ~]# yum check dependencies
Loaded plugins: langpacks, versionlock
rdma-core-devel-58mlnx43-1.58112.x86_64 has missing requires of pkgconfig(libnl-3.0)
rdma-core-devel-58mlnx43-1.58112.x86_64 has missing requires of pkgconfig(libnl-route-3.0)
Error: check [‘dependencies’]Powered by Discourse, best viewed with JavaScript enabled"
1233,how-to-enable-bluefield-snap-on-dpu,"Recently I was researching how to use Bluefield’s storage acceleration feature. I learned that SNAP technology enables hardware-accelerated virtualization of NVMe storage, interfering with presenting networked storage as a local NVMe SSD.Now I have a question, how to enable SNAP function on mlx5 Bluefield DPU? Are there any relevant operation guides? Thanks!To use SNAP you need contact NV sales to get license.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1234,how-to-use-extended-atomics,"I’m hoping someone can help me get started using extended atomics. Specifically, I’m interested in the masked compare and swap operation. As of now, I am finding it hard to track down any documentation on the subject. The one doc on the Advanced Transports page (of the 5.3 InfiniBand driver) mentions that it is not currently supported by rdma-core. However, I did find some patches that appear to be adding it in the future. Is there an alternative I should be using in the meantime? Any suggestions would be greatly appreciated!Thanks,JacobHi Jacob,I reviewed our RN/UM from our latest 5.3 driver and I do not see such statement.Since MLNX_OFED 5.1 we have fully transition to Upstream Verbs from Legacy/Experimental Verbs though I do not see an issue documented using Atomic Operations with Upstream/RDMA-core.What document did you consult?From our web page: Linux InfiniBand DriversYou will have access to our RN/UM & RDMA Aware Network Programing User Manual for more references.Sophie.Hi Sophie,Thanks for the quick response! Just to clarify, I am curious about how to use extended atomics, not regular atomics. Also, I was mistaken about the version number. I was actually referring to this which states at the bottom that the extended atomics aren’t supported in rdma-core.Typically, when programming RDMA-aware applications I will include the <infiniband/verbs.h> header and be on my way, but that doesn’t support extended atomics. The documentation in the RDMA Aware Network Programming User Manual (v1.7) references some structs and macros for extended atomics but those symbols are not defined in infiniband/verbs.h. Is there an alternative header and library that I should be using? Is there sample code anywhere with a working example of extended atomics? I apologize if I am missing something but it is not clear what I should be doing.Thanks so much for the help,JacobHi Jacob,After validating internally with our engineering team, extended atomics are not and will not be supported with RDMA-CORE. Driver version 5.1 and on documentation will be updated accordingly.Sophie.Powered by Discourse, best viewed with JavaScript enabled"
1235,sdk-manager-to-install-doca-1-2-0-for-bf-2-dpu-flash-os-on-target-device-stuck-at-99,"Hi! I have a BF os runnning previously, and I would like to reinstall the whole BF OS.
When I uninstall all sdk, and run sdkmanager again, it just failed and stuck at 99% when doing the flashing OS on target device.Here is the screenshot of the report from terminal after I cancel it (because it takes over 2 hours)
Before the flashing, everything is installed successfully according to the log.
image1259×736 193 KB
I am not really sure what could be wrong, as I have tried sdkmanager and successfully installed one before.Any help is much appreciated!helpI am using command line sdk manager to install doca1.5.1 and also stuck at 99% while brushing into os, did you have any solution yet?Powered by Discourse, best viewed with JavaScript enabled"
1236,roce-device-on-linux-bond-with-connectx5,"I have a setup similar to https://www.openfabrics.org/images/eventpresos/2016presentations/303RDMAUserSpc.pdf slide 6, where we have a linux bond on top of mlx5_0, mlx5_1.  Has anyone had similar setup?  How to create RDMA device on top of bond0?You can follow below to create RoCE LAG.https://docs.nvidia.com/networking/pages/viewpage.action?pageId=107485819#RDMAoverConvergedEthernet(RoCE)-RoCELAGhttps://enterprise-support.nvidia.com/s/article/How-to-Configure-RoCE-over-LAG-ConnectX-4-ConnectX-5-ConnectX-6These didn’t help. roce_lag_enable is no where to be found.roce_lag_enable no needThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1237,spdk-mlx5-crypto-devs-get-warning-dev-mlx5-0-uses-wrapped-import-method-0x1-which-is-not-supported-by-mlx5-accel-module,"mlx5 crypto device is only support plain text mode now.
How can I change  wrapped mode to plain mode?root@localhost:~# spdk_tgt --wait-for-rpc
SPDK_XLIO_PATH is not defined. XLIO socket implementation is disabled.
[2023-06-09 06:55:54.367452] Starting SPDK v23.01 / DPDK 22.11.1 initialization…
[2023-06-09 06:55:54.367826] [ DPDK EAL parameters: spdk_tgt --no-shconf -c 0x1 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid12451 ]
EAL: Requested device 0000:03:00.0 cannot be used
auxiliary bus: Requested device mlx5_core.sf.1 cannot be used
EAL: Bus (auxiliary) probe failed.
TELEMETRY: No legacy callbacks, legacy socket not created
[2023-06-09 06:55:54.470552] app.c: 712:spdk_app_start: NOTICE: Total cores available: 1
[2023-06-09 06:55:54.632423] reactor.c: 926:reactor_run: NOTICE: Reactor started on core 0
[2023-06-09 06:57:01.538062] accel_rpc.c: 169:rpc_accel_assign_opc: NOTICE: Operation encrypt will be assigned to module dpdk_cryptodev
[2023-06-09 06:57:02.062057] accel_rpc.c: 169:rpc_accel_assign_opc: NOTICE: Operation decrypt will be assigned to module dpdk_cryptodev
[2023-06-09 06:57:28.152013] accel_sw.c: 681:sw_accel_module_init: NOTICE: Accel framework software module initialized.
[2023-06-09 06:57:28.183577] mlx5_crypto.c:  99:spdk_mlx5_crypto_devs_get: WARNING: dev mlx5_0 uses wrapped import method (0x1) which is not supported by mlx5 accel module
[2023-06-09 06:57:28.183677] mlx5_crypto.c: 104:spdk_mlx5_crypto_devs_get: NOTICE: Crypto dev mlx5_0
[2023-06-09 06:57:28.184235] accel_mlx5.c:1209:accel_mlx5_crypto_ctx_mempool_create: NOTICE: Total pool size 2048, cache size 1536
[2023-06-09 06:57:29.682703] accel_mlx5.c:1295:accel_mlx5_init: NOTICE: Accel framework mlx5 initialized, found 1 devices.
[2023-06-09 06:57:29.682867] accel.c:2225:spdk_accel_initialize: ERROR: Invalid module name of dpdk_cryptodev
[2023-06-09 06:57:29.682911] subsystem.c: 143:spdk_subsystem_init_next: ERROR: Init subsystem accel failedroot@localhost:~# lspci -s 03:00.0 -vv
03:00.0 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)
Subsystem: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller
Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+
Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- SERR- <PERR- INTx-
Latency: 0
Interrupt: pin B routed to IRQ 74
Region 0: Memory at e200000000 (64-bit, prefetchable) [size=32M]
Region 2: Memory at e202000000 (64-bit, prefetchable) [size=8M]
Expansion ROM at e000000000 [disabled] [size=1M]
Capabilities: [60] Express (v2) Endpoint, MSI 00
DevCap:	MaxPayload 512 bytes, PhantFunc 0, Latency L0s unlimited, L1 unlimited
ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset+ SlotPowerLimit 0.000W
DevCtl:	CorrErr- NonFatalErr- FatalErr- UnsupReq-
RlxdOrd+ ExtTag+ PhantFunc- AuxPwr- NoSnoop+ FLReset-
MaxPayload 128 bytes, MaxReadReq 512 bytes
DevSta:	CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-
LnkCap:	Port #0, Speed 16GT/s, Width x16, ASPM not supported
ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+
LnkCtl:	ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
LnkSta:	Speed 16GT/s (ok), Width x16 (ok)
TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-
DevCap2: Completion Timeout: Range ABC, TimeoutDis+ NROPrPrP- LTR-
10BitTagComp+ 10BitTagReq- OBFF Not Supported, ExtFmt- EETLPPrefix-
EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-
FRS- TPHComp- ExtTPHComp-
AtomicOpsCap: 32bit- 64bit- 128bitCAS-
DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR- OBFF Disabled,
AtomicOpsCtl: ReqEn-
LnkCap2: Supported Link Speeds: 2.5-16GT/s, Crosslink- Retimer+ 2Retimers+ DRS-
LnkCtl2: Target Link Speed: 16GT/s, EnterCompliance- SpeedDis-
Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-
Compliance De-emphasis: -6dB
LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete+ EqualizationPhase1+
EqualizationPhase2+ EqualizationPhase3+ LinkEqualizationRequest-
Retimer- 2Retimers- CrosslinkRes: unsupported
Capabilities: [48] Vital Product Data
Product Name: BlueField-2 E-Series DPU, 200GbE/HDR single-port QSFP56, Secure Boot Enabled, Crypto Enabled, 16GB on-board DDR, 1GbE OOB management, HHHL
Read-only fields:
[PN] Part number: MBF2M345A-HECOT
[EC] Engineering changes: A6
[V2] Vendor specific: MBF2M345A-HECOT
[SN] Serial number: MT2206X03614
[V3] Vendor specific: 704a98ffd089ec1180001070fd2a949c
[VA] Vendor specific: MLX:MN=MLNX:CSKU=V2:UUID=V3:PCI=V0:MODL=BF2M345A
[V0] Vendor specific: PCIeGen4 x16
[VU] Vendor specific: MT2206X03614ECMLNXS0D0F0
[RV] Reserved: checksum good, 1 byte(s) reserved
End
Capabilities: [9c] MSI-X: Enable+ Count=64 Masked-
Vector table: BAR=0 offset=00002000
PBA: BAR=0 offset=00003000
Capabilities: [c0] Vendor Specific Information: Len=18 <?>
	Capabilities: [40] Power Management version 3
		Flags: PMEClk- DSI- D1- D2- AuxCurrent=375mA PME(D0-,D1-,D2-,D3hot-,D3cold+)
		Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=0 PME-
	Capabilities: [100 v1] Advanced Error Reporting
		UESta:	DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
		UEMsk:	DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
		UESvrt:	DLP+ SDES- TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-
		CESta:	RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-
		CEMsk:	RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+
		AERCap:	First Error Pointer: 08, ECRCGenCap+ ECRCGenEn- ECRCChkCap+ ECRCChkEn-
			MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-
		HeaderLog: 00000000 00000000 00000000 00000000
	Capabilities: [150 v1] Alternative Routing-ID Interpretation (ARI)
		ARICap:	MFVC- ACS-, Next Function: 1
		ARICtl:	MFVC- ACS-, Function Group: 0
	Capabilities: [1c0 v1] Secondary PCI Express
		LnkCtl3: LnkEquIntrruptEn- PerformEqu-
		LaneErrStat: 0
	Capabilities: [320 v1] Lane Margining at the Receiver <?>
Capabilities: [370 v1] Physical Layer 16.0 GT/s <?>
	Capabilities: [420 v1] Data Link Feature <?>
Kernel driver in use: mlx5_core
Kernel modules: mlx5_coreroot@localhost:~# flint -d /dev/mst/mt41686_pciconf0 q
Image type:            FS4
FW Version:            24.37.1300
FW Release Date:       11.5.2023
Product Version:       24.37.1300
Rom Info:              type=UEFI Virtio net version=21.4.10 cpu=AMD64,AARCH64
type=UEFI Virtio blk version=22.4.10 cpu=AMD64,AARCH64
type=UEFI version=14.30.13 cpu=AMD64,AARCH64
type=PXE version=3.7.102 cpu=AMD64
Description:           UID                GuidsNumber
Base GUID:             1070fd03002a949c        8
Base MAC:              1070fd2a949c            8
Image VSD:             N/A
Device VSD:            N/A
PSID:                  MT_0000000716
Security Attributes:   secure-fwroot@localhost:~# sudo flint -d /dev/mst/mt41686_pciconf0 vFS4 failsafe image-I- FW image verification succeeded. Image is bootable.root@localhost:~# sudo mlxreg -d /dev/mst/mt41686_pciconf0 --get --reg_name CRYPTO_OPERATIONAL
Sending access register…Hi Jackylu33,Thank you for posting your query on NVIDIA community.Based on internal, in order to check if/how to change wrapped mode to plain mode, a support ticket will be needed. The support ticket can be opened by emailing "" Enterprise-support@nvidia.com ""Please note that an active support contract would be required for the same. For contracts information, please feel free to reach out to our contracts team at "" Networking-Contracts@nvidia.com ""Thanks,
Namrata.Powered by Discourse, best viewed with JavaScript enabled"
1238,connect-x4-4117-adaptor-cannot-get-throughput-on-server-2019-dc-more-than-10gbps-adaptor-connected-to-dell-s5048f-on-switch,"HI, I have 2 x Dell R730XD servers each with a 4117 Connect X card, all drivers installed 100%, yet when I run iperf test I cannot get mote than 10Gbps from test. Cables are also rated for 25Gbps. Any idea’s on what to checkDriver Version : 2.60.23957.0Firmware Version : 14.27.6122Port Number : 1Bus Type : PCI-E 8.0 GT/s x8Link Speed : 25.0 Gbps/Full DuplexPart Number : 0MRT0DSerial Number : IL0MRT0D74031998003PDevice Id : 4117Revision Id : 0Current MAC Address : B8-59-9F-D1-11-DAPermanent MAC Address : B8-59-9F-D1-11-DANetwork Status : ConnectedAdapter Friendly Name : SLOT 6 Port 1Port Type : ETHIPv4 Address #1 : 192.168.1.108IPv6 Address #1 : fe80::5ca0:c53f:17e6:dcb2%5I would suggest that you implement the following steps:install first the latest Mellanox WInMFT tool in the Win2019 server (https://mellanox.com/products/adapter-software/firmware-tools)Run “mlxlink” tool PowerShell command to confirm that the actual link of your adapter vs. Dell switch is indeed 25 Gb/sFor example# mlxlink.bat -d mt4117_pciconf0 -c+++++++++++++++++++++++++++++++++++Operational InfoState : ActivePhysical state : LinkUpSpeed : 25GbE+++++++++++++++++++++++++++++++++++if 25Gb/s is confirmed, then check with Dell folks if the switch Link-type is properly configureduse the WinOF-2 v2.60 User-Manual to perform driver/fw/pci “performance fine tuning” on the Win2019 win server(https://docs.mellanox.com/display/winof2v26051000/Introduction)Iperf tool is more like oriented for Linux whilst NTttcp test-tool is Windows orientedPowered by Discourse, best viewed with JavaScript enabled"
1239,sharp-failed-to-initialize-sharp-collectives,"I am encountering SHARP-related error when testing qe/6.8
System: AMD EPYC 7543 and 8 x A100-SXM-80GB
OS: CentOS 7.9.2009 with 3.10.0-1160 kernel
Env: HPC-X from nvidia_hpc_sdk/21.9The system is exactly same as in my previous post , with the exceptions that we are using HPC-X from SDK/21.9.Since there is not much information on the impact of SHARP collectives on scientific computing softwares such as GROMACS/LAMMPS/QE, we are conducting a systematic investigation.[Problem description]
QE immediately crashes when SHARP is enabled.So clearly its a daemon-related issue. My questions are:Regards.Hi
What is the status ?
It seems like there is issue for sharpd.
Thanks,Hi,We solved the problem by pointing HPCX_SHARP_DIR to Mellanox’s OFED installation directory.
(https://docs.nvidia.com/networking/display/SHARPv261/Setting+up+NVIDIA+SHARP+Environment)When checking the debug message, we encountered the following non-critical error:We appreciate if you can clarify the following difference between SHARP binaries distributed with HPC-X and MLNX_OFED:Regards.Powered by Discourse, best viewed with JavaScript enabled"
1240,how-to-enable-mellanox-drivers-for-vpp-in-ubuntu-20-04,"Then, install the following packages:Next, copy the Mellanox library to the /usr/lib directory:Then, make the deb packages that will later be installed:I have followed this process for enable the Mellanox drivers in VPP but I am still unable to see the drivers in VPPwhen I do #show interface’s in VPPso How to enable Mellanox drivers for VPP ?please can anyone help me this issuePowered by Discourse, best viewed with JavaScript enabled"
1241,announcing-new-open-source-rdma-api,"NI has just released as open source a new abstraction layer for RDMA, easyRDMA. This API is currently oriented mostly for streaming use cases of RDMA and is cross-platform between Windows and Linux. The goal is to make RDMA easy-to-use for end applications that need to stream data between systems in a low-latency, high-performance manner.An easy-to-use, cross-platform, MIT-licensed RDMA library from NI. - GitHub - ni/easyrdma: An easy-to-use, cross-platform, MIT-licensed RDMA library from NI.EricHi Eric, I just started playing with this package. Looks very good and interesting. Just curious if you are able to build Windows or Linux RDMA executables from this? Thanks, PaulYes, the whole point of this library is to let you make Windows and Linux binaries that can easily use RDMA (and communicate to each other).Powered by Discourse, best viewed with JavaScript enabled"
1242,does-mellanox-nics-5-x-support-rdma-when-iommu-is-enabled-on-systems,"I am trying to configure a test system with AMD CPU’s, Mellanox NICs and a GPU (AMD or Nvidia) to perform RDMA. The system needs to have IOMMU enabled.Perform RDMA from GPU memoryPerform RDMA from system memoryLastly, does Mellanox have a PCIe reference card that can be used to test system setup for functionality and performance.Regards,RameshHi Ramesh,Please contact our support team for further assistance at networking-support@nvidia.comThanks,SamerPowered by Discourse, best viewed with JavaScript enabled"
1243,connect-rdma-between-host-and-dpu,"I am using the cloudlab r7525 node and the bluefield 2 on it.
I follow the suggestion from my previous post to install the DOCA on the DPU.
https://docs.nvidia.com/doca/sdk/installation-guide-for-linux/index.html#installing-software-on-hostNow I can ssh to the DPU. However, there is no network between the host and the DPU. I want to use RDMA between them.
The ip link on the dpu is shown as below:
ubuntu@localhost:~$ ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: tmfifo_net0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
link/ether 00:1a:ca:ff:ff:01 brd ff:ff:ff:ff:ff:ff
3: oob_net0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc fq_codel state DOWN mode DEFAULT group default qlen 1000
link/ether 0c:42:a1:da:aa:50 brd ff:ff:ff:ff:ff:ff
4: p0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq master ovs-system state DOWN mode DEFAULT group default qlen 1000
link/ether 0c:42:a1:da:aa:4a brd ff:ff:ff:ff:ff:ff
5: p1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq master ovs-system state DOWN mode DEFAULT group default qlen 1000
link/ether 0c:42:a1:da:aa:4b brd ff:ff:ff:ff:ff:ff
6: pf0hpf: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
link/ether 0a:b7:7b:37:08:6e brd ff:ff:ff:ff:ff:ff
7: pf1hpf: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
link/ether c6:68:bf:e8:78:66 brd ff:ff:ff:ff:ff:ff
8: en3f0pf0sf0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
link/ether 62:68:db:37:8d:38 brd ff:ff:ff:ff:ff:ff
9: enp3s0f0s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 02:69:57:54:5b:91 brd ff:ff:ff:ff:ff:ff
10: en3f1pf1sf0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
link/ether 92:28:11:e6:78:b3 brd ff:ff:ff:ff:ff:ff
11: enp3s0f1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 02:39:79:94:6d:95 brd ff:ff:ff:ff:ff:ff
12: ovs-system: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
link/ether 2a:3b:d7:f4:c3:47 brd ff:ff:ff:ff:ff:ff
13: ovsbr1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
link/ether 0c:42:a1:da:aa:4a brd ff:ff:ff:ff:ff:ff
14: ovsbr2: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
link/ether 0c:42:a1:da:aa:4b brd ff:ff:ff:ff:ff:ffThe ip link on the host is shown below:
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eno1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 4c:d9:8f:22:01:d8 brd ff:ff:ff:ff:ff:ff
3: eno2: <BROADCAST,MULTICAST> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000
link/ether 4c:d9:8f:22:01:d9 brd ff:ff:ff:ff:ff:ff
13: tmfifo_net0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UNKNOWN mode DEFAULT group default qlen 1000
link/ether 00:1a:ca:ff:ff:02 brd ff:ff:ff:ff:ff:ff
18: eno33np0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
link/ether 1c:34:da:72:f9:3e brd ff:ff:ff:ff:ff:ff
19: eno34np1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
link/ether 1c:34:da:72:f9:3f brd ff:ff:ff:ff:ff:ff
20: ens5f0np0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 0c:42:a1:da:aa:46 brd ff:ff:ff:ff:ff:ff
21: ens5f1np1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 0c:42:a1:da:aa:47 brd ff:ff:ff:ff:ff:ffI have assigned ip to both ens5f0np0 and ens5f1np1 on the host. And I have tried to assign ip to the DPU’s interface(pf0hpf/enp3s0f0s0). None of them can work. I wonder what might be a possible solution or cause for this?Hello cxinyic,Thank you for posting your inquiry to the NVIDIA Developer Forums.Please review the ‘No connectivity between network interfaces of source host to destination device’ section of the ‘Connectivity Troubleshooting’ section of the Bluefield OS guide:
https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/Connectivity+TroubleshootingPlease also review the ‘Verifying Host Connection on Linux’ and ‘Verifying Connection from Host to BlueField’ section in the ‘Virtual Switch on DPU’ section for host-to-ARM connection validation:
https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/Virtual+Switch+on+DPUIf after following those guidelines, you are still unable to configure host-to-DPU networking, and you have valid support entitlement, we recommend opening a support ticket with Enterprise Support via the following link: https://enterprise-support.nvidia.com/s/create-caseThanks, and best regards,
NVIDIA Enterprise ExperiencePowered by Discourse, best viewed with JavaScript enabled"
1244,devices-health-compromised-firmware-internal-error,"Hello, guysPlease help me to investigate following issue.
I have two Dell servers with two MCX4121A-ACA_Ax card installed. OS Ubuntu 22.04 LTS.
Recently I have done firmware upgrade to latest version: 14.32.1010 and now I have following error in dmesg on both of the servers for both of cards:[   14.944178] mlx5_core 0000:3b:00.0: poll_health:971:(pid 0): device’s health compromised - reached miss count
[   14.946551] mlx5_core 0000:3b:00.0: print_health_info:491:(pid 0): Health issue observed, firmware internal error, severity(3) ERROR:
[   14.951182] mlx5_core 0000:3b:00.0: print_health_info:495:(pid 0): assert_var[0] 0x00000000
[   14.953561] mlx5_core 0000:3b:00.0: print_health_info:495:(pid 0): assert_var[1] 0x000000b9
[   14.955588] mlx5_core 0000:3b:00.0: print_health_info:495:(pid 0): assert_var[2] 0x00000040
[   14.957305] mlx5_core 0000:3b:00.0: print_health_info:495:(pid 0): assert_var[3] 0x00000000
[   14.958959] mlx5_core 0000:3b:00.0: print_health_info:495:(pid 0): assert_var[4] 0x00000000
[   14.960531] mlx5_core 0000:3b:00.0: print_health_info:495:(pid 0): assert_var[5] 0x00000000
[   14.962061] mlx5_core 0000:3b:00.0: print_health_info:498:(pid 0): assert_exit_ptr 0x008771e4
[   14.963559] mlx5_core 0000:3b:00.0: print_health_info:499:(pid 0): assert_callra 0x00810ba4
[   14.964931] mlx5_core 0000:3b:00.0: print_health_info:500:(pid 0): fw_ver 14.32.1010
[   14.965864] mlx5_core 0000:3b:00.0: print_health_info:502:(pid 0): time 0
[   14.966787] mlx5_core 0000:3b:00.0: print_health_info:503:(pid 0): hw_id 0x0000020b
[   14.967690] mlx5_core 0000:3b:00.0: print_health_info:504:(pid 0): rfr 0
[   14.968584] mlx5_core 0000:3b:00.0: print_health_info:505:(pid 0): severity 3 (ERROR)
[   14.969454] mlx5_core 0000:3b:00.0: print_health_info:506:(pid 0): irisc_index 2
[   14.970310] mlx5_core 0000:3b:00.0: print_health_info:507:(pid 0): synd 0x1: firmware internal error
[   14.971166] mlx5_core 0000:3b:00.0: print_health_info:509:(pid 0): ext_synd 0x805b
[   14.972008] mlx5_core 0000:3b:00.0: print_health_info:510:(pid 0): raw fw_ver 0xe02003f2Here is device hw query output:flint -d 3b:00.1 -ocr hw query-W- Firmware flash cache access is enabled. Running in this mode may cause the firmware to hang.
HW Info:
HwDevId                 523
HwRevId                 0x0
Flash Info:
Type                    W25QxxBV
TotalSize               0x1000000
Banks                   0x1
SectorSize              0x1000
WriteBlockSize          0x10
CmdSet                  0x80
QuadEn                  1
Flash0.WriteProtected   Top,8-SubSectors
JEDEC_ID                0x1840efNow I need to get these servers to production but think that this error can get me to network interruption issues.Please help and advice.Could anyone help on investigating this?Hi Amogilny
Thank you for contacting us.From the HW query ouput, we do not see any issue on the HW.
The error only occurs in HCA boot-up flow, and does not occur when HCA is running (after boot-up).
So the issue will not have impact on normal operation.
If you face same issue during normal operation, please re-install or upgrade the firmware.Thank you,
NVIDIA Network SupportHello Mansunc
Thanks for clearing this out.Currently we do not have any impact on the network operation.
Just wanted to understand that this is not an issue to worry as our new servers are just starting to work in production.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1245,why-is-the-interface-names-changing,"Using Ubuntu 18.04 and their kernels the interface name comes up as enp2s0f0.When installing the mlnx-en-dkms package (version 5.2-1.0.4.0.g13e6dd1) driver provided by mellanox the interface name comes up as enp2s0f0np0How can I prevent / preserve the name change between reboots / reloading the mlx5_core kernel module? Is there a single udev rule that will keep the name the same where I don’t have to specify a specific rule for each mellanox mac?Or is systemd causing this some how?Hi,Please refer to the below section in MLNX_OFED UM for PersistentNaminghttps://docs.mellanox.com/display/OFEDv522200/Ethernet+Interface#EthernetInterface-PersistentNamingThanks,SamerSure that works but I’m looking for a rule a little more dynamic without needing to know the mac adddress of each interface. Really just want it to match the name scheme that the in kernel driver uses.what is the “np” part of new interface name mean/referrs?Powered by Discourse, best viewed with JavaScript enabled"
1246,interconnection-between-hdr-and-fdr-switches-cont,"I’ve connected an FDR switch SX6025 (leaf) to HDR switch QM8790 (tor).  The connection works fine (seeing beegfs ib rdma on libvirt nodes), but when trying to query or update the firmware of SwitchX, I getroot@v001:~# flint -d /dev/mst/SW_MT51000_SwitchX_lid-0x0041 q
FATAL - Can’t find device id.
-E- Cannot open Device: /dev/mst/SW_MT51000_SwitchX_lid-0x0041. MFE_UNSUPPORTED_DEVICEHCA’s are all updated to the latest Connect-X3 firmware.Any suggestions on how I can update the firmware of this MSX6025F-1SFR?I’m using FDR cables.  Suggestions?Reference:Hi, Tore,
Could you let us know the mft tool version are you use ?Thanks,Powered by Discourse, best viewed with JavaScript enabled"
1247,can-run-roce-and-posix-socket-on-a-port-simultaneously,"I have a RDMA card with 2 prots, I want to know if I can run RoCE and Posix Socket **use only one port （NOT use 2 port, one run RoCE, the other use POSIX Socket）**simultaneously ？Hello Liu,Thank you for posting your inquiry on the NVIDIA Networking Community.Yes, our dual-port adapters are capable of running RoCE traffic and regular POSIX Socket (tcp/udp) simultaneously.Thank you and regards,~NVIDIA Networking Technical SupportHello Martijn, Thanks For you ​reply.There may be some ambiguity in my question.for example , my rdma card has port#1 and port#2, I only use port#1.My question is can I run Roce traffci and posix Socket simultaneously on port#1?Powered by Discourse, best viewed with JavaScript enabled"
1248,connectx5-asap2-ovs-vxlan-offload-bond-not-working-properly,"Hello there,Maybe someone can help me.Last time I was here with the same problem, I’ve been told to try with the latest openvswitch build so I did:ovs-vsctl (Open vSwitch) 2.11.90It improved things a little, but still not perfect.Setup:Dual port ConnectX5 (MT27800) latest firmware.Ubuntu 18.04 Linux kernel: 4.18.0-25-generic(No ofed drivers as the ofed driver fails to enable eswitch with this: (0000:3b:00.0): E-Switch: Failed setting eswitch to offloads)LACP active on both of the ports (port0 and port1) on the connectx5, systemd activates the lacp and the SRIOV sub interfaces before networking starts. OVS offloading in ovs enabled.SRIOV subinterfaces on port0 created (boot time), port1 doesn’t have any SRIOV subinterfaces created.When the vm’s traffic (using asap2) goes through the port0 (left port on the cart), all works fine, everything gets offloaded:listening on eth3, link-type EN10MB (Ethernet), capture size 262144 bytes15:32:36.042217 IP 10.100.140.15 > 10.100.140.3: ICMP echo request, id 1357, seq 1, length 6415:32:36.042451 IP 10.100.140.3 > 10.100.140.15: ICMP echo reply, id 1357, seq 1, length 6415:32:41.166466 ARP, Request who-has 10.100.140.3 tell 10.100.140.15, length 4615:32:41.166588 ARP, Reply 10.100.140.3 is-at fa:16:3e:04:b7:60, length 46As we see only 2 packet hits the virtual port as seen in the docs.When I down port0 and the flow moves to port1 (right port on the card), this happens:tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth3, link-type EN10MB (Ethernet), capture size 262144 bytes15:38:47.401961 IP 10.100.140.15 > 10.100.140.3: ICMP echo request, id 1362, seq 40, length 6415:38:48.425970 IP 10.100.140.15 > 10.100.140.3: ICMP echo request, id 1362, seq 41, length 6415:38:49.449992 IP 10.100.140.15 > 10.100.140.3: ICMP echo request, id 1362, seq 42, length 64In this case offload happens in only one direction which is not ideal.When I re-enable the port0 the traffic will still flow on the port1 and still gonna be half offloaded. If I disable the port1 once again, the traffic finally moves back to port0 and gets fully offloaded.Is this a bug? A feature? What am I doing wrong? :(Any help would be appreciated.Thank you very much!ZoltanHi Zoltan,In order to use ASAP2 complete solution you must install Mellanox OFED driver (v4.4 and above), as well as iproute2 and openvswitch packages.Bonding (SR-IOV VF LAG) is supported with the following combination:OVS: v2.11.90 (and above)Driver: MLNX_OFED 4.6Ubuntu: 18.04Kernel: 4.15We need to understand why MLNX_OFED driver fails to enable eswitch.Please go over the following articles to make sure all steps were taken in order to properly configure SR-IOV & ASAP2.SR-IOV: https://community.mellanox.com/s/article/howto-configure-sr-iov-for-connectx-4-connectx-5-with-kvm–ethernet-xASAP2: http://www.mellanox.com/related-docs/prod_software/ASAP2_Hardware_Offloading_for_vSwitches_User_Manual_v4.6.pdfPlease make sure you change the e-switch mode from legacy to switchdev on the PF device. Example:Best Regards,ChenHi Chen,Thanks for reply. I’m Zoltan colleague, we tested with ofed and default kernel drivers. In case of ofed driver 4.6 version is used, but when we try to switch nic to “switchdev” mode its failed with the following error.root@compute-10:/home/ubuntu# devlink dev eswitch set pci/0000:3b:00.0 mode switchdevdevlink answers: Invalid argument155.794991] mlx5_core 0000:3b:00.0: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)[ 156.184276] (0000:3b:00.0): E-Switch: Failed setting eswitch to offloads[ 156.184278] (0000:3b:00.0): E-Switch: E-Switch enable SRIOV: nvfs(4) mode (1)iprouter version:ii iproute2 4.15.0-2ubuntu1 amd64 networking and traffic control toolsopenvswitch:ruslanloman/openvswitch v2.11.90 f858879fc864 2 months ago 616MBThanks!I just noticed your suggestion by expanding post. We’ll double check our configuration and back to you.Thanks!This command: echo switchdev > /sys/class/net/enp59s0f0/compat/devlink/modeFails with this:[ 1731.458706] (0000:3b:00.0): E-Switch: disable SRIOV: active vports(5) mode(1)[ 1731.486864] (0000:3b:00.0): E-Switch: E-Switch destroying group TSAR but group not empty (group:0)[ 1731.492961] (0000:3b:00.0): E-Switch: E-Switch enable SRIOV: nvfs(4) mode (2)[ 1731.769567] bond0: Releasing backup interface enp59s0f0[ 1732.767455] mlx5_core 0000:3b:00.0: mlx5_cmd_check:775:(pid 759): CREATE_FLOW_TABLE(0x930) op_mod(0x0) failed, status bad parameter(0x3), syndrome (0x31ed04)[ 1732.774734] mlx5_core 0000:3b:00.0: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)[ 1732.856055] mlx5_core 0000:3b:00.0: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)[ 1732.894535] mlx5_core 0000:3b:00.0 enp59s0f0: renamed from eth0[ 1732.918184] mlx5_core 0000:3b:00.0: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)[ 1733.081470] mlx5_core 0000:3b:00.0 enp59s0f0: Failed to init debugfs files for enp59s0f0[ 1733.082916] mlx5_core 0000:3b:00.0 enp59s0f0: Link up[ 1733.089188] bond0: Enslaving enp59s0f0 as a backup interface with an up link[ 1733.097292] mlx5_core 0000:3b:00.0: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)[ 1733.157105] mlx5_core 0000:3b:00.0: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)[ 1733.252980] mlx5_core 0000:3b:00.0: mlx5_cmd_check:775:(pid 759): ALLOC_Q_COUNTER(0x771) op_mod(0x0) failed, status limits exceeded(0x8), syndrome (0x587239)[ 1733.258902] infiniband (null): mlx5_ib_alloc_counters:5525:(pid 759): couldn’t allocate queue counter for port 128, err -12[ 1733.893295] bond0: Releasing backup interface enp59s0f0[ 1734.140687] mlx5_core 0000:3b:00.0: MLX5E: StrdRq(1) RqSz(8) StrdSz(2048) RxCqeCmprss(0)[ 1734.589849] (0000:3b:00.0): E-Switch: Failed setting eswitch to offloads[ 1734.589853] (0000:3b:00.0): E-Switch: E-Switch enable SRIOV: nvfs(4) mode (1)[ 1734.597871] mlx5_core 0000:3b:00.0 enp59s0f0: renamed from eth0[ 1734.629834] (0000:3b:00.0): E-Switch: SRIOV enabled: active vports(5)[ 1735.525786] mlx5_core 0000:3b:00.0 enp59s0f0: Failed to init debugfs files for enp59s0f0[ 1735.529158] mlx5_core 0000:3b:00.0 enp59s0f0: Link up[ 1735.536264] 8021q: adding VLAN 0 to HW filter on device enp59s0f0[ 1735.554672] bond0: Enslaving enp59s0f0 as a backup interface with an up linkIs it a bug? Or what am I missing?We are experiencing a strange problem related to VF LAG. The outbound traffic of VFs goes through a single PF while the other PF in the same bond is not employed.The system and OFED version we are using are as follows:System: CentOS Linux release 7.4.1708 (Core) with kernel 4.18.0-80OFED: 4.6-1.0.1We have debugged a lot and the problem is not resolved.According to https://community.mellanox.com/s/article/Debugging-VF-LAG-issues-with-ASAP2, the tx queues of a VF are evenly distributed among PFs.We have tested with instructions in the page and find that the outbound traffic is distributed among different tx queues of a VF, however, the actual outbound traffic is restricted to a single PF.We have disabled xps for all network interfaces.TX bytes of different queues of the tested VF are:
tx bytes of a VF680×604 22.9 KB
TX bytes of the two PFs are as below (eth0 is in the left part and eth1 is in the right part)
tx bytes of PFs1764×348 31.1 KB
Is there any way to debug the mapping between tx queues (of a VF) and PFs?Thank you for your responsePowered by Discourse, best viewed with JavaScript enabled"
1249,bluefield-2-in-bluefield-x-mode-does-not-see-a100-gpu,"How does one get the A100 GPU to show up on the DPU?In our case, DPU is running as “Embedded CPU”,   according to Nvidia this means it is in “BlueField X Mode”But the GPU does not show up.Installed the Cuda drivers on a Bluefield 2 DPU.  However, it does not see the A100 GPU on its PCIe bus.
The BlueField 2 shows :installed from runfileWhat BFB image version have you installed on the DPU?NVIDIA DOCA Software Framework Accelerate application development for the NVIDIA BlueField DPU Get Early Access NVIDIA® DOCA™ is the key to unlocking the potential of the NVIDIA BlueField® data processing unit (DPU) to offload, accelerate, and...(We recommend our latest 1.5.1).Have you power cycled the host after BFB image installation/FW upgrade + use of mlxconfig to change the mode from Standard to Bluefield-X mode?Did you verify the GPU ownership from lspci?IE:root@host:~# lspci | grep -i nvNoneubuntu@dpu:~$ lspci | grep -i nv06:00.0 3D controller: NVIDIA Corporation GA20B8 (rev a1)Check if UEFI secure boot is enabled, run “mokutil --sb-state” from the ARM. If it is enabled, disable it via these instructions. https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/UEFI+Secure+Boot#UEFISecureBoot-DisablingUEFISecureBoot. Hopefully, the Nvidia driver will load after disabling UEFI secure boot.At last, should our deployment documentation been followed, I would suggest opening an Nvidia support case to further troubleshoot.Thank you for your note.  Having run the suggested lspci commands had realized that the GPU ownership was remaining with the host.  then realized that the UEFI secure boot would need to be disabled to proceed.  At the same time began to question whether or not a BlueField-2 needs to be in a converged accelerator to take ownership, so had paused this experiment.Does a discrete BlueField-2 (as opposed to one in a converged accelerator) have the ability to go into BlueField-X mode and take control of the GPU?  If so I may go back and disable the UEFI secure boot as suggested to proceed with this experiment.  (the Bluefield-2 I am using does reflect the status that it is in BlueField-X mode following the use of mlxconfig)  Thank you, BrandtI believed I spoke too fast and mixed up NVIDIA BlueField-2 DPU  & Converged Adapter IE:MBF2H332A-AECOT
NVIDIA BlueField-2 P-Series DPU 25GbE Dual-Port SFP56, PCIe Gen4 x8, Crypto and Secure Boot Enabled, 16GB on-board DDR, 1GbE OOB management&Product Name: ROY BlueField-2 + GA100 PCIe Gen4 x8; two 100Gbe/EDR QSFP28 ports, FHFL” “Part number: 699-21004-0230-300”.
The A100X combines an A100 Tensor Core GPU with a NVIDIA BlueField-2 data processing unit on a single module.2 different products (Converged Adapter or BF-2 adapter).Does a discrete BlueField-2 (as opposed to one in a converged accelerator) have the ability to go into BlueField-X mode and take control of the GPU?I would suggest opening a support case so we can validate internally should there be limitations of the functionalities/features or no support for GPU/non converged.Powered by Discourse, best viewed with JavaScript enabled"
1250,hdr-infiniband-and-connectx-6-vpi-interfaces,"Using infiniband-diags (35), we see ~twice the latency rate is the equipment is rated for:[root@ne09 ~]# ib_send_lat ne09-ib#bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec]2 1000 1.42 3.63 1.46 1.47 0.05 1.55 3.632 1000 1.23 2.97 1.29 1.31 0.06 1.42 2.972 1000 1.23 3.41 1.28 1.30 0.08 1.40 3.412 1000 1.29 3.93 1.41 1.41 0.08 1.50 3.932 1000 1.19 3.12 1.22 1.23 0.06 1.31 3.12We are not seeing the scaling performance in our application, so this led us to investigate the latency.The interfaces are rated at 0.6usec, but we are seeing 1.2-1.4 (from above)This is with both the OFED mlx5.5 from Mellanox, and mlx5.0 drivers from CentOS 8.2.Which tool is recommended to measure latency? Am I interpreting the output of ib_send_lat correctly?Thanks in advance,Anne HammondHi MammondYour understanding is right. But you need to add more to latency calculations.ex) if you lab Topo like below.CX6-------switch------switch------CX6ex) CX6 (600)— switch port to port latency (90)----switch port to port latency (90)----CX(6) + Cable latency (in case Fiber 5ns per 1M) + 2Byte serialization/de-serialization.i think 1.2 - 1.4 is reasonable.But if you want to measure it more accurate, please refer to below, “Performance Tuning for Mellanox Adapter”Performance Tuning for Mellanox Adapters | SalesforcePowered by Discourse, best viewed with JavaScript enabled"
1251,need-help-understanding-roce-packet-receive-using-connectx-5-ethernet-nic-how-does-the-firmware-work,"We are porting parts of the Mellanox OFED stack using the Ubuntu 18.04 version 5.1-2.5.8.0 release to another OS using the ConnectX-5 ethernet NIC. We have the following question: How does the NIC firmware direct a RoCE v2 packet, received via the wire interface, to the applicable destination QP? What are the conditions that could cause a RoCE v2 packet to be silently dropped so that it will not show up at the expected/programmed QP?We have ported ud_pingpoing to our target OS and are able to send RoCE packets to a Linux host configured with OFED 5.1-2.5.8.0. The Linux host receives that packet and responds with a RoCE packet. We are able to see that this packet is received on the target ConnectX-5 NIC from the rx counters. However, the programmed QP and CQ are not detecting this RoCE packet. We suspect that it is being dropped.The firmware version on our ConnectX-5 cards is: 16.28.2006 and the board_id is MT_0000000012.I’m also wondering this…doing something similiar. Do you have SF (Scalable Functions) enabled:https://docs.mellanox.com/display/BlueFieldDPUOSv370/Scalable+Functionshttps://docs.mellanox.com/display/BlueFieldSWv36011699/Mediated+Devices (this should really be linked in the page above.)No, we are not using scalable functions for our port.Hi Glen,For RoCE (RDMA_Write/Read) we use RC QP. We establish RC connection. In transmitted packets, related IB headers include QP number (for RC) or DCT/DCR (in case of DC).For DC we add QP to multicast group. so may be destination QP was not added to multicast group.Best Regards,VikiPowered by Discourse, best viewed with JavaScript enabled"
1252,xdp-hw-offload-for-mt28800-family-connectx-5-ex-nic,"Does the MT28800 Family [ConnectX-5 Ex] NIC support XDP HW offload for eBPF programs?Hi Asir,Let me do a quick validation internally and I will update you.Sophie.Hi Asir,After validating with our engineering team, at this moment we don’t have any HW offload solution for it.Sophie.Thank you, Sophie.Powered by Discourse, best viewed with JavaScript enabled"
1253,connectx-6-dx-led-behavior-and-connection-status,"I have 2 question, please help to figure out.Hellow howard_lee,This is Cho from Nvidia Technical Support.
Regarding 1st question, please check the LEDs status
There are two I/O LEDs per port to indicate speed and link status.
LED1 is a bicolor LED (amber and green) and LED2 is a single color LED (green).2nd question, please try to execute command both interfaces.Thank you,
ChoPowered by Discourse, best viewed with JavaScript enabled"
1254,connectx-5-differences-between-mcx512a-acat-and-mcx512f-acat,"Hi! Could you please suggest are there any performance or other differences between the MCX512A-ACAT and MCX512F-ACAT cards? This is to understand which cards we should order to get the maximum performance (or features) as the price difference is negligible.They both 25GbE dual port cards, but the  first card has PCIe x8 slot and the second one has the x16.
I’m curious as PCIe x8 3.0 slot can push up to 64 Gbit/s, which should be enough to handle two 25GbE ports.Hello gd051, and welcome to the NVIDIA Developer Forums!Your analysis is correct - the only difference between these two adapters is the slot form factor / underlying bus width. Both widths are, indeed, capable of pushing the max throughput supported by these adapters. The only difference is the form factor.You can see a full list of specifications for our ConnectX-5 Ethernet adapters here:
https://docs.nvidia.com/networking/display/ConnectX5EN/SpecificationsThanks, and best regards;
NVIDIA Enterprise SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1255,register-now-the-first-ever-dpu-hackathon,"Hurry Up!!The hackathon will begin on Wednesday December 8th at 9:00 a.m. PST, and will end on Thursday December 9th at 3:00 p.m. PST.In the European DPU online hackathon, you can be part of the next AI-driven data center and cloud revolution! Get hands-on NVIDIA® DOCA™ SDK experience and take part in supercharging NVIDIA data processing unit (DPU) capabilities for AI, networking, security, and storage.Work with other DPU pioneers, in a 32-hour hackathon to develop the future software-defined, hardware accelerated data center infrastructure on a chip.You’ll build your innovative accelerated application, meet NVIDIA experts and mentors, and show what you can accomplish with NVIDIA DOCA, our new data center on a chip architecture.Hurry Up!!The hackathon will begin on Wednesday December 8th at 9:00 a.m. PST, and will end on Thursday December 9th at 3:00 p.m. PST.Thanks to all of our participants! This was a huge success!https://developer.nvidia.com/blog/nvidia-bluefield-european-hackathon-fuels-data-center-innovation-with-pioneering-dpu-based-applications-demonstrations/Powered by Discourse, best viewed with JavaScript enabled"
1256,mellanox-technologies-mt28800-family-connectx-5-ex-and-connectx-5-full-mode-latency-increase-a-lot-759us-1116us,"1, Ethernet controller infomation​lspci | grep -i mellanox3f:00.0 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex]3f:00.1 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex]b3:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]b3:00.1 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]​ibv_devinfo -d mlx5_0hca_id: mlx5_0transport: InfiniBand (0)fw_ver: 16.29.1016node_guid: 0c42:a103:0060:0610sys_image_guid: 0c42:a103:0060:0610vendor_id: 0x02c9vendor_part_id: 4121hw_ver: 0x0board_id: MT_0000000013phys_port_cnt: 1port: 1state: PORT_ACTIVE (4)max_mtu: 4096 (5)active_mtu: 1024 (3)sm_lid: 0port_lid: 0port_lmc: 0x00link_layer: Ethernet​ib_write_lat -d mlx5_0 -p 2500 -R -F -x 3 -a --report_gbitsib_write_lat -d mlx5_0 -p 2500 -R -F -a --report_gbits ${IP} -x 3​​
企业微信截图_16168129354716.png950×817 18.3 KB

企业微信截图_16168129248073.png968×796 18.1 KB
ibstat…​CA ‘mlx5_0’CA type: MT4121Number of ports: 1Firmware version: 16.29.1016Hardware version: 0Node GUID: 0x0c42a10300600610System image GUID: 0x0c42a10300600610Port 1:State: ActivePhysical state: LinkUpRate: 100Base lid: 0LMC: 0SM lid: 0Capability mask: 0x00010000Port GUID: 0x0e42a1fffe600610Link layer: Ethernetethtool ens12f0Settings for ens12f0:Supported ports: [ FIBRE ]Supported link modes: 1000baseKX/Full10000baseKR/Full40000baseKR4/Full40000baseCR4/Full40000baseSR4/Full40000baseLR4/Full25000baseCR/Full25000baseKR/Full25000baseSR/Full50000baseCR2/Full50000baseKR2/Full100000baseKR4/Full100000baseSR4/Full100000baseCR4/Full100000baseLR4_ER4/FullSupported pause frame use: SymmetricSupports auto-negotiation: YesSupported FEC modes: None RSAdvertised link modes: 1000baseKX/Full10000baseKR/Full40000baseKR4/Full40000baseCR4/Full40000baseSR4/Full40000baseLR4/Full25000baseCR/Full25000baseKR/Full25000baseSR/Full50000baseCR2/Full50000baseKR2/Full100000baseKR4/Full100000baseSR4/Full100000baseCR4/Full100000baseLR4_ER4/FullAdvertised pause frame use: NoAdvertised auto-negotiation: YesAdvertised FEC modes: RSLink partner advertised link modes: Not reportedLink partner advertised pause frame use: NoLink partner advertised auto-negotiation: YesLink partner advertised FEC modes: Not reportedSpeed: 100000Mb/sDuplex: FullPort: FIBREPHYAD: 0Transceiver: internalAuto-negotiation: onSupports Wake-on: dWake-on: dCurrent message level: 0x00000004 (4)linkLink detected: yes​mlnx_qos -i ens12f0DCBX mode: OS controlledPriority trust state: dscpdscp2prio mapping:prio:0 dscp:07,06,05,04,03,02,01,00,prio:1 dscp:15,14,13,12,11,10,09,08,prio:2 dscp:23,22,21,20,19,18,17,16,prio:3 dscp:31,30,29,28,27,26,25,24,prio:4 dscp:39,38,37,36,35,34,33,32,prio:5 dscp:47,46,45,44,43,42,41,40,prio:6 dscp:55,54,53,52,51,50,49,48,prio:7 dscp:63,62,61,60,59,58,57,56,Receive buffer size (bytes): 130944,130944,0,0,0,0,0,0,Cable len: 7PFC configuration:priority 0 1 2 3 4 5 6 7enabled 0 0 0 1 0 0 0 0buffer 0 0 0 1 0 0 0 0tc: 0 ratelimit: unlimited, tsa: vendorpriority: 1tc: 1 ratelimit: unlimited, tsa: vendorpriority: 0tc: 2 ratelimit: unlimited, tsa: vendorpriority: 2tc: 3 ratelimit: unlimited, tsa: vendorpriority: 3tc: 4 ratelimit: unlimited, tsa: vendorpriority: 4tc: 5 ratelimit: unlimited, tsa: vendorpriority: 5tc: 6 ratelimit: unlimited, tsa: vendorpriority: 6tc: 7 ratelimit: unlimited, tsa: vendorpriority: 7Hello,I this case, in order to achieve a better adapter performance, I can recommend to follow the performance tuning guide.Regards,AnatolyPowered by Discourse, best viewed with JavaScript enabled"
1257,direct-connection-routing-with-3x-connectx-4-lx-en,"Hello,
I have 3 servers with ConnectX-4 adapters that I am trying to direct connect (no switch) using both ports on each card in a triangular topology.  I have set unique static IP addresses on each port (6, all on the same subnet) and I can ping between the addresses on two of the three adapters but the third adapter is unable to echo the ping request to either of the other two (although tcpdump shows that the requests are received, and all interfaces are active and link connections are good).How can a directly connected interface NOT be able to ping the other?  Do I need to build route tables between the 6 addresses?  What is the right way to get the 3 adapters (6 interfaces) talking?Any help or guidance is appreciated.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
1258,setup-infiniband-on-kubernetes,"I have a k8s cluster and the worker nodes have mellanox connectx-5 nics. I would like to deploy some pods in k8s and run mpi in it.I can see the mellanox pci device inside the container21:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]I enabled security context in my pods (following this toturial https://community.mellanox.com/s/article/kubernetes-ipoib-sriov-networking-with-connectx4-connectx5):securityContext:privileged: truecapabilities:add: [ “IPC_LOCK” ]resources:limits:rdma/hca: 1I tried installing the nic drivers but it fails during instllation because it can’t query device 21:00:0Detected sles15sp2 x86_64. Disabling installing 32bit rpms…Note: This program will create mlnx-en TGZ for sles15sp2 under /tmp/mlnx-en-5.5-1.0.3.2-4.12.14-197.78_9.1.60-cray_shasta_c directory.See log file /tmp/mlnx-en-5.5-1.0.3.2-4.12.14-197.78_9.1.60-cray_shasta_c/mlnx_iso.2159_logs/mlnx_ofed_iso.2159.logChecking if all needed packages are installed…Building mlnx-en RPMS . Please wait…Creating metadata-rpms for 4.12.14-197.78_9.1.60-cray_shasta_c …WARNING: If you are going to configure this package as a repository, then please noteWARNING: that it contains unsigned rpms, therefore, you need to disable the gpgcheckWARNING: by setting ‘gpgcheck=0’ in the repository conf file.Created /tmp/mlnx-en-5.5-1.0.3.2-4.12.14-197.78_9.1.60-cray_shasta_c/mlnx-en-5.5-1.0.3.2-sles15-ext.tgzrpm -e --allmatches --nodeps cray-libxpmem-devel-headers cray-libxpmem0 cray-libxpmem-devel cray-xpmemInstalling /tmp/mlnx-en-5.5-1.0.3.2-4.12.14-197.78_9.1.60-cray_shasta_c/mlnx-en-5.5-1.0.3.2-sles15-ext/tmp/mlnx-en-5.5-1.0.3.2-4.12.14-197.78_9.1.60-cray_shasta_c/mlnx-en-5.5-1.0.3.2-sles15-ext/install --forceDetected sles15sp2 x86_64. Disabling installing 32bit rpms…Logs dir: /tmp/mlnx-en.818334.logsGeneral log file: /tmp/mlnx-en.818334.logs/general.logThis program will install the mlnx-en package on your machine.Note that all other Mellanox, OEM, OFED, RDMA or Distribution IB packages will be removed.Those packages are removed due to conflicts with mlnx-en, do not reinstall them.Uninstalling MLNX_EN driverStarting mlnx-en-5.5-1.0.3.2 installation …Preparing… ########################################mlnx-tools-5.2.0-0.55103 ########################################Installing mlnx-en-utils 5.5 RPMPreparing… ########################################Updating / installing…mlnx-en-utils-5.5-1.0.3.0.gf3bf963.sle########################################Installing mlnx_en 5.5 RPMPreparing… ########################################Updating / installing…mlnx_en-5.5-1.0.3.0.gf3bf963.kver.4.12########################################depmod: WARNING: Ignored deprecated option -rdepmod: WARNING: could not open modules.order at /lib/modules/4.12.14-197.78_9.1.60-cray_shasta_c: No such file or directorydepmod: WARNING: could not open modules.builtin at /lib/modules/4.12.14-197.78_9.1.60-cray_shasta_c: No such file or directoryInstalling mlnx-en-sources 5.5 RPMPreparing… ########################################Updating / installing…mlnx-en-sources-5.5-1.0.3.0.gf3bf963.s########################################Installing mlnx-en-doc 5.5 RPMPreparing… ########################################Updating / installing…mlnx-en-doc-5.5-1.0.3.0.gf3bf963.sles1########################################Installing user level RPMs:Preparing… ########################################ofed-scripts-5.5-OFED.5.5.1.0.3 ########################################Preparing… ########################################mstflint-4.16.0-1.55103 ########################################Device (21:00.0):21:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]Link Width: x16PCI Link Speed: 8GT/sInstallation finished successfully.Preparing… ################################# [100%]Updating / installing…1:mlnx-fw-updater-5.5-1.0.3.2 ################################# [100%]Initializing…Attempting to perform Firmware update…Querying Mellanox devices firmware …Device #1:Device Type: N/APart Number: –Description:PSID:PCI Device Name: 21:00.0Port1 MAC: N/APort1 GUID: N/APort2 MAC: N/APort2 GUID: N/AVersions: Current AvailableFW –Status: Failed to open device-E- Failed to query 21:00.0 device, error : No such file or directory. MFE_CR_ERRORLog File: /tmp/xu4EnDC0UbReal log file: /tmp/mlnx-en.818334.logs/fw_update.logConfiguring /etc/security/limits.conf.To load the new driver, run:/etc/init.d/mlnx-en.d restartI would like to get some assistance to find out the issue the installer is havingthank you very muchHi Masber,Thank you for posting your question on our community. Based on the information provided, the driver installation has completed successfully:"" Device (21:00.0):21:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]Link Width: x16PCI Link Speed: 8GT/sInstallation finished successfully.………To load the new driver, run:/etc/init.d/mlnx-en.d restart""The error is reported while checking the latest firmware for the card. THus, in order to understand why it failed to retrieve device information, it would be great if you can share the following outputs:#ethtool -i #mst start#mst status -v (To find name of MST device)#flint -d  qIn addition, please share the log /tmp/mlnx-en.818334.logs/fw_update.logThanks,Namrata.Hi Masber,In addition to the above request, I would also like to bring to your notice that based on your contact details, unfortunately, we did not find a support contract for your Account in our database. Thus, if an in depth debug is required in order to address your issue, I would like to request contacting our contracts team for a valid support contract. The contracts team can be reached at the following email id → Networking-contracts@nvidia.comThanks,Namrata.Powered by Discourse, best viewed with JavaScript enabled"
1259,link-flapping-for-6s-on-dgx-have-you-seen-this,"I’m seeing a link flap (see logs for DGX and switch below) in the log on the DGX where the interface goes down for 6s.  This same pattern is happening on dozens of nodes.  The log below is a standard pattern.  Have you seen this before?  I’m thinking it may be related to a software issue since if it were cabling or hardware, I would expect some randomness in the logs and downtime, not just 6ms all of the time.  Would like to get community thoughts on this issue.System Information
Manufacturer: NVIDIA
Product Name: DGXA100 920-23687-2531-001
Version: v1.0
Serial Number: xxxxxxxxxxxxxxx
UUID: be9ee235-ff5c-03ca-1000-1565ace0aac0
Wake-up Type: Power Switch
SKU Number: Default string
Family: DGXA100Specific NIC I’m interested is runningdriver: mlx5_core
version: 5.4-3.1.0
firmware-version: 20.32.1010 (MT_0000000225)Regarding link flapping, I can see clear indication of the event on both the DGX node and the upstream switch. There is a common pattern for the flap, typically 6ms between going down and up again. The DGX node side port goes down/down while the switch side port goes up/down. Here is a typical pattern that I see. DGX reports Link down, then fee switch ports up/down status. In almost every case so far (looking at 7 instances of flapping, this is the order of events). Learn node reports, then upstream fee switch reports:Log Source Type Date Time MessageDGX-NODE node 1/27/2022 16:38:27 mlx5_core 0000:e1:00.0 enp225s0f0: Link downDGX-NODE node 1/27/2022 16:38:27 front0: (slave enp225s0f0): speed changed to 0 on port 2DGX-NODE node 1/27/2022 16:38:27 front0: (slave enp225s0f0): link status definitely down, disabling slaveDGX-NODE node 1/27/2022 16:38:34 mlx5_core 0000:e1:00.0 enp225s0f0: Link upDGX-NODE node 1/27/2022 16:38:34 front0: (slave enp225s0f0): link status definitely up, 200000 Mbps full duplexDGX-NODE node 1/27/2022 16:38:34 front0: (slave enp225s0f0): speed changed to 0 on port 2arista-switch switch 1/27/2022 16:38:49 Ebra: %LINEPROTO-5-UPDOWN: Line protocol on Interface Ethernet4/4/1, changed state to downarista-switch switch 1/27/2022 16:38:49 Lag: %LAG-5-MEMBER_REMOVED: Interface Ethernet4/4/1 has left Port-Channel50 due to: partner not in syncarista-switch switch 1/27/2022 16:38:56 Ebra: %LINEPROTO-5-UPDOWN: Line protocol on Interface Ethernet4/4/1, changed state to uparista-switch switch 1/27/2022 16:38:56 Lag: %LAG-5-MEMBER_ADDED: Interface Ethernet4/4/1 has joined Port-Channel50arista-switch switch 1/27/2022 16:38:57 Lldp: %LLDP-5-NEIGHBOR_NEW: LLDP neighbor with chassisId b8ce.f616.ff8a and portId “enp225s0f0” added on interface Ethernet4/4/1Hello David,Hope your day is going well.As a Nvidia enterprise support case has been opened to investigate this issue further, we will proceed with closing this.Thank you,
Abigail.Powered by Discourse, best viewed with JavaScript enabled"
1260,spectrum-and-full-bgp-feed,"HiI spotted some Spectrum3 docs saying: ‘Up to 1M IPv4 route entries’ - I was wondering if people are using these devices as internet routers with full BGP feeds.This page says Spectrum3 can do 512k IPv4 prefixes:Cloud, storage, and AI interconnects with visibility at scale.This one says 1M is possible:
https://nvdam.widen.net/s/6269c25wv8/nv-spectrum-sn4000-product-briefProbably almost big enough, but not sensible for 5 year plan.Alot of people using these types of switches as front-end routers are filtering routes.  Spotify for instance did a huge explanation of what they do essentially analyzing where their main traffic sources come from and adjust routes based on that vs accepting full routes.  The whole thing was presented at NANOG, really cool stuff: SDN Router @ Spotify on Software Gone Wild « ipSpace.net blogUnless you do some kind of route compression, either BGP->RIB (usually done with off-box BGP best-path and reinjection, for example, both Junos and IOS-XR provide service level APIs to do so, also Spotify and similar), or RIB->FIB, you should not take full DMZ feed (at time of writing ~ 940K IPv4 + 175K IPv6)Powered by Discourse, best viewed with JavaScript enabled"
1261,installling-doca-with-cli,"i am trying to install DOCA via CLI. I run the following command:
sdkmanager --cli install  --logintype devzone --product DOCA --host --targetos Linux --version 1.1 --target BLUEFIELD2_DPU_TARGETS --deselect ‘BlueField OS’ --select ‘DOCA Components’ --flash allI continue with the installation until this error appears.
│info: Some packages could not be installed. This may mean that you have
│info: requested an impossible situation or if you are using the unstable
│info: distribution that some required packages have not yet been created
│info: or been moved out of Incoming.
│info: The following information may help to resolve the situation:
│info:
│info: The following packages have unmet dependencies:
│info:  doca-sdk : Depends: libvma-dev (= 9.3.1-1) but
│info:  doca-sdk : Depends: libvma-dev (= 9.3.1-1) but it is not going to be installed
│info:             Depends: doca-dpi-devel (= 1.1.024-1) but it is not going to be installed
│error: E: Unable to correct problems, you have held broken packages.
│info: [ Package Install Finished with Error ]
│info: [host] [ 81.05 MB used. Disk Avail: 254.55 GB ]
│info: [ NV_DOCA_SDK_HOST_COMP Install took 11s ]
│error: [error]: Error when apt install failed; [exec_command]: sudo apt-get update; sleep 0.5; sudo apt-get -y  install doca-sdk=1.1-0* ; [error]: exit status 100; [deb_path]: /home/name/Downloads/nvidia/ │
│sdkm_downloads/doca-repo-ubuntu1804_1.1-0.1.7.1.1.024.5.4.1.0.3.0_amd64.deb
│info:
│error: command terminated with error
│info: APT system is broken and requires manual fix
│info: Depends on failed component
│info: Depends on failed component
│info: Depends on failed component
│info: Start to check if only one device connected…Powered by Discourse, best viewed with JavaScript enabled"
1262,occational-initialization-failures-of-nvshmem-program,"Hi,The installed MLNX OFED is 5.0, and wonder if that could be the cause?Please advice!Rgds,
Torhi TorThere’s a lot reason can caused ibv_modify_qp failed.
It’s hard to get reason from current logs, I suggest you can retest with ofed 5.7Powered by Discourse, best viewed with JavaScript enabled"
1263,mlnx-winof2-installation-was-interrupted,"Hello everyone,I am trying to install MLNX_WinOF2 driver for my ConnectX-5 card. Unfortunately it ends up with error saying “The wizard was interrupted before MLNX_WinOF2 could be completely installed.” as you can see on image below. I also attached installation log file to this post.I hope someone can help me, thanks.MLNX_WinOF2_log.txt (714 KB)Any solution to this?Also have the same problem. No clue what the issue is.Powered by Discourse, best viewed with JavaScript enabled"
1264,embedded-switch-documentation,"Hi AllWhere can I find documentation on the eSwitch ? Such as programing the eSwitch ? While going through the DOCA SDK docs, I see all of the offload only mentioned when configuring something on the OVS. The doc also mentioned that “Offloading rules can also be added directly, and not just through OVS, using the tc utility. To enable TC ingress on all the representors (i.e. uplink, PF, and VF)”, so is the Linux Kernel the “embedded switch” ?When configuring a flow rule in the OVS in offload mode. Does that automatically create corresponding iptable rule?I just need some clarification on exactly what this eSwitch is. So far it sounds just like Kernel NetworkingThank you
PeterI have same question!Hey there,Historically, the eSwitch (part of the connectx subsystem) wasn’t really “programmable”, and that’s exactly what DOCA is positioned to help do (albiet at an abstraction layer). The way that you would “program” the eswitch was a bit indirectly by using ASAP^2, which accelerates OVS and other kernel networking constructs.197.98 KBDOCA Flow is intended to be the API that would most closely map to programming the eSwitch.Hope this helpsPowered by Discourse, best viewed with JavaScript enabled"
1265,mlx5-ctrl-flow-returns-12,"When starting up the interface in DPDK the mlx5_ctrl_flow function returns -12. With OFED version 5.3-1.0.0.1it does work. Any idea what could be the issue ? Was there any breaking change in 5.4-1.0.3 ?Setup:Ubuntu 20.04 kernel 5.4.0-84
NIC Connect X5
driver: mlx5_core
version: 5.4-1.0.3
firmware-version: 16.32.1010 (MT_0000000008)Using DPDK 17.11.04.Hello customer_0815,Thank you for posting your query on our community. There are no known issues with DPDK 17.11 and Mellanox OFED 5.4-1.0.3 at the moment.The error code “-12” returned by the mlx5_ctrl_flow function in DPDK usually indicates an invalid argument. We will need more information to debug this issue further. If the issue still persists, I suggest you to collect a sysinfo-snapshot and open a support case for further investigation of the issue. The support ticket can be opened by emailing ""Networking-support@nvidia.com ""Please note that an active support contract would be required for the same. If you do not have a current support contract, please reach out to our Contracts team at networking-contracts@nvidia.comThank you,
-Nvidia Network SupportPowered by Discourse, best viewed with JavaScript enabled"
1266,perftest-in-mlnx-ofed-rev-5-8-1-1-2-1-lts-non-complaint-as-per-red-hat-security-advisories,"Mellanox OFED drivers compiled from ISO for kernel 4.18.0-372.26.1.el8_6.x86_64 and installed in RHEL 8.6 Server.
Post installation found perftest-4.5-0.18.gfcddfe0.58112.x86_64 in OFED ISO is non complaint as per Red Hat Security Advisories.
Is there any solution/procedure to install compatible perftest version in kernel 4.18.0-372.26.1.el8_6.x86_64 .$ cat /etc/redhat-release
Red Hat Enterprise Linux release 8.6 (Ootpa)$ uname -r
4.18.0-372.26.1.el8_6.x86_64$ lspci -nn -q -s 5d:00
5d:00.0 Ethernet controller [0200]: Mellanox Technologies MT27710 Family [ConnectX-4 Lx] [15b3:1015]
5d:00.1 Ethernet controller [0200]: Mellanox Technologies MT27710 Family [ConnectX-4 Lx] [15b3:1015]$ ethtool -i eno5np0
driver: mlx5_core
version: 5.8-1.1.2
firmware-version: 14.32.1010 (HP_2690110034)
expansion-rom-version:
bus-info: 0000:5d:00.0
supports-statistics: yes
supports-test: yes
supports-eeprom-access: no
supports-register-dump: no
supports-priv-flags: yes$ dnf updateinfo -q
Updates Information Summary: available
1 Bugfix notice(s)
1 Enhancement notice(s)$ dnf updateinfo list -q updates bugfix
RHBA-2021:4412 bugfix perftest-4.5-1.el8.x86_64$ dnf updateinfo list -q updates enhancement
RHEA-2022:2014 enhancement perftest-4.5-12.el8.x86_64Hello sm.senthilraja,Welcome, and thank you for posting your inquiry to the NVIDIA Developer Community.When installation of MLNX_OFED is performed, the install script replaces the OS’s RDMA stack with our own libraries and modules. This is noted in the user manual: https://docs.nvidia.com/networking/display/MLNXOFEDv581011/Installing+MLNX_OFEDOS-provided utilities, drivers and libraries weren’t compiled against our libraries. The perftest package included with MLNX_OFED and the perftest package included in the OS repository (rhel-8-for-x86_64-baseos-rpms) are very different - though their version numbering is similar.The RHBA/RHEA advisories (and dnf updateinfo) are assuming the following:As the system no longer runs on the OS-provided RDMA stack, the perftest package included with MLNX_OFED is not built against the OS RDMA stack, and the version number is therefore irrelevant, the fixes/enhancements in those RedHat advisories do not apply here.Instead, the release notes for MLNX_OFED should be consulted in order to track bugfixes:
https://docs.nvidia.com/networking/display/MLNXOFEDv581121LTS/Bug+Fixes+in+This+Version
… and new features/changes:
https://docs.nvidia.com/networking/display/MLNXOFEDv581121LTS/Changes+and+New+FeaturesThanks,
NVIDIA Enterprise ExperienceThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1267,sn2100-compatible-qsfp-dac-cable,"We are searching for a compatible cable that allows us to connect a Sophos XG Firewall  to a Mellanox SN2100. It should be QSFP+ 40GB copper. The list narrows down to just one option on the Sophos side.Is our assumption right that this cable will be compatible with Mellanox SN2100 as this is a passive copper cable?Syrotech GOXQ-CAB-QSFP-3M 40Q QSFP+ DAC CableThanks,
MHello M,Apart from the Cumulus HCL (URL below), we will ensure any Mellanox cable will work on our system. For other cables that have not been tested we cannot guarantee they will work as they have not been tested. Even if the cable is passive copper cable the transceiver still has to interact with the switch. The quality of the cable may vary and it also may not follow standards fully. If your NOS on the switch is Onyx or other, then please use HCL has just reference.If you run into any problem, please open a case and we can try to assist you.Find hardware compatible with NVIDIA Cumulus Linux.Powered by Discourse, best viewed with JavaScript enabled"
1268,one-node-out-of-21-cannot-connect,"I have a 21 node HPC with Infiniband as the primary interconnect. All of the nodes are connected to a single Mellanox SX6036 switch via DAC cables and all but one compute node is able to fully connect. The nodes are all running CentOS 7 and the compute nodes boot via PXE thanks to Bright Cluster Manager 8. All compute nodes are built by Supermicro and have built-in Connect-X 3 interfaces. The network itself has the SM on the switch and contains two partitions, the default, set for 10Gbps as a fail over, and one for 56Gbps FDR.
On the problem node I have already tried a different switch port, new cable, and new NIC. None changed what is happening.The issue was first noticed from a mount failing that used the Infiniband interface. Checking the output of ip addr gave this.Checking the dmesg log I found the following with the last line repeated multiple times.Here is the ibstatus output from the node.If anyone has any ideas I would be grateful. Also let me know if any more info is needed.Hello Chris.Woelkers,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Based on the output, looks like you configured “connected_mode” but the MTU is set to datagram value. You should check on a good node if it is datagram or connected_mode.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1269,installing-ubuntu22-04-on-bluefield-2-dpu,"Hi.I used sdkmanager to install ubuntu22.04 on BlueField-2 DPU, but it stuck on flashing.Does it mean that flash failed?Also I tried to flash OS manually using bfb-install command, but it also showed me the same logs.How can I flash Ubuntu22.04 to bluefield-2?
If 22.04 is not available, how can I download bfb file for 20.04?Hello cerotyki,Thank you for posting your inquiry to the NVIDIA Developer Forums.The Ubuntu 22.04 .bfb image is available as part of DOCA 2.0.2 - you can acquire the image via the menu at the bottom of the following page:
https://developer.nvidia.com/networking/doca
Version 2.0.2 → “DOCA + BlueField OS Ubuntu Server 22.04 Image”DOCA 1.5.2-LTS uses an Ubuntu 20.04 image for the .bfb, if the above image fails please attempt using this DOCA version and image instead:
Version 1.5.2-LTS → “DOCA + BlueField OS Ubuntu Server 20.04 Image”Please follow the instructions in the following guide for image flash operations:
https://docs.nvidia.com/networking/display/BlueField2DPUENUG/BlueField+DPU+Administrator+Quick+Start+Guide#BlueFieldDPUAdministratorQuickStartGuide-UpdatingBlueFieldBFBImageShould none of the above work, and if this device is covered by support entitlement, please open a support ticket with NVIDIA Enterprise Experience for further debug:
https://enterprise-support.nvidia.com/s/create-caseBest,
NVIDIA Enterprise ExperiencePowered by Discourse, best viewed with JavaScript enabled"
1270,device-name-changed-after-update-the-driver-to-5-8lts,"RHEL 8.4 + inbox driver：3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
link/ether 62:4b:f3:42:9b:58 brd ff:ff:ff:ff:ff:ff
inet6 fe80::d009:170c:f8c5:f629/64 scope link noprefixroute
valid_lft forever preferred_lft foreverAfter installed 5.8LTS driver3: enp7s0np0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
link/ether c2:43:34:18:a4:e7 brd ff:ff:ff:ff:ff:ff permaddr 72:6a:26:98:44:f8
inet6 fe80::82fb:daca:4ea0:e8ce/64 scope link noprefixroute
valid_lft forever preferred_lft foreverthere is a suffix (np0) added to device name.  anyone knows why?
What I have done is:so, the driver did influence the device name, and only  influence the Nvidia’s netcard .Hi lvzhipeng,You will find the information in our documentation:
https://docs.nvidia.com/networking/display/MLNXOFEDv582030LTS/Ethernet+Interface#EthernetInterface-PersistentNamingPersistentNamingThis change was implemented since version 5.6:https://docs.nvidia.com/networking/display/MLNXOFEDv561033/Changes+and+New+Features#ChangesandNewFeatures-CustomerAffectingChangesThank you and regards,Nvidia SupportPowered by Discourse, best viewed with JavaScript enabled"
1271,connectx-5-cannot-bring-up-link-after-connection-state-bouncing,"On a ConnectX-5 NIC, if cable is repeatedly connected and disconnected on a frequent basis, the link state will eventually remain down and cannot become up without NIC reset.The server tested is running BC-Linux 8.2 with latest MLNX_OFED driver and firmware. Two dual-port ConnectX-5 NICs are installed, each of which has one port connected to switch with a DAC cable. However, only one NIC is necessary to reproduce this issue. The port ens1f0np0 is used here.If negotiation had been disabled (for both server and switch), the error message would be ‘Other issues’:Resetting the NIC with mlxfwreset is usually enough to bring link back up. Any ideas on how to resolve this issue? Thanks!Hello hzx,Thank you for posting your inquiry to the NVIDIA Developer Forums.In order to rule out any hardware incompatibilities (which could be responsible for such a behavior), we recommend using a supported cable from the following list:
https://docs.nvidia.com/networking/display/ConnectX5Firmwarev16351012/Firmware+Compatible+ProductsAdditionally, it’s possible that some non-standard adapter configuration was applied, which could be indirectly responsible for this behavior. We recommend resetting the adapter configuration to default values and retesting, using the ‘mlxconfig’ tool:…And rebooting.If this behavior is not resolved by the above, we would recommend opening a case with Enterprise Support via the following link for further investigation:
https://enterprise-support.nvidia.com/s/create-caseBest,
NVEX Networking SupportResetting configuration with mlxconfig worked. Thank you.Curiously, restoring the original configuration manually did not trigger the issue again. The cause might be some internal states of the NIC.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1272,where-to-get-updated-bfb-images,"Hi all,How can we get the updated BFB images for the bluefield 2? The sdkmanager is a bit complex to run remotely, the password did not work probably because of Keyboard conversion so I’d like to flash manually.Thanks,
TomHello,The latest BFB images can be found in the “BlueField DPU Runtime and Driver Downloads” section of the following page:https://developer.nvidia.com/networking/docaThe accompanying “BLUEFIELD DPU PLATFORM OPERATING SYSTEM v3.8.5 DOCUMENTATION” can be found here:https://docs.nvidia.com/networking/display/BlueFieldDPUOSv385/Thank you,
Nvidia SupportAh thanks! I never scrolled past the sponsors I guess, thinking it’s the end of the page…This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1273,does-connectx-6-still-limit-the-host-chaining-feature-to-ethernet-only-or-will-the-feature-also-work-when-infiniband-is-used,"Hi,I am considering purchase of ConnectX-6 VPI card (specifically: MCX653106A-ECAT). It is planned to be used in a small switchless cluster, so I would like to use the Host Chaining feature and connect the cards to each other directly using InfiniBand. The ConnectX-5 firmware known bugs says the Host Chaining requires both ports to be configured to Ethernet (https://docs.mellanox.com/display/ConnectX5Firmwarev16291016/Known+Issues, issue ref: 1178792).Does this limitation also apply to ConnectX-6 cards ? I have searched through the ConnectX-6 firmware documentation, but nothing is mentioned about the Host Chaining.Thanks in advance for your help.Cheers,WaldemarHi Waldemar,The limitation also applies to ConnectX-6 cards.Both VPI and EN adapter cards support Host Chaining only through Ethernet protocol, InfiniBand is currently not supported.How to enable Host Chaining using mlnxconfig:Set HOST_CHAINING_MODE=1.Restart the servers for the changes to take effect.Allocate all ports on the same subnet and restart the networking stack as many times as required.You should be able to ping all ports from all other ports. 5. Set the MTU up to 9000.Regards,ChenHi,Thanks for the confirmation. Is there any plan to fix the issue ? It seems it has been discovered almost 16 months ago…Cheers,WaldemarPowered by Discourse, best viewed with JavaScript enabled"
1274,dedup-software-driver-or-reference-code,"I see refence to an onboard accelerator for dedup and data hashing in the BF-2 datasheet here:449.77 KBWhen I looked in the SDK, I found no information on how to use this feature. Is there an API or reference driver code or an example application that shows how to use the deduplication accelerator?Powered by Discourse, best viewed with JavaScript enabled"
1275,4-9-ofed-support-for-rhel9,"I have a cluster with ConnectX-3 cards running RHEL7. I need to upgrade the cluster to a newer version of RHEL. I see that drivers are available for RHEL8 but not for RHEL9. Are there any plans for RHEL9 support? I want to figure out if I should upgrade to RHEL8 right away or wait for RHEL9 support?CX3 EOL long time, the latest OFED for that is 4.9 LTS, if you want RHEL9, you need update hardware either.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1276,could-not-join-netdev-no-space-left-on-device-mellanox-connectx-3-pro-en-hp-544-qsfp-764284-b21,"I am trying to run 200 vlan on one port and I am getting errors. I updated the driver to the latest 4.9 LTS and firmware too, but it didn’t help. I noticed that the maximum can be only 126 vlan, but not more. If I put a network adapter with an Intel chip, then everything is fine. Tell me what is the problem?Ubuntu Server 18.04 LTSMellanox ConnectX-3 Pro EN / HP 544+QSFP (764284-B21)Logs:systemd-networkd[3499]: vlan972: netdev readysystemd-networkd[3499]: vlan951: netdev readysystemd-networkd[3499]: vlan601: netdev readysystemd-networkd[3499]: ens4d1: Could not join netdev: No space left on devicesystemd-networkd[3499]: ens4d1: Failedsystemd-networkd[3499]: ens2d1: Link UPsystemd-networkd[3499]: ens2d1: Gained carriersystemd-networkd[3499]: vlan959: Link is not managed by ussystemd-networkd[3499]: vlan991: Link is not managed by uspci 0000:12:00.0: BAR 9: no space for [mem size 0x20000000 64bit pref]pci 0000:12:00.0: BAR 9: failed to assign [mem size 0x20000000 64bit pref]pci 0000:12:00.0: BAR 6: no space for [mem size 0x00100000 pref]pci 0000:12:00.0: BAR 6: failed to assign [mem size 0x00100000 pref]pci 0000:12:00.0: BAR 2: assigned [mem 0xc4000000000-0xc4001ffffff 64bit pref]pci 0000:12:00.0: BAR 9: assigned [mem 0xc4002000000-0xc4021ffffff 64bit pref]netplan example:…vlan602:id: 602link: ens4d1dhcp4: nodhcp6: novlan603:id: 603link: ens4d1dhcp4: nodhcp6: novlan604:id: 604link: ens4d1dhcp4: nodhcp6: novlan605:id: 605link: ens4d1dhcp4: nodhcp6: no…does it help in my case?GRUB_CMDLINE_LINUX_DEFAULT="" pci=realloc=off""I understand that Mellanox ConnectX-3 and Mellanox ConnectX-4 supports a maximum of 128 vlan per port? why then is it not written in the specification?Problem solvedMellanox ConnectX errors “Could not join netdev: No space left on device”Powered by Discourse, best viewed with JavaScript enabled"
1277,how-to-reset-cumulus-mellanox-switch-running-with-4-4-3,"We have transfered the switch to another location and need to configure newly, want to reset the switch to factory default, is there any way to do the same#net del all  → does not helped meDid you do a “net commit” afterwards? If so, could you elaborate in more detail on what happened?I did it, but it says .frr file cant be deleted and changes remain in pending.Assuming that you no longer need anything else on the switch either, the easiest way would be to load a new image on the box. That also allows you to go to 5.2 as well, but if you don’t want to go through that:Then try it againBy the way, thanks so much for your response,I want to keep the same version.So this well remove all? do i need to reboot after applying it?This will just delete the frr config. Do a net del all again after.Let me try and update, waiting for console… :)Its giving me below error nowUser  Timestamp                   Commandroot  2019-02-14 12:47:21.688524  net del all“/bin/systemctl disable hostapd.service” failed: Command ‘[’/bin/systemctl’, ‘disable’, ‘hostapd.service’]’ returned non-zero exit status 1
‘/bin/systemctl reload frr’ failed.  See /var/log/netd.log for details.
“net commit” failed for frr.  All changes will remain in “net pending”.Clearly there is an issue between nclue and the underlying files. Might be possible to solve, but I would suggest to just load the image again. Much faster solution.Powered by Discourse, best viewed with JavaScript enabled"
1278,cannot-update-firmware-for-mt27710-family-connectx-4-lx,"Hi there,My MT27710 Family [ConnectX-4 Lx] has PSID SM_2001000001034 and on attempt to update to another firmware version (fw-ConnectX4Lx-rel-14_26_4012-MCX4121A-ACA_Ax-UEFI-14.19.17-FlexBoot-3.5.805.bin) script is  saying :Did not find any firmware with  PSID SM_XXXXXXXXX  support at the download page.
I would expect PSID as MT_2420110034 and smooth firmware update as for my others MT27710.
Should I use “–force” option or I am missing something?
How to update firmware to the fw-ConnectX4Lx-rel-14_26_4012-MCX4121A-ACA_Ax-UEFI-14.19.17-FlexBoot-3.5.805 ?Thank you in advanceHello thomas_anderson,Thank you for posting your inquiry on the NVIDIA Developer Forum - Infrastructure and Networking - Section.Based on the output shared, you are using a SuperMicro OEM ConnectX-4 Lx adapter (PSID starts with SM_xxx). For obtaining the latest f/w available for PSID, we recommended to contact SuperMicro Support so they can provide you with the latest f/w for this adapter.Even though the adapter shows as a Mellanox adapter, our OEMs are then ones who will provide the f/w based on their modifications.Thank you and regards,
~NVIDIA Networking Technical SupportThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1279,1000base-t-transceiver-in-sn2010,"I’m buying a SN2010 from HPE and I need a 1GB copper link. Datasheet list this one (JD089B - HPE X120 1G SFP RJ45 T Transceiver) as compatible, but not sure if I can stick this module directly into a SFP28 port of the switch. I think I read that for a QSFP28 you would need an adapter to place the module.Thnx!JD089B Transceiver will fit into any of the SFP cage of the SN2010.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1280,test-case-yi,"testing communityPowered by Discourse, best viewed with JavaScript enabled"
1281,no-link-with-connext-x3-debian-10,"Hi there, we have no connection with a connectX-3. No lights nothing. The card is found and the driver is installed. I´ve tried it with many cables, switches etc. All components are 10G. The cable is also shown in the compability list.lspci | grep Mellanox04:00.0 Ethernet controller: Mellanox Technologies MT27500 Family [ConnectX-3]mst statusMST modules:MST PCI module loadedMST PCI configuration module loadedMST devices:/dev/mst/mt4099_pciconf0 - PCI configuration cycles access.domain:bus:dev.fn=0000:04:00.0 addr.reg=88 data.reg=92 cr_bar.gw_offset=-1Chip revision is: 01/dev/mst/mt4099_pci_cr0 - PCI direct access.domain:bus:dev.fn=0000:04:00.0 bar=0x93900000 size=0x100000Chip revision is: 01ethtool eno49Settings for eno49:Supported ports: [ FIBRE ]Supported link modes: 1000baseKX/Full10000baseKX4/Full10000baseKR/Full40000baseCR4/Full40000baseSR4/Full56000baseCR4/Full56000baseSR4/FullSupported pause frame use: Symmetric Receive-onlySupports auto-negotiation: YesSupported FEC modes: Not reportedAdvertised link modes: 1000baseKX/Full10000baseKX4/Full10000baseKR/Full40000baseCR4/Full40000baseSR4/FullAdvertised pause frame use: SymmetricAdvertised auto-negotiation: YesAdvertised FEC modes: Not reportedSpeed: Unknown!Duplex: Unknown! (255)Port: FIBREPHYAD: 0Transceiver: internalAuto-negotiation: offSupports Wake-on: dWake-on: dCurrent message level: 0x00000014 (20)link ifdownLink detected: noIt´s only a single port ethernet card with 10G but there a 2 ports… The newest firmware is installed.Anyone an idea?ThanksHello Hermann,Thank you for posting your question on the Mellanox Community.We have a few questions to better understand the issue you are experiencing.https://community.mellanox.com/s/article/howto-change-the-ethernet-port-speed-of-mellanox-adapters--linux-xFor example ethtool -s eno49 speed 10000 autoneg offThanks and regards,~Mellanox Technical SupportPowered by Discourse, best viewed with JavaScript enabled"
1282,ovs-dpdk-bond-with-connectx-5,"Hello there,
I used OVS-dpdk bond with ConnectX-5 . The configuration is following:
ovs-vsctl add-br br-int – set bridge br-int datapath_type=netdev
ip addr add ip/mask dev br-int
ovs-vsctl add-bond br-int dpdkbond dpdk0 dpdk1  – set Interface dpdk0 type=dpdk options:dpdk-devargs=0000:98:00.0  – set Interface dpdk1 type=dpdk options:dpdk-devargs=0000:98:00.1test throughput with iperf3, the result is 718 Mbits/sec.bond information is following:
root@POD65-CLU01-GPU02:~# ovs-appctl bond/show dpdkbond
---- dpdkbond ----
bond_mode: balance-slb
bond may use recirculation: no, Recirc-ID : -1
bond-hash-basis: 0
updelay: 0 ms
downdelay: 0 ms
next rebalance: 4444 ms
lacp_status: off
lacp_fallback_ab: false
active slave mac: b8:ce:f6:91:99:95(dpdk0)slave dpdk0: enabled
active slave
may_enable: true
hash 15: 222 kB loadslave dpdk1: enabled
may_enable: trueI tried active−backup mode, the result is same.But I just deleted one of dpdk port with command:
ovs-vsctl del-bond-iface dpdkbond dpdk1the result of throughput is 5.31 Gbits/sec.Looks like ovs-dpdk bond would seriously influence throughput of network.dpdk version: 19.11.10
ovs version: 2.13.3Is there any configuration missing?
Any help would be appreciated.
Thank you very much!
LongzhenPowered by Discourse, best viewed with JavaScript enabled"
1283,rx-discards-phy,"Have problem with new Mellanox ConnectX-4 Lx EN 50Gbps.Use it on old server Dual X5650/128Gb DDR3 1333/PCI-E 2.0 x8.Default settings on RHEL 8.3 + this:ethtool --set-priv-flags eth2 rx_cqe_compress onethtool -C eth2 adaptive-rx offethtool -G eth2 rx 8192 tx 8192setpci -s 06:00.0 68.w=5936ethtool -A eth2 autoneg off rx off tx offifconfig eth2 txqueuelen 20000ethtool -L eth2 combined 12service irqbalance stop<irq smp_affinity to 12 cores with NUMA node #0, as card)>I test card with XDP program XDP_DROP, and see errors in ethtool -S and packet lose:rx_xdp_drop: 3801290644rx_discards_phy: 1296930300rx_buffer_passed_thres_phy: 7049089607rx_pci_signal_integrity: 0tx_pci_signal_integrity: 12outbound_pci_stalled_rd: 0outbound_pci_stalled_wr: 0outbound_pci_stalled_rd_events: 0outbound_pci_stalled_wr_events: 1076rx_discards_phy grows along with rx_xdp_drop, amounting to about 27%. outbound_pci_stalled_wr is in the range 50-70. outbound_pci_stalled_wr_events is growing.Test traffic 6Mpps / 3Gbps, of which ~ 1.7Mpps are dropped. What am I doing wrong? Thanks.Hi ,Please refer to the below communityhttps://community.mellanox.com/s/article/understanding-mlx5-ethtool-countersrx_discards_phyThe number of received packets dropped due to lack of buffers on a physical port. If this counter is increasing, it implies that the adapter is congested and cannot absorb the traffic coming from the network.Regarding performance tuning , please refer to the below community :https://community.mellanox.com/s/article/performance-tuning-for-mellanox-adaptersIn case you need further assistance and debug , please reach our support at :networking-support@nvidia.comThanks,SamerOf course, I fully followed all the tuning recommendations and error help. Can you give a specific answer to the questions? The card is 25Gbit, the PCI bus speed in this case is 32GB / s, the processor is loaded no more than 25%, all interrupts are distributed and attached to their cores. But the card cannot “accept” even 3Gbit, and the error counter starts to grow from 600-700Mbps. What are the specific recommendations for solving the problem?Powered by Discourse, best viewed with JavaScript enabled"
