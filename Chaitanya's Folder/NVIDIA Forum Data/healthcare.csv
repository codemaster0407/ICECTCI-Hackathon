,query,data
0,audio-videos-streaming-sample,"Hi,Are there any reference apps that showcase low latency audio and video streaming from one Jetson to another?Hello, are you more interested in Jetson devices? In which case the Jetson forums might be more helpful Jetson & Embedded Systems - NVIDIA Developer ForumsFor the Clara Holoscan SDK which is supported on Clara devkits and x86 machines, we can find the sample apps at clara-holoscan-embedded-sdk/apps at main · NVIDIA/clara-holoscan-embedded-sdk · GitHub and documentation for the sample apps at Clara Holoscan Sample Applications — Clara Holoscan 0.3.0 documentation.Yes, but which sample app showcases the low latency audio and video streaming?
Could not find it.Currently the Clara Holoscan SDK is only supported on Clara AGX devkit, NVIDIA IGX Orin devkit, and x86 workstations. Could you confirm that you are using one of the hardware options? The SDK is not currently supported on Jetson.All the sample app (endoscopy_tool_tracking/_gxf, hi_speed_endoscopy, ultrasound_segmentation/_gxf) would showcase low latency video streaming, please see their documentation. Is there a functionality in particular you’re looking for?Powered by Discourse, best viewed with JavaScript enabled"
1,how-does-parabricks-pipelines-perform-data-cleaning,"Hello, we are trying to use pbrun to receive the off-machine data-NextSeq550AR, and generally clean up the data, such as using fastQC and Trimmomatic software, when using BWA-men for mapping, do you need to do this when using pbrun?Hi,None of fastQC nor Trimmomatic is included.
You will still need to clean your data using this software before using Parabricks.Regards,
MyriemePowered by Discourse, best viewed with JavaScript enabled"
2,help-with-installation,"I’m trying to follow this guide NVIDIA Clara Parabricks on the AWS Cloud to run parabricks on AWS.I pulled the docker image and don’t know what to do next. In the instruction, it says ‘sudo parabricks/installer.py’
but where can I find the installer.py script?I can skip this step and pull the image from AWS but how can I request for the trial license instead?Does anyone run into this situation like me? Please help me on thisHello,For the newest version of Parabricks you can follow this guide
https://docs.nvidia.com/clara/parabricks/4.1.0/tutorials/cloudguides/aws.htmlBestthanks, is there a trial license for Parabricks or I have to pay everytime I run deepvariant on parabricks? ThanksThe latest version of Parabricks no longer requires a license to run itI followed exactly the tutorial in Running Parabricks on AWS - NVIDIA Docs, creating instance with ‘g4dn.4xlarge’. When I execute the command to run pbrun germline, I received this error complaining that my GPU does not have enough of RAM.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3,singularity-image,"Hi, I just received a license key for the 90-day COVID research Parabricks package. We have a multi-user cluster on which we run Singularity rather than Docker containers, due to security issues. Is it possible to get Parabricks as a Singularity container? Or would you recommend that I try to build a Singularity container from the provided Docker image myself?
Thanks for any suggestions.Hello Susan,Glad to hear you’ve received the 90-day COVID Parabricks package and we certainly appreciate your work to advance this research. For Singularity; you have a few options:1 - If you have docker image of Parabricks, the Singularity container will be created using this image and so you won’t need sudo access.
2 - If the Singularity container will be created using the definition file than you need sudo access. If you need to use singularity but the machine you want to use does not have Docker and you do not have sudo access, the thing that can be done, is a local installation on another machine where you have sudo access and Singularity 3 or above installed. Follow for that the instructions provided at NVIDIA Clara Documentation) , or for convenience, as follows:# 1- Log in to a machine with sudo access and singularity 3.0 or higher.# 2- To download the software:$ wget -O parabricks. tar .gz  "" < DOWNLOAD_LINK > ""# 3- Unzip the package$  tar -xvzf parabricks. tar .gz# 4- Do a local installation:$ mkdir localdir$  sudo . /parabricks/installer .py -- install -location localdir --container singularity# 5- Everything will be installed in localdir/parabricks folder. Tar this folder.$  cd localdir$  tar -cvzf parabricks_install. tar .gz parabricks# 6- and copy it to the node which will be used for testing and has singularity v3.x.# 7- Untar the tar.gz file in some <INSTALL_DIR>$ cd <INSTALL_DIR>$  tar -xvzf parabricks_install. tar .gzPowered by Discourse, best viewed with JavaScript enabled"
4,parabricks3-7-a100-cudasafecall-failed-at-parabricks-src-mem-chain-kernel-cu-136-invalid-device-symbol,"Hi there,I am running pbrun with an A100 on a supernode, and I am getting the following error. How do we solve this case?The error is:base on the docker image nvidia/cuda:11.2.0-cudnn8-devel-ubuntu18.04Hey @tj_tsaiThanks for all the information. Parabricks has two installation paths, one for non-ampere devices, and one for ampere devices. It looks like your installation is built for the non-ampere version, but you do have ampere hardware. This is an easy fix. If you re-run your installer, but this time with the --ampere flag, this should fix your issue.Let me know if you still run into issues. Thanks!Hey @gburnett ,
We have solved it. Thank you for your help.Before: parabricks.deb
After: parabricks-ampere.debThe following documentation does not mention the Ampere version nor does it require the user to install Ampere versionNode Locked License, Debian Package Installation

image1248×869 84.3 KB
And we also didn’t notice the Ampere version since the word Ampere is covered.
Nvidia Clara Parabricks Bare Metal Debian Package

Screenshot from 2022-05-06 18-22-151022×597 30.6 KB
This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
5,unauthorized-authentication-required,"I just begin to install the Parabicks with the trial installer package, but failed with an “unauthorized” error.But I can download the image manually with docker pull.Hey @lkuang,Can you send the exact command you used to install?Thanks,
Gary BurnettI have several tries, like:
sudo ./parabricks/installer.py,
sudo ./parabricks/installer.py --ampere
orPS: I test on a DGX-A100 Station.Hey @lkuang ,This might be an issue with the version of the installer you are using. You might be using an older version of the installer. Do you know when you downloaded this one? I would recommend downloading the latest one from NGC if you haven’t.Hi Gary,I download the latest version, and install it again. But the error is the same.
image1504×625 144 KB
Hi @gburnett,The “Unautherized” error is passed by used specified intallation director option  “–install-location” .However, now there this a “GPG error”, which I think is related with recent GPG repository updateing and still not be managed now.Hi @lkuang,We are working on pushing a new package with a fix for the GPG repo. It should be out very soon. Thank you for your patience.Hey @lkuang ,The new version is up on NGC and you can access it by downloading the most recent version.Thank you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
6,error-accessing-s3-bucket-with-parabricks,"Hi, first time using Parabricks and I’m hitting an error when running a deepvariant_germline pipeline configured to write the output files to an S3 bucket.  I have parabricks running in an AWS ec2 instance (from the AWS marketplace) and can run the pipeline successfully on the instance using the sample data provided in the tutorials.Command I am running:The error:Traceback:The ec2 instance has an IAM role giving it access to the bucket, and I have confirmed that I can copy files from the instance to the S3 bucket with the aws cli, but the parabricks command is not allowing the output paths to be S3 paths, even though the documentation indicates an S3 output path can be used.If anyone has suggestions of what I am missing please let me know, thanks!Hey @alec8,Unfortunately this is an error in the documentation. We no longer support S3 paths directly as the output for files. I’d recommend you write to your local ec2 instance storage and upload to S3 as a separate step. Or, you can try using EFS storage, which is more suited for these read/writes.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
7,problem-with-gpu,"According to this userguide Running Parabricks on AWS - NVIDIA Docs, the ec2 instance type should be “g4dn.4xlarge”. I followed and when executing the germline pipeline, I received this error:So I change the instance type to “g4dn.8xlarge”, and got a new error:What should I do now?Hey @huyen.nguyen,There could be a few causes for this error. You are using the g4dn.4xlarge machine which has GPUs so that is all correct. Which AMI are you using? It could be that there is no driver or an incompatible version of cuda is on the machine.Can you also run nvidia-smi on this machine and verify that the GPUs are usable?Thank you!Hi Gburnett,I used a “Deep Learning’ AMI but I don’t remember which one but I am sure it supports g4 instances.However, if I type the command ‘nvidia-smi’ it says the command does not exit. Does that mean the gpu driver is not installed?ThanksCan you share output of:
sudo lspci | grep -i nvidiaAnd this command:
sudo docker run --rm --runtime=nvidia --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smiHiI am trying to run fq2bam using this tutorial here: Tutorials - NVIDIA DocsI wanted to check if the moderator @gburnett and/or @andjoseph have tried out the stuff as mentioned in those tutorials on an AWS g4dn.12xlarge or any AWS g4 machine.As experienced by @huyen.nguyen,  I am also getting a similar error - I am using g4dn.12xlarge and nvidia/clara/clara-parabricks:4.1.1-1Here is the nvidia-smi infoThis is the stderr of the fq2bam jobHowever, if I use nvidia/clara/clara-parabricks:4.0.1-1, the above runs fine. Hence, something is wrong with 4.1.1-1For the sake of completeness, here is the output when using nvidia/clara/clara-parabricks:4.0.1-1As evident from nvidia-smi from both the above runs:nvidia/clara/clara-parabricks:4.0.1-1 is using CUDA 11.7nvidia/clara/clara-parabricks:4.1.1-1 is using CUDA 12.0so - it could very well be that the CUDA version 12.0 is causing these particular issues with nvidia/clara/clara-parabricks:4.1.1-1Hi,The latest version of Parabricks’s fq2bam requires 24GB of memory.
Can you please try running with --low-memory option?BestHi @mdemouthBased on your suggestion, I tried this  below with nvcr.io/nvidia/clara/clara-parabricks:4.1.1-1  and get this errorHi @mdemouth @gburnettI now tried this with nvcr.io/nvidia/clara/clara-parabricks:4.1.0-1 and I get this error [src/PBCuBGZFWriter.cu:590] CUDA_CHECK() failed with out of memory (2), exitingFull log message:Let me know if you need additional info.Hi @avenkatraman,Please try without using--gpusort --gpuwrite with T4.HI @mdemouth ,Does use --low-memory affect the output?Hi @mdemouthWhere does --gpusort --gpuwrite come in handy then? The nvidia docs for fq2bam best performance suggest their usageI am trying this out on a g4dn.12xlarge which has 4gpus 48cpus 192GiB mem and 900GB NVME SSD - I am using docker and nvcr.io/nvidia/clara/clara-parabricks:4.1.1-1As you can see from the below pb_fq2bam finishes the BWA-MEM portion of it,  but errs out during MarkDuplicates, BQSR with some CUDA errorsThanks.Powered by Discourse, best viewed with JavaScript enabled"
8,monai-label-community,"Check out the MONAI Label community forum for support: Issues · Project-MONAI/MONAILabel · GitHubPowered by Discourse, best viewed with JavaScript enabled"
9,pb3-8-installation-error-dssg-doesnt-exist-in-the-container,"When intalling the pb3.8 with singularity, an unexpected error “/dssg doesn’t exist in the container”. Here is a screenshot.

7a01527e7a7ae90f69888ca90a0ec511395×1066 43.8 KB
PS: The host OS is CentOS 8.3.2011.
Thanks .Hey @lkuang,It looks like it’s having troubling mounting to that location. Can you share the command you used to run the installer and can you maybe try to ls that mount directory (/dssg/home/…) to make sure it exists.Thanks!Hi @gburnett,Thanks for you relpy.
The screenshot shows the installation command (cmd), the directory, etc.

16564694270561920×1150 274 KB
The /dssg is actually the physical directory.It is confirmed the singularity setup make the container mount the directory automatically.  After unseting it, now parabricks can be installed correctly.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
10,cuda-toolkit-12-0-0-tensorrt-sdk-is-needed,"Hello,I’m trying to use CUDA toolkit 12.0.0 and the corresponding TensorRT SDK is not available.Could you please help to get a specific version of TensorRT which is compatible with CUDA Toolkit 12.0.0 ?Best regards,
IliesTensorRT 8.6 should be coming in the new few months.Do you have date/time for this release ?Same question here. A “few months” covers a lot of ground.Hello,You can expect to hear about the next TensorRT update at the upcoming GTC on the week of Mar 20.Cheers,
JoohoonHey everyone,TensortRT 8.6 EA is available. Please check in the download TensorRT 8.6 EARegards,
IliesThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
11,mutectcaller-invalid-tumor-sample-name-input,"I was trying to use the stand-alone toll “mutectcaller”, but in the argument --tumor-name always gave me trouble, but in the documentation there’s no example in order to understand what could be a valid name. Could you please give me some advice?Powered by Discourse, best viewed with JavaScript enabled"
12,how-to-make-nvidia-clara-parabricks-4-0-work-on-linux-arm64-platform,"As I known, currently NVIDIA Clara Parabricks 4.0  can seems work on Linux/AMD64, for example when I run as follows:
$  docker run -it nvcr.io/nvidia/clara/clara-parabricks:4.0.1-1 /usr/bin/bash
but get errors:
WARNING: The requested image’s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
standard_init_linux.go:220: exec user process caused “exec format error”
libcontainer: container start initialization failed: standard_init_linux.go:220: exec user process caused “exec format error”
currently I have not found any code of this NVIDIA Clara Parabricks 4.0  , so can you give some advice to transplant on Linux/ARM64 ?Hey @yinwei0108,Currently Parabricks doesn’t support ARM.Powered by Discourse, best viewed with JavaScript enabled"
13,cannot-delete-a-license-server,"Hi, there,I created a license server without checking “Create legacy server” for a trail.But I could not delete it.

image1447×494 55.4 KB
I could not find where to delete or release resources.

image1497×936 143 KB
And I was trying to delete the service instance,

image1497×936 108 KB

image1497×936 145 KB

it showed:The service instance “0011w000024umcvqae-2022-08-04_09-21” has license server(s) installed on it.
Please return the licenses before deleting the service instance.I don’t how to return the licenses. Please help guide me to return the licenses.I have found the solution by googleing.I have deleted the related resources.Powered by Discourse, best viewed with JavaScript enabled"
14,profiling-parabricks-germline-with-nvprof,"Hi!
I want to do some benchmarks of the full germline using different input files but I cannot manage to use nvprof on it.
How can I run nvprof on pbrun given that the application is inside a singularity image?
I tried the following trivial command but it didn’t work:
nvprof -o profile.nvprof pbrun germline […] 
Thanks in advance.Powered by Discourse, best viewed with JavaScript enabled"
15,how-to-deploy-parabricks-using-aws-ami,"Planning to deploy Parabricks on AWS, is there an AMI for ease of deployment?If you are planning to deploy Parabricks on AWS, we created an AMI for ease of deployment. If you are interested in using the AMI, please send an email to parabricks-support@nvidia.com with the subject line: AWS AMI for Parabricks, along with your AWS Account ID. Our support team will white list the AMI so you can access it from your AWS console.We also have a recipe to run the AMI on AWS Batch. If you are interested, please email us at parabrick-support@nvidia.com  with your GitHub ID and we will give you access to the repository.Researchers and scientists using NVIDIA Parabricks on AWS may be eligible for AWS promotional credits through the AWS Diagnostic Development Initiative (DDI) Program.Powered by Discourse, best viewed with JavaScript enabled"
16,how-to-download-clara-parabricks-pipelines-trial-version,"Here are the instructions to download the trial package when you have been granted access.NGC_PARABRICKS_SETUP_instructions.pdf (7.4 MB)Thanks. I have finally installed parabrick. But while trying it on one sample, I am getting an error “cudaMemGetInfo returned 30” . The command and error are shown below:date; pbrun somatic --ref /hg19/ucsc.hg19.fasta --in-tumor-fq /test/SRR3163205_1.fastq.gz /test/SRR3163205_2.fastq.gz --out-vcf /test/SRR3163205.vcf.gz --out-tumor-bam /test/SRR3163205_tumor.bam; dateWed Jan  6 18:49:59 IST 2021
Please visit https://docs.nvidia.com/clara/#parabricks for detailed documentation[Parabricks Options Mesg]: Automatically generating ID prefix
[Parabricks Options Mesg]: Read group created for
/home/akansha/vivekruhela/parabricks_free_trial_vv3.2.0_2021-02-28/test/SRR3163205_1.fastq.gz and
/home/akansha/vivekruhela/parabricks_free_trial_vv3.2.0_2021-02-28/test/SRR3163205_2.fastq.gz
[Parabricks Options Mesg]: @RG\tID:SRR3163205.1.1\tLB:lib1\tPL:bar\tSM:tumor\tPU:SRR3163205.1.1cudaMemGetInfo returned 30
 → unknown error
Please contact Parabricks-Support@nvidia.com for any questions
Exiting…Could not run fq2bam as part of tumor sample processing for somatic pipeline
Exiting pbrun …
Wed Jan  6 18:50:01 IST 2021Kindly suggest. Thanks.Hello, I have followed this stuff, but no files downloaded to my computerHi Vivekr1,can you please give me more information about :Can you also make sure not to run the software within the directory where you installed it.Regards,
MyriemeHi nur.1233We are sorry you are facing this error. We are aware of the download issue using the UI.
We are working on a fix and I will let you know when it is back to normal.
If you want to download it before, you can use the CLI as explained in the second part of the document.Best,
MyriemeHere is the output from nvidia-smi

Screenshot_2021-01-07_15-50-271024×768 79.9 KB
I am using docker version of parabrick.Vivekr1,We are sorry but it seems that the GPUs you are using don’t meet the minimum requirement for Clara Parabricks Pipelines software :https://docs.nvidia.com/clara/parabricks/v3.2/text/getting_started.html#prerequisitesDoes that means, for 2 GPU server there should be 100 GB CPU RAM and 24 threads. It has nothing to do with 2 units of 11 GB GPUs. Please correct me if wrong.But my server has 126 GB RAM and 40 threads. Although 3 jobs are running and 40 GB of RAM and 30 threads are engaged.I was referring to
Any GPU that supports CUDA architecture 60, 61, 70, 75 and has 12GB GPU RAM or more. It has been tested on NVIDIA P100, NVIDIA V100, and NVIDIA T4 GPUs.If I replace 11 GB GPU with 12 GB GPU, will that work?? I hope the rest of the requirements are fulfilled except GPUs. So, I need confirmation so that I can initiate the process for GPU replacement.Hello everyone,
Could anyone please tell me if it is possible to execute Parabricks on Google Colab Pro, as they also provide high end GPUs?
If possible, could I have some instructions on how to go ahead with it?
Thanks a lotHi, I filled the submit multiple times, but I didn’t received any e-mail for the activation of the trail or for the installation. Can you give some hint if I had only to wait for the e-mail or if there’s somethig I can do?EDIT: I recived the e-mail, so I was able to download the tar.gz file.Powered by Discourse, best viewed with JavaScript enabled"
17,could-not-run-fq2bam-as-part-of-germline-pipeline,"hi,I am  getting the following error below when running pbrun germline. Sometime it works (can run completely), sometime it throws the error below. It happened in both v3.5.0 & v3.7.0.For v3.5.0:For v3.7.0How can I solve this?Hi @tj_tsaiCan you please let us know what is  tmp-dir disk size and memory size on the server?
And I would recommend you to not use the option --x3 .what is tmp-dir disk size?what is memory size?I would recommend you to not use the option --x3 .Where the --x3 option is provided from the earlier NV members for debug use.hey @mdemouth
Do you have any clues or tools to debug the error case (killed when running germline)?
or is there any other ways to dump more information about the situation?Hi @tj_tsai ,Is this a shared node on a cluster? Are there other things running on this system?hi @mdemouth ,
Yes, I run it in a pod (in a node on a cluster).
There are other things running in other pods on the same node.hi @mdemouth,
if parabricks is running in a shared node (with 128GB of CPU RAN),
is it recommended to use the parameter --memory-limit 128 (unit: GB) or some other way?Hi,This is likely what produces the issue.
We recommend to reserve the full node for such job to prevent this issue to happen.--memory-limit  tells how much RAM is available for Parabricks job. But it does not guarantee the system has this much RAM on a shared nodeBest,
Myrieme@mdemouthAbout the --memory-limit experiments.I have tried the parameter --memory-limit in an EC2 instance of type gdnn.2xlarge (T4, 8vCPU, 32GB) on AWS. If I run germline without the parameter --memory-limit (i.e. by default), the output.vcf can be generated successfully. But if I pass the parameter --memory-limit 28 or 26 or 20 to germline, it will get the OOM signal (killed by OOM-killer) when running Program3 (Marking Duplicates, BQSR). Actually, germline just works for less than or equal to 16.The strange thing is that the instance doesn’t use shared memory, why does it get OOM with --memory-limit 28 on a VM with 32GB of RAM (avaialble: 30GB) ?Here is the germline script.
image1012×702 118 KB
This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
18,fq2bam-rel3-nanopore-wgs-288418386-fab39088-fastq-gz,"Hi. Please help to debug.
Sample downloaded from
aws s3 cp s3://nanopore-human-wgs/rel3-nanopore-wgs-288418386-FAB39088.fastq.gz .Hi Alex,We are sorry that you are facing this error.
Unfortunately,  fq2bam is only intended for short reads, and here your you use a long reads fastq file.MyriemeHi, mdemouth.Thank you for your reply.
Another question. Originally BWA MEM works with  Oxford Nanopore reads. Is this option -x implemented in Parabriks? Or Parabricks does work exclusively with Illumina?Powered by Discourse, best viewed with JavaScript enabled"
19,parabricks-crashes-when-primary-user-group-is-not-allowed-to-use-gpu-devices,"Hello,I hope this is the right place to post this issue, if not please redirect me.after installing the parabricks software I am unable to start the germline pipeline. Executing “pbrun version” and “pbrun germline --help” both work fine, with the expected output.The administrator of the system I am using decided to permit access to the gpu devices /dev/nvidiaX only for users of a specific group (484). Because we are using other means to authenticate users, my users primary group is fixed at a different number (say 50000). Unfortunately, in the way that pbutils.py is retrieving the uid:gid combination for docker run (getpwnam) it refers to the passwd database of the system.Therefore parabricks is always invoked with docker run -u USERID:50000, but nvidia devices in the container are still locked to the group 484.I worked around this issue by modifying the proper line in pbmaster.py, to just always send the correct gid. Can this be fixed properly somehow (f.e. by retrieving this data from the executing shell)? Note that pbrun germline --help retrieves the data from the gpu devices correctly, so it seems to be doing something different than the actual pipeline execution…Thank you  for your interest in Clara Parabricks Software, and sorry to hear you had some hard time running it.
As you mention, our software gets the group ID from the primary group.
A request for enhancement have been initiated.
Thank youPowered by Discourse, best viewed with JavaScript enabled"
20,v4-0-vs-v3-6-germline-pipeline-had-difference-in-header-in-gvcf-file,"Hi Parabricks team,I was running Parabricks Germline pipeline with most of my sample ran on version 3.6, and some of my new samples ran with version 4.0 with the same options, the only difference is Docker Image.
But when I do joint-calling, I found out that the GVCF header is slightly different in DS, show as below:When I checked documents, I did’t see any note of the difference of Germline options. That’s why I assume the default stay the same between v3.6 and v4.0.
So, I’m wondering if there are any difference in v4.0, compared to v3.6, might cause this problem?
And how can I output “<ID=DS,Number=0,Type=Flag,Description=“Were any of the samples downsampled?”>” in header on my next run on version 4.0?
I try to consist my gvcf header to avoid re-run my old sample and/or manually correct the header one-by-one. Thanks for help!Po-YingHey @fup,Different versions of Parabricks germline are are based on different versions of GATK so there will be some subtle differences in versions. The latest version 4.0.0-1 uses BWA version 0.7.15 should match results from GATK version 4.2.0.0. I believe that 3.6 results should match with GATK 4.0 or even earlier, so that could explain the differences in the GVCF header.Powered by Discourse, best viewed with JavaScript enabled"
21,tensor-interoperability-with-multiaiinferenceop,"We want to pass an input tensor from a custom Holoscan operator into MultiAIInferenceOp in Holoscan v0.5.0.However, we get the following error:2023-06-12 00:02:12.516 ERROR /workspace/holoscan-sdk/modules/holoinfer/src/utils/infer_utils.cpp@25: Error in Multi AI Inference Codelet, Sub-module->Data_per_tensor, Tensor output_tensor not foundIt appears that MultiAIInferenceOp is not able to locate the tensor “output_tensor”.In our custom operator, do we need to structure or name our output tensor so that MultiAIInferenceOp can find it?We based our python code on the Holoscan documentation and examples on tensor interoperability. We have the following code in our compute() method in our custom operator (ImageProcessingOp):We also have the following in our .yaml file:We also labeled the input/output ports between the custom operator (ImageProcessingOp) and the MultiAIInferenceOp:Here is the full error message for reference:We have been successful in passing tensors between custom operators and even between built-in Holoscan operators (eg. FormatConverterOp) and a custom operator but have been unable to do this with MultiAIInferenceOp.Thank you for any assistance or guidance you can provide!Hello, I have a guess as to what may be happening.When we do op_output.emit(out_message, ""output_tensor"") and I assume in setup() spec.output(""output_tensor""), “output_tensor” is specifying the output port name, which is different from the tensor name. Please see the example under You can add multiple tensors to a single holoscan.gxf.Entity object by calling the add() method multiple times with a unique name for each tensor, as in the example below: in Creating Operators - NVIDIA DocsIn the MultiAI Ultrasound example, we see that plax_cham_pre_proc is the output tensor name from the preprocessor (FormatConverter) holohub/multiai_ultrasound.yaml at main · nvidia-holoscan/holohub · GitHub, and when connecting the operators we’re not using tensor names but port names, in this case the outgoing port name from the FormatConverter is “”, holohub/multiai_ultrasound.py at main · nvidia-holoscan/holohub · GitHub, and then in the downstream operator it referenes the tensor name plax_cham_pre_proc again holohub/multiai_ultrasound.yaml at main · nvidia-holoscan/holohub · GitHub.Given the info above, could you try to add a unique name to the tensor when adding to the output in your custom op?and reference that name in the MultiAIinference spec in yaml:Thank you very much for your help and the detailed response! We were able to name the tensor using the approach you suggested and get it working.Great to hear that it’s working!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
22,troubleshooting-download-example-fastq-files,"Hi there,Doc: Download Example FASTQ Files
I followed the instructions and found something different.Q1: Convert SRA to FASTQ files(It might take 35~39 hours for fetching.)I tested the command fastq-dump and found that it didn’t convert the SRA files to FASTQ files.
Instead, it fetched the FASTQ files directly from the NCBI SRA source.The conversion command should be:(prefix files with ./)
(It might take 13~15 hours for conversion.)Q2:  paired reads have different names: “SRR7890824.1.2”, “SRR7890824.1.1”Doc:  somatic pipelineI followed the instructions and got the following error.

image935×375 39 KB
Where SRR7890824.1.2 means accession.spot.readid
The readid should not be placed in fastq files, otherwise it will cause the somatic pipeline to throw an error.Therefore, the conversion command should be corrected to:(don’t take the parameter -I)or to fix pbrun somatic to accept the fastq format accession.spot.readid?Could you clarify the two questions?Hello @tj_tsai ,This seems to be indeed an error. Thank you for bringing this to our attention.
We will update the documentation.Best,
MyriemeThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
23,nvidia-parabricks-does-pbrun-support-gpu-options,"hi, thereWhen I ran pbrun fq2bam ... to test a small dataset, I met a memory error as shown below.Here is the GPU summary of the Driver and CUDA versions
image726×443 48.7 KB
Is there any methods to restrict the GPU usages ( memory<= 11GB) to fulfill my requirements?Not enough available memory for these GPUs…Attached the log:According to the article “There is no CUDA API to limit memory usage, with or without nvidia-docker.”,there is a comment:There is no CUDA API to limit memory usage, with or without nvidia-docker.so bad…Hi,we are sorry, but one of the requirements of Clara Parabricks Pipelines is GPU with 12 GB memory RAM or more.https://docs.nvidia.com/clara/parabricks/v3.0/text/local_installation.htmlMyriemePowered by Discourse, best viewed with JavaScript enabled"
24,use-genotypegvcf-with-genomicsdb-workspace,"Hello!Im trying to perform joint genotyping on one or more samples pre-called with HaplotypeCaller using  gatk genotypegvcf (https://gatk.broadinstitute.org/hc/en-us/articles/360046224151-GenotypeGVCFs).I have created GenomicsDB workspace with GenomicsDBImport, but it looks that pbrun genotypegvcf doesn’t work when GenomicsDB.I am able to converts variant calls in g.vcf format to VCF, but why does GenomicsDB doesn’t work?Command I’m using:Output:genotypegvcf doesn’t recognize GenomicsDBHi,We are sorry but the input --in-gvcf should be a g.vcf or g.vcf.gz file, and not a GenomicsDB workspace.Best,
MyriemeThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
25,planning-to-set-up-the-parabricks-environment,"Hi there,Our company has set up the Parabricks testing environment and is ready to discuss purchasing licenses. After some survey, we have 3 questions to clarify.Q1: What does “license server” mean?
Our hardware (for example):Suppose we want to buy licenses for T4x8 (Total: 32). The option for the licenses should be: GPU Based License + Flexera based. Is this correct?And about Flexera basedTo be able to download NVIDIA Parabricks software licenses, you must create at least one license server on the NVIDIA Licensing Portal and allocate licenses in your entitlements to the server.Does “license server” mean our local node(s) or a virtual group name, or something else?Q2: As stated above, Is there more information about “license server”?Q3: Can pbrun verify the license without accessing to the Internet when running the pbrun germline (for example)?Best regards,
TJ TsaiHi TJ,We have 2 different license types.Node-lock - which works similar to the eval license. You provide the GPU UUIDs to us and we create a specific license for those GPUs. If you have 8 GPUs, the license would be built exactly to those 8 GPUs and cannot be used anywhere else.Flexera License Server - This offers more flexibility as the licenses are not directly tied to GPUs. If you buy 8 licenses, you add them to the Flexera LS, and they can be ‘borrowed’ by Parabricks and then returned after your job finishes. So you can share those 8 licenses amongst multiple people using Parabricks.‘license server’ gets made in the NVIDIA License Portal and then you also create a Flexera License Server. This is an application you download and can put on any machine, as long as you can access it from the machine you’re running Parabricks on. Can be the same machine, can be a VM, etc.Q2: Here is a link to our most detailed license server documentation:Documentation for system administrators that explains how to install, configure, and manage the NVIDIA virtual GPU software license server.Q3: If you have node-lock license, you don’t need the internet as it is baked into the install. For Flexera LS, you need to be able to access the LS over the network or on the same machine.@andjosephWe have more questions.Q4: For Node-lock, if we buy a license for 8 GPUs, and then suppose one of these GPUs doesn’t work and we buy a new GPU to replace it, can we get an updated license?Q5: If pbrun crashes, such as OOM (killed by OOMKiller, sometimes occurs in shared nodes), can pbrun actively return the “borrowed flag”? If not, Can the license server get the crash signal and get the “borrowed flag” back?Q6: How can client sides know where the license server is?
Flexera License, Debian Package — Clara Parabricks v3.7 documentation
where this document seems unclear.For the server side (the license server):For the client sides (super node x 4):Are these configurations correct?Q7: I have an account “tj_tsai@asus.com”, but I couldnot log in to “NVIDIA Licensing Portal” (NVIDIA License Portal). Is there any limit to login the portal? (Currently, my Parabricks free trial has not expired.)Q8: Are there any trials for Flexera License Server? So we can evaluate the Flexera-License case in super nodes.Q4: Yes, we can create a new license file for you.Q5: If the licenses don’t get returned we can help you return them manually or you can delete the database and re-upload the license.bin file giving you a fresh license server:Documentation for system administrators that explains how to install, configure, and manage the NVIDIA virtual GPU software license server.Q6: I’m not sure I understand what you’re asking, can you clarify? The License Server is wherever you installed it. So if you installed it on a machine with IP address 10.10.10.10 you would put “flexera-server:10.10.10.10:7070” in the command.When you run a Parabricks job, it will pull licenses from the license server you specify.Q7: When you purchase Parabricks you will get instructions on registering and creating an enterprise account. Trial users don’t have an enterprise account with NVIDIA License Portal access.Q8: Unfortunately we don’t have trials with Flexera LShi @andjosephQ6 : How can client sides know where the license server is?For Q6, I give a clearer description as follows.Suppose we have the following network information (IP info):
A: License Server: 192.168.1.2
B: super-node1: 192.168.1.2 (same as License Server, i.e. in the same machine)
C: super-node2: 192.168.1.3
D: super-node3: 192.168.1.4
E: super-node4: 192.168.1.5WhereHow do we set up the relationship between the server side and the client sides?For A (server side),or upload the license.bin via the web UI:
 5.1. Installing a License

image1024×768 101 KB
For B-E (client sides),Are the above configurations correct?Then, suppose a user executes the pbrun command on super-node4 (IP: 192.168.1.5), the pbrun program will automatically pull the license.bin file from the Flexera server (IP: 192.168.1.2) and place it in super-node4:/usr/local/parabricks/license.bin. ??Thank you so much for answering so many questions.You always need to upload the license.bin file to the web UI. Everything else looks good though.Also, note, this is only for Debian install. For docker and singularity, you point it to the flexera server in the install and it automatically configures that config.txt file for you.It doesn’t pull the license.bin file from the web server and place it anywhere. You put the license.bin file on the flexera server, and then this web app monitors how many licenses you are using. When you run Parabricks, it borrows the licenses, and then returns them when the job is finished.returns them when the job is finished.Does it mean, users only pay the license fee when they are actually running pbrun? Or the fee is charged no matter they use it or not, since day 1?The fee is charged no matter they use it or not, since day 1?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
26,how-to-generate-junctions-file-for-star-fusion,"Hi there,I am trying to run starfusion where it requires the “Chimeric.out.junction” file created by STAR.When I run the star alignment pipeline with the following command I don’t get the “Chimeric.out.junction” file.   is there something I should change?Hello rcorbett1,we are looking at this right now and will get back to you soon.Best,
MyriemeHi rcorbett1,it seems you did not pass the parameter to get the chimeric output.
Please try again using --min-chim-segment with a value greater than 0.Best
MyriemeThanks. That works.This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
27,unable-to-download-parabrick,"I am trying to download parabrick tar file after requesting its trial version. I also got an confirmation mail from parabrick developer team but I am not able to download the tar file of parabrick tool. Can anyone suggest me how can I download this?
Whenever I try to download from my account, I see the status as downloaded but I actually didn’t. I checked that there is no problem with pop-ups in my browser. Kindly suggest at the earliest. Thanks

Screenshot (28)1920×1080 289 KB
Hi Vivekr1,please find here the instructions to download the package.Regards,
MyriemeSir,Thanks for your response. I have successfully installed parabricks. Now I am facing an error or “CudaMemGetInfo returned 30” while running it. I have added a link for complete details of command and error. Kindly suggest.Here is the link:https://forums.developer.nvidia.com/t/how-to-download-clara-parabricks-pipelines-trial-version/165217Powered by Discourse, best viewed with JavaScript enabled"
28,incorrect-username-or-password-error-when-flash-clara-agx,"Got the following error when a window “SDK Manager is about to flash your Clara AGX developer kit module” popped up:
Error: Incorrect username or password. Please try again.Please helpHi @zhuberger,Please see this topic: Read this topic for SDK Manager IssuesLet us know if you still have issues.Thanks,
Tom KHi Tom,Thank you for your response. I was stuck at this step. The only difference is that I choose default Jetpack 5.0 in step 1. Should I change it back to Jetpack 4.5.1? Thanks.

image.png953×747 124 KB
Sorry, I don’t have experience with the Clara Dev kit. Will need to get a Clara person to chime in here. Let me find someone for you.I did the following just now:Hi @zhuberger,It looks like the engineer that supports Clara Holoscan is OOO until next week. I will make sure they jump in to assist ASAP.Thank you very much, Tom! Have a good weekendSame to you!Hello!
The error “Incorrect username or password” could occur if the unit had pre-existing account setup that you’re not aware of. If it’s a new unit, the combination should be ubuntu/ubuntu.
But that same error shouldn’t appear when you use Manual Setup in the recovery mode / reset mode. If you recall, what is the error you encounter in Manual Setup?By the way have you been able to find our Clara Holoscan developer page? And more specifically, for getting started, please see the Clara AGX Dev Kit User Guide and Installing the Clara Holoscan SDK with SDK Manager.Hi,Thank you for the clarification. With manual setup, it said can not find matching device in recovery mode.
IMG_20220510_085229.jpg3000×4000 2.89 MB
For username and password, there are two accounts on Clara AGX ubuntu system. How can I recover the default username and password?Also, the two pre-existing accounts are still there even if I set it to recovery mode and resetting mode.I believe once a unit has been flashed in your case, we cannot recover the default username and password ubuntu/ubuntu. In recovery mode/reset mode, we don’t need to enter any username/password. The error “Can not find matching device in recovery mode.” might indicate to you that you need to put the device into recovery/reset again. Because if we put the unit in rec/reset mode, then try to do flashing and fail, the unit is sometimes no longer in rec/reset mode anymoreHi,I have completed the installation successfully just now. Thank you very much for your guide. What I did differently this time:choose Clara Holoscan package in step 1When the flash window popped up, I reset Clara AGX by pressing the two tiny buttons 16 and 17 together.Once it is reset properly, Clara AGX ubuntu is not running. I only see some green light through the back of the box. Meanwhile, the SDK manager on the host pops up another window, showing a device is detected, please pick one. I picked Clara AGX, then just followed the guide to finish up the installation. Ubuntu has been reinstalled on Clara AGX, so please backup your data if there is any.I tried to reset with button 16 and 17 last week, but it didn’t work. I guess I didn’t press them at the same time properly.
Screenshot from 2022-05-10 08-36-37.jpg2794×1718 412 KB
Great to hear you’ve got it working!Hi, another quick question: Is CSI HDMI input board already in this silver box? For a CSI camera, I just connect it to the HDMI port on Clara AGX, is that correct?Here is one note I found in the doc “The CSI camera module, including the CSI HDMI input board included with the Clara AGX Developer Kit, will enumerate as /dev/video0 and any additional USB cameras will start to enumerate with /dev/video1. If there is no CSI module attached, USB cameras will start to enumerate with /dev/video0.”
Screenshot from 2022-05-10 12-44-18.png1414×368 62.2 KB
Hi Zhu, sorry for the confusion from the documentation. We can connect a HDMI camera to the HDMI(2CSI) port (port 11), but currently do not support CSI camera input.got it. That’s very helpful. thank you!Powered by Discourse, best viewed with JavaScript enabled"
29,parabricks-clara-on-a16-gpu,"We are considering  a DELL R750 server with 2x A16 GPUs and 64c/128t CPUs. Can this server also run the Parabricks Clara package in terms of CUDA, computing power, and license. The  Parabricks Clara will use the deep learning tool deepVariantPowered by Discourse, best viewed with JavaScript enabled"
30,how-to-identify-jetson-orin-developer-kit-as-igx-orin-developer-kit-use-sdk-manager,"Hi ，
Manual said that IGX uses Jetson orin+a6000. We attempted to install the NVIDIA Holoscan SDK into Jetson orin using sdk manager, but the sdk manager was unable to discover IGX devices. Is there any way to make Jetson orin be discovered as an IGX device? ThanksHello, from Holoscan SDK v0.2 and onwards you can install the Holoscan SDK separately from the SDK Manager via any of the methods described in SDK Installation - NVIDIA Docs. The SDK Manager is one of the ways to flash your developer kits, and there is not a way to have the Jetson AGX Orin developer kit discovered/recognized as IGX Orin developer kit in SDK Manager. You could still use the Jetson AGX Orin as itself and try installing the Holoscan SDK on there.May I ask how we need to switch to the graphics display on AGX ORIN? We tried AGX Xavier using the L4T32.5.1 version is OK, but to AGX ORIN, running the nvgpuswitch.sh script will get an error. The message is that the kernel version does not match. I tried to modify the version specified in this script, but the script also failed to run.Please create a separate post for a new topic, thank you!
If you are referring to an issue related to a specific devkit such as the AGX Orin Developer Kit, please find the corresponding forum under  Autonomous Machines - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
31,cudamapper-bam-output,"Apologies for the duplicate post.  I have posted this question to github but not heard any response.Hi folks,
I'm trying cudamapper with this command:
`/opt/GenomeWorks-2021.02.2…/bin/cudamapper PAG33026_pass_concat.fastq.gz ../hg19a.fa -B > cudamapper_2021_02_02_GM24385.bam`

which outputs quite a lot of output to stdout/stderr, from which I have reported the unique lines below:
```
Initialized GenomeWorks logger with log level ERROR
-C / --target-indices-in-host-memory not set, using -Q / --query-indices-in-host-memory value: 10
-c / --target-indices-in-device-memory not set, using -q / --query-indices-in-device-memory value: 5
 Query file: PAG33026_pass_concat.fastq.gz, number of reads: 9983679
Target file: ../hg19a.fa, number of reads: 84
Programmatically looking for max cached memory
Using device memory cache of 24899308094 bytes
Device 0 took batch 1 out of 1790 batches in total
[E::sam_hrecs_update_hashes] Duplicate entry ""5"" in sam header
print_sam: could not add header value
[E::sam_hrecs_update_hashes] Duplicate entry ""5"" in sam header
print_sam: could not add header value
[E::sam_hrecs_update_hashes] Duplicate entry ""5"" in sam header
print_sam: could not add header value
...
[E::sam_hrecs_update_hashes] Duplicate entry ""19"" in sam header
print_sam: could not add header value
[E::sam_hrecs_update_hashes] Duplicate entry ""22"" in sam header
print_sam: could not add header value
[E::sam_hrecs_update_hashes] Duplicate entry ""5"" in sam header
print_sam: could not add header value
[E::bgzf_flush] File write failed (wrong size)
terminate called after throwing an instance of 'std::runtime_error'
  what():  ERROR, print_sam: could not write alignment
Aborted (core dumped)
```
In practise I think I get the ""Duplicate entry ""N"" "" error for each read that is processed.I am trying to use cudamapper to align long reads to the human reference, which works when outputting to PAF format.   When I try to generate SAM or BAM output I get errors that end with:…
[E::sam_hrecs_update_hashes] Duplicate entry “5” in sam header
print_sam: could not add header value
[E::sam_hrecs_update_hashes] Duplicate entry “5” in sam headerAborted (core dumped)My command:
 /opt/GenomeWorks-2021.02.2/bin/cudamapper PAG33026_pass_concat.fastq.gz ../hg19a.fa -B > cudamapper_2021_02_02_GM24385.bamPowered by Discourse, best viewed with JavaScript enabled"
32,run-parabricks-docker-container-with-workflow-language-snakemake-nextflow,"Workflow languages like snakemake and nextflow support specifying a docker container for individual pipeline steps, which is nice for clearly documenting the environment:https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html?highlight=docker#running-jobs-in-containers
https://www.nextflow.io/docs/latest/docker.htmlIs it possible to use these features with the parabricks docker image? I am not sure how it would work given 1) inserting the license and 2) nvidia-docker. An example snakemake snippet might be:Clara Parabricks is supported in WDL, NextFlow and CromwellPowered by Discourse, best viewed with JavaScript enabled"
33,reg-usage-of-somatic-pipeline,"Hi team,Our organization recently acquired the free license for Parabricks to test the performance.
While running the somatic pipeline provided, I realized that interval file (bed file) option is not available for the somatic pipeline to restrict  the  variant calling processes only to the specified regions. whereas this interval file option is available for mutectcaller (which is a part of somatic pipeline) .
Also, in the the documentation CNVkit is available in the Parabricks suite, however , when I am trying to run the CNVkit, it throws an error saying CNVkit is ‘not supported in this release. Exiting’Kindly let us know how to use the bed file for variant calling in the somatic pipeline and run the CNVkit…pbrun version: v2.5.0Thanks in advance…Hello poornachandraroyal33,For the whole pipeline command, can you let me know in what way it is not working? Which command are you running and how is it failing?For CNVkit, the error message is correct, the current release is not supporting CNVkit. Can you point me to where in the documentation CNVkit is referred to? I will get this corrected.Many thanks,
MikeHi Mevella,Thanks for the reply.For the point 1:
We are using following command for running the whole somatic pipeline:pbrun somatic --ref genome.fa --in-tumor-fq sample.fastq1.gz sample.fastq2.gz  --out-tumor-bam sample_output_somatic.bam --knownSites Homo_sapiens_assembly38.known_indels.vcf.gz --num-cpu-threads 16 --tmp-dir  --num-gpus 1 --out-vcf sample_output_somatic.vcfThe pipeline is working perfectly fine without failing. However, we would like to know ‘is there is any specific way to restrict the variant calling process using the interval file similar to GATK-mutect2’ ?
Also, this filtering option is available with the ‘pbrun mutectcaller’ command as shown below (-intervel-file):pbrun mutectcaller --ref genome.fa --in-tumor-bam sample_output_recal.bam  --out-vcf sample_output.vcf --tmp-dir temp  –interval-file AgilentV7.bed --tumor-name sampleFor the point2:
CNVkit is available for the v3.0.0 as mentioned below:Reference the latest NVIDIA Clara Documentation.
However same is not present with v2.5.0, which is provided for free trail.Hi poornachandraroyal33,For the point 1:
Right now --interval-file is only available for mutectcaller.For the point 2:
CNVkit is not available for Free for Covid-19 version, we will update the documentationThanks
MyriemePowered by Discourse, best viewed with JavaScript enabled"
34,is-clara-hardware-platform-or-s-w-sdk,"Hi Nvidia team,I am completely new to this Nvidia platform. I came across this Clara system and found it very interesting. I had few doubts about Clara and thought of checking with you.Thanks,
Swamy.Hi Swamy,Welcome to the NVIDIA Developer forums. Your questions will get better visibility in the Clara category. I will move your post over for you.Best,
Tom KPowered by Discourse, best viewed with JavaScript enabled"
35,gtc-healthcare-conference-sessions-april-12-16-2021,"See what’s next in personalized medicine, next-generation clinics, biomedical research to treat disease, and more.Register FREE, Streamed online.REGISTER FREEPowered by Discourse, best viewed with JavaScript enabled"
36,flexera-ls-http-response-code-said-error-503,"Hi there,We have tried to install Tomcat and License Server software on a local bare metal. It works for pbrun germline.But it fails on an Azure VM using the same installation way. The error message is below:But it works for pbrun licenseinfoCould you help to check what is causing it?
image1920×943 125 KB

image1390×554 109 KB

image1404×593 203 KB

image1266×209 21 KB

image1746×829 121 KB
Flexera software, Flexera License Server, Flexera LS, http error, 50As an Enterprise customer, you’re entitled to Enterprise Support. Please file an Enterprise Support case so that we can assist with this issue.That being said, your config.txt is pointing to a different IP address than your screenshot of the web sever. The flexera server needs to be accessible from the node you’re trying to call it from. Make sure you can ping that server and make sure the IP address matches.Sorry, I pasted the wrong image for config.txt (this was for the bare metal).Are there any possible causes for HTTP response code said error (503)?
image1216×498 161 KB
We could not ping that server, and we will check the networking status. Thank you.We use Parabricks setup on twcc.ai to connect the Azure VM. It works.The root case is caused by our company’s policy.We also found that ping does not work for Azure VMs because the ICMP protocol is not permitted through the Azure load balancer. (See VMs not able to ping in the internet but able to browse websites.)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
37,fq2bam-e-bwa-idx-load-from-disk-fail-to-locate-the-index-files,"I am trying to run fq2bam on AWS with nextflow
Container image used: nvcr.io/nvidia/clara/clara-parabricks-fq2bam:4.0.0-1
Command run:
pbrun fq2bam 
–ref reference.fa
–in-fq R1.fq.gz R2.fq.gz 
–out-bam out.bamI have all the index files at the same location as the reference.fa file
reference.fa, reference.fa.fai, reference.fa.sa, reference.fa.amb, reference.fa.bwt, reference.fa.pac, reference.fa.annI am able run bwa-mem with the same set of fastqs, reference and index filesBut I am seeing this error with pbrun: [E::bwa_idx_load_from_disk] fail to locate the index filesCould anyone help me resolve the this issue.Thank youError log:
[Parabricks Options Mesg]: @RG\tID:H375CDSX5.1\tLB:lib1\tPL:bar\tSM:sample\tPU:H375CDSX5.1
[PB Info 2023-Jul-06 09:08:44] ------------------------------------------------------------------------------
[PB Info 2023-Jul-06 09:08:44] ||                 Parabricks accelerated Genomics Pipeline                 ||
[PB Info 2023-Jul-06 09:08:44] ||                              Version 4.0.0-1                             ||
“[PB Info 2023-Jul-06 09:08:44] ||                       GPU-BWA mem, Sorting Phase-I                       ||”
[PB Info 2023-Jul-06 09:08:44] ------------------------------------------------------------------------------
[E::bwa_idx_load_from_disk] fail to locate the index files
[PB Info 2023-Jul-06 09:08:44] ------------------------------------------------------------------------------
“[PB Info 2023-Jul-06 09:08:44] ||        Program:                      GPU-BWA mem, Sorting Phase-I        ||”
[PB Info 2023-Jul-06 09:08:44] ||        Version:                                           4.0.0-1        ||
[PB Info 2023-Jul-06 09:08:44] ||        Start Time:                       Thu Jul  6 09:08:44 2023        ||
[PB Info 2023-Jul-06 09:08:44] ||        End Time:                         Thu Jul  6 09:08:44 2023        ||
[PB Info 2023-Jul-06 09:08:44] ||        Total Time:                                      0 seconds        ||
[PB Info 2023-Jul-06 09:08:44] ------------------------------------------------------------------------------
For technical support visit  … 4.0.0/Help.html
Exiting…
Please visit … clara/#parabricks for detailed documentation
Could not run fq2bam
Exiting pbrun …Hey @pavan_kumar.kotha,I would maybe try re-indexing the files with samtools index. I agree that it is strange, if it worked with bwa, it should work with fq2bam.I found the solution here: GitHub - clara-parabricks-workflows/parabricks-nextflow: Accelerated genomics workflows in NextFlowIt worked when I passed the reference and index files as tar archive.It turned out the issue was with AWS itself. The reference and index files were symlinks pointing to files at different locations. So pbrun couldn’t find the index files at the same absolute path as the reference.fa.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
38,clara-parabricks-4-1-0-1-sif-can-not-recognize-a100-cards,"hi, trying to run fq2bam with a A100 node, get this error message:[Parabricks Options Mesg]: Checking argument compatibility
[Parabricks Options Mesg]: Automatically generating ID prefix
[Parabricks Options Mesg]: Read group created for /home/crick/working/haplotypecalling/Data/sample_1.fq.gz and
/home/crick/working/haplotypecalling/Data/sample_2.fq.gz
[Parabricks Options Mesg]: @RG\tID:HK3TJBCX2.1\tLB:lib1\tPL:bar\tSM:sample\tPU:HK3TJBCX2.1
[PB Info 2023-Jun-01 00:50:13] ------------------------------------------------------------------------------
[PB Info 2023-Jun-01 00:50:13] ||                 Parabricks accelerated Genomics Pipeline                 ||
[PB Info 2023-Jun-01 00:50:13] ||                              Version 4.1.0-1                             ||
[PB Info 2023-Jun-01 00:50:13] ||                       GPU-BWA mem, Sorting Phase-I                       ||
[PB Info 2023-Jun-01 00:50:13] ------------------------------------------------------------------------------
[M::bwa_idx_load_from_disk] read 0 ALT contigs
[PB Error 2023-Jun-01 00:50:14][ParaBricks/src/pbOpts.cu:107] Bad argument value: Number of GPUs requested (4) is more than number of GPUs (0in the system., exiting.
For technical support visit Help - NVIDIA Docs
Exiting…Could not run fq2bam
Exiting pbrun …===
on the host:[crick@csctmp-xe8545-2 haplotypecalling]$ nvidia-smi
Thu Jun  1 01:29:36 2023
±--------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------±---------------------±---------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-80GB           On | 00000000:01:00.0 Off |                    0 |
| N/A   20C    P0               57W / 500W|      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
±----------------------------------------±---------------------±---------------------+
|   1  NVIDIA A100-SXM4-80GB           On | 00000000:41:00.0 Off |                    0 |
| N/A   19C    P0               56W / 500W|      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
±----------------------------------------±---------------------±---------------------+
|   2  NVIDIA A100-SXM4-80GB           On | 00000000:81:00.0 Off |                    0 |
| N/A   21C    P0               58W / 500W|      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
±----------------------------------------±---------------------±---------------------+
|   3  NVIDIA A100-SXM4-80GB           On | 00000000:C1:00.0 Off |                    0 |
| N/A   18C    P0               59W / 500W|      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
±----------------------------------------±---------------------±---------------------+±--------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
±--------------------------------------------------------------------------------------+In the container:[crick@csctmp-xe8545-2 haplotypecalling]$ singularity shell clara-parabricks_4.1.0-1.sif
Singularity> nvidia-smi
Thu Jun  1 01:30:58 2023
±--------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------±---------------------±---------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-80GB           On | 00000000:01:00.0 Off |                    0 |
| N/A   20C    P0               57W / 500W|      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
±----------------------------------------±---------------------±---------------------+
|   1  NVIDIA A100-SXM4-80GB           On | 00000000:41:00.0 Off |                    0 |
| N/A   19C    P0               56W / 500W|      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
±----------------------------------------±---------------------±---------------------+
|   2  NVIDIA A100-SXM4-80GB           On | 00000000:81:00.0 Off |                    0 |
| N/A   21C    P0               58W / 500W|      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
±----------------------------------------±---------------------±---------------------+
|   3  NVIDIA A100-SXM4-80GB           On | 00000000:C1:00.0 Off |                    0 |
| N/A   18C    P0               59W / 500W|      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
±----------------------------------------±---------------------±---------------------+±--------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
±--------------------------------------------------------------------------------------+
Singularity>Any clue for it?Thanks,Weibtw, I have --nv and --num-gpus in my code. My code works with V100 cards. Also in sigularity.config I modified “always use nv = yes” “always use rocm = yes”Hi,
I observed the exact same error on 4xA100 cards, running on Docker.It works now for me.
I re-installed cuda 12 correctly and disabled MIG service.Thanks,Parabricks does not support MIG mode.Powered by Discourse, best viewed with JavaScript enabled"
39,streaming-raw-samples-over-100gbps-connectx-6-channel-into-holoscan-operator,"Bottom Line up Front: How do we stream sensor sample data (just a stream of 16 bit integers) from our sensor/PC into a Holoscan operator on the Clara AGX with with high throughput and low latency.We are currently designing a project for the Clara AGX dev kit. Our use case seems a little different than most of the image-based processing that Holoscan seems to focus on and is more aligned with the “Radar Pipeline” example in Holohub. So our current design is as follows:
We have the SoftRoCE drivers installed on the PC, but we did hear that there was upcoming or possible UDP support in one of the recent Nvidia Holoscan Developer talks.So the question is the following, what format is recommended for the PC to use? And then what drivers/libraries do I need on the Clara AGX side to make best use of the ConnectX into a Holoscan pipeline?Below is the current design for the packet structure, but it too is fungible as we control the PC data format as well.

iqstream896×408 951 Bytes
Hi @josh.anderson2. Great question! Thanks for posting on the Holoscan forums.We spoke about this at the developer day, but we anticipate two different types of Network Operators to be released to Holohub:We’re aiming to release the Basic Network operators within the next few weeks and Advanced Network operators sometime later this year.That said, for the advanced operators, you can be sure to denote your header/packet size, meaning the first N bytes of the packet, with the advanced operator, will be sent to CPU while the remaining M bytes in the packet will go to GPU.Powered by Discourse, best viewed with JavaScript enabled"
40,error-when-run-cnvkit,"I run CNVKit but have error:When I change to Parabricks v3.0.0. It show the error here:
image756×124 21.6 KB
Hi,Sorry to hear that you are having trouble running CNVKit.
Can you please run the same command with –x3 option and send us the output?Thank you
MyriemeThanks a lot for your support. I re-run with option --x3 and get the result:

image1862×232 75.8 KB

and

image730×53 4.56 KB
I found the error is wrong permission on my server. When I change -u=1014:8002 to -u=0:0 it run again no errorPowered by Discourse, best viewed with JavaScript enabled"
41,replacing-the-trial-with-the-official-license,"Hi,I want to replace the trial license with a new node-locked license file(not a trial license).The install location is /opt/parabricks. Can I just run the following command?
$ sudo cp license.bin /opt/parabricks/I’d like to know there are any additional issues or concerns about replacing the trial with the official license? Or do I need extra steps?
Hello,Copying the node-lock license file the way you suggest will work just fine.Thanks!Thank you for your help!!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
42,download-link-for-clara-parabricks-4-x-singularity-image,"I was searching for a link to download singularity image for clara parabricks 4.x Thanks.Hey @javed.shaikh,We only provide the Docker image on NGC. But you should be able to singularity build the docker image and run it on singularity.Powered by Discourse, best viewed with JavaScript enabled"
43,bamsort-failed-with-attributeerror,"It failed when run bamsort.

image1136×257 12.4 KB

The cmd used:
pbrun bamsort --ref fix_WYDT04_HiFiasm_genome.fa --in-bam DY90_rmdup.bam --out-bam output.bam --sort-order coordinatePS:  haplotypecaller , deepvariant , rna_gatk , etc.  can be run correctly.Thanks for any advice.Hey @lkuang,I was just able to replicate your error. I am submitting a bug to our engineering team.Thank you for your patience.Hi @gburnett, Does this issue also exist in PB3.8?Hi, @gburnett What is the status of this bug.Hey @lkuang ,This is fixed in the latest version of Parabricks 3.8.1. You should be able to download it from the trial portal.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
44,too-many-chunks-allocated-memory-of-size-xyz-not-available-error-code-gxf-exceeding-preallocated-size,"If you are encountering the following errorholoscan_error1648×407 379 KBThe error likely comes from BlockMemoryPool when it doesn’t have enough blocks available. There are two things to try:
(1) If you have a shared pool across multiple operators block_pool = make_shared<BlockMemoryPool>(...);, use independent block memory pool objects for each operator.
(2) Increase the number of blocks in the memory pool by increasing the block_size and/or num_blocks parameter.Powered by Discourse, best viewed with JavaScript enabled"
45,computer-vision-solutions-page-launched,"NVIDIA Clara Community,NVIDIA launched a Computer Vision Solutions Landing and Computer Vision Solutions Frequently Asked Questions Pages. Both pages are designed to help you discover our computer vision-related work across the company (software development kits, platforms, libraries, and other resources). Feel free to reach out to me with feedback- I am working with a set of developers to update and make this page more helpful. What computer vision problems are you trying to solve? Leave us a comment!Best,
MikeB_NVPowered by Discourse, best viewed with JavaScript enabled"
46,how-to-build-a-matching-genome-index-that-works-with-the-star-version-used-in-parabricks-v3-8-0-1,"I am using the rna_fq2bam pipeline in parabricks v3.8.0.1.I have prepared a genome index beforehand with the most recent version of STAR from GitHub. The genome index works with the GitHub version of STAR. However, when I am trying to use that genome index with the parabricks rna_fq2bam pipeline, I get an error (see below).Ideally I would like to build the index with STAR from parabricks as as well, to work only with one version, but it is not clear how to do that because the pipeline isolates me from using STAR directly. Could you please advise how to build a matching genome index that works with the STAR version used in parabricks v3.8.0.1?Here is the error::[kratz@gcg01 ~]$ pbrun rna_fq2bam --in-fq SRR5927736_GSM2740270_HUVECs_Late_passage_rep2_Homo_sapiens_RNA-Seq_1.fastq.gz SRR5927736_GSM2740270_HUVECs_Late_passage_rep2_Homo_sapiens_RNA-Seq_2.fastq.gz --genome-lib-dir gbuild --output-dir sample_X/ --out-bam OUT_BAM --ref gindex/GRCh38.primary_assembly.genome.fa
Please visit NVIDIA Clara - NVIDIA Docs for detailed documentation[Parabricks Options Mesg]: Automatically generating ID prefix
[Parabricks Options Mesg]: Read group created for
/home/kratz/SRR5927736_GSM2740270_HUVECs_Late_passage_rep2_Homo_sapiens_RNA-Seq_1.fastq.gz and
/home/kratz/SRR5927736_GSM2740270_HUVECs_Late_passage_rep2_Homo_sapiens_RNA-Seq_2.fastq.gz
[Parabricks Options Mesg]: @RG\tID:SRR5927736.1.1\tLB:lib1\tPL:bar\tSM:sample\tPU:SRR5927736.1.1
[PB Info 2023-Apr-30 10:01:02] ------------------------------------------------------------------------------
[PB Info 2023-Apr-30 10:01:02] ||                 Parabricks accelerated Genomics Pipeline                 ||
[PB Info 2023-Apr-30 10:01:02] ||                              Version 3.8.0-1                             ||
[PB Info 2023-Apr-30 10:01:02] ||                                   star                                   ||
[PB Info 2023-Apr-30 10:01:02] ||                  Contact: Parabricks-Support at […]
[PB Info 2023-Apr-30 10:01:02] ------------------------------------------------------------------------------
[PB Info 2023-Apr-30 10:01:03]  … started STAR run
[PB Info 2023-Apr-30 10:01:03]  … loading genome
[PB Error 2023-Apr-30 10:01:03][src/ErrorWarning.cpp:30]
EXITING: FATAL INPUT ERROR: unrecognized parameter name “genomeType” in input “genomeParameters.txt”
SOLUTION: use correct parameter name (check the manual)… FATAL ERROR, exiting. Exit code 102, exiting.
For technical support visit […]
Exiting…Could not run rna_fq2bam
Exiting pbrun …Powered by Discourse, best viewed with JavaScript enabled"
47,launching-simultaneous-parabricks-jobs-fail,"Hi,
I have installed Parabricks version 3.0 and I am testing it with singularity. I can run a job at a time successfully, both on a single GPU and on multiple GPUs.However, I am unable to unable to run two simultaneous jobs of parabricks, getting the following error:The intent is to install Parabricks as a module in a shared space on a cluster for easy of access for multiple user.Hello Mohsin,Thanks for your interest in Parabricks and apologies for the troubles/inconvenience. There’s a limitation with Parabricks at the moment which will require you to copy the installation directory for each run and install with the --disable-fakeroot flag. That way you should be able to do multiple runs without any issue using singularity.Hope this helps.Powered by Discourse, best viewed with JavaScript enabled"
48,rivermax-testing-recommendation,"It’s recommended that the Rivermax generic_receiver be started prior to starting the generic_sender when testing Rivermax between two Clara AGX DevKits. The DevKits should be connected using a suitable QSFP interconnect between the ConnectX QSFP ports on the DevKits (note: the latest firmware ConnectX firmware update disables the ConnectX RJ45 port on the Clara AGX DevKit). The following are recommended alternative steps for running the Rivermax generic send/receive test between a sender and receiver Clara AGX:Powered by Discourse, best viewed with JavaScript enabled"
49,vqsr-applyvqsr-not-doing-any-filtering,"The vqsr module of parabricks v3.7 exits with the following message:I don’t understand what the error is. Both the recal and the tranches file were created and seems to be ok.The command line is the same as for the QuickStart example.Best regards
HaraldHey @harald.gro,Just to clarify, you used the sample data to run this? So your commands look exactly like the QuickStart example below?Sorry, no. The input data was from my own WGS dataset. The rest of the command was the same.Edit: I also tried running the compatible gatk ApplyVQSR (docker v4.2.5.0) command and that worked without any issues (after creating a required index file for the output.recal file).Hi @harald.gro ,Can you please let us know :Thankspbrun --version:  pbrun: 3.7.0-1.ampereCommand line:These are the GPUs that were provided.A small question about the GPU’s. The vqsr app doesn’t mention any options for assigning number of GPUs? Are there anything missing in the documentation?Powered by Discourse, best viewed with JavaScript enabled"
50,cudasafecall-invalid-device-symbol-error,"Hi,I am using Clara Parabricks v3.5.0 (with a trial license) on an HPC system with PBS Pro and I am getting a CudaSafeCall() invalid device symbol error. The system admin recommended I ask the developers for advice. The script and stdout are below:The PBS script:The stderr:stdout:Powered by Discourse, best viewed with JavaScript enabled"
51,how-could-i-try-clara-parabricks,"Hello.
I applied for free one-month trial parabricks license key from https://www.nvidia.com/en-us/healthcare/clara-parabricks/ page.
So I got ‘NVIDIA Clara Parabricks Pipelines - Your 1-month license key’ e-mail from parabricks-support@nvidia.com.
However, when I clicked ‘Install Parabricks Pipelines’ link in the e-mail, it just showed the error "" AccessDenied Request has expired2020-10-21T16:15:15Z2020-10-22T07:05:25Z13422F28E10EAB8Fw58ngE4rJAa66Km6pKdR8hSFB7ubnYnh8GLGxgeNqkwG7+BAQBnp9GRyOIkDtMtEoXSISTughbo="".
How could I try parabricks pipeline?Greetings,Our apologies for this mishap; it appears that there’s a new policy for the link expiration that we’ll be addressing separately - here’s an updated link pointer for you to use instead:wget -O parabricks.tar.gz “https://s3.amazonaws.com/parabricks.licenses/v311_NOV_END/parabricks.tar.gz?AWSAccessKeyId=AKIAJGDUNN2G2ZAH3Q3A&Signature=MzWbwsjmXWdzyACJT2JUH1XlMAs%3D&Expires=1608564579”-AboodThank you for your reply.JeongminHi,
Is it possible to run Parabricks toolkit on a CUDA-enabled GPU laptop? I have a high-end laptop with 6GB Nvidia 1660Ti and Intel Core i7 9-th gen, with 16GB of RAM. I want to practice GATK4 with GPU for my later project. I am a researcher at University of Delhi.
Please help.
Thank youPowered by Discourse, best viewed with JavaScript enabled"
52,capture-card-vendor-try-to-implement-v4l2-driver-with-nvidia-rdma,"Hello,I am a developer of captured card vendor.We have a problem when enabling GPUDirect RDMA on Ubuntu.Kernel rejects to load of our driver because the driver uses GPL symbols and proprietary symbols.sudo modprobe LXV4L2D_SC0710
dmesg
[ 9217.151262] LXV4L2D_SC0710: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 9217.151267] LXV4L2D_SC0710: Unknown symbol nvidia_p2p_dma_unmap_pages (err -2)
[ 9217.151330] LXV4L2D_SC0710: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 9217.151332] LXV4L2D_SC0710: Unknown symbol nvidia_p2p_get_pages (err -2)
[ 9217.151359] LXV4L2D_SC0710: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 9217.151361] LXV4L2D_SC0710: Unknown symbol nvidia_p2p_put_pages (err -2)
[ 9217.151381] LXV4L2D_SC0710: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 9217.151382] LXV4L2D_SC0710: Unknown symbol nvidia_p2p_dma_map_pages (err -2)
[ 9217.151399] LXV4L2D_SC0710: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 9217.151400] LXV4L2D_SC0710: Unknown symbol nvidia_p2p_free_dma_mapping (err -2)
[ 9217.151404] LXV4L2D_SC0710: module using GPL-only symbols uses symbols from proprietary module nvidia.
[ 9217.151405] LXV4L2D_SC0710: Unknown symbol nvidia_p2p_free_page_table (err -2)BR,
DavidHello David,
In order to help you, we will need more information on the capture card manufacturer and model.  Can you please provide more context?
thanks!Hi David, You could update your stack to R515+ through GitHub - NVIDIA/open-gpu-kernel-modules: NVIDIA Linux open GPU kernel module source. Holoscan OpenEmbedded recipes release v0.3.0 solves the issue with enabling RDMA by using OpenRM r515 drivers, please take a look at GitHub - NVIDIA/meta-tegra-clara-holoscan-mgx at v0.3.0.
Regarding If using open source Nvidia driver, some old Quadro cards don't on the supportting list. : could you tell us a little more about which hardware you’re using?Hi Ylazimy & Jinl,I successfully used R515 open-source driver on a PC and enabled Nvidia GPUDirect.
Next, I will try to use R515 open-source driver on Clara AGX Developer Kit.
Because I’m not familiar with  OpenEmbedded/Yocto, this will take more time to finish.For now, has two customers
Customer A
Platform: Clara AGX Developer Kit.
OS: Holopack 1.1, l4t 34.1.2
GPU: Quadro 6000
Using Holoscan SDK: Yes
Captured card: SC0710 HDMI or SDVoE (https://www.yuan.com.tw/products/capture/4k/sc710n1_l_hdmi2.htm)Customer B
Platform: x86 PC
OS: Ubuntu 20.04
GPU: Quadro A6000
Using Holoscan SDK: Unknow, they don’t ask.
Captured card: SC0710 HDMICould you tell us a little more about which hardware you’re using
– This is just because I have a very old Quadro which is M4000, I think most customers will use the newer Quadro.
So just forget this.Thank you for your help. If I meet a new problem, I will post a new question.BR,
davidThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
53,plotting-text-programmatically-with-holoviz-in-python,"Hi all,We implemented an endoscopy tool tracking prototype, similar to the one you provided in the holohub repository. For our application, we do inference on a YOLOv8 model, which works really well.In order to visualize the result, we’d like to plot the bounding box and the label with holoviz. In addition, we also want to plot the confidence value.However, I did not figure out how to add string to the output message in the compute function of our PostprocessingOP and emit the message, such that the HolovizOP can utilize the string.I did not find any example application in the holohub repository doing exactly that. In all cases, text is statically specified within the YAML file.Could you provide a working example in python?Thanks in advance.Hello, very cool to hear about your tool tracking app with Yolo v8! For dynamically defining text at each call to compute(), the feature is not yet available in Holoscan and coming in a future release! Please stay tuned.In the meanwhile, while we can’t dynamically display the exact value at this moment, one thing you could do is pre-define a few text strings of confidence ranges and display the corresponding range that the value falls into at each call to compute().Thanks for your rapid response and your approach. I never thought about doing it like that, but I it will surely be sufficient for our use case.Is there a general timeline on what is being implemented in upcoming releases?For this feature of dynamically defining text to render, the timeline is estimated around the end of July.Perfect. Thanks for the responseThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
54,nvidia-parabricks-what-is-the-correct-path-of-license-bin-in-a-container,"Hi, thereI referred to the article below.
https://ngc.nvidia.com/catalog/containers/hpc:parabricks
And ran the pbrun tools via docker.When I executed the fq2bam tool, the log shows that
“The license file license.bin was not found in the installation folder”I tried to solve this case in different ways, but all failed…including putting the license.bin inAnd, the following config was from the container I executed.
$docker inspect container_idTherefore, I would like to know how to configure license.bin in the container in the right way.Hello,We are sorry you were not able to use our software.To be able to use Clara Parabricks Pipelines, you need to have access to the installer.You can get a trial version using this link NVIDIA Clara Parabricks: 90 Day Free TrialRegards,
MyriemeYeah, I have got the trial parabricks license key fromI am not clear about what the differences between A and B are.
Where:Now, I know that both of them are launched by nvidia-docker.The image can be:And the key point of license.bin is(Please correct me if I’m wrong.)Powered by Discourse, best viewed with JavaScript enabled"
55,failed-to-run-pbrun-deepvariant,"Hi, thereWhen I tried to run pbrun deepvariant with the official sample dataset, I got the following error.where the error is:Invalid modelFileName /usr/local/cuda/.pb/binaries//model/60/shortread/deepvariant.eng. Exiting…But it is fine to run pbrun germline  (with HaplotypeCaller) or the single standalone tool pbrun haplotypecaller.
image730×383 39.7 KB
How can I resolve the deepvariant error?Hi tsungjung411,sorry you are having issues.
We are looking into this and will get back to you soon.Best,
MyriemeHi tsungjung411,We are sorry but Parabricks deepvariant does not support this GPU.Best,
MyriemePowered by Discourse, best viewed with JavaScript enabled"
56,parabricks-mem-from-pbrun-germline-command-hanging-and-not-finishing,"Our pbrun germline command seems to be stuck.  We run this job on our high performance compute cluster, requesting for
max, 64 cores, 4 GPUs and 175GB memory.Here’s the command in our pipeline that’s stuck.Based on my calculations, there should be (1183551056+128623608)/4=328043666 (line counts on each of the R1 fastqs) reads so I’m not sure how the progress is
counting reads either.Running htop to monitor this job, I noticed the following interesting observations:What is the PARABRICKS mem command?  What is it doing and is this expected behaviour?  How long is this command
expected to run typically?Here are more numbers for fastq sizes, CPU, memory, GPUs.220414_A00692_0278_ML220267_SAM001-FAM001_MAN-20220202_ILMNDNAPCRFREE_L000_R2.fastq.gz=2.6G
220414_A00692_0278_ML220267_SAM001-FAM001_MAN-20220202_ILMNDNAPCRFREE_L000_R1.fastq.gz=2.5G
220218_A01221_0103_ML220267_SAM001-FAM001_MAN-20220202_ILMNDNAPCRFREE_L000_R2.fastq.gz=24G
220218_A01221_0103_ML220267_SAM001-FAM001_MAN-20220202_ILMNDNAPCRFREE_L000_R1.fastq.gz=22GUpdate 2022-06-22:For some reason this exact same job started working again without us making any changes on the cluster node nor pipeline.  After finishing, here are the observations for a successful run on this same job:Update 2022-06-27:Our job failed and running again with --x1 --x3 options, here’s some additional output before it failed, (exit code was 255).  In fact, it failed several times and every time I restarted it, it progressed a bit further.  Our pipeline processes samples one at a time and by progressing further, I mean it went on to process and complete processing samples that it previously failed.  This pipeline does not re-process previously successful samples.  Each time it failed, below is the output just before the failure.Hey @tommy.li,It could be an issue with memory. Can you try running on 2 GPUs instead of 4? Sometimes 4 GPUs can use more than 175 memory. I will loop in the wider team and see if they have anything to say, too.Thanks!Hi @tommy.li,Does this issue happen only for one sample? where the input is is not that important.
But temp directory is a really important and local disk or shared storage might make a difference here.
Does it hang every time?Thank you.It could be an issue with memory. Can you try running on 2 GPUs instead of 4? Sometimes 4 GPUs can use more than 175 memory. I will loop in the wider team and see if they have anything to say, too.We ordered more memory and will be getting that soon.  This should help us isolate/rule out whether it’s a memory issue or not.Does this issue happen only for one sample? where the input is is not that important.
But temp directory is a really important and local disk or shared storage might make a difference here.
Does it hang every time?Our batch run has 42 genome samples but our pipeline processes these using parabricks one at a time.  It’s happened to more than one sample so it’s not a data issue.  In fact, I managed to complete this batch after a few restarts (i.e. first run 32 went through, restart, another 5 went through, restart, another 4 went through, restart and the final one went through.When you say “temp directory is a really important”, does parabricks use /tmp or is this configured somewhere?  Also, how much space is needed for this temp directory?Hi Tommy,We can pass the temp folder by passing as a parameter, by default we use the current directory. We need around 4X size of output BAM on the storage for temporary files.
Let us know if this helps.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
57,no-software-to-combine-gvcf-files-in-4-0,"It’s a very important step to combine multiple samples’ gvcf files together in the pipeline of joint calling.
In GATK, it could be done with CombineGVCFs.
But in Parabricks 4.0, I can’t find the corresponding software.
And in previous version, some join calling functions has been implemented, such as CombineGVCFs (but can only input 2 or 3 gvcfs) and GLNexus.So is there any future plan to add the function back and free the limitation of the number of gvcf files?Do you mean this tool genotypegvcf? @minerw1024Thanks for your reply.But it’s a pity that I cannot use that in the  joint calling for multiple gvcf files.I tried genotypegvcf in followed code:It throws no error. But after it’s finished, I used bcftools to check the result file, the outputs shows only the last gvcf file is converted:So what is the right command to use it ?I’m not sure, but you may try to use this option --in-selectvariants-dir just to specify the gvcf files directory(e.g. /workdir/gvcf/).I am also having issues with the genotypegvcf command. It seems to only be taking one g.vcf file in the input. If I specify a directory with multiple g.vcf files it throws this error → [PB Info 2023-Jan-04 08:22:55] ------------------------------------------------------------------------------
[PB Info 2023-Jan-04 08:22:55] ||                 Parabricks accelerated Genomics Pipeline                 ||
[PB Info 2023-Jan-04 08:22:55] ||                              Version 4.0.0-1                             ||
[PB Info 2023-Jan-04 08:22:55] ||                               genotypegvcf                               ||
[PB Info 2023-Jan-04 08:22:55] ------------------------------------------------------------------------------
[PB Warning 2023-Jan-04 08:22:55][src/PBLocalFile.cpp:48] Failed to open file /home/user/project/gvcf_test/0.g.vcf
[PB e[31mErrore[0m 2023-Jan-04 08:22:55][-unknown-:0] Received signal: 11Exiting…Could not run genotypegvcf
Exiting pbrun …Interestingly 0.g.vcf is not one of my gvcf files … Im not sure where that is coming from. THis is the code Im runningsingularity exec --nv -B $PWD,$genome_path,$gvcf_path /packages/7x/parabricks/4.0/parabricks.simg 
pbrun genotypegvcf --ref ${genome_path}/${genome_prefix} 
–in-selectvariants-dir ${gvcf_path}
–out-vcf ${gvcf_path}/${output_file}hello. Did you find a better way for joint calling?Yours,
changshengNo one can answer it from NVIDIA? That makes PB4.0 less useful as before.For this step we recommend using GLNexus.it also can be accelerated by GPU?
Does the genotypegvcf in PB4.0 only convert one g.vcf file?It seems GLNexus is no longer present in parabricks v4.0.Since this is the standard method to merge gVCF files from DeepVariant, can this be implemented in v4.0 (as it was in v3.8)?Does GLnexus the best way to do joint calling ? I compared the results of joint calling VCF which was produced by GATK and GLnexus , there are many sites differentPowered by Discourse, best viewed with JavaScript enabled"
58,cnvkit-with-aws-parabricks,"I subscribed AWS Parabricks at AWS marketplace.Parabricks is automatically, and already downloaded in AWS, so I cannot handle some options during installation.I want to use CNVkit but it should be installed during installation phase with --cnvkit option.How can I use CNVkit?Hi jsh9120331,Unfortunately, the AWS AMI does not support CNVKit.To be able to use CNVKit on an AWS instance you will need to apply for Parabricks License and install it yourself.Regards,
MyriemePowered by Discourse, best viewed with JavaScript enabled"
59,can-i-run-vanilla-star-accelerated-through-parabricks-3-8-0-1,"I have access to a system where the most recent version of parabricks is 3.8.0.1.
I want to use the STAR aligner to align paired-end RNA-seq reads to a genome (I have already prepared the genome index).
Can I do that accelerated through parabricks 3.8.0.1?
There seems to be some predefined pipeline “fq2bam” which uses bwa… but I just want to run STAR, not some wrapper. Is that possible? A naive “pbrun star” or pbrun STAR"" doesn’t seem to work.Powered by Discourse, best viewed with JavaScript enabled"
60,information-known-issue-sdk-v0-1-nvgpuswitch-py-script-currently-broken-due-to-the-change-in-gpg-signing-keys-for-the-cuda-repository,"This issue applies to new installs of Holoscan SDK 0.1 after April 27, or existing installs which have never been switched to dGPU mode.The issue: At this moment the nvgpuswitch.py script is broken when switching to dGPU mode. After running sudo nvgpuswitch.py install dGPU  and rebooting, the failures caused by using the old signing key will not be detected and will leave the system in an unusable state requiring a reset.We are actively working on creating a fix for this issue. In the meanwhile, the preventative workaround for this issue is to run the following command on the developer kit before running nvgpuswitch.py to switch to dGPU mode:$ sudo sed --in-place -e 's/7fa2af80\.pub/3bf863cc\.pub/' /usr/local/bin/nvgpuswitch.pyThis issue started occurring on April 27 when the CUDA repository signing key was updated (the nvgpuswitch.py script from Holoscan SDK 0.1 installs the old signing key).  Enabling dGPU prior to April 27 worked correctly, and would have installed the older signed package files in the local cache on the DevKit, so switching in and out of dGPU mode may continue to work on those systems. For those systems on which dGPU had been enabled prior to April 27, the nvgpuswitch.py script should not be patched unless enabling dGPU fails.Powered by Discourse, best viewed with JavaScript enabled"
61,permissionerror-errno-13-permission-denied-pblicense-bin,"Where is the pblicense.bin file

image1811×478 32.4 KB
Hello,when you installed the software did you use the option : --extra-tools. This option is mandatory to be able to use cnvkit.
https://docs.nvidia.com/clara/parabricks/v3.6/text/variant_callers.html#cnvkitRegards,
MyriemePowered by Discourse, best viewed with JavaScript enabled"
62,control-video-record-at-runtime,"Is there possible to use ops::VideoStreamRecorderOp operator to achieve the following goals at runtime:Hi there,If you’d like to, could you tell us a bit more about your use case and application needs? We’re always looking for feedback for improving the SDK.Thanks for your answer.
Here are our requirements:Powered by Discourse, best viewed with JavaScript enabled"
63,parabricks-gpu-request,"Hi, I am having trouble trying to request GPU access to run Parabricks.
I keep getting the error “Number of GPUs requested (8) is more than number of GPUs (1) in the system. Exiting …”
Here are the resources I tried to allocate:#!/bin/bash
#SBATCH --job-name=bwa-seq4
#SBATCH --nodes=1
#SBATCH --ntasks=1                  		
#SBATCH --cpus-per-task=16
#SBATCH --mem=64gb
#SBATCH --time=0:200:00
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=xxx@ufl.edu
#SBATCH --error=Bwa_seq4_%A.err
#SBATCH --output=Bwa_seq4_%A.out
#SBATCH --account=reed
#SBATCH --qos=reed
#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --constraint=rtx6000I have access to 274GB of MEM and 78 CPUs in our HPC. I also tried the command NVIDIA_VISIBLE_DEVICES=“1” before the pbrun command. Nothing seems to work to override the default 8 GPU request.
Does this means I need to have access to 8 GPUs to run parabricks?Hi,to be able to run with specific GPUs on a multi GPU system you need to provide options : --num-gpus NUM_GPUS and --gpu-devices GPU_DEVICES.Please refer to help or documentation for more information about those options.MyriemeThank you so much! It worked after adding --num-gpus with the pbrunPowered by Discourse, best viewed with JavaScript enabled"
64,fq2bam-error-received-signal-11,"Hi Nvidia Team,I’m receiving an error when trying to run the fq2bam tutorial sample on a Slurm cluster using a singularity container.I see the following output:Do you have any insights into what this error is?
Thanks,
ScottRunning again with the following command got me a little more information out:The output:So I finally got the tutorial to run from the singularity container, and just for anyone that might happen to make the same error as me, this was the command that finally worked.The --pwd /workdir turned out to be quite important.
It wasn’t on the docker command given in the tutorial: FQ2BAM Tutorial - NVIDIA DocsBut it was present in the tool doc: fq2bam - NVIDIA DocsNow to have some fun checking out what Parabricks can do!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
65,stitching-using-clara-xavier-sdk,"Hi, I’m stitching a group of cameras video images in one to produce a 360° video from a medical device. What will be the best/recommended way to start?
We are migrating our current stitch from OpenCV Intel Arch to NVDIA Jetson. ThanksHello, is your inquiry more about the I/O capability of multiple camera inputs to Clara AGX or about the software capabilities for stitching?Hi, A little bit more details of our case:
We are working for an advanced multi camera system with very limited space for medical applications. All camera signals are assembled in a single stream with a USB3 interface. Input is a single stream (5-6 cameras in the same stream), USB3 cypress video stream. No compression, raw 8 or 10 bits RGB. Minimum 2400x1600 30 fps. Desirable to handle 8Mp raw sensors in the near future or 4MP double exposure at 60fps
Our Output can be a compressed video on USB3 interface or directly to a standard computer PCI for additional user interface post processing. Single HDMI output.
In terms of latency requirements: Real time requirement with absolute worst case below 4 frames delay @30fps (133ms). Average should no be more than 2 frames @30fps or 66ms.
We currently have a working product with lower resolution cameras and a different architecture -Nvidia GPU cards to handle the OpenCV with CUDA - working on Windows 10 and multiple USB camera streams. We are developing an improved version and considering all image processing on the Nvidia platform. We want to migrate the image processing to the Jetson platform by Oct '21.Thanks for your reply.Sorry. Our question is more about the software capabilities for stitching, understanding that I/O capability is not a problem.Thank you for the info! Are you currently using a windows based machine with a GPU? Since our Clara AGX is ARM based, you would need to make sure that Cypress provides ARM64 drivers.For the data IO, if we’re limited to using the USB interface, Clara AGX has two USB 3.0 ports with 5 Gbps each, which would not work with the setup of 5 channels of 2400x1600 8b 4:2:2 @ 30fps which translates to 1.84 Gbps per camera, or about 11 Gbps for 5 cameras. If the IO can be made through other interfaces that is a different story.Hi,
We have an aggregate stream of 5 cameras with a total of 2.6 gpbs. The stream is on USB3, It means, we have only a single USB3 video stream no compression RAW pixel data 10 bits. What about the software stitching capabilities ? Thanks@nancyqf  Thanks for the information.  Are you currently using any stitching software?
Clara AGX SDK does not have any stitching software built in.OpenCV using opencv-python package runs on Clara AGX.  There are many stiching algorithms written with OpenCV, would that work for you?  A quick search turned up several options, such as:Image Stitching with OpenCV and Python
OpenCV - High level stitching API (Stitcher class):Hi, thanks for your help. I’m confirming then that we should use OpenCV for image stitching even on Clara or Jetson? We will start to doing this in our demo. Thanks again.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
66,using-clara-parabricks-for-genomic-analysis-if-choosing-a30-vs-a40-what-is-the-different,"We can’t find any comparison or information that compare A30 vs A40 for Using Clara Parabricks for genomic analysis, We wish to know how to choose which GPU or what is different or performance difference on these 2 product. So we can make a decision on which one to purchase.Powered by Discourse, best viewed with JavaScript enabled"
67,known-issues-with-clara-holoscan-sdk-v0-2-release,"With the Clara Holoscan SDK v0.2 release, there are a few known issues[3655489] Installing dGPU drivers can remove nvgpuswitch.py script from the executable
search path. Explicitly including /opt/nvidia/l4t-gputools/bin in the PATH
environment variable ensures this script can be found for execution.[3599303] Linux kernel is not built with security hardening flags. Future releases will
include a Linux kernel built with security hardening flags.[3633688] RDMA on the Clara Holoscan Developer Kit is not functional. This will be fixed
in future PCIe switch firmware updates. RDMA for the Clara AGX
Development Kit is functional and unaffected by this issue.[3671100] Configuring the PCIe BAR size on the Clara Holoscan Devkit to 8GB can
cause the NVIDIA device driver to fail to load. Setting to the default BAR size
of 256MB allows the device drivers to load. Future software updates may
enable larger BAR sizes.[3675821] The largest resolution of HDMI-CSI input is restricted to 2K in iGPU mode and
new UEFI firmware does not autodetect the HDMI-CSI converter. 4K may be
supported in future software updates, and manual detection of the converter
can be forced by executing the /opt/nvidia/jetson-io/jetson-io.py tool to
configure the CSI Connector / HDMI CSI input.[3690268] The 10GbE ethernet port on the Clara AGX Developer Kit is not enabled due
to firmware issue, with fix pending.Any suggestions please, thanks in advance,AbdelkrimHello, perhaps this troubleshooting suggestion could be of help?Hello,
Unfortunately not, The GLFW is initialized but not created.
""passing * -u $(id -u):$(id -g) to docker run, or running xhost +local:docker on your host "", do not work for me
I tested also the this container from Clara Holoscan Sample Applications | NVIDIA NGC, an dI got the same error.Thanks,
AbdelkrimHi,
I had to add -e XAUTHORITY in docker run,
docker run --runtime=nvidia --rm -it -v /media/m2:/m2 -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY -e XAUTHORITY -e NVIDIA_DRIVER_CAPABILITIES=all nvcr.io/nvidia/clara-holoscan/clara_holoscan_sample_runtime:v0.2.0-arm64it works now
Thanks,
AbdelkrimPowered by Discourse, best viewed with JavaScript enabled"
68,gpu-error-when-run-sudo-nvgpuswitch-py-install-dgpu-i,"[9/12] Executing.
apt update && apt install -y cuda-11-1=11.1.1-1 cuda-drivers=460.32.03-1 cuda-drivers-460=460.32.03-1 libnvidia-cfg1-460=460.73.01-0ubuntu1 libnvidia-common-460=460.73.01-0ubuntu1 libnvidia-compute-460=460.73.01-0ubuntu1 libnvidia-decode-460=460.73.01-0ubuntu1 libnvidia-encode-460=460.73.01-0ubuntu1 libnvidia-extra-460=460.73.01-0ubuntu1 libnvidia-fbc1-460=460.73.01-0ubuntu1 libnvidia-gl-460=460.73.01-0ubuntu1 libnvidia-ifr1-460=460.73.01-0ubuntu1 nvidia-compute-utils-460=460.73.01-0ubuntu1 nvidia-dkms-460=460.73.01-0ubuntu1 nvidia-driver-460=460.73.01-0ubuntu1 nvidia-kernel-common-460=460.73.01-0ubuntu1 nvidia-kernel-source-460=460.73.01-0ubuntu1 nvidia-utils-460=460.73.01-0ubuntu1 xserver-xorg-video-nvidia-460=460.73.01-0ubuntu1 nvidia-container-runtime=3.4.2-1 nvidia-container-toolkit=1.5.1-1 nvidia-l4t-camera=32.5.1-20210219084708 nvidia-l4t-configs=32.5.1-20210219084708 nvidia-l4t-core=32.5.1-20210219084708 nvidia-l4t-firmware=32.5.1-20210219084708 nvidia-l4t-gstreamer=32.5.1-20210219084708 nvidia-l4t-init=32.5.1-20210219084708 nvidia-l4t-initrd=32.5.1-20210219084708 nvidia-l4t-multimedia=32.5.1-20210219084708 nvidia-l4t-multimedia-utils=32.5.1-20210219084708 nvidia-l4t-tools=32.5.1-20210219084708 nvidia-l4t-xusb-firmware=32.5.1-20210219084708 nvidia-l4t-kernel=4.9.201-tegra-32.5.1-20210219084708 nvidia-l4t-kernel-dtbs=4.9.201-tegra-32.5.1-20210219084708 nvidia-l4t-kernel-headers=4.9.201-tegra-32.5.1-20210219084708 tensorrt=7.2.2.0-1+cuda11.1 python-libnvinfer=7.2.2-1+cuda11.1 python-libnvinfer-dev=7.2.2-1+cuda11.1 python3-libnvinfer=7.2.2-1+cuda11.1 python3-libnvinfer-dev=7.2.2-1+cuda11.1 graphsurgeon-tf=7.2.2-1+cuda11.1 onnx-graphsurgeon=7.2.2-1+cuda11.1 uff-converter-tf=7.2.2-1+cuda11.1Get:2 Index of /compute/cuda/repos/ubuntu1804/sbsa  InRelease [1,575 B]
Hit:3 https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/arm64  InRelease
Hit:4 https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/arm64  InRelease
Hit:1 https://repo.download.nvidia.com/jetson/jetson-clara45/dgpu-rm r32.5 InRelease
Err:2 Index of /compute/cuda/repos/ubuntu1804/sbsa  InRelease
The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC
Hit:5 Index of /ubuntu-ports bionic InRelease
Hit:6 Index of /ubuntu-ports bionic-updates InRelease
Hit:7 Index of /ubuntu-ports bionic-backports InRelease
Hit:8 Index of /ubuntu-ports bionic-security InRelease
Reading package lists… Done
W: GPG error: Index of /compute/cuda/repos/ubuntu1804/sbsa  InRelease: The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC
E: The repository ‘Index of /compute/cuda/repos/ubuntu1804/sbsa  InRelease’ is not signed.
N: Updating from such a repository can’t be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
ERROR: Install dGPU drivers failed!Tried to update GPU repo key by following:Updating the CUDA Linux GPG Repository Key | NVIDIA Technical BlogThen got the following error:
[9/12] Executing.
apt update && apt install -y cuda-11-1=11.1.1-1 cuda-drivers=460.32.03-1 cuda-drivers-460=460.32.03-1 libnvidia-cfg1-460=460.73.01-0ubuntu1 libnvidia-common-460=460.73.01-0ubuntu1 libnvidia-compute-460=460.73.01-0ubuntu1 libnvidia-decode-460=460.73.01-0ubuntu1 libnvidia-encode-460=460.73.01-0ubuntu1 libnvidia-extra-460=460.73.01-0ubuntu1 libnvidia-fbc1-460=460.73.01-0ubuntu1 libnvidia-gl-460=460.73.01-0ubuntu1 libnvidia-ifr1-460=460.73.01-0ubuntu1 nvidia-compute-utils-460=460.73.01-0ubuntu1 nvidia-dkms-460=460.73.01-0ubuntu1 nvidia-driver-460=460.73.01-0ubuntu1 nvidia-kernel-common-460=460.73.01-0ubuntu1 nvidia-kernel-source-460=460.73.01-0ubuntu1 nvidia-utils-460=460.73.01-0ubuntu1 xserver-xorg-video-nvidia-460=460.73.01-0ubuntu1 nvidia-container-runtime=3.4.2-1 nvidia-container-toolkit=1.5.1-1 nvidia-l4t-camera=32.5.1-20210219084708 nvidia-l4t-configs=32.5.1-20210219084708 nvidia-l4t-core=32.5.1-20210219084708 nvidia-l4t-firmware=32.5.1-20210219084708 nvidia-l4t-gstreamer=32.5.1-20210219084708 nvidia-l4t-init=32.5.1-20210219084708 nvidia-l4t-initrd=32.5.1-20210219084708 nvidia-l4t-multimedia=32.5.1-20210219084708 nvidia-l4t-multimedia-utils=32.5.1-20210219084708 nvidia-l4t-tools=32.5.1-20210219084708 nvidia-l4t-xusb-firmware=32.5.1-20210219084708 nvidia-l4t-kernel=4.9.201-tegra-32.5.1-20210219084708 nvidia-l4t-kernel-dtbs=4.9.201-tegra-32.5.1-20210219084708 nvidia-l4t-kernel-headers=4.9.201-tegra-32.5.1-20210219084708 tensorrt=7.2.2.0-1+cuda11.1 python-libnvinfer=7.2.2-1+cuda11.1 python-libnvinfer-dev=7.2.2-1+cuda11.1 python3-libnvinfer=7.2.2-1+cuda11.1 python3-libnvinfer-dev=7.2.2-1+cuda11.1 graphsurgeon-tf=7.2.2-1+cuda11.1 onnx-graphsurgeon=7.2.2-1+cuda11.1 uff-converter-tf=7.2.2-1+cuda11.1E: Conflicting values set for option Signed-By regarding source https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/sbsa/ /: /usr/share/keyrings/cuda-archive-keyring.gpg !=
E: The list of sources could not be read.
ERROR: Install dGPU drivers failed!Hello, I have not seen this error before, but did you try the suggestion under Duplicate .list entries in the developer blog?Hi, I have just solved the conflicting issue. Here is the new error:[9/12] Executing.
apt update && apt install -y cuda-11-1=11.1.1-1 cuda-drivers=460.32.03-1 cuda-drivers-460=460.32.03-1 libnvidia-cfg1-460=460.73.01-0ubuntu1 libnvidia-common-460=460.73.01-0ubuntu1 libnvidia-compute-460=460.73.01-0ubuntu1 libnvidia-decode-460=460.73.01-0ubuntu1 libnvidia-encode-460=460.73.01-0ubuntu1 libnvidia-extra-460=460.73.01-0ubuntu1 libnvidia-fbc1-460=460.73.01-0ubuntu1 libnvidia-gl-460=460.73.01-0ubuntu1 libnvidia-ifr1-460=460.73.01-0ubuntu1 nvidia-compute-utils-460=460.73.01-0ubuntu1 nvidia-dkms-460=460.73.01-0ubuntu1 nvidia-driver-460=460.73.01-0ubuntu1 nvidia-kernel-common-460=460.73.01-0ubuntu1 nvidia-kernel-source-460=460.73.01-0ubuntu1 nvidia-utils-460=460.73.01-0ubuntu1 xserver-xorg-video-nvidia-460=460.73.01-0ubuntu1 nvidia-container-runtime=3.4.2-1 nvidia-container-toolkit=1.5.1-1 nvidia-l4t-camera=32.5.1-20210219084708 nvidia-l4t-configs=32.5.1-20210219084708 nvidia-l4t-core=32.5.1-20210219084708 nvidia-l4t-firmware=32.5.1-20210219084708 nvidia-l4t-gstreamer=32.5.1-20210219084708 nvidia-l4t-init=32.5.1-20210219084708 nvidia-l4t-initrd=32.5.1-20210219084708 nvidia-l4t-multimedia=32.5.1-20210219084708 nvidia-l4t-multimedia-utils=32.5.1-20210219084708 nvidia-l4t-tools=32.5.1-20210219084708 nvidia-l4t-xusb-firmware=32.5.1-20210219084708 nvidia-l4t-kernel=4.9.201-tegra-32.5.1-20210219084708 nvidia-l4t-kernel-dtbs=4.9.201-tegra-32.5.1-20210219084708 nvidia-l4t-kernel-headers=4.9.201-tegra-32.5.1-20210219084708 tensorrt=7.2.2.0-1+cuda11.1 python-libnvinfer=7.2.2-1+cuda11.1 python-libnvinfer-dev=7.2.2-1+cuda11.1 python3-libnvinfer=7.2.2-1+cuda11.1 python3-libnvinfer-dev=7.2.2-1+cuda11.1 graphsurgeon-tf=7.2.2-1+cuda11.1 onnx-graphsurgeon=7.2.2-1+cuda11.1 uff-converter-tf=7.2.2-1+cuda11.1Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/sbsa  InRelease
Hit:1 https://repo.download.nvidia.com/jetson/jetson-clara45/jetson-rm/t194 r32.5 InRelease
Hit:2 https://repo.download.nvidia.com/jetson/jetson-clara45/jetson-rm/common r32.5 InRelease
Hit:4 Index of /ubuntu-ports bionic InRelease
Get:5 Index of /ubuntu-ports bionic-updates InRelease [88.7 kB]
Get:6 Index of /ubuntu-ports bionic-backports InRelease [74.6 kB]
Get:7 Index of /ubuntu-ports bionic-security InRelease [88.7 kB]
Get:8 Index of /ubuntu-ports bionic-updates/main arm64 DEP-11 Metadata [291 kB]
Get:9 Index of /ubuntu-ports bionic-updates/universe arm64 DEP-11 Metadata [295 kB]
Get:10 Index of /ubuntu-ports bionic-backports/universe arm64 DEP-11 Metadata [9,276 B]
Get:11 Index of /ubuntu-ports bionic-security/main arm64 DEP-11 Metadata [49.2 kB]
Get:12 Index of /ubuntu-ports bionic-security/universe arm64 DEP-11 Metadata [54.8 kB]
Fetched 952 kB in 2s (456 kB/s)
Reading package lists… Done
Building dependency tree
Reading state information… Done
9 packages can be upgraded. Run ‘apt list --upgradable’ to see them.
Reading package lists… Done
Building dependency tree
Reading state information… Done
E: Version ‘3.4.2-1’ for ‘nvidia-container-runtime’ was not found
E: Version ‘1.5.1-1’ for ‘nvidia-container-toolkit’ was not found
E: Version ‘7.2.2.0-1+cuda11.1’ for ‘tensorrt’ was not found
E: Version ‘7.2.2-1+cuda11.1’ for ‘python-libnvinfer’ was not found
E: Version ‘7.2.2-1+cuda11.1’ for ‘python-libnvinfer-dev’ was not found
E: Version ‘7.2.2-1+cuda11.1’ for ‘python3-libnvinfer’ was not found
E: Version ‘7.2.2-1+cuda11.1’ for ‘python3-libnvinfer-dev’ was not found
E: Version ‘7.2.2-1+cuda11.1’ for ‘graphsurgeon-tf’ was not found
E: Unable to locate package onnx-graphsurgeon
E: Version ‘7.2.2-1+cuda11.1’ for ‘uff-converter-tf’ was not found
ERROR: Install dGPU drivers failed!It has been solved. Thank you for your attention!Powered by Discourse, best viewed with JavaScript enabled"
69,evaluation-license-trouble,"Hi,im keen to test the Parabricks software package but I am unable to download the license/installer .tar.gz file which apparently should have been available with the License E-Mail I got.When clicking on the link “Install NVIDIA Parabricks” (which points to go.nvidianews.com, which points to some AWS Bucket), I get an access denied error. Am I missing some step for retrieval of this data, or is there some other requirement I should be aware of?I had the same experience. The download link looks like it contains an “expires” timestamp that is in the past.Powered by Discourse, best viewed with JavaScript enabled"
70,fq2bam-unexpected-issue-1-return-code-2,"Hi, I was trying to use the command fq2bam, follows the command line:singularity exec --nv --bind /archive/s1/data_energon/:/media/,/hpcshare/genomics/references/gatk_bundle/:/mnt ./pipelines/parabricks/parabricks-release-v3.2.0.sif 
/parabricks/run_pipeline.py fq2bam --x1 --x3
–ref /mnt/reference/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta 
–in-fq /media/181119_A00558_0012_BHF5V5DSXX/10643_06_KID_S6_R1_001.fastq.gz /media/181119_A00558_0012_BHF5V5DSXX/10643_06_KID_S6_R2_001.fastq.gz 
–knownSites /mnt/resources/resources_broad_hg38_v0_hapmap_3.3.hg38.vcf.gz 
–out-bam /homenfs/smarangoni/out_gpu.bam 
–out-recal-file recal_gpu.txt 
–license-file ./pipelines/parabricks/license.binAnd the programm returned:Please visit https://docs.nvidia.com/clara/#parabricks for detailed documentation[Parabricks Options Mesg]: Checking argument compatibility
[Parabricks Options Mesg]: Automatically generating ID prefix
[Parabricks Options Mesg]: Read group created for /media/181119_A00558_0012_BHF5V5DSXX/10643_06_KID_S6_R1_001.fastq.gz
and /media/181119_A00558_0012_BHF5V5DSXX/10643_06_KID_S6_R2_001.fastq.gz
[Parabricks Options Mesg]: @RG\tID:HF5V5DSXX.1\tLB:lib1\tPL:bar\tSM:sample\tPU:HF5V5DSXX.1
Unexpected Issue #1, Return code: 2
Please contact Parabricks-Support@nvidia.com for any questions
There is a forum for Q&A as well at Clara Parabricks - NVIDIA Developer Forums
Exiting…So I ask, what type of error is “Unexpected Issue #1, Return code: 2”Thanks in adviceHi,
I am having the same problem and cannot understand where the error is coming from.
The only difference is that I am running a full pipeline using NVIDIA test samples as input:Did you receive any sort of support on this?
Thanks!Hi, in the end I resolved my problem, using the command “pbrun” instead using the singularity container. Have you tried using the “–x1” and “–x3” in order to increase the verbosity? Are the error text the same I posted ? In case it is different, can you post the text of the error so I can understand ?I just ran pbrun fq2bam using –x1 --x3. The run uses 2 GPUs by default. I include the last part of the output as the first part finished without any error:Maybe it is a problem when writing the bam file? I still have no clue…I didn’t find the source of the problem but reinstalling parabricks solved it.Powered by Discourse, best viewed with JavaScript enabled"
71,integrate-other-applications-to-holoviz,"Hi,
I’ve developed two applications with QT/QML and GLFW. I want these two applications as overlay on endoscopy video.  Is there possible to display QT/QML and GLFW applications on Holoviz which has displayed endoscopy video streaming?
Thanks!This may be difficult. Assuming the QT and GLFW apps are running ins separate processes, both apps would need to run in offscreen mode, read back the frame buffer, pass it to the Holoscan process to pass it into the Holoviz operator. Also when the apps have an UI the mouse position, clicks and keyboard needs to be passed to the apps somehow. This is not possible today.
You could try to use the desktop compositor in some way, keeping the Holoviz window in the background and the other apps on top, probably borderless.
But overall we’d say it’s not possible to overlay apps on Holoviz today.Thanks for the clarification.
If the QT/QML and GLFW applications don’t need mouse and keyboard for user interaction, is there possible for Holoscan reading back the frame buffer and showing the GUI?
And are there any recommendations for showing GUI (ImGUI?) and 3D model using Holoscan SDK?Some initial answers to the questions: with our next SDK release it’s possible to freely place input images within the Holoviz window. If the QT/QML and GLFW frame buffers can be read back, then Holoviz can composite them to one window. ImGui can be used with the Holoviz C++ API, but it not supported with the Holoviz operator.Good to hear that. And how can QT/QML and GLFW frame buffers be read back? Will it be implemented in the future release?Not sure for QT/QML, but for GLFW it depends on the API used, when using OpenGL glReadPixels  can be used. That image then needs to be converted to a gxf VideoBuffer or Tensor and passed to Holoviz. There will be a Python Holoviz example in the next SDK release (coming soon) showing how to use layer views. (C++ is similar)Looking forward to the new feature.Powered by Discourse, best viewed with JavaScript enabled"
72,parabrick-rna-fq2bam-are-running-for-several-hours,"Hi,
I run pbrun rna_fq2bam using Parabricks version 3.6.1-1ampere with the setting of 2 GPU and 128 Gb
It takes more than 5 hr (from my sbacth setting), and have not finished.It repeatedly reported the same thing after just 30 sec as attached below.

Parabrick terminal2022-02-031270×601 82 KB
Here is the last part in Log.outGenome: size given as a parameter = 3352331456
SA: size given as a parameter = 25751061669
SAindex: size given as a parameter = 1
Read from SAindex: pGe.gSAindexNbases=14  nSAi=357913940
nGenome=3352331456;  nSAbyte=25751061669
GstrandBit=32   SA number of indices=6242681616
Shared memory is not used for genomes. Allocated a private copy of the genome.
Genome file size: 3352331456 bytes; state: good=1 eof=0 fail=0 bad=0
Loading Genome … done! state: good=1 eof=0 fail=0 bad=0; loaded 3352331456 bytes
SA file size: 25751061669 bytes; state: good=1 eof=0 fail=0 bad=0
Loading SA … done! state: good=0 eof=1 fail=1 bad=0; loaded 4574441472 bytes
Loading SAindex … done: 1565873619 bytes
Finished loading the genome: Wed Feb  2 17:42:30 2022Processing splice junctions database sjdbN=360128,   pGe.sjdbOverhang=100
alignIntronMax=alignMatesGapMax=0, the max intron size will be approximately determined by (2^winBinNbits)*winAnchorDistNbins=589824I ran STAR for CPU mapping, and it took just 3 min.
Could you help why parabrick take a long time to do this?Thank you in advancePowered by Discourse, best viewed with JavaScript enabled"
73,cant-run-parabricks-cudamemgetinfo-returned-802-system-not-yet-initialized,"I’m compile Parabricks with singularity  and try to run and found
cudaMemGetInfo returned 802
 → system not yet initialized[test@c0 ~]$ pbrun fq2bam --ref parabricks_sample/Ref/Homo_sapiens_assembly38.fasta --in-fq parabricks_sample/Data/sample_1.fq.gz parabricks_sample/Data/sample_2.fq.gz --out-bam output.bam  --gpu-devices 0,1,2,3,4,5,6,7 --num-gpus 1
Please visit https://docs.nvidia.com/clara/#parabricks for detailed documentationPowered by Discourse, best viewed with JavaScript enabled"
74,problem-using-interval-file-and-parabricks-deepvariant-in-4-10-and-4-0-0,"I am getting an error, very long run times, and stalling on the same chromsomal region when using the --interval-file option with parabricks deepvariant in version 4.1.0 and 4.0.0. When running on whole exome bam files using 4 GPUs the run usually takes less than 5 minutes when no bed file is specified. However, specifiying a bed file with --interval-file option slows it down and leads to an error:I have attached log files for 4.0.0 and 4.1.0 runs showing the error.Below are the details on the GPUs that we are using4.0.0.log (3.1 KB)
4.1.0.log (480.4 KB)Hello @padraic.corcoran,Do you have the commands you used to run this? We can have the engineering team take a look.Thank you!Below is the command:I should also add that this was run as part of a snakemake pipeline using singularity. Notably, the error was not detected as failed job in the pipeline and I only became aware of it when checking the parabricks deepvariant log files.Powered by Discourse, best viewed with JavaScript enabled"
75,nvidia-parabricks-got-an-error-on-running-marking-duplicates-with-the-official-parabricks-samples,"Hi there,I got an error on running Marking Duplicates.Use the official data from parabricks_sample.Same as official instructions.Here is the GPU summary of the Driver and CUDA versions.

image720×340 35.6 KB
How can I fix the issue?Hello,we are sorry that you are facing this error.
Can you please try to run the same command adding --x3 to the command line and share the full log?Also can you please give what’s your working directory?Thank you
MyriemeHere is the full log.Same as the official instruction, and plus --x3.The differences between the logs without and with --x3 are:Hello Nvidia team,I found that it also occurs in parabricks/release:v3.0.0.
Originally, It works in nvcr.io/hpc/parabricks:v2.5.0
image1080×86 26.6 KB
Do you have any plan or schedule to fix the error or is there any guidelines to avoid the error?Best regards,
TJ TsaiHello,We are sorry but your system does not meet the requirement for Clara Parabricks Pipelines to run.
For two GPUs you need at least 24 CPU threads. This error is happening because you only have 8 CPU threads.Regards,
MyriemePowered by Discourse, best viewed with JavaScript enabled"
76,monai-framework-is-very-slow-while-computing-stain-normalisation,"Hello,I am new to MONAI framework and I am working on stain normalization approach to generate images similar to target image using MONAI framework.
I have checked the blog for the implementation of Stain normalization and I found out that the implementation is using numpy rather than cupy or something similar.Here is the implementation, I found from MONAI blog :from future import annotationsimport numpy as npfrom monai.transforms.transform import Transform[docs]
class ExtractHEStains(Transform):
“”""Class to extract a target stain from an image, using stain deconvolution (see Note).[docs]
class NormalizeHEStains(Transform):
“”""Class to normalize patches/images to a reference or target image stain (see Note).For me when I am running on a folder which contains 5400 webp tiles it is taking around half an hour. I have more than 2000 such folders and it takes at least a month to complete this procedure.Could u please let me know if there is anything I could do to speed up the process.Powered by Discourse, best viewed with JavaScript enabled"
77,jetson-aja-kona-hdmi-when-i-run-aja-kona-hdmi-test-the-screen-can-only-show-a-black-window,"**• Hardware Platform (Jetson / GPU)**Jetson
• DeepStream Version 6.0
**• JetPack Version (valid for Jetson only)**4.5
**• Issue Type: questionsI want to use Jetson to run AJA KONA HDMI.
when I run the test samplelucas@lucas-desktop:~/aja/ntv2sdklinux_16.1.0.3$ gst-launch-1.0 ajavideosrc mode=4Kp30-rgba input-mode=hdmi nvmm=true ! nv3dsink
Setting pipeline to PAUSED …
Pipeline is live and does not need PREROLL …
Setting pipeline to PLAYING …
New clock: GstSystemClock
Redistribute latency…
^Chandling interrupt.
Interrupt: Stopping pipeline …
Execution ended after 0:00:08.679657011
Setting pipeline to PAUSED …
Setting pipeline to READY …
Setting pipeline to NULL …
Freeing pipeline …I used my laptop as the source of HDMI input.
Clara Holoscan SDK User Guide indicated that RGBA mode must be used with the ajavideosrc plugin when it is used whth NVIDIA plugins.so I tried other sink types
eg.
lucas@lucas-desktop:~/aja/ntv2sdklinux_16.1.0.3$ gst-launch-1.0 ajavideosrc mode=4Kp30 input-mode=hdmi nvmm=true ! autovideosink
Setting pipeline to PAUSED …
Pipeline is live and does not need PREROLL …
Setting pipeline to PLAYING …
New clock: GstSystemClock
nvbufsurface : Unsupported output format (10)
Error(-1) in buffer allocation** (gst-launch-1.0:19153): CRITICAL **: 19:47:27.079: gst_nvds_buffer_pool_alloc_buffer: assertion ‘mem’ failed
Caught SIGSEGV
#0 0x0000007f9ac2fe28 in __GI___poll (fds=0x5581af0ab0, nfds=548058419768, timeout=) at …/sysdeps/unix/sysv/linux/poll.c:41
#1 0x0000007f9ad3ce08 in () at /usr/lib/aarch64-linux-gnu/libglib-2.0.so.0
#2 0x000000558162a460 in ()
Spinning. Please run ‘gdb gst-launch-1.0 19153’ to continue debugging, Ctrl-C to quit, or Ctrl-\ to dump core.
^\Quit (core dumped)all other sink types shows the same context as above.If I tried the command without “nvmm=true”
eg.
gst-launch-1.0 ajavideosrc mode=1080p60-rgba input-mode=hdmi ! nv3dsinkIt can show a right image, but the colors are almost wrong.How can I solve the problem .THX!Hello,
The strange colorspace when using a laptop output had been observed before, do you happen to have a hdmi camera? It seems currently your AJA RDMA functionality isn’t working - have you been able to verify that your AJA drivers are built without problem with command ${AJA_BASE}/${NTV2_SDK}/bin/rdmawhacker?Just to confirm the platform setup, are you using a Clara AGX Devkit with a Kona HDMI card plugged in to the outer PCIe slot?Hi,
I am sure that the Kona HDMI card plugged successfully, because if I tried “gst-launch-1.0 ajavideosrc mode=1080p60-rgba input-mode=hdmi ! nv3dsink”, it can show a wrong color image.at my Jetson platform, the command ${AJA_BASE}/${NTV2_SDK}/bin/rdmawhacker will show a error note :
“## ERROR: GPU buffer lock failed”I can not resolve this, so I ignored it.Reason of the error was confused, I just run the install commands as the guide indicated:export AJA_BASE=${HOME}/aja
mkdir ${AJA_BASE}
mv ~/Downloads/ntv2sdklinux_16.1.0.3.zip ${AJA_BASE}
mv ~/Downloads/ntv2sdklinux_16.1.0.3_arm.zip ${AJA_BASE}cd ${AJA_BASE}
export NTV2_SDK=ntv2sdklinux_16.1.0.3
unzip ${NTV2_SDK}.zip
unzip ${NTV2_SDK}_arm.zipcp ${AJA_BASE}/${NTV2_SDK}_arm/lib/* ${AJA_BASE}/${NTV2_SDK}/lib/cd ${AJA_BASE}/${NTV2_SDK}/ajadriver/linux
export AJA_RDMA=1
makesudo ${AJA_BASE}/${NTV2_SDK}/bin/load_ajantv2And now, it can show the right things as:“loaded ajantv2 driver module”
“created node /dev/ajantv20”I continue installing.
cd ${AJA_BASE}/${NTV2_SDK}/ajaapps/crossplatform/rdmawhacker
make
${AJA_BASE}/${NTV2_SDK}/bin/rdmawhackerthe error will show as below:
“## ERROR: GPU buffer lock failed”
16bb47a7da0435bc180bfc0b8ec96741704×1279 156 KB


2jpg1704×1279 201 KB
Hello, I see that you’re using a Jetson AGX Xavier. Since the documentation Chapter 9 is intended for Clara AGX devkit which has an additional discrete GPU, the steps for Clara AGX GPU RDMA would not work on Jetson AGX Xavier with the integrated GPU, since there is no discrete GPU on Jetson AGX Xavier.Please see https://resources.nvidia.com/en-us-enabling-smart-hospitals-ai-ep/nvidia-clara-agx-dev?lx=KWlJE5 for the components in a Clara AGX devkit. The Jetson AGX Xavier is one component within the devkit.Hi,
I tried export AJA_IGPU=1 as your reply.
But it still prints the error as before.The rdmawhacker compiling is not related to the following content.
And even it’s related,  I can be sure of every path I exported is correct (I input all paths manually.)Is there any other possible problem?look forward to your reply.By the way,  the images I uploaded should be checked from bottom to top.Thx!

41704×1279 312 KB


51704×1279 318 KB


61704×1279 313 KB
Hello, just to check, did you rebuild the kernel module with both AJA_RDMA=1 and AJA_IGPU=1 enabled? This should not be at the step of building rdmawhacker, but should be at the step of building the kernel module:$ cd ${AJA_BASE}/${NTV2_SDK}/ajadriver/linux
$ export AJA_RDMA=1
$ export AJA_IGPU=1
$ makeOur colleague was able to set up with Jetson + AJA card with the above step. However, please keep in mind that it is a untested configuration that our SDK is not officially supporting.
May I ask the application and use case you are developing with the Jetson + AJA card configuration?Hello
I just tried, the rdmawhacker could be executed correctly.
But there are still the same questions as before:
If I run “gst-launc-1.0 ajavideosrc mode=1080p30-rgba input-mode=hdmi nvmm=true ! nv3dsink”, it will show a black image.
If I run “gst-launc-1.0 ajavideosrc mode=1080p30-rgba input-mode=hdmi ! nv3dsink”, it will show a wrong color image, and the color slants green.

11704×1279 293 KB


21704×1279 182 KB


31704×1279 202 KB


41704×1279 266 KB
This is expected since nvmm=true is specifying the RDMA to discrete GPU (on the Clara AGX devkit) to be enabled. Since there is no discrete GPU in your setup, only the integrate GPU onboard Jetson, that param cannot be set.RE colorspace: could you try with another (HDMI) camera?I have no HDMI camera to try, but I tried lots of HDMI source, such as NUC.
I tried the commands on my x86 platform(ubuntu 18), it still has the same problems.
(gst-launch-1.0 ajavideosrc mode=1080p30-rgba input-mode=hdmi nvmm=true ! videoconvert ! autovideosink
or
gst-launch-1.0 ajavideosrc mode=1080p30 input-mode=hdmi ! videoconvert ! autovideosink)the x86 platform has no problem of rdmawhacker.There is a discrete GPU at x86 platform. This should be confirmed.configure of the x86 platform:
RTX 6000
Ubuntu 18
CUDA 11.4
driver 470
deepstream 6.0
tensorrt 8.0We have not tested the AJA Gstreamer plugin on x86 platforms. Please note that our platform Clara AGX devkit is ARM based, therefore some instructions are ARM-specific, such as the zip file downloaded in this section AJA Video Systems — Clara Holoscan 0.2.0 documentation.I got it.
That’s to say, the platform Jetson could not be used on this function.
And it’s not confirmed whether x86 platform would work.
Is that so ?As stated in documentation, the Clara Holoscan SDK is meant to be run on the Clara AGX Developer Kit and incoming Clara Holoscan Developer Kit. Both the Clara AGX devkit and Clara Holoscan devkit are ARM based, not x86 based, and build on top of Jetson. The Jetson AGX Xavier is only one hardware component in the Clara AGX Developer kit, but the Clara AGX Developer Kit has more components than Jetson such as the smart NIC and discrete GPU.The Clara Holoscan SDK is not meant to be installed and flashed onto x86 systems. While there are some apps that can be run on a x86 host where specified in the documentation, the AJA build instructions were not meant for x86.https://docs.nvidia.com/clara-holoscan/sdk-user-guide/introduction.htmlOK，I got it.
Thank you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
78,cnvkit-not-found-in-parabrick,"While running pbrun for CNVkit, I am getting an error as dependencies not detected. I didn’t found any specific instruction regarding installation of pbrun for cnvkit so I didn’t know about cnvkit dependencies. Can you suggest me what extra dependencies I need to install for CNVkit with pbrun. I am using Parabrick version 3.2.0. Here is the screenshot of the error message for your reference. Thanks.
Screenshot from 2021-01-29 17-05-231183×267 44.1 KB
Hi vivekr1To be able to use cnvkit, you need to use the option --cnvkit during the installation.Please find all options you can use during the installation process:
https://docs.nvidia.com/clara/parabricks/v3.2/text/local_installation.html#installation-optionsBest
MyriemeOh, I missed that. Thanks for reminding.Powered by Discourse, best viewed with JavaScript enabled"
79,somatic-pipeline-does-not-use-normal-fastq-at-all,"I am trying to run tumor-normal analysis for somatic pipeline in Parabricks
However, parabricks somatic pipeline does not align normal fastq and I cannot find any normal.bam file in the output folder that I specified…
Moreover the generated VCF file has the size too big to be the one that generated by tumor-normal analysis
Please help me with this issueMy command line was: ""sudo pbrun somatic --ref inputs/refs/reference.fasta --in-se-tumor-fq inputs/tumor/VT-tumor-gz_S1_L001_R1_001.fastq.gz --out-vcf outputs/output.vcf --out-tumor-bam outputs/tumor.bam --in-se-normal-fq inputs/normal/VT-normal-gz_S1_L001_R1_001.fastq.gz --out-normal-bam outputs/normal.bam ""Thank youPowered by Discourse, best viewed with JavaScript enabled"
80,using-nvidia-docker-docker-19-03-5-gpu-detection-issue,"When using this combination - results in error in the installer.py ““docker does not have nvidia runtime. Please add nvidia runtime to docker or install nvidia-docker. Exiting…””Looking at the installer.py it appears it is not setup correctly,  I have updated this file and posted at - parabricks-changes/installer.py at master · prabindh/parabricks-changes · GitHubDiff can be seen at - changes for newer nvidia-docker runtime that does not require nvidia-… · prabindh/parabricks-changes@b39f61b · GitHubCan someone confirm this issue / change ?Thank you for your interest in Parabricks.native docker gpu support was added from docker 19.03. Parabricks will be supporting native integration of gpu during installation in the upcoming version of Parabricks due to release soon. For the time being, in order to overcome this, you may use nvidia-docker2 and docker 18.06.Powered by Discourse, best viewed with JavaScript enabled"
81,parabrick-3-7-quality-control,"The example command in the document of PB3.7 cannot be excuted normally.  I change the --mapq to --map-quality as shown in the help.There are many warnings as:Thank you for this. I will file a bug.Powered by Discourse, best viewed with JavaScript enabled"
82,holoscan-sdk-question,"I’m looking to capture video frames from the VideoStreamReplayerOp operator and manipulate the frames using opencv. My question is: Is there a Holoscan SDK method or gxf::Entity that allows me to capture the video frame data so that it can be changed by opencv?Hi Tim, it sounds like you might want to create a native C++/Python operator in Holoscan that contains your custom OpenCV operations, and connect it to the output of VideoStreamReplayer. You can do this via tensor interop to and from Holoscan tensors. For Python, the documentation is here Creating Operators - NVIDIA Docs; for C++ Creating Operators - NVIDIA Docs. Some examples are on GitHub: holoscan-sdk/examples/tensor_interop at main · nvidia-holoscan/holoscan-sdk · GitHub, holoscan-sdk/examples/native_operator at main · nvidia-holoscan/holoscan-sdk · GitHubWhich language are you creating your application in?Did you mean the INPUT of the VidoStreamReplayer?I’m looking to capture video frames from the VideoStreamReplayerOp operatorCould you elaborate on this? From your first post I’m understanding it as, you are replaying a saved video, and want to use OpenCV to modify the video frames.The VideoStreamReplayerOp reads a file and plays that file, it is usually the first operator in an app if you’re replaying a saved file (vs streaming from a live source), so I did mean the output of this operator in the message above.Got it. Thx!Powered by Discourse, best viewed with JavaScript enabled"
83,can-pbrun-germline-take-stdin,"Hi,
according to GATK best practices unmapped reads should be stored as .ubam files.
pbrun or bwa do not take .bam as input.
Currently we do sth like this:INTERLEAVE=true generates an interleaved .fastq, which bwa accepts via -YI can not do the same in pbrun:Output:
[Parabricks Options Error]: Input file /dev/stdin not found. Exiting…Anyone tried something similar yet?Best,
MaxHey @Benutzer1756,I have not tested this, but my theory for why it doesn’t work is because the pbrun command is a wrapper. The work is done by a docker container. The container uses a different shell instance and therefore probably can’t read in from the original shell’s /dev/stdinHi, thanks for your response. This is too bad, as the issue expands to variant calling and recalibration, which are not able to read or write gzipped vcfs. As g.vcfs can get quite big, this costs a lot of time and space.
Do you happen to know a workaround for this issue?Hey @Benutzer1756, which variant callers are you trying to use that don’t read or write gzipped files?Hi,
pbrun germline with --gvcf and --out-variants.g.vcf.gz produces no gzipped vcf.gz (which can get pretty large for WGS)
pbrun haplotypecaller on the other hand does gzip the output, if a g.vcf.gz is specified.pbrun genotypegvcf accepts .g.vcf.gz but does not produce gzipped
pbrun vqsr does not accept gzipped vcf files, neither as input nor as training set.I think working with ungzipped vcfs from haplotypecaller on is fine, as the size is managable (~1GB per 30xWGS) but the g.vcf produced by pbrun germline should be compressed.edit: to come back to the original question: it would also be great if pbrun germline and fastq2bam accept unaligned .bam-files as input to concord with GATK best practicesBest,
MaxHey @Benutzer1756,Thank you for this feedback. This is not available in the current version of Parabricks, but I’ve sent this as a feature request to the engineering team for future versions.Powered by Discourse, best viewed with JavaScript enabled"
84,booleancondition-condition-to-stop-operator,"We have written a custom replayer operator in Holoscan SDK v0.5.1 with the Python API and want the operator to stop emitting frames when we detect the end of the video file.We do not want to use a CountCondition because we are replaying at variable speeds and do not know the timing of the end of the video when the operator is instantiated. Is there documentation or sample code on how to use BooleanCondition in the Python API? Is there another way that we can directly communicate with the scheduler to stop the operator (based on a condition that we check within the running operator)?Hello, thank you for your question and we will take the feedback to add an example on how to user BooleanCondition in the Python API in the future.(1) We can create an instance of a BooleanCondition like this and use it in an OperatorAt some point later you can change the bool value of the condition with myCondition.disable_tick() and myCondition.enable_tick(). For more details on disable_tick() please see Creating an Application - NVIDIA Docs.(2) In addition to the method shown above of explicitly creating and passing a BooleanCondition to an operator, in the SDK there are also examples in C++ of defining the boolean condition as a Parameter in the class (but it can also just be a private member) and creating it by default during operator initialization. There are two operators for this: VideoStreamReplayerOp and HolovizOp.We were able to get this working with your help! Thank you very much for pointing us to the right resources and for your detailed guidance.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
85,how-parabricks-works-with-petagene,"if i add binary of petagene to, say fq2bam, will i get petagene compressed bam file right away?Powered by Discourse, best viewed with JavaScript enabled"
86,aja-capture-card-no-csc-available-for-ntv2-channel1,"Hello,I encountered an error when running the aja capture example.For setting up our new AJA Corvid 44 I followed the instructions in the Holoscan documentation. Both, drivers as well as SDK are working fine. ntv2enumerateboards correctly shows our device and testrdma does not return faulty frames. Even ntv2capture from the aja SDK inside the container seems to correctly capture frames.ntv2enumerateboards output:When running the aja capture script, however, I get the following error:Error message:@jinl suggested in another post that the hdmi output from notebooks can alter the colorspace. Could that be the reason in my case? I did, however, also test the setup with a proper camera setup (Panasonic AW-Series). For HDMI to SDI conversion I used the Blackmagic Design MC HDMI-SDI 12G.Hello! Could you let us know a little more about the format of the input signal (resolution and framerate)?Hi there, we have a bit more info on this topic. The “No CSC available for NTV2_CHANNEL1” error would be due to a missing Color Space Converter unit on the board, which is used to convert from a YUV input source to the RGB that is used internally by Holoscan.From your log above the device info saysCorvid44-8k,  and from the AJA documentation it shows that the Corvid44-8K doesn’t have any CSCs.
The only way around not having a CSC would be to feed an RGB signal into the SDI input.According to the docs for the converter (Blackmagic Design MC HDMI-SDI 12G), that also shows only YUV for SDI output color space. (Micro Converters – Tech Specs | Blackmagic Design)
If you have a different AJA card with CSC please try that. If you don’t have access to a different AJA card, you will need a different converter that outputs RGB.Powered by Discourse, best viewed with JavaScript enabled"
87,fq2bam-marking-duplicates-bqsr-high-memory-use-job-killed-oom,"Because v4.1.1-1 isn’t handling --align-only and --no-markdups as expected for me (see my other recent posts), fq2bam is running Marking Duplicates, BQSR. However, here I run into memory problems.Consistent with the stated hardware requirements, I queued a job using Slurm as follows:After properly aligning and sorting, it logged:Since the job was queued with net 5 x 24 = 120G CPU RAM, it is apparent that memory usage was accumulating and the job killed when it hit the job limit.  This raises two issues/questions:Why does Marking Duplicates, BQSR use so much memory? Is it expected?More importantly, is there a mechanism to tell fq2bam how much memory is available to it?The job is running on a shared node, and I do not necessarily have access to all RAM on the machine. Even if I try to ensure that I am the only job running on a node, I still need to provide a memory request, and thus will have a job memory limit.I read in other posts about an option called --memory-limit, but I do not see it in current documentation so assume it was dropped in recent versions?   It seems an important option to me.(as an aside, for me this issue would become moot if --align-only worked, I don’t actually want to do sorting or dup marking)While I still think --memory-limit is a valuable option to have, I may have found a workaround for shared clusters.From Slurm sbatch docs:–exclusive[={user|mcs}]
The job allocation can not share nodes with other running jobs … the job is allocated all CPUs and GRES on all nodes in the allocation, but is only allocated as much memory as it requested. This is by design to support gang scheduling, because suspended jobs still reside in memory. To request all the memory on a node, use –mem=0.The last bit I had missed previously - so, it is possible to request all memory on a node without having to provide a specific mem number.A further discovery, rather by accident.  To make #SBATCH --exclusive work meaningfully on our cluster, I had to switch to V100 GPUs with 16G memory (prior were A40), so I added the --low-memory flag. Unexpectedly, one or more of those changes altered the behavior of Marking Duplicates, BQSR.  CPU memory no longer accumulates progressively, so no OOM:It would be great if someone had insight what is happening with this, it doesn’t appear to be documented.Powered by Discourse, best viewed with JavaScript enabled"
88,clara-agx-developer-kit-development-guide,"Hi,I have access to the Clara AGX Developer kit, however I am a bit stumped on how to develop for it or develop on it. Our group typically trains/develops deep learning models on pytorch. How do we train and use models on the AGX developer kit?It seems quite difficult to install and use applications on the AGX developer kit - for instance something like VS code doesn’t install/open correctly and a bunch of other applications do not work correctly. I followed the set up guide, and was able to get some L4T docker images installed, but they also usually had some errors (e.g. error import open cv in python). Is it only possible to use clara holoscan on the agx developer kit, am I missing something?Hello, if you want to validate something in PyTorch on the Clara AGX Developer Kit, you could use the container PyTorch | NVIDIA NGC which supports the devkit with multi arch support. Just out of curiosity, what training workloads do you plan to run on the devkit? Since it has one RTX 6000 GPU.For VS code, if you’re not able to install it locally on the devkit, perhaps you could use the Remote -SSH or Remote Development extension to connect to the devkit from another machine?About the L4T docker images, on NGC there are several essential docker images that support multi-arch which includes the devkit, such as TensorRT | NVIDIA NGC and TensorFlow | NVIDIA NGC in addition to the PyTorch one linked above.For the question “Is it only possible to use clara holoscan on the agx developer kit, am I missing something?” I’m not sure I understand, could you elaborate a little more?Thanks for links - I’ll try the other NGC containers to see if I can reproduce the PyTorch workflow. Some of our training loads are quite minimal - e.g. under 1000 256x256 2D medical images, and it would be great to be able to train and deploy on the same machine for convenience.To rephrase my question - what are the steps to achieve real-time inference once you have, say, a trained pytorch model? Does this require clara holoscan? We would like to keep the training–deploy cycle as simple as possible.Oh I see, yes that description of a small training workload makes sense.
For deployment, I’d suggest the Clara Holoscan SDK. I would recommend to convert the PyTorch model into TensorRT for deployement. Our SDK has reference pipelines in GXF running models in the TensorRT format for streaming use cases.The SDK GitHub repo: GitHub - NVIDIA/clara-holoscan-embedded-sdk: AI computing SDK for medical devices with low latency streaming workflows
The SDK Documentation: NVIDIA Clara Holoscan DocumentationPowered by Discourse, best viewed with JavaScript enabled"
89,pbrun-dbsnp-return-1,"I am using parabrick for my wes data. After somatic variant calling, I am trying to annotate variants using dbsnp database (if there are any known germline variants present by chance) with pbrun dbsnp module but I am not getting either any error or warning. Initially, I thought there might be an issue with either vcf file or dbsnp file so I generated vcf file again and downloaded the dbsnp database vcf file again and generated their tabix index. And I also tried with Gnomd database germline resources which also contains the rsids. In both the caes it shows only -1 which is not clear to me. Here is the screenshot of the command and its response. Kindly suggest.
Screenshot from 2021-01-30 10-41-051240×900 149 KB
Hi,Thanks for your interest in Parabricks. Seems the run completed but the progress meter didn’t catch up fast enough. Do you get an output and is it as expected ?Powered by Discourse, best viewed with JavaScript enabled"
90,empty-fusion-output-in-this-star-fusion-version-fusion-predictions-tsv,"When calling fusions on a sample with newer version of STAR aligner and STAR fusion we are getting a good number of Fusion calls and junction read counts among them, but when running with para clara bricks pbrun STAR aligner and STAR-fusion we are getting empty file, Chimeric Junctions file in latter case is generated of very less size
Note: we are using same parameter, as well as default and all sorts of parameters
Our Star version:2.7.8a , and STAR-Fusion version: 1.11.0Thanks in advance@gburnett , is newer versions of STAR aligner and STAR-Fusion going to replace older versions inside para clara bricks rna_fq2bam ,as older version are not compatible with libraries made with newer STAR !As far as I know, there are no plans for that.Powered by Discourse, best viewed with JavaScript enabled"
91,deepvariant-no-channel-insert-size-error,"Hi,I am getting the following error when trying to use “–no-channel-insert-size” with deepvariant on version 4.0.0-1 using “pbrun deepvariant   --in-bam in.bam   --ref ref.fna   --out-variants out.vcf --no-channel-insert-size”:[PB Error 2022-Dec-03 22:50:11][src/trtManager.cu:320] The dimension of model file doesn’t match with the confirguation. Model:100,221,7. Confirguation:100,221,6, exiting.
[PB Error 2022-Dec-03 22:50:12][./inc/common.h:108] NvInfer ERROR: CUDA initialization failure with error 4. Please check your CUDA installation:  Installation Guide Linux :: CUDA Toolkit Documentation, exiting.Considering the error (“the dimension of model file doesn’t match with the confirguation … 100,221,7 … 100,221,6”), it appears that the pre-trained weights for Deepvariant don’t match the model that is being constructed; that is, it is probably trying to use the weights with the channel-insert-size layer on a model that does not have the layer (or vice-versa). Can you please help with loading the correct weights/model?Hey @orianl,I’ve just replicated your results and submitted a bug to the engineering team to investigate. Please hang tight. Thank you for letting us know.Thank you! Is there anything that can be done in the meantime? I’ve noticed that there is an option for manually selecting the model via “–pb-model-file”, could that be used to load the v1.3 Deepvariant model? If so, where can that model file be found?Hi @orianl,Baseline Google DeepVariant changed the defaulted channel size to 7 in v1.4 release (it was 6 before). As we updated DeepVariant version in Parabricks v4.0 to be compatible with v1.4 of the baseline, we now have an engine that only support channel size 7.You can indeed use an engine from Parabricks v3.8. If you have access to this version you can download the engine from inside the container and use it as input for --pb-model-filePowered by Discourse, best viewed with JavaScript enabled"
92,parabricks-v4-0-how-to-run-parabricks-on-kubernetes-k8s,"Hi there,For Parabricks .3.7, 3.8, we can setup parabricks.deb in Dockerfile, and then run the image on K8s.But for Parbricks 4.0 installation, it is launched by docker container. In this case, we will face the docker-in-docker (dind) issue on K8s. Will you support the Debian Package Installation for v4.0, like v3.7, v3.8?Best regards,
TJWe do not plan to have Debian package for v4.0 but you should be able to run the new v4.0 docker container on K8s without issue now.After testing, I am able to run new v4.0 docker container on K8s.i.e. it directly calls python3 to run tasks on a docker container.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
93,evaluation-license-download-links-are-invalid,"A genetics researcher at our institution would like to evaluate Parabricks on the HPC cluster.I filled out the trial license form here: NVIDIA Clara Parabricks: 90 Day Free Trial
I received an email with subject “NVIDIA Clara Parabricks Pipelines - Your 1 month license key” and a link to “Install NVIDIA Clara Parabricks Pipelines”. However,Clicking the link redirects at least once to Amazon S3, where it is rejected with a 403 errorThe XML body of the 403 indicates the link expired on 2020-08-21 (seems to coincide with the “Expires” timestamp embedded in the URL)I notice the S3 URL also includes the text “…_end_aug…” which is in agreement with the August expires date.The most recent response from a moderator on October 6 includes a link to download version 3:https://s3.amazonaws.com/parabricks.licenses/v310_OCT_END/parabricks.tar.gz?AWSAccessKeyId=AKIAJGDUNN2G2ZAH3Q3A&Expires=1602532746&Signature=yVstD6Ga%2FQVZawj05bmeWFFGXDY%3DAt the time I found this, the “Expires=” timestamp in the URL had also been reached, but I see the URL includes “…OCT_END…” so it seems a valid download location does exist for this month.Is there anything that can be done to work around the issues with the expired links being sent by the trial license form?Thank youHi, thanks for reaching out to us.
Could you please try the following link, which expires at the end of Nov?
“https://s3.amazonaws.com/parabricks.licenses/v311_NOV_END/parabricks.tar.gz?AWSAccessKeyId=AKIAJGDUNN2G2ZAH3Q3A&Expires=1603296915&Signature=7LXvM92YJvFxRuXRA5WSPNnxLPU%3D”
Thank you!Yes, that worked perfectly. Thank you very much!You are welcome! :DHi,
I just signed up for the parabricks pipeline trial an am getting a similar error from the v311_NOV_END link that was emailed to me:
Request has expired
2020-10-21T16:15:15ZIs there a newer link available for downloading the trial?Thanks!Hi Christoph,Welcome to the NVIDIA Developer forums. I have the team looking to resolve this ASAP.
Thanks for your patience.TomGreetings Christoph,Our apologies for this mishap; it appears that there’s a new policy for the link expiration that we’ll be addressing separately - here’s an updated link pointer for you to use instead:wget -O parabricks.tar.gz “https://s3.amazonaws.com/parabricks.licenses/v311_NOV_END/parabricks.tar.gz?AWSAccessKeyId=AKIAJGDUNN2G2ZAH3Q3A&Signature=MzWbwsjmXWdzyACJT2JUH1XlMAs%3D&Expires=1608564579”-AboodHi,
Thanks for the quick reply - the new link worked great.
Best,
CHi, can you update this link please?Powered by Discourse, best viewed with JavaScript enabled"
94,gromacs,"GROMACS is a molecular dynamics application designed to simulate Newtonian equations of motion for systems with hundreds to millions of particles. GROMACS is designed to simulate biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions.Learn MorePowered by Discourse, best viewed with JavaScript enabled"
95,working-with-gxf-entities-video,"Hello,
I am a new with Holoscan and want to ask some questions on the gxf entities video files.With the best wishes,
ValeriyHi Valeriy, just to make sure we’re on the same page, we’re talking about file pairs such as {surgical_video.gxf_entities, surgical_video.gxf_index}.Thank you very much for your explanations!Powered by Discourse, best viewed with JavaScript enabled"
96,clara-parabricks-4-on-hopper,"Could it be possible to run parabricks/4 on H100 gpu? I tried to run docker image, and failed. Since I didn’t see Hopper was mentioned in the doc, I’m wondering if there is some hardware restriction before spending more time troubleshooting it.docker run --gpus all --mount type=bind,source=/yuq3/parabricks,target=/parab -it nvcr.io/nvidia/clara/clara-parabricks:4.0.1-1 /usr/bin/bashYou are correct. Hopper support is coming in the next version of Parabricks.Powered by Discourse, best viewed with JavaScript enabled"
97,parabricks-license-key-for-novel-coronavirus-research,"Hello,I received an email granting me free access for 90 days to Parabricks.  Is it possible to use the Parabricks 90 day free license on Google Cloud Platform?How will I apply the license on GCP?Regards,
Vithal
vithal.madhira@palilasoftware.comThanks for choosing our solutions. You might please use this link
https://developer.nvidia.com/nvidia-parabricks to request the access. I don’t think we provide the service on GPC currently. Please let me know if there is further aid you need.Thank you,EddieHi Vithal,Thank you for your interest in Parabricks. In order to run Parabricks on GCP; perhaps you can start with a Deep Learning machine image in order to obtain a virtual machine (VM) with that meets the minimum requirements. Then you should be able to run the install script that comes as part of license email. At that point it’s like doing a local installation where the Parabricks container is pulled from NGC and you should be able to launch parabricks.Thank you for your response. I will try it.Powered by Discourse, best viewed with JavaScript enabled"
98,nvidia-parabricks-tesla-k40c-hardware-requirement-question,"I would like to run the NVIDIA Parabricks. Mostly interested in alignment, variant calling speed improvements. I can’t figure out if my GPU meets the minimum requirements. It’s Tesla K40c. Which CUDA architecture does it support?The following are required to install Parabricks:The following are the hardware requirements (?)Another important point is I am running Windows 10 64-bit Enterprise.I want to do CNV calling, run the germline pipeline from fastq to bam + vcf  from whole genome sequence data. My research is in rare developmental disorders and I have trio data. My CNV calling pipeline currently runs many days to complete. I want to be able to do it in less than an hour.Thank youThank you for your interest in Parabricks. K40 is the Kepler architecture and unfortunately that doesn’t meet the minimum Compute Capabiltiy (CUDA architecture) requirements. Only Pascal (ex. P100), Volta (ex. V100) and Turing (ex. T4) are supported architectures.do i have other options running the same tools utilizing GPU?Unfortunately you only really have two options as far as I know:either buy compatible GPUs - so P100, V100, T4, or I’m assuming A100 now as well.using GPU enabled cloud images (i.e. via AWS)I want to do CNV calling, run the germline from fastq to bam + vcf from whole genome sequence data. My research is in rare developmental disorders and I have trio data. My CNV calling pipeline currently runs many days to complete. I want to be able to do it in less than an hour.We have the same use case and are using Parabricks on 2x Tesla V100 cards. Our time to take a 30X whole genome sample from fastq to vcf via the germline pipeline is around 2 hours, which is a great speed up from the 20+ it takes on ~36 CPU cores. We’re finding it really nice to use for our clinical exome testing, being able to take 100X coverage exomes from fastq t0 vcf in under 10 mins.If you can make a business case to purchase GPUs and are constantly running pipelines that would benefit I think it makes sense to invest in the hardware (I’m trying to get work to invest in more GPUs). However if you aren’t running lots of samples through then the cloud based approach might be worth a look?Just my two cents.Can we get any word from Nvidia whether Geforce cards could be used in Parabricks pipelines? The newly released 3070/3080/3090 cards seem more than capable and are far cheaper than their Tesla/A100 counterparts. The 3090 in fact has double the CUDA cores of one of our V100, yet it is magnitudes of dollars cheaper.Is there anyway to run GPU with Docker Desktop (Windows 10)?Hello all,
As we know that AWS is quite expensive for many of us to afford. However, we have an affordable option as Google Colab Pro that also provides us high end GPUs.
I need to know from Nvidia people that is it possible to execute Parabricks pipeline on Colab? Are the GPUs in Colab pro compatible with the pipeline?
ThanksPowered by Discourse, best viewed with JavaScript enabled"
99,can-i-install-clara-holoscan-sdk-on-the-jetson-agx-orin-developer-kit,"According to Install Clara Holoscan Software :: NVIDIA SDK Manager Documentation , for the Clara Holoscan SDK, Jetson AGX Xavier modules and the Clara AGX Developer Kit are the only hardware supported.
And according to the introduction of Advanced Computation for AI-Powered Medical Devices | NVIDIA , the NVIDIA Clara Holoscan developer kit is built with a NVIDIA AGX Orin™ module.
So, is it possible to install Clara Holoscan SDK on the Jetson AGX Orin Developer Kit? I hope to use the existing hardware to play with the Clara holoscan SDK.
Thanks a lot!Hello there! For the current v0.1 version of the Clara Holoscan SDK, the Clara AGX Developer Kit is supported. The Clara Holoscan Developer Kit is coming soon, and so is the SDK support for the Clara Holoscan Developer Kit.  Please note that the Clara Holoscan SDK is officially compatible with only Clara developer kits and heavily utilizes the discrete GPU. Once SDK support for the Clara Holoscan Developer Kit comes out, you could experiment with the SDK and some functionalities might work on the Jetson AGX Orin Developer Kit , but please keep in mind that the Jetson AGX Orin Developer Kit is not an officially tested platform for the SDK.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
100,how-could-i-compare-variant-calling-results,"Hello, I saw a post on your homepage that the results of the HaplotypeCaller of Parabricks are the same as those of GATK4 HaplotypeCaller.
Does this really have 100% matching results?How did you compare the results? I compared the results of the VCF file for both of Parabricks and GATK4 using Illumina/hap.py, but the results were not the same. I’d like to know how you compared the variant calling results.I look forward to your answer.Thank you.Hey @qhtjrmin,Here is the page where we discuss how to compare the outputs of Parabricks vs GATK:Reference the latest NVIDIA Clara Documentation.The results will depend on which versions of Parabricks and GATK you are comparing. Parabricks Version 2 will align with GATK 4.0.4 and Parabricks Version 3 will align with GATK 4.1Hi,I’m currently trying to compare outputs with the CPU version I’m using, but it’s GATK 4.2.0.0
The results are still similar (39M calls i.e. 97% overlap), but not fully (1.2M extra calls i.e. 3% and 3.9M missing calls i.e. 5.7%).
I did not compare to the CPU version of GATK 4.1 but I suppose that these differences are coming from the difference of version from GATK.Is there a plan to align with v.4.2 in the near future?ThanksHi Vincent,Your supposition is right. The differences between the results are indeed due to the the difference of version.We are planning on to incorporate GATK 4.2 in our next major release.Thanks
MyriemeHi @mdemouth ,That’s perfect! Thanks for the precision.Is there a page where I can follow the coming updates/next releases?
I don’t know how often you release a new major version. Is this in the order of months? Or years?ThanksPowered by Discourse, best viewed with JavaScript enabled"
101,welcome-to-the-nvidia-parabricks-developer-forum,"Welcome to our Developer Forum!This Developer Forum is open to the entire Parabricks community and is a great place to ask questions, exchange knowledge, and connect with fellow Parabricks users.If you have purchased a license which entitles you to NVIDIA Enterprise Support, you are welcome to post on this forum, but know that the best way to get an issue or question resolved is by contacting our dedicated Enterprise Support Team. You can contact our support team via web portal, phone, or web form. You can find this information listed on our Enterprise Customer Support page.If you are interested in purchasing a license which would entitle you to NVIDIA Enterprise Support, please fill out this web form and a member of our team will be in contact with you.Answers to common questions can often be found in our documentation. You can find documentation for the latest version here. Archived documentation for previous versions of Parabricks can also be found by scrolling to the bottom of the page.NVIDIA Parabricks is an accelerated compute framework for next-generation sequencing data, supporting end-to-end data analysis workflows for DNA and RNAApplications. Running on a suite of NVIDIA GPU platforms, Parabricks provides accelerated versions of gold-standard tools for read alignment, processing and quality control (QC), and variant calling. Parabricks can also be easily deployed in common bioinformatics workflow managers for improved scalability, customizability, and intertwining of GPU- and CPU-powered tasks.Powered by Discourse, best viewed with JavaScript enabled"
102,can-i-use-quantmode,"I want to use  quantMode option for STAR alignment.
Is there any way to use it?Hey @min890315,At this time these are the commands we support for STAR.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
103,nvidia-parabricks-pipeline-on-google-colab-pro,"Hello all,I am a research scholar at University of Delhi, and I need to try out Nvidia Parabricks Pipeline for my research.
Could you please guide me how to execute it on Google Colab (Pro)? Also, it would be very kind if you could provide me details on how to execute it on AWS; details such as minimal services required to execute it there.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
104,clara-agx-dev-kit-wont-boot-after-nvgpuswitch-py-install-dgpu-failure,"Hello,I performed a fresh install/flash on my Clara AGX using SDK Manager 1.7.1.8928 with the new Clara Holoscan SDK.  I ran the “nvgpuswitch.py install dGPU” command, but it failed to install and gave me a dependency error related to the nvidia-container-runtime.  I attempted to run an apt update and apt upgrade, but during the process the internet connection failed on the AGX (not any of my other computers).  I rebooted the AGX, but it will no longer boot.I have tried to hold both reset and recovery buttons simultaneously for 20 seconds, but it does not reset the AGX.  I have attempted to boot to recovery mode with the recovery button and the power button simultaneously but this does not work.Do you have any suggestions for how I might be able to reset / re-flash the AGX?It seems that dGPU mode is no longer compatible with the Clara Holoscan SDK image?Thank you for your input.After leaving the machine off and unplugged for ~10 minutes, I was able to boot to recovery mode by holding the recovery button while pressing the front power button.  This then allowed me to Flash the system again in Manual Mode from the SDKManager.  So that problem is solved.Great to hear that booting to recovery mode helped with reflashing.For future references, if booting to recovery mode doesn’t help the host system to detect Clara AGX for reflashing, then we can try going into the reset mode (pressing reset button + recovery button) for the host system to detect Clara AGX for reflashing.Thank you.  I will remember that in the future.Unfortunately, it seems that there is still a dependency error when converting from iGPU mode to dGPU mode with the nvgpuswitch.py script.  This is with JetPack4.5.1.  The dependency error is related to nvidia-container-runtime and nvidia-container-toolkit.  I am trying different versions of these two packages, without luck so far.Any recommendations would be appreciated.  Thank you.Could you let us know which error occurs and what is your nvidia-container-runtime/nvidia-container-toolkit versions? (The nvgpuswitch.py script should be taking care of that for us)After a new flash, I run:sudo /opt/nvidia/clara-holoscan-sdk/clara-holoscan-tools/bin/nvgpuswitch.py install dGPUwhich gives the following error:Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:The following packages have unmet dependencies:
nvidia-container-runtime : Depends: nvidia-container-toolkit (>= 1.4.2) but 1.0.1-1 is to be installed
E: Unable to correct problems, you have held broken packages.
ERROR: Install dGPU drivers failed!Currently, I have:
nvidia-container-runtime=3.1.0-1
nvidia-container-toolkit=1.0.1-1The nvgpuswitch.py script tries to install nvidia-container-runtime=3.4.2-1.  I tried changing the script to use the most recent version 3.7.0-1, but this seems to cause downstream problems when reinstalling the holoscan SDK.Thank you for your help.Thanks for the detailed message. Could you try to install nvidia-container-toolkit version 1.5.1-1 (sudo apt-get install nvidia-container-toolkit=1.5.1-1)?If I update nvidia-container-toolkit to version 1.5.1-1, then nvgpuswitch.py does run without halting errors.  I can then reboot, and the video output comes from the discrete GPU as expected.  nvidia-smi shows the RTX 6000.However, after doing this DeepStream is uninstalled.  There is no deepstream-app present.  Also, the clara-holoscan-deepstream-sample folder has been removed.  I tried to reinstall the SDK and DeepStream via the SDKManager without re-flashing, but this also yields errors.  I tried to copy the deepstream deb package file over to the Clara and install manually, but again encounter package dependency errors that I can’t seem to resolve.Please see section 7.1 Reinstalling Clara Holoscan SDK Packages in the Documentation https://developer.nvidia.com/Clara-Holoscan-SDK-Documentation for reinstalling packages once in dGPU mode. For a complete Checklist for Setting up the Developer Kit, please see the User Guide at https://developer.nvidia.com/clara-agx-development-kit-user-guide. You can find more documentation for the Holoscan SDK on the developer page https://developer.nvidia.com/clara-holoscan-sdk.Thank you.  That did the trick.  Ultimately, 1) flashing, 2) updating nvidia-container-toolkit to version 1.5.1-1, 3) running nvgpuswitch.py, then 4) reinstalling libvisionworks, libvisionworks-dev, deepstream, and clara holoscan manually did the trick.  The Endoscopy Example runs now with discrete GPU.
Thank you for your help!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
105,fq2bam-marking-duplicates-bqsr-executing-despite-no-markdups-etc,"I’m running Parabricks 4.1.1-1 fq2bam as follows:Although the documentation states:The user can turn-off marking of duplicates by adding the –no-markdups option. The BQSR step is only performed if the –knownSites input and –out-recal-file output options are providedAfter successful alignment and sorting, the job above resulted in:Am I misinterpreting? It appears to be running at least one of Marking Duplicates, or BQSR despite setting --no-markdups and not setting -–knownSites or -–out-recal-file.Is this a bug or my error or misunderstanding?  It’s an immediate problem for me because the mark dups/BQSR eventually got killed OOM (a different issue…)Powered by Discourse, best viewed with JavaScript enabled"
106,nvidia-flare-federated-learning-datakind-weight-diff-or-datakind-weights,"I am working with Nvflare for Federated Learning in medical imaging.I would like to know if it is preferred to use DataKind.WEIGHT_DIFF with respect to DataKind.WEIGHTS for transferring FL control variables to the server. Is there a fundamental implementation difference, besides the fact of combining weights or their differences ? Are there any advantages of using WEIGHT_DIFF, like smaller errors or faster convergence ?
To sum up, is using WEIGHT_DIFF  the best practice, or there is actually no difference, and it depends on the user’s preferences ?Thanks in advance.GonzaloPowered by Discourse, best viewed with JavaScript enabled"
107,clara-parabricks-deepvariant-error,"i am getting this error while running clara parabricks deepvariant from singularity container,
gpu specs → Tesla V100-PCIE-32GB
run command → singularity run --nv clara-parabricks_4.0.0-1.sif pbrun deepvariant --ref /genome.fa --in-bam fq2bam_output.bam --out-variants output/deepvariant_output.vcfalso, i am getting a similar error for haplotypecaller as well,Hello @devendra.kumar,Thank you for submitting your question. Do you happen to have the full log file so I can look through it more closely?Thanks!Log fileThank you @devendra.kumar,Are you able to try this run on 2 GPUs, so I can rule out some issues?@gburnett …
We have only one GPU at our facility !Though fq2bam runs smoothly !What is the status of this bug ? @gburnettHey Devendra,It look like your bam file could be corrupted. Can you run samtools quickcheck to verify?Powered by Discourse, best viewed with JavaScript enabled"
108,fq2bam-takes-the-same-time-with-or-without-markdup-bqsr-step,"Is it normal for fq2bam to run with the same time with or without markdup, with or without BQSR step?Hey @luobin.yang,These run in parallel to the sorting so timing should not change that much.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
109,known-issue-in-cupy-affecting-tensor-interop,"If you are passing CuPy coordinates from a native Python operator to Holoviz, if you run into the error [2023-02-28 00:34:25.313] [holoscan] [error] [gxf_wrapper.cpp:68] Exception occurred for operator: 'holoviz' - size should not be zero while trying to render texts in Holoviz, or other errors while rendering other types in Holoviz that indicates a discrepancy in the array values passed from the native Python operator and the array values received by Holoviz, then it may be due to an existing CuPy issue Unexpected stride when performing cupy.asarray on numpy.ndarray with leading dim 1 · Issue #5897 · cupy/cupy · GitHub. This issue affects any CuPy arrays that have leading dimension 1 when calling astype or cp.array.
For example, when defining a label_coords that the native Python operator outputs viathe following works without errorand the following will generate the error:If you need to call astype(), a temporary workaround is to pass order='C' to the astype() call. This will work whether or not copy is True or False.Powered by Discourse, best viewed with JavaScript enabled"
110,unable-to-obtain-the-latest-version-of-clara-parabrick-image-4-0-1-1,"I am getting an error when I am trying to obtain the latest version of clara-parabricks:4.0.1-1 using the command:
docker pull nvcr.io/nvidia/clara/clara-parabricks:4.0.1-1
However, when I was able to obtain clara-parabricks:4.0.0-1 using the same command for version 4.0.0-1.
Please let me know how can I install the latest version.I have received the same error. Also note that version 4.0.1-1 does not appear in the NGC catalog.Hi,We are sorry but the latest version of parabricks is 4.0.0-1.
So please use : docker pull nvcr.io/nvidia/clara/clara-parabricks:4.0.0-1We will update the documentation to reflect the right tag.Thank you
Best
MyriemeThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
111,any-one-working-on-covid-19-or-any-gnomic-project,"I am interested to work on Gnomic projectI am exploring super rare childhood neurological disease associated with de novo germline mutations. the big issue is the time it takes to process whole genome sequencing data from fastq to bam and vcf. I havn’t started yet but I have an older GPU (Tesla K40c) trying to find out if it meets the minimum requirements for Parabricks. If not I might still explore other options. also looking for doing super fast CNV calling, currently it takes 4 days. I like to do it in less than an hourPowered by Discourse, best viewed with JavaScript enabled"
112,holohub-application-build-errors,"hello,
I’m testing holohub applications( GitHub - nvidia-holoscan/holohub: Central repository for applications and operators for Holoscan)
in nvcr.io/nvidia/clara-holoscan/holoscan:v0.5.1-dgpu, on an IGX Orin-A6000.
the following operations are successful:
./run build
./run build endoscopy_tool_tracking
./run launch endoscopy_tool_tracking cpp
but an error came out while
./run build ultrasound_segmentation
did I miss something?

image1424×982 35 KB
Hello, could you let us know a little bit more about your setup, such as whether you have the deployment stack installed, or Holopack installed, and which version? Could you please also confirm that your HoloHub repo is up to date?If this isn’t the first time building in the repo, I’d also suggest trying removing the build directory and building again.BTW in HoloHub the command ./run build should be building sample applications which include ultrasound_segmentation.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
113,namd,"NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR.Learn MorePowered by Discourse, best viewed with JavaScript enabled"
114,python-script-nvgpuswitch-py-igpu-to-dgpu-missing-additional-sdk-clara-holoscan-missing,"Hello,I am unable to find the python script nvgpuswitch.py that would allow me to switch to the dGPU. As I read in the Kit User Guide, this script is supposed to be in the folder /usr/local/bin on the Clara Hologram platform, therefore the folder contains only
lrwxrwxrwx  1 root root   22 Feb  1 14:43 nsys → /etc/alternatives/nsys*All the components have been properly installed with the SDK Manager.
Am I missing something?
Thanks for your help in advance.Hello,The script should be located in /usr/local/bin after installation, and also should be in /opt/nvidia/clara-holoscan-sdk/clara-holoscan-tools/bin/. Could you check if it is at least in the second location?Hello,Thank you for your reply.
The folder /opt/nvidia/clara-holoscan-sdk/ doesn’t exist.I also previously tried to find the python script file using “find” command on the Clara Holoscan platform, therefore no result.
I guess the Clara Holoscan SDK is not installed, therefore it is not listed in the SDK Manager…When I launch the SDK Manager,  in the Step 1, I have :
Product Category : Jetson
Hardware config: Host machine (is ticked) and Target Hardware : Clara AGX Developer Kit modules (ticked as well)
Target Operating System : Linux JetPack 4.5.1 (rev 1)
Additional SDKS : DeepStream (which I also ticked)In the guide ""Install Clara Holoscan Software with SDK Manager, I can see more kits available there as RiverMax and Clara Holoscan, which are not available in my case.If I am correct and that it is not the Clara Holoscan SDK missing, how can I install it knowing it is not available in the SDK Manager?What is your SDKManager version? Perhaps it needs updating before it shows all available “Additional SDKs”. It definitely should show all 3: DeepStream 5.1 (don’t select), Rivermax (select), and Clara Holoscan (select).The SDK Mananger version is 1.7.2.9007
Where can I get the last version?1.7.2 seems actually to be the last version

SDKM1195×770 327 KB
That version should be okay. Just to check, did you log into SDKManager with an account that has joined the developer program? https://developer.nvidia.com/clara-holoscan-sdk-programThanks a lot! I took for granted I had joined as a developer therefore was not the case.
After joining and relogging, Clara Holoscan SDK is now available.Great to hear that you’re now able to get started!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
115,invalid-sample-name,"Hi,
I want to run using Mutectcaller (Parabricks accelerated) version 4.0.0-2 on UK Biobank Research Analysis Platform. I started analysis and received invalid tumor sample name error. I share a screenshot about input names and log file.  In the documentation, I had read “This MUST match the SM tag in the tumor BAM file.” I don’t know what the  SM tag in the BAM file I use as input file is. How can I find this?
Can you make a suggestion for tumor sample name?
Many thanks.

Ekran Resmi 2023-04-06 09.58.061307×358 33 KB


Ekran Resmi 2023-04-06 09.59.45851×255 29.9 KB
Hey @cevikb ,I was able to find the SM tag on my BAM file using samtools.Specifically “samtools view -H output.bam”. This printed out a lot of text, but one of the last lines was tagged @RG and had that information. Here’s what mine looked like:@RG     ID:HK3TJBCX2.1  LB:lib1 PL:bar  SM:tumor        PU:HK3TJBCX2.1Hopefully that helps!Hi @gburnett ,Thank you very much for your answer. I could find SM tag of my BAM file in the log file using FQ-to-BAM Pipeline (Parabricks accelerated).@RG\tID:HVVWTDSXX.4\tLB:lib1\tPL:bar\tSM:sample\tPU:HVVWTDSXX.4I tried to run again with a few sample names;Name of sample for tumor reads. (tumor_name)
HVVWTDSXXName of sample for tumor reads. (tumor_name)
HVVWTDSXX.4Name of sample for tumor reads. (tumor_name)
@RG\tID:HVVWTDSXX.4\tLB:lib1\tPL:bar\tSM:sample\tPU:HVVWTDSXX.4but I received invalid sample name error Is there other advise?Thanks in advance.Hey @cevikb,Based on the output you shared, it looks like the sample name is “sample”. Have you tried that?ThanksI tried just “sample” and It worked. Thank you for your useful comment.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
116,output-both-vcf-and-gvcf-when-using-deepvariant,"Hello,the standard deepvariant tool can output both VCF and gVCF for the same sample at the end of variant calling. This is often quite convenient to compute stats on the regular VCF while having the gVCF for subsequent cohort merging.
The standard tool also generates an HTML QC report.Is it possible to have the same outputs when running deepvariant in clara parabricks? Especially, can we have the VCF and gVCF simultaneously? It would be great to have the HTML report as well, but this is not essential.So far, I’ve tried various combinations of --out-variants and --gvcf but I can only get variants in one of the 2 formats, but not both.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
117,computer-vision-developer-page-looking-for-beta-testers,"Clara Developer Community,I am looking for a handful of developers to test drive a new computer vision page ahead of our release scheduled for the end of the month. I am looking for feedback on the visual flow and design of a computer vision-specific for developers highlighting all of Nvidia’s solutions and resources. If you are interested in being part of this special beta test, send me a private message directly. Depending on schedules, beta testing will take place in one 30 minute session over the next week or two.Best,
MikeB_NVPowered by Discourse, best viewed with JavaScript enabled"
118,fq2bam-align-only-flag-leads-to-segmentation-fault,"I was happy to see the --align-only flag in Parabricks fq2bam v4.1, as my pipeline downstream of bwa-mem takes name-sorted bam.However, in my hands, the --align-only flag leads to a segmentation fault.I am running the clara-parabricks-4.1.1-1 container using singularity on a cluster server running Slurm. The job is on a shared node, using only 1 of several GPUs. This all seems fine, since the job executes properly without --align-only.The command is:This seems like a bug, perhaps one that only appears with a specific combination of options?Thanks for any help, I’d like to use the new flag.Powered by Discourse, best viewed with JavaScript enabled"
119,does-parabricks-support-multi-host-parallelism,"Hello,I have installed parabricks pipeline with singularity option, and I’m using an HPC that has nodes with one V100 card on them (per server). The pipeline worked on one node(i.e. one GPU). However, when I allocated two nodes to run on two GPUs I had the following error:“Number of GPUs requested (2) is more than number of GPUs (1) in the system”and the job configuration was:#SBATCH --partition=gpu
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=64000So, Does parabricks support multi-host parallelism? If so how can I manage to solve the issue?Thanks!Hello @syd04Parabricks does not support multi-host parallelism. Your best best is to assign each node it’s own job, as opposed to trying to spread one job across multiple nodes. Parabricks DOES support multi-GPU, so you can use the full node for each job.Powered by Discourse, best viewed with JavaScript enabled"
120,converting-std-vector-to-holoscan-tensor,"How to convert a std::vector of size 2764800 to a Holoscan tensor of shape {1280.720,3}Hello there, while the current tensor interop example shows the process of outputting a 1D std::vector as a Holoscan tensor for tensor interop, there should be an example in HoloHub soon that contains the process of creating a host Holoscan tensor that is more than 1D from a std::vector or a MatX tensor for tensor interop in the next update.Powered by Discourse, best viewed with JavaScript enabled"
121,cuda-toolkit-12-0-0-cudnn-is-needed,"Hello,I’m trying to use CUDA toolkit 12.0.0 and the corresponding cuDNN is not available.Could you please help to get a specific version of cuDNN which is compatible with CUDA Toolkit 12.0.0 ?Best regards,
Iliesthey should release out new version of cudnn soon, I think…cuDNN 8.8 should be coming in the next few months.Do you have time/date for cuDNN release ?A few months? Is there a pre-release, at least?Sorry for the late reply, the cuDNN 8.8 release is out with release notes here Release Notes :: NVIDIA Deep Learning cuDNN DocumentationHello everyone,cuDNN 8.8.1 for CUDA 12.x is available. Please check the download link cuDNN v8.8.1 (March 8th, 2023), for CUDA 12.xRegards,
IliesThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
122,ubuntu-nvidia-cuda-11-8-installation-cryosparc,"(Topic not quite right, not sure how to create topics and tags)
Installing Cryosparc, requires cuda, recommended 11.8, requires NVidia drivers.Case 1 - Ubuntu 22.04, drivers installed, installed cuda_11.8.0_520.61.05_linux.run but # nvidia-smi
shows cuda driver 12.0, I did something wrong but not seeing where.Case 2 - Ubuntu 20.04, drivers not installed, unclear what drivers to use, nvidia-smi tells me it can’t talk with the driver no matter what driver I try.
Tried installing cuda_11.8.0_520.61.05_linux.run but three levels of log down, in /var/log/nvidia-installer.log
I find that I think is the cause.Please see the log entries ‘Kernel module load error’ and ‘Kernel messages’ at the end of the file ‘/var/log/nvidia-installer.log’ for more information.
 → Kernel module load error: Operation not permitted
 → Kernel messages:
exe=“/usr/bin/dbus-daemon” sauid=103 hostname=? addr=? terminal=?’
[   52.446548] audit: type=1107 audit(1687467881.645:51): pid=2227 uid=103 auid=4294967295 ses=4294967295 subj=unconfined msg='apparmor=“DENIED” operation=“dbus_method_call”  bus=“system” path=“/org/freedesktop/ColorManager/devices/cups_Brother_MFC_L8900CDW_series” interface=“org.freedesktop.ColorManager.Device” member=“AddProfile” mask=“send” name=“org.freedesktop.ColorManager” pid=2573 label=“snap.cups.cupsd” peer_pid=2427 peer_label=“unconfined”Looks like apparmor - but I’ve turned it off, disabled, and rebooted.Thanks in advance for any guidance on either gpu-enabled ubuntu machine.
Brian
brian.cuttler@health.ny.govPowered by Discourse, best viewed with JavaScript enabled"
123,deepstream-sdk-in-windows,"Is it impossible to use Deepstream SDK in Windows until now?Hello, this would not be a right forum as it’s the Holoscan SDK forum. Please post to the corresponding forum, thank you.Powered by Discourse, best viewed with JavaScript enabled"
124,error-for-germline-pipeline-on-aws-instance,"I’m on a trial run.
I successfully installed and ran a germline pipeline on AWS instance p3.8xlarge.Next, we decided to run the pipeline on the biggest instance out there (p4d.24xlarge).Installation seemed to have been successful. But when I start germline pipeline,  it ended up with errors.The command I used:pbrun germline --ref /mnt/d1/Ref/Homo_sapiens_assembly38.fasta --in-fq /mnt/d1/PD01/FQ/H1_S120_L003_R1_001.fastq.gz /mnt/d1/PD01/FQ/H1_S120_L003_R2_001.fastq.gz --knownSites /mnt/d1/Ref/Homo_sapiens_assembly38.dbsnp138.vcf --knownSites /mnt/d1/Ref/Mills_and_1000G_gold_standard.indels.hg38.vcf --out-bam /mnt/d1/PD01/germline/H1.parabricks.bam --out-variants /mnt/d1/PD01/germline/H1.parabricks.g.vcf.gz --out-recal-file /mnt/d1/PD01/germline/H1.pararabricks_recal.txt --read-group-sm H1 --read-group-lb BCH --read-group-pl ILLUMINA --gvcf --num-gpus 8 --tmp-dir /mnt/d2/tmp/ --keep-tmpError message I see:GPU-BWA mem
ProgressMeter   Reads           Base Pairs Aligned
cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:148 : invalid device symbol
cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:148 : invalid device symbol
cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:148 : invalid device symbol
cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:148 : invalid device symbol
cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:148 : invalid device symbol
cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:148 : invalid device symbol
cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:148 : invalid device symbol
cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:148 : invalid device symbol
Please contact Parabricks-Support@nvidia.com for any questionsIs it a license issue? Or am I missing some setting or library for GPU/Cuda?Thank youHi in-hee.lee,p4d.24xlarge is an instance with A100.  You need to  install ampere based container.
To do so you need to provide the option --ampere during installation process.https://docs.nvidia.com/clara/parabricks/v3.2/text/local_installation.html#installation-optionsRegards,
MyriemePowered by Discourse, best viewed with JavaScript enabled"
125,haplotypecaller-batch-mode,"when  haplotypecaller runs in batch mode, it get errors, as belowsingularity exec --nv clara-parabricks_4.0.1-1.sif  pbrun haplotypecaller --batch  --ref ref.fa --in-bam /data/bam/ --out-variants /date/gvcf/  --gvcf
Please visit NVIDIA Clara - NVIDIA Docs for detailed documentation[E::hts_hopen] Failed to open file /data/bam/
[E::hts_open_format] Failed to open file “/data/bam/” : Is a directory
samtools view: failed to open “/data/bam/” for reading: Is a directory
[E::hts_hopen] Failed to open file /data/bam/
[E::hts_open_format] Failed to open file “/data/bam/” : Is a directory
samtools view: failed to open “/data/bam/” for reading: Is a directory
/usr/local/parabricks/binaries//bin/htvc /data/ref.fa /data/bam/ 2 -o /data/gvcf -nt 5
[PB Info 2023-Jul-03 00:37:15] ------------------------------------------------------------------------------
[PB Info 2023-Jul-03 00:37:15] ||                 Parabricks accelerated Genomics Pipeline                 ||
[PB Info 2023-Jul-03 00:37:15] ||                              Version 4.0.1-1                             ||
[PB Info 2023-Jul-03 00:37:15] ||                         GPU-GATK4 HaplotypeCaller                        ||
[PB Info 2023-Jul-03 00:37:15] ------------------------------------------------------------------------------
[PB Error 2023-Jul-03 00:37:15][src/haplotype_vc.cpp:854] Expected file “/data/bam/” to be of type bam, cram or text, exiting.Hi @bioinfor_worker ,sorry but --batch is deprecated. The documentation was not updated.If you would like to run on a batch style you can however.
You will need to create two files, one with the list of paths to your bam files (for example, bamList.txt) and one with the path to the generated vcf files (for example vcfList.txt).You can run with :
pbrun haplotypecaller --ref ref.fa --in-bam bamList.txt --out-variants vcfList.txtPlease let us know if you have any issue.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
126,fq2bam-sorting-by-queryname,"can we sort bam file by query name instead of coordinate?Hi,In the current version we cannot sort bam file by query name.
We are planning to include this on our future release.Best,
MyriemePowered by Discourse, best viewed with JavaScript enabled"
127,warning-across-different-models-of-device,"Try to run the Whole-Genome Variant Calling with Deepvariant on dgx-a100 station, and come aross some warning as follows:But it seems that does not effect the normally ending of the program.  I just would like to know if the warning bring some negative effection. Thanks.Hey @lkuang,As long as you are using the ampere version of Parabricks, then it’s fine. If you ran the installer using --ampere or selected “yes” when asked about ampere during the installation, then you are running the ampere version.Thanks @gburnett .  I use the ampere version.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
128,can-rtx-3090-use-parabricks-pipelines-trial-version,"I applied for a trial license and there was an error in operation, please help me

err1745×361 29.9 KB
Hi,Can you please provide more information :Best,
MyriemeExit the directory, it’s working, thank you
image1909×333 20.3 KB
Has the trial expired? Is it only one month? I see that the publicity on your official website says three months. Can it be extended? thank you!Hello @9003805,When you received your trial license, it was just for one month. The change to 3 months is very recent. You are welcome to apply for a new license, and that new license will be valid for 3 months. Cheers.Powered by Discourse, best viewed with JavaScript enabled"
129,how-to-use-deepvariant-by-rtx4090,"Hello. I tried DeevVariant by RTX4090
But I got error…
fq2bam is success.Please help me.Thanks you.Hi @yuusuke19951005 ,We are sorry but RTX4090 is not a supported GPU for DeepVariant.
Please see here all the supported GPUs.Regards,
MyriemeThank you.
Anyway, in the future, can RTX4090 handle Parabricks-DeepVariant ?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
130,troubleshooting-somatic-pipeline,"Hi there,I followed the instruction from the doc somatic pipeline and got the following error.
image1069×233 20.3 KB
The command you provided seems to be missing the parameter --out-normal-recal-file for the normal sample.Can you clarify?After I fixed the error by adding the parameter --out-normal-recal-file, now I can run the somatic pipeline normally and get a vcf file.Before vs After:

image1868×661 111 KB
Best regards,
TJ TsaiPowered by Discourse, best viewed with JavaScript enabled"
131,can-parabricks-run-on-the-jetson-agx-xavier-64gb,"I wonder if it is possible to run Parabricks on a Jetson AGX Xavier. I am aware that it is aarch64, but the memory available to the GPU cores is practically all free RAM on the system.Has anyone given it a try? We have several jetson units in the lab. It would ge a good development and prototyping system. Performance is secondary at this point.Hey @juergen.hench,To my knowledge no one has tried it, but I’d be curious to see if it works!Powered by Discourse, best viewed with JavaScript enabled"
132,the-mutectcaller-parabricks-accelerated-version-4-0-0-2,"Hi,I have a question about the Mutectcaller (Parabricks accelerated) version 4.0.0-2 on UK Biobank Research Analysis Platform.
In my project,  I’d like to do somatic variant calling of SNPs and INDELs with the UKB WES data using the Mutectcaller (Parabricks accelerated) on RAP. I’d like to run somatic variant calling using the Mutectcaller tool in tumor-only mode with a panel of normals (PON).Is Parabricks Mutectcaller from version 4.0.0.-2 support Panel of Normals to process variants? If yes, How to run this tool tumor-only mode with a panel of normals (PON)? I read this section in the documentation.https://docs.nvidia.com/clara/parabricks/4.0.0/documentation/tooldocs/man_mutectcaller.html#man-mutectcallerAs far as I understand, Firstly I should creat the input.pon and then I should run this tool in tumor-only mode with using input.pon file. If that is the case I have 2 questions.Question 1:  When I run this tool using sample of normal BAMs/CRAMs, I will not get just one output VCF file. Because this tool is just to convert to VCF, not merge. This being the case, How can I create the input.pon?Question 2: Let’s assume that I was able create input.pon file,  How will I select the VCF as input?This tool only accepts these file as input;ref file .bwa-index.tar.gz
interval file .bed
tumor .BAM/CRAM
tumor recal file
normal .BAM/CRAM
normal recal fileThank you very much in advance for helps.Powered by Discourse, best viewed with JavaScript enabled"
133,parabricks-4-0-0-1-illegal-instruction-core-dumped-in-haplotypecaller-step,"parabricks:4.0.0-1 , with nvidia/cuda:12.2.0-devel-ubuntu22.04;
system: Ubuntu20.04/ 512G memory/ 1 p100  16G card/ 38T disk space/It works perfectly on one of my workstation, but on  another, encountered the errors while in ‘haplotypecaller’ step:for i in cat list; do docker run     --gpus “device=0”       --rm       --volume $(pwd):/workdir       --volume $(pwd):/outputdir     nvcr.io/nvidia/clara/clara-parabricks:4.0.0-1     pbrun haplotypecaller      --ref /workdir/ref/hg38.fa      --gvcf       --in-bam /workdir/$i.bam       --out-variants /outputdir/$i.g.vcf; doneIllegal instruction (core dumped)
/usr/local/parabricks/binaries//bin/htvc /workdir/ref/hg38.fa /workdir/asp1.bam 1 -o /outputdir/asp1.g.vcf -nt 5 -g=========
Any suggestions? Thanks, everybody!Powered by Discourse, best viewed with JavaScript enabled"
134,unable-to-run-megatron-20b-gpt-model-on-nemo-q-amodel,"Dear NVIDIA Nemo team,We are planning to develop a question-answering chatbot based on Nemo. I noticed that NVIDIA provides pre-trained LLM model weights for Megatron 1.3b, Megatron 5b, and Megatron 20b(https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B,https://huggingface.co/nvidia/nemo-megatron-gpt-5B,https://huggingface.co/nvidia/nemo-megatron-gpt-20B). I would like to use our own custom dataset to train a QA model based on Megatron with Nemo. Could you please advise me on how to proceed?Thank you very much for your time and assistance.Best regards,
RaymondPowered by Discourse, best viewed with JavaScript enabled"
135,unstable-runtime-length-with-clara-parabricks-4-0-0-1,"Hello,I used two RTX4090s to run the clara-parabricks:4.0.0-1 program. At first, when running a sample, GPU-BWA mem and GPU-GATK4 HaplotypeCaller ran very fast, basically the running time was in ten minutes. This is a Satisfactory speed.
However, as the number of samples increases, its speed will gradually decrease, and the GPU occupancy rate of the corresponding program is also very low.It runs for hours, which is not ideal.
It’s not clear to me what is causing the difference in runtime. Actually, I might need a faster run because I have a lot of samples to process.Looking forward to getting your advice！
Best
yulongPowered by Discourse, best viewed with JavaScript enabled"
136,parabrick-somatic-pipeline-run-error,"Hi, teamWhen I run somatic pipelines on 1 GPU in a docker container, I see the error shown in an attachment.
error1199×545 150 KB
I have enough disk space, 5 TB to be precise. I indicated a path for temporary files. Could you please assist me in resolving this issue?Powered by Discourse, best viewed with JavaScript enabled"
137,can-parabricsk-haplotypecaller-generate-gvcf-gz-and-the-genotypegvcf-use-gvcf-gz,"parabricks 4.0.1, parabricsk haplotypecaller can’t generate gvcf.gz, but only gvcf, also the genotypegvcf .Parabricks haplotypecaller can generate gvcf.gz. Are you having issues generating one?Powered by Discourse, best viewed with JavaScript enabled"
138,genome-analysis-toolkit-on-cuda-boosting,"Dear all,For the COVID-19 study,
I have already choose  MD711-SU to build a Analysis tool.In order to speed up the results, I decided to use NVIDIA GPU on Genome Analysis Toolkit.If I want to install a GPU into this machine, can anyone recommend which  card I should choose ?Thank you !MD711-SU spec =>DFI’s MD711-SU is a high-performance fanless embedded box PC designed to work with medical equipment and all the accompanying demands. It supports high-end 6th Generation Intel® Core™ i7 processor and is built with high quality industrial-grade...Powered by Discourse, best viewed with JavaScript enabled"
139,an-http-error-was-returned-from-the-server,"Dear sir,We got a network update in our institute yesterday,and I found a http error when using pbrun.

Image 0531020×231 10.7 KB

If I go to http://myserver:7070, it shows:
{
“key” : “glsErr.restNoSuchApi”,
“message” : “No such REST API and METHOD combination supported: GET with uri=REDACTED for user (unknown)”,
“arguments” : [ “GET”, “uri=REDACTED”, “(unknown)” ]
}Powered by Discourse, best viewed with JavaScript enabled"
140,parabricks-v4-0-0-1-for-ampere,"Hi,Is there a dedicated version of Parabricks v4.0.0-1 for ampere devices? We ask as we are encountering an error very similar to this post, and yet it seems that there is no longer a dedicated ampere version for v4.0.0-1 (see the container installation page). Note that we are able to successfully run the same script in a v3.8.0-ampere container.Thanks,
OrianThere seems to be only an all-in-one version for both ampere & non-ampere GPU devices. Last week, I tested Germline pipeline on A100, A30 and T4 GPU devices, and all of them were passed.where:Powered by Discourse, best viewed with JavaScript enabled"
141,implentation-rdma-in-a-gstreamer-plugin,"Hello,I already created a plugin for a frame grabber card and I want to include rDMA to GPU into it. I was looking at the code of clara-holoscan-gstnvvideotestsrc to get an idea how to do this task.I see they are using gstnvdsbufferpool.h and nvbufsurface.h to implement this functionality, so i based my code on this example.
So far it looks like this to create a buffer pool of the size of nvbufSurface object. As i understand, this object will point to the actual memory on GPU.My question is : I can’t see where i can define the size of the actual buffer on the GPU. It’s not clear in the code from the example gstnvvideotestsrc. If you could point me in the right direction, I would really appreciate it.Thank you!The size of the allocated GPU buffers is determined internally by the gst_nvds_buffer_pool based on the buffer width and height specified in the caps (m_caps) that were provided to gst_buffer_pool_config_set_params. While you don’t explicitly set the buffer size, it can be retrieved from a buffer that has been allocated by this pool from the dataSize field in the mapped NvBufSurfaceParams.thank you for the answer.I have another question regarding how to write in GPU memory.I understand from the function :
Function gst_buffer_pool_acquire_bufferI can get a buffer that contain a  NvBufSurface and from this, as I understand, I could access the data
with surf->surface[0].dataPtrSo i’m wondering if i write at the location pointed by this pointer, would it be directly on the GPU? is it the right way of doing things?thank you!Powered by Discourse, best viewed with JavaScript enabled"
142,cuda-toolkit-12-0-0-cublaslt-cublas-stubs-libraries-requires-glibc,"Hello,I’m trying to use CUDA toolkit 12.0.0 and I got QA issue regarding the runtime dependency.Kindly see the log below.FYI, I don’t see anything specific that require GLIBC when checking the symbols needed by the libraries mentioned in the title.Attached file contains the complete logs.readelf_syms_libcublas.log (161.8 KB)
readelf_syms_libcublasLt.log (68.3 KB)Could you please explain why GLIBC is needed ?Any help would be much appreciated.Best regards,
IliesHi Ilies, apologies for the late reply, could you confirm the changes made to GitHub - nvidia-holoscan/meta-tegra-holoscan: OpenEmbedded/Yocto layer for NVIDIA Clara Holoscan MGX for building the Yocto recipe? As well as which compiler is used that produces the runtime issue?Regarding why GLIBC is needed:  after checking with CUDA experts, their pointer is that: RDEPENDS seems to document run-time package requirements in Yocto Linux. CUDA package should naturally depend on the libc package because many of its libraries require libc to function.Hey @jinlThanks for your replay.First of all, I’m not referring to CUDA toolkit 11.7 but CUDA toolkit 12.0Please check the difference between ./usr/local/cuda-11.7/targets/sbsa-linux/lib/stubs/libcublasLt.so and ./usr/local/cuda-12.0/targets/sbsa-linux/lib/stubs/libcublasLt.so and the need of the GLIBC in both case.GCC version 11.3 is usedRegards,
IliesHi @ilies.chergui,The dependency on GLIBC appeared during the rework of the stub libraries and was not intentional on its own: for some reason the previous UNDEF symbol printf was replaced with puts@GLIBC_2.17, likely due to the compiler update or some other change. Given that the actual cuBLAS library requires GLIBC anyways (please see the system requirements), this change should not be a problem.Powered by Discourse, best viewed with JavaScript enabled"
143,cnvkit-not-accelerated-at-all,"I successfully run CNVkit with Parabricks v3.6 on 2 GPU and I obtained some output.
However, it doesn’t seem to be accelerated at all for 2 reasons:Can you please help me in order to explain such discrepancies?
How should one expect to have CNVkit being accelerated? Do I miss some options, maybe?BTW, here is my command line:pbrun cnvkit --ref hg38.fa --in-bam myinput.bam --keep-tmp --tmp-dir /raid/tmp --generate-vcf --output-dir .Many thanksPowered by Discourse, best viewed with JavaScript enabled"
144,cant-pull-the-stated-most-recent-release-of-parabricks,"On the following webpage:
https://docs.nvidia.com/clara/parabricks/4.0.1/gettingstarted.html#getting-the-softwareThe command for getting the software is shown as:
docker pull nvcr.io/nvidia/clara/clara-parabricks:v4.0.1-1But it results in an error:
docker pull nvcr.io/nvidia/clara/clara-parabricks:v4.0.1-1
Error response from daemon: manifest for nvcr.io/nvidia/clara/clara-parabricks:v4.0.1-1 not found: manifest unknown: manifest unknownI found a similar topic on this forum from December 2022 saying that version 4.0.1-1 has not been released and the docs would be updated to reflect such.
Is that still the case?
Is version 4.0.0-1 still the most recent release?Hey Scott,The latest version is 4.0.1-1. If you remove the “v” from the version number, then you should be able to pull the image. Try this command:docker pull nvcr.io/nvidia/clara/clara-parabricks:4.0.1-1Thank you so much!  That was indeed the issue.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
145,parabricks-fq2bam-wrong-number-of-arguments-error,"Hi,After launching fq2bam with the following command:pbrun fq2bam  
–ref /projects/020-Aubergine-SNPdiscovery/Solanum_melongena_consortium/assembly/V3/Eggplant_V3_Chromosomes_plusChr0.fa 
–in-fq paired/B1303_1.fq.gz paired/B1303_2.fq.gz 
–out-bam paired/BWA_output/B1303.sort.pcr_rem.RG.bam 
–bwa-options M 
–read-group-sm B1303 
–read-group-lb 1 
–read-group-pl illumina 
–read-group-id-prefix B1303 
–tmp-dir paired/BWA_output/this is the output that I getting, where the last line contains the error.Any help would be appreciated!Thanks
KostasHey @alexiou5177,Can you run the command again, but add flags --x1 and --x3? This will (hopefully) give us some more verbose output to debug with.Thank you!Hi @gburnett,Thanks for your quick reply. I have included the --x1 --x3 tags and below you have the output. Note: is the format of the value of --bwa-options I am using, correct? At the help menu, you are mentioning that the accepted values are -M, -Y and -T. I have used the first option but as “M” and not as “-M” since it was the only one working. I am mentioning this just in case I am doing a stupid error and this generates downstream problems.KostasHey Kostas,I would try using -M instead of M for the bwa options, as outlined in the documentation below:
image935×65 2.23 KB
Hi @gburnett,Using either --bwa-options -M or --bwa-options '-M' gives an error of a missing argument for --bwa-optionspbrun: error: argument --bwa-options: expected one argumentcommand:pbrun fq2bam --x1 --x3 --ref /projects/020-Aubergine-SNPdiscovery/Solanum_melongena_consortium/assembly/V3/Eggplant_V3_Chromosomes_plusChr0.fa --in-fq paired/B1303_1.fq.gz paired/B1303_2.fq.gz --out-bam paired/BWA_output/B1303.sort.pcr_rem.RG.bam --bwa-options -M --read-group-sm B1303 --read-group-lb 1 --read-group-pl illumina --read-group-id-prefix B1303 --tmp-dir paired/BWA_output/KostasHi Kostas,can you please try --bwa-options=-MBestIt’s working!!Maybe it will be useful for the users to add this detail about the format in the help section.Thanks a lot,
KostasHappy to hear that!Hi @gburnett,One last question related to fq2bam. When I try to launch the tool on the same cluster node for another sample, I get the following error:FATAL:   while loading overlay images: failed to open overlay image /opt/rh7/parabricks-20201117/parabricks/pb-overlay.img: while locking ext3 partition from /opt/rh7/parabricks-20201117/parabricks/pb-overlay.img: can’t open /opt/rh7/parabricks-20201117/parabricks/pb-overlay.img for writing, currently in use by another processAny ideas as a workaround for running multiple samples?Thanks,
KostasAre you using singularity?yesTo solve this error please reinstall the software using the option  --disable-fakeroot .Best
MyriemeThanks, Myrieme. I will give it a try.Best
KostasPowered by Discourse, best viewed with JavaScript enabled"
146,torchani,"TorchANI is a PyTorch implementation of ANI(Accurate NeurAl networK engINe for Molecular Energies), created and maintained by the Roitberg group. TorchANI contains classes like AEVComputer, ANIModel, and EnergyShifter that can be pipelined to compute molecular energies from the 3D coordinates of molecules. It also include tools to: deal with ANI datasets(e.g. ANI-1, ANI-1x, ANI-1ccx, ANI-2x) at torchani.data , import various file formats of NeuroChem at torchani.neurochem, and more at torchani.utils.Learn MorePowered by Discourse, best viewed with JavaScript enabled"
147,can-this-run-on-an-rtx-3090-with-the-latest-drivers-invalid-device-symbol-error,"Hi,I’ve followed the quick start guide, installed everything successfully and installed the latest 455 driver on Ubuntu 20.04 server.When I try to run the sample in the quick start I get the following:Hey @robmpreston,Which GPU are you trying to run on? A100 by any chance?On the RTX 3090… when will this be supported?Thanks will try.Can you paste the exact error and the command you ran?@gburnetthmm… if you are on a trial license, it may have expired. the issue doesn’t seem to be related to your hardware, but to connecting to the Parabricks registry.I have the same error, but my license just started from 3 days ago
“”""
GPU-BWA mem
ProgressMeter	Reads		Base Pairs Aligned
cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:148 : invalid device symbol
Please contact Parabricks-Support@nvidia.com for any questions
There is a forum for Q&A as well at Clara Parabricks - NVIDIA Developer Forums
Exiting…Could not run fq2bam as part of germline pipeline
Exiting pbrun …
“”""
please help me! I ran on server AMD ram 64GB, 16 threads, Nvidia 3090Did you use --ampere flag with installer ?When I posted this it didn’t exist, but thats what I use now :)Powered by Discourse, best viewed with JavaScript enabled"
148,parabricks-4-0-0-documentation-not-accessible,"Unable to access the Documentation of Parabricks 4.0.0. Getting page not found error: https://docs.nvidia.com/clara/parabricks/4.0.0/GettingStarted.htmlHi @prema,Thanks for bringing this to our attention. I have the team looking into this issue now.
Thanks for your patience while we work on this.TomThanks for point this out. Sorry it didn’t work.
Try this URL : Clara Parabricks v4.0.0BTW where did you get the URL that you used was it on one of our web pages ?On this page you put above , there are some other link, such as :https://docs.nvidia.com/clara/parabricks/4.0.0/Tutorials.html, etc. but I got an error: “Page Not Found.This page no longer exists. It is a non-supported format.” Please check that. Thank you !HI @zhenglin,We are still working on the issue. I will post the correct links here once this has been fixed.Thanks,
TomHello, @prema and @zhenglin,Please try the links now. Let me know if you still have issues accessing the pages.Thanks,
TomPowered by Discourse, best viewed with JavaScript enabled"
149,run-parabricks-and-found-cudamemgetinfo-returned-802,"I’m run parabricks with sample data and found
cudaMemGetInfo returned 802
 → system not yet initializedThis command I run
pbrun fq2bam --ref parabricks_sample/Ref/Homo_sapiens_assembly38.fasta --in-fq parabricks_sample/Data/sample_1.fq.gz parabricks_sample/Data/sample_2.fq.gz --out-bam output.bam  --gpu-devices 0,1,2,3,4,5,6,7 --num-gpus 1cudaMemGetInfo returned 802
 → system not yet initialized
For technical support, updated user guides and other Parabricks documentation can be found at
Answers to most FAQ’s can be found on the developer forum
Customers with paid Parabricks licenses have direct access to support and can contact EnterpriseSupport@nvidia.com
Users of free evaluation licenses can contact parabricks-eval-support@nvidia.com for troubleshooting any questions.
Exiting…Could not run fq2bam
Exiting pbrun …Hey @user120120,It seems like your CUDA environment might not be set up properly. To test this with Docker you can run:docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smiand it should return the output for nvidia-smi.Can you try this and let me know what comes back?Thanks!I’m install parabricks with singularity 3.7.1-5.1.ohpc.2.1 with this commandHere’s output from “docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi”[root@c0 ~]# docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
Unable to find image ‘nvidia/cuda:11.0-base’ locally
11.0-base: Pulling from nvidia/cuda
54ee1f796a1e: Pull complete
f7bfea53ad12: Pull complete
46d371e02073: Pull complete
b66c17bbf772: Pull complete
3642f1a6dfb3: Pull complete
e5ce55b8b4b9: Pull complete
155bc0332b0a: Pull complete
Digest: sha256:774ca3d612de15213102c2dbbba55df44dc5cf9870ca2be6c6e9c627fa63d67a
Status: Downloaded newer image for nvidia/cuda:11.0-base
Sun Jan  9 12:01:48 2022
±----------------------------------------------------------------------------+
| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
|-------------------------------±---------------------±---------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM…  Off  | 00000000:07:00.0 Off |                    0 |
| N/A   29C    P0    57W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   1  NVIDIA A100-SXM…  Off  | 00000000:0B:00.0 Off |                    0 |
| N/A   30C    P0    58W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   2  NVIDIA A100-SXM…  Off  | 00000000:48:00.0 Off |                    0 |
| N/A   28C    P0    54W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   3  NVIDIA A100-SXM…  Off  | 00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0    58W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   4  NVIDIA A100-SXM…  Off  | 00000000:88:00.0 Off |                    0 |
| N/A   28C    P0    57W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   5  NVIDIA A100-SXM…  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   30C    P0    58W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   6  NVIDIA A100-SXM…  Off  | 00000000:C8:00.0 Off |                    0 |
| N/A   28C    P0    57W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   7  NVIDIA A100-SXM…  Off  | 00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0    57W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+±----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
±----------------------------------------------------------------------------+and this from host[root@c0 ~]# nvidia-smi
Sun Jan  9 19:14:10 2022
±----------------------------------------------------------------------------+
| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
|-------------------------------±---------------------±---------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM…  Off  | 00000000:07:00.0 Off |                    0 |
| N/A   29C    P0    57W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   1  NVIDIA A100-SXM…  Off  | 00000000:0B:00.0 Off |                    0 |
| N/A   30C    P0    58W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   2  NVIDIA A100-SXM…  Off  | 00000000:48:00.0 Off |                    0 |
| N/A   28C    P0    54W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   3  NVIDIA A100-SXM…  Off  | 00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0    58W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   4  NVIDIA A100-SXM…  Off  | 00000000:88:00.0 Off |                    0 |
| N/A   28C    P0    58W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   5  NVIDIA A100-SXM…  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   31C    P0    58W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   6  NVIDIA A100-SXM…  Off  | 00000000:C8:00.0 Off |                    0 |
| N/A   29C    P0    57W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+
|   7  NVIDIA A100-SXM…  Off  | 00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0    57W / 400W |      0MiB / 81251MiB |      0%      Default |
|                               |                      |             Disabled |
±------------------------------±---------------------±---------------------+±----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
±----------------------------------------------------------------------------+Hey @user120120,It looks like all the GPUs are disabled on the machine for some reason. I would talk to the system admin about that.It’s also possible that the GPUs are running in MIG mode (multi-instance GPU), which is not supported in Parabricks and can lead to issues. So I would say your plan of attack is:I check in my host, MIG mode are disable

nvidia-smi494×830 60.5 KB
I reinstall parabricks and run, it’s same errorI test on T4 card host  parapricks can running
but on  ampere  card it’s not running
pbrun1874×600 107 KB
Hey @user120120,I’m not entirely sure what’s going on here. Can you try running on another GPU? I notice that you’re restricting to GPU 0 in your pbrun command, can you try tunning on GPU 1, or another one? It could be something wrong with that 1 GPU. Everything else that you’re doing seems correct.I got it.
I install nvidia-frabicmanager  for manage nvswitch multi GPUand run parabricks again
[root@c0 ~]# /opt/parabrick/pbrun fq2bam --ref parabricks_sample/Ref/Homo_sapiens_assembly38.fasta --in-fq parabricks_sample/Data/sample_1.fq.gz parabricks_sample/Data/sample_2.fq.gz --out-bam output.bam --gpu-devices 0,1,2,3,4,5,6,7 --num-gpus 8
Please visit https://docs.nvidia.com/clara/#parabricks for detailed documentation[Parabricks Options Mesg]: Checking argument compatibility
[Parabricks Options Mesg]: Automatically generating ID prefix
[Parabricks Options Mesg]: Read group created for /root/parabricks_sample/Data/sample_1.fq.gz and
/root/parabricks_sample/Data/sample_2.fq.gz
[Parabricks Options Mesg]: @RG\tID:HK3TJBCX2.1\tLB:lib1\tPL:bar\tSM:sample\tPU:HK3TJBCX2.1
[PB Info 2022-Jan-13 14:54:37] Logger not initialized!
[PB Info 2022-Jan-13 14:54:37] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:54:37] ||                 Parabricks accelerated Genomics Pipeline                 ||
[PB Info 2022-Jan-13 14:54:37] ||                          Version 3.7.0-1.ampere                          ||
[PB Info 2022-Jan-13 14:54:37] ||                       GPU-BWA mem, Sorting Phase-I                       ||[PB Info 2022-Jan-13 14:54:37] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:54:39] Logger already initialized, continuing with current settings.
[M::bwa_idx_load_from_disk] read 0 ALT contigs
[PB Info 2022-Jan-13 14:54:40] GPU-BWA mem
[PB Info 2022-Jan-13 14:54:40] ProgressMeter	Reads		Base Pairs Aligned
[PB Info 2022-Jan-13 14:54:57] 5043564		510000000
[PB Info 2022-Jan-13 14:55:00] 10087128	1190000000
[PB Info 2022-Jan-13 14:55:02] 15130692	1710000000
[PB Info 2022-Jan-13 14:55:05] 20174256	2340000000
[PB Info 2022-Jan-13 14:55:07] 25217820	2810000000
[PB Info 2022-Jan-13 14:55:10] 30261384	3500000000
[PB Info 2022-Jan-13 14:55:13] 35304948	4070000000
[PB Info 2022-Jan-13 14:55:15] 40348512	4710000000
[PB Info 2022-Jan-13 14:55:18] 45392076	5250000000
[PB Info 2022-Jan-13 14:55:21] 50435640	5850000000
[PB Info 2022-Jan-13 14:55:30]
GPU-BWA Mem time: 49.923947 seconds
[PB Info 2022-Jan-13 14:55:30] GPU-BWA Mem is finished.[main] CMD: PARABRICKS mem -Z ./pbOpts.txt /root/parabricks_sample/Ref/Homo_sapiens_assembly38.fasta /root/parabricks_sample/Data/sample_1.fq.gz /root/parabricks_sample/Data/sample_2.fq.gz @RG\tID:HK3TJBCX2.1\tLB:lib1\tPL:bar\tSM:sample\tPU:HK3TJBCX2.1
[main] Real time: 51.271 sec; CPU: 1516.921 sec
[PB Info 2022-Jan-13 14:55:30] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:55:30] ||        Program:                      GPU-BWA mem, Sorting Phase-I        ||
[PB Info 2022-Jan-13 14:55:30] ||        Version:                                    3.7.0-1.ampere        ||
[PB Info 2022-Jan-13 14:55:30] ||        Start Time:                       Thu Jan 13 14:54:37 2022        ||
[PB Info 2022-Jan-13 14:55:30] ||        End Time:                         Thu Jan 13 14:55:30 2022        ||
[PB Info 2022-Jan-13 14:55:30] ||        Total Time:                                     53 seconds        ||
[PB Info 2022-Jan-13 14:55:30] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:55:31] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:55:31] ||                 Parabricks accelerated Genomics Pipeline                 ||
[PB Info 2022-Jan-13 14:55:31] ||                          Version 3.7.0-1.ampere                          ||
[PB Info 2022-Jan-13 14:55:31] ||                             Sorting Phase-II                             ||
[PB Info 2022-Jan-13 14:55:31] ||                  Contact: Parabricks-Support@nvidia.com                  ||
[PB Info 2022-Jan-13 14:55:31] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:55:32] progressMeter - Percentage
[PB Info 2022-Jan-13 14:55:32] 0.0	 0.00 GB
[PB Info 2022-Jan-13 14:55:42] Sorting and Marking: 10.000 seconds
[PB Info 2022-Jan-13 14:55:42] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:55:42] ||        Program:                                  Sorting Phase-II        ||
[PB Info 2022-Jan-13 14:55:42] ||        Version:                                    3.7.0-1.ampere        ||
[PB Info 2022-Jan-13 14:55:42] ||        Start Time:                       Thu Jan 13 14:55:31 2022        ||
[PB Info 2022-Jan-13 14:55:42] ||        End Time:                         Thu Jan 13 14:55:42 2022        ||
[PB Info 2022-Jan-13 14:55:42] ||        Total Time:                                     11 seconds        ||
[PB Info 2022-Jan-13 14:55:42] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:55:42] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:55:42] ||                 Parabricks accelerated Genomics Pipeline                 ||
[PB Info 2022-Jan-13 14:55:42] ||                          Version 3.7.0-1.ampere                          ||
[PB Info 2022-Jan-13 14:55:42] ||                         Marking Duplicates, BQSR                         ||
[PB Info 2022-Jan-13 14:55:42] ||                  Contact: Parabricks-Support@nvidia.com                  ||
[PB Info 2022-Jan-13 14:55:42] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:55:43] progressMeter -	Percentage
[PB Info 2022-Jan-13 14:55:53] 100.0	 0.00 GB
[PB Info 2022-Jan-13 14:55:53] BQSR and writing final BAM:  10.035 seconds
[PB Info 2022-Jan-13 14:55:53] ------------------------------------------------------------------------------
[PB Info 2022-Jan-13 14:55:53] ||        Program:                          Marking Duplicates, BQSR        ||
[PB Info 2022-Jan-13 14:55:53] ||        Version:                                    3.7.0-1.ampere        ||
[PB Info 2022-Jan-13 14:55:53] ||        Start Time:                       Thu Jan 13 14:55:42 2022        ||
[PB Info 2022-Jan-13 14:55:53] ||        End Time:                         Thu Jan 13 14:55:53 2022        ||
[PB Info 2022-Jan-13 14:55:53] ||        Total Time:                                     11 seconds        ||
[PB Info 2022-Jan-13 14:55:53] ------------------------------------------------------------------------------Powered by Discourse, best viewed with JavaScript enabled"
150,out-of-memory-errors-running-pbrun-fq2bam-through-singularity-on-a100s-via-slurm,"Hello,I am trying to run fq2bam through pbrun on my university’s HPC, which uses slurm scheduling, but it is failing with CUDA out-of-memory errors.I am using 4 A100s and 256gb system ram, which I ask for in the slurm script like so:In this script, I double-check that the GPUs are accessible, with nvidia-smi, which produces this:The rest of my script loads singularity and tries to run pbrun fq2bam (the variables in the singularity call are all to accurate file paths)Unfortunately, the job fails with the following:I’m not sure what the issue is since I’ve requested a large amount of memory. I’ve tried reducing tasks (cpus), using 2 instead of 4 GPUs, etc., but haven’t figured it out. Thanks in advance for your help!Hey @chaco001,I can see that the GPUs are active on your node, but it seems like they’re not accessible inside the singularity image. I would try running nvidia-smi inside the Singularity image to see if that works. If it does, then I would try to run your Parabricks job. If not, there may be some missing flags in the singularity run command.Thank you for your help! Two things. 1. I think I tried running nvidia-smi correctly within the image. I did this interactively on the node after asking for only two GPUs. Does this information suggest the GPUs are accessible within singularity?Powered by Discourse, best viewed with JavaScript enabled"
151,human-par-pipeline-error,"I try with Clara Parabricks v3.6.1 free trial version. I found “UnicodeDecodeError: ‘utf-8’ …” when using Human_PAR pipeline on haplotypecaller step. The error messages as below :
Traceback (most recent call last):
File “/parabricks/run_pipeline.py”, line 7, in 
sys.exit(PB.pb_main())
File “PB.pyx”, line 1710, in PB.pb_main
File “/parabricks/pbargs_check.py”, line 550, in pbargs_check
check_haplotypecaller(runArgs.runArgs)
File “/parabricks/pbargs_check.py”, line 179, in check_haplotypecaller
check_human_par(runArgs)
File “/parabricks/pbargs_check.py”, line 76, in check_human_par
chrom_reads = f.readlines()
File “/usr/lib/python3.7/codecs.py”, line 322, in decode
(result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xc5 in position 26: invalid continuation byteCould not run haplotypecaller as part of human par germline pipeline
Exiting pbrun …The Germline pipeline is worked fine. Anyone have same problem ?ThanksHey @chumpol.nga,Can you send the command that you ran to generate this error? On first glance, it looks like something could be corrupted in one of your input files. Have you checked for that?Thank youHi @gburnett,
I have test on my data and also the example file from Parabricks. The command that I used as follow :pbrun human_par --ref Homo_sapiens_assembly38.fasta --tmp-dir /raid/scratch --in-fq sample_1.fq.gz sample_2.fq.gz “@RG\tID:sample1\tLB:lib1\tPL:PL1\tSM:sample1\tPU:unit1” --knownSites Homo_sapiens_assembly38.known_indels.vcf.gz --range-male 1-10 --range-female 150-250 --out-bam sample1.cram --gvcf --out-variants sample1.par.g.vcf.gz --out-recal-file sample1.recal.txt 2>&1 | tee sample1_PAR_output.logI also attached the output in this email.Regards,
Chumpolsample1_PAR_output.log (9.65 KB)Hey @chumpol.nga,I am still working on debugging this. I ran the code and got a different error. I will talk to the engineering team about this.Powered by Discourse, best viewed with JavaScript enabled"
152,clara-agx-developer-kit-external-memory,"Hello,Since the CLARA HDD is only 32GB, can you please help with the best extension possibilities? Are there SATA drives available which can be used? Or SDD external drives are better?Regards,MaheshHello,The Clara AGX devkit current comes with a 250 GB SATA SSD installed.Thank you. This helped as we needed to mount it separately.Regards,MaheshThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
153,bwa-version-0-7-15-in-parabrick,"Hi, I am using parabrick pipeline for analysing whole genome data. However, I noticed that the bwa version used in the pipeline is old (0.7.15) as 0.7.17 is available since 2017. Is there any specific reason for using 0.7.15 and not 0.7.17. Will it impact downstream analysis (variant calling etc)?Hey @a.farswan,BWA generates the same exact BAM file for all versions starting with 0.7.15 so there should be no effect on your downstream analysis.Powered by Discourse, best viewed with JavaScript enabled"
154,input-for-starfusion,"Hi. I wanted to try running the starfusion function of Parabricks. However, I can’t seem to find any input file for the “–chimeric-junction” option. I don’t have any “Chimeric.out.junction”/“sample.out.junction” file after having run rna_fq2bam. Is there an option I need to set to get it or am I supposed to get it another way?Best regards
HaraldPowered by Discourse, best viewed with JavaScript enabled"
155,computer-vision-developer-day-monday-2-28-1145-a-m-pacific,"Attention All Computer Vision Developers:This coming Monday, February 28th, between 11:00-11:45 A.M. Pacific we are holding our First Computer Vision Developer Day of 2022 ahead of Graphics Processing Unit Technology Conference (GTC) '22. We will have members from NVIDIA developer relations, inception, and product teams in attendance that want to hear from you: the computer vision developer.Here is the agenda:Direct message me if you are interested in attending. If you have questions or have a specific project or need you are interested in discussing, please email cv-dev@nvidia.com. Until then, feel free to check out our refreshed Computer Vision Solutions Landing Page.Best,MikeB_NVPowered by Discourse, best viewed with JavaScript enabled"
156,bionemo-early-access,"BioNeMo Framework and Cloud Service are now available for early access.At GTC Fall 2022, NVIDIA introduced BioNeMo, an AI-powered drug discovery large language model framework for training and deploying large biomolecular transformer AI models at supercomputing scale.Shortly after GTC, we opened the BioNeMo framework for running on your own infrastructure to a select group of customers. Now, we are opening both the BioNeMo framework and cloud service for early access by application.To apply for access, please fill out the questionnaire using the link below. You will be prompted to join the NVIDIA developer program and the entire process should take roughly 5 minutes.Apply Now. .Hi, I applied to BioNeMo’s early access, do you know the timescale for approval to test the platform? ThanksI’ve applied as well and wondering when I may hear back as to yes or no.Thanks,
Warren EnglishPowered by Discourse, best viewed with JavaScript enabled"
157,monai-core-community,"Check out the MONAI Core community forum for support: Issues · Project-MONAI/MONAI · GitHubPowered by Discourse, best viewed with JavaScript enabled"
158,v4-0-half-of-standalone-tools-disappeared,"Hi there,I’m an engineer maintaining Parabricks images in our customers’ cloud infrastructure. I am studying the differences between v3.8 and v4.0, and I found that half of the standalone tools disappeared from v3.8 to v4.0, such as cnvkit, vcfqc, muse etc. in v3.8. Are these tools no longer supported, and should we tell our customers to use old Parabricks (with license server)?
0011847×720 216 KB


0021845×798 185 KB
I saw somewhere mentioned by the NVIDIA team that they would focus on fewer core tools since v.4. I think this is good for keeping timely porting and implementation of core tools. Yet, I hope the NVIDIA team can make the “legacy” tools continuously available in a release that is not requiring the license setting as they were in the previous  v.3.8 release.Powered by Discourse, best viewed with JavaScript enabled"
159,not-enough-available-memory-for-these-gpus,"Today, I received a 1 month trial account of parabricks and was happy to test it out. My computer’s specs are
Main Memory: 256 GB
Graphic cards: NVIDIA RTX 3070 (8GB GPU memory) x2But when I ran fq2bam,Not enough available memory for these GPUs…It stops with this error message.
I checked again and found that among the minimum hardware requirements
GPU memory is 12GB or more.Isn’t this too much of a minimum requirement?
Does the Parabricks team have any plans to relax this minimum requirement?I agree with @hermes2014, I stumbled upon the same issue, using HaplotypeCaller option.
I have the exact same problem, using the exact same card.But as said 12Gb GPU seems hard-to-reach requirements. Is this really needed from the tools?
Seems that there could be a “buffer-size” option to store less reads in memory, or for smaller genomes (I’m working with small non-human genomes).Thanks in advanceI ran into the exact same issue here. And the situation is confusing:
(1). When I tried using an RTX 3080 Ti with 12043 MB available memory, Parabricks refuses to run, indicating “not enough free memory for these GPUS”.
(2). However, when I replaced the RTX 3080 Ti with an RTX 3060, with the exact same amount of memory, Parabricks runs without any problem.I would love to know what could possibly be the problem? Is it really a memory capacity issue?Parabricks needs 12 GB of available memory to run. It may be the case that, at runtime, there weren’t 12 GB available.That said, we don’t support running Parabricks on GeForce, not do we support running Parabricks on 1 GPU.ng Parabricks on GeForce, not do we support running Parabricks on 1 GPU.Thank you for your clarification!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
160,parabricks-haplotypecaller-options,"Hi, I am having trouble specifying --haplotypecaller-options.
How do you pass this argument?
I want to set the -standard-min-confidence-threshold-for-calling 20. But I can’t seem to get it to work. Documentation says pass it as a string. Can you give me an example?
I tried every possibility that I can think of.
eg: --haplotypecaller-options -standard-min-confidence-threshold-for-calling 20
–haplotypecaller -standard-min-confidence-threshold-for-calling 20
–haplotypecaller standard-min-confidence-threshold-for-calling 20After reading another post, I tried setting it like this;–haplotypecaller-options=-standard-min-confidence-threshold-for-calling 20It gives me this error now.pbrun: error: unrecognized arguments: 20Hi nwijewardena,Can you please try using :–haplotypecaller-options=”-standard-min-confidence-threshold-for-calling 20”As explained in the documentation :
https://docs.nvidia.com/clara/parabricks/v3.2/text/variant_callers.html#haplotypecallerBest
MyriemePowered by Discourse, best viewed with JavaScript enabled"
161,gtc-2019-on-demand,"Were you able to attend all the sessions at GTC 2019 in San Jose?Not to worry. We know there were over 500 sessions and it’s actually impossible to attend every single one. We also know that not everyone is able to attend in person.That is why we are now making all of the recorded sessions available to members of our developer program.Click here to go to our GTC On-Demand site where you can search by speaker, keyword, Session ID, Topic, …Then click on the “>Watch Now” link to start streaming.
Note that you must be logged in to watch the recording.Powered by Discourse, best viewed with JavaScript enabled"
162,request-for-parabricks-tar-gz-file,"Hi, I would like to request for parabricks.tar.gz file to test DeepVariant and other variant callers with parabricks. I tried to follow the instruction to request access from https://developer.nvidia.com/clara-parabricks, yet I don’t see any interface that I can request for the access. Could you advice where and how could I access to parabricks.tar.gz file? Thanks!Hi,please use this link to request a trial license.Cheers,
MyriemePowered by Discourse, best viewed with JavaScript enabled"
163,panel-of-normal-in-parabrick-mutect2-caller,"I am using Parabrick to extract somatic mutations in WES data while giving the following command, I am getting an error of Unknown argument specified ""-pon"".The command and the arguments are as follows:With the above command, I am passing an addtional supported mutect2 arguments like panel of normal, allow all possible annotations etc. But this is not working and getting an error of Unknown argument specified. Kindly suggest. Thanks.Hi vivekr,The only supported  mutect2 argument is -pcr-indel-model <NONE, HOSTILE, AGGRESSIVE, CONSERVATIVE>https://docs.nvidia.com/clara/parabricks/v3.2/text/variant_callers.html#mutectcallerBest,
MyriemeOk. Thanks.Powered by Discourse, best viewed with JavaScript enabled"
164,want-to-run-parabricks-toolkit-on-a-local-system-not-on-a-server-for-genome-analysis-practice-please-help-and-guide,"Hi,
I am a researcher, and I want to try Nvidia Parabricks toolkit on my system. I have previously executed GATK4 on my system, but now want to try it using GPU. Kindly help me to start with it. System configuration is:I do not want to run full scale Genome sequence right now. For the time being, I need to familiarise myself with the basics. So, help will be highly appreciated.Thanks in advance.Unfortunately you’re out of luck with a GTX 1660 Ti, or in fact any GeForce card at this stage. You only really have two options as far as I know:either buy compatible GPUs - so Tesla P100, V100, T4, or I’m assuming the A100 now as well.using GPU enabled cloud images (i.e. via AWS)I’m hoping we’ll see Nvidia provide the option for non-Tesla/A100 grade GPUs to be used in the future, especially since the 3000 series that is selling now (or sold out everywhere :-) ) is such a leap in ‘power’. We have a couple of Tesla V100 cards which we use for Parabricks and Nanopore base calling, and I’m very interested to see that the 3080/3090 actually have more CUDA cores than our super expensive server grade cards.Hey miles.benton84,Thanks for your reply. I can understand that it is not possible to execute Parabricks on my native machine. If you could help, then could you please tell me if it is possible to execute the pipeline on Google Colab Pro, as they also provide high end NVIDIA GPUs and in a much affordable cost.ThanksPowered by Discourse, best viewed with JavaScript enabled"
165,issue-running-tensorrt-demos-on-clara-agx-within-docker-pytorch-container,"I am using a Clara AGX developer kit, and I am trying to run a TensorRT demo - specifically, the diffusion demo at this link: TensorRT/demo/Diffusion at main · NVIDIA/TensorRT · GitHub.I am launching the NGC container using docker as instructed in the Git.
However, when I try to build the plugin libraries—specifically, running: make -j$(nproc), I run into the error:
fatal error: cuda_runtime_api.h: No such file or directoryCuda seems to work, as the nvidia-smi output is as expected:

Screen Shot 2023-02-14 at 4.53.31 PM1374×724 68.5 KB
I’m not sure how to resolve this issue. For additional context, I setup the Clara AGX using the SDK Manager.Hello there! Sorry for the late reply.Are you able to find cuda_runtime_api.h on bare metal?I just tried to follow the steps in TensorRT/demo/Diffusion at main · NVIDIA/TensorRT · GitHub skipping the part "" (Optional) Install latest TensorRT release"", and could finish running make -j$(nproc) without the error.
I find the file on bare metal at: /usr/local/cuda-11.6/targets/sbsa-linux/include/cuda_runtime_api.h on the Clara AGX devkit, and at /usr/local/cuda-11.8/targets/sbsa-linux/include/cuda_runtime_api.h within the launched container.No worries—not sure what the issue was earlier, but I followed the steps again and I am able to build successfully. I am able to locate the necessary cuda files on bare metal.However, upon trying to launch the model (>python3 demo-diffusion.py --help) after building, I’ll receive the error: “ModuleNotFoundError: No module named ‘cuda’”, so it seems a cuda issue persists.Good to hear you got past the initial issue! I would suggest to raise the issue on one of the CUDA forums CUDA - NVIDIA Developer ForumsThanks, I’ll raise an issue there.
Prior to launching the model, I’m getting the following error when trying to install requirements.txt in the container:ERROR: Could not find a version that satisfies the requirement torch==1.12.1+cu116 (from versions: 1.8.0, 1.8.1, 1.9.0, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1)
ERROR: No matching distribution found for torch==1.12.1+cu116I’ve tried installing this requirement directly as instructed by the PyTorch website in the container, like so:pip3 install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116However, I get the same ‘No matching distribution error’. I’ll raise an issue on the CUDA forum about this, but just thought I’d mention this here. Thanks!Oh I see, the torch installation on arm+dGPU could be a little tricky, I will ask around and see if someone has the install recipe. In the meanwhile, one thing that you could try is using the PyTorch base image nvcr.io/nvidia/pytorch:22.10-py3 instead of the TRT base image nvcr.io/nvidia/tensorrt:22.10-py3. The PyT base image supports both x86 and arm64, you could see the details here PyTorch | NVIDIA NGC@rchand18 Following up on the previous message: the existing pip wheels would not work for the Clara AGX devkit, since their arm builds support Mac M1/M2. For using Pytorch on the devkit, you could build PyTorch from source, or use the NGC PyT container.I tried following the same steps but in the PyTorch base image, but i still get the sameERROR: No matching distribution found for torch==1.12.1+cu116issue. I’ll try building from source and see if it yields different results.That error is likely because the 22.10 image has a higher CUDA/Pytorch version than 11.6/1.12.1. Please see PyTorch Release 22.10 for the software versions in each Docker image release. Perhaps you could try an earlier version of the PyTorch Docker image although that may require you to use an earlier version of the TensorRT Diffusion demo repo. Building from source can be a good option.Powered by Discourse, best viewed with JavaScript enabled"
166,nemo-llm,"Is there a step by step tutorial on how to connect NeMo LLM with set of filesPowered by Discourse, best viewed with JavaScript enabled"
167,bwa-alternate-aware-alignment-with-parabrick,"Hi, may I know if the bwa-mem alignment software can be made to be alternate contigs aware?
The topic is covered in GATK4 belowThis exploratory tutorial provides instructions and example data to map short reads to a reference genome with alternate haplotypes. Instructions are suitable for indexing and mapping reads to GRCh...Regards
SolyrisPowered by Discourse, best viewed with JavaScript enabled"
168,where-can-i-find-free-compute-resources,"For fighting COVID-19, what are some free compute resources that can be used with Parabricks?For fighting COVID-19,  here are some free compute resources that are available:
AWS Diagnostic Development Initiative 
E2E Free Cloud GPU against COVID 
Oracle’s Machine Image Hi there. Interesting thread. Thanks for the useful information.Hello all,
Can we execute Parabricks Pipeline on Google Colab?
ThanksGoogle collab is not a platform that we have tested or currently support for Parabricks. We may consider Google collab support in the future but not at the current time. However, if collab provides a VM that meets the minimum requirements for PB to run (as per documentation), then feel free to pursue that as an option. We would love to learn about your experience.Thank you!Powered by Discourse, best viewed with JavaScript enabled"
169,struggling-to-produce-identical-results-between-parabricks-fq2bam-with-apptainer-and-bwa-and-gatk,"Hi,I’ve been testing Parabricks on our local HPC (with A30s) and comparing it to bwa and gatk following the instructions in the links below:
https://docs.nvidia.com/clara/parabricks/4.0.0/Documentation/ToolDocs/man_fq2bam.html#man-fq2bamI’ve been using the example dataset recommended by the tutorial.Since I’m testing on an HPC environment, I’ve been pulling the Parabricks container with apptainer 1.1.0:the Slurm run script I’m using is:If I compare the final bam files with du, I get:and the recommended bam diff command doesn’t return anything.Comparing the bqsr reports:The final results are similar, but not identical as claimed on the documentation. Is there something that I’m doing wrong?Hey @edoy,You have done everything right. There might be slight differences based on software versions. Parabricks 4.0.0-1 is compatible with:BWA mem:          0.7.15
Picard:                 2.18.25What did you use to run your CPU comparison?Thanks@gburnett
Apologies for the lack of follow up.I managed to confirm the output bam files were functionally identical using the picard compareSams tool. But the bqsr reports are nevertheless different.I’m using the bwa and gatk cpu equivalent version specified in the documentation:
https://docs.nvidia.com/clara/parabricks/4.0.0/Documentation/ToolDocs/CompatibleCpuSoftwareVersions.htmlPowered by Discourse, best viewed with JavaScript enabled"
170,error-in-merging-variant-callsets-from-multiple-callers-into-one-vcf-file,"Here is my cmd used:Output is like this:

image841×190 33.6 KB
Then I use bgzip to get HG002.hiseqx.pcr-free.30x.pb.deepvariant.vcf.zp and HG002.hiseqx.pcr-free.30x.pb.haplotypecaller.vcf.gz and runErrors become this:

image1159×149 30.6 KB
Hey @lkuang,Have it looks like your vcf files still need to be indexed. For this I would recommend tabix.Did these VCFs come from Parabricks, or did you run them with another tool?Hi @gburnett , I do that following the instructions in the pb3.7 manual. The VCF files are generated from pb3.7.I will try tabix.Thanks @gburnett. I just understand that the bgzip and tabix are both necessary.  So it’s OK now.BTW, I think it will be good if these steps cmd or instructions could be added in the manual as it is an step-by-step how-to.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
171,holoviz-output-to-web-based-app-via-rtsp,"Hello,We would like to embed the HoloViz video in a light weight web-based application. The result could to look like demo but in real-time.First off all, do you think a browser-based visualization approach like that is possible without significant loss in performance and latency (without constraining our real-time requirement)?I thought about streaming our holoviz video (using ffmpeg or gstreamer) to a rtsp-server and embedding the stream in our frontend. What could be some useful frameworks for that?What could be an alternative (web-based or not web-based) for building more complex guis around the holoviz video?Maybe you can give us some general ideas on how to proceed with our application. Thanks.Hello, we have two unreleased Holoscan operators that receive/send data over WebSockets. Checking with the team on some pointers on how to build an operator that encode/decodes a WebRTC stream as well as the estimate timeline for those two operators to be pushed to Holohub.Very nice to hear that WebSockets will be supportet in future releases.What would be your framework of choice to build complex guis and how would you embed the holoviz video in that?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
172,example-run-has-expired,"Hi there,
image1383×263 35.6 KB
When I tried to download the sample provided by the official doc, it shows that the link has expired.See STEP 4: Example run.Could you help provide a new one?Best regards,
TJ TsaiHi TJ,please try this commandRegards,
MyriemePowered by Discourse, best viewed with JavaScript enabled"
173,resources-for-parabricks-cli-experts-have-an-immediate-need,"Hi all,I have a client who is interested in building a GUI / SaaS product around Parabricks CLI.Mostly I don’t believe it requires that I actually have the hardware in hand or pay hefty fees to run a VM other than some specific testing scenarios.So what ideally I would like is to find an expert resource who I can consult with to help bridge the gap between client needs and what we are building.If you can help or can point me in the right direction, I’d appreciate it!Happy to pay fair / higher hourly rates for ongoing consultation for at least a few months.Powered by Discourse, best viewed with JavaScript enabled"
174,structural-variant-caller,"Are there any plans to include GPU accelerated structural variant callers with Parabricks?As of right now, there are not.Powered by Discourse, best viewed with JavaScript enabled"
175,how-to-pass-argument-not-listed-in-the-option-to-run-pbrun-germline-pipeline-or-standalone-process-apply-bqsr,"Hi All,How could I pass a specific argument to run pbrun germline pipeline or standalone process apply bqsr The arg is not currently listed in the option, but one of the parameters of gatk ApplyBQSR --emit-original-quals.This way I could also keep the original base quality score in the final out-put bam file. Basically to allow custom argument which available in the native (gatk) tool parameters, which will not affect/conflict to the gpu settings.Possibility of tweaking specific python scripts in pbrun would do?Thanks in advance.Best
JustinHi Justin,We do not support all the GATK options right now but the popular options and any feedback we get about the option from the community is greatly appreciated.
Its not possible to tweak the python scripts to achieve this. We will try our best to include –emit-original-quals in the future release.Thanks,
AshishPowered by Discourse, best viewed with JavaScript enabled"
176,nvidia-gtc-21-access-technical-training-and-sessions-built-for-developers,"Access technical training and sessions built for developers.
Register for Free - GTC 2022: #1 AI ConferencePowered by Discourse, best viewed with JavaScript enabled"
177,information-solving-the-error-when-switching-from-igpu-to-dgpu-mode-in-clara-holoscan-sdk-v0-1,"When switching from iGPU to dGPU mode (sudo nvgpuswitch.py install dGPU), currently there is a known error in Holoscan SDK v0.1:
nvidia-container-runtime : Depends: nvidia-container-toolkit (>= 1.4.2) but 1.0.1-1 is to be installedTo resolve this error, run
sudo apt-get install nvidia-container-toolkit=1.5.1-1
before running the nvgpuswitch.py scriptPowered by Discourse, best viewed with JavaScript enabled"
178,where-are-the-models-for-the-a30-for-parabricks-4-0-1-1,"Hi,The link to the download of parabricks deepvariant model files for 4.0.1-1 does not make any mention of model files for the A30 GPU, which were available in the 4.0.0-1 download.Deepvariant Models converted to TensorRT for GPU types not directly supported by the Parabricks container.Where are the models for the A30 for parabricks 4.0.1-1?Thanks,PadraicHey @padraic.corcoran,Great catch. You can use the model files from the 4.0.0-1 in your 4.0.1-1 workflows. There were no changes made between those updates.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
179,issue-build-sample-holoscan-my-recorder-code,"I’m a developer new to the Holoscan SDK. In my effort to learn and understand the gxf/holoscan framework, I’ve attempting to build and run the several of the provided code samples.   I’m following the steps provided in by this link: Clara Holoscan Development Guide - NVIDIA DocsThe MyRecoder extension build worked just fine, however the when building the my_recorder_op.cpp, the build fails with the following error message:[485/568] Building CXX object apps/my_recorder_app/CMakeFiles/my_recorder_app.dir/my_recorder_op.cpp.o
FAILED: apps/my_recorder_app/CMakeFiles/my_recorder_app.dir/my_recorder_op.cpp.o
/usr/bin/c++  -I/workspace/holoscan-sdk/apps/workspace/holoscan-sdk/include -I/workspace/holoscan-sdk/apps/workspace/holoscan-sdk/include/core -O3 -DNDEBUG -std=c++17 -MD -MT apps/my_recorder_app/CMakeFiles/my_recorder_app.dir/my_recorder_op.cpp.o -MF apps/my_recorder_app/CMakeFiles/my_recorder_app.dir/my_recorder_op.cpp.o.d -o apps/my_recorder_app/CMakeFiles/my_recorder_app.dir/my_recorder_op.cpp.o -c /workspace/holoscan-sdk/apps/my_recorder_app/my_recorder_op.cpp
In file included from /workspace/holoscan-sdk/apps/my_recorder_app/my_recorder_op.cpp:2:
/workspace/holoscan-sdk/apps/my_recorder_app/my_recorder_op.hpp:4:10: fatal error: holoscan/core/gxf/gxf_operator.hpp: No such file or directory
4 | #include “holoscan/core/gxf/gxf_operator.hpp”
|          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.I’m not a expert with CMake/make but It appears to be an issue with CMake/Make.Note: I am following the steps outlined in the link provided above.Please advise.Hello! It appears you’re following the SDK documentation from v0.3, we have now v0.4 SDK released and the equivalent of the steps linked are here: Creating Operators - NVIDIA Docs (Please note v0.5 is coming out soon) Could you confirm which SDK version you’re using?Could you please also let us know in which environment you’re building in, including installation method and hardware? SDK Installation - NVIDIA DocsPowered by Discourse, best viewed with JavaScript enabled"
180,using-the-clara-in-an-embedded-environment,"Hello,
I’m sorry if this is not in the right section.
After using the AGX Xavier Developer Kit for a while, I’m looking for something with more PCIe slots and network connectivity, while not taking up too much more space. The carrier board the Clara uses certainly has the PCIe slots and network connectivity, but it’s not clear whether it would be possible to strip some off components to make the overall package smaller.Thank you.Hello,To answer the questions:Thank you!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
181,clara-agx-sdk-manager-installation-failed,"The following packages have unmet dependencies:
sdkmanager:amd64 : Depends: libgconf-2-4:amd64 but it is not installable
Depends: libcanberra-gtk-module:amd64 but it is not installable
Depends: locales:amd64 but it is not installable
E: Unable to correct problems, you have held broken packages.Hello,Thanks for joining the NVIDIA Developer forums! I am moving your topic to the Clara category so the team has visibility.Cheers,
TomThank you.Hi @ocean0922, I found this post that may be relevant SDK Manager install fail, could you confirm which machine / OS you’re installing SDKManager on?Hi jinl  ,
Clara AGX equipment ,Virtual Machine InstallationCould you please elaborate? Do you mean you are running a VM on Clara AGX, and running SDKM within the VM? Please see the related post’s solution:After apt get update, the problem has been resolved. Thank you！Powered by Discourse, best viewed with JavaScript enabled"
182,is-it-still-possible-to-download-clara-holoscan-sdk-v0-1-0,"I’m trying to use the gstreamer element nvvideotestsrc but i can’t seem to find it. I want to use it to understand how to add RDMA support to my gstreamer plug in.I saw the in the Clara Holoscan SDK v0.1.0 :
“The nvvideotestsrc plugin. The source code for this plugin provides some basic placeholders and documentation that help point developers in the right direction when wanting to add RDMA support to a GStreamer source plugin. Specifically, the gst_nv_video_test_src_fill method – which is the GStreamer callback responsible for filling the GPU buffers – contains the following.”Thank you for the helpHello, even though we strongly recommend users to move to v0.2, if you really want to install Holoscan SDK v0.1, you still can. First, flash your devkit with Jetpack 4.5 and select “Clara Holoscan” in Additional SDKs.
JP4.51920×1235 106 KB
The rest of the setup steps are similar to the ones in devkit user guide. The v0.1 SDK documentation is here Table of Contents — Clara Holoscan SDK 0.1 documentation. According to the documentation:The source code for the GPU Video Test Source plugin is installed to the following path.
/opt/nvidia/clara-holoscan-sdk/clara-holoscan-gstnvvideotestsrcHello,The problem is that I don’t have the option to select Clara Holoscan like you are showing in your picture, I can only install DeepStream. I also don’t have Rivermax optionMy account permission  in Membership Information says :It sounds like it might be that the Target Operating System is selected as “JetPack 5.0 -HP” in this case. We need to select the older verison “Jetpack 4.5.1” in order for Additional SDKs to show Rivermax and Clara Holoscan as options.It seems like I can’t select an older Operating System to install.My target hardware is the Clara Holoscan and my host computer is running ubuntu 20.04It says “JetPack 4.5.1 (rev. 1) Not available for clara Holoscan Developer Kit modules” when I select Clara Holoscan developer Kit modules as my target HardwareI see it says available on ubuntu 1604 and ubuntu 1804 for JetPack 4.5.1 (rev. 1) when i select Clara AGX Developer Kit modules as target Hardware. Do i really need to install ubuntu 1804 on my host machine to be able to flash my clara Holoscan with JepPack 4.5.1?My goal here is to enable RDMA into dGPU memory from my GStreamer plugin created especially for my frame grabber connected into the PCIe slots.Thank youHi there. Just to make sure, do you have a Clara AGX Developer Kit? It is not the same hardware as Clara Holoscan developer kit.
If you don’t have a ubuntu 18.04 machine around which is very understandable, perhaps you could try a VM as host, for example VMWare workstation. (We have not had success using VirtualBox in the past). Sorry for the inconvenience and we are planning on improvements with the setup process.Hi,I do have the Clara AGX Developer Kit.I’ll try to install it with a virtual machine with VMWare workstation.Thank youhi jinl,I installed ubuntu18.04 on a computer and downloaded the sdkmanager, but I encounter an error while trying to log in. It says “Failed to get client token”, I made sure to log out from my other computer before logging in with the one running ubuntu18.04, but still can’t log in.Can you help me with that?Could it be possible that your issue is similar to this one I cannot log in SDK MANAGER ? Have you seen the documentation on proxy settings? SDK Manager Settings :: NVIDIA SDK Manager DocumentationAlso if you don’t mind sharing, what are the main reason you are staying within the gstreamer framework and your concerns with moving away from gstreamer. Happy to chat 1-1 as well over email!Hi jinl,It was in fact a proxy problem, thank you!Yes we can chat, email me at ljodoin@matrox.comThank youPowered by Discourse, best viewed with JavaScript enabled"
183,coarse-to-fine-contextual-memory-for-medical-imaging,"Join us for GTC Digital, where we will host 2 hour trainings workshops at significantly discounted rates to help you continue your training in deep learning or accelerated computing at home.This includes:
[Tuesday, April 7th] Coarse-to-Fine Contextual Memory for Medical Imaging - Conference Session Catalog | GTC 2022 | NVIDIA (2 Hour Training - $39)
[Wednesday, April 8th] Deployment for Intelligent Video Analytics using TensorRT - Conference Session Catalog | GTC 2022 | NVIDIA (2 Hour Training - $39)
[Thursday, April 9th] Data Augmentation and Segmentation with Generative Networks for Medical Imaging - Conference Session Catalog | GTC 2022 | NVIDIA (2 Hour Training - $39)Space is limited - register today before it’s too late.Powered by Discourse, best viewed with JavaScript enabled"
184,where-to-find-the-version-of-tools-used-in-the-pipelines-for-the-vcf-files,"Hi,Taking as an example the HaplotypeCaller from GATK4, we have this type of information in the header of the VCF (here, displaying only the start of the row):Is this information available somewhere from the output of Parabricks germline pipeline?If not, I think it would be nice to have something similar in the VCF generated by Parabricks for the different pipelines. Maybe also add something likeThank youThank you for the suggestion @alaa.badredine. I sent this feedback to the engineering team.Powered by Discourse, best viewed with JavaScript enabled"
185,failed-to-run-sample-failed-to-create-the-vulkan-instance-clara-holoscan-sdk-0-4,"Hi, I am trying to run : python3 ./apps/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=replayer using docker in x86 Platform, I got this error:
Vulkan Version:any ideas?
ThanksHello Abdelkrim1,Thank you for your interest in Clara Holoscan and apologies for the troubles. This is a known issue and there’s a workaround that’s documented in the “Running the container” section of the NGC catalog page for the Holoscan container:The Holoscan container includes the built Holoscan libraries, GXF extensions, headers, example source code, and sample datasets. It is the simplest way to run sample streaming applications or create your own application using the Holoscan SDK.specifically:Could you try running this command before launching docker?  Hope this helps.Thanks!Thanks for the quick reply!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
186,x11-forwarding-over-ssh-to-run-holoviz-remotely,"Hello,I tried to run my holoscan application remotely over SSH. I did configure X11 forwarding on the server (clara holoscan devkit).The sshd_config file on the server looks like this:On the client (local computer) I run the following command:
ssh user@address -v -AXThe debug information returns that forwarding is activated. I can open any graphical application (e.g. xterm ) on the server without any issue. From inside a container on the server, however, I get the following error when running an application which uses holoviz:I ran the container following your guide with --net host, -v /tmp/.X11-unix:/tmp/.X11-unix and -e DISPLAY=$DISPLAY.Do you know how to set the display inside the container such that holoviz window is forwarded?Any help would be much appreciated.Disclaimer: this is not an officially tested solution. I have had success previously with forwarding from within a container on a remote server by a solution I found onlineSimilarly when running the Holoscan container, I add the following to the docker run command: -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH.Let us know how this work out!The suggested approach works quite well. Thanks for the help.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
187,error-with-clara-parabricks-4-0-1-1-fq2bam-bad-argument-value-number-of-gpus-requested-is-more-than-number-of-gpus-in-system,"HiI am trying out clara-parabricks:4.0.1-1.sif on a g4dn.metal EC2 which has 8 GPUsThis is how I created my sif file:And I get this error  when trying to run fq2bamThis is nvidia-smi on the hostThis below is from inside the container after invoking it on command like by singularity shell --nv clara-parabricks_4.0.1-1.sifWould appreciate any help.Thanks in advance.Hi there,
I observed the same issue with docker and version nvcr.io/nvidia/clara/clara-parabricks:4.1.0-1 .Kind regards,
DanielHello,sorry to hear you are having issues.
Can you please let me know if you are able to run any other CUDA application and/or CUDA samples, to make sure that this is not a driver issue.Hey,
I re-installed cuda12. It seems that previously it was not installed properly.
Furthermore, I disabled MIG service.
Now it works for me.Thank you for your reply.
Parabricks does not support MIG mode.BestPowered by Discourse, best viewed with JavaScript enabled"
188,sdk-v0-2-docker-build-errors-in-using-a-development-container,"If the following errors are encountered while running the docker build  step in Using a development container in the GitHub repoIt is likely because the step docker login nvcr.io was not run. See https://ngc.nvidia.com/setup/api-key on how to set it up.=> ERROR [13/21] RUN git clone GitHub - glfw/glfw: A multi-platform library for OpenGL, OpenGL ES, Vulkan, window and input src -b 3.2.1     && cmake -S src -B build -D CMAKE_POSITION_INDEP  65.7s
[13/21] RUN git clone GitHub - glfw/glfw: A multi-platform library for OpenGL, OpenGL ES, Vulkan, window and input src -b 3.2.1     && cmake -S src -B build -D CMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON         -D GLFW_BUILD_DOCS:BOOL=OFF         -D GLFW_BUILD_EXAMPLES:BOOL=OFF         -D GLFW_BUILD_TESTS:BOOL=OFF     && cmake --build build -j     && cmake --install build --prefix /opt/glfw     && cd … && rm -rf glfw:
#15 0.597 Cloning into ‘src’…
#15 65.65 error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.
#15 65.65 fatal: the remote end hung up unexpectedly
#15 65.65 fatal: early EOF
#15 65.65 fatal: index-pack failed
executor failed running [/bin/sh -c git clone GitHub - glfw/glfw: A multi-platform library for OpenGL, OpenGL ES, Vulkan, window and input src -b ${GLFW_TAG}     && cmake -S src -B build -D CMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON         -D GLFW_BUILD_DOCS:BOOL=OFF         -D GLFW_BUILD_EXAMPLES:BOOL=OFF         -D GLFW_BUILD_TESTS:BOOL=OFF     && cmake --build build -j     && cmake --install build --prefix /opt/glfw     && cd … && rm -rf glfw]: exit code: 128=> ERROR [internal] load metadata for nvcr.io/nvidia/tensorrt:22.03-py3                                                             5.7s
[internal] load metadata for nvcr.io/nvidia/tensorrt:22.03-py3:
failed to solve with frontend dockerfile.v0: failed to create LLB definition: failed to authorize: rpc error: code = Unknown desc = failed to fetch anonymous token: unexpected status: 401 UnauthorizedPowered by Discourse, best viewed with JavaScript enabled"
189,autodock-gpu,"The AutoDock-GPU Suite is a growing collection of methods for computational docking and virtual screening, for use in structure-based drug discovery and exploration of the basic mechanisms of biomolecular structure and function. More info on AutoDock-GPU be located at https://ccsb.scripps.edu/autodock/ and official github pageSee here for a document describing prerequisites and setup steps for all HPC containers and instructions for pulling NGC containers.Learn MoreThanks for sharing the details about the AutoDock-GPU Suite. It’s an exciting set of tools for computational docking and virtual screening, specifically designed for structure-based drug discovery and exploring biomolecular structure and function. It’s great to see the advancements in this field!Powered by Discourse, best viewed with JavaScript enabled"
190,received-signal-11-while-running-fq2bam-on-the-sample-dataset,"I have installed Parabricks using Docker with a trial license. When running fq2bam as described in the manual (https://docs.nvidia.com/clara/parabricks/v3.2/text/getting_started.html) to test the installation the program aborts with an error reading “Received signal: 11” while in GPU-BWA mem, Sorting Phase-I, version 3.6.1-1.ampere.When trying to reproduce the error, I sometimes get “Received signal: 7” or “Received signal: 6” with the additional error message
PARABRICKS: /dnabricks//sortcommon/inc/compressfile.h:283: size_t LoopReader::LRread(int, char*, size_t): Assertion `size < 32 * BLOCK_BYTES' failed.Is there anything I can do to fix this?Hey @mnutsua,Thank you for trying out the Parabricks software. Can you send the exact command you used and the full terminal output?I would also recommend using the latest version of the documentation, which can be found here: https://docs.nvidia.com/clara/parabricks/3.7.0/index.htmlPowered by Discourse, best viewed with JavaScript enabled"
191,pbrun-glnexus-multiple-gvcf,"Hello,
how can I input several gvcf beside --in 1.gvcf --in 2.gvcf? Is there any option to put them in a file?
ThanksPowered by Discourse, best viewed with JavaScript enabled"
192,does-nvidia-clara-agx-support-packages-outside-of-clara-agx-sdk,"Hi,I haven’t received the Clara AGX box yet but would like to ask questions on the supported software because we want to transfer some C++ software we developed in a Ubuntu 16.04 environment to the Clara AGX machine.So my question is, will I be able to use the Clara AGX just like a normal machine with Ubuntu operating system on it and install all packages just like on a normal PC?Thank you so much for your help!Hello,
We have the Clara AGX SDK that is tested with and can be installed with the Clara AGX hardware, which is Ubuntu 18.04. I would recommend you transfer and test your programs once you have installed the Clara AGX SDK.Hi Jin,Thank you for your reply. Does that mean if the software can run in Ubuntu 18.04, it will be able to run on the Clara AGX machine?It really depends on which dependencies your software requires. Is there some particular library version such as cuda version / cudnn version / TensorRT version that it needs? The Clara AGX SDK is an ARM based system like other Jetson systems, so that is also a difference from x86 Linux systems if your current work is on one.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
193,namespace-object-has-no-attribute-gpu-devices,"Sir,I am getting the following error while running pbrun applybqsr on fastq files (both command line and error traceback):Command:Error meaasge:Kindly suggest how to rectify this error. Thanks.I temporarily modified the code to pass the checking.Before:After:Then you can run pbrun applybqsr.Powered by Discourse, best viewed with JavaScript enabled"
194,mutectcaller-interval-file,"Hi,I’d like to do somatic variant calling of SNPs and INDELs with the UKB WES data using the Mutectcaller (Parabricks accelerated) version 4.0.0-2 on UK Biobank Research Analysis Platform RAP. I started an analysis and received interval file error. I attached run associated log file. Before I started analysis, I selected a bed file as interval file. The bed file contains just specific genes. The aim of adding the bed file is to filter WES data according to spesific genes. I’m intereted in spesific genes not all genes. So, I’d like to get a small size VCF file end of the analysis. How can i do this? I wanted to do analysis using a bed file but I could not. What would you suggest for solving this issue?AppInternalError app-GPzvXG8Jbj6K4yBbpYbxYG4J.txt (8.6 KB)Many thanks.Powered by Discourse, best viewed with JavaScript enabled"
195,error-when-run-germline-pipeline-with-na12878,"Hi,We use eight Tesla P100 to run germline pipeline with NA12878 data, But getting the following error:[M::bwa_idx_load_from_disk] read 0 ALT contigsGPU-BWA mem
ProgressMeter	Reads		Base Pairs Aligned
[09:55:00]	5061412		750000000
[09:55:22]	10122400		1510000000
[09:55:44]	15182642		2280000000
[09:56:07]	20242532		3030000000
[09:56:30]	25301622		3810000000
[09:56:53]	30361222		4570000000
[09:57:15]	35421664		5320000000
[09:57:37]	40481834		6090000000
[09:58:00]	45541066		6830000000
[09:58:24]	50599860		7600000000
[09:58:46]	55658706		8350000000
[09:59:09]	60719092		9090000000
cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:3920 : an illegal memory access was encountered
GPU: 3, cudaSafeCall() failed at ParaBricks/src/mem_chain_kernel.cu:3889 : an illegal memory access was encountered
GPU: 0, cudaSafeCall() failed at ParaBricks/src/mem_chain2aln_kernel.cu:5145 : an illegal memory access was encountered
GPU: 1, cudaSafeCall() failed at ParaBricks/src/mem_chain2aln_kernel.cu:5145 : an illegal memory access was encounteredAny suggestions or advice will be appreciatedThanks for your interest in Parabricks. Would you kindly try running with a new version of Parabricks - version 3.1 for example?But we only found v2.5.0 and v2.3.2 on the NGC, could you kindly provide the link or website to get version 3.1? ThanksThanks for clarifying. While we update our NGC repository with the latest Parabricks code version; please use this link to run Parabricks:wget -O parabricks.tar.gz "" https://s3.amazonaws.com/parabricks.licenses/v310_OCT_END/parabricks.tar.gz?AWSAccessKeyId=AKIAJGDUNN2G2ZAH3Q3A&Expires=1602532746&Signature=yVstD6Ga%2FQVZawj05bmeWFFGXDY%3D ""We are still working on running the parabricks on our server, but it looks like the trial license has expired. Is it possible the extend the trail license to the end of year?Powered by Discourse, best viewed with JavaScript enabled"
196,license-parabricks,"When does the license time start for the 30 days trail when I’ve already revived the download link but have not installed it yet.Need to know, as we need to test it and its around holiday season right now.ThanksPowered by Discourse, best viewed with JavaScript enabled"
197,i-dont-have-the-settings-of-additional-sdks-on-the-first-step-sdkmanager,"I don’t have the settings of Additional SDKs on the first step SDKmanager to install the Clara Holoscan (Deepstream and Rivermax) as shown in the youtube set up tutorial Clara AGX Developer Kit Unboxing and Setup - YouTube.
screenshotSDKmanager1042×677 233 KB
What should I do to get access?Thank youHello, for now, please apply to the developer program and log into SDK Manager with the same account that you used to apply. Then the Clara Holoscan SDK option should show up. Please see the Download section on our https://developer.nvidia.com/clara-holoscan-sdk page for applying to the developer program.Hello, I did log in with my account used to apply but the Clara Holoscan SDk option doesn’t show up. What else can i do?Thank you!Could you attach a screenshot here?Hello , yess here is the screen shot

image1333×765 150 KB

As you can see, i don’t have the clara holoscan and rivermax option at the bottom even though i have the membership NVIDIA Clara Holoscan SDK programThank youHello, with the latest release of Jetpack 5.0 - HP, please note that Rivermax for Clara Holoscan is no longer part of the SDK Manager installation, and you can follow the latest devkit user guide on installing the Clara Holoscan SDK via GitHub.can i install the Clara Holoscan SDK directly on the holoscan from github or do i still need a host machine?We would install Clara Holoscan SDK directly from GitHub without any host machine.Powered by Discourse, best viewed with JavaScript enabled"
198,failed-to-map-buffer-for-dma-from-using-aja-to-run-sample-app,"Ran sample applications on docker container using AJA as input and got the ‘Failed to map buffer for DMA, Failed to setup AJA buffers’ error as below:Hello, just to double check, are you in dGPU mode on Clara AGX devkit? And that you have set up the AJA drivers according to AJA Video Systems — Clara Holoscan 0.2.0 documentationPlease note we are updating the docker run command in this section AJA Video Systems — Clara Holoscan 0.2.0 documentation.Yes, the AGX is in dGPU mode. I ran the sample app without aja (pre-recorded video source) with no problem. The aja card is installed and I add --device /dev/ajantv20:/dev/ajantv20 to docker run command.Is it possible to be the RDMA problem?Powered by Discourse, best viewed with JavaScript enabled"
199,v3-8-a100-germline-got-stuck-in-the-bwa-program,"Hi, there,We got stuck twice in the BWA program in the germline piepeline.
One case was for the simplest fastq (3 records each for R1 & R2, 150 bases per records)
Another case was for WGS (R1 & R2) (sequencing depth: 44.48x)
(We’ve run the germline pipeline for this WGS more than 20 times.)The abnormal case like below:The normal case is:Are there any possible causes or clues to the symptom? (We’ve never met the abnormal case for non-ampere versions)Powered by Discourse, best viewed with JavaScript enabled"
200,merging-bam-files-from-different-fq2bam-runs,"Hi,
I have a single sample that was sequenced from two or three different libraries on the same platform. I want to run the fq2bam on paired-end reads separately by having different --read-group-lb tag and then merging BAM files before I proceed with BQSR (see below).Is there a way to merge BAM files using parabricks? Also, if it isn’t possible should just merge using samtools and then return to pbrun for downstream pipeline?Hey @samarth,Parabricks does not have a tool to merge bam files. I would recommend using samtools for that, then coming back to Parabricks for the downstream pipeline.I presumed that it was the solution but wanted to double check.
Thanks @gburnett for responding.Powered by Discourse, best viewed with JavaScript enabled"
201,computer-vision-solutions-page-refreshed,"Computer Vision Developers:We have updated the Computer Vision Solutions Landing Page, particularly for beginners and those new to NVIDIA software. In short, here are some high-level updates:If you have feedback on the Computer Vision Solutions Landing Page, NVIDIA computer vision software, or anything-related to computer vision, please reach out. Our team would be happy to hear from you.Best,
MikeB_NVPowered by Discourse, best viewed with JavaScript enabled"
202,how-to-register-a-holoscan-component,"All - I’m new to Holoscan development and I have a question regarding registering Holoscan components.My question is:  Is it possible to register a Native C++ operator as a Holoscan component (e.g. operator.so lib)?Background:
I have have developed a Holoscan operator/application that leverages opencv to modify video images from video_stream_replayer. I’m using the basic_workflow example as a template that seems to work just fine. Ideally, I’d like to register the operator portion of the application as a Holoscan Component and was wondering if its possible.Please advise.Hello, could you please tell us a bit more on the question?If you’re wondering about a plugin system (loading operators as a shared library in runtime on demand), we don’t support a plugin system for native Holoscan Operator yet.If you’re wondering about using the native Holoscan operator you developed in a modular way in other Holoscan applications: (In SDK v0.4, all operators were in a single CMake target and Holoscan library.) Since SDK v0.5, operators are now separated using standard CMake (separate add_library for operators and core). You could see on HoloHub for examples:Manual said that IGX uses Jetson orin+a6000. We attempted to install the NVIDIA Holoscan SDK into Jetson orin using sdk manager, but the sdk manager was unable to discover IGX devices. Is there any way to make Jetson orin be discovered as an IGX device?thanksHello, please open a separate topic on the corresponding forum, thank you.Powered by Discourse, best viewed with JavaScript enabled"
203,does-the-germline-pipeline-call-through-the-applybqsr-process,"Hi, therehttps://docs.nvidia.com/clara/parabricks/v3.5/text/germline_pipeline.html

image1802×312 9.59 KB

The figure shows that it will call the ApplyBQSR process.When I tried to run the germline pipeline, I found that there was no ApplyBQSR info in the log.Here is the log from the sample run.Below is the binary list from the container.The germline pipeline seems to just use:Does the germline pipeline call through the ApplyBQSR process?Hey,The BQSR is in the first step in the output log file.It’s not in the banner (which can be confusing), but it’s in theI  mean ApplyBQSR (apply the BQSR report) instead of BQSR (generate the BQSR report). Or does BQSR include ApplyBQSR?Parabricks v3.5 - doc:I tried the tools below:I found:Powered by Discourse, best viewed with JavaScript enabled"
204,parabricks-on-3-gpus,"Can Parabricks be used on 3 GPUs or only in “pairs” of 2, 4, 6, etc.?Hey @fscheufens,The software should definitely run on 3 GPUs. I’m not sure what the performance will be like since we don’t test that config, but hopefully it will be faster than 2 GPUs and slower than 4 GPUs.Thanks.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
205,multiaiinferenceop-error-when-multiple-outputs-per-model-bindings-x-nullptr,"I am trying to do inference on an onnx segmentation model which takes one images as input and outputs the detection boxes (output0) and the segmentation mask (output1).model:It actually works fine when using my own inference operator written with cupy. So the onnx model and the converted trt model should be correct. When using the holoscan MultiAIInferenceOP however I get an error:YAML:error message:Is the config for the MultiAIInferenceOP correct? I did not find out how to declare two outputs for a single model and assign them by their real name (output0, output1).Setup:
NVIDIA Clara AGX Developer Kit with Holoscan SDK v0.5.0.Hello, you have accurately found the issue, which is that in v0.5 MultiAIInferenceOp/Holoinfer didn’t support multiple output tensors in the same model. For the next release v0.6, this will be added along with other features in MultiAIInferenceOp/Holoinfer. For now, if you only have one single model to run inference on, you could revert to the previously used TensorRTInferenceOp which should support multiple output tensors in one single model. Please note that TensorRTInferenceOp will be deprecated in the future once MultiAIInferenceOp/Holoinfer has the equivalent features added.Okay, I see. Then we will for now revert to the TensorRTInferenceOp. Thanks for the help.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
206,using-singularity-for-the-docker-image,"Running the Parabricks docker image using Singularity doesn’t get me the Singularity terminal. Has it been tested yet?Hey @tnnandi,Are you running the latest Parabricks 4.0 container? Which version of Parabricks are you running?Thanks!Yes, I’m using 4.0.0-1 from Nvidia Clara Parabricks | NVIDIA NGC. FYI: I’m using the same Singularity installation to load GATK4 images so I don’t think Singularity is the issue. Also, I’m having the same problem for other NVIDIA containers too, e.g. RAPIDS.Thanks, I could resolve the issue related to the container: I had to first check out a GPU node on the cluster before executing “run singularity shell --nv image_name.img”.Hey @tnnandi,I’m glad you were able to figure it out. Let us know if you have any other questions!Thanks,Powered by Discourse, best viewed with JavaScript enabled"
207,how-to-add-pass-to-filter-after-generating-vcf-with-somatic-pipeline,"I’m using the Parabricks somatic pipeline (SOMATIC PIPELINE — Clara Parabricks Pipelines 3.6 documentation)
After generating vcf files, I want to add the PASS value in the FILTER field.
The problem here is that I cannot find a tool inside Parabricks that allow me to do so.
Neither I can do that outside of Parabricks using gatk FilterMutectCalls, because the somatic pipeline do not return the [output vcf].stats , that is automatically generated by gatk Mutect2.Without that step the Parabricks Somatic Pipeline is unuseful.
AYHi Youssef,The version of Parabricks is based on GATK 4.1. This version does not need stats file.
So you can filter mutect calls using GATK 4.1 filterMutectCalls.We are planning on providing support for GATK 4.2 in our next release. This will have support of stat files.Regards,
MyriemePowered by Discourse, best viewed with JavaScript enabled"
208,fsq2bam-slowing-down,"doing fastq alignment, I got message as follows:[PB Info 2023-May-18 15:09:07] [M::runCPUThreads] read 66668 sequences (10000200 bp)…
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (1, 27528, 2, 1)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR…
[M::mem_pestat] (25, 50, 75) percentile: (317, 367, 421)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (109, 629)
[M::mem_pestat] mean and std.dev: (369.16, 80.67)
[M::mem_pestat] low and high boundaries for proper pairs: (5, 733)
[M::mem_pestat] skip orientation RF as there are not enough pairs
[M::mem_pestat] skip orientation RR as there are not enough pairsAfter those message, the process seems hang-up. Any hints?Thanks,WeiHi @wei.xing ,Can you please let know what GPUs you are using? What is the command you tried? and what version of Parabricks you used?And also make sure that you are not running something else on the systemBesthi, the issue was sorted. It was due to fastq files (generated by fastq dump), but not gpu or bam mem. Anyway, thanks for supporting.Happy to hear that it was sorted.Thank youThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
209,im-having-a-very-difficult-time-installing-nvidia-sdk-manager,"I’ve been trying for weeks to install the Nvidia SDK Manager. We want it to interface to the Clara AGX box. I got a bit further yesterday but got the following error message.
"" Access to APT repository and ability to install Debian packages with it.Your system is not ready for install, see specific errors below. Once fixed, click ‘Retry’ to verify system readiness again.Access to APT repository and ability to install Debian packages with it.: Apt repository check failure (1. sudo -S apt-get update && sudo -S apt-get check; 2. dpkg --audit). E: Repository ‘http://dl.google.com/linux/chrome/deb stable InRelease’ changed its ‘Origin’ value from ‘Google, Inc.’ to ‘Google LLC’ N: This must be accepted explicitly before updates for this repository can be applied. See apt-secure(8) manpage for details.""Can someone please advise?This seems not Jetson platform, but Clara platform related issue, let me move it into Clara forum for resolution.  ThanksHello, just to make sure, you are having troubles installing SDK Manager on your host machine, and haven’t gotten to the step of flashing the Clara AGX yet?
Have you tried to follow the suggestions in the message: 1. sudo -S apt-get update && sudo -S apt-get check; 2. dpkg --audit?BTW once you have gotten SDK Manager installed, please see https://developer.nvidia.com/clara-holoscan-sdk page, especially NVIDIA Clara AGX Dev Kit User Guide for setting up the Clara AGX devkit.Hi @user142999 ,
just checking , is your problem resolved and have you been able install the packages you needed.Powered by Discourse, best viewed with JavaScript enabled"
210,can-i-use-genomicsdbimport-later,"The only joint calling functions in Clara prabricks are Combine GVCFs and Glnexus.
Do you have any plans for adding GenomicsDBImport in GATK or other joint calling functions?Hey @min890315,We do not have plans for adding other joint calling functions. We are, however, working to improve our GLNexus support.Powered by Discourse, best viewed with JavaScript enabled"
211,getting-started-with-clara-holoscan-sdk,"NVIDIA Clara™ Holoscan includes Healthcare-specific acceleration libraries, pre-trained AI Models, and reference applications for computational medical devices in ultrasound, endoscopy, surgical robotics, digital pathology, radiation therapy, patient monitoring CT, MRI, X-ray, Genomics, and Microscopy.Find the complete set of developer resources on the Clara Holoscan SDK developer page.First, Apply for developer access to the Clara Holoscan SDK Program.Second, download and install the SDK Manager. After logging in to SDK Manager, you’ll be able to flash your device.Finally, download the Clara Holoscan SDK from GitHub.The NVIDIA Clara Developer kit helps facilitate accelerated processing with 3rd party sensor front ends, High-Speed I/O with the ConnectX SmartNIC, the flexibility and efficiency of the Jetson SoC, and a powerful RTX-series Discrete GPU.Find more about your hardware options on our Clara Developer Kits Page.Powered by Discourse, best viewed with JavaScript enabled"
212,long-read-analysis-tools,"Hi folks,
I see on one of your pages (https://developer.nvidia.com/clara-parabricks)  that there are some long read analysis steps being reported, but when I look a the commands inside pbrun all I see are short read commands.  How do I access any long read analyses (mapping or variant calling)?
thanks
RichardHey @rcorbett1,Our Parabricks software is split into two packages (it can be confusing, I know). Parabricks Pipelines is what you have downloaded. That’s the one for short reads that uses pbrun, Parabricks Toolkits is for long reads and can be found on GitHub: https://github.com/clara-parabricks.Hopefully that helps!Ah. I see.    So for mapping and variant calling of long reads would you be recommending these?dev/cudamapperSDK for GPU accelerated genome assembly and analysisContribute to NVIDIA-Genomics-Research/DL4VC development by creating an account on GitHub.YesPowered by Discourse, best viewed with JavaScript enabled"
213,how-parabricks-hide-docker-container-content,"Hi
I run Parabricks container and attach to the container but I don’t see any things inside it.
How Parabricks do it? I need it to encrypt my container.
Thank you.Hey @dongoctuan.0101,The best way to run Parabricks is to stay outside of the container, using the pbrun wrapper. After you’ve run the installation, you should be able to run commands like pbrun germline ...This will invoke the Docker container. Does that solution work for you?Thank you,
I am a customer of Parabricks for a long time. I known that pbrun spawns a container, but when I attach to the container, I can not see any thing. I really want to use technology like this to hide data from my customer.
If you can share it or some suggestion, I am very happy.Hey @dongoctuan.0101,Just to clarify. You want to create your own container, that hides data in the same way that the pbrun command is invisible from inside the container?Powered by Discourse, best viewed with JavaScript enabled"
214,does-parabricks-support-the-grch38-refseq,"Hi there,I would like to know if Parabricks supports the GRCh38 reference sequence, as the GRCh38 RefSeq contains not only  ATCG+N but also B, K, M, R, S, W, Y bases. I could not find any relevant information in the documentation, and the Homo_sapiens_assembly38.fasta provided by NVIDIA uses UCSC bases (which only uses ATCG+N bases).Hey @tj_tsai, our examples use UCSC but GRCh38 is also supported.Powered by Discourse, best viewed with JavaScript enabled"
215,minor-error-found-on-sample-script,"Hi,I found minor error on sample script from link - cnvkit — Clara Parabricks v3.7 documentationSample script:
$ pbrun cnvkit 
–ref Ref/Homo_sapiens_assembly38.fasta 
–in-bam mark_dups_gpu.bam
–out-file output.vcfThe option --out-file does not exist. The correct one is ‘–output-dir’.Hi,Thank you! We are going to fix this issue.Best,
MyriemeThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
216,does-fastq2bam-tool-have-the-ability-to-remove-marked-duplicates,"Title pretty much says it. GATK MarkDuplicates has an option to remove sequencing duplicates, and I was wondering if Parabricks fastq2bam had a similar option. Didn’t see it anywhere in the tool documentation.Hey @vedant.ktParabricks fq2bam can only mark duplicates, it can’t delete them.Powered by Discourse, best viewed with JavaScript enabled"
217,clara-holoscan-or-agx-hardware-spec,"Hi, is it possible to get more hardware information about either Holoscan or AGX? We need see how well it could be integrated into our system.Hello, which hardware specs are you interested in specifically? If you haven’t already, take a look at the devkit user guide with hardware info here for the Clara AGX Developer Kit: https://developer.nvidia.com/clara-agx-developer-kit-user-guideHi thanks for your reply. do you have more detailed hardware specification, if we want to further investigate, how we could connect our ‘customer card’, how we could modify the ID to match our requirement?Hello, what IO does your card have? If it is PCIe, there are PCIe slots on the devkit, as specified in the user guide linked above. For other IO, this page may be helpful NVIDIA Clara AGX Developer Kit for AI-Enabled Medical Devices. If you could tell us more about which card you have in mind and what requirements for connection there are, that can be helpful too.Hi, ‘customer card’  means our own PCBs. We want to see if we could integrate Clara into our product with different customized interface to support our own devices. Also we will probably remove many of the physical connectors to meet our requirement for a better ID design.
I am not clear, if this developer kit is only development or ? If we want to move forward with Clara architecture and make our own product, how should we proceed?Hello, to answer the question whether this developer kit is only for development: yes, it is only meant for the development phase. Since you’re interested in exploring whether you can integrate the Clara hardware design into your own product, that would be another step on top of developing using the developer kit, which is going into production with our production hardware. Would you like to have a call so we can see how we can help in your efforts?Hi, I’m Yaniv Lazimy, the Technical Product Manager for Medical Devices.  I would like to discuss your application with you.  Can you please respond with your name, company affiliation, and email?thanks!Please send your contact info to ylazimy@nvidia.comPowered by Discourse, best viewed with JavaScript enabled"
218,best-nvidia-platform-for-ont-minion,"Dear All,One of my researching colleagues at our university has already acquired a MinION sequencer from Oxford Nanopore Tech., and would like to apply for an NVIDIA research grant for a suitable GPU-based platform for base-calling of .fast5 files produced by the MinION as well as perhaps performing alignments on both MinION and MiSeq sequencing data.I’d appreciate any constructive input on the suitability of the Clara AGX (or perhaps other NVIDIA) platform for the above purposes, thanks.Best,
CBUpdate:Initially, using ONT’s guppy_basecaller software we’d like to process a semi-weekly run consisting of ~400GB from the MinION in ~3 days or less.Thanks.Hello!Would you be able to describe what tools you use for alignment and any other processing steps you might be running (e.g. assembly, polishing, or variant analysis) and what organisms you are working with?Additionally it’s possible with GPUs to perform basecalling in realtime with a MinION (and other ONT hardware) so you would not need to perform a batch based weekly run job and immediately have fasta files available. This reduces the amount of storage needed as well. Is that something of interest?Thanks,
-EmmettThanks @Emmett for your reply.Re organisms, I can only say mammalian for now. Re other processing steps, I’m not sure my colleague is interested in assembly so much as the other types you’ve mentioned. Thus, any software recommendations would be appreciated.Also, I suspect that the high accuracy requirement for running guppy_basecaller would prevent us from base calling in realtime, true?Your thoughts?You should be able to run the high accuracy basecaller in realtime for a MinION on the majority of our GPUs. Based on external benchmarks in [0] a Jetson Xavier AGX should be fast enough for real time basecalling.At the moment we release GPU accelerated alignment tools in Parabricks and GenomeWorks, but these are suited for x86 workstations so a traditional GPU (like a V100, A100, etc) would be the best. These libraries contain other common genomics pipeline tools accelerated for the GPU and are continuing to grow.[0] a collection of my notes while working on nanopore basecalling on the Jetson Xavier · GitHub-EmmettThanks. So for running basecalling as well as tools downstream from basecalling, would you recommend, say, A100 over the Jetson Xavier AGX? If so, what features for the x86 workstation?There unfortunately isn’t a one size fits all for the variety of workloads that exist in this space.A Jetson AGX would be able to basecall in realtime, which means you no longer need to store fast5 files, and can keep the significantly smaller fasta/fastq files around. This can have large bandwidth and storage savings.Some secondary analysis steps run well on a Jetson AGX, such as running Kraken2 with one of our reference docker images, while others require 8 GPUs or even cluster setups. I’d suggest reaching out on the Parabricks forums for more information about secondary analysis pipelines using x86.Hope this helps,
-EmmettI’m going to weigh in here if I may. Sorry, I don’t frequent these boards much (at all), but I have a lot of experience with Oxford Nanopore sequencing and GPU compute. First a collection of resources that may be useful to you:I have a document on selecting an appropriate GPU and compute set up for Nanopore data generation and analysis, I try to update this regularly: GPU musings (with an eye on genomics) - HackMDI am also very interested in finding a “sweet spot” in terms of price vs performance for GPUs when being used for basecalling. Here is a document where I have started some benchmarking and will keep updating with additional GPUs and information: GPU price / performance comparisons for Nanopore basecalling - HackMDIf you are interested in what it looks like if you run windows I have a quick note on that (spoiler: use Linux!): Nanopore Guppy GPU basecalling on Windows using WSL2 - HackMDFor the last 2-3 years it has been a project of mine getting Nanopore sequencing and software running on Nvidia Jetson devices (ARM based devices in general). This has been highly successful, and I maintain a GitHub repo with notes and instructions: GitHub - sirselim/jetson_nanopore_sequencing: A place to collate notes and resources of our journey into porting nanopore sequencing over to accessible, portable technology.If you are interested in using free GPUs in the cloud, we have a guide to do this using Google Colab: My notes on setting up basecalling on Google Colab · GitHubOK, so that’s an overview of some hopefully useful bits and bobs. I would now like to comment on a few things. Firstly, you really want to hold on to your fast5 files, doing so allows you to return to them again and again as models improve accuracy and allow the deteciton of additional base modifications. Real-time basecalling while sequencing is awesome, and something like an RTX3060 can keep up easily in high accuracy mode (HAC), even in super high accuracy (SUP) with some tweaking. But you will always want to go back and basecall again after the fact. The Jetson Xavier AGX will not keep up with HAC calling in real-time (we’ve tried), but in FAST mode it’s great, you can actually run 2x MinIONs. The Clara AGX could easily keep up, but it’s not widely available yet. While the A10 is an amazing card, I would advise to spend that amount of money on other things. Something like a 308Ti or a 3090 is more than enough for 95% of people (probably 99.9% of people to be honest).Sorry for the long post, happy to comment more if useful.-MilesThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
219,create-holoscan-application-diagram,"Hello,I would like to visualize our Holoscan pipeline using graphviz but I can’t manage to extract structural information from any Holoscan object.How did you create the pipeline diagrams used in the Holoscan user guide?This is a great point you’re bringing up! In v0.6 the access to the graph API in Python will be added to eliminate this error you’re seeing, and you could use something like networkx + matplotlib. We can provide an example on this after the v0.6 release.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
220,core-usage-with-fq2bam,"Is there any way to limit core usage with the fq2bam tool?  We’re testing parabricks 4.0.0-1 with a Singularity container (converted from Docker) on an academic cluster.  A typical GPU node might have 28-32 cores and 4 GPUs.  fq2bam can be limited to use 2 GPUs with the “–num-gpus” flag. It would be very useful to limit fq2bam to say 14 or 16 CPU cores so that other compute jobs can execute on the remaining GPUs.  The bwa step tends to use 18+ cores when it runs.Hey @bgregor,If you’re using Parabricks 4.0, you should be able to limit the cores in the Docker run command using --cpus or a similar flag. Does that solution work for you?ThanksPowered by Discourse, best viewed with JavaScript enabled"
221,nvidia-hackathon-at-bio-it-may-5-6-2022-boston-ma,"NVIDIA is hosting a Hackathon at the Bio-IT conference May 5-6 in Boston, MA and focusing on Adverse Events NLP in healthcare/life sciences, its a great opportunity to meet like minded developers and get some quality time with the NVIDIA Team.https://www.bio-itworldexpo.com/fair-data-hackathonPowered by Discourse, best viewed with JavaScript enabled"
222,error-in-inference-operator-sub-module-tick-inference-execution-message-error-in-inference-operator-sub-module-tick-data-extraction,"If you get an error like below while running your app:This is likely due to the InferenceOp’s specification of the input tensor name (whether in inference_map or in_tensor_names) not matching the output tensor name of the upstream operator.Powered by Discourse, best viewed with JavaScript enabled"
223,healthcare-use-cases,"Hi All,We are creating the health care use cases with the use of Pretrained model i.e. TRT-pose model. I would like to know is their any further model/configuration or business rules readily available within any NVIDIA suite which we can use to identify the the activities like walking, running, sitting, falling etc.?Thanks in advance!
Regards,
ChetanPowered by Discourse, best viewed with JavaScript enabled"
224,user-pass-ssh-no-longer-works-after-flashing-clara-agx,"I recently performed a flash and installation using the SDK manager on a Clara AGX. I had already setup a user/pass on the Clara prior to the flash, and I used these credentials to authorize the flash and installation. The flash and SDK installations seemed to all finished smoothly.The IP address of the Clara was changed after the flash, and I was able to verify the new IP using ifconfig. However, when I try to SSH to this new IP address using my previous user/pass credentials, it says “Permission denied”. The default ubuntu/ubuntu credentials don’t work either, and I’m wondering how I can still access the machine via SSH, as it seems like my previous user credentials were destroyed.Hello. After you re-flashed your devkit, did you set up the same user / password on it during flashing?When prompted, (e.g. in these steps: 
Screen Shot 2023-02-03 at 7.45.08 PM722×1290 149 KB

I entered the user/password that I had set up previously, but outside of that, I did not set up any new credentials. Was this meant to be done via the SDK manager?Since you checked the “User current username/password” in your first screenshot, the same should log you in on the same reflashed unit. ubuntu/ubuntu is only for the first flashing, not for any re-flashing after.This sounds like it might be a general case similar to verification - ssh remote host identification has changed - Stack Overflow, does it look familiar?Yes, it was the same issue - thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
225,megamolbart,"MegaMolBART is a model that understands chemistry and can be used for a variety of cheminformatics applications in drug discovery. The embeddings from its encoder can be used as features for predictive models. Alternatively, the encoder and decoder can be used together to generate novel molecules by sampling the model’s latent space.Learn MorePowered by Discourse, best viewed with JavaScript enabled"
226,monai-deploy-community,"Check out the MONAI Deploy community forum for support: Issues · Project-MONAI/monai-deploy-app-sdk · GitHubPowered by Discourse, best viewed with JavaScript enabled"
227,is-there-a-smith-waterman-implementation-available-in-parabricks,"Hi just wondered if there were CUDA implementations of the well known alignment algorithms, I thought they might be part of Parabricks?Powered by Discourse, best viewed with JavaScript enabled"
228,holosan-sdk-question,"I want to convert the output tensor stream to mp4 format, how should I do it？Hello! There may be two points of interest.You could see this application where it uses H.264 elementary stream from endoscopy sample data as input and the output of the pipeline is again recorded to a H.264 elementary stream on the disk holohub/applications/h264_endoscopy_tool_tracking at main · nvidia-holoscan/holohub · GitHubYou could also see the application holohub/applications/endoscopy_tool_tracking at main · nvidia-holoscan/holohub · GitHub documented here Holoscan Sample Applications - NVIDIA Docs for the use of Video Stream Recorder, which will record to the tensor format. After saving to tensor format, you can modify the script holoscan-sdk/convert_video_to_gxf_entities.py at main · nvidia-holoscan/holoscan-sdk · GitHub to convert a tensor entity to mp4 format. (This script is originally designed for converting a mp4 to a tensor entity for being streamed in the video replayer) The modification is coming soon to the future releases.Powered by Discourse, best viewed with JavaScript enabled"
229,clara-agx-could-not-detect-a-board,"Unable to recognize Clara Agx ？Clara AGX Show: Could not detect a board
Why can’t the device connect to the sdk manager
Unable to flash, the system can still enter
I have also tried recovery mode and may not have operated correctly
Do you have a tutorial on initializing devices1231424×918 168 KB
471375×893 76.5 KBHi there, can you confirm that this is a Clara AGX developer kit you’re flashing? The steps under “Follow these steps to reset the Clara AGX Developer Kit” should put the unit into recovery mode for Manual Setup in SDKM.  holoscan-docs/devkits/clara-agx/clara_agx_user_guide.md at main · nvidia-holoscan/holoscan-docs · GitHubHi ,
Perhaps some configurations have been deleted,
and the icon on the hard drive seems different from before.
It may be due to a mistake when using disks.
I want to reinstall the system. Is there any way?
11706×1280 182 KB
21706×1280 139 KBHi there, yes you can reflash your system by following the same user guide holoscan-docs/devkits/clara-agx/clara_agx_user_guide.md at main · nvidia-holoscan/holoscan-docs · GitHubI have already read the document. My problem is that the physical machine cannot recognize Clara AGX, but it can recognize the core board Jetson AGX Xavier. My device is clearly Clara AGX. Previously, it could be identified as Clara AGX
Screenshot from 2023-07-03 15-35-381920×1080 99.4 KB
Screenshot from 2023-07-03 15-35-541920×1080 262 KBOh I see, it’s likely that this may be from the developer account signed-in to SDK Manager not being registered in the Holoscan developer program.  Could you confirm you have done step 1?ok,  I tryThank you, I succeededPowered by Discourse, best viewed with JavaScript enabled"
230,vk-error-extension-not-present-while-trying-to-run-the-video-replayer-example,"Hello,I try to run the video_replayer example inside of the Docker container holoscan:v0.5.1-dgpu on my laptop.
But I have the following error:
2023-06-16 13:44:36.719] [holoscan] [info] [context.cpp:47] Vulkan Version:
[2023-06-16 13:44:36.719] [holoscan] [info] [context.cpp:47]  - available:  1.2.131
[2023-06-16 13:44:36.719] [holoscan] [info] [context.cpp:47]  - requesting: 1.2.0
[2023-06-16 13:44:36.720] [holoscan] [warning] [context.cpp:50] VK_ERROR_EXTENSION_NOT_PRESENT: VK_KHR_external_memory_capabilities - 0
terminate called after throwing an instance of ‘std::runtime_error’
what():  Failed to create the Vulkan instance.
Aborted (core dumped)I tried to implement “nvidia_icd_json=$(find /usr/share /etc -path ‘*/vulkan/icd.d/nvidia_icd.json’ -type f 2>/dev/null | grep .) || (echo “nvidia_icd.json not found” >&2 && false)” command before I run this container (it was adviced in a one topic on this forum) but it didn’t help.On my laptop I have: GPU Quadro T1000
NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6Could you please tell me what I am doing wrong?With the best wishes,
ValeriyHi Valeriy, could you show us the result ofIt gives an empty output both on my laptop and inside of the Docker container.Could you try passing in three env vars -e DRI_PRIME=1 -e  __NV_PRIME_RENDER_OFFLOAD=1 -e   __GLX_VENDOR_LIBRARY_NAME=nvidia to your docker run command?A correction: the env variable may need to be set outside of the docker container, rather than set inside the container via -e.Thank you very much!Powered by Discourse, best viewed with JavaScript enabled"
231,parabrick-deepvariant-run-error,"When I run deepvariant on HPC 8GPUs by a docker container limited to 6 gpus. It shows the error

image1352×415 65.6 KB
Can you help me?Thanks for your interest in Parabricks and sorry for your troubles. Although there are some hints to this in the snapshot you provide; but could you please provide a summary of the Driver and CUDA versions (perhaps the output of nvidia-smi). Also, can you share the hardware configuration of the server in question (GPU type, memory etc)?thank you for your replay. I attach the driver and cuda version here. My HPC server is HPE Appolo Gen 10 which uses 8 GPUs x 32GB, 512GB RAM and installed ubuntu1804. I install parabricks v2.4.2 and v2.5.0 by docker. But both of them show same error. Cần you @aquraini help me?
image740×552 73.7 KB
@dongoctuan.0101 there is a message asking you to check if you have enough disk space in the output and temp (/tmp y default) directories. Can you confirm that you are not running out of disk space?@mvella I am sure. Output will written to NFS 13TB. I think the problem is cudnn. Maybe it can not initialize cudnn.Powered by Discourse, best viewed with JavaScript enabled"
232,mutectcaller-with-tumor-normal-pair-returned-only-haeder-vcf,"Hi there,I’m testing the somatic pipeline, both tumor only and tumor-normal pair. While the tumor only gave me results, the tumor-normal pair returned me a vcf that contain only the header.
There’s something I could do?
Thanks in advice.Hi Stefano,can you please provide the command line you are using?This happens when sample or read group ID is the same for normal and tumor bamBest
MyriemeHi Myrieme,Ok first I used the somatic pipeline command line:time ./parabricks_install/parabricks/pbrun somatic 
–ref /hpcshare/genomics/references/gatk_bundle/reference/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta 
–in-tumor-fq /hpcshare/smarangoni/pbrun_file_io/PD_twins/SG_PD-7107_B/PD-7107_S4_R1_001.fastq.gz /hpcshare/smarangoni/pbrun_file_io/PD_twins/SG_PD-7107_B/PD-7107_S4_R2_001.fastq.gz 
–knownSites /hpcshare/genomics/references/gatk_bundle/resources/resources_broad_hg38_v0_hapmap_3.3.hg38.vcf.gz 
–out-tumor-bam /hpcshare/smarangoni/pbrun_file_io/PD_twins/PD-7107_B.bam 
–out-vcf /hpcshare/smarangoni/pbrun_file_io/PD_twins/PD-7107.vcf 
–out-tumor-recal-file /hpcshare/smarangoni/pbrun_file_io/PD_twins/PD-7107_B_recal.txt 
–in-normal-fq /hpcshare/smarangoni/pbrun_file_io/PD_twins/SG_PD-7107_P/PD-7108_S6_R1_001.fastq.gz /hpcshare/smarangoni/pbrun_file_io/PD_twins/SG_PD-7107_P/PD-7108_S6_R2_001.fastq.gz 
–out-normal-bam /hpcshare/smarangoni/pbrun_file_io/PD_twins/PD-7107_P.bam 
–out-normal-recal-file /hpcshare/smarangoni/pbrun_file_io/PD_twins/PD-7107_P_recal.txtThen I also used the mutectcaller tool using this command line:time ./parabricks_install/parabricks/pbrun mutectcaller 
–ref /hpcshare/genomics/references/gatk_bundle/reference/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta 
–in-normal-bam /hpcshare/smarangoni/pbrun_file_io/PD_twins/PD-7107_P.bam 
–in-normal-recal-file /hpcshare/smarangoni/pbrun_file_io/PD_twins/PD-7107_P_recal.txt 
–out-vcf /hpcshare/smarangoni/pbrun_file_io/PD_twins/PD-7107.vcf 
–normal-name normal 
–tumor-name tumor 
–in-tumor-bam /hpcshare/smarangoni/pbrun_file_io/PD_twins/PD-7107_B.bam 
–in-tumor-recal-file /hpcshare/smarangoni/pbrun_file_io/PD_twins/PD-7107_B_recal.txt.Powered by Discourse, best viewed with JavaScript enabled"
233,could-not-run-fq2bam-when-try-align-a-sequence,"hi,
I have UBUNTU 20.04 and two Gpus P40 .Getting the following error below when running parabricks with mode pbrun ""f2qbam. ""
I am using the parabricks container on singularity and testing with the files that i got in nvidia tutorials , Homo_sapiens_assembly38.fasta#Calling the singularity run with --nv parameter
bin/singularity run --nv parabricks/clara-parabricks_4.0.1-1.sif pbrun fq2bam --num-gpus 2 --ref ~/local/src/singularity-ce/parabricks/parabricks_sample/Ref/Homo_sapiens_assembly38.fasta --in-fq ~/local/src/singularity-ce/parabricks/parabricks_sample/Data/sample_1.fq.gz ~/local/src/singularity-ce/parabricks/parabricks_sample/Data/sample_2.fq.gz --out-bam ~/local/src/singularity-ce/parabricks/outputdir/fq2bam_output.bamAlways  on the same error mensage :[Parabricks Options Mesg]: Checking argument compatibility
[Parabricks Options Mesg]: Automatically generating ID prefix
[Parabricks Options Mesg]: Read group created for /home/danilo_barboza/local/src/singularity-
ce/parabricks/parabricks_sample/Data/sample_1.fq.gz and /home/danilo_barboza/local/src/singularity-
ce/parabricks/parabricks_sample/Data/sample_2.fq.gz
[Parabricks Options Mesg]: @RG\tID:HK3TJBCX2.1\tLB:lib1\tPL:bar\tSM:sample\tPU:HK3TJBCX2.1
g 1 b 0 B 1 P 8 s 1 r 0 o 2 m 1 z 4 f 2 v 0 M 2 X 0 name /home/danilo_barboza/local/src/singularity-ce/parabricks/outputdir/fq2bam_output.bam
/usr/local/parabricks/binaries//bin/bwa mem /home/danilo_barboza/local/src/singularity-ce/parabricks/parabricks_sample/Ref/Homo_sapiens_assembly38.fasta /home/danilo_barboza/local/src/singularity-ce/parabricks/parabricks_sample/Data/sample_1.fq.gz /home/danilo_barboza/local/src/singularity-ce/parabricks/parabricks_sample/Data/sample_2.fq.gz @RG\tID:HK3TJBCX2.1\tLB:lib1\tPL:bar\tSM:sample\tPU:HK3TJBCX2.1 -Z ./pbOpts.txt
[PB Info 2023-Mar-22 22:52:43] ------------------------------------------------------------------------------
[PB Info 2023-Mar-22 22:52:43] ||                 Parabricks accelerated Genomics Pipeline                 ||
[PB Info 2023-Mar-22 22:52:43] ||                              Version 4.0.1-1                             ||
[PB Info 2023-Mar-22 22:52:43] ||                       GPU-BWA mem, Sorting Phase-I                       ||
[PB Info 2023-Mar-22 22:52:43] ------------------------------------------------------------------------------
[M::bwa_idx_load_from_disk] read 0 ALT contigs
[PB Info 2023-Mar-22 22:53:03] GPU-BWA mem
[PB Info 2023-Mar-22 22:53:03] ProgressMeter	Reads		Base Pairs Aligned
For technical support visit Help - NVIDIA Docs
Exiting…Could not run fq2bam
Exiting pbrun …
INFO:    Cleaning up image…Memory
$ cat /proc/meminfo
MemTotal:       1056256480 kB
MemFree:        47753832 kB
MemAvailable:   993996072 kB
Buffers:         3202116 kB
Cached:         933634784 kB
SwapCached:      4841092 kBCPU
[sudo] password for danilo_barboza:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          224
On-line CPU(s) list:             0-223
Thread(s) per core:              2
Core(s) per socket:              28
Socket(s):                       4
NUMA node(s):                    4
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           85GPUS±----------------------------------------------------------------------------+
| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |
|-------------------------------±---------------------±---------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla P40           Off  | 00000000:25:00.0 Off |                    0 |
| N/A   49C    P0   114W / 250W |   2342MiB / 23040MiB |     79%      Default |
|                               |                      |                  N/A |
±------------------------------±---------------------±---------------------+
|   1  Tesla P40           Off  | 00000000:5B:00.0 Off |                    0 |
| N/A   22C    P8     9W / 250W |      2MiB / 23040MiB |      0%      Default |
|                               |                      |                  N/A |
±------------------------------±---------------------±---------------------+±----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A   1624076      C   java                             2340MiB |Please helppPowered by Discourse, best viewed with JavaScript enabled"
234,gtc-healthcare-and-developer-life-sciences-summit-march-2023,"Join us for NVIDIA GTC March 20-23. It’s all virtual and free to attend.As always, the cornerstone of GTC is the keynote by NVIDIA CEO and founder Jensen Huang that spotlights all our latest breakthroughs. It will be live on March 21 at 8 a.m. PDT (4 p.m. CET) and doesn’t require registration, so don’t miss it.For all you developers out there, check out these  technical talks, demos, and hands-on workshops on applications and frameworks advancing AI in healthcare. Healthcare Conference Session Catalog | NVIDIA GTCPowered by Discourse, best viewed with JavaScript enabled"
235,run-2-apps-on-2-holoscan-docker-images-and-make-connection-between-ports,"I’m developing an app using holoscan sdk. I want to run 2 docker images which will be running 2 apps. But one output port of one operator from first holoscan image should be connected to one input port of one operator from second holoscan image. Is it possible? if yes then how?Great question. Assuming from your question that the 2 apps are both Holoscan apps, in our next release there will be a new distributed feature that will allow us to run one Holoscan app across different nodes, and you can construct your two current apps as two fragments of the same app. We can revisit this post once the next release comes out for more details.Thanks @jinl for the reply.
In my application there are several operators. In one version of my app the first operator generates data and passes it on to the next operator and the cycle continues. But in the new version of my app I don’t want to generate data in the first operator. I want a way so that my first operator can receive data on it’s input port buffer from an external source (External source doesn’t necessarily need to be holoscan app running in a container).
Is there any way to do this in the current version of holoscan?Could you tell us more on what the intended data format is that you’d want to receive in the first operator?
We could see the Basic Networking Ping application and the Advanced Networking Benchmark application for how to use network operators.
If you want your first operator to receive sensor data, HoloHub Applications also include examples of using video capture cards (AJA and Deltacast), USB or HDMI video source, USB-based RTL-SDR dongle, etc.@jinl My intended data format is actually numpy array. I want my first operator to wait untill it receives some data on it’s input port buffer (I’m hoping that some external source would be able to put the numpy array on that buffer in some way). I’m actually developing the app in python. So it would be really helpful if you could share some information or example code implemented in python.Unfortunately there aren’t exact examples we can share at this moment. If you’d like your first native Python op to read in data from a buffer that another Python process writes to, it would not be a Holoscan SDK-specific implementation. It would be the same as two non-Holoscan applications with one writing to and one reading from a buffer.You could still refer to the two applications using network operators above.If some external source is sending the data, it could be via some IPC mechanisms, either unix pipes, named pipes, or unix domain sockets. There are likely python equivalents for these.Powered by Discourse, best viewed with JavaScript enabled"
236,error-in-germline-pipeline-argument-ref-invalid-isfilereadable,"When using pbrun germline I always end up with the follwing error:
run_pipeline.py: error: argument --ref: invalid IsFileReadable value: '/data/Parabricks/parabricks_sample/Ref/Homo_sapiens_assembly38.fasta'The file is perfectly accessible. I can’t explain where the error comes from.
Kind regards,
ThomasHey @cts1,Can you paste the exact command you’re running that yields this error?Is this resolved? Otherwise @cts1 try unzipping the gz-fies. Worked for me.Powered by Discourse, best viewed with JavaScript enabled"
237,can-i-install-clara-holoscan-sdk-on-the-jetson-agx-xavier-developer-kit,"According to Install Clara Holoscan Software :: NVIDIA SDK Manager Documentation , for the Clara Holoscan SDK, Jetson AGX Xavier modules and the Clara AGX Developer Kit are the only hardware supported.
And according to the introduction of [Advanced Computation for AI-Powered Medical Devices | NVIDIA ] the NVIDIA Clara Holoscan developer kit is built with a NVIDIA AGX Orin™ module.
So, is it possible to install Clara Holoscan SDK on the Jetson AGX Xavier Developer Kit? I hope to use the existing hardware to play with the Clara holoscan SDK.
Thanks a lot!Hi there, at the moment, the Clara Holoscan SDK is compatible with Clara AGX Developer Kit, NVIDIA IGX Orin Developer Kit and x86 workstations, and it is not tested on Jetson AGX Xavier. Build AI for Medical Devices | NVIDIA Clara Holoscan SDK
We’d love to learn about which aspects in the Clara Holoscan SDK are interesting to you and the applications (if any) you have in mind.How i do setup on x86 workstations.The process for setting up on x86 workstations should be similar to setting up on the devkits. Clara Holoscan Sample Applications | NVIDIA NGC and the README from GitHub - NVIDIA/clara-holoscan-embedded-sdk: AI computing SDK for medical devices with low latency streaming workflows still apply.I have Jetson AGX Xavier SDK  and of Nvidia Titan RTX GPU, Can i able to do setup on these configurations.Assembling a Jetson AGX Xavier module and a discrete GPU outside of the configuration of a Clara devkit is not currently tested with the Clara Holoscan SDK. But you could still try out the SDK on a x86 workstation.Powered by Discourse, best viewed with JavaScript enabled"
238,starting-to-use-federated-learning-and-clara-for-medical-imgaes,"Hi,I am new for using Clara toolbox for federated learning and I want the steps for installation and using this application for training DL models . I installed on my PC Desktop ubuntu 18.04Looking for your supportPowered by Discourse, best viewed with JavaScript enabled"
239,haplotypecaller-v3-5-annotation-group-option,"Does annotation-group option can be repeated multiple times?parabricks accept multiple annotation-groupPowered by Discourse, best viewed with JavaScript enabled"
